[
    {
        "paper_title": "Reinforcement learning-based register renaming policy for simultaneous multithreading CPUs[Formula presented]",
        "paper_author": "Zhan H.",
        "publication": "Expert Systems with Applications",
        "citied_by": "1",
        "cover_date": "2021-12-30",
        "Abstract": "Simultaneous multithreading (SMT) improves the performance of superscalar CPUs by exploiting thread-level parallelism with shared entries for better utilization of resources. A key issue for this out-of-order execution is that the occupancy latency of a physical rename register can be undesirably long due to many program execution-dependent factors that result in performance degradation. Such an issue becomes even more problematic in an SMT environment in which these registers are shared among concurrently running threads. Smartly managing this critical shared resource to ensure that slower threads do not block faster threads’ execution is essential to the advancement of SMT performance. In this paper, an actor–critic style reinforcement learning (RL) algorithm is proposed to dynamically assigning an upper-bound (cap) of the rename registers any thread is allowed to use according to the threads’ real-time demand. In particular, a critic network projects the current Issue Queues (IQ) usage, register file usage, and the cap value to a reward; an actor network is trained to project the current IQ usage and register file usage to the optimal real-time cap value via ascending the instructions per cycle (IPC) gradient within the trajectory distribution. The proposed method differs from the state-of-the-art (Wang and Lin, 2018) as the cap for the rename registers for each thread is adjusted in real-time according to the policy and state transition from self-play. The proposed method shows an improvement in IPC up to 162.8% in a 4-threaded system, 154.8% in a 6-threaded system and up to 101.7% in an 8-threaded system. The code is now available open source at https://github.com/98k-bot/RL-based-SMT-Register-Renaming-Policy.",
        "DOI": "10.1016/j.eswa.2021.115717",
        "affiliation_name": "Edward E. Whitacre Jr. College of Engineering",
        "affiliation_city": "Lubbock",
        "affiliation_country": "United States",
        "affiliation_id": "60149446",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "An approach to managing innovation to protect financial sector against cybercrime",
        "paper_author": "Kuzmenko O.V.",
        "publication": "Polish Journal of Management Studies",
        "citied_by": "11",
        "cover_date": "2021-12-29",
        "Abstract": "Ensuring the cyber security management is an ever-increasing challenge for the financial institutions and the national financial regulators. The main purpose of the research is to improve cyber security management through analyzing large data volumes of information which helps to identify potential cyber threats at an early stage. The factors of the rapid cybercrime growth via supervised learning models with associated learning (SVM) were identified and evaluated in the paper. The object of research is 21 EU countries. The paper presents the results of an empirical analysis, which showed that the cyber threats are caused by the growth of using online banking (0.49), improvement of internet user skills (0.42), expansion of activities online (0.41). The results of the research can be useful for financial institutions, national regulators and cybersecurity professionals.",
        "DOI": "10.17512/pjms.2021.24.2.17",
        "affiliation_name": "Hungarian University of Agriculture and Life Sciences",
        "affiliation_city": "Godollo",
        "affiliation_country": "Hungary",
        "affiliation_id": "60026055",
        "affiliation_state": "Pest"
    },
    {
        "paper_title": "A study on smart city research activity using bibliometric and natural language processing methods",
        "paper_author": "Wang J.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "3",
        "cover_date": "2021-12-22",
        "Abstract": "Smart cities have become a new urban development paradigm and draw much interest from the research community and society. Based on academic publications of smart city-related research, this study employs bibliometrics, natural language machine learning methods to analyze 10,000 papers indexed by Web of Science from 2009 to 2020. Bibliometrics results show that: (1) A total of 114 countries or regions worldwide have participated in smart city research, and China is the country with the highest amount of participation in the field of smart cities. (2) Smart city research has gone through three stages: the initial stage (2009-2012), the in-depth advancement stage (2013-2016), and the leap-up stage (2017-2020). Researchers paid more attention to urban attractiveness indicators such as sustainability in the early stage. In the later period, most of the research topics were clustered on improving the overall function of the city. Latent Dirichlet Allocation (LDA) topic model results revealed that research topics could be categorized into five aspects: policy research on the status quo of smart cities, data analysis and application, infrastructure construction, urban governance, and network security. Current research on smart city technologies mainly focuses on theoretical systems, technologies, and application fields. There is a lack of in-depth research and exploration in long-term construction and operation mechanisms. This research provides insight into the research status of smart city technologies and helps researchers decide on future study direction.",
        "DOI": "10.1145/3512576.3512638",
        "affiliation_name": "Chinese Academy of Agricultural Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60017705",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Revisiting academic health sciences systems a decade later: discovery to health to population to society",
        "paper_author": "Dzau V.J.",
        "publication": "The Lancet",
        "citied_by": "22",
        "cover_date": "2021-12-18",
        "Abstract": "NA",
        "DOI": "10.1016/S0140-6736(21)01752-9",
        "affiliation_name": "Emory Healthcare",
        "affiliation_city": "Atlanta",
        "affiliation_country": "United States",
        "affiliation_id": "60012837",
        "affiliation_state": "GA"
    },
    {
        "paper_title": "A Machine Learning Approach to Predict Groundwater Levels in California Reveals Ecosystems at Risk",
        "paper_author": "Rohde M.M.",
        "publication": "Frontiers in Earth Science",
        "citied_by": "19",
        "cover_date": "2021-12-17",
        "Abstract": "Groundwater dependent ecosystems (GDEs) are increasingly threatened worldwide, but the shallow groundwater resources that they are reliant upon are seldom monitored. In this study, we used satellite-based remote sensing to predict groundwater levels under groundwater dependent ecosystems across California, USA. Depth to groundwater was modelled for a 35-years period (1985–2019) within all groundwater dependent ecosystems across the state (n = 95,135). Our model was developed within Google Earth Engine using Landsat satellite imagery, climate data, and field-based groundwater data [n = 627 shallow (< 30 m) monitoring wells] as predictors in a Random Forest model. Our findings show that 1) 44% of groundwater dependent ecosystems have experienced a significant long-term (1985–2019) decline in groundwater levels compared to 28% with a significant increase; 2) groundwater level declines have intensified during the most recent two decades, with 39% of groundwater dependent ecosystems experiencing declines in the 2003–2019 period compared to 27% in the 1985–2002 period; and 3) groundwater declines are most prevalent within GDEs existing in areas of the state where sustainable groundwater management is absent. Our results indicate that declining shallow groundwater levels may be adversely impacting California’s groundwater dependent ecosystems. Particularly where groundwater levels have fallen beneath plant roots or streams thereby affecting key life processes, such as forest recruitment/succession, or hydrological processes, such as streamflow that affects aquatic habitat. In the absence of groundwater monitoring well data, our model and findings can be used to help state and local water agencies fill in data gaps of shallow groundwater conditions, evaluate potential effects on GDEs, and improve sustainable groundwater management policy in California.",
        "DOI": "10.3389/feart.2021.784499",
        "affiliation_name": "Nature Conservancy",
        "affiliation_city": "Arlington",
        "affiliation_country": "United States",
        "affiliation_id": "60017383",
        "affiliation_state": "VA"
    },
    {
        "paper_title": "Enhance load forecastability: Optimize data sampling policy by reinforcing user behaviors",
        "paper_author": "Xie G.",
        "publication": "European Journal of Operational Research",
        "citied_by": "5",
        "cover_date": "2021-12-16",
        "Abstract": "Load forecasting has long been a key task for reliable power systems planning and operation. Over the recent years, advanced metering infrastructure has proliferated in industry. This has given rise to many load forecasting methods based on frequent measurements of power states obtained by smart meters. Meanwhile, real-world constraints arising in this new setting present both challenges and opportunities to achieve high load forecastability. The bandwidth constraints often imposed on the transmission between data concentrators and utilities are one of them, which limit the amount of data that can be sampled from customers. There lacks a sampling-rate control policy that is self-adaptive to users’ load behaviors through online data interaction with the smart grid environment. In this paper, we formulate the bandwidth-constrained sampling-rate control problem as a Markov decision process (MDP) and provide a reinforcement learning (RL)-based algorithm to solve the MDP for an optimal sampling-rate control policy. The resulting policy can be updated in real time to accommodate volatile load behaviors observed in the smart grid. Numerical experiments show that the proposed RL-based algorithm outperforms competing algorithms and delivers superior predictive performance.",
        "DOI": "10.1016/j.ejor.2021.03.032",
        "affiliation_name": "Virginia Tech College of Engineering",
        "affiliation_city": "Blacksburg",
        "affiliation_country": "United States",
        "affiliation_id": "60157272",
        "affiliation_state": "VA"
    },
    {
        "paper_title": "Neural networks with disabilities: An introduction to complementary artificial intelligence",
        "paper_author": "Terziyan V.",
        "publication": "Neural Computation",
        "citied_by": "7",
        "cover_date": "2021-12-15",
        "Abstract": "Machine learning is a good tool to simulate human cognitive skills as it is about mapping perceived information to various labels or action choices, aiming at optimal behavior policies for a human or an artificial agent operating in the environment. Regarding autonomous systems, objects and situations are perceived by some receptors as divided between sensors. Reactions to the input (e.g., actions) are distributed among the particular capability providers or actuators. Cognitive models can be trained as, for example, neural networks. We suggest training such models for cases of potential disabilities. Disability can be either the absence of one or more cognitive sensors or actuators at different levels of cognitive model. We adapt several neural network architectures to simulate various cognitive disabilities. The idea has been triggered by the “coolability” (enhanced capability) paradox, according to which a person with some disability can be more efficient in using other capabilities. Therefore, an autonomous system (human or artificial) pretrained with simulated disabilities will be more efficient when acting in adversarial conditions. We consider these coolabilities as complementary artificial intelligence and argue on the usefulness if this concept for various applications.",
        "DOI": "10.1162/neco_a_01449",
        "affiliation_name": "University of Jyväskylä",
        "affiliation_city": "Jyvaskyla",
        "affiliation_country": "Finland",
        "affiliation_id": "60032398",
        "affiliation_state": "Central Finland"
    },
    {
        "paper_title": "A comparative analysis of Statistical and Computational Intelligence methodologies for the prediction of traffic-induced fine particulate matter and NO<inf>2</inf>",
        "paper_author": "Kokkinos K.",
        "publication": "Journal of Cleaner Production",
        "citied_by": "15",
        "cover_date": "2021-12-15",
        "Abstract": "With the urbanization increase, urban mobility and transportation induce higher traffic volumes causing environmental, economic and social impacts. This is due to continuous usage of fossil fuel energy resources generating air pollutants, such as nitrogen oxides (NOx), sulfur dioxide (SO2), carbon monoxide (CO), ozone (O3) and particulate matter (PM10 and PM2.5), which impact on climate and air quality and adversely affect the human health. The present paper aims at training an ensemble of forecasting methodologies for traffic-induced pollutant emissions and implementing it for predicting PM10, PM2.5 and NO2 for the case study of Cambridge, UK inner-city region. Such an ensemble enables decision makers to evaluate the impact of various transportation policies and measures on human health and the ecosystem, and subsequently contribute towards urban resilience and sustainability. Since the chemical synthesis of air pollution is triggered by meteorological factors, the forecasting incorporates them along with the traffic volumes. We opted to combine Statistical and Computational Intelligence learning methods including Adaptive Neuro Fuzzy Inference Systems (ANFIS), Long Short-Term Memory (LSTM) recurrent neural networks and Extreme Learning Machines (ELM). Initially, Multivariate Imputation by Chained Equation (MICE) and trend and seasonality removal was performed at data preprocessing and then Principal Component Analysis (PCA) highlighted the principal parameters for ANFIS to predict next day's PM10, PM2.5 and NO2 values. LSTM and ELM methods estimated next day values and compared with the ANFIS model results for hourly time series data of length 2703. The performance of the embedded models was quantified by the Root Mean Square Error (RMSE), Mean Absolute Error (MAE), Mean Square Error (MSE), and Coefficient of Determination (R2) indices. The ensemble was found to be superior in predicting PM10, PM2.5 and NO2 emissions when compared with existing traditional models.",
        "DOI": "10.1016/j.jclepro.2021.129500",
        "affiliation_name": "University of Thessaly",
        "affiliation_city": "Volos",
        "affiliation_country": "Greece",
        "affiliation_id": "60025812",
        "affiliation_state": "Thessaly"
    },
    {
        "paper_title": "Integrating satellite-derived climatic and vegetation indices to predict smallholder maize yield using deep learning",
        "paper_author": "Zhang L.",
        "publication": "Agricultural and Forest Meteorology",
        "citied_by": "79",
        "cover_date": "2021-12-15",
        "Abstract": "Timely and accurately estimating smallholder crop yield is essential for optimizing agronomic management, guiding investment and policy-making to reduce poverty and improve food security. However, the productivity of most smallholder farms around the world is still poorly estimated due to absence of technologies and field-scale data, especially for the largest smallholders across China. Here, we integrated 11,857 of field-surveyed yields across maize cultivation areas in China and heterogeneous geospatial data to predict field-level maize yield using three data-driven approaches, i.e., Least Absolute Shrinkage and Selection Operator (LASSO), Light Gradient Boosting Machine (LightGBM) and Long Short-Term Memory (LSTM). We determined the most suitable vegetation index (VI), compared the performances of satellite-derived and ground-observed climate data, and identified the optimal combination of input variables and the best method for maize yield estimation. We found that the green chlorophyll vegetation index (GCVI) outperformed the traditional visible and near-infrared-based VIs. Combining Land Surface Temperature (LST), cumulative precipitation (Pgs) and standardized precipitation index (SPI) explained about 70% of the yield variation, which was comparable to ground-observed indices. Integrating GCVI improved R2 by 0.01–0.20 depending on the methods, suggesting valuable information on biotic or abiotic stress contained by GCVI. Maize yield could be predicted 1–2-month ahead before harvesting in all agro-ecological zones. The data collected during silking period contributed more information. The LSTM model did better than LightGBM and LASSO, because of its neural network characterizing the cumulative effects of environmental factors on yield. Our study demonstrates an effective mean for large-scale crop yield estimation using publicly available data, particularly for smallholder systems where ground observations are currently limited and sparse.",
        "DOI": "10.1016/j.agrformet.2021.108666",
        "affiliation_name": "Ministry of Agriculture of the People's Republic of China",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60087826",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Urban soils as a spatial indicator of quality for urban socio-ecological systems",
        "paper_author": "Bonilla-Bedoya S.",
        "publication": "Journal of Environmental Management",
        "citied_by": "8",
        "cover_date": "2021-12-15",
        "Abstract": "The development of criteria and indicators to quantify the transition to sustainability of the urban socio-ecological systems quality is determinant for planning policies and the 21st century urban agenda. This study models the spatial variation in the concentration and distribution of some macronutrients, micronutrients, and trace nutrients in the soil of a high-altitude city in the Andes. Meanwhile, machine learning methods were employed to study some interactions between the different dimensions that constitute an urban socio-ecosystem that caused these variations. We proposed a methodology that considered two phases: a) field work to collect data on 300 soil samples; laboratory analysis to measure the concentrations of 24 macronutrients, micronutrients, and trace nutrients; and the design of geophysical, spectral, and urban co-variables; b) statistical and geo-informatics analysis, where multivariate analysis grouped the elements into factors; and, machine learning integrated with co-variables was applied to derive the intensity of each factor across the city. Multivariate statistics described the variation in soil co-concentrations with a moderate percentage (42%). Four factors were determined that grouped some of the analyzed elements, as follows: F1 (Zn, S, Cu, Pb, Ni, and Cr), F2 (Ba, Ag, K, In, and Mg), F3 (B, V, Li, and Sr), and F4 (Si and Mn). The percentage R2 out-of-bag of the spatial model were: F1 = 20%, F2 = 8%, F3 = 14%, and F4 = 10%. Our outputs show that the enrichment and contamination by anthropogenic factors, such as the increase in population density, land use, road network, and traffic generated by fossil fuel vehicles, should be prioritized in urban planning decisions.",
        "DOI": "10.1016/j.jenvman.2021.113556",
        "affiliation_name": "Universidad Regional Amazónica Ikiam",
        "affiliation_city": "Tena",
        "affiliation_country": "Ecuador",
        "affiliation_id": "60114072",
        "affiliation_state": "Napo"
    },
    {
        "paper_title": "Quantum Reinforcement Learning Applied to Board Games",
        "paper_author": "Teixeira M.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2021-12-14",
        "Abstract": "Reinforcement learning is a machine learning paradigm where an agent learns how to optimize its behavior solely through its interaction with the environment. It has been extensively studied and successfully applied to complex problems of many different domains in the past decades, i.e., robotics, games, scheduling. However, the performance of these algorithms becomes limited as the complexity and dimension of the state-action space increases. Recent advances in quantum computing and quantum information have sparked interest in possible applications to machine learning. By taking advantage of quantum mechanics, it is possible to efficiently process immense quantities of information and improve computational speed. In this work, we combined quantum computing with reinforcement learning and studied its application to a board game to assess the benefits that it can introduce, namely its impact on the learning efficiency of an agent. From the results, we concluded that the proposed quantum exploration policy improved the convergence rate of the agent and promoted a more efficient exploration of the state space.",
        "DOI": "10.1145/3486622.3493944",
        "affiliation_name": "Universidade do Porto",
        "affiliation_city": "Porto",
        "affiliation_country": "Portugal",
        "affiliation_id": "60007249",
        "affiliation_state": "Porto"
    },
    {
        "paper_title": "Tunable Neural Encoding of a Symbolic Robotic Manipulation Algorithm",
        "paper_author": "Katz G.E.",
        "publication": "Frontiers in Neurorobotics",
        "citied_by": "2",
        "cover_date": "2021-12-14",
        "Abstract": "We present a neurocomputational controller for robotic manipulation based on the recently developed “neural virtual machine” (NVM). The NVM is a purely neural recurrent architecture that emulates a Turing-complete, purely symbolic virtual machine. We program the NVM with a symbolic algorithm that solves blocks-world restacking problems, and execute it in a robotic simulation environment. Our results show that the NVM-based controller can faithfully replicate the execution traces and performance levels of a traditional non-neural program executing the same restacking procedure. Moreover, after programming the NVM, the neurocomputational encodings of symbolic block stacking knowledge can be fine-tuned to further improve performance, by applying reinforcement learning to the underlying neural architecture.",
        "DOI": "10.3389/fnbot.2021.744031",
        "affiliation_name": "Department of Computer Science",
        "affiliation_city": "College Park",
        "affiliation_country": "United States",
        "affiliation_id": "60151568",
        "affiliation_state": "MD"
    },
    {
        "paper_title": "Artificial Intelligence and Machine Learning in Sport Research: An Introduction for Non-data Scientists",
        "paper_author": "Chmait N.",
        "publication": "Frontiers in Sports and Active Living",
        "citied_by": "50",
        "cover_date": "2021-12-08",
        "Abstract": "In the last two decades, artificial intelligence (AI) has transformed the way in which we consume and analyse sports. The role of AI in improving decision-making and forecasting in sports, amongst many other advantages, is rapidly expanding and gaining more attention in both the academic sector and the industry. Nonetheless, for many sports audiences, professionals and policy makers, who are not particularly au courant or experts in AI, the connexion between artificial intelligence and sports remains fuzzy. Likewise, for many, the motivations for adopting a machine learning (ML) paradigm in sports analytics are still either faint or unclear. In this perspective paper, we present a high-level, non-technical, overview of the machine learning paradigm that motivates its potential for enhancing sports (performance and business) analytics. We provide a summary of some relevant research literature on the areas in which artificial intelligence and machine learning have been applied to the sports industry and in sport research. Finally, we present some hypothetical scenarios of how AI and ML could shape the future of sports.",
        "DOI": "10.3389/fspor.2021.682287",
        "affiliation_name": "Victoria University Melbourne, Institute for Health and Sport",
        "affiliation_city": "Melbourne",
        "affiliation_country": "Australia",
        "affiliation_id": "60111858",
        "affiliation_state": "VIC"
    },
    {
        "paper_title": "The Clinical Course of Alcohol Use Disorder Depicted by Digital Biomarkers",
        "paper_author": "Zetterström A.",
        "publication": "Frontiers in Digital Health",
        "citied_by": "4",
        "cover_date": "2021-12-07",
        "Abstract": "Aims: This study introduces new digital biomarkers to be used as precise, objective tools to measure and describe the clinical course of patients with alcohol use disorder (AUD). Methods: An algorithm is outlined for the calculation of a new digital biomarker, the recovery and exacerbation index (REI), which describes the current trend in a patient's clinical course of AUD. A threshold applied to the REI identifies the starting point and the length of an exacerbation event (EE). The disease patterns and periodicity are described by the number, length, and distance between EEs. The algorithms were tested on data from patients from previous clinical trials (n = 51) and clinical practice (n = 1,717). Results: Our study indicates that the digital biomarker-based description of the clinical course of AUD might be superior to the traditional self-reported relapse/remission concept and conventional biomarkers due to higher data quality (alcohol measured) and time resolution. We found that EEs and the REI introduce distinct tools to identify qualitative and quantitative differences in drinking patterns (drinks per drinking day, phosphatidyl ethanol levels, weekday and holiday patterns) and effect of treatment time. Conclusions: This study indicates that the disease state—level, trend and periodicity—can be mathematically described and visualized with digital biomarkers, thereby improving knowledge about the clinical course of AUD and enabling clinical decision-making and adaptive care. The algorithms provide a basis for machine-learning-driven research that might also be applied for other disorders where daily data are available from digital health systems.",
        "DOI": "10.3389/fdgth.2021.732049",
        "affiliation_name": "Ridgeview Instruments AB",
        "affiliation_city": "Uppsala",
        "affiliation_country": "Sweden",
        "affiliation_id": "60274509",
        "affiliation_state": "Uppsala"
    },
    {
        "paper_title": "Predictive auto-scaling with OpenStack Monasca",
        "paper_author": "Lanciano G.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "4",
        "cover_date": "2021-12-06",
        "Abstract": "Cloud auto-scaling mechanisms are typically based on reactive automation rules that scale a cluster whenever some metric, e.g., the average CPU usage among instances, exceeds a predefined threshold. Tuning these rules becomes particularly cumbersome when scaling-up a cluster involves non-negligible times to bootstrap new instances, as it happens frequently in production cloud services. To deal with this problem, we propose an architecture for auto-scaling cloud services based on the status in which the system is expected to evolve in the near future. Our approach leverages on time-series forecasting techniques, like those based on machine learning and artificial neural networks, to predict the future dynamics of key metrics, e.g., resource consumption metrics, and apply a threshold-based scaling policy on them. The result is a predictive automation policy that is able, for instance, to automatically anticipate peaks in the load of a cloud application and trigger ahead of time appropriate scaling actions to accommodate the expected increase in traffic. We prototyped our approach as an open-source OpenStack component, which relies on, and extends, the monitoring capabilities offered by Monasca, resulting in the addition of predictive metrics that can be leveraged by orchestration components like Heat or Senlin. We show experimental results using a recurrent neural network and a multi-layer perceptron as predictor, which are compared with a simple linear regression and a traditional non-predictive auto-scaling policy. However, the proposed framework allows for the easy customization of the prediction policy as needed.",
        "DOI": "10.1145/3468737.3494104",
        "affiliation_name": "Scuola Normale Superiore di Pisa",
        "affiliation_city": "Pisa",
        "affiliation_country": "Italy",
        "affiliation_id": "60030674",
        "affiliation_state": "RM"
    },
    {
        "paper_title": "On Detecting Growing-Up Behaviors of Malicious Accounts in Privacy-Centric Mobile Social Networks",
        "paper_author": "Yang Z.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "5",
        "cover_date": "2021-12-06",
        "Abstract": "Privacy-centric mobile social network (PC-MSN), which allows users to build intimate and private social circles, is an increasingly popular type of online social networks (OSNs). Because of strict usage policy enforced by PC-MSNs (such as restricted account and content access), malicious accounts (or users) have to act like normal accounts to accumulate credentials before committing malicious activities. Therefore, analysis merely relying on static account profile information or social graphs is ineffective to detect such growing-up accounts. Besides, existing behavior-based malicious account detection methods fail to effectively detect growing-up accounts who pretend to be benign and have similar behaviors to benign users during the growing-up stage. In this paper, we present the first comprehensive study of growing-up behaviors of malicious accounts in WeChat, one of the major PC-MSNs with billions of daily active users across the globe. Our analysis reveals that the behavior patterns of growing-up accounts are very similar to that of benign users, and yet quite different from typical malicious accounts. Based on this observation, we design Muses, a detection system that can automatically identify subtle yet effective behaviors (features) to distinguish growing-up accounts before they engage in obvious malicious campaigns. Muses is unsupervised so that it can adapt to new malicious campaigns even if the behavior patterns of malicious accounts are unknown a priori. In particular, Muses addresses the limitations of the previous supervised techniques, i.e., requiring manually labeled training sets, which is time-consuming and costly. We evaluate Muses on a large-scale anonymized dataset from WeChat with roughly 440k accounts. The experimental results show that Muses achieves 2x recall, with similar precision, compared with the previous methods. Specifically, Muses detects over 82% growing-up accounts with a precision of 90% and achieves an AUC of 0.95. Notably, Muses can also effectively detect growing-up accounts even if malicious users applied various evasion strategies.",
        "DOI": "10.1145/3485832.3488013",
        "affiliation_name": "Tencent",
        "affiliation_city": "Shenzhen",
        "affiliation_country": "China",
        "affiliation_id": "60114181",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Distilling Arbitration Logic from Traces using Machine Learning: A Case Study on NoC",
        "paper_author": "Zhou Y.",
        "publication": "Proceedings - Design Automation Conference",
        "citied_by": "4",
        "cover_date": "2021-12-05",
        "Abstract": "Arbitration logic is extensively used in modern computer architectures to dynamically determine how shared hardware resources are allocated or accessed. Recent work has shown that machine learning techniques can learn non-obvious yet effective arbitration policies, which in simulation demonstrate superior performance over human-designed heuristics. However, existing methods based on deep learning are too expensive to be directly implemented as an arbitration unit in hardware. While some prior efforts managed to manually analyze and reduce a deep learning model into relatively small circuits in certain cases, such ad hoc and labor-intensive approaches cannot easily generalize. In this work, we propose a new methodology to automatically 'distill' the arbitration logic from simulation traces. Starting by training a deep learning model, we leverage tree-based models as a bridge to convert the more complex model to a compact logic implementation. This paper presents a case study of the proposed methodology on a network-on-chip port arbitration task. Compared with an array of combinational multipliers that exactly computes the neural network output, our arbitration logic achieves up to 282x area reduction without significant performance degradation. Under the training traffic, our arbitration logic achieves up to 64x reduction in average packet latency and up to 5% increase in network throughput over the FIFO arbitration policy. The distilled arbitration policy is also able to generalize to different injection rates and traffic patterns.",
        "DOI": "10.1109/DAC18074.2021.9586301",
        "affiliation_name": "Shanghai Jiao Tong University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60025084",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Cyber-securing IoT infrastructure by modeling network traffic",
        "paper_author": "Gharakheili H.H.",
        "publication": "Security and Privacy in the Internet of Things: Architectures, Techniques, and Applications",
        "citied_by": "3",
        "cover_date": "2021-12-03",
        "Abstract": "In recent years, there has been growing recognition that machine learning is not a \"silver bullet\" that can magically detect all cyber-security threats, but is instead more effective in a narrower context - when the threat model is clear and the scope of the target activity is narrow. This chapter begins by highlighting Internet-of-Things (IoT) network threats and attack vectors, as well as existing countermeasures and their limitations. Next, a systematic approach to model the network behavior of IoT devices is developed, automatically enforcing their behavior and monitoring real-time activity using a set of flow-based anomaly detectors. The chapter outlines Software-Defined Networking-based system to enforce Manufacturer Usage Description (MUD) policies and dynamically inspect exception traffic (nonconforming to MUD profile) which is a small fraction of total packets to/from IoT devices.",
        "DOI": "10.1002/9781119607755.ch6",
        "affiliation_name": "University of New South Wales, School of Electrical Engineering and Telecommunications",
        "affiliation_city": "Sydney",
        "affiliation_country": "Australia",
        "affiliation_id": "60082337",
        "affiliation_state": "NSW"
    },
    {
        "paper_title": "A Mean-variance Optimization DQN in Autonomous Driving",
        "paper_author": "Zhao Z.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2021-12-03",
        "Abstract": "Deep reinforcement learning is a popular and effective approach for autonomous driving to achieve safe and complex self-driving with little or no human input. However, in the random and intricate driving scenarios with surrounding traffic, conventional RL-algorithms based on greedy policy might be unstable and inefficient in training because of their lack in risk sensitivity. This paper proposes an RL-learning approach for autonomous driving which includes the estimation of expected cumulative future reward and its standard deviation. We utilize the difference between expectation and standard deviation as the decision foundation to improve the risk sensitivity of policy and training performance. The proposed algorithm is implemented in the CARLA simulation environment and the results demonstrate that vehicle agent based on our learning algorithm is able to learn more efficiently and drive more safely.",
        "DOI": "10.1145/3518781.3519208",
        "affiliation_name": "SAIC",
        "affiliation_city": "San Diego",
        "affiliation_country": "United States",
        "affiliation_id": "60015916",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Contrasting urban greenness across cities with varying trends in above-normal weather events",
        "paper_author": "George J.S.",
        "publication": "Nature-Based Solutions",
        "citied_by": "1",
        "cover_date": "2021-12-01",
        "Abstract": "Effective urban management demands global empirical research findings. The work aims to determine whether green areas are more significant in cities where above-normal wet events have plateaued. This work compares the green surface cover characteristics of the entire city and identified zones of 54 cities representing cities worldwide. These cities experience varying amounts of above-normal weather conditions. This article classifies the cities in the data set into two groups: Group 1 – frequency of extreme wet weather events has plateaued, and Group 2 – frequency of extreme weather events are increasing or decreasing at higher rates. Next, the work uses structural features of co-located spatial networks, roads and terrain, to identify zones. Finally, high-resolution satellite imagery quantified urban green spaces in the city and identified zones of the two city groups using geospatial and machine learning methods for 2000 and 2020. The data is compared for two groups to understand the pattern of green spaces in cities. Empirical analysis results recognise a higher greenness index in the zones and the entire city amongst the cities of Group1. The pattern in the temporal change in the greenness index of the zones is helpful to formulate a priority plan for locating inter-linked green spaces in urban areas paving the way for Nature-based Solutions. The findings can guide planners and policy-makers to identify and prioritise locations to incorporate a robust green infrastructure network to build resilient cities by 'nature bringing a solution’, the essential concept of Nature-based Solutions.",
        "DOI": "10.1016/j.nbsj.2021.100008",
        "affiliation_name": "Indian Institute of Technology Kharagpur",
        "affiliation_city": "Kharagpur",
        "affiliation_country": "India",
        "affiliation_id": "60004750",
        "affiliation_state": "WB"
    },
    {
        "paper_title": "A PRESCRIPTIVE ANALYTICS FRAMEWORK FOR OPTIMAL POLICY DEPLOYMENT USING HETEROGENEOUS TREATMENT EFFECTS",
        "paper_author": "McFowland E.",
        "publication": "MIS Quarterly: Management Information Systems",
        "citied_by": "9",
        "cover_date": "2021-12-01",
        "Abstract": "We define a prescriptive analytics framework that addresses the needs of a constrained decision-maker facing, ex ante, unknown costs and benefits of multiple policy levers. The framework is general in nature and can be deployed in any utility-maximizing context, public or private. It relies on randomized field experiments for causal inference, machine learning for estimating heterogeneous treatment effects, and on the optimization of an integer linear program for converting predictions into decisions. The net result is the discovery of individual-level targeting of policy interventions to maximize overall utility under a budget constraint. The framework is set in the context of the four pillars of analytics and is especially valuable for companies that already have an existing practice of running A/B tests. The key contribution of this work is to develop and operationalize a framework to exploit both within- and between-treatment arm heterogeneity in the utility response function in order to derive benefits from future (optimized) prescriptions. We demonstrate the value of this framework as compared to benchmark practices—i.e., the use of the average treatment effect, uplift modeling, as well as an extension to contextual bandits—in two different settings. Unlike these standard approaches, our framework is able to recognize, adapt to, and exploit the (potential) presence of different subpopulations that experience varying costs and benefits within a treatment arm while also exhibiting differential costs and benefits across treatment arms. As a result, we find a targeting strategy that produces an order of magnitude improvement in expected total utility for the case where significant within- and between-treatment arm heterogeneity exists.",
        "DOI": "10.25300/MISQ/2021/15684",
        "affiliation_name": "Carlson School of Management",
        "affiliation_city": "Minneapolis",
        "affiliation_country": "United States",
        "affiliation_id": "60123802",
        "affiliation_state": "MN"
    },
    {
        "paper_title": "Monitoring rice cropping system in Cambodia and its influencing factors using time series MODIS images",
        "paper_author": "Huang C.",
        "publication": "Resources Science",
        "citied_by": "4",
        "cover_date": "2021-12-01",
        "Abstract": "Cambodia has abundant arable land resources, suitable temperature, and great potential for rice plantation. Timely acquisition of rice cropping system information is important for regional rice production management, disaster risk assessment, and food policy formulation. Most traditional rice remote sensing monitoring studies only provide spatial patterns of rice distribution at the interannual scale, and information on rice planting and harvesting at the intraannual scale is often lacking. In this study, first all available MODIS time series data in a year were used to construct an image-based MODIS NDVI annual time series curve; the maximum value, minimum value, mean value, and standard deviation were selected to calculate the image- by- image time series statistical parameter features, and the FastDTW algorithm was used to calculate the similarity features between the image-by-image time series curve and the rice reference time series curve, and then the time series statistical features were combined with the time series curve similarity features, and the rice maturation information was extracted by supervised classification through machine learning using a random forest classifier. Finally, the rice phenological features extracted from the time series curves were combined to generate rice harvest time information for the identification of rice cultivation types. The study showed that rice cultivation in Cambodia is mainly concentrated in the lowland plains around the Tonle Sap Lake and the lower Mekong River. Although the thermal conditions in Cambodia are suitable for rice cultivation throughout the year, water access constraints have a significant impact on the spatial and temporal patterns of rice cultivation in the country. The rice maturity mode indicates that production was dominated by single-season rice, which accounted for about 80% of the annual rice cultivation area and had a stable distribution area; double-season rice accounted for about 20% of the area and showed a large interannual variation in the spatial distribution of cultivation. Wet season rice was the main type of rice cultivation in the country, and the planted area accounted for about 70% of the annual rice area with little interannual variation; dry season rice and ex-rainy season rice accounted for about 30% of the area, with significant interannual variation in spatial distribution. The analysis of rice cropping patterns in 2011 and 2016 showed that irrigation conditions and flooding had important spatial and temporal effects on rice cultivation in Cambodia. This study identified the main influencing factors through high-precision monitoring of the interannual and intraannual spatial and temporal patterns of rice cultivation in Cambodia, which provide a reference for the development of a locally adapted and resilient rice cultivation system to ensure food security in the country.",
        "DOI": "10.18402/resci.2021.12.03",
        "affiliation_name": "Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60031150",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "QSPCA: A two-stage efficient power control approach in D2D communication for 5G networks",
        "paper_author": "Chandra S.",
        "publication": "Intelligent and Converged Networks",
        "citied_by": "10",
        "cover_date": "2021-12-01",
        "Abstract": "The existing literature on device-to-device (D2D) architecture suffers from a dearth of analysis under imperfect channel conditions. There is a need for rigorous analyses on the policy improvement and evaluation of network performance. Accordingly, a two-stage transmit power control approach (named QSPCA) is proposed: First, a reinforcement Q-learning based power control technique and; second, a supervised learning based support vector machine (SVM) model. This model replaces the unified communication model of the conventional D2D setup with a distributed one, thereby requiring lower resources, such as D2D throughput, transmit power, and signal-to-interference-plus-noise ratio as compared to existing algorithms. Results confirm that the QSPCA technique is better than existing models by at least 15.31% and 19.5% in terms of throughput as compared to SVM and Q-learning techniques, respectively. The customizability of the QSPCA technique opens up multiple avenues and industrial communication technologies in 5G networks, such as factory automation.",
        "DOI": "10.23919/ICN.2021.0021",
        "affiliation_name": "National Institute of Technology Patna",
        "affiliation_city": "Patna",
        "affiliation_country": "India",
        "affiliation_id": "60104351",
        "affiliation_state": "BR"
    },
    {
        "paper_title": "Machine learning models of tobacco susceptibility and current use among adolescents from 97 countries in the Global Youth Tobacco Survey, 2013-2017",
        "paper_author": "Kim N.",
        "publication": "PLOS Global Public Health",
        "citied_by": "7",
        "cover_date": "2021-12-01",
        "Abstract": "Adolescents are particularly vulnerable to tobacco initiation and escalation. Identifying factors associated with adolescent tobacco susceptibility and use can guide tobacco prevention efforts. Novel machine learning (ML) approaches efficiently identify interactive relations among factors of tobacco risks and identify high-risk subpopulations that may benefit from targeted prevention interventions. Nationally representative cross-sectional 2013–2017 Global Youth Tobacco Survey (GYTS) data from 97 countries (28 high-income and 69 low-and middle-income countries) from 342,481 adolescents aged 13–15 years (weighted N = 52,817,455) were analyzed using ML regression tree models, accounting for sampling weights. Predictors included demographics (sex, age), geography (region, country-income), and self-reported exposure to tobacco marketing, secondhand smoke, and tobacco control policies. 11.9% (95% CI 11.1%-12.6%) of tobacco-naïve adolescents were susceptible to tobacco use and 11.7% (11.0%-12.5%) of adolescents reported using any tobacco product (cigarettes, other smoked tobacco, smokeless tobacco) in the past 30 days. Regression tree models found that exposure or receptivity to tobacco industry promotions and secondhand smoke exposure predicted increased risks of susceptibility and use, while support for smoke-free air policies predicted decreased risks of tobacco susceptibility and use. Anti-tobacco school education and health warning messages on product packs predicted susceptibility or use, but their protective effects were not evident across all adolescent subgroups. Sex, region, and country-income moderated the effects of tobacco promotion and control factors on susceptibility or use, showing higher rates of susceptibility and use in males and high-income countries, Africa and the Americas (susceptibility), and Europe and Southeast Asia (use). Tobacco policy-related factors robustly predicted both tobacco susceptibility and use in global adolescents, and interacted with adolescent characteristics and other environments in complex ways that stratified adolescents based on their tobacco risk. These findings emphasize the importance of efficient ML modeling of interactions in tobacco risk prediction and suggest a role for targeted prevention strategies for high-risk adolescents.",
        "DOI": "10.1371/journal.pgph.0000060",
        "affiliation_name": "School of Computer, Data &amp; Information Sciences",
        "affiliation_city": "Madison",
        "affiliation_country": "United States",
        "affiliation_id": "60153202",
        "affiliation_state": "WI"
    },
    {
        "paper_title": "Soil carbon-food synergy: sizable contributions of small-scale farmers",
        "paper_author": "Iizumi T.",
        "publication": "CABI Agriculture and Bioscience",
        "citied_by": "8",
        "cover_date": "2021-12-01",
        "Abstract": "Background: Benefits to agricultural yield improvement, soil degradation prevention, and climate mitigation are central to the synergies of soil organic carbon (SOC) build-up. However, the contributions of small-scale farmers, the main target of recent agricultural and rural development policies, to SOC enhancement are understudied. Here, we present a global analysis of small-scale farmers’ contributions to the potential of additional SOC stocks and the associated increase in crop production. Methods: We applied random forest machine learning models to global gridded datasets on crop yield (wheat, maize, rice, soybean, sorghum and millet), soil, climate and agronomic management practices from the 2000s (n = 1808 to 8123). Using the established crop-specific SOC-yield relationships, the potentials of additional SOC build-up and crop production increase were simulated. The estimated SOC increase was converted into global decadal mean temperature change using the temperature sensitivity to cumulative total anthropogenic CO2 emissions from preindustrial levels. The amount of inorganic nitrogen (N) input that would result in the same yield outcome as the SOC build-up was derived from the crop-specific N-yield relationships. Results: SOC contributes to yields in addition to management and climatic factors. Additional SOC sums up to 12.78 GtC (11.55–14.05 GtC) of global SOC stock, which earns 38.24 Mt (22.88–57.48 Mt) of additional crop production and prevents warming by 0.030 °C (0.019–0.041 °C). This production increase equates to what would be achieved by an inorganic N input of 5.82 Mt N (3.89–7.14 Mt N). Small-scale farmers account for 28% (26–30%) of the additional SOC build-up and 17% (15–20%) of the production increase. Key crops and regions in terms of small-scale farmers’ contributions include Sub-Saharan African maize and rice, Latin American and Caribbean soybean and maize, and South Asian rice and wheat. Conclusions: The contribution of small-scale farmers to the potential increase in SOC stock and crop production is sizable, which in theory further leads to saving inorganic N input. These findings emphasize the importance of linking soil management to sustainable land and climate mitigation with institutions and policy for small-scale farmers. Such a joint policy would assist multiple development goals.",
        "DOI": "10.1186/s43170-021-00063-6",
        "affiliation_name": "Institute for Agro-Environmental Sciences, NARO",
        "affiliation_city": "Tsukuba",
        "affiliation_country": "Japan",
        "affiliation_id": "60006155",
        "affiliation_state": "Ibaraki"
    },
    {
        "paper_title": "Simulation of Urban Land Use Growth Scenarios Using the Cellular Automata Method of SLEUTH",
        "paper_author": "Jahanishakib F.",
        "publication": "Journal of Environmental Studies",
        "citied_by": "3",
        "cover_date": "2021-12-01",
        "Abstract": "Accelerating urban expansion is increasingly challenging the sustainable use of land, since modeling urban growth is important in order to adapt to balanced development. This study was carried out with the aim of simulating the future urban expansion of Birjand Metropolitan from 2020 to 2050 using Cellular Automata (CA) methodology in the SLEUTH modeling considering two scenarios: historical and environmental growth. Calibration results of the SLEUTH model showed Diffusion (65) and slope resistance coefficients (42) have the most effect on historical and environmental growth, respectively. The model was calibrated in three stages: coarse, fine and final, using the best fitted values of OSM and Leesale indices. The simulation results showed that urban expansion in the historical and environmental growth scenarios will be 2201.85 and 2150.91 hectares, respectively, so increasing the area of Birjand metropolis is inevitable in both scenarios. A comparison of the two scenarios denoted that in the historical growth scenario, the urban growth rate is higher and the vegetation destruction is maximal. The findings of this study can help policy makers and managers in formulating informed urban planning strategies to have the least destructive effect on the environment in the future.",
        "DOI": "10.22059/JES.2021.328447.1008215",
        "affiliation_name": "University of Zabol",
        "affiliation_city": "Zabol",
        "affiliation_country": "Iran",
        "affiliation_id": "60089331",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Temporal Variations and Spatial Disparities in Public Sentiment Toward COVID-19 and Preventive Practices in the United States: Infodemiology Study of Tweets",
        "paper_author": "Kahanek A.",
        "publication": "JMIR Infodemiology",
        "citied_by": "2",
        "cover_date": "2021-12-01",
        "Abstract": "Background: During the COVID-19 pandemic, US public health authorities and county, state, and federal governments recommended or ordered certain preventative practices, such as wearing masks, to reduce the spread of the disease. However, individuals had divergent reactions to these preventive practices. Objective: The purpose of this study was to understand the variations in public sentiment toward COVID-19 and the recommended or ordered preventive practices from the temporal and spatial perspectives, as well as how the variations in public sentiment arerelated to geographical and socioeconomic factors. Methods: The authors leveraged machine learning methods to investigate public sentiment polarity in COVID-19-related tweets from January 21, 2020 to June 12, 2020. The study measured the temporal variations and spatial disparities in public sentiment toward both general COVID-19 topics and preventive practices in the United States. Results: In the temporal analysis, we found a 4-stage pattern from high negative sentiment in the initial stage to decreasing and low negative sentiment in the second and third stages, to the rebound and increase in negative sentiment in the last stage. We also identified that public sentiment to preventive practices was significantly different in urban and rural areas, while poverty rate and unemployment rate were positively associated with negative sentiment to COVID-19 issues. Conclusions: The differences between public sentiment toward COVID-19 and the preventive practices imply that actions need to be taken to manage the initial and rebound stages in future pandemics. The urban and rural differences should be considered in terms of the communication strategies and decision making during a pandemic. This research also presents a framework to investigate time-sensitive public sentiment at the county and state levels, which could guide local and state governments and regional communities in making decisions and developing policies in crises.",
        "DOI": "10.2196/31671",
        "affiliation_name": "University of North Texas",
        "affiliation_city": "Denton",
        "affiliation_country": "United States",
        "affiliation_id": "60024438",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "Smoothing and the environmental manifold",
        "paper_author": "Unnithan Kumar S.",
        "publication": "Ecological Informatics",
        "citied_by": "11",
        "cover_date": "2021-12-01",
        "Abstract": "How the observed occurrences of a species relate to environmental gradients is a fundamental question in community ecology. In this paper, we present a new approach to address this question, using the smoothing function, a method not previously recruited into this ecological context. Using simulation techniques, we explore its accuracy in recovering known species distributions from simulated noisy data, and we compare the smoothing function's predictive abilities to two widely used methods in this field, the generalised linear model (GLM) and random forest machine learning. In studying the smoothing function, we are led to consider a new analytical tool for ecology, which we call the environmental manifold. It is given by the shape of the data cloud of sampled environmental predictor variables, and has deep relevance to ecological niche theory. Hitherto not considered in ecological analyses, it plays a fundamental role in understanding the species-environment relationship, and we utilise it to compare the performance and behaviour of these three methods. The results of our analysis find both random forest and smoothing to be robust to the complexities of the species-environment relationship, and also, to a degree, the shape of the environmental manifold. In contrast, the GLM's accuracy depends heavily on the complexity of the species-environment relationship, and is also affected by the geometry of the environmental manifold. Furthermore, the smoothing function is seen to be more accurate than random forest in every combination of species-environment relationship and environmental manifold shape, and also less affected by sampling bias. This suggests the promising role that such smoothing functions can have in ecological analyses. Our results also support the robustness of random forest machine learning to nonlinearity in both the species-environment relationship, and for the first time, the complexity of the shape of the environmental manifold. We conclude by discussing the implications and uses of the environmental manifold in ecological practice and theory, including its importance for niche theory, understanding species distributions, and conservation policy.",
        "DOI": "10.1016/j.ecoinf.2021.101472",
        "affiliation_name": "University of Oxford",
        "affiliation_city": "Oxford",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60026851",
        "affiliation_state": "Oxfordshire"
    },
    {
        "paper_title": "A three-level hierachical framework for additive manufacturing",
        "paper_author": "Ren Y.M.",
        "publication": "Digital Chemical Engineering",
        "citied_by": "9",
        "cover_date": "2021-12-01",
        "Abstract": "Metal alloy additive manufacturing (AM) has gained wide industrial interest in the past decade due to its capability to efficiently deliver complicated mechanical parts with high quality. However, due to a lack of understanding of the fundamental correlation between the operating conditions and build quality, the exploration of the optimal operating policy of the AM process is costly and difficult. In this work, a data-driven process optimization framework has been proposed for the additive manufacturing process, integrating machine learning, finite-element method (FEM) modeling, and cloud-edge data storage/transfer optimization. A three-level hierarchy of local machines, factory clouds, and a research center is introduced with each level responsible for its dedicated tasks. In addition, to ensure the efficiency of data transfer and storage, an edge-cloud data transfer scheme is constructed, which serves as a guideline for the data flow in the AM framework. Moreover, an overview of the connections between the proposed framework and the Industry 4.0 framework is presented.",
        "DOI": "10.1016/j.dche.2021.100001",
        "affiliation_name": "UCLA Samueli School of Engineering",
        "affiliation_city": "Los Angeles",
        "affiliation_country": "United States",
        "affiliation_id": "60153950",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Statistical Machine Learning Approaches to Liver Disease Prediction",
        "paper_author": "Mostafa F.",
        "publication": "Livers",
        "citied_by": "30",
        "cover_date": "2021-12-01",
        "Abstract": "Medical diagnoses have important implications for improving patient care, research, and policy. For a medical diagnosis, health professionals use different kinds of pathological methods to make decisions on medical reports in terms of the patients’ medical conditions. Recently, clinicians have been actively engaged in improving medical diagnoses. The use of artificial intelligence and machine learning in combination with clinical findings has further improved disease detection. In the modern era, with the advantage of computers and technologies, one can collect data and visualize many hidden outcomes such as dealing with missing data in medical research. Statistical machine learning algorithms based on specific problems can assist one to make decisions. Machine learning (ML), data-driven algorithms can be utilized to validate existing methods and help researchers to make potential new decisions. The purpose of this study was to extract significant predictors for liver disease from the medical analysis of 615 humans using ML algorithms. Data visualizations were implemented to reveal significant findings such as missing values. Multiple imputations by chained equations (MICEs) were applied to generate missing data points, and principal component analysis (PCA) was used to reduce the dimensionality. Variable importance ranking using the Gini index was implemented to verify significant predictors obtained from the PCA. Training data ((Formula presented.)) for learning and testing data ((Formula presented.)) in the ML methods were used for predicting classifications. The study compared binary classifier machine learning algorithms (i.e., artificial neural network, random forest (RF), and support vector machine), which were utilized on a published liver disease data set to classify individuals with liver diseases, which will allow health professionals to make a better diagnosis. The synthetic minority oversampling technique was applied to oversample the minority class to regulate overfitting problems. The RF significantly contributed ((Formula presented.)) to a higher accuracy score of 98.14% compared to the other methods. Thus, this suggests that ML methods predict liver disease by incorporating the risk factors, which may improve the inference-based diagnosis of patients.",
        "DOI": "10.3390/livers1040023",
        "affiliation_name": "Texas Tech University Health Sciences Center at Lubbock",
        "affiliation_city": "Lubbock",
        "affiliation_country": "United States",
        "affiliation_id": "60032964",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "Machine learning-based approach: Global trends, research directions, and regulatory standpoints",
        "paper_author": "Pugliese R.",
        "publication": "Data Science and Management",
        "citied_by": "168",
        "cover_date": "2021-12-01",
        "Abstract": "The field of machine learning (ML) is sufficiently young that it is still expanding at an accelerating pace, lying at the crossroads of computer science and statistics, and at the core of artificial intelligence (AI) and data science. Recent progress in ML has been driven both by the development of new learning algorithms theory, and by the ongoing explosion in the availability of vast amount of data (often referred to as “big data”) and low-cost computation. The adoption of ML-based approaches can be found throughout science, technology and industry, leading to more evidence-based decision-making across many walks of life, including healthcare, biomedicine, manufacturing, education, financial modeling, data governance, policing, and marketing. Although the past decade has witnessed the increasing interest in these fields, we are just beginning to tap the potential of these ML algorithms for studying systems that improve with experience. In this paper, we present a comprehensive view on geo worldwide trends (taking into account China, the USA, Israel, Italy, the UK, and the Middle East) of ML-based approaches highlighting rapid growth in the last 5 years attributable to the introduction of related national policies. Furthermore, based on the literature review, we also discuss the potential research directions in this field, summarizing some popular application areas of machine learning technology, such as healthcare, cyber-security systems, sustainable agriculture, data governance, and nanotechnology, suggesting that the “dissemination of research” in the ML scientific community has undergone the exceptional growth in the time range of 2018–2020, reaching a value of 16,339 publications. Finally, we report the challenges and the regulatory standpoints for managing ML technology. Overall, we hope that this work will help to explain the geo trends of ML approaches and their applicability in various real-world domains, as well as serve as a reference point for both academia and industry professionals, particularly from a technical, ethical and regulatory point of view.",
        "DOI": "10.1016/j.dsm.2021.12.002",
        "affiliation_name": "Asst Grande Ospedale Metropolitano Niguarda",
        "affiliation_city": "Milan",
        "affiliation_country": "Italy",
        "affiliation_id": "60015852",
        "affiliation_state": "MI"
    },
    {
        "paper_title": "Who determines United States Healthcare out-of-pocket costs? Factor ranking and selection using ensemble learning",
        "paper_author": "Zhang C.",
        "publication": "Health Information Science and Systems",
        "citied_by": "3",
        "cover_date": "2021-12-01",
        "Abstract": "Purpose: Healthcare out-of-pocket (OOP) costs consist of the annual expenses paid by individuals or families that are not reimbursed by insurance. In the U.S, broadening healthcare disparities are caused by the rapid increase in OOP costs. With a precise forecast of the OOP costs, governments can improve the design of healthcare policies to better control the OOP costs. This study designs a purely data-driven ensemble learning procedure to achieve a collection of factors that best predict OOP costs. Methods: We propose a voting ensemble learning procedure to rank and select factors of OOP costs based on the Medical Expenditure Panel Survey dataset. The method involves utilizing votes from the base learners forward subset selection, backward subset selection, random forest, and LASSO. Results: The top-ranking factors selected by our proposed method are insurance type, age, asthma, family size, race, and number of physician office visits. The predictive models using these factors outperform the models that employ the factors commonly considered by the literature through improving the prediction error (test MSE of the OOP costs’ log-odds) from 0.462 to 0.382. Conclusion: Our results indicate a set of factors which best explain the OOP costs behavior based on a purely data-driven solution. These findings contribute to the discussions regarding demand-side needs for containing rapidly rising OOP costs. Instead of estimating the impact of a single factor on OOP costs, our proposed method allows for the selection of arbitrary-sized factors to best explain OOP costs.",
        "DOI": "10.1007/s13755-021-00153-9",
        "affiliation_name": "Claremont Graduate University",
        "affiliation_city": "Claremont",
        "affiliation_country": "United States",
        "affiliation_id": "60003812",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Software Identification by Standard Machine Learning Tools",
        "paper_author": "Sukhoparov M.E.",
        "publication": "Automatic Control and Computer Sciences",
        "citied_by": "0",
        "cover_date": "2021-12-01",
        "Abstract": "Abstract: This article considers tools for controlling software installed on personal computers of automated system users. The flaws of these software solutions are grounded, and an approach to identifying executable files with the help of a machine learning algorithm is developed and presented. This algorithm consists in the gradient decision tree boosting on the basis of such libraries as XGBoost, LightGBM, CatBoost. The identification of programs with the help of XGBoost and LightGBM is executed. The experimental results are compared with the results of earlier studies conducted by other authors. The findings show that the developed method allows for identifying violations in the adopted security policy during information processing in automated systems.",
        "DOI": "10.3103/S0146411621080459",
        "affiliation_name": "Russian Academy of Sciences",
        "affiliation_city": "Moscow",
        "affiliation_country": "Russian Federation",
        "affiliation_id": "60021331",
        "affiliation_state": "Moscow Oblast"
    },
    {
        "paper_title": "Industry 4.0 - Policy-based approaches to efficient implementation in SMEs",
        "paper_author": "Łabȩdzka J.",
        "publication": "Engineering Management in Production and Services",
        "citied_by": "9",
        "cover_date": "2021-12-01",
        "Abstract": "Industry 4.0 (I4.0), driven by the need to access real-time insights and information across the manufacturing process, creates a disruptive impact on industries. Large-scale machine-to-machine communication, virtual reality (VR), the Internet of Things (IoT), simulation technologies and network management are integrated for increased automation, machine learning, self-controlled social and technical systems (Smart Factories). The uptake of advanced manufacturing solutions represents a challenge for businesses and SMEs in particular. SMEs possess neither the organisational capability nor financial resources to systematically investigate the potential and risks of introducing Industry 4.0. However, the so-called Fourth Industrial Revolution is a matter of technology and cooperation between European regions to share knowledge concerning alternative regional and national approaches to reinforcing the I4.0 uptake. Therefore, this paper primarily aims to analyse practical experience on how European policies related to the European Regional Development Fund (ERDF) can unlock the full potential of Industry 4.0 and overcome the fragmentation of Industry 4.0 solutions. Case studies of successful transfer of I4.0 to SMEs in Europe and supporting regional policy instruments presented in the paper could inspire and enable the potential of digitalisation by dealing with main challenges hampering their diffusion into the business ecosystem.",
        "DOI": "10.2478/emj-2021-0032",
        "affiliation_name": "Łukasiewicz - Instytut Technologii Eksploatacji",
        "affiliation_city": "Radom",
        "affiliation_country": "Poland",
        "affiliation_id": "60088503",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "INODE: Building an End-to-End Data Exploration System in Practice",
        "paper_author": "Amer-Yahia S.",
        "publication": "SIGMOD Record",
        "citied_by": "10",
        "cover_date": "2021-12-01",
        "Abstract": "A full-fledged data exploration system must combine different access modalities with a powerful concept of guiding the user in the exploration process, by being reactive and anticipative both for data discovery and for data linking. Such systems are a real opportunity for our community to cater to users with different domain and data science expertise. We introduce INODE - an end-to-end data exploration system - that leverages, on the one hand, Machine Learning and, on the other hand, semantics for the purpose of Data Management (DM). Our vision is to develop a classic unified, comprehensive platform that provides extensive access to open datasets, and we demonstrate it in three significant use cases in the fields of Cancer Biomarker Research, Research and Innovation Policy Making, and Astrophysics. INODE offers sustainable services in (a) data modeling and linking, (b) integrated query processing using natural language, (c) guidance, and (d) data exploration through visualization, thus facilitating the user in discovering new insights. We demonstrate that our system is uniquely accessible to a wide range of users from larger scientific communities to the public. Finally, we briefly illustrate how this work paves the way for new research opportunities in DM.",
        "DOI": "10.1145/3516431.3516436",
        "affiliation_name": "Université Grenoble Alpes",
        "affiliation_city": "Saint Martin d'Heres",
        "affiliation_country": "France",
        "affiliation_id": "60104653",
        "affiliation_state": "Auvergne-Rhone-Alpes"
    },
    {
        "paper_title": "Understanding the Middle East through the eyes of Japan's Newspapers: A topic modelling and sentiment analysis approach",
        "paper_author": "Ghasiya P.",
        "publication": "Digital Scholarship in the Humanities",
        "citied_by": "6",
        "cover_date": "2021-12-01",
        "Abstract": "For Japan-a country that has always been described with virtually no major natural resources such as oil, gas, and coal-the Middle Eastern region has a special place in its economic and foreign policy. In 2017, 39% of Japan's energy came from oil, and 87% of Japan's imported oil came from the Middle East, predominantly Saudi Arabia and the UAE. The above facts are enough to discern the critical significance of the Middle Eastern region for Japan. For Japan to have an unhindered supply of oil and other natural resources, it is pertinent that this region remains peaceful. In this scenario, the Middle East-related articles in Japan's newspapers can help understand Japan's perspective towards the Middle East. This paper would first apply the topic modelling approach non-negative matrix factorization (NMF) on Middle East-related articles from three newspapers of Japan. After discovering crucial topics, we would utilize traditional supervised machine learning algorithms to determine the overall and topic-specific sentiments from the collected headlines. Our topic modelling results discovered that the Japanese media widely reported issues like Islamic State, the refugee crisis, the Syrian civil war, Qasem Soleimani killing, and Iran nuclear deal. Further, the news related to Saudi Arabia, Syria, and Trump garnered high negative sentiment.",
        "DOI": "10.1093/llc/fqab019",
        "affiliation_name": "Kyushu University",
        "affiliation_city": "Fukuoka",
        "affiliation_country": "Japan",
        "affiliation_id": "60011047",
        "affiliation_state": "Fukuoka"
    },
    {
        "paper_title": "Gene-editable materials for future transportation infrastructure: a review for polyurethane-based pavement",
        "paper_author": "Hong B.",
        "publication": "Journal of Infrastructure Preservation and Resilience",
        "citied_by": "21",
        "cover_date": "2021-12-01",
        "Abstract": "With the rapid development of society and industry, novel technologies and materials related to pavement engineering are constantly emerging. However, with the continuous improvement of people’s demands, pavement engineering also faces more and more enormous challenges that the pavement materials must have excellent engineering properties and environmental benefits. Meanwhile, the intelligence is the mainstream development direction of modern society, and the development trend of future transportation infrastructure. Materials Genome Initiative, a program for the development of new materials that materials design is conducted by up-front simulations and predictions, followed by key validation experiments, the rapid development of science and technology and AI toolset (big data and machine learning) provide new opportunities and strong technical supports for pavement materials development that shorten the development-application cycle of new material, reduce cost and promote the application of new carriers such as intelligent sensing components in transportation engineering, to achieve the intelligence of transportation engineering. However, traditional pavement materials possess several unavoidable shortcomings, indicating that it is exceedingly difficult for them to meet the above requirements for future pavement materials. Therefore, the development of future new pavement materials, which can be designed on-demand as well as possessing enough mechanical properties, high durability, practical functionality, and high environmental protection, is urgent. In recent years, as a “designable” polymer material with various excellent engineering performances, polyurethane (PU) has been widely applied in pavement practices by changing the chemical structures of raw materials and their mix proportions, for instance pavement repairing material, permeable pavement material, tunnel paving material and bridge deck paving materials, etc. Although PU material has been widely applied in practices, a systematically summarization is still quite necessary for further understanding the working mechanism of PU materials and optimization it’s engineering applications. To fill the gap, this article puts forward the special requirements for future transportation infrastructure materials, and introduces the basic properties and working mechanism of PU materials in order to make up for the defects of conventional road materials. Based on this, this article also summarizes the engineering performances and environmental benefits of applying PU as the binder for different road infrastructure materials in recent years. Considering the gene-editable nature of polyurethane, further research of the on-demand design principles of PU pavement materials is recommended. The establishment of raw material gene database, material terminal performance database and their structure-activity relationship are highlighted. The current research is essential to the practice guidance and further optimization of the PU materials for road infrastructures, which in line with the future Carbon neutral policy.",
        "DOI": "10.1186/s43065-021-00039-w",
        "affiliation_name": "Harbin Institute of Technology",
        "affiliation_city": "Harbin",
        "affiliation_country": "China",
        "affiliation_id": "60019616",
        "affiliation_state": "Heilongjiang"
    },
    {
        "paper_title": "Deep reinforcement learning for shared control of mobile robots",
        "paper_author": "Tian C.",
        "publication": "IET Cyber-systems and Robotics",
        "citied_by": "2",
        "cover_date": "2021-12-01",
        "Abstract": "Shared control of mobile robots integrates manual input with auxiliary autonomous controllers to improve the overall system performance. However, prior work that seeks to find the optimal shared control ratio needs an accurate human model, which is usually challenging to obtain. In this study, the authors develop an extended Twin Delayed Deep Deterministic Policy Gradient (DDPG) (TD3X)-based shared control framework that learns to assist a human operator in teleoperating mobile robots optimally. The robot's states, shared control ratio in the previous time step, and human's control input is used as inputs to the reinforcement learning (RL) agent, which then outputs the optimal shared control ratio between human input and autonomous controllers without knowing the human model. Noisy softmax policies are developed to make the TD3X algorithm feasible under the constraint of a shared control ratio. Furthermore, to accelerate the training process and protect the robot, a navigation demonstration policy and a safety guard are developed. A neural network (NN) structure is developed to maintain the correlation of sensor readings among heterogeneous input data and improve the learning speed. In addition, an extended DAGGER (DAGGERX) human agent is developed for training the RL agent to reduce human workload. Robot simulations and experiments with humans in the loop are conducted. The results show that the DAGGERX human agent can simulate real human inputs in the worst-case scenarios with a mean square error of 0.0039. Compared to the original TD3 agent, the TD3X-based shared control system decreased the average collision number from 387.3 to 44.4 in a simplistic environment and 394.2 to 171.2 in a more complex environment. The maximum average return increased from 1043 to 1187 with a faster converge speed in the simplistic environment, while the performance is equally good in the complex environment because of the use of an advanced human agent. In the human subject tests, participants' average perceived workload was significantly lower in shared control than that in exclusively manual control (26.90 vs. 40.07, p = 0.013).",
        "DOI": "10.1049/csy2.12036",
        "affiliation_name": "Clemson University College of Engineering, Computing and Applied Sciences",
        "affiliation_city": "Clemson",
        "affiliation_country": "United States",
        "affiliation_id": "60139609",
        "affiliation_state": "SC"
    },
    {
        "paper_title": "Projecting armed conflict risk in Africa towards 2050 along the SSP-RCP scenarios: a machine learning approach",
        "paper_author": "Hoch J.M.",
        "publication": "Environmental Research Letters",
        "citied_by": "23",
        "cover_date": "2021-12-01",
        "Abstract": "In the past decade, several efforts have been made to project armed conflict risk into the future. This study broadens current approaches by presenting a first-of-its-kind application of machine learning (ML) methods to project sub-national armed conflict risk over the African continent along three Shared Socioeconomic Pathway (SSP) scenarios and three Representative Concentration Pathways towards 2050. Results of the open-source ML framework CoPro are consistent with the underlying socioeconomic storylines of the SSPs, and the resulting out-of-sample armed conflict projections obtained with Random Forest classifiers agree with the patterns observed in comparable studies. In SSP1-RCP2.6, conflict risk is low in most regions although the Horn of Africa and parts of East Africa continue to be conflict-prone. Conflict risk increases in the more adverse SSP3-RCP6.0 scenario, especially in Central Africa and large parts of Western Africa. We specifically assessed the role of hydro-climatic indicators as drivers of armed conflict. Overall, their importance is limited compared to main conflict predictors but results suggest that changing climatic conditions may both increase and decrease conflict risk, depending on the location: in Northern Africa and large parts of Eastern Africa climate change increases projected conflict risk whereas for areas in the West and northern part of the Sahel shifting climatic conditions may reduce conflict risk. With our study being at the forefront of ML applications for conflict risk projections, we identify various challenges for this arising scientific field. A major concern is the limited selection of relevant quantified indicators for the SSPs at present. Nevertheless, ML models such as the one presented here are a viable and scalable way forward in the field of armed conflict risk projections, and can help to inform the policy-making process with respect to climate security.",
        "DOI": "10.1088/1748-9326/ac3db2",
        "affiliation_name": "International Peace Research Institute, Oslo",
        "affiliation_city": "Oslo",
        "affiliation_country": "Norway",
        "affiliation_id": "60031105",
        "affiliation_state": "Oslo"
    },
    {
        "paper_title": "Vigorous IDS on Nefarious Operations and Threat Analysis Using Ensemble Machine Learning",
        "paper_author": "Musa U.S.",
        "publication": "Revue d'Intelligence Artificielle",
        "citied_by": "2",
        "cover_date": "2021-12-01",
        "Abstract": "The geometric increase in the usage of computer networking activities poses problems with the management of network normal operations. These issues had drawn the attention of network security researchers to introduce different kinds of intrusion detection systems (IDS) which monitor data flow in a network for unwanted and illicit operations. The violation of security policies with nefarious motive is what is known as intrusion. The IDS therefore examine traffic passing through networked systems checking for nefarious operations and threats, which then sends warnings if any of these malicious activities are detected. There are 2 types of detection of malicious activities, misuse detection, in this case the information about the passing network traffic is gathered, analyzed, which is then compared with the stored predefined signatures. The other type of detection is the Anomaly detection which is detecting all network activities that deviates from regular user operations. Several researchers have done various works on IDS in which they employed different machine learning (ML), evaluating their work on various datasets. In this paper, an efficient IDS is built using Ensemble machine learning algorithms which is evaluated on CIC-IDS2017, an updated dataset that contains most recent attacks. The results obtained show a great increase in the rate of detection, increase in accuracy as well as reduction in the false positive rates (FPR).",
        "DOI": "10.18280/ria.350604",
        "affiliation_name": "LLoyd Institute of Engineering &amp; Technology",
        "affiliation_city": "Greater Noida",
        "affiliation_country": "India",
        "affiliation_id": "60271867",
        "affiliation_state": "UP"
    },
    {
        "paper_title": "Gradus et al. Respond to \"Machine Learning and Suicide Prevention: New Directions\"",
        "paper_author": "Gradus J.L.",
        "publication": "American Journal of Epidemiology",
        "citied_by": "0",
        "cover_date": "2021-12-01",
        "Abstract": "NA",
        "DOI": "10.1093/aje/kwab113",
        "affiliation_name": "VA Boston Healthcare System",
        "affiliation_city": "West Roxbury",
        "affiliation_country": "United States",
        "affiliation_id": "60105918",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Measuring anomalies in cigarette sales using official data from Spanish provinces: Are the anomalies detected by the Empty Pack Surveys (EPSs) used by Transnational Tobacco Companies (TTCs) the only anomalies?",
        "paper_author": "Cadahia P.",
        "publication": "Tobacco Induced Diseases",
        "citied_by": "3",
        "cover_date": "2021-12-01",
        "Abstract": "INTRODUCTION There is a literature that questions the veracity of the studies commissioned by transnational tobacco companies (TTCs) to measure the illicit tobacco trade. Furthermore, there are studies that have indicated that the empty pack surveys (EPSs) ordered by TTCs overestimate the size of this trade. This study simultaneously analyzed whether the EPSs established in each of the 47 Spanish provinces were accurate and measured anomalies observed in provinces where sales exceed expected values. METHODS To achieve the objectives of this study, provincial data on cigarette sales, prices and GDP per capita were used. These data were modeled with machine learning techniques that are widely used to detect anomalies in other areas. RESULTS The magnitude of the average anomaly in provinces where sales are higher than their expected values exceeds 40%, while the average anomaly in provinces where sales are lower than their expected values (as detected by the EPSs) is <15%. Furthermore, the results reveal that there is a clear geographical pattern to the provinces in which sales below reasonable values are observed. In addition, the values provided by the EPSs in Spain, as indicated in the previous literature, are slightly overestimated. Finally, some regions bordering other countries or that are highly influenced by tourism have observed sales that are higher than their expected values. CONCLUSIONS Cooperation between countries in their tobacco control policies can have better effects than policies developed based on information from a single country. The lack of control over the transactions of tourists and the inhabitants of bordering countries can cause important anomalies that distort the understanding of tobacco consumption that governments have based on official data.",
        "DOI": "10.18332/tid/143321",
        "affiliation_name": "International University of La Rioja",
        "affiliation_city": "Logrono",
        "affiliation_country": "Spain",
        "affiliation_id": "60104175",
        "affiliation_state": "La Rioja"
    },
    {
        "paper_title": "Is preclinical research in cancer biology reproducible enough?",
        "paper_author": "Kane P.B.",
        "publication": "eLife",
        "citied_by": "11",
        "cover_date": "2021-12-01",
        "Abstract": "The Reproducibility Project: Cancer Biology (RPCB) was established to provide evidence about reproducibility in basic and preclinical cancer research, and to identify the factors that influence reproducibility more generally. In this commentary we address some of the scientific, ethical and policy implications of the project. We liken the basic and preclinical cancer research enterprise to a vast ‘diagnostic machine’ that is used to determine which clinical hypotheses should be advanced for further development, including clinical trials. The results of the RPCB suggest that this diagnostic machine currently recommends advancing many findings that are not reproducible. While concerning, we believe that more work needs to be done to evaluate the performance of the diagnostic machine. Specifically, we believe three questions remain unanswered: How often does the diagnostic machine correctly recommend against advancing real effects to clinical testing?; what are the relative costs to society of false positive and false negatives?; and how well do scientists and others interpret the outputs of the machine?.",
        "DOI": "10.7554/eLife.67527",
        "affiliation_name": "McGill Faculty of Medicine and Health Sciences",
        "affiliation_city": "Montreal",
        "affiliation_country": "Canada",
        "affiliation_id": "60029502",
        "affiliation_state": "QC"
    },
    {
        "paper_title": "Exploring stakeholder attitudes towards AI in clinical practice",
        "paper_author": "Scott I.A.",
        "publication": "BMJ Health and Care Informatics",
        "citied_by": "74",
        "cover_date": "2021-12-01",
        "Abstract": "Objectives Different stakeholders may hold varying attitudes towards artificial intelligence (AI) applications in healthcare, which may constrain their acceptance if AI developers fail to take them into account. We set out to ascertain evidence of the attitudes of clinicians, consumers, managers, researchers, regulators and industry towards AI applications in healthcare. Methods We undertook an exploratory analysis of articles whose titles or abstracts contained the terms € artificial intelligence' or € AI' and € medical' or € healthcare' and € attitudes', € perceptions', € opinions', € views', € expectations'. Using a snowballing strategy, we searched PubMed and Google Scholar for articles published 1 January 2010 through 31 May 2021. We selected articles relating to non-robotic clinician-facing AI applications used to support healthcare-related tasks or decision-making. Results Across 27 studies, attitudes towards AI applications in healthcare, in general, were positive, more so for those with direct experience of AI, but provided certain safeguards were met. AI applications which automated data interpretation and synthesis were regarded more favourably by clinicians and consumers than those that directly influenced clinical decisions or potentially impacted clinician-patient relationships. Privacy breaches and personal liability for AI-related error worried clinicians, while loss of clinician oversight and inability to fully share in decision-making worried consumers. Both clinicians and consumers wanted AI-generated advice to be trustworthy, while industry groups emphasised AI benefits and wanted more data, funding and regulatory certainty. Discussion Certain expectations of AI applications were common to many stakeholder groups from which a set of dependencies can be defined. Conclusion Stakeholders differ in some but not all of their attitudes towards AI. Those developing and implementing applications should consider policies and processes that bridge attitudinal disconnects between different stakeholders.",
        "DOI": "10.1136/bmjhci-2021-100450",
        "affiliation_name": "The University of Queensland",
        "affiliation_city": "Brisbane",
        "affiliation_country": "Australia",
        "affiliation_id": "60031004",
        "affiliation_state": "QLD"
    },
    {
        "paper_title": "Predictive maintenance decision-making for serial production lines based on deep reinforcement learning",
        "paper_author": "Cui P.",
        "publication": "Jisuanji Jicheng Zhizao Xitong/Computer Integrated Manufacturing Systems, CIMS",
        "citied_by": "5",
        "cover_date": "2021-12-01",
        "Abstract": "Predictive maintenance is designed to perform maintenance activities based on the condition of equipment, which can improve the business bottom line by reducing maintenance cost and improving production performance. The modeling, analysis and decision-making of serial production lines with machine degradation process were studied. A Markov chain model was developed by analyzing the dynamics of a serial production line with machine failures and predictive maintenance, and the analytical formulas of transient performance measures were derived. A predictive maintenance decision model was established as a Markov decision process to minimize the work-in-process, backlog and maintenance costs. A deep reinforcement learning method was utilized to explore optimum maintenance policies, which was obtained through the training of neural network with dataset generated from Markov chain model. Case study was performed to validate the effectiveness of the proposed decision model. The results indicated that the maintenance and production related costs were significantly reduced.",
        "DOI": "10.13196/j.cims.2021.12.004",
        "affiliation_name": "Northwestern Polytechnical University",
        "affiliation_city": "Xi'an",
        "affiliation_country": "China",
        "affiliation_id": "60003977",
        "affiliation_state": "Shaanxi"
    },
    {
        "paper_title": "Child stature, maternal education, and early childhood development in Nigeria",
        "paper_author": "Skoufias E.",
        "publication": "PLoS ONE",
        "citied_by": "7",
        "cover_date": "2021-12-01",
        "Abstract": "Data from the 2016–17 Multiple Indicator Cluster Survey from Nigeria are used to study the relationship between child stature, mother’s years of education, and indicators of early childhood development (ECD). The relationships are contrasted between two empirical approaches: the conventional approach whereby control variables are selected in an ad-hoc manner, and the double machine-learning (DML) approach that employs data-driven methods to select controls from a much wider set of variables and thus reducing potential omitted variable bias. Overall, the analysis confirms that maternal education and the incidence of chronic malnutrition have a significant direct effect on measures of early childhood development. The point estimates based on the ad-hoc specification tend to be larger in absolute value than those based on the DML specification. Frequently, the point estimates based on the ad-hoc specification fall inside the confidence interval of the DML point estimates, suggesting that in these cases the omitted variable bias is not serious enough to prevent making causal inferences based on the ad-hoc specification. However, there are instances where the omitted variable bias is sufficiently large for the ad hoc specification to yield a statistically significant relationship when in fact the more robust DML specification suggests there is none. The DML approach also reveals a more complex picture that highlights the role of context. In rural areas, mother’s education affects early childhood development both directly and indirectly through its impact on the nutritional status of both older and younger children. In contrast, in urban areas, where the average level of maternal education is much higher, increases in a mother’s education have only a direct effect on child ECD measures but no indirect effect through child nutrition. Thus, DML provides a practical and feasible approach to reducing threats to internal validity for robust inferences and policy design based on observational data.",
        "DOI": "10.1371/journal.pone.0260937",
        "affiliation_name": "The World Bank, USA",
        "affiliation_city": "Washington, D.C.",
        "affiliation_country": "United States",
        "affiliation_id": "60112834",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Digital topics on cultural heritage investigated: how can data-driven and data-guided methods support to identify current topics and trends in digital heritage?",
        "paper_author": "Münster S.",
        "publication": "Built Heritage",
        "citied_by": "15",
        "cover_date": "2021-12-01",
        "Abstract": "In research and policies, the identification of trends as well as emerging topics and topics in decline is an important source of information for both academic and innovation management. Since at present policy analysis mostly employs qualitative research methods, the following article presents and assesses different approaches – trend analysis based on questionnaires, quantitative bibliometric surveys, the use of computer-linguistic approaches and machine learning and qualitative investigations. Against this backdrop, this article examines digital applications in cultural heritage and, in particular, built heritage via various investigative frameworks to identify topics of relevance and trendlines, mainly for European Union (EU)-based research and policies. Furthermore, this article exemplifies and assesses the specific opportunities and limitations of the different methodical approaches against the backdrop of data-driven vs. data-guided analytical frameworks. As its major findings, our study shows that both research and policies related to digital applications for cultural heritage are mainly driven by the availability of new technologies. Since policies focus on meta-topics such as digitisation, openness or automation, the research descriptors are more granular. In general, data-driven approaches are promising for identifying topics and trendlines and even predicting the development of near future trends. Conversely, qualitative approaches are able to answer “why” questions with regard to whether topics are emerging due to disruptive innovations or due to new terminologies or whether topics are becoming obsolete because they are common knowledge, as is the case for the term “internet”.",
        "DOI": "10.1186/s43238-021-00045-7",
        "affiliation_name": "Friedrich-Schiller-Universität Jena",
        "affiliation_city": "Jena",
        "affiliation_country": "Germany",
        "affiliation_id": "60029507",
        "affiliation_state": "Thuringen"
    },
    {
        "paper_title": "What Makes a Tax Policy Popular? Predicting Referendum Votes from Policy Text",
        "paper_author": "Martin I.W.",
        "publication": "Socius",
        "citied_by": "1",
        "cover_date": "2021-12-01",
        "Abstract": "What kinds of taxation are most politically sustainable in a democracy? The authors answer this question by applying natural language processing and machine learning techniques to a large, new corpus of digitized documents describing municipal tax policies of heterogeneous design that have been directly subjected to popular referendum in the state of California. The authors find that tax policies of different description vary systematically in their popularity with voters. In particular, official textual summaries of tax policy differ along two social dimensions that are associated with voters’ willingness to approve the tax. The authors interpret these dimensions as risk pooling and community orientation and show that measuring these dimensions can modestly improve the ability to predict the popularity of a tax, relative to a conventional regression specification that omits information about qualitative policy design. The authors discuss implications for the study of the sociology of taxation.",
        "DOI": "10.1177/23780231211066069",
        "affiliation_name": "University of California, San Diego",
        "affiliation_city": "La Jolla",
        "affiliation_country": "United States",
        "affiliation_id": "60030612",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Evaluating fishing capacity based on dea and regression analysis of china’s offshore fishery",
        "paper_author": "Liu S.",
        "publication": "Journal of Marine Science and Engineering",
        "citied_by": "4",
        "cover_date": "2021-12-01",
        "Abstract": "The analysis of offshore fishing capacity is of great significance and practical value to the sustainable utilization and conservation of marine fishery resources. Based on the 2004–2020 China Fishery Statistical Yearbook, data envelopment analysis (DEA) was applied for measuring fishing capacity using a number of fishing vessels, total power, total tonnage, and the number of professional fishermen as the input measures and the annual catch as the output measure. Capacity utilization had a calculated range from 80.7 to 100%, and its average is 93.5%. In the first four years of 2003–2007, the excess investment rate of fishing vessels, total tonnage, total power, and fishermen was low (<5%). There was a consistent sharp upward trend in 2007, a gradual downward trend from 2007 to 2015, and an upward trend after reaching a low point in 2015, with the highest gross tonnage of fishing vessels reaching 25.5%. Four regression models that incorporate machine learning algorithms are used, including Lasso, Ridge, KNN, and Polynomial Features. The goodness of fit for the four models was used as the evaluation index, and the offshore annual catch based on the evaluation index was proposed. The forecasting annual catch of the polynomial model can reach 0.98. Furthermore, a comparative simulation of the DEA incorporating the polynomial model was carried out. The results show that DEA can evaluate input factors under the conditions of a given range, and the polynomial model has more advantages in forecasting annual catches. Furthermore, the combined application of DEA and polynomial model was used to analyze and discuss the management policies of China’s offshore fishery, which can provide help and reference for future management.",
        "DOI": "10.3390/jmse9121402",
        "affiliation_name": "Shanghai Ocean University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60102070",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Sports participation and value of elite sports in predicting well-being",
        "paper_author": "Silva A.",
        "publication": "Sports",
        "citied_by": "4",
        "cover_date": "2021-12-01",
        "Abstract": "This work contributes to an emerging literature focused on the role of physical activity on the subjective well-being of populations. Unlike the existing literature, it proposes an approach that uses algorithms to predict subjective well-being. The aims of this study were to determine the relative importance of sports participation and perceived value of elite sports on the subjective well-being of individuals. A total of 511 participants completed an online questionnaire. The statistical analysis used several machine learning techniques, including three algorithms, Decision Tree Classifier (DTC), Random Forest Classifier (RFC), and Gradient Boosting Classifier (GBC). In the three algorithms tested, sports participation, expressed as the weekly frequency and the time spent engaging in vigorous physical activity, showed a greater importance (between 47% and 53%) in determining subjective well-being. It also highlights the effect of perceived value of elite sport on the prediction of subjective well-being. This study provides evidence for public sport policy makers/authorities and for managers of physical activity and sport development programs. The surprising effect of the perceived value of elite sport on the prediction of subjective well-being.",
        "DOI": "10.3390/sports9120173",
        "affiliation_name": "Polytechnic Institute of Leiria",
        "affiliation_city": "Leiria",
        "affiliation_country": "Portugal",
        "affiliation_id": "60028370",
        "affiliation_state": "Leiria"
    },
    {
        "paper_title": "Event study: Advanced machine learning and statistical technique for analyzing sustainability in banking stocks",
        "paper_author": "Dogra V.",
        "publication": "Mathematics",
        "citied_by": "13",
        "cover_date": "2021-12-01",
        "Abstract": "Machine learning has grown in popularity in recent years as a method for evaluating financial text data, with promising results in stock price projection from financial news. Various research has looked at the relationship between news events and stock prices, but there is little evidence on how different sentiments (negative, neutral, and positive) of such events impact the performance of stocks or indices in comparison to benchmark indices. The goal of this paper is to analyze how a specific banking news event (such as a fraud or a bank merger) and other co-related news events (such as government policies or national elections), as well as the framing of both the news event and news-event sentiment, impair the formation of the respective bank’s stock and the banking index, i.e., Bank Nifty, in Indian stock markets over time. The task is achieved through three phases. In the first phase, we extract the banking and other co-related news events from the pool of financial news. The news events are further categorized into negative, positive, and neutral sentiments in the second phase. This study covers the third phase of our research work, where we analyze the impact of news events concerning sentiments or linguistics in the price movement of the respective bank’s stock, identified or recognized from these news events, against benchmark index Bank Nifty and the banking index against benchmark index Nifty50 for the short to long term. For the short term, we analyzed the movement of banking stock or index to benchmark index in terms of CARs (cumulative abnormal returns) surrounding the publication day (termed as D) of the news event in the event windows of (−1,D), (D,1), (−1,1), (D,5), (−5,−1), and (−5,5). For the long term, we analyzed the movement of banking stock or index to benchmark index in the event windows of (D,30), (−30,−1), (−30,30), (D,60), (−60,−1), and (−60,60). We explore the deep learning model, bidirectional encoder representations from transformers, and statistical method CAPM for this research.",
        "DOI": "10.3390/math9243319",
        "affiliation_name": "Chandigarh University",
        "affiliation_city": "Mohali",
        "affiliation_country": "India",
        "affiliation_id": "60111704",
        "affiliation_state": "PB"
    },
    {
        "paper_title": "An operational Sentinel-2 based monitoring system for the management and control of direct aids to the farmers in the context of the Common Agricultural Policy (CAP): A case study in mainland Portugal",
        "paper_author": "Navarro A.",
        "publication": "International Journal of Applied Earth Observation and Geoinformation",
        "citied_by": "7",
        "cover_date": "2021-12-01",
        "Abstract": "The European Union (EU) member states are expected to develop new procedures, based on automatic earth observation data analysis, for the management and control of direct aids to the farmers, as part of the Common Agricultural Policy (CAP) reform of 2020. Here, we propose an operational monitoring system based on Sentinel-2 surface reflectance (SR) data and machine learning (ML) algorithms, consisting of a hierarchical approach triggering 3 color-coded warning alerts to distinguish among compliant (green), non-compliant (red), and inconclusive (yellow) parcels comparatively to the farmer's declaration. A Random Forest (RF) model is applied to a 5-day interpolated SR time series to generate a preliminary crop map, where all the parcels whose predicted and declared crop type match are flagged as compliant. Next, a refinement procedure is adopted to improve the discrimination between temporary and permanent crops. At this stage, VI temporal metrics and texture are used as input to a Support Vector Machine (SVM) classifier trained using only the previous compliant parcels. Through a set of decision rules, SVM crop class predictions are flagged as compliant, non-compliant and inconclusive. The system was tested for a significant area in mainland Portugal, using the 2019 Land Parcel Information System (LPIS) data. The system returned 96.5%, 2.5% and 1% of the parcels as compliant, inconclusive, and non-compliant, respectively. Comparison with field inspections for the subsidy control of 2019 revealed that only 1.1% of the correct declarations were classified by the system as non-compliant (5% as admissible value), while less than 5% of the real non-compliant declarations passed through the system (10–20%).",
        "DOI": "10.1016/j.jag.2021.102469",
        "affiliation_name": "Faculdade de Ciências da Universidade de Lisboa",
        "affiliation_city": "Lisbon",
        "affiliation_country": "Portugal",
        "affiliation_id": "60002769",
        "affiliation_state": "Lisboa"
    },
    {
        "paper_title": "The prediction of carbon emission information in yangtze river economic zone by deep learning",
        "paper_author": "Huang H.",
        "publication": "Land",
        "citied_by": "28",
        "cover_date": "2021-12-01",
        "Abstract": "This study aimed to respond to the national “carbon peak” mid-and long-term policy plan, comprehensively promote energy conservation and emission reduction, and accurately manage and predict carbon emissions. Firstly, the proposed method analyzes the Yangtze River Economic Belt as well as its “carbon peak” and carbon emissions. Secondly, a support vector regression (SVR) machine prediction model is proposed for the carbon emission information prediction of the Yangtze River Economic Zone. This experiment uses a long short-term memory neural network (LSTM) to train the model and realize the experiment’s prediction of carbon emissions. Finally, this study obtained the fitting results of the prediction model and the training model, as well as the prediction results of the prediction model. Information indicators such as the scale of industry investment, labor efficiency output, and carbon emission intensity that affect carbon emissions in the “Yangtze River Economic Belt” basin can be used to accurately predict the carbon emissions information under this model. Therefore, the experiment shows that the SVR model for solving complex nonlinear problems can achieve a relatively excellent prediction effect under the training of LSTM. The deep learning model adopted herein realized the accurate prediction of carbon emission information in the Yangtze River Economic Zone and expanded the application space of deep learning. It provides a reference for the model in related fields of carbon emission information prediction, which has certain reference significance.",
        "DOI": "10.3390/land10121380",
        "affiliation_name": "Hefei Normal University",
        "affiliation_city": "Hefei",
        "affiliation_country": "China",
        "affiliation_id": "60104724",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Sustainability: A public policy, a concept, or a competence? efforts on the implementation of sustainability as a transversal competence throughout higher education programs",
        "paper_author": "Membrillo-Hernández J.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "24",
        "cover_date": "2021-12-01",
        "Abstract": "The concept of sustainability emerged globally in the 1987 Brundtland Report. Initially, it comprised three dimensions: environmental, social, and economic. Over time, sustainability became a global necessity that led to the establishment in 2015 of the 17 Sustainable Development Goals (SDGs), so that sustainability became a public policy of extreme urgency. Thirty-four years later, there is an imperative need to expand the original concept not in a public policy but in a competence that graduates of higher education develop, regardless of their studied academic program. We propose sustainability as a transversal competence. Our work describes the path that a higher education institution in Mexico, Tecnologico de Monterrey, has followed to accomplish this task. The new educational model Tec21 based on challenge-based learning experiences has a focus on the development of sustainability competences and actions ownership towards solving the problems described in the 17 SDGs. Our proposed definition for the sustainability transversal competence is: “The student possesses the knowledge, skills and attitudes necessary for the successful performance of the task and the resolution of problems related to the challenges and opportunities for sustainability in today’s world”. Thus, education is both an objective and a means to achieve all the other SDGs.",
        "DOI": "10.3390/su132413989",
        "affiliation_name": "Tecnológico de Monterrey",
        "affiliation_city": "Monterrey",
        "affiliation_country": "Mexico",
        "affiliation_id": "60007966",
        "affiliation_state": "NLE"
    },
    {
        "paper_title": "Smart technologies for sustainable water management: An urban analysis",
        "paper_author": "Aivazidou E.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "30",
        "cover_date": "2021-12-01",
        "Abstract": "As projections highlight that half of the global population will be living in regions facing severe water scarcity by 2050, sustainable water management policies and practices are more im-perative than ever. Following the Sustainable Development Goals for equitable water access and prudent use of natural resources, emerging digital technologies may foster efficient monitoring, control, optimization, and forecasting of freshwater consumption and pollution. Indicatively, the use of sensors, Internet of Things, machine learning, and big data analytics has been catalyzing smart water management. With two-thirds of the global population to be living in urban areas by 2050, this research focuses on the impact of digitization on sustainable urban water management. More specifically, existing scientific literature studies were explored for providing meaningful insights on smart water technologies implemented in urban contexts, emphasizing supply and distribution networks. The review analysis outcomes were classified according to three main pillars identified: (i) level of analysis (i.e., municipal or residential/industrial); (ii) technology used (e.g., sensors, algo-rithms); and (iii) research scope/focus (e.g., monitoring, optimization), with the use of a systematic approach. Overall, this study is expected to act as a methodological tool and guiding map of the most pertinent state-of-the-art research efforts to integrate digitalization in the field of water stewardship and improve urban sustainability.",
        "DOI": "10.3390/su132413940",
        "affiliation_name": "Centre for Research and Technology-Hellas",
        "affiliation_city": "Thessaloniki",
        "affiliation_country": "Greece",
        "affiliation_id": "60026208",
        "affiliation_state": "Macedonia"
    },
    {
        "paper_title": "Stargazing through the lens of AI in clinical oncology",
        "paper_author": "Lehman C.D.",
        "publication": "Nature Cancer",
        "citied_by": "9",
        "cover_date": "2021-12-01",
        "Abstract": "NA",
        "DOI": "10.1038/s43018-021-00307-4",
        "affiliation_name": "Harvard Medical School",
        "affiliation_city": "Boston",
        "affiliation_country": "United States",
        "affiliation_id": "60002746",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Vaccine technologies and platforms for infectious diseases: Current progress, challenges, and opportunities",
        "paper_author": "Ghattas M.",
        "publication": "Vaccines",
        "citied_by": "90",
        "cover_date": "2021-12-01",
        "Abstract": "Vaccination is a key component of public health policy with demonstrated cost-effective benefits in protecting both human and animal populations. Vaccines can be manufactured under multiple forms including, inactivated (killed), toxoid, live attenuated, Virus-like Particles, synthetic peptide, polysaccharide, polysaccharide conjugate (glycoconjugate), viral vectored (vector-based), nucleic acids (DNA and mRNA) and bacterial vector/synthetic antigen presenting cells. Several processes are used in the manufacturing of vaccines and recent developments in medical/biomedical engineering, biology, immunology, and vaccinology have led to the emergence of innovative nucleic acid vaccines, a novel category added to conventional and subunit vaccines. In this review, we have summarized recent advances in vaccine technologies and platforms focusing on their mechanisms of action, advantages, and possible drawbacks.",
        "DOI": "10.3390/vaccines9121490",
        "affiliation_name": "MIT School of Engineering",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States",
        "affiliation_id": "60140949",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Reinforcement learning assisted oxygen therapy for COVID-19 patients under intensive care",
        "paper_author": "Zheng H.",
        "publication": "BMC Medical Informatics and Decision Making",
        "citied_by": "9",
        "cover_date": "2021-12-01",
        "Abstract": "Background: Patients with severe Coronavirus disease 19 (COVID-19) typically require supplemental oxygen as an essential treatment. We developed a machine learning algorithm, based on deep Reinforcement Learning (RL), for continuous management of oxygen flow rate for critically ill patients under intensive care, which can identify the optimal personalized oxygen flow rate with strong potentials to reduce mortality rate relative to the current clinical practice. Methods: We modeled the oxygen flow trajectory of COVID-19 patients and their health outcomes as a Markov decision process. Based on individual patient characteristics and health status, an optimal oxygen control policy is learned by using deep deterministic policy gradient (DDPG) and real-time recommends the oxygen flow rate to reduce the mortality rate. We assessed the performance of proposed methods through cross validation by using a retrospective cohort of 1372 critically ill patients with COVID-19 from New York University Langone Health ambulatory care with electronic health records from April 2020 to January 2021. Results: The mean mortality rate under the RL algorithm is lower than the standard of care by 2.57% (95% CI: 2.08–3.06) reduction (P < 0.001) from 7.94% under the standard of care to 5.37% under our proposed algorithm. The averaged recommended oxygen flow rate is 1.28 L/min (95% CI: 1.14–1.42) lower than the rate delivered to patients. Thus, the RL algorithm could potentially lead to better intensive care treatment that can reduce the mortality rate, while saving the oxygen scarce resources. It can reduce the oxygen shortage issue and improve public health during the COVID-19 pandemic. Conclusions: A personalized reinforcement learning oxygen flow control algorithm for COVID-19 patients under intensive care showed a substantial reduction in 7-day mortality rate as compared to the standard of care. In the overall cross validation cohort independent of the training data, mortality was lowest in patients for whom intensivists’ actual flow rate matched the RL decisions.",
        "DOI": "10.1186/s12911-021-01712-6",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Boston",
        "affiliation_country": "United States",
        "affiliation_id": "60141202",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Informing equitable water and food policies through accurate spatial information on irrigated areas in smallholder farming systems",
        "paper_author": "Magidi J.",
        "publication": "Water (Switzerland)",
        "citied_by": "14",
        "cover_date": "2021-12-01",
        "Abstract": "Accurate information on irrigated areas’ spatial distribution and extent are crucial in enhancing agricultural water productivity, water resources management, and formulating strategic policies that enhance water and food security and ecologically sustainable development. However, data are typically limited for smallholder irrigated areas, which is key to achieving social equity and equal distribution of financial resources. This study addressed this gap by delineating disaggregated smallholder and commercial irrigated areas through the random forest algorithm, a non-parametric machine learning classifier. Location within or outside former apartheid “homelands” was taken as a proxy for smallholder, and commercial irrigation. Being in a medium rainfall area, the huge irrigation potential of the Inkomati-Usuthu Water Management Area (UWMA) is already well developed for commercial crop production outside former homelands. However, information about the spatial distribution and extent of irrigated areas within former homelands, which is largely informal, was missing. Therefore, we first classified cultivated lands in 2019 and 2020 as a baseline, from where the Normalised Difference Vegetation Index (NDVI) was used to distinguish irrigated from rainfed, focusing on the dry winter period when crops are predominately irrigated. The mapping accuracy of 84.9% improved the efficacy in defining the actual spatial extent of current irrigated areas at both smallholder and commercial spatial scales. The proportion of irrigated areas was high for both commercial (92.5%) and smallholder (96.2%) irrigation. Moreover, smallholder irrigation increased by over 19% between 2019 and 2020, compared to slightly over 7% in the commercial sector. Such information is critical for policy formulation regarding equitable and inclusive water allocation, irrigation expansion, land reform, and food and water security in smallholder farming systems.",
        "DOI": "10.3390/w13243627",
        "affiliation_name": "International Water Management Institute Ghana",
        "affiliation_city": "Accra",
        "affiliation_country": "Ghana",
        "affiliation_id": "60071884",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Is it all the same? Mapping and characterizing deprived urban areas using worldview-3 superspectral imagery. a case study in nairobi, kenya",
        "paper_author": "Georganos S.",
        "publication": "Remote Sensing",
        "citied_by": "16",
        "cover_date": "2021-12-01",
        "Abstract": "In the past two decades, Earth observation (EO) data have been utilized for studying the spatial patterns of urban deprivation. Given the scope of many existing studies, it is still unclear how very-high-resolution EO data can help to improve our understanding of the multidimensionality of deprivation within settlements on a city-wide scale. In this work, we assumed that multiple facets of deprivation are reflected by varying morphological structures within deprived urban areas and can be captured by EO information. We set out by staying on the scale of an entire city, while zooming into each of the deprived areas to investigate deprivation through land cover (LC) variations. To test the generalizability of our workflow, we assembled multiple WorldView-3 datasets (multispectral and shortwave infrared) with varying numbers of bands and image features, allowing us to explore computational efficiency, complexity, and scalability while keeping the model architecture consistent. Our workflow was implemented in the city of Nairobi, Kenya, where more than sixty percent of the city population lives in deprived areas. Our results indicate that detailed LC information that characterizes deprivation can be mapped with an accuracy of over seventy percent by only using RGB-based image features. Including the near-infrared (NIR) band appears to bring significant improvements in the accuracy of all classes. Equally important, we were able to categorize deprived areas into varying profiles manifested through LC variability using a gridded mapping approach. The types of deprivation profiles varied significantly both within and between deprived areas. The results could be informative for practical interventions such as land-use planning policies for urban upgrading programs.",
        "DOI": "10.3390/rs13244986",
        "affiliation_name": "Universidad de Navarra",
        "affiliation_city": "Pamplona",
        "affiliation_country": "Spain",
        "affiliation_id": "60033320",
        "affiliation_state": "Navarre"
    },
    {
        "paper_title": "The economic impact of schistosomiasis",
        "paper_author": "Rinaldo D.",
        "publication": "Infectious Diseases of Poverty",
        "citied_by": "22",
        "cover_date": "2021-12-01",
        "Abstract": "Background: The economic impact of schistosomiasis and the underlying tradeoffs between water resources development and public health concerns have yet to be quantified. Schistosomiasis exerts large health, social and financial burdens on infected individuals and households. While irrigation schemes are one of the most important policy responses designed to reduce poverty, particularly in sub-Saharan Africa, they facilitate the propagation of schistosomiasis and other diseases. Methods: We estimate the economic impact of schistosomiasis in Burkina Faso via its effect on agricultural production. We create an original dataset that combines detailed household and agricultural surveys with high-resolution geo-statistical disease maps. We develop new methods that use the densities of the intermediate host snails of schistosomiasis as instrumental variables together with panel, spatial and machine learning techniques. Results: We estimate that the elimination of schistosomiasis in Burkina Faso would increase average crop yields by around 7%, rising to 32% for high infection clusters. Keeping schistosomiasis unchecked, in turn, would correspond to a loss of gross domestic product of approximately 0.8%. We identify the disease burden as a shock to the agricultural productivity of farmers. The poorest households engaged in subsistence agriculture bear a far heavier disease burden than their wealthier counterparts, experiencing an average yield loss due to schistosomiasis of between 32 and 45%. We show that the returns to water resources development are substantially reduced once its health effects are taken into account: villages in proximity of large-scale dams suffer an average yield loss of around 20%, and this burden decreases as distance between dams and villages increases. Conclusions: This study provides a rigorous estimation of how schistosomiasis affects agricultural production and how it is both a driver and a consequence of poverty. It further quantifies the tradeoff between the economics of water infrastructures and their impact on public health. Although we focus on Burkina Faso, our approach can be applied to any country in which schistosomiasis is endemic. Graphical Abstract: [Figure not available: see fulltext.]",
        "DOI": "10.1186/s40249-021-00919-z",
        "affiliation_name": "Institut de hautes études internationales et du développement,Geneve",
        "affiliation_city": "Geneva",
        "affiliation_country": "Switzerland",
        "affiliation_id": "60100879",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Climate-based regionalization and inclusion of spectral indices for enhancing transboundary land-use/cover classification using deep learning and machine learning",
        "paper_author": "Kavhu B.",
        "publication": "Remote Sensing",
        "citied_by": "18",
        "cover_date": "2021-12-01",
        "Abstract": "Accurate land use and cover data are essential for effective land-use planning, hydrological modeling, and policy development. Since the Okavango Delta is a transboundary Ramsar site, managing natural resources within the Okavango Basin is undoubtedly a complex issue. It is often difficult to accurately map land use and cover using remote sensing in heterogeneous landscapes. This study investigates the combined value of climate-based regionalization and integration of spectral bands with spectral indices to enhance the accuracy of multi-temporal land use/cover classification using deep learning and machine learning approaches. Two experiments were set up, the first entailing the integration of spectral bands with spectral indices and the second involving the combined integration of spectral indices and climate-based regionalization based on Koppen–Geiger climate zones. Landsat 5 TM and Landsat 8 OLI images, machine learning classifiers (random forest and extreme gradient boosting), and deep learning (neural network and deep neural network) clas-sifiers were used in this study. Supervised classification using a total of 5140 samples was conducted for the years 1996, 2004, 2013, and 2020. Average overall accuracy and Kappa coefficients were used to validate the results. The study found that the integration of spectral bands with indices improves the accuracy of land use/cover classification using machine learning and deep learning. Post-feature selection combinations yield higher accuracies in comparison to combinations of bands and indices. A combined integration of spectral indices with bands and climate-based regionalization did not significantly improve the accuracy of land use/cover classification consistently for all the classifiers (p < 0.05). However, post-feature selection combinations and climate-based regionalization significantly improved the accuracy for all classifiers investigated in this study. Findings of this study will improve the reliability of land use/cover monitoring in complex heterogeneous TDBs.",
        "DOI": "10.3390/rs13245054",
        "affiliation_name": "Zimbabwe Parks and Wildlife Management Authority",
        "affiliation_city": "Causeway",
        "affiliation_country": "Zimbabwe",
        "affiliation_id": "60069306",
        "affiliation_state": "Harare"
    },
    {
        "paper_title": "Benchmarking ghg emissions forecasting models for global climate policy",
        "paper_author": "Tudor C.",
        "publication": "Electronics (Switzerland)",
        "citied_by": "21",
        "cover_date": "2021-12-01",
        "Abstract": "Climate change and pollution fighting have become prominent global concerns in the twenty-first century. In this context, accurate estimates for polluting emissions and their evolution are critical for robust policy-making processes and ultimately for solving stringent global climate challenges. As such, the primary objective of this study is to produce more accurate forecasts of greenhouse gas (GHG) emissions. This in turn contributes to the timely evaluation of the progress achieved towards meeting global climate goals set by international agendas and also acts as an early-warning system. We forecast the evolution of GHG emissions in 12 top polluting economies by using data for the 1970–2018 period and employing six econometric and machine-learning models (the exponential smoothing state-space model (ETS), the Holt–Winters model (HW), the TBATS model, the ARIMA model, the structural time series model (STS), and the neural network autoregression model (NNAR)), along with a naive model. A battery of robustness checks is performed. Results confirm a priori expectations and consistently indicate that the neural network autoregression model (NNAR) presents the best out-of-sample forecasting performance for GHG emissions at different forecasting horizons by reporting the lowest average RMSE (root mean square error) and MASE (mean absolute scaled error) within the array of predictive models. Predictions made by the NNAR model for the year 2030 indicate that total GHG emissions are projected to increase by 3.67% on average among the world’s 12 most polluting countries until 2030. Only four top polluters will record decreases in total GHG emissions values in the coming decades (i.e., Canada, the Russian Federation, the US, and China), although their emission levels will remain in the upper decile. Emission increases in a handful of developing economies will see significant growth rates (a 22.75% increase in GHG total emissions in Brazil, a 15.75% increase in Indonesia, and 7.45% in India) that are expected to offset the modest decreases in GHG emissions projected for the four countries. Our findings, therefore, suggest that the world’s top polluters cannot meet assumed pollution reduction targets in the form of NDCs under the Paris agreement. Results thus highlight the necessity for more impactful policies and measures to bring the set targets within reach.",
        "DOI": "10.3390/electronics10243149",
        "affiliation_name": "Bucharest University of Economic Studies",
        "affiliation_city": "Bucharest",
        "affiliation_country": "Romania",
        "affiliation_id": "60107810",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Can central bank speeches predict financial market turbulence? Evidence from an adaptive NLP sentiment index analysis using XGBoost machine learning technique",
        "paper_author": "Petropoulos A.",
        "publication": "Central Bank Review",
        "citied_by": "20",
        "cover_date": "2021-12-01",
        "Abstract": "Central Bank speeches usually function as aggregators of internal quantitative and qualitative analysis of the institutions regarding the macro economy, the monetary policy and the health of the financial systems. Speeches usually function as a summary of the current status of a countries economic health, the undergoing trends and some future perspectives of the global economy. In this study departing from classical econometrics we employ natural language processing technologies in combination with machine learning techniques in order to filter out the most important signals in the corpus of speeches and translate into a sentiment index for forecasting the future financial markets behaviour. In our analysis, it is evident that central banker's expectations on economy tend to exhibit a predictive ability for financial markets turmoil. Using a combination of dictionaries which are either predefined or build based on historical speeches of the corpus we train an Extreme Gradient Boosting model that generates a sentiment index which signals turmoil with acceptable accuracy when passing a specific threshold.",
        "DOI": "10.1016/j.cbrev.2021.12.002",
        "affiliation_name": "Bank of Greece",
        "affiliation_city": "Athens",
        "affiliation_country": "Greece",
        "affiliation_id": "60003090",
        "affiliation_state": "Attica"
    },
    {
        "paper_title": "Toward accelerated training of parallel support vector machines based on voronoi diagrams",
        "paper_author": "Alfaro C.",
        "publication": "Entropy",
        "citied_by": "3",
        "cover_date": "2021-12-01",
        "Abstract": "Typical applications of wireless sensor networks (WSN), such as in Industry 4.0 and smart cities, involves acquiring and processing large amounts of data in federated systems. Important challenges arise for machine learning algorithms in this scenario, such as reducing energy consumption and minimizing data exchange between devices in different zones. This paper introduces a novel method for accelerated training of parallel Support Vector Machines (pSVMs), based on ensembles, tailored to these kinds of problems. To achieve this, the training set is split into several Voronoi regions. These regions are small enough to permit faster parallel training of SVMs, reducing computational payload. Results from experiments comparing the proposed method with a single SVM and a standard ensemble of SVMs demonstrate that this approach can provide comparable performance while limiting the number of regions required to solve classification tasks. These advantages facilitate the development of energy-efficient policies in WSN.",
        "DOI": "10.3390/e23121605",
        "affiliation_name": "Universidad Rey Juan Carlos",
        "affiliation_city": "Madrid",
        "affiliation_country": "Spain",
        "affiliation_id": "60018940",
        "affiliation_state": "Madrid"
    },
    {
        "paper_title": "Adaptive Virtual Machine Consolidation Method Based on Deep Reinforcement Learning",
        "paper_author": "Yu X.",
        "publication": "Jisuanji Yanjiu yu Fazhan/Computer Research and Development",
        "citied_by": "2",
        "cover_date": "2021-12-01",
        "Abstract": "The problem of service quality optimization with energy consumption restriction has always been one of the big challenges for virtual machine (VM) resource management in data centers. Although existing work has reduced energy consumption and improved system service quality to a certain extent through VM consolidation technology, these methods are usually difficult to achieve long-term optimal management goals. Moreover, their performance is susceptible to the change of application scenarios, such that they are difficult to be replaced and will produce much management cost. In view of the problem that VM resource management in data center is hard to achieve long-term optimal energy efficiency and service quality, and also has poor flexibility in policy adjustment, this paper proposes an adaptive VM consolidation method based on deep reinforcement learning. This method builds an end-to-end decision-making model from data center system state to VM migration strategy through state tensor representation, deterministic action output, convolution neural network and weighted reward mechanism; It also designs an automatic state generation mechanism and an inverting gradient limitation mechanism to improve deep deterministic strategy gradient algorithm, speed up the convergence speed of VM migration decision-making model, and guarantee the approximately optimal management performance. Simulation experiment results based on real VM load data show that compared with popular VM consolidation methods in open source cloud platforms, this method can effectively reduce energy consumption and improve system service quality.",
        "DOI": "10.7544/issn1000-1239.2021.20200366",
        "affiliation_name": "Institute of Computing Technology Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60030904",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Spatio-Clock Synchronous Constraint Guided Safe Reinforcement Learning for Autonomous Driving",
        "paper_author": "Wang J.",
        "publication": "Jisuanji Yanjiu yu Fazhan/Computer Research and Development",
        "citied_by": "4",
        "cover_date": "2021-12-01",
        "Abstract": "Autonomous driving systems integrate complex interactions between hardware and software. In order to ensure the safe and reliable operations, formal methods are used to provide rigorous guarantees to satisfy logical specifications and safety-critical requirements in the design stage. As a widely employed machine learning architecture, deep reinforcement learning (DRL) focuses on finding an optimal policy that maximizes a cumulative discounted reward by interacting with the environment, and has been applied to autonomous driving decision-making modules. However, black-box DRL-based autonomous driving systems cannot provide guarantees of safe operation and reward definition interpretability techniques for complex tasks, especially when they face unfamiliar situations and reason about a greater number of options. In order to address these problems, spatio-clock synchronous constraint is adopted to augment DRL safety and interpretability. Firstly, we propose a dedicated formal properties specification language for autonomous driving domain, i.e., spatio-clock synchronous constraint specification language, and present domain-specific knowledge requirements specification that is close to natural language to make the reward functions generation process more interpretable. Secondly, we present domain-specific spatio-clock synchronous automata to describe spatio-clock autonomous behaviors, i.e., controllers related to certain spatio- and clock-critical actions, and present safe state-action space transition systems to guarantee the safety of DRL optimal policy generation process. Thirdly, based on the formal specification and policy learning, we propose a formal spatio-clock synchronous constraint guided safe reinforcement learning method with the goal of easily understanding the safe reward function. Finally, we demonstrate the effectiveness of our proposed approach through an autonomous lane changing and overtaking case study in the highway scenario.",
        "DOI": "10.7544/issn1000-1239.2021.20211023",
        "affiliation_name": "Nanjing University of Aeronautics and Astronautics",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60021666",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "A bibliometric analysis and benchmark of machine learning and automl in crash severity prediction: The case study of three colombian cities",
        "paper_author": "Angarita-Zapata J.S.",
        "publication": "Sensors",
        "citied_by": "15",
        "cover_date": "2021-12-01",
        "Abstract": "Traffic accidents are of worldwide concern, as they are one of the leading causes of death globally. One policy designed to cope with them is the design and deployment of road safety systems. These aim to predict crashes based on historical records, provided by new Internet of Things (IoT) technologies, to enhance traffic flow management and promote safer roads. Increasing data availability has helped machine learning (ML) to address the prediction of crashes and their severity. The literature reports numerous contributions regarding survey papers, experimental comparisons of various techniques, and the design of new methods at the point where crash severity prediction (CSP) and ML converge. Despite such progress, and as far as we know, there are no comprehensive research articles that theoretically and practically approach the model selection problem (MSP) in CSP. Thus, this paper introduces a bibliometric analysis and experimental benchmark of ML and automated machine learning (AutoML) as a suitable approach to automatically address the MSP in CSP. Firstly, 2318 bibliographic references were consulted to identify relevant authors, trending topics, keywords evolution, and the most common ML methods used in related-case studies, which revealed an opportunity for the use AutoML in the transportation field. Then, we compared AutoML (AutoGluon, Auto-sklearn, TPOT) and ML (CatBoost, Decision Tree, Extra Trees, Gradient Boosting, Gaussian Naive Bayes, Light Gradient Boosting Machine, Random Forest) methods in three case studies using open data portals belonging to the cities of Medellín, Bogotá, and Bucaramanga in Colombia. Our experimentation reveals that AutoGluon and CatBoost are competitive and robust ML approaches to deal with various CSP problems. In addition, we concluded that general-purpose AutoML effectively supports the MSP in CSP without developing domain-focused AutoML methods for this supervised learning problem. Finally, based on the results obtained, we introduce challenges and research opportunities that the community should explore to enhance the contributions that ML and AutoML can bring to CSP and other transportation areas.",
        "DOI": "10.3390/s21248401",
        "affiliation_name": "Universidad Cooperativa de Colombia",
        "affiliation_city": "Medellin",
        "affiliation_country": "Colombia",
        "affiliation_id": "60087108",
        "affiliation_state": "Antioquia"
    },
    {
        "paper_title": "Packet flow capacity autonomous operation based on reinforcement learning",
        "paper_author": "Barzegar S.",
        "publication": "Sensors",
        "citied_by": "7",
        "cover_date": "2021-12-01",
        "Abstract": "As the dynamicity of the traffic increases, the need for self-network operation becomes more evident. One of the solutions that might bring cost savings to network operators is the dynamic capacity management of large packet flows, especially in the context of packet over optical net-works. Machine Learning, particularly Reinforcement Learning, seems to be an enabler for auto-nomicity as a result of its inherent capacity to learn from experience. However, precisely because of that, RL methods might not be able to provide the required performance (e.g., delay, packet loss, and capacity overprovisioning) when managing the capacity of packet flows, until they learn the optimal policy. In view of that, we propose a management lifecycle with three phases: (i) a self-tuned threshold-based approach operating just after the packet flow is set up and until enough data on the traffic characteristics are available; (ii) an RL operation based on models pre-trained with a generic traffic profile; and (iii) an RL operation with models trained for real traffic. Exhaustive sim-ulation results confirm the poor performance of RL algorithms until the optimal policy is learnt and when traffic characteristics change over time, which prevents deploying such methods in operators’ networks. In contrast, the proposed lifecycle outperforms benchmarking approaches, achieving no-ticeable performance from the beginning of operation while showing robustness against traffic changes.",
        "DOI": "10.3390/s21248306",
        "affiliation_name": "Universitat Politècnica de Catalunya",
        "affiliation_city": "Barcelona",
        "affiliation_country": "Spain",
        "affiliation_id": "60007592",
        "affiliation_state": "Barcelona"
    },
    {
        "paper_title": "Investigating young employee stressors in contemporary society based on user-generated contents",
        "paper_author": "Wang N.",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "2",
        "cover_date": "2021-12-01",
        "Abstract": "Understanding stressors is an effective measure to decrease employee stress and improve employee mental health. The extant literature mainly focuses on a singular stressor among various aspects of their work or life. In addition, the extant literature generally uses questionnaires or interviews to obtain data. Data obtained in such ways are often subjective and lack authenticity. We propose a novel machine–human hybrid approach to conduct qualitative content analysis of user-generated online content to explore the stressors of young employees in contemporary society. The user-generated online contents were collected from a famous Q&A platform in China and we adopted natural language processing and deep learning technology to discover knowledge. Our results identified three kinds of new stressors, that is, affection from leaders, affection from the social circle, and the gap between dream and reality. These new identified stressors were due to the lack of social security and regulation, frequent occurrences of social media fearmongering, and subjective cognitive bias, respectively. In light of our findings, we offer valuable practical insights and policy recommendations to relieve stress and improve mental health of young employees. The primary contributions of our work are two-fold, as follows. First, we propose a novel approach to explore the stressors of young employees in contemporary society, which is applicable not only in China, but also in other countries and regions. Second, we expand the scope of job demands-resources (JD-R) theory, which is an important framework for the classification of employee stressors.",
        "DOI": "10.3390/ijerph182413109",
        "affiliation_name": "Shanghai University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60023813",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Automatic risk adjustment for profit maximization in renewable dominated short-term electricity markets",
        "paper_author": "Bottieau J.",
        "publication": "International Transactions on Electrical Energy Systems",
        "citied_by": "2",
        "cover_date": "2021-12-01",
        "Abstract": "State-of-the-art trading strategies in short-term electricity markets use risk awareness for reducing, inter alia, their exposure to the volatility of electricity prices. To ensure an optimal balance between risk and profit, risk-aversion parameters are traditionally fine-tuned via an offline out-of-sample analysis. Such a computationally-intensive analysis is typically run once, which yields time-invariant risk policies. Instead, this paper proposes the use of machine learning to select, in an online fashion, optimal risk-aversion parameters. This novel automatic risk-tuning approach offers the benefit of continuously adjusting the risk policy based on the dynamically changing market operating conditions. Our approach is tested on two risk-aversion parameters, that is, the confidence level of the conditional value-at-risk and the budget of uncertainty, respectively considering scenario-based and robust optimization frameworks. A set of performed case studies—focusing on the very short-term dispatch of a market actor participating in electricity markets—using real-world market data from the Belgian power system demonstrate the ability of the proposed methodology to outperform traditional offline risk policies.",
        "DOI": "10.1002/2050-7038.13152",
        "affiliation_name": "KU Leuven Energie Instituut",
        "affiliation_city": "Heverlee",
        "affiliation_country": "Belgium",
        "affiliation_id": "60121270",
        "affiliation_state": "Vlaams-Brabant"
    },
    {
        "paper_title": "Learning efficient navigation in vortical flow fields",
        "paper_author": "Gunnarson P.",
        "publication": "Nature Communications",
        "citied_by": "46",
        "cover_date": "2021-12-01",
        "Abstract": "Efficient point-to-point navigation in the presence of a background flow field is important for robotic applications such as ocean surveying. In such applications, robots may only have knowledge of their immediate surroundings or be faced with time-varying currents, which limits the use of optimal control techniques. Here, we apply a recently introduced Reinforcement Learning algorithm to discover time-efficient navigation policies to steer a fixed-speed swimmer through unsteady two-dimensional flow fields. The algorithm entails inputting environmental cues into a deep neural network that determines the swimmer’s actions, and deploying Remember and Forget Experience Replay. We find that the resulting swimmers successfully exploit the background flow to reach the target, but that this success depends on the sensed environmental cue. Surprisingly, a velocity sensing approach significantly outperformed a bio-mimetic vorticity sensing approach, and achieved a near 100% success rate in reaching the target locations while approaching the time-efficiency of optimal navigation trajectories.",
        "DOI": "10.1038/s41467-021-27015-y",
        "affiliation_name": "California Institute of Technology Division of Engineering and Applied Science",
        "affiliation_city": "Pasadena",
        "affiliation_country": "United States",
        "affiliation_id": "60143898",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Planning maize hybrids adaptation to future climate change by integrating crop modelling with machine learning",
        "paper_author": "Zhang L.",
        "publication": "Environmental Research Letters",
        "citied_by": "22",
        "cover_date": "2021-12-01",
        "Abstract": "Crop hybrid improvement is an efficient and environmental-friendly option to adapt to climate change and increase grain production. However, the adaptability of existing hybrids to a changing climate has not been systematically investigated. Therefore, little is known about the appropriate timing of hybrid adaptation. Here, using a novel hybrid model which coupled CERES-Maize with machine learning, we critically investigated the impacts of climate change on maize productivity with an ensemble of hybrid-specific estimations in China. We determined when and where current hybrids would become unviable and hybrid adaptation need be implemented, as well as which hybrid traits would be desirable. Climate change would have mostly negative impacts on maize productivity, and the magnitudes of yield reductions would highly depend on the growth cycle of the hybrids. Hybrid replacement could partially, but not completely, offset the yield loss caused by projected climate change. Without adaptation, approximately 53% of the cultivation areas would require hybrid renewal before 2050 under the RCP 4.5 and RCP 8.5 emission scenarios. The medium-maturing hybrids with a long grain-filling duration and a high light use efficiency would be promising, although the ideotypic traits could be different for a specific environment. The findings highlight the necessity and urgency of breeding climate resilient hybrids, providing policy-makers and crop breeders with the early signals of when, where and what hybrids will be required, which stimulate proactive investment to facilitate breeding. The proposed crop modelling approach is scalable, largely data-driven and can be used to tackle the longstanding problem of predicting hybrids' future performance to accelerate development of new crop hybrids.",
        "DOI": "10.1088/1748-9326/ac32fd",
        "affiliation_name": "Ministry of Agriculture of the People's Republic of China",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60087826",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Trend of the spread of COVID-19 in Indonesia using the machine learning prophet algorithm",
        "paper_author": "Hayati N.",
        "publication": "Indonesian Journal of Electrical Engineering and Computer Science",
        "citied_by": "1",
        "cover_date": "2021-12-01",
        "Abstract": "Based on information on the BNPB website on 2 September 2020, the positive rate for coronavirus disease (COVID-19) in Indonesia reached 25.25% on 30 August 2020. This is a big challenge for the Indonesian government to reduce the positivity rate to meet the standards safe accepted by World Health Organization (WHO) is 5%. To ensure the accuracy of government policies, accurate data predictions are needed. Therefore, the prophet's machine learning algorithm can be used to see trends in the spread of COVID-19 in the next one year. This algorithm has a fairly high level of accuracy because the data contains time variables which are adjusted to the dataset. In several previous research, the dataset was vast uncertain and small. Meanwhile in this research, data was taken from 2 March 2020 to 12 February 2021 on the KawalCOVID19 website. This data is used to predict from 13 February 2021 to 12 February 2022. There are 3 data used; namely data confirmed, recovered and died. Based on the analysis, the confirmed patient was 22.60-42.11%, died amounted to 21.67%-39.00%, and recovered by 22.53-41.82%. The prediction percentage that the average cases died was 2.43% every day. The accuracy of data confirmed was 43.97%, died was 72.50% and recovered was 84.24%.",
        "DOI": "10.11591/ijeecs.v24.i3.pp1780-1788",
        "affiliation_name": "Nasional University",
        "affiliation_city": "South Jakarta",
        "affiliation_country": "Indonesia",
        "affiliation_id": "120504773",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Linking multi-media modeling with machine learning to assess and predict lake chlorophyll a concentrations",
        "paper_author": "Feng Chang C.",
        "publication": "Journal of Great Lakes Research",
        "citied_by": "5",
        "cover_date": "2021-12-01",
        "Abstract": "Eutrophication and excessive algal growth pose a threat on aquatic organisms and the health of the public, environment, and the economy. Understanding what drives excessive algal growth can inform mitigation measures and aid in advance planning to minimize impacts. We demonstrate how simulated data from weather, hydrological, and agroecosystem numerical prediction models can be combined with machine learning (ML) to assess and predict chlorophyll a (chl a) concentrations, a proxy for lake eutrophication and algal biomass. The study area is Lake Erie for a 16-year period, 2002–2017. A total of 20 environmental variables from linked and coupled physical models are used as input features to train the ML model with chl a observations from 16 measuring stations. Included are meteorological variables from the Weather Research and Forecasting (WRF) model, hydrological variables from the Variable Infiltration Capacity (VIC) model, and agricultural management practice variables from the Environmental Policy Integrated Climate (EPIC) agroecosystem model. The consolidation of these variables is conducive to a successful prediction of chl a. Aside from the synergistic effects that weather, hydrology, and fertilizers have on eutrophication and excessive algal growth, we found that the application of different forms of both P and N fertilizers are highly ranked for the prediction of chl a concentration. The developed ML model successfully predicts chl a with a coefficient of determination of 0.81, bias of −0.12 μg/l and RMSE of 4.97 μg/l. The developed ML-based modeling approach can be used for impact assessment of agriculture practices in a changing climate that affect chl a concentrations in Lake Erie.",
        "DOI": "10.1016/j.jglr.2021.09.011",
        "affiliation_name": "University of Connecticut Avery Point Campus",
        "affiliation_city": "Groton",
        "affiliation_country": "United States",
        "affiliation_id": "60024223",
        "affiliation_state": "CT"
    },
    {
        "paper_title": "Quantitative impact analysis of climate change on residents’ health conditions with improving eco-efficiency in china: A machine learning perspective",
        "paper_author": "Wang X.",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "1",
        "cover_date": "2021-12-01",
        "Abstract": "Climate change affects public health, and improving eco-efficiency means reducing the various pollutants that are the result of economic activities. This study provided empirical evidence of the quantitative impact analysis of climate change on the health conditions of residents across China due to improvements that have been made to eco-efficiency. First, the indicators that were collected present adequate graphical trends and regional differences with a priori evidence about their relationships to each other; second, the present study applied Sensitivity Evaluation with Support Vector Machines (SE-SVM) to Chinese provincial panel data, taking the Visits to Hospitals, Outpatients with Emergency Treatment, and Number of Inpatients as proxy variables for the health conditions of the residents in each area and temperature, humidity, precipitation, and sunshine as the climate change variables, simultaneously incorporating the calculated eco-efficiency with six controlling indicators; third, we compared in-sample forecasting to acquire the optimal model in order to conduct elasticity analysis. The results showed that (1) temperature, humidity, precipitation, and sunshine performed well in forecasting the health conditions of the residents and that climate change was a good forecaster for resident health conditions; (2) from the national perspective, climate change had a positive relationship with Visits to Hospitals and Outpatients with Emergency Treatment but a negative relationship with the Number of Inpatients; (3) An increase in regional eco-efficiency of 1% increase the need for Visits to Hospitals and Outpatients with Emergency Treatment by 0.2242% and 0.2688%, respectively, but decreased the Number of Inpatients by 0.6272%; (4) increasing the regional eco-efficiency did not show any positive effects for any individual region because a variety of local activities, resource endowment, and the level of medical technology available in each region played different roles. The main findings of the present study are helpful for decision makers who are trying to optimize policy formulation and implementation measures in the cross-domains of economic, environmental, and public health.",
        "DOI": "10.3390/ijerph182312842",
        "affiliation_name": "Chongqing Normal University",
        "affiliation_city": "Chongqing",
        "affiliation_country": "China",
        "affiliation_id": "60010752",
        "affiliation_state": "Chongqing"
    },
    {
        "paper_title": "Machine learning in Earth and environmental science requires education and research policy reforms",
        "paper_author": "Fleming S.W.",
        "publication": "Nature Geoscience",
        "citied_by": "28",
        "cover_date": "2021-12-01",
        "Abstract": "NA",
        "DOI": "10.1038/s41561-021-00865-3",
        "affiliation_name": "USDA Natural Resources Conservation Service",
        "affiliation_city": "Washington, D.C.",
        "affiliation_country": "United States",
        "affiliation_id": "60204083",
        "affiliation_state": "DC"
    },
    {
        "paper_title": "Deep reinforcement learning-based accurate control of planetary soft landing",
        "paper_author": "Xu X.",
        "publication": "Sensors",
        "citied_by": "13",
        "cover_date": "2021-12-01",
        "Abstract": "Planetary soft landing has been studied extensively due to its promising application prospects. In this paper, a soft landing control algorithm based on deep reinforcement learning (DRL) with good convergence property is proposed. First, the soft landing problem of the powered descent phase is formulated and the theoretical basis of Reinforcement Learning (RL) used in this paper is introduced. Second, to make it easier to converge, a reward function is designed to include process rewards like velocity tracking reward, solving the problem of sparse reward. Then, by including the fuel consumption penalty and constraints violation penalty, the lander can learn to achieve velocity tracking goal while saving fuel and keeping attitude angle within safe ranges. Then, simulations of training are carried out under the frameworks of Deep deterministic policy gradient (DDPG), Twin Delayed DDPG (TD3), and Soft Actor Critic (SAC), respectively, which are of the classical RL frameworks, and all converged. Finally, the trained policy is deployed into velocity tracking and soft landing experiments, results of which demonstrate the validity of the algorithm proposed.",
        "DOI": "10.3390/s21238161",
        "affiliation_name": "Harbin Institute of Technology",
        "affiliation_city": "Harbin",
        "affiliation_country": "China",
        "affiliation_id": "60019616",
        "affiliation_state": "Heilongjiang"
    },
    {
        "paper_title": "Analysis of factors influencing forest loss in south Korea: Statistical models and machine-learning model",
        "paper_author": "Park J.",
        "publication": "Forests",
        "citied_by": "5",
        "cover_date": "2021-12-01",
        "Abstract": "Analyzing the current status of forest loss and its causes is crucial for understanding and preparing for future forest changes and the spatial pattern of forest loss. We investigated spatial patterns of forest loss in South Korea and assessed the effects of various factors on forest loss based on spatial heterogeneity. We used the local Moran’s I to classify forest loss spatial patterns as high–high clusters, low–low clusters, high–low outliers, and high–low outliers. Additionally, to assess the effect of factors on forest loss, two statistical models (i.e., ordinary least squares regression (OLS) and geographically weighted regression (GWR) models) and one machine-learning model (i.e., random forest (RF) model) were used. The accuracy of each model was determined using the R2, RMSE, MAE, and AICc. Across South Korea, the forest loss rate was highest in the Seoul–Incheon–Gyeonggi region. Moreover, high–high spatial clusters were found in the Seoul–Incheon–Gyeonggi and Daejeon– Chungnam regions. Among the models, the GWR model was the most accurate. Notably, according to the GWR model, the main factors driving forest loss were road density, cropland area, number of households, and number of tertiary industry establishments. However, the factors driving forest loss had varying degrees of influence depending on the location. Therefore, our findings suggest that spatial heterogeneity should be considered when developing policies to reduce forest loss.",
        "DOI": "10.3390/f12121636",
        "affiliation_name": "Kangwon National University",
        "affiliation_city": "Chuncheon",
        "affiliation_country": "South Korea",
        "affiliation_id": "60000872",
        "affiliation_state": "Gangwon-do"
    },
    {
        "paper_title": "The contribution of trees outside of forests to landscape carbon and climate change mitigation in west Africa",
        "paper_author": "Skole D.L.",
        "publication": "Forests",
        "citied_by": "11",
        "cover_date": "2021-12-01",
        "Abstract": "While closed canopy forests have been an important focal point for land cover change monitoring and climate change mitigation, less consideration has been given to methods for large scale measurements of trees outside of forests. Trees outside of forests are an important but often overlooked natural resource throughout sub-Saharan Africa, providing benefits for livelihoods as well as climate change mitigation and adaptation. In this study, the development of an individual tree cover map using very high-resolution remote sensing and a comparison with a new automated machine learning mapping product revealed an important contribution of trees outside of forests to landscape tree cover and carbon stocks in a region where trees outside of forests are important components of livelihood systems. Here, we test and demonstrate the use of allometric scaling from remote sensing crown area to provide estimates of landscape-scale carbon stocks. Prominent biomass and carbon maps from global-scale remote sensing greatly underestimate the “invisible” carbon in these sparse tree-based systems. The measurement of tree cover and carbon in these landscapes has important application in climate change mitigation and adaptation policies.",
        "DOI": "10.3390/f12121652",
        "affiliation_name": "Institut Sénégalais de Recherches Agricoles Dakar",
        "affiliation_city": "Dakar",
        "affiliation_country": "Senegal",
        "affiliation_id": "60071553",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Drug resistance mutations in HIV: new bioinformatics approaches and challenges",
        "paper_author": "Blassel L.",
        "publication": "Current Opinion in Virology",
        "citied_by": "32",
        "cover_date": "2021-12-01",
        "Abstract": "Drug resistance mutations appear in HIV under treatment pressure. Resistant variants can be transmitted to treatment-naive individuals, which can lead to rapid virological failure and can limit treatment options. Consequently, quantifying the prevalence, emergence and transmission of drug resistance is critical to effectively treating patients and to shape health policies. We review recent bioinformatics developments and in particular describe: (1) the machine learning approaches intended to predict and explain the level of resistance of HIV variants from their sequence data; (2) the phylogenetic methods used to survey the emergence and dynamics of resistant HIV transmission clusters; (3) the impact of deep sequencing in studying within-host and between-host genetic diversity of HIV variants, notably regarding minority resistant variants.",
        "DOI": "10.1016/j.coviro.2021.09.009",
        "affiliation_name": "Edinburgh Medical School",
        "affiliation_city": "Edinburgh",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60110825",
        "affiliation_state": "Scotland"
    },
    {
        "paper_title": "Mobility in blue-green spaces does not predict COVID-19 transmission: A global analysis",
        "paper_author": "Venter Z.S.",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "9",
        "cover_date": "2021-12-01",
        "Abstract": "Mobility restrictions during the COVID-19 pandemic ostensibly prevented the public from transmitting the disease in public places, but they also hampered outdoor recreation, despite the importance of blue-green spaces (e.g., parks and natural areas) for physical and mental health. We assess whether restrictions on human movement, particularly in blue-green spaces, affected the transmission of COVID-19. Our assessment uses a spatially resolved dataset of COVID-19 case numbers for 848 administrative units across 153 countries during the first year of the pandemic (February 2020 to February 2021). We measure mobility in blue-green spaces with planetary-scale aggregate and anonymized mobility flows derived from mobile phone tracking data. We then use machine learning forecast models and linear mixed-effects models to explore predictors of COVID-19 growth rates. After controlling for a number of environmental factors, we find no evidence that increased visits to blue-green space increase COVID-19 transmission. By contrast, increases in the total mobility and relaxation of other non-pharmaceutical interventions such as containment and closure policies predict greater transmission. Ultraviolet radiation stands out as the strongest environmental mitigant of COVID-19 spread, while temperature, humidity, wind speed, and ambient air pollution have little to no effect. Taken together, our analyses produce little evidence to support public health policies that restrict citizens from outdoor mobility in blue-green spaces, which corroborates experimental studies showing low risk of outdoor COVID-19 transmission. However, we acknowledge and discuss some of the challenges of big data approaches to ecological regression analyses such as this, and outline promising directions and opportunities for future research.",
        "DOI": "10.3390/ijerph182312567",
        "affiliation_name": "European Commission Joint Research Centre",
        "affiliation_city": "Brussels",
        "affiliation_country": "Belgium",
        "affiliation_id": "60103695",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Prevalence and predicting factors of perceived stress among Bangladeshi university students using machine learning algorithms",
        "paper_author": "Rois R.",
        "publication": "Journal of Health, Population and Nutrition",
        "citied_by": "33",
        "cover_date": "2021-12-01",
        "Abstract": "Background: Stress-related mental health problems are one of the most common causes of the burden in university students worldwide. Many studies have been conducted to predict the prevalence of stress among university students, however most of these analyses were predominantly performed using the basic logistic regression (LR) model. As an alternative, we used the advanced machine learning (ML) approaches for detecting significant risk factors and to predict the prevalence of stress among Bangladeshi university students. Methods: This prevalence study surveyed 355 students from twenty-eight different Bangladeshi universities using questions concerning anthropometric measurements, academic, lifestyles, and health-related information, which referred to the perceived stress status of the respondents (yes or no). Boruta algorithm was used in determining the significant prognostic factors of the prevalence of stress. Prediction models were built using decision tree (DT), random forest (RF), support vector machine (SVM), and LR, and their performances were evaluated using parameters of confusion matrix, receiver operating characteristics (ROC) curves, and k-fold cross-validation techniques. Results: One-third of university students reported stress within the last 12 months. Students’ pulse rate, systolic and diastolic blood pressures, sleep status, smoking status, and academic background were selected as the important features for predicting the prevalence of stress. Evaluated performance revealed that the highest performance observed from RF (accuracy = 0.8972, precision = 0.9241, sensitivity = 0.9250, specificity = 0.8148, area under the ROC curve (AUC) = 0.8715, k-fold accuracy = 0.8983) and the lowest from LR (accuracy = 0.7476, precision = 0.8354, sensitivity = 0.8250, specificity = 0.5185, AUC = 0.7822, k-fold accuracy = 07713) and SVM with polynomial kernel of degree 2 (accuracy = 0.7570, precision = 0.7975, sensitivity = 0.8630, specificity = 0.5294, AUC = 0.7717, k-fold accuracy = 0.7855). Overall, the RF model performs better and authentically predicted stress compared with other ML techniques, including individual and interaction effects of predictors. Conclusion: The machine learning framework can be detected the significant prognostic factors and predicted this psychological problem more accurately, thereby helping the policy-makers, stakeholders, and families to understand and prevent this serious crisis by improving policy-making strategies, mental health promotion, and establishing effective university counseling services.",
        "DOI": "10.1186/s41043-021-00276-5",
        "affiliation_name": "Jahangirnagar University",
        "affiliation_city": "Dhaka",
        "affiliation_country": "Bangladesh",
        "affiliation_id": "60029276",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Current applications of artificial intelligence in vascular surgery",
        "paper_author": "Fischer U.M.",
        "publication": "Seminars in Vascular Surgery",
        "citied_by": "20",
        "cover_date": "2021-12-01",
        "Abstract": "Basic foundations of artificial intelligence (AI) include analyzing large amounts of data, recognizing patterns, and predicting outcomes. At the core of AI are well-defined areas, such as machine learning, natural language processing, artificial neural networks, and computer vision. Although research and development of AI in health care is being conducted in many medical subspecialties, only a few applications have been implemented in clinical practice. This is true in vascular surgery, where applications are mostly in the translational research stage. These AI applications are being evaluated in the realms of vascular diagnostics, perioperative medicine, risk stratification, and outcome prediction, among others. Apart from the technical challenges of AI and research outcomes on safe and beneficial use in patient care, ethical issues and policy surrounding AI will present future challenges for its successful implementation. This review will give a brief overview and a basic understanding of AI and summarize the currently available and used clinical AI applications in vascular surgery.",
        "DOI": "10.1053/j.semvascsurg.2021.10.008",
        "affiliation_name": "South Texas Veterans Health Care System",
        "affiliation_city": "San Antonio",
        "affiliation_country": "United States",
        "affiliation_id": "60105859",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "Landslide susceptibility mapping using artificial neural network tuned by metaheuristic algorithms",
        "paper_author": "Mehrabi M.",
        "publication": "Environmental Earth Sciences",
        "citied_by": "68",
        "cover_date": "2021-12-01",
        "Abstract": "As a frequent natural disaster, landslides incur significant economic and human losses worldwide. The main idea of this paper is to propose novel integrative models for landslide susceptibility evaluation in a prone area of Chaharmahal va Bakhtiari Province in Iran. To do this, four metaheuristic techniques, namely chimp optimization algorithm (ChOA), crow search algorithm (CSA), satin bowerbird optimization (SBO), and water cycle algorithm (WCA) are used to supervise the training of an artificial neural network (ANN). A spatial database is created by 170 historical landslides and 14 most common conditioning factors. The models are optimized in terms of hyper-parameters and elite ones are then used to produce the susceptibility maps. During the training phase, all models could acquire a reliable understanding of the landslide pattern. The obtained accuracy index of area under the receiving operating characteristic curve (AUROC) showed that the maps produced by the WCA-ANN (AUROC = 0.925) and SBO-ANN (AUROC = 0.900) are more accurate than those of the ChOA-ANN (AUROC = 0.851) and CSA-ANN (AUROC = 0.855). Considering this assessment, the suggested hybrid models are reliable enough for landslide susceptibility modeling. Accordingly, the generated susceptibility map can be used for urban planning and generating alert systems over the studied area.",
        "DOI": "10.1007/s12665-021-10098-7",
        "affiliation_name": "Duy Tan University",
        "affiliation_city": "Da Nang",
        "affiliation_country": "Viet Nam",
        "affiliation_id": "60111656",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Machine-learning-based prediction of land prices in Seoul, South Korea",
        "paper_author": "Kim J.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "8",
        "cover_date": "2021-12-01",
        "Abstract": "The accurate estimation of real estate value helps the development of real estate policies that can respond to the complexities and instability of the real estate market. Previously, statistical methods were used to estimate real estate value, but machine learning methods have gained popularity because their predictions are more accurate. In contrast to existing studies that use various machine learning methods to estimate the transactions or list prices of real estate properties without separating the building and land prices, this study estimates land price using a large amount of land-use information obtained from various land-and building-related datasets. The random forest and XGBoost methods were used to estimate 52,900 land prices in Seoul, South Korea, from January 2017 to December 2020. The models were also separately trained for different land uses and different time periods. Overall, the results revealed that XGBoost yields a higher prediction accuracy. Whereas the XGBoost models were more accurate on the 2020 data than on the 2017–2020 data when analyzing residential areas, the random forest models were more accurate on the 2017–2020 data than on the 2020 data. Further analysis will extend the prediction model to consider submarkets determined by price volatility and locality.",
        "DOI": "10.3390/su132313088",
        "affiliation_name": "University of Texas of the Permian Basin",
        "affiliation_city": "Odessa",
        "affiliation_country": "United States",
        "affiliation_id": "60023800",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "Advancing health equity with artificial intelligence",
        "paper_author": "Thomasian N.M.",
        "publication": "Journal of Public Health Policy",
        "citied_by": "61",
        "cover_date": "2021-12-01",
        "Abstract": "Population and public health are in the midst of an artificial intelligence revolution capable of radically altering existing models of care delivery and practice. Just as AI seeks to mirror human cognition through its data-driven analytics, it can also reflect the biases present in our collective conscience. In this Viewpoint, we use past and counterfactual examples to illustrate the sequelae of unmitigated bias in healthcare artificial intelligence. Past examples indicate that if the benefits of emerging AI technologies are to be realized, consensus around the regulation of algorithmic bias at the policy level is needed to ensure their ethical integration into the health system. This paper puts forth regulatory strategies for uprooting bias in healthcare AI that can inform ongoing efforts to establish a framework for federal oversight. We highlight three overarching oversight principles in bias mitigation that maps to each phase of the algorithm life cycle.",
        "DOI": "10.1057/s41271-021-00319-5",
        "affiliation_name": "Department of Computer Science",
        "affiliation_city": "Providence",
        "affiliation_country": "United States",
        "affiliation_id": "60280409",
        "affiliation_state": "RI"
    },
    {
        "paper_title": "Inflation Rate Forecasting Using Extreme Learning Machine and Improved Particle Swarm Optimization",
        "paper_author": "Mahmudy W.F.",
        "publication": "International Journal of Intelligent Engineering and Systems",
        "citied_by": "3",
        "cover_date": "2021-12-01",
        "Abstract": "Inflation is an important tool to assess the current condition of a nation economy. Uncontrolled inflation rate may have a lot of negative impacts on economic development. Forecasting can be used to control the inflation by making appropriate economic policies. However, uncertainty pattern of the inflation rate may make it hard to forecast. This study proposes an extreme learning machine (ELM) for the inflation rate forecasting. As part of the machine learning algorithm, the ELM has an ability to address uncertainty in data input pattern. However, ELM had weakness in determining initial weights that may produce inaccurate results. So, we propose particle swarm optimization (PSO) to determine good initial weights for the ELM. PSO is a metaheuristic method that gives good results in local searches but requires longer computation time to locate its particles on the global optimum point in the vast search space area. To overcome this problem, auto-speed acceleration algorithm is employed to drive particles of the PSO in searching of the global optimum with lower computation time. The performance of the proposed approach is evaluated using root mean square error (RMSE). A series of computational experiments prove that the proposed approach achieves better results with the average RMSE of 0.01926. This result is better than RMSE of 0.02020 achieved by the original version of ELM.",
        "DOI": "10.22266/ijies2021.1231.09",
        "affiliation_name": "Brawijaya University",
        "affiliation_city": "Malang",
        "affiliation_country": "Indonesia",
        "affiliation_id": "60069392",
        "affiliation_state": "East Java"
    },
    {
        "paper_title": "It's not easy being green",
        "paper_author": "The Lancet Digital Health ",
        "publication": "The Lancet Digital Health",
        "citied_by": "1",
        "cover_date": "2021-12-01",
        "Abstract": "NA",
        "DOI": "10.1016/S2589-7500(21)00257-0",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Comparative study of groundwater level forecasts using hybrid neural network models",
        "paper_author": "Afkhamifar S.",
        "publication": "Proceedings of the Institution of Civil Engineers: Water Management",
        "citied_by": "6",
        "cover_date": "2021-12-01",
        "Abstract": "Groundwater is the world’s central supply of fresh water. Water supply policies, particularly in dry seasons, thus need to be based on accurate modelling of groundwater level (GWL) fluctuations. In the work reported in this paper, a hybrid wavelet-transform-based extreme learning machine (ELM) model was investigated for predicting GWL. Two other popular models – a wavelet-transform based artificial neural network and a wavelet-transform-based adaptive neuro-fuzzy interference system – were used to evaluate the model. GWL data and mean temperatures of observation wells in an Iranian watershed between 1981 and 2017 were used in the study. The performance of the models was assessed be evaluating their root mean square error, correlation coefficient and mean absolute error. The wavelet-transform-based ELM model outperformed the other two models with a correlation coefficient of 0.983 during a 1 month period. The model was also superior to the others in terms of training and testing speeds.",
        "DOI": "10.1680/jwama.20.00062",
        "affiliation_name": "Islamic Azad University",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran",
        "affiliation_id": "60031777",
        "affiliation_state": "Tehran"
    },
    {
        "paper_title": "Machine learning methods for “wicked” problems: exploring the complex drivers of modern slavery",
        "paper_author": "Lavelle-Hill R.",
        "publication": "Humanities and Social Sciences Communications",
        "citied_by": "11",
        "cover_date": "2021-12-01",
        "Abstract": "Forty million people are estimated to be in some form of modern slavery across the globe. Understanding the factors that make any particular individual or geographical region vulnerable to such abuse is essential for the development of effective interventions and policy. Efforts to isolate and assess the importance of individual drivers statistically are impeded by two key challenges: data scarcity and high dimensionality, typical of many “wicked problems”. The hidden nature of modern slavery restricts available data points; and the large number of candidate variables that are potentially predictive of slavery inflate the feature space exponentially. The result is a “small n, large p” setting, where overfitting and significant inter-correlation of explanatory variables can render more traditional statistical approaches problematic. Recent advances in non-parametric computational methods, however, offer scope to overcome such challenges and better capture the complex nature of modern slavery. We present an approach that combines non-linear machine-learning models and strict cross-validation methods with novel variable importance techniques, emphasising the importance of stability of model explanations via a Rashomon-set analysis. This approach is used to model the prevalence of slavery in 48 countries, with results bringing to light the importance of new predictive factors—such as a country’s capacity to protect the physical security of women, which has been previously under-emphasised in quantitative models. Further analyses uncover that women are particularly vulnerable to exploitation in areas where there is poor access to resources. Our model was then leveraged to produce new out-of-sample estimates of slavery prevalence for countries where no survey data currently exists.",
        "DOI": "10.1057/s41599-021-00938-z",
        "affiliation_name": "The Alan Turing Institute",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60111768",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "The role of good governance in the race for global vaccination during the COVID-19 pandemic",
        "paper_author": "Tatar M.",
        "publication": "Scientific Reports",
        "citied_by": "28",
        "cover_date": "2021-12-01",
        "Abstract": "Governments have developed and implemented various policies and interventions to fight the COVID-19 pandemic. COVID-19 vaccines are now being produced and distributed globally. This study investigated the role of good governance and government effectiveness indicators in the acquisition and administration of COVID-19 vaccines at the population level. Data on six World Bank good governance indicators for 172 countries for 2019 and machine-learning methods (K-Means Method and Principal Component Analysis) were used to cluster countries based on these indicators and COVID-19 vaccination rates. XGBoost was used to classify countries based on their vaccination status and identify the relative contribution of each governance indicator to the vaccination rollout in each country. Countries with the highest COVID-19 vaccination rates (e.g., Israel, United Arab Emirates, United States) also have higher effective governance indicators. Regulatory Quality is the most important indicator in predicting COVID-19 vaccination status in a country, followed by Voice and Accountability, and Government Effectiveness. Our findings suggest that coordinated global efforts led by the World Health Organization and wealthier nations may be necessary to assist in the supply and distribution of vaccines to those countries that have less effective governance.",
        "DOI": "10.1038/s41598-021-01831-0",
        "affiliation_name": "Institute for Advanced Studies in Basic Sciences, Zanjan",
        "affiliation_city": "Zanjan",
        "affiliation_country": "Iran",
        "affiliation_id": "60026492",
        "affiliation_state": "Zanjan"
    },
    {
        "paper_title": "Antibiotic resistance in microbes: History, mechanisms, therapeutic strategies and future prospects",
        "paper_author": "Uddin T.M.",
        "publication": "Journal of Infection and Public Health",
        "citied_by": "603",
        "cover_date": "2021-12-01",
        "Abstract": "Antibiotics have been used to cure bacterial infections for more than 70 years, and these low-molecular-weight bioactive agents have also been used for a variety of other medicinal applications. In the battle against microbes, antibiotics have certainly been a blessing to human civilization by saving millions of lives. Globally, infections caused by multidrug-resistant (MDR) bacteria are on the rise. Antibiotics are being used to combat diversified bacterial infections. Synthetic biology techniques, in combination with molecular, functional genomic, and metagenomic studies of bacteria, plants, and even marine invertebrates are aimed at unlocking the world's natural products faster than previous methods of antibiotic discovery. There are currently only few viable remedies, potential preventive techniques, and a limited number of antibiotics, thereby necessitating the discovery of innovative medicinal approaches and antimicrobial therapies. MDR is also facilitated by biofilms, which makes infection control more complex. In this review, we have spotlighted comprehensively various aspects of antibiotics viz. overview of antibiotics era, mode of actions of antibiotics, development and mechanisms of antibiotic resistance in bacteria, and future strategies to fight the emerging antimicrobial resistant threat.",
        "DOI": "10.1016/j.jiph.2021.10.020",
        "affiliation_name": "Sarhad University of Science and IT",
        "affiliation_city": "Peshawar",
        "affiliation_country": "Pakistan",
        "affiliation_id": "60276003",
        "affiliation_state": "Khyber Pakhtunkhwa"
    },
    {
        "paper_title": "Spatial disparity of individual and collective walking behaviors: A new theoretical framework",
        "paper_author": "Jiang Y.",
        "publication": "Transportation Research Part D: Transport and Environment",
        "citied_by": "45",
        "cover_date": "2021-12-01",
        "Abstract": "The creation of walkable environments, and the promotion of walkability for health and environmental benefits have been widely advocated. However, the term “walkability” is often associated with two related but distinct walking behaviors: individual and collective walking behaviors. It is unclear whether spatial disparity exists between them, and whether built environment characteristics have distinctive effects on them. This research was the first to explore the spatial disparity between the two types of walking behaviors. Collective walking behaviors were measured using the citywide pedestrian volume, extracted from 219,248 street view images. Individual walking behaviors were measured form a population-level survey. Spatial mismatches were found between the two types of walking behaviors and built environment elements had stronger associations with collective walking behaviors. Therefore, it is prudent to theoretically differentiate collective and individual walking behaviors, and targeted planning policies must be developed to promote one or both types of walking behaviors.",
        "DOI": "10.1016/j.trd.2021.103096",
        "affiliation_name": "City University of Hong Kong Shenzhen Research Institute",
        "affiliation_city": "Shenzhen",
        "affiliation_country": "China",
        "affiliation_id": "60176271",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Truck body type classification using a deep representation learning ensemble on 3D point sets",
        "paper_author": "Li Y.",
        "publication": "Transportation Research Part C: Emerging Technologies",
        "citied_by": "10",
        "cover_date": "2021-12-01",
        "Abstract": "Understanding the spatiotemporal distribution of commercial vehicles is essential for facilitating strategic pavement design, freight planning, and policy making. Hence, transportation agencies have been increasingly interested in collecting truck body configuration data due to its strong association with industries and freight commodities, to better understand their distinct operational characteristics and impacts on infrastructure and the environment. The rapid advancement of Light Detection and Ranging (LiDAR) technology has facilitated the development of non-intrusive detection solutions that are able to accurately classify truck body types in detail. This paper proposes a new truck classification method using a LiDAR sensor oriented to provide a wide field-of-view of roadways. In order to enrich the sparse point cloud obtained from the sensor, point clouds originating from the same truck across consecutive frames were grouped and combined using a two-stage vehicle reconstruction framework to generate a dense three-dimensional (3D) point cloud representation of each truck. Subsequently, PointNet – a deep representation learning algorithm – was adopted to train the classification model from reconstructed point clouds. The model utilizes low-level features extracted from the 3D point clouds and detects key features associated with each truck class. Finally, model ensemble techniques were explored to reduce the generalization error by averaging the results of seven PointNet models and further enhancing the overall model performance. The optimal number of models in the ensemble was determined through a comprehensive sensitivity analysis with the consideration of the average correct classification rate (CCR), the variability of the prediction results, and the computation efficiency. The developed model is capable of distinguishing passenger vehicles and 29 different truck body configurations with an average CCR of 83 percent. The average correct classification rate of the developed method on the test dataset was 90 percent for trucks pulling a large trailer(s).",
        "DOI": "10.1016/j.trc.2021.103461",
        "affiliation_name": "Samueli School of Engineering",
        "affiliation_city": "Irvine",
        "affiliation_country": "United States",
        "affiliation_id": "60142622",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Crowd-Out and Emergency Department Utilization",
        "paper_author": "Ellis C.M.",
        "publication": "Journal of Health Economics",
        "citied_by": "6",
        "cover_date": "2021-12-01",
        "Abstract": "When consumers gain Medicaid, their cost of healthcare changes. The direction of this change determines how utilization changes. The previously uninsured see a stark decrease in the price of primary care after gaining public insurance. Due to charity care, they may face an increase in the price of emergency department care. The previously insured see a reduction in emergency department prices and decreased access to primary care. We examine the impact of the prior insurance status of the newly publicly insured on substitution between healthcare. We base our identification on California's LIHP and ACA Medicaid expansions. One challenge we face is estimating crowd-out. We use machine learning techniques to predict prior insurance status based on observable covariates in cross-sectional data. We find an increase in emergency department utilization caused entirely by those crowded-out whose access to primary care has decreased. We find the opposite utilization patterns for the previously uninsured.",
        "DOI": "10.1016/j.jhealeco.2021.102542",
        "affiliation_name": "Fox School of Business",
        "affiliation_city": "Philadelphia",
        "affiliation_country": "United States",
        "affiliation_id": "60122660",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "Low-value care and excess out-of-pocket expenditure among older adults with incident cancer – A machine learning approach",
        "paper_author": "Iloabuchi C.",
        "publication": "Journal of Cancer Policy",
        "citied_by": "9",
        "cover_date": "2021-12-01",
        "Abstract": "Objective: To evaluate the association of low-value care with excess out-of-pocket expenditure among older adults diagnosed with incident breast, prostate, colorectal cancers, and Non-Hodgkin's Lymphoma. Methods: We used a retrospective cohort study design with 12-month baseline and follow-up periods. We identified a cohort of older adults (age ≥ 66 years) diagnosed with breast, prostate, colorectal cancers, or Non-Hodgkin's lymphoma between January 2014 and December 2014. We assessed low-value care and patient out-of-pocket expenditure in the follow-up period. We identified relevant low-value services using ICD9/ICD10 and CPT/HCPCS codes from the linked health claims and patient out-of-pocket expenditure from Medicare claim files and expressed expenditure in 2016 USD. Results: About 29 % of older adults received at least one low-value care procedure during the follow-up period. Low-value care differed by gender, and rates were higher in women with colorectal cancer (32.7 %) vs. (28.8 %) and NHL (40 %) vs. (39 %) compared to men. Individuals who received one or more low-value care procedures had significantly higher mean out-of-pocket expenditure ($8,726 ± $7,214) vs. ($6,802 ± $6,102). XGBOOST, a machine learning algorithm revealed that low-value care was among the five leading predictors of OOP expenditure. Conclusion: One in four older adults with incident cancer received low-value care in 12-months after a cancer diagnosis. Across all cancer populations, individuals who received low-value care had significantly higher out-of-pocket expenditure. Excess out-of-pocket expenditure was driven by low-value care, fragmentation of care, and an increasing number of pre-existing chronic conditions. Policy Statement: This study focuses on health policy issues, specifically value-based care and its findings have important clinical and policy implications for Centers for Medicare and Medicaid Services (CMS) which has issued a roadmap for states to accelerate the adoption of value-based care, with the Department of Health and Human Services (HHS) setting a goal of converting 50 % of traditional Medicare payment systems to alternative payment models tied to value-based care by 2022.",
        "DOI": "10.1016/j.jcpo.2021.100312",
        "affiliation_name": "WVU Health Sciences Center Morgantown",
        "affiliation_city": "Morgantown",
        "affiliation_country": "United States",
        "affiliation_id": "60028124",
        "affiliation_state": "WV"
    },
    {
        "paper_title": "Neural predictor based quantum architecture search",
        "paper_author": "Zhang S.X.",
        "publication": "Machine Learning: Science and Technology",
        "citied_by": "51",
        "cover_date": "2021-12-01",
        "Abstract": "Variational quantum algorithms (VQAs) are widely speculated to deliver quantum advantages for practical problems under the quantum–classical hybrid computational paradigm in the near term. Both theoretical and practical developments of VQAs share many similarities with those of deep learning. For instance, a key component of VQAs is the design of task-dependent parameterized quantum circuits (PQCs) as in the case of designing a good neural architecture in deep learning. Partly inspired by the recent success of AutoML and neural architecture search (NAS), quantum architecture search (QAS) is a collection of methods devised to engineer an optimal task-specific PQC. It has been proven that QAS-designed VQAs can outperform expert-crafted VQAs in various scenarios. In this work, we propose to use a neural network based predictor as the evaluation policy for QAS. We demonstrate a neural predictor guided QAS can discover powerful quantum circuit ansatz, yielding state-of-the-art results for various examples from quantum simulation and quantum machine learning. Notably, neural predictor guided QAS provides a better solution than that by the random-search baseline while using an order of magnitude less of circuit evaluations. Moreover, the predictor for QAS as well as the optimal ansatz found by QAS can both be transferred and generalized to address similar problems.",
        "DOI": "10.1088/2632-2153/ac28dd",
        "affiliation_name": "Tencent",
        "affiliation_city": "Shenzhen",
        "affiliation_country": "China",
        "affiliation_id": "60114181",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Vision-Language Navigation Policy Learning and Adaptation",
        "paper_author": "Wang X.",
        "publication": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "citied_by": "12",
        "cover_date": "2021-12-01",
        "Abstract": "Vision-language navigation (VLN) is the task of navigating an embodied agent to carry out natural language instructions inside real 3D environments. In this paper, we study how to address three critical challenges for this task: the cross-modal grounding, the ill-posed feedback, and the generalization problems. First, we propose a novel Reinforced Cross-Modal Matching (RCM) approach that enforces cross-modal grounding both locally and globally via reinforcement learning (RL). Particularly, a matching critic is used to provide an intrinsic reward to encourage global matching between instructions and trajectories, and a reasoning navigator is employed to perform cross-modal grounding in the local visual scene. Evaluation on a VLN benchmark dataset shows that our RCM model significantly outperforms baseline methods by 10 percent on Success Rate weighted by Path Length (SPL) and achieves the state-of-the-art performance. To improve the generalizability of the learned policy, we further introduce a Self-Supervised Imitation Learning (SIL) method to explore and adapt to unseen environments by imitating its own past, good decisions. We demonstrate that SIL can approximate a better and more efficient policy, which tremendously minimizes the success rate performance gap between seen and unseen environments (from 30.7 to 11.7 percent).",
        "DOI": "10.1109/TPAMI.2020.2972281",
        "affiliation_name": "UC Santa Barbara College of Engineering",
        "affiliation_city": "Santa Barbara",
        "affiliation_country": "United States",
        "affiliation_id": "60142701",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Analysis of the effectiveness of face-coverings on the death ratio of COVID-19 using machine learning",
        "paper_author": "Lafzi A.",
        "publication": "Scientific Reports",
        "citied_by": "4",
        "cover_date": "2021-12-01",
        "Abstract": "The recent outbreak of the COVID-19 led to death of millions of people worldwide. To stave off the spread of the virus, the authorities in the US employed different strategies, including the mask mandate order issued by the states’ governors. In the current work, we defined a parameter called average death ratio as the monthly average of the number of daily deaths to the monthly average number of daily cases. We utilized survey data to quantify people’s abidance by the mask mandate order. Additionally, we implicitly addressed the extent to which people abide by the mask mandate order, which may depend on some parameters such as population, income, and education level. Using different machine learning classification algorithms, we investigated how the decrease or increase in death ratio for the counties in the US West Coast correlates with the input parameters. The results showed that for the majority of counties, the mask mandate order decreased the death ratio, reflecting the effectiveness of such a preventive measure on the West Coast. Additionally, the changes in the death ratio demonstrated a noticeable correlation with the socio-economic condition of each county. Moreover, the results showed a promising classification accuracy score as high as 90%.",
        "DOI": "10.1038/s41598-021-01005-y",
        "affiliation_name": "Swanson School of Engineering",
        "affiliation_city": "Pittsburgh",
        "affiliation_country": "United States",
        "affiliation_id": "60159192",
        "affiliation_state": "PA"
    }
]