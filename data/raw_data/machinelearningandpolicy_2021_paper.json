[
    {
        "paper_title": "Reinforcement learning-based register renaming policy for simultaneous multithreading CPUs[Formula presented]",
        "paper_author": "Zhan H.",
        "publication": "Expert Systems with Applications",
        "citied_by": "1",
        "cover_date": "2021-12-30",
        "Abstract": "Simultaneous multithreading (SMT) improves the performance of superscalar CPUs by exploiting thread-level parallelism with shared entries for better utilization of resources. A key issue for this out-of-order execution is that the occupancy latency of a physical rename register can be undesirably long due to many program execution-dependent factors that result in performance degradation. Such an issue becomes even more problematic in an SMT environment in which these registers are shared among concurrently running threads. Smartly managing this critical shared resource to ensure that slower threads do not block faster threads’ execution is essential to the advancement of SMT performance. In this paper, an actor–critic style reinforcement learning (RL) algorithm is proposed to dynamically assigning an upper-bound (cap) of the rename registers any thread is allowed to use according to the threads’ real-time demand. In particular, a critic network projects the current Issue Queues (IQ) usage, register file usage, and the cap value to a reward; an actor network is trained to project the current IQ usage and register file usage to the optimal real-time cap value via ascending the instructions per cycle (IPC) gradient within the trajectory distribution. The proposed method differs from the state-of-the-art (Wang and Lin, 2018) as the cap for the rename registers for each thread is adjusted in real-time according to the policy and state transition from self-play. The proposed method shows an improvement in IPC up to 162.8% in a 4-threaded system, 154.8% in a 6-threaded system and up to 101.7% in an 8-threaded system. The code is now available open source at https://github.com/98k-bot/RL-based-SMT-Register-Renaming-Policy.",
        "DOI": "10.1016/j.eswa.2021.115717",
        "affiliation_name": "Edward E. Whitacre Jr. College of Engineering",
        "affiliation_city": "Lubbock",
        "affiliation_country": "United States",
        "affiliation_id": "60149446",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "An approach to managing innovation to protect financial sector against cybercrime",
        "paper_author": "Kuzmenko O.V.",
        "publication": "Polish Journal of Management Studies",
        "citied_by": "11",
        "cover_date": "2021-12-29",
        "Abstract": "Ensuring the cyber security management is an ever-increasing challenge for the financial institutions and the national financial regulators. The main purpose of the research is to improve cyber security management through analyzing large data volumes of information which helps to identify potential cyber threats at an early stage. The factors of the rapid cybercrime growth via supervised learning models with associated learning (SVM) were identified and evaluated in the paper. The object of research is 21 EU countries. The paper presents the results of an empirical analysis, which showed that the cyber threats are caused by the growth of using online banking (0.49), improvement of internet user skills (0.42), expansion of activities online (0.41). The results of the research can be useful for financial institutions, national regulators and cybersecurity professionals.",
        "DOI": "10.17512/pjms.2021.24.2.17",
        "affiliation_name": "Hungarian University of Agriculture and Life Sciences",
        "affiliation_city": "Godollo",
        "affiliation_country": "Hungary",
        "affiliation_id": "60026055",
        "affiliation_state": "Pest"
    },
    {
        "paper_title": "A study on smart city research activity using bibliometric and natural language processing methods",
        "paper_author": "Wang J.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "3",
        "cover_date": "2021-12-22",
        "Abstract": "Smart cities have become a new urban development paradigm and draw much interest from the research community and society. Based on academic publications of smart city-related research, this study employs bibliometrics, natural language machine learning methods to analyze 10,000 papers indexed by Web of Science from 2009 to 2020. Bibliometrics results show that: (1) A total of 114 countries or regions worldwide have participated in smart city research, and China is the country with the highest amount of participation in the field of smart cities. (2) Smart city research has gone through three stages: the initial stage (2009-2012), the in-depth advancement stage (2013-2016), and the leap-up stage (2017-2020). Researchers paid more attention to urban attractiveness indicators such as sustainability in the early stage. In the later period, most of the research topics were clustered on improving the overall function of the city. Latent Dirichlet Allocation (LDA) topic model results revealed that research topics could be categorized into five aspects: policy research on the status quo of smart cities, data analysis and application, infrastructure construction, urban governance, and network security. Current research on smart city technologies mainly focuses on theoretical systems, technologies, and application fields. There is a lack of in-depth research and exploration in long-term construction and operation mechanisms. This research provides insight into the research status of smart city technologies and helps researchers decide on future study direction.",
        "DOI": "10.1145/3512576.3512638",
        "affiliation_name": "Chinese Academy of Agricultural Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60017705",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Revisiting academic health sciences systems a decade later: discovery to health to population to society",
        "paper_author": "Dzau V.J.",
        "publication": "The Lancet",
        "citied_by": "22",
        "cover_date": "2021-12-18",
        "Abstract": "NA",
        "DOI": "10.1016/S0140-6736(21)01752-9",
        "affiliation_name": "Emory Healthcare",
        "affiliation_city": "Atlanta",
        "affiliation_country": "United States",
        "affiliation_id": "60012837",
        "affiliation_state": "GA"
    },
    {
        "paper_title": "A Machine Learning Approach to Predict Groundwater Levels in California Reveals Ecosystems at Risk",
        "paper_author": "Rohde M.M.",
        "publication": "Frontiers in Earth Science",
        "citied_by": "19",
        "cover_date": "2021-12-17",
        "Abstract": "Groundwater dependent ecosystems (GDEs) are increasingly threatened worldwide, but the shallow groundwater resources that they are reliant upon are seldom monitored. In this study, we used satellite-based remote sensing to predict groundwater levels under groundwater dependent ecosystems across California, USA. Depth to groundwater was modelled for a 35-years period (1985–2019) within all groundwater dependent ecosystems across the state (n = 95,135). Our model was developed within Google Earth Engine using Landsat satellite imagery, climate data, and field-based groundwater data [n = 627 shallow (< 30 m) monitoring wells] as predictors in a Random Forest model. Our findings show that 1) 44% of groundwater dependent ecosystems have experienced a significant long-term (1985–2019) decline in groundwater levels compared to 28% with a significant increase; 2) groundwater level declines have intensified during the most recent two decades, with 39% of groundwater dependent ecosystems experiencing declines in the 2003–2019 period compared to 27% in the 1985–2002 period; and 3) groundwater declines are most prevalent within GDEs existing in areas of the state where sustainable groundwater management is absent. Our results indicate that declining shallow groundwater levels may be adversely impacting California’s groundwater dependent ecosystems. Particularly where groundwater levels have fallen beneath plant roots or streams thereby affecting key life processes, such as forest recruitment/succession, or hydrological processes, such as streamflow that affects aquatic habitat. In the absence of groundwater monitoring well data, our model and findings can be used to help state and local water agencies fill in data gaps of shallow groundwater conditions, evaluate potential effects on GDEs, and improve sustainable groundwater management policy in California.",
        "DOI": "10.3389/feart.2021.784499",
        "affiliation_name": "Nature Conservancy",
        "affiliation_city": "Arlington",
        "affiliation_country": "United States",
        "affiliation_id": "60017383",
        "affiliation_state": "VA"
    },
    {
        "paper_title": "Enhance load forecastability: Optimize data sampling policy by reinforcing user behaviors",
        "paper_author": "Xie G.",
        "publication": "European Journal of Operational Research",
        "citied_by": "5",
        "cover_date": "2021-12-16",
        "Abstract": "Load forecasting has long been a key task for reliable power systems planning and operation. Over the recent years, advanced metering infrastructure has proliferated in industry. This has given rise to many load forecasting methods based on frequent measurements of power states obtained by smart meters. Meanwhile, real-world constraints arising in this new setting present both challenges and opportunities to achieve high load forecastability. The bandwidth constraints often imposed on the transmission between data concentrators and utilities are one of them, which limit the amount of data that can be sampled from customers. There lacks a sampling-rate control policy that is self-adaptive to users’ load behaviors through online data interaction with the smart grid environment. In this paper, we formulate the bandwidth-constrained sampling-rate control problem as a Markov decision process (MDP) and provide a reinforcement learning (RL)-based algorithm to solve the MDP for an optimal sampling-rate control policy. The resulting policy can be updated in real time to accommodate volatile load behaviors observed in the smart grid. Numerical experiments show that the proposed RL-based algorithm outperforms competing algorithms and delivers superior predictive performance.",
        "DOI": "10.1016/j.ejor.2021.03.032",
        "affiliation_name": "Virginia Tech College of Engineering",
        "affiliation_city": "Blacksburg",
        "affiliation_country": "United States",
        "affiliation_id": "60157272",
        "affiliation_state": "VA"
    },
    {
        "paper_title": "Neural networks with disabilities: An introduction to complementary artificial intelligence",
        "paper_author": "Terziyan V.",
        "publication": "Neural Computation",
        "citied_by": "7",
        "cover_date": "2021-12-15",
        "Abstract": "Machine learning is a good tool to simulate human cognitive skills as it is about mapping perceived information to various labels or action choices, aiming at optimal behavior policies for a human or an artificial agent operating in the environment. Regarding autonomous systems, objects and situations are perceived by some receptors as divided between sensors. Reactions to the input (e.g., actions) are distributed among the particular capability providers or actuators. Cognitive models can be trained as, for example, neural networks. We suggest training such models for cases of potential disabilities. Disability can be either the absence of one or more cognitive sensors or actuators at different levels of cognitive model. We adapt several neural network architectures to simulate various cognitive disabilities. The idea has been triggered by the “coolability” (enhanced capability) paradox, according to which a person with some disability can be more efficient in using other capabilities. Therefore, an autonomous system (human or artificial) pretrained with simulated disabilities will be more efficient when acting in adversarial conditions. We consider these coolabilities as complementary artificial intelligence and argue on the usefulness if this concept for various applications.",
        "DOI": "10.1162/neco_a_01449",
        "affiliation_name": "University of Jyväskylä",
        "affiliation_city": "Jyvaskyla",
        "affiliation_country": "Finland",
        "affiliation_id": "60032398",
        "affiliation_state": "Central Finland"
    },
    {
        "paper_title": "A comparative analysis of Statistical and Computational Intelligence methodologies for the prediction of traffic-induced fine particulate matter and NO<inf>2</inf>",
        "paper_author": "Kokkinos K.",
        "publication": "Journal of Cleaner Production",
        "citied_by": "15",
        "cover_date": "2021-12-15",
        "Abstract": "With the urbanization increase, urban mobility and transportation induce higher traffic volumes causing environmental, economic and social impacts. This is due to continuous usage of fossil fuel energy resources generating air pollutants, such as nitrogen oxides (NOx), sulfur dioxide (SO2), carbon monoxide (CO), ozone (O3) and particulate matter (PM10 and PM2.5), which impact on climate and air quality and adversely affect the human health. The present paper aims at training an ensemble of forecasting methodologies for traffic-induced pollutant emissions and implementing it for predicting PM10, PM2.5 and NO2 for the case study of Cambridge, UK inner-city region. Such an ensemble enables decision makers to evaluate the impact of various transportation policies and measures on human health and the ecosystem, and subsequently contribute towards urban resilience and sustainability. Since the chemical synthesis of air pollution is triggered by meteorological factors, the forecasting incorporates them along with the traffic volumes. We opted to combine Statistical and Computational Intelligence learning methods including Adaptive Neuro Fuzzy Inference Systems (ANFIS), Long Short-Term Memory (LSTM) recurrent neural networks and Extreme Learning Machines (ELM). Initially, Multivariate Imputation by Chained Equation (MICE) and trend and seasonality removal was performed at data preprocessing and then Principal Component Analysis (PCA) highlighted the principal parameters for ANFIS to predict next day's PM10, PM2.5 and NO2 values. LSTM and ELM methods estimated next day values and compared with the ANFIS model results for hourly time series data of length 2703. The performance of the embedded models was quantified by the Root Mean Square Error (RMSE), Mean Absolute Error (MAE), Mean Square Error (MSE), and Coefficient of Determination (R2) indices. The ensemble was found to be superior in predicting PM10, PM2.5 and NO2 emissions when compared with existing traditional models.",
        "DOI": "10.1016/j.jclepro.2021.129500",
        "affiliation_name": "University of Thessaly",
        "affiliation_city": "Volos",
        "affiliation_country": "Greece",
        "affiliation_id": "60025812",
        "affiliation_state": "Thessaly"
    },
    {
        "paper_title": "Integrating satellite-derived climatic and vegetation indices to predict smallholder maize yield using deep learning",
        "paper_author": "Zhang L.",
        "publication": "Agricultural and Forest Meteorology",
        "citied_by": "79",
        "cover_date": "2021-12-15",
        "Abstract": "Timely and accurately estimating smallholder crop yield is essential for optimizing agronomic management, guiding investment and policy-making to reduce poverty and improve food security. However, the productivity of most smallholder farms around the world is still poorly estimated due to absence of technologies and field-scale data, especially for the largest smallholders across China. Here, we integrated 11,857 of field-surveyed yields across maize cultivation areas in China and heterogeneous geospatial data to predict field-level maize yield using three data-driven approaches, i.e., Least Absolute Shrinkage and Selection Operator (LASSO), Light Gradient Boosting Machine (LightGBM) and Long Short-Term Memory (LSTM). We determined the most suitable vegetation index (VI), compared the performances of satellite-derived and ground-observed climate data, and identified the optimal combination of input variables and the best method for maize yield estimation. We found that the green chlorophyll vegetation index (GCVI) outperformed the traditional visible and near-infrared-based VIs. Combining Land Surface Temperature (LST), cumulative precipitation (Pgs) and standardized precipitation index (SPI) explained about 70% of the yield variation, which was comparable to ground-observed indices. Integrating GCVI improved R2 by 0.01–0.20 depending on the methods, suggesting valuable information on biotic or abiotic stress contained by GCVI. Maize yield could be predicted 1–2-month ahead before harvesting in all agro-ecological zones. The data collected during silking period contributed more information. The LSTM model did better than LightGBM and LASSO, because of its neural network characterizing the cumulative effects of environmental factors on yield. Our study demonstrates an effective mean for large-scale crop yield estimation using publicly available data, particularly for smallholder systems where ground observations are currently limited and sparse.",
        "DOI": "10.1016/j.agrformet.2021.108666",
        "affiliation_name": "Ministry of Agriculture of the People's Republic of China",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60087826",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Urban soils as a spatial indicator of quality for urban socio-ecological systems",
        "paper_author": "Bonilla-Bedoya S.",
        "publication": "Journal of Environmental Management",
        "citied_by": "8",
        "cover_date": "2021-12-15",
        "Abstract": "The development of criteria and indicators to quantify the transition to sustainability of the urban socio-ecological systems quality is determinant for planning policies and the 21st century urban agenda. This study models the spatial variation in the concentration and distribution of some macronutrients, micronutrients, and trace nutrients in the soil of a high-altitude city in the Andes. Meanwhile, machine learning methods were employed to study some interactions between the different dimensions that constitute an urban socio-ecosystem that caused these variations. We proposed a methodology that considered two phases: a) field work to collect data on 300 soil samples; laboratory analysis to measure the concentrations of 24 macronutrients, micronutrients, and trace nutrients; and the design of geophysical, spectral, and urban co-variables; b) statistical and geo-informatics analysis, where multivariate analysis grouped the elements into factors; and, machine learning integrated with co-variables was applied to derive the intensity of each factor across the city. Multivariate statistics described the variation in soil co-concentrations with a moderate percentage (42%). Four factors were determined that grouped some of the analyzed elements, as follows: F1 (Zn, S, Cu, Pb, Ni, and Cr), F2 (Ba, Ag, K, In, and Mg), F3 (B, V, Li, and Sr), and F4 (Si and Mn). The percentage R2 out-of-bag of the spatial model were: F1 = 20%, F2 = 8%, F3 = 14%, and F4 = 10%. Our outputs show that the enrichment and contamination by anthropogenic factors, such as the increase in population density, land use, road network, and traffic generated by fossil fuel vehicles, should be prioritized in urban planning decisions.",
        "DOI": "10.1016/j.jenvman.2021.113556",
        "affiliation_name": "Universidad Regional Amazónica Ikiam",
        "affiliation_city": "Tena",
        "affiliation_country": "Ecuador",
        "affiliation_id": "60114072",
        "affiliation_state": "Napo"
    },
    {
        "paper_title": "Quantum Reinforcement Learning Applied to Board Games",
        "paper_author": "Teixeira M.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2021-12-14",
        "Abstract": "Reinforcement learning is a machine learning paradigm where an agent learns how to optimize its behavior solely through its interaction with the environment. It has been extensively studied and successfully applied to complex problems of many different domains in the past decades, i.e., robotics, games, scheduling. However, the performance of these algorithms becomes limited as the complexity and dimension of the state-action space increases. Recent advances in quantum computing and quantum information have sparked interest in possible applications to machine learning. By taking advantage of quantum mechanics, it is possible to efficiently process immense quantities of information and improve computational speed. In this work, we combined quantum computing with reinforcement learning and studied its application to a board game to assess the benefits that it can introduce, namely its impact on the learning efficiency of an agent. From the results, we concluded that the proposed quantum exploration policy improved the convergence rate of the agent and promoted a more efficient exploration of the state space.",
        "DOI": "10.1145/3486622.3493944",
        "affiliation_name": "Universidade do Porto",
        "affiliation_city": "Porto",
        "affiliation_country": "Portugal",
        "affiliation_id": "60007249",
        "affiliation_state": "Porto"
    },
    {
        "paper_title": "Tunable Neural Encoding of a Symbolic Robotic Manipulation Algorithm",
        "paper_author": "Katz G.E.",
        "publication": "Frontiers in Neurorobotics",
        "citied_by": "2",
        "cover_date": "2021-12-14",
        "Abstract": "We present a neurocomputational controller for robotic manipulation based on the recently developed “neural virtual machine” (NVM). The NVM is a purely neural recurrent architecture that emulates a Turing-complete, purely symbolic virtual machine. We program the NVM with a symbolic algorithm that solves blocks-world restacking problems, and execute it in a robotic simulation environment. Our results show that the NVM-based controller can faithfully replicate the execution traces and performance levels of a traditional non-neural program executing the same restacking procedure. Moreover, after programming the NVM, the neurocomputational encodings of symbolic block stacking knowledge can be fine-tuned to further improve performance, by applying reinforcement learning to the underlying neural architecture.",
        "DOI": "10.3389/fnbot.2021.744031",
        "affiliation_name": "Department of Computer Science",
        "affiliation_city": "College Park",
        "affiliation_country": "United States",
        "affiliation_id": "60151568",
        "affiliation_state": "MD"
    },
    {
        "paper_title": "Artificial Intelligence and Machine Learning in Sport Research: An Introduction for Non-data Scientists",
        "paper_author": "Chmait N.",
        "publication": "Frontiers in Sports and Active Living",
        "citied_by": "50",
        "cover_date": "2021-12-08",
        "Abstract": "In the last two decades, artificial intelligence (AI) has transformed the way in which we consume and analyse sports. The role of AI in improving decision-making and forecasting in sports, amongst many other advantages, is rapidly expanding and gaining more attention in both the academic sector and the industry. Nonetheless, for many sports audiences, professionals and policy makers, who are not particularly au courant or experts in AI, the connexion between artificial intelligence and sports remains fuzzy. Likewise, for many, the motivations for adopting a machine learning (ML) paradigm in sports analytics are still either faint or unclear. In this perspective paper, we present a high-level, non-technical, overview of the machine learning paradigm that motivates its potential for enhancing sports (performance and business) analytics. We provide a summary of some relevant research literature on the areas in which artificial intelligence and machine learning have been applied to the sports industry and in sport research. Finally, we present some hypothetical scenarios of how AI and ML could shape the future of sports.",
        "DOI": "10.3389/fspor.2021.682287",
        "affiliation_name": "Victoria University Melbourne, Institute for Health and Sport",
        "affiliation_city": "Melbourne",
        "affiliation_country": "Australia",
        "affiliation_id": "60111858",
        "affiliation_state": "VIC"
    },
    {
        "paper_title": "The Clinical Course of Alcohol Use Disorder Depicted by Digital Biomarkers",
        "paper_author": "Zetterström A.",
        "publication": "Frontiers in Digital Health",
        "citied_by": "4",
        "cover_date": "2021-12-07",
        "Abstract": "Aims: This study introduces new digital biomarkers to be used as precise, objective tools to measure and describe the clinical course of patients with alcohol use disorder (AUD). Methods: An algorithm is outlined for the calculation of a new digital biomarker, the recovery and exacerbation index (REI), which describes the current trend in a patient's clinical course of AUD. A threshold applied to the REI identifies the starting point and the length of an exacerbation event (EE). The disease patterns and periodicity are described by the number, length, and distance between EEs. The algorithms were tested on data from patients from previous clinical trials (n = 51) and clinical practice (n = 1,717). Results: Our study indicates that the digital biomarker-based description of the clinical course of AUD might be superior to the traditional self-reported relapse/remission concept and conventional biomarkers due to higher data quality (alcohol measured) and time resolution. We found that EEs and the REI introduce distinct tools to identify qualitative and quantitative differences in drinking patterns (drinks per drinking day, phosphatidyl ethanol levels, weekday and holiday patterns) and effect of treatment time. Conclusions: This study indicates that the disease state—level, trend and periodicity—can be mathematically described and visualized with digital biomarkers, thereby improving knowledge about the clinical course of AUD and enabling clinical decision-making and adaptive care. The algorithms provide a basis for machine-learning-driven research that might also be applied for other disorders where daily data are available from digital health systems.",
        "DOI": "10.3389/fdgth.2021.732049",
        "affiliation_name": "Ridgeview Instruments AB",
        "affiliation_city": "Uppsala",
        "affiliation_country": "Sweden",
        "affiliation_id": "60274509",
        "affiliation_state": "Uppsala"
    },
    {
        "paper_title": "Predictive auto-scaling with OpenStack Monasca",
        "paper_author": "Lanciano G.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "4",
        "cover_date": "2021-12-06",
        "Abstract": "Cloud auto-scaling mechanisms are typically based on reactive automation rules that scale a cluster whenever some metric, e.g., the average CPU usage among instances, exceeds a predefined threshold. Tuning these rules becomes particularly cumbersome when scaling-up a cluster involves non-negligible times to bootstrap new instances, as it happens frequently in production cloud services. To deal with this problem, we propose an architecture for auto-scaling cloud services based on the status in which the system is expected to evolve in the near future. Our approach leverages on time-series forecasting techniques, like those based on machine learning and artificial neural networks, to predict the future dynamics of key metrics, e.g., resource consumption metrics, and apply a threshold-based scaling policy on them. The result is a predictive automation policy that is able, for instance, to automatically anticipate peaks in the load of a cloud application and trigger ahead of time appropriate scaling actions to accommodate the expected increase in traffic. We prototyped our approach as an open-source OpenStack component, which relies on, and extends, the monitoring capabilities offered by Monasca, resulting in the addition of predictive metrics that can be leveraged by orchestration components like Heat or Senlin. We show experimental results using a recurrent neural network and a multi-layer perceptron as predictor, which are compared with a simple linear regression and a traditional non-predictive auto-scaling policy. However, the proposed framework allows for the easy customization of the prediction policy as needed.",
        "DOI": "10.1145/3468737.3494104",
        "affiliation_name": "Scuola Normale Superiore di Pisa",
        "affiliation_city": "Pisa",
        "affiliation_country": "Italy",
        "affiliation_id": "60030674",
        "affiliation_state": "RM"
    },
    {
        "paper_title": "On Detecting Growing-Up Behaviors of Malicious Accounts in Privacy-Centric Mobile Social Networks",
        "paper_author": "Yang Z.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "5",
        "cover_date": "2021-12-06",
        "Abstract": "Privacy-centric mobile social network (PC-MSN), which allows users to build intimate and private social circles, is an increasingly popular type of online social networks (OSNs). Because of strict usage policy enforced by PC-MSNs (such as restricted account and content access), malicious accounts (or users) have to act like normal accounts to accumulate credentials before committing malicious activities. Therefore, analysis merely relying on static account profile information or social graphs is ineffective to detect such growing-up accounts. Besides, existing behavior-based malicious account detection methods fail to effectively detect growing-up accounts who pretend to be benign and have similar behaviors to benign users during the growing-up stage. In this paper, we present the first comprehensive study of growing-up behaviors of malicious accounts in WeChat, one of the major PC-MSNs with billions of daily active users across the globe. Our analysis reveals that the behavior patterns of growing-up accounts are very similar to that of benign users, and yet quite different from typical malicious accounts. Based on this observation, we design Muses, a detection system that can automatically identify subtle yet effective behaviors (features) to distinguish growing-up accounts before they engage in obvious malicious campaigns. Muses is unsupervised so that it can adapt to new malicious campaigns even if the behavior patterns of malicious accounts are unknown a priori. In particular, Muses addresses the limitations of the previous supervised techniques, i.e., requiring manually labeled training sets, which is time-consuming and costly. We evaluate Muses on a large-scale anonymized dataset from WeChat with roughly 440k accounts. The experimental results show that Muses achieves 2x recall, with similar precision, compared with the previous methods. Specifically, Muses detects over 82% growing-up accounts with a precision of 90% and achieves an AUC of 0.95. Notably, Muses can also effectively detect growing-up accounts even if malicious users applied various evasion strategies.",
        "DOI": "10.1145/3485832.3488013",
        "affiliation_name": "Tencent",
        "affiliation_city": "Shenzhen",
        "affiliation_country": "China",
        "affiliation_id": "60114181",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Distilling Arbitration Logic from Traces using Machine Learning: A Case Study on NoC",
        "paper_author": "Zhou Y.",
        "publication": "Proceedings - Design Automation Conference",
        "citied_by": "4",
        "cover_date": "2021-12-05",
        "Abstract": "Arbitration logic is extensively used in modern computer architectures to dynamically determine how shared hardware resources are allocated or accessed. Recent work has shown that machine learning techniques can learn non-obvious yet effective arbitration policies, which in simulation demonstrate superior performance over human-designed heuristics. However, existing methods based on deep learning are too expensive to be directly implemented as an arbitration unit in hardware. While some prior efforts managed to manually analyze and reduce a deep learning model into relatively small circuits in certain cases, such ad hoc and labor-intensive approaches cannot easily generalize. In this work, we propose a new methodology to automatically 'distill' the arbitration logic from simulation traces. Starting by training a deep learning model, we leverage tree-based models as a bridge to convert the more complex model to a compact logic implementation. This paper presents a case study of the proposed methodology on a network-on-chip port arbitration task. Compared with an array of combinational multipliers that exactly computes the neural network output, our arbitration logic achieves up to 282x area reduction without significant performance degradation. Under the training traffic, our arbitration logic achieves up to 64x reduction in average packet latency and up to 5% increase in network throughput over the FIFO arbitration policy. The distilled arbitration policy is also able to generalize to different injection rates and traffic patterns.",
        "DOI": "10.1109/DAC18074.2021.9586301",
        "affiliation_name": "Shanghai Jiao Tong University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60025084",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Cyber-securing IoT infrastructure by modeling network traffic",
        "paper_author": "Gharakheili H.H.",
        "publication": "Security and Privacy in the Internet of Things: Architectures, Techniques, and Applications",
        "citied_by": "3",
        "cover_date": "2021-12-03",
        "Abstract": "In recent years, there has been growing recognition that machine learning is not a \"silver bullet\" that can magically detect all cyber-security threats, but is instead more effective in a narrower context - when the threat model is clear and the scope of the target activity is narrow. This chapter begins by highlighting Internet-of-Things (IoT) network threats and attack vectors, as well as existing countermeasures and their limitations. Next, a systematic approach to model the network behavior of IoT devices is developed, automatically enforcing their behavior and monitoring real-time activity using a set of flow-based anomaly detectors. The chapter outlines Software-Defined Networking-based system to enforce Manufacturer Usage Description (MUD) policies and dynamically inspect exception traffic (nonconforming to MUD profile) which is a small fraction of total packets to/from IoT devices.",
        "DOI": "10.1002/9781119607755.ch6",
        "affiliation_name": "University of New South Wales, School of Electrical Engineering and Telecommunications",
        "affiliation_city": "Sydney",
        "affiliation_country": "Australia",
        "affiliation_id": "60082337",
        "affiliation_state": "NSW"
    },
    {
        "paper_title": "A Mean-variance Optimization DQN in Autonomous Driving",
        "paper_author": "Zhao Z.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2021-12-03",
        "Abstract": "Deep reinforcement learning is a popular and effective approach for autonomous driving to achieve safe and complex self-driving with little or no human input. However, in the random and intricate driving scenarios with surrounding traffic, conventional RL-algorithms based on greedy policy might be unstable and inefficient in training because of their lack in risk sensitivity. This paper proposes an RL-learning approach for autonomous driving which includes the estimation of expected cumulative future reward and its standard deviation. We utilize the difference between expectation and standard deviation as the decision foundation to improve the risk sensitivity of policy and training performance. The proposed algorithm is implemented in the CARLA simulation environment and the results demonstrate that vehicle agent based on our learning algorithm is able to learn more efficiently and drive more safely.",
        "DOI": "10.1145/3518781.3519208",
        "affiliation_name": "SAIC",
        "affiliation_city": "San Diego",
        "affiliation_country": "United States",
        "affiliation_id": "60015916",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Contrasting urban greenness across cities with varying trends in above-normal weather events",
        "paper_author": "George J.S.",
        "publication": "Nature-Based Solutions",
        "citied_by": "1",
        "cover_date": "2021-12-01",
        "Abstract": "Effective urban management demands global empirical research findings. The work aims to determine whether green areas are more significant in cities where above-normal wet events have plateaued. This work compares the green surface cover characteristics of the entire city and identified zones of 54 cities representing cities worldwide. These cities experience varying amounts of above-normal weather conditions. This article classifies the cities in the data set into two groups: Group 1 – frequency of extreme wet weather events has plateaued, and Group 2 – frequency of extreme weather events are increasing or decreasing at higher rates. Next, the work uses structural features of co-located spatial networks, roads and terrain, to identify zones. Finally, high-resolution satellite imagery quantified urban green spaces in the city and identified zones of the two city groups using geospatial and machine learning methods for 2000 and 2020. The data is compared for two groups to understand the pattern of green spaces in cities. Empirical analysis results recognise a higher greenness index in the zones and the entire city amongst the cities of Group1. The pattern in the temporal change in the greenness index of the zones is helpful to formulate a priority plan for locating inter-linked green spaces in urban areas paving the way for Nature-based Solutions. The findings can guide planners and policy-makers to identify and prioritise locations to incorporate a robust green infrastructure network to build resilient cities by 'nature bringing a solution’, the essential concept of Nature-based Solutions.",
        "DOI": "10.1016/j.nbsj.2021.100008",
        "affiliation_name": "Indian Institute of Technology Kharagpur",
        "affiliation_city": "Kharagpur",
        "affiliation_country": "India",
        "affiliation_id": "60004750",
        "affiliation_state": "WB"
    },
    {
        "paper_title": "A PRESCRIPTIVE ANALYTICS FRAMEWORK FOR OPTIMAL POLICY DEPLOYMENT USING HETEROGENEOUS TREATMENT EFFECTS",
        "paper_author": "McFowland E.",
        "publication": "MIS Quarterly: Management Information Systems",
        "citied_by": "9",
        "cover_date": "2021-12-01",
        "Abstract": "We define a prescriptive analytics framework that addresses the needs of a constrained decision-maker facing, ex ante, unknown costs and benefits of multiple policy levers. The framework is general in nature and can be deployed in any utility-maximizing context, public or private. It relies on randomized field experiments for causal inference, machine learning for estimating heterogeneous treatment effects, and on the optimization of an integer linear program for converting predictions into decisions. The net result is the discovery of individual-level targeting of policy interventions to maximize overall utility under a budget constraint. The framework is set in the context of the four pillars of analytics and is especially valuable for companies that already have an existing practice of running A/B tests. The key contribution of this work is to develop and operationalize a framework to exploit both within- and between-treatment arm heterogeneity in the utility response function in order to derive benefits from future (optimized) prescriptions. We demonstrate the value of this framework as compared to benchmark practices—i.e., the use of the average treatment effect, uplift modeling, as well as an extension to contextual bandits—in two different settings. Unlike these standard approaches, our framework is able to recognize, adapt to, and exploit the (potential) presence of different subpopulations that experience varying costs and benefits within a treatment arm while also exhibiting differential costs and benefits across treatment arms. As a result, we find a targeting strategy that produces an order of magnitude improvement in expected total utility for the case where significant within- and between-treatment arm heterogeneity exists.",
        "DOI": "10.25300/MISQ/2021/15684",
        "affiliation_name": "Carlson School of Management",
        "affiliation_city": "Minneapolis",
        "affiliation_country": "United States",
        "affiliation_id": "60123802",
        "affiliation_state": "MN"
    },
    {
        "paper_title": "Monitoring rice cropping system in Cambodia and its influencing factors using time series MODIS images",
        "paper_author": "Huang C.",
        "publication": "Resources Science",
        "citied_by": "4",
        "cover_date": "2021-12-01",
        "Abstract": "Cambodia has abundant arable land resources, suitable temperature, and great potential for rice plantation. Timely acquisition of rice cropping system information is important for regional rice production management, disaster risk assessment, and food policy formulation. Most traditional rice remote sensing monitoring studies only provide spatial patterns of rice distribution at the interannual scale, and information on rice planting and harvesting at the intraannual scale is often lacking. In this study, first all available MODIS time series data in a year were used to construct an image-based MODIS NDVI annual time series curve; the maximum value, minimum value, mean value, and standard deviation were selected to calculate the image- by- image time series statistical parameter features, and the FastDTW algorithm was used to calculate the similarity features between the image-by-image time series curve and the rice reference time series curve, and then the time series statistical features were combined with the time series curve similarity features, and the rice maturation information was extracted by supervised classification through machine learning using a random forest classifier. Finally, the rice phenological features extracted from the time series curves were combined to generate rice harvest time information for the identification of rice cultivation types. The study showed that rice cultivation in Cambodia is mainly concentrated in the lowland plains around the Tonle Sap Lake and the lower Mekong River. Although the thermal conditions in Cambodia are suitable for rice cultivation throughout the year, water access constraints have a significant impact on the spatial and temporal patterns of rice cultivation in the country. The rice maturity mode indicates that production was dominated by single-season rice, which accounted for about 80% of the annual rice cultivation area and had a stable distribution area; double-season rice accounted for about 20% of the area and showed a large interannual variation in the spatial distribution of cultivation. Wet season rice was the main type of rice cultivation in the country, and the planted area accounted for about 70% of the annual rice area with little interannual variation; dry season rice and ex-rainy season rice accounted for about 30% of the area, with significant interannual variation in spatial distribution. The analysis of rice cropping patterns in 2011 and 2016 showed that irrigation conditions and flooding had important spatial and temporal effects on rice cultivation in Cambodia. This study identified the main influencing factors through high-precision monitoring of the interannual and intraannual spatial and temporal patterns of rice cultivation in Cambodia, which provide a reference for the development of a locally adapted and resilient rice cultivation system to ensure food security in the country.",
        "DOI": "10.18402/resci.2021.12.03",
        "affiliation_name": "Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60031150",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "QSPCA: A two-stage efficient power control approach in D2D communication for 5G networks",
        "paper_author": "Chandra S.",
        "publication": "Intelligent and Converged Networks",
        "citied_by": "10",
        "cover_date": "2021-12-01",
        "Abstract": "The existing literature on device-to-device (D2D) architecture suffers from a dearth of analysis under imperfect channel conditions. There is a need for rigorous analyses on the policy improvement and evaluation of network performance. Accordingly, a two-stage transmit power control approach (named QSPCA) is proposed: First, a reinforcement Q-learning based power control technique and; second, a supervised learning based support vector machine (SVM) model. This model replaces the unified communication model of the conventional D2D setup with a distributed one, thereby requiring lower resources, such as D2D throughput, transmit power, and signal-to-interference-plus-noise ratio as compared to existing algorithms. Results confirm that the QSPCA technique is better than existing models by at least 15.31% and 19.5% in terms of throughput as compared to SVM and Q-learning techniques, respectively. The customizability of the QSPCA technique opens up multiple avenues and industrial communication technologies in 5G networks, such as factory automation.",
        "DOI": "10.23919/ICN.2021.0021",
        "affiliation_name": "National Institute of Technology Patna",
        "affiliation_city": "Patna",
        "affiliation_country": "India",
        "affiliation_id": "60104351",
        "affiliation_state": "BR"
    },
    {
        "paper_title": "Machine learning models of tobacco susceptibility and current use among adolescents from 97 countries in the Global Youth Tobacco Survey, 2013-2017",
        "paper_author": "Kim N.",
        "publication": "PLOS Global Public Health",
        "citied_by": "7",
        "cover_date": "2021-12-01",
        "Abstract": "Adolescents are particularly vulnerable to tobacco initiation and escalation. Identifying factors associated with adolescent tobacco susceptibility and use can guide tobacco prevention efforts. Novel machine learning (ML) approaches efficiently identify interactive relations among factors of tobacco risks and identify high-risk subpopulations that may benefit from targeted prevention interventions. Nationally representative cross-sectional 2013–2017 Global Youth Tobacco Survey (GYTS) data from 97 countries (28 high-income and 69 low-and middle-income countries) from 342,481 adolescents aged 13–15 years (weighted N = 52,817,455) were analyzed using ML regression tree models, accounting for sampling weights. Predictors included demographics (sex, age), geography (region, country-income), and self-reported exposure to tobacco marketing, secondhand smoke, and tobacco control policies. 11.9% (95% CI 11.1%-12.6%) of tobacco-naïve adolescents were susceptible to tobacco use and 11.7% (11.0%-12.5%) of adolescents reported using any tobacco product (cigarettes, other smoked tobacco, smokeless tobacco) in the past 30 days. Regression tree models found that exposure or receptivity to tobacco industry promotions and secondhand smoke exposure predicted increased risks of susceptibility and use, while support for smoke-free air policies predicted decreased risks of tobacco susceptibility and use. Anti-tobacco school education and health warning messages on product packs predicted susceptibility or use, but their protective effects were not evident across all adolescent subgroups. Sex, region, and country-income moderated the effects of tobacco promotion and control factors on susceptibility or use, showing higher rates of susceptibility and use in males and high-income countries, Africa and the Americas (susceptibility), and Europe and Southeast Asia (use). Tobacco policy-related factors robustly predicted both tobacco susceptibility and use in global adolescents, and interacted with adolescent characteristics and other environments in complex ways that stratified adolescents based on their tobacco risk. These findings emphasize the importance of efficient ML modeling of interactions in tobacco risk prediction and suggest a role for targeted prevention strategies for high-risk adolescents.",
        "DOI": "10.1371/journal.pgph.0000060",
        "affiliation_name": "School of Computer, Data &amp; Information Sciences",
        "affiliation_city": "Madison",
        "affiliation_country": "United States",
        "affiliation_id": "60153202",
        "affiliation_state": "WI"
    },
    {
        "paper_title": "Soil carbon-food synergy: sizable contributions of small-scale farmers",
        "paper_author": "Iizumi T.",
        "publication": "CABI Agriculture and Bioscience",
        "citied_by": "8",
        "cover_date": "2021-12-01",
        "Abstract": "Background: Benefits to agricultural yield improvement, soil degradation prevention, and climate mitigation are central to the synergies of soil organic carbon (SOC) build-up. However, the contributions of small-scale farmers, the main target of recent agricultural and rural development policies, to SOC enhancement are understudied. Here, we present a global analysis of small-scale farmers’ contributions to the potential of additional SOC stocks and the associated increase in crop production. Methods: We applied random forest machine learning models to global gridded datasets on crop yield (wheat, maize, rice, soybean, sorghum and millet), soil, climate and agronomic management practices from the 2000s (n = 1808 to 8123). Using the established crop-specific SOC-yield relationships, the potentials of additional SOC build-up and crop production increase were simulated. The estimated SOC increase was converted into global decadal mean temperature change using the temperature sensitivity to cumulative total anthropogenic CO2 emissions from preindustrial levels. The amount of inorganic nitrogen (N) input that would result in the same yield outcome as the SOC build-up was derived from the crop-specific N-yield relationships. Results: SOC contributes to yields in addition to management and climatic factors. Additional SOC sums up to 12.78 GtC (11.55–14.05 GtC) of global SOC stock, which earns 38.24 Mt (22.88–57.48 Mt) of additional crop production and prevents warming by 0.030 °C (0.019–0.041 °C). This production increase equates to what would be achieved by an inorganic N input of 5.82 Mt N (3.89–7.14 Mt N). Small-scale farmers account for 28% (26–30%) of the additional SOC build-up and 17% (15–20%) of the production increase. Key crops and regions in terms of small-scale farmers’ contributions include Sub-Saharan African maize and rice, Latin American and Caribbean soybean and maize, and South Asian rice and wheat. Conclusions: The contribution of small-scale farmers to the potential increase in SOC stock and crop production is sizable, which in theory further leads to saving inorganic N input. These findings emphasize the importance of linking soil management to sustainable land and climate mitigation with institutions and policy for small-scale farmers. Such a joint policy would assist multiple development goals.",
        "DOI": "10.1186/s43170-021-00063-6",
        "affiliation_name": "Institute for Agro-Environmental Sciences, NARO",
        "affiliation_city": "Tsukuba",
        "affiliation_country": "Japan",
        "affiliation_id": "60006155",
        "affiliation_state": "Ibaraki"
    },
    {
        "paper_title": "Simulation of Urban Land Use Growth Scenarios Using the Cellular Automata Method of SLEUTH",
        "paper_author": "Jahanishakib F.",
        "publication": "Journal of Environmental Studies",
        "citied_by": "3",
        "cover_date": "2021-12-01",
        "Abstract": "Accelerating urban expansion is increasingly challenging the sustainable use of land, since modeling urban growth is important in order to adapt to balanced development. This study was carried out with the aim of simulating the future urban expansion of Birjand Metropolitan from 2020 to 2050 using Cellular Automata (CA) methodology in the SLEUTH modeling considering two scenarios: historical and environmental growth. Calibration results of the SLEUTH model showed Diffusion (65) and slope resistance coefficients (42) have the most effect on historical and environmental growth, respectively. The model was calibrated in three stages: coarse, fine and final, using the best fitted values of OSM and Leesale indices. The simulation results showed that urban expansion in the historical and environmental growth scenarios will be 2201.85 and 2150.91 hectares, respectively, so increasing the area of Birjand metropolis is inevitable in both scenarios. A comparison of the two scenarios denoted that in the historical growth scenario, the urban growth rate is higher and the vegetation destruction is maximal. The findings of this study can help policy makers and managers in formulating informed urban planning strategies to have the least destructive effect on the environment in the future.",
        "DOI": "10.22059/JES.2021.328447.1008215",
        "affiliation_name": "University of Zabol",
        "affiliation_city": "Zabol",
        "affiliation_country": "Iran",
        "affiliation_id": "60089331",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Temporal Variations and Spatial Disparities in Public Sentiment Toward COVID-19 and Preventive Practices in the United States: Infodemiology Study of Tweets",
        "paper_author": "Kahanek A.",
        "publication": "JMIR Infodemiology",
        "citied_by": "2",
        "cover_date": "2021-12-01",
        "Abstract": "Background: During the COVID-19 pandemic, US public health authorities and county, state, and federal governments recommended or ordered certain preventative practices, such as wearing masks, to reduce the spread of the disease. However, individuals had divergent reactions to these preventive practices. Objective: The purpose of this study was to understand the variations in public sentiment toward COVID-19 and the recommended or ordered preventive practices from the temporal and spatial perspectives, as well as how the variations in public sentiment arerelated to geographical and socioeconomic factors. Methods: The authors leveraged machine learning methods to investigate public sentiment polarity in COVID-19-related tweets from January 21, 2020 to June 12, 2020. The study measured the temporal variations and spatial disparities in public sentiment toward both general COVID-19 topics and preventive practices in the United States. Results: In the temporal analysis, we found a 4-stage pattern from high negative sentiment in the initial stage to decreasing and low negative sentiment in the second and third stages, to the rebound and increase in negative sentiment in the last stage. We also identified that public sentiment to preventive practices was significantly different in urban and rural areas, while poverty rate and unemployment rate were positively associated with negative sentiment to COVID-19 issues. Conclusions: The differences between public sentiment toward COVID-19 and the preventive practices imply that actions need to be taken to manage the initial and rebound stages in future pandemics. The urban and rural differences should be considered in terms of the communication strategies and decision making during a pandemic. This research also presents a framework to investigate time-sensitive public sentiment at the county and state levels, which could guide local and state governments and regional communities in making decisions and developing policies in crises.",
        "DOI": "10.2196/31671",
        "affiliation_name": "University of North Texas",
        "affiliation_city": "Denton",
        "affiliation_country": "United States",
        "affiliation_id": "60024438",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "Smoothing and the environmental manifold",
        "paper_author": "Unnithan Kumar S.",
        "publication": "Ecological Informatics",
        "citied_by": "11",
        "cover_date": "2021-12-01",
        "Abstract": "How the observed occurrences of a species relate to environmental gradients is a fundamental question in community ecology. In this paper, we present a new approach to address this question, using the smoothing function, a method not previously recruited into this ecological context. Using simulation techniques, we explore its accuracy in recovering known species distributions from simulated noisy data, and we compare the smoothing function's predictive abilities to two widely used methods in this field, the generalised linear model (GLM) and random forest machine learning. In studying the smoothing function, we are led to consider a new analytical tool for ecology, which we call the environmental manifold. It is given by the shape of the data cloud of sampled environmental predictor variables, and has deep relevance to ecological niche theory. Hitherto not considered in ecological analyses, it plays a fundamental role in understanding the species-environment relationship, and we utilise it to compare the performance and behaviour of these three methods. The results of our analysis find both random forest and smoothing to be robust to the complexities of the species-environment relationship, and also, to a degree, the shape of the environmental manifold. In contrast, the GLM's accuracy depends heavily on the complexity of the species-environment relationship, and is also affected by the geometry of the environmental manifold. Furthermore, the smoothing function is seen to be more accurate than random forest in every combination of species-environment relationship and environmental manifold shape, and also less affected by sampling bias. This suggests the promising role that such smoothing functions can have in ecological analyses. Our results also support the robustness of random forest machine learning to nonlinearity in both the species-environment relationship, and for the first time, the complexity of the shape of the environmental manifold. We conclude by discussing the implications and uses of the environmental manifold in ecological practice and theory, including its importance for niche theory, understanding species distributions, and conservation policy.",
        "DOI": "10.1016/j.ecoinf.2021.101472",
        "affiliation_name": "University of Oxford",
        "affiliation_city": "Oxford",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60026851",
        "affiliation_state": "Oxfordshire"
    },
    {
        "paper_title": "A three-level hierachical framework for additive manufacturing",
        "paper_author": "Ren Y.M.",
        "publication": "Digital Chemical Engineering",
        "citied_by": "9",
        "cover_date": "2021-12-01",
        "Abstract": "Metal alloy additive manufacturing (AM) has gained wide industrial interest in the past decade due to its capability to efficiently deliver complicated mechanical parts with high quality. However, due to a lack of understanding of the fundamental correlation between the operating conditions and build quality, the exploration of the optimal operating policy of the AM process is costly and difficult. In this work, a data-driven process optimization framework has been proposed for the additive manufacturing process, integrating machine learning, finite-element method (FEM) modeling, and cloud-edge data storage/transfer optimization. A three-level hierarchy of local machines, factory clouds, and a research center is introduced with each level responsible for its dedicated tasks. In addition, to ensure the efficiency of data transfer and storage, an edge-cloud data transfer scheme is constructed, which serves as a guideline for the data flow in the AM framework. Moreover, an overview of the connections between the proposed framework and the Industry 4.0 framework is presented.",
        "DOI": "10.1016/j.dche.2021.100001",
        "affiliation_name": "UCLA Samueli School of Engineering",
        "affiliation_city": "Los Angeles",
        "affiliation_country": "United States",
        "affiliation_id": "60153950",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Statistical Machine Learning Approaches to Liver Disease Prediction",
        "paper_author": "Mostafa F.",
        "publication": "Livers",
        "citied_by": "30",
        "cover_date": "2021-12-01",
        "Abstract": "Medical diagnoses have important implications for improving patient care, research, and policy. For a medical diagnosis, health professionals use different kinds of pathological methods to make decisions on medical reports in terms of the patients’ medical conditions. Recently, clinicians have been actively engaged in improving medical diagnoses. The use of artificial intelligence and machine learning in combination with clinical findings has further improved disease detection. In the modern era, with the advantage of computers and technologies, one can collect data and visualize many hidden outcomes such as dealing with missing data in medical research. Statistical machine learning algorithms based on specific problems can assist one to make decisions. Machine learning (ML), data-driven algorithms can be utilized to validate existing methods and help researchers to make potential new decisions. The purpose of this study was to extract significant predictors for liver disease from the medical analysis of 615 humans using ML algorithms. Data visualizations were implemented to reveal significant findings such as missing values. Multiple imputations by chained equations (MICEs) were applied to generate missing data points, and principal component analysis (PCA) was used to reduce the dimensionality. Variable importance ranking using the Gini index was implemented to verify significant predictors obtained from the PCA. Training data ((Formula presented.)) for learning and testing data ((Formula presented.)) in the ML methods were used for predicting classifications. The study compared binary classifier machine learning algorithms (i.e., artificial neural network, random forest (RF), and support vector machine), which were utilized on a published liver disease data set to classify individuals with liver diseases, which will allow health professionals to make a better diagnosis. The synthetic minority oversampling technique was applied to oversample the minority class to regulate overfitting problems. The RF significantly contributed ((Formula presented.)) to a higher accuracy score of 98.14% compared to the other methods. Thus, this suggests that ML methods predict liver disease by incorporating the risk factors, which may improve the inference-based diagnosis of patients.",
        "DOI": "10.3390/livers1040023",
        "affiliation_name": "Texas Tech University Health Sciences Center at Lubbock",
        "affiliation_city": "Lubbock",
        "affiliation_country": "United States",
        "affiliation_id": "60032964",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "Machine learning-based approach: Global trends, research directions, and regulatory standpoints",
        "paper_author": "Pugliese R.",
        "publication": "Data Science and Management",
        "citied_by": "169",
        "cover_date": "2021-12-01",
        "Abstract": "The field of machine learning (ML) is sufficiently young that it is still expanding at an accelerating pace, lying at the crossroads of computer science and statistics, and at the core of artificial intelligence (AI) and data science. Recent progress in ML has been driven both by the development of new learning algorithms theory, and by the ongoing explosion in the availability of vast amount of data (often referred to as “big data”) and low-cost computation. The adoption of ML-based approaches can be found throughout science, technology and industry, leading to more evidence-based decision-making across many walks of life, including healthcare, biomedicine, manufacturing, education, financial modeling, data governance, policing, and marketing. Although the past decade has witnessed the increasing interest in these fields, we are just beginning to tap the potential of these ML algorithms for studying systems that improve with experience. In this paper, we present a comprehensive view on geo worldwide trends (taking into account China, the USA, Israel, Italy, the UK, and the Middle East) of ML-based approaches highlighting rapid growth in the last 5 years attributable to the introduction of related national policies. Furthermore, based on the literature review, we also discuss the potential research directions in this field, summarizing some popular application areas of machine learning technology, such as healthcare, cyber-security systems, sustainable agriculture, data governance, and nanotechnology, suggesting that the “dissemination of research” in the ML scientific community has undergone the exceptional growth in the time range of 2018–2020, reaching a value of 16,339 publications. Finally, we report the challenges and the regulatory standpoints for managing ML technology. Overall, we hope that this work will help to explain the geo trends of ML approaches and their applicability in various real-world domains, as well as serve as a reference point for both academia and industry professionals, particularly from a technical, ethical and regulatory point of view.",
        "DOI": "10.1016/j.dsm.2021.12.002",
        "affiliation_name": "Asst Grande Ospedale Metropolitano Niguarda",
        "affiliation_city": "Milan",
        "affiliation_country": "Italy",
        "affiliation_id": "60015852",
        "affiliation_state": "MI"
    },
    {
        "paper_title": "Who determines United States Healthcare out-of-pocket costs? Factor ranking and selection using ensemble learning",
        "paper_author": "Zhang C.",
        "publication": "Health Information Science and Systems",
        "citied_by": "3",
        "cover_date": "2021-12-01",
        "Abstract": "Purpose: Healthcare out-of-pocket (OOP) costs consist of the annual expenses paid by individuals or families that are not reimbursed by insurance. In the U.S, broadening healthcare disparities are caused by the rapid increase in OOP costs. With a precise forecast of the OOP costs, governments can improve the design of healthcare policies to better control the OOP costs. This study designs a purely data-driven ensemble learning procedure to achieve a collection of factors that best predict OOP costs. Methods: We propose a voting ensemble learning procedure to rank and select factors of OOP costs based on the Medical Expenditure Panel Survey dataset. The method involves utilizing votes from the base learners forward subset selection, backward subset selection, random forest, and LASSO. Results: The top-ranking factors selected by our proposed method are insurance type, age, asthma, family size, race, and number of physician office visits. The predictive models using these factors outperform the models that employ the factors commonly considered by the literature through improving the prediction error (test MSE of the OOP costs’ log-odds) from 0.462 to 0.382. Conclusion: Our results indicate a set of factors which best explain the OOP costs behavior based on a purely data-driven solution. These findings contribute to the discussions regarding demand-side needs for containing rapidly rising OOP costs. Instead of estimating the impact of a single factor on OOP costs, our proposed method allows for the selection of arbitrary-sized factors to best explain OOP costs.",
        "DOI": "10.1007/s13755-021-00153-9",
        "affiliation_name": "Claremont Graduate University",
        "affiliation_city": "Claremont",
        "affiliation_country": "United States",
        "affiliation_id": "60003812",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Software Identification by Standard Machine Learning Tools",
        "paper_author": "Sukhoparov M.E.",
        "publication": "Automatic Control and Computer Sciences",
        "citied_by": "0",
        "cover_date": "2021-12-01",
        "Abstract": "Abstract: This article considers tools for controlling software installed on personal computers of automated system users. The flaws of these software solutions are grounded, and an approach to identifying executable files with the help of a machine learning algorithm is developed and presented. This algorithm consists in the gradient decision tree boosting on the basis of such libraries as XGBoost, LightGBM, CatBoost. The identification of programs with the help of XGBoost and LightGBM is executed. The experimental results are compared with the results of earlier studies conducted by other authors. The findings show that the developed method allows for identifying violations in the adopted security policy during information processing in automated systems.",
        "DOI": "10.3103/S0146411621080459",
        "affiliation_name": "Russian Academy of Sciences",
        "affiliation_city": "Moscow",
        "affiliation_country": "Russian Federation",
        "affiliation_id": "60021331",
        "affiliation_state": "Moscow Oblast"
    },
    {
        "paper_title": "Industry 4.0 - Policy-based approaches to efficient implementation in SMEs",
        "paper_author": "Łabȩdzka J.",
        "publication": "Engineering Management in Production and Services",
        "citied_by": "9",
        "cover_date": "2021-12-01",
        "Abstract": "Industry 4.0 (I4.0), driven by the need to access real-time insights and information across the manufacturing process, creates a disruptive impact on industries. Large-scale machine-to-machine communication, virtual reality (VR), the Internet of Things (IoT), simulation technologies and network management are integrated for increased automation, machine learning, self-controlled social and technical systems (Smart Factories). The uptake of advanced manufacturing solutions represents a challenge for businesses and SMEs in particular. SMEs possess neither the organisational capability nor financial resources to systematically investigate the potential and risks of introducing Industry 4.0. However, the so-called Fourth Industrial Revolution is a matter of technology and cooperation between European regions to share knowledge concerning alternative regional and national approaches to reinforcing the I4.0 uptake. Therefore, this paper primarily aims to analyse practical experience on how European policies related to the European Regional Development Fund (ERDF) can unlock the full potential of Industry 4.0 and overcome the fragmentation of Industry 4.0 solutions. Case studies of successful transfer of I4.0 to SMEs in Europe and supporting regional policy instruments presented in the paper could inspire and enable the potential of digitalisation by dealing with main challenges hampering their diffusion into the business ecosystem.",
        "DOI": "10.2478/emj-2021-0032",
        "affiliation_name": "Łukasiewicz - Instytut Technologii Eksploatacji",
        "affiliation_city": "Radom",
        "affiliation_country": "Poland",
        "affiliation_id": "60088503",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "INODE: Building an End-to-End Data Exploration System in Practice",
        "paper_author": "Amer-Yahia S.",
        "publication": "SIGMOD Record",
        "citied_by": "10",
        "cover_date": "2021-12-01",
        "Abstract": "A full-fledged data exploration system must combine different access modalities with a powerful concept of guiding the user in the exploration process, by being reactive and anticipative both for data discovery and for data linking. Such systems are a real opportunity for our community to cater to users with different domain and data science expertise. We introduce INODE - an end-to-end data exploration system - that leverages, on the one hand, Machine Learning and, on the other hand, semantics for the purpose of Data Management (DM). Our vision is to develop a classic unified, comprehensive platform that provides extensive access to open datasets, and we demonstrate it in three significant use cases in the fields of Cancer Biomarker Research, Research and Innovation Policy Making, and Astrophysics. INODE offers sustainable services in (a) data modeling and linking, (b) integrated query processing using natural language, (c) guidance, and (d) data exploration through visualization, thus facilitating the user in discovering new insights. We demonstrate that our system is uniquely accessible to a wide range of users from larger scientific communities to the public. Finally, we briefly illustrate how this work paves the way for new research opportunities in DM.",
        "DOI": "10.1145/3516431.3516436",
        "affiliation_name": "Université Grenoble Alpes",
        "affiliation_city": "Saint Martin d'Heres",
        "affiliation_country": "France",
        "affiliation_id": "60104653",
        "affiliation_state": "Auvergne-Rhone-Alpes"
    },
    {
        "paper_title": "Understanding the Middle East through the eyes of Japan's Newspapers: A topic modelling and sentiment analysis approach",
        "paper_author": "Ghasiya P.",
        "publication": "Digital Scholarship in the Humanities",
        "citied_by": "6",
        "cover_date": "2021-12-01",
        "Abstract": "For Japan-a country that has always been described with virtually no major natural resources such as oil, gas, and coal-the Middle Eastern region has a special place in its economic and foreign policy. In 2017, 39% of Japan's energy came from oil, and 87% of Japan's imported oil came from the Middle East, predominantly Saudi Arabia and the UAE. The above facts are enough to discern the critical significance of the Middle Eastern region for Japan. For Japan to have an unhindered supply of oil and other natural resources, it is pertinent that this region remains peaceful. In this scenario, the Middle East-related articles in Japan's newspapers can help understand Japan's perspective towards the Middle East. This paper would first apply the topic modelling approach non-negative matrix factorization (NMF) on Middle East-related articles from three newspapers of Japan. After discovering crucial topics, we would utilize traditional supervised machine learning algorithms to determine the overall and topic-specific sentiments from the collected headlines. Our topic modelling results discovered that the Japanese media widely reported issues like Islamic State, the refugee crisis, the Syrian civil war, Qasem Soleimani killing, and Iran nuclear deal. Further, the news related to Saudi Arabia, Syria, and Trump garnered high negative sentiment.",
        "DOI": "10.1093/llc/fqab019",
        "affiliation_name": "Kyushu University",
        "affiliation_city": "Fukuoka",
        "affiliation_country": "Japan",
        "affiliation_id": "60011047",
        "affiliation_state": "Fukuoka"
    },
    {
        "paper_title": "Gene-editable materials for future transportation infrastructure: a review for polyurethane-based pavement",
        "paper_author": "Hong B.",
        "publication": "Journal of Infrastructure Preservation and Resilience",
        "citied_by": "21",
        "cover_date": "2021-12-01",
        "Abstract": "With the rapid development of society and industry, novel technologies and materials related to pavement engineering are constantly emerging. However, with the continuous improvement of people’s demands, pavement engineering also faces more and more enormous challenges that the pavement materials must have excellent engineering properties and environmental benefits. Meanwhile, the intelligence is the mainstream development direction of modern society, and the development trend of future transportation infrastructure. Materials Genome Initiative, a program for the development of new materials that materials design is conducted by up-front simulations and predictions, followed by key validation experiments, the rapid development of science and technology and AI toolset (big data and machine learning) provide new opportunities and strong technical supports for pavement materials development that shorten the development-application cycle of new material, reduce cost and promote the application of new carriers such as intelligent sensing components in transportation engineering, to achieve the intelligence of transportation engineering. However, traditional pavement materials possess several unavoidable shortcomings, indicating that it is exceedingly difficult for them to meet the above requirements for future pavement materials. Therefore, the development of future new pavement materials, which can be designed on-demand as well as possessing enough mechanical properties, high durability, practical functionality, and high environmental protection, is urgent. In recent years, as a “designable” polymer material with various excellent engineering performances, polyurethane (PU) has been widely applied in pavement practices by changing the chemical structures of raw materials and their mix proportions, for instance pavement repairing material, permeable pavement material, tunnel paving material and bridge deck paving materials, etc. Although PU material has been widely applied in practices, a systematically summarization is still quite necessary for further understanding the working mechanism of PU materials and optimization it’s engineering applications. To fill the gap, this article puts forward the special requirements for future transportation infrastructure materials, and introduces the basic properties and working mechanism of PU materials in order to make up for the defects of conventional road materials. Based on this, this article also summarizes the engineering performances and environmental benefits of applying PU as the binder for different road infrastructure materials in recent years. Considering the gene-editable nature of polyurethane, further research of the on-demand design principles of PU pavement materials is recommended. The establishment of raw material gene database, material terminal performance database and their structure-activity relationship are highlighted. The current research is essential to the practice guidance and further optimization of the PU materials for road infrastructures, which in line with the future Carbon neutral policy.",
        "DOI": "10.1186/s43065-021-00039-w",
        "affiliation_name": "Harbin Institute of Technology",
        "affiliation_city": "Harbin",
        "affiliation_country": "China",
        "affiliation_id": "60019616",
        "affiliation_state": "Heilongjiang"
    },
    {
        "paper_title": "Deep reinforcement learning for shared control of mobile robots",
        "paper_author": "Tian C.",
        "publication": "IET Cyber-systems and Robotics",
        "citied_by": "2",
        "cover_date": "2021-12-01",
        "Abstract": "Shared control of mobile robots integrates manual input with auxiliary autonomous controllers to improve the overall system performance. However, prior work that seeks to find the optimal shared control ratio needs an accurate human model, which is usually challenging to obtain. In this study, the authors develop an extended Twin Delayed Deep Deterministic Policy Gradient (DDPG) (TD3X)-based shared control framework that learns to assist a human operator in teleoperating mobile robots optimally. The robot's states, shared control ratio in the previous time step, and human's control input is used as inputs to the reinforcement learning (RL) agent, which then outputs the optimal shared control ratio between human input and autonomous controllers without knowing the human model. Noisy softmax policies are developed to make the TD3X algorithm feasible under the constraint of a shared control ratio. Furthermore, to accelerate the training process and protect the robot, a navigation demonstration policy and a safety guard are developed. A neural network (NN) structure is developed to maintain the correlation of sensor readings among heterogeneous input data and improve the learning speed. In addition, an extended DAGGER (DAGGERX) human agent is developed for training the RL agent to reduce human workload. Robot simulations and experiments with humans in the loop are conducted. The results show that the DAGGERX human agent can simulate real human inputs in the worst-case scenarios with a mean square error of 0.0039. Compared to the original TD3 agent, the TD3X-based shared control system decreased the average collision number from 387.3 to 44.4 in a simplistic environment and 394.2 to 171.2 in a more complex environment. The maximum average return increased from 1043 to 1187 with a faster converge speed in the simplistic environment, while the performance is equally good in the complex environment because of the use of an advanced human agent. In the human subject tests, participants' average perceived workload was significantly lower in shared control than that in exclusively manual control (26.90 vs. 40.07, p = 0.013).",
        "DOI": "10.1049/csy2.12036",
        "affiliation_name": "Clemson University College of Engineering, Computing and Applied Sciences",
        "affiliation_city": "Clemson",
        "affiliation_country": "United States",
        "affiliation_id": "60139609",
        "affiliation_state": "SC"
    },
    {
        "paper_title": "Projecting armed conflict risk in Africa towards 2050 along the SSP-RCP scenarios: a machine learning approach",
        "paper_author": "Hoch J.M.",
        "publication": "Environmental Research Letters",
        "citied_by": "23",
        "cover_date": "2021-12-01",
        "Abstract": "In the past decade, several efforts have been made to project armed conflict risk into the future. This study broadens current approaches by presenting a first-of-its-kind application of machine learning (ML) methods to project sub-national armed conflict risk over the African continent along three Shared Socioeconomic Pathway (SSP) scenarios and three Representative Concentration Pathways towards 2050. Results of the open-source ML framework CoPro are consistent with the underlying socioeconomic storylines of the SSPs, and the resulting out-of-sample armed conflict projections obtained with Random Forest classifiers agree with the patterns observed in comparable studies. In SSP1-RCP2.6, conflict risk is low in most regions although the Horn of Africa and parts of East Africa continue to be conflict-prone. Conflict risk increases in the more adverse SSP3-RCP6.0 scenario, especially in Central Africa and large parts of Western Africa. We specifically assessed the role of hydro-climatic indicators as drivers of armed conflict. Overall, their importance is limited compared to main conflict predictors but results suggest that changing climatic conditions may both increase and decrease conflict risk, depending on the location: in Northern Africa and large parts of Eastern Africa climate change increases projected conflict risk whereas for areas in the West and northern part of the Sahel shifting climatic conditions may reduce conflict risk. With our study being at the forefront of ML applications for conflict risk projections, we identify various challenges for this arising scientific field. A major concern is the limited selection of relevant quantified indicators for the SSPs at present. Nevertheless, ML models such as the one presented here are a viable and scalable way forward in the field of armed conflict risk projections, and can help to inform the policy-making process with respect to climate security.",
        "DOI": "10.1088/1748-9326/ac3db2",
        "affiliation_name": "International Peace Research Institute, Oslo",
        "affiliation_city": "Oslo",
        "affiliation_country": "Norway",
        "affiliation_id": "60031105",
        "affiliation_state": "Oslo"
    },
    {
        "paper_title": "Vigorous IDS on Nefarious Operations and Threat Analysis Using Ensemble Machine Learning",
        "paper_author": "Musa U.S.",
        "publication": "Revue d'Intelligence Artificielle",
        "citied_by": "2",
        "cover_date": "2021-12-01",
        "Abstract": "The geometric increase in the usage of computer networking activities poses problems with the management of network normal operations. These issues had drawn the attention of network security researchers to introduce different kinds of intrusion detection systems (IDS) which monitor data flow in a network for unwanted and illicit operations. The violation of security policies with nefarious motive is what is known as intrusion. The IDS therefore examine traffic passing through networked systems checking for nefarious operations and threats, which then sends warnings if any of these malicious activities are detected. There are 2 types of detection of malicious activities, misuse detection, in this case the information about the passing network traffic is gathered, analyzed, which is then compared with the stored predefined signatures. The other type of detection is the Anomaly detection which is detecting all network activities that deviates from regular user operations. Several researchers have done various works on IDS in which they employed different machine learning (ML), evaluating their work on various datasets. In this paper, an efficient IDS is built using Ensemble machine learning algorithms which is evaluated on CIC-IDS2017, an updated dataset that contains most recent attacks. The results obtained show a great increase in the rate of detection, increase in accuracy as well as reduction in the false positive rates (FPR).",
        "DOI": "10.18280/ria.350604",
        "affiliation_name": "LLoyd Institute of Engineering &amp; Technology",
        "affiliation_city": "Greater Noida",
        "affiliation_country": "India",
        "affiliation_id": "60271867",
        "affiliation_state": "UP"
    },
    {
        "paper_title": "Gradus et al. Respond to \"Machine Learning and Suicide Prevention: New Directions\"",
        "paper_author": "Gradus J.L.",
        "publication": "American Journal of Epidemiology",
        "citied_by": "0",
        "cover_date": "2021-12-01",
        "Abstract": "NA",
        "DOI": "10.1093/aje/kwab113",
        "affiliation_name": "VA Boston Healthcare System",
        "affiliation_city": "West Roxbury",
        "affiliation_country": "United States",
        "affiliation_id": "60105918",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Measuring anomalies in cigarette sales using official data from Spanish provinces: Are the anomalies detected by the Empty Pack Surveys (EPSs) used by Transnational Tobacco Companies (TTCs) the only anomalies?",
        "paper_author": "Cadahia P.",
        "publication": "Tobacco Induced Diseases",
        "citied_by": "3",
        "cover_date": "2021-12-01",
        "Abstract": "INTRODUCTION There is a literature that questions the veracity of the studies commissioned by transnational tobacco companies (TTCs) to measure the illicit tobacco trade. Furthermore, there are studies that have indicated that the empty pack surveys (EPSs) ordered by TTCs overestimate the size of this trade. This study simultaneously analyzed whether the EPSs established in each of the 47 Spanish provinces were accurate and measured anomalies observed in provinces where sales exceed expected values. METHODS To achieve the objectives of this study, provincial data on cigarette sales, prices and GDP per capita were used. These data were modeled with machine learning techniques that are widely used to detect anomalies in other areas. RESULTS The magnitude of the average anomaly in provinces where sales are higher than their expected values exceeds 40%, while the average anomaly in provinces where sales are lower than their expected values (as detected by the EPSs) is <15%. Furthermore, the results reveal that there is a clear geographical pattern to the provinces in which sales below reasonable values are observed. In addition, the values provided by the EPSs in Spain, as indicated in the previous literature, are slightly overestimated. Finally, some regions bordering other countries or that are highly influenced by tourism have observed sales that are higher than their expected values. CONCLUSIONS Cooperation between countries in their tobacco control policies can have better effects than policies developed based on information from a single country. The lack of control over the transactions of tourists and the inhabitants of bordering countries can cause important anomalies that distort the understanding of tobacco consumption that governments have based on official data.",
        "DOI": "10.18332/tid/143321",
        "affiliation_name": "International University of La Rioja",
        "affiliation_city": "Logrono",
        "affiliation_country": "Spain",
        "affiliation_id": "60104175",
        "affiliation_state": "La Rioja"
    },
    {
        "paper_title": "Is preclinical research in cancer biology reproducible enough?",
        "paper_author": "Kane P.B.",
        "publication": "eLife",
        "citied_by": "11",
        "cover_date": "2021-12-01",
        "Abstract": "The Reproducibility Project: Cancer Biology (RPCB) was established to provide evidence about reproducibility in basic and preclinical cancer research, and to identify the factors that influence reproducibility more generally. In this commentary we address some of the scientific, ethical and policy implications of the project. We liken the basic and preclinical cancer research enterprise to a vast ‘diagnostic machine’ that is used to determine which clinical hypotheses should be advanced for further development, including clinical trials. The results of the RPCB suggest that this diagnostic machine currently recommends advancing many findings that are not reproducible. While concerning, we believe that more work needs to be done to evaluate the performance of the diagnostic machine. Specifically, we believe three questions remain unanswered: How often does the diagnostic machine correctly recommend against advancing real effects to clinical testing?; what are the relative costs to society of false positive and false negatives?; and how well do scientists and others interpret the outputs of the machine?.",
        "DOI": "10.7554/eLife.67527",
        "affiliation_name": "McGill Faculty of Medicine and Health Sciences",
        "affiliation_city": "Montreal",
        "affiliation_country": "Canada",
        "affiliation_id": "60029502",
        "affiliation_state": "QC"
    },
    {
        "paper_title": "Exploring stakeholder attitudes towards AI in clinical practice",
        "paper_author": "Scott I.A.",
        "publication": "BMJ Health and Care Informatics",
        "citied_by": "74",
        "cover_date": "2021-12-01",
        "Abstract": "Objectives Different stakeholders may hold varying attitudes towards artificial intelligence (AI) applications in healthcare, which may constrain their acceptance if AI developers fail to take them into account. We set out to ascertain evidence of the attitudes of clinicians, consumers, managers, researchers, regulators and industry towards AI applications in healthcare. Methods We undertook an exploratory analysis of articles whose titles or abstracts contained the terms € artificial intelligence' or € AI' and € medical' or € healthcare' and € attitudes', € perceptions', € opinions', € views', € expectations'. Using a snowballing strategy, we searched PubMed and Google Scholar for articles published 1 January 2010 through 31 May 2021. We selected articles relating to non-robotic clinician-facing AI applications used to support healthcare-related tasks or decision-making. Results Across 27 studies, attitudes towards AI applications in healthcare, in general, were positive, more so for those with direct experience of AI, but provided certain safeguards were met. AI applications which automated data interpretation and synthesis were regarded more favourably by clinicians and consumers than those that directly influenced clinical decisions or potentially impacted clinician-patient relationships. Privacy breaches and personal liability for AI-related error worried clinicians, while loss of clinician oversight and inability to fully share in decision-making worried consumers. Both clinicians and consumers wanted AI-generated advice to be trustworthy, while industry groups emphasised AI benefits and wanted more data, funding and regulatory certainty. Discussion Certain expectations of AI applications were common to many stakeholder groups from which a set of dependencies can be defined. Conclusion Stakeholders differ in some but not all of their attitudes towards AI. Those developing and implementing applications should consider policies and processes that bridge attitudinal disconnects between different stakeholders.",
        "DOI": "10.1136/bmjhci-2021-100450",
        "affiliation_name": "The University of Queensland",
        "affiliation_city": "Brisbane",
        "affiliation_country": "Australia",
        "affiliation_id": "60031004",
        "affiliation_state": "QLD"
    },
    {
        "paper_title": "Predictive maintenance decision-making for serial production lines based on deep reinforcement learning",
        "paper_author": "Cui P.",
        "publication": "Jisuanji Jicheng Zhizao Xitong/Computer Integrated Manufacturing Systems, CIMS",
        "citied_by": "5",
        "cover_date": "2021-12-01",
        "Abstract": "Predictive maintenance is designed to perform maintenance activities based on the condition of equipment, which can improve the business bottom line by reducing maintenance cost and improving production performance. The modeling, analysis and decision-making of serial production lines with machine degradation process were studied. A Markov chain model was developed by analyzing the dynamics of a serial production line with machine failures and predictive maintenance, and the analytical formulas of transient performance measures were derived. A predictive maintenance decision model was established as a Markov decision process to minimize the work-in-process, backlog and maintenance costs. A deep reinforcement learning method was utilized to explore optimum maintenance policies, which was obtained through the training of neural network with dataset generated from Markov chain model. Case study was performed to validate the effectiveness of the proposed decision model. The results indicated that the maintenance and production related costs were significantly reduced.",
        "DOI": "10.13196/j.cims.2021.12.004",
        "affiliation_name": "Northwestern Polytechnical University",
        "affiliation_city": "Xi'an",
        "affiliation_country": "China",
        "affiliation_id": "60003977",
        "affiliation_state": "Shaanxi"
    },
    {
        "paper_title": "Child stature, maternal education, and early childhood development in Nigeria",
        "paper_author": "Skoufias E.",
        "publication": "PLoS ONE",
        "citied_by": "7",
        "cover_date": "2021-12-01",
        "Abstract": "Data from the 2016–17 Multiple Indicator Cluster Survey from Nigeria are used to study the relationship between child stature, mother’s years of education, and indicators of early childhood development (ECD). The relationships are contrasted between two empirical approaches: the conventional approach whereby control variables are selected in an ad-hoc manner, and the double machine-learning (DML) approach that employs data-driven methods to select controls from a much wider set of variables and thus reducing potential omitted variable bias. Overall, the analysis confirms that maternal education and the incidence of chronic malnutrition have a significant direct effect on measures of early childhood development. The point estimates based on the ad-hoc specification tend to be larger in absolute value than those based on the DML specification. Frequently, the point estimates based on the ad-hoc specification fall inside the confidence interval of the DML point estimates, suggesting that in these cases the omitted variable bias is not serious enough to prevent making causal inferences based on the ad-hoc specification. However, there are instances where the omitted variable bias is sufficiently large for the ad hoc specification to yield a statistically significant relationship when in fact the more robust DML specification suggests there is none. The DML approach also reveals a more complex picture that highlights the role of context. In rural areas, mother’s education affects early childhood development both directly and indirectly through its impact on the nutritional status of both older and younger children. In contrast, in urban areas, where the average level of maternal education is much higher, increases in a mother’s education have only a direct effect on child ECD measures but no indirect effect through child nutrition. Thus, DML provides a practical and feasible approach to reducing threats to internal validity for robust inferences and policy design based on observational data.",
        "DOI": "10.1371/journal.pone.0260937",
        "affiliation_name": "The World Bank, USA",
        "affiliation_city": "Washington, D.C.",
        "affiliation_country": "United States",
        "affiliation_id": "60112834",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Digital topics on cultural heritage investigated: how can data-driven and data-guided methods support to identify current topics and trends in digital heritage?",
        "paper_author": "Münster S.",
        "publication": "Built Heritage",
        "citied_by": "15",
        "cover_date": "2021-12-01",
        "Abstract": "In research and policies, the identification of trends as well as emerging topics and topics in decline is an important source of information for both academic and innovation management. Since at present policy analysis mostly employs qualitative research methods, the following article presents and assesses different approaches – trend analysis based on questionnaires, quantitative bibliometric surveys, the use of computer-linguistic approaches and machine learning and qualitative investigations. Against this backdrop, this article examines digital applications in cultural heritage and, in particular, built heritage via various investigative frameworks to identify topics of relevance and trendlines, mainly for European Union (EU)-based research and policies. Furthermore, this article exemplifies and assesses the specific opportunities and limitations of the different methodical approaches against the backdrop of data-driven vs. data-guided analytical frameworks. As its major findings, our study shows that both research and policies related to digital applications for cultural heritage are mainly driven by the availability of new technologies. Since policies focus on meta-topics such as digitisation, openness or automation, the research descriptors are more granular. In general, data-driven approaches are promising for identifying topics and trendlines and even predicting the development of near future trends. Conversely, qualitative approaches are able to answer “why” questions with regard to whether topics are emerging due to disruptive innovations or due to new terminologies or whether topics are becoming obsolete because they are common knowledge, as is the case for the term “internet”.",
        "DOI": "10.1186/s43238-021-00045-7",
        "affiliation_name": "Friedrich-Schiller-Universität Jena",
        "affiliation_city": "Jena",
        "affiliation_country": "Germany",
        "affiliation_id": "60029507",
        "affiliation_state": "Thuringen"
    },
    {
        "paper_title": "What Makes a Tax Policy Popular? Predicting Referendum Votes from Policy Text",
        "paper_author": "Martin I.W.",
        "publication": "Socius",
        "citied_by": "1",
        "cover_date": "2021-12-01",
        "Abstract": "What kinds of taxation are most politically sustainable in a democracy? The authors answer this question by applying natural language processing and machine learning techniques to a large, new corpus of digitized documents describing municipal tax policies of heterogeneous design that have been directly subjected to popular referendum in the state of California. The authors find that tax policies of different description vary systematically in their popularity with voters. In particular, official textual summaries of tax policy differ along two social dimensions that are associated with voters’ willingness to approve the tax. The authors interpret these dimensions as risk pooling and community orientation and show that measuring these dimensions can modestly improve the ability to predict the popularity of a tax, relative to a conventional regression specification that omits information about qualitative policy design. The authors discuss implications for the study of the sociology of taxation.",
        "DOI": "10.1177/23780231211066069",
        "affiliation_name": "University of California, San Diego",
        "affiliation_city": "La Jolla",
        "affiliation_country": "United States",
        "affiliation_id": "60030612",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Evaluating fishing capacity based on dea and regression analysis of china’s offshore fishery",
        "paper_author": "Liu S.",
        "publication": "Journal of Marine Science and Engineering",
        "citied_by": "4",
        "cover_date": "2021-12-01",
        "Abstract": "The analysis of offshore fishing capacity is of great significance and practical value to the sustainable utilization and conservation of marine fishery resources. Based on the 2004–2020 China Fishery Statistical Yearbook, data envelopment analysis (DEA) was applied for measuring fishing capacity using a number of fishing vessels, total power, total tonnage, and the number of professional fishermen as the input measures and the annual catch as the output measure. Capacity utilization had a calculated range from 80.7 to 100%, and its average is 93.5%. In the first four years of 2003–2007, the excess investment rate of fishing vessels, total tonnage, total power, and fishermen was low (<5%). There was a consistent sharp upward trend in 2007, a gradual downward trend from 2007 to 2015, and an upward trend after reaching a low point in 2015, with the highest gross tonnage of fishing vessels reaching 25.5%. Four regression models that incorporate machine learning algorithms are used, including Lasso, Ridge, KNN, and Polynomial Features. The goodness of fit for the four models was used as the evaluation index, and the offshore annual catch based on the evaluation index was proposed. The forecasting annual catch of the polynomial model can reach 0.98. Furthermore, a comparative simulation of the DEA incorporating the polynomial model was carried out. The results show that DEA can evaluate input factors under the conditions of a given range, and the polynomial model has more advantages in forecasting annual catches. Furthermore, the combined application of DEA and polynomial model was used to analyze and discuss the management policies of China’s offshore fishery, which can provide help and reference for future management.",
        "DOI": "10.3390/jmse9121402",
        "affiliation_name": "Shanghai Ocean University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60102070",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Sports participation and value of elite sports in predicting well-being",
        "paper_author": "Silva A.",
        "publication": "Sports",
        "citied_by": "4",
        "cover_date": "2021-12-01",
        "Abstract": "This work contributes to an emerging literature focused on the role of physical activity on the subjective well-being of populations. Unlike the existing literature, it proposes an approach that uses algorithms to predict subjective well-being. The aims of this study were to determine the relative importance of sports participation and perceived value of elite sports on the subjective well-being of individuals. A total of 511 participants completed an online questionnaire. The statistical analysis used several machine learning techniques, including three algorithms, Decision Tree Classifier (DTC), Random Forest Classifier (RFC), and Gradient Boosting Classifier (GBC). In the three algorithms tested, sports participation, expressed as the weekly frequency and the time spent engaging in vigorous physical activity, showed a greater importance (between 47% and 53%) in determining subjective well-being. It also highlights the effect of perceived value of elite sport on the prediction of subjective well-being. This study provides evidence for public sport policy makers/authorities and for managers of physical activity and sport development programs. The surprising effect of the perceived value of elite sport on the prediction of subjective well-being.",
        "DOI": "10.3390/sports9120173",
        "affiliation_name": "Polytechnic Institute of Leiria",
        "affiliation_city": "Leiria",
        "affiliation_country": "Portugal",
        "affiliation_id": "60028370",
        "affiliation_state": "Leiria"
    }
]