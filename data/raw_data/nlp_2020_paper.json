[
    {
        "paper_title": "Variability and automatic language processing",
        "publication": "Lingvisticae Investigationes",
        "citied_by": "1",
        "cover_date": "2020-12-31",
        "Abstract": "Through this contribution we would like to propose an analysis of the variation, mainly diatopic, applied to fixed sequences, through a resource we have developed for Automatic Language Processing. The large number of variants that appear in corpora has led us to carry out an exhaustive analysis of these constructions in order to allow native and non-native users, and NLP applications, to use these fixed sequences correctly. The textual data are varied, from qualite potentially degraded and voluminous. Finally, the other difficulty that NLP must face to access the meaning of texts are the intrinsic characteristics of natural languages.",
        "DOI": "10.1075/li.00050.gre",
        "paper_author": "Grezka A.",
        "affiliation_name": "University Sorbonne Paris Nord",
        "affiliation_city": "Villetaneuse",
        "affiliation_country": "France",
        "affiliation_id": "60009647",
        "affiliation_state": "Ile-de-France"
    },
    {
        "paper_title": "Review on the usage of deep learning models in multi-modal sentiment analysis",
        "publication": "IEIE Transactions on Smart Processing and Computing",
        "citied_by": "3",
        "cover_date": "2020-12-30",
        "Abstract": "In recent years, Sentiment Analysis is reshaping the business operations of many organizations by monitoring their brand reputation on social media and acquiring insights from customer's feedback. Sentiment Analysis is one of the classification tools that identifies and extracts the subjective information of a product. This subjective information can be stated in different ways, such as feedback, discussions, blogs, podcasts, and video logs. This type of information generated by the empowered customers is known as user-generated content, which is traditionally in the form of words. The analysis was performed on a huge number of words using Natural Language Processing (NLP), which is a Unimodal Sentiment Analysis. With the rapid growth in the usage of the Internet, social media turned out to be a platform to share the thoughts of the individuals. This caused researchers to migrate from the traditional Unimodal analysis to Multimodal Sentiment Analysis, which includes video, audio, and images. This approach leverages the use of emotion and content and helps identify the scope and polarity of an individual's sentiment. With the latest deep learning algorithms, Multimodal Sentiment Analysis can solve the problem of sarcasm identification. Multi-Modal Sentiment Analysis generates more accurate results compared to Uni Modal Sentiment Analysis. Therefore, this study aimed to define Sentiment Analysis and review the approaches and techniques in Sentiment Analysis from conventional Unimodal to Multimodal. In addition, this paper discusses a Multimodal Sentiment Analysis architecture using a transformers attention net.",
        "DOI": "10.5573/IEIESPC.2020.9.6.435",
        "paper_author": "Naga Durga Saile K.",
        "affiliation_name": "Vignans Foundation for Science Technology and Research University",
        "affiliation_city": "Guntur",
        "affiliation_country": "India",
        "affiliation_id": "60104649",
        "affiliation_state": "AP"
    },
    {
        "paper_title": "Retrieval based chatbot on Tarumanagara University with multilayer perceptron",
        "publication": "IOP Conference Series: Materials Science and Engineering",
        "citied_by": "3",
        "cover_date": "2020-12-30",
        "Abstract": "Natural Language Processing or NLP is a branch of Artificial Intelligence that focuses on processing language. One of the implementation of NLP is in the field of customer service in the form of chatbots. Tarumanagara University itself already has a customer service called Admission Tarumanagara. Admission can be contacted by telephone or sending an e-mail message. Making Chatbot will add ways to get information about Tarumanagara University. Chatbot is a computer program based on artificial intelligence that can simulate conversations between people. The system created is a Retrieval Based Chatbot system that can recognize user questions and answer user questions directly. Making a chatbot system will use a model with the Multilayer Perceptron method. The process for the system will go through two stages, namely the preprocessing stage and the introduction stage. The preprocessing stage consists of case lowering, tokenizing, and stemming. After that the model is made and the introduction phase of the trained model is carried out. Chatbot that has been made can be accessed using the LINE application via LINE ID @764vvcfm.",
        "DOI": "10.1088/1757-899X/1007/1/012146",
        "paper_author": "Ciayandi A.",
        "affiliation_name": "Faculty of Information Technology",
        "affiliation_city": "Semarang",
        "affiliation_country": "Indonesia",
        "affiliation_id": "125715606",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Energy Management Strategy on Fuel Cell Hybrid Electric Articulated Vehicle Based on Vehicular Networking",
        "publication": "SAE Technical Papers",
        "citied_by": "1",
        "cover_date": "2020-12-30",
        "Abstract": "In this paper, an energy management method based on vehicular networking is proposed for the dual power sources fuel cell electric articulated vehicle. Vehicular networking includes a cloud computing center, which predicts the information of power demand for the real-time driving condition based on the history data analysis, and solves the energy management strategy for the dual power sources utilizing the Radau pseudospectral method (RPM). The global interpolation polynomial is used to approximate the state variables and control variables in the system. The derivative of the interpolation polynomial approximates the differential equation of the state variables in the dynamic equation. Further, the optimal control problem (OCP) is transformed into nonlinear problem (NLP) to be solved. The simulation result of the proposed strategy show that the capacity degradation of the fuel cell can be reduced while meeting the power output demand, which means the lifetime of the fuel cell could be extended.",
        "DOI": "10.4271/2020-01-5162",
        "paper_author": "Liang J.",
        "affiliation_name": "Guangdong University of Technology",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60007155",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Switchable dissipative soliton resonance and noise like pulses regimes in a mode-locked double-clad thulium doped fiber laser",
        "publication": "Laser Physics",
        "citied_by": "12",
        "cover_date": "2020-12-30",
        "Abstract": "We report an experimental study of switchable rectangular pulses generation in a passively mode-locked double-clad thulium-doped fiber laser operating in the anomalous dispersion regime. A fiber loop mirror and a polarization imbalanced nonlinear optical loop mirror (NOLM) forms the linear cavity. The loop in the NOLM includes a quarter-wave retarder, which allows switching between two regimes of rectangular pulse generation. The laser generates rectangular noise-like pulses (NLPs) or dissipative soliton resonance (DSR) pulses with 1.18 MHz repetition rate at ∼1990 nm. At the maximum pump power of 10 W, rectangular pulses with energy of up to 540 nJ in DSR operation and 482 nJ in NLP operation were obtained with output average power of 640 mW and 571 mW, respectively.",
        "DOI": "10.1088/1555-6611/abd3f6",
        "paper_author": "Durán-Sánchez M.",
        "affiliation_name": "Instituto Nacional de Astrofisica Optica y Electronica",
        "affiliation_city": "Puebla",
        "affiliation_country": "Mexico",
        "affiliation_id": "60030699",
        "affiliation_state": "PUE"
    },
    {
        "paper_title": "Tens-embedding: A Tensor-based document embedding method",
        "publication": "Expert Systems with Applications",
        "citied_by": "10",
        "cover_date": "2020-12-30",
        "Abstract": "A human is capable of understanding and classifying a text but a computer can understand the underlying semantics of a text when texts are represented in a way comprehensible by computers. The text representation is a fundamental stage in natural language processing (NLP). One of the main drawbacks of existing text representation approaches is that they only utilize one aspect or view of a text e.g. They only consider texts by their words while the topic information can be extracted from text as well. The term-document and document-topic matrix are two views of a text and contain complementary information. We use the strength of both views to extract a richer representation. In this paper, we propose three different text representation methods with the help of these two matrices and tensor factorization to utilize the power of both views. The proposed approach (Tens-Embedding) was applied in the tasks of text classification, sentence-level and document-level sentiment analysis and text clustering wherein the conducted experiments on 20newsgroups, R52, R8, MR and IMDB datasets indicated the superiority of the proposed method in comparison with other document embedding techniques.",
        "DOI": "10.1016/j.eswa.2020.113770",
        "paper_author": "Rahimi Z.",
        "affiliation_name": "Amirkabir University of Technology",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran",
        "affiliation_id": "60007751",
        "affiliation_state": "Tehran"
    },
    {
        "paper_title": "Opinion Mining on US Airline Twitter Data Using Machine Learning Techniques",
        "publication": "16th International Computer Engineering Conference, ICENCO 2020",
        "citied_by": "30",
        "cover_date": "2020-12-29",
        "Abstract": "The Airline sector is an important field nowadays in the market. In order to keep that sector alive and up to date we have to consider opinion mining. Text sentiment analysis is a Natural Language Processing (NLP) technique to analyze text. In this research, we will use opinion mining one of the text sentiment applications to investigate customer feedback about airline services. One of the largest opinion mining sources is Twitter which contains a huge number of tweets that needs to be processed and analyzed to make a decision and enhance a certain service. In this research, we proposed a machine learning model to categorize Twitter posts into positive, negative and neutral categories. We implemented our model on a dataset containing tweets of 6 different Airlines in the US. We started our model by preprocessing steps where we cleaned tweets and extracted features to represent them as a feature vector and finally, we built our Bag of Words (BoW) model. In the classification phase, we applied 6 machine learning techniques Support Vector Machine (SVM), Logistic Regression (LR), Random Forest (RF), XgBoost (XGB), Naïve Bayes (NB) and Decision Tree (DT) to classify tweets. Finally, in the validation phase, we split data into 70% training and 30% testing, for the purpose of testing and validating the data we used the K-Fold Cross-Validation technique. Finally, we calculated Accuracy, Precision, Recall and F1-score for each classifier. After comparing the results of each classifier, we found that SVM had the highest accuracy of 83.31 %.",
        "DOI": "10.1109/ICENCO49778.2020.9357390",
        "paper_author": "Saad A.I.",
        "affiliation_name": "College of Computing and Information Technology",
        "affiliation_city": "Heliopolis",
        "affiliation_country": "Egypt",
        "affiliation_id": "60273296",
        "affiliation_state": "Cairo"
    },
    {
        "paper_title": "Word Embeddings and Neural Network Architectures for Arabic Sentiment Analysis",
        "publication": "16th International Computer Engineering Conference, ICENCO 2020",
        "citied_by": "5",
        "cover_date": "2020-12-29",
        "Abstract": "With the development of the new era of technology and social networks, forums, blogs, and online sales, combined with the increase of Arabs expressing their opinions, it became an urgent matter to do this research. This paper investigates different deep neural network architectures to perform Arabic sentiment classification combined with word embedding approaches. The models used are Recurrent Neural network (RNN), Bidirectional multi-layer long short-term memory (LSTM), and FastText. Different hyperparameters are used to train each model. In addition, a neural network of Multi-Layer Bidirectional Long Short-Term Memory trained on top of Glove Arabic word embedding with 1.75 billion tokens and 1.5 million words beat the existing methods.",
        "DOI": "10.1109/ICENCO49778.2020.9357377",
        "paper_author": "Fawzy M.",
        "affiliation_name": "College of Computing and Information Technology",
        "affiliation_city": "Heliopolis",
        "affiliation_country": "Egypt",
        "affiliation_id": "60273296",
        "affiliation_state": "Cairo"
    },
    {
        "paper_title": "Apply transfer learning to cybersecurity: Predicting exploitability of vulnerabilities by description",
        "publication": "Knowledge-Based Systems",
        "citied_by": "59",
        "cover_date": "2020-12-27",
        "Abstract": "Thousands of software vulnerabilities are archived and disclosed to the public each year, posing severe cybersecurity threats to the whole society. Predicting the exploitability of vulnerabilities is crucial for decision-makers to prioritize their efforts and patch the most critical vulnerabilities. Software vulnerability descriptions are accessible features in early stage and contain rich semantic information. Therefore, descriptions are wildly used for exploitability prediction in both industry and academia. However, comparing with other corpora, the size of vulnerability description corpus is too small to train a comprehensive Natural Language Processing (NLP) model. To gain a better performance, this paper proposes a framework named ExBERT to accurately predict if a vulnerability will be exploited or not. ExBERT essentially is an improved Bidirectional Encoder Representations from Transformers (BERT) model for exploitability prediction. First, we fine-tune a pre-trained BERT using collected domain-specific corpus. Then, we design a Pooling Layer and a Classification Layer on top of the fine-tuned BERT model to extract sentence-level semantic features and predict the exploitability of vulnerabilities. Results on 46,176 real-word vulnerabilities have demonstrated that the proposed ExBERT framework achieves 91.12% on accuracy and 91.82% on precision, outperforming the state-of-the-art approach with 89.0% on accuracy and 81.8% on precision.",
        "DOI": "10.1016/j.knosys.2020.106529",
        "paper_author": "Yin J.",
        "affiliation_name": "La Trobe University",
        "affiliation_city": "Melbourne",
        "affiliation_country": "Australia",
        "affiliation_id": "60006925",
        "affiliation_state": "VIC"
    },
    {
        "paper_title": "DECAB-LSTM: Deep Contextualized Attentional Bidirectional LSTM for cancer hallmark classification",
        "publication": "Knowledge-Based Systems",
        "citied_by": "21",
        "cover_date": "2020-12-27",
        "Abstract": "The great number of online scientific publications on cancer research makes large scale data mining possible. The hallmarks or characteristics of cancer can be used to distinguish cancerous cells from normal cells. Therefore, it is extremely necessary to organize and categorize a sea of scientific articles into the corresponding hallmarks by predicting whether or not they contain the information of interest. In the past, many research works tended to employ traditional machine learning methods that characterize feature engineering. Deep learning-based methods have achieved state-of-the-art performance in a wide range of Natural Language Processing (NLP) tasks. However, there is only a limited number of work with a focus on deep learning techniques for the task of cancer hallmark text classification. To advance this task, a novel neural architecture DEep Contextualized Attentional Bidirectional LSTM (DECAB-LSTM) was proposed, capable of learning to attend to the valuable information in a sentence by introducing contextual attention mechanism. We also investigated the effect of a good word embedding for the cancer hallmark text classification. We trained our model on a benchmark dataset and reported the accuracy, f score, and AUC metrics. Compared to several baselines like Logistic regression, Support Vector Machines, Convolutional Neural Networks, fastText, etc., the proposed model have achieved state-of-the-art performance over baselines, demonstrating its great potential in the empirical application to cancer research.",
        "DOI": "10.1016/j.knosys.2020.106486",
        "paper_author": "Jiang L.",
        "affiliation_name": "Macrowing Inc.",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "124003874",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A comparative recapitulating study of recent sentiment analysis approaches",
        "publication": "Proceedings of 2020 IEEE International Women in Engineering (WIE) Conference on Electrical and Computer Engineering, WIECON-ECE 2020",
        "citied_by": "0",
        "cover_date": "2020-12-26",
        "Abstract": "In accordance with the convention, sentiments are defined as a thought, judgement or attitude prompted by feelings or a specific notion. With explosion of social networks, every individual/user actively manifests and share their ideas or emotions in disparate life aspects. Past few years have shown Sentiment Analysis as a pivotal activity in various systems and thus become a center of attention for the researchers. Considerable amount of research has already been done to inspect and examine user's sentiments, which are extracted either from web pages or social media, through lexicon, machine learning or deep learning techniques. This paper presents a vital contribution in this field of research through comprehensive exploration of most recent articles/studies, extremely aiming to highlight the applicability of sentiment analysis in software engineering field as well as through bio-inspired algorithms along with other modi operandi, and thereby helping practitioners to get detailed insights of some of the lately exercised feature extraction tactics, methodologies and datasets considered for the analysis of sentiment-specific studies.",
        "DOI": "10.1109/WIECON-ECE52138.2020.9397928",
        "paper_author": "Gupta A.",
        "affiliation_name": "SRM Institute of Science and Technology, NCR Campus",
        "affiliation_city": "Ghaziabad",
        "affiliation_country": "India",
        "affiliation_id": "60117286",
        "affiliation_state": "UP"
    },
    {
        "paper_title": "Exploring Natural Language Processing Techniques in Social Media Analysis during a Pandemic: Understanding a corpus of Facebook posts using Word2vec and LDA",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "5",
        "cover_date": "2020-12-25",
        "Abstract": "People around the world have used social media extensively to communicate and express opinions especially during this time of the rapid spread of COVID-19. Nowadays, the various narratives of social media users are important that can be used in creating measures to curb the deadly disease. However, the manual collection of data from social media such as Facebook and its analysis can take time. Thus, this study attempted to use natural language processing (NLP) techniques such as topic modeling and word embedding to identify the concepts contained in the posts and comments of Facebook users in the Philippines regarding the pandemic. This study harvested posts and comments in Facebook groups that are primarily Filipino citizens that express opinions and suggestions in COVID-19 responses. Using Latent Dirichlet Allocation (LDA), this study was able to generate 10 topics related to the concepts of (1) self-discipline, (2) prayers for the frontliners, (3) total lockdown, (4) following government guidelines and protocols, and (5) flattening the curve of the disease. Meanwhile, word groups generated by Word2vec developed concepts such as (1) mass testing, (2) hope for faster recovery, and (3) expectation from the government. The average cosine similarity for word groups is 0.92, which implies strong relatedness of each word per group. This study proved that the use of NLP techniques helped in analyzing the themes of Facebook posts and comments related to the pandemic.",
        "DOI": "10.1145/3446999.3447012",
        "paper_author": "Gorro K.D.",
        "affiliation_name": "University of San Carlos",
        "affiliation_city": "Cebu",
        "affiliation_country": "Philippines",
        "affiliation_id": "60071487",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Harmonic Means between TF-IDF and Angle of Similarity to Identify Prospective Applicants in a Recruitment Setting",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2020-12-24",
        "Abstract": "Recruitment industry is better and bigger than ever. There is no denying that technology plays a major role in helping recruiters evolve and adopt with the pace of recruitment on a global scale. With the increasing population, the demand for manpower has been relative to the growth and challenging needs of recruiters; be it online or traditional way of outsourcing. In this study, we propose a combination of angle or similarity and term frequency-inverse document frequency to easily classify prospective job applicants. The results show that the two models are relative to each other, value-wise and harmonic means. Their values are synchronized to a certain extent based on our query. This is helpful because recruiters may save a lot of time in classifying prospective applicants. It can also be concluded that harmonic similarity is viable in combining the two models. As a future work, it is possible to develop a full featured application to be deployed in a production setting.",
        "DOI": "10.1145/3446132.3446414",
        "paper_author": "Bituin R.C.",
        "affiliation_name": "Systems Plus College Foundation",
        "affiliation_city": null,
        "affiliation_country": null,
        "affiliation_id": "125930339",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Siamese Multiplicative LSTM for Semantic Text Similarity",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "4",
        "cover_date": "2020-12-24",
        "Abstract": "Learning the Semantic Textual Similarity (STS) is a critical issue for many NLP tasks such as question answering, document summarization and etc.. In this paper, we combine the Multiplicative LSTM structure with a Siamese architecture which learn to project word embeddings of each sentence into a fixed-dimensional embedding space to represent this sentence. Then these sentence embeddings can be used to evaluate the STS task. We compare with several similar architectures and the proposed method has achieved better results and is competitive with the best state-of-the-art siamese neural network architecture.",
        "DOI": "10.1145/3446132.3446160",
        "paper_author": "Lv C.",
        "affiliation_name": "Ltd.",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "126174096",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Fast Confidence Prediction of Uncertainty based on Knowledge Graph Embedding",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "2",
        "cover_date": "2020-12-24",
        "Abstract": "The uncertainty is an inherent feature of Knowledge Graph (KG), which is often modelled as confidence scores of relation facts. Although Knowledge Graph Embedding (KGE) has been a great success recently, it is still a big challenge to predict confidence of unseen facts in KG in the continuous vector space. There are several reasons for this situation. First, the current KGE is often concerned with the deterministic knowledge, in which unseen facts' confidence are treated as zero, otherwise as one. Second, in the embedding space, uncertainty features are not well preserved. Third, approximate reasoning in embedding spaces is often unexplainable and not intuitive. Furthermore, the time and space cost of obtaining embedding spaces with uncertainty preserved are always very high. To address these issues, considering Uncertain Knowledge Graph (UKG), we propose a fast and effective embedding method, UKGsE, in which approximate reasoning and calculation can be quickly performed after generating an Uncertain Knowledge Graph Embedding (UKGE) space in a high speed and reasonable accuracy. The idea is that treating relation facts as short sentences and pre-handling are benefit to the learning and training confidence scores of them. The experiment shows that the method is suitable for the downstream task, confidence prediction of relation facts, whether they are seen in UKG or not. It achieves the best tradeoff between efficiency and accuracy of predicting uncertain confidence of knowledge. Further, we found that the model outperforms state-of-the-art uncertain link prediction baselines on CN15k dataset.",
        "DOI": "10.1145/3446132.3446186",
        "paper_author": "Yang S.",
        "affiliation_name": "Kunming University of Science and Technology",
        "affiliation_city": "Kunming",
        "affiliation_country": "China",
        "affiliation_id": "60020675",
        "affiliation_state": "Yunnan"
    },
    {
        "paper_title": "An Approach to Cloud Platform Log Anomaly Detection Based on Natural Language Processing and LSTM",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "9",
        "cover_date": "2020-12-24",
        "Abstract": "Cloud platform logs record platform runtime information and are important data for cloud platform anomaly detection. Due to the complex log format and rich semantic information, simple statistical analysis methods cannot fully capture log information. And the cloud platform architecture is constantly being updated, log statements are constantly evolving, and new abnormal logs may appear. In addition, most of the existing methods only perform anomaly detection on log templates, and the information is relatively one-sided, which limits the types of anomalies they can detect. Aiming at the problems that most of the current methods will not be able to diagnose or misjudge the unknown log status and miss the abnormality, this paper proposes an anomaly detection method LogNL based on (Natural Language Processing, NLP) and LSTM (Long Short Term Memory, LSTM). LogNL first uses automatic analysis methods to extract log templates and parameters, uses TF-IDF (Term Frequency-Inverse Document Frequency, TF-IDF) to obtain template feature representations, and then constructs parameter value vectors for logs of different templates, and finally uses LSTM network-based construction of pattern anomaly detection models and parameter value anomaly detection models to achieve cloud Platform log anomaly detection. Experiments on two real cloud platform log data sets show that LogNL has higher accuracy than existing supervised learning methods and unsupervised learning methods.",
        "DOI": "10.1145/3446132.3446415",
        "paper_author": "Zhu B.",
        "affiliation_name": "Nanjing University of Aeronautics and Astronautics",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60021666",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "Machine Learning-Based Data Mining Method for Sentiment Analysis of the Sewol Ferry Disaster's Effect on Social Stress",
        "publication": "Frontiers in Psychiatry",
        "citied_by": "8",
        "cover_date": "2020-12-23",
        "Abstract": "The Sewol Ferry Disaster which took place in 16th of April, 2014, was a national level disaster in South Korea that caused severe social distress nation-wide. No research at the domestic level thus far has examined the influence of the disaster on social stress through a sentiment analysis of social media data. Data extracted from YouTube, Twitter, and Facebook were used in this study. The population was users who were randomly selected from the aforementioned social media platforms who had posted texts related to the disaster from April 2014 to March 2015. ANOVA was used for statistical comparison between negative, neutral, and positive sentiments under a 95% confidence level. For NLP-based data mining results, bar graph and word cloud analysis as well as analyses of phrases, entities, and queries were implemented. Research results showed a significantly negative sentiment on all social media platforms. This was mainly related to fundamental agents such as ex-president Park and her related political parties and politicians. YouTube, Twitter, and Facebook results showed negative sentiment in phrases (63.5, 69.4, and 58.9%, respectively), entity (81.1, 69.9, and 76.0%, respectively), and query topic (75.0, 85.4, and 75.0%, respectively). All results were statistically significant (p < 0.001). This research provides scientific evidence of the negative psychological impact of the disaster on the Korean population. This study is significant because it is the first research to conduct sentiment analysis of data extracted from the three largest existing social media platforms regarding the issue of the disaster.",
        "DOI": "10.3389/fpsyt.2020.505673",
        "paper_author": "Lee M.J.",
        "affiliation_name": "Korea University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea",
        "affiliation_id": "60005273",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Proceedings of 2020 7th International Conference on Networking, Systems and Security, NSysS 2020",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2020-12-22",
        "Abstract": "The proceedings contain 15 papers. The topics discussed include: can COVID-19 change the Big5 personality traits of healthcare workers?; attacks on health workers during COVID-19 pandemic - data exploration and news article detection using NLP and GRU model; application of machine learning based hospital up-gradation policy for Bangladesh; a benchmark study on machine learning methods using several feature extraction techniques for news genre detection from Bangla news articles & titles; machine learning based malware detection on encrypted traffic: a comprehensive performance study; efficient feature selection for detecting botnets based on network traffic and behavior analysis; and solving the maze of diagnosing Parkinson’s disease based on portable EEG sensing to be adaptable to go in-the-wild.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Attacks on Health Workers during COVID-19 Pandemic-Data Exploration and News Article Detection using NLP and GRU model",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2020-12-22",
        "Abstract": "A major less spoken impact of COVID-19 is the irrational behavior from people. We are experiencing abnormal behaviour from individuals all over the world. It ranges from \"absurd conspiracy theories\"to \"panic buying of tissue paper\". Likewise, one such example would be attack on health workers-the front line defenders. An outrageous surge is visible regarding attacks on health workers (e.g., doctors, nurses, security personnel) during COVID-19 pervasive. This kind of unsocial behaviour is unexpected and should be prohibited. In this paper, we observe the news articles related to attacks on health workers. We explore data from numerous direction and inspect the relationship between attack-news and various aspects of countries such as literacy rate, GDP, and etc. Furthermore, we apply topic modeling in order to find the bag-of-words that describe best these sorts of news. We reveal masked-emotions within words through emotion lexicon. In addition, we find a way to detect attack-news instantly using Natural Language Processing (NLP) with the help of deep-learning techniques. Gated Recurrent Unit (GRU), a deep learning model works best for our purpose with accuracy up to 94%. Finally, an architecture is proposed for timely vigilance. Henceforth, we believe that our proposed solution will help the policy makers immensely to design their strategies accordingly.",
        "DOI": "10.1145/3428363.3428366",
        "paper_author": "Alam M.M.",
        "affiliation_name": "Bangladesh University of Business and Technology",
        "affiliation_city": "Dhaka",
        "affiliation_country": "Bangladesh",
        "affiliation_id": "60170531",
        "affiliation_state": "Dhaka"
    },
    {
        "paper_title": "Event Detection and Knowledge Mining from Unlabelled Bengali News Articles",
        "publication": "ETCCE 2020 - International Conference on Emerging Technology in Computing, Communication and Electronics",
        "citied_by": "4",
        "cover_date": "2020-12-21",
        "Abstract": "Text mining is one of the most important tasks in Natural Language Processing (NLP). It is often very difficult to detect events of interest from vast unstructured or data that is not properly labeled. In this paper, we present a machine learning-based approach to discriminate particular event-related news from its text content. We focus here particularly on violence and related events. We have created a dataset from popular Bengali newspapers and used a keyword-based search method to select violence-related news articles. Based on the dataset, we first trained a supervised learning model. We have experimented with six classification algorithms where Logistic Regression outperformed each of the contemporary algorithms in both full and reduced feature set acquiring 75.7 % and 83.4 % accuracy respectively. Using the models, we then further analyzed violence-related news data to find out important insights from unlabeled data.",
        "DOI": "10.1109/ETCCE51779.2020.9350891",
        "paper_author": "Khandokar I.A.",
        "affiliation_name": "United International University",
        "affiliation_city": "Dhaka",
        "affiliation_country": "Bangladesh",
        "affiliation_id": "60002203",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Machine learning application for support for automated control systems users",
        "publication": "Journal of Physics: Conference Series",
        "citied_by": "3",
        "cover_date": "2020-12-21",
        "Abstract": "The article presents the results of the analysis of determining the possibility of Machine Learning (ML) using for solving the problems of incident classification of users on the example of enterprise resource planning (ERP) systems of JSC Russian Railways and choosing a rational method for solving this problem. The presented problem is a special case of the Natural Language Processing Problem (NLP) with the use of neural networks. The article covers findings of classification of user queries: binary and multi class, based on semantic analysis. In 2018, 1.4 million incidents were reported from users of ERP systems, only 25% of them are suitable for ML applications. Resolving other incidents requires the participation of a support specialist. Implementation of the software will reduce the waiting time for incident processing by 64% of the average. This will increase staff productivity by 5-7%. This paper explicitly proves that using convolutional neural networks for user incident classification problem has high potential in JSC Russian Railways.",
        "DOI": "10.1088/1742-6596/1680/1/012019",
        "paper_author": "Khromov S.K.",
        "affiliation_name": "Russian University of Transport",
        "affiliation_city": "Moscow",
        "affiliation_country": "Russian Federation",
        "affiliation_id": "60018918",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Simultaneous generation and real-time observation of loosely bound solitons and noise-like pulses in a dispersion-managed fiber laser with net-normal dispersion",
        "publication": "Optics Express",
        "citied_by": "19",
        "cover_date": "2020-12-21",
        "Abstract": "We demonstrate the coexisting dynamics of loosely bound solitons and noise-like pulses (NLPs) in a passively mode-locked fiber laser with net-normal dispersion. The total pulse number of the single soliton bunch under the NLP operation regime almost increases linearly with increasing pump power, whereas the average pulse spacing decreases accordingly. Furthermore, pulse-to-pulse separation between adjacent soliton pulses in one soliton bunch keeps in the range of hundreds of picoseconds, which decreases from left to right with the change of time. Besides, the real-time observation has been performed by utilizing the time-stretch method, showing that each one of the loosely bound solitons on the NLP operation is actually composed of chaotic wave packets with random intensities. These findings obtained will facilitate the in-depth understanding of nonlinear pulse behaviors in ultrafast optics.",
        "DOI": "10.1364/OE.413597",
        "paper_author": "Wang Z.",
        "affiliation_name": "Shenzhen University",
        "affiliation_city": "Shenzhen",
        "affiliation_country": "China",
        "affiliation_id": "60000937",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Generating UML Class Diagram using NLP Techniques and Heuristic Rules",
        "publication": "Proceedings - STA 2020: 2020 20th International Conference on Sciences and Techniques of Automatic Control and Computer Engineering",
        "citied_by": "26",
        "cover_date": "2020-12-20",
        "Abstract": "Several tools and approaches have been proposed to generate Unified Modeling Language (UML) diagrams. Researchers focus on automating the process of extracting valuable information from Natural Language (NL) text to generate UML models. The existing approaches show less accurateness because of the ambiguity of NL. In this paper, we present a method for generation class models from software specification requirements using NL practices and a set of heuristic rules to facilitate the transformation process. The NL requirements are converted into a formal and controlled representation to increase the accuracy of the generated class diagram. A set of pre-defined rules has been developed to extract OO concepts such as classes, attributes, methods, and relationships to generate a UML class diagram from the given requirements specifications. The approach has been applied and evaluated practically, where the results show that the approach is both feasible and acceptable.",
        "DOI": "10.1109/STA50679.2020.9329301",
        "paper_author": "Abdelnabi E.A.",
        "affiliation_name": "University of Benghazi",
        "affiliation_city": "Benghazi",
        "affiliation_country": "Libya",
        "affiliation_id": "60072597",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "One-step optimization strategy in the simulated moving bed process with asynchronous movement of ports: A VariCol case study",
        "publication": "Journal of Chromatography A",
        "citied_by": "9",
        "cover_date": "2020-12-20",
        "Abstract": "The VariCol process is a variant of the conventional simulated moving bed (SMB) process, distinguished by the asynchronous shifting of the inlet and outlet ports of the chromatographic column train. This feature allows for a more flexible operation in column utilization and can also achieve higher separation performances. However, to take full benefit out of it, the operating parameters, such as the strategy for port switching, must be optimal. in this paper, a novel methodology for optimizing those parameters, based on a single NLP (non-linear programming), is proposed. The main advantage of this approach is that it significantly reduces the complexity of the original MINLP (mixed-integer non-linear programming) formulation currently discussed in the literature. The proposed optimization problem is built, considering that the average column configuration of three zones provides the necessary and sufficient information to describe the VariCol process. Several optimization scenarios for the enantioseparation of 1,1´-bi-2-naphthol and aminoglutethimide were considered to evaluate the proposed methodology and to compare the performance of VariCol and SMB processes. The results have shown that with the single NLP approach, it is possible to explore the optimal solution in all the VariCol process domains with less computational effort than other optimization strategies reported in the literature. That is a great advantage, especially in the context of real-time applications.",
        "DOI": "10.1016/j.chroma.2020.461672",
        "paper_author": "Calderón Supelano R.",
        "affiliation_name": "Universidade Federal do Rio de Janeiro",
        "affiliation_city": "Rio de Janeiro",
        "affiliation_country": "Brazil",
        "affiliation_id": "60000036",
        "affiliation_state": "RJ"
    },
    {
        "paper_title": "Computer-aided blended extractant design and screening for co-extracting phenolic, polycyclic aromatic hydrocarbons and nitrogen heterocyclic compounds pollutants from coal chemical wastewater",
        "publication": "Journal of Cleaner Production",
        "citied_by": "20",
        "cover_date": "2020-12-20",
        "Abstract": "An innovative design and screening methodology of blended extractants was proposed, for co-extracting phenolic, polycyclic aromatic hydrocarbons (PAHs) and nitrogen heterocyclic compounds (NHCs) pollutants from coal chemical wastewater. The methodology includes two major issues, generating single-component candidate extractants and screening optimal binary extractants. In this work, two single-component extractant candidate sets, which present the potential to be good extractants of phenolic and PAHs/NHCs pollutants respectively, were generated by data searching of the Existing Commercial Compound Database (ECCD). The binary extractant candidate set was generated by traversal inter-combination between these two single-component candidate sets. To screen the target blended extractants from the generated candidates, a series of non-linear programming (NLP) models were set up and solved via special tailored strategy. Pseudo-component was introduced to represent various pollutants in coal chemical wastewater which reducing the size and complexity of NLP problems. Miscibility constraints of blended extractants and grid parallel computing method were adopted to reduce the time cost. Case study showed that the selected binary extractants can achieve co-extracting phenolic and PAHs/NHCs pollutants. It will benefit the sustainable development of coal chemical industry in China.",
        "DOI": "10.1016/j.jclepro.2020.122334",
        "paper_author": "Xu R.",
        "affiliation_name": "Institute of Process Engineering Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60027470",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Product Review Sentiment Analysis by Using NLP and Machine Learning in Bangla Language",
        "publication": "ICCIT 2020 - 23rd International Conference on Computer and Information Technology, Proceedings",
        "citied_by": "29",
        "cover_date": "2020-12-19",
        "Abstract": "In this era of internet technology, in Bangladesh, online marketing or e-commerce businesses were already thriving. Due to the COVID-19 pandemic, as people are in lockdown, online shopping became the main platform for shopping as it is the safest way. It accelerated the businesses to come online. More online product service providers makes it better for people but also raises the question of product quality and services. So it is easy for new customers to get scammed while shopping online. Our goal is to make a system that will analyze the customer's feedback from online shopping and provide a ratio of the positive and negative feedback written in Bangla from the previous customers using Natural Language Processing (NLP). We have collected over 1000 feedback and comments on the product to conduct the research. We used sentiment analysis along with some classification algorithms like KNN, Decision Tree, Support Vector Machine (SVM), Random Forest and Logistic Regression. With the highest accuracy of 88.81%, SVM outperformed all the other algorithms.",
        "DOI": "10.1109/ICCIT51783.2020.9392733",
        "paper_author": "Shafin M.A.",
        "affiliation_name": "Daffodil International University",
        "affiliation_city": "Dhaka",
        "affiliation_country": "Bangladesh",
        "affiliation_id": "60027523",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Sentiment Classification in Bangla Textual Content: A Comparative Study",
        "publication": "ICCIT 2020 - 23rd International Conference on Computer and Information Technology, Proceedings",
        "citied_by": "14",
        "cover_date": "2020-12-19",
        "Abstract": "Sentiment analysis has been widely used to understand our views on social and political agendas or user experiences over a product. It is one of the cores and well-researched areas in NLP. However, for low-resource languages, like Bangla, one of the prominent challenge is the lack of resources. Another important limitation, in the current literature for Bangla, is the absence of comparable results due to the lack of a well-defined train/test split. In this study, we explore several publicly available sentiment labeled datasets and designed classifiers using both classical and deep learning algorithms. In our study, the classical algorithms include SVM and Random Forest, and deep learning algorithms include CNN, FastText, and transformer-based models. We compare these models in terms of model performance and time-resource complexity. Our finding suggests transformer-based models, which have not been explored earlier for Bangla, outperform all other models. Furthermore, we created a weighted list of lexicon content based on the valence score per class. We then analyzed the content for high significance entries per class, in the datasets. For reproducibility, we make publicly available data splits and the ranked lexicon list. The presented results can be used for future studies as a benchmark.",
        "DOI": "10.1109/ICCIT51783.2020.9392681",
        "paper_author": "Hasan M.A.",
        "affiliation_name": "Cognitive Insight Limited",
        "affiliation_city": "Dhaka",
        "affiliation_country": "Bangladesh",
        "affiliation_id": "124210591",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Sentiment Analysis and Product Review Classification in E-commerce Platform",
        "publication": "ICCIT 2020 - 23rd International Conference on Computer and Information Technology, Proceedings",
        "citied_by": "24",
        "cover_date": "2020-12-19",
        "Abstract": "Online shopping is becoming one of the most de-manding everyday needs, nowadays. These days people are feeling comfortable shopping online. The number of its customers is increasing day by day as well as raising some problems. The major problem is that the customers can not choose the quality-full product by reading every review of an online product. Besides, the product reviews are helpful to improve the services of an e-commerce site but required huge manpower and time. We have focused on Bangla text and aimed to solve these problems by the application of Deep Neural Network (DNN) and Natural Language Processing (NLP). In this study, we have proposed two deep learning NLP models: one is for sentiment analysis and the other one is for Product Review Classification intended to improve both the quality and services. Significantly, our proposed models result in high accuracy: 0.84 and 0.69 for both Sentiment Analysis and Product Review Classification, respectively. Undoubtedly, these models can help the customers to choose the right product and the service provider to improve their services.",
        "DOI": "10.1109/ICCIT51783.2020.9392710",
        "paper_author": "Munna M.H.",
        "affiliation_name": "University of Rajshahi",
        "affiliation_city": "Rajshahi",
        "affiliation_country": "Bangladesh",
        "affiliation_id": "60020210",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Harnessing Ensemble of Data Preprocessing and Hand-crafted Features for Irony Detection in Tweets",
        "publication": "ICCIT 2020 - 23rd International Conference on Computer and Information Technology, Proceedings",
        "citied_by": "4",
        "cover_date": "2020-12-19",
        "Abstract": "In the decade of microblogging, people are more inclined to use figurative languages such as irony to express their opinions towards various issues. Determining the ironic orientation of tweets is crucial for various natural language processing (NLP) tasks especially in opinion mining due to its twisted nature. However, the noisy and informal characteristics of tweets demand a formidable system to address the challenges of detecting ironic tweets. In this paper, we propose an approach for irony detection in tweets. We focus on an ensemble of preprocessing techniques to address the noisy tweet characteristics and idiosyncratic natures. Besides, we utilize an effective combination of n-gram features and various hand-crafted features in a unified supervised classification model. We leverage the lexical syntax, parts-of-speech (POS), and opinionated lexicons to extract the hand-crafted features and employ a feature selection technique to select the best combination. We present a thorough analysis of the contribution of various features and preprocessing techniques using the SemEva1-2018 benchmark irony detection dataset in tweets. Experimental findings demonstrate the efficacy of our method over several state-of-the-art methods.",
        "DOI": "10.1109/ICCIT51783.2020.9392711",
        "paper_author": "Tasneem F.",
        "affiliation_name": "University of Chittagong",
        "affiliation_city": "Chittagong",
        "affiliation_country": "Bangladesh",
        "affiliation_id": "60033059",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Evaluating Machine Learning Algorithms for Bengali Fake News Detection",
        "publication": "ICCIT 2020 - 23rd International Conference on Computer and Information Technology, Proceedings",
        "citied_by": "30",
        "cover_date": "2020-12-19",
        "Abstract": "In this world of modern technologies and media, online news publications and portals are increasing at a high speed. That is why, nowadays, it has become almost impossible to check out the traditional fact of news headlines and examine them due to the increase in the number of content writers, online media portals, and news portals. Mostly, fake headlines are filled with bogus or misleading content. They attract the commoners by putting phony words or misleading fraudulent content in the headlines to increase their views and share. But, these fake and misleading headlines create havoc in the commoner's life and misguide them in many ways. That is why we took a step so that the commoners can differentiate between fake and real news. We proposed a model that can successfully detect whether the story is fake or accurate based on the news headlines. We created a novel data set of Bengali language and achieved our aim and reached the target using the Gaussian Naive Bayes algorithm. We have used other algorithms, but the Gaussian Naive Algorithm has performed well in our model. This algorithm used a text feature dependent on TF-IDF and an Extra Tree Classifier to choose the attribute. In our model, using Gaussian Naive Bayes we got 87% accuracy which is comparatively best than any other algorithm we used in this model.",
        "DOI": "10.1109/ICCIT51783.2020.9392662",
        "paper_author": "Mugdha S.B.S.",
        "affiliation_name": "United International University",
        "affiliation_city": "Dhaka",
        "affiliation_country": "Bangladesh",
        "affiliation_id": "60002203",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Bangla News Trend Observation using LDA Based Topic Modeling",
        "publication": "ICCIT 2020 - 23rd International Conference on Computer and Information Technology, Proceedings",
        "citied_by": "6",
        "cover_date": "2020-12-19",
        "Abstract": "Topic Modelling is an essential field of natural language processing (NLP) that can be considered as a type of statistical model for extracting the abstract topics that have occurred in a collection of documents. Bangla is among the most popular and used languages around the world and nowadays innumerable Bangla texts are generated through digital and social media. So the significance of extracting knowledge from these data is invaluable for various sectors. However, the number of works in this field is inadequate because of the lack of proper datasets, tools, and applications. Therefore, preparing a convenient dataset in Bangla can be a great help for topic modeling as well as for other NLP related research. In this paper, we have addressed some of those complications by creating a proper dataset. Also, we have demonstrated a method of observing the Bangla media trend by applying Latent Dirichlet Allocation (LDA) on newspaper articles. The result of our experiment suggests that the proposed method can be an admissible way of utilizing news media data to observe media trends overtime properly.",
        "DOI": "10.1109/ICCIT51783.2020.9392719",
        "paper_author": "Alam K.M.",
        "affiliation_name": "Khulna University",
        "affiliation_city": "Khulna",
        "affiliation_country": "Bangladesh",
        "affiliation_id": "60027315",
        "affiliation_state": "Khulna"
    },
    {
        "paper_title": "Combating Domestic Violence during COVID-19 Pandemic in Bangladesh: Using a Mobile Application integrated with an Effective Solution",
        "publication": "ICCIT 2020 - 23rd International Conference on Computer and Information Technology, Proceedings",
        "citied_by": "6",
        "cover_date": "2020-12-19",
        "Abstract": "Domestic violence (DV) is not new. Yet, researchers and human rights experts are reporting an alarming rise in DV against women since countries began locking down areas to stop the virus from spreading. They are now calling for a way to assist victims without risking infection of the virus. This paper proposes a mobile application that addresses DV in the context of the COVID-19 pandemic. It consists of two interconnected components-the Women's Support Division (WSD) and Conversational Interactive Response (CIR). The CIR module is a Natural Language Processing (NLP) chatbot for answering questions. One of the sub-modules, the Action Key module sends notifications with GPS location to acquaintances. It was found to have a performance time of 8.64 milliseconds which ensures the proposed system transmits data faster than other systems. The system usability scale (SUS) result of our proposed application has an average 66.71% score which indicates the system is fit for use. Finally, data for DV in Bangladesh is analyzed from 2014 to April 2020. Most applications addressed violence against women who are outdoors. Few have focused on DV, which is increasing indoors during the COVID-19 pandemic.",
        "DOI": "10.1109/ICCIT51783.2020.9392691",
        "paper_author": "Hossain M.E.",
        "affiliation_name": "Daffodil International University",
        "affiliation_city": "Dhaka",
        "affiliation_country": "Bangladesh",
        "affiliation_id": "60027523",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Cyber Attack Detection Method Based on NLP and Ensemble Learning Approach",
        "publication": "ICCIT 2020 - 23rd International Conference on Computer and Information Technology, Proceedings",
        "citied_by": "11",
        "cover_date": "2020-12-19",
        "Abstract": "In recent years the use of web applications has gone through fast growth. Billions of transactions, sharing information over the Internet have become a common phenomenon because of web applications. Users send necessary and confidential information to web-based applications and store them into databases. Web applications and connected databases are accessible through the Internet, which makes them prone to cyber-attacks. SQL Injection is a cyber-attack that is most pervasive. Attackers steal intended information by injecting SQL codes. Users of web applications could face damaging effects for SQL Injection. Through this research work, a SQL Injection detection method is proposed, which uses Natural Language Processing (NLP) and Ensemble Learning Algorithm. NLP generates feature patterns, extracts features, transforms each text into numerical representation in the form of a vector, and generates a bag-of-words model (BoW model). BoW model trains Random Forest Classifier. The Random Forest Classifier is an ensemble learning algorithm. This algorithm combines several machine learning algorithms and builds one predictive model to get a better detection ability. The trained classifier classifies SQL Injection payloads with higher accuracy from the dataset.",
        "DOI": "10.1109/ICCIT51783.2020.9392682",
        "paper_author": "Ahmed M.",
        "affiliation_name": "University of Skill Enrichment and Technology",
        "affiliation_city": "Dhaka",
        "affiliation_country": "Bangladesh",
        "affiliation_id": "126076700",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Bangla Documents Classification using Transformer Based Deep Learning Models",
        "publication": "2020 2nd International Conference on Sustainable Technologies for Industry 4.0, STI 2020",
        "citied_by": "31",
        "cover_date": "2020-12-19",
        "Abstract": "Document classification or categorization assign documents to a predefined domain category. The improvement of document classification techniques has been noticeable worldwide recently. Many transformer-based models have been introduced for different languages, which shows significant improvement in this area of Natural Language Processing. In this paper, we have classified Bangla text documents with the most recent transformer or attention mechanism-based models. We have applied the BERT (Bidirectional Encoder Representations from Transformers) and ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately) model for Bangla text classification. Both of them are pre-training text encoders and we have applied fine-tuning approach for the downstream(classification) task. Here, we have used three different Bangla text datasets for our experiment. Both of the models provide outstanding performance for two out of three datasets we have used.",
        "DOI": "10.1109/STI50764.2020.9350394",
        "paper_author": "Rahman M.M.",
        "affiliation_name": "Crowd Realty",
        "affiliation_city": "Tokyo",
        "affiliation_country": "Japan",
        "affiliation_id": "125125091",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Detecting fake news using deep learning and NLP",
        "publication": "Confluence of AI, Machine, and Deep Learning in Cyber Forensics",
        "citied_by": "12",
        "cover_date": "2020-12-18",
        "Abstract": "Fake news is the word making more talk these days be it election, COVID 19 pandemic, or any social unrest. Many social websites have started to fact check the news or articles posted on their websites. The reason being these fake news creates confusion, chaos, misleading the community and society. In this cyber era, citizen journalism is happening more where citizens do the collection, reporting, dissemination, and analyse news or information. This means anyone can publish news on the social websites and lead to unreliable information from the readers' points of view as well. In order to make every nation or country safe place to live by holding a fair and square election, to stop spreading hatred on race, religion, caste, creed, also to have reliable information about COVID 19, and finally from any social unrest, we need to keep a tab on fake news. This chapter presents a way to detect fake news using deep learning technique and natural language processing.",
        "DOI": "10.4018/978-1-7998-4900-1.ch007",
        "paper_author": "Sadasivam U.M.",
        "affiliation_name": "Birla Institute of Technology and Science, Pilani",
        "affiliation_city": "Pilani",
        "affiliation_country": "India",
        "affiliation_id": "60000414",
        "affiliation_state": "RJ"
    },
    {
        "paper_title": "Detailed Study of Deep Learning Models for Natural Language Processing",
        "publication": "Proceedings - IEEE 2020 2nd International Conference on Advances in Computing, Communication Control and Networking, ICACCCN 2020",
        "citied_by": "7",
        "cover_date": "2020-12-18",
        "Abstract": "Natural Language Processing involves computational processing, and understanding of human languages. With the increase in computation power, deep learning models are being used for various NLP tasks. Further availability of large datasets of various languages enables the training of deep learning models. Multiple processing layers are used by the deep learning methods for learning representations of data which are hierarchical in nature and which gives excellent results for different NLP tasks. This paper reviews the important models and methods in deep learning which are applied to natural language problems. In particular, Convolutional Neural Network, Recurrent Neural Network, Long Short-Term Memory, Gated Recurrent Unit, Recursive Neural Network have been described. Also, their advantages and suitability to various natural language processing applications such as text classification, sentiment analysis, etc. have been reviewed.",
        "DOI": "10.1109/ICACCCN51052.2020.9362989",
        "paper_author": "Gupta M.",
        "affiliation_name": "Guru Gobind Singh Indraprastha University",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India",
        "affiliation_id": "60012304",
        "affiliation_state": "DL"
    },
    {
        "paper_title": "An approach towards making Honey Encryption easily available",
        "publication": "Proceedings - IEEE 2020 2nd International Conference on Advances in Computing, Communication Control and Networking, ICACCCN 2020",
        "citied_by": "2",
        "cover_date": "2020-12-18",
        "Abstract": "Honey encryption (HE) was proposed by Juels and Ristenpart in 2014 as a solution against brute force attacks. As honey encryption provides fake messages when a wrong password is entered, it gives an extra layer of security to the user as all these honey messages which consist of sweet words in distribution transforming encoder(DTE). As technology is progressing there are major concerns among users about security. In this paper we are providing ways for making honey encryption available for general and practical use. We have proposed an Application program interface (API) for this which would help in proper improvement in readily making honey encryption for the programmers so as to decrease the hassle of programming from scratch. We have proposed some methods for message space reduction in honey encryption and methods for its application on natural language message. Through this paper our approach for making honey encryption readily available so as to make work easy on it so that it could speed up the progress of honey encryption in research.",
        "DOI": "10.1109/ICACCCN51052.2020.9362866",
        "paper_author": "Nagraj S.",
        "affiliation_name": "Galgotias College of Engineering &amp; Technology",
        "affiliation_city": "Greater Noida",
        "affiliation_country": "India",
        "affiliation_id": "60102234",
        "affiliation_state": "UP"
    },
    {
        "paper_title": "Diving deep in Deep Convolutional Neural Network",
        "publication": "Proceedings - IEEE 2020 2nd International Conference on Advances in Computing, Communication Control and Networking, ICACCCN 2020",
        "citied_by": "40",
        "cover_date": "2020-12-18",
        "Abstract": "Artificial Neural networks have been proved most efficient in Deep Learning mainly because of large number of datasets it can handle. The most widely used is the Convolutional Neural Network (CNN). It has been proved useful for computer vision, pattern recognition and Natural Language Processing (NLP). CNN is so vastly used, as, unlike traditional Neural Nets, it reduces number of parameters and focus more on domain specific features. There are various CNN architectures proposed, such as LeNet, AlexNet, GoogleNet. In this paper, we talk about structure of CNN and all the models of CNN which are proposed till date.",
        "DOI": "10.1109/ICACCCN51052.2020.9362907",
        "paper_author": "Arora D.",
        "affiliation_name": "Guru Gobind Singh Indraprastha University",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India",
        "affiliation_id": "60012304",
        "affiliation_state": "DL"
    },
    {
        "paper_title": "A Comparative Study of Dictionary-based and Machine Learning-based Named Entity Recognition in Pashto",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "2",
        "cover_date": "2020-12-18",
        "Abstract": "Information Extraction (IE) is the process of extracting structured information from unstructured text using natural language processing (NLP). One important sub-task of IE is the extraction of names of persons, places, and organizations, called Named Entity Recognition (NER). NER plays an important role in many NLP applications such as Question Answering, Machine Translation, and Text Summarization. It has been widely studied for high-resource languages like English. However, no research has taken place in this regard for Pashto. We hypothesized that based on the research done for English and other languages in the area of NER a system can be developed for Pashto. We have developed two NER systems for detecting names of persons, places, and organizations in Pashto text. First, a dictionary-based NER that uses three dictionaries containing names of persons, locations, and organizations, respectively. Second, a learning-based approach that uses Hidden Markov Model (HMM) for the task. We have evaluated both systems on a dataset collected from sports news. Our evaluation showed F-Measure of 82% for HMM and 60% for dictionary-based NER. Our findings highlight that HMM outperforms dictionary based NER.",
        "DOI": "10.1145/3443279.3443307",
        "paper_author": "Momand R.",
        "affiliation_name": "Kabul University",
        "affiliation_city": "Kabul",
        "affiliation_country": "Afghanistan",
        "affiliation_id": "60072828",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Age Inference on Twitter using SAGE and TF-IGM",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "3",
        "cover_date": "2020-12-18",
        "Abstract": "Social media is increasingly influential in day-to-day life. People are more than ever sharing, posting, liking, and following different activities on disparate social media. Deriving specific attributes of users based on their online behavior is a growing research field. In this study, a novel methodology is proposed for determining the age of Twitter users. We classify three separate age groups, namely, 18 - 24, 25 - 54, 55 >. We compute numerous linguistic features from the tweets of users, obtain significant terms extracted by the SAGE algorithms, and retrieve relevant meta-data of users by extracting information on their followed interests on Twitter using TF-IGM. The final logistic regression model obtains a macro F1-score of 78%. This way, effectively combining NLP and IR techniques for attribute inference on social media.",
        "DOI": "10.1145/3443279.3443300",
        "paper_author": "Cornelisse J.",
        "affiliation_name": "Universiteit van Amsterdam",
        "affiliation_city": "Amsterdam",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60002483",
        "affiliation_state": "Noord-Holland"
    },
    {
        "paper_title": "Research on Information Extraction of Municipal Solid Waste Crisis using BERT-LSTM-CRF",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "2",
        "cover_date": "2020-12-18",
        "Abstract": "There is much research on the phenomenon of municipal solid waste (MSW) and its improvement measures, and the method of information extraction be adopted to obtain the potential knowledge of MSW from the existing relevant research literature. Due to the complexity and diversity of the MSW, unsupervised training of target texts can be achieved through information data based on manual annotation. According to the characteristics of the BERT language model, a common method in natural language processing(NLP), the pre-trained BERT(Bidirectional Encoder Representation from Transformers) model with LSTM-CRF(Long Short Term Memory-Conditional Random Field) architecture is used in the information extraction of MSW crisis to extract entities and relationships between entities from natural language texts. By the method of calculating and evaluating the extraction effect, it provided technical support for further study of its crisis conversion.",
        "DOI": "10.1145/3443279.3443314",
        "paper_author": "Wan T.",
        "affiliation_name": "Wuhan University of Technology",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60022414",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Bilingual Word Embedding with Sentence Combination CNN for 1-to-N Sentence Alignment",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2020-12-18",
        "Abstract": "Sentence alignment, as one of the most active and fundamental tasks in the field of natural language processing (NLP), is usually realized in two categories of methods. One is traditional methods which are firstly proposed, the other, which are adopted later, is based on the Neural Network method. Presently, under the limitation that the existing mainstream data corpora are mostly in the form of 1-to-1, the alignment models with relatively good performance mainly apply to the cases of 1-to-1 sentence alignment. However, under the circumstance that a sentence contains too much information, 1-to-N sentence alignment can actually have a better effect on sentence translation tasks, compared with the 1-to-1 form, since it is more flexible and can reduce the complexity of the original sentence. As a result, we attempt to exploit neural networks with relatively good performance in the cases of 1-to-1 to fit in the cases of 1-to-N. In this paper, a novel 1-N Bilingual word Embedding with Sentence Combination CNN Improved Framework (1-NBESCC) is proposed in order to align 1-to-N sentences more precisely. Experiments show that our proposed model performs as good as the traditional methods such as BLEUALIGN in 1-to-1 situation, but much better in 1-to-N situation.",
        "DOI": "10.1145/3443279.3443287",
        "paper_author": "Ren X.",
        "affiliation_name": "Beijing University of Posts and Telecommunications",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60016930",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Sentiment Analysis for Review Rating Prediction in a Travel Journal",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "4",
        "cover_date": "2020-12-18",
        "Abstract": "This paper presents sentiment analysis to predict numerical rating of text reviews in a web-based travel journal application. The application allows users to record and provide text reviews on tourist spots visited. Text reviews undergo parts-of-speech (POS) tagging, rule-based phrase chunking and dependency parsing to extract opinion phrases in noun-adjective and noun-verb pairs from the original text. Each pair is further classified to one of the four categories: accommodation, food, entertainment and tourist attraction using the noun against a curated bag-of-words (BOW) to ensure that only relevant statements are included in the scoring. Word Sense Disambiguation is performed to correctly identify the word sense that matches the meaning of the sentence using WordNet. SentiWordNet, a lexical resource for sentiment analysis, was used to determine polarity score representing the emotional intensity of the review. The system predicted star rating was compared with the actual author rating in Google Maps and with human annotator ratings who are asked to label the text reviews. The predicted rating scored low mean absolute error (MAE) between the system and human rating which means that the rating predicted is closer to human interpretation of the text reviews. Overall rating prediction accuracy is 82%.",
        "DOI": "10.1145/3443279.3443282",
        "paper_author": "Cuizon J.C.",
        "affiliation_name": "University of San Jose-Recoletos",
        "affiliation_city": "Cebu",
        "affiliation_country": "Philippines",
        "affiliation_id": "60089610",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Comparative Analyses of Bert, Roberta, Distilbert, and Xlnet for Text-Based Emotion Recognition",
        "publication": "2020 17th International Computer Conference on Wavelet Active Media Technology and Information Processing, ICCWAMTIP 2020",
        "citied_by": "153",
        "cover_date": "2020-12-18",
        "Abstract": "Transformers' feat is attributed to its better language understanding abilities to achieve state-of-the-art results in medicine, education, and other major NLP tasks. This paper analyzes the efficacy of BERT, RoBERTa, DistilBERT, and XLNet pre-trained transformer models in recognizing emotions from texts. The paper undertakes this by analyzing each candidate model's output compared with the remaining candidate models. The implemented models are fine-tuned on the ISEAR data to distinguish emotions into anger, disgust, sadness, fear, joy, shame, and guilt. Using the same hyperparameters, the recorded model accuracies in decreasing order are 0.7431, 0.7299, 0.7009, 0.6693 for RoBERTa, XLNet, BERT, and DistilBERT, respectively.",
        "DOI": "10.1109/ICCWAMTIP51612.2020.9317379",
        "paper_author": "Adoma A.F.",
        "affiliation_name": "University of Electronic Science and Technology of China",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China",
        "affiliation_id": "60005465",
        "affiliation_state": "Sichuan"
    },
    {
        "paper_title": "Comparison of Pretrained Embeddings to Identify Hate Speech in Indian Code-Mixed Text",
        "publication": "Proceedings - IEEE 2020 2nd International Conference on Advances in Computing, Communication Control and Networking, ICACCCN 2020",
        "citied_by": "29",
        "cover_date": "2020-12-18",
        "Abstract": "Two or more languages used in the same sentence is known as the code-mixed text. The phenomenon is abundant in social media due to multilingualism. It poses a considerable challenge for classic NLP tools trained on monolingual corpora. Automatic hate speech detection in code-mixed text becomes even more challenging due to non-standard variations in the spelling, grammar and writing in foreign scripts. Pre-Trained models provide word embedding trained on massive monolingual corpora, which are now ubiquitous forms of word representation to classifying text. In this paper, we compare pretrained models and create an ensemble model for code-mixed data of hate speech classification task on Hindi-English data. We have also experimented with using word embedding for CNN networks and showed that XLNet performs better for hate speech detection in code-mixed text.",
        "DOI": "10.1109/ICACCCN51052.2020.9362731",
        "paper_author": "Banerjee S.",
        "affiliation_name": "Jaypee Institute of Information Technology",
        "affiliation_city": "Noida",
        "affiliation_country": "India",
        "affiliation_id": "60080305",
        "affiliation_state": "UP"
    },
    {
        "paper_title": "Summarizing online product reviews in bengali based on similarity using sequence to sequence RNNs",
        "publication": "Proceedings of 2020 11th International Conference on Electrical and Computer Engineering, ICECE 2020",
        "citied_by": "1",
        "cover_date": "2020-12-17",
        "Abstract": "By text summarization, large texts or documents can be concise while preserving the context of the main document. This research work is focused on summarizing online product reviews in Bengali Language. By using online user feedback, future buyers will be able to take decisions whether to buy the product or not. However, it is very difficult for a buyer to decide by reading manually all the reviews. Machine learning techniques are available nowadays for text summarization which is a promising solution for extracting information from those user reviews. There are many well-known summarization tools for English language but in Bengali, there are very few and insufficient tools for text summarization. In this paper, a model has been for abstract Bangla text summarization on online product reviews using a Recurrent Neural Network(RNN). Long ShortTerm Memory (LSTM) and Sequence-to-Sequence (Seq2Seq) based RNN has been applied here. Experimented results underscore that the training loss is reduced to 0.0034 and able to generate a frequent predictive summary from original texts or documents. This research work also includes a Word Mover's Distance(WMD) method which checks the similarity between human and machine summary. It is observed that, the applied WMD in this work for text similarity model has been able to achieve better performance than the Jaccard method.",
        "DOI": "10.1109/ICECE51571.2020.9393048",
        "paper_author": "Islam M.J.",
        "affiliation_name": "Comilla University",
        "affiliation_city": "Comilla",
        "affiliation_country": "Bangladesh",
        "affiliation_id": "126186734",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Social media content categorization using supervised based machine learning methods and natural language processing in bangla language",
        "publication": "Proceedings of 2020 11th International Conference on Electrical and Computer Engineering, ICECE 2020",
        "citied_by": "7",
        "cover_date": "2020-12-17",
        "Abstract": "Social media has acquired the primary platform for people to connect. Millions of posts generate from social media consistently. The people of Bangladesh are habitually comfortable sharing their opinion on social media in the Bangla language. It is often arduous to place them in distinct categories relying on textual information. Classifying social media posts are challenging. It tends to be complicated to scrutinize when scripted in Bangla language. Our aspiration is to categorize these opinions from social platforms to enable searching, filtering, and organizing based on post sentiment. We employed the Sentiment Analysis to interpret the persuasion of the posts. We introduced a model that will classify the Bangla posts in several categories by using Support Vector Machine (SVM), K-Nearest Neighbor (KNN), Decision Tree, Random Forest, Logistic regression algorithms. We adopted the algorithm that provides the most reliable performance to classify the social media post with quite proficient in Bangla Language.",
        "DOI": "10.1109/ICECE51571.2020.9393095",
        "paper_author": "Alam M.R.",
        "affiliation_name": "Daffodil International University",
        "affiliation_city": "Dhaka",
        "affiliation_country": "Bangladesh",
        "affiliation_id": "60027523",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Indian Sign Language Interpretation and Sentence Formation",
        "publication": "2020 IEEE Pune Section International Conference, PuneCon 2020",
        "citied_by": "11",
        "cover_date": "2020-12-16",
        "Abstract": "People with speech and hearing disabilities approximately constitute 1 percentage of the total Indian population. A person who is hearing and speech impaired is not able to compete or work with a normal person in a normal environment because of the lack of a proper communication medium.Sign Language is used for communication amongst them. Sign Language is the most natural and expressive way for the hearing and speech impaired. This paper proposes a method that recognizes Sign Language and converts it to normal text and speech for fast and improved communication amongst them and also with others. The focus is on the Indian Sign Language (ISL) specifically as there is no substantial work on ISL rendering the above requirements for these people.The paper focuses on developing a real-time hands-on system that takes video inputs of gestures in the specified ROI and performs gesture recognition using various feature extraction techniques and Hybrid-CNN model trained using the ISL database created. The correctly identified gesture tokens are sent to a Rule-Based Grammar and for Web Search query to generate various sentences and a Multi-Headed BERT grammar corrector provides grammatically precise and correct sentences as the final output.",
        "DOI": "10.1109/PuneCon50868.2020.9362383",
        "paper_author": "Gangadia D.",
        "affiliation_name": "Dwarkadas Jivanlal Sanghvi College of Engineering",
        "affiliation_city": "Mumbai",
        "affiliation_country": "India",
        "affiliation_id": "60114754",
        "affiliation_state": "MH"
    },
    {
        "paper_title": "Interpretable Sentiment Analysis based on Deep Learning: An overview",
        "publication": "2020 IEEE Pune Section International Conference, PuneCon 2020",
        "citied_by": "7",
        "cover_date": "2020-12-16",
        "Abstract": "Sentiment analysis (SA) or emotion AI or opinion mining uses natural language processing (NLP). Sentiment Analysis identify, study, quantify, obtain, tacit states and subject related information. Broad spectrum of areas influenced due to Sentiment Analysis such as policy making by the government, finding mental health of individuals, finding misuse of drugs in healthcare, fraud detection in the financial sector, covid-19 awareness and impact, Cyber-crime etc. As the amplitude of social media data increases day by day, there is a need to automatically address sentiment analysis. Deep learning handles it very well. It gives very good accuracy but incomprehensibility in decision strategy. For better decision-making trust, believe, fairness, reliability, and unbiasing is important. This paper explores the work done in this area along with popular techniques to address interpretability in sentiment analysis and its evaluation criteria.",
        "DOI": "10.1109/PuneCon50868.2020.9362361",
        "paper_author": "Jawale S.",
        "affiliation_name": "Datta Meghe College of Engineering",
        "affiliation_city": "Navi Mumbai",
        "affiliation_country": "India",
        "affiliation_id": "60272696",
        "affiliation_state": "MH"
    },
    {
        "paper_title": "A novel approach for identifying social media posts indicative of depression",
        "publication": "Proceedings - 2020 IEEE International Symposium on Sustainable Energy, Signal Processing and Cyber Security, iSSSC 2020",
        "citied_by": "2",
        "cover_date": "2020-12-16",
        "Abstract": "In this paper, we put forward a novel approach for identifying the social media posts that are indicative of depression with the help of Long Short-Term Memory (LSTM) and Natural Language Processing (NLP) methodologies, Word embedding, n-gram tokenization, and word2vec to identify the text that expresses feelings of depression and its related sentiments. The approach accurately predicts sentiment in the text through Deep Learning, which removes false positives by considering the immediate context of words. The data for this experiment has been scraped from public forums on Reddit-a popular social media website. Labeling is made before analyzing the data, which allows post about a common topic to be grouped. Posts from multiple groups discussing depression and self-harm are taken as the positive class, while posts from miscellaneous, random groups are taken as the negative class. Given the diversity of the negative class, the dataset may be said to be representative of a real-world scenario. The model developed has applications across a broad spectrum of domains, such as deployed on social media and communication forums frequented by children to detect possible harmful tendencies.",
        "DOI": "10.1109/iSSSC50941.2020.9358866",
        "paper_author": "Mahapatra A.",
        "affiliation_name": "Indira Gandhi Institute of Technology, Sarang",
        "affiliation_city": "Dhenkanal",
        "affiliation_country": "India",
        "affiliation_id": "60071283",
        "affiliation_state": "OR"
    },
    {
        "paper_title": "Sentence level language identification in Gujarati-Hindi code-mixed scripts",
        "publication": "Proceedings - 2020 IEEE International Symposium on Sustainable Energy, Signal Processing and Cyber Security, iSSSC 2020",
        "citied_by": "9",
        "cover_date": "2020-12-16",
        "Abstract": "On social media, people often use to chat in multilingual code. It happens when a person knows more than one language. Multilingual speakers often use multiple languages to communicate when speaking and writing. This language interchanging involves complex grammar, and the terms used for these are code-mixing and code-switching. Due to the increasing social media usage and conversation on online chat platforms, many textual and non-textual communication happens in different languages. As a result, a large number of multilingual code-mixed corpus generates in digital form. To apply any Natural Language Processing (NLP) techniques on such types of corpus to build an Information Retrieval (IR) system, which may not give desired outcomes because of the noisy nature of inputs. This type of noise in the scenario of multiple languages in a multilingual code-mixed corpus and identifying particular language at the sentence level is problematic because it involves various forms of a single language. i.e., code-mixed sentences involve different languages and are written in the Roman script and the native script. In this paper, we address the solution to the language identification problem in the Gujarati code-mixed script in which the Gujarati language is written in various forms, i.e., romanized and non-romanized forms. In the context of the current problem, we first time created our own dataset and for experiment we have used a large dataset for the Gujarati language in which Gujarati mixed-up with Hindi and English. Further, we have used various machine learning classifiers. We have first time used this much large amount of data of Gujarati and received the highest accuracy 92% with a Support Vector Classifier.",
        "DOI": "10.1109/iSSSC50941.2020.9358837",
        "paper_author": "Kazi M.",
        "affiliation_name": "Marwadi University",
        "affiliation_city": "Rajkot",
        "affiliation_country": "India",
        "affiliation_id": "60138770",
        "affiliation_state": "GJ"
    },
    {
        "paper_title": "CEREX@FIRE-2020: Overview of the Shared Task on Cause-effect Relation Extraction",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2020-12-16",
        "Abstract": "Extraction of causal relations from text is an important problem in Natural Language Processing (NLP). The extracted relations play important roles in several downstream analytical and predictive tasks like identification of actionable items, question-answering and isolation of predictor variables for a predictive system. Curating causal relations from text documents can also help in automatically building causal networks which are also useful for reasoning tasks. The proposed CEREX track aims to find a suitable model for automatic detection of causal sentences and extraction of the exact cause, effect and the causal connectives from textual mentions.",
        "DOI": "10.1145/3441501.3441514",
        "paper_author": "Sinha M.",
        "affiliation_name": "Indian Institute of Technology Kharagpur",
        "affiliation_city": "Kharagpur",
        "affiliation_country": "India",
        "affiliation_id": "60004750",
        "affiliation_state": "WB"
    },
    {
        "paper_title": "Uncovering Machine Learning-Ready Data from Public Clinical Trial Resources: A case-study on normalization across Aggregate Content of ClinicalTrials.gov",
        "publication": "Proceedings - 2020 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2020",
        "citied_by": "1",
        "cover_date": "2020-12-16",
        "Abstract": "The state of clinical data is a barrier to the development of machine learning models to improve healthcare. Uncontrolled clinical freetext is common in both the patient and clinical trials: the resulting spelling, grammatical errors, phrasing variation, and other resulting variability results in difficult-to-leverage data. As part of our effort to harmonize the Aggregate Analysis of ClinicalTrials.gov (AACT) drop-withdrawal reasons to a controlled vocabulary, we explored two solutions. Elastic's fuzzy matching capability matched entries in the AACT Drop-Withdrawal table to a list of user-specified terms (74.6% coverage). The second approach was a custom pipeline employing NLP preprocessing, Levenshtein Distance (Fuzzy Matching), and semantic similarity mapping using a pre-trained FastText Model (98% coverage). Although manual oversight is still required, the amount of effort to harmonize with a controlled vocabulary is notably reduced. This work enables the rapid harmonization of clinical databases, allowing them to be leveraged for machine learning and analytics.",
        "DOI": "10.1109/BIBM49941.2020.9313362",
        "paper_author": "Hutchison E.R.",
        "affiliation_name": "AstraZeneca",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60004219",
        "affiliation_state": "Cambridgeshire"
    },
    {
        "paper_title": "Extracting Protein-Protein Interactions Affected by Mutations via Auxiliary Task and Domain Pre-trained Model",
        "publication": "Proceedings - 2020 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2020",
        "citied_by": "5",
        "cover_date": "2020-12-16",
        "Abstract": "Extracting protein-protein interaction affected by genetic mutation from biomedical literature automatically is an essential step toward the ultimate goal of precision medicine. However, the existing methods fail to be accurate enough to meet the needs in practice. In this paper, considering the significant progress made by the pre-training model in a wide variety of NLP tasks, we use BioBERT to obtain the representation of the text and adopt a multi-task learning strategy to improve the performance. Evaluated on the BioCreative VI PPIm data set, our proposed model achieves a new state-of-the-art performance that surpassed the previous one by 4.86% in F1-score. The source code is available at https://github.com/dlutwy/ppim.",
        "DOI": "10.1109/BIBM49941.2020.9313120",
        "paper_author": "Wang Y.",
        "affiliation_name": "Dalian University of Technology",
        "affiliation_city": "Dalian",
        "affiliation_country": "China",
        "affiliation_id": "60004538",
        "affiliation_state": "Liaoning"
    },
    {
        "paper_title": "ScSNVIndel. accurate and efficient calling of SNVs and indels from single cell sequencing using integrated Bi-LSTM",
        "publication": "Proceedings - 2020 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2020",
        "citied_by": "0",
        "cover_date": "2020-12-16",
        "Abstract": "Single-cell data are sparse and have coverage fluctuations, making it difficult, in comparison with data obtained from next-generation sequencing (NGS), to call single nucleotide variants (SNVs) and indels. Furthermore, most existing sequencing methods are unable to effectively call whole-genome SNVs and indels from single cell sequencing (SCS) data. In this study, we propose a new method for the efficient identification of SNVs and indels from SCS data, called scSNVIndel. scSNVIndel uses bidirectional long short-term memory (Bi-LSTM) as its base and integrates new natural language processing (NLP) technology. It automatically extracts features and accurately calls SNVs and indels when using SCS data, which is characterized by uneven and discontinuous coverage. Moreover, scSNVIndel can call variants from the sequence directly, retaining valuable information from the SCS data, as it does not convert the sequence into an image like the DeepVariant method. The results show that scSNVIndel performs better in terms of accuracy and recall for calling variants, when compared with other existing methods. scSNVIndel is currently an open-source method, available at https://github.com/CSuperlei/scSNVIndel, and its usage methods are published on the following website: https://www.aiguqu.com/2020/06/18/scSNVIndel/.",
        "DOI": "10.1109/BIBM49941.2020.9313484",
        "paper_author": "Cai L.",
        "affiliation_name": "Beijing University of Chemical Technology",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60021843",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Attention Mechanism with BERT for Content Annotation and Categorization of Pregnancy-Related Questions on a Community QA Site",
        "publication": "Proceedings - 2020 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2020",
        "citied_by": "11",
        "cover_date": "2020-12-16",
        "Abstract": "In recent years, the social web has been increasingly used for health information seeking, sharing, and subsequent health-related research. Women often use the Internet or social networking sites to seek information related to pregnancy in different stages. They may ask questions about birth control, trying to conceive, labor, or taking care of a newborn or baby. Classifying different types of questions about pregnancy information (e.g., before, during, and after pregnancy) can inform the design of social media and professional websites for pregnancy education and support. This research aims to investigate the attention mechanism built-in or added on top of the BERT model in classifying and annotating the pregnancy-related questions posted on a community QA site. We evaluated two BERT-based models and compared them against the traditional machine learning models for question classification. Most importantly, we investigated two attention mechanisms: the built-in selfattention mechanism of BERT and the additional attention layer on top of BERT for relevant term annotation. The classification performance showed that the BERT-based models worked better than the traditional models, and BERT with an additional attention layer can achieve higher overall precision than the basic BERT model. The results also showed that both attention mechanisms work differently on annotating relevant content, and they could serve as feature selection methods for text mining in general.",
        "DOI": "10.1109/BIBM49941.2020.9313379",
        "paper_author": "Luo X.",
        "affiliation_name": "Indiana University-Purdue University Indianapolis",
        "affiliation_city": "Indianapolis",
        "affiliation_country": "United States",
        "affiliation_id": "60024609",
        "affiliation_state": "IN"
    },
    {
        "paper_title": "Cross2Self-attentive Bidirectional Recurrent Neural Network with BERT for Biomedical Semantic Text Similarity",
        "publication": "Proceedings - 2020 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2020",
        "citied_by": "12",
        "cover_date": "2020-12-16",
        "Abstract": "Estimating the similarity of biomedical sentence pair is an important component in such natural language processing (NLP) tasks as text retrieval and text summarization with great amount of biomedical information growing. Deep learning-based approaches have been successfully applied to the task, but they often rely on traditional pre-trained context-independent word embedding. Bidirectional Encoder Representations from Transformers (BERT) is recently employed to pre-train contextualized word/sentence representation models via bidirectional Transformers, outperforming the state-of-the-art for many NLP tasks. The mutual semantic influence between sentences is important for estimating semantic textual similarity, which is neglected in existing methods including BERT. On the other hand, biomedical corpora mainly consist of syntactic complex and long sentences. Owing to the above-mentioned issues, we proposed a hybrid architecture, integrating the pre-trained BERT and downstream bidirectional recurrent neural network (bi-RNN). The proposed model enhanced the sentence semantic representation via employing the self-attention instead of global attention to perform cross attention between sentences. Meanwhile, bi-RNN reduced redundant information in the output of BERT. Experimental results show that the best fine-tuned models consistently outperform previous methods and advance the state-of-the-art for clinical semantic textual similarity in OHNLP 2018 task 2, with up to 0.6% increase in Pearson correlation coefficient.",
        "DOI": "10.1109/BIBM49941.2020.9313452",
        "paper_author": "Li Z.",
        "affiliation_name": "Dalian University of Technology",
        "affiliation_city": "Dalian",
        "affiliation_country": "China",
        "affiliation_id": "60004538",
        "affiliation_state": "Liaoning"
    },
    {
        "paper_title": "Opioid2FHIR: A system for extracting FHIR-compatible opioid prescriptions from clinical text",
        "publication": "Proceedings - 2020 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2020",
        "citied_by": "4",
        "cover_date": "2020-12-16",
        "Abstract": "Background: The opioid crisis is a national public health emergency in US. Especially, prescription opioids contributed significantly to drug overdose deaths. To improve the surveillance of prescription opioid overdose, it is critical to accurately collect prescription opioid information and calculate morphine milligram equivalents (MMEs). However, plenty of detailed information is only contained in the free text Sig component of electronic health record (EHR) prescriptions and need to be extracted first. Moreover, it is also indispensable to normalize opioid information extracted from multiple heath care facilities to clinical data standards such as Fast Healthcare Interoperability Resources (FHIR) for efficient clinical decision support. However, few efforts are spent in this direction at present. Methods: In this study, we designed and implemented a system that can automatically extract opioid information from free text in Sig and map them to FHIR. The system, named as Opioid2FHIR, applied multiple natural language processing (NLP) techniques for opioid information extraction and normalization. In order to reduce manual efforts, a general-purpose medication IE model was first leveraged. Based on 1, 000 opioid prescription records randomly selected from MIMICIII, post-processing rules were designed to adapt the IE model to opioid medications. Concept normalization models were also built to transform and map the extracted medication elements to fine-granular standard concepts in FHIR. The system was evaluated on another 1, 000 opioid prescription records in MIMICIII. Results: Opioid2FHIR obtained an F-measure of 0.963 for medication information extraction and an accuracy of 0.987 for medical concept normalization. Conclusions: A clinical NLP application to EHR opioid scripts would fill a current gap in available batch script processing tools and would greatly enhance individual prescription processing limitations of prescription drug monitoring programs and clinical MME calculators.",
        "DOI": "10.1109/BIBM49941.2020.9313258",
        "paper_author": "Wang J.",
        "affiliation_name": "MELAX TECHNOLOGIES, INC.",
        "affiliation_city": "Houston",
        "affiliation_country": "United States",
        "affiliation_id": "121975726",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "OntoSem: An Ontology Semantic Representation Methodology for Biomedical Domain",
        "publication": "Proceedings - 2020 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2020",
        "citied_by": "5",
        "cover_date": "2020-12-16",
        "Abstract": "Ontologies are essential description tools for biomedical concepts and entities, supporting biomedical fundamental research such as semantic similarity analysis, protein-protein interaction prediction and so on. An increasing amount of ontology-like domain knowledge is published in scientific publications, meanwhile, advanced natural language processing (NLP) techniques have been widespread to extract information from text resources automatically, both of which facilitate the exploration of the semantic representation of biomedical ontologies. We propose a novel distributional semantic representation methodology based on the combination of two pre-trained and domain-specific word embedding tools, the non-contextualized Word2Vec and the context-dependent NCBI-blueBERT, to enhance the encoding ability for biomedical ontologies. Furthermore, we utilize a randomly initialized bidirectional LSTM to project the obtained word vector sequence to a fixed-length sentence vector, facilitating a flexible and uniform way for the computation of downstream tasks. We evaluate our method in two categories of tasks: the similarity access of ontology terms, and the ontology annotationbased protein-protein interaction classification. Experimental results demonstrate that our method provides encouraging results compared to the baselines in all tests. Our approach offers promising opportunities for representing ontologies semantics and in turn characterizing entities including proteins in biomedical research.",
        "DOI": "10.1109/BIBM49941.2020.9313128",
        "paper_author": "Zhao L.",
        "affiliation_name": "Harbin Institute of Technology",
        "affiliation_city": "Harbin",
        "affiliation_country": "China",
        "affiliation_id": "60019616",
        "affiliation_state": "Heilongjiang"
    },
    {
        "paper_title": "Clinical Phrase Mining with Language Models",
        "publication": "Proceedings - 2020 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2020",
        "citied_by": "2",
        "cover_date": "2020-12-16",
        "Abstract": "A vast amount of vital clinical data is available within unstructured texts such as discharge summaries and procedure notes in Electronic Medical Records (EMRs). Automatically transforming such unstructured data into structured units is crucial for effective data analysis in the field of clinical informatics. Recognizing phrases that reveal important medical information in a concise and thorough manner is a fundamental step in this process. Existing systems that are built for opendomain texts are designed to detect mostly non-medical phrases, while tools designed specifically for extracting concepts from clinical texts are not scalable to large corpora and often leave out essential context surrounding those detected clinical concepts. We address these issues by proposing a framework, CliniPhrase, which adapts domain-specific deep neural network based language models (such as ClinicalBERT) to effectively and efficiently extract high-quality phrases from clinical documents with a limited amount of training data. Experimental results on the MIMIC-III dataset show that our method can outperform the current state-of-the-art techniques by up to 18% in terms of F1 measure while being very efficient (up to 48 times faster).11Our source code, pre-trained models and documentations are available online at: https://github.com/kaushikmani/PhraseMiningLM",
        "DOI": "10.1109/BIBM49941.2020.9313496",
        "paper_author": "Mani K.",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Columbus",
        "affiliation_country": "United States",
        "affiliation_id": "60149838",
        "affiliation_state": "OH"
    },
    {
        "paper_title": "Generating Training Data for Concept-Mining for an 'Interface Terminology' Annotating Cardiology EHRs",
        "publication": "Proceedings - 2020 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2020",
        "citied_by": "1",
        "cover_date": "2020-12-16",
        "Abstract": "Clinical data stored in EHRs could provide valuable knowledge for research if it were annotated properly. However, almost no EHR notes are currently annotated as the performance of off the shelf annotation tools is unsatisfactory. Concentrating on the cardiology specialty, we propose to design a Cardiology Interface Terminology dedicated to the annotation of EHR notes in cardiology. This interface terminology will be developed by the addition of high granularity concepts, mined from cardiology EHR notes, to an initial version reusing SNOMED CT cardiology subhierarchies. Using text mining NLP tools with machine learning for extending this interface terminology requires proper training data. In this paper, we discuss concept-mining of EHR notes, using concatenation and anchoring operations iteratively to create such training data. This approach can be applied to other medical specialties.",
        "DOI": "10.1109/BIBM49941.2020.9313435",
        "paper_author": "Keloth V.K.",
        "affiliation_name": "New Jersey Institute of Technology",
        "affiliation_city": "Newark",
        "affiliation_country": "United States",
        "affiliation_id": "60022904",
        "affiliation_state": "NJ"
    },
    {
        "paper_title": "Evaluating SIAMESE Architecture Neural Models for Arabic Textual Similarity and Plagiarism Detection",
        "publication": "ISIA 2020 - Proceedings, 4th International Symposium on Informatics and its Applications",
        "citied_by": "3",
        "cover_date": "2020-12-15",
        "Abstract": "Semantic text similarity in NLP is the study of the degree of resemblance between texts using a certain metric. It has many applications in tasks such as question answering, information retrieval, document clustering, topic detection, topic tracking, questions generation, machine translation, text summarizing and others. Nowadays, neural models are outperforming existing state of the art approaches in major NLP tasks and it is not surprising to see the STS community researchers adopt these models although there are still few works for Arabic language. As Siamese neural architecture has proven recently its relevance for STS in other languages, we evaluate in this work three models within this architecture for Arabic Textual similarity and plagiarism detection: BiLSTM and CNN which we call basic models and a BERT Transformer model.",
        "DOI": "10.1109/ISIA51297.2020.9416550",
        "paper_author": "Aliane A.A.",
        "affiliation_name": "Centre de Recherche sur l'Information Scientifique et Technique",
        "affiliation_city": "Ben Aknoun",
        "affiliation_country": "Algeria",
        "affiliation_id": "60070446",
        "affiliation_state": "Algiers"
    },
    {
        "paper_title": "Annotating Arabic Texts with Linked Data",
        "publication": "ISIA 2020 - Proceedings, 4th International Symposium on Informatics and its Applications",
        "citied_by": "2",
        "cover_date": "2020-12-15",
        "Abstract": "The evolution of the traditional Web towards the semantic Web allows the machine to be a first-order citizen on the Web and increases discoverability of and accessibility to the unstructured data on the Web. This evolution enables the Linked Data technology to be used as background knowledge bases for unstructured data, notably the texts, available nowadays on the Web. For the Arabic language, the current situation is less brightness; the content of the Arabic language on the Web doesn't reflect the importance of this language. Given the fact that Arabic is one of the most important languages in the Web, and unfortunately it is under-resourced, so creating linguistic resources for it now is a necessity. Thus, we developed a linguistic approach for annotating Arabic textual corpus with Linked Data, especially DBpedia, which is Linked Open Data (LOD) extracted from Wikipedia. This approach uses natural language techniques to shedding light on Arabic text with Linked Open Data. The evaluation results of this approach are encouraging, despite the high complexity of our independent-domain knowledge base and the reduced resources in Arabic natural language processing.",
        "DOI": "10.1109/ISIA51297.2020.9416543",
        "paper_author": "Bouziane A.",
        "affiliation_name": "Centre Universitaire Salhi Ahmed -Naama",
        "affiliation_city": "Naama",
        "affiliation_country": "Algeria",
        "affiliation_id": "60105423",
        "affiliation_state": "Naâma"
    },
    {
        "paper_title": "BERT-GT: Cross-sentence n-ary relation extraction with BERT and Graph Transformer",
        "publication": "Bioinformatics",
        "citied_by": "37",
        "cover_date": "2020-12-15",
        "Abstract": "Motivation: A biomedical relation statement is commonly expressed in multiple sentences and consists of many concepts, including gene, disease, chemical and mutation. To automatically extract information from biomedical literature, existing biomedical text-mining approaches typically formulate the problem as a cross-sentence n-ary relation-extraction task that detects relations among n entities across multiple sentences, and use either a graph neural network (GNN) with long short-term memory (LSTM) or an attention mechanism. Recently, Transformer has been shown to outperform LSTM on many natural language processing (NLP) tasks. Results: In this work, we propose a novel architecture that combines Bidirectional Encoder Representations from Transformers with Graph Transformer (BERT-GT), through integrating a neighbor-attention mechanism into the BERT architecture. Unlike the original Transformer architecture, which utilizes the whole sentence(s) to calculate the attention of the current token, the neighbor-attention mechanism in our method calculates its attention utilizing only its neighbor tokens. Thus, each token can pay attention to its neighbor information with little noise. We show that this is critically important when the text is very long, as in cross-sentence or abstract-level relation-extraction tasks. Our benchmarking results show improvements of 5.44% and 3.89% in accuracy and F1-measure over the state-of-the-art on n-ary and chemical-protein relation datasets, suggesting BERT-GT is a robust approach that is applicable to other biomedical relation extraction tasks or datasets.",
        "DOI": "10.1093/bioinformatics/btaa1087",
        "paper_author": "Lai P.T.",
        "affiliation_name": "National Center for Biotechnology Information (NCBI)",
        "affiliation_city": "Bethesda",
        "affiliation_country": "United States",
        "affiliation_id": "60087823",
        "affiliation_state": "MD"
    },
    {
        "paper_title": "Construction of visualization domain-specific knowledge graph of crop diseases and pests based on deep learning",
        "publication": "Nongye Gongcheng Xuebao/Transactions of the Chinese Society of Agricultural Engineering",
        "citied_by": "39",
        "cover_date": "2020-12-15",
        "Abstract": "The knowledge graph describes the concepts, entities, and their relationships in the objective world in a structured form. It has a better ability to organize, manage, and understand massive amounts of information, and can structure heterogeneous knowledge in the field. It can be widely used in medical, biological, financial, etc. In view of the current situation in the field of crop diseases and insect pests, there are multiple relationship pairs between the same entity and multiple entities, multi-source heterogeneous data, poor aggregation ability, low utilization, and the possibility of knowledge sharing. Combining Natural Language Processing (NLP) and text mining technologies, this study focused on data acquisition, ontology construction, knowledge extraction, and knowledge storage, researched on the construction of crops diseases and insect pests knowledge graph based on deep learning. Firstly, this study used the Scrapy crawler framework of the Python programming language to crawl data from web pages related to crop diseases and insect pests, and performed data cleaning and data supplementation through data preprocessing methods. Secondly, according to the characteristics of the domain corpus, the Protégé ontology construction tool was used to complete the semi-automatic construction of the crop diseases and insect pests ontology predefined the set of properties and relations and set the corresponding domains and ranges. Then, based on the ontology, the rule method was used to extract semi-structured knowledge, and the deep learning method was used to extract unstructured knowledge. In the process of unstructured knowledge extraction, a text annotation mode \"Main_Entity+Relation+BIESO\" (ME+R+BIESO) adapted to the domain corpus was also proposed. Based on a predefined set of relationships, entities and relationships were simultaneously annotated, it contained entity and relationship information at the same time, and directly modeling the triples instead of separately modeling entities and relationships. The corresponding triples were also directly obtained through analysis, which not only saved at least half of the cost of labeling but also realized the joint extraction of entity relations and solved the problem of overlapping relation extraction. And this study used the Bidirectional Encoder Representation from Transformers (BERT)- Bi-directional Long-Short Term Memory (BiLSTM)+ Conditional Random Field (CRF) end-to-end model to experiment on the crop diseases and insect pests dataset. First, this study used the BERT pre-training language model to encode words, extracted text features, and used the generated vector as the input of the BiLSTM layer; BiLSTM integrated contextual information into the model at the same time, and performed bidirectional encoding to achieve effective prediction of label sequences; finally, this study used the CRF module to decode the output result of BiLSTM, and the label transition probability and constraint conditions were obtained through training and learning, and the entity label category of each character was obtained. The experimental results showed that the precision was 94.06%, the recall was 89.02%, and the F1 value reached 91.34%, which was much better than the pipeline method and classic models such as BiLSTM+CRF and Convolutional Neural Networks (CNN)+BiLSTM+CRF in the joint extraction method. The joint extraction of entity relations based on this annotation mode not only improved the efficiency and accuracy of annotation but also solved the problem of overlapping relations in the corpus. Finally, the extracted knowledge was stored in the graph database to realize the visual display of the knowledge graph and deep knowledge mining and reasoning. Combined the deep learning technology to realize the semi-automatic construction of the knowledge graph, which was of great significance for the detection of crop diseases and insect pests, forecasting and early warning, and the establishment of prevention models in the intelligent production system. It could provide a high-quality knowledge base for crop diseases and insect pests question answering systems, recommendation systems, search engines, and other applications, which could be effectively applied to crop variety selection, pest prevention and control, and fertilization and irrigation.",
        "DOI": "10.11975/j.issn.1002-6819.2020.24.021",
        "paper_author": "Wu S.",
        "affiliation_name": "Chinese Academy of Agricultural Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60017705",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Training Doc2Vec on a Corpus of Persian Poems to Answer Thematic Similarity Multiple-Choice Questions",
        "publication": "2020 10th International Symposium on Telecommunications: Smart Communications for a Better Life, IST 2020",
        "citied_by": "1",
        "cover_date": "2020-12-15",
        "Abstract": "This paper reports our improvement over the previous benchmark of the task of answering poetic verses' thematic similarity multiple-choice questions (MCQs). These questions, which frequently appear in the Iranian national university entrance exam, require the test-taker to select the verse which is thematically similar to the stem verse. These questions would test a natural language processing (NLP) model's ability to not only understand, but interpret a poetic verse. In this experiment, we trained a Doc2Vec model on a corpus of Persian poems and proceeded to use the trained model to get the vector representations of the poetic verses. Subsequently, the poetic verse among the options with the highest cosine similarity to the stem verse was selected as the correct answer by the model. This model managed to answer %38 of the questions correctly, which was an improvement of %6 over the previous benchmark. Furthermore, it was observed that the model's approach to answering the questions seemed different from the previous model, as there were relatively few questions that the models had both answered correctly. The ultimate goal of this task is to estimate the difficulty of such questions by using the performance of the models as a feature. The fact that the two models did not behave similarly can prove useful, as the model whose performance has the most correlation with that of a human test-taker can be used for the future task of thematic similarity MCQ difficulty estimation.",
        "DOI": "10.1109/IST50524.2020.9345918",
        "paper_author": "Akef S.",
        "affiliation_name": "Sharif University of Technology",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran",
        "affiliation_id": "60027666",
        "affiliation_state": "Tehran"
    },
    {
        "paper_title": "Deep Learning Approach in Sentiment Analysis: A Review",
        "publication": "Proceedings of ICCES 2020 - 2020 15th International Conference on Computer Engineering and Systems",
        "citied_by": "6",
        "cover_date": "2020-12-15",
        "Abstract": "Sentiment Analysis (SA) is the field that combines Natural Language Processing (NLP), Computational Linguistics (CL) and text analysis to study people's opinions through, by extracting and analyzing subjective information from different resources as the Web, social media and similar sources and so help in drawing public's sentiments or attitude toward certain people, products or ideas and extracting the contextual polarity of the information. This review focuses on recent work in SA using Deep Learning (DL)techniques in the sentiment classification process, it is based on the articles published through ScienceDirect and Springer databases in the interval from 2016 to 2020.It sheds the light on different DL algorithms used, different applications of SA systems. 58 articles studied in ScienceDirect While 26 articles in Springer satisfying the same criteria with the total of 84 articles studied and analyzed in this review. The review concerns with DL techniques, language, domain, and performance results.",
        "DOI": "10.1109/ICCES51560.2020.9334625",
        "paper_author": "Khalil E.A.H.",
        "affiliation_name": "National Research Centre",
        "affiliation_city": "Giza",
        "affiliation_country": "Egypt",
        "affiliation_id": "60014618",
        "affiliation_state": "Giza"
    },
    {
        "paper_title": "An Hybrid Functions Based State-Control Parametrization Method to the Optimal Control of State and Input Delayed Bilinear Systems",
        "publication": "Proceedings of the International Conference on Advanced Systems and Emergent Technologies, IC_ASET 2020",
        "citied_by": "0",
        "cover_date": "2020-12-15",
        "Abstract": "The paper deals with quadratic optimal control of bilinear time-delay systems. A direct approach is proposed based on Hybrid functions of block pulse and Legendre polynomials. Kronecker product operational matrix of the hybrid base is investigated as well as appropriate shifting matrix for delay modeling is extended from existing block pulse functions results. The complex analytical problem is then reduced to a nonlinear programming problem that could be solved with available NLP solvers. Simulations with comparison to literature results show efficiency and relative computational rapidity of the approach.",
        "DOI": "10.1109/IC_ASET49463.2020.9318291",
        "paper_author": "Bouafoura M.K.",
        "affiliation_name": "University of Carthage, Ecole Polytechnique de Tunisie",
        "affiliation_city": "La Marsa",
        "affiliation_country": "Tunisia",
        "affiliation_id": "60034954",
        "affiliation_state": "Tunis"
    },
    {
        "paper_title": "Optimal 3D time-energy trajectory planning for AUVs using ocean general circulation models",
        "publication": "Ocean Engineering",
        "citied_by": "10",
        "cover_date": "2020-12-15",
        "Abstract": "This paper develops a new approach for solving optimal time and energy trajectory planning problems for Autonomous Underwater Vehicles (AUVs) in transient, 3D, ocean currents. Realistic forecasts using an Ocean General Circulation Model (OGCM) are used for this purpose. The approach is based on decomposing the problem into a minimal time problem, followed by minimal energy subproblems. In both cases, a non-linear programming (NLP) formulation is adopted. The scheme is demonstrated for time-energy trajectory planning problems in the Gulf of Aden. In particular, the numerical experiments illustrate the capability of generating Pareto optimal solutions in a broad range of mission durations. In addition, the analysis also highlights how the methodology effectively exploits both the vertical structure of the current field, as well as its unsteadiness, namely to minimize travel time and energy consumption.",
        "DOI": "10.1016/j.oceaneng.2020.108057",
        "paper_author": "Albarakati S.",
        "affiliation_name": "King Abdullah University of Science and Technology",
        "affiliation_city": "Thuwal",
        "affiliation_country": "Saudi Arabia",
        "affiliation_id": "60092945",
        "affiliation_state": "Makkah"
    },
    {
        "paper_title": "Graph classification algorithm based on graph structure embedding",
        "publication": "Expert Systems with Applications",
        "citied_by": "12",
        "cover_date": "2020-12-15",
        "Abstract": "With the application of data mining in many fields such as information science, bioinformatics, and network intrusion detection, more and more data are showing new features such as strong structuration and complex relationships between data. As a complex data structure, a graph can be used to describe the relationship between things. Traditional graph classification methods based on graph feature vector construction need to select a feature vector construction criterion in advance, such as graph-based theoretical indicators or graph-based topology occurrences, and then extract features from each graph in the graph set according to the designated criterion. However, the construction method of the graph feature vector is easy to lose the graph structural information and requires strong professional knowledge. Inspired by the Word2Vec and Doc2Vec models in the Natural Language Processing (NLP), this paper first constructs a “word list” of graph data consisting of subgraphs. Then a neural network for training graph embedding is designed with the graph itself as its input, and the “word” in the graph and the attribute features of the graph are used as its output, so that the neural network automatically learns the graph embedding corresponding to each graph. The graph embedding not only reflects the features of the graph itself but also includes the relative relationship among graphs. Finally, on the basis of the well-trained graph embedding, the common classifier can be used to classify graphs. Based on real-world bioinformatics and social data sets, the experiments demonstrate that the proposed graph classification algorithm has advantages over the existing graph classification algorithms based on feature vector construction.",
        "DOI": "10.1016/j.eswa.2020.113715",
        "paper_author": "Ma T.",
        "affiliation_name": "Nanjing University of Information Science &amp; Technology",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60064143",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "Machine Translation Evaluation using Textual Entailment for Arabic",
        "publication": "2020 7th International Conference on Social Network Analysis, Management and Security, SNAMS 2020",
        "citied_by": "1",
        "cover_date": "2020-12-14",
        "Abstract": "Textual entailment is a generic task aiming to capture semantic inference between two text fragments. It has been applied so far to improve many natural language applications such as Question Answering and Information Extraction for the English language. This work aims to exploit results obtained for Textual Entailment in machine translation evaluation to Arabic. We show that the output of a system built for textual entailment recognition for Arabic can be used to serve as a metric to evaluate machine translation into Arabic. Despite the simplicity of the concepts and techniques., this metric has realized competitive results in comparison to state-of-the-art tools in terms of correlation with human judgments.",
        "DOI": "10.1109/SNAMS52053.2020.9336580",
        "paper_author": "Marouani M.E.",
        "affiliation_name": "Faculty of Science, Ibn Tofail University",
        "affiliation_city": "Kenitra",
        "affiliation_country": "Morocco",
        "affiliation_id": "60030441",
        "affiliation_state": "Rabat-Sale-Kenitra"
    },
    {
        "paper_title": "An empirical study on patent novelty detection: A novel approach using machine learning and natural language processing",
        "publication": "2020 7th International Conference on Social Network Analysis, Management and Security, SNAMS 2020",
        "citied_by": "10",
        "cover_date": "2020-12-14",
        "Abstract": "Patent, a form of intellectual property often be in the first place when it comes to securing an invention. The legal boundaries created then will become key stages of turning an invention into a commercial product. In recent years, the unprecedented growth of patent applications has induced a great challenge to patent examiners. Novelty detection is one major step considered before and after filing a patent application to assure claimed inventions are new and non-obvious. This itself is considered as a salient stage of prior art search by patent applicants, patent examiners, patent attorneys, patent agent professionals. Management in terms of critical analysis of such a large scale of documents has become a challenge since missing an optimal, effective, and efficient system. To this end, we come up with a novel experimental case study to foster highly recursive and interactive tasks. We developed and investigated more than 50 machine learning models on the considered dataset. The contributions of this work include: (1) outlined and anticipated the importance of novelty detection in the patent domain, (2) develop various baseline models for novelty detection, (3) utilize immense contributions of deep learning towards NLP to improve baseline models, (4) assess the performance of every model by using different word embeddings like word2vec, glove, fasttext, and domain-specific embeddings, (5) a novel application of NBSVM algorithm on our dataset, and considered as exceptionally good of our models. We articulated the fulfillment of models using training and validation curves to prove seemingly negligible overfit or no overfit, in the hope that effective automation in novelty detection helps in driving down the routine prior art search efforts.",
        "DOI": "10.1109/SNAMS52053.2020.9336557",
        "paper_author": "Chikkamath R.",
        "affiliation_name": "Universität Passau",
        "affiliation_city": "Passau",
        "affiliation_country": "Germany",
        "affiliation_id": "60014151",
        "affiliation_state": "Bayern"
    },
    {
        "paper_title": "Zero-Order Optimization-Based Iterative Learning Control",
        "publication": "Proceedings of the IEEE Conference on Decision and Control",
        "citied_by": "6",
        "cover_date": "2020-12-14",
        "Abstract": "We consider an optimization-based iterative learning control (ILC) approach for nonlinear systems where the control input is obtained as the solution of a constrained nonlinear program (NLP). The NLP formulation is based on a - possibly nonlinear - nominal model corrected by the output error which has been observed in the previous trial. Assuming that the sensitivities of the nominal model are sufficiently close to the sensitivities of the real system, we show local convergence of the proposed ILC method to a generally suboptimal solution and derive a bound on the loss of optimality. Even though the algorithm does not require any exact sensitivity information of the true process, it can recover the optimal control input in two special cases: Assuming that the sensitivities are sufficiently similar, the optimal solution is obtained (1) if it lies at a vertex of the feasible set, i.e. it is fully determined by the constraints, or (2) if a reference tracking problem is considered and optimal tracking is feasible.",
        "DOI": "10.1109/CDC42340.2020.9303854",
        "paper_author": "Baumgartner K.",
        "affiliation_name": "Universität Freiburg",
        "affiliation_city": "Freiburg im Breisgau",
        "affiliation_country": "Germany",
        "affiliation_id": "60025641",
        "affiliation_state": "Baden-Wurttemberg"
    },
    {
        "paper_title": "Checklist as an effective means of information delivery in on-demand learning",
        "publication": "IEEE International Conference on Industrial Engineering and Engineering Management",
        "citied_by": "1",
        "cover_date": "2020-12-14",
        "Abstract": "On-demand classes have become popular in the last decade due to their convenience and asynchronous nature of teaching. However, due to the lack of precise communication and effective information delivery, students need to put a significant effort to follow and comprehend on-demand lectures. In this paper, we investigate the practicability of using checklists to improve information delivery in on-demand learning. A case study is conducted on six on-demand lectures covering three lecture topics. We assess the report writings of over 141 student participants, divided into experimental and controlled groups. Natural Language Processing (NLP) and part-of-speech segmentation are used to analyse survey responses. The findings indicate a statistically significant difference in performance between instructions delivered through a checklist and simple text. At an average, 18% of the students avoid making mistakes when a checklist is used as the mode of instruction delivery. Our pilot study proves checklists' potential as a feasible and effective tool for information delivery in on-demand education.",
        "DOI": "10.1109/IEEM45057.2020.9309804",
        "paper_author": "Konjengbam A.",
        "affiliation_name": "Shizuoka University",
        "affiliation_city": "Shizuoka",
        "affiliation_country": "Japan",
        "affiliation_id": "60103680",
        "affiliation_state": "Shizuoka"
    },
    {
        "paper_title": "An Integral Penalty-Barrier Direct Transcription Method for Optimal Control",
        "publication": "Proceedings of the IEEE Conference on Decision and Control",
        "citied_by": "4",
        "cover_date": "2020-12-14",
        "Abstract": "Some direct transcription methods can fail to converge, e.g. when there are singular arcs. We recently introduced a convergent direct transcription method for optimal control problems, called the penalty-barrier finite element method (PBF). PBF converges under very weak assumptions on the problem instance. PBF avoids the ringing between collocation points, for example, by avoiding collocation entirely. Instead, equality path constraint residuals are forced to zero everywhere by an integral quadratic penalty term. We highlight conceptual differences between collocation- and penalty-type direct transcription methods. Theoretical convergence results for both types of methods are reviewed and compared. Formulas for implementing PBF are presented, with details on the formulation as a nonlinear program (NLP), sparsity and solution. Numerical experiments compare PBF against several collocation methods with regard to robustness, accuracy, sparsity and computational cost. We show that the computational cost, sparsity and construction of the NLP functions are roughly the same as for orthogonal collocation methods of the same degree and mesh. As an advantage, PBF converges in cases where collocation methods fail. PBF also allows one to trade off computational cost, optimality and violation of differential and other equality equations against each other.",
        "DOI": "10.1109/CDC42340.2020.9304216",
        "paper_author": "Neuenhofen M.P.",
        "affiliation_name": "Imperial College London",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60015150",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Commonsense Knowledge Adversarial Dataset that Challenges ELECTRA",
        "publication": "16th IEEE International Conference on Control, Automation, Robotics and Vision, ICARCV 2020",
        "citied_by": "4",
        "cover_date": "2020-12-13",
        "Abstract": "Commonsense knowledge is critical in human reading comprehension. While machine comprehension has made significant progress in recent years, the ability in handling commonsense knowledge remains limited. Synonyms are one of the most widely used commonsense knowledge. Constructing adversarial dataset is an important approach to find weak points of machine comprehension models and support the design of solutions. To investigate machine comprehension models' ability in handling the commonsense knowledge, we created a Question and Answer Dataset with common knowledge of Synonyms (QADS). QADS are questions generated based on SQuAD 2.0 by applying commonsense knowledge of synonyms. The synonyms are extracted from WordNet. Words often have multiple meanings and synonyms. We used an enhanced lesk algorithm to perform word sense disambiguation to identify synonyms for the context. ELECTRA achieves the state-of-art result on the SQuAD 2.0 dataset in 2019. With about 1/10 scale, ELECTRA can achieve similar performance as BERT does. However, QADS shows that ELECTRA has little ability to handle commonsense knowledge of synonyms. In our experiment, ELECTRA-small can achieve 70% accuracy on SQuAD 2.0, but only 20% on QADS. ELECTRA-large did not perform much better. Its accuracy on SQuAD 2.0 is 88% but dropped significantly to 26% on QADS. In our earlier experiments, BERT, although also failed badly on QADS, was not as bad as ELECTRA. The result shows that even top-performing NLP models have little ability to handle commonsense knowledge which is essential in reading comprehension.",
        "DOI": "10.1109/ICARCV50220.2020.9305451",
        "paper_author": "Lin G.",
        "affiliation_name": "Nanyang Technological University",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore",
        "affiliation_id": "60005510",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "MiningBreastCancer: Selection of Candidate Gene Associated with Breast Cancer via Comparison between Data Mining of TCGA and Text Mining of PubMed",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2020-12-12",
        "Abstract": "In 2016, 12,676 new cases of breast cancer were diagnosed among Taiwan women. In 2018 the standardized death rate of breast cancer was 12.5 per 100,000 persons. Previous studies have integrated data and text mining to yield fusion genes, identify genetic factors for breast cancer and select single-gene feature sets for colon cancer discrimination. However, our study is the first to select significantly different expression between breast normal tissue and cancer using TCGA data and biostatistics, excluding know genes using abstracts from PubMed and natural language processing. The top twenty genes for research potential from the selection of Mining-BreastCancer are EML3, ABCB9, GRASP, KANK3, GPR146, ZNF623, CCDC9, ADCY4, DLL1, ADAM33, GRRP1, LRRN4CL, C14orf180, ABCD4, ABCC6P1, PEAR1, FAM43A, C20orf160, KIF21A and PP-FIA3. Few studies for these genes exist, but they hold significantly different expressions between breast cancer and normal tissue, each pathologic tumor and lymph node, or between each pathologic metastasis. These results show that MiningBreastCancer can help scientists select genes for research potential. MiningBreastCancer is available through http://bio.yungyun.com.tw/MiningBreastCancer.aspx.",
        "DOI": "10.1145/3440943.3444718",
        "paper_author": "Chen C.C.",
        "affiliation_name": "Chia-Nan University of Pharmacy and Science Taiwan",
        "affiliation_city": "Tainan",
        "affiliation_country": "Taiwan",
        "affiliation_id": "60011234",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Implementing AI Systems: Transform Your Business in 6 Steps",
        "publication": "Implementing AI Systems: Transform Your Business in 6 Steps",
        "citied_by": "1",
        "cover_date": "2020-12-11",
        "Abstract": "AI is one of the fastest growing corners of the tech world. But there remains one big problem: many AI projects fail. The fact is that AI is unique among IT projects. The technology requires a different mindset, in terms of understanding probabilities, data structures and complex algorithms. There is also a need to deal with complex issues like ethics and privacy. This is where Implementing AI Systems comes in. You'll learn the step-by-step process for successful implementations of AI, backed up with numerous case studies from top companies. This book puts everything you need to know into one place - that is, it's the handbook you need for AI. You'll focus primarily on understanding the core concepts for AI like NLP, Machine Learning, Deep Learning and so on. This book will help you find the right areas to apply AI. What You'll Learn • Put together an effective data strategy • Create models and how to successfully test them • Evaluate AI tools • Assemble the right team • Scale AI across an organization Who This Book Is For Primarily for managers, IT professionals and executives of mid-size and large companies wanting to implement AI in their organization.",
        "DOI": "10.1007/978-1-4842-6385-3",
        "paper_author": "Taulli T.",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "SOS Intelligent Emergency Rescue System:Tap Once to Trigger Voice Input",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2020-12-11",
        "Abstract": "Traffic accidents worldwide have caused ever-increasing death numbers. By observing the significant number of casualties caused by traffic accidents, we can see worldwide road safety standards dropping significantly. In many cases, acquaintances or else urgent situation services are not notified within time. This can result in emergency services being delayed, which can, in turn, lead to serious harmor even death. To address the issue, in this paper we developa SOS Rescue mobile applicationbased on artificial intelligence and voice-recognition as well as Google STT (speech-to-text). The. STT system is based on methods and technologies that enable computers and computerized equipment to recognize as well as translate spoken language into text. Via Google cloud services, users of our system will find the application vastly efficient in recognizing the voice of the wounded, and it can automatically convert voice into text-form. Also, it can detect the real-time position of an accident, and automatically send a message to the responder. It will collect relevant information in text form via voice recognition in addition to a link of the audio file and real-time positioning. Just taping once to trigger our SOS Rescue System, it starts working automatically.",
        "DOI": "10.1145/3445815.3445846",
        "paper_author": "Javed M.",
        "affiliation_name": "Guangxi Normal University",
        "affiliation_city": "Guilin",
        "affiliation_country": "China",
        "affiliation_id": "60020464",
        "affiliation_state": "Guangxi"
    },
    {
        "paper_title": "Fine-Grained Chinese Named Entity Recognition with RoBERTa and Convolutional Attention Networks",
        "publication": "2020 IEEE 6th International Conference on Computer and Communications, ICCC 2020",
        "citied_by": "4",
        "cover_date": "2020-12-11",
        "Abstract": "In the field of Natural Language Processing (NLP), traditional Chinese Named Entity Recognition (NER) tasks often only involve the recognition of a few types of entities. But current real-world applications require more fine-grained types of entities for more detailed downstream NLP tasks. Since fine-grained Chinese NER is challenging for existing models, novel models need to be introduced. We propose a model with RoBERTa as word embedding, a convolutional attention layer and CRF layer for outputting the entity labels. This model has a better overall performance than baseline models on the CLUENER2020 fine-grained Chinese NER dataset.",
        "DOI": "10.1109/ICCC51575.2020.9345307",
        "paper_author": "Cui Y.",
        "affiliation_name": "Beijing University of Technology",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60022281",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Toxic Comment Detection using LSTM",
        "publication": "Proceedings of 2020 3rd International Conference on Advances in Electronics, Computers and Communications, ICAECC 2020",
        "citied_by": "25",
        "cover_date": "2020-12-11",
        "Abstract": "While online communication media acts as a platform for people to connect, collaborate and discuss, overcoming the barriers for communication, some take it as a medium to direct hateful and abusive comments that may prejudice an individual's emotional and mental well being. Explosion of online communication makes it virtually impossible for filtering out the hateful tweets manually, and hence there is a need for a method to filter out the hate-speech and make social media cleaner and safer to use. The paper aims to achieve the same by text mining and making use of deep learning models constructed using LSTM neural networks that can near accurately identify and classify hate-speech and filter it out for us. The model that we have developed is able to classify given comments as toxic or nontoxic with 94.49% precision, 92.79% recall and 94.94% Accuracy score.",
        "DOI": "10.1109/ICAECC50550.2020.9339521",
        "paper_author": "Dubey K.",
        "affiliation_name": "Thadomal Shahani Engineering College",
        "affiliation_city": "Mumbai",
        "affiliation_country": "India",
        "affiliation_id": "60100085",
        "affiliation_state": "MH"
    },
    {
        "paper_title": "A Comparative Study of Neural Network for Text Classification",
        "publication": "2020 IEEE Conference on Telecommunications, Optics and Computer Science, TOCS 2020",
        "citied_by": "1",
        "cover_date": "2020-12-11",
        "Abstract": "This With the popularity of artificial intelligence in recent years, Natural Language Processing (NLP) technology has also become the focus of research. NLP technology's unique machine translation and text sentiment analysis functions can prevent people from experiencing poor language communication when travelling abroad and help artificial intelligence understand people's language better. This article has made corresponding practice and analysis for the critical requirement of 'text classification' in NLP. In the experiment, we used the Internet Movie Database (IMDB) film review forum as the dataset. Recurrent Neural Network (RNN) and the corresponding variants of RNN (Long Short Term Memory (LSTM)) are analyzed and compared from the theoretical aspect. Moreover, we introduced a bidirectional mechanism to optimize RNN and reduce the influence of parameter changes on model training by comparing specific neural network structures. We found the benefits of LSTM in text classification applications compared with RNN and simple neural networks by comparing experiments. Besides, we explored the role of the bidirectional mechanism for RNN. Finally, we create a two-way LSTM model for text classification model and obtain the model training results indicating less overfitting and less loss than other structures.",
        "DOI": "10.1109/TOCS50858.2020.9339702",
        "paper_author": "Peng X.",
        "affiliation_name": "Jilin University",
        "affiliation_city": "Changchun",
        "affiliation_country": "China",
        "affiliation_id": "60007711",
        "affiliation_state": "Jilin"
    },
    {
        "paper_title": "Early detection of vulnerabilities from news websites using machine learning models",
        "publication": "Proceedings - RoEduNet IEEE International Conference",
        "citied_by": "14",
        "cover_date": "2020-12-11",
        "Abstract": "The drawbacks of traditional methods of cybernetic vulnerability detection relate to the required time to identify new threats, to register them in the Common Vulnerabilities and Exposures (CVE) records, and to score them with the Common Vulnerabilities Scoring System (CVSS). These problems can be mitigated by early vulnerability detection systems relying on social media and open-source data. This paper presents a model that aims to identify emerging cybernetic vulnerabilities in cybersecurity news articles, as part of a system for automatic detection of early cybernetic threats using Open Source Intelligence (OSINT). Three machine learning models were trained on a novel dataset of 1000 labeled news articles to create a strong baseline for classifying cybersecurity articles as relevant (i.e., introducing new security threats), or irrelevant: Support Vector Machines, a Multinomial Naïve Bayes classifier, and a finetuned BERT model. The BERT model obtained the best performance with a mean accuracy of 88.45% on the test dataset. Our experiments support the conclusion that Natural Language Processing (NLP) models are an appropriate choice for early vulnerability detection systems in order to extract relevant information from cybersecurity news articles.",
        "DOI": "10.1109/RoEduNet51892.2020.9324852",
        "paper_author": "Iorga D.",
        "affiliation_name": "Universitatea din Bucuresti",
        "affiliation_city": "Bucharest",
        "affiliation_country": "Romania",
        "affiliation_id": "60001478",
        "affiliation_state": "Bucharest"
    },
    {
        "paper_title": "Multi aad Self Attention Hand Pose Estimation",
        "publication": "E3S Web of Conferences",
        "citied_by": "0",
        "cover_date": "2020-12-11",
        "Abstract": "In This paper, we propose a hand pose estimation neural networks architecture named MSAHP which can improve PCK (percentage correct keypoints) greatly by fusing self-attention module in CNN (Convolutional Neural Networks). The proposed network is based on a ResNet (Residual Neural Network) backbone and concatenate discriminative features through multiple different scale feature maps, t an multiple aad self-attention module was used to focus on the salient feature map area. In recent years, self-attention mechanism was applicated widely in NLP and speech recognition, which can improve greatly key metrics. But in compute vision especially for hand pose estimation, we did not find the application. Experiments on hand pose estimation dataset demonstrate the improved PCK of our MSAHP than the existing state-of-t a-art hand pose estimation methods. Specifically, the proposed method can achieve 93.68% PCK score on our mixed test dataset.",
        "DOI": "10.1051/e3sconf/202021803023",
        "paper_author": "Zhang Z.",
        "affiliation_name": "Wuhan Donghu University",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60108803",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "LSOMP: Large Scale Ordinance Mining Portal",
        "publication": "Proceedings - 2020 IEEE International Conference on Big Data, Big Data 2020",
        "citied_by": "1",
        "cover_date": "2020-12-10",
        "Abstract": "We propose a novel scalable Web portal called LSOMP (Large Scale Ordinance Mining Portal) to analyze ordinances and their tweets (of the order of thousands and millions). It entails commonsense knowledge (CSK) and natural language processing (NLP), disseminating ordinance-tweet mining results via interactive graphics and Question Answering (QA).",
        "DOI": "10.1109/BigData50022.2020.9378354",
        "paper_author": "Du X.",
        "affiliation_name": "Montclair State University",
        "affiliation_city": "Montclair",
        "affiliation_country": "United States",
        "affiliation_id": "60012621",
        "affiliation_state": "NJ"
    },
    {
        "paper_title": "Predicting Escalations in Customer Support: Analysis of Data Mining Challenge Results",
        "publication": "Proceedings - 2020 IEEE International Conference on Big Data, Big Data 2020",
        "citied_by": "11",
        "cover_date": "2020-12-10",
        "Abstract": "We summarize IEEE Big Data Cup: Predicting Escalations in Customer Support - a data mining competition organized jointly by companies Information Builders and QED Software at the KnowledgePit platform, in the frame of the 2020 IEEE International Conference on Big Data. We discuss the motivation for organizing this event and highlight the factors that make it such a challenging topic. We describe the data provided to participants and formulate the competition task. We also provide an overview of competition results with a detailed analysis of a few selected solutions. Finally, we present a novel functionality of the KnowledgePit platform - an analytic module that allows organizers to investigate selected solutions using a convenient GUI and provides in-depth insights about their quality.",
        "DOI": "10.1109/BigData50022.2020.9378024",
        "paper_author": "Janusz A.",
        "affiliation_name": "University of Warsaw, Institute of Informatics",
        "affiliation_city": "Warsaw",
        "affiliation_country": "Poland",
        "affiliation_id": "60112534",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Named Entity Recognition on Morphologically Rich Language: Exploring the Performance of BERT with varying Training Levels",
        "publication": "Proceedings - 2020 IEEE International Conference on Big Data, Big Data 2020",
        "citied_by": "2",
        "cover_date": "2020-12-10",
        "Abstract": "Named Entity Recognition (NER) is an information extraction task that aims to automatically identify named entities in a given text. Named entities are special types of nouns or noun groups that refer to specific entities including as person, location, organization, date, time, money and percentage. NER also facilitates various other Natural Language Processing (NLP) related tasks such as summarization and question answering. It is a vastly studied problem especially on English texts, however number of NER studies on Turkish is very limited. Being a morphologically rich language, Turkish has an agglutinative structure and hence automated analysis and information extraction performance is generally lower than those on English. The previous studies mostly use conventional supervised learning and sequence tagging methods, such as Conditional Random Fields (CRF). Only few studies use deep neural models for NER problem on Turkish texts. In this work, we particularly focus on the recent neural model, Google BERT, and analyze its performance on Turkish texts. In addition to fully trained BERT model, we investigate the performance of different training levels from fully trained to fully pre-trained.",
        "DOI": "10.1109/BigData50022.2020.9377869",
        "paper_author": "Kilic Y.P.",
        "affiliation_name": "Middle East Technical University (METU)",
        "affiliation_city": "Ankara",
        "affiliation_country": "Turkey",
        "affiliation_id": "60004305",
        "affiliation_state": "Ankara"
    },
    {
        "paper_title": "Word embedding for job market spatial representation: Tracking changes and predicting skills demand",
        "publication": "Proceedings - 2020 IEEE International Conference on Big Data, Big Data 2020",
        "citied_by": "3",
        "cover_date": "2020-12-10",
        "Abstract": "What will the job market of the future look like? What jobs will be popular, and which skills will they require? Modeling the temporal progression of the job market, as represented by job ads, may help us answer this question. This paper represents a first step in this direction. In order to build a spatial representation of job market that allows to track changes in skills' demand, authors are training models to classify job tasks. Different natural language processing and classification approaches were compared, including term frequency - inverse document frequency, principal components analysis, word2vec, GloVe, fastText and BERT models, and feedforward neural networks, support vector machines, and bidirectional long short term memory recurrent neural networks. BERT obtained the best accuracy results with 52% for 94 classes and 65% for 22 classes.",
        "DOI": "10.1109/BigData50022.2020.9377850",
        "paper_author": "Jaramillo C.M.",
        "affiliation_name": "NYU Tandon School of Engineering",
        "affiliation_city": "New York",
        "affiliation_country": "United States",
        "affiliation_id": "60108318",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "On the Large-scale Graph Data Processing for User Interface Testing in Big Data Science Projects",
        "publication": "Proceedings - 2020 IEEE International Conference on Big Data, Big Data 2020",
        "citied_by": "38",
        "cover_date": "2020-12-10",
        "Abstract": "In functional User Interface testing, test scenarios are written with respect to the requirements that are specified by test analysts. Usually, a test analyst focuses on base URLs and HTML components while collecting requirements of User Interface test scenarios. A base URL is essentially a unit segment of large scale graph data. It has mostly dynamic shape and is used to navigate pages amongst application's pages. We argue that even though dynamic URLs have additional important information about the content of the page, they are not being utilized in generating User Interface test scenarios. In this study, we address this lack of capability and focus on the development of a methodology that can support the usage of large-scale dynamic URL datasets in UI test script generation. Our proposed methodology is designed as an add-on tool that can be used on the top of the existing UI test automation tools to improve testing quality. We introduce a higher quality testing methodology to make the results more accurate, and we discuss the proposed methodology and give an overview of the implementation details followed by the evaluation results. We perform various performance evaluations to investigate how well the proposed algorithms scale under increasing data sizes. The results are promising and show the usability of the proposed methodology.",
        "DOI": "10.1109/BigData50022.2020.9378153",
        "paper_author": "Uygun Y.",
        "affiliation_name": "Yıldız Teknik Üniversitesi",
        "affiliation_city": "Istanbul",
        "affiliation_country": "Turkey",
        "affiliation_id": "60019963",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Challenges and Barriers in Applying Natural Language Processing to Medical Examiner Notes from Fatal Opioid Poisoning Cases",
        "publication": "Proceedings - 2020 IEEE International Conference on Big Data, Big Data 2020",
        "citied_by": "7",
        "cover_date": "2020-12-10",
        "Abstract": "We detail the challenges and barriers in applying natural language processing techniques to a collection of medical examiner case investigation notes related to fatal opioid poisonings. Major advances in biomedical informatics have made natural language processing (NLP) of medical texts both a realistic and useful task. Biomedical NLP tools are typically designed to process documents originating from biomedical libraries or electronic health records (EHRs). The usefulness of biomedical NLP tools on texts authored outside of EHRs is unclear, despite an abundance of medicolegal documents existing at the intersection of medicine and law. In particular, we detail our experiences processing unstructured text and extracting semantic concepts using case investigation notes; these notes were authored by trained investigative professionals working in a medical examiner's office and describe cases containing deaths related to fatal opioid poisonings. Applying NLP to case notes is a particularly important step in generalizing the advances of biomedical NLP for other related domains and giving guidance to data scientists working with unstructured data generated outside of EHRs.",
        "DOI": "10.1109/BigData50022.2020.9378443",
        "paper_author": "Harris D.R.",
        "affiliation_name": "University of Kentucky",
        "affiliation_city": "Lexington",
        "affiliation_country": "United States",
        "affiliation_id": "60015941",
        "affiliation_state": "KY"
    },
    {
        "paper_title": "Deep Learning to Predict Hospitalization at Triage: Integration of Structured Data and Unstructured Text",
        "publication": "Proceedings - 2020 IEEE International Conference on Big Data, Big Data 2020",
        "citied_by": "37",
        "cover_date": "2020-12-10",
        "Abstract": "Overcrowding in Emergency Departments (ED) is considered as an international issue, which could have adverse impacts on multiple care outcomes such as the length of stay for example. Part of the solution could lie in the early prediction of the patient outcome as discharge or hospitalization. This study applies Deep Learning to this end. A large-scale dataset of about 260K ED records was provided by the Amiens-Picardy University Hospital in France. In general, our approach is based on integrating structured data with unstructured textual notes recorded at the triage stage. The key idea is to apply a multi-input of mixed data for training a classification model to predict hospitalization. In a simultaneous manner, the model training utilizes the numeric features along with textual data. On one hand, a standard Multi-Layer Perceptron (MLP) model is used with the standard set of features (i.e. numeric and categorical). On the other hand, a Convolutional Neural Network (CNN) is used to operate over the textual data. The two components of learning are conducted independently in parallel. The empirical results demonstrated that the classifier could achieve a very good accuracy with ROC-AUC≈0.83. The study is conceived to contribute to the mounting efforts of applying Natural Language Processing in the healthcare domain.",
        "DOI": "10.1109/BigData50022.2020.9378073",
        "paper_author": "Arnaud E.",
        "affiliation_name": "CHU Amiens Picardie",
        "affiliation_city": "Amiens",
        "affiliation_country": "France",
        "affiliation_id": "60002810",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Fine-Tuning Language Models for Semi-Supervised Text Mining",
        "publication": "Proceedings - 2020 IEEE International Conference on Big Data, Big Data 2020",
        "citied_by": "4",
        "cover_date": "2020-12-10",
        "Abstract": "The dimensionality of traditional text representation is large, but the underlying text data is sparse. This makes text clustering a very challenging task. Using language models and deep contextualized representations is promising in many Natural Language Processing (NLP) tasks. However, some task-specific guidance is necessary to adapt language models to a novel domain or to particular downstream tasks. We present an empirical study of a pipeline for semi-supervised text clustering tasks. Our proposed method utilizes a small number of labeled samples to fine-tune pre-trained language models. This fine-tuning step adapts the language models to produce task-specific contextualized representations, improving the performance of downstream text clustering tasks. We evaluate two clustering algorithms using the output of three different language models on six real-world text mining tasks to demonstrate to what extent this pipeline can improve text clustering accuracy and the amount of labeled samples needed for improvement. Our experiments show that for topic mining in novel domains or surfacing the intentions of abstracts, language models begin to produce better task-specific representations using a labeled subset as small as 0.5% of the task data. On the other hand, to find topics in domains that are overlapping with pre-training corpora, language models need labeled subsets closer to 1.0% of the task data to overcome the catastrophic forgetting problem. Further experiments show the downstream clustering accuracy gain begins to slow down or plateau if language models are fine-tuned with more than 5% of the task data. There is a trade-off between the desired downstream clustering quality and the cost for labeling and fine-tuning language models. Fine-tuned with 2.5% of the task data, our approach matches or exceeds the current state-of-the-art for several clustering tasks and provides baseline results for two novel clustering tasks. These results provide solid guidance to utilize powerful language models for text clustering and information retrieval practitioners.",
        "DOI": "10.1109/BigData50022.2020.9377810",
        "paper_author": "Chen X.",
        "affiliation_name": "NextIT Corporation",
        "affiliation_city": "Spokane",
        "affiliation_country": "United States",
        "affiliation_id": "102026287",
        "affiliation_state": "WA"
    },
    {
        "paper_title": "Clinical Sublanguage Trend and Usage Analysis from a Large Clinical Corpus",
        "publication": "Proceedings - 2020 IEEE International Conference on Big Data, Big Data 2020",
        "citied_by": "1",
        "cover_date": "2020-12-10",
        "Abstract": "The field of clinical natural language processing (NLP) has been built on the analysis of clinical sublanguage characteristics. It is well recognized that not only does clinical sublanguage differ from general English (or other languages) but also clinical sublanguage differs among clinical subspecialties and among corpora originating from different healthcare systems. A less recognized aspect is that clinical sublanguage, like all languages, evolves over time. This paper analyses the evolution of clinical sublanguage using a large, national clinical text corpus spanning 15 years. Through the analyses of document types, length, ngrams, and concepts, we found strong evidence that clinical sublanguage does evolve and such changes have implications for NLP development and maintenance. Although the analysis is performed on one corpus, our observations of sublanguage changes are generalizable.",
        "DOI": "10.1109/BigData50022.2020.9378203",
        "paper_author": "Shao Y.",
        "affiliation_name": "The George Washington University School of Medicine and Health Sciences",
        "affiliation_city": "Washington, D.C.",
        "affiliation_country": "United States",
        "affiliation_id": "60025286",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Interpretation of Sentiment Analysis with Human-in-the-Loop",
        "publication": "Proceedings - 2020 IEEE International Conference on Big Data, Big Data 2020",
        "citied_by": "4",
        "cover_date": "2020-12-10",
        "Abstract": "Human-in-the-Loop has been receiving special attention from the data science and machine learning community. It is essential to realize the advantages of human feedback and the pressing need for manual annotation to improve machine learning performance. Recent advancements in natural language processing (NLP) and machine learning have created unique challenges and opportunities for digital humanities research. In particular, there are ample opportunities for NLP and machine learning researchers to analyze data from literary texts and use these complex source texts to broaden our understanding of human sentiment using the human-in-the-loop approach. This paper presents our understanding of how human annotators differ from machine annotators in sentiment analysis tasks and how these differences can contribute to designing systems for the \"human in the loop\"sentiment analysis in complex, unstructured texts. We further explore the challenges and benefits of the human-machine collaboration for sentiment analysis using a case study in Greek tragedy and address some open questions about collaborative annotation for sentiments in literary texts. We focus primarily on (i) an analysis of the challenges in sentiment analysis tasks for humans and machines, and (ii) whether consistent annotation results are generated from multiple human annotators and multiple machine annotators. For human annotators, we have used a survey-based approach with about 60 college students. We have selected six popular sentiment analysis tools for machine annotators, including VADER, CoreNLP's sentiment annotator, TextBlob, LIME, Glove+LSTM, and RoBERTa. We have conducted a qualitative and quantitative evaluation with the human-in-the-loop approach and confirmed our observations on sentiment tasks using the Greek tragedy case study.",
        "DOI": "10.1109/BigData50022.2020.9378221",
        "paper_author": "Yeruva V.K.",
        "affiliation_name": "University of Missouri-Kansas City",
        "affiliation_city": "Kansas City",
        "affiliation_country": "United States",
        "affiliation_id": "60007056",
        "affiliation_state": "MO"
    },
    {
        "paper_title": "A Concern Analysis of Federal Reserve Statements: The Great Recession vs: The COVID-19 Pandemic",
        "publication": "Proceedings - 2020 IEEE International Conference on Big Data, Big Data 2020",
        "citied_by": "0",
        "cover_date": "2020-12-10",
        "Abstract": "It is important and informative to compare and contrast major economic crises in order to confront novel and unknown cases such as the COVID-19 pandemic. The 2006 Great Recession and then the 2019 pandemic have a lot to share in terms of unemployment rate, consumption expenditures, and interest rates set by Federal Reserve. In addition to quantitative historical data, it is also interesting to compare the contents of Federal Reserve statements for the period of these two crises and find out whether Federal Reserve cares about similar concerns or there are some other issues that demand separate and unique monetary policies. This paper conducts an analysis to explore the Federal Reserve concerns as expressed in their statements for the period of 2005 to 2020. The concern analysis is performed using natural language processing (NLP) algorithms and a trend analysis of concern is also presented. We observe that there are some similarities between the Federal Reserve statements issued during the Great Recession with those issued for the 2019 COVID-19 pandemic.",
        "DOI": "10.1109/BigData50022.2020.9377828",
        "paper_author": "Gutierrez L.F.",
        "affiliation_name": "Edward E. Whitacre Jr. College of Engineering",
        "affiliation_city": "Lubbock",
        "affiliation_country": "United States",
        "affiliation_id": "60149446",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "Evaluating the Accuracy of Cloud NLP Services Using Ground-Truth Experiments",
        "publication": "Proceedings - 2020 IEEE International Conference on Big Data, Big Data 2020",
        "citied_by": "4",
        "cover_date": "2020-12-10",
        "Abstract": "Cloud services for natural language processing (NLP) increasingly establish as viable alternatives to self-maintained and self-trained NLP pipelines. In particular, they feature low access barriers and management overhead, a pay-as-you-go pricing model, and elastic scalability allowing to process large amounts of natural language data ad hoc. Any deliberation about employing cloud NLP services in practice does, however, face the challenge that so far, little is known about the accuracy provided by such services as well as about how to conduct respective quality assessments.In this paper, we therefore present a method for evaluating the accuracy provided by cloud NLP services and apply it to cloud services for three prominent NLP tasks offered by Amazon, Google, Microsoft, and IBM. Our results show significantly different accuracies as well as different dependencies on the specifics of input data among the covered providers. Our insights therefore allow for a more evidence-based quality-driven choice of the provider to be used for NLP in practice. Furthermore, the general approach employed may also serve as a blueprint for additional future evaluations of cloud NLP services for other tasks or offered by other providers.",
        "DOI": "10.1109/BigData50022.2020.9378188",
        "paper_author": "Pallas F.",
        "affiliation_name": "Technische Universität Berlin",
        "affiliation_city": "Berlin",
        "affiliation_country": "Germany",
        "affiliation_id": "60011604",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Generating Chinese Classical Poetry with Quatrain Generation Model (QGM) Using Encoder-Decoder LSTM",
        "publication": "Proceedings - 2020 IEEE International Conference on Big Data, Big Data 2020",
        "citied_by": "1",
        "cover_date": "2020-12-10",
        "Abstract": "Chinese classical poetry has strict formats and complicated linguistic rules including harmonious rhyme, level and oblique tones (Pingze). Therefore, using NLP to generate classical poetry that meets the aforementioned constraints has always attracted much attention. Although many studies have shown that deep learning has good results in generating poetry, however, the use of unconstrained natural language may cause the generated poems to fail in conforming to the metric rules, and also make the expression of the meaning of the poems incomplete. In view of this, this study uses the quatrains of the Tang and Song Dynasties as the training sample, adopts Encoder Decoder LSTM as the quatrain generation model (QGM), and then uses Latent Dirichlet Allocation to expand the meaning of related words of each consecutive sentence of the poetry. The features and contributions of this study are as follows: (1) We propose a QGM which learns poetry knowledge from four consecutive sentences as corpus respectively in the poetry. (2) The QGM is leveraged to strengthen the meaningfulness, grammaticality, and poeticness of a poetry by extending the keywords to a collection of semantic related words. (3) We elaborate the evaluation of poetry generation with BLEU-2 metric, cooperating with human evaluation. This study shows that our model can effectively improve the coherence of the sentences with the requirement of the metric rules.",
        "DOI": "10.1109/BigData50022.2020.9378383",
        "paper_author": "Huang C.M.",
        "affiliation_name": "National Yunlin University of Science and Technology",
        "affiliation_city": "Douliou",
        "affiliation_country": "Taiwan",
        "affiliation_id": "60014261",
        "affiliation_state": "Yunlin"
    },
    {
        "paper_title": "Building A Non-Personalized Recommender System by Learning Product and Basket Representation",
        "publication": "Proceedings - 2020 IEEE International Conference on Big Data, Big Data 2020",
        "citied_by": "2",
        "cover_date": "2020-12-10",
        "Abstract": "In this paper, we addressed the problem of learning product and basket representation for a non-personalized recommendation system where the baskets do not have a specific owner. The recommendation models tend to exploit as much information as possible along with basket patterns to improve performance. We focus on the representation problem for the baskets without any customer information. Deep learning-based architectures have solved many representation problems such as natural language processing (NLP) and computer vision (CV) so far. While the NLP model takes a bag of words as input, the recommendation models take a basket of products as input. The learning algorithm uses co-occurrence information and therefore exploits the idea that the things that appear in a similar environment share similar meaning. But traditional representation approaches such as one-hot encoding have dimensionality problems when the number of entities increases. On the other hand, neural models can solve this dimensionality curse and transform each entity into a short and dense vector, namely embeddings. We successfully designed unsupervised and super-vised architectures to solve the product and basket embeddings for a recommendation engine. Our experiments show that the proposed deep learning architecture showed better performance than baseline approaches in terms of many metrics. We also discussed and addressed many product representation related problems throughout the paper.",
        "DOI": "10.1109/BigData50022.2020.9377963",
        "paper_author": "Yildirim S.",
        "affiliation_name": "Istanbul Bilgi Üniversitesi",
        "affiliation_city": "Istanbul",
        "affiliation_country": "Türkiye",
        "affiliation_id": "60009715",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Toward Robust Food Ontology Mapping",
        "publication": "Proceedings - 2020 IEEE International Conference on Big Data, Big Data 2020",
        "citied_by": "3",
        "cover_date": "2020-12-10",
        "Abstract": "Data normalization methodologies are extremely welcome to link extracted information from textual data to different semantic resources. These methodologies have been previously well researched especially in the biomedical domain, where health concepts were normalized and described using semantic tags. Recently, a methodology for normalizing food concepts has been proposed, based on Named-Entity Recognition methods resulting in the FoodOntoMap semantic resource. In this paper, we propose and evaluate a new architecture for linking phrases (i.e. textual name for foods) to concepts from semantic resources in the Food and Nutrition domain. We represent the food phrases (i.e. their textual name) in continuous vector space using state-of-the-art Natural Language Processing (NLP) embedding algorithms, and evaluate their proximity with respect to the annotated semantic food concepts. Additionally, indexing was incorporated to improve efficiency.The GloVe embedding with mean pooling provided best evaluation results, with maximum recall of 74% for the Snomed CT semantic dataset, which is promising result, but also opens a space for future improvement of the phrase representations, and their incorporation in this system.",
        "DOI": "10.1109/BigData50022.2020.9378066",
        "paper_author": "Stojanov R.",
        "affiliation_name": "Ss. Cyril and Methodius University in Skopje",
        "affiliation_city": "Skopje",
        "affiliation_country": "North Macedonia",
        "affiliation_id": "60072629",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Illicit Activity Detection in Large-Scale Dark and Opaque Web Social Networks",
        "publication": "Proceedings - 2020 IEEE International Conference on Big Data, Big Data 2020",
        "citied_by": "2",
        "cover_date": "2020-12-10",
        "Abstract": "Many online chat applications live in a grey area between the legitimate web and the dark net. The Telegram network in particular can aid criminal activities. Telegram hosts 'chats' which consist of varied conversations and advertisements. These chats take place among automated 'bots' and human users. Classifying legitimate activity from illegitimate activity can aid law enforcement in finding criminals. Social network analysis of Telegram chats presents a difficult problem. Users can change their username or create new accounts. Users involved in criminal activity often do this to obscure their identity. This makes establishing the unique identity behind a given username challenging. Thus we explored classifying users from their language usage in their chat messages.The volume and velocity of Telegram chat data place it well within the domain of big data. Machine learning and natural language processing (NLP) tools are necessary to classify this chat data. We developed NLP tools for classifying users and the chat group to which their messages belong. We found that legitimate and illegitimate chat groups could be classified with high accuracy. We also were able to classify bots, humans, and advertisements within conversations.",
        "DOI": "10.1109/BigData50022.2020.9378229",
        "paper_author": "Shah D.",
        "affiliation_name": "Georgia State University",
        "affiliation_city": "Atlanta",
        "affiliation_country": "United States",
        "affiliation_id": "60012387",
        "affiliation_state": "GA"
    }
]