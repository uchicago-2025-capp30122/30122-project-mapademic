[
    {
        "paper_title": "Agricultural Robots: Technology Progress, Challenges and Trends",
        "paper_author": "Zhao C.",
        "publication": "Smart Agriculture",
        "citied_by": "21",
        "cover_date": "2023-12-30",
        "Abstract": "[Significance] Autonomous and intelligent agricultural machinery, characterized by green intelligence, energy efficiency, and reduced emissions, as well as high intelligence and man-machine collaboration, will serve as the driving force behind global agricultural technology advancements and the transformation of production methods in the context of smart agriculture development. Agricultural robots, which utilize intelligent control and information technology, have the unique advantage of replacing manual labor. They occupy the strategic commanding heights and competitive focus of global agricultural equipment and are also one of the key development directions for accelerating the construction of China's agricultural power. World agricultural powers and China have incorporated the research, development, manufacturing, and promotion of agricultural robots into their national strategies, respectively strengthening the agricultural robot policy and planning layout based on their own agricultural development characteristics, thus driving the agricultural robot industry into a stable growth period. [Progress] This paper firstly delves into the concept and defining features of agricultural robots, alongside an exploration of the global agricultural robot development policy and strategic planning blueprint. Furthermore, sheds light on the growth and development of the global agricultural robotics industry; Then proceeds to analyze the industrial backdrop, cutting-edge advancements, developmental challenges, and crucial technology aspects of three representative agricultural robots, including farmland robots, orchard picking robots, and indoor vegetable production robots. Finally, summarizes the disparity between Chinese agricultural robots and their foreign counterparts in terms of advanced technologies. (1) An agricultural robot is a multi-degree-of-freedom autonomous operating equipment that possesses accurate perception, autonomous decision-making, intelligent control, and automatic execution capabilities specifically designed for agricultural environments. When combined with artificial intelligence, big data, cloud computing, and the Internet of Things, agricultural robots form an agricultural robot application system. This system has relatively mature applications in key processes such as field planting, fertilization, pest control, yield estimation, inspection, harvesting, grafting, pruning, inspection, harvesting, transportation, and livestock and poultry breeding feeding, inspection, disinfection, and milking. Globally, agricultural robots, represented by plant protection robots, have entered the industrial application phase and are gradually realizing commercialization with vast market potential. (2) Compared to traditional agricultural machinery and equipment, agricultural robots possess advantages in performing hazardous tasks, executing batch repetitive work, managing complex field operations, and livestock breeding. In contrast to industrial robots, agricultural robots face technical challenges in three aspects. Firstly, the complexity and unstructured nature of the operating environment. Secondly, the flexibility, mobility, and commoditization of the operation object. Thirdly, the high level of technology and investment required. (3) Given the increasing demand for unmanned and less manned operations in farmland production, China's agricultural robot research, development, and application have started late and progressed slowly. The existing agricultural operation equipment still has a significant gap from achieving precision operation, digital perception, intelligent management, and intelligent decision-making. The comprehensive performance of domestic products lags behind foreign advanced counterparts, indicating that there is still a long way to go for industrial development and application. Firstly, the current agricultural robots predominantly utilize single actuators and operate as single machines, with the development of multi-arm cooperative robots just emerging. Most of these robots primarily engage in rigid operations, exhibiting limited flexibility, adaptability, and functionality. Secondly, the perception of multi-source environments in agricultural settings, as well as the autonomous operation of agricultural robot equipment, relies heavily on human input. Thirdly, the progress of new teaching methods and technologies for human-computer natural interaction is rather slow. Lastly, the development of operational infrastructure is insufficient, resulting in a relatively low degree of \"mechanization\". [Conclusions and Prospects] The paper anticipates the opportunities that arise from the rapid growth of the agricultural robotics industry in response to the escalating global shortage of agricultural labor. It outlines the emerging trends in agricultural robot technology, including autonomous navigation, self-learning, real-time monitoring, and operation control. In the future, the path planning and navigation information perception of agricultural robot autonomy are expected to become more refined. Furthermore, improvements in autonomous learning and cross-scenario operation performance will be achieved. The development of real-time operation monitoring of agricultural robots through digital twinning will also progress. Additionally, cloud-based management and control of agricultural robots for comprehensive operations will experience significant growth. Steady advancements will be made in the innovation and integration of agricultural machinery and techniques.",
        "DOI": "10.12133/j.smartag.SA202312030",
        "affiliation_name": "Beijing Academy of Agriculture and Forestry Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Explaining the challenges of accountability in machine learning systems beyond technical obstacles",
        "paper_author": "Palvadi S.K.",
        "publication": "Quantum Innovations at the Nexus of Biomedical Intelligence",
        "citied_by": "0",
        "cover_date": "2023-12-29",
        "Abstract": "The ability to make a note regarding machine learning systems decisions for people is becoming increas¬ingly sought after, particularly in situations where decisions have significant repercussions for those affected and where capability in terms of maintaining is required. To increase comprehension based on referred to as \"black box\" mechanism, explaining ability is frequently cited as a technical obstacle in the design of ML systems and decision procedures. The quantities that ML systems aim to optimize must be specified by their users. This leads to the revealing of policy trade-offs that may have previously been hidden or implicit. Important decisions, as well as judgments, help what may need to be explicitly discussed in public debate as ML's use in policy expands.",
        "DOI": "10.4018/979-8-3693-1479-1.ch003",
        "affiliation_name": "K L Deemed to be University",
        "affiliation_city": "Vaddeswaram",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Fortifying the digital forge: Unleashing cybersecurity in the interconnected world of digital manufacturing",
        "paper_author": "Dodiya K.R.",
        "publication": "Emerging Technologies in Digital Manufacturing and Smart Factories",
        "citied_by": "3",
        "cover_date": "2023-12-29",
        "Abstract": "In our interconnected world, cybersecurity is paramount due to IoT, cloud computing, and automation's impact on manufacturing. This chapter underscores digital manufacturing's dependence on interconnected systems and cloud infrastructure, acknowledging risks like data breaches, IP theft, and operational disruptions. It advocates a comprehensive cybersecurity approach encompassing technical measures, organizational policies, staff training, and incident response. The chapter delves into threats like malware, phishing, ransomware, and supply chain attacks, emphasizing continuous monitoring, threat intelligence, and vulnerability management. Additionally, it explores emerging trends like AI and machine learning in cybersecurity, legacy system security, and collaborative efforts among industry players, government agencies, and cybersecurity experts to safeguard digital manufacturing. This chapter aids manufacturers, security experts, and researchers in building secure systems in our connected digital landscape.",
        "DOI": "10.4018/979-8-3693-0920-9.ch014",
        "affiliation_name": "Gujarat University",
        "affiliation_city": "Gujarat",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Correction: Exploration on the mechanism of therapeutic and toxic bidirectional effects of Haizao Yuhu decoction based on machine learning and data mining (Medical Data Mining, (2023), 6, 4, 21, 10.53388/MDM202306021)",
        "paper_author": "Chen Y.H.",
        "publication": "Medical Data Mining",
        "citied_by": "0",
        "cover_date": "2023-12-26",
        "Abstract": "Medical Data Mining published an article entitled Exploration on the mechanism of therapeutic and toxic bidirectional effects of Haizao Yuhu decoction based on machine learning and data mining on 05 September 2023. The proof of this article was confirmed by author on 05 September 2023 without any questions. But on 11 September, 2023, the authors contacted the editorial office to state that the following formula is missing at the end of first paragraph in the section “The stability test of the network”, the error was not found and corrected during the article processing (formula presented) The editorial office considers this correction request to be accordance with the policy of Medical Data Mining and agrees to make the correction. So the full content of first paragraph in the section “The stability test of the network” should be as follows: The performance of the multi-layer network depends on its stability, namely invulnerability. When some nodes in the network are destroyed, the network still maintains connectivity. The invulnerability of the network is widely used in complex microbial communities or flora communities. We introduced this invulnerability measurement to the multilayer network, which can reflect the biological significance of the integrity and stability of the multilayer network of the HYD in the treatment of goiter and causing DILI. The detailed principle and calculation formula were recorded in the article of Wu et al. [16]. The specific calculation formula was: (Formula presented).",
        "DOI": "10.53388/MDM202306026",
        "affiliation_name": "Beijing University of Chinese Medicine",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "GDPR-oriented intelligent checking method of privacy policies compliance",
        "paper_author": "Li X.",
        "publication": "Chinese Journal of Network and Information Security",
        "citied_by": "2",
        "cover_date": "2023-12-25",
        "Abstract": "The implementation of the EU’s General Data Protection Regulation (GDPR) has resulted in the imposition of over 300 fines since its inception in 2018. These fines include significant penalties for prominent companies like Google, which were penalized for their failure to provide transparent and comprehensible privacy policies. The GDPR, known as the strictest data protection laws in history, has made companies worldwide more cautious when offering cross-border services, particularly to the European Union. The regulation's territorial scope stipulates that it applies to any company providing services to EU citizens, irrespective of their location. This implies that companies worldwide, including domestic enterprises, are required to ensure compliance with GDPR in their privacy policies, especially those involved in international operations. To meet this requirement, an intelligent detection method was introduced. Machine learning and automation technologies were utilized to automatically extract privacy policies from online service companies. The policies were converted into a standardized format with a hierarchical structure. Through natural language processing, the privacy policies were classified, allowing for the identification of relevant GDPR concepts. In addition, a constructed GDPR taxonomy was used in the detection mechanism to identify any missing concepts as required by GDPR. This approach facilitated intelligent detection of GDPR-oriented privacy policy compliance, providing support to domestic enterprises while they provided cross-border services to EU users. Analysis of the corpus samples reveals the current situation that mainstream online service companies generally fail to meet GDPR compliance requirements.",
        "DOI": "10.11959/j.issn.2096-109x.2023088",
        "affiliation_name": "Shanghai Jiao Tong University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Optimal deep machine learning framework for vibration mitigation of seismically-excited uncertain building structures",
        "paper_author": "Rad A.B.",
        "publication": "Structural Engineering and Mechanics",
        "citied_by": "1",
        "cover_date": "2023-12-25",
        "Abstract": "Deep extreme learning machine (DELM) and multi-verse optimization algorithms (MVO) are hybridized for designing an optimal and adaptive control framework for uncertain buildings. In this approach, first, a robust model predictive control (RMPC) scheme is developed to handle the problem uncertainty. The optimality and adaptivity of the proposed controller are provided by the optimal determination of the tunning weights of the linear programming (LP) cost function for clustered external loads using the MVO. The final control policy is achieved by collecting the clustered data and training them by DELM. The efficiency of the introduced control scheme is demonstrated by the numerical simulation of a ten-story benchmark building subjected to earthquake excitations. The results represent the capability of the proposed framework compared to robust MPC (RMPC), conventional MPC (CMPC), and conventional DELM algorithms in structural motion control.",
        "DOI": "10.12989/sem.2023.88.6.535",
        "affiliation_name": "University of Tabriz",
        "affiliation_city": "Tabriz",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "A machine learning-based admissions support system with a case study on D.K.M. College for Women, Vellore",
        "paper_author": "Vasumathy M.",
        "publication": "Design and Implementation of Higher Education Learners' Learning Outcomes (HELLO)",
        "citied_by": "0",
        "cover_date": "2023-12-22",
        "Abstract": "Building efficient higher education systems is tough for India and other developing countries, especially when it comes to female students. Despite the government's efforts, India did not gain anything from its superb and creative educational policy. The Indian government is cognizant of the particular challenges the current global situation poses for the higher education industry. At Vellore's D.K.M. College for Women, the post-graduation admission rate has considerably declined over the last 12 months. The work is largely focused on the identification of concerns, obstacles, and declining factors of post-graduation admittance from the perspectives of students, parents, teaching staffs, and management. The architecture for the course recommender system and the information flow across it are proposed in this chapter. This approach forecasts the ideal subject pairing, or the subjects that pupils will be most interested in. Here, the authors use a learning management system like Moodle to gather information from students about their course preferences.",
        "DOI": "10.4018/978-1-6684-9472-1.ch017",
        "affiliation_name": "Dhanabhagiyam Krishnasamy Mudaliar College for Women",
        "affiliation_city": "Vellore",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Accelerating and enhancing the generation of socioeconomic data to inform forced displacement policy and response",
        "paper_author": "Brock P.M.",
        "publication": "Data and Policy",
        "citied_by": "0",
        "cover_date": "2023-12-22",
        "Abstract": "There are now an estimated 114 million forcibly displaced people worldwide, some 88% of whom are in low- and middle-income countries. For governments and international organizations to design effective policies and responses, they require comparable and accessible socioeconomic data on those affected by forced displacement, including host communities. Such data is required to understand needs, as well as interactions between complex drivers of displacement and barriers to durable solutions. However, high-quality data of this kind takes time to collect and is costly. Can the ever-increasing volume of open data and evolving innovative techniques accelerate and enhance its generation? Are there applications of alternative data sources, advanced statistics, and machine-learning that could be adapted for forced displacement settings, considering their specific legal and ethical dimensions? As a catalytic bridge between the World Bank and UNHCR, the Joint Data Center on Forced Displacement convened a workshop to answer these questions. This paper summarizes the emergent messages from the workshop and recommendations for future areas of focus and ways forward for the community of practice on socioeconomic data on forced displacement. Three recommended areas of future focus are: enhancing and optimizing household survey sampling approaches; estimating forced displacement socioeconomic indicators from alternative data sources; and amplifying data accessibility and discoverability. Three key features of the recommended approach are: strong complementarity with the existing data-collection-to-use-pipeline; data responsibility built-in and tailored to forced displacement contexts; and iterative assessment of operational relevance to ensure continuous focus on improving outcomes for those affected by forced displacement.",
        "DOI": "10.1017/dap.2023.47",
        "affiliation_name": "The World Bank, USA",
        "affiliation_city": "Washington, D.C.",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Developing a Dynamic Decision-Support Framework for Higher Education Management Systems through Real-time Information Extraction",
        "paper_author": "Pantin R.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2023-12-21",
        "Abstract": "In the changing world of education, it is crucial for institutions to have the ability to make well informed decisions based on data. This paper presents an innovative Decision Support Framework (DDSF), for managing higher education systems. The DDSF utilizes real time information extraction to improve the decision-making process. It combines data analytics and user centric design to provide insights to education administrators. By gathering data from sources like records, financial reports and social media analytics the DDSF uses natural language processing (NLP) and machine learning (ML) algorithms to extract and interpret real time information. This information is then organized in a manner for analysis. The framework is adaptable allowing for the inclusion of emerging data streams ensuring relevance and usefulness. To evaluate the effectiveness of the DDSF we conducted a pilot study at a university. The results indicated improvements in meeting student needs optimizing resource allocation and enhancing operational efficiency. The framework also enables policy development by foreseeing challenges and identifying opportunities, within the education sector. The DDSF brings about a way of managing education by providing a strong platform for institutions to succeed in a competitive and constantly evolving environment. It emphasizes the significance of extracting real time information to guide planning and operational excellence. Future studies will concentrate on expanding the framework, for use, across institutions and incorporating analytics to improve foresight abilities.",
        "DOI": "10.1145/3644713.3644786",
        "affiliation_name": "Tashkent State University of Economics",
        "affiliation_city": "Tashkent",
        "affiliation_country": "Uzbekistan"
    },
    {
        "paper_title": "Comparative Analysis of Machine Learning Algorithms for Inflation Rate Classification and Economic Trend Forecasting",
        "paper_author": "Khashimova N.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-21",
        "Abstract": "Predictive modeling of inflation rates is critical for economic policy and risk management. This study compares the efficacy of K-Nearest Neighbors (KNN) and logistic regression algorithms in classifying inflation into 'Low,' 'Medium,' and 'High' categories, which are essential for monitoring and managing economic stability. Through meticulous preprocessing, including outlier removal, normalization, and imputation, the dataset was refined for accurate analysis. The study established inflation thresholds using empirical data and economic theory to create structured categories. The EDA revealed the fluctuating nature of inflation and its long-term trends, providing a clear picture of economic conditions. Logistic regression demonstrated a higher accuracy rate and better performance metrics over KNN, suggesting it as a superior model for inflation classification. The findings emphasize the importance of precise model selection in economic forecasting and propose logistic regression as a robust tool for policy-makers and economists in strategic economic planning.",
        "DOI": "10.1145/3644713.3644749",
        "affiliation_name": "Tashkent State University of Economics",
        "affiliation_city": "Tashkent",
        "affiliation_country": "Uzbekistan"
    },
    {
        "paper_title": "Optimizing geographic locations for electric vehicle battery recycling preprocessing facilities in California",
        "paper_author": "Haynes M.W.",
        "publication": "RSC Sustainability",
        "citied_by": "3",
        "cover_date": "2023-12-20",
        "Abstract": "Spent lithium-ion batteries (LIBs) at end of life pose several safety risks. Specifically, LIBs have the potential to self-ignite during transport, release toxic compounds during incineration, and can leach contaminants into landfills. Spent LIBs, which are classified as hazardous waste, are also subject to numerous policies and require disposal by certified personnel and companies. These requirements result in an increase in transport costs and volume compared to other waste. Efforts to improve LIB recycling focus primarily on reducing costs to make recycling economically profitable. The greatest emphasis is placed on improving recycling technologies; however, transport costs significantly impact the total cost of LIB recycling. Here, we provide a procedure for choosing an unsupervised machine learning clustering heuristic to identify optimal locations for LIB recycling preprocessing facilities in California. The identified decentralized facility locations minimize the transportation distance and the cost of shipping spent electric vehicle batteries between end-use sector facilities and potential second-use locations.",
        "DOI": "10.1039/d3su00319a",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Atlanta",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Multimodal analysis of disinformation and misinformation",
        "paper_author": "Wilson A.",
        "publication": "Royal Society Open Science",
        "citied_by": "5",
        "cover_date": "2023-12-20",
        "Abstract": "The use of disinformation and misinformation campaigns in the media has attracted much attention from academics and policy-makers. Multimodal analysis or the analysis of two or more semiotic systems - language, gestures, images, sounds, among others - in their interrelation and interaction is essential to understanding dis-/misinformation efforts because most human communication goes beyond just words. There is a confluence of many disciplines (e.g. computer science, linguistics, political science, communication studies) that are developing methods and analytical models of multimodal communication. This literature review brings research strands from these disciplines together, providing a map of the multi- and interdisciplinary landscape for multimodal analysis of dis-/misinformation. It records the substantial growth starting from the second quarter of 2020 - the start of the COVID-19 epidemic in Western Europe - in the number of studies on multimodal dis-/misinformation coming from the field of computer science. The review examines that category of studies in more detail. Finally, the review identifies gaps in multimodal research on dis-/misinformation and suggests ways to bridge these gaps including future cross-disciplinary research directions. Our review provides scholars from different disciplines working on dis-/misinformation with a much needed bird's-eye view of the rapidly emerging research of multimodal dis-/misinformation.",
        "DOI": "10.1098/rsos.230964",
        "affiliation_name": "Oxford Social Sciences Division",
        "affiliation_city": "Oxford",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Revisiting the dynamics of gaseous ammonia and ammonium aerosols during the COVID-19 lockdown in urban Beijing using machine learning models",
        "paper_author": "Lyu Y.",
        "publication": "Science of the Total Environment",
        "citied_by": "3",
        "cover_date": "2023-12-20",
        "Abstract": "The concentration of atmospheric ammonia (NH3) in urban Beijing substantially decreased during the COVID-19 lockdown (24 January to 3 March 2020), likely due to the reduced human activities. However, quantifying the impact of anthropogenic interventions on NH3 dynamics is challenging, as both meteorology and chemistry mask the real changes in observed NH3 concentrations. Here, we applied machine learning techniques based on random forest models to decouple the impacts of meteorology and emission changes on the gaseous NH3 and ammonium aerosol (NH4+) concentrations in Beijing during the lockdown. Our results showed that the meteorological conditions were unfavorable during the lockdown and tended to cause an increase of 8.4 % in the NH3 concentration. In addition, significant reductions in NOx and SO2 emissions could also elevate NH3 concentrations by favoring NH3 gas-phase partitioning. However, the observed NH3 concentration significantly decreased by 35.9 % during the lockdown, indicating a significant reduction in emissions or enhanced chemical sinks. Rapid gas-to-particle conversion was indeed found during the lockdown. Thus, the observed reduced NH3 concentrations could be partially explained by the enhanced transformation into NH4+. Therefore, the sum of NH3 and NH4+ (collectively, NHx) is a more reliable tracer than NH3 or NH4+ alone to estimate the changes in NH3 emissions. Compared to that under the scenario without lockdowns, the NHx concentration decreased by 26.4 %. We considered that this decrease represents the real decrease in NH3 emissions in Beijing due to the lockdown measures, which was less of a decrease than that based on NH3 only (35.9 %). This study highlights the importance of considering chemical sinks in the atmosphere when applying machine learning techniques to link the concentrations of reactive species with their emissions.",
        "DOI": "10.1016/j.scitotenv.2023.166946",
        "affiliation_name": "National Satellite Meteorological Center Beijing",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Soil carbon sequestration potential of cultivated lands and its controlling factors in China",
        "paper_author": "Wang S.",
        "publication": "Science of the Total Environment",
        "citied_by": "5",
        "cover_date": "2023-12-20",
        "Abstract": "Understanding soil organic carbon (SOC) stocks and carbon sequestration potential in cultivated lands can have significant benefit for mitigating climate change and emission reduction. However, there is currently a lack of spatially explicit information on this topic in China, and our understanding of the factors that influence both saturated SOC level (SOCS) and soil organic carbon density (SOCD) remains limited. This study predicted SOCS and SOCD of cultivated lands across mainland China based on point SOC measurements, and mapped its spatial distribution using environmental variables as predictors. Based on the differentiation between SOCS and SOCD, the soil organic carbon sequestration potentials (SOCP) of cultivated land were calculated. Boosted regression trees (BRT), random forest (RF), and support vector machine (SVM) were evaluated as prediction models, and the RF model presented the best performance in predicting SOCS and SOCD based on 10-fold cross-validation. A total of 991 topsoil (0–20 cm) SOC measurements and 12 environmental variables explaining topography, climate, organism, soil properties, and human activity were used as predictors in the model. Both SOCS and SOCD suggested higher SOC levels in northeast China and lower levels in central China. The cultivated lands in China had the potential to sequester about 2.13 ± 0.96 kg m−2 (3.25 Pg) SOC in the top 20 cm soil depth. Northeastern China had the largest SOCP followed by Northern China, and Southwestern China had the lowest SOCP. The primary environmental variables that affected the spatial variation of SOCS were mean annual temperature, followed by clay content and normalized difference vegetation index (NDVI). The assessment and mapping of SOCP in China's cultivated lands holds significance importance as it can provide valuable insights to policymakers and researchers about SOCP, and aid in formulating climate change mitigation strategies.",
        "DOI": "10.1016/j.scitotenv.2023.167292",
        "affiliation_name": "Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Nitrate prediction in groundwater of data scarce regions: The futuristic fresh-water management outlook",
        "paper_author": "Mahlknecht J.",
        "publication": "Science of the Total Environment",
        "citied_by": "16",
        "cover_date": "2023-12-20",
        "Abstract": "Nitrate contamination in groundwater poses a significant threat to water quality and public health, especially in regions with limited data availability. This study addresses this challenge by employing machine learning (ML) techniques to predict nitrate (NO3−-N) concentrations in Mexico's groundwater. Four ML algorithms—Extreme Gradient Boosting (XGB), Boosted Regression Trees (BRT), Random Forest (RF), and Support Vector Machines (SVM)—were executed to model NO3−-N concentrations across the country. Despite data limitations, the ML models achieved robust predictive performances. XGB and BRT algorithms demonstrated superior accuracy (0.80 and 0.78, respectively). Notably, this was achieved using ∼10 times less information than previous large-scale assessments. The novelty lies in the first-ever implementation of the ‘Support Points-based Split Approach’ during data pre-processing. The models considered initially 68 covariates and identified 13–19 significant predictors of NO3−-N concentration spanning from climate, geomorphology, soil, hydrogeology, and human factors. Rainfall, elevation, and slope emerged as key predictors. A validation incorporated nationwide waste disposal sites, yielding an encouraging correlation. Spatial risk mapping unveiled significant pollution hotspots across Mexico. Regions with elevated NO3−-N concentrations (>10 mg/L) were identified, particularly in the north-central and northeast parts of the country, associated with agricultural and industrial activities. Approximately 21 million people, accounting for 10 % of Mexico's population, are potentially exposed to elevated NO3−-N levels in groundwater. Moreover, the NO3−-N hotspots align with reported NO3−-N health implications such as gastric and colorectal cancer. This study not only demonstrates the potential of ML in data-scarce regions but also offers actionable insights for policy and management strategies. Our research underscores the urgency of implementing sustainable agricultural practices and comprehensive domestic waste management measures to mitigate NO3−-N contamination. Moreover, it advocates for the establishment of effective policies based on real-time monitoring and collaboration among stakeholders.",
        "DOI": "10.1016/j.scitotenv.2023.166863",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Davis",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Educational Missions and Tactics in the EU Artificial Intelligence Strategy in the Era of Digital Transformation: A Policy Analysis Based on EU Documents Published During 2018-2022",
        "paper_author": "Xiong Y.",
        "publication": "Advances in Transdisciplinary Engineering",
        "citied_by": "0",
        "cover_date": "2023-12-15",
        "Abstract": "In the era of digital transformation, AI, cloud computing, big data, blockchain, and other intelligent technologies have increasingly profound impacts on work, study, life, and especially education. The Publishing of The Age of Artificial Intelligence: Towards a European Strategy for Human-Centric Machines marked the official launch of the EU Artificial Intelligence Strategy. By analyzing policy documents published during 2018 and 2022 for the EU Artificial Intelligence Strategy, it is found that the strategy has set three strategic goals, namely achieving an 'Ecosystem of Excellence', building an 'Ecosystem of Trust', and establishing an 'Ecosystem of Digital Education'. To ensure its implementation, the European Commission has published a series of policy documents successively. As the main driver of the AI strategy, education plays an important role in the development of AI, and the EU has adopted various education tactics in the series of documents. Given the particularity of the EU as a regional cooperation organization, this strategy values philosophy, norms, and cooperation. It also provides a good reference for AI development and digital transformation of society in other countries and brings new opportunities for international cooperation in the field of AI.",
        "DOI": "10.3233/ATDE230999",
        "affiliation_name": "Wenzhou University",
        "affiliation_city": "Wenzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Development of a Machine Learning Framework Based on Occupant-Related Parameters to Predict Residential Electricity Consumption in the Hot and Humid Climate",
        "paper_author": "Qavidel Fard Z.",
        "publication": "Energy and Buildings",
        "citied_by": "0",
        "cover_date": "2023-12-15",
        "Abstract": "Occupant-related variables constitute one of the most significant groups of factors influencing residential building energy consumption. However, prediction methods often oversimplify these parameters, leading to substantial discrepancies between predicted and actual consumption. To address this issue, the present study aims to develop a machine learning framework for predicting electricity consumption in residential buildings based on occupant-related factors. The study incorporates twenty-six inputs, including occupant characteristics such as demographics, occupancy, behavior, and behavioral efficiency, two time-related factors, and three extra parameters related to equipment (refrigerator age, hot water source, and type of electricity meter) for training and testing the Random Forest (RF) algorithm in both regression and classification forms. The results indicate that the trained RF regressor exhibits well performance (R2Train = 0.989, R2Test = 0.916, MAETrain = 0.81, MAETest = 2.21, RMSETrain = 1.27, and RMSETest = 3.45). Furthermore, feature importance analysis reveals that the most significant parameter is the time of year, representing weather conditions, followed by the number of occupants, neighborhood, indoor set-point range, mean age of occupants, window opening, and cooling system mode. Even after removing the least impactful factors, the model maintains strong performance with the 16 most important variables (R2Train = 0.986, R2Test = 0.910, MAETrain = 0.83, MAETest = 2.25, RMSETrain = 1.31, and RMSETest = 3.49). Additionally, the RF classifier is designed for problems with 2, 4, 6, and 8 classes based on energy consumption ranges. The results of this model demonstrate that the 2-class model achieves the highest performance (AccuracyTest = 0.963, MAETest = 0.04, and RMSETest = 0.19). However, it lacks detailed categorization of homes based on electricity consumption. On the other hand, the 4-class and 6-class models strike a good balance between prediction performance and the level of detail. In conclusion, the proposed method can accurately predict residential electricity consumption and can serve as a valuable reference for researchers and utility managers when formulating energy reduction policies and comparing the effectiveness of different strategies.",
        "DOI": "10.1016/j.enbuild.2023.113678",
        "affiliation_name": "Shahid Beheshti University",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "Advancing COP26 climate goals: Leveraging energy innovation, governance readiness, and socio-economic factors for enhanced climate resilience and sustainability",
        "paper_author": "Sarkodie S.A.",
        "publication": "Journal of Cleaner Production",
        "citied_by": "8",
        "cover_date": "2023-12-15",
        "Abstract": "Climate change adaptation and mitigation remain critical to achieving sustainable development while reducing climate vulnerability, particularly among climate-exposed and sensitive regions. Yet, achieving a balance between climate-resilience pathways, high economic productivity, high human development, and energy efficiency appears complex, leading to potential trade-offs. Here, we examine the overarching effect of the diversified energy portfolio, socio-economic drivers, and governance adaptation readiness on Climate change vulnerability across 212 economies. Contrary to the poor conventional panel techniques reported in the existing literature, we employ novel machine learning and dynamic panel estimation techniques that control for chaos, nonlinearity, mutual coupling, and heterogeneity in dynamic systems. The convergent cross-mapping causality technique reveals mutual coupling effects between energy portfolio, governance readiness, socio-economic drivers, and climate change vulnerability. The rapidly increasing population and increasing demand for resources under the business-as-usual society and economic structure that normalizes unsustainable development pathways due to weak governance structures create ineffective climate-resilient policies that lead to unabated emissions with consequences on climate change. The effect of social and governance readiness leads the transformation process to attain sustainable development. Thus, high social and governance readiness spurs climate resilience through climate change adaptation and mitigation to achieve sustainable development. Alternative (renewables) and nuclear energy have displacement effects on fossil fuels, yet, the magnitude of displacement is not large enough to replace future fossil fuel consumption. Conversely, a low-carbon future is still attainable by replacing the fossil energy portfolio with more natural gas and carbon-abatement technologies. Our study demonstrates that energy innovations are useful climate-resilience pathways that lessen climate change vulnerability.",
        "DOI": "10.1016/j.jclepro.2023.139757",
        "affiliation_name": "Nord Universitet",
        "affiliation_city": "Bodo",
        "affiliation_country": "Norway"
    },
    {
        "paper_title": "A Free and open-source microgrid optimization tool: SAMA the solar alone Multi-Objective Advisor",
        "paper_author": "Ali Sadat S.",
        "publication": "Energy Conversion and Management",
        "citied_by": "8",
        "cover_date": "2023-12-15",
        "Abstract": "Although there are many software tools for simulations of hybrid energy systems, Homer Pro and Homer Grid are the only tools offering optimal sizing of hybrid energy systems. Originally developed as open-source software, they have been closed, which limits user-developers abilities to adapt and improve the software. In addition, all current proprietary offering suffer from the following limitations: i) high costs that limit accessibility to low-resource labs and individuals, ii) incapable of conducting multi-objective optimization, iii) constrained on any new innovations in hybrid energy system design and operation, iv) difficulties for users to specifically define prices and costs in inputs, v) inability to model different electric utility billing structures, vi) complex pricing methodology, and vii) the absence of machine learning-based predictive modeling. To overcome these limitations, this paper introduces and validates SAMA (Solar Alone Multi-objective Advisor), an open-source microgrid optimization software program designed to optimize hybrid energy system sizes economically using metaheuristic algorithms based on specific load profiles and meteorological data. Through rigorous validation exercises, SAMA's outcomes are compared against those generated by the Homer Pro software across distinct climatic conditions and geographical locations (i.e., Sacramento, California in the marine west coast and New Bern in humid continental climate zones of U.S., respectively). The results demonstrate congruence between SAMA and Homer Pro. SAMA can be used for a diverse array of applications from renewable energy systems to off-grid and grid-tied configurations, and from policy formulation to feasibility assessment. Unique features of SAMA include multi-objective optimizer, levelized emission optimization, different pre-defined utility billing structures, and a top-down pricing methodology. These features further enhance SAMA's adaptability, allowing users to tailor the software to their specific needs and system configurations. Finally, the paper anticipates future advancements that could refine component modeling, accelerate optimization processes, and incorporate advanced data analysis techniques such as machine learning to predict energy demand and production patterns.",
        "DOI": "10.1016/j.enconman.2023.117686",
        "affiliation_name": "Ivey Business School",
        "affiliation_city": "London",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Patterns and drivers of carbon stock change in ecological restoration regions: A case study of upper Yangtze River Basin, China",
        "paper_author": "Quan Y.",
        "publication": "Journal of Environmental Management",
        "citied_by": "7",
        "cover_date": "2023-12-15",
        "Abstract": "Balancing ecology and human development has been a long and wide concern. The upper Yangtze River Basin (UYRB) of China has implemented large important ecological restoration projects since the last century. These restoration practices have changed land use patterns within the UYRB, consequently impacting the local carbon cycle. The most noteworthy project is the Grain for Green Program, which returns cropland to natural vegetation (forest and grassland). Yet the effects of restoration on land use change, carbon sequestration, and associated food production remain unclear. This study utilized remote sensing data and conversion coefficients to analyze the ecological-policy-induced land use changes of the UYRB from 2000 to 2020 and their impacts on terrestrial carbon sequestration. Linear regression, machine learning, and structural equation modeling (SEM) were utilized to evaluate the correlations between environmental and socio-economic factors and the distribution of carbon stocks. The results indicated positive effects of ecological activities on the UYRB, despite decreases in cropland. Over the past 20 years, the UYRB had sequestered carbon by a total amount of 1796 ± 926 Mt C. The spatial distribution of sequestered carbon demonstrated a strong correlation with slopes, followed by temperatures. The SEM results indicated that agricultural production and carbon sequestration were enhanced synergically under land use changes. This further demonstrated the effectiveness of these land policies in achieving a balance between crop productivity and ecology protection. We emphasized the importance of vegetation restoration in achieving carbon neutrality and the necessity to continue these projects. We suggested a more reasonable land management for the future UYRB based on the characteristics of each geographical subregion. This work serves as an example of effective land management to other locations worldwide perusing the harmony of ecological restoration and human development.",
        "DOI": "10.1016/j.jenvman.2023.119376",
        "affiliation_name": "Southwest University",
        "affiliation_city": "Chongqing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Driving style-aware energy management for battery/supercapacitor electric vehicles using deep reinforcement learning",
        "paper_author": "Wu Y.",
        "publication": "Journal of Energy Storage",
        "citied_by": "9",
        "cover_date": "2023-12-15",
        "Abstract": "Driving style can significantly affect the energy consumption, battery lifespan, and driving economy of electric vehicles. In this context, this paper proposes a novel driving style-aware energy management strategy for electric vehicles with battery/supercapacitor hybrid energy storage systems based on deep reinforcement learning. Firstly, a semi-supervised support vector machine-based driving style recognition method is presented to recognize the driving style, where twenty features are extracted from limited labeled velocity/acceleration data and then reduced to six dimensions by locally linear embedding. The six dimension features are used to obtain accurate recognition results. Then a proximal policy optimization-based energy management strategy is proposed with the driving style as an additional input state, to optimize the power allocation and minimize the battery capacity loss cost. Extensive results illustrate the effectiveness of the proposed methods, e.g., the proposed driving style recognition method can recognize the real-time driving style with an accuracy of over 95%. Taking the recognized style as input, the proposed driving style-aware energy management strategy can reduce the battery capacity loss cost by 3.30–4.19% and 1.77–8.15%, compared with no driving style and incorrect driving style input energy management methods, respectively.",
        "DOI": "10.1016/j.est.2023.109199",
        "affiliation_name": "Changsha University",
        "affiliation_city": "Changsha",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A novel online learning-based linear quadratic regulator for vanadium redox flow battery in DC microgrids",
        "paper_author": "Liu Y.",
        "publication": "Journal of Power Sources",
        "citied_by": "1",
        "cover_date": "2023-12-15",
        "Abstract": "This paper proposes a novel learning-based linear quadratic regulator (LQR) to overcome the long-lasting problem of model dependency in the existing vanadium redox flow battery (VRB) control approaches. Compared to the conventional model-dependent control methods, such as PI control and model predictive control (MPC), the proposed method automatically updates the optimal control policy through the online learning mechanism without any knowledge of the VRB system dynamics. The ability of the proposed method to handle uncertainties is verified by simulations under various scenarios.",
        "DOI": "10.1016/j.jpowsour.2023.233672",
        "affiliation_name": "The University of Western Australia",
        "affiliation_city": "Perth",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Closed-Loop Soft Robot Control Frameworks with Coordinated Policies Based on Reinforcement Learning and Proprioceptive Self-Sensing",
        "paper_author": "Ju H.",
        "publication": "Advanced Functional Materials",
        "citied_by": "4",
        "cover_date": "2023-12-15",
        "Abstract": "Recent advances in soft robots have been achieved by using compliant materials and exploiting the advantages of the soft structural designs of living organisms. Living organisms (which have theoretically infinite degrees of freedom) are not only mechanically soft but are also capable of smooth harmonic motions, thanks to global coordination and the individual sensing and control of local tissues. Despite improvements in structural designs, few soft robot control frameworks for global object-oriented behaviors are reported. Such a framework will require the use of multiple segments, with local sensing and independent control using coordinated policies. Here, a class of reinforcement learning based control frameworks for soft robots (with high degrees of freedom) is presented, and their ability to conduct global tasks is demonstrated. Coordinated control policies are formulated to control multiple segments with independently controllable embedded actuators, based on localized proprioceptive self-sensing capabilities. The control frameworks are employed to develop soft physical robots. Demonstrations and experiments include the forward and backward locomotion of multichannel soft robotic flatworms. This approach is applicable to multifunctional, high degrees of freedom soft robots, as demonstrated by experiments with light-sensitive locomotion.",
        "DOI": "10.1002/adfm.202304642",
        "affiliation_name": "Gwangju Institute of Science and Technology",
        "affiliation_city": "Gwangju",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "A hybrid machine learning framework for forecasting house price",
        "paper_author": "Zhan C.",
        "publication": "Expert Systems with Applications",
        "citied_by": "13",
        "cover_date": "2023-12-15",
        "Abstract": "House price prediction is one of the most important factors affecting national real estate policies. However, developing an accurate housing price prediction model is a significant challenge for the real estate market. This study presents a framework of house price prediction models that address this issue by improving forecasting performance, explicitly demonstrating the novelty in the Hybrid Bayesian Optimization (HBO) models combined with Stacking (HBOS), Bagging (HBOB), and Transformer (HBOT) techniques. These hybrid models employ Bayesian Optimization for hyperparameter tuning, leading to superior prediction accuracy and stability. Additionally, the proposed framework can assess a statistical and accurate assessment of the predictive performance of house price forecasting models in different scenarios. Furthermore, we constructed a multi-source dataset containing 1,898,175 transactions of the Hong Kong real estate market covering a period from January 2, 1996, to May 13, 2021. This dataset, another major contribution to the field, enables comprehensive model testing and could be a valuable resource for future research. Then, the proposed hybrid models are compared with 18 benchmark models. Thirteen evaluation metrics are used to evaluate the predictive performance, while the non-parametric testing, including Friedman, Iman–Davenport, and Nemenyi post-hoc tests methods, are adopted to assess the significance of differences in the predictive performance of each model. The experimental results show that the HBOS models are superior to the other benchmark models for application in the house price prediction problem. The HBOS-CatBoost model showed superior performance in terms of RMSE compared to both the HBOB-XGBoost and HBOT-ConvLSTM models, with relative RMSE reductions of 5.11% and 25.56%, respectively. The main contributions of this work are the creation of a rich multi-source dataset, the proposal of novel hybrid models for improved house price prediction, and a comprehensive performance evaluation framework. These findings offer a significant step forward in the housing price prediction field.",
        "DOI": "10.1016/j.eswa.2023.120981",
        "affiliation_name": "Nanfang College of Sun Yet-sen University",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Practical machine learning: Forecasting daily financial markets directions",
        "paper_author": "Henrique B.M.",
        "publication": "Expert Systems with Applications",
        "citied_by": "9",
        "cover_date": "2023-12-15",
        "Abstract": "Financial time series prediction has many applications in economics, but producing profitable strategies certainly has a special place among them, a daunting challenge. Statistical and machine learning techniques are intensively researched in the search for a holy grail of stock markets forecasting. However, it is not clear to prospecting researchers how good those popular models are regarding useful predictions on a real scenario. This paper contributes to that discussion, providing decisive evidences contrary to the use of basic out-of-the-box models, specifically Artificial Neural Networks (ANN), Support Vector Machines (SVM), Random Forest (RF) and Naive-Bayes (NB). Results consider optimistic and unreal variables often found in literature, as well as a more close-to-real simulation of the models usage. Specifically, current day closing prices direction forecasting results are contrasted with those on next day forecasts. As expected, when forecasting the current day, accuracy is almost perfect. However, when used to forecast next day closing direction, with a strict data separation policy and without direction or snooping bias, ANN, SVM, RF and NB produce results essentially equal to random guessing. The main achieved result is the demonstration of how a machine learning approach would fare in a support decision system for forecasting short-term future market direction, regardless of the level of market development, considering more than 100 securities in a 10 years period. Consequences for algorithmic trading relate to discouraging usage of the considered models as implemented here. On a more abstract sense, this paper presents more evidence to the Efficient Market Hypothesis (EMH).",
        "DOI": "10.1016/j.eswa.2023.120840",
        "affiliation_name": "Universidade de Brasília",
        "affiliation_city": "Brasilia",
        "affiliation_country": "Brazil"
    },
    {
        "paper_title": "Understanding User Equilibrium States of Road Networks using Big Trajectory Data",
        "paper_author": "Chen B.Y.",
        "publication": "International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",
        "citied_by": "0",
        "cover_date": "2023-12-14",
        "Abstract": "User equilibrium (UE) has long been regarded as the cornerstone of transport planning studies. Despite its fundamental importance, our understanding of the actual UE state of road networks has remained surprisingly incomplete. Using big datasets of taxi trajectories, this study investigates the UE states of road networks in Wuhan. Effective indicators, namely relative gaps, are introduced to quantify how actual traffic states deviate from theoretical UE states. Advanced machine learning techniques, including XGBoost and SHAP values, are employed to analyze nonlinear relationships between network disequilibrium states and seven influencing factors extracted from trajectory data. The results reveal significant gaps between actual traffic states and the theoretical UE states at various times of the day during both weekdays and weekends. The XGBoost analysis shows that differences in travel distances, travel speeds, and signalized intersection numbers among alternative routes are the primary causes of road network disequilibrium. The results of this study could have several important methodological and policy implications for using the UE models in transport applications.",
        "DOI": "10.5194/isprs-archives-XLVIII-1-W2-2023-331-2023",
        "affiliation_name": "Wuhan University",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "UNDERSTANDING THE IMPACTS OF CROP DIVERSIFICATION IN THE CONTEXT OF CLIMATE CHANGE: A MACHINE LEARNING APPROACH",
        "paper_author": "Giannarakis G.",
        "publication": "International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",
        "citied_by": "0",
        "cover_date": "2023-12-14",
        "Abstract": "The concept of sustainable intensification in agriculture necessitates the implementation of management practices that prioritize sustainability without compromising productivity. However, the effects of such practices are known to depend on environmental conditions, and are therefore expected to change as a result of a changing climate. We study the impact of crop diversification on productivity in the context of climate change. We leverage heterogeneous Earth Observation data and contribute a data-driven approach based on causal machine learning for understanding how crop diversification impacts may change in the future. We apply this method to the country of Cyprus throughout a 4-year period. We find that, on average, crop diversification significantly benefited the net primary productivity of crops, increasing it by 2.8%. The effect generally synergized well with higher maximum temperatures and lower soil moistures. In a warmer and more drought-prone climate, we conclude that crop diversification exhibits promising adaptation potential and is thus a sensible policy choice with regards to agricultural productivity for present and future.",
        "DOI": "10.5194/isprs-archives-XLVIII-1-W2-2023-1379-2023",
        "affiliation_name": "Cyprus University of Technology",
        "affiliation_city": "Limassol",
        "affiliation_country": "Cyprus"
    },
    {
        "paper_title": "Classification of Urban Land Use and Land Cover with K-Nearest Neighbour Classifier in the City of Cape Town, South Africa – Cape Flats Case Study",
        "paper_author": "Lefulebe B.E.",
        "publication": "International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",
        "citied_by": "0",
        "cover_date": "2023-12-14",
        "Abstract": "The rapid growth of cities, owing to rural-urban migration and high birth rate has resulted in encroachment of the land use and land cover (LULC) in the Cape Flats, situated north of Cape Town, Western Cape, South Africa. PlanetScope imagery and K-Nearest Neighbour (KNN) classifier are used to map and monitor the following LULC classes within the Cape Flats; waterbodies, trees, built-up, and vegetation, we further do a time series map between 2016 and 2021. The results showed that LULC changes between 2016 to 2021 are as follows: a decrease of 2.1% in vegetation, 0.4% in water bodies, and 11% in trees. Built-up areas, on the other hand, showed a significant increase of 13.6% over five years. The LULC changes in the Cape Flats were mainly triggered by an increase in built-up areas due to household construction to accommodate the increased population resulting from rural-to-urban migration and high birth rate. Classification accuracy from 2016 and 2021 was as follows; overall accuracy of 98.31% and 0.97 kappa coefficient in 2016, while overall accuracy 96.54% and kappa coefficient 0.95 in 2021. A combination of machine learning and high-resolution imagery showcased that high classification results can be achieved in monitoring subtle LULC changes. We recommended that all relevant stakeholders, including government officials and municipalities, formulate and adopt policies to protect against LULC degradation.",
        "DOI": "10.5194/isprs-archives-XLVIII-1-W2-2023-967-2023",
        "affiliation_name": "University of the Free State",
        "affiliation_city": "Bloemfontein",
        "affiliation_country": "South Africa"
    },
    {
        "paper_title": "Can AI deliver advice that is judgement-free for science policy?",
        "paper_author": "Canali S.",
        "publication": "Nature",
        "citied_by": "1",
        "cover_date": "2023-12-14",
        "Abstract": "NA",
        "DOI": "10.1038/d41586-023-03949-9",
        "affiliation_name": "Politecnico di Milano",
        "affiliation_city": "Milan",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "CarbonScaler: Leveraging Cloud Workload Elasticity for Optimizing Carbon-Efficiency",
        "paper_author": "Hanafy W.A.",
        "publication": "Proceedings of the ACM on Measurement and Analysis of Computing Systems",
        "citied_by": "14",
        "cover_date": "2023-12-12",
        "Abstract": "Cloud platforms are increasing their emphasis on sustainability and reducing their operational carbon footprint. A common approach for reducing carbon emissions is to exploit the temporal flexibility inherent to many cloud workloads by executing them in periods with the greenest energy and suspending them at other times. Since such suspend-resume approaches can incur long delays in job completion times, we present a new approach that exploits the elasticity of batch workloads in the cloud to optimize their carbon emissions. Our approach is based on the notion of “carbon scaling,” similar to cloud autoscaling, where a job dynamically varies its server allocation based on fluctuations in the carbon cost of the grid’s energy. We develop a greedy algorithm for minimizing a job’s carbon emissions via carbon scaling that is based on the well-known problem of marginal resource allocation. We implement a CarbonScaler prototype in Kubernetes using its autoscaling capabilities and an analytic tool to guide the carbon-efficient deployment of batch applications in the cloud. We then evaluate CarbonScaler using real-world machine learning training and MPI jobs on a commercial cloud platform and show that it can yield i) 51% carbon savings over carbon-agnostic execution; ii) 37% over a state-of-the-art suspend-resume policy; and iii) 8% over the best static scaling policy.",
        "DOI": "10.1145/3626788",
        "affiliation_name": "University of Massachusetts Amherst",
        "affiliation_city": "Amherst",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Employability challenges and opportunities under STEM learning with a special focus on apprenticeship and internship",
        "paper_author": "Nayak P.",
        "publication": "STEM: A Multi-Disciplinary Approach to Integrate Pedagogies, Inculcate Innovations and Connections",
        "citied_by": "0",
        "cover_date": "2023-12-11",
        "Abstract": "Employability is a big issue with most HR Professionals and Managers agreeing that most fresh graduates, especially from traditional courses like B. Com, B. Sc., BBM, BCA, BSW, and BA, are not competent to begin work even after the induction program. The skills and knowledge they possess are mostly not industry-relevant and very few candidates have the attitude necessary to pick up necessary skills and knowledge efficiently. STEM Learning is almost unavoidable for future work. In India the new National Education Policy provides an opportunity for the University, while planning the pedagogy of all their courses, to take inputs from HR Managers, Industry Representatives, Chamber of Commerce, and MSME Department Personnel. Similarly, the UN, ILO, EU, and various Government bodies are also banking on STEM learning to address both economic growth and employability issues, which if left unresolved may lead to social problems shortly. Globally STEM is seen as the most reliable and sustainable solution to overcome the challenges brought forth by emerging technologies like AI, IoT, 3D Printing, Machine Learning, Robotics, and other beneficial but disruptive technologies (International Labour Organization, 2021; Ministry of Education, 2021). Most employed persons use only part of their STEM learning in their day-to-day work. It is also a fact that different professions and different tasks need different knowledge, skills, and experience in STEM applications. Different technology in the same industry at the same levels of work also creates different needs for STEM application for employees. This is where Internships and Apprenticeships can help individuals and specific industries design and deliver their unique STEM requirements among potential employees. While formal education syllabi are designed generically keeping in mind the overall job market requirements, the specifics KSE (Knowledge, Skill & Experience) can be delivered through Internships.",
        "DOI": "NA",
        "affiliation_name": "Visvesvaraya Technological University",
        "affiliation_city": "Belagavi",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Twitter Sentiment Analysis on Political Tweets",
        "paper_author": "Wyawahare M.",
        "publication": "AIP Conference Proceedings",
        "citied_by": "1",
        "cover_date": "2023-12-11",
        "Abstract": "Twitter is one of the most important social media platforms that is used to keep oneself updated about any political or any news. Twitter generally has a word limit, which also limits the words to be detected in the tweets. Twitter has developed into one of the primary platforms for political discourse and public policy discussions. This paper proposes a Twitter sentiment analyser for political tweets using Machine Learning (ML) and Natural Language Processing (NLP). This paper envisions a model which performs sentiment analysis on tweets discussing a particular political or policy topic from a particular date and performs sentiment analysis on that data. The system is trained with a dataset of 160000 tweets commenting on Indian politics. The system uses NLP for pre-processing and feature extraction. Classification is performed using 3 classifiers namely Random forest, Support Vector Machine (SVM) and Bernoulli Naive Bayes. The highest accuracy obtained was 88.36% using SVM classifier.",
        "DOI": "10.1063/5.0182743",
        "affiliation_name": "Vishwakarma Institute of Technology",
        "affiliation_city": "Pune",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Attacks and Defences for ML-enhanced Access Control",
        "paper_author": "Llamas J.M.",
        "publication": "Middleware Demos, Posters and Doctoral Symposium 2023: Proceedings of the 24th International Middleware Conference Demos, Posters and Doctoral Symposium, Part of: Middleware 2023",
        "citied_by": "0",
        "cover_date": "2023-12-11",
        "Abstract": "As technological systems grow in complexity, the task of managing authorisation and access control within distributed systems becomes increasingly daunting. Machine learning (ML) emerges as a solution capable of adapting to this intricate landscape by drawing insights from historical data and swiftly determining who should be granted access to specific resources. While the incorporation of machine learning into authorisation and access control yields numerous benefits, it also introduces concerns surrounding how to safeguard the integrity of these ML models that are deployed and utilised in a distributed setting. These challenges represent the focal point of this doctoral research endeavour. The primary objective of this study is to delve into the dynamics of attacks and defences within an hybrid access control middleware, which combines conventional rule-based policies with ML-based classifiers. Additionally, this research will explore managerial aspects essential for enabling dynamic and adaptive authorisation measures.",
        "DOI": "10.1145/3626564.3629102",
        "affiliation_name": "KU Leuven",
        "affiliation_city": "Leuven",
        "affiliation_country": "Belgium"
    },
    {
        "paper_title": "Discovering Fatigued Movements for Virtual Character Animation",
        "paper_author": "Cheema N.",
        "publication": "Proceedings - SIGGRAPH Asia 2023 Conference Papers, SA 2023",
        "citied_by": "1",
        "cover_date": "2023-12-10",
        "Abstract": "Virtual character animation and movement synthesis have advanced rapidly during recent years, especially through a combination of extensive motion capture datasets and machine learning. A remaining challenge is interactively simulating characters that fatigue when performing extended motions, which is indispensable for the realism of generated animations. However, capturing such movements is problematic, as performing movements like backflips with fatigued variations up to exhaustion raises capture cost and risk of injury. Surprisingly, little research has been done on faithful fatigue modeling. To address this, we propose a deep reinforcement learning-based approach, which - for the first time in literature - generates control policies for full-body physically simulated agents aware of cumulative fatigue. For this, we first leverage Generative Adversarial Imitation Learning (GAIL) to learn an expert policy for the skill; Second, we learn a fatigue policy by limiting the generated constant torque bounds based on endurance time to non-linear, state- and time-dependent limits in the joint-actuation space using a Three-Compartment Controller (3CC) model. Our results demonstrate that agents can adapt to different fatigue and rest rates interactively, and discover realistic recovery strategies without the need for any captured data of fatigued movement.",
        "DOI": "10.1145/3610548.3618176",
        "affiliation_name": "Aalto University",
        "affiliation_city": "Espoo",
        "affiliation_country": "Finland"
    },
    {
        "paper_title": "Trends in the occurrence and accumulation of microplastics in urban soil of Nanjing and their policy implications",
        "paper_author": "Zhou Y.",
        "publication": "Science of the Total Environment",
        "citied_by": "4",
        "cover_date": "2023-12-10",
        "Abstract": "Urban soil is an important sink of terrestrial microplastics (MPs), and understanding their distribution over time is essential for effective pollution management. Here, based on soil MP data from Nanjing, a typical megacity in eastern China, this study analyzed MP accumulation trends using decision tree and time series network based on soil attributes, POI (point of interest), and human activity factors such as urban industrial structure, transportation, water use. We also evaluated the impact of plastic policy interventions. In the past 15 years, MPs in urban soil in Nanjing have gradually increased, and highly polluted areas have also grown. From 2010 to 2020, the concentration of MPs in urban soil increased from 326.7 items/kg to 480.9 items/kg, with high pollution areas expanding from only 2.0 km2 (0.7 %) to 48.7 km2 (14.9 %). The accumulation of MPs was also influenced by changing factors due to urbanization. In the early 21st century, residential areas had the largest effect, while in the later period, public passenger transport and domestic water consumption were the dominant factors. The scenarios simulation suggests recent plastic intervention policies have helped alleviate this rate of increase, but MP source management (e.g., laundry fibers, tire wear) still needs improvement. By the proposed method, the past trend of microplastics in urban soil and their relationship with soil properties and human activities can be accurately revealed, which will be helpful for the formulation of countermeasures to mitigate regional soil MP pollution.",
        "DOI": "10.1016/j.scitotenv.2023.166144",
        "affiliation_name": "Nanjing University of Information Science &amp; Technology",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Neural Network-Based Financial Distress Prediction for Listed Companies in China",
        "paper_author": "Kong L.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-08",
        "Abstract": "As the global economy gradually enters the post-pandemic era, the prediction of financial distress in listed companies is of significant importance to investors and regulatory authorities. This study employs financial data from Chinese listed companies, integrating macroeconomic factors and industry policy information, and applies neural network and LassoNet regression to construct predictive models. By analyzing data from a-share listed companies from 2000 to 2018, this research considers multi-dimensional factors including financial ratios, corporate governance, macroeconomics, and industry policies, aiming to enhance the accuracy of corporate financial distress prediction. The expected results suggest that the neural network model incorporating macroeconomic factors and industry policies can effectivelyidentify the significant factors influencing financial distress and improve the performance of financial distress prediction.",
        "DOI": "10.1145/3659211.3659245",
        "affiliation_name": "Liaoning Normal University",
        "affiliation_city": "Dalian",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Research on Intelligent Tourism of Ancient Villages under the Perspective of Artificial Intelligence - Take Guilin Yueling Village for example",
        "paper_author": "Li N.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-08",
        "Abstract": "Combined with the relevant development of artificial intelligence big data, the construction of tourism perception analysis model is discussed, based on the existing government information query platform to establish a big data analysis model, which provides an interface for data aggregation and, data prediction Dynamic streaming data processing in the model, real-time collection and aggregation from the government platform, and combined with machine learning algorithms to propose a recommendation strategy aimed at improving tourism perception, which can help government departments formulate relevant support policies for tourist attractions, and finally, the model performance was evaluated through actual data collection, and the results showed that this model can better analyse and predict the tourism perception of residents.",
        "DOI": "10.1145/3659211.3659321",
        "affiliation_name": "Guilin University of Electronic Technology",
        "affiliation_city": "Guilin",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Improve the efficiency of deep reinforcement learning through semantic exploration guided by natural language",
        "paper_author": "Guo Z.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-08",
        "Abstract": "Reinforcement learning is a powerful technique for learning from trial and error, but it often requires a large number of interactions to achieve good performance. In some domains, such as sparse-reward tasks, an oracle that can provide useful feedback or guidance to the agent during the learning process is really of great importance. However, querying the oracle too frequently may be costly or impractical, and the oracle may not always have a clear answer for every situation. Therefore, we propose a novel method for interacting with the oracle in a selective and efficient way, using a retrieval-based approach. Since the interaction can be modeled as a sequence of templated questions and answers, and that there is a large corpus of previous interactions available. We use a neural network to encode the current state of the agent and the oracle, and retrieve the most relevant question from the corpus to ask the oracle. The agent's policy and value function was updated by the oracle's answer. We evaluate our method on an object manipulation task. Our method can significantly improve the efficiency of RL by reducing the number of interactions needed to reach a certain level of performance, compared to baselines that do not use the oracle or use it in a naive way.",
        "DOI": "10.1145/3638584.3638661",
        "affiliation_name": "Institute of Automation Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A-share Trading Strategy Based on MTL-DDPG",
        "paper_author": "Deng W.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-08",
        "Abstract": "With the development of artificial intelligence, more investors are applying machine learning and deep learning algorithms to financial time series, aiming to improve traditional quantitative trading strategies. However, most of the current research focus on mature institutional-type markets and strategies proposed by the current research lack dynamic adjustment capability, especially in a complex environment with high volatility and strong noise, are difficult to derive a stable and profitable portfolio. In response to the above problems, we fully combine the advantages of artificial intelligence and traditional financial market theory techniques to construct a quantitative trading strategy model MTL-DDPG (Multi Time-scale LSTM Deep Deterministic Policy Gradient) that can obtain excess returns. By conducting back testing and comparing with other classical quantitative trading strategies, we verify that our proposed quantitative trading strategy, MTL-DDPG, achieves good results in the A-share market by obtaining excess returns, and the selected portfolio outperforms the benchmark both in terms of total return and risk management.",
        "DOI": "10.1145/3638584.3638600",
        "affiliation_name": "Beijing University of Technology",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A New Transfer Learning-Based Traffic Classification Algorithm for a Multi-Domain SDN Network",
        "paper_author": "Hoang N.T.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2023-12-07",
        "Abstract": "To enhance the efficiency and resource utilization of a computer network, it is imperative to classify network traffic and implement distinct priority policies. Network traffic classification plays a pivotal role across various domains, including network administration, cybersecurity, and network resource optimization. As encrypted network data undergoes diverse evolution, as evident in datasets from tech giants like Google, Facebook, and YouTube, traditional traffic classification methods have given way to machine learning-based approaches. Given that computer networks are primarily deployed as distributed multi-domain systems, employing machine learning for traffic classification becomes challenging when a new network domain appears with a limited dataset. One potential remedy is to employ transfer learning, allowing knowledge transfer from a pre-trained model in an established domain to a new one. In this paper, we present two contributions. First, a novel algorithm called Multi-class TrAdaBoost-CNN is introduced to tackle the challenge of cross-domain classification in encrypted network services. This algorithm extends the Multi-class TrAdaBoost approach by incorporating a Convolutional Neural Network (CNN) as a weak learner. Secondly, extensive experiments are conducted on two distinct domains characterized by imbalanced data distributions to assess the efficacy of our proposed method. The experimental results clearly demonstrate that our algorithm outperforms the traditional CNN model, achieving remarkable accuracy improvements of up to 16%, even when dealing with extremely limited data.",
        "DOI": "10.1145/3628797.3628804",
        "affiliation_name": "Hanoi University of Civil Engineering",
        "affiliation_city": "Hanoi",
        "affiliation_country": "Viet Nam"
    },
    {
        "paper_title": "ChatGPT one year on: who is using it, how and why?",
        "paper_author": "Ghassemi M.",
        "publication": "Nature",
        "citied_by": "12",
        "cover_date": "2023-12-07",
        "Abstract": "In just a year, ChatGPT has permeated scientific research. Seven scientists reveal what they have learnt about how the chatbot should — and shouldn’t — be used. [Figure not available: see fulltext.].",
        "DOI": "10.1038/d41586-023-03798-6",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Modeling Indirect Greenhouse Gas Emissions Sources from Urban Wastewater Treatment Plants: Integrating Machine Learning Models to Compensate for Sparse Parameters with Abundant Observations",
        "paper_author": "Huang Y.",
        "publication": "Environmental Science and Technology",
        "citied_by": "5",
        "cover_date": "2023-12-05",
        "Abstract": "Electricity consumption and sludge yield (SY) are important indirect greenhouse gas (GHG) emission sources in wastewater treatment plants (WWTPs). Predicting these byproducts is crucial for tailoring technology-related policy decisions. However, it challenges balancing mass balance models and mechanistic models that respectively have limited intervariable nexus representation and excessive requirements on operational parameters. Herein, we propose integrating two machine learning models, namely, gradient boosting tree (GBT) and deep learning (DL), to precisely pointwise model electricity consumption intensity (ECI) and SY for WWTPs in China. Results indicate that GBT and DL are capable of mining massive data to compensate for the lack of available parameters, providing a comprehensive modeling focusing on operation conditions and designed parameters, respectively. The proposed model reveals that lower ECI and SY were associated with higher treated wastewater volumes, more lenient effluent standards, and newer equipment. Moreover, ECI and SY showed different patterns when influent biochemical oxygen demand is above or below 100 mg/L in the anaerobic-anoxic-oxic process. Therefore, managing ECI and SY requires quantifying the coupling relationships between biochemical reactions instead of isolating each variable. Furthermore, the proposed models demonstrate potential economic-related inequalities resulting from synergizing water pollution and GHG emissions management.",
        "DOI": "10.1021/acs.est.3c06482",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "An intellectual design case of compressor airfoils based on reinforcement learning",
        "paper_author": "Xu X.",
        "publication": "Engineering Computations (Swansea, Wales)",
        "citied_by": "1",
        "cover_date": "2023-12-05",
        "Abstract": "Purpose: In general, the existing compressor design methods require abundant knowledge and inspiration. The purpose of this study is to identify an intellectual design optimization method that enables a machine to learn how to design it. Design/methodology/approach: The airfoil design process was solved using the reinforcement learning (RL) method. An intellectual method based on a modified deep deterministic policy gradient (DDPG) algorithm was implemented. The new method was applied to agents to learn the design policy under dynamic constraints. The agents explored the design space with the help of a surrogate model and airfoil parameterization. Findings: The agents successfully learned to design the airfoils. The loss coefficients of a controlled diffusion airfoil improved by 1.25% and 3.23% in the two- and four-dimensional design spaces, respectively. The agents successfully learned to design under various constraints. Additionally, the modified DDPG method was compared with a genetic algorithm optimizer, verifying that the former was one to two orders of magnitude faster in policy searching. The NACA65 airfoil was redesigned to verify the generalization. Originality/value: It is feasible to consider the compressor design as an RL problem. Trained agents can determine and record the design policy and adapt it to different initiations and dynamic constraints. More intelligence is demonstrated than when traditional optimization methods are used. This methodology represents a new, small step toward the intelligent design of compressors.",
        "DOI": "10.1108/EC-07-2022-0502",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Log2Policy: An Approach to Generate Fine-Grained Access Control Rules for Microservices from Scratch",
        "paper_author": "Xu S.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "3",
        "cover_date": "2023-12-04",
        "Abstract": "Microservice application architecture is one of the most widely used service architectures in the industry. To prevent a compromised microservice from abusing other microservices, authorization policy is applied to regulate the access among them. However, configuring access control policy manually is challenging due to the complexity and dynamic nature of microservice applications. In this paper, we present Log2Policy, a novel approach to generate microservice authorization policy based on access logs. Our approach consists of three fundamental techniques: (1) a log-based topological graph generation mechanism that automatically infers the invocation logic among microservices, (2) a machine learning based attributes mining method that extracts the relevant attributes of requests, and (3) a policy upgrade mechanism based on traffic management that can significantly reduce the upgrade time. We have implemented a prototype of Log2Policy on mainstream microservice infrastructures and have evaluated it with several microservice applications. The results show that Log2Policy can generate fine-grained and effective access control rules and upgrade them with negligible overhead.",
        "DOI": "10.1145/3627106.3627137",
        "affiliation_name": "Institute of Information Engineering",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "PSP-Mal: Evading Malware Detection via Prioritized Experience-based Reinforcement Learning with Shapley Prior",
        "paper_author": "Zhan D.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2023-12-04",
        "Abstract": "With the widespread application of machine learning techniques in malware detection, researchers have proposed various adversarial attack methods to generate adversarial examples (AEs) of malware, thereby evading detection. Previous studies have shown that the reinforcement learning (RL) framework can enable black-box attacks by performing a sequence of function-preserving operations, which produces functional evasive malware samples. However, it is difficult to obtain the useful guidance and feedbacks from the environment for agent training in the black-box scenario, which results in the RL framework being unable to learn the effective evasion policy. In this paper, we propose the Shapley prior and establish a prior-guidance-based RL framework, namely PSP-Mal, to generate AEs against Portable Executable (PE) malware detectors. Our framework improves on existing methods in three aspects: 1) We explore feature effects of the black-box model by computing Shapley values and further propose the Shapley prior to represent the expected impact of operations. 2) A novel prioritized experience utilization mechanism is established regarding the Shapley prior guidance in the RL framework. 3) The actions are expanded into item-content pairs and we use the Thompson sampling to choose effective content, which helps to reduce randomness and ensure repeatability. We compare the attack performance of our framework with other methods, and experimental results demonstrate that our algorithm is more effective. The evasion rates of PSP-Mal against the LightGBM models trained on EMBER and SOREL-20M reach 76.88% and 72.03%, respectively.",
        "DOI": "10.1145/3627106.3627178",
        "affiliation_name": "Peoples Liberation Army Engineering University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Enhancing family education pattern recognition with a random forest algorithm",
        "paper_author": "Xia J.",
        "publication": "Journal of Intelligent and Fuzzy Systems",
        "citied_by": "0",
        "cover_date": "2023-12-02",
        "Abstract": "It is essential to understand family educational patterns to develop effective educational interventions and policies. Academic success, socioemotional development, and the general well-being of adolescents are all significantly affected by family dynamics and practices. Due to the wide range and variety of familial conditions, it cannot be easy to analyze and recognize these patterns. This paper offers an intelligent strategy for effectively identifying and analyzing family education patterns using the random forest (RF) algorithm. We begin by collecting an extensive data set that includes family factors, educational practices, and student outcomes. The raw data is first preprocessed using min-max normalization. Furthermore, we employ principal component analysis (PCA) to extract the pertinent attributes from the preprocessed data. The best features are chosen using the reptile search optimization (RSO) algorithm to increase the accuracy of the random forest. Comparing the empirical results to state-of-the-art methods demonstrates the suggested RF technique's higher effectiveness in identifying family education patterns. The value of performance metrics such as accuracy (94.7%), precision (95.2%), recall (95.7%), and F1 score (96.1%). This strategy uses the RF to deliver insightful data that may guide targeted interventions, regulations, and individualized procedures to help students and their families succeed in education.",
        "DOI": "10.3233/JIFS-234976",
        "affiliation_name": "Capital Normal University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "(R)evolutions of Thought: Artificial Intelligence and Education Futures",
        "paper_author": "Abrams S.S.",
        "publication": "Teachers College Record",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "As part of the 125th issue of Teachers College Record, this commentary provides an overview of technological innovation with a focus on the emergence of machine learning and artificial intelligence (AI). The presence and use of AI is a pressing contemporary topic in education, raising questions about the information and perspective(s) AI might privilege, as well as the evolving ethical concerns related to the blurred and increasingly indistinguishable boundaries of human and nonhuman entities and practices.",
        "DOI": "10.1177/01614681241230173",
        "affiliation_name": "University of South Africa",
        "affiliation_city": "Pretoria",
        "affiliation_country": "South Africa"
    },
    {
        "paper_title": "Artificial intelligence: pioneering value-based purchasing in healthcare",
        "paper_author": "Al-Hajjar S.",
        "publication": "International Journal of Pediatrics and Adolescent Medicine",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "NA",
        "DOI": "10.4103/ijpam.ijpam_37_24",
        "affiliation_name": "King Faisal Specialist Hospital &amp; Research Centre",
        "affiliation_city": "Riyadh",
        "affiliation_country": "Saudi Arabia"
    },
    {
        "paper_title": "Identifying Key Issues in Climate Change Litigation: A Machine Learning Text Analytic Approach",
        "paper_author": "Raghupathi W.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "As climate change, environmental, social, and governance (ESG), along with sustainability, become increasingly crucial for businesses and society, there is a noticeable scarcity of information and transparency regarding corporate practices. Often, government agency enforcement actions lead to litigation and are ultimately resolved by court decisions. Moreover, in instances when there is perceived inadequacy in government enforcement, citizens frequently turn to the courts for preventive judgments against businesses or agencies. In an effort to shed light on the multifaceted aspects of climate change, we adopted a novel, exploratory approach to analyze climate change-related litigation cases. Utilizing a blend of machine learning-based text analytics, we have extracted key insights from individual case narratives. Our analysis encompassed over four hundred cases from the Westlaw database through various keyword searches. The emergent topics from our case dataset revolved around four critical environmental themes: forest, land, water, and air emissions. Our findings provide insight into the nature and dimensions of climate change and also carry significant policy implications, laying the groundwork for future research in this domain.",
        "DOI": "10.3390/su152316530",
        "affiliation_name": "Brooklyn College",
        "affiliation_city": "New York",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Do Public-Led Housing Site Development Projects Affect Local Housing Prices: A Proposal for a Comprehensive Policy Evaluation Methodology",
        "paper_author": "Kim J.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "As the population in urban areas increases, the importance of adequate public-led development policies for sustainable cities with affordable housing is becoming more highlighted. In this regard, this study aims to determine the effectiveness of public-led urban development policies for sustainable growth in urban areas, specifically measuring the effect of housing site development projects on housing prices. The geographical scope of the study is the project sites and their surrounding areas in South Korea, and the temporal background is from 2006 to 2023. The project sites were subdivided into four groups by using the Self-Organization Map (SOM), a machine-learning-based clustering analysis, to collect characteristics of each region. Then, the impact of the policy and the prediction of the real estate market of each cluster were analyzed by applying the DID and LSTM models, which have recently been proven to show a high validity. The results show that each cluster had different characteristics and effects from the development projects, depending not simply on the location, but on several characteristics, including the level of size, infrastructure installation, input cost, etc. Furthermore, it is expected for future studies that more detailed research should be conducted with larger datasets of the regional characteristics.",
        "DOI": "10.3390/su152316495",
        "affiliation_name": "Konkuk University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "An Analytical Study Predicting Future Conditions and Application Strategies of Concrete Bridge Pavement Based on Pavement Management System Database",
        "paper_author": "Lee J.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "2",
        "cover_date": "2023-12-01",
        "Abstract": "South Korea is implementing various policies to address the aging of infrastructures and improve road infrastructure management. Moreover, numerous research projects aiming at the development of necessary technologies for the proper implementation of these policies are underway. This study specifically aims to overcome existing problems in bridge pavement maintenance, such as the inaccuracy of future condition predictions and the selection of incorrect evaluation indicators. Our goal is to provide a new approach for the improved management of the bridge pavement management system (BPMS). To address the issues of accuracy in future condition prediction and evaluation indicator selection within the existing maintenance system, we utilized particle filtering, a Kalman filter method among machine learning techniques. This method allows for the prediction of future conditions, based on the nonlinearly collected bridge pavement conditions within BPMS. Furthermore, we proposed a systematic bridge pavement management strategy. This strategy utilizes traffic volume (ESALs; equivalent single axle loadings), a factor that can influence the future condition of bridge pavement, in correlation with the future condition predicted through particle filtering within BPMS.",
        "DOI": "10.3390/su152416680",
        "affiliation_name": "Korea Institute of Civil Engineering and Building Technology (KICT)",
        "affiliation_city": "Goyang",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "Advanced Digital Tools for Data-Informed and Performance-Driven Design: A Review of Building Energy Consumption Forecasting Models Based on Machine Learning",
        "paper_author": "Di Stefano A.G.",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "3",
        "cover_date": "2023-12-01",
        "Abstract": "Cities and buildings represent the core of human life, the nexus of economic activity, culture, and growth. Although cities cover less than 10% of the global land area, they are notorious for their substantial energy consumption and consequential carbon dioxide (CO2) emissions. These emissions significantly contribute to reducing the carbon budget available to mitigate the adverse impacts of climate change. In this context, the designers’ role is crucial to the technical and social response to climate change, and providing a new generation of tools and instruments is paramount to guide their decisions towards sustainable buildings and cities. In this regard, data-informed digital tools are a viable solution. These tools efficiently utilise available resources to estimate the energy consumption in buildings, thereby facilitating the formulation of effective urban policies and design optimisation. Furthermore, these data-driven digital tools enhance the application of algorithms across the building industry, empowering designers to make informed decisions, particularly in the early stages of design. This paper presents a comprehensive literature review on artificial intelligence-based tools that support performance-driven design. An exhaustive keyword-driven exploration across diverse bibliographic databases yielded a consolidated dataset used for automated analysis for discerning the prevalent themes, correlations, and structural nuances within the body of literature. The primary findings indicate an increasing emphasis on master plans and neighbourhood-scale simulations. However, it is observed that there is a lack of a streamlined framework integrating these data-driven tools into the design process.",
        "DOI": "10.3390/app132412981",
        "affiliation_name": "Politecnico di Milano",
        "affiliation_city": "Milan",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Multidimensional Data Analysis for Enhancing In-Depth Knowledge on the Characteristics of Science and Technology Parks",
        "paper_author": "Francés O.",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "Featured Application: This work expands the understanding of science and technology parks (STPs) and can also guide STP decision makers by incorporating data science and artificial intelligence tools, as well as guiding policy makers on how to promote innovation. The role played by science and technology parks (STPs) in technology transfer, industrial innovation, and economic growth is examined in this paper. The accurate monitoring of their evolution and impact is hindered by the lack of uniformity in STP models or goals, and the scarcity of high-quality datasets. This work uses existing terminologies, definitions, and core features of STPs to conduct a multidimensional data analysis that explores and evaluates the 21 core features which describe the key internal factors of an STP. The core features are gathered from a reliable and updatable dataset of Spanish STPs. The methodological framework can be replicated for other STP contexts and is based on descriptive techniques and machine-learning tools. The results of the study provide an overview of the general situation of STPs in Spain, validate the existence and characteristics of three types of STPs, and identify the typical features of STPs. Moreover, the prototype STP can be used as a benchmark so that other STPs can identify the features that need to be improved. Finally, this work makes it possible to carry out classifications of STPs, in addition to prediction and decision making for innovation ecosystems.",
        "DOI": "10.3390/app132312595",
        "affiliation_name": "Universitat d'Alacant",
        "affiliation_city": "Alicante",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "The Impact of an Increase in Bank Competition on Export Product Quality: Evidence from China",
        "paper_author": "Tu C.",
        "publication": "Annals of Financial Economics",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "Few studies specifically testified to how increased bank competition affects export product quality. This paper fills this gap in this area by examining whether and how increased bank competition affects export product quality. The core results of our findings are as follows. First, the statistical analysis results show that a series of bank reforms have enhanced competition in the banking industry in China. Second, the theoretical model shows that bank reform positively influences export product quality. Subsequently, employing a panel sample of 77,817 firms that exported 4,787 products (HS-6) to 230 destinations from 2000 to 2006, we adopt a difference-in-differences approach to test the association between a rise in bank competition and the quality of exported products. The baseline results which are robust in different model settings support that an increase in bank competition increases export product quality. The further empirical test shows that increased bank competition upgrades export product quality through the following channels: bank competition increase leads to enterprises' financial stability enhancement which, in turn, leads to R&D investment upgrading which then leads to export product quality enhancement. Finally, the heterogeneity checks documented that the positive effect of bank competition upgrading on export product quality is pronounced for non-SOEs, and for firms in both the growth and mature stages. Moreover, an increase in bank competition makes financial resources more accessible in areas with weak financial infrastructure. Our findings shed light on the policy implications for emerging economies. Our study expands the theoretical framework of bank competition research on export product quality and inspires the development of strategies for exporting enterprises.",
        "DOI": "10.1142/S2010495223500124",
        "affiliation_name": "Zhejiang Sci-Tech University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Logic Separation: Discrete Modelling of Pattern Recognition",
        "paper_author": "Aslanyan L.",
        "publication": "Pattern Recognition and Image Analysis",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "Abstract: Herein, a historical analytical survey of work of “Discrete Modelling of Pattern Recognition” (DM-Lab) research group in Armenia is presented. The group is since 1973, supervised by worldwide recognized scientist Yurii Ivanovich Zhuravlev and lead by his former student Levon Aslanyan. The general start of attention to computational mathematics and computational systems in Armenia is concerned with the times of cybernetics as a research direction, and the names of great scientists and policy makers, such as Andranik Iosifyan and Sergei Mergelyan. In early 1950’s a large number of HighTech and defense related organizations were established in area, and their theoretical, scientific cluster was formed around the Yerevan Research Institute of Mathematical Machines, and Computer Center of Academy of Sciences and Yerevan State University. This was the time for intensive stuff and student exchanges inside the larger country USSR. Rimma Podlovchenko, Rafik Tonoyan, Igor’ Zaslavski, Yuri Shoukourian started teaching at Yerevan State University in 70’s, a number of students were delegated to the recognized cybernetical centers, in Moscow, Kiev, Novosibirsk. And one of the results of these developments was appearance of DM-Lab in Armenia, composed by alumnus of Novosibirsk and Moscow State Universities, led by Levon Aslanyan, and supervised globally by RF Academician Yuri Ivanovich Zhuravlev. Further research and education activities lead to defenses of candidate and doctoral dissertations, in Armenia, and at the council of Computer Center of Academy of Sciences of Russian Federation. The initial stuff of DM-Lab group included Gevorg Tonoyan, Levon Asatryan, Vilik Karakhanyan. Local members of the group were Hasmik Sahakyan, Vladimir Sahakyan, Irina Arsenyan, Levon Kazaryan and large number of young PhD students. Research directions at the DM-Lab were and are related to the pattern recognition theory – to mathematical models of forming and analyzing learning sets, studying their properties such as the class compactness hypothesis, in terms of isoperimetry; to forming the logic of interrelations of classes, in terms of logic separation; setting up new approaches in data mining area, etc. All these studies involve intensive research over the years, addressing topics related to the geometry of n-dimensional unite cube and lattices in general, Boolean function minimization, discrete optimization problems, and algorithmic studies coming from data science and artificial intelligence. International relations and activities of the group includes: long term representation of Armenia in the ISO technical groups, representation of Armenia in ICT research programmes of European Council, membership at the ITHEA virtual research institute with its conferences and publishing house. 10’s of research projects were implemented during these years. Projects were funded by UNDP, NATO Research, INTAS, EC Esprit, IST and Horizone, RFBR, and other international and local sources. 16 candidate and 2 doctoral theses were defended by the group members.",
        "DOI": "10.1134/S1054661823040077",
        "affiliation_name": "National Academy of Sciences of the Republic of Armenia",
        "affiliation_city": "Yerevan",
        "affiliation_country": "Armenia"
    },
    {
        "paper_title": "What Legal Frameworks Should Govern Use of Genetic Test Results by Private Health Insurers in New Zealand?",
        "paper_author": "Janes H.",
        "publication": "Journal of law and medicine",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "The rising cost of private health insurance and constraints within public health systems are global concerns. Genetic testing presents a transformative opportunity for health care to enhance health outcomes and optimise resource allocation through personalised medicine, early diagnosis, targeted treatments, managed care, and improved drug development. However, ethical and policy issues arise, including privacy, discrimination and equitable access to testing. Balancing these against potential health benefits poses a complex challenge. While some advocate for restricting health insurers from using genetic data, others argue that well-regulated private insurance can ensure affordability, improved health outcomes, and innovative care adoption. This article explores examples of improved health outcomes through genetic testing, identifies areas of risk related to insurers' use of genetic data, evaluates the adequacy of New Zealand's legal framework, and emphasises the need for ethical and equitable policy solutions. The broader issues of data governance, biases in algorithms, and implications of artificial intelligence and machine learning warrant separate exploration.",
        "DOI": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Recommendation Systems for e-Shopping: Review of Techniques for Retail and Sustainable Marketing",
        "paper_author": "Stalidis G.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "6",
        "cover_date": "2023-12-01",
        "Abstract": "In recent years, the interest in recommendation systems (RSs) has dramatically increased, as they have become main components of all online stores. The aims of an RS can be multifaceted, related not only to the increase in sales or the convenience of the customer, but may include the promotion of alternative environmentally friendly products or to strengthen policies and campaigns. In addition to accurate suggestions, important aspects of contemporary RSs are therefore to align with the particular marketing goals of the e-shop and with the stances of the targeted audience, ensuring user acceptance, satisfaction, high impact, and achieving sustained usage by customers. The current review focuses on RS related to retail shopping, highlighting recent research efforts towards enhanced e-shops and more efficient sustainable digital marketing and personalized promotion. The reported research was categorized by main approach, key methods, and specialized e-commerce problems addressed, while technological aspects were linked with marketing aspects. The increasing number of papers in the field showed that it has become particularly popular, following the explosive growth in e-commerce and mobile shopping. The problems addressed have expanded beyond the performance of the core algorithms to the business aspects of recommendation, considering user acceptance and impact maximization techniques. Technologies have also shifted from the improvement of classic filtering techniques to complex deep learning architectures, in order to deal with issues such as contextualization, sequence-based methods, and automatic feature extraction from unstructured data. The upcoming goals seem to be even more intelligent recommendations that more precisely adapt not only to users’ explicit needs and hidden desires but also to their personality and sensitivity for more sustainable choices.",
        "DOI": "10.3390/su152316151",
        "affiliation_name": "International Hellenic University",
        "affiliation_city": "Thessaloniki",
        "affiliation_country": "Greece"
    },
    {
        "paper_title": "Construction and Application of Carbon Emission Prediction Model for China蒺s Textile and Garment Industry Based on Improved WOA-LSTM",
        "paper_author": "Shao C.",
        "publication": "Journal of Beijing Institute of Fashion Technology (Natural Science Edition)",
        "citied_by": "7",
        "cover_date": "2023-12-01",
        "Abstract": "Carbon emissions prediction in the textile and garment industry makes the industry爷s carbon reduction policies more practical. In order to predict the industry's carbon emissions more accurately, this paper proposed a prediction model based on Long鄄Short Term Memory neural network (LSTM), which was optimized by an improved Whale Optimization Algorithm (WOA). The machine learning method was introduced to provide a basis for explo鄄ring the industry's carbon reduction path. Firstly, the LSTM model used WOA to optimize the key parameters, and chaotic mapping initialization population and adaptive weights were taken to improve the algorithm. At the same time, an improved WOA-LSTM model was constructed. Secondly, the carbon emissions of the textile and garment industry were calculated from 1990 to 2020, and the STIRPAT model was used to screen the influencing factors of carbon emissions in the industry. Comparative analysis was used to confirm the model's performance, and several scenarios were used to predict the industry's carbon emissions trend. Experiments show that the prediction accuracy is significantly improved, and the MAE, RMSE, and MAPE values for the model test set are 4. 868, 4. 984, and 0. 024, respectively. Meanwhile, the industry will primarily rely on technological innovation to achieve carbon neu鄄trality. This study offers new perspectives to research on carbon emissions prediction in industrial sectors.",
        "DOI": "10.16454/j.cnki.issn.1001-0564.2023.04.010",
        "affiliation_name": "Beijing Institute of Fashion Technology",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Safe reinforcement learning and its applications in robotics: A survey",
        "paper_author": "Zhang C.X.",
        "publication": "Kongzhi Lilun Yu Yingyong/Control Theory and Applications",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "Reinforcement learning is a kind of machine learning method that realizes sequential optimization decisions by interacting with the environment. It has been applied in games, recommendation systems and natural language processing. However, it is still a challenge to ensure the safety of reinforcement learning algorithms when applied to robotics in the real world. In recent years, the safe reinforcement learning methods for robotics systems have become a hot research direction, gaining extensive attention in robotics and reinforcement learning communities. This paper surveys important achievements and development tendency of safe reinforcement learning based on the existing work and focuses on their applicability in robotics. This paper first introduces the general problem description of safe reinforcement learning. Then we focus on the latest significant progress in this field from the perspective of method and performance, including constraint policy optimization, control barrier function, safety filter and adversarial training methods, and their applications in autonomous driving vehicles, unmanned aerial vehicles and other robotic systems. Finally, the future research direction of this field is prospected and discussed.",
        "DOI": "10.7641/CTA.2023.30247",
        "affiliation_name": "National University of Defense Technology China",
        "affiliation_city": "Changsha",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Who's Watching TV?",
        "paper_author": "Clark J.",
        "publication": "Information Systems Research",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "This work addresses the problem of \"user disambiguation\"-estimating the likelihood of each member of a small group using a shared account or device. The main focus is on television set-top box (STB) viewership data in multiperson households, in which it is impossible to tell with certainty which household members watch what. The first main contribution is formulating user disambiguation as a predictive problem. The second contribution is a solution for estimating the likelihood that each individual in a multiperson household watches each TV segment. Kernel theories from the marketing, economics, and sociology literature inform the design of our method. This method learns priors for viewership in single-person households and then adapts them to the specifics of each multiperson household's viewership history. Finally, we formalize two ad hoc heuristics that are currently used in industry (and research) for estimating audience composition of STB data and conduct a comparative analysis using simulated data, real large-scale viewership data, and a fully labeled panel-based data source. We find that our method has superior performance and practical value. The proposed solution has implications for advertisers, researchers who seek better understanding of TV viewership, and anyone using data generated by shared devices or accounts. A major TV provider has deployed this new method for use in its TV ad-targeting system. No personally identifiable information was gathered or used in conducting this study. To the extent any data were analyzed, it was anonymous and/or aggregated data, consistent with the carrier's privacy policy.",
        "DOI": "10.1287/isre.2023.1204",
        "affiliation_name": "Leonard N. Stern School of Business",
        "affiliation_city": "New York",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Blame avoidance and credit-claiming dynamics in government policy communications: evidence from leadership tweets in four OECD countries during the 2020-2022 COVID-19 pandemic",
        "paper_author": "Leong C.",
        "publication": "Policy and Society",
        "citied_by": "6",
        "cover_date": "2023-12-01",
        "Abstract": "Government information activities are often thought to be motivated by a classic calculus of blame minimization and credit maximization. However, the precise interactions of “blame” and “credit” communication activities in government are not well understood, and questions abound about how they are deployed in practice. This paper uses Natural Language Processing (NLP) machine-learning sentiment analysis of a unique dataset composed of several thousand tweets of high-level political leaders in four OECD countries-namely the Prime Ministers of the United Kingdom, Ireland, Australia, and Canada-during 2020-2022 to examine the relationships existing between “blame” and “credit” communication strategies and their relation to the changing severity of the COVID-19 pandemic, both in an objective and subjective sense. In general, the study suggests that during this high-impact, long-lasting, and waxing and waning crisis, political leaders acted in accordance with theoretical expectations when it came to communicating credit seeking messages during the periods when the COVID situation was thought to be improving, but they did not exclusively rely upon communicating blame or scapegoating when the situation was considered to be deteriorating. The consequences of this finding for blame and credit-based theories of government communication are then discussed.",
        "DOI": "10.1093/polsoc/puad029",
        "affiliation_name": "Lee Kuan Yew School of Public Policy",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore"
    },
    {
        "paper_title": "Navigating the Ethical and Privacy Concerns of Big Data and Machine Learning in Decision Making",
        "paper_author": "Taherdoost H.",
        "publication": "Intelligent and Converged Networks",
        "citied_by": "2",
        "cover_date": "2023-12-01",
        "Abstract": "In recent years, the fields of big data and machine learning have gained significant attention for their potential to revolutionize decision-making processes. The vast amounts of data generated by various sources can provide valuable insights to inform decisions across a range of domains, from business and finance to healthcare and social policy. Machine learning algorithms enable computers to learn from data and improve their performance over time, thereby enhancing their ability to make predictions and identify patterns. This article provides a comprehensive overview of how big data and machine learning can improve decision-making processes between 2017-2022. It covers key concepts and techniques involved in these tools, including data collection, data preprocessing, feature selection, model training, and evaluation. The article also discusses the potential benefits and limitations of these tools and explores the ethical and privacy concerns associated with their use. In particular, it highlights the need for transparency and fairness in decision-making algorithms and the importance of protecting individuals' privacy rights. The review concludes by highlighting future research opportunities and challenges in this rapidly evolving field, including the need for more robust and interpretable models, as well as the integration of human decision making with machine learning algorithms. Ultimately, this review aims to provide insights for researchers and practitioners seeking to leverage big data and machine learning to improve decision-making processes in various domains.",
        "DOI": "10.23919/ICN.2023.0023",
        "affiliation_name": "University Canada West",
        "affiliation_city": "Vancouver",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Digitization in teacher education—quality enhancement, status quo, and professionalization approaches",
        "paper_author": "Rubach C.",
        "publication": "Unterrichtswissenschaft",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "This special issue brings together a diverse range of research that reflects the multifaceted landscape within the realm of digitization in teacher education. The contributions in this special issue range from underscoring how machine learning is not just an analytical tool but a beacon for identifying systemic challenges, i.e., dropouts and its impacts, and informing policy decisions within teacher education. They extend to contributions that bring to light the critical evaluation tools now at our disposal, which diagnose pressing issues in teacher education—such as the stagnation in digital competencies among (pre-service) teachers and their underestimation of digital media’s potential. The findings of the present contributions also highlight a crucial point: the need to reshape curricula to embed digital competencies as fundamental pillars of teacher education. Moreover, they reveal the effectiveness of innovative strategies such as reverse mentoring, collaborative peer learning, and leveraging educator role models as catalysts for change. Together, this special issue acts as a compass, directing the course of teacher education to a tech-savvy and responsive future.",
        "DOI": "10.1007/s42010-024-00193-6",
        "affiliation_name": "Eberhard Karls Universität Tübingen",
        "affiliation_city": "Tubingen",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "A Review of Deep Learning Strategies for Enhancing Cybersecurity in Networks",
        "paper_author": "Bhuvaneshwari A.J.",
        "publication": "Journal of Scientific and Industrial Research",
        "citied_by": "3",
        "cover_date": "2023-12-01",
        "Abstract": "Rapid technological improvements have brought significant hazards to sensitive data and information. Cyberspace has connected various data structures, ranging from private communications/transactions to government activities. Cyberattacks are growing more complex which emphasizes the need to improve cybersecurity. Cyber security is more crucial as everything becomes more digital and as the number of connected devices keeps increasing. Cyber security techniques are used to keep networks, applications, and devices safe from intruders. Cloud and IoT technologies have expanded the complexity of computing, communication, and networking infrastructures, making cybercrime prevention more difficult. It takes a long time to develop threat recognition algorithms by the existing methods. Innovative strategies, like employing deep learning tools for cybersecurity, are anticipated to provide a solution to the issue. Deep learning approaches have many benefits which include the ability to solve complex problems quickly, high levels of automation, the best use of informal information, the capacity to generate excellent results at a lower cost, and the ability to recognize complex interactions. A diverse range of applications can be employed in deep learning models to make decisions based on predictions in the daily routine. The significant benefits of deep learning-enabled cyber security have improved security and reduced risks. The intensity of this systematic study provides consolidated knowledge about recent trends and serves as a foundation for future research in Deep learning-enabled Cybersecurity. This paper highlights the potential challenges and current cybersecurity issues with cutting-edge Deep Learning technologies.",
        "DOI": "10.56042/jsir.v82i12.1702",
        "affiliation_name": "SSN College of Engineering, Kalavakkam",
        "affiliation_city": "Kanchipuram",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Intelligent Intrusion Detection System Snort and SVM",
        "paper_author": "El Aeraj O.",
        "publication": "Revue d'Intelligence Artificielle",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "Despite significant advances in IT security, current solutions fail to guarantee protection against malicious threats, often consisting of subtle and potentially damaging variants. To counter these risks, it remains essential to adopt robust security policies and devices such as firewalls and intrusion detection systems. However, these systems have their drawbacks, not least the propensity to generate false positives, leading to erroneous alerts and compromising the overall effectiveness of the security system. Faced with these challenges, an innovative approach was adopted, making use of machine learning, in particular support vector machines (SVM) written in Python programming language, in conjunction with the Snort IDS. This approach exploits the Snort IDS traffic training dataset, identifying attacks such as denial of service using alarm-generating rules. The data is then converted to a usable format and used as input for the machine learning model. This model separates the data into training and test sets in order to evaluate performance, using metrics such as F1 score, precision and recall. The results of this study demonstrate exceptional performance, with a precision rate of 99%, a true positive rate of 162, a false positive rate of 1, a true negative rate of 160 and a false negative rate of zero. These results highlight the robustness of the proposed approach, positioning it favorably in relation compared to other intrusion detection techniques.",
        "DOI": "10.18280/ria.370627",
        "affiliation_name": "Hassan II University of Casablanca",
        "affiliation_city": "Casablanca",
        "affiliation_country": "Morocco"
    },
    {
        "paper_title": "Assessing the Potential of AI–ML in Urban Climate Change Adaptation and Sustainable Development",
        "paper_author": "Srivastava A.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "19",
        "cover_date": "2023-12-01",
        "Abstract": "This study addresses a notable gap in the climate change literature by examining the potential of artificial intelligence and machine learning (AI–ML) in urban climate change adaptation and sustainable development across major global continents. While much attention has been given to mitigation strategies, this study uniquely delves into the AI–ML’s underexplored role in catalyzing climate change adaptation in contemporary and future urban centers. The research thoroughly explores diverse case studies from Africa, Asia, Australasia, Europe, North America, and South America, utilizing a methodological framework involving six-step and five-step models for systematic literature reviews. The findings underscore AI–ML achievements, illuminate challenges, and emphasize the need for context-specific and collaborative approaches. The findings imply that a one-size-fits-all approach is insufficient. Instead, successful adaptation strategies must be intricately linked to the particular characteristics, vulnerabilities, and intricacies of each region. Furthermore, the research underscores the importance of international collaboration, knowledge sharing, and technology transfer to expedite the integration of AI–ML into climate adaptation strategies globally. The study envisions a promising trajectory for AI–ML in the climate adaptation domain, emphasizing the necessity for ongoing research, innovation, and practical AI–ML applications. As climate change remains a defining challenge, this research predicts an increasingly pivotal role for AI–ML in constructing climate-resilient urban centers and promoting sustainable development. Continuous efforts to advance AI–ML technologies, establish robust policy frameworks, and ensure universal access are crucial for harnessing AI–ML’s transformative capabilities to combat climate change consequences.",
        "DOI": "10.3390/su152316461",
        "affiliation_name": "Indian Institute of Technology Kharagpur",
        "affiliation_city": "Kharagpur",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Toxic phytoplankton in eutrophic regional seas: An overview",
        "paper_author": "Karydis M.",
        "publication": "Global Nest Journal",
        "citied_by": "2",
        "cover_date": "2023-12-01",
        "Abstract": "Toxic algal blooms have become a major environmental problem over the last few decades because of their impact on fisheries, leisure activities, national income and human health. It is known that Harmful Algal Blooms (HABs) can occur naturally but the frequency of occurrence as well as their geographical distribution are alarming. HABs, beyond the scientific interest, have been an issue of concern for policy makers due to the high cost for implementing management practices. Unlike other marine environmental problems, the causes of HAB formation are not known so far with certainty and a high degree of uncertainty remains, regarding possible triggering mechanisms. Various factors, apart from nutrient concentrations, seem to be connected with this phenomenon: abundance, presence and absence of phytoplankton species, presence of grazers, weather conditions, seawater temperature and water mass circulation patterns, have already been reported in the scientific literature as potential factors. However, there are strong indications that eutrophic conditions play a paramount role in HABs formation. Machine learning methods, applied over the last few years to predict HAB’s occurrences, have also confirmed the role of nutrients. In the present work, toxic algal blooms in regional seas characterized by eutrophic conditions that is the Mediterranean Sea, the Black Sea, the Baltic Sea, the North Sea, Wider Caribbean Region and the South China Sea are reviewed. Relevant issues including the drivers of eutrophication triggering HAB’s events as well as effects on ecosystem services and socio-economic consequences are also considered.",
        "DOI": "10.30955/gnj.005388",
        "affiliation_name": "University of the Aegean",
        "affiliation_city": "Mytilene",
        "affiliation_country": "Greece"
    },
    {
        "paper_title": "Transfer-Ensemble Learning: A Novel Approach for Mapping Urban Land Use/Cover of the Indian Metropolitans",
        "paper_author": "Barman P.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "Land use and land cover (LULC) classification plays a significant role in the analysis of climate change, evidence-based policies, and urban and regional planning. For example, updated and detailed information on land use in urban areas is highly needed to monitor and evaluate urban development plans. Machine learning (ML) algorithms, and particularly ensemble ML models support transferability and efficiency in mapping land uses. Generalization, model consistency, and efficiency are essential requirements for implementing such algorithms. The transfer-ensemble learning approach is increasingly used due to its efficiency. However, it is rarely investigated for mapping complex urban LULC in Global South cities, such as India. The main objective of this study is to assess the performance of machine and ensemble-transfer learning algorithms to map the LULC of two metropolitan cities of India using Landsat 5 TM, 2011, and DMSP-OLS nightlight, 2013. This study used classical ML algorithms, such as Support Vector Machine-Radial Basis Function (SVM-RBF), SVM-Linear, and Random Forest (RF). A total of 480 samples were collected to classify six LULC types. The samples were split into training and validation sets with a 65:35 ratio for the training, parameter tuning, and validation of the ML algorithms. The result shows that RF has the highest accuracy (94.43%) of individual models, as compared to SVM-RBF (85.07%) and SVM-Linear (91.99%). Overall, the ensemble model-4 produces the highest accuracy (94.84%) compared to other ensemble models for the Kolkata metropolitan area. In transfer learning, the pre-trained ensemble model-4 achieved the highest accuracy (80.75%) compared to other pre-trained ensemble models for Delhi. This study provides innovative guidelines for selecting a robust ML algorithm to map urban LULC at the metropolitan scale to support urban sustainability.",
        "DOI": "10.3390/su152416593",
        "affiliation_name": "Central University of Punjab",
        "affiliation_city": "Bathinda",
        "affiliation_country": "India"
    },
    {
        "paper_title": "A Supply Chain-Oriented Model to Predict Crude Oil Import Prices in South Korea Based on the Hybrid Approach",
        "paper_author": "Jo J.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "2",
        "cover_date": "2023-12-01",
        "Abstract": "Although numerous studies have explored key variables for forecasting crude oil prices, the role of supply chain factors has often been overlooked. In the face of global risks such as COVID-19, the Russia–Ukraine war, and the U.S.–China trade dispute, supply chain management (SCM) has evolved beyond an individual company’s concern. This research investigates the impact of a supply chain-oriented variable on the forecasting of crude oil import prices in South Korea. Our findings reveal that models incorporating the Global Supply Chain Pressure Index (GSCPI) outperform those without it, emphasizing the importance of monitoring supply chain-related variables for stabilizing domestic prices for policy makers. Additionally, we propose a novel hybrid factor-based approach that integrates time series and machine learning models to enhance the prediction performance of oil prices. This endeavor is poised to serve as a foundational step toward developing methodologically sound forecasting models for oil prices, offering valuable insights for policymakers.",
        "DOI": "10.3390/su152416725",
        "affiliation_name": "Korea Maritime Institute",
        "affiliation_city": "Busan",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "A Hybrid Model of Variational Mode Decomposition and Long Short-Term Memory for Next-Hour Wind Speed Forecasting in a Hot Desert Climate",
        "paper_author": "Alkhayat G.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "2",
        "cover_date": "2023-12-01",
        "Abstract": "Advancements in technology, policies, and cost reductions have led to rapid growth in wind power (WP) production. One of the major challenges in wind energy production is the instability of WP generation due to weather changes. Efficient power grid management requires accurate power output forecasting. New wind energy forecasting methods based on deep learning (DL) are delivering competitive performance versus traditional methods, like numerical weather prediction (NWP), statistical models and machine learning (ML) models. This is truer for short-term prediction. Since there is a relationship between methods, climates and forecasting complexity, forecasting methods do not always perform the same depending on the climate and terrain of the data source. This paper presents a novel model that combines the variational mode decomposition (VMD) method with a long short-term memory (LSTM) model for next-hour wind speed (WS) prediction in a hot desert climate, such as the climate in Saudi Arabia. The proposed model performance is compared to two other hybrid models, six DL models and four ML models using different feature sets. Also, the proposed model is tested on data from different climates, Caracas and Toronto. The proposed model showed a forecast skill (FS) between 61% and 74% based on mean absolute error (MAE), 64% and 72% based on root mean square error (RMSE), and 59% and 68% based on mean absolute percentage error (MAPE) for locations in Saudi Arabia.",
        "DOI": "10.3390/su152416759",
        "affiliation_name": "Islamic University of Madinah",
        "affiliation_city": "Medina",
        "affiliation_country": "Saudi Arabia"
    },
    {
        "paper_title": "A landmark environmental law looks ahead",
        "paper_author": "Wible B.",
        "publication": "Science",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "NA",
        "DOI": "10.1126/science.adn3245",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Data-driven service model to profile healthcare needs and optimise the operation of community-based care: a multi-source data analysis using predictive artificial intelligence",
        "paper_author": "Leung E.",
        "publication": "Hong Kong Medical Journal",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "NA",
        "DOI": "10.12809/hkmj235154",
        "affiliation_name": "Chinese University of Hong Kong, Faculty of Medicine",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong"
    },
    {
        "paper_title": "Suicide Risk Assessment and Prevention Tools in the UK: Current Landscape and Future Directions",
        "paper_author": "Arowosegbe A.",
        "publication": "Psychiatry International",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "Suicide is a major global public health problem, with profound implications for individuals, families, and communities. In the United Kingdom (UK), despite efforts to detect and manage suicidal ideas, suicide rates persist, especially among middle-aged men and women, particularly those aged 45 to 54 years. Recent global challenges, such as the COVID-19 pandemic, climate change, conflict, and the environmental crisis, have raised concerns about an increase in suicide rates, particularly among young people. As a result, a population-wide preventive approach based on evidence is imperative to mitigate the projected increase in suicides. To evaluate the effectiveness of suicide prevention strategies, there is a need for an objective and universally accepted risk assessment approach that does not currently exist. This review examines the current landscape of suicide prevention in the United Kingdom and evaluates the strengths and limitations of existing suicide risk assessments tools. The current suicide prevention tools used, including machine learning and mobile applications are discussed. Also, the epidemiological trends in the various regions of the UK, risk factors including age, sex, and socio-economic status are assessed to provide context. Through this discourse, we hope to provide valuable insight for clinicians, researchers, and policy makers about the current landscape of suicide, especially within the United Kingdom, while presenting recommendations regarding areas that require further research and improvement. Accordingly, suicide prevention is and will continue to be a major focus of both the national health service and research in the UK in the strive to reduce the rate of suicide across all regions. Indeed, headways have been made in the use of technology in preventing suicide both locally and globally. However, research should in the future investigate the value of personalized interventions tailored to the various risk factors of suicide and based on appropriate screening and assessment tools.",
        "DOI": "10.3390/psychiatryint4040032",
        "affiliation_name": "UCL Division of Medicine",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Macroeconomic Predictions Using Payments Data and Machine Learning",
        "paper_author": "Chapman J.T.E.",
        "publication": "Forecasting",
        "citied_by": "3",
        "cover_date": "2023-12-01",
        "Abstract": "This paper assesses the usefulness of comprehensive payments data for macroeconomic predictions in Canada. Specifically, we evaluate which type of payments data are useful, when they are useful, why they are useful, and whether machine learning (ML) models enhance their predictive value. We find payments data with a factor model can help improve accuracy up to 25% in predicting GDP, retail, and wholesale sales; and nonlinear ML models can further improve the accuracy up to 20%. Furthermore, we find the retail payments data are more useful than the data from the wholesale system; and they add more value during crisis and at the nowcasting horizon due to the timeliness. The contribution of the payments data and ML models is small and linear during low and normal economic growth periods. However, their contribution is large, asymmetrical, and nonlinear during crises such as COVID-19. Moreover, we propose a cross-validation approach to mitigate overfitting and use tools to overcome interpretability in the ML models to improve their effectiveness for policy use.",
        "DOI": "10.3390/forecast5040036",
        "affiliation_name": "Banque du Canada",
        "affiliation_city": "Ottawa",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Edge-Driven Multi-Agent Reinforcement Learning: A Novel Approach to Ultrasound Breast Tumor Segmentation",
        "paper_author": "Karunanayake N.",
        "publication": "Diagnostics",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "A segmentation model of the ultrasound (US) images of breast tumors based on virtual agents trained using reinforcement learning (RL) is proposed. The agents, living in the edge map, are able to avoid false boundaries, connect broken parts, and finally, accurately delineate the contour of the tumor. The agents move similarly to robots navigating in the unknown environment with the goal of maximizing the rewards. The individual agent does not know the goal of the entire population. However, since the robots communicate, the model is able to understand the global information and fit the irregular boundaries of complicated objects. Combining the RL with a neural network makes it possible to automatically learn and select the local features. In particular, the agents handle the edge leaks and artifacts typical for the US images. The proposed model outperforms 13 state-of-the-art algorithms, including selected deep learning models and their modifications.",
        "DOI": "10.3390/diagnostics13243611",
        "affiliation_name": "King Mongkut's Institute of Technology Ladkrabang",
        "affiliation_city": "Bangkok",
        "affiliation_country": "Thailand"
    },
    {
        "paper_title": "Inclusive and Safe Mobility Needs of Senior Citizens: Implications for Age-Friendly Cities and Communities",
        "paper_author": "Bokolo A.J.",
        "publication": "Urban Science",
        "citied_by": "8",
        "cover_date": "2023-12-01",
        "Abstract": "Municipalities are concerned with addressing social issues such as mobility inclusion and safety by increasing access to transport facilities and services for all groups in society to create equitable and equal access for all citizens. Moreover, the public transportation systems provided in cities have to be inclusive and safe, driven by emerging technologies such as Artificial Intelligence (AI)-based services that provide personalized recommendation to improve mobility inclusion and safety for all citizens in society, especially vulnerable road users such as senior citizens or older people. But at the moment, there are few studies that have investigated how municipalities can provide inclusive and safe public transportation in general and for senior citizens, particularly those aged 65 and above. Therefore, this study aimed to examine how to provide inclusive and safe mobility for senior citizens to improve out-of-home mobility services for senior citizens towards age-friendly cities and communities. Accordingly, a systematic literature review grounded on secondary data was adopted to investigate inclusive and safe mobility needs for senior citizens. The data were collected from previous research and existing documents, and a descriptive data analysis was carried out to provide insights on urban transportation policies related to senior citizens. Furthermore, case studies were adopted to present polices and strategies employed in Norway, Canada, the United States of America, the United Kingdom, Sweden, and Northern Ireland to identify measures employed to address the public transportation needs of an aging society, focusing on the provision of inclusive and safe mobility to senior citizens. Further findings from this study included the possible use of emerging technologies such as AI-based machine learning for inclusive and safe mobility.",
        "DOI": "10.3390/urbansci7040103",
        "affiliation_name": "HØgskolen i Østfold",
        "affiliation_city": "Halden",
        "affiliation_country": "Norway"
    },
    {
        "paper_title": "Early Detection of Dicamba and 2,4-D Herbicide Drifting Injuries on Soybean with a New Spatial–Spectral Algorithm Based on LeafSpec, an Accurate Touch-Based Hyperspectral Leaf Scanner",
        "paper_author": "Niu Z.",
        "publication": "Remote Sensing",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "In soybeans, off-target damage from the use of dicamba and 2,4-D herbicides for broadleaf weed control can significantly impact sensitive vegetation and crops. The early detection and assessment of such damage are critical for plant diagnostic labs and regulatory agencies to inform regulated usage policies. However, the existing technologies that calculate the average spectrum often struggle to detect and differentiate the damage caused by these herbicides, as they share a similar mode-of-action. In this study, a high-precision spatial and spectral imaging solution was tested for the early detection of dicamba and 2,4-D-induced damage in soybeans. A 2021 study was conducted using LeafSpec, a touch-based hyperspectral leaf scanner, to detect damage on soybean leaves. VIS-NIR (visible–near infrared) hyperspectral images were captured from 180 soybean plants exposed to nine different herbicide treatments at different intervals after spraying. Leaf damage was distinguished as early as 2 h after treatment (HAT) using pairwise partial least squares discriminant analysis (PLS-DA) models based on spectral data. Leaf color distribution, texture, and morphological features were analyzed to separate herbicide dosages. By fully exploiting the spatial and spectral information from high-resolution hyperspectral images, classification accuracy was improved from 57.4% to over 80% for all evaluation dates. This work demonstrates the potential and advantages of using spectral and spatial features of LeafSpec hyperspectral images for the early and accurate detection of herbicide damage in soybean plants.",
        "DOI": "10.3390/rs15245771",
        "affiliation_name": "College of Agriculture",
        "affiliation_city": "West Lafayette",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Sizing with Technical Indicators of Microgrids with Battery Energy Storage Systems: A Systematic Review",
        "paper_author": "Vasconcelos A.",
        "publication": "Energies",
        "citied_by": "2",
        "cover_date": "2023-12-01",
        "Abstract": "Worldwide, governmental organizations are restructuring energy policies, making them cleaner, encouraging transformation and energy transition by integrating renewable sources, engaging in environmental preservation, and, notably, meeting the growing demand for sustainable energy models, such as solar and wind energy. In the electricity sector, reducing carbon emissions is crucial to facilitating the integration of microgrids (MGs) with renewable sources and Battery Energy Storage Systems (BESSs). This work constitutes a systematic review that thoroughly analyzes the sizing of MGs with BESSs. The unpredictability and variability of renewable sources justify the complexity of this analysis and the loads connected to the system. Additionally, the sizing of a BESS depends primarily on the application, battery technology, and the system’s energy demand. This review mapped and identified existing computational and optimization methodologies for structured sizing in technical indicators of an MG with a BESS based on articles published between 2017 and 2021. A protocol was defined in which articles were filtered in multiple stages, undergoing strategic refinements to arrive at the final articles to address the Research Questions (RQs). The final number of articles was 44, and within these, technical indicators related to the RQs were addressed, covering the most relevant works and comparing them technically, including how each explains the objective and result of their work. The rejected articles did not meet the criteria established by the defined protocol, such as exclusion criteria, quality criteria, and RQs. In conclusion, studies employing the integration of machine learning coupled with optimization techniques exhibit a significant contribution to results, as historical data can aid machine learning for data prediction.",
        "DOI": "10.3390/en16248095",
        "affiliation_name": "Universidade de Pernambuco",
        "affiliation_city": "Recife",
        "affiliation_country": "Brazil"
    },
    {
        "paper_title": "Mapping Deprived Urban Areas Using Open Geospatial Data and Machine Learning in Africa",
        "paper_author": "Owusu M.",
        "publication": "Urban Science",
        "citied_by": "6",
        "cover_date": "2023-12-01",
        "Abstract": "Reliable data on slums or deprived living conditions remain scarce in many low- and middle-income countries (LMICs). Global high-resolution maps of deprived areas are fundamental for both research- and evidence-based policies. Existing mapping methods are generally one-off studies that use proprietary commercial data or other physical or socio-economic data that are limited geographically. Open geospatial data are increasingly available for large areas; however, their unstructured nature has hindered their use in extracting useful insights to inform decision making. In this study, we demonstrate an approach to map deprived areas within and across cities using open-source geospatial data. The study tests this methodology in three African cities—Accra (Ghana), Lagos (Nigeria), and Nairobi (Kenya) using a three arc second spatial resolution. Using three machine learning classifiers, (i) models were trained and tested on individual cities to assess the scalability for large area application, (ii) city-to-city comparisons were made to assess how the models performed in new locations, and (iii) a generalized model to assess our ability to map across cities with training samples from each city was designed. Our best models achieved over 80% accuracy in all cities. The study demonstrates an inexpensive, scalable, and transferable approach to map deprived areas that outperforms existing large area methods.",
        "DOI": "10.3390/urbansci7040116",
        "affiliation_name": "Faculty of Geo-Information Science and Earth Observation – ITC",
        "affiliation_city": "Enschede",
        "affiliation_country": "Netherlands"
    },
    {
        "paper_title": "Bicycle Data-Driven Application Framework: A Dutch Case Study on Machine Learning-Based Bicycle Delay Estimation at Signalized Intersections Using Nationwide Sparse GPS Data",
        "paper_author": "Yuan Y.",
        "publication": "Sensors",
        "citied_by": "2",
        "cover_date": "2023-12-01",
        "Abstract": "Data-driven approaches are helpful for quantitative justification and performance evaluation. The Netherlands has made notable strides in establishing a national protocol for bicycle traffic counting and collecting GPS cycling data through initiatives such as the Talking Bikes program. This article addresses the need for a generic framework to harness cycling data and extract relevant insights. Specifically, it focuses on the application of estimating average bicycle delays at signalized intersections, as this is an essential variable in assessing the performance of the transportation system. This study evaluates machine learning (ML)-based approaches using GPS cycling data. The dataset provides comprehensive yet incomplete information regarding one million bicycle rides annually across The Netherlands. These ML models, including random forest, k-nearest neighbor, support vector regression, extreme gradient boosting, and neural networks, are developed to estimate bicycle delays. The study demonstrates the feasibility of estimating bicycle delays using sparse GPS cycling data combined with publicly accessible information, such as weather information and intersection complexity, leveraging the burden of understanding local traffic conditions. It emphasizes the potential of data-driven approaches to inform traffic management, bicycle policy, and infrastructure development.",
        "DOI": "10.3390/s23249664",
        "affiliation_name": "Faculteit Civiele Techniek en Geowetenschappen, TU Delft",
        "affiliation_city": "Delft",
        "affiliation_country": "Netherlands"
    },
    {
        "paper_title": "Joint Task Offloading and Resource Allocation for Intelligent Reflecting Surface-Aided Integrated Sensing and Communication Systems Using Deep Reinforcement Learning Algorithm",
        "paper_author": "Yang L.",
        "publication": "Sensors",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "This paper investigates an intelligent reflecting surface (IRS)-aided integrated sensing and communication (ISAC) framework to cope with the problem of spectrum scarcity and poor wireless environment. The main goal of the proposed framework in this work is to optimize the overall performance of the system, including sensing, communication, and computational offloading. We aim to achieve the trade-off between system performance and overhead by optimizing spectrum and computing resource allocation. On the one hand, the joint design of transmit beamforming and phase shift matrices can enhance the radar sensing quality and increase the communication data rate. On the other hand, task offloading and computation resource allocation optimize energy consumption and delay. Due to the coupled and high dimension optimization variables, the optimization problem is non-convex and NP-hard. Meanwhile, given the dynamic wireless channel condition, we formulate the optimization design as a Markov decision process. To tackle this complex optimization problem, we proposed two innovative deep reinforcement learning (DRL)-based schemes. Specifically, a deep deterministic policy gradient (DDPG) method is proposed to address the continuous high-dimensional action space, and the prioritized experience replay is adopted to speed up the convergence process. Then, a twin delayed DDPG algorithm is designed based on this DRL framework. Numerical results confirm the effectiveness of proposed schemes compared with the benchmark methods.",
        "DOI": "10.3390/s23249896",
        "affiliation_name": "Dublin City University",
        "affiliation_city": "Dublin",
        "affiliation_country": "Ireland"
    },
    {
        "paper_title": "Proximal Policy Optimization-Based Reinforcement Learning and Hybrid Approaches to Explore the Cross Array Task Optimal Solution",
        "paper_author": "Corecco S.",
        "publication": "Machine Learning and Knowledge Extraction",
        "citied_by": "2",
        "cover_date": "2023-12-01",
        "Abstract": "In an era characterised by rapid technological advancement, the application of algorithmic approaches to address complex problems has become crucial across various disciplines. Within the realm of education, there is growing recognition of the pivotal role played by computational thinking (CT). This skill set has emerged as indispensable in our ever-evolving digital landscape, accompanied by an equal need for effective methods to assess and measure these skills. This research places its focus on the Cross Array Task (CAT), an educational activity designed within the Swiss educational system to assess students’ algorithmic skills. Its primary objective is to evaluate pupils’ ability to deconstruct complex problems into manageable steps and systematically formulate sequential strategies. The CAT has proven its effectiveness as an educational tool in tracking and monitoring the development of CT skills throughout compulsory education. Additionally, this task presents an enthralling avenue for algorithmic research, owing to its inherent complexity and the necessity to scrutinise the intricate interplay between different strategies and the structural aspects of this activity. This task, deeply rooted in logical reasoning and intricate problem solving, often poses a substantial challenge for human solvers striving for optimal solutions. Consequently, the exploration of computational power to unearth optimal solutions or uncover less intuitive strategies presents a captivating and promising endeavour. This paper explores two distinct algorithmic approaches to the CAT problem. The first approach combines clustering, random search, and move selection to find optimal solutions. The second approach employs reinforcement learning techniques focusing on the Proximal Policy Optimization (PPO) model. The findings of this research hold the potential to deepen our understanding of how machines can effectively tackle complex challenges like the CAT problem but also have broad implications, particularly in educational contexts, where these approaches can be seamlessly integrated into existing tools as a tutoring mechanism, offering assistance to students encountering difficulties. This can ultimately enhance students’ CT and problem-solving abilities, leading to an enriched educational experience.",
        "DOI": "10.3390/make5040082",
        "affiliation_name": "IDSIA Dalle Molle Institute for Artificial Intelligence",
        "affiliation_city": "Viganello",
        "affiliation_country": "Switzerland"
    },
    {
        "paper_title": "The application of machine learning approaches to determine the predictors of anemia among under five children in Ethiopia",
        "paper_author": "Kebede Kassaw A.",
        "publication": "Scientific Reports",
        "citied_by": "11",
        "cover_date": "2023-12-01",
        "Abstract": "Health professionals need a strong prediction system to reach appropriate disease diagnosis, particularly for under-five child with health problems like anemia. Diagnosis and treatment delay can potentially lead to devastating disease complications resulting in childhood mortality. However, the application of machine learning techniques using a large data set provides scientifically sounded information to solve such palpable critical health and health-related problems. Therefore, this study aimed to determine the predictors of anemia among under-5 year’s age children in Ethiopia using a machine learning approach. A cross-sectional study design was done using the Ethiopian Demographic and Health Survey 2016 data set. A two-stage stratified cluster sampling technique was employed to select the samples. The data analysis was conducted using Statistical Package for Social Sciences/SPSS version 25 and R-software. Data were derived from Ethiopian Demographic and Health Survey. Boruta algorism was applied to select the features and determine the predictors of anemia among under-5 years-old children in Ethiopia. The machine learning algorism showed that number of children, distance to health facilities, health insurance coverage, youngest child’s stool disposal, residence, mothers’ wealth index, type of cooking fuel, number of family members, mothers’ educational status and receiving rotavirus vaccine were the top ten important predictors for anemia among under-five children. Machine-learning algorithm was applied to determine the predictors of anemia among under- 5 year’s age children in Ethiopia. We have identified the determinant factors by conducting a feature importance analysis with the Boruta algorithm. The most significant predictors were number of children, distance to health facility, health insurance coverage, youngest child’s stool disposal, residence, mothers’ wealth index, and type of cooking fuel. Machine learning model plays a paramount role for policy and intervention strategies related to anemia prevention and control among under-five children.",
        "DOI": "10.1038/s41598-023-50128-x",
        "affiliation_name": "Woldia University",
        "affiliation_city": "Woldia",
        "affiliation_country": "Ethiopia"
    },
    {
        "paper_title": "Crop Yield Assessment Using Field-Based Data and Crop Models at the Village Level: A Case Study on a Homogeneous Rice Area in Telangana, India",
        "paper_author": "Mandapati R.",
        "publication": "AgriEngineering",
        "citied_by": "3",
        "cover_date": "2023-12-01",
        "Abstract": "Crop yield estimation has gained importance due to its vital significance for policymakers and decision-makers in enacting schemes, ensuring food security, and assessing crop insurance losses due to biotic and abiotic stress. This research focused on rice yield estimation at the field level in the Karimnagar district of Telangana during 2021 and 2022 by employing the leaf area index (LAI) as the primary criterion for integrating remote sensing technology and crop simulation models. Using Sentinel-2 satellite data, the rice crop was mapped with the help of ground data and machine learning algorithms, attaining an accuracy of 93.04%. Crop management data for the DSSAT tool were collected during the field visits; the model results revealed a 0.80 correlation between observed and predicted yields. Due to its strong correlation with LAI (0.82), the normalized difference vegetation index (NDVI) was selected as the critical element for integration with the model. A spatial LAI map was generated using the linear equation developed between the NDVI and LAI. The relationship between LAI and yield was used to create a spatial yield map. The study’s findings show that assimilating remote sensing data with crop models enhances the precision of rice yield prediction for insurance companies and policy- and decision-makers.",
        "DOI": "10.3390/agriengineering5040117",
        "affiliation_name": "Centurion University of Technology and Management",
        "affiliation_city": "Paralakhemundi",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Comprehensive Growth Index (CGI): A Comprehensive Indicator from UAV-Observed Data for Winter Wheat Growth Status Monitoring",
        "paper_author": "Tang Y.",
        "publication": "Agronomy",
        "citied_by": "2",
        "cover_date": "2023-12-01",
        "Abstract": "Crop growth monitoring plays an important role in estimating the scale of food production and providing a decision-making basis for agricultural policies. Moreover, it can allow understanding of the growth status of crops, seedling conditions, and changes in a timely manner, overcoming the disadvantages of traditional monitoring methods such as low efficiency and inaccuracy. In order to realize rapid and non-destructive monitoring of winter wheat growth status, this study introduced an equal weight method and coefficient of variation method to construct new comprehensive growth indicators based on drone images and measured data obtained from field experiments. The accuracy of the indicators in evaluating the growth of winter wheat can be judged by the construction, and the effects of different machine learning methods on the construction of indicators can be compared. Correlation analysis and variable screening were carried out on the constructed comprehensive growth indicators and the characteristic parameters extracted by the drone, and the comprehensive growth index estimation model was constructed using the selected parameter combination. Among them, when estimating the comprehensive growth index (CGIavg), the optimal model at the jointing stage is the support vector regression (SVR) model: R2 is 0.77, RMSE is 0.095; at the booting stage, the optimal model is the Gaussian process regression (GPR) model: R2 is 0.71, RMSE is 0.098; at the flowering stage, the optimal model is the SVR model: R2 is 0.78, RMSE is 0.087. When estimating the comprehensive growth index based on the coefficient of variation method (CGIcv), the optimal model at the jointing stage is the multi-scale retinex (MSR) model: R2 is 0.73, RMSE is 0.084; at the booting stage, the optimal model is the GPR model: R2 is 0.74, RMSE is 0.092; at the flowering stage, the optimal model is the SVR model, R2 is 0.78: RMSE is 0.085. The conclusion shows that the method of constructing the comprehensive growth index is superior to the function of a single parameter to some extent, providing a new way for wheat growth monitoring and process management.",
        "DOI": "10.3390/agronomy13122883",
        "affiliation_name": "Yangzhou University",
        "affiliation_city": "Yangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "MaxEnt model strategies to studying current and future potential land suitability dynamics of wheat, soybean and rice cultivation under climatic change scenarios in East Asia",
        "paper_author": "Ali S.",
        "publication": "PLoS ONE",
        "citied_by": "9",
        "cover_date": "2023-12-01",
        "Abstract": "Climate change and variability are projected to alter the geographic suitability of lands for crops cultivation. Accurately predicting changes in the potential current and future land suitability distribution dynamics of wheat (Triticum aestivum), soybean (Glycine max) and rice (Oryza sativa) crops due to climate change scenarios is critical to adapting and mitigating the impacts of bioclimatic changes, and plays a significant role in securing food security in East Asia region. This study compiled large datasets of wheat, soybean and rice occurrence locations from GBIF and 19 bioclimatic variables obtained from the WorldClim database that affect crops growth. We recognized potential future suitable distribution regions for crops under the one socioeconomic pathway, (SSP585) for 2021–2040 and 2041–2060, using the MaxEnt model. The accuracy of the MaxEnt was highly significant with mean AUC values ranging from 0.833 to 0.882 for all models evaluated. The jackknife test revealed that for wheat, Bio4 and Bio12 contributed 17.6% and 12.6%, for soybean Bio10 and Bio12 contributed 15.6% and 49.5%, while for rice Bio12 and Bio14 contributed 12.9% and 36.0% to the MaxEnt model. In addition, cultivation aptitude for wheat, soybean, and rice increased in southeast China, North Korea, South Korea, and Japan, while decreasing in Mongolia and northwest China. Climate change is expected to increase the high land suitability for wheat, soybean, and rice in East Asia. Simulation results indicate an average decrease of unsuitable areas of -98.5%, -41.2% and -36.3% for wheat, soybean and rice from 2060 than that of current land suitability. In contrast, the high land suitable for wheat, soybean and rice cultivation is projected to increase by 75.1%, 68.5% and 81.9% from 2060 as compared with current. The findings of this study are of utmost importance in the East Asia region as they present an opportunity for policy makers to develop appropriate adaptation and mitigation strategies required to sustain crops distribution under future climates. Although the risks of wheat, soybean and rice cultivation may be significantly higher in the future because of high temperatures, heat waves, and droughts caused by climate change.",
        "DOI": "10.1371/journal.pone.0296182",
        "affiliation_name": "Hazara University Pakistan",
        "affiliation_city": "Mansehra",
        "affiliation_country": "Pakistan"
    },
    {
        "paper_title": "Prediction model of dental caries in 12-year-old children in Sichuan Province based on machine learning",
        "paper_author": "Yan X.",
        "publication": "Hua Xi Kou Qiang Yi Xue Za Zhi / West China Journal of Stomatology",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "Objective The machine learning algorithm was used to construct a prediction model of children’s dental caries to determine the risk factors of dental caries in children and put forward targeted measures and policy suggestions to improve children’s oral health. Methods Stratified cluster random sampling was adopted in this study. In accordance with different policies and measures in Sichuan Province, 12-year-old students from 3−4 middle schools in eight cities of Sichuan Province were randomly selected for questionnaire survey, oral examination, and physical examination. Multivariate logistic regression analysis of risk factors for dental caries in 12-year-old children was conducted. The dataset was randomly divided into training set and validation set at a ratio of 7∶3. Four machine learning algorithms, including random forest, decision tree, extreme gradient boosting (XGBoost), and Logistic regression, were constructed using R version 4.1.1, and the prediction effects of the four prediction models were evaluated using the area under receiver operating characteristic curve (AUC). Re⁃ sults A total of 4 439 children aged 12 years were included in this study. The incidence of permanent teeth caries was 50.93%. The results of multivariate logistic regression analysis showed that body mass index, highest educational background of the father, highest educational background of the mother, whether to brush teeth, how many times a day, use of toothpaste when brushing teeth, duration of brushing teeth, mouthwash after meals, eating before going to bed after brushing teeth, sweet drinks, snacks, going to dental clinic to examine teeth, and age of brushing teeth were the factors influencing children’s dental caries (P<0.05). The AUC values predicted by random forest, decision tree, Logistic regression, and XGBoost were 0.840, 0.755, 0.799, and 0.794, respectively. In the random forest model, the variable with the highest contribution was eating before bed after brushing. Conclusion A prediction model of dental caries in children was established on the basis of random forest, showing good prediction effect. Taking preventive measures for the main factors affecting the occurrence of dental caries in children is beneficial.",
        "DOI": "10.7518/hxkq.2023.2023124",
        "affiliation_name": "Chengdu Medical College",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A Multi-Agent Reinforcement Learning Method for Omnidirectional Walking of Bipedal Robots",
        "paper_author": "Mou H.",
        "publication": "Biomimetics",
        "citied_by": "2",
        "cover_date": "2023-12-01",
        "Abstract": "Achieving omnidirectional walking for bipedal robots is considered one of the most challenging tasks in robotics technology. Reinforcement learning (RL) methods have proved effective in bipedal walking tasks. However, most existing methods use state machines to switch between multiple policies and achieve omnidirectional gait, which results in shaking during the policy switching process for bipedal robots. To achieve a seamless transition between omnidirectional gait and transient motion for full-size bipedal robots, we propose a novel multi-agent RL method. Firstly, a multi-agent RL algorithm based on the actor–critic framework is designed, and policy entropy is introduced to improve exploration efficiency. By learning agents with parallel initial state distributions, we minimize reliance on gait planner effectiveness in the Robot Operating System (ROS). Additionally, we design a novel heterogeneous policy experience replay mechanism based on Euclidean distance. Secondly, considering the periodicity of bipedal robot walking, we develop a new periodic gait function. Including periodic objectives in the policy can accelerate the convergence speed of training periodic gait functions. Finally, to enhance the robustness of the policy, we construct a novel curriculum learning method by discretizing Gaussian distribution and incorporate it into the robot’s training task. Our method is validated in a simulation environment, and the results show that our method can achieve multiple gaits through a policy network and achieve smooth transitions between different gaits.",
        "DOI": "10.3390/biomimetics8080616",
        "affiliation_name": "Universität Hamburg",
        "affiliation_city": "Hamburg",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Towards improved screening of toxins for Parkinson’s risk",
        "paper_author": "Shan L.",
        "publication": "npj Parkinson's Disease",
        "citied_by": "11",
        "cover_date": "2023-12-01",
        "Abstract": "Parkinson’s disease (PD) is a chronic, progressive and disabling neurodegenerative disorder. The prevalence of PD has risen considerably over the past decades. A growing body of evidence suggest that exposure to environmental toxins, including pesticides, solvents and heavy metals (collectively called toxins), is at least in part responsible for this rapid growth. It is worrying that the current screening procedures being applied internationally to test for possible neurotoxicity of specific compounds offer inadequate insights into the risk of developing PD in humans. Improved screening procedures are therefore urgently needed. Our review first substantiates current evidence on the relation between exposure to environmental toxins and the risk of developing PD. We subsequently propose to replace the current standard toxin screening by a well-controlled multi-tier toxin screening involving the following steps: in silico studies (tier 1) followed by in vitro tests (tier 2), aiming to prioritize agents with human relevant routes of exposure. More in depth studies can be undertaken in tier 3, with whole-organism (in)vertebrate models. Tier 4 has a dedicated focus on cell loss in the substantia nigra and on the presumed mechanisms of neurotoxicity in rodent models, which are required to confirm or refute the possible neurotoxicity of any individual compound. This improved screening procedure should not only evaluate new pesticides that seek access to the market, but also critically assess all pesticides that are being used today, acknowledging that none of these has ever been proven to be safe from a perspective of PD. Importantly, the improved screening procedures should not just assess the neurotoxic risk of isolated compounds, but should also specifically look at the cumulative risk conveyed by exposure to commonly used combinations of pesticides (cocktails). The worldwide implementation of such an improved screening procedure, would be an essential step for policy makers and governments to recognize PD-related environmental risk factors.",
        "DOI": "10.1038/s41531-023-00615-9",
        "affiliation_name": "Donders Institute for Brain, Cognition and Behaviour",
        "affiliation_city": "Nijmegen",
        "affiliation_country": "Netherlands"
    },
    {
        "paper_title": "Toward a Balanced Approach: Bridging the Military, Policy, and Technical Communities",
        "paper_author": "Seraphin A.",
        "publication": "Ethics and International Affairs",
        "citied_by": "3",
        "cover_date": "2023-12-01",
        "Abstract": "The development of new technologies that enable autonomous weapon systems poses a challenge to policymakers and technologists trying to balance military requirements with international obligations and ethical norms. Some have called for new international agreements to restrict or ban lethal autonomous weapon systems. Given the tactical and strategic value of the technologies and the proliferation of threats, the military continues to explore the development of new autonomous technologies to execute national security missions. The rapid global diffusion and dualuse nature of autonomous systems necessitate a proactive approach and a shared understanding of the technical realities, threats, military relevance, and strategic implications of these technologies from these communities. Ultimately, developing AI-enabled defense systems that adhere to global norms and relevant treaty obligations, leverage emerging technologies, and provide operational advantages is possible. The development of a workable and realistic regulatory framework governing the use of lethal autonomous weapons and the artificial intelligence that underpins autonomy will be best supported through a coordinated effort of the regulatory community, technologists, and military to create requirements that reflect the global proliferation and rapidly evolving threat of autonomous weapon systems. This essay seeks to demonstrate that: (1) the lack of coherent dialogue between the technical and policy communities can create security, ethical, and legal dilemmas; and (2) bridging the military, technical, and policy communities can lead to technology with constraints that balance the needs of military, technical, and policy communities. It uses case studies to show why mechanisms are needed to enable early and continuous engagement across the technical, policymaking, and operational communities. The essay then uses twelve interviews with AI and autonomy experts, which provide insight into what the technical and policymaking communities consider fundamental to the progression of responsible autonomous development. It also recommends practical steps for connecting the relevant stakeholders. The goal is to provide the Department of Defense with concrete steps for building organizational structures or processes that create incentives for engagement across communities.",
        "DOI": "10.1017/S0892679423000321",
        "affiliation_name": "Emerging Technologies Institute",
        "affiliation_city": null,
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Revolutionizing Biological Science: The Synergy of Genomics in Health, Bioinformatics, Agriculture, and Artificial Intelligence",
        "paper_author": "Biswas A.",
        "publication": "OMICS A Journal of Integrative Biology",
        "citied_by": "10",
        "cover_date": "2023-12-01",
        "Abstract": "With climate emergency, COVID-19, and the rise of planetary health scholarship, the binary of human and ecosystem health has been deeply challenged. The interdependence of human and nonhuman animal health is increasingly acknowledged and paving the way for new frontiers in integrative biology. The convergence of genomics in health, bioinformatics, agriculture, and artificial intelligence (AI) has ushered in a new era of possibilities and applications. However, the sheer volume of genomic/multiomics big data generated also presents formidable sociotechnical challenges in extracting meaningful biological, planetary health and ecological insights. Over the past few years, AI-guided bioinformatics has emerged as a powerful tool for managing, analyzing, and interpreting complex biological datasets. The advances in AI, particularly in machine learning and deep learning, have been transforming the fields of genomics, planetary health, and agriculture. This article aims to unpack and explore the formidable range of possibilities and challenges that result from such transdisciplinary integration, and emphasizes its radically transformative potential for human and ecosystem health. The integration of these disciplines is also driving significant advancements in precision medicine and personalized health care. This presents an unprecedented opportunity to deepen our understanding of complex biological systems and advance the well-being of all life in planetary ecosystems. Notwithstanding in mind its sociotechnical, ethical, and critical policy challenges, the integration of genomics, multiomics, planetary health, and agriculture with AI-guided bioinformatics opens up vast opportunities for transnational collaborative efforts, data sharing, analysis, valorization, and interdisciplinary innovations in life sciences and integrative biology.",
        "DOI": "10.1089/omi.2023.0197",
        "affiliation_name": "Amity University Jharkhand",
        "affiliation_city": "Ranchi",
        "affiliation_country": "India"
    },
    {
        "paper_title": "From policy to practice: Lessons learned from an open science funding initiative",
        "paper_author": "Dumanis S.B.",
        "publication": "PLoS Computational Biology",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "NA",
        "DOI": "10.1371/journal.pcbi.1011626",
        "affiliation_name": "University of California, Berkeley",
        "affiliation_city": "Berkeley",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Towards understanding policy design through text-as-data approaches: The policy design annotations (POLIANNA) dataset",
        "paper_author": "Sewerin S.",
        "publication": "Scientific Data",
        "citied_by": "5",
        "cover_date": "2023-12-01",
        "Abstract": "Despite the importance of ambitious policy action for addressing climate change, large and systematic assessments of public policies and their design are lacking as analysing text manually is labour-intensive and costly. POLIANNA is a dataset of policy texts from the European Union (EU) that are annotated based on theoretical concepts of policy design, which can be used to develop supervised machine learning approaches for scaling policy analysis. The dataset consists of 20,577 annotated spans, drawn from 18 EU climate change mitigation and renewable energy policies. We developed a novel coding scheme translating existing taxonomies of policy design elements to a method for annotating text spans that consist of one or several words. Here, we provide the coding scheme, a description of the annotated corpus, and an analysis of inter-annotator agreement, and discuss potential applications. As understanding policy texts is still difficult for current text-processing algorithms, we envision this database to be used for building tools that help with manual coding of policy texts by automatically proposing paragraphs containing relevant information.",
        "DOI": "10.1038/s41597-023-02801-z",
        "affiliation_name": "Lee Kuan Yew School of Public Policy",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore"
    },
    {
        "paper_title": "A New Method for Estimating Groundwater Changes Based on Optimized Deep Learning Models—A Case Study of Baiquan Spring Domain in China",
        "paper_author": "Zhang J.",
        "publication": "Water (Switzerland)",
        "citied_by": "2",
        "cover_date": "2023-12-01",
        "Abstract": "Estimating groundwater level (GWL) changes is crucial for the sustainable management of water resources in the face of urbanization and population growth. Existing prediction methods for GWL variations have limitations due to their inability to account for the diverse and irregular patterns of change. This paper introduces an innovative approach to GWL prediction that leverages multisource data and offers a comprehensive analysis of influencing factors. Our methodology goes beyond conventional approaches by incorporating historical GWL data, examining the impacts of precipitation and extraction, as well as considering policy-driven influences, especially in nations like China. The main contribution of this study is the development of a novel hierarchical framework (HGP) for GWL prediction, which progressively integrates correlations among different hierarchical information sources. In our experimental analysis, we make a significant discovery: extraction has a more substantial impact on GWL changes compared to precipitation. Building on this insight, our HGP model demonstrates superior predictive performance when evaluated on real-world datasets. The results show that HGP can increase NSE and R2 scores by 2.8% during the test period compared to the current more accurate deep learning method: ANFIS. This innovative model not only enhances GWL prediction accuracy but also provides valuable insight for effective water resource management. By incorporating multisource data and a novel hierarchical framework, our approach advances the state of the art in GWL prediction, contributing to more sustainable and informed decision making in the context of groundwater resource management.",
        "DOI": "10.3390/w15234129",
        "affiliation_name": "China University of Mining &amp; Technology, Beijing",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A Policy Gradient Algorithm to Alleviate the Multi-Agent Value Overestimation Problem in Complex Environments",
        "paper_author": "Yang Y.",
        "publication": "Sensors",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "Multi-agent reinforcement learning excels at addressing group intelligent decision-making problems involving sequential decision-making. In particular, in complex, high-dimensional state and action spaces, it imposes higher demands on the reliability, stability, and adaptability of decision algorithms. The reinforcement learning algorithm based on the multi-agent deep strategy gradient incorporates a function approximation method using discriminant networks. However, this can lead to estimation errors when agents evaluate action values, thereby reducing model reliability and stability and resulting in challenging convergence. With the increasing complexity of the environment, there is a decline in the quality of experience collected by the experience playback pool, resulting in low efficiency of the sampling stage and difficulties in algorithm convergence. To address these challenges, we propose an innovative approach called the empirical clustering layer-based multi-agent dual dueling policy gradient (ECL-MAD3PG) algorithm. Experimental results demonstrate that our ECL-MAD3PG algorithm outperforms other methods in various complex environments, demonstrating a remarkable 9.1% improvement in mission completion compared to MADDPG within the context of complex UAV cooperative combat scenarios.",
        "DOI": "10.3390/s23239520",
        "affiliation_name": "University of Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A Machine Learning Approach for Investment Analysis in Renewable Energy Sources: A Case Study in Photovoltaic Farms",
        "paper_author": "Ioannou K.",
        "publication": "Energies",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "Farmland offers excellent conditions for developing solar energy while farmers seem to appreciate its notable revenues. The increasing adoption of photovoltaics (PVs) on farmland raises various concerns with the most important being the loss of productive farmland and the increased farmland prices, which may prevent young farmers from entering the farming occupation. The latter can threaten the future of agriculture in countries that are already facing the problem of rural population ageing. The aim of this paper is to examine the effect of crop type on farmers’ willingness to install photovoltaics on their farmland. To that end, this study applies four machine learning (ML) algorithms (categorical regression, decision trees and random forests, support vector machines) on a dataset obtained from a questionnaire survey on farmers in a Greek agricultural area. The results from the application of the algorithms allowed us to quantify and relate farmers’ willingness to invest in PVs with three major crop types (cotton, wheat, sunflower) which play a very important role in food security. Results also provide support for making policy interventions by defining the rate of productive farmland for photovoltaics and also for designing policies to support farmers to start and maintain farming operations.",
        "DOI": "10.3390/en16237735",
        "affiliation_name": "Forest Research Institute",
        "affiliation_city": "Thessaloniki",
        "affiliation_country": "Greece"
    },
    {
        "paper_title": "The allometric relationship between carbon emission and economic development in Yangtze River Delta: fusion of multi-source remote sensing nighttime light data",
        "paper_author": "Xu J.",
        "publication": "Environmental science and pollution research international",
        "citied_by": "3",
        "cover_date": "2023-12-01",
        "Abstract": "Exploring the allometric relationship between carbon emission and economic development can provide guidance for policy-makers who hope to accelerate carbon emission reduction and achieve high-quality development. First, based on the established DMSP/OLS and NPP/VIIRS nighttime light datasets, this study simulated the carbon emissions of the Yangtze River Delta from 2000 to 2020. Second, our research analyzed the spatiotemporal evolution characteristics of carbon emissions. Third, adopting allometric growth model, we explored the allometric relationship between economic development and carbon emissions in Yangtze River Delta. The main conclusions are as follows. First, four prediction models, namely, linear fitting, support vector machine, random forest, and CNN-BiLSTM deep learning, were compared to simulate the accuracy of carbon emissions. Consequently, the CNN-BiLSTM deep learning estimation model presented the best accuracy. Second, both the carbon emissions in YRD as a whole showed an increasing trend, with the largest growth rate appearing in Shanghai and the smallest growth rate occurring in Lishui. Moreover, the high-carbon emission areas were mainly distributed in the core city cluster, which are enclosed by Shanghai, Nanjing, and Hangzhou. Finally, the allometric relationship between economic development and carbon emissions was dominated by one-level negative during the sample period, and the relative growth rate of carbon emissions is lower than that of the economic development, which made the YRD at a basic coordinate stage of weak expansion of economy.",
        "DOI": "10.1007/s11356-023-30692-5",
        "affiliation_name": "Chuzhou University",
        "affiliation_city": "Chuzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Security-aware 5G RAN slice mapping with tiered isolation in physical-layer secured metro-aggregation elastic optical networks using heuristic-assisted DRL",
        "paper_author": "Wang Y.",
        "publication": "Journal of Optical Communications and Networking",
        "citied_by": "2",
        "cover_date": "2023-12-01",
        "Abstract": "The optical transport network (OTN) encryption technology is attractive to solve the physical-layer security in services for the light-path provision process. This paper mainly explores the security-aware 5G radio access network (RAN) slice mapping problem with the tiered isolation (TI) policy, which decides the solution for aggregating service into the physical-layer secured metro-aggregation elastic optical networks (MA-EONs). We first introduce the physical-layer secured OTNs and illustrate their differences from the traditional optical networks. Then, we formulate the 5G RAN slice mapping problem in physical-layer secured MA-EONs as an exact integer linear programming (ILP) model to minimize the average cost (AC), which consists of the number of utilized processing pools (PPs)/general-purpose processors (GPPs)/virtual machines (VMs), and maximum frequency slot index (MFSI) on the light-paths, meanwhile satisfying the given slices latency, isolation, and security requirements. After that, to overcome the non-scalability problem of the ILP model, a heuristic-assisted deep reinforcement learning (HA-DRL) algorithm is proposed to obtain a near-optimal solution for large-scale network scenarios, where the classical shortest path algorithm is employed in the DRL to shrink the size of the exploration space and accelerate the convergence process. Finally, we evaluate the proposed ILP model and HA-DRL algorithm through extensive simulations. Simulation results indicate that our proposed HA-DRL method can find approximate solutions to the ILP model in the small-scale network scenario. Furthermore, the HA-DRL method can also achieve higher resource efficiency compared with benchmark heuristic first-fit algorithms in the large-scale network scenario. In comparison to the first-fit algorithm benchmark, the proposed HA-DRL can achieve up to 9.4% AC reduction in large-scale network scenarios.",
        "DOI": "10.1364/JOCN.499551",
        "affiliation_name": "Purple Mountain Laboratory",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Groundwater Vulnerability in a Megacity Under Climate and Economic Changes: A Coupled Sociohydrological Analysis",
        "paper_author": "Li B.",
        "publication": "Water Resources Research",
        "citied_by": "2",
        "cover_date": "2023-12-01",
        "Abstract": "Groundwater depletion has become increasingly challenging, and many cities worldwide have adopted drastic policies to relieve water stress due to socioeconomic growth. Located on the declining aquifer of the North China Plain, Beijing, for example, has developed plans to limit the size of the city’s population. However, the effect of population displacement under uncertain macroeconomic and climate change remains ambiguous. We adopt a sociohydrological model, with explicit consideration of the dynamics of human-water interactions, to explore the groundwater vulnerability of Beijing. We investigate how human response might shape the development trajectories of the groundwater-population-economy system under different macroscale economic and climate scenarios. Furthermore, we use a machine learning algorithm to identify the decisive factors to be considered for reducing groundwater vulnerability. Our results show that while rapid external economic development or larger annual average precipitation would enable recovery of the groundwater table in the short term, they may slacken human water shortage awareness and result in more acute groundwater depletion in the long run. Strengthening policymaker perceptions of groundwater depletion would prompt timely response policies for controlling population size. Improving the quantity and quality of labor force input to economic development would avoid downturns in the economy due to labor shortages. The outcomes of this study suggest that these strategies would effectively reduce groundwater vulnerability in the long run without causing severe socioeconomic recession. These findings highlight the importance of endogenizing human behavioral dynamics in sustainable urban water management.",
        "DOI": "10.1029/2022WR033943",
        "affiliation_name": "The Grainger College of Engineering",
        "affiliation_city": "Urbana",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Machine learning analysis of government's public risk communication during COVID-19 lockdown in Wuhan, China",
        "paper_author": "Guo C.",
        "publication": "International Journal of Disaster Risk Reduction",
        "citied_by": "3",
        "cover_date": "2023-12-01",
        "Abstract": "The COVID-19 pandemic has caused millions of deaths worldwide since 2020, and has led to sizable health and economic costs. Although COVID-19 cases were first detected in Wuhan, over time the death rate attributable to COVID-19 was one of the lowest globally. Understanding the Chinese government's focuses and strategies can offer insights into early pandemic control in the future. This study aimed to explore the strategies and practices of government risk communication adopted during the COVID-19 lockdown in Wuhan, China; and provide implications for effective health risk communication at the early stage of epidemic response. The 90 government press conference records during the Wuhan lockdown (from January 22, 2020 to May 1, 2020) were divided into three batches and preprocessed. Topic modeling, i.e., the Latent Dirichlet Allocation, was used to computationally extract the topics in each batch. We identified important topics early in the lockdown period, such as “medical team's work”, “assuring supplies for society”, “patients' detection and isolation”; in the middle batch such as “patient treatment and hospitalization”, “enterprises' resumption of work and production”, “epidemic prevention and control”; and later in the lockdown including “policies supporting enterprises”, “ensuring employment”, as well as “blood donation”. We found that communicators from various government sectors provided consistent, concise information during the pandemic. Sectors involved included health, transportation, employment, agriculture, banking, industrial recovery, and resource deployment. Our results suggest that pandemic control requires not only effective public health policies but also collaboration and collective action across diverse societal systems.",
        "DOI": "10.1016/j.ijdrr.2023.104119",
        "affiliation_name": "Guangzhou University",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Multi-Objective Optimization in Air-to-Air Communication System Based on Multi-Agent Deep Reinforcement Learning",
        "paper_author": "Lin S.",
        "publication": "Sensors",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "With the advantages of real-time data processing and flexible deployment, unmanned aerial vehicle (UAV)-assisted mobile edge computing systems are widely used in both civil and military fields. However, due to limited energy, it is usually difficult for UAVs to stay in the air for long periods and to perform computational tasks. In this paper, we propose a full-duplex air-to-air communication system (A2ACS) model combining mobile edge computing and wireless power transfer technologies, aiming to effectively reduce the computational latency and energy consumption of UAVs, while ensuring that the UAVs do not interrupt the mission or leave the work area due to insufficient energy. In this system, UAVs collect energy from external air-edge energy servers (AEESs) to power onboard batteries and offload computational tasks to AEESs to reduce latency. To optimize the system’s performance and balance the four objectives, including the system throughput, the number of low-power alarms of UAVs, the total energy received by UAVs and the energy consumption of AEESs, we develop a multi-objective optimization framework. Considering that AEESs require rapid decision-making in a dynamic environment, an algorithm based on multi-agent deep deterministic policy gradient (MADDPG) is proposed, to optimize the AEESs’ service location and to control the power of energy transfer. While training, the agents learn the optimal policy given the optimization weight conditions. Furthermore, we adopt the K-means algorithm to determine the association between AEESs and UAVs to ensure fairness. Simulated experiment results show that the proposed MODDPG (multi-objective DDPG) algorithm has better performance than the baseline algorithms, such as the genetic algorithm and other deep reinforcement learning algorithms.",
        "DOI": "10.3390/s23239541",
        "affiliation_name": "Beijing University of Technology",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Study on Parameter Inversion Model Construction and Evaluation Method of UAV Hyperspectral Urban Inland Water Pollution Dynamic Monitoring",
        "paper_author": "Chen J.",
        "publication": "Water (Switzerland)",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "The problem of environmental water pollution is becoming increasingly important. Inland rivers and lakes form interconnected water networks with fragile water ecosystems, and urban water pollution problems occur frequently. Chemical oxygen demand (COD), dissolved oxygen (DO), total phosphorus (TP), total nitrogen (TN), and ammonia nitrogen (NH3-N) in inland rivers are important indicators to evaluate water health quality. Timely and accurate reflection of dynamic changes to the key indices of urban river health status are of vital practical significance to adjust water treatment policy and ensure the stability of the aquatic environment and people’s health. This study used COD, DO, TP, TN and NH3-N as typical water quality parameters for a reservoir in Guangxi Province, China and established a set of standardized processes covering UAV hyperspectral sampling and ground spectral correction, spectral data preprocessing, and modeling. In combination with machine learning and statistical analysis, an inversion method for measuring urban inland water pollution from UAV hyperspectral imaging with different dynamic monitoring parameters was proposed. And we compared the different combinations of preprocessing algorithm-regression algorithm and dimensionality reduction algorithm to get a unified model for quantitative estimation of water quality parameter concentration. We evaluated the performance of the proposed model according to root mean square error (RMSE), mean absolute error (MAE), mean absolute percentage error (MAPE), and coefficient of determination ((Formula presented.)). The experimental results showed that our model was superior to other algorithms in RMSE, MAE, MAPE, and (Formula presented.). The MAPE of this model ranged from 0.01 to 0.12 and (Formula presented.) ranged from 0.84 to 0.98 in all water quality parameters. In general, this study provides an effective tool for decision-makers to investigate the source and physical mechanism of water pollution and establish a graded water quality evaluation model.",
        "DOI": "10.3390/w15234131",
        "affiliation_name": "University of Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Intelligent traffic signal controller for heterogeneous traffic using reinforcement learning",
        "paper_author": "R M S.",
        "publication": "Green Energy and Intelligent Transportation",
        "citied_by": "2",
        "cover_date": "2023-12-01",
        "Abstract": "A traffic signal controller is an essential part of a signalized intersection to alleviate congestion and pollution by ensuring safety. However, the available research solutions are focused on homogeneous traffic scenarios, whereas heterogeneous traffic is the reality in most countries. Hence, a traffic signal control scheme suitable for heterogeneous traffic conditions is proposed in the current study using Reinforcement Learning. A novel reward function with an objective to reduce the traffic residual is defined and a combination of exploration and exploitation optimal policy is applied which made the system learn quickly. The proposed scheme can choose the appropriate phase sequence with optimal signal lengths based on traffic demand on each approaching road. The simulation results proved that the proposed model is well-suited for heterogeneous traffic conditions and its performance against the actuated traffic signal controller is significant in reducing the green time wastage and mean waiting time at the intersection.",
        "DOI": "10.1016/j.geits.2023.100124",
        "affiliation_name": "Siddaganga Institute of Technology",
        "affiliation_city": "Tumkur",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Exploring user satisfaction and improvement priorities in electric vehicle segments",
        "paper_author": "Zhao D.",
        "publication": "Transportation Research Part D: Transport and Environment",
        "citied_by": "5",
        "cover_date": "2023-12-01",
        "Abstract": "This study is the first to explore satisfaction and asymmetry in full- and mini-sized electric vehicles (Mini EVs). An integrated framework, SI Gap-ML-IG (Satisfaction Importance Gap-Machine Learning-Importance Grid), is proposed to identify factor attributes, asymmetric effects, and improvement priorities. The SI Gap reveals a large difference between user perception and expectation. Five machine learning algorithms and a traditional linear regression are then compared, and numerical results show that Random Forest is the most suitable method for predicting overall satisfaction and extracting derived importance. Improvements in noise isolation and actual range are shown to be urgently needed for both EV segments. For Mini EVs, three additional factors—safety, comfort, and purchase subsidy—need improving. For full-size EVs, improvements are needed in delivery time, charging queue time, and purchase price. These findings provide new insights and policy implications for the EV industry and government policymakers.",
        "DOI": "10.1016/j.trd.2023.103996",
        "affiliation_name": "Chang'an University",
        "affiliation_city": "Xi'an",
        "affiliation_country": "China"
    },
    {
        "paper_title": "What we pay in the shadows: Labor tax evasion, minimum wage hike and employment",
        "paper_author": "Gavoille N.",
        "publication": "Journal of Public Economics",
        "citied_by": "3",
        "cover_date": "2023-12-01",
        "Abstract": "The interactions between minimum wage policy and tax evasion remain largely unknown. We study the firm-level employment effects of a large and biting minimum wage increase in the context of widespread wage underreporting. We apply machine learning to classify firms as either tax-compliant or tax-evading. We then show that firms engaged in labor tax evasion are insensitive to the minimum wage shock. Our results indicate that these firms use wage underreporting as an adjustment margin, converting part of their formerly undeclared cash payments into official wages. Increasing the minimum wage improves tax enforcement, but comes at the cost of negative employment consequences for compliant firms.",
        "DOI": "10.1016/j.jpubeco.2023.105027",
        "affiliation_name": "Stockholm School of Economics in Riga",
        "affiliation_city": "Riga",
        "affiliation_country": "Latvia"
    },
    {
        "paper_title": "Predicting priority management areas for land use/cover change in the transboundary Okavango basin based on machine learning",
        "paper_author": "Kavhu B.",
        "publication": "Heliyon",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "Remote sensing and modelling of land use/land cover (LULC) change is useful to reveal the extent and spatial patterns of landscape changes at various environments and scales. Predicting susceptibility to LULC change is crucial for policy formulation and land management. However, the use of machine learning (ML) for modelling LULC change is limited. This study modelled LULC change susceptibility in the Okavango basin using ML techniques. Areas with high LULC change susceptibility are termed priority management areas (PMAs) in this study. Trajectories of LULC change between 1996 and 2020 are derived from existing LULC change maps of the Okavango basin. Overlay analysis is then used to detect patches of LULC change transitions. Three LULC transitional categories are adopted for modelling PMAs, namely 1) from natural to anthropogenic classes (Category A); 2) from anthropogenic to natural classes (Category B); and 3) from natural to another natural class (Category C). An ensemble of ML algorithms is calibrated with categories of LULC change and social-ecological drivers of change to produce maps showing the susceptibility of LULC change in the basin. Thereafter, thresholding is done on probability maps of susceptibility to LULC change based on the maximum sum of sensitivity and specificity (max SSS) to delineate PMAs. Results for trajectories of LULC change indicate that anthropogenic activities (croplands, built-up areas, and barelands) generally expanded, displacing natural areas (wetlands, woodlands, water, and shrubland) from 1996 to 2020. Regarding PMAs, anthropogenic-related PMAs (Category A ∼34 560 km2) covered a larger area compared to the natural ones (Categories B∼33 407 km2) and (Categories C∼15 040 km2). The findings of this study emphasize the value of ensemble ML modelling in identifying PMAs and guiding transboundary land use planning. Overall, this study highlights the role of anthropogenic activities in driving land use changes in Transboundary Drainage Basins (TDBs) and suggests a need to promote sustainable practices in predicted PMAs through comprehensive planning to ensure water availability in the Okavango basin.",
        "DOI": "10.1016/j.heliyon.2023.e22762",
        "affiliation_name": "Zimbabwe Parks and Wildlife Management Authority",
        "affiliation_city": "Causeway",
        "affiliation_country": "Zimbabwe"
    },
    {
        "paper_title": "Safe Reinforcement Learning-based Driving Policy Design for Autonomous Vehicles on Highways",
        "paper_author": "Nguyen H.D.",
        "publication": "International Journal of Control, Automation and Systems",
        "citied_by": "6",
        "cover_date": "2023-12-01",
        "Abstract": "Safe decision-making strategy of autonomous vehicles (AVs) plays a critical role in avoiding accidents. This study develops a safe reinforcement learning (safe-RL)-based driving policy for AVs on highways. The hierarchical framework is considered for the proposed safe-RL, where an upper layer executes a safe exploration-exploitation by modifying the exploring process of the ε-greedy algorithm, and a lower layer utilizes a finite state machine (FSM) approach to establish the safe conditions for state transitions. The proposed safe-RL-based driving policy improves the vehicle’s safe driving ability using a Q-table that stores the values corresponding to each action state. Moreover, owing to the trade-off between the ε-greedy values and safe distance threshold, the simulation results demonstrate the superior performance of the proposed approach compared to other alternative RL approaches, such as the ε-greedy Q-learning (GQL) and decaying ε-greedy Q-learning (DGQL), in an uncertain traffic environment. This study’s contributions are twofold: it improves the autonomous vehicle’s exploration-exploitation and safe driving ability while utilizing the advantages of FSM when surrounding cars are inside safe-driving zones, and it analyzes the impact of safe-RL parameters in exploring the environment safely.",
        "DOI": "10.1007/s12555-023-0255-4",
        "affiliation_name": "Technische Universität Wien",
        "affiliation_city": "Vienna",
        "affiliation_country": "Austria"
    },
    {
        "paper_title": "Towards adequate policy enhancement: An AI-driven decision tree model for efficient recognition and classification of EPA status via multi-emission parameters",
        "paper_author": "Awomuti A.",
        "publication": "City and Environment Interactions",
        "citied_by": "5",
        "cover_date": "2023-12-01",
        "Abstract": "Accurate and timely evaluation and assessment of emission data and its impact on environmental status has been a key challenge due to the conventional manual approach utilized for independently computing most emission parameters. To resolve this long-standing issue, we proposed an Artificial Intelligence (AI)-driven Decision Tree model to adequately classify Environmental Protection Agency (EPA) status based on multiple Emission Parameters. The model's performance was systematically evaluated using multiple emission parameters obtained from a two-stroke motorcycle dataset collected in Nigeria across various metrics such as K-S Statistics, Confusion Matrix, Correlation Heat Map, Decision Tree, Validation Curve, and Threshold Plot. The K-S Statistics plot's experimental results showed a considerable correlation between HC, CO, and the target variable, with values ranging from 0.75 to 0.80. At the same time, CO2 and O2 do not correlate with the target variable with values between 0.00 and 0.09. The Confusion Matrix revealed that the proposed model has an overall accuracy of 99.9% with 481 true positive predictions and 75 true negative predictions, indicating the effectiveness of the proposed AI-driven model. In conclusion, our proposed AI-driven model can effectively classify EPA status based on multiple emission parameters with high accuracy, which may spur positive advancement in policy enhancement for proper environmental management.",
        "DOI": "10.1016/j.cacint.2023.100127",
        "affiliation_name": "Key Laboratory of Yangtze River Water Environment Ministry of Education",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A review of hybrid renewable energy systems: Solar and wind-powered solutions: Challenges, opportunities, and policy implications",
        "paper_author": "Hassan Q.",
        "publication": "Results in Engineering",
        "citied_by": "200",
        "cover_date": "2023-12-01",
        "Abstract": "The review comprehensively examines hybrid renewable energy systems that combine solar and wind energy technologies, focusing on their current challenges, opportunities, and policy implications. Despite the individual merits of solar and wind energy systems, their intermittent nature and geographical limitations have spurred interest in hybrid solutions that maximize efficiency and reliability through integrated systems. A critical analysis of available literature indicates that hybrid systems significantly mitigate energy intermittency issues, enhance grid stability, and can be more cost-effective due to shared infrastructure. The review identifies key challenges, such as system optimization, energy storage, and seamless power management, and discusses technological innovations like machine learning algorithms and advanced inverters that hold the potential for overcoming these hurdles. Importantly, the review elucidates the role of policy in accelerating the adoption of these systems by highlighting successful case studies of government incentives, public-private partnerships, and regulatory frameworks that have fostered investments in hybrid renewable energy systems. The study concludes with the outcomes obtained that signify the potential for hybrid renewable energy systems to not only meet but exceed future energy demands sustainably, provided there is concerted effort in research, investment, and policy-making.",
        "DOI": "10.1016/j.rineng.2023.101621",
        "affiliation_name": "Al-Kitab University",
        "affiliation_city": "Kirkuk",
        "affiliation_country": "Iraq"
    },
    {
        "paper_title": "Upholding the Tradition of Excellence of The Journal of Nutrition",
        "paper_author": "Davis T.A.",
        "publication": "Journal of Nutrition",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "NA",
        "DOI": "10.1016/j.tjnut.2023.11.006",
        "affiliation_name": "Baylor College of Medicine",
        "affiliation_city": "Houston",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Corrigendum to ‘Machine learning prediction of hospital patient need for post-acute care using an admission mobility measure is robust across patient diagnoses’ [Health Policy and Technology 12 (2023) 100,754] (Health Policy and Technology (2023) 12(2), (S2211883723000308), (10.1016/j.hlpt.2023.100754))",
        "paper_author": "Young D.L.",
        "publication": "Health Policy and Technology",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "NA",
        "DOI": "10.1016/j.hlpt.2023.100825",
        "affiliation_name": "University of Nevada, Las Vegas",
        "affiliation_city": "Las Vegas",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Association of social vulnerability and influenza vaccination rates for Annual Medicare Enrollees at the county-level in the United States",
        "paper_author": "Tatar M.",
        "publication": "Preventive Medicine",
        "citied_by": "2",
        "cover_date": "2023-12-01",
        "Abstract": "Introduction: Influenza is a preventable acute respiratory illness with a high potential to cause serious complications and is associated with high mortality and morbidity in the US. We aimed to determine the specific community-level vulnerabilities for different race/ethnic communities that are most predictive of influenza vaccination rates. Methods: We conducted a machine learning analysis (XGBoost) to identify community-level social vulnerability features that are predictive of influenza vaccination rates among Medicare enrollees across counties in the US and by race/ethnicity. Results: Population density per square mile in a county is the most important feature in predicting influenza vaccination in a county, followed by unemployment rates and the percentage of mobile homes. The gain relative importance of these features are 11.6%, 9.2%, and 9%, respectively. Among whites, population density (17% gain relative importance) was followed by the percentage of mobile homes (9%) and per capita income (8.7%). For Black/African Americans, the most important features were population density (12.8%), percentage of minorities in the county (8.0%), per capita income (6.9%), and percent of over-occupied housing units (6.8%). Finally, for Hispanics, the top features were per capita income (8.4%), percentage of mobile homes (8.0%), percentage of non-institutionalized persons with a disability (7.9%), and population density (7.6%). Conclusions: Our study may have implications for the success of large vaccination programs in counties with high social vulnerabilities. Further, our findings suggest that policies and interventions seeking to increase rates of vaccination in race/ethnic minority communities may need to be tailored to address their specific socioeconomic vulnerabilities.",
        "DOI": "10.1016/j.ypmed.2023.107782",
        "affiliation_name": "Computer Science Department",
        "affiliation_city": "Logan",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Plot size misperceptions and soil health: A New research agenda",
        "paper_author": "Murphy D.M.A.",
        "publication": "Soil Security",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "Farmer misperceptions of plot size can potentially lead to greater than intended levels of fertilizer use, which can increase the threat of soil fertility decline and non responsiveness of crops to inorganic fertilizer application. In addition, due to diminishing marginal returns of fertilizer use on grain yields, overestimation of plot size can potentially decrease fertilizer profitability. In this study, we use data collected from randomly selected farmers in rural western Kenya that include both the estimated sizes of the agricultural plots as reported by the farmers and their GPS-measured sizes. We find that 65 percent of plots are overestimated by at least 20 percent. We also find that inorganic nitrogen use is strongly increasing in plot size misestimation: a 10% increase in plot misestimation was associated with a 2.6 percent increase in commercial nitrogen application. Next, we build a random forest model to examine the agronomic efficiency of the farmer intended fertilizer use rates vs. actual rates. We find that misestimation of plot sizes is associated with an average decrease in return on investment of 5% (12%) at application rates of 60 kg N/ha (120 kg N/ha). Building on this exercise, we review the literature linking soil health and plot size misperceptions and conclude by proposing a new research agenda to examine how plot size misperceptions affect soil health and farmer well-being, more broadly.",
        "DOI": "10.1016/j.soisec.2023.100117",
        "affiliation_name": "Colgate University",
        "affiliation_city": "Hamilton",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Association of modern sexism with demographic and socioeconomic factors: a machine learning approach",
        "paper_author": "Kyriazos T.",
        "publication": "Social Network Analysis and Mining",
        "citied_by": "4",
        "cover_date": "2023-12-01",
        "Abstract": "This study uses machine learning techniques to explore the relationships between contemporary sexist attitudes and demographic and socioeconomic factors. A total of 1110 Greek adults participated in the study from November 2021 to February 2022, recruited online through undergraduate psychology students using network sampling. The sample comprised 67.48% women and 32.52% men aged 18–80 (M = 29.58, SD = 13.53). Demographic and socioeconomic factors such as age, marital status, whether or not children are present, education, occupation, and income were collected. Nine linear, nonlinear, and nonparametric machine learning models examined the impact of demographics and socioeconomic factors on modern sexism. After data-splitting (train dataset 50%, test dataset 50%), the nine machine learning models were trained to classify the top 33% scorers in the modern sexism scale. The model input variables were only demographics to avoid overlapping of inputs–outputs. A tenfold cross-validation method was then implemented in the training session to select the optimal machine learning model among the nine tested. The ctree algorithm was an optimal classification (Train-accuracy = 0.69, Test-accuracy = 0.71). The analysis revealed that gender, occupation, and having children significantly shaped contemporary sexist attitudes. The study highlights the need for targeted interventions and policies to promote gender equality and challenge harmful stereotypes.",
        "DOI": "10.1007/s13278-023-01164-y",
        "affiliation_name": "Panteion University of Social and Political Sciences",
        "affiliation_city": "Athens",
        "affiliation_country": "Greece"
    },
    {
        "paper_title": "Vulnerability of pangolin SARS-CoV-2 lineage assignment to adversarial attack",
        "paper_author": "Meiseles A.",
        "publication": "Artificial Intelligence in Medicine",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "Pangolin is the most popular tool for SARS-CoV-2 lineage assignment. During COVID-19, healthcare professionals and policymakers required accurate and timely lineage assignment of SARS-CoV-2 genomes for pandemic response. Therefore, tools such as Pangolin use a machine learning model, pangoLEARN, for fast and accurate lineage assignment. Unfortunately, machine learning models are susceptible to adversarial attacks, in which minute changes to the inputs cause substantial changes in the model prediction. We present an attack that uses the pangoLEARN architecture to find perturbations that change the lineage assignment, often with only 2–3 base pair changes. The attacks we carried out show that pangolin is vulnerable to adversarial attack, with success rates between 0.98 and 1 for sequences from non-VoC lineages when pangoLEARN is used for lineage assignment. The attacks we carried out are almost never successful against VoC lineages because pangolin uses Usher and Scorpio – the non-machine-learning alternative methods for VoC lineage assignment. A malicious agent could use the proposed attack to fake or mask outbreaks or circulating lineages. Developers of software in the field of microbial genomics should be aware of the vulnerabilities of machine learning based models and mitigate such risks.",
        "DOI": "10.1016/j.artmed.2023.102722",
        "affiliation_name": "Ben-Gurion University of the Negev",
        "affiliation_city": "Beer-Sheva",
        "affiliation_country": "Israel"
    },
    {
        "paper_title": "Understanding the social determinants of child mortality in Latin America over the last two decades: a machine learning approach",
        "paper_author": "Chivardi C.",
        "publication": "Scientific Reports",
        "citied_by": "2",
        "cover_date": "2023-12-01",
        "Abstract": "The reduction of child mortality rates remains a significant global public health challenge, particularly in regions with high levels of inequality such as Latin America. We used machine learning (ML) algorithms to explore the relationship between social determinants and child under-5 mortality rates (U5MR) in Brazil, Ecuador, and Mexico over two decades. We created a municipal-level cohort from 2000 to 2019 and trained a random forest model (RF) to estimate the relative importance of social determinants in predicting U5MR. We conducted a sensitivity analysis training two more ML models and presenting the mean square error, root mean square error, and median absolute deviation. Our findings indicate that poverty, illiteracy, and the Gini index were the most important variables for predicting U5MR according to the RF. Furthermore, non-linear relationships were found mainly for Gini index and U5MR. Our study suggests that long-term public policies to reduce U5MR in Latin America should focus on reducing poverty, illiteracy, and socioeconomic inequalities. This research provides important insights into the relationships between social determinants and child mortality rates in Latin America. The use of ML algorithms, combined with large longitudinal data, allowed us to evaluate the effects of social determinants on health more carefully than traditional models.",
        "DOI": "10.1038/s41598-023-47994-w",
        "affiliation_name": "Instituto de Salud Global de Barcelona",
        "affiliation_city": "Barcelona",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "Mapping the global free expression landscape using machine learning",
        "paper_author": "Ortega-Martorell S.",
        "publication": "SN Applied Sciences",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "Freedom of expression is a core human right, yet the forces that seek to suppress it have intensified, increasing the need to develop tools that can measure the rates of freedom globally. In this study, we propose a novel freedom of expression index to gain a nuanced and data-led understanding of the level of censorship across the globe. For this, we used an unsupervised, probabilistic machine learning method, to model the status of the free expression landscape. This index seeks to provide legislators and other policymakers, activists and governments, and non-governmental and intergovernmental organisations, with tools to better inform policy or action decisions. The global nature of the proposed index also means it can become a vital resource/tool for engagement with international and supranational bodies.",
        "DOI": "10.1007/s42452-023-05554-x",
        "affiliation_name": "Liverpool John Moores University",
        "affiliation_city": "Liverpool",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Hourly forecasting on PM<inf>2.5</inf> concentrations using a deep neural network with meteorology inputs",
        "paper_author": "Liang Y.",
        "publication": "Environmental Monitoring and Assessment",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "The PM2.5 (particulate matter with a diameter of fewer than 2.5 µm) has become a global topic in environmental science. The neural network that based on the non-linear regression algorithm, e.g., deep learning, is now believed to be one of the most facile and advanced approaches in PM2.5 concentration prediction. In this study, we proposed a PM2.5 predictor using deep learning as infrastructure and meteorological data as input, for predicting the next hour PM2.5 concentration in Beijing Aotizhongxin monitor point. We efficiently use the parameter’s spatiotemporal correlation by concatenating the dataset with time series. The predicted PM2.5 concentration was based on meteorology changes over a period. Therefore, the accuracy would increase with the period growing. By extracting the intrinsic features between meteorological and PM2.5 concentration, a fast and accurate prediction was carried out. The R square score reached maximum of 0.98 and remained an average of 0.9295 in the whole test. The average bias of the model is 9 μg on the validation set and 1 μg on the training set. Moreover, the differences between the predictions and expectations can be further regarded as the estimation for the emission change. Such results can provide scientific advice to supervisory and policy workers.",
        "DOI": "10.1007/s10661-023-12081-0",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Boston",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Atmospheric elemental carbon pollution and its regional health disparities in China",
        "paper_author": "Hang Y.",
        "publication": "Environmental Research Letters",
        "citied_by": "3",
        "cover_date": "2023-12-01",
        "Abstract": "Previous studies have reported that atmospheric elemental carbon (EC) may pose potentially elevated toxicity when compared to total ambient fine particulate matter (PM2.5). However, most research on EC has been conducted in the US and Europe, whereas China experiences significantly higher EC pollution levels. Investigating the health impact of EC exposure in China presents considerable challenges due to the absence of a monitoring network to document long-term EC levels. Despite extensive studies on total PM2.5 in China over the past decade and a significant decrease in its concentration, changes in EC levels and the associated mortality burden remain largely unknown. In our study, we employed a combination of satellite remote sensing, available ground observations, machine learning techniques, and atmospheric big data to predict ground EC concentrations across China for the period 2005-2018, achieving a spatial resolution of 10 km. Our findings reveal that the national average annual mean EC concentration has remained relatively stable since 2005, even as total PM2.5 levels have substantially decreased. Furthermore, we calculated the all-cause non-accidental deaths attributed to long-term EC exposure in China using baseline mortality data and pooled mortality risk from a cohort study. This analysis unveiled significant regional disparities in the mortality burden resulting from long-term EC exposure in China. These variations can be attributed to varying levels of effectiveness in EC regulations across different regions. Specifically, our study highlights that these regulations have been effective in mitigating EC-related health risks in first-tier cities. However, in regions characterized by a highconcentration of coal-power plants and industrial facilities, additional efforts are necessary to control emissions. This observation underscores the importance of tailoring environmental policies and interventions to address the specific challenges posed by varying emission sources and regional contexts.",
        "DOI": "10.1088/1748-9326/ad0862",
        "affiliation_name": "Aerospace Information Research Institute",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Assessing damage to agricultural fields from military actions in Ukraine: An integrated approach using statistical indicators and machine learning",
        "paper_author": "Kussul N.",
        "publication": "International Journal of Applied Earth Observation and Geoinformation",
        "citied_by": "6",
        "cover_date": "2023-12-01",
        "Abstract": "The ongoing full-scale Russian invasion of Ukraine has led to widespread damage of agricultural lands, jeopardizing global food security. Timely detection of impacted fields enables quantification of production losses, guiding recovery policies and monitoring military actions. This study presents a robust methodology to automatically identify agricultural areas damaged by wartime ground activities using free Sentinel-2 satellite data. The 10 m resolution spectral bands and vegetation indices are leveraged, alongside their statistical metrics over time, as inputs to a Random Forest classifier. The algorithm efficiently pinpoints damaged fields, with accuracy metrics around 0.85. Subsequent anomaly detection delineates damages within the fields by combining spectral bands and indices. Applying the methodology over 22 biweekly periods in 2022, approximately 500 thousand ha of cropland across 10 regions of Ukraine were classified as damaged, with the most significant impacts occurring from March to September. The algorithm provides updated damage information despite cloud cover and vegetation shifts. The approach demonstrates the efficacy of automated satellite monitoring to assess agricultural impacts of military actions, supporting recovery analysis and documentation of war crimes.",
        "DOI": "10.1016/j.jag.2023.103562",
        "affiliation_name": "The World Bank, USA",
        "affiliation_city": "Washington, D.C.",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Interpretable Machine Learning Approach to Predicting Electric Vehicle Buying Decisions",
        "paper_author": "Naseri H.",
        "publication": "Transportation Research Record",
        "citied_by": "11",
        "cover_date": "2023-12-01",
        "Abstract": "To address the problem of climate change emissions from the transport sector, many countries are promoting electric vehicles (EVs). To support such efforts, it is essential to know what influences the choice of an EV over a traditional internal combustion engine vehicle (ICEV). To study this, a discrete choice experiment was developed, and 2,015 valid responses were gathered from Canadian adults with a driver’s license. In place of a more traditional analysis, a machine learning approach, XGBoost, was applied. However, two key issues were addressed with respect to its application. First, a practical question related to how best to split the training and testing data was examined. A new technique based on the Coyote optimization algorithm (COA) is developed that automatically determines the split that leads to the greatest prediction accuracy. The policy-relevant results of the analysis found that an individual’s Climate Change-Stage of Change (CC-SoC) and the price ratio of EVs to ICEVs are the most important direct influences. The interaction effect of the first two (CC-SoC and price ratio) is also influential. However, this leads to the second key issue: interpretability. Although high prediction accuracy (87.1%) was achieved, the black-box nature of the approach limits its policy relevance. As such, this research applied a technique, Accumulated Local Effects (ALE), that can determine the strength and direction of influence of the variable. This research demonstrates how machine learning can be applied to a policy-relevant question and provide information that is useful to policy decision makers.",
        "DOI": "10.1177/03611981231169533",
        "affiliation_name": "Concordia Institute for Information Systems Engineering",
        "affiliation_city": "Montreal",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Retraction notice to “An optimization based feature extraction and machine learning techniques for named entity identification” [Optik 272 (2023) 170348] (Optik (2023) 272, (S0030402622016060), (10.1016/j.ijleo.2022.170348))",
        "paper_author": "Govindarajan S.",
        "publication": "Optik",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "This article has been retracted: please see Elsevier Policy on Article Withdrawal (https://www.elsevier.com/locate/withdrawalpolicy). This article has been retracted at the request of the Editor-in-Chief. The journal was alerted to a PubPeer post, which states that this article contains tortured phrases, as detailed here: PubPeer - An optimization based feature extraction and machine learnin. The journal requested that the authors provide an explanation to these concerns, as well as raw data and evidence of collaboration between authors. The authors did not respond. The Editor-in-Chief reviewed the case and decided to retract the article.",
        "DOI": "10.1016/j.ijleo.2023.171433",
        "affiliation_name": "Imam Ja'afar Al-Sadiq University",
        "affiliation_city": "Baghdad",
        "affiliation_country": "Iraq"
    },
    {
        "paper_title": "Robot Learning Incorporating Human Interventions in the Real World for Autonomous Surgical Endoscopic Camera Control",
        "paper_author": "Ou Y.",
        "publication": "Journal of Medical Robotics Research",
        "citied_by": "3",
        "cover_date": "2023-12-01",
        "Abstract": "Recent studies in surgical robotics have focused on automating common surgical subtasks such as grasping and manipulation using deep reinforcement learning (DRL). In this work, we consider surgical endoscopic camera control for object tracking e.g. using the endoscopic camera manipulator (ECM) from the da Vinci Research Kit (dVRK) (Intuitive Inc., Sunnyvale, CA, USA) as a typical surgical robot learning task. A DRL policy for controlling the robot joint space movements is first trained in a simulation environment and then continues the learning in the real world. To speed up training and avoid significant failures (in this case, losing view of the object), human interventions are incorporated into the training process and regular DRL is combined with generative adversarial imitation learning (GAIL) to encourage imitating human behaviors. Experiments show that an average reward of 159.8 can be achieved within 1000 steps compared to only 121.8 without human interventions, and the view of the moving object is lost only twice during the training process out of 3 trials. These results show that human interventions can improve learning speed and significantly reduce failures during the training process.",
        "DOI": "10.1142/S2424905X23400044",
        "affiliation_name": "University of Alberta",
        "affiliation_city": "Edmonton",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Comparative analysis of machine learning and analytical hierarchy analysis for artificial groundwater recharge map development",
        "paper_author": "Al-Ruzouq R.",
        "publication": "Environmental Earth Sciences",
        "citied_by": "3",
        "cover_date": "2023-12-01",
        "Abstract": "A proactive policy for tackling the global water crisis involves the application of artificial groundwater recharge (AGR). AGR site selection is a complex challenge, particularly in large study areas. Considerable research attempted to locate AGR sites using field data collection or conventional decision modeling techniques, such as analytic hierarchy process (AHP). However, the present study utilizes machine learning (ML) techniques with geographic information system (GIS) and remote sensing images to develop a high-efficiency AGR map for the United Arab Emirates. In this study, nine thematic layers were considered: precipitation, drainage density, total dissolved solids, groundwater level, geology, geomorphology, lineament density, elevation and distance from residences. The study applied three ML models, namely support vector machine, multilayer perceptron and random forest (RF), to estimate the relative importance of each thematic layer through feature importance analysis. The AHP approach was also used for comparison. The weights for each thematic layer were determined through a literature review and expert opinions. Results showed that the RF model performed best, with an overall prediction accuracy of 99%. The developed AGR maps were categorized according to their suitability for AGR potential, with approximately 10% of the study area categorized as high. The results of the AHP and RF approaches were relatively similar, indicating that the qualitative approach of AHP was validated by the data-driven approach of RF. The present study presents a framework that can be applied in other climate regions with data availability. This framework can also help environmental agencies and practitioners understand the role of ML in AGR site selection. The results also demonstrate the effectiveness of combining GIS, remote sensing and ML techniques to produce high-efficiency AGR maps.",
        "DOI": "10.1007/s12665-023-11237-y",
        "affiliation_name": "University of Sharjah",
        "affiliation_city": "Sharjah",
        "affiliation_country": "United Arab Emirates"
    },
    {
        "paper_title": "Using Technology to Deliver Cardiovascular Care in African Countries",
        "paper_author": "Mahmoud Z.",
        "publication": "Current Cardiology Reports",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "Purpose of Review: This review aims to explore the applications of digital technology in cardiovascular care across African countries. It highlights the opportunities and challenges associated with leveraging technology to enhance patient self-monitoring, remote patient-clinician interactions, telemedicine, clinician and patient education, and research facilitation. The purpose is to highlight how technology can transform cardiovascular care in Africa. Recent Findings: Recent findings indicate that the increasing penetration of mobile phones and internet connectivity in Africa offers a unique opportunity to improve cardiovascular care. Smartphone-based applications and text messaging services have been employed to promote self-monitoring and lifestyle management, although challenges related to smartphone ownership and digital literacy persist. Remote monitoring of patients by clinicians using home-based devices and wearables shows promise but requires greater accessibility and validation studies in African populations. Telemedicine diagnosis and management of cardiovascular conditions demonstrates significant potential but faces adoption challenges. Investing in targeted clinician and patient education on novel digital technology and devices as well as promoting technology-assisted research for participant recruitment and data collection can facilitate cardiovascular care advancements in Africa. Summary: Technology has the potential to revolutionize cardiovascular care in Africa by improving access, efficiency, and patient outcomes. However, barriers related to limited resources, supportive infrastructure, digital literacy, and access to devices must be addressed. Strategic actions, including investment in digital infrastructure, training programs, community collaboration, and policy advocacy, are crucial to ensuring equitable integration of digital health solutions.",
        "DOI": "10.1007/s11886-023-01988-2",
        "affiliation_name": "Washington University School of Medicine in St. Louis",
        "affiliation_city": "St. Louis",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Bridging the gap: Assessing the effects of railway infrastructure investments in Northwest China",
        "paper_author": "Yu L.B.",
        "publication": "China Economic Review",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "This paper investigates the economic effects of the opening of the only high-speed rail (HSR) line in northwest China, which connects the northwestern provinces along the Silk Road land route. Northwest China faces two major geographical challenges for economic development: its landlocked location and rugged terrain. To assess the impact of the HSR line on economic activity in counties along this Silk Road land route, we utilise a recently developed machine-learning extended nightlight dataset spanning from 2000 to 2019. For this analysis, we employ the ridge augmented synthetic control method (Ben-Michael et al., 2021). Additionally, we propose an algorithm that automates the donor pool selection process while ensuring optimal pre-treatment comparability. Our findings demonstrate that the opening of the Lanzhou-Xinjiang HSR line resulted in both winners and losers. While there are indications that HSR contributes to progress towards breaking through the Hu Huanyong Line, a geographical demarcation in China that is of vast economic significance, not all counties benefited from the opening of the HSR line.",
        "DOI": "10.1016/j.chieco.2023.102076",
        "affiliation_name": "City University of Macau",
        "affiliation_city": "Taipa",
        "affiliation_country": "Macao"
    },
    {
        "paper_title": "Meta-reinforcement learning via orbitofrontal cortex",
        "paper_author": "Hattori R.",
        "publication": "Nature Neuroscience",
        "citied_by": "10",
        "cover_date": "2023-12-01",
        "Abstract": "The meta-reinforcement learning (meta-RL) framework, which involves RL over multiple timescales, has been successful in training deep RL models that generalize to new environments. It has been hypothesized that the prefrontal cortex may mediate meta-RL in the brain, but the evidence is scarce. Here we show that the orbitofrontal cortex (OFC) mediates meta-RL. We trained mice and deep RL models on a probabilistic reversal learning task across sessions during which they improved their trial-by-trial RL policy through meta-learning. Ca2+/calmodulin-dependent protein kinase II-dependent synaptic plasticity in OFC was necessary for this meta-learning but not for the within-session trial-by-trial RL in experts. After meta-learning, OFC activity robustly encoded value signals, and OFC inactivation impaired the RL behaviors. Longitudinal tracking of OFC activity revealed that meta-learning gradually shapes population value coding to guide the ongoing behavioral policy. Our results indicate that two distinct RL algorithms with distinct neural mechanisms and timescales coexist in OFC to support adaptive decision-making.",
        "DOI": "10.1038/s41593-023-01485-3",
        "affiliation_name": "Department of Neurosciences",
        "affiliation_city": "La Jolla",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Stacked machine learning models for predicting species richness and endemism for Mediterranean endemic plants in the Mareotis subsector in Egypt",
        "paper_author": "Bedair H.",
        "publication": "Plant Ecology",
        "citied_by": "6",
        "cover_date": "2023-12-01",
        "Abstract": "An effective method for identifying species and evaluating the effects of changes caused by humans on specific species is the application of species distribution modelling (SDM) in desert environments. The fact that many dry lands and deserts throughout the world are situated in inhospitable regions may be the reason why such applications are still infrequently used on plant species in Egypt's Mediterranean region. Henceforth, the current study aims to map species richness and weighted endemism of Mediterranean endemics in the Mareotis subsector in Egypt and determine the environmental variables influencing distribution of these taxa. We produced a map of species distribution range using Ensemble SDMs. Further, stacked machine learning ensemble models derived from Random Forest (RF) and MaxEnt models were applied on 382 Mediterranean endemics distribution data to estimate and map diversity and endemism using two indices: species richness (SR) and weighted endemism index (WEI). The best models for ensemble modelling were chosen based on Kappa values and the Area Under the Receiver Operator Curve (AUC). The results showed that the models had a good predictive ability (Area Under the Curve (AUC) for all SDMs was > 0.75), indicating high accuracy in forecasting the potential geographic distribution of Mediterranean endemics. The main bioclimatic variables that impacted potential distributions of most species were wind speed, elevation and minimum temperature of coldest month. According to our models, six hotspots were determined for Mediterranean endemics in the present study. The highest species richness was recorded in Sallum, Matrouh wadis and Omayed, followed by Burg El-Arab, Ras El-Hekma and Lake Mariut. Indeed, species richness and endemism hotspots are promising areas for conservation planning. This study can help shape policy and mitigation efforts to protect and preserve Mediterranean endemics in the coastal desert of Egypt. These hotspots should be focused on by policy makers and stakeholders and declared as protectorates in the region. The largest number of species per area would be protected by focusing primarily on the hotspots with high species richness.",
        "DOI": "10.1007/s11258-023-01366-6",
        "affiliation_name": "Faculty of Science",
        "affiliation_city": "Tanta",
        "affiliation_country": "Egypt"
    },
    {
        "paper_title": "HLEL: A wetland classification algorithm with self-learning capability, taking the Sanjiang Nature Reserve I as an example",
        "paper_author": "Jiang W.",
        "publication": "Journal of Hydrology",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "Wetland ecosystems have been severely damaged by atmospheric pollution and other environmental problems for decades. Reasonable protection, restoration, and utilization of wetlands have become priorities for environmental protection. Obtaining high-precision wetland map products is essential for ecological research as well as formulation and implementation of conservation policies. This study proposes a hierarchical labeling ensemble learning (HLEL) algorithm with self-learning capabilities to achieve high-precision wetland land cover and land use (LULC) classification by adapting self-learning hierarchical results. This study evaluated the classification performance of HLEL, random forest (RF), support vector machine (SVM), artificial neural network (ANN), and Gaussian Naive Bayes' (GNB) with GF-6 WFV multispectral images as the database and Sanjiang National Nature Reserve I in China as the study area. The results showed that the HLEL algorithm has an advantage over a single algorithm when using the feature set filtered by ReliefF, even if the complete feature set is used without feature selection. The overall accuracy of the classification results is improved by 0.4%, 5.8%, 1.3%, and 9.0% when compared to the four single algorithms RF, SVM, ANN, and GNB, respectively. In robustness tests, HLEL consistently outperformed the other algorithms for different training sample set sizes, with overall accuracy improvements of 0.4–6.9%, 5.8–11.5%, 1.3–4.4%, and 9.0–14.0% compared with RF, SVM, ANN, and GNB, respectively. The overall accuracy standard deviation of HLEL is 1.22%, proving that HLEL has low sensitivity to the size of the training data. The HLEL algorithm demonstrates exceptional precision when applied to diverse datasets. The overall accuracy of HLEL in Sentinel 2, Landsat 8, and GF-6 WFV increased by 3.1% to 22.7%, 3.1% to 39.0%, and 0.4% to 9.0%, respectively, when compared to the overall accuracy of the basic classifiers. The results demonstrate the feasibility of HLEL, an algorithm with high accuracy and data robustness, in applying wetland LULC classification and providing a new solution for wetland cover mapping.",
        "DOI": "10.1016/j.jhydrol.2023.130446",
        "affiliation_name": "Central South University of Forestry and Technology",
        "affiliation_city": "Changsha",
        "affiliation_country": "China"
    },
    {
        "paper_title": "High-resolution imaging coupled with deep learning model for classifying water body of Soyang Lake, South Korea",
        "paper_author": "Ramayanti S.",
        "publication": "Geosciences Journal",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "Precise monitoring of natural phenomena is essential to reduce potential risk when a disaster occurs. The capability of developing high-resolution satellite images has provided advantages in the Earth’s surface observation. High-resolution images, including KOMPSAT-2/3 and PlanetScope images, have enabled more precise observation of the Earth’s surface. Combining the high-resolution KOMPSAT-2/3 and PlanetScope images and a capable image classifier tool can contribute to improving the confidence level of the observation. Recently, deep learning models provided enhanced capability compared to traditional machine learning, especially in image classification. In this study, we aim to investigate performance comparison between deep learning model using U-Net architecture and traditional machine learning represented by support vector machine (SVM) and random forest (RF) model with a focus on identifying water bodies on a lake. Significant differences in surface area of water of Soyang Lake occurred in 2015 and 2022 was the area of interest of this study. Based on confusion matrix analysis, the best classification results were indicated by deep learning model represented by U-Net with overall accuracy ranging from 97% to 100% and a Kappa coefficient of 0.91–1.00. The two traditional machine learning models, SVM and RF, lower up to 11% in overall accuracy and 0.15 for Kappa coefficient values. We also estimated the surface area of water based on the best classification results and found that the changes in water surface area were correlated with the monthly precipitation with a coefficient correlation of 0.89. This study should give new insight into the classification method for high-resolution satellite images. By providing the precise classification result, it could be contributed to providing proper mitigation and design policy related to natural phenomena on the earth’s surface, especially for monitoring the surface area of the lake water.",
        "DOI": "10.1007/s12303-023-0032-7",
        "affiliation_name": "Kangwon National University",
        "affiliation_city": "Chuncheon",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "Energy poverty prediction and effective targeting for just transitions with machine learning",
        "paper_author": "Spandagos C.",
        "publication": "Energy Economics",
        "citied_by": "17",
        "cover_date": "2023-12-01",
        "Abstract": "The prevalence of energy poverty as a major challenge in numerous countries, the escalating energy crisis and the need to build just supporting mechanisms within the net zero energy transition add impetus to improving our ability to accurately predict energy vulnerable households. In Europe, this is hindered by limited recognition of the fact that energy vulnerable households are not necessarily income poor (and vice versa). Artificial Intelligence, and machine learning techniques in particular, may be applied to improve the targeting mechanism of energy poverty schemes, enabling accurate prediction of energy vulnerable households via objective, publicly available data. However, such applications are still limited, especially across a large number of countries. In response to the above, we develop an innovative machine learning framework for accurate prediction and fair targeting of energy poor households across all the current members of the European Union, and the United Kingdom. While we explore various machine learning algorithms, most of our analysis is performed using a Random Forest classifier. Our approach to explore energy poverty beyond income reveals household-level and country-level predictors of energy poverty, such as dwelling condition, energy efficiency, social protection payments and gas supplier switching rates. We also demonstrate how machine learning algorithms offer straightforward visualization of the mechanism that determines the energy poor classification, improving the transparency of alleviation schemes and assisting policy-makers in setting effective thresholds for assistance allocation. Finally, we evaluate the potential fairness of alleviation schemes and demonstrate that basing their targeting exclusively on income-relevant or social welfare-relevant criteria would be ineffective and result in significant numbers of energy poor households being excluded from energy assistance.",
        "DOI": "10.1016/j.eneco.2023.107131",
        "affiliation_name": "University of New Hampshire Durham",
        "affiliation_city": "Durham",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Reinforcement learning for humanitarian relief distribution with trucks and UAVs under travel time uncertainty",
        "paper_author": "van Steenbergen R.",
        "publication": "Transportation Research Part C: Emerging Technologies",
        "citied_by": "9",
        "cover_date": "2023-12-01",
        "Abstract": "Effective humanitarian relief operations are challenging in the aftermath of disasters, as trucks are often faced with considerable travel time uncertainties due to damaged transportation networks. Efficient deployment of Unmanned Aerial Vehicles (UAVs) potentially mitigates this problem, supplementing truck fleets in an impactful manner. To plan last-mile relief distribution in this setting, we introduce a multi-trip, split-delivery vehicle routing problem with trucks and UAVs, soft time windows, and stochastic travel times for last-mile relief distribution, formulated as a stochastic dynamic program. Within a finite time horizon, we aim to maximize a weighted objective function comprising the number of goods delivered, the number of different locations visited, and late arrival penalties. Our study offers insights into dealing with travel time uncertainty in humanitarian logistics by (i) deploying Unmanned Aerial Vehicles (UAVs) as partial substitutes for trucks, (ii) evaluating dynamic solutions generated by two deep reinforcement learning (RL) approaches – specifically value function approximation (VFA) and policy function approximation (PFA) – and (iii) comparing the RL solutions with solutions stemming from mathematical programming and dynamic heuristics. Experiments are performed on both Solomon-based instances and two real-world cases. The real-world cases – the 2015 Nepal earthquake and the 2018 Indonesia tsunami – are based on locally collected field data and real-world UAV specifications, and aim to provide practical insights. The experimental results show that dynamic decision-making improves both performance and robustness of humanitarian operations, achieving reductions in lateness penalties of around 85% compared to static solutions based on expected travel times. Furthermore, the results show that replacing half of the trucks with UAVs improves the weighted objective value by 11% to 56%, benefitting both reliability and location coverage. The results indicate that both the deployment of UAVs and the use of dynamic methods successfully mitigate travel time uncertainties in humanitarian operations.",
        "DOI": "10.1016/j.trc.2023.104401",
        "affiliation_name": "Universiteit Twente",
        "affiliation_city": "Enschede",
        "affiliation_country": "Netherlands"
    },
    {
        "paper_title": "Scalable reinforcement learning approaches for dynamic pricing in ride-hailing systems",
        "paper_author": "Lei Z.",
        "publication": "Transportation Research Part B: Methodological",
        "citied_by": "7",
        "cover_date": "2023-12-01",
        "Abstract": "Dynamic pricing is a widely applied strategy by ride-hailing companies, such as Uber and Lyft, to match the trip demand with the availability of drivers. Deciding proper pricing policies is challenging and existing reinforcement learning (RL)-based solutions are restricted in solving small-scale problems. In this study, we contribute to RL-based approaches that can address the dynamic pricing problem in real-world-scale ride-hailing systems. We first characterize the dynamic pricing problem with a clear distinction between historical prices and current prices. We then translate our dynamic pricing problem into Markov Decision Process (MDP) and prove the existence of a deterministic stationary optimal policy. Our solutions are based on an off-policy reinforcement learning algorithm called twin-delayed deep determinant policy gradient (TD3) that performs offline learning of the optimal pricing policy using historical data and applies the learned policy to the next time slot, e.g., one week. We enhance TD3 by creating three mechanisms to reduce our model complexity and enhance training effectiveness. Extensive numerical experiments are conducted on both small grid networks (16 zones) and the NYC network (242 zones) to demonstrate the performance of the proposed algorithm. The results show our algorithm can efficiently find the optimal pricing policy for both the small and large networks, and can significantly enhance the platform profit and service efficiency.",
        "DOI": "10.1016/j.trb.2023.102848",
        "affiliation_name": "Lyles School of Civil and Construction Engineering",
        "affiliation_city": "West Lafayette",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Moderating manipulation: Demystifying extremist tactics for gaming the (regulatory) system",
        "paper_author": "Mattheis A.A.",
        "publication": "Policy and Internet",
        "citied_by": "4",
        "cover_date": "2023-12-01",
        "Abstract": "Due to its ease of scalability and broad applicability, the use of artificial intelligence (AI) and machine learning in platform management has gained prominence. This has led to widespread debates about the use of deplatforming as the default tool for repeated or severe violations of terms or service. But technologically deterministic approaches are not infallible and can be predictable based on their actions. This opens the door for manipulation of media content and technological affordances to become tactical options for actors seeking to subvert regulation. Existing discussions often neglect the topic of manipulation of content, algorithms, or platform affordances as a primary aspect of the strategies used by extremists in relation to the difficulties of moderation from a policy perspective. This study argues that it is essential to understand how extremists and conspiracy theorists use manipulation tactics to ‘game’ the current policy, regulatory and legislative systems of content moderation. Developing approaches that attend to manipulation as a strategy and focus on platform and context-specific tactics will generate more effective policies, platform rules, AI developments and moderation procedures. This study analyses and demystifies three primary tactics, which the authors categorize as numerology, borderlands and merchandising, regularly used by extremists online in their strategies to ‘game’ content moderation. We provide case examples from a variety of ideologies including far-right, QAnon and male supremacism to highlight the tactics rather than ideological nature of such manipulation. We conclude with a discussion of how demystification processes could be incorporated into content moderation settings. This study contributes new insights about evasion tactics to the content moderation discussion and expands current understanding of how platforms can develop sociotechnical remedial measures.",
        "DOI": "10.1002/poi3.381",
        "affiliation_name": "Faculty of Social Sciences",
        "affiliation_city": "Southampton",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "A dataset for benchmarking Neotropical anuran calls identification in passive acoustic monitoring",
        "paper_author": "Cañas J.S.",
        "publication": "Scientific Data",
        "citied_by": "7",
        "cover_date": "2023-12-01",
        "Abstract": "Global change is predicted to induce shifts in anuran acoustic behavior, which can be studied through passive acoustic monitoring (PAM). Understanding changes in calling behavior requires automatic identification of anuran species, which is challenging due to the particular characteristics of neotropical soundscapes. In this paper, we introduce a large-scale multi-species dataset of anuran amphibians calls recorded by PAM, that comprises 27 hours of expert annotations for 42 different species from two Brazilian biomes. We provide open access to the dataset, including the raw recordings, experimental setup code, and a benchmark with a baseline model of the fine-grained categorization problem. Additionally, we highlight the challenges of the dataset to encourage machine learning researchers to solve the problem of anuran call identification towards conservation policy. All our experiments and resources have been made available at https://soundclim.github.io/anuraweb/ .",
        "DOI": "10.1038/s41597-023-02666-2",
        "affiliation_name": "Pontificia Universidad Javeriana, Cali",
        "affiliation_city": "Cali",
        "affiliation_country": "Colombia"
    },
    {
        "paper_title": "The influence of sociodemographic factors on students' attitudes toward AI-generated video content creation",
        "paper_author": "Pellas N.",
        "publication": "Smart Learning Environments",
        "citied_by": "30",
        "cover_date": "2023-12-01",
        "Abstract": "Artificial Intelligence (AI) and Machine Learning (ML) technologies offer the potential to support digital content creation and media production, providing opportunities for individuals from diverse sociodemographic backgrounds to engage in creative activities and enhance their multimedia video content. However, less attention has been paid to recent research exploring any possible relationships between AI-generated video creation and the sociodemographic variables of undergraduate students. This study aims to investigate the multifaceted relationship between AI-generated video content and sociodemographics by examining its implications for inclusivity, equity, and representation in the digital media landscape. An empirical study about the use of AI in video content creation was conducted with a diverse cohort of three hundred ninety-eighth undergraduate (n = 398) students. Participants voluntarily took part and were tasked with conceiving and crafting their AI-generated video content. All instruments used were combined into a single web-based self-report questionnaire that was delivered to all participants via email. Key research findings demonstrate that students have a favorable disposition when it comes to incorporating AI-supported learning tasks. The factors fostering this favorable attitude among students include their age, the number of devices they use, the time they dedicate to utilizing technological resources, and their level of experience. Nevertheless, it is the student’s participation in AI training courses that exerts a direct impact on students’ ML attitudes, along with their level of contentment with the reliability of these technologies. This study contributes to a more comprehensive understanding of the transformative power of AI in video content creation and underscores the importance of considering instructional contexts and policies to ensure a fair and equitable digital media platform for students from diverse sociodemographic backgrounds.",
        "DOI": "10.1186/s40561-023-00276-4",
        "affiliation_name": "University of Western Macedonia",
        "affiliation_city": "Kozani",
        "affiliation_country": "Greece"
    },
    {
        "paper_title": "Adaptive device sampling and deadline determination for cloud-based heterogeneous federated learning",
        "paper_author": "Zhang D.",
        "publication": "Journal of Cloud Computing",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "As a new approach to machine learning, Federated learning enables distributned traiing on edge devices and aggregates local models into a global model. The edge devices that participate in federated learning are highly heterogeneous in terms of computing power, device state, and data distribution, making it challenging to converge models efficiently. In this paper, we propose FedState, which is an adaptive device sampling and deadline determination technique for cloud-based heterogeneous federated learning. Specifically, we consider the cloud as a central server that orchestrates federated learning on a large pool of edge devices. To improve the efficiency of model convergence in heterogeneous federated learning, our approach adaptively samples devices to join each round of training and determines the deadline for result submission based on device state. We analyze existing device usage traces to build device state models in different scenarios and design a dynamic importance measurement mechanism based on device availability, data utility, and computing power. We also propose a deadline determination module that dynamically sets the deadline according to the availability of all sampled devices, local training time, and communication time, enabling more clients to submit local models more efficiently. Due to the variability of device state, we design an experience-driven algorithm based on Deep Reinforcement Learning (DRL) that can dynamically adjust our sampling and deadline policies according to the current environment state. We demonstrate the effectiveness of our approach through a series of experiments with the FMNIST dataset and show that our method outperforms current state-of-the-art approaches in terms of model accuracy and convergence speed.",
        "DOI": "10.1186/s13677-023-00515-6",
        "affiliation_name": "Central South University",
        "affiliation_city": "Changsha",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Dynamic simulation based on feature transfer learning with source domain adaptive optimization: Application of data-driven model for aero-engines",
        "paper_author": "Jia X.",
        "publication": "Measurement: Journal of the International Measurement Confederation",
        "citied_by": "8",
        "cover_date": "2023-12-01",
        "Abstract": "Sensors are difficult to arrange inside complex aero-engine systems because of harsh operating environment. Aero-engine thermodynamic models can be used as virtual sensors to provide the basis for diagnosis and control. The modeling based on mechanism equations can be well explained, but its over-reliance on component characteristics lead to poor handling of strong dynamic. This paper proposed a novel source-domain-adaptive dynamic modeling framework of Feature Similarity-based Transfer Learning (FSTL) based on Deep Deterministic Policy Gradients (DDPG), which can mitigate for these shortcomings to a certain extent through little measurement data. The field startup signals are used to form the target domain, and the reward function is constructed by utilizing the domain similarity of feature fusion through FSTL. A dynamic model of a gas engine with a rated speed of 9000 rpm is carried out using the proposed learn-to-learn framework. The verification results of the startup process indicate that FSTL-DDPG has the most outstanding performance of 0.9536 regression coefficient and 0.46 % mean square error, which are improved by about 16.9 % and 62.0 % compared with FSTL and Dynamic Mode Decomposition combined with Long Short-Term Memory (DMD-LSTM). Qualitative and quantitative analysis revealed that FSTL-DDPG has better simulation performance and matching degree of physical laws.",
        "DOI": "10.1016/j.measurement.2023.113786",
        "affiliation_name": "Key Laboratory for Power Machinery and Engineering of Ministry of Education",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Social impacts reflected in CSR reports: Method of extraction and link to firms innovation capacity",
        "paper_author": "Nechaev I.",
        "publication": "Journal of Cleaner Production",
        "citied_by": "3",
        "cover_date": "2023-12-01",
        "Abstract": "Assessing and comprehending the social impact of firms at global and local level is a pressing concern for both researchers and policy-makers. To address this concern, our paper contributes to the stream of literature that studies the content of Corporate Social Responsibility (CSR) reports (which are also referred to as non-financial statements, sustainability reports or parts of annual reports) using text mining methods. We present a novel approach called Standard-based Impact Classification method (SBIC method), which employs natural language processing (NLP) and supervised machine learning techniques to identify the types of social impacts reflected in CSR reports. We deploy a Random Forest model which we train on reports adhering to Global Reporting Initiative (GRI) framework, enabling the identification of social impact in the majority of CSR reports that do not conform to this standard. Our proposed SBIC method serves as a valuable tool for comparing the social impacts generated by firms, industries, or countries. We showcase an application of our approach by examining the relationship between a company's social impact and its innovation capacity. Our findings support the existing literature consensus that CSR activities generally exhibit a positive correlation with a firm's ability to innovate. Furthermore, we reveal that specific types of social impacts have a more pronounced influence on innovation capacity.",
        "DOI": "10.1016/j.jclepro.2023.139256",
        "affiliation_name": "Aalborg University Business School",
        "affiliation_city": "Aalborg",
        "affiliation_country": "Denmark"
    },
    {
        "paper_title": "Soil erosion susceptibility mapping in Bangladesh",
        "paper_author": "Sadia H.",
        "publication": "Ecological Indicators",
        "citied_by": "10",
        "cover_date": "2023-12-01",
        "Abstract": "This study aims to draw a scientific framework for plotting soil erosion susceptibility in the Chittagong Hill Tracts of Bangladesh by comparing existing approaches. Data-driven machine learning techniques (including Classification and Regression Tree (CART), Artificial Neural Network (ANN), Support Vector Machine (SVM), and Random Forest (RF)) and a knowledge-based approach (AHP) are used in this study to pinpoint areas of Chittagong that are particularly susceptible to soil erosion while taking into account 18 soil erosion-regulating parameters. Furthermore, the effectiveness of the selected data-driven machine learning models and knowledge-based models was assessed by utilizing soil erosion and non-erosion sites. When evaluating the fidelity of each model using the ROC and AUC, the RF model was shown to be the most accurate and predictive. There is no poor performer among these models; all have AUCs greater than 67 % (RF = 0.86, ANN = 0.73, SVM = 0.67, CART = 0.67, and AHP = 0.82). According to the findings of the Random Forest model, approximately 71.55 percent of the area exhibited a moderate level of susceptibility to soil erosion. In relation to the land area, the high and low zones accounted for 16.91 percent and 11.54 percent, respectively. The specific area shares of 2256.25, 9548.08, and 1539.67 square kilometers were attributed to the high, moderate, and low danger zones, respectively. The best models' results after comparing models of data-driven and knowledge-based approaches can help to estimate soil erosion risk zones and provide insight into establishing appropriate policies to minimize this issue. In addition, the methods used in this research might be applicable to assessing the vulnerability and risk of soil erosion events in other areas. As they begin long-term planning to reduce soil erosion, local authorities and policymakers will find the study's results on practical policies and management options quite helpful.",
        "DOI": "10.1016/j.ecolind.2023.111182",
        "affiliation_name": "Khulna University of Engineering and Technology",
        "affiliation_city": "Khulna",
        "affiliation_country": "Bangladesh"
    },
    {
        "paper_title": "Adaptive Mobile Robot Scheduling in Multiproduct Flexible Manufacturing Systems Using Reinforcement Learning",
        "paper_author": "Waseem M.",
        "publication": "Journal of Manufacturing Science and Engineering",
        "citied_by": "4",
        "cover_date": "2023-12-01",
        "Abstract": "The integration of mobile robots in material handling in flexible manufacturing systems (FMS) is made possible by the recent advancements in Industry 4.0 and industrial artificial intelligence. However, effectively scheduling these robots in real-time remains a challenge due to the constantly changing, complex, and uncertain nature of the shop floor environment. Therefore, this paper studies the robot scheduling problem for a multiproduct FMS using a mobile robot for loading/unloading parts among machines and buffers. The problem is formulated as a Markov Decision Process, and the Q-learning algorithm is used to find an optimal policy for the robot's movements in handling different product types. The performance of the system is evaluated using a reward function based on permanent production loss and the cost of demand dissatisfaction. The proposed approach is validated through a numerical case study that compares the proposed policy to a similar policy with different reward function and the first-come-first-served policy, showing a significant improvement in production throughput of approximately 23%.",
        "DOI": "10.1115/1.4062941",
        "affiliation_name": "University of Virginia School of Engineering and Applied Science",
        "affiliation_city": "Charlottesville",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Impact of COVID-19 on Indian politics: analyzing political leaders interactions and sentiments on Twitter",
        "paper_author": "Borah A.",
        "publication": "Social Network Analysis and Mining",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "Amidst the persistent COVID-19 pandemic, there has been a profound disruption in political, economic, and social disruption in the entire world. India has emerged as one of the most affected countries by this pandemic globally. The government has taken extensive measures to combat the disease and is disseminating essential information regarding the same on social media, particularly Twitter. Restricted or polarized interactions and diverging opinions among the politicians may hinder the formulation of important policies and measures for managing this crucial situation. This paper, therefore, aims to perform an in-depth investigation on the Twitter activities of Indian political leaders in response to COVID-19. The study presents an analysis of their tweet sentiments and formation of networks during political discussions. The analysis has been done on three different topics pertaining to COVID-19: preventive measures, lockdown, and vaccination separately. Firstly, the communication ties formed between the politicians during discussions on the respective topics are investigated based on network analysis of their mentions and retweets. The communities formed in the interaction networks and the extent of polarization between the communities is then examined. Secondly, sentiment analysis of the tweets have been performed using some well-known machine learning classifiers to identify the sentiment leaning of the politicians and the communities toward the issue. This combined approach of network and sentiment based analysis provides better characterization of political communities and their leanings regarding the pandemic. The findings revealed the presence of polarized communication during retweets while high level of cross-party interactions during mentions. The politicians have been identified to have overall positive response toward preventive measures and vaccination while majority have shown negative sentiments toward lockdown.",
        "DOI": "10.1007/s13278-023-01153-1",
        "affiliation_name": "Indian Institute of Technology Guwahati",
        "affiliation_city": "Guwahati",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Mapping sugarcane residue burnt areas in smallholder farming systems using machine learning approaches",
        "paper_author": "PNVR K.",
        "publication": "Smart Agricultural Technology",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "Satellite remote sensing methods have been proven to quantify the burnt areas resulting from different fire activities reliably. However, they were reported to perform poorly to identify crop residue burnt areas, particularly in smallholder systems due to lack of frequent and high spatial resolution satellite data. In this study, we used Harmonized Landsat Sentinel-2 (HLS) observations and evaluated two machine learning classifiers (i.e., Support Vector Machines (SVM) and Artificial Neural Networks (ANN)) to map sugarcane burnt areas for the 2019–20 season in a smallholder farming region in Thailand. Results showed that both classifiers performed well in identifying the spatial patterns of sugarcane burnt areas in the region. The ANN outperformed SVM at both pixel and regional scales. At pixel level, ANN accuracy was 93.4 % while SVM's best-performing Polynomial kernel accuracy was 82.7 %. The ANN estimated average percent burnt area (51.1 %) in the region was closer to reported value (48.7 %) by Thailand's Office of Cane and Sugar Board (OCSB), compared to the SVM estimate (62.9 %). The total estimated burnt areas by ANN and SVM (315 and 418 thousand ha, respectively) deviated more from OCSB's area (240 thousand ha) compared to percent burnt area. However, area estimates from classifiers had significantly better accuracy than the estimates of MODIS burnt products. Overall, this study demonstrated that HLS observations provided required spectral information to build promising models to map burnt areas in smallholder systems with higher accuracy than global products. Our mapping algorithm using the ANN classifier showed the potential to monitor sugarcane burnt areas reliably, and contribute to the successful implementation of regulatory policies in Thailand.",
        "DOI": "10.1016/j.atech.2023.100347",
        "affiliation_name": "A. James Clark School of Engineering",
        "affiliation_city": "College Park",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "A PIE analysis of China’s commercial space development",
        "paper_author": "Han Y.",
        "publication": "Humanities and Social Sciences Communications",
        "citied_by": "3",
        "cover_date": "2023-12-01",
        "Abstract": "The commercial space industry seems to draw worldwide attention in the recent two decades to respond to the increasingly broad market demand. Compared with Western countries, the development of China’s commercial space industry is still in its infancy and faces many daunting challenges. Few studies have considered the dual perspective of government and market of commercial space industry in China. Aiming at exploring the influencing factors and future directions of the development of China’s commercial space industry, this paper proposed the theoretical analysis framework consisting of Policy, Innovation and Economics, and conducted a synergistic analysis of government and market in China’s commercial space industry. Specifically, based on the interview data, assisted by a machine learning approach, we have conducted an industry analysis of the influencing factors of commercial space in China. The results show that China’s commercial space industry is rapidly developing with many characteristics. Specifically, at present, there is a lack of specific and operational policies at the implementation level. Despite limited disruptive innovation, commercial space companies have achieved breakthroughs in many key areas. The overall industry is exploring more stable and sustainable profit models. The findings of this study contribute to the topic of industry development in business management literature by underpinning the policy discussion, technological analysis and direction for the future development of China’s commercial space industry. The industry practitioners may also benefit from the practices that were discussed in this research.",
        "DOI": "10.1057/s41599-023-02274-w",
        "affiliation_name": "Technology and Engineering Center for Space Utilization",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Integrating street view images and deep learning to explore the association between human perceptions of the built environment and cardiovascular disease in older adults",
        "paper_author": "Xu J.",
        "publication": "Social Science and Medicine",
        "citied_by": "6",
        "cover_date": "2023-12-01",
        "Abstract": "Understanding how built environment attributes affect health remains important. While many studies have explored the objective characteristics of built environments that affect health outcomes, few have examined the role of human perceptions of built environments on physical health. Baidu Street View images and computer vision technological advances have helped researchers overcome the constraints of traditional methods of measuring human perceptions (e.g., these methods are laborious, time-consuming, and costly), allowing for large-scale measurements of human perceptions. This study estimated human perceptions of the built environment (e.g., beauty, boredom, depression, safety, vitality, and wealth) by adopting Baidu Street View images and deep learning algorithms. Negative binomial regression models were employed to analyze the relationship between human perceptions and cardiovascular disease in older adults (e.g., ischemic heart disease and cerebrovascular disease). The results indicated that wealth perception is negatively related to the risk of cardiovascular disease. However, depression and vitality perceptions are positively associated with the risk of cardiovascular disease. Furthermore, we found no relationship between beauty, boredom, safety perceptions, and the risk of cardiovascular disease. Our findings highlight the importance of human perceptions in the development of healthy city planning and facilitate a comprehensive understanding of the relationship between built environment characteristics and health outcomes in older adults. They also demonstrate that street view images have the potential to provide insights into this complicated issue, assisting in the formulation of refined interventions and health policies.",
        "DOI": "10.1016/j.socscimed.2023.116304",
        "affiliation_name": "Collaborative Innovation Center of Geospatial Technology",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Data-enabled learning, network effects, and competitive advantage",
        "paper_author": "Hagiu A.",
        "publication": "RAND Journal of Economics",
        "citied_by": "19",
        "cover_date": "2023-12-01",
        "Abstract": "We model dynamic competition between firms which improve their products through learning from customer data, either by pooling different customers' data (across-user learning) or by learning from repeated usage of the same customers (within-user learning). We show how a firm's competitive advantage is affected by the shape of firms' learning functions, asymmetries between their learning functions, the extent of data accumulation, and customer beliefs. We also explore how public policies toward data sharing, user privacy, and killer data acquisitions affect competitive dynamics and efficiency. Finally, we show conditions under which a consumer coordination problem arises endogenously from data-enabled learning.",
        "DOI": "10.1111/1756-2171.12453",
        "affiliation_name": "Questrom School of Business",
        "affiliation_city": "Boston",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Sensitivity Analysis and Feature Selection for Drilling-Oriented Models",
        "paper_author": "Tariq S.",
        "publication": "Journal of Energy Resources Technology, Transactions of the ASME",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "Data-driven models have risen in popularity during the past 10 years, which increase the effectiveness and durability of systems without necessitating a lot of human involvement. Despite all of their advantages, they remain the limitations in terms of model interpretation, data selection, and model evaluation. Sensitivity analysis (SA) is a powerful tool to decipher behaviors of data-driven models to analyze the correlations among inputs and outputs of models, and quantify the severity of inputs' influence on outputs to effectively interpret these black-box models. Feature selection (FS) is a preprocessing approach used in data-driven modeling to select the crucial parameters as inputs fed to models. For the most of the existing works, the FS is well used to select inputs through the analysis on the drilling data correlations, while SA is seldom employed for data-driven model evaluation and interpretation in drilling applications. Data-driven rate of penetration (ROP) models have consistently outperformed many conventional ROP models, most likely as a result of their strong data analysis capabilities, capacity to learn from data in order to recognize data patterns, and effective policies for making logical decisions automatically. A data-driven ROP model was developed from a benchmarking field drilling dataset in this work. Following the ROP modelling, sensitivity analysis methods were employed to identify the input variables that had the greatest influence on ROP estimations. The FS techniques and the sensitivity analysis were combined during the data preprocessing to identify the most important aspects for modelling. The outcomes demonstrate that using the robust sensitivity analysis techniques to overcome the limits of machine learning models allows for the best interpretation and understanding of the produced data-driven models.",
        "DOI": "10.1115/1.4062382",
        "affiliation_name": "Universitetet i Stavanger",
        "affiliation_city": "Stavanger",
        "affiliation_country": "Norway"
    },
    {
        "paper_title": "Integrating artificial intelligence, machine learning, and deep learning approaches into remediation of contaminated sites: A review",
        "paper_author": "Janga J.K.",
        "publication": "Chemosphere",
        "citied_by": "23",
        "cover_date": "2023-12-01",
        "Abstract": "The growing number of contaminated sites across the world pose a considerable threat to the environment and human health. Remediating such sites is a cumbersome process with the complexity originating from the need for extensive sampling and testing during site characterization. Selection and design of remediation technology is further complicated by the uncertainties surrounding contaminant attributes, concentration, as well as soil and groundwater properties, which influence the remediation efficiency. Additionally, challenges emerge in identifying contamination sources and monitoring the affected area. Often, these problems are overly simplified, and the data gathered is underutilized rendering the remediation process inefficient. The potential of artificial intelligence (AI), machine-learning (ML), and deep-learning (DL) to address these issues is noteworthy, as their emergence revolutionized the process of data management/analysis. Researchers across the world are increasingly leveraging AI/ML/DL to address remediation challenges. Current study aims to perform a comprehensive literature review on the integration of AI/ML/DL tools into contaminated site remediation. A brief introduction to various emerging and existing AI/ML/DL technologies is presented, followed by a comprehensive literature review. In essence, ML/DL based predictive models can facilitate a thorough understanding of contamination patterns, reducing the need for extensive soil and groundwater sampling. Additionally, AI/ML/DL algorithms can play a pivotal role in identifying optimal remediation strategies by analyzing historical data, simulating scenarios through surrogate models, parameter-optimization using nature inspired algorithms, and enhancing decision-making with AI-based tools. Overall, with supportive measures like open-data policies and data integration, AI/ML/DL possess the potential to revolutionize the practice of contaminated site remediation.",
        "DOI": "10.1016/j.chemosphere.2023.140476",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Chicago",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "An intelligently adjusted carbon price forecasting approach based on breakpoints segmentation, feature selection and adaptive machine learning",
        "paper_author": "Zhao S.",
        "publication": "Applied Soft Computing",
        "citied_by": "6",
        "cover_date": "2023-12-01",
        "Abstract": "Accurate carbon price prediction is conducive to the stable operation and development of carbon financial markets. Affected by major policies and economies, the laws of carbon price may show huge changes, generating various breakpoints that hinder the prediction work. Therefore, a hybrid model based on adaptive segmentation and feature clustering is developed to forecast carbon price, which responds to the growing requirement of the high precision prediction. It solves the issue that the accuracy and stability of model are limited by the breakpoint via the adaptive processing and the fully learning for data. Meanwhile, the proposed adaptive structure improves the robustness and adaptability of prediction model, achieving accurate prediction for carbon emission trading markets with different features. The eight carbon emission trading markets in China are used to evaluate prediction performance. The obtained results indicated that the proposed model was effective and robust, with the average mean absolute error and root mean square error of only 0.2272 and 0.3321, respectively. According to the comparative analysis, the segmentation based on breakpoint and adaptive prediction based on feature clustering improve the model forecasting accuracy by 69.87% and 45.51%, respectively. Hence, the proposed model can meet the requirements of carbon financial markets and provide a benchmark for the carbon emission reduction work.",
        "DOI": "10.1016/j.asoc.2023.110948",
        "affiliation_name": "Kunming University of Science and Technology",
        "affiliation_city": "Kunming",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Dynamic Goal Tracking for Differential Drive Robot Using Deep Reinforcement Learning",
        "paper_author": "Shahid M.",
        "publication": "Neural Processing Letters",
        "citied_by": "2",
        "cover_date": "2023-12-01",
        "Abstract": "To ensure the steady navigation for robot stable controls are one of the basic requirements. Control values selection is highly environment dependent. To ensure reusability of control parameter, system needs to generalize over the environment. Adding adaptability in robots to perform effectively in the environments with no prior knowledge reinforcement leaning is a promising approach. However, tuning hyper parameters and attaining correlation between state space and reward function to train a stable reinforcement learning agent is a challenge. This paper is focused, to design a continuous reward function to minimize the sparsity and stabilizes the policy convergence, to attain control generalization for differential drive robot. To achieve that, Twin Delayed Deep Deterministic Policy Gradient is implemented on PyBullet Racecar model in Open-AIGym environment. System was trained to achieve smart primitive control policy, moving forward in the direction of goal by maintaining an appropriate distance from walls to avoid collision. Resulting policy was tested on unseen environments including dynamic goal environment, boundary free environment and continuous path environment on which it outperformed Deep Deterministic Policy Gradient.",
        "DOI": "10.1007/s11063-023-11390-2",
        "affiliation_name": "National University of Sciences and Technology",
        "affiliation_city": "Islamabad",
        "affiliation_country": "Pakistan"
    },
    {
        "paper_title": "Comparative Analysis of Drought Modeling and Forecasting Using Soft Computing Techniques",
        "paper_author": "Jariwala K.A.",
        "publication": "Water Resources Management",
        "citied_by": "4",
        "cover_date": "2023-12-01",
        "Abstract": "Drought modeling is vital for managing water scarcity in arid regions. It allows proactive planning, resource allocation, and policy development. A combination of statistical models and machine learning techniques is necessary to capture the complexity of drought dynamics effectively. In this study, we compare the performance of the ARIMAX hybrid statistical method and the ANN and Fuzzy-based machine learning method ANFIS for drought modeling. Among the various models examined, the most promising results are obtained using a combination of ANFIS and ARIMAX, which are subsequently employed for drought event forecasting. Notably, ANFIS exhibits lower accuracy for long-term forecasting compared to ARIMAX. The study's novelty lies in the unequivocal demonstration of the ARIMAX (3,0,2) (3,0,2,12) model's superior performance in predicting meteorological drought events. This underscores the potential of ARIMAX models in leveraging historical data for adeptly forecasting drought. Furthermore, this model is applied to multiple locations to generate a drought forecasting and risk map for future years.",
        "DOI": "10.1007/s11269-023-03642-6",
        "affiliation_name": "S. V. National Institute of Technology",
        "affiliation_city": "Surat",
        "affiliation_country": "India"
    },
    {
        "paper_title": "A machine learning framework to assess the impact of the COVID-19 pandemic on electricity consumption patterns",
        "paper_author": "Rozhkov A.",
        "publication": "Cities",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "Energy self-sufficiency and resilience are essential elements for long-term urban sustainability. Energy supply in many cities in the United States has been highly dependent on fossil fuels and excessive energy consumption, both at home and outside (i.e., residential and non-residential energy consumption). In this work, we identify electricity consumption spatial trends and profiles across neighborhoods in the city of Chicago using a self-organizing map (SOM) machine learning method as a main tool. Toward this goal, we use an anonymous dataset provided by Commonwealth Edison (ComEd; Chicago's main electricity provider), which includes electricity consumption data of individual residential and non-residential accounts in Chicago. We aggregate electricity consumption to the zip codes level, apply machine learning clustering methods to categorize electricity consumption patterns, and finally couple it with land use variables to find spatial interrelations. The output demonstrates how specific profiles of electricity consumption emerge across zip codes, which is the first step for developing tailored policies to lower energy consumption. The result of this study can help engineers, urban planners, and policymakers develop actionable strategies for electricity consumption management.",
        "DOI": "10.1016/j.cities.2023.104639",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Chicago",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "A ML-based economic protection development level using Decision Tree and Ensemble Algorithms",
        "paper_author": "Dou Q.",
        "publication": "Soft Computing",
        "citied_by": "3",
        "cover_date": "2023-12-01",
        "Abstract": "Economic progress has been founded on environmental pressure by generating all types of environmental harm, such as an increase in greenhouse gases in the atmosphere and significant climate change, which has forced people to reflect. Building a people-oriented, comprehensive, coordinated system to compile experience and lessons assists the government in formulating appropriate policies according to the meaning of the scientific development viewpoint. Based on the above, this research designs Cost–Benefit Decision Tree (CBDT) Algorithm for the economic protection development level using a Decision Tree (DT) and Ensemble Algorithm. In the design of ML Algorithm, this research work initially introduces the Decision Tree model for economic protection evaluation by emphasizing its value as a basic machine-learning framework for fully examining many aspects of economic protection development. Second, it investigates the critical role of Ensemble Algorithms, such as Random Forest in augmenting the capabilities of DT. Third, the paper focuses on the development of a complete Economic Protection Evaluation Index System using the DT Algorithm. To provide a robust, scientifically sound, and practicable framework for evaluating economic protection, this approach integrates scientificity, comparability, systematization, hierarchy, relative decoupling, and data availability principles. Finally, the study constructs the suggested ML Algorithm using the insights from the preceding parts. This algorithm combines the Decision Tree and Random Forest models to generate a single, effective tool for evaluating economic development levels. The experimental findings suggest that the proposed hybrid assessment technique is effective across several dimensions. The proposed CBDT demonstrated its capacity to adapt to changing data circumstances in evaluating low-carbon efficiency using real-world data from 30 major Chinese cities. Furthermore, the algorithm regularly beats rival techniques regarding efficiency improvements, with an average improvement of 12.5% across all cities. Notably, it outperforms other algorithms regarding computational efficiency, with a quicker execution time and an exceptional accuracy rate of 93.2%. The proposed ML Algorithm improves decision-making in the economic protection development level field by effectively balancing cost and benefit considerations. It will improve accuracy and efficiency to outperform existing approaches, thereby advancing economic protection evaluation and decision-making processes in this domain.",
        "DOI": "10.1007/s00500-023-09324-0",
        "affiliation_name": "Shanxi Vocational and Technical College of Finance and Trade",
        "affiliation_city": "Taiyuan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Forecasting disruptions in global food value chains to tackle food insecurity: The role of AI and big data analytics – A bibliometric and scientometric analysis",
        "paper_author": "Tamasiga P.",
        "publication": "Journal of Agriculture and Food Research",
        "citied_by": "26",
        "cover_date": "2023-12-01",
        "Abstract": "Globalization and interconnected supply chains have led to complex disruptions in global value chains, caused by various factors such as natural disasters, climate events, geopolitical conflicts, and economic crises. Recent breakthroughs in AI, machine learning, blockchain, and big data analytics offer new possibilities for forecasting and managing these disruptions effectively. This study examines the role of AI in forecasting and managing disruptions within global value chain to tackle food insecurity. We conducted a bibliometric and scientometric analysis using comprehensive data from Scopus and Web of Science to explore emerging research trends, influential publications, leading institutions, collaborations, themes, policy implications, and future research avenues. The research revealed an average yearly growth rate of 13.78 % in publications from 1973 to 2022. China, the United Kingdom, and the United States lead in AI applications to address supply chain disruptions, particularly concerning food insecurity. Frequently used keywords include “food security,” \"supply chain management,\" “agriculture,” \"modelling,\" “climate change,” and “COVID-19.” Themes identified focus on the impact of COVID-19 on food supply chains, achieving food security amidst climate change, leveraging predictive models in agriculture, and assessing the impact of disruptions on food price volatility and global supply chain risk assessment approaches. The insights gained from this research offer valuable guidance for policymakers and researchers to enhance food security. The identified themes provide direction for future research efforts in advancing food security amidst uncertainties and disruptions in global value chains.",
        "DOI": "10.1016/j.jafr.2023.100819",
        "affiliation_name": "University of Botswana",
        "affiliation_city": "Gaborone",
        "affiliation_country": "Botswana"
    },
    {
        "paper_title": "Is the relationship between the perceived service quality and passenger loyalty linear or non-linear? A novel model-independent interpretation method is applied",
        "paper_author": "Sun S.",
        "publication": "Transport Policy",
        "citied_by": "2",
        "cover_date": "2023-12-01",
        "Abstract": "Maintaining passenger loyalty to Public Transportation (PT) services is an effective way to sustain ridership of this eco-friendly transport mode. Although numerous studies have investigated how passengers' perception of service quality influences their loyalty and proposed key determinants to secure their PT loyalty, it remains unclear whether these relationships are independent of the methods used for linear or non-linear analysis in different contexts. Given the impact of this question on relevant policy-making, this study simultaneously investigated the relationships between passengers' perceived service quality of specific attributes and their overall loyalty using both linear (Structural Equation Modeling, SEM) and non-linear (Gradient Boosting Decision Tree, GBDT) methods. To interpret the results of the two models, the Total Accumulated Local Effects (TALE) method, a variant of the Accumulated Local Effects (ALE) method, was proposed as a model-independent approach. The case study was conducted in Xiamen, China. The results showed that TALE had an advantage over the conventional ALE method because it considered the impact of the actual score distribution of the samples on the results, making the interpretation more realistic. Furthermore, it was found that there were no significant differences in the results of the normalized TALE in GBDT and SEM for most explanatory variables, both indicating similar non-linear trends. Therefore, regardless of the employed methods, the natural existence of non-linear relationships between increased perceived service quality and improved overall PT loyalty was observed. After determining the relationship between the perceived service quality and overall loyalty, the Importance-Performance Analysis (IPA) method was combined with the TALE of each explanatory variable to suggest key determinants that would enhance passengers’ overall loyalty.",
        "DOI": "10.1016/j.tranpol.2023.10.003",
        "affiliation_name": "Dalian Maritime University",
        "affiliation_city": "Dalian",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Modelling habitat suitability of the critically endangered Agarwood (Aquilaria malaccensis) in the Indian East Himalayan region",
        "paper_author": "Hazarika A.",
        "publication": "Biodiversity and Conservation",
        "citied_by": "8",
        "cover_date": "2023-12-01",
        "Abstract": "Agarwood (Aquilaria malaccensis) is a critically endangered species on the IUCN red list globally. It is highly valued for its aromatic wood, popularly called agar, which has been overexploited in its natural range. The natural habitat of Agarwood is also drastically changing due to anthropogenic activities, and the tree is no longer in existence in the wild except in some intact forest patches. In the Indian East Himalayan region, Agarwood is planted in indigenous agroforestry and communal lands. Here, species distribution modelling (SDM) was used to establish Agarwood’s current and future habitat suitability under two representative climate change scenarios (RCP4.5 and RCP8.5). We implemented an ensemble of five SDM models, including two regression and three machine learning methods, using datasets from 82 locations. The results revealed that of the total area of 269,159 km2 of the Indian East Himalayan region, 2282 km2 is currently highly suitable. However, it is predicted to decrease by 34.28% under RCP4.5 and 14.64% under RCP8.5 by 2050, whereas 44.88% of the highly suitable area would reduce under RCP4.5 and 27.18% under RCP8.5 by 2070. Under the current climate scenario, the state of Assam has a substantial area potentially suitable for Agarwood, representing 99.21% of the state’s land area (2264 km2). However, there will be a tremendous loss of suitable habitats in future climate change scenarios. Our study highlights that precipitation of the driest quarter, precipitation of the warmest quarter and mean temperature of the driest quarter were the most influential bioclimatic variables. In addition, sand, clay content and organic carbon density were critical predictors of the distribution of Agarwood. This study has provided baseline data on the potentially suitable habitat of the species to guide the formulation and implementation of policies and practices for the conservation of Agarwood in the Indian East Himalayan region.",
        "DOI": "10.1007/s10531-023-02727-3",
        "affiliation_name": "Addis Ababa University",
        "affiliation_city": "Addis Ababa",
        "affiliation_country": "Ethiopia"
    },
    {
        "paper_title": "Antenna Placement Optimization for Distributed MIMO Radar Based on a Reinforcement Learning Algorithm",
        "paper_author": "Zhu J.",
        "publication": "Scientific Reports",
        "citied_by": "5",
        "cover_date": "2023-12-01",
        "Abstract": "This paper studies an optimization problem of antenna placement for multiple heading angles of the target in a distributed multiple-input multiple-output (MIMO) radar system. An improved method to calculate the system’s coverage area in light of the changing target heading is presented. The antenna placement optimization problem is mathematically modelled as a sequential decision problem for compatibility with reinforcement learning solutions. A reinforcement learning agent is established, which uses the long short-term memory (LSTM)-based proximal policy optimization (PPO) method as the core algorithm to solve the antenna placement problem. Finally, the experimental findings demonstrate that the method can enhance the coverage area of antenna placement and thus has reference value for providing new ideas for the antenna placement optimization of distributed MIMO radar.",
        "DOI": "10.1038/s41598-023-43076-z",
        "affiliation_name": "China Electronic Technology Group Corporation",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Advances in energy system modeling, sector coupling, and emission reduction strategies",
        "paper_author": "Prina M.G.",
        "publication": "e-Prime - Advances in Electrical Engineering, Electronics and Energy",
        "citied_by": "5",
        "cover_date": "2023-12-01",
        "Abstract": "The critical need to mitigate climate change brings with it prospects to transform societies globally onto more sustainable pathways. The articles in this special issue of e-Prime - Advances in Electrical Engineering, Electronics and Energy provide timely research advances that support integrated approaches for effective climate mitigation based on the sustainable development of energy, water and environment systems. The articles are organized into three main themes: (1) smart energy systems and sector coupling, (2) Energy Systems Modeling and Analysis, and (3) Emission Reduction Strategies and policies. The Decarbonization and Sector Coupling theme includes articles analyzing electrification and heat pump integration in industrial processes, optimization of nuclear-renewable hybrid energy systems, electric vehicle growth projections in Brazil, and impacts on electricity demand and fossil fuel reduction. The Energy Systems Modeling and Analysis theme covers contributions on offshore wind farm design software tools, optimization of future European energy mixes considering risks, climate change impacts on wind resources in the Black Sea region, and visualizations for home electricity monitoring systems. The Resource and Emission Reduction Strategies theme comprises articles evaluating policy instruments for CO2 utilization, machine learning for projecting passenger vehicle fleet growth in Brazil, and resulting energy and emissions outcomes. Together, these articles provide timely research advances across a range of topics relevant to integrated approaches for climate change mitigation through sustainable energy, water and environment systems. The articles emphasize opportunities for decarbonization, renewable energy deployment, resource efficiency, and emission reductions across multiple sectors and systems.",
        "DOI": "10.1016/j.prime.2023.100316",
        "affiliation_name": "Sveučilište u Zagrebu, Fakultet Strojarstva i Brodogradnje",
        "affiliation_city": "Zagreb",
        "affiliation_country": "Croatia"
    },
    {
        "paper_title": "Evaluation of various machine learning prediction methods for particulate matter PM<inf>10</inf> in Kuwait",
        "paper_author": "Alsaber A.",
        "publication": "International Journal of Information Technology (Singapore)",
        "citied_by": "7",
        "cover_date": "2023-12-01",
        "Abstract": "Air pollution poses a serious threat to public health and for the environment, thus predicting air quality is very crucial for the health and well-being of individuals and the environment. Economic development drives rapid industrialization and urbanization, which are significant sources of air pollution in developing countries. Kuwait’s rapid urbanization and vehicular traffic, along with dust storms, make it a prime location for research for environmental pollution. Keeping this in view, a study was designed to evaluates various machine learning prediction methods for particulate matter concentrations (PM10) in Kuwait. The prediction models were developed using three different algorithms, including k-nearest neighbor (KNN), artificial neural network (ANN) and support vector regression (SVR). The models were developed using a 3-year dataset collected by Kuwait Environmental Public Authority (K-EPA) for two stations selected in this study (Al-Ahmadi and Al-Salam). The performance of the models was evaluated using various metrics, including Mean Biased Error (MBE), Root Mean Squared Error (RMSE), Normalized Root Mean Squared Error (nRMSE) and Coefficient of Determination (R2). The results show that both stations experienced severe air quality issues and that particulate matter concentrations (PM10) are strongly influenced by the different meteorological and pollutant variables. The findings show that for the Al-Ahmadi location, artificial neural networks (ANN) (Rcal2 = 0.885, Rval2 = 0.775) and K-nearest neighbor (KNN) (Rcal2 = 0.895, Rval2 = 0.613) were good, while for Al-Salam, KNN (Rcal2 = 0.945, Rval2 = 0.715) was a better choice to predict PM10 . These models can be used by the decision-makers to impose pollution controls, evaluate policies, or plan targeted actions to reduce particle matter.",
        "DOI": "10.1007/s41870-023-01521-2",
        "affiliation_name": "American University of Kuwait",
        "affiliation_city": "Kuwait City",
        "affiliation_country": "Kuwait"
    },
    {
        "paper_title": "Political referenda and investment: Evidence from Scotland",
        "paper_author": "Azqueta-Gavaldon A.",
        "publication": "European Journal of Political Economy",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "We present evidence that referenda have a significant, detrimental outcome on investment. Employing an unsupervised machine learning algorithm over the period 2008-2017, we construct three important uncertainty indices underlying reports in the Scottish news media: Scottish independence (IndyRef)-related uncertainty; Brexit-related uncertainty; and Scottish policy-related uncertainty. Examining the relationship of these indices with investment on a longitudinal panel of 3,589 Scottish firms, the evidence suggests that Brexit-related uncertainty associates more strongly than IndyRef-related uncertainty to investment. Our preferred specification suggests that a one standard-deviation increase in Brexit uncertainty foreshadows a reduction in investment by 8% on average in the following year. Besides we find that the uncertainty associated with the Scottish referendum for independence while negligible at the aggregate level, relates more strongly with the investment of listed firms as well as those operating on the border with England. In addition, we present evidence of greater sensitivity to these indices among firms that are financially constrained or whose investment is to a greater degree irreversible.",
        "DOI": "10.1016/j.ejpoleco.2023.102474",
        "affiliation_name": "Arcturis Data",
        "affiliation_city": "Kidlington",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Modeling challenges for Industry 4.0 implementation in new energy systems towards carbon neutrality: Implications for impact assessment policy and practice in emerging economies",
        "paper_author": "Moktadir M.A.",
        "publication": "Resources, Conservation and Recycling",
        "citied_by": "16",
        "cover_date": "2023-12-01",
        "Abstract": "New energy systems (NESs) in a circular economy (CE) perspective may contribute significantly to achieving emerging economies’ carbon peaking and neutrality goals. However, CE-based NESs in developing countries towards carbon peaking and neutrality remain unexplored in scientific research and development. Additionally, challenges for I4.0 implementation in the CE context for NESs are still under research gaps in emerging economies. The previous research mainly focused on qualitative discussions on carbon neutrality, I4.0 challenges, and opportunities for renewable energy generation from waste biomass. Therefore, this scientific work, for the first time, offered an innovative decision support model integrating qualitative management tool OGSM (Objective, Goals, Strategies, and Measures), Dombi t-norm and Dombi t-conorm operator based Interval-valued Fermatean fuzzy number (IVFFN) Delphi and IVFFN-Decision Making Trial and Evaluation Laboratory (DEMATE) as a new extension of DEMATEL for assessing interactions among I4.0 implementing challenges in a CE context for NESs towards carbon neutrality which can handle border scale of uncertainty and ambiguity. The findings indicated that out of fifteen I4.0 challenges, nine were identified as causal challenges and the rest as effect group challenges, suggesting that improving causal challenges can notably enhance the effect group challenges. “Lack of aptitude to integrate sophisticated I4.0 infrastructure for NESs” has been identified as the top priority causal challenge that significantly impacts the effect group challenges. Further, based on the findings of IVFFN-DEMATEL, short-, mid-, and long-term action plans are proposed for real-time decision-makers to improve the existing situations. The output of the study may contribute robustly to the real-time scenarios of NESs by supporting decision-makers, researchers, government authorities, and stakeholders towards developing sustainable CE-based NESs by integrating I4.0 technologies, which will help to achieve a carbon-neutral economy in emerging economies.",
        "DOI": "10.1016/j.resconrec.2023.107246",
        "affiliation_name": "The Hong Kong Polytechnic University",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong"
    },
    {
        "paper_title": "COVID-Net Biochem: an explainability-driven framework to building machine learning models for predicting survival and kidney injury of COVID-19 patients from clinical and biochemistry data",
        "paper_author": "Aboutalebi H.",
        "publication": "Scientific Reports",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "Since the World Health Organization declared COVID-19 a pandemic in 2020, the global community has faced ongoing challenges in controlling and mitigating the transmission of the SARS-CoV-2 virus, as well as its evolving subvariants and recombinants. A significant challenge during the pandemic has not only been the accurate detection of positive cases but also the efficient prediction of risks associated with complications and patient survival probabilities. These tasks entail considerable clinical resource allocation and attention. In this study, we introduce COVID-Net Biochem, a versatile and explainable framework for constructing machine learning models. We apply this framework to predict COVID-19 patient survival and the likelihood of developing Acute Kidney Injury during hospitalization, utilizing clinical and biochemical data in a transparent, systematic approach. The proposed approach advances machine learning model design by seamlessly integrating domain expertise with explainability tools, enabling model decisions to be based on key biomarkers. This fosters a more transparent and interpretable decision-making process made by machines specifically for medical applications. More specifically, the framework comprises two phases: In the first phase, referred to as the “clinician-guided design” phase, the dataset is preprocessed using explainable AI and domain expert input. To better demonstrate this phase, we prepared a benchmark dataset of carefully curated clinical and biochemical markers based on clinician assessments for survival and kidney injury prediction in COVID-19 patients. This dataset was selected from a patient cohort of 1366 individuals at Stony Brook University. Moreover, we designed and trained a diverse collection of machine learning models, encompassing gradient-based boosting tree architectures and deep transformer architectures, specifically for survival and kidney injury prediction based on the selected markers. In the second phase, called the “explainability-driven design refinement” phase, the proposed framework employs explainability methods to not only gain a deeper understanding of each model’s decision-making process but also to identify the overall impact of individual clinical and biochemical markers for bias identification. In this context, we used the models constructed in the previous phase for the prediction task and analyzed the explainability outcomes alongside a clinician with over 8 years of experience to gain a deeper understanding of the clinical validity of the decisions made. The explainability-driven insights obtained, in conjunction with the associated clinical feedback, are then utilized to guide and refine the training policies and architectural design iteratively. This process aims to enhance not only the prediction performance but also the clinical validity and trustworthiness of the final machine learning models. Employing the proposed explainability-driven framework, we attained 93.55% accuracy in survival prediction and 88.05% accuracy in predicting kidney injury complications. The models have been made available through an open-source platform. Although not a production-ready solution, this study aims to serve as a catalyst for clinical scientists, machine learning researchers, and citizen scientists to develop innovative and trustworthy clinical decision support solutions, ultimately assisting clinicians worldwide in managing pandemic outcomes.",
        "DOI": "10.1038/s41598-023-42203-0",
        "affiliation_name": "University of Waterloo",
        "affiliation_city": "Waterloo",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "A data-driven newsvendor problem: A high-dimensional and mixed-frequency method",
        "paper_author": "Yang C.H.",
        "publication": "International Journal of Production Economics",
        "citied_by": "2",
        "cover_date": "2023-12-01",
        "Abstract": "In this paper, a data-driven newsvendor problem is studied by mapping high-dimensional and mixed-frequency features of historical data to replenishment decisions. Instead of relying on a known demand distribution, we propose using machine learning algorithms that incorporate demand features into the replenishment decisions to solve single- and multi-product newsvendor problems. In particular, our algorithms simultaneously optimize the demand estimation and replenishment decisions. To extract key features from the historical data, we propose a frequency alignment method to transform high-dimensional mixed-frequency data and historical data into the same frequency. We then propose two variable selection policies based on the empirical risk minimization principle, and employ the regularization method to tackle the parameter proliferation issue. In addition, a feature-based machine learning algorithm is designed to solve the multi-product newsvendor problem. Finally, we numerically justify the performances of proposed machine learning algorithms. We find that (1) data-informed replenishment decisions can effectively leverage the identified key demand features to avoid losing demand information, and (2) the optimal replenishment quantity behaves robustly and shows minimal variation across different cost structures. Our work provides meaningful insights for newsvendors making replenishment decisions under stochastic demand.",
        "DOI": "10.1016/j.ijpe.2023.109042",
        "affiliation_name": "Broad College of Business",
        "affiliation_city": "East Lansing",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "A novel machine learning-based framework for channel bandwidth allocation and optimization in distributed computing environments",
        "paper_author": "Xu M.",
        "publication": "Eurasip Journal on Wireless Communications and Networking",
        "citied_by": "4",
        "cover_date": "2023-12-01",
        "Abstract": "Efficient utilization of network resources, particularly channel bandwidth allocation, is critical for optimizing the overall system performance and ensuring fair resource allocation among multiple distributed computing nodes. Traditional methods for channel bandwidth allocation, based on fixed allocation schemes or static heuristics, often need more adaptability to dynamic changes in the network and may not fully exploit the system’s potential. To address these limitations, we employ reinforcement learning algorithms to learn optimal channel allocation policies by intermingling with the environment and getting feedback on the outcomes of their actions. This allows devices to adapt to changing network conditions and optimize resource usage. Our proposed framework is experimentally evaluated through simulation experiments. The results demonstrate that the framework consistently achieves higher system throughput than conventional static allocation methods and state-of-the-art bandwidth allocation techniques. It also exhibits lower latency values, indicating faster data transmission and reduced communication delays. Additionally, the hybrid approach shows improved resource utilization efficiency, efficiently leveraging the strengths of both Q-learning and reinforcement learning for optimized resource allocation and management.",
        "DOI": "10.1186/s13638-023-02310-y",
        "affiliation_name": "China United Network Communications Corporation Shangqiu Branch",
        "affiliation_city": "Shangqiu",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Stable skill improvement of quadruped robot based on privileged information and curriculum guidance",
        "paper_author": "Jiang H.",
        "publication": "Robotics and Autonomous Systems",
        "citied_by": "5",
        "cover_date": "2023-12-01",
        "Abstract": "Quadruped robots have attracted many researchers due to their unique advantages. In the field of control, deep reinforcement learning (DRL) saves the complex and tedious design of traditional control algorithms and motivates the robot to learn the motion patterns by itself, which is a promising alternative approach. However, due to the lack of prior information that is difficult to obtain in the physical world, policy usually exhibits a single motion state when faced with multiple scenarios. In this paper, we adopt a teacher–student policy architecture that uses the student policy to implicitly identify the teacher policy containing a privileged information, enabling the robot to adjust its motion depending on the environment it is in. We propose the synthetic terrain-command curriculum that assigns different levels of command curriculum depending on the difficulty of the terrain, which is useful for improving policy adaptation and stability. Policy is often trapped in local optima due to the nonlinearity of the robot system and unreasonable parameters. We propose a self-correction mechanism to jump out of this state in time, so that the policy can be steadily improved. We demonstrate our policy on the real quadruped platform named SDUQuad-48, and the results show that the learned policy exhibits state-of-the-art performance in both high-speed locomotion and complex terrain adaptation.",
        "DOI": "10.1016/j.robot.2023.104550",
        "affiliation_name": "Shandong University",
        "affiliation_city": "Jinan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Attitudes and Latent Class Choice Models using Machine Learning",
        "paper_author": "Lahoz L.T.",
        "publication": "Journal of Choice Modelling",
        "citied_by": "3",
        "cover_date": "2023-12-01",
        "Abstract": "Latent Class Choice Models (LCCM) are extensions of discrete choice models (DCMs) that capture unobserved heterogeneity in the choice process by segmenting the population based on the assumption of preference similarities. We present a method of efficiently incorporating attitudinal indicators in the specification of LCCM, by introducing Artificial Neural Networks (ANN) to formulate latent variables constructs. This formulation overcomes structural equations in its capability of exploring the relationship between the attitudinal indicators and the decision choice, given the Machine Learning (ML) flexibility and power in capturing unobserved and complex behavioural features, such as attitudes and beliefs. All of this while still maintaining the consistency of the theoretical assumptions presented in the Generalized Random Utility model and the interpretability of the estimated parameters. We test our proposed framework for estimating a Car-Sharing (CS) service subscription choice with stated preference data from Copenhagen, Denmark. The results show that our proposed approach provides a complete and realistic segmentation, which helps design better policies.",
        "DOI": "10.1016/j.jocm.2023.100452",
        "affiliation_name": "Technical University of Denmark",
        "affiliation_city": "Lyngby",
        "affiliation_country": "Denmark"
    },
    {
        "paper_title": "Gaze-probe joint guidance with multi-task learning in obstetric ultrasound scanning",
        "paper_author": "Men Q.",
        "publication": "Medical Image Analysis",
        "citied_by": "4",
        "cover_date": "2023-12-01",
        "Abstract": "In this work, we exploit multi-task learning to jointly predict the two decision-making processes of gaze movement and probe manipulation that an experienced sonographer would perform in routine obstetric scanning. A multimodal guidance framework, Multimodal-GuideNet, is proposed to detect the causal relationship between a real-world ultrasound video signal, synchronized gaze, and probe motion. The association between the multi-modality inputs is learned and shared through a modality-aware spatial graph that leverages useful cross-modal dependencies. By estimating the probability distribution of probe and gaze movements in real scans, the predicted guidance signals also allow inter- and intra-sonographer variations and avoid a fixed scanning path. We validate the new multi-modality approach on three types of obstetric scanning examinations, and the result consistently outperforms single-task learning under various guidance policies. To simulate sonographer's attention on multi-structure images, we also explore multi-step estimation in gaze guidance, and its visual results show that the prediction allows multiple gaze centers that are substantially aligned with underlying anatomical structures.",
        "DOI": "10.1016/j.media.2023.102981",
        "affiliation_name": "University of Oxford",
        "affiliation_city": "Oxford",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "A Data-Driven Recommendation System for Construction Safety Risk Assessment",
        "paper_author": "Mostofi F.",
        "publication": "Journal of Construction Engineering and Management",
        "citied_by": "9",
        "cover_date": "2023-12-01",
        "Abstract": "Subjectivity and uncertainty of risk assessment (RA) procedures can be improved by replacing guesswork with data-driven approaches such as machine learning (ML). Although a plethora of ML prediction techniques have been introduced to improve the reliability of RA procedures, the utilization of ML-based recommendation systems that can leverage data from multiple aspects has remained unexplored. In this study, a novel RA recommendation system (RARS) was developed to achieve more reliable, objective, and inclusive safety decisions that can prioritize hazard items and formulate related risky scenarios. To this end, a semisupervised graph representation learning framework, node2vec, was utilized to receive semantic and dependency information from safety records to recommend the components of potential accident scenarios (hazards, hazardous cases, dangerous activities, and risky behaviors) based on the given decision objective. The RARS's ability to provide flexible and user-oriented safety recommendations was explored on a real-life construction accident data set. This allows construction safety practitioners to dynamically evaluate possible risky scenarios with details regarding different influential risk factors and accordingly devise more reliable site safety strategies and relevant policies.",
        "DOI": "10.1061/JCEMD4.COENG-13437",
        "affiliation_name": "Karadeniz Technical University",
        "affiliation_city": "Trabzon",
        "affiliation_country": "Turkey"
    },
    {
        "paper_title": "The Impact of Built Environment on the Commuting Distance of Middle/Low-income Tenant Workers in Mega Cities Based on Nonlinear Analysis in Machine Learning",
        "paper_author": "Shen L.",
        "publication": "Urban Rail Transit",
        "citied_by": "2",
        "cover_date": "2023-12-01",
        "Abstract": "The issues of housing and traffic in China's mega cities have become increasingly pressing problems, particularly for middle/low-income tenant workers. These tenants are from less advantaged socioeconomic backgrounds, which has resulted in a significant geographical separation between their workplace and their residence. Although a large number of studies have confirmed that built environment factors have a solid impact on residents’ commuting distance, few studies have investigated the mechanism underlying the nonlinear influence on middle/low-income tenants. This paper aims to provide an in-depth analysis of the key factors and nonlinear influencing mechanism of the built environment on middle/low-income tenant workers’ commuting distance by establishing a gradient-boosting decision tree model, using Beijing as an empirical case. The paper reveals three primary findings: (1) An important nonlinear relationship between the surrounding built environment and peoples’ jobs–housing spatial proximity can be observed for those middle/low-income tenant workers who use slow and public modes of commuting. Specifically, the density of public transport stations, road networks, and workplaces, and the land use mix play a dominant role. (2) A limited effect of built environment factors can be found for the same group of tenant workers who choose cars as their mode of commuting. (3) The differences in self-selected commuting modes have a significant mediating effect on the relationship between the built environment and jobs–housing situation among middle/low-income tenant workers. Given this, effective policy guidance for residents’ travel modes is necessary to optimize the built environment indicators to achieve the best effect. In addition, we should consider giving priority to the matching indicators such as land use mix and resident population density. Another possibility is to strengthen the connection to the public transport stations, which in turn can optimize the walkability in residential environments.",
        "DOI": "10.1007/s40864-023-00202-4",
        "affiliation_name": "Chinese Academy of Surveying and Mapping",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Optimizing LSTM and Bi-LSTM models for crop yield prediction and comparison of their performance with traditional machine learning techniques",
        "paper_author": "Kiran Kumar V.",
        "publication": "Applied Intelligence",
        "citied_by": "10",
        "cover_date": "2023-12-01",
        "Abstract": "Advance prediction of crop yield is very critical in the context of ensuring food security as the region specific challenges in social and environmental conditions often infringe plan of policy makers. This study presents a generic methodology to configure and fine tune the state-of-the-art Long Short-Term Memory (LSTM) based Deep Learning (DL) model through hyperparameter optimization for prediction of yield (annual crop production) in Wheat, Groundnut and Barely over India based on multiple independent input variables identified using multicollinearity test. The Monte Carlo cross-validation method is used to validate the optimized LSTM models. Results from the LSTM model tuning showed that among the 4 optimizers tested, Adam was found to perform better irrespective of the crop and Bi-LSTM outperformed sLSTM in terms of prediction accuracy. The percentage reduction in error with Bi-LSTM compared to sLSTM in predicting wheat and groundnut crop yield was 39% and 13% respectively while in case of barley crop, error reduction was marginal (0.34%). The performance of optimized Bi-LSTM model is compared with the performance of traditional machine learning (ML) models such as support vector regression (SVR) and SVR polynomial {2nd and 3rd order}, Auto Regressive Integrated Moving Average (ARIMA) and ARIMAX (ARIMA with exogenous variables) and Vector Auto-regression (VAR). The Bi-LSTM model is found to be superior to ML models; the percentage reduction in mean absolute scaled error with the Bi-LSTM compared to the best performing ML model was 94%, 72%, and 71% in predicting wheat, groundnut and barley yield respectively. This study showed that by choosing proper explanatory (independent) variable and hyperparameter optimization, a simple (single layer) structure of deep neural network (LSTM) outperformed traditional ML models in terms of accuracy for crop yield prediction application.",
        "DOI": "10.1007/s10489-023-05005-5",
        "affiliation_name": "Council of Scientific and Industrial Research India",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India"
    },
    {
        "paper_title": "The Effect of the Chinese Industry Sector in Predicting Oil Price: Evidence from Information Geometric Causal Inference and GWO-ELM",
        "paper_author": "Liang J.",
        "publication": "Fluctuation and Noise Letters",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "The COVID-19 outbreak and the implementation of peak and carbon neutral policies have severely impacted oil price volatility and the industrial sector. Exploring the impact mechanisms between oil prices and industries is particularly important for accurate forecasting of crude oil prices. As one of the world's largest commodity consumers, China's crude oil market is more representative and susceptible to external factors than that of developed countries. In this paper, we propose an analytical forecasting framework based on the causal effects between Shanghai crude oil prices and various industries in China to improve the forecasting accuracy of crude oil prices. Information geometric causal inference (IGCI) is applied to detect causal relationships between 31 different industries in China and Shanghai crude oil prices in the three time periods before, during and after COVID-19, and industries with strong causal information effects on crude oil prices in the long run are screened out as additional features. An oil price forecasting model based on Gray Wolf Optimization and Extreme Learning Machine (GWO-ELM) is proposed. Considering the small amount of data for Shanghai crude oil, this paper proposes a cross-learning data approach to solve the problem. Experimental results show that the GWO-ELM model outperforms RF, LSTM, GRU, and migration learning-based Tr-LSTM and Tr-Adaboost models in the task of Shanghai crude oil futures price prediction, and find that industry characteristics with long-term causal effects on oil prices can improve the model prediction accuracy. Our proposed analytical prediction can capture the oil price trend more accurately through the information of the industry and solve the problem of insufficient training data for the model. The application of this framework is expected to provide new methods and ideas for data mining of crude oil and other futures prices.",
        "DOI": "10.1142/S021947752350044X",
        "affiliation_name": "Sichuan Normal University",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Exploring carbon dioxide emissions forecasting in China: A policy-oriented perspective using projection pursuit regression and machine learning models",
        "paper_author": "Chang L.",
        "publication": "Technological Forecasting and Social Change",
        "citied_by": "20",
        "cover_date": "2023-12-01",
        "Abstract": "Achieving a balance between future greenhouse gas reduction and sustained economic growth is of utmost importance. This study leverages machine learning (ML), specifically projection pursuit regression (PPR), to evaluate the key factors that influence CO2 emission predictions in China. The analysis notably identifies the escalating electricity consumption as a primary influencing factor. Based on empirical findings, it is evident that building electricity consumption will continue to rise steadily until 2050 unless new restrictions or technological advancements are implemented. Relying solely on the reduced carbon intensity of electricity will not enable China to achieve carbon neutrality. Therefore, there is a pressing need for more energy-efficient building retrofits and technologies to reduce power consumption in both residential and commercial properties. This policy-oriented study underscores its practical implications, offering valuable insights to policymakers for developing targeted CO2 reduction strategies that align with sustainable development and climate goals.",
        "DOI": "10.1016/j.techfore.2023.122872",
        "affiliation_name": "Adnan Kassar School of Business",
        "affiliation_city": "Beirut",
        "affiliation_country": "Lebanon"
    },
    {
        "paper_title": "News Framing of Air Pollution Risks in Taiwanese Newspapers: A Longitudinal Study of Changes",
        "paper_author": "Tan Y.",
        "publication": "Mass Communication Research",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "The 2020s have witnessed a steady increase in the rates of haze-related diseases in Taiwan. Indeed, German sociologist Ulrich Beck (1992) criticized current development in societies that has produced unintended and unforeseen side-effects to modern life. He described that the new risks are historically unprecedented, invisible, technologically sophisticated, and highly uncertain. Social knowledge of contemporary environmental risks, independent from reality, is mainly acquired from mass media. Social constructionists argue that the media influence what and how things come to be defined as risks and facility developments of risk consciousness (Kitzinger, 1999; Beck, 1992). Therefore, to understand how Taiwanese society responds to haze-related risks, it is important to first know how news media report these risks and their consequences. The answers to these questions are not only needed to understand how media influence public understanding of the air pollution issue, but also to help researchers make more specific suggestions on how to promote public awareness of air-related risks and protection behaviors through news media in Taiwan. The concept of framing offers a powerful framework for understanding how a news report provides the “schemata of interpretation” that enable individuals to make sense of an otherwise meaningless succession of public events (Goffman, 1974). To frame is to make a persistent selection, emphasis, and exclusion (Gitlin, 1980). Consistent with this emphasis-based definition, Entman (1993) proposed a classical definition of framing with a focus on problem definition, causal interpretation, moral evaluation, and/or treatment recommendation for the issue described. As suggested by van der Meer (2018), we use these four functions to organize our analysis and discuss the topic themes that we obtain from unsupervised machine learning. Because uncertainty is the core component of risks (黃 俊 儒, 2014) and affective heuristics are the key determinant of risk perception and risk prevention behaviors (Nabi et al., 2020), both uncertainty and tone are analyzed in terms of their trends and their relationships with topic prevalence. The study examines air-pollution news from three major newspapers in Taiwan, including United Daily, Apple Daily, and Liberty Times from 2017 to 2021. Their news articles have been downloaded from their websites with web scrapers. The keywords for searching include “air quality” and “air pollution”. After manual cleaning of irrelevant articles, the total number of articles is 8,509. Automated content analysis of newspaper articles is conducted to measure media coverage in terms of their volumes, topics, tone, and uncertainty. Tone and uncertainty are measured with established dictionaries (CLIWC, Linguistic Inquiry, and Word Count). The topical themes are measured with unsupervised machine learning (topic modeling), whose algorithms learn hidden clusters (topics) in text data. A typical topic model observes word frequencies in each document in terms of a suitable weighted mixture of topical word frequencies where the weights indicate the different proportions of topics that appear in the document (Guo et al., 2016). Guo et al. (2016) found that LDA-based analysis performs better than a dictionary-based approach in many aspects. In this study we employ the most widely used topic model algorithm, Latent Dirichlet Allocation (LDA), with the R package of STM. Ten topics are chosen according to the four criteria from the R package of ldatuning. Current conditions for air pollution are operationally defined by 24-hour-average PM2.5 concentrations for each day. Such data are publicly available from the official websites of the Environmental Protection Administration, Executive Yuan of Taiwan (www.epa.gov.tw) and the U.S. Environmental Protection Agency (www.epa.gov). Among all kinds of harmful airs (e.g., PM2.5, PM10, O3, CO, SO2, NO2), we choose PM2.5 as the main indicator, because scholars use it the most often (Apte et al., 2018; Hayes et al., 2020; Colmer et al., 2020), and governments as the main index for overall air quality. Trend analyses are first performed to examine the linear and non-linear trends in term of news coverage volume, tone, and uncertainty with the R package of Forecast. The R package of STM, with the prevalence of each topic as a dependent variable, allows us to model the main effects of time, tone, uncertainty, and their interaction. Statistically, time series analysis (Vector Autoregressions Model, VAR) and Granger causality tests are conducted to examine the relationship between news media coverage and real-word condition of PM2.5 concentrations. The R package Var is then used to automatically determine the best time lag for each agenda-setting relationship. The time unit is set at one day. The analysis is conducted for each shared topic yielded by LDA. Findings show that the amount of media coverage does not decrease with the overall improvement of air pollution, but rather increases. The tone of a report deteriorates slightly over time. However, uncertainty in the content of the report remains generally unchanged. Employing Entman’s (1993) definition of frame functions (problem definition, causal interpretation, treatment recommendation, and moral evaluation), this study summarizes, compares, and discusses trends in the topical themes over the past five years. The results present that newspapers only devote a small proportion of coverage to the health hazards of air pollution, and this proportion has been decreasing year by year. The discussion of pollution sources gradually narrows down to industrial and agricultural pollution, fires, and transportation, ignoring pollution from the construction industry. Although the three major newspapers have discussed numerous solutions to air pollution, the solutions all relate to government policies, which has led to a rise in topics related to political elections year by year. The politicization of air-pollution news may lead the public to expect solutions from governmental regulations and even party elections, ignoring the seriousness of health risks and reducing their willingness to take individual protective actions (Adger, Quinn, Lorenzoni, Murphy & Sweeney, 2012). Finally, based on the literature, this study discusses the theoretical and practical implications of the results.",
        "DOI": "10.30386/MCR.202301.0002",
        "affiliation_name": "National Sun Yat-Sen University",
        "affiliation_city": "Kaohsiung",
        "affiliation_country": "Taiwan"
    },
    {
        "paper_title": "Estimating the effect of climate change exposure on firm value using climate policy uncertainty: A text-based approach",
        "paper_author": "Ongsakul V.",
        "publication": "Journal of Behavioral and Experimental Finance",
        "citied_by": "12",
        "cover_date": "2023-12-01",
        "Abstract": "Exploiting innovative measures of climate policy uncertainty and firm-specific climate change exposure derived from state-of-the-art textual analysis, we examine the impact of climate policy uncertainty on firm-specific climate change vulnerability and find that a rise in policy uncertainty exacerbates firm-specific exposure significantly. Then, using climate policy uncertainty to generate exogenous variation in firm-specific vulnerability in an instrumental-variable analysis, we show that companies more susceptible to climate change experience significantly lower firm value. In particular, an increase in firm-specific vulnerability by one standard deviation diminishes firm value by 5.4-5.9%. Climate policy uncertainty originates at the national level and is probably exogenous to individual firms’ characteristics. Our results are thus less vulnerable to endogeneity and likely reflect causality, rather than a mere correlation. Finally, the adverse effect of climate risk on firm value is more pronounced for firms with more fixed assets and for those located in the states or cities where the threat of climate change is more acute but is less evident for firms that pay higher dividends.",
        "DOI": "10.1016/j.jbef.2023.100842",
        "affiliation_name": "Sasin School of Management, Bangkok",
        "affiliation_city": "Bangkok",
        "affiliation_country": "Thailand"
    },
    {
        "paper_title": "Deep reinforcement learning for dynamic flexible job shop scheduling problem considering variable processing times",
        "paper_author": "Zhang L.",
        "publication": "Journal of Manufacturing Systems",
        "citied_by": "32",
        "cover_date": "2023-12-01",
        "Abstract": "In recent years, the uncertainties and complexity in the production process, due to the boosted customized requirements, has dramatically increased the difficulties of Dynamic Flexible Job Shop Scheduling (DFJSP). This paper investigates a new DFJSP model taking into account the minimum completion time under the condition of machine processing time uncertainty, e.t. VPT-FJSP problem. In the formulated VPT-FJSP process, each workpiece needs to be processed by required machine at a certain time slot where Markov decision process (MDP) and reinforcement learning methods are adopted to solve VPT-FJSP. The agent designed in this paper employs the Proximal Policy Optimization(PPO) algorithm in deep reinforcement learning, which includes the Actor-Critic network. The input of the network is to extract the processing information matrix and to embed some advanced states in the workshop by graph neural network, which enables the agent to learn the complete state of the environment. Finally, we train and test the proposed framework on the canonical FJSP benchmark, and the experimental results show that our framework can make agent better than genetic algorithm and ant colony optimization in most cases, 94.29% of static scheduling. It is also shown superiority compared to the scheduling rules in dynamic environment and has demonstrated strong robustness in solving VPT-FJSP. Furthermore, this study conducted tests to assess the generalization capability of the agent on VPT-FJSP at different scales. In terms of exploring Makespan minimization, the agent outperformed four priority scheduling rules. These results indicate that the proposed dynamic scheduling framework and PPO algorithm are more effective in achieving superior solutions.",
        "DOI": "10.1016/j.jmsy.2023.09.009",
        "affiliation_name": "Shenzhen Institute of Advanced Technology",
        "affiliation_city": "Shenzhen",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Does the lack of resources matter in a dual economy: Decoding MSMEs productivity and growth",
        "paper_author": "Bravo-Ortega C.",
        "publication": "Economic Analysis and Policy",
        "citied_by": "3",
        "cover_date": "2023-12-01",
        "Abstract": "In this paper, we examine the factors influencing labor productivity and sales growth among micro, small, and medium-sized enterprises (MSMEs) within a middle-income economy. Although MSMEs play a pivotal role in employment within middle-income countries, they often display a lower economic value-added, pointing to underlying challenges in labor productivity. Using a longitudinal dataset of firms and employing both quantile regressions and machine learning techniques, we find that SMEs led by older, male, and more seasoned managers tend to exhibit higher productivity. Similarly, companies with a larger proportion of highly educated employees, affiliation to business groups, and engagement in R&D activities demonstrate superior performance. Finally, to improve the performance of MSMEs in developing economies, our results suggest that implementing targeted, well-defined vertical public support programs would be an effective public policy approach.",
        "DOI": "10.1016/j.eap.2023.08.022",
        "affiliation_name": "Universidad Adolfo Ibáñez",
        "affiliation_city": "Santiago",
        "affiliation_country": "Chile"
    },
    {
        "paper_title": "Simulating Seoul's greenbelt policy with a machine learning-based land-use change model",
        "paper_author": "Jun M.J.",
        "publication": "Cities",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "This study builds a machine-learning-based land-use change (ML-LUC) model to analyze the effect of green belt (GB) regulation in the Seoul metropolitan area (SMA) and predict the spatially explicit development potential of the land within the GB under the assumption of a no-GB policy scenario. It stands out for its ML-LUC application to simulate counterfactual planning for urban land use regulation. After comparing the predictive power of extreme gradient boosting (XGB), random forest (RF), and artificial neural network (ANN), the ML-LUC model utilizes the XGB algorithm due to its outperformance. Three scenarios based on SMA's land market demand were simulated to estimate the potential population and employment within the GB under the no-GB policy: high, moderate, and low land market demand. The results suggest 0.6 to 1.5 million residents, 0.2 to 0.5 million manufacturing jobs, and 0.4 to 1.0 million service sector jobs could have been located within the GB, accounting for 3 % to 6 % of total population and 5 % to 13 % of all employment in SMA. The findings imply the GB regulation prevents population and employment from locating within the GB, pushing them to central Seoul or suburbs beyond the GB under a closed-city assumption.",
        "DOI": "10.1016/j.cities.2023.104580",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "A multi-dimensional spatial index for the quantification of food insecurity",
        "paper_author": "Dawood F.",
        "publication": "Journal of Agriculture and Food Research",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "Food insecurity is a multifaceted problem. It is, in fact, one of the most significant concerns of the 21st century. According to the United Nations, nearly one in every three people, or 2.3 billion people worldwide, experienced a moderate to severe degree of food insecurity in 2021. As a result, there is a rising recognition of the critical need for successfully identifying, monitoring, and improving the food security of vulnerable populations. An effective method for anticipating the degree to which a region may be considered food insecure, however, is required before attempting to launch remedies for such a situation. To this end, we propose a novel machine learning-based spatial index in this article for estimating the degree of food insecurity experienced by a population. A real-world case study pertaining to South Africa is presented which demonstrates the applicability of our modelling approach. In particular, our roadmap designed towards developing the index is based on a spatial analysis of the study region and the construction of an appropriate data set. Following that, the strength of machine learning algorithms is harnessed to estimate the risk of food insecurity. Finally, the results of the aforementioned components are analysed to determine the causes and features of food insecure regions. The demonstration of the roadmap presented in this article exhibits promising results in its ability to classify 92.3% of food-insecure regions in South Africa correctly, thereby potentially supporting decision makers in their quest to improve food security policies and programmes. The research presented here may, in future, be extended by including additional descriptive features in the data set, increasing decision support utility through actionable recommendations, and exploring the temporal aspect of food security by incorporating cross-sectional data.",
        "DOI": "10.1016/j.jafr.2023.100768",
        "affiliation_name": "Stellenbosch University",
        "affiliation_city": "Stellenbosch",
        "affiliation_country": "South Africa"
    },
    {
        "paper_title": "PI-ELM: Reinforcement learning-based adaptable policy improvement for dynamical system",
        "paper_author": "Hu Y.",
        "publication": "Information Sciences",
        "citied_by": "10",
        "cover_date": "2023-12-01",
        "Abstract": "Behavioral cloning of imitation learning is theoretically sound that can capture and generate the motor skills from expert demonstrations, but they suffer poor adaptability with a small dataset in a new environment. This study improves adaptability, by proposing a novel reinforcement learning strategy for low-level behavioral learning with a small number of expert demonstrations. Specifically, the policy improvement-based reinforcement learning framework is divided into two phases: the low level is based on supervised learning using extreme learning machine (ELM) to clone the behavior from demonstrations, which can further be represented as a dynamical system with policy parameters; and the high level reinforcement learning improves the adaptability of ELM in new tasks. In this paper, we bridge the gap between machine learning and stochastic optimal control systems and propose the improved path integral-based reinforcement learning PI-ELM strategy to learn the policy parameters from low-level ELM. The proposed framework's performance and effectiveness are illustrated through several task experiments. The results indicate that our method can significantly improve the adaptability of imitation learning in new scenarios, including single task obstacle avoidance, via-points, antidisturbance, or hybrid tasks.",
        "DOI": "10.1016/j.ins.2023.119700",
        "affiliation_name": "Nantong University",
        "affiliation_city": "Nantong",
        "affiliation_country": "China"
    },
    {
        "paper_title": "The impact of carbon policy on corporate risk-taking with a double/debiased machine learning based difference-in-differences approach",
        "paper_author": "Xing L.",
        "publication": "Finance Research Letters",
        "citied_by": "4",
        "cover_date": "2023-12-01",
        "Abstract": "This study adopts the more cutting-edge DMLDID (double/debiased machine learning based difference-in-differences) approach to demonstrate the impact of carbon policy on corporate risk-taking, and the strong conclusion suggests that carbon policy significantly reduces corporate risk-taking. The further analysis concludes that carbon policy negatively impacts corporate risk-taking by reducing investor attention and raising financing constraints. Also carbon policy significantly reduces the risk-taking of SOEs and heavy polluters. This study has been shown to have great significance for firms to resist external policy risks, mitigate internal business risks, develop emission reduction strategies, and achieve sustainable corporate development.",
        "DOI": "10.1016/j.frl.2023.104502",
        "affiliation_name": "Yunnan University of Finance and Economics",
        "affiliation_city": "Kunming",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Exploring the variable importance in random forests under correlations: a general concept applied to donor organ quality in post-transplant survival",
        "paper_author": "Wies C.",
        "publication": "BMC Medical Research Methodology",
        "citied_by": "3",
        "cover_date": "2023-12-01",
        "Abstract": "Random Forests are a powerful and frequently applied Machine Learning tool. The permutation variable importance (VIMP) has been proposed to improve the explainability of such a pure prediction model. It describes the expected increase in prediction error after randomly permuting a variable and disturbing its association with the outcome. However, VIMPs measure a variable’s marginal influence only, that can make its interpretation difficult or even misleading. In the present work we address the general need for improving the explainability of prediction models by exploring VIMPs in the presence of correlated variables. In particular, we propose to use a variable’s residual information for investigating if its permutation importance partially or totally originates from correlated predictors. Hypotheses tests are derived by a resampling algorithm that can further support results by providing test decisions and p-values. In simulation studies we show that the proposed test controls type I error rates. When applying the methods to a Random Forest analysis of post-transplant survival after kidney transplantation, the importance of kidney donor quality for predicting post-transplant survival is shown to be high. However, the transplant allocation policy introduces correlations with other well-known predictors, which raises the concern that the importance of kidney donor quality may simply originate from these predictors. By using the proposed method, this concern is addressed and it is demonstrated that kidney donor quality plays an important role in post-transplant survival, regardless of correlations with other predictors.",
        "DOI": "10.1186/s12874-023-02023-2",
        "affiliation_name": "Hochschule Darmstadt",
        "affiliation_city": "Darmstadt",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "The price of quietness: How a pandemic affects city dwellers’ response to road traffic noise",
        "paper_author": "Wang Y.p.",
        "publication": "Sustainable Cities and Society",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "Using the outbreak of COVID-19 in Singapore as a quasi-natural experiment, we investigate tenants’ changing responses to road traffic noise in the rental housing market, using 46,980 transaction records between 2006 and 2022. Our difference-in-differences estimates show that road traffic noise decreases housing rents by 3.8% immediately after the pandemic outbreak and further declines by 12.7% in the subsequent year—equivalent to 186.7 US dollars per month. The results are robust to parallel trend analysis, permutation placebo tests, and tests using alternative distance thresholds or distance to the nearest main road. Then, we adopt a machine learning text analysis of 10,425 rental housing advertisements, showing that tenants’ preference for quietness increases by approximately 10% from 2019 into 2020. The new work-from-home business model and rising traffic from delivery services can explain for this pattern. To the best of our knowledge, this is the first paper using a large volume of transaction records to quantify city dwellers’ willingness to pay for quietness in the COVID-19 context. Our results have policy implications for other nations and post-pandemic era on the interaction among urban planning, transport networks, and human settlements, and shed light on the pathway to achieve sustainable development goals.",
        "DOI": "10.1016/j.scs.2023.104882",
        "affiliation_name": "Zhejiang University of Technology",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Evaluating the association between morphological characteristics of urban land and pluvial floods using machine learning methods",
        "paper_author": "Lin J.",
        "publication": "Sustainable Cities and Society",
        "citied_by": "43",
        "cover_date": "2023-12-01",
        "Abstract": "Spatial pattern of urban land plays a significant role in the occurrence of pluvial floods. While landscape metrics have been widely used to reflect land use spatial patterns, the association between the morphological spatial pattern of urban land and pluvial floods is much less understood. Unlike landscape metrics, morphological spatial pattern analysis could discover more spatial information on the formation and layout of land use. To fill such knowledge gap, this research evaluated whether the morphological characteristics of urban land matter to pluvial floods based on machine learning techniques. Pearson's correlation test and random forest algorithm were employed to discover the linear and non-linear associations between the density of flood hotspots and a set of underlying influencing factors, respectively. A case study in a low-lying coastal city has highlighted the strong influence of land use morphological characteristics on pluvial floods with baseline influencing factors considered. The density of flood hotspots was positively associated with the core, loop, edge, and bridge of urban land at sub-watershed level, but negatively associated with islet urban land. While it is unfeasible to restrain rapid urban sprawl and population growth in many developing countries, this research could give specific guidance for upgrading the morphological spatial pattern of urban land. A proper design of key morphological elements may overcome the negative influence of urban expansion on pluvial floods. These findings could help urban planners and policy makers to highlight the importance of land use morphological spatial patterns.",
        "DOI": "10.1016/j.scs.2023.104891",
        "affiliation_name": "South China Institute Of Environmental Sciences",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Intelligent airborne monitoring of irregularly shaped man-made marine objects using statistical Machine Learning techniques",
        "paper_author": "Kuru K.",
        "publication": "Ecological Informatics",
        "citied_by": "8",
        "cover_date": "2023-12-01",
        "Abstract": "The marine economy has historically been highly diversified and prolific due to the fact that the Earth's oceans comprise two-thirds of its total surface area. As technology advances, leading enterprises and ecological organisations are building and mobilising new devices supported by cutting-edge marine mechatronics solutions to explore and harness this challenging environment. Automated tracking of these types of industries and the marine life around them can help us figure out what's causing the current changes in species numbers, predict what could happen in the future, and create the right policies to help reduce the environmental impact and make the planet more sustainable. The objective of this study is to create a new platform for the automated detection of irregularly shaped man-made marine objects (ISMMMOs) in large datasets derived from marine aerial survey imagery. In this context, a novel nonparametric methodology, which harbours several hybrid statistical Machine Learning (ML) methods, was developed to automatically segment ISMMMOs on the sea surface in large surveys. This methodology was validated on a wide range of marine domains, providing robust empirical proof of concept. This approach enables the detection of ISMMMOs automatically, without any prior training, with accuracy (ACC), Matthews correlation coefficient (MCC), negative predictive value (NPV), positive predictive value (PPV), specificity (Sp) and sensitivity(Se) over 0.95. The outlined methodology can be utilised for a variety of purposes, but it's especially useful for researchers and policymakers who want to keep an eye on how the maritime industry is deploying and make sure the right policies are in place to meet regulatory and legal requirements to promote maritime tech innovation and shape what the future looks like for the marine ecosystem. For the first time in the literature, a method, the so-called ISMMMOD, has been developed to automate the detection of all types of ISMMMOs by statistical ML techniques that require no prior training, which will pioneer the monitoring of human footprint in the marine ecosystem.",
        "DOI": "10.1016/j.ecoinf.2023.102285",
        "affiliation_name": "APEM Limited",
        "affiliation_city": "Stockport",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Massive multi-player multi-armed bandits for IoT networks: An application on LoRa networks",
        "paper_author": "Dakdouk H.",
        "publication": "Ad Hoc Networks",
        "citied_by": "2",
        "cover_date": "2023-12-01",
        "Abstract": "More and more manufacturers, as part of the transition towards Industry 4.0, are using Internet of Things (IoT) networks for more efficient production. The wide and extensive expansion of IoT devices and the variety of applications generate different challenges, mainly in terms of reliability and energy efficiency. In this paper, we propose an approach to optimize the performance of IoT networks by making the IoT devices intelligent using machine learning techniques. We formulate the optimization problem as a massive multi-player multi-armed bandit and introduce two novel policies: Decreasing-Order-Reward-Greedy (DORG) focuses on the number of successful transmissions, while Decreasing-Order-Fair-Greedy (DOFG) also guarantees some measure of fairness between the devices. We then present an efficient way to manage the trade-off between transmission energy consumption and packet losses in Long-Range (LoRa) networks using our algorithms, by which LoRa nodes adjust their emission parameters (Spreading Factor and transmitting power). We implement our algorithms on a LoRa network simulator and show that such learning techniques largely outperform the Adaptive Data Rate (ADR) algorithm currently implemented in LoRa devices, in terms of both energy consumption and packet losses.",
        "DOI": "10.1016/j.adhoc.2023.103283",
        "affiliation_name": "IMT Atlantique",
        "affiliation_city": "Nantes",
        "affiliation_country": "France"
    },
    {
        "paper_title": "A collaborative iterated greedy algorithm with reinforcement learning for energy-aware distributed blocking flow-shop scheduling",
        "paper_author": "Bao H.",
        "publication": "Swarm and Evolutionary Computation",
        "citied_by": "19",
        "cover_date": "2023-12-01",
        "Abstract": "Energy-aware scheduling has attracted increasing attention mainly due to economic benefits as well as reducing the carbon footprint at companies. In this paper, an energy-aware scheduling problem in a distributed blocking flow-shop with sequence-dependent setup times is investigated to minimize both makespan and total energy consumption. A mixed-integer linear programming model is constructed and a cooperative iterated greedy algorithm based on Q-learning (CIG) is proposed. In the CIG, a top-level Q-learning is focused on enhancing the utilization ratio of machines to minimize makespan by finding a scheduling policy from four sequence-related operations. A bottom-level Q-learning is centered on improving energy efficiency to reduce total energy consumption by learning the optimal speed governing policy from four speed-related operations. According to the structure characteristics of solutions, several properties are explored to design an energy-saving strategy and acceleration strategy. The experimental results and statistical analysis prove that the CIG is superior to the state-of-the-art competitors with improvement percentages of 20.16 % over 2880 instances from the well-known benchmark set in the literature.",
        "DOI": "10.1016/j.swevo.2023.101399",
        "affiliation_name": "Liaocheng University",
        "affiliation_city": "Liaocheng",
        "affiliation_country": "China"
    },
    {
        "paper_title": "The implications for potential marginal land resources of cassava across worldwide under climate change challenges",
        "paper_author": "Li Y.",
        "publication": "Scientific Reports",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "The demand for energy plants is foreseen to grow as worldwide energy and climate policies promote the use of bioenergy for climate change mitigation. To avoid competing with food production, it’s critical to assess future changes in marginal land availability for energy plant development. Using a machine learning method, boosted regression tree, this study modeled potential marginal land resources suitable for cassava under current and different climate change scenarios, based on cassava occurrence records and environmental covariates. The findings revealed that, currently, over 80% of the 1357.24 Mha of available marginal land for cassava cultivation is distributed in Africa and South America. Under three climate change scenarios, by 2030, worldwide suitable marginal land resources were predicted to grow by 39.71Mha, 66.21 Mha, and 39.31Mha for the RCP4.5, RCP6.0, and RCP8.5 scenarios, respectively; by 2050, the potential marginal land suitable for cassava will increase by 38.98Mha, 83.02 Mha, and 55.43Mha, respectively; by 2080, the global marginal land resources were estimated to rise by 40.82 Mha, 99.74 Mha, and 21.87 Mha from now, respectively. Our results highlight the impacts of climate change on potential marginal land resources of cassava across worldwide, which provide the basis for assessing bioenergy potential in the future.",
        "DOI": "10.1038/s41598-023-42132-y",
        "affiliation_name": "Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Mapping wildfire ignition probability and predictor sensitivity with ensemble-based machine learning",
        "paper_author": "Tong Q.",
        "publication": "Natural Hazards",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "Wildfire ignition models can help in identifying risk factors and mapping high-risk areas, which addresses an urgent issue as wildfires become increasingly destructive. Despite advancements in data collection and data analysis, challenges persist as ignitions depend on numerous interconnected factors at a fine spatial resolution. Predicting wildfire ignitions from data is an imbalanced classification problem, given the vast number of non-ignition data compared to ignition data. To address this issue, this study proposes an ensemble-based model for binary classification of wildfire ignitions. The data are collected for a 24,867 km2 area in northern California from January 2014 to May 2022 and includes 76 predictors covering topographic, land cover, anthropogenic, and climatic data. Different base classifiers are evaluated and the random forest is found the most performant, yielding a recall of 0.67 and a specificity of 0.87. Feature importance analysis shows that the Topographic Wetness Index is the most important climatic predictor, while population density and land cover development are also highly rated. Comparison of yearly average of computed daily probabilities with ignition data shows that the model accurately captures the spatial pattern of ignitions, which can reveal high-risk areas. The model is then used to assess how climatic and anthropogenic factors impact wildfire ignition frequency. The projected scenarios show that the number and spread of ignitions would significantly increase with an increase in population in sparsely populated areas, while climatic factors have secondary effects in isolation but in combination may compound the risk. As current land development and climate change trends are expected to increase the frequency and severity of wildfires, data-based models can provide insights to inform policy and mitigate risk.",
        "DOI": "10.1007/s11069-023-06172-x",
        "affiliation_name": "Whiting School of Engineering",
        "affiliation_city": "Baltimore",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "DDPG-based load frequency control for power systems with renewable energy by DFIM pumped storage hydro unit",
        "paper_author": "Shi L.",
        "publication": "Renewable Energy",
        "citied_by": "6",
        "cover_date": "2023-12-01",
        "Abstract": "—By exploring the frequency regulation capability of doubly-fed induction machine pumped storage hydro (DFIM-PSH) unit in pumping mode, an improved load frequency control (LFC) strategy is proposed based on deep deterministic policy gradient (DDPG) algorithm for a power system with high penetration of renewable energy and power electronics devices. Using the unit frequency control module of DFIM-PSH in pumping mode, LFC model of regional grid with DFIM-PSH is further established combined with the characteristics of the novel power system. Taking the operation constraints of DFIM-PSH into consideration, DDPG is introduced to optimize the frequency regulation instructions of different units with the goal of minimizing system's frequency deviation and units' regulation power change. By introducing random changes of model parameters and various external disturbances in the pre-learning stage, the adaptability of the proposed LFC strategy in environments with strong uncertainty is further improved. Finally, based on the verification of frequency regulation performance of DFIM-PSH in pumping mode, simulations under scenarios of different wind power penetration and disturbances are carried out. The simulation results show that the proposed LFC strategy can effectively improve the frequency characteristics of the novel power system and has strong robustness.",
        "DOI": "10.1016/j.renene.2023.119274",
        "affiliation_name": "School of Engineering &amp; Computer Science",
        "affiliation_city": "Waco",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "The analysis of traffic data of wildfire evacuation: the case study of the 2020 Glass Fire",
        "paper_author": "Rohaert A.",
        "publication": "Fire Safety Journal",
        "citied_by": "4",
        "cover_date": "2023-12-01",
        "Abstract": "Evacuation is a crucial policy to mitigate wildfire impacts. Understanding traffic dynamics during a wildfire evacuation can help authorities to improve in improving emergency management plans, thus improving life safety. In this study, we developed a methodology to extract historical traffic data from vehicle detector stations and automate the analysis of traffic dynamics for actual wildfire evacuations. This has been implemented in an open-access tool called Traffic Dynamic Analyser (TDA) which generates speed-density and flow-density relationships from data using both commonly used macroscopic traffic models as well as machine learning techniques (e.g., support vector regression). The use of the methodology is demonstrated with a case study of the 2020 Glass Fire in California, USA. The results from TDA showed a slight reduction in speeds and flows on US Highway 101 during the evacuation scenario, compared with the routine scenario. Moreover, background traffic has been shown to play a key role in the 2020 Glass Fire compared with previous wildfire evacuation scenarios (e.g., the 2019 Kincade fire). The case study showed that the methodology implemented in the TDA can be used to understand traffic evacuation dynamics in wildfire scenarios and to validate evacuation models.",
        "DOI": "10.1016/j.firesaf.2023.103909",
        "affiliation_name": "Lunds Universitet",
        "affiliation_city": "Lund",
        "affiliation_country": "Sweden"
    },
    {
        "paper_title": "Incentive-based demand response under incomplete information based on the deep deterministic policy gradient",
        "paper_author": "Ma S.",
        "publication": "Applied Energy",
        "citied_by": "10",
        "cover_date": "2023-12-01",
        "Abstract": "Incentive-based demand response (IBDR), as an important measure to encourage the users to participate in the demand-side management, is commonly modeled as the Stackelberg game with the complete information. However, it is difficult to acquire the users' complete information due to privacy protections. In this paper, a Markov decision process (MDP) game model is proposed to address IBDR under the incomplete information, which is based on a deep deterministic policy gradient algorithm (DDPG). Considering differences on the users' load, the K-means method is used to classify different users according to the daily load rate and the peak-to-valley difference, such that different types of user load will have different incentive prices to participate in demand responses. The proposed DDPG algorithm can improve the calculation efficiency of the Nash equilibrium solution of the IBDR under the incomplete information, as it can deal with the multi-dimensional continuous state and action spaces. Simulation results show that the proposed approach can achieve the Nash equilibrium under the incomplete information and has the higher calculation accuracy and the lower calculation time in comparison to the Q learning method.",
        "DOI": "10.1016/j.apenergy.2023.121838",
        "affiliation_name": "Guangxi University",
        "affiliation_city": "Nanning",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Counterfactual-attention multi-agent reinforcement learning for joint condition-based maintenance and production scheduling",
        "paper_author": "Zhang N.",
        "publication": "Journal of Manufacturing Systems",
        "citied_by": "8",
        "cover_date": "2023-12-01",
        "Abstract": "Maintenance and production scheduling are interactive activities that should be considered simultaneously to maintain production systems’ reliability and high production delivery rate. This study investigates a problem of joint maintenance and production scheduling in a multi-stage hybrid flow shop experiencing machine deterioration processes. To address the structural dependency among production stages and machines, a decentralized partially observable Markov decision process (Dec-POMDP) is considered. However, the large state and action space pose a challenge for existing reinforcement learning methods to provide satisfactory solutions. To overcome this problem, the study proposes a counterfactual attention multi-agent reinforcement learning (CAMARL) solution framework that comprises three functional modules: The attention mechanism module, which adaptively shrinks the dimension of the state space; the action abstraction representation module that deals with the high-dimension of the action space; and the coordination control unit, which accelerates the exploration for the optimal production policy. Numerical experiments demonstrate the effectiveness of the proposed method by comparing it with seven benchmarks under different production scenarios.",
        "DOI": "10.1016/j.jmsy.2023.08.011",
        "affiliation_name": "Peking University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A spatial pyramid pooling-based deep reinforcement learning model for dynamic job-shop scheduling problem",
        "paper_author": "Wu X.",
        "publication": "Computers and Operations Research",
        "citied_by": "12",
        "cover_date": "2023-12-01",
        "Abstract": "The dynamic job-shop scheduling problem (DJSP) is a typical of scheduling tasks where rescheduling is performed when encountering unexpected events such as random job arrivals and rush order. However, the current rescheduling approaches cannot reuse the trained scheduling policies or the experiences due to the variant size of scheduling problems. In this paper, we propose a deep reinforcement learning (DRL) scheduling model for DJSP based on spatial pyramid pooling networks (SPP-Net). A new state representation is proposed based on the machine matrix and remaining time matrix which is decomposed from the scheduling instance matrix. And a new reward function is derived from the area of total scheduling time where the accumulated reward is negatively linearly dependent with the make-span of a scheduling task. Moreover, a size-agnostic scheduling policy is designed based on the SPP-Net and SoftMax function, which is trained by the proximal policy optimization (PPO). Besides, various paired priority dispatching rules (PDR) are used as available actions. Static experiments on classic benchmark instances show that our scheduling model achieves better results on average than existing DRL methods. In addition, dynamic scheduling experiments are tested and our model obtains better results than the PDR scheduling methods in reasonable time when encountering unexpected events such as random job arrivals and rush order.",
        "DOI": "10.1016/j.cor.2023.106401",
        "affiliation_name": "Nanjing University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Targeted demand response for flexible energy communities using clustering techniques",
        "paper_author": "Pelekis S.",
        "publication": "Sustainable Energy, Grids and Networks",
        "citied_by": "18",
        "cover_date": "2023-12-01",
        "Abstract": "The present study proposes clustering techniques for designing demand response (DR) programs targeting commercial and residential prosumers. The goal is to alter the consumption behavior of the prosumers within a distributed energy community in Italy. This aggregation aims to: (a) minimize the reverse power flow at the primary substation, occurring when generation from solar panels in the local grid exceeds consumption, and (b) shift the system wide peak demand, that typically occurs during late afternoon. Regarding the clustering stage, we consider daily prosumer load profiles and divide them across the extracted clusters. Three popular machine learning algorithms are employed, namely k-means, k-medoids and agglomerative clustering. We evaluate the methods using multiple metrics including a novel metric proposed within this study, namely peak performance score (PPS). The k-means algorithm with dynamic time warping distance considering 14 clusters exhibits the highest performance with a PPS of 0.689. Subsequently, we analyze each extracted cluster with respect to load shape, entropy, and load types. These characteristics are used to distinguish the clusters that have the potential to serve the optimization objectives by matching them to proper DR schemes including time of use, critical peak pricing, and real-time pricing. Our results confirm the effectiveness of the proposed clustering algorithm in generating meaningful flexibility clusters, while the derived DR pricing policy encourages consumption during off-peak hours. The developed methodology is robust to the low availability and quality of training datasets and can be used by aggregator companies for segmenting energy communities and developing personalized DR policies.",
        "DOI": "10.1016/j.segan.2023.101134",
        "affiliation_name": "National Technical University of Athens (NTUA)",
        "affiliation_city": "Athens",
        "affiliation_country": "Greece"
    },
    {
        "paper_title": "Recommendations for the use of pediatric data in artificial intelligence and machine learning ACCEPT-AI",
        "paper_author": "Muralidharan V.",
        "publication": "npj Digital Medicine",
        "citied_by": "21",
        "cover_date": "2023-12-01",
        "Abstract": "ACCEPT-AI is a framework of recommendations for the safe inclusion of pediatric data in artificial intelligence and machine learning (AI/ML) research. It has been built on fundamental ethical principles of pediatric and AI research and incorporates age, consent, assent, communication, equity, protection of data, and technological considerations. ACCEPT-AI has been designed to guide researchers, clinicians, regulators, and policymakers and can be utilized as an independent tool, or adjunctively to existing AI/ML guidelines.",
        "DOI": "10.1038/s41746-023-00898-5",
        "affiliation_name": "Stanford University School of Medicine",
        "affiliation_city": "Stanford",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Two-timescale autonomous energy management strategy based on multi-agent deep reinforcement learning approach for residential multicarrier energy system",
        "paper_author": "Zhang B.",
        "publication": "Applied Energy",
        "citied_by": "11",
        "cover_date": "2023-12-01",
        "Abstract": "In the residential multicarrier energy system (RMES), autonomous energy management (AEM) is beneficial for prosumers' costs that actively controls generation, energy conversion, and storage in real-time. Conventional model-based AEM methods rely on forecast models of distributed energy resources and proper system parameters, which are difficult for practical application. This article proposes a novel model-free two-timescale real-time AEM strategy for the RMES. The energy management problem in the RMES is time-decomposed into two timescales (hourly-ahead external energy trading and 15-min ahead internal energy conversion), formulated as Markov Games. Then, defining a multi-agent system not only enables separate energy management diagrams but also accelerates learning. Intelligent agents account for optimizing energy trading and energy conversion to minimize daily costs under the training of a deep deterministic policy gradient algorithm. In order to learn a collaborative strategy, all agents are trained centrally while being executed based on local information in a decentralized manner for fast response. A deterministic study indicates that the proposed AEM strategy can effectively and flexibly schedule the operation of components with respect to different price signals and load profiles. In the stochastic study that considers the error thresholds of solar panels generation and loads, the difference between the energy cost of test scenarios with a trained strategy and the no-regret learning method is equal to 0.32%, which is appropriate. In addition, the proposed method not only achieves a reduction in power imbalance but also exhibits lower energy costs when compared to various benchmark methods.",
        "DOI": "10.1016/j.apenergy.2023.121777",
        "affiliation_name": "School of Electrical and Electronic Engineering",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore"
    },
    {
        "paper_title": "Value function estimators for Feynman–Kac forward–backward SDEs in stochastic optimal control",
        "paper_author": "Hawkins K.P.",
        "publication": "Automatica",
        "citied_by": "2",
        "cover_date": "2023-12-01",
        "Abstract": "Two novel numerical estimators are proposed for solving forward–backward stochastic differential equations (FBSDEs) appearing in the Feynman–Kac representation of the value function in stochastic optimal control problems. In contrast to the current numerical approaches, which are based on the discretization of the continuous-time FBSDE, we propose a converse approach, namely, we obtain a discrete-time approximation of the value function, and then we derive a discrete-time estimator that resembles the continuous-time counterpart. The proposed approach allows for the construction of higher accuracy estimators along with an error analysis. The approach is applied to the policy improvement step in a reinforcement learning framework. Numerical results, along with the corresponding error analysis, demonstrate that the proposed estimators show significant improvement in terms of accuracy over classical Euler–Maruyama-based estimators. In the case of LQ problems, we demonstrate that our estimators result in near machine-precision level accuracy, in contrast to previously proposed methods that can potentially diverge on the same problems.",
        "DOI": "10.1016/j.automatica.2023.111281",
        "affiliation_name": "The University of Alabama",
        "affiliation_city": "Tuscaloosa",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Predicting transport mode choice preferences in a university district with decision tree-based models",
        "paper_author": "Díaz-Ramírez J.",
        "publication": "City and Environment Interactions",
        "citied_by": "6",
        "cover_date": "2023-12-01",
        "Abstract": "Modeling and prediction of mode choice are essential to support more sustainable and safer transportation decisions. There is plenty of literature in this decade showing that machine learning (ML) models have been effective predicting techniques, although not easily interpretable. When these techniques are used, there is a lack of connection with the data-gathering step, which is crucial to the technique selection and appropriate analysis of results. Based on a systematic literature review on mode choice studies, we present a methodology that interconnects the data-gathering process as a fundamental part of the descriptive phase when ML classification methods are used to predict mode choice preferences. The case study presented occurs in a university context whose descriptive phase shows interesting behavior patterns and highly imbalanced data in terms of mode choice. We show how decision tree methods allow us to tackle this issue in a contextualized manner and permit sensitivity analysis to test policies promoting changes in the modal split that aim for more sustainable mobility for the community of the university.",
        "DOI": "10.1016/j.cacint.2023.100118",
        "affiliation_name": "Universidad de Monterrey",
        "affiliation_city": "San Pedro Garza Garcia",
        "affiliation_country": "Mexico"
    },
    {
        "paper_title": "Federal policy announcements and capital reallocation: Insights from inflow and outflow trends in the U.S.",
        "paper_author": "Qiu Y.",
        "publication": "Journal of International Money and Finance",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "This paper delves into the impact of FOMC policy announcements and meeting minutes on the reallocation of US assets at the fund level. We address the challenge of uncertain reallocation scenarios through employing a data imputation technique with random forest forecasts and fund-level features. We then quantify the most predictive information from FOMC policy statements and meeting minutes through constructed diffusion indices using a supervised learning approach. By conducting predictive fixed-effect regression analyses, we unveil the significant role of incorporating textual predictors derived from FOMC statements. Our findings reveal that positive and negative signals in FOMC announcements have contrasting effects on changes in US asset allocation. Additional exercises demonstrate that net assets and reallocation channels matter for the responses of funds to FOMC statements. We also find substantial evidence of the increased attention paid to FOMC announcements after the 2008 crisis. This heightened attention notably impacts funds that adjust US holdings using their own capital. Furthermore, we examine the presence of home bias among US and non-OECD funds in comparison to other OECD-based funds. Our analysis suggests the potential presence of a home bias among US and non-OECD funds.",
        "DOI": "10.1016/j.jimonfin.2023.102936",
        "affiliation_name": "Shanghai University of Finance and Economics",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "MAFSIDS: a reinforcement learning-based intrusion detection model for multi-agent feature selection networks",
        "paper_author": "Ren K.",
        "publication": "Journal of Big Data",
        "citied_by": "13",
        "cover_date": "2023-12-01",
        "Abstract": "Large unbalanced datasets pose challenges for machine learning models, as redundant and irrelevant features can hinder their effectiveness. Furthermore, the performance of intrusion detection systems (IDS) can be further degraded by the emergence of new network attack types. To address these issues, we propose MAFSIDS (Multi-Agent Feature Selection Intrusion Detection System), a DQL (Deep Q-Learning) based IDS. MAFSIDS comprises a feature self-selection algorithm and a DRL (Deep Reinforcement Learning) attack detection module. The feature self-selection algorithm leverages a multi-agent reinforcement learning framework, which redefines the feature selection problem by converting the traditional 2 N feature selection space into N agent representations. This approach reduces model complexity and enhances the search strategy for feature selection. To ensure accurate feature representation and expedite the feature selection process, we have also developed a GCN (Graph Convolutional Network) method that extracts deeper features from the data. The DRL attack detection module utilizes the Mini-Batchs technique to encode the data, allowing reinforcement learning to be applied in a supervised learning context. This integration improves accuracy. Additionally, the policy network in this module is designed to be minimalist, enhancing model efficiency. To evaluate the performance of our model, we conducted comprehensive simulation experiments using Python. We tested the model using the CSE-CIC-IDS2018 and NSL-KDD datasets, achieving impressive accuracy rates of 96.8% and 99.1%, as well as F1-Scores of 96.3% and 99.1%, respectively. The selected feature subset successfully eliminates approximately 80% of redundant features compared to the original feature set. Furthermore, we compared our proposed model with other popular machine-learning models.",
        "DOI": "10.1186/s40537-023-00814-4",
        "affiliation_name": "Sun Yat-Sen University",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "The 510(k) Third Party Review Program: Promise and Potential",
        "paper_author": "Miller B.J.",
        "publication": "Journal of Medical Systems",
        "citied_by": "3",
        "cover_date": "2023-12-01",
        "Abstract": "Every year, the Food and Drug Administration (FDA) clears approximately 3,000 medical devices for marketing via the 510(k) pathway. These constitute 99% of all devices approved for human use and includes the premarket review of many devices incorporating newer technology such as artificial intelligence (AI), machine learning (ML), and other software. As the complexity of these novel technologies and the number of applications is expected to increase in the coming years, statutory changes such as the 2016 21st Century Cures Act, regulations, and guidance documents have increased both the volume and complexity of device review. Thus, the ability to streamline the review of less complex, low-to-moderate risk devices through the 510(k) pathway will maximize the FDA’s capability to address other important, future-oriented regulatory questions. For over twenty five years, third party review organizations have served a defined function to assist with the review of 510(k) applications for a set of enumerated device classes. This paper reviews the history of FDA device regulation, the evolution of the 510(k) review pathway, and the recent history of the 510(k) third party review program. Finally, the paper addresses policy concerns from all stakeholders – including the FDA – along with policy suggestions to improve the third party review program and FDA device regulation writ large.",
        "DOI": "10.1007/s10916-023-01986-5",
        "affiliation_name": "Johns Hopkins Carey Business School",
        "affiliation_city": "Baltimore",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Wholesale Food Price Index Forecasts with the Neural Network",
        "paper_author": "Xu X.",
        "publication": "International Journal of Computational Intelligence and Applications",
        "citied_by": "18",
        "cover_date": "2023-12-01",
        "Abstract": "Food price forecasts in the agricultural sector have always been a vital matter to a wide variety of market participants. In this work, we approach this forecast problem for the weekly wholesale food price index in the Chinese market during a 10-year period of January 1, 2010-January 3, 2020. To facilitate the analysis, we propose the use of the nonlinear auto-regressive neural network. Technically, we investigate forecast performance, based upon the relative root mean square error (RRMSE) as the evaluation metrics, corresponding to one hundred and twenty settings that cover different algorithms for model estimations, numbers of hidden neurons and delays, and ratios for splitting the data. Our experimental result suggests the construction of the neural network with three delays and 10 hidden neurons, which is trained through the Levenberg-Marquardt algorithm, as the forecast model. It leads to high accuracy and stabilities with the RRMSEs of 1.93% for the training phase, 2.16% for the validation phase, and 1.95% for the testing phase. Comparisons of forecast accuracy between the proposed model and some other machine learning models, as well as traditional time-series econometric models, suggest that our proposed model leads to statistically significant better performance. Our results could benefit different forecast users, such as policymakers and various market participants, in policy analysis and market assessments.",
        "DOI": "10.1142/S1469026823500244",
        "affiliation_name": "NC State University",
        "affiliation_city": "Raleigh",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "A Reinforcement Learning Framework With Region-Awareness and Shared Path Experience for Efficient Routing in Networks-on-Chip",
        "paper_author": "Khan K.",
        "publication": "IEEE Design and Test",
        "citied_by": "4",
        "cover_date": "2023-12-01",
        "Abstract": "Editor's notes: In this article, the authors introduce a regional congestion-aware reinforcement learning (RL)-based routing policy for Network-on-Chip (NoC) architectures. - Mahdi Nikdast, Colorado State University, USA - Miquel Moreto, Barcelona Supercomputing Center, Spain - Masoumeh (Azin) Ebrahimi, KTH Royal Institute of Technology, Sweden - Sujay Deb, IIIT Delhi, India",
        "DOI": "10.1109/MDAT.2023.3306719",
        "affiliation_name": "Colorado State University",
        "affiliation_city": "Fort Collins",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Optimal planning of hybrid energy storage systems using curtailed renewable energy through deep reinforcement learning",
        "paper_author": "Kang D.",
        "publication": "Energy",
        "citied_by": "27",
        "cover_date": "2023-12-01",
        "Abstract": "Energy management systems are becoming increasingly important to utilize the continuously growing curtailed renewable energy. Promising energy storage systems, such as batteries and green hydrogen, should be employed to maximize the efficiency of energy stakeholders. However, optimal decision-making, i.e., planning the leveraging between different strategies, is confronted with the complexity and uncertainties of large-scale problems. A sophisticated deep reinforcement learning methodology with a policy-based algorithm is proposed to achieve real-time optimal energy storage systems planning under the curtailed renewable energy uncertainty. A quantitative performance comparison proved that the deep reinforcement learning agent outperforms the scenario-based stochastic optimization algorithm, even with a wide action and observation space. A robust performance, with maximizing net profit and a stable system, confirmed the uncertainty rejection capability of the deep reinforcement learning under a large uncertainty of the curtailed renewable energy. Action mapping was performed to visually assess the action the deep reinforcement learning agent took according to the state. The corresponding results confirmed that the deep reinforcement learning agent learns how the deterministic solution performs and demonstrates more than 90% profit accuracy compared to the solution.",
        "DOI": "10.1016/j.energy.2023.128623",
        "affiliation_name": "Seoul National University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "Development and validation of a reinforcement learning model for ventilation control during emergence from general anesthesia",
        "paper_author": "Lee H.",
        "publication": "npj Digital Medicine",
        "citied_by": "6",
        "cover_date": "2023-12-01",
        "Abstract": "Ventilation should be assisted without asynchrony or cardiorespiratory instability during anesthesia emergence until sufficient spontaneous ventilation is recovered. In this multicenter cohort study, we develop and validate a reinforcement learning-based Artificial Intelligence model for Ventilation control during Emergence (AIVE) from general anesthesia. Ventilatory and hemodynamic parameters from 14,306 surgical cases at an academic hospital between 2016 and 2019 are used for training and internal testing of the model. The model’s performance is also evaluated on the external validation cohort, which includes 406 cases from another academic hospital in 2022. The estimated reward of the model’s policy is higher than that of the clinicians’ policy in the internal (0.185, the 95% lower bound for best AIVE policy vs. −0.406, the 95% upper bound for clinicians’ policy) and external validation (0.506, the 95% lower bound for best AIVE policy vs. 0.154, the 95% upper bound for clinicians’ policy). Cardiorespiratory instability is minimized as the clinicians’ ventilation matches the model’s ventilation. Regarding feature importance, airway pressure is the most critical factor for ventilation control. In conclusion, the AIVE model achieves higher estimated rewards with fewer complications than clinicians’ ventilation control policy during anesthesia emergence.",
        "DOI": "10.1038/s41746-023-00893-w",
        "affiliation_name": "Biomedical Research Institute",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "Machine learning-enabled regional multi-hazards risk assessment considering social vulnerability",
        "paper_author": "Zhang T.",
        "publication": "Scientific Reports",
        "citied_by": "17",
        "cover_date": "2023-12-01",
        "Abstract": "The regional multi-hazards risk assessment poses difficulties due to data access challenges, and the potential interactions between multi-hazards and social vulnerability. For better natural hazards risk perception and preparedness, it is important to study the nature-hazards risk distribution in different areas, specifically a major priority in the areas of high hazards level and social vulnerability. We propose a multi-hazards risk assessment method which considers social vulnerability into the analyzing and utilize machine learning-enabled models to solve this issue. The proposed methodology integrates three aspects as follows: (1) characterization and mapping of multi-hazards (Flooding, Wildfires, and Seismic) using five machine learning methods including Naïve Bayes (NB), K-Nearest Neighbors (KNN), Logistic Regression (LR), Random Forest (RF), and K-Means (KM); (2) evaluation of social vulnerability with a composite index tailored for the case-study area and using machine learning models for classification; (3) risk-based quantification of spatial interaction mechanisms between multi-hazards and social vulnerability. The results indicate that RF model performs best in both hazard-related and social vulnerability datasets. The most cities at multi-hazards risk account for 34.12% of total studied cities (covering 20.80% land). Additionally, high multi-hazards level and socially vulnerable cities account for 15.88% (covering 4.92% land). This study generates a multi-hazards risk map which show a wide variety of spatial patterns and a corresponding understanding of where regional high hazards potential and vulnerable areas are. It emphasizes an urgent need to implement information-based prioritization when natural hazards coming, and effective policy measures for reducing natural-hazards risks in future.",
        "DOI": "10.1038/s41598-023-40159-9",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Boise",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "The price and income elasticities of natural gas demand in Azerbaijan: Is there room to export more?",
        "paper_author": "Gurbanov S.",
        "publication": "Humanities and Social Sciences Communications",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "Natural gas is frequently introduced as a “transitional fuel”. Because burning natural gas emits less carbon dioxide emissions than burning either oil or coal. Additionally, the intermittent nature of low-carbon electricity generation creates imperative for using natural gas for power generation. The role of natural gas is currently under scrutiny as climate change transforms into a climate crisis. Meanwhile, share of natural gas in the primary energy consumption of Azerbaijan is 69%, while 94% of the country’s electricity is currently being generated in natural gas-fired power plants. In this manner, this paper estimates income and price elasticities of natural gas demand for the Azerbaijan case. In this study, various sets of estimation techniques are utilized. By modeling natural gas demand with different estimation methods, including Autoregressive Distributed Lagged Structural Time Series Modeling, Dynamic Ordinary Least Squares Method, Fully Modified Ordinary Least Squares, Canonical Cointegrating Regression, and General to Specific under Autometrics multi-path search machine learning algorithm, we try to find if there is room for the country to export more. All these utilized estimation methods confirmed the long-run income elasticity to be around 0.8, while the long-run price elasticity is around −0.1. Both estimations provide insights in terms of energy security and electricity security for policymakers during the implementation process of climate, energy, and environmental policy. Findings of this study classify natural as a necessity and normal good for the Azerbaijan case. The main policy implication of this study is that policymakers must enable and facilitate the availability of close renewable substitutes for residential and commercial customers. Estimated elasticities suggest that with rising national income, demand for natural gas will keep increasing, and efficient consumption will not be attainable with increasing prices. In the pursuit of export potential, findings suggest that it is more relevant to free up natural gas allocated from power generation by substituting it with renewable energy sources.",
        "DOI": "10.1057/s41599-023-01987-2",
        "affiliation_name": "Vistula University",
        "affiliation_city": "Warsaw",
        "affiliation_country": "Poland"
    },
    {
        "paper_title": "Determinants of public preferences on low-carbon energy sources: Evidence from the United Kingdom",
        "paper_author": "Lee J.",
        "publication": "Energy",
        "citied_by": "5",
        "cover_date": "2023-12-01",
        "Abstract": "We empirically derive the determinants of British public preferences for different low-carbon energy sources using machine learning algorithm-based variable selection methods (ridge, lasso, and elastic net regression models). We seek to understand the drivers of support for solar, wind, biomass, and nuclear energy, which are the largest low-carbon energy sources and together account for the majority of UK power generation. Explanatory variables examined include those related to demographics, knowledge, perceptions of climate change, and government policy. We carry out a comparative study by synthesising the results of our independent analyses for each energy source and find that the preferred energy sources vary with respondents’ views on anticipated climate change impacts. Those who believe that potential effects of climate change will be catastrophic tend to prefer renewable energy sources whereas those less concerned about climate change tend to prefer nuclear power. The public also prefers energy sources about which they are more familiar or knowledgeable. Energy transition policies should be adjusted to better consider the factors that drive public acceptance of low-carbon energy sources, including exploring ways to secure policy support that considers energy source-specific characteristics and expanding public awareness of climate change and individual technologies, particularly among certain demographics.",
        "DOI": "10.1016/j.energy.2023.128704",
        "affiliation_name": "Cambridge Judge Business School",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Search-and-rescue in the Central Mediterranean Route does not induce migration: Predictive modeling to answer causal queries in migration research",
        "paper_author": "Rodríguez Sánchez A.",
        "publication": "Scientific Reports",
        "citied_by": "6",
        "cover_date": "2023-12-01",
        "Abstract": "State- and private-led search-and-rescue are hypothesized to foster irregular migration (and thereby migrant fatalities) by altering the decision calculus associated with the journey. We here investigate this ‘pull factor’ claim by focusing on the Central Mediterranean route, the most frequented and deadly irregular migration route towards Europe during the past decade. Based on three intervention periods—(1) state-led Mare Nostrum, (2) private-led search-and-rescue, and (3) coordinated pushbacks by the Libyan Coast Guard—which correspond to substantial changes in laws, policies, and practices of search-and-rescue in the Mediterranean, we are able to test the ‘pull factor’ claim by employing an innovative machine learning method in combination with causal inference. We employ a Bayesian structural time-series model to estimate the effects of these three intervention periods on the migration flow as measured by crossing attempts (i.e., time-series aggregate counts of arrivals, pushbacks, and deaths), adjusting for various known drivers of irregular migration. We combine multiple sources of traditional and non-traditional data to build a synthetic, predicted counterfactual flow. Results show that our predictive modeling approach accurately captures the behavior of the target time-series during the various pre-intervention periods of interest. A comparison of the observed and predicted counterfactual time-series in the post-intervention periods suggest that pushback policies did affect the migration flow, but that the search-and-rescue periods did not yield a discernible difference between the observed and the predicted counterfactual number of crossing attempts. Hence we do not find support for search-and-rescue as a driver of irregular migration. In general, this modeling approach lends itself to forecasting migration flows with the goal of answering causal queries in migration research.",
        "DOI": "10.1038/s41598-023-38119-4",
        "affiliation_name": "Hertie School",
        "affiliation_city": "Berlin",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Commentary: Predicting adverse outcomes in pregnant patients positive for SARS-CoV-2 by a machine learning approach",
        "paper_author": "Salmeri N.",
        "publication": "BMC Pregnancy and Childbirth",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "SARS-CoV-2 infection poses a significant risk increase for adverse pregnancy outcomes both from maternal and fetal sides. A recent publication in BMC Pregnancy and Childbirth presented a machine learning algorithm to predict this risk. This commentary will discuss potential implications and applications of this study for future global health policies.",
        "DOI": "10.1186/s12884-023-05864-3",
        "affiliation_name": "IRCCS Ospedale San Raffaele",
        "affiliation_city": "Milan",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Impacts of air pollution and meteorological conditions on dry eye disease among residents in a northeastern Chinese metropolis: a six-year crossover study in a cold region",
        "paper_author": "Lu C.W.",
        "publication": "Light: Science and Applications",
        "citied_by": "18",
        "cover_date": "2023-12-01",
        "Abstract": "The purpose of this study is to explore the associations among dry eye disease (DED), air pollution, and meteorological conditions in the cold region of a northeastern Chinese metropolis (i.e., Changchun). Data on ambient air pollutants and meteorological parameters as well as diagnosed DED outpatients during 2015–2021 were collected. The associations between DED and environmental factors were analysed at multiple time scales using various statistical methods (i.e., correlation, regression and machine learning). Among the 10,809 DED patients (21,617 eyes) studied, 64.60% were female and 35.40% were male. A higher frequency of DED was observed in March and April, followed by January, August and October. Individual and multiple factor models showed the positive importance of particles with aerodynamic diameters <10 μm (PM10), carbon monoxide (CO), and ozone (O3) among normal air pollutants and air pressure (AP), air temperature (AT) and wind speed (WS) among normal meteorological parameters. Air pollutants (PM10, nitrogen dioxide: NO2) and meteorological parameters (AT, AP) have combined impacts on DED occurrence. For the first time, we further explored the associations of detailed components of atmospheric particles and DED, suggesting potential emission sources, including spring dust from bare soil and roads and precursor pollutants of summer O3 formation from vehicles and industry in Northeast China. Our results revealed the quantitative associations among air pollutants, meteorological conditions and DED outpatients in cold regions, highlighting the importance of coordinated policies in air pollution control and climate change mitigation.",
        "DOI": "10.1038/s41377-023-01207-1",
        "affiliation_name": "The First Bethune Hospital of Jilin University",
        "affiliation_city": "Changchun",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Gross electricity consumption forecasting using LSTM and SARIMA approaches: A case study of Türkiye",
        "paper_author": "Bilgili M.",
        "publication": "Energy",
        "citied_by": "45",
        "cover_date": "2023-12-01",
        "Abstract": "Gross electricity consumption (GEC) forecasts are an essential tool for policymakers in developing countries. It is widely acknowledged that GEC forecasting models contribute significantly to more effective electricity management policies, behavioral changes within the energy supply industry, and reduced energy consumption. In this regard, it is essential to evaluate the approaches that allow users to anticipate their future energy usage based on their own consumption history data and other variables. This motivates researchers to develop efficient GEC forecasting models using historical time series data and appropriate estimation strategies. In this study, therefore, a machine-learning model employing a deep-learning technique based on a long short-term memory (LSTM) neural network was utilized to forecast GEC in Türkiye. The LSTM model was compared to the seasonal autoregressive integrated moving average (SARIMA) model to determine the amount of gross energy usage. Although the results are close to each other, the LSTM model outperformed the SARIMA model in general, with the lowest MAPE (2.42%), MAE (215.35 GWh), and RMSE (329.9 GWh) values and the greatest R-value (0.9992).",
        "DOI": "10.1016/j.energy.2023.128575",
        "affiliation_name": "Çukurova Üniversitesi",
        "affiliation_city": "Adana",
        "affiliation_country": "Turkey"
    },
    {
        "paper_title": "UDL: a cloud task scheduling framework based on multiple deep neural networks",
        "paper_author": "Li Q.",
        "publication": "Journal of Cloud Computing",
        "citied_by": "3",
        "cover_date": "2023-12-01",
        "Abstract": "Cloud task scheduling and resource allocation (TSRA) constitute a core issue in cloud computing. Batch submission is a common user task deployment mode in cloud computing systems. In this mode, it has been a challenge for cloud systems to balance the quality of user service and the revenue of cloud service provider (CSP). To this end, with multi-objective optimization (MOO) of minimizing task latency and energy consumption, we propose a cloud TSRA framework based on deep learning (DL). The system solves the TSRA problems of multiple task queues and virtual machine (VM) clusters by uniting multiple deep neural networks (DNNs) as task scheduler of cloud system. The DNNs are divided into exploration part and exploitation part. At each scheduling time step, the model saves the best outputs of all scheduling policies from each DNN to the experienced sample memory pool (SMP), and periodically selects random training samples from SMP to train each DNN of exploitation part. We designed a united deep learning (UDL) algorithm based on this framework. Experimental results show that the UDL algorithm can effectively solve the MOO problem of TSRA for cloud tasks, and performs better than benchmark algorithms such as heterogeneous distributed deep learning (HDDL) in terms of task scheduling performance.",
        "DOI": "10.1186/s13677-023-00490-y",
        "affiliation_name": "Guangdong University of Petrochemical Technology",
        "affiliation_city": "Maoming",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Computation Offloading and Resource Allocation Based on DT-MEC-Assisted Federated Learning Framework",
        "paper_author": "He Y.",
        "publication": "IEEE Transactions on Cognitive Communications and Networking",
        "citied_by": "20",
        "cover_date": "2023-12-01",
        "Abstract": "Traditional centralized machine learning uses a large amount of data for model training, which may face some privacy and security problems. On the other hand, federated learning (FL), which focuses on privacy protection, also faces challenges such as core network congestion and limited mobile device (MD) resources. The computation offloading technology of mobile edge computing (MEC) can effectively alleviate these challenges, but it ignores the effect of user mobility and the unpredictable MEC environment. In this paper, we first propose an architecture that combines digital twin (DT) and MEC technologies with the FL framework, where the DT network can virtually imitate the statue of physical entities (PEs) and network topology to be used for real-time data analysis and network resource optimization. The computation offloading technology of MEC is used to alleviate resource constraints of MDs and the core network congestion. We further leverage the FL to construct DT models based on PEs' running data. Then, we jointly optimize the problem of computation offloading and resource allocation to reduce the straggler effect in FL based on the framework. Since the solution of the objective function is a stochastic programming problem, we model a Markov decision process (MDP), and use the deep deterministic policy gradient (DDPG) algorithm to solve this objective function. The simulation results prove the feasibility of the proposed scheme, and the scheme can significantly reduce the total cost by about 50% and improve the communication performance compared with baseline schemes.",
        "DOI": "10.1109/TCCN.2023.3298926",
        "affiliation_name": "Mohamed Bin Zayed University of Artificial Intelligence",
        "affiliation_city": "Abu Dhabi",
        "affiliation_country": "United Arab Emirates"
    },
    {
        "paper_title": "Semi-Asynchronous Model Design for Federated Learning in Mobile Edge Networks",
        "paper_author": "Zhang J.",
        "publication": "IEEE Transactions on Vehicular Technology",
        "citied_by": "7",
        "cover_date": "2023-12-01",
        "Abstract": "Federated learning (FL) is a distributed machine learning (ML). Distributed clients train locally and exclusively need to upload the model parameters to learn the global model collaboratively under the coordination of the aggregation server. Although the privacy of the clients is protected, which requires multiple rounds of data upload between the clients and the server to ensure the accuracy of the global model. Inevitably, this results in latency and energy consumption issues due to limited communication resources. Therefore, mobile edge computing (MEC) has been proposed to solve communication delays and energy consumption in federated learning. In this paper, we first analyze how to select the gradient values that help the global model converge quickly and establish theoretical analysis about the relationship between the convergence rate and the gradient direction. To efficiently reduce the energy consumption of clients during training, on the premise of ensuring the local training accuracy and the convergence rate of the global model, we adopt the deep deterministic policy gradient (DDPG) algorithm, which adaptively allocates resources according to different clients' requests to minimize the energy consumption. To improve flexibility and scalability, we propose a new the semi-Asynchronous federated update model, which allows clients to aggregate asynchronously on the server, and accelerates the convergence rate of the global model. Empirical results show that the proposed Algorithm $\\mathbf {1}$ not only accelerates the convergence speed of the global model, but also reduces the size of parameters that need to be uploaded. Besides, the proposed Algorithm $\\mathbf {2}$ reduces the time difference caused by user heterogeneity. Eventually, the semi-Asynchronous update model is better than the synchronous update model in communication time.",
        "DOI": "10.1109/TVT.2023.3298787",
        "affiliation_name": "Mohamed Bin Zayed University of Artificial Intelligence",
        "affiliation_city": "Abu Dhabi",
        "affiliation_country": "United Arab Emirates"
    },
    {
        "paper_title": "Self-Supervised Digital Histopathology Image Disentanglement for Arbitrary Domain Stain Transfer",
        "paper_author": "Ling Y.",
        "publication": "IEEE Transactions on Medical Imaging",
        "citied_by": "4",
        "cover_date": "2023-12-01",
        "Abstract": "Diagnosis of cancerous diseases relies on digital histopathology images from stained slides. However, the staining varies among medical centers, which leads to a domain gap of staining. Existing generative adversarial network (GAN) based stain transfer methods highly rely on distinct domains of source and target, and cannot handle unseen domains. To overcome these obstacles, we propose a self-supervised disentanglement network (SDN) for domain-independent optimization and arbitrary domain stain transfer. SDN decomposes an image into features of content and stain. By exchanging the stain features, the staining style of an image is transferred to the target domain. For optimization, we propose a novel self-supervised learning policy based on the consistency of stain and content among augmentations from one instance. Therefore, the process of training SDN is independent on the domain of training data, and thus SDN is able to tackle unseen domains. Exhaustive experiments demonstrate that SDN achieves the top performance in intra-dataset and cross-dataset stain transfer compared with the state-of-the-art stain transfer models, while the number of parameters in SDN is three orders of magnitude smaller parameters than that of compared models. Through stain transfer, SDN improves AUC of downstream classification model on unseen data without fine-tuning. Therefore, the proposed disentanglement framework and self-supervised learning policy have significant advantages in eliminating the stain gap among multi-center histopathology images.",
        "DOI": "10.1109/TMI.2023.3298361",
        "affiliation_name": "Fudan University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Impact of historical pattern of human activities and natural environment on wetland in Heilongjiang River Basin",
        "paper_author": "Song C.",
        "publication": "Frontiers of Environmental Science and Engineering",
        "citied_by": "8",
        "cover_date": "2023-12-01",
        "Abstract": "Mid and high latitude wetlands are becoming fragmented and losing ecosystem functions at a much faster rate than many other ecosystems. This is due in part to increasing human activities and climate change. In this study, we analyzed wetland distribution and spatial pattern changes for the Heilongjiang River Basin over the past 100 yr. We identified the driving factors and quantified the relative importance of each factor based on landscape pattern metrics and machine learning algorithms. Our results show that wetlands have been fragmented into smaller and regular patches with dominant factors that varied at different periods. Geographic features play the most important role in patterns of wetland change for the entire basin (with 50%–60% of relative importance). Human activities are more important than climate change at the century scale, but less important when magnified at the decadal scale. In the early 1900s, human activities were relatively low and localized and remained that way in the subsequent decades. Thus, the effect of human activities on wetland area of the entire basin is weaker when examined at the magnified decadal scale. The results also show that human activities are more important on the Chinese side of the Heilongjiang River Basin, in the Zeya-Bureya Plain on the Russian side, and at lower altitudes (0–100 m). Revealing the spatial and temporal processes and driving factors over the past 100 yr helps researchers and policymakers understand and anticipate wetland change and design effective conservation and restoration policies.[Figure not available: see fulltext.].",
        "DOI": "10.1007/s11783-023-1751-8",
        "affiliation_name": "Northeast Normal University",
        "affiliation_city": "Changchun",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Platform-independent and curriculum-oriented intelligent assistant for higher education",
        "paper_author": "Sajja R.",
        "publication": "International Journal of Educational Technology in Higher Education",
        "citied_by": "21",
        "cover_date": "2023-12-01",
        "Abstract": "Miscommunication between instructors and students is a significant obstacle to post-secondary learning. Students may skip office hours due to insecurities or scheduling conflicts, which can lead to missed opportunities for questions. To support self-paced learning and encourage creative thinking skills, academic institutions must redefine their approach to education by offering flexible educational pathways that recognize continuous learning. To this end, we developed an AI-augmented intelligent educational assistance framework based on a powerful language model (i.e., GPT-3) that automatically generates course-specific intelligent assistants regardless of discipline or academic level. The virtual intelligent teaching assistant (TA) system, which is at the core of our framework, serves as a voice-enabled helper capable of answering a wide range of course-specific questions, from curriculum to logistics and course policies. By providing students with easy access to this information, the virtual TA can help to improve engagement and reduce barriers to learning. At the same time, it can also help to reduce the logistical workload for instructors and TAs, freeing up their time to focus on other aspects of teaching and supporting students. Its GPT-3-based knowledge discovery component and the generalized system architecture are presented accompanied by a methodical evaluation of the system’s accuracy and performance.",
        "DOI": "10.1186/s41239-023-00412-7",
        "affiliation_name": "College of Liberal Arts and Sciences",
        "affiliation_city": "Iowa City",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Quantum reinforcement learning via policy iteration",
        "paper_author": "Cherrat E.A.",
        "publication": "Quantum Machine Intelligence",
        "citied_by": "11",
        "cover_date": "2023-12-01",
        "Abstract": "Quantum computing has shown the potential to substantially speed up machine learning applications, in particular for supervised and unsupervised learning. Reinforcement learning, on the other hand, has become essential for solving many decision-making problems and policy iteration methods remain the foundation of such approaches. In this paper, we provide a general framework for performing quantum reinforcement learning via policy iteration. We validate our framework by designing and analyzing quantum policy evaluation methods for infinite-horizon discounted problems by building quantum states that approximately encode the value function of a policy, and quantum policy improvement methods by post-processing measurement outcomes on these quantum states. Last, we study the theoretical and experimental performance of our quantum algorithms on two environments from OpenAI’s Gym.",
        "DOI": "10.1007/s42484-023-00116-1",
        "affiliation_name": "Institut de Recherche en Informatique Fondamentale (IRIF)",
        "affiliation_city": "Paris",
        "affiliation_country": "France"
    },
    {
        "paper_title": "Predicting COVID-19 exposure risk perception using machine learning",
        "paper_author": "Bakkeli N.Z.",
        "publication": "BMC Public Health",
        "citied_by": "4",
        "cover_date": "2023-12-01",
        "Abstract": "Background: Self-perceived exposure risk determines the likelihood of COVID-19 preventive measure compliance to a large extent and is among the most important predictors of mental health problems. Therefore, there is a need to systematically identify important predictors of such risks. This study aims to provide insight into forecasting and understanding risk perceptions and help to adjust interventions that target various social groups in different pandemic phases. Methods: This study was based on survey data collected from 5001 Norwegians in 2020 and 2021. Interpretable machine learning algorithms were used to predict perceived exposure risks. To detect the most important predictors, the models with best performance were chosen based on predictive errors and explained variances. Shapley additive values were used to examine individual heterogeneities, interpret feature impact and check interactions between the key predictors. Results: Gradient boosting machine exhibited the best model performance in this study (2020: RMSE=.93, MAE=.74, RSQ=.22; 2021: RMSE=.99, MAE=.77, RSQ=.12). The most influential predictors of perceived exposure risk were compliance with interventions, work-life conflict, age and gender. In 2020, work and occupation played a dominant role in predicting perceived risks whereas, in 2021, living and behavioural factors were among the most important predictors. Findings show large individual heterogeneities in feature importance based on people’s sociodemographic backgrounds, work and living situations. Conclusion: The findings provide insight into forecasting risk groups and contribute to the early detection of vulnerable people during the pandemic. This is useful for policymakers and stakeholders in developing timely interventions targeting different social groups. Future policies and interventions should be adapted to the needs of people with various life situations.",
        "DOI": "10.1186/s12889-023-16236-z",
        "affiliation_name": "OsloMet – StorbyUniversitetet",
        "affiliation_city": "Oslo",
        "affiliation_country": "Norway"
    },
    {
        "paper_title": "Spatial distribution of solar PV deployment: an application of the region-based convolutional neural network",
        "paper_author": "Kim S.Y.",
        "publication": "EPJ Data Science",
        "citied_by": "4",
        "cover_date": "2023-12-01",
        "Abstract": "Solar photovoltaic (PV) deployment plays a crucial role in the transition to renewable energy. However, comprehensive models that can effectively explain the variations in solar PV deployment are lacking. This study aims to address this gap by introducing two innovative models: (i) a computer vision model that can estimate spatial distribution of solar PV deployment across neighborhoods using satellite images and (ii) a machine learning (ML) model predicting such distribution based on 43 factors. Our computer vision model using Faster Regions with Convolutional Neural Network (Faster RCNN) achieved a mean Average Precision (mAP) of 81% for identifying solar panels and 95% for identifying roofs. Using this model, we analyzed 652,795 satellite images from Colorado, USA, and found that approximately 7% of households in Colorado have rooftop PV systems, while solar panels cover around 2.5% of roof areas in the state as of early 2021. Of our 16 predictive models, the XGBoost models performed the best, explaining approximately 70% of the variance in rooftop solar deployment. We also found that the share of Democratic party votes, hail and strong wind risks, median home value, the percentage of renters, and solar PV permitting timelines are the key predictors of rooftop solar deployment in Colorado. This study provides insights for business and policy decision making to support more efficient and equitable grid infrastructure investment and distributed energy resource management.",
        "DOI": "10.1140/epjds/s13688-023-00399-1",
        "affiliation_name": "College of Engineering, Design and Computing",
        "affiliation_city": "Denver",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "The Skeletal Oncology Research Group Machine Learning Algorithm (SORG-MLA) for predicting prolonged postoperative opioid prescription after total knee arthroplasty: an international validation study using 3,495 patients from a Taiwanese cohort",
        "paper_author": "Tsai C.C.",
        "publication": "BMC Musculoskeletal Disorders",
        "citied_by": "5",
        "cover_date": "2023-12-01",
        "Abstract": "Background: Preoperative prediction of prolonged postoperative opioid use (PPOU) after total knee arthroplasty (TKA) could identify high-risk patients for increased surveillance. The Skeletal Oncology Research Group machine learning algorithm (SORG-MLA) has been tested internally while lacking external support to assess its generalizability. The aims of this study were to externally validate this algorithm in an Asian cohort and to identify other potential independent factors for PPOU. Methods: In a tertiary center in Taiwan, 3,495 patients receiving TKA from 2010–2018 were included. Baseline characteristics were compared between the external validation cohort and the original developmental cohorts. Discrimination (area under receiver operating characteristic curve [AUROC] and precision-recall curve [AUPRC]), calibration, overall performance (Brier score), and decision curve analysis (DCA) were applied to assess the model performance. A multivariable logistic regression was used to evaluate other potential prognostic factors. Results: There were notable differences in baseline characteristics between the validation and the development cohort. Despite these variations, the SORG-MLA (https://sorg-apps.shinyapps.io/tjaopioid/) remained its good discriminatory ability (AUROC, 0.75; AUPRC, 0.34) and good overall performance (Brier score, 0.029; null model Brier score, 0.032). The algorithm could bring clinical benefit in DCA while somewhat overestimating the probability of prolonged opioid use. Preoperative acetaminophen use was an independent factor to predict PPOU (odds ratio, 2.05). Conclusions: The SORG-MLA retained its discriminatory ability and good overall performance despite the different pharmaceutical regulations. The algorithm could be used to identify high-risk patients and tailor personalized prevention policy.",
        "DOI": "10.1186/s12891-023-06667-5",
        "affiliation_name": "National Taiwan University Hospital",
        "affiliation_city": "Taipei",
        "affiliation_country": "Taiwan"
    },
    {
        "paper_title": "An integrated control strategy for simultaneous robot assignment, tool change and preventive maintenance scheduling using Heterogeneous Graph Neural Network",
        "paper_author": "Bhatta K.",
        "publication": "Robotics and Computer-Integrated Manufacturing",
        "citied_by": "10",
        "cover_date": "2023-12-01",
        "Abstract": "There has been a leap in the field of smart manufacturing with the advancement of automation systems, robotic technology, big data analytics, and state-of-the-art Artificial Intelligence (AI) and Machine Learning (ML) algorithms. Three very important aspects of smart manufacturing systems are system productivity, product quality, and maintenance of machines and equipment. These three issues are strongly interrelated and collectively determine the performance of a smart manufacturing system. Although there has been significant studies in production control, quality control and maintenance scheduling to address each of these aspects individually, there has been a lack of sufficient studies taking all of them into consideration in one control scheme. In this paper, a mobile multi-skilled robot operated Flexible Manufacturing System (FMS) is considered and a model that integrates robots, individual workstation processes and product quality is developed using a Heterogeneous Graph Structure. Heterogeneous Graph Neural Network (HGNN) is used to aggregate local information from different nodes of the graph model to create node embeddings that represent global information. A control problem is then formulated in the Decentralized Partially Observable Markov Decision Process (Dec-POMDP) framework to simultaneously consider robot assignment, product quality and maintenance scheduling. The problem is solved using Multi-Agent Reinforcement Learning (MARL). A case study is presented to demonstrate the effectiveness of the HGNN-MARL control strategy by comparing it to three baselines and the naive MARL policy without HGNN.",
        "DOI": "10.1016/j.rcim.2023.102594",
        "affiliation_name": "University of Virginia School of Engineering and Applied Science",
        "affiliation_city": "Charlottesville",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Personalized Incentives as Feedback Design in Generalized Nash Equilibrium Problems",
        "paper_author": "Fabiani F.",
        "publication": "IEEE Transactions on Automatic Control",
        "citied_by": "2",
        "cover_date": "2023-12-01",
        "Abstract": "We investigate both stationary and time-varying, nonmonotone-generalized Nash equilibrium problems that exhibit symmetric interactions among the agents, which are known to be potential. As may happen in practical cases, however, we envision a scenario in which the formal expression of the underlying potential function is not available, and we design a semidecentralized Nash-equilibrium-seeking algorithm. In the proposed two-layer scheme, a coordinator iteratively integrates possibly noisy and sporadic agents' feedback to learn the pseudogradients of the agents and then design personalized incentives for them. On their side, the agents receive those personalized incentives, compute a solution to an extended game, and then return feedback measurements to the coordinator. In the stationary setting, our algorithm returns a Nash equilibrium in case the coordinator is endowed with standard learning policies while it returns a Nash equilibrium up to a constant, yet adjustable, error in the time-varying case. As a motivating application, we consider the ride-hailing service provided by several competing companies with mobility as a service orchestration, necessary to both handle competition among firms and avoid traffic congestion.",
        "DOI": "10.1109/TAC.2023.3287218",
        "affiliation_name": "École Nationale Supérieure de Techniques Avancées",
        "affiliation_city": "Palaiseau",
        "affiliation_country": "France"
    },
    {
        "paper_title": "Structural Analysis of the Evolution Mechanism of Online Public Opinion and its Development Stages Based on Machine Learning and Social Network Analysis",
        "paper_author": "Liu Z.",
        "publication": "International Journal of Computational Intelligence Systems",
        "citied_by": "10",
        "cover_date": "2023-12-01",
        "Abstract": "Internet public opinion is a complex and changeable system, and its trend development is characterized by explosive, evolutionary uncertainty, concealment and interactivity due to the participation of the vast number of Internet users. Today, with the rapid development of network information technology, public opinion has an increasing influence on the stable development of society. Computational intelligence is the frontier field of artificial intelligence development, and computational intelligence is used to mine and analyze public opinion text information and study the evolution of online public opinion. This paper uses the Changchun Changsheng Vaccine Incident as an example, and the netizens’ degree of attention to emergency-related keyword searches in the Baidu Index as a descriptive variable for the development of network public opinion. After applying the optimal segmentation algorithm, the development of public opinion is divided into phases. On this basis, a social network analysis is adopted to analyze the spatial and topological structure of each phase of network public opinion, using data from the Sina Weibo platform. Based on optimal segmentation, the development of network public opinion of the Changchun Changsheng Vaccine Incident can be divided into four phases, namely latent, spreading, control, and stable; each phase has different spatial and topological characteristics. Corresponding policy suggestions on network public opinion governance are put forward for each phase.",
        "DOI": "10.1007/s44196-023-00277-8",
        "affiliation_name": "Jishou University",
        "affiliation_city": "Jishou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "The trilemma among CO<inf>2</inf> emissions, energy use, and economic growth in Russia",
        "paper_author": "Magazzino C.",
        "publication": "Scientific Reports",
        "citied_by": "36",
        "cover_date": "2023-12-01",
        "Abstract": "This paper examines the relationship among CO2 emissions, energy use, and GDP in Russia using annual data ranging from 1990 to 2020. We first conduct time-series analyses (stationarity, structural breaks, cointegration, and causality tests). Then, we performed some Machine Learning experiments as robustness checks. Both approaches underline a bidirectional causal flow between energy use and CO2 emissions; a unidirectional link running from CO2 emissions to real GDP; and the predominance of the “neutrality hypothesis” for energy use-GDP nexus. Therefore, energy conservation measures should not adversely affect the economic growth path of the country. In the current geopolitical scenario, relevant policy implications may be derived.",
        "DOI": "10.1038/s41598-023-37251-5",
        "affiliation_name": "Niccolò Cusano University",
        "affiliation_city": "Rome",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Which Combination of Trade Provisions Promotes Trade in Value-Added? An Application of Machine Learning to Cross-Country Data",
        "paper_author": "Sharma S.",
        "publication": "Economic Papers",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "Over time, the trade agreements are witnessing a substantial change in their provisions by encompassing provisions beyond their conventional trade domain, such as labour market regulations, environmental regulations and competition policies. Theoretically, studies argued the role of signing an agreement with deep provisions to promote trade in value-added, but empirical verification in favour of a few is rarely available. The present study attempts to identify this set of provisions included in deep trade agreements (DTAs) that positively impact the bilateral trade in value added. Using the traditional gravity model framework and its estimation through modern econometric and machine learning tools, the study shows that incorporating provisions relating to establishing and preserving economic rights in trade agreements promotes trade in value-added among member countries. Notably, the study found the combination of three main policy areas: technical barriers to trade, competition policy and labour market regulations. Both econometric and machine learning methods confirm the significant impact of these three provisions. Understanding the significance of specific provisions holds relevance in the current scenario where major trading economies are calibrating trade agreements. From the policy perspective, disentangling a set of provisions might be relevant for designing and negotiating trade agreements.",
        "DOI": "10.1111/1759-3441.12398",
        "affiliation_name": "Birla Institute of Technology and Science, Pilani",
        "affiliation_city": "Pilani",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Feasible intervention combinations for achieving a safe exit of the Zero-COVID policy in China and its determinants: an individual-based model study",
        "paper_author": "Cheng Q.",
        "publication": "BMC Infectious Diseases",
        "citied_by": "2",
        "cover_date": "2023-12-01",
        "Abstract": "Background: Although several pathways have been proposed as the prerequisite for a safe phase-out in China, it is not clear which of them are the most important for keeping the mortality rate low, what thresholds should be achieved for these most important interventions, and how the thresholds change with the assumed key epidemiological parameters and population characteristics. Methods: We developed an individual-based model (IBM) to simulate the transmission of the Omicron variant in the synthetic population, accounting for the age-dependent probabilities of severe clinical outcomes, waning vaccine-induced immunity, increased mortality rates when hospitals are overburdened, and reduced transmission when self-isolated at home after testing positive. We applied machine learning algorithms on the simulation outputs to examine the importance of each intervention parameter and the feasible intervention parameter combinations for safe exits, which is defined as having mortality rates lower than that of influenza in China (14.3 per 100, 000 persons). Results: We identified vaccine coverage in those above 70 years old, number of ICU beds per capita, and the availability of antiviral treatment as the most important interventions for safe exits across all studied locations, although the thresholds required for safe exits vary remarkably with the assumed vaccine effectiveness, as well as the age structure, age-specific vaccine coverage, community healthcare capacity of the studied locations. Conclusions: The analytical framework developed here can provide the basis for further policy decisions that incorporate considerations about economic costs and societal impacts. Achieving safe exits from the Zero-COVID policy is possible, but challenging for China’s cities. When planning for safe exits, local realities such as the age structure and current age-specific vaccine coverage must be taken into consideration.",
        "DOI": "10.1186/s12879-023-08382-x",
        "affiliation_name": "University of California, Berkeley",
        "affiliation_city": "Berkeley",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Transferring human emotions to robot motions using Neural Policy Style Transfer",
        "paper_author": "Fernandez-Fernandez R.",
        "publication": "Cognitive Systems Research",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "Neural Style Transfer (NST) was originally proposed to use feature extraction capabilities of Neural Networks as a way to perform Style Transfer with images. Pre-trained image classification architectures were selected for feature extraction, leading to new images showing the same content as the original but with a different style. In robotics, Style Transfer can be employed to transfer human motion styles to robot motions. The challenge lies in the lack of pre-trained classification architectures for robot motions that could be used for feature extraction. Neural Policy Style Transfer TD3 (NPST3) is proposed for the transfer of human motion styles to robot motions. This framework allows the same robot motion to be executed in different human-centered motion styles, such as in an “angry”, “happy”, “calm”, or “sad” fashion. The Twin Delayed Deep Deterministic Policy Gradient (TD3) network is introduced for the generation of control policies. An autoencoder network is in charge of feature extraction for the Style Transfer step. The Style Transfer step can be performed both offline and online: offline for the autonomous executions of human-style robot motions, and online for adapting at runtime the style of e.g., a teleoperated robot. The framework is tested using two different robotic platforms: a robotic manipulator designed for telemanipulation tasks, and a humanoid robot designed for social interaction. The proposed approach was evaluated for both platforms, performing a total of 147 questionnaires asking human subjects to recognize the human motion style transferred to the robot motion for a predefined set of actions.",
        "DOI": "10.1016/j.cogsys.2023.05.010",
        "affiliation_name": "Universidad Complutense de Madrid",
        "affiliation_city": "Madrid",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "Modelling differential urban growth dynamics for growth decentralisation: a study on Tiruchirappalli metropolitan and sub-tier towns, India",
        "paper_author": "Prakash K.",
        "publication": "Asia-Pacific Journal of Regional Science",
        "citied_by": "4",
        "cover_date": "2023-12-01",
        "Abstract": "Urbanisation requires careful planning and monitoring to overcome challenges like overpopulation, inadequate housing, and sanitation. Significant investments are necessary to reorganise urban areas or promote sub-tier urban centres as an approach of growth decentralisation. This study examined urban growth dynamics in Tiruchirappalli and surrounding sub-tier urban centres within a 40-km radius between 1996, 2008, and 2020. Researchers produced highly accurate land use/cover maps using unsupervised classification techniques and simulated these maps using a CA–Markov model powered by an Artificial Neural Network (ANN) that uses Multi-Layer Perceptron (MLP) algorithm to predict land use changes for the years 2035 and 2050. Statistical methods have quantified land use/cover change rate, growth deviation, and degree of freedom/diversity to explain urban built-up growth dynamics. The CA-Markov simulations show that urban built-up areas are likely to remain the major land use for potential growth, with 174.9 sq. km and 209.3 sq. km in 2035 and 2050, respectively. Urban built-up was the leading class in terms of growth between 1996–2008 and 2008–2020, and growth deviation was high in multiple zones of Tiruchirappalli and Thiruverumbur, indicating significant variation between observed and expected growth rates. The degree of disparity showed a decreasing trend between 1996–2008 and 2008–2020, with higher disparity values recorded in Tiruchirappalli and Thiruverumbur than other urban centres due to the global recession and fiscal policies. At the current rate of growth, Tiruchirappalli urban may experience a significant loss of agricultural land and environmental damage from urban pollutants in surrounding water bodies and fertile lands. The study emphasizes mutual growth of sub-tier urban centres, as suggested by the Indian planning body (NITI Aayog), is a significant intervention to address the negative impacts of urban spatial growth.",
        "DOI": "10.1007/s41685-023-00301-x",
        "affiliation_name": "Bharathidasan University",
        "affiliation_city": "Tiruchirappalli",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Is technology truly improving the customer experience? Analysing the intention to use open banking in Indonesia",
        "paper_author": "Iman N.",
        "publication": "International Journal of Bank Marketing",
        "citied_by": "22",
        "cover_date": "2023-12-01",
        "Abstract": "Purpose: Open banking, with its promise to revolutionise electronic transactions through open application programming interfaces (APIs), aims to bridge the gap between banks and non-banks, enhancing lending, payments, investments and funds distribution. However, does this bold innovation truly resonate with consumers? This study delves into consumer intentions to adopt open banking in Indonesia by leveraging the technology readiness model, scrutinising its antecedents and moderating factors, and identifying the key attributes that users anticipate. Design/methodology/approach: Through quantitative and qualitative approaches, this study answers the following questions: (1) Are financial service users ready to use open banking/open API applications? (2) What are the key attributes that consumer expects of open banking/open API? First, the authors developed a structural model based on the technology readiness model, distributed the questionnaire in eight major cities in Indonesia, analysed it using PLS-SEM and utilised a machine learning approach to unpack the main attributes expected from open banking. Findings: This study’s findings indicate that customers are generally prepared to embrace open banking innovations. Nonetheless, to enhance public acceptance, certain factors should be emphasised, including organisational support, user-friendly technology, a comprehensive range of features, consumer financial literacy and banks' readiness to adopt open banking. In contrast to prior research, this study reveals that loyalty to traditional banking positively moderates the connection between customer value and the intention to utilise open banking. Additionally, the authors did not observe a significant moderating effect of financial literacy on the relationship between perceived customer value and the intention to use open banking. Originality/value: To the best of the authors’ knowledge, this study is one of the few that comprehensively analyses the consumers' readiness for open banking in developing contexts. This study is expected to produce a theoretical contribution as well as effective and optimal policies for the financial services sector.",
        "DOI": "10.1108/IJBM-09-2022-0427",
        "affiliation_name": "Universitas Gadjah Mada",
        "affiliation_city": "Yogyakarta",
        "affiliation_country": "Indonesia"
    },
    {
        "paper_title": "Increasing the accuracy of the asthma diagnosis using an operational definition for asthma and a machine learning method",
        "paper_author": "Joo H.",
        "publication": "BMC Pulmonary Medicine",
        "citied_by": "4",
        "cover_date": "2023-12-01",
        "Abstract": "Introduction: Analysis of the National Health Insurance data has been actively carried out for the purpose of academic research and establishing scientific evidences for health care service policy in asthma. However, there has been a limitation for the accuracy of the data extracted through conventional operational definition. In this study, we verified the accuracy of conventional operational definition of asthma, by applying it to a real hospital setting. And by using a machine learning technique, we established an appropriate operational definition that predicts asthma more accurately. Methods: We extracted asthma patients using the conventional operational definition of asthma at Seoul St. Mary’s hospital and St. Paul’s hospital at the Catholic University of Korea between January 2017 and January 2018. Among these extracted patients of asthma, 10% of patients were randomly sampled. We verified the accuracy of the conventional operational definition for asthma by matching actual diagnosis through medical chart review. And then we operated machine learning approaches to predict asthma more accurately. Results: A total of 4,235 patients with asthma were identified using a conventional asthma definition during the study period. Of these, 353 patients were collected. The patients of asthma were 56% of study population, 44% of patients were not asthma. The use of machine learning techniques improved the overall accuracy. The XGBoost prediction model for asthma diagnosis showed an accuracy of 87.1%, an AUC of 93.0%, sensitivity of 82.5%, and specificity of 97.9%. Major explanatory variable were ICS/LABA,LAMA and LTRA for proper diagnosis of asthma. Conclusions: The conventional operational definition of asthma has limitation to extract true asthma patients in real world. Therefore, it is necessary to establish an accurate standardized operational definition of asthma. In this study, machine learning approach could be a good option for building a relevant operational definition in research using claims data.",
        "DOI": "10.1186/s12890-023-02479-4",
        "affiliation_name": "Yonsei University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "Using the interest theory of rights and Hohfeldian taxonomy to address a gap in machine learning methods for legal document analysis",
        "paper_author": "Izzidien A.",
        "publication": "Humanities and Social Sciences Communications",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "Rights and duties are essential features of legal documents. Machine learning algorithms have been increasingly applied to extract information from such texts. Currently, their main focus is on named entity recognition, sentiment analysis, and the classification of court cases to predict court outcome. In this paper it is argued that until the essential features of such texts are captured, their analysis can remain bottle-necked by the very technology being used to assess them. As such, the use of legal theory to identify the most pertinent dimensions of such texts is proposed. Specifically, the interest theory of rights, and the first-order Hohfeldian taxonomy of legal relations. These principal legal dimensions allow for a stratified representation of knowledge, making them ideal for the abstractions needed for machine learning. This study considers how such dimensions may be identified. To do so it implements a novel heuristic based in philosophy coupled with language models. Hohfeldian relations of ‘rights-duties’ vs. ‘privileges-no-rights’ are determined to be identifiable. Classification of each type of relation to accuracies of 92.5% is found using Sentence Bidirectional Encoder Representations from Transformers. Testing is carried out on religious discrimination policy texts in the United Kingdom.",
        "DOI": "10.1057/s41599-023-01693-z",
        "affiliation_name": "University of Cambridge",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "A Deep Q-Network based hand gesture recognition system for control of robotic platforms",
        "paper_author": "Cruz P.J.",
        "publication": "Scientific Reports",
        "citied_by": "16",
        "cover_date": "2023-12-01",
        "Abstract": "Hand gesture recognition (HGR) based on electromyography signals (EMGs) and inertial measurement unit signals (IMUs) has been investigated for human-machine applications in the last few years. The information obtained from the HGR systems has the potential to be helpful to control machines such as video games, vehicles, and even robots. Therefore, the key idea of the HGR system is to identify the moment in which a hand gesture was performed and it’s class. Several human-machine state-of-the-art approaches use supervised machine learning (ML) techniques for the HGR system. However, the use of reinforcement learning (RL) approaches to build HGR systems for human-machine interfaces is still an open problem. This work presents a reinforcement learning (RL) approach to classify EMG-IMU signals obtained using a Myo Armband sensor. For this, we create an agent based on the Deep Q-learning algorithm (DQN) to learn a policy from online experiences to classify EMG-IMU signals. The HGR proposed system accuracy reaches up to 97.45 ± 1.02 % and 88.05 ± 3.10 % for classification and recognition respectively, with an average inference time per window observation of 20 ms. and we also demonstrate that our method outperforms other approaches in the literature. Then, we test the HGR system to control two different robotic platforms. The first is a three-degrees-of-freedom (DOF) tandem helicopter test bench, and the second is a virtual six-degree-of-freedom (DOF) UR5 robot. We employ the designed hand gesture recognition (HGR) system and the inertial measurement unit (IMU) integrated into the Myo sensor to command and control the motion of both platforms. The movement of the helicopter test bench and the UR5 robot is controlled under a PID controller scheme. Experimental results show the effectiveness of using the proposed HGR system based on DQN for controlling both platforms with a fast and accurate response.",
        "DOI": "10.1038/s41598-023-34540-x",
        "affiliation_name": "Escuela Politécnica Nacional",
        "affiliation_city": "Quito",
        "affiliation_country": "Ecuador"
    },
    {
        "paper_title": "Application of supervised machine learning algorithms for classification and prediction of type-2 diabetes disease status in Afar regional state, Northeastern Ethiopia 2021",
        "paper_author": "Ebrahim O.A.",
        "publication": "Scientific Reports",
        "citied_by": "15",
        "cover_date": "2023-12-01",
        "Abstract": "Ethiopia has been challenged by the growing magnitude of diabetes in general and type-2 diabetes in particular. Knowledge extraction from stored dataset can be an important base for better decision on diabetes rapid diagnosis, suggestive on prediction for early intervention. Thus, this study was addressed these problem by application of supervised machine learning algorithms for classification and prediction of type 2 diabetes disease status and might provide context-specific information to program planners and policy makers so that, priority will be given to the more affected groups. To apply supervised machine learning algorithms; compare these algorithms and select the best algorithm based on their performance for classification and prediction of type-2 diabetic disease status (positive or negative) in public hospitals of Afar regional state, Northeastern Ethiopia. This study was conducted at Afar regional state from February to June of 2021. Decision tree; pruned J 48, Artificial neural network, K-nearest neighbor, Support vector machine, Binary logistic regression, Random forest, and Naïve Bayes supervised machine learning algorithms were applied using secondary data from the medical database record review. A total of 2239 sample Dataset diagnosed for diabetes from 2012 to April 22/2020 (1523 with type-2 diabetes and 716 without type-2 diabetes) was checked for its completeness prior to analysis. For all algorithms, WEKA3.7 tool was used for analysis purposes. Moreover, all algorithms were compared based on their correctly classification rate, kappa statistics, confusion matrix, area under the curve, sensitivity, and specificity. From the seven major supervised machine learning algorithms, the best classification and prediction results were obtained from random forest [correctly classified rate (93.8%), kappa statistics (0.85), sensitivity (0.98), area under the curve (0.97) and confusion matrix (out of 454 actual positive prediction for 446)] which was followed by decision tree pruned J 48 [correctly classified rate (91.8%), kappa statistics (0.80), sensitivity (0.96), area under the curve (0.91) and confusion matrices (out of 454 actual positive prediction for 438)] and k-nearest neighbor [correctly classified rate (89.8%), kappa statistics (0.76), sensitivity (0.92), area under the curve (0.88) and confusion matrices (out of 454 actual positive prediction for 421)]. Random forest, Decision tree pruned J48 and k-nearest neighbor algorithms have better classification and prediction performance for classifying and predicting type-2 diabetes disease status. Therefore, based on this performance, random forest algorithm can be judged as suggestive and supportive for clinicians at the time of type-2 diabetes diagnosis.",
        "DOI": "10.1038/s41598-023-34906-1",
        "affiliation_name": "Samara University, Ethiopia",
        "affiliation_city": "Semera",
        "affiliation_country": "Ethiopia"
    },
    {
        "paper_title": "Analyzing the impact of COVID-19 on the electricity demand in Austin, TX using an ensemble-model based counterfactual and 400,000 smart meters",
        "paper_author": "Dai T.Y.",
        "publication": "Computational Urban Science",
        "citied_by": "6",
        "cover_date": "2023-12-01",
        "Abstract": "The COVID-19 pandemic caused lifestyle changes and has led to the new electricity demand patterns in the presence of non-pharmaceutical interventions such as work-from-home policy and lockdown. Quantifying the effect on electricity demand is critical for future electricity market planning yet challenging in the context of limited smart metered buildings, which leads to limited understanding of the temporal and spatial variations in building energy use. This study uses a large scale private smart meter electricity demand data from the City of Austin, combined with publicly available environmental data, and develops an ensemble regression model for long term daily electricity demand prediction. Using 15-min resolution data from over 400,000 smart meters from 2018 to 2020 aggregated by building type and zip code, our proposed model precisely formalizes the counterfactual universe in the without COVID-19 scenario. The model is used to understand building electricity demand changes during the pandemic and to identify relationships between such changes and socioeconomic patterns. Results indicate the increase in residential usage , demonstrating the spatial redistribution of energy consumption during the work-from-home period. Our experiments demonstrate the effectiveness of our proposed framework by assessing multiple socioeconomic impacts with the comparison between the counterfactual universe and observations.",
        "DOI": "10.1007/s43762-023-00095-w",
        "affiliation_name": "The University of Texas at Austin",
        "affiliation_city": "Austin",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Data-driven automated control algorithm for floating-zone crystal growth derived by reinforcement learning",
        "paper_author": "Tosa Y.",
        "publication": "Scientific Reports",
        "citied_by": "3",
        "cover_date": "2023-12-01",
        "Abstract": "The complete automation of materials manufacturing with high productivity is a key problem in some materials processing. In floating zone (FZ) crystal growth, which is a manufacturing process for semiconductor wafers such as silicon, an operator adaptively controls the input parameters in accordance with the state of the crystal growth process. Since the operation dynamics of FZ crystal growth are complicated, automation is often difficult, and usually the process is manually controlled. Here we demonstrate automated control of FZ crystal growth by reinforcement learning using the dynamics predicted by Gaussian mixture modeling (GMM) from small numbers of trajectories. Our proposed method of constructing the control model is completely data-driven. Using an emulator program for FZ crystal growth, we show that the control model constructed by our proposed model can more accurately follow the ideal growth trajectory than demonstration trajectories created by human operation. Furthermore, we reveal that policy optimization near the demonstration trajectories realizes accurate control following the ideal trajectory.",
        "DOI": "10.1038/s41598-023-34732-5",
        "affiliation_name": "Nagoya University",
        "affiliation_city": "Nagoya",
        "affiliation_country": "Japan"
    },
    {
        "paper_title": "Causal Machine Learning and its use for public policy",
        "paper_author": "Lechner M.",
        "publication": "Swiss Journal of Economics and Statistics",
        "citied_by": "11",
        "cover_date": "2023-12-01",
        "Abstract": "In recent years, microeconometrics experienced the ‘credibility revolution’, culminating in the 2021 Nobel prices for David Card, Josh Angrist, and Guido Imbens. This ‘revolution’ in how to do empirical work led to more reliable empirical knowledge of the causal effects of certain public policies. In parallel, computer science, and to some extent also statistics, developed powerful (so-called Machine Learning) algorithms that are very successful in prediction tasks. The new literature on Causal Machine Learning unites these developments by using algorithms originating in Machine Learning for improved causal analysis. In this non-technical overview, I review some of these approaches. Subsequently, I use an empirical example from the field of active labour market programme evaluation to showcase how Causal Machine Learning can be applied to improve the usefulness of such studies. I conclude with some considerations about shortcomings and possible future developments of these methods as well as wider implications for teaching and empirical studies.",
        "DOI": "10.1186/s41937-023-00113-y",
        "affiliation_name": "University of St. Gallen",
        "affiliation_city": "St Gallen",
        "affiliation_country": "Switzerland"
    },
    {
        "paper_title": "Machine learning reduces soft costs for residential solar photovoltaics",
        "paper_author": "Dong C.",
        "publication": "Scientific Reports",
        "citied_by": "6",
        "cover_date": "2023-12-01",
        "Abstract": "Further deployment of rooftop solar photovoltaics (PV) hinges on the reduction of soft (non-hardware) costs—now larger and more resistant to reductions than hardware costs. The largest portion of these soft costs is the expenses solar companies incur to acquire new customers. In this study, we demonstrate the value of a shift from significance-based methodologies to prediction-oriented models to better identify PV adopters and reduce soft costs. We employ machine learning to predict PV adopters and non-adopters, and compare its prediction performance with logistic regression, the dominant significance-based method in technology adoption studies. Our results show that machine learning substantially enhances adoption prediction performance: The true positive rate of predicting adopters increased from 66 to 87%, and the true negative rate of predicting non-adopters increased from 75 to 88%. We attribute the enhanced performance to complex variable interactions and nonlinear effects incorporated by machine learning. With more accurate predictions, machine learning is able to reduce customer acquisition costs by 15% ($0.07/Watt) and identify new market opportunities for solar companies to expand and diversify their customer bases. Our research methods and findings provide broader implications for the adoption of similar clean energy technologies and related policy challenges such as market growth and energy inequality.",
        "DOI": "10.1038/s41598-023-33014-4",
        "affiliation_name": "University of Wisconsin-Madison",
        "affiliation_city": "Madison",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Profiling low-proficiency science students in the Philippines using machine learning",
        "paper_author": "Bernardo A.B.I.",
        "publication": "Humanities and Social Sciences Communications",
        "citied_by": "10",
        "cover_date": "2023-12-01",
        "Abstract": "Filipino students’ performance in global assessments of science literacy has always been low, and this was confirmed again in the PISA 2018, where Filipino learners’ average science literacy scores ranked second to last among 78 countries. In this study, machine learning approaches were used to analyze PISA data from the student questionnaire to test models that best identify the poorest-performing Filipino students. The goal was to explore factors that could help identify the students who are vulnerable to very low achievement in science and that could indicate possible targets for reform in science education in the Philippines. The random forest classifier model was found to be the most accurate and more precise, and Shapley Additive Explanations indicated 15 variables that were most important in identifying the low-proficiency science students. The variables related to metacognitive awareness of reading strategies, social experiences in school, aspirations and pride about achievements, and family/home factors, include parents’ characteristics and access to ICT with internet connections. The results of the factors highlight the importance of considering personal and contextual factors beyond the typical instructional and curricular factors that are the foci of science education reform in the Philippines, and some implications for programs and policies for science education reform are suggested.",
        "DOI": "10.1057/s41599-023-01705-y",
        "affiliation_name": "De La Salle University",
        "affiliation_city": "Manila",
        "affiliation_country": "Philippines"
    },
    {
        "paper_title": "Modelling the pulse population-wide nucleic acid screening in mitigating and stopping COVID-19 outbreaks in China",
        "paper_author": "Li Q.",
        "publication": "BMC Infectious Diseases",
        "citied_by": "2",
        "cover_date": "2023-12-01",
        "Abstract": "Background: During 2021-2022, mainland China experienced multiple times of local COVID-19 outbreaks in several cities, including Yangzhou, Xi’an etc., and the Chinese government persistently adopted the zero-COVID policy in combating with the local outbreaks. Methods: We develop a mathematical model with pulse population-wide nucleic acid screening, part of the zero-COVID policy, to reveal its role in controlling the spread of COVID-19. We calibrate the model by fitting the COVID-19 epidemic data of the local outbreaks in Yangzhou and Xi’an, China. Sensitivity analysis is conducted to investigate the impact of population-wide nucleic acid screening on controlling the outbreak of COVID-19. Results: Without the screening, the cumulative number of confirmed cases increases by 77.7 % and 62.2 % in Yangzhou and Xi’an, respectively. Meanwhile, the screening program helps to shorten the lockdown period for more than one month when we aim at controlling the cases into zero. Considering its role in mitigating the epidemics, we observe a paradox phenomenon of the screening rate in avoiding the runs on medical resource. That is, the screening will aggravate the runs on medical resource when the screening rate is small, while it helps to relieve the runs on medical resource if the screening rate is high enough. We also conclude that the screening has limited effects on mitigating the epidemics if the outbreak is in a high epidemic level or there has already been runs on medical resources. Alternatively, a smaller screening population per time with a higher screening frequency may be a better program to avoid the runs on medical resources. Conclusions: The population-wide nucleic acid screening strategy plays an important role in quickly controlling and stopping the local outbreaks under the zero-COVID policy. However, it has limited impacts and even increase the potential risk of the runs on medical resource for containing the large scale outbreaks.",
        "DOI": "10.1186/s12879-023-08265-1",
        "affiliation_name": "Xi'an University of Technology",
        "affiliation_city": "Xi'an",
        "affiliation_country": "China"
    },
    {
        "paper_title": "How Does the Fed Make Decisions: A Machine Learning Augmented Taylor Rule",
        "paper_author": "Wu B.",
        "publication": "Journal of Fixed Income",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "The Federal funds rate is a cornerstone of asset pricing that has a significant impact on asset valuation and portfolio performance. However, estimating it reliably can be a challenging issue given that the FOMC makes monetary policy decisions based on complex economic conditions. The authors leveraged existing literatures’ findings on factors and combined those major factor categories into the new model, which includes inflation, labor markets, financial condition, and proxy of global market, and the authors selected the optimal data series to optimize the effectiveness of detecting Fed decisions through a classification factor selection process. Also, the authors improved the regression from fixed coefficients to gradient boosting nonlinear regression approach to reflect the dynamic interconnections among all the factors and their lags through different periods. Upon conducting out-of-sample forecasting, with these selected factors and machine learning gradient boosting regression, the out-of-sample RMSE improved by 77% from traditional Taylor rule model, which offered an alternative robust solution for forecasting the Federal fund rates that can be further applied to asset pricing and investing.",
        "DOI": "10.3905/jfi.2022.32.3.049",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "An archetypes approach to malaria intervention impact mapping: a new framework and example application",
        "paper_author": "Bertozzi-Villa A.",
        "publication": "Malaria Journal",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "Background: As both mechanistic and geospatial malaria modeling methods become more integrated into malaria policy decisions, there is increasing demand for strategies that combine these two methods. This paper introduces a novel archetypes-based methodology for generating high-resolution intervention impact maps based on mechanistic model simulations. An example configuration of the framework is described and explored. Methods: First, dimensionality reduction and clustering techniques were applied to rasterized geospatial environmental and mosquito covariates to find archetypal malaria transmission patterns. Next, mechanistic models were run on a representative site from each archetype to assess intervention impact. Finally, these mechanistic results were reprojected onto each pixel to generate full maps of intervention impact. The example configuration used ERA5 and Malaria Atlas Project covariates, singular value decomposition, k-means clustering, and the Institute for Disease Modeling’s EMOD model to explore a range of three-year malaria interventions primarily focused on vector control and case management. Results: Rainfall, temperature, and mosquito abundance layers were clustered into ten transmission archetypes with distinct properties. Example intervention impact curves and maps highlighted archetype-specific variation in efficacy of vector control interventions. A sensitivity analysis showed that the procedure for selecting representative sites to simulate worked well in all but one archetype. Conclusion: This paper introduces a novel methodology which combines the richness of spatiotemporal mapping with the rigor of mechanistic modeling to create a multi-purpose infrastructure for answering a broad range of important questions in the malaria policy space. It is flexible and adaptable to a range of input covariates, mechanistic models, and mapping strategies and can be adapted to the modelers’ setting of choice.",
        "DOI": "10.1186/s12936-023-04535-0",
        "affiliation_name": "Curtin University",
        "affiliation_city": "Perth",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "A multi-modal machine learning approach to detect extreme rainfall events in Sicily",
        "paper_author": "Vitanza E.",
        "publication": "Scientific Reports",
        "citied_by": "9",
        "cover_date": "2023-12-01",
        "Abstract": "In 2021 almost 300 mm of rain, nearly half of the average annual rainfall, fell near Catania (Sicily Island, Italy). Such events took place in just a few hours, with dramatic consequences on the environmental, social, economic, and health systems of the region. These phenomena are now very common in various countries all around the world: this is the reason why, detecting local extreme rainfall events is a crucial prerequisite for planning actions, able to reverse possibly intensified dramatic future scenarios. In this paper, the Affinity Propagation algorithm, a clustering algorithm grounded on machine learning, was applied, to the best of our knowledge, for the first time, to detect extreme rainfall areas in Sicily. This was possible by using a high-frequency, large dataset we collected, ranging from 2009 to 2021 which we named RSE (the Rainfall Sicily Extreme dataset). Weather indicators were then been employed to validate the results, thus confirming the presence of recent anomalous rainfall events in eastern Sicily. We believe that easy-to-use and multi-modal data science techniques, such as the one proposed in this study, could give rise to significant improvements in policy-making for successfully contrasting climate change.",
        "DOI": "10.1038/s41598-023-33160-9",
        "affiliation_name": "Università degli Studi di Siena",
        "affiliation_city": "Siena",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "A machine learning approach to predict self-protecting behaviors during the early wave of the COVID-19 pandemic",
        "paper_author": "Taye A.D.",
        "publication": "Scientific Reports",
        "citied_by": "5",
        "cover_date": "2023-12-01",
        "Abstract": "Using a unique harmonized real‐time data set from the COME-HERE longitudinal survey that covers five European countries (France, Germany, Italy, Spain, and Sweden) and applying a non-parametric machine learning model, this paper identifies the main individual and macro-level predictors of self-protecting behaviors against the coronavirus disease 2019 (COVID-19) during the first wave of the pandemic. Exploiting the interpretability of a Random Forest algorithm via Shapely values, we find that a higher regional incidence of COVID-19 triggers higher levels of self-protective behavior, as does a stricter government policy response. The level of individual knowledge about the pandemic, confidence in institutions, and population density also ranks high among the factors that predict self-protecting behaviors. We also identify a steep socioeconomic gradient with lower levels of self-protecting behaviors being associated with lower income and poor housing conditions. Among socio-demographic factors, gender, marital status, age, and region of residence are the main determinants of self-protective measures.",
        "DOI": "10.1038/s41598-023-33033-1",
        "affiliation_name": "University of Luxembourg",
        "affiliation_city": "Esch-sur-Alzette",
        "affiliation_country": "Luxembourg"
    },
    {
        "paper_title": "Reliability of electric vehicle charging infrastructure: A cross-lingual deep learning approach",
        "paper_author": "Liu Y.",
        "publication": "Communications in Transportation Research",
        "citied_by": "36",
        "cover_date": "2023-12-01",
        "Abstract": "Vehicle electrification has emerged as a global strategy to address climate change and emissions externalities from the transportation sector. Deployment of charging infrastructure is needed to accelerate technology adoption; however, managers and policymakers have had limited evidence on the use of public charging stations due to poor data sharing and decentralized ownership across regions. In this article, we use machine learning based classifiers to reveal insights about consumer charging behavior in 72 detected languages including Chinese. We investigate 10 years of consumer reviews in East and Southeast Asia from 2011 to 2021 to enable infrastructure evaluation at a larger geographic scale than previously available. We find evidence that charging stations at government locations result in higher failure rates with consumers compared to charging stations at private points of interest. This evidence contrasts with predictions in the U.S. and European markets, where the performance is closer to parity. We also find that networked stations with communication protocols provide a relatively higher quality of charging services, which favors policy support for connectivity, particularly for underserved or remote areas.",
        "DOI": "10.1016/j.commtr.2023.100095",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Atlanta",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Machine learning models, trusted research environments and UK health data: ensuring a safe and beneficial future for AI development in healthcare",
        "paper_author": "Kerasidou C.",
        "publication": "Journal of Medical Ethics",
        "citied_by": "6",
        "cover_date": "2023-12-01",
        "Abstract": "Digitalisation of health and the use of health data in artificial intelligence, and machine learning (ML), including for applications that will then in turn be used in healthcare are major themes permeating current UK and other countries’ healthcare systems and policies. Obtaining rich and representative data is key for robust ML development, and UK health data sets are particularly attractive sources for this. However, ensuring that such research and development is in the public interest, produces public benefit and preserves privacy are key challenges. Trusted research environments (TREs) are positioned as a way of balancing the diverging interests in healthcare data research with privacy and public benefit. Using TRE data to train ML models presents various challenges to the balance previously struck between these societal interests, which have hitherto not been discussed in the literature. These challenges include the possibility of personal data being disclosed in ML models, the dynamic nature of ML models and how public benefit may be (re)conceived in this context. For ML research to be facilitated using UK health data, TREs and others involved in the UK health data policy ecosystem need to be aware of these issues and work to address them in order to continue to ensure a’safe’ health and care data environment that truly serves the public.",
        "DOI": "10.1136/jme-2022-108696",
        "affiliation_name": "University of Dundee School of Medicine",
        "affiliation_city": "Dundee",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Reinforcement Learning-Based Wind Farm Control: Toward Large Farm Applications via Automatic Grouping and Transfer Learning",
        "paper_author": "Dong H.",
        "publication": "IEEE Transactions on Industrial Informatics",
        "citied_by": "17",
        "cover_date": "2023-12-01",
        "Abstract": "The high system complexity and strong wake effects bring significant challenges to wind farm operations. Conventional wind farm control methods may lead to degraded power generation efficiency. A reinforcement learning (RL)-based approach is proposed in this article to handle these issues, which can increase the long-term farm-level power generation subject to strong wake effects while without requiring analytical wind farm models. The proposed method is significantly distinct from existing RL-based wind farm control approaches, whose computational complexities usually increase heavily with the increase of total turbine numbers. In contrast, our method can greatly reduce training loads and enhance learning efficiency via two novel designs: first, automatic grouping and second, multiagent-based transfer learning (MATL). Automatic grouping can divide a large wind farm into small turbine groups by analyzing the aerodynamic interactions between turbines and utilizing some key principles from the graph theory. It enables the separated conduction of RL algorithms on small turbine groups, avoiding the complex training process and high computational costs of applying RL on the entire farm. Based on automatic grouping, MATL can further reduce the computational complexity by allowing agents (i.e., wind turbines) to inherit control policies under potential group changes. Case studies with a dynamical simulator show that the proposed method achieves clear power generation increases than the benchmark. It also dramatically reduces computational costs compared with typical RL-based wind farm control methods, paving the way for the application of RL in general wind farms.",
        "DOI": "10.1109/TII.2023.3252540",
        "affiliation_name": "Faculty of Science, Engineering and Medicine",
        "affiliation_city": "Coventry",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Influence of land-sea breeze on PM <inf>2.5</inf> prediction in central and southern Taiwan using composite neural network",
        "paper_author": "Kibirige G.W.",
        "publication": "Scientific Reports",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "PM2.5 prediction plays an important role for governments in establishing policies to control the emission of excessive atmospheric pollutants to protect the health of citizens. However, traditional machine learning methods that use data collected from ground-level monitoring stations have reached their limit with poor model generalization and insufficient data. We propose a composite neural network trained with aerosol optical depth (AOD) and weather data collected from satellites, as well as interpolated ocean wind features. We investigate the model outputs of different components of the composite neural network, concluding that the proposed composite neural network architecture yields significant improvements in overall performance compared to each component and the ensemble model benchmarks. The monthly analysis also demonstrates the superiority of the proposed architecture for stations where land-sea breezes frequently occur in the southern and central Taiwan in the months when land-sea breeze dominates the accumulation of PM2.5.",
        "DOI": "10.1038/s41598-023-29845-w",
        "affiliation_name": "National Chengchi University",
        "affiliation_city": "Taipei",
        "affiliation_country": "Taiwan"
    },
    {
        "paper_title": "A deep learning architecture for energy service demand estimation in transport sector for Shared Socioeconomic Pathways",
        "paper_author": "Joshi S.",
        "publication": "Scientific Reports",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "Meeting current global passenger and freight transport energy service demands accounts for 20% of annual anthropogenic CO2 emissions, and mitigating these emissions remains a considerable challenge for climate policy. Pursuant to this, energy service demands play a critical role in the energy systems and integrated assessment models but fail to get the attention they warrant. This study introduces a novel custom deep learning neural network architecture (called TrebuNet) that mimics the physical process of firing a trebuchet to model the nuanced dynamics inherent in energy service demand estimation. Here we show, how TrebuNet is designed, trained, and used to estimate transport energy service demand. We find that the TrebuNet architecture shows superior performance compared with traditional multivariate linear regression and state of the art methods like densely connected neural network, Recurrent Neural Network, and Gradient Boosted machine learning algorithms when evaluated for regional demand projection for all modes of transport demands at short, decadal, and medium-term time horizons. Finally, TrebuNet introduces a framework to project energy service demand for regions having multiple countries spanning different socio-economic development pathways which can be replicated for wider regression-based task for timeseries having non-uniform variance.",
        "DOI": "10.1038/s41598-023-30555-6",
        "affiliation_name": "MaREI, the SFI Research Centre for Energy, Climate and Marine",
        "affiliation_city": "Cork",
        "affiliation_country": "Ireland"
    },
    {
        "paper_title": "High-Tech Modernism: Limits &amp; Extensions",
        "paper_author": "Janeway W.H.",
        "publication": "Daedalus",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "High-tech modernism is a powerful construct for reading the broad range of effects of digitalization on society. This response to Henry Farrell and Marion Fourcade’s essay “The Moral Economy of High-Tech Modernism” first notes that high-tech modernism seems initially specified for application to advanced, quasi-liberal political econ-omies. It then identifies three dimensions along which that construct could usefully be extended: 1) to take account of the limits of machine learning techniques of data anal-ysis; 2) to consider the manner in which algorithmic digitalization transforms both the content and the management of work; and 3) to examine political responses to high-tech modernism, reminiscent of Karl Polanyi’s “double movement,” increasingly observable across a spectrum that runs from competition policy to the labor market.",
        "DOI": "10.1162/daed_a_01984",
        "affiliation_name": "University of Cambridge",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Robustness of quantum reinforcement learning under hardware errors",
        "paper_author": "Skolik A.",
        "publication": "EPJ Quantum Technology",
        "citied_by": "15",
        "cover_date": "2023-12-01",
        "Abstract": "Variational quantum machine learning algorithms have become the focus of recent research on how to utilize near-term quantum devices for machine learning tasks. They are considered suitable for this as the circuits that are run can be tailored to the device, and a big part of the computation is delegated to the classical optimizer. It has also been hypothesized that they may be more robust to hardware noise than conventional algorithms due to their hybrid nature. However, the effect of training quantum machine learning models under the influence of hardware-induced noise has not yet been extensively studied. In this work, we address this question for a specific type of learning, namely variational reinforcement learning, by studying its performance in the presence of various noise sources: shot noise, coherent and incoherent errors. We analytically and empirically investigate how the presence of noise during training and evaluation of variational quantum reinforcement learning algorithms affect the performance of the agents and robustness of the learned policies. Furthermore, we provide a method to reduce the number of measurements required to train Q-learning agents, using the inherent structure of the algorithm.",
        "DOI": "10.1140/epjqt/s40507-023-00166-1",
        "affiliation_name": "Istituto Nazionale di Fisica Nucleare, Sezione di Pavia",
        "affiliation_city": "Pavia",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Precision information extraction for rare disease epidemiology at scale",
        "paper_author": "Kariampuzha W.Z.",
        "publication": "Journal of Translational Medicine",
        "citied_by": "7",
        "cover_date": "2023-12-01",
        "Abstract": "Background: The United Nations recently made a call to address the challenges of an estimated 300 million persons worldwide living with a rare disease through the collection, analysis, and dissemination of disaggregated data. Epidemiologic Information (EI) regarding prevalence and incidence data of rare diseases is sparse and current paradigms of identifying, extracting, and curating EI rely upon time-intensive, error-prone manual processes. With these limitations, a clear understanding of the variation in epidemiology and outcomes for rare disease patients is hampered. This challenges the public health of rare diseases patients through a lack of information necessary to prioritize research, policy decisions, therapeutic development, and health system allocations. Methods: In this study, we developed a newly curated epidemiology corpus for Named Entity Recognition (NER), a deep learning framework, and a novel rare disease epidemiologic information pipeline named EpiPipeline4RD consisting of a web interface and Restful API. For the corpus creation, we programmatically gathered a representative sample of rare disease epidemiologic abstracts, utilized weakly-supervised machine learning techniques to label the dataset, and manually validated the labeled dataset. For the deep learning framework development, we fine-tuned our dataset and adapted the BioBERT model for NER. We measured the performance of our BioBERT model for epidemiology entity recognition quantitatively with precision, recall, and F1 and qualitatively through a comparison with Orphanet. We demonstrated the ability for our pipeline to gather, identify, and extract epidemiology information from rare disease abstracts through three case studies. Results: We developed a deep learning model to extract EI with overall F1 scores of 0.817 and 0.878, evaluated at the entity-level and token-level respectively, and which achieved comparable qualitative results to Orphanet’s collection paradigm. Additionally, case studies of the rare diseases Classic homocystinuria, GRACILE syndrome, Phenylketonuria demonstrated the adequate recall of abstracts with epidemiology information, high precision of epidemiology information extraction through our deep learning model, and the increased efficiency of EpiPipeline4RD compared to a manual curation paradigm. Conclusions: EpiPipeline4RD demonstrated high performance of EI extraction from rare disease literature to augment manual curation processes. This automated information curation paradigm will not only effectively empower development of the NIH Genetic and Rare Diseases Information Center (GARD), but also support the public health of the rare disease community.",
        "DOI": "10.1186/s12967-023-04011-y",
        "affiliation_name": "National Center for Advancing Translational Sciences (NCATS)",
        "affiliation_city": "Bethesda",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "A maximum entropy approach for the modelling of car-sharing parking dynamics",
        "paper_author": "Daniotti S.",
        "publication": "Scientific Reports",
        "citied_by": "4",
        "cover_date": "2023-12-01",
        "Abstract": "The science of cities is a relatively new and interdisciplinary topic aimed at studying and characterizing the collective processes that shape the growth and dynamics of urban populations. Amongst other open problems, the forecast of mobility trends in urban spaces is a lively research topic that aims at assisting the design and implementation of efficient transportation policies and inclusive urban planning. To this end, many Machine-Learning models have been put forward to predict mobility patterns. However, most of them are not interpretable -as they build on complex hidden representations of the system configurations- or do not allow for model inspection, thus limiting our understanding of the underlying mechanisms driving the citizen’s daily routines. Here, we tackle this problem by building a fully interpretable statistical model that, incorporating only the minimum number of constraints, can predict different phenomena arising in the city. Using data on the movements of car-sharing vehicles in several Italian cities, we infer a model using the Maximum Entropy (MaxEnt) principle. The model allows for an accurate spatio-temporal prediction of car-sharing vehicles’ presence in different city areas and, thanks to its simple yet general formulation, to precisely perform anomaly detection (e.g., detect strikes and bad weather conditions from car-sharing data only). We compare the forecasting capabilities of our model with different state-of-the-art models explicitly made for time-series forecasting: SARIMA models and Deep Learning Models. We find that MaxEnt models are highly predictive, outperforming SARIMAs while having similar performances of deep Neural Networks - but with advantages of being more interpretable, more flexibile—i.e., they can be applied to different tasks- and being computationally efficient. Our results show that statistical inference might play a fundamental role in building robust and general models describing urban systems phenomena.",
        "DOI": "10.1038/s41598-023-30134-9",
        "affiliation_name": "Complexity Science Hub Vienna",
        "affiliation_city": "Vienna",
        "affiliation_country": "Austria"
    },
    {
        "paper_title": "Evolution of the digital operating room: the place of video technology in surgery",
        "paper_author": "Cheikh Youssef S.",
        "publication": "Langenbeck's Archives of Surgery",
        "citied_by": "21",
        "cover_date": "2023-12-01",
        "Abstract": "Purpose: The aim of this review was to collate current evidence wherein digitalisation, through the incorporation of video technology and artificial intelligence (AI), is being applied to the practice of surgery. Applications are vast, and the literature investigating the utility of surgical video and its synergy with AI has steadily increased over the last 2 decades. This type of technology is widespread in other industries, such as autonomy in transportation and manufacturing. Methods: Articles were identified primarily using the PubMed and MEDLINE databases. The MeSH terms used were “surgical education”, “surgical video”, “video labelling”, “surgery”, “surgical workflow”, “telementoring”, “telemedicine”, “machine learning”, “deep learning” and “operating room”. Given the breadth of the subject and the scarcity of high-level data in certain areas, a narrative synthesis was selected over a meta-analysis or systematic review to allow for a focussed discussion of the topic. Results: Three main themes were identified and analysed throughout this review, (1) the multifaceted utility of surgical video recording, (2) teleconferencing/telemedicine and (3) artificial intelligence in the operating room. Conclusions: Evidence suggests the routine collection of intraoperative data will be beneficial in the advancement of surgery, by driving standardised, evidence-based surgical care and personalised training of future surgeons. However, many barriers stand in the way of widespread implementation, necessitating close collaboration between surgeons, data scientists, medicolegal personnel and hospital policy makers.",
        "DOI": "10.1007/s00423-023-02830-7",
        "affiliation_name": "King's Health Partners",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "What dictates income in New York City? SHAP analysis of income estimation based on Socio-economic and Spatial Information Gaussian Processes (SSIG)",
        "paper_author": "Bai R.",
        "publication": "Humanities and Social Sciences Communications",
        "citied_by": "7",
        "cover_date": "2023-12-01",
        "Abstract": "Income inequality presents a key challenge to urban sustainability across the developed economies. Traditionally, accurate high granularity income data are generally obtained from field surveys. However, due to privacy considerations, field subjects are hesitant to provide accurate personal income data. A Socio-economic & Spatial-Information-GP (SSIG) model is thereby developed to estimate district-based high granularity income for New York City (NYC). As compared to the state-of-the-art Gaussian Processes (GP) income estimation model based entirely on spatial information, SSIG incorporates socio-economic domain-specific knowledge into a GP model. For SSIG to be explainable, SHapley Additive exPlanations (SHAP) analysis is undertaken to evaluate the relative contribution of various key individual socio-economic variables to district-based per-capita and median household income in NYC. Differentiating from traditional income inequality studies based predominantly on linear or log-linear regression model, SSIG presents a novel income-based model architecture, capable of modelling complex non-linear relationships. In parallel, SHAP analysis serves an effective analytical tool for identifying the key attributes to income inequality. Results have shown that SSIG surpasses other state-of-the-art baselines in estimation accuracy, as far as per-capita and median household income estimation at the Tract-level and the ZIP-level in NYC are concerned. SHAP results have indicated that having a bachelor or a postgraduate degree can accurately predict income in NYC, despite that between-district income inequality due to Sex/Race remains prevalent. SHAP has further confirmed that between-district income gap is more associated with Race than Sex. Furthermore, ablation study shows that socio-economic information is more predictive of income at the ZIP-level, relative to the spatial information. This study carries significant implications for policy-making in a developed context. To promote urban economic sustainability in NYC, policymakers can attend to the growing income disparity (income inequality) contributed by Sex and Race, while giving more higher education opportunities to residents in the lower-income districts, as the estimated per-capita income is more sensitive to the proportion of adults ≥25 holding a bachelor’s degree. Finally, interpretative SHAP analysis is useful for investigating the relative contribution of socio-economic inputs to any predicted outputs in future machine-learning-driven socio-economic analyses.",
        "DOI": "10.1057/s41599-023-01548-7",
        "affiliation_name": "The University of Hong Kong",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong"
    },
    {
        "paper_title": "Bridging the research-practice gap in supply chain risks induced by the COVID-19",
        "paper_author": "Wang Q.",
        "publication": "Benchmarking",
        "citied_by": "4",
        "cover_date": "2023-12-01",
        "Abstract": "Purpose: This study aims to explore the gap between research and practice on supply chain risks due to COVID-19 by exploring the changes in global emphasis on supply chain risk research. Design/methodology/approach: This work designed a research framework to compare the research of supply chain risks before and during the COVID-19 pandemic based on machining learning and text clustering and using the relevant publications of the web of science database. Findings: The results show that scholars' attention to supply chain crisis has increased in the wake of the COVID-19 outbreak, but there are differences among countries. The United Kingdom, India, Australia, the USA and Italy have greatly increased their emphasis on risk research, while the supply chain risk research growth rate in other countries, including China, has been lower than the global level. Compared with the pre-pandemic period, the research of business finance, telecommunications, agricultural economics policy, business and public environmental occupational health increased significantly during the pandemic. The hotspots of supply chain risk research have changed significantly during the pandemic, focusing on routing problem, organizational performance, food supply chain, dual-channel supply chain, resilient supplier selection, medical service and machine learning. Research limitations/implications: This study has limitations in using a single database. Social implications: This work compared the changes in global and various countries' supply chain risk research before and during the pandemic. On the one hand, it helps to judge the degree of response of scholars to the global supply chain risk brought about by COVID-19. On the other hand, it is beneficial for supply chain practitioners and policymakers to gain an in-depth understanding of the relationship between the COVID-19 pandemic and supply chain risk, which might provide insights into not only addressing the supply chain risk but also the recovery of the supply chain. Originality/value: The initial exploration of the changing extent of supply chain risk research in the context of COVID-19 provided in this paper is a unique and earlier attempt that extends the findings of the existing literature. Secondly, this research provides a feasible analysis strategy for supply chain risk research, which provides a direction and paradigm for exploring more effective supply chain research to meet the challenges of COVID-19.",
        "DOI": "10.1108/BIJ-02-2022-0111",
        "affiliation_name": "China University of Petroleum (East China)",
        "affiliation_city": "Qingdao",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A multi-granular stacked regression for forecasting long-term demand in Emergency Departments",
        "paper_author": "James C.",
        "publication": "BMC Medical Informatics and Decision Making",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "Background: In the United Kingdom, Emergency Departments (EDs) are under significant pressure due to an ever-increasing number of attendances. Understanding how the capacity of other urgent care services and the health of a population may influence ED attendances is imperative for commissioners and policy makers to develop long-term strategies for reducing this pressure and improving quality and safety. Methods: We developed a novel multi-granular stacked regression (MGSR) model using publicly available data to predict future mean monthly ED attendances within Clinical Commissioning Group regions in England. The MGSR combines measures of population health and health service capacity in other related settings. We assessed model performance using the R-squared statistic, measuring variance explained, and the Mean Absolute Percentage Error (MAPE), measuring forecasting accuracy. We used the MGSR to forecast ED demand over a 4-year period under hypothetical scenarios where service capacity is increased, or population health is improved. Results: Measures of service capacity explain 41 ± 4% of the variance in monthly ED attendances and measures of population health explain 62 ± 22%. The MGSR leads to an overall improvement in performance, with an R-squared of 0.79 ± 0.02 and MAPE of 3% when forecasting mean monthly ED attendances per CCG. Using the MGSR to forecast long-term demand under different scenarios, we found improving population health would reduce peak ED attendances per CCG by approximately 1000 per month after 2 years. Conclusion: Combining models of population health and wider urgent care service capacity for predicting monthly ED attendances leads to an improved performance compared to each model individually. Policies designed to improve population health will reduce ED attendances and enhance quality and safety in the long-term.",
        "DOI": "10.1186/s12911-023-02109-3",
        "affiliation_name": "University Hospitals Bristol and Weston NHS Foundation Trust",
        "affiliation_city": "Bristol",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "A value-based deep reinforcement learning model with human expertise in optimal treatment of sepsis",
        "paper_author": "Wu X.D.",
        "publication": "npj Digital Medicine",
        "citied_by": "28",
        "cover_date": "2023-12-01",
        "Abstract": "Deep Reinforcement Learning (DRL) has been increasingly attempted in assisting clinicians for real-time treatment of sepsis. While a value function quantifies the performance of policies in such decision-making processes, most value-based DRL algorithms cannot evaluate the target value function precisely and are not as safe as clinical experts. In this study, we propose a Weighted Dueling Double Deep Q-Network with embedded human Expertise (WD3QNE). A target Q value function with adaptive dynamic weight is designed to improve the estimate accuracy and human expertise in decision-making is leveraged. In addition, the random forest algorithm is employed for feature selection to improve model interpretability. We test our algorithm against state-of-the-art value function methods in terms of expected return, survival rate, action distribution and external validation. The results demonstrate that WD3QNE obtains the highest survival rate of 97.81% in MIMIC-III dataset. Our proposed method is capable of providing reliable treatment decisions with embedded clinician expertise.",
        "DOI": "10.1038/s41746-023-00755-5",
        "affiliation_name": "The Thomas J. Watson College of Engineering and Applied Science",
        "affiliation_city": "Binghamton",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Safe reinforcement learning under temporal logic with reward design and quantum action selection",
        "paper_author": "Cai M.",
        "publication": "Scientific Reports",
        "citied_by": "9",
        "cover_date": "2023-12-01",
        "Abstract": "This paper proposes an advanced Reinforcement Learning (RL) method, incorporating reward-shaping, safety value functions, and a quantum action selection algorithm. The method is model-free and can synthesize a finite policy that maximizes the probability of satisfying a complex task. Although RL is a promising approach, it suffers from unsafe traps and sparse rewards and becomes impractical when applied to real-world problems. To improve safety during training, we introduce a concept of safety values, which results in a model-based adaptive scenario due to online updates of transition probabilities. On the other hand, a high-level complex task is usually formulated via formal languages, including Linear Temporal Logic (LTL). Another novelty of this work is using an Embedded Limit-Deterministic Generalized Büchi Automaton (E-LDGBA) to represent an LTL formula. The obtained deterministic policy can generalize the tasks over infinite and finite horizons. We design an automaton-based reward, and the theoretical analysis shows that an agent can accomplish task specifications with the maximum probability by following the optimal policy. Furthermore, a reward shaping process is developed to avoid sparse rewards and enforce the RL convergence while keeping the optimal policies invariant. In addition, inspired by quantum computing, we propose a quantum action selection algorithm to replace the existing ε-greedy algorithm for the balance of exploration and exploitation during learning. Simulations demonstrate how the proposed framework can achieve good performance by dramatically reducing the times to visit unsafe states while converging optimal policies.",
        "DOI": "10.1038/s41598-023-28582-4",
        "affiliation_name": "P.C. Rossin College of Engineering &amp; Applied Science",
        "affiliation_city": "Bethlehem",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "PISA reading achievement: identifying predictors and examining model generalizability for multilingual students",
        "paper_author": "Dai S.",
        "publication": "Reading and Writing",
        "citied_by": "6",
        "cover_date": "2023-12-01",
        "Abstract": "Reading research in the United States has mainly focused on early or, less frequently, middle grades and on monolingual (MN or English-only) rather than on multilingual (ML) students. To address these gaps, we focused on factors contributing to high school ML students’ reading achievement. In particular, we first used machine learning to identify predictors of high school students’ reading achievement on PISA 2018. We then conducted multilevel modeling on the entire sample (baseline model) and tested the model’s generalizability to ML and MN populations. Results suggest that ML students would benefit from instruction focused on enhancing their reading self-efficacy and increased learning opportunities for extracurricular reading activities. The results also suggest that students, especially ML students, would benefit from schools avoiding grade retention policies and focusing on minimizing truancy and supporting positive peer and teacher relationships. Limitations of the study and future directions are discussed.",
        "DOI": "10.1007/s11145-022-10357-4",
        "affiliation_name": "East China Normal University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "The role of biostatistics in the response to COVID-19: a Belgian and international perspective",
        "paper_author": "Molenberghs G.",
        "publication": "Israel Journal of Health Policy Research",
        "citied_by": "3",
        "cover_date": "2023-12-01",
        "Abstract": "In this commentary to Dattner et al. (Israel J Health Policy Res. 11:22, 2022), we highlight similarities and differences in the role that biostatistics and biostatisticians have been playing in the COVID-19 response in Belgium and Israel. We bring out implications and opportunities for our field and for science. We argue that biostatistics has an important place in the multidisciplinary COVID-19 response, in terms of research, policy advice, and science and public communication. In Belgium, biostatisticians located in various institutes, collaborated with epidemiologists, vaccinologists, infectiologists, immunologists, social scientists, and government policy makers to provide rapid and science-informed policy advice. Biostatisticians, who can easily be mobilized to work together in pandemic response, also played a role in public communication.",
        "DOI": "10.1186/s13584-023-00554-z",
        "affiliation_name": "Universiteit Hasselt",
        "affiliation_city": "Hasselt",
        "affiliation_country": "Belgium"
    },
    {
        "paper_title": "Product progression: a machine learning approach to forecasting industrial upgrading",
        "paper_author": "Albora G.",
        "publication": "Scientific Reports",
        "citied_by": "9",
        "cover_date": "2023-12-01",
        "Abstract": "Economic complexity methods, and in particular relatedness measures, lack a systematic evaluation and comparison framework. We argue that out-of-sample forecast exercises should play this role, and we compare various machine learning models to set the prediction benchmark. We find that the key object to forecast is the activation of new products, and that tree-based algorithms clearly outperform both the quite strong auto-correlation benchmark and the other supervised algorithms. Interestingly, we find that the best results are obtained in a cross-validation setting, when data about the predicted country was excluded from the training set. Our approach has direct policy implications, providing a quantitative and scientifically tested measure of the feasibility of introducing a new product in a given country.",
        "DOI": "10.1038/s41598-023-28179-x",
        "affiliation_name": "European Commission Joint Research Centre",
        "affiliation_city": "Brussels",
        "affiliation_country": "Belgium"
    },
    {
        "paper_title": "A deep reinforcement learning-based wireless body area network offloading optimization strategy for healthcare services",
        "paper_author": "Chen Y.",
        "publication": "Health Information Science and Systems",
        "citied_by": "19",
        "cover_date": "2023-12-01",
        "Abstract": "Wireless body area network (WBAN) is widely adopted in healthcare services, providing remote real-time and continuous healthcare monitoring. With the massive increase of detective sensor data, WBAN is largely restricted by limited storage and computation capacity, resulting in severely decreased efficiency and reliability. Mobile edge computing (MEC) technique can be combined with WBAN to resolve this issue. This paper studies the joint optimization problem of computational offloading and resource allocation (JCORA) in MEC for healthcare service scenarios. We formulate JCORA as a Markov decision process and propose a deep deterministic policy gradient-based WBAN offloading strategy (DDPG-WOS) to optimize time delay and energy consumption in interfered transmission channels. This scheme employs MEC to mitigate the computation pressure on a single WBAN and increase the transmission ability. Further, DDPG-WOS optimizes the offloading strategy-making process by considering the channel condition, transmission quality, computation ability and energy consumption. Simulation results verify the effectiveness of the proposed optimization schema in reducing energy consumption and computation latency and increasing the utility of WBAN compared to two competitive solutions.",
        "DOI": "10.1007/s13755-023-00212-3",
        "affiliation_name": "Victoria University Melbourne, Institute of Sustainable Industries and Liveable Cities",
        "affiliation_city": "Melbourne",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Cost and cost-effectiveness of four different SARS-CoV-2 active surveillance strategies: evidence from a randomised control trial in Germany",
        "paper_author": "Nguyen H.T.",
        "publication": "European Journal of Health Economics",
        "citied_by": "4",
        "cover_date": "2023-12-01",
        "Abstract": "Introduction: The COVID-19 pandemic has entered its third year and continues to affect most countries worldwide. Active surveillance, i.e. testing individuals irrespective of symptoms, presents a promising strategy to accurately measure the prevalence of SARS-CoV-2. We aimed to identify the most cost-effective active surveillance strategy for COVID-19 among the four strategies tested in a randomised control trial between 18th November 2020 and 23rd December 2020 in Germany. The four strategies included: (A1) direct testing of individuals; (A2) direct testing of households; (B1) testing conditioned on upstream COVID-19 symptom pre-screening of individuals; and (B2) testing conditioned on upstream COVID-19 symptom pre-screening of households. Methods: We adopted a health system perspective and followed an activity-based approach to costing. Resource consumption data were collected prospectively from a digital individual database, daily time records, key informant interviews and direct observations. Our cost-effectiveness analysis compared each strategy with the status quo and calculated the average cost-effective ratios (ACERs) for one primary outcome (sample tested) and three secondary outcomes (responder recruited, case detected and asymptomatic case detected). Results: Our results showed that A2, with cost per sample tested at 52,89 EURO, had the lowest ACER for the primary outcome, closely followed by A1 (63,33 EURO). This estimate was much higher for both B1 (243,84 EURO) and B2 (181,06 EURO). Conclusion: A2 (direct testing at household level) proved to be the most cost-effective of the four evaluated strategies and should be considered as an option to strengthen the routine surveillance system in Germany and similar settings.",
        "DOI": "10.1007/s10198-022-01561-8",
        "affiliation_name": "German Cancer Research Center",
        "affiliation_city": "Heidelberg",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Digital medicine: the next big leap advancing cardiovascular science",
        "paper_author": "Kharlamov A.",
        "publication": "BMC Cardiovascular Disorders",
        "citied_by": "1",
        "cover_date": "2023-12-01",
        "Abstract": "Solid clinical and academic leadership remains necessary to ensure that healthcare based on digital technologies is relevant, meaningful, and stands on the best possible evidence. This compendium accompanying the “Digital Technologies in Cardiovascular Disorders” article collection in BMC Cardiovascular Disorders summarizes recent knowledge about robust and advanced digital tools for preventing, monitoring, diagnosing, and treating cardiovascular diseases.",
        "DOI": "10.1186/s12872-022-02971-5",
        "affiliation_name": "Københavns Universitet",
        "affiliation_city": "Copenhagen",
        "affiliation_country": "Denmark"
    },
    {
        "paper_title": "The Red Hen Anonymizer and the Red Hen Protocol for de-identifying audiovisual recordings",
        "paper_author": "Khasbage Y.",
        "publication": "Linguistics Vanguard",
        "citied_by": "0",
        "cover_date": "2023-12-01",
        "Abstract": "Scientists of multimodal communication have no established policy or default tool for sharing de-identified audiovisual recordings. Recently, new technology has been developed that enables researchers to de-identify voice and appearance. These software tools can produce output in JSON format that specifies bodypose and face and hand keypoints in numerical form, suitable for computer search, machine learning, and sharing. The Red Hen Anonymizer is a new tool for de-identification. This article presents the Red Hen Anonymizer and discusses guidelines for its use.",
        "DOI": "10.1515/lingvan-2022-0017",
        "affiliation_name": "University of Jyväskylä",
        "affiliation_city": "Jyvaskyla",
        "affiliation_country": "Finland"
    },
    {
        "paper_title": "A statistical analysis of tweets on covid-19 vaccine hesitancy utilizing opinion mining: an Indian perspective",
        "paper_author": "Verma R.",
        "publication": "Social Network Analysis and Mining",
        "citied_by": "11",
        "cover_date": "2023-12-01",
        "Abstract": "The world witnessed the emergence of a deadly virus in December 2019, later named COVID-19. The virus was found to be highly contagious, and so people across the world were highly prone to be affected by the virus. Being a virus-borne disease, developing a vaccine was one of the most promising remedies. Thus, research organizations across the globe started working on developing the vaccine. However, it was later found by many researchers that a large number of people were hesitant to receive the vaccine. This paper aims to study the acceptance and hesitancy levels of people in India and compares them with the acceptance and hesitancy levels of people from the UK, the USA, and the rest of the world by analyzing their tweets on Twitter. For this study, 2,98,452 tweets were fetched from January 2020 to March 2022 from Twitter, and 1,84,720 tweets from 1,22,960 unique users were selected based on their country of origin. Machine learning based Sentiment analysis is then used to evaluate and analyze the tweets. The paper also proposes an NLP-based algorithm to perform opinion mining on Twitter data. The study found the public sentiment of the Indian population to be 63% positive, 28% neutral, and 9% negative. While the worldwide sentiment distribution is 45% positive, 34% neutral, and 21% negative, the USA has 42% positive, 34% neutral, and 23% negative and the UK has 50% positive, 29% neutral, and 21% negative. Also, sentiment analysis for individual vaccines in Indian context resulted in “Covaxin” with the highest positive sentiment at 43% followed by “Covishield” at 36%. The outcome of this work yields an insight into the public perception of the COVID-19 vaccine and thus can be used to formulate policies for existing and future vaccine campaigns. This study becomes more relevant as it is the consolidated opinion of Indian people, which is versatile in nature.",
        "DOI": "10.1007/s13278-022-01015-2",
        "affiliation_name": "Chandigarh College of Engineering and Technology",
        "affiliation_city": "Chandigarh",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Employee attrition prediction in a pharmaceutical company using both machine learning approach and qualitative data",
        "paper_author": "Mozaffari F.",
        "publication": "Benchmarking",
        "citied_by": "8",
        "cover_date": "2023-12-01",
        "Abstract": "Purpose: This research intends to develop a model for predicting employees at a high-risk attrition and identify the most important factors affecting them. Design/methodology/approach: In this study, using the triangulation technique of a mixed research method, the employee attrition problem is investigated by identifying its affecting factors. For that matter, data related to the human resources department of a pharmaceutical company in Iran are used. And to achieve the intended goal, advanced data mining algorithms and interviews with human resource managers are applied. Findings: A model for predicting employees at a high-risk attrition is presented based on the gradient boosting machine algorithm with 89% accuracy. The use of the mixed research approach shows that qualitative and quantitative methods can be more effective in identifying the factors affecting employee churn or loss of staff. The results also contain a new situation arising out of the COVID-19 pandemic and remote working scenarios having impact on employee attrition. Finally, human resource policies are presented based on variables related to each of the identified factors. Originality/value: The novel contributions of this study include real data related to a leading pharmaceutical company as well as a combination of two quantitative and qualitative methods. The hybrid approach can identify the reasons for attrition and, consequently, retention policies to benefit from the advantage of both approaches. Data mining can be useful to identify the factors, which are usually not mentioned in termination interviews, such as direct managers. On the other hand, the results obtained from termination interviews can also include features that the authors cannot identify through data mining, which are specifically related to the characteristics of the pharmaceutical industry such as building a more professional career path. From a practical perspective, since this company specializes in pharmaceutical marketing in a new way and is primarily comprised graduates, it is important to note that the churn of specialized people disperses organizational and technological know-how. On the other hand, the pharmacist community in Iran is small, and their attrition might adversely affect not only the reputation of an organization but the employer's brand as well. So, this research would help other similar firms in retaining their valuable human capital.",
        "DOI": "10.1108/BIJ-11-2021-0664",
        "affiliation_name": "Allameh Tabataba'i University",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "A farmer-centric agricultural decision support system for market dynamics in a volatile agricultural supply chain",
        "paper_author": "Tripathi P.K.",
        "publication": "Benchmarking",
        "citied_by": "7",
        "cover_date": "2023-12-01",
        "Abstract": "Purpose: In a volatile agricultural postharvest market, producers require more personalized information about market dynamics for informed decisions on the marketed surplus. However, this adaptive strategy fails to benefit them if the selection of a computational price predictive model to disseminate information on the market outlook is not efficient, and the associated risk of perishability, and storage cost factor are not assumed against the seemingly favourable market behaviour. Consequently, the decision of whether to store or sell at the time of crop harvest is a perennial dilemma to solve. With the intent of addressing this challenge for agricultural producers, the study is focused on designing an agricultural decision support system (ADSS) to suggest a favourable marketing strategy to crop producers. Design/methodology/approach: The present study is guided by an eclectic theoretical perspective from supply chain literature that included agency theory, transaction cost theory, organizational information processing theory and opportunity cost theory in revenue risk management. The paper models a structured iterative algorithmic framework that leverages the forecasting capacity of different time series and machine learning models, considering the effect of influencing factors on agricultural price movement for better forecasting predictability against market variability or dynamics. It also attempts to formulate an integrated risk management framework for effective sales planning decisions that factors in the associated costs of storage, rental and physical loss until the surplus is held for expected returns. Findings: Empirical demonstration of the model was simulated on the dynamic markets of tomatoes, onions and potatoes in a north Indian region. The study results endorse that farmer-centric post-harvest information intelligence assists crop producers in the strategic sales planning of their produce, and also vigorously promotes that the effectiveness of decision making is contingent upon the selection of the best predictive model for every future market event. Practical implications: As a policy implication, the proposed ADSS addresses the pressing need for a robust marketing support system for the socio-economic welfare of farming communities grappling with distress sales, and low remunerative returns. Originality/value: Based on the extant literature studied, there is no such study that pays personalized attention to agricultural producers, enabling them to make a profitable sales decision against the volatile post-harvest market scenario. The present research is an attempt to fill that gap with the scope of addressing crop producer's ubiquitous dilemma of whether to sell or store at the time of harvesting. Besides, an eclectic and iterative style of predictive modelling has also a limited implication in the agricultural supply chain based on the literature; however, it is found to be a more efficient practice to function in a dynamic market outlook.",
        "DOI": "10.1108/BIJ-12-2021-0780",
        "affiliation_name": "Institute of Management Studies",
        "affiliation_city": "Varanasi",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Online shielding for reinforcement learning",
        "paper_author": "Könighofer B.",
        "publication": "Innovations in Systems and Software Engineering",
        "citied_by": "8",
        "cover_date": "2023-12-01",
        "Abstract": "Besides the recent impressive results on reinforcement learning (RL), safety is still one of the major research challenges in RL. RL is a machine-learning approach to determine near-optimal policies in Markov decision processes (MDPs). In this paper, we consider the setting where the safety-relevant fragment of the MDP together with a temporal logic safety specification is given, and many safety violations can be avoided by planning ahead a short time into the future. We propose an approach for online safety shielding of RL agents. During runtime, the shield analyses the safety of each available action. For any action, the shield computes the maximal probability to not violate the safety specification within the next k steps when executing this action. Based on this probability and a given threshold, the shield decides whether to block an action from the agent. Existing offline shielding approaches compute exhaustively the safety of all state-action combinations ahead of time, resulting in huge computation times and large memory consumption. The intuition behind online shielding is to compute at runtime the set of all states that could be reached in the near future. For each of these states, the safety of all available actions is analysed and used for shielding as soon as one of the considered states is reached. Our approach is well-suited for high-level planning problems where the time between decisions can be used for safety computations and it is sustainable for the agent to wait until these computations are finished. For our evaluation, we selected a 2-player version of the classical computer game Snake. The game represents a high-level planning problem that requires fast decisions and the multiplayer setting induces a large state space, which is computationally expensive to analyse exhaustively.",
        "DOI": "10.1007/s11334-022-00480-4",
        "affiliation_name": "Graz University of Technology, Institute of Software Technology",
        "affiliation_city": "Graz",
        "affiliation_country": "Austria"
    },
    {
        "paper_title": "Dynamic User-Scheduling and Power Allocation for SWIPT Aided Federated Learning: A Deep Learning Approach",
        "paper_author": "Li Y.",
        "publication": "IEEE Transactions on Mobile Computing",
        "citied_by": "15",
        "cover_date": "2023-12-01",
        "Abstract": "Federated learning (FL) has been considered as a promising paradigm for enabling distributed machine learning (ML) in wireless networks. To address the limited energy capacity of wireless devices, we propose a simultaneous wireless information and power transfer (SWIPT) aided FL, in which one FL server (FLS) co-located at a cellular base station (BS) uses SWIPT to simultaneously broadcast the global model to wireless user-devices (UDs) and provide wireless power transfer to them. The UDs then use the harvested energy to train their local models and further transmit the local models to the FLS for aggregation. To improve the spectrum efficiency, we consider that the UDs form a non-orthogonal multiple access (NOMA) group for simultaneously sending their local models over the same spectrum channel. Taking the UDs' time-varying available energy and channel conditions into account, we propose a dynamic optimization of the UDs-scheduling, the BS's transmit-power allocation, and the UDs' power-splitting factors for SWIPT, with the objective of minimizing the long-term energy consumption while ensuring the FL convergence. The optimization problem, however, is challenging to solve since it is a finite-horizon dynamic programming problem but with an unknown stopping time, and moreover, the action space covers both discrete and continuous variables. To address these difficulties, we first execute a series of equivalent transformations to reduce the number of decision variables and then formulate the problem as a stochastic shortest path problem, based on which we propose an actor-critic deep reinforcement learning algorithm with the proximal policy optimization to efficiently learn the policy that dynamically adjusts the UDs-scheduling for FL as well as the BS's transmit-power for SWIPT. Numerical results validate the effectiveness and performance of our proposed algorithm. The results demonstrate that our proposed algorithm can effectively reduce the long-term energy consumption in comparison with two baseline algorithms.",
        "DOI": "10.1109/TMC.2022.3201622",
        "affiliation_name": "State Key Laboratory of Internet of Things for Smart City",
        "affiliation_city": "Taipa",
        "affiliation_country": "Macao"
    },
    {
        "paper_title": "Solving the Zero-Sum Control Problem for Tidal Turbine System: An Online Reinforcement Learning Approach",
        "paper_author": "Fang H.",
        "publication": "IEEE Transactions on Cybernetics",
        "citied_by": "23",
        "cover_date": "2023-12-01",
        "Abstract": "A novel completely mode-free integral reinforcement learning (CMFIRL)-based iteration algorithm is proposed in this article to compute the two-player zero-sum games and the Nash equilibrium problems, that is, the optimal control policy pairs, for tidal turbine system based on continuous-time Markov jump linear model with exact transition probability and completely unknown dynamics. First, the tidal turbine system is modeled into Markov jump linear systems, followed by a designed subsystem transformation technique to decouple the jumping modes. Then, a completely mode-free reinforcement learning algorithm is employed to address the game-coupled algebraic Riccati equations without using the information of the system dynamics, in order to reach the Nash equilibrium. The learning algorithm includes one iteration loop by updating the control policy and the disturbance policy simultaneously. Also, the exploration signal is added for motivating the system, and the convergence of the CMFIRL iteration algorithm is rigorously proved. Finally, a simulation example is given to illustrate the effectiveness and applicability of the control design approach.",
        "DOI": "10.1109/TCYB.2022.3186886",
        "affiliation_name": "Jiangnan University",
        "affiliation_city": "Wuxi",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Exploiting Propagation Delay in Underwater Acoustic Communication Networks via Deep Reinforcement Learning",
        "paper_author": "Geng X.",
        "publication": "IEEE Transactions on Neural Networks and Learning Systems",
        "citied_by": "21",
        "cover_date": "2023-12-01",
        "Abstract": "— This article proposes a novel deep-reinforcement learning-based medium access control (DL-MAC) protocol for underwater acoustic networks (UANs) where one agent node employing the proposed DL-MAC protocol coexists with other nodes employing traditional protocols, such as time division multiple access (TDMA) or q-Aloha. The DL-MAC agent learns to exploit the large propagation delays inherent in underwater acoustic communications to improve system throughput by either a synchronous or an asynchronous transmission mode. In the sync-DL-MAC protocol, the agent action space is transmission or no transmission, while in the async-DL-MAC, the agent can also vary the start time in each transmission time slot to further exploit the spatiotemporal uncertainty of the UANs. The deep Q-learning algorithm is applied to both sync-DL-MAC and async-DL-MAC agents to learn the optimal policies. A theoretical analysis and computer simulations demonstrate the performance gain obtained by both DL-MAC protocols. The async-DL-MAC protocol outperforms the sync-DL-MAC protocol significantly in sum throughput and packet success rate by adjusting the transmission start time and reducing the length of time slot.",
        "DOI": "10.1109/TNNLS.2022.3170050",
        "affiliation_name": "P.C. Rossin College of Engineering &amp; Applied Science",
        "affiliation_city": "Bethlehem",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "ARTMAN 2023 - Proceedings of the 2023 Workshop on Recent Advances in Resilient and Trustworthy ML Systems in Autonomous Networks",
        "paper_author": "NA",
        "publication": "ARTMAN 2023 - Proceedings of the 2023 Workshop on Recent Advances in Resilient and Trustworthy ML Systems in Autonomous Networks",
        "citied_by": "0",
        "cover_date": "2023-11-30",
        "Abstract": "The proceedings contain 5 papers. The topics discussed include: how resilient is privacy-preserving machine learning towards data-driven policy? Jakarta COVID-19 patient study case; model selection for continuous operation of automated vulnerability assessment system; breaking boundaries: balancing performance and robustness in deep wireless traffic forecasting; towards robust misbehavior detection in power control systems: a gradient quantization approach; and hybrid explainable intrusion detection system: global vs. local approach.",
        "DOI": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "How Resilient is Privacy-preserving Machine Learning Towards Data-Driven Policy? Jakarta COVID-19 Patient Study Case",
        "paper_author": "Nasution B.I.",
        "publication": "ARTMAN 2023 - Proceedings of the 2023 Workshop on Recent Advances in Resilient and Trustworthy ML Systems in Autonomous Networks",
        "citied_by": "0",
        "cover_date": "2023-11-30",
        "Abstract": "With the rise of personal data law in various countries, data privacy has recently become an essential issue. One of the well-known techniques used in overcoming privacy issues during analysis is differential privacy. However, many studies have shown that differential privacy decreased the machine learning model performance. It becomes problematic for any organization like the government to draw a policy from accurate insights from citizen statistics while maintaining citizen privacy. This study reviews differential privacy in machine learning algorithms and evaluates its performance on real COVID-19 patient data, using Jakarta, Indonesia as a case study. Besides that, we also validate our study with two additional datasets, the public Adult dataset from University of California, Irvine, and an Indonesia socioeconomic dataset. We find that using differential privacy tends to reduce accuracy and may lead to model failure in imbalanced data, particularly in more complex models such as random forests. The finding emphasizes differential privacy usage in government is practical for the trustworthy government but with distinct challenges. We discuss limitations and recommendations for any organization that works with personal data to leverage differential privacy in the future.",
        "DOI": "10.1145/3605772.3624003",
        "affiliation_name": "Telkom University",
        "affiliation_city": "Bandung",
        "affiliation_country": "Indonesia"
    },
    {
        "paper_title": "ERMN: An enhanced meta-learning approach for state of health estimation of lithium-ion batteries",
        "paper_author": "Ma G.",
        "publication": "Journal of Energy Storage",
        "citied_by": "4",
        "cover_date": "2023-11-30",
        "Abstract": "To ensure the safety of lithium-ion batteries (LIBs), accurately estimating the state of health (SOH) of LIBs is crucial for end-users. Challenged by new working conditions and limited training samples, conventional machine learning-based methods have performed unsatisfactorily on SOH estimation of LIBs. To this end, this article introduces an enhanced regressive matching network (ERMN) method for the accurate SOH estimation of LIBs. The method extends the matching network from classification tasks to regression tasks by filtering a certain number of support samples, which can therefore be used for the SOH estimation—a typical regression task. A new loss function is designed to further enhance the distribution of the filtered support SOHs, ensuring their distinguishability and concentration. Additionally, the particle swarm optimization algorithm is incorporated into the ERMN method to automate the selection of hyperparameters. Experimental results from four case studies, considering different charge policies and sample sizes, demonstrate the effectiveness and generalization of the proposed ERMN approach.",
        "DOI": "10.1016/j.est.2023.108628",
        "affiliation_name": "Huazhong University of Science and Technology",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Reinforcement learning algorithms: A brief survey",
        "paper_author": "Shakya A.K.",
        "publication": "Expert Systems with Applications",
        "citied_by": "142",
        "cover_date": "2023-11-30",
        "Abstract": "Reinforcement Learning (RL) is a machine learning (ML) technique to learn sequential decision-making in complex problems. RL is inspired by trial-and-error based human/animal learning. It can learn an optimal policy autonomously with knowledge obtained by continuous interaction with a stochastic dynamical environment. Problems considered virtually impossible to solve, such as learning to play video games just from pixel information, are now successfully solved using deep reinforcement learning. Without human intervention, RL agents can surpass human performance in challenging tasks. This review gives a broad overview of RL, covering its fundamental principles, essential methods, and illustrative applications. The authors aim to develop an initial reference point for researchers commencing their research work in RL. In this review, the authors cover some fundamental model-free RL algorithms and pathbreaking function approximation-based deep RL (DRL) algorithms for complex uncertain tasks with continuous action and state spaces, making RL useful in various interdisciplinary fields. This article also provides a brief review of model-based and multi-agent RL approaches. Finally, some promising research directions for RL are briefly presented.",
        "DOI": "10.1016/j.eswa.2023.120495",
        "affiliation_name": "Indian Institute of Technology Roorkee",
        "affiliation_city": "Roorkee",
        "affiliation_country": "India"
    },
    {
        "paper_title": "WILDetect: An intelligent platform to perform airborne wildlife census automatically in the marine ecosystem using an ensemble of learning techniques and computer vision",
        "paper_author": "Kuru K.",
        "publication": "Expert Systems with Applications",
        "citied_by": "10",
        "cover_date": "2023-11-30",
        "Abstract": "The habitats of marine life, characteristics of species, and the diverse mix of maritime industries around these habitats are of interest to many researchers, authorities, and policymakers whose aim is to conserve the earth's biological diversity in an ecologically sustainable manner while being in line with indispensable industrial developments. Automated detection, locating, and monitoring of marine life along with the industry around the habitats of this ecosystem may be helpful to (i) reveal current impacts, (ii) model future possible ecological trends, and (iii) determine required policies which would lead accordingly to a reduced ecological footprint and increased sustainability. New automatic techniques are required to observe this large environment efficiently. Within this context, this study aims to develop a novel platform to monitor marine ecosystems and perform bio census in an automated manner, particularly for birds in regional aerial surveys since birds are a good indicator of overall ecological health. In this manner, a new non-parametric approach, WILDetect, has been built using an ensemble of supervised Machine Learning (ML) and Reinforcement Learning (RL) techniques. It employs several hybrid techniques to segment, split and count maritime species – in particular, birds – in order to perform automated censuses in a highly dynamic marine ecosystem. The efficacy of the proposed approach is demonstrated by experiments performed on 26 surveys which include Northern gannets (Morus bassanus) by utilising retrospective data analysis techniques. With this platform, by combining multiple techniques, gannets can be detected and split automatically with very high sensitivity (Se) (≈ 0.97), specificity (Sp) (≈ 0.99), and accuracy (Acc) (≈ 0.99) — these values are validated by precision (Pr) (≈ 0.98). Moreover, the evaluation of the system by the APEM staff, which uses a completely new evaluation dataset gathered from recent surveys, shows the viability of the proposed techniques. The experimental results suggest that similar automated data processing techniques – tailored for specific species – can be helpful both in performing time-intensive marine wildlife censuses efficiently and in establishing ecological platforms/models to understand the underlying causes of trends in species populations along with the ecological change.",
        "DOI": "10.1016/j.eswa.2023.120574",
        "affiliation_name": "APEM Limited",
        "affiliation_city": "Stockport",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Analysing political opinions using machine learning",
        "paper_author": "Joshi P.",
        "publication": "International Journal of Public Sector Performance Management",
        "citied_by": "0",
        "cover_date": "2023-11-28",
        "Abstract": "In the era of digital world, text is not confined to textbooks or newspapers anymore. People use platforms like Twitter, Facebook, Quora, and other social media platforms to express their opinions over certain products, movies, social, economic or political causes. Huge chunks of textual data are available on these platforms for analysis. This paper tries to leverage deep learning and natural language processing (NLP) to use the publicly available text data to predict outcomes of Indian general elections by analysing the tweets with hashtags for various parties, using opinion mining to define polarity in the opinions. It tries to adopt a hybrid approach using NLP. The results from the analysis help in highlighting the potential of machine learning in predicting the election results and identifying the political inclination of people towards specific policies thus, indicating the efficiency of using social media to predict real-world outcomes.",
        "DOI": "10.1504/IJPSPM.2023.135035",
        "affiliation_name": "Simon Fraser University",
        "affiliation_city": "Burnaby",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Modeling Inverse Demand Function with Explainable Dual Neural Networks",
        "paper_author": "Cao Z.",
        "publication": "ICAIF 2023 - 4th ACM International Conference on AI in Finance",
        "citied_by": "0",
        "cover_date": "2023-11-27",
        "Abstract": "Financial contagion has been widely recognized as a fundamental risk to the financial system. Particularly potent is price-mediated contagion, wherein forced liquidations by firms depress asset prices and propagate financial stress, enabling crises to proliferate across a broad spectrum of seemingly unrelated entities. Price impacts are currently modeled via exogenous inverse demand functions. However, in real-world scenarios, only the initial shocks and the final equilibrium asset prices are typically observable, leaving actual asset liquidations largely obscured. This missing data presents significant limitations to calibrating the existing models. To address these challenges, we introduce a novel dual neural network structure that operates in two sequential stages: the first neural network maps initial shocks to predicted asset liquidations, and the second network utilizes these liquidations to derive resultant equilibrium prices. This data-driven approach can capture both linear and non-linear forms without pre-specifying an analytical structure; furthermore, it functions effectively even in the absence of observable liquidation data. Experiments with simulated datasets demonstrate that our model can accurately predict equilibrium asset prices based solely on initial shocks, while revealing a strong alignment between predicted and true liquidations. Our explainable framework contributes to the understanding and modeling of price-mediated contagion and provides valuable insights for financial authorities to construct effective stress tests and regulatory policies.",
        "DOI": "10.1145/3604237.3626887",
        "affiliation_name": "Stevens Institute of Technology",
        "affiliation_city": "Hoboken",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Causal inference preliminary",
        "paper_author": "Yao L.",
        "publication": "Machine Learning for Causal Inference",
        "citied_by": "1",
        "cover_date": "2023-11-25",
        "Abstract": "Causal inference has been a critical research topic across many domains, such as statistics, computer science, education, public policy, and economics, for decades. Nowadays, estimating causal effects from observational data has become an appealing research direction owing to the large amount of available data and low budget requirements compared with randomized controlled trials. Embraced by the rapidly developed machine learning area, various causal effect estimation methods for observational data have emerged. In this chapter, we provide a comprehensive review of causal inference methods, including the basic definitions, assumptions, and illustrative examples. This chapter is reprinted from our work Yao et al. (ACM Trans Knowl Dis Data (TKDD) 15(5):1-46, 2021).",
        "DOI": "10.1007/978-3-031-35051-1_2",
        "affiliation_name": "Ant group",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Pilot free trade zones and investment efficiency - - A study based on double machine learning and staggered DID",
        "paper_author": "Meng L.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-11-24",
        "Abstract": "Based on the exogenous shock event of the establishment of the Pilot Free Trade Zone (FTZ), this paper takes the data of China's A-share listed companies from 2012 to 2022 as a sample, and further empirically tests whether the FTZ policy can inhibit the inefficient investment of enterprises by adopting a more stringent staggered difference method (DID) on the basis of the test through the double machine learning method. The results found that the establishment of FTZ can inhibit the enterprise's inefficient investment, alleviate underinvestment and inhibit overinvestment, and this empirical result still holds after a series of robustness tests. This paper provides a basis for the FTZ policy to inhibit enterprise inefficient investment, which is of guiding significance for further improving the FTZ policy, improving enterprise investment efficiency, and promoting the high-quality development of enterprises.",
        "DOI": "10.1145/3656766.3656858",
        "affiliation_name": "Lanzhou University of Technology",
        "affiliation_city": "Lanzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Machine learning governance for managers",
        "paper_author": "Lazzeri F.",
        "publication": "Machine Learning Governance for Managers",
        "citied_by": "0",
        "cover_date": "2023-11-24",
        "Abstract": "Machine Learning Governance for Managers provides readers with the knowledge to unlock insights from data and leverage AI solutions. In today's business landscape, most organizations face challenges in scaling and maintaining a sustainable machine learning model lifecycle. This book offers a comprehensive framework that covers business requirements, data generation and acquisition, modeling, model deployment, performance measurement, and management, providing a range of methodologies, technologies, and resources to assist data science managers in adopting data and AI-driven practices. Particular emphasis is given to ramping up a solution quickly, detailing skills and techniques to ensure the right things are measured and acted upon for reliable results and high performance. Readers will learn sustainable tools for implementing machine learning with existing IT and privacy policies, including versioning all models, creating documentation, monitoring models and their results, and assessing their causal business impact. By overcoming these challenges, bottom-line gains from AI investments can be realized. Organizations that implement all aspects of AI/ML model governance can achieve a high level of control and visibility over how models perform in production, leading to improved operational efficiency and a higher ROI on AI investments. Machine Learning Governance for Managers helps to effectively control model inputs and understand all the variables that may impact your results. Don't let challenges in machine learning hinder your organization's growth - unlock its potential with this essential guide.",
        "DOI": "10.1007/9783031318054",
        "affiliation_name": "Microsoft Corporation",
        "affiliation_city": "Redmond",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Development and evaluation of Reinforcement Learning models for the FOSSBot Open-Source educational robot",
        "paper_author": "Kazazis G.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-11-24",
        "Abstract": "Machine learning algorithms play a crucial role in addressing real-world tasks by harnessing data-driven insights to optimize and enhance decision-making processes. Their application in robotics usually relies on simulations as an essential testing ground, allowing us to refine solutions, minimize risks, and fine-tune algorithms before deploying them in practical, physical environments. In this work, we evaluate the performance of two popular reinforcement learning algorithms, namely Proximal Policy Optimization (PPO) and Deep Q Network (DQN): i) on an obstacle detection and avoidance task, and ii) on a navigation task. For this purpose, we rely on the FOSSBot open-source educational robot, which contains ultrasonic sensors, infrared obstacle sensors, an Inertial Measurement Unit, and odometers. More specifically, we work on a simulation of FOSSBot in the CoppeliaSim environment. The evaluation results on the grid environment are very promising for our algorithms since the success rates (i.e. reaching the destination without collision) for both algorithms is above 93%. As for the simulated environment, the results show the superiority of PPO over DQN, with PPO scoring a success rate of 74%, making it a promising navigation algorithm candidate for the real-world FOSSBot.",
        "DOI": "10.1145/3635059.3635064",
        "affiliation_name": "Harokopio University of Athens",
        "affiliation_city": "Athens",
        "affiliation_country": "Greece"
    },
    {
        "paper_title": "A Reward Modulated Spiked Timing Depended Plasticity inspired algorithm applied on a MultiLayer Perceptron",
        "paper_author": "Giannakas G.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2023-11-24",
        "Abstract": "The creation of a framework in which traditional Machine Learning and neuromorphic algorithms compete to solve a shared Reinforcement Learning environment is presented in this work. In addition, this configuration allows the exploitation of modern and widely-used Machine Learning libraries. The PyTorch framework is used to investigate the expanded capabilities and potential of training an action-critic network pair comprised of specialised units using a custom learning algorithm. The policy and value networks utilised in this context are fully interconnected MultiLayer Perceptrons. The training procedure employs two distinct algorithms: an algorithm inspired by Reward Modulated Spiked Timing Dependent Plasticity and the conventional Back Propagation technique. A comparative evaluation and analysis of the findings is performed.",
        "DOI": "10.1145/3635059.3635066",
        "affiliation_name": "University of Thessaly",
        "affiliation_city": "Volos",
        "affiliation_country": "Greece"
    },
    {
        "paper_title": "Leveraging Artificial Intelligence (AI) in Strengthening the Legal Framework for Regulation of Wildlife and Forest Crimes in Nigeria",
        "paper_author": "Gbadegesin O.A.",
        "publication": "Environmental Policy and Law",
        "citied_by": "1",
        "cover_date": "2023-11-24",
        "Abstract": "Artificial intelligence (AI) applications and machine learning models have extended beyond merely solving computer-related problems to tackle environmental sustainability issues. One such case is countermanding the demand and supply of illegal forest and wildlife products. In Nigeria, the challenges of improving its ability to combat the high rate of wildlife and forest crimes within its borders remain daunting. Despite existing laws, Nigeria remains both a source and a key intermediary for wildlife smuggling and crimes involving protected species. This paper analyses the potential of AI as a tool for strengthening the existing legal framework on wildlife crimes in Nigeria. The findings of this paper demonstrate that the current framework for preventing forest and wildlife crime is being breached due to a lack of resources available to enforcement authorities. Thus, leveraging AI’s potential presents innovative solutions to strengthen compliance with these laws.",
        "DOI": "10.3233/EPL-230011",
        "affiliation_name": "University of Ibadan",
        "affiliation_city": "Ibadan",
        "affiliation_country": "Nigeria"
    },
    {
        "paper_title": "Design of Graphdiyne and Holey Graphyne-Based Single Atom Catalysts for CO<inf>2</inf> Reduction With Interpretable Machine Learning",
        "paper_author": "Ren M.",
        "publication": "Advanced Functional Materials",
        "citied_by": "18",
        "cover_date": "2023-11-23",
        "Abstract": "Using electrochemical CO2 reduction reaction (CO2RR) to synthesize value-added hydrocarbons provides a useful solution for environmental issues and energy crisis. However, this process is impeded by the low activity and selectivity of electrocatalysts toward targeted products. Employing density functional theory computations, the graphdiyne and holey graphyne supported single-atom catalysts (SACs, M/GDY and M/HGY) are demonstrated to be the promising candidates for the CO2RR. By taking full elemental diversity of metal sites across the periodic table, 25 catalysts are found to effectively activate CO2 and inhibit competitive hydrogen evolution, and 8 of them show higher activity for CH4 production than Cu(211). Remarkably, changing supports are found to greatly affect limiting potentials and reaction pathways, even leading to an “inert-active” transition for some metal centers. The resulting SACs, including Mn/GDY, Co/HGY, Ru/GDY, and Os/GDY, can achieve high activity with low limiting potentials of ≈ −0.22 to −0.58 V. Machine learning enables to identify the critical role of the polarized charge and magnetic moment of metal atoms in affecting the activity. The built machine learning model also shows an interpretable capability to predict the activity of the other types of SACs, offering a great promise to quick screening of high-performance SACs.",
        "DOI": "10.1002/adfm.202213543",
        "affiliation_name": "Beijing University of Chemical Technology",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Using neural networks to calibrate agent based models enables improved regional evidence for vaccine strategy and policy",
        "paper_author": "Chopra A.",
        "publication": "Vaccine",
        "citied_by": "2",
        "cover_date": "2023-11-22",
        "Abstract": "Distribution and administration strategy are critical to successful population immunization efforts. Agent-based modeling (ABM) can reflect the complexity of real-world populations and can experimentally evaluate vaccine strategy and policy. However, ABMs historically have been limited in their time-to-development, long runtime, and difficulty calibrating. Our team had several technical advances in the development of our GradABMs: a novel class of scalable, fast and differentiable simulations. GradABMs can simulate million-size populations in a few seconds on commodity hardware, integrate with deep neural networks and ingest heterogeneous sources. This allows for rapid and real-world sensitivity analyses. Our first epidemiological GradABM (EpiABMv1) enabled simulation interventions over real million-scale populations and was used in vaccine strategy and policy during the COVID-19 pandemic. Literature suggests decisions aided by evidence from these models saved thousands of lives. Our most recent model (EpiABMv2) extends EpiABMv1 to allow improved regional calibration using deep neural networks to incorporate local population data, and in some cases different policy recommendations versus our prior models. This is an important advance for our model to be more effective at vaccine strategy and policy decisions at the local public health level.",
        "DOI": "10.1016/j.vaccine.2023.08.060",
        "affiliation_name": "College of Computing",
        "affiliation_city": "Atlanta",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Proceedings of Technology and Policy for Supporting Implementation of COVID-19 Response and Recovery Plan in Southeast Asia, ITTP-COVID19 2021",
        "paper_author": "NA",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2023-11-21",
        "Abstract": "The proceedings contain 28 papers. The topics discussed include: incorporating collaborative and self-directed oral fluency-building activities to address Filipino grade 8 students’ anxiety in English and improve oral fluency; profile of diabetes mellitus therapy in COVID-19 Patients with Diabetes Mellitus in Kogabwilhan II Emergency Hospital Surabaya; real-time face emotion recognition for telediagnosis using machine learning; real-time transesophageal echocardiography (TEE) patient safety indicator visualization according to joint commission international standards; ASEAN MSMEs internationalization policy harmonization in new normal era; the role of IL-21 and IL-27 in COVID-19 patients receiving convalescent plasma; and tourism zoning guideline for ASEAN Mobility mapping in COVID-19 new norm.",
        "DOI": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Water, migration, and development",
        "paper_author": "Jagerskog A.",
        "publication": "Routledge Handbook of Water and Development",
        "citied_by": "0",
        "cover_date": "2023-11-21",
        "Abstract": "This chapter aims to present the state of the art of research on water, migration, and development and how they are interlinked. In policy debate as well as in some research a linear and close to causal relationship has been assumed between water scarcity and migration. This chapter provides a critical perspective on some of these assumptions and highlights the complexities that is behind the decision to migrate. New research undertaken by the World Bank, which includes the analysis of large quantities of migration data through Machine Learning, found that around 10% of migration is directly water related, underscoring that many of the reasons to migrate are related to people seeking job and livelihood opportunities. Encouraging a critical perspective and emphasizing complexity the chapter highlights the Middle East as a potential precursor to better understand the water, migration, and development agenda, as water scarcity is, generally, more prevalent in that region than in others. Finally, ideas for policy measures - including people-centred investments - are discussed.",
        "DOI": "10.4324/9781003095545-27",
        "affiliation_name": "The World Bank, USA",
        "affiliation_city": "Washington, D.C.",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "A Novel Domain Knowledge-Informed Machine Learning Approach for Modeling Solid Waste Management Systems",
        "paper_author": "He R.",
        "publication": "Environmental Science and Technology",
        "citied_by": "4",
        "cover_date": "2023-11-21",
        "Abstract": "Sustainability challenges, such as solid waste management, are usually scientifically complex and data scarce, which makes them not amenable to science-based analytical forms or data-intensive learning paradigms. Deep integration between data science and sustainability science in highly complementary manners offers new opportunities for tackling these conundrums. This study develops a novel hybrid neural network (HNN) model that imposes the holistic decision-making context of solid waste management systems (SWMS) on a traditional neural network (NN) architecture. Equipped with adaptable hybridization designs of hand-crafted model structure, constrained or predetermined parameters, and a customized loss function, the HNN model is capable of learning various technical, economic, and social aspects of SWMS from a small and heterogeneous data set. In comparison, the versatile HNN model not only outperforms traditional NN models in convergence rates, which leads to a 22% lower mean testing error of 0.20, but also offers superior interpretability. The HNN model is capable of generating insights into the enabling factors, policy interventions, and driving forces of SWMS, laying a solid foundation for data-driven decision making.",
        "DOI": "10.1021/acs.est.3c04214",
        "affiliation_name": "School of Computer Science",
        "affiliation_city": "Pittsburgh",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Using Explainable Machine Learning to Interpret the Effects of Policies on Air Pollution: COVID-19 Lockdown in London",
        "paper_author": "Ma L.",
        "publication": "Environmental Science and Technology",
        "citied_by": "6",
        "cover_date": "2023-11-21",
        "Abstract": "Activity changes during the COVID-19 lockdown present an opportunity to understand the effects that prospective emission control and air quality management policies might have on reducing air pollution. Using a regression discontinuity design for causal analysis, we show that the first UK national lockdown led to unprecedented decreases in road traffic, by up to 65%, yet incommensurate and heterogeneous responses in air pollution in London. At different locations, changes in air pollution attributable to the lockdown ranged from −50% to 0% for nitrogen dioxide (NO2), 0% to +4% for ozone (O3), and −5% to +0% for particulate matter with an aerodynamic diameter less than 10 μm (PM10), and there was no response for PM2.5. Using explainable machine learning to interpret the outputs of a predictive model, we show that the degree to which NO2 pollution was reduced in an area was correlated with spatial features (including road freight traffic and proximity to a major airport and the city center), and that existing inequalities in air pollution exposure were exacerbated: pollution reductions were greater in places with more affluent residents and better access to public transport services.",
        "DOI": "10.1021/acs.est.2c09596",
        "affiliation_name": "Imperial College London",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Machine Learning Predicts the Methane Clumped Isotopologue (<sup>12</sup>CH<inf>2</inf>D<inf>2</inf>) Distributions Constrain Biogeochemical Processes and Estimates the Potential Budget",
        "paper_author": "Wang X.",
        "publication": "Environmental Science and Technology",
        "citied_by": "3",
        "cover_date": "2023-11-21",
        "Abstract": "Methane (CH4) is a matter of environmental concern; however, global methane isotopologue data remain inadequate. This is due to the challenges posed by high-resolution testing technology and the need for larger sample volumes. Here, worldwide methane clumped isotope databases (n = 465) were compiled. We compared machine-learning (ML) models and used random forest (RF) to predict new Δ12CH2D2 distributions, which cover valuable and hard-to-replicate methane clumped isotope experimental data. Our RF model yields a reliable and continuous database including ruminants, acetoclastic methane, multiple pyrolysis, and controlled experiments. We showed the effectiveness of utilizing a new data set to quantify isotopologue fractionations in biogeochemical methane processes, as well as predicting the steady-state atmospheric methane clumped isotope composition (Δ13CH3D of +2.26 ± 0.71‰ and Δ12CH2D2 of +62.06 ± 4.42‰) with notable biological contributions. Our measured summer and winter water emitted gases (n = 6) demonstrated temperature-driven seasonal microbial community evolution determined by atmospheric clumped isotope temporal variations (Δ 13CH3D ∼ −0.91 ± 0.25 ‰ and Δ12CH2D2 ∼ +3.86 ± 0.84 ‰), which in turn is relevant for future models quantifying the contribution of methane sources and sinks. Predicting clumped isotopologues translates our methane geochemical understanding into quantifiable variables for modeling that can continue to improve predictions and potentially inform global greenhouse gas emissions and mitigation policy.",
        "DOI": "10.1021/acs.est.3c00184",
        "affiliation_name": "D'Amore-McKim School of Business",
        "affiliation_city": "Boston",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Systematic Performance Evaluation of Reinforcement Learning Algorithms Applied to Wastewater Treatment Control Optimization",
        "paper_author": "Croll H.C.",
        "publication": "Environmental Science and Technology",
        "citied_by": "18",
        "cover_date": "2023-11-21",
        "Abstract": "Treatment of wastewater using activated sludge relies on several complex, nonlinear processes. While activated sludge systems can provide high levels of treatment, including nutrient removal, operating these systems is often challenging and energy intensive. Significant research investment has been made in recent years into improving control optimization of such systems, through both domain knowledge and, more recently, machine learning. This study leverages a novel interface between a common process modeling software and a Python reinforcement learning environment to evaluate four common reinforcement learning algorithms for their ability to minimize treatment energy use while maintaining effluent compliance within the Benchmark Simulation Model No. 1 (BSM1) simulation. Three of the algorithms tested, deep Q-learning, proximal policy optimization, and synchronous advantage actor critic, generally performed poorly over the scenarios tested in this study. In contrast, the twin delayed deep deterministic policy gradient (TD3) algorithm consistently produced a high level of control optimization while maintaining the treatment requirements. Under the best selection of state observation features, TD3 control optimization reduced aeration and pumping energy requirements by 14.3% compared to the BSM1 benchmark control, outperforming the advanced domain-based control strategy of ammonia-based aeration control, although future work is necessary to improve robustness of RL implementation.",
        "DOI": "10.1021/acs.est.3c00353",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Ames",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Attribution of Air Quality Benefits to Clean Winter Heating Policies in China: Combining Machine Learning with Causal Inference",
        "paper_author": "Song C.",
        "publication": "Environmental Science and Technology",
        "citied_by": "40",
        "cover_date": "2023-11-21",
        "Abstract": "Heating is a major source of air pollution. To improve air quality, a range of clean heating policies were implemented in China over the past decade. Here, we evaluated the impacts of winter heating and clean heating policies on air quality in China using a novel, observation-based causal inference approach. During 2015-2021, winter heating causally increased annual PM2.5, daily maximum 8-h average O3, and SO2 by 4.6, 2.5, and 2.3 μg m-3, respectively. From 2015 to 2021, the impacts of winter heating on PM2.5 in Beijing and surrounding cities (i.e., “2 + 26” cities) decreased by 5.9 μg m-3 (41.3%), whereas that in other northern cities only decreased by 1.2 μg m-3 (12.9%). This demonstrates the effectiveness of stricter clean heating policies on PM2.5 in “2 + 26” cities. Overall, clean heating policies caused the annual PM2.5 in mainland China to reduce by 1.9 μg m-3 from 2015 to 2021, potentially avoiding 23,556 premature deaths in 2021.",
        "DOI": "10.1021/acs.est.2c06800",
        "affiliation_name": "University of Birmingham",
        "affiliation_city": "Birmingham",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Unpacking the nonlinear relationships and interaction effects between urban environment factors and the urban nighttime heat index",
        "paper_author": "Park S.",
        "publication": "Journal of Cleaner Production",
        "citied_by": "4",
        "cover_date": "2023-11-20",
        "Abstract": "In this study, we identified the urban environmental factors affecting the nighttime heat index (NHI), which has policy implications for improving the urban thermal environment using Smart Seoul Urban Data Sensor (S-DoT) data and an interpretable machine learning methodology. S-DoT sensors are ideal for deriving the heat index at a micro-scale in space and time, unlike ground-based observations and satellite-based imagery traditionally used to measure urban temperatures. Moreover, the interpretable machine learning methodology can overcome the nonlinearity argument of the influential factors identified in previous studies. The main results of this study are as follows. First, the Sky View Factor generally has a negative association with the NHI, which means that a higher SVF is associated with a lower NHI. Conversely, the surface roughness commonly has a positive association with the NHI, which signifies that a higher surface roughness results in a higher NHI. Second, the building view factor significantly increases the NHI, and the gross floor area of the building has a positive association with the NHI regardless of its use. Third, although the center of the city has a high temperature, if it has physical environment factors similar to the outskirts of the city, the NHI there may be relatively low compared to the nighttime air temperature. This study is meaningful because it empirically shows the nonlinear relationships and interaction effects between urban environment factors and the NHI and suggests policy implications to improve the urban nighttime thermal environment.",
        "DOI": "10.1016/j.jclepro.2023.139407",
        "affiliation_name": "Hanyang University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "Environmental tipping points for global soil carbon fixation microorganisms",
        "paper_author": "Hao Y.",
        "publication": "iScience",
        "citied_by": "2",
        "cover_date": "2023-11-17",
        "Abstract": "Carbon fixation microorganisms (CFMs) are important components of the soil carbon cycle. However, the global distribution of CFMs and whether they will exceed the environmental tipping points remain unclear. According to the machine learning models, total carbon content, nitrogen fertilizer, and precipitation play dominant roles in CFM abundance. Obvious stimulation and inhibition effects on CFM abundance only happened at low levels of total carbon and precipitation, where the tipping points were 6.1 g·kg−1 and 22.38 mm, respectively. The abundance of CFMs in response to nitrogen fertilizer changed from positive to negative (tipping point at 9.45 kg ha−1·y−1). Approximately 46% of CFM abundance decline happened in cropland at 2100. Our work presents the distribution of carbon-fixing microorganisms on a global scale and then points out the sensitive areas with significant abundance changes. The previously described information will provide references for future soil quality prediction and policy decision-making.",
        "DOI": "10.1016/j.isci.2023.108251",
        "affiliation_name": "Ministry of Agriculture of the People's Republic of China",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Development of corporate treasury operations in russian leasing companies",
        "paper_author": "Shirokova M.A.",
        "publication": "E3S Web of Conferences",
        "citied_by": "0",
        "cover_date": "2023-11-16",
        "Abstract": "In the context of the digital economy small Russian companies with several settlement accounts, minimal currency risks, and the absence of the need to control credit resources and investments, practically do not create an autonomous treasury department. Treasury functions are performed by an accountant and are controlled and managed by a chief financial officer. With the expansion of the companys activities, in order to create a unified strategy for the policy in the field of cash flow control and the development of financial risk management procedures, a centralized treasury is formed. According to research by the largest international audit companies, due to the strengthening of strategic emphasis in cash management, the role, degree of centralization, and importance of corporate treasury in Russian financial management have been growing significantly over the past 10 years the study examines the development of the corporate treasury in leasing companies in Russia; the authors propose several ways of managing corporate treasury operations in leasing companies, in particular, development and application of a cash flow management system, and outline a methodology of separate accounting of the results of corporate treasury activities. It is concluded that the use of information support, based on big data analysis and machine learning, contributing to the consolidation of all the companys cash flows in a single database, is required. In this regard, the results obtained can be useful to organizations that use corporate treasury in their activities.",
        "DOI": "10.1051/e3sconf/202344904008",
        "affiliation_name": "Plekhanov Russian University of Economics",
        "affiliation_city": "Moscow",
        "affiliation_country": "Russian Federation"
    },
    {
        "paper_title": "Annual Reports’ Tone and Violation Behavior Identification of Listed Companies: Evidence from Textual Analysis Based on Machine Learning",
        "paper_author": "Li S.",
        "publication": "Modern Economic Science",
        "citied_by": "1",
        "cover_date": "2023-11-15",
        "Abstract": "Corporate fraud disrupts market order, undermines investor rights, and poses a significant threat to the capital market's healthy development. The annual financial report, a crucial means of information disclosure for listed companies, plays a pivotal role. It provides external investors with insights into a company's business status and future development trends, aiding in informed decision-making and reducing information asymmetry. However, compared to Europe and the United States, China's regulatory system is comparatively weaker, potentially allowing listed companies to manipulate the tone of their reports, exacerbating information asymmetry. Investigating the connection between textual tone and listed companies' violation behaviors can unveil potential attempts to mislead investors, shedding light on their actual financial status. Analysing the textual tone of annual reports and a company's violation behaviors enables investors to glean incremental information, identify potential fraudulent activities, mitigate risks, and make well-informed decisions. Hence, this research holds great significance in fostering a healthy and stable capital market. Utilizing data from non-financial listed firms in the A-share markets of Shanghai and Shenzhen Stock Exchange spanning 2009 to 2019, we investigate the correlation between textual tone and corporate fraud based on an extensive dataset of 11,040 annual reports. Employing machine learning, we measure the textual tone of the Management Discussion and Analysis (MD&A). Our findings reveal a general trend; a more positive MD&- A tone corresponds to a reduced likelihood of violations. Upon closer examination of the MD&A sections, we observe that the negative correlation between sentiment and corporate violations is weaker in the outlook part than in the overview section. Furthermore, our research indicates that heightened information transparency strengthens the negative association between text sentiment and company violations. We also conduct thorough analyses considering the impact of regulatory policies on information disclosure, addressing potential endogeneity issues and adjusting the textual tone for regression. Encouragingly, the main conclusions remain robust. Our study further uncovers a stronger correlation between MD&A tone and operating and information disclosure fraud compared to violations involving company leadership. Therefore, the value of MD&A text sentiment should be accorded significant importance by both listed companies and investors, regulatory authorities should strengthen their oversight on the information disclosure practices of listed companies. This paper extends the existing literature in two aspects. First, different from previous studies on the influencing factors of corporate violations from the internal governance and external supervision, this paper studies the correlation between management tone and companies' violations from the perspective of textual sentiment, which expands the research on the identification of corporate violations. Second, different from other domestic scholars who use the emotion dictionary to measure management tone, this paper adopts machine learning and improves the measurement of MD&A tone, which can broaden the domestic research in this field and provide some reference for subsequent research. The findings of this study demonstrate the extent to which MD&A tone play a crucial role in identifying violations committed by listed companies. This not only aids regulatory authorities in formulating more targeted policies and measures, but also contributes to standardizing market behaviors and enhancing regulatory efficiency.",
        "DOI": "10.20069/j.cnki.DJKX.202306008",
        "affiliation_name": "Xi'an Jiaotong University",
        "affiliation_city": "Xi'an",
        "affiliation_country": "China"
    },
    {
        "paper_title": "CCSW'23: 2023 Cloud Computing Security Workshop",
        "paper_author": "Regazzoni F.",
        "publication": "CCS 2023 - Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security",
        "citied_by": "0",
        "cover_date": "2023-11-15",
        "Abstract": "Clouds and massive-scale computing infrastructures are starting to dominate computing and will likely continue to do so for the foreseeable future. Major cloud operators are now comprising millions of cores hosting substantial fractions of corporate and government IT infrastructure. CCSW is the world's premier forum bringing together researchers and practitioners in all security aspects of cloud-centric and outsourced computing, including: • Side channel attacks • Cryptographic protocols for cloud security • Secure cloud resource virtualization mechanisms • Secure data management outsourcing (e.g., database as a service) • Privacy and integrity mechanisms for outsourcing • Foundations of cloud-centric threat models • Secure computation outsourcing • Remote attestation mechanisms in clouds • Sandboxing and VM-based enforcements • Trust and policy management in clouds • Secure identity management mechanisms • Cloud-aware web service security paradigms and mechanisms • Cloud-centric regulatory compliance issues and mechanisms • Business and security risk models and clouds • Cost and usability models and their interaction with security in clouds • Scalability of security in global-size clouds • Binary analysis of software for remote attestation and cloud protection • Network security (DOS, IDS etc.) mechanisms for cloud contexts • Security for emerging cloud programming models • Energy/cost/efficiency of security in clouds • mOpen hardware for cloud • Machine learning for cloud protection CCSW especially encourages novel paradigms and controversial ideas that are not on the above list. The workshop has historically acted as a fertile ground for creative debate and interaction in security-sensitive areas of computing impacted by clouds. This year marked the 14th anniversary of CCSW. In the past decade, CCSW has had a significant impact in our research community.",
        "DOI": "10.1145/3576915.3624024",
        "affiliation_name": "Università della Svizzera italiana",
        "affiliation_city": "Lugano",
        "affiliation_country": "Switzerland"
    },
    {
        "paper_title": "Physics-informed model-based reinforcement learning (PI-MBRL) to control building heating systems in low training data regimes",
        "paper_author": "Saeed M.H.",
        "publication": "BuildSys 2023 - Proceedings of the10th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation",
        "citied_by": "1",
        "cover_date": "2023-11-15",
        "Abstract": "Reinforcement learning (RL) is a data-driven technique for learning optimal control policies in complex systems. Classical RL methods rely completely on data with no information about physics of underlying system, due to which they are oblivious of the physical constraints of the system while deciding the next control action. This effect is especially pronounced in low data regimes (when training data is limited). In this research work, we discuss the potential of model-based reinforcement learning coupled with physical information to control the thermal system of a single-zone building. We compare two RL methods: the classical vanilla deep Q-networks (v-DQN) and a novel physics-informed model-based reinforcement learning (PI-MBRL) algorithm. The results show that the PI-MBRL outperforms the v-DQN when trained on just 4 weeks of heating data of the building and provides greater thermal comfort when tested on two entire months. These results indicate how injecting the physical knowledge can improve performance in low training data regimes.",
        "DOI": "10.1145/3600100.3626270",
        "affiliation_name": "KU Leuven",
        "affiliation_city": "Leuven",
        "affiliation_country": "Belgium"
    },
    {
        "paper_title": "Elucidating the Impacts of Meteorology and Emission Changes on Concentrations of Major Air Pollutants in Major Cities in the Yangtze River Delta Region Using a Machine Learning De-Weather Method",
        "paper_author": "Fu W.X.",
        "publication": "Huanjing Kexue/Environmental Science",
        "citied_by": "2",
        "cover_date": "2023-11-15",
        "Abstract": "This study applied a de-weather method based on a machine learning technique to quantify the contribution of meteorology and emission changes to air quality from 2015 to 2021 in four cities in the Yangtze River Delta Region. The results showed that the significant reductions in PM2. 5, NO2 , and SO2 emissions(57. 2%- 68. 2%, 80. 7%-94. 6%, and 81. 6%- 96. 1%, respectively) offset the adverse effects of meteorological conditions, resulting in lower pollutant concentrations. The meteorological contribution of maximum daily 8-h average O3 (MDA8_O3 ) showed a stronger effect than that of others(23. 5%-42. 1%), and meteorological factors promoted the increase in MDA8_O3 concentrations(4. 7%); however, emission changes overall resulted in a decrease in MDA8_O3 concentrations(- 3. 2%). NO2 and MDA8_O3 decreased more rapidly from 2019 to 2021, mainly because the emissions played a stronger role in reducing pollutant concentrations than from 2015 to 2018. However, emissions changes had weaker reduction effects on PM2. 5 and SO2 from 2019 to 2021 than from 2015 to 2018. De-weather methods could effectively seperate the effects of meteorology and emission changes on pollutant trends, which helps to evaluate the real effects of emission control policies on pollutant concentrations.",
        "DOI": "10.13227/j.hjkx.202301119",
        "affiliation_name": "Nanjing University of Information Science &amp; Technology",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Reinforcement Learning-based path tracking for underactuated UUV under intermittent communication",
        "paper_author": "Liu Z.",
        "publication": "Ocean Engineering",
        "citied_by": "4",
        "cover_date": "2023-11-15",
        "Abstract": "This paper studies the path control of a six-degree-of-freedom underactuated Unmanned Underwater Vehicle (UUV) under limited communication conditions. Considering the large number of coupling between six-degree-of-freedom underactuated UUV of unknown dynamic models, traditional model-based control methods are difficult to effectively solve the three-dimensional path control problem. A self-attention based soft actor and critic (A-SAC) algorithm is designed to learn effective control policy from random paths. The problem of limited target acquisition by UUV in the actual underwater environment is effectively overcome, which is mainly caused by the inability of UUV to consistently receive information about their expected path. A new state space is designed and a self-attention mechanism is introduced to improve the efficiency of using discontinuous path information. Furthermore, the validation experiment compares classical Reinforcement Learning methods such as DDPG, PPO, and etc. Compared to other existing methods, the proposed A-SAC algorithm can more quickly and effectively learn the path control policy for a six-degree-of-freedom UUV that operates in a complex environment.",
        "DOI": "10.1016/j.oceaneng.2023.116076",
        "affiliation_name": "Zhejiang University of Water Resources and Electric Power",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "PM<inf>2.5</inf> air pollution prediction through deep learning using meteorological, vehicular, and emission data: A case study of New Delhi, India",
        "paper_author": "Shakya D.",
        "publication": "Journal of Cleaner Production",
        "citied_by": "17",
        "cover_date": "2023-11-15",
        "Abstract": "Particulate matter (PM2.5) concentration is an air pollutant that can lead to serious health complications in humans. The detection of this air pollutant is essential so that government agencies can formulate policies to take effective measures. This study proposes and analyzes a Gated Recurrent Unit Based Encoder-Decoder (GRU-ED) method for predicting 1-hourly, 8-hourly, and 24-hourly PM2.5 concentrations in New Delhi, India, for three years (from 2008 to 2010). The study uses different input parameter combinations of meteorological (M), vehicle (V) population, and emissions (E) data. In all, the authors tested the proposed GRU-ED method with four models: Model 1: Vehicle population + Emission [no meteorological (VE)], Model 2: Meteorological + Emission [no vehicle population (ME)], Model 3: Meteorological + Vehicle population [no emission (MV)], and Model 4: Meteorological + Vehicle population + Emission (MVE). It is observed that the proposed GRU-ED method performed better than traditional machine learning predictive methods (Random Forest, Extreme Gradient Boosting, Artificial Neural Networks, and Long Short-Term Memory (LSTM)) in terms of forecast value accuracy. The GRU-ED method with Model 4 is found to be the most accurate forecasting model for 1-hourly PM2.5 concentration prediction (R2 = 0.959, NSE = 0.953, MAE = 1.770, RRMSE = 0.002, and MAPE = 0.190). It is also observed that among the meteorological, vehicle, and emission parameters, the presence of the meteorological parameter has a significant impact on the prediction accuracy.",
        "DOI": "10.1016/j.jclepro.2023.139278",
        "affiliation_name": "Indian Institute of Technology Indore",
        "affiliation_city": "Indore",
        "affiliation_country": "India"
    },
    {
        "paper_title": "DEMRL: Dynamic estimation meta reinforcement learning for path following on unseen unmanned surface vehicle",
        "paper_author": "Jin K.",
        "publication": "Ocean Engineering",
        "citied_by": "5",
        "cover_date": "2023-11-15",
        "Abstract": "Reinforcement learning has been widely used for unmanned surface vehicle (USV) control tasks. However, the requirement of numerous training samples limits its transferability to new USVs. In this article, we propose a dynamic estimation meta reinforcement learning (DEMRL) approach that enables few-shot learning for the path following control policy. We first present a dynamic estimation method to learn a latent dynamic context feature. The learned context contains the hidden information of USV dynamics with only a few estimation samples. We then propose a meta reinforcement learning based training framework to learn the generalizable path following control policy. After that, given the prior knowledge from dynamic context, the well-trained policy can easily adapt to the target USV during the rapid adaptation process. This proposed method represents the initial effort in tackling the few-shot learning challenge associated with training reinforcement learning based USV path-following policies. Extensive experiments demonstrate that the proposed method can achieve promising path following performance for unseen USV with very few training data and training volume.",
        "DOI": "10.1016/j.oceaneng.2023.115958",
        "affiliation_name": "State Key Laboratory of Ocean Engineering",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Deep reinforcement learning based energymanagement strategy considering running costs and energy source aging for fuel cell hybrid electric vehicle",
        "paper_author": "Huang Y.",
        "publication": "Energy",
        "citied_by": "15",
        "cover_date": "2023-11-15",
        "Abstract": "The main contribution of this study is to integrate energy source aging and running costs into the deep reinforcement learning (DRL) based EMS of fuel cell hybrid electric vehicles (FCHEV). For the FCHEV, a multi-objective energy management strategy (EMS) based on twin delayed deep deterministic policy gradient (TD3) is proposed, which aims to simultaneously reduce energy source degradation and lower running costs. To achieve this, the paper innovatively designs the reward function and it's comparative approach. Additionally, it verifies the superiority of the proposed EMS over other EMS based on continuous action space algorithm, including previous action guided deep deterministic policy gradient (PA-DDPG) and soft actor-critic (SAC). Lastly, the agent's action output is changed from fuel cell (FC) current to FC power ratio, and a comparative analysis on results generated by different action outputs is conducted. Simulation results show that the proposed EMS can reduce the running costs while extending the lifespan of battery and FC efficiently. This work holds significant practical significance in the energy distribution of automobiles.",
        "DOI": "10.1016/j.energy.2023.129177",
        "affiliation_name": "Wenzhou University",
        "affiliation_city": "Wenzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Twin delayed deep deterministic policy gradient based energy management strategy for fuel cell/battery/ultracapacitor hybrid electric vehicles considering predicted terrain information",
        "paper_author": "Tao F.",
        "publication": "Energy",
        "citied_by": "8",
        "cover_date": "2023-11-15",
        "Abstract": "For fuel cell/battery/ultracapacitor hybrid electric vehicles (FCHEV), the complex topology and terrain information pose challenges to fuel cell efficiency, power sources lifespan, and fuel economy. In this paper, a hierarchical energy management strategy (EMS) considering predicted terrain information is constructed to optimize the demand power allocation of FCHEV based on twin delayed deep deterministic policy gradient (TD3). Firstly, to improve the fuel economy of FCHEV, an adaptive fuzzy filter-based upper-level frequency separation is designed considering negative power. Secondly, to achieve the minimum fuel consumption, a TD3-based lower-level EMS is designed, where the terrain information is predicted through multi long short-term memory network and considered in the TD3 framework. The optimal state of charge obtained by dynamic programming (DP) based on the demand power and predicted terrain information, is considered in the reward function of TD3, as well as the optimal experience solved by DP based on historical road data is integrated into the designed hybrid experience replay. Finally, the effectiveness and optimality of the TD3-based EMS considering predicted terrain information are verified by a series of comprehensive simulations under different driving cycles. The simulation results under CYC_SC03 and test driving cycles indicate that compared to traditional TD3-based EMS, the fuel economy is increased by 33.3% and 27.5%, respectively, and can reach the benchmark level of 80% and 80.9% of DP, respectively.",
        "DOI": "10.1016/j.energy.2023.129173",
        "affiliation_name": "Henan University of Science and Technology",
        "affiliation_city": "Luoyang",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Characteristics of secondary inorganic aerosols and contributions to PM<inf>2.5</inf> pollution based on machine learning approach in Shandong Province",
        "paper_author": "Li T.",
        "publication": "Environmental Pollution",
        "citied_by": "9",
        "cover_date": "2023-11-15",
        "Abstract": "Primary emissions of particulate matter and gaseous pollutants, such as SO2 and NOx have decreased in China following the implementation of a series of policies by the Chinese government to address air pollution. However, controlling secondary inorganic aerosol pollution requires attention. This study examined the characteristics of the secondary conversion of nitrate (NO3−) and sulfate (SO42−) in three coastal cities of Shandong Province, namely Binzhou (BZ), Dongying (DY), and Weifang (WF), and an inland city, Jinan (JN), during December 2021. Furthermore, the Shapley Additive Explanation (SHAP), an interpretable attribution technique, was adopted to accurately calculate the contributions of secondary formations to PM2.5. The nitrogen oxidation rate exhibited a significant dependence on the concentration of O3. High humidity facilitates sulfur oxidation. Compared to BZ, DY, and WF, the secondary conversion of NO3− and SO42− was more intense in JN. The light-gradient boosting model outperformed the random forest and extreme-gradient boosting models, achieving a mean R2 value of 0.92. PM2.5 pollution events in BZ, DY, and WF were primarily attributable to biomass burning, whereas pollution in Jinan was contributed by the secondary formation of NO3− and vehicle emissions. Machine learning and the SHAP interpretable attribution technique offer a precise analysis of the causes of air pollution, showing high potential for addressing environmental concerns.",
        "DOI": "10.1016/j.envpol.2023.122612",
        "affiliation_name": "Shandong University",
        "affiliation_city": "Jinan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Predicting carbon futures prices based on a new hybrid machine learning: Comparative study of carbon prices in different periods",
        "paper_author": "Zhang X.",
        "publication": "Journal of Environmental Management",
        "citied_by": "18",
        "cover_date": "2023-11-15",
        "Abstract": "Accurate prediction of carbon price is of great significance to national energy security and climate environment policies. This paper comes up with a new forecasting model variational mode decomposition, convolutional neural network, bidirectional long short-term memory, and multi-layer perceptron (VMD–CNN–BILSTM-MLP) to predict EUA carbon futures prices in two periods of five years before and after the introduction of emission reduction policies. The parameters of the VMD model are determined by genetic algorithm (GA) firstly, carbon futures prices are broken down into subsequences of different frequencies using the model. The MLP model is then applied to predict the highest frequency sequence. The CNN-BILSTM model is applied to predict other subsequences later. Finally, the predicted values of each subsequence are linearly added to obtain the final result of the entire model. The prediction effect of the model is mainly tested by root mean squared error (RMSE), mean absolute error (MAE), mean absolute percentage error (MAPE), coefficient of determination (R2) and the modification of Diebold-Mariano test (MDM). In both periods, the proposed model predicts better than the other models, and the prediction effect of carbon futures price in the first five years is a little better than that in the second five years. In general, the experiment of predicting carbon futures prices in two different periods, the experiment of changing the proportion of data set and the experiment of predicting the whole sample all prove that the mixed model proposed in this paper has good prediction effect.",
        "DOI": "10.1016/j.jenvman.2023.118962",
        "affiliation_name": "Business School of Sichuan University",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Prediction of transportation energy demand in Türkiye using stacking ensemble models: Methodology and comparative analysis",
        "paper_author": "Hoxha J.",
        "publication": "Applied Energy",
        "citied_by": "29",
        "cover_date": "2023-11-15",
        "Abstract": "The transportation sector accounts for 61.5% of global oil consumption and is responsible for 29% of the world's total energy demand. Passenger transportation utilizes around 50%–60% of the energy used for transportation-related activities. Accurate prediction of future transportation energy consumption is essential for governments to make well-informed decisions regarding transportation infrastructure development and utilization, which supports the United Nations’ Sustainable Development Goals (SDGs) and advances the shift to a net-zero carbon economy. With the expected increase in population, vehicles, and economic growth, it is essential to predict the energy demand to ensure sustainable urban transportation. This is crucial not only for economic prosperity but also for promoting human health and mitigating carbon emissions. Therefore, transportation energy demand prediction plays a vital role in designing sustainable future urban transportation and making informed energy investment and policy decisions. This study proposes a novel methodology and investigates for the application of machine learning stacking ensemble method with hyperparameter tuning and multicollinearity removal to predict transportation energy demand in Turkey based on historic data from 1975–2019. The dataset includes GDP, year, vehicle miles traveled, population, oil price, passenger miles traveled, and ton-miles traveled as features. A performance evaluation and comparison of 19 machine learning algorithms is first carried out to find the best candidate for the stacking ensemble models, including eXtreme Gradient Boosting algorithm. This performance comparison uses all features and also only two of them during the training phase, and it takes into consideration a 4-fold cross-validation. A combination of permutation importance and hierarchical clustering algorithm on the Spearman rank-order correlations is used for dimensionality reduction of the dataset. Extra Tree Regressor and ADABoost Regressor, which are both placed in the second level of the suggested models, are two meta-regressors that are proposed for stacking ensembles because they perform better compared to single machine learning algorithm. In total, eight stacking ensemble models – four for each of the meta-regressors – were developed and investigated considering all features and only two of them separately. Six metrics – R-squared, MSE, MAE, RMSE, RMSLE, and MAPE – are used to assess all models. The Extra Trees Regressor can be used as a meta-regressor in the best proposed stacking ensemble model to predict the energy demand for transportation. This model achieves an R-squared value of approximately 0.99 when all the features are taken into consideration. When only two features from the dataset are considered the same stacking ensemble model can achieve an accuracy of 0.98. These findings have the potential to contribute to the development of more accurate models and results, which can, in turn, lead to improved strategies for managing future transportation energy demand. Additionally, this research can support the advancement of alternative technologies that promote sustainable urban development, ultimately helping to move towards a net-zero carbon economy.",
        "DOI": "10.1016/j.apenergy.2023.121765",
        "affiliation_name": "American University of the Middle East",
        "affiliation_city": "Al Ahmadi",
        "affiliation_country": "Kuwait"
    },
    {
        "paper_title": "Climate change and coastal morphodynamics: Interactions on regional scales",
        "paper_author": "Chowdhury P.",
        "publication": "Science of the Total Environment",
        "citied_by": "11",
        "cover_date": "2023-11-15",
        "Abstract": "Climate change and its impacts, combined with unchecked human activities, intensify pressures on coastal environments, resulting in modification of the coastal morphodynamics. Coastal zones are intricate and constantly changing areas, making the monitoring and interpretation of data a challenging task, especially in remote beaches and regions with limited historical data. Traditionally, remote sensing and numerical methods have played a vital role in analysing earth observation data and supporting the monitoring and modelling of complex coastal ecosystems. However, the emergence of artificial intelligence-based techniques has shown promising results, offering the additional advantage of filling data gaps, predicting data in data-scarce regions, and analysing multidimensional datasets collected over extended periods of time and larger spatial scales. The main objective of this study is to provide a comprehensive review of the existing literature, discussing both traditional methods and various emerging artificial intelligence-based approaches used in studying the coastal dynamics, shoreline change analysis, and coastal monitoring. Ultimately, the study proposes a climate resilience framework to enhance coastal zone management practices and policies, fostering resilience among coastal communities. The outcome of this study aligns with and supports particularly SDG 13 of the UN (Climate Action) and advances it by identifying relevant methods in coastal erosion studies and proposing integrated management plans informed by real-time data collection and analysis/modelling using physics-based models.",
        "DOI": "10.1016/j.scitotenv.2023.166432",
        "affiliation_name": "Centre for the Environment Fisheries and Aquaculture Science",
        "affiliation_city": "Lowestoft",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "The predictive management in campus heating system based on deep reinforcement learning and probabilistic heat demands forecasting",
        "paper_author": "Chen M.",
        "publication": "Applied Energy",
        "citied_by": "8",
        "cover_date": "2023-11-15",
        "Abstract": "As a promising technology for replacing the rule-based decision-making in region heating systems (RHS), deep reinforcement learning (DRL) is a practical solution to identify the optimal control for heating equipment. However, as residential customers perform more casual energy-consumption behaviors, the intermittency and volatility of heat demands make managing heat supply and storage much harder for DRL agents. This study proposes a novel predictive management method for campus heating systems (CHS) with air-source heat pumps (AHP) and thermostatic water tanks. The novelty of the proposed method lies in the combination of the heat demands forecasting model and DRL-based adaptively controlling for heat supply equipment, which is firstly proposed to improve the heating supply reliability and reduce the storage dependence for CHS. Specifically, an enhanced rule, namely minimum length hamming encoding, and an input array constructing method is introduced to deal with discrete feature data and then improve the accuracy of deterministic heat demands forecasting based on long-short term memory (LSTM), and the Kernel density estimation (KDE) are employed to obtain the prediction intervals (PIs) from H-step ahead heat demands forecasting series. Followed by these, the twin delayed deep deterministic policy gradient, a model-free DRL control algorithm, is adopted for adaptively adjusting the output flow rate of AHP and then the storage of the hot water tank. To demonstrate the validity of the proposed method, a case study is presented where a campus heat demands forecasting achieves a maximum accuracy gain of 4.52%, and an optimal AHP operating controlling determined from PIs achieves a better cost reduction and supply reliability, which is superior over the conventional method using real-time heat demands or deterministic forecasting results as input.",
        "DOI": "10.1016/j.apenergy.2023.121710",
        "affiliation_name": "North China Electric Power University (Baoding)",
        "affiliation_city": "Baoding",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Forecasting solar energy production in Spain: A comparison of univariate and multivariate models at the national level",
        "paper_author": "Cabello-López T.",
        "publication": "Applied Energy",
        "citied_by": "17",
        "cover_date": "2023-11-15",
        "Abstract": "Renewable energies, such as solar power, offer a clean and cost-effective energy source. However, their integration into national electricity grids poses challenges due to their dependence on climate and geography. While numerous studies have focused on solar energy time series, few have specifically addressed the critical task of forecasting solar energy production at the national level. Accurate national-level forecasting is crucial for optimizing energy management, informing policy development, and promoting environmental sustainability. This study aims to address the challenges associated with the significant variability in renewable energy production and its impact on grid stability by improving the accuracy of existing forecasting approaches. To achieve this goal, we evaluate the effectiveness of univariate and multivariate approaches for time series forecasting of national solar energy production data from ESIOS (the Spanish System Operator). Our primary focus is on leveraging external solar variables, such as solar irradiance data. To this end, we propose a methodology to integrate solar irradiance forecasts with historical data from solar power plants in Spain to improve the performance of multivariate models. Subsequently, we compare the performance of classical regression techniques and state-of-the-art deep learning algorithms, presenting univariate and multivariate models for three forecast horizons (1 h, 24 h, and 48 h). Finally, we assess the performance of our best univariate and multivariate models by comparing them with the official forecast of ESIOS. Our findings indicate that the best-performing models are deep-learning multivariate approaches, which benefit from incorporating solar irradiance forecasts, particularly for longer forecast horizons (24 h and 48 h), and avoid the detrimental effects of the Hughes Phenomenon, which seems to hamper non-deep-learning forecasters. The top-performing models, based on Convolutional Networks and Convolutional + Recurrent Neural Networks, outperform ESIOS by reducing mean absolute error by 41% and 47.58%, respectively.",
        "DOI": "10.1016/j.apenergy.2023.121645",
        "affiliation_name": "Universidad de Sevilla",
        "affiliation_city": "Sevilla",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "Energy-optimal routing for electric vehicles using deep reinforcement learning with transformer",
        "paper_author": "Tang M.",
        "publication": "Applied Energy",
        "citied_by": "21",
        "cover_date": "2023-11-15",
        "Abstract": "This paper presents an end-to-end deep reinforcement learning (DRL) approach aimed at efficiently determining energy-optimal routes for a group of electric logistic vehicles, with the objective of minimizing operating costs. First, an Energy-Minimization Electric Vehicle Routing Problem (EM-EVRP) is formulated with an energy consumption model for electric vehicles, rather than Distance Minimization EVRP commonly favored in the literature. The energy consumption model incorporates several factors such as vehicle dynamics, road information, and charging losses. Then, the problem is reformulated based on the Markov decision process and solved using the transformer-based DRL method. The policy network is designed following the Transformer structure, including an encoder, a feature embedding module, and a decoder, where the feature embedding module is added to provide contextual information. Finally, extensive experiments demonstrate the superior of the proposed DRL method over existing learning-based methods and conventional methods, in solving both EM-EVRP and DM-EVRP. Notably, the formulated EM-EVRP achieves greater cost reduction than the traditional DM-EVRP.",
        "DOI": "10.1016/j.apenergy.2023.121711",
        "affiliation_name": "National University of Singapore",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore"
    },
    {
        "paper_title": "Energy management for regional microgrids considering energy transmission of electric vehicles between microgrids",
        "paper_author": "Jiao F.",
        "publication": "Energy",
        "citied_by": "9",
        "cover_date": "2023-11-15",
        "Abstract": "As the proliferation of electric vehicles (EVs) continues to accelerate, the inherent attributes of EVs warrant meticulous consideration in the realm of energy dispatch. In order to evaluate the ability of EVs as mobile energy storage, this paper presents an energy management framework for the microgrids' online dispatch, which accounts for the spatio-temporal energy transmission of EVs between microgrids. The energy management framework contains two iterative processes: optimizing charging price and guiding charging dispatch. To sufficiently capture the uncertainties of the renewable energy and load demand, chance-constrained optimization is utilized to determine the charging price by reasonable power allocation. To achieve a continuous and efficient control policy, a normalized advantage function-deep Q learning network (NAF-DQN) is developed for EV dispatch under V2G technology. The above two processes as a coupled optimization problem are solved alternately until convergence. Numerical cases considering energy transmission of EVs between microgrids are studied to demonstrate the superiority of the proposed dispatch framework. The simulation results indicate improved computational efficiency and higher-quality solution.",
        "DOI": "10.1016/j.energy.2023.128410",
        "affiliation_name": "Beijing Institute of Technology",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Machine Learning in RIS-Assisted NOMA IoT Networks",
        "paper_author": "Zou Y.",
        "publication": "IEEE Internet of Things Journal",
        "citied_by": "17",
        "cover_date": "2023-11-15",
        "Abstract": "A reconfigurable intelligent surface (RIS)-assisted downlink nonorthogonal multiple access (NOMA) Internet of Things (IoT) network is proposed, where a Quality-of-Service (QoS)-based NOMA clustering scheme is conceived to effectively utilize the limited wireless resources among IoT devices. A throughput maximization problem is formulated by jointly optimizing the phase shifts of the RIS and the power allocation of the base station (BS) from the short-term and long-term perspectives. We aim to investigate and compare the performance of deep learning (DL) and deep reinforcement learning (DRL) algorithms for solving the formulated problems. In particular, the DL method utilizes model-agnostic-metalearning (MAML) to enhance the generalization capability of the neural network and to accelerate the convergence rate. For the DRL method, the deep deterministic policy gradient (DDPG) algorithm is employed to incorporate continuous phase-shift variables. It shows that the DL method only focuses on the maximization of the instantaneous throughput, whereas the DRL method can coordinate the power consumption over different time slots to maximize the long-term throughput. Numerical results demonstrate that: 1) the proposed QoS-based NOMA clustering scheme achieves higher IoT throughput than the conventional channel-based scheme; 2) the implementation of RISs induces approximately 5%-25% throughput gain as the number of RIS elements increases from 8 to 64; 3) DL and DRL achieve a similar throughput performance for the short-term optimization, while DRL is superior for the long-term optimization, especially when the total transmit power is limited.",
        "DOI": "10.1109/JIOT.2023.3245288",
        "affiliation_name": "Singapore University of Technology and Design",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore"
    },
    {
        "paper_title": "Evading text based emotion detection mechanism via adversarial attacks",
        "paper_author": "Bajaj A.",
        "publication": "Neurocomputing",
        "citied_by": "16",
        "cover_date": "2023-11-14",
        "Abstract": "Textual Emotion Analysis (TEA) seeks to extract and assess the emotional states of users from the text. Various Deep Learning (DL) algorithms have emerged rapidly and demonstrated success in numerous disciplines, including audio, image, and natural language processing. The trend has shifted a growing number of researchers from standard machine learning to DL for scientific study. Using DL approaches, we offer an overview of TEA in this paper. After introducing the background for emotion analysis, including the definition of emotion, emotion classification methods, and application domains of emotion analysis, we demonstrated that, despite the immense success of deep learning models in NLP-related tasks, they are susceptible to adversarial attacks, which can lead to incorrect emotion classification. An adversarial text is constructed by altering a few words or characters so as to keep the overall semantic similarity of emotion for a human reader while tricking the machine into making erroneous predictions. This study demonstrates the vulnerability of emotion categorization by generating adversarial text using a variety of cutting-edge attack techniques. Comprehensive experiments are performed to assess the effectiveness of the attack methods against several widely-used models, such as Word-CNN, Bi-LSTM, and four powerful transformer models, namely BERT, DistilBERT, ALBERT, and RoBERTa. These models were trained on an emotion dataset utilized for the purpose of emotion classification. We evaluated and analyzed the behavior of different models under a variety of attack conditions to determine which is the most and least vulnerable. Also, we determine which perturbation technique affects transformer models the most. Using Attack Success Rates (ASR) as our evaluation metric, we have assessed the potential outcomes. The findings reveal that methodologies for classifying emotion prediction can be circumvented, which has implications for existing policy measures.",
        "DOI": "10.1016/j.neucom.2023.126787",
        "affiliation_name": "Delhi Technological University",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Different firm responses to the COVID-19 pandemic shocks: machine-learning evidence on the Vietnamese labor market",
        "paper_author": "Le L.H.V.",
        "publication": "International Journal of Emerging Markets",
        "citied_by": "9",
        "cover_date": "2023-11-14",
        "Abstract": "Purpose: This paper aims to identify the disproportionate impacts of the COVID-19 pandemic on labor markets. Design/methodology/approach: The authors conduct a large-scale survey on 16,000 firms from 82 industries in Ho Chi Minh City, Vietnam, and analyze the data set by using different machine-learning methods. Findings: First, job loss and reduction in state-owned enterprises have been significantly larger than in other types of organizations. Second, employees of foreign direct investment enterprises suffer a significantly lower labor income than those of other groups. Third, the adverse effects of the COVID-19 pandemic on the labor market are heterogeneous across industries and geographies. Finally, firms with high revenue in 2019 are more likely to adopt preventive measures, including the reduction of labor forces. The authors also find a significant correlation between firms' revenue and labor reduction as traditional econometrics and machine-learning techniques suggest. Originality/value: This study has two main policy implications. First, although government support through taxes has been provided, the authors highlight evidence that there may be some additional benefit from targeting firms that have characteristics associated with layoffs or other negative labor responses. Second, the authors provide information that shows which firm characteristics are associated with particular labor market responses such as layoffs, which may help target stimulus packages. Although the COVID-19 pandemic affects most industries and occupations, heterogeneous firm responses suggest that there could be several varieties of targeted policies-targeting firms that are likely to reduce labor forces or firms likely to face reduced revenue. In this paper, the authors outline several industries and firm characteristics which appear to more directly be reducing employee counts or having negative labor responses which may lead to more cost–effect stimulus.",
        "DOI": "10.1108/IJOEM-02-2021-0292",
        "affiliation_name": "IPAG Business School",
        "affiliation_city": "Paris",
        "affiliation_country": "France"
    },
    {
        "paper_title": "Monitoring of Spatio-Temporal Carbon Stock Variation in Dudhwa Tiger Reserve, Uttar Pradesh, India using Remote Sensing and Machine Learning Based Approach",
        "paper_author": "Ghosh S.",
        "publication": "GeoWildLife 2023 - Proceedings of the 1st ACM SIGSPATIAL International Workshop on AI-driven Spatio-temporal Data Analysis for Wildlife Conservation",
        "citied_by": "0",
        "cover_date": "2023-11-13",
        "Abstract": "Temporal variation in forest cover, the largest terrestrial ecosystem on Earth, influences the climate at both local, regional, and global scales through physical, chemical, and biological processes. At the same time, forests sequester and store more carbon dioxide than any other terrestrial ecosystem and act as a “natural brake” in climate variation. Here, we have made an attempt to assess the spaio-temporal variation in forest biomass combining field-based and remote sensing and machine learning approaches. For this purpose, Fractional Vegetation Cover (FVC) layers based on Linear Spectral Unmixing (LSU) Algorithm have been developed using cloud-free multi-temporal LANDSAT data for Dudhwa Tiger Reserve, Uttra Pradesh, India from the year 2001 to 2022. Linear Regression Model (LRM) have been developed between Field based forest biomass and FVC on the basis of field data collected from 60 sampling plots of 0.1 ha across three different forest strata, namely, Very Dense Forest (VDF), Moderate Dense Forest (MDF) and Open Forest (OF). LRM indicates strong positive correlation having R2 values 0.718 for VDF, 0.73 for MDF and 0.76 for OF forest strata. Also, the predicted biomass thus obtained shows strong positive correlation with observed biomass. Results highlights that in VDF, carbon stock shows a decreasing trend till 2018 (332 t/ ha) since the year 2001 (347 t/ha) before further increase during present year (339 t/ha). Simultaneously, temporal variation in FVC also suggests the same trend for the forest cover under VDF strata which is playing pivotal role in increasing trend of forest carbon stock from the 2018 onwards. Also, we have compared between best possible FVC model based on three vegetation index (NDVI, MSAVI and EVI) which highlights the FVC model based on NDVI shows highly significant correlation (R2=0.73, p<0.005) with the field-based forest biomass. Degradation matrix also developed using the temporal FVC layers for the delineation of degradation patches and trend analysis of forest degradation. Outcome of the paper will be helpful for the policy makers in visualizing proper development plan to regulate the Land-use and forest cover dynamics for achieving the higher carbon sequestration rate which would in turn helps to maintain the balance in the global climate scenerio.",
        "DOI": "10.1145/3615893.3628761",
        "affiliation_name": "The Energy and Resources Institute India",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Mirage: Towards Low-interruption Services on Batch GPU Clusters with Reinforcement Learning",
        "paper_author": "Ding Q.",
        "publication": "Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2023",
        "citied_by": "1",
        "cover_date": "2023-11-12",
        "Abstract": "Accommodating long-running deep learning (DL) training and inference jobs is challenging on GPU clusters that use traditional batch schedulers, such as Slurm. Given fixed wall clock time limits, DL researchers usually need to run a sequence of batch jobs and experience long interruptions on overloaded machines. Such interruptions significantly lower the research productivity and QoS for services that are deployed in production. To mitigate the issues from interruption, we propose the design of a proactive provisioner and investigate a set of statistical learning and reinforcement learning (RL) techniques, including random forest, xgboost, Deep Q-Network, and policy gradient. Using production job traces from three GPU clusters, we train each model using a subset of the trace and then evaluate their generality using the remaining validation subset. We introduce Mirage, a Slurm-compatible resource provisioner that integrates the candidate ML methods. Our experiments show that the Mirage can reduce interruption by 17 - 100% and safeguard 23%-76% of jobs with zero interruption across varying load levels on the three clusters.",
        "DOI": "10.1145/3581784.3607042",
        "affiliation_name": "University of Wisconsin-Madison",
        "affiliation_city": "Madison",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Intrusion detection system based on machine learning models: An empirical analysis",
        "paper_author": "Panjaitan M.B.",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2023-11-10",
        "Abstract": "As of the exponential increase in the use of computer networks, there are now problems associated with maintaining the network's availability, integrity, and secrecy. Because of this, network administrators have no choice but to implement a wide variety of intrusion detection systems (IDS), which are designed to assist in monitoring network traffic to identify unauthorized and hostile actions. A security policy violation with the intention to harm is known as an intrusion. As a result, an intrusion detection system will monitor the traffic moving through computer systems on a network to check for malicious actions and known dangers. When it discovers such threats, the system will send up an alarm. There are two types of attacks that can be identified by an intrusion detection system: signature-based detection and misuse detection. In signature-based detection, an IDS uses information gathered from a database to analyze and compare the attack signatures to those that have been saved. The second type of detection is called anomaly detection, which considers the likelihood of a particular action happening outside the typical pattern of behavior. This paper aims to provide an overview of the various efforts being carried out to develop an effective IDS using machine learning and deep learning. The results of the study will be used to evaluate the performance of different classifiers. In addition, the paper also presents the results of the various studies that were carried out. These findings will be used to develop further improvements and enhance the performance of the IDS.",
        "DOI": "10.1063/5.0180641",
        "affiliation_name": "Muscat College",
        "affiliation_city": "Muscat",
        "affiliation_country": "Oman"
    },
    {
        "paper_title": "Explainability and human intervention in autonomous scanning probe microscopy",
        "paper_author": "Liu Y.",
        "publication": "Patterns",
        "citied_by": "7",
        "cover_date": "2023-11-10",
        "Abstract": "The broad adoption of machine learning (ML)-based autonomous experiments (AEs) in material characterization and synthesis requires strategies development for understanding and intervention in the experimental workflow. Here, we introduce and realize a post-experimental analysis strategy for deep kernel learning-based autonomous scanning probe microscopy. This approach yields real-time and post-experimental indicators for the progression of an active learning process interacting with an experimental system. We further illustrate how this approach can be applied to human-in-the-loop AEs, where human operators make high-level decisions at high latencies setting the policies for AEs, and the ML algorithm performs low-level, fast decisions. The proposed approach is universal and can be extended to other techniques and applications such as combinatorial library analysis.",
        "DOI": "10.1016/j.patter.2023.100858",
        "affiliation_name": "Tickle College of Engineering",
        "affiliation_city": "Knoxville",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Significantly mitigating PM<inf>2.5</inf> pollution level via reduction of NO<inf>x</inf> emission during wintertime",
        "paper_author": "Fu S.",
        "publication": "Science of the Total Environment",
        "citied_by": "3",
        "cover_date": "2023-11-10",
        "Abstract": "Despite considerable decreases in fine particulate matter (PM2.5) in Chinese megacities over the past decade, many second- and third-tier cities that distribute abundant industrial enterprises are still facing great challenges for PM2.5 further reduction under the recent policy background of eliminating heavily-polluted weather. In view of core effects of NOx on PM2.5, the deeper reductions of NOx in these cities are expected to break the plateau of PM2.5 decline, however, the link between NOx emission and PM2.5 mass loading is currently lacking. Herein, we progressively construct an evaluation system for PM2.5 productions based on daily NOx emissions in a typical industrial city (Jiyuan), considering a sequence of nested parameters involving evolutions of NO2 into nitric acid and then nitrate, and contributions of nitrate to PM2.5. The evaluation system was subsequently validated to better reproduce real increasing processes for PM2.5 pollution based on 19 pollution cases, with root mean square errors of 19.2 ± 16.4 %, suggesting the feasibility of developing NOx emission indicators linked to goals of mitigating atmospheric PM2.5. Additionally, further comparative results reveal that currently high NOx emissions in this industrial city severely hinder the achievement of atmospheric PM2.5 environmental capacity targets, especially in the scenarios of high initial PM2.5 level, low planetary boundary layer height and long pollution duration. It is anticipated that these methodologies and findings would supply guidelines for further regional PM2.5 mitigation, in which source-oriented NOx indicators could also provide some orientations for industrial cleaner production such as denitrification and low nitrogen combustion.",
        "DOI": "10.1016/j.scitotenv.2023.165350",
        "affiliation_name": "University of Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Using machine learning and remote sensing to track land use/land cover changes due to armed conflict",
        "paper_author": "Mhanna S.",
        "publication": "Science of the Total Environment",
        "citied_by": "22",
        "cover_date": "2023-11-10",
        "Abstract": "Armed conflicts have detrimental impacts on the environment, including land systems. The prevailing understanding of the relation between Land Use/Land Cover (LULC) and armed conflict fails to fully recognize the complexity of their dynamics – a shortcoming that could undermine food security and sustainable land/water resources management in conflict settings. The Syrian portion of the transboundary Orontes River Basin (ORB) has been a site of violent conflict since 2013. Correspondingly, the Lebanese and Turkish portions of the ORB have seen large influxes of refugees. A major challenge in any geoscientific investigation in this region, specifically the Syrian portion, is the unavailability of directly-measured “ground truth” data. To circumvent this problem, we develop a novel methodology that combines remote sensing products, machine learning techniques and quasi-experimental statistical analysis to better understand LULC changes in the ORB between 2004 and 2022. Through analysis of the resulting annual LULC maps, we can draw several quantitative conclusions. Cropland areas decreased by 21–24 % in Syria's conflict hotspot zones after 2013, whereas a 3.4-fold increase was detected in Lebanon. The development of refugee settlements was also tracked in Lebanon and on the Syrian/Turkish borders, revealing different LULC patterns that depend on settlement dynamics. The results highlight the importance of understanding the heterogenous spatio-temporal LULC changes in conflict-affected and refugee-hosting countries. The developed methodology is a flexible, cloud-based approach that can be applied to wide variety of LULC investigations related to conflict, policy and climate.",
        "DOI": "10.1016/j.scitotenv.2023.165600",
        "affiliation_name": "Université de Neuchâtel, Le Centre d'Hydrogéologie et de Géothermie",
        "affiliation_city": "Neuchatel",
        "affiliation_country": "Switzerland"
    },
    {
        "paper_title": "Using social media information to predict the credit risk of listed enterprises in the supply chain",
        "paper_author": "Yao G.",
        "publication": "Kybernetes",
        "citied_by": "3",
        "cover_date": "2023-11-09",
        "Abstract": "Purpose: Social media data from financial websites contain information related to enterprise credit risk. Mining valuable new features in social media data helps to improve prediction performance. This paper proposes a credit risk prediction framework that integrates social media information to improve listed enterprise credit risk prediction in the supply chain. Design/methodology/approach: The prediction framework includes four stages. First, social media information is obtained through web crawler technology. Second, text sentiment in social media information is mined through natural language processing. Third, text sentiment features are constructed. Finally, the new features are integrated with traditional features as input for models for credit risk prediction. This paper takes Chinese pharmaceutical enterprises as an example to test the prediction framework and obtain relevant management enlightenment. Findings: The prediction framework can improve enterprise credit risk prediction performance. The prediction performance of text sentiment features in social media data is better than that of most traditional features. The time-weighted text sentiment feature has the best prediction performance in mining social media information. Practical implications: The prediction framework is helpful for the credit decision-making of credit departments and the policy regulation of regulatory departments and is conducive to the sustainable development of enterprises. Originality/value: The prediction framework can effectively mine social media information and obtain an excellent prediction effect of listed enterprise credit risk in the supply chain.",
        "DOI": "10.1108/K-12-2021-1376",
        "affiliation_name": "Hefei University of Technology",
        "affiliation_city": "Hefei",
        "affiliation_country": "China"
    },
    {
        "paper_title": "GAA-PPO: A novel graph adversarial attack method by incorporating proximal policy optimization",
        "paper_author": "Yang S.",
        "publication": "Neurocomputing",
        "citied_by": "5",
        "cover_date": "2023-11-07",
        "Abstract": "The Graph Convolutional Network (GCN) has demonstrated impressive performance in processing graph structured data. However recent studies have revealed that GCN is vulnerable to adversarial attacks, where a small amount of data modification can significantly affect the performance of the GCN models. While most existing studies node injection attacks with graph reinforcement learning by considering gradient information, they still suffer from the problems that the step size of the policy gradient is difficult to determine, and the attack effect needs to be further improved. In light of the above issues, this paper proposes a Graph Adversarial Attack method by incorporating Proximal Policy Optimization named GAA-PPO, which fills subtasks of sequentially generating features and links for injected nodes without modifying existing nodes or edges. GAA-PPO comprises two main components: node injection attack network (actor network) and value prediction network (critic network). Specifically, the actor network leverages a node generator and an edge sampler to generate appropriate features and edges for the injected nodes. Notably, a novel edge sampler that incorporates Approximation Personalized Propagation of Neural Prediction (APPNP) is introduced to effectively propagate malicious features of the injected nodes. On the other hand, the critic network evaluates the performance of the perturbed graph at each stage. To enhance the stability of the algorithm, GAA-PPO employs the importance sampling technique of Proximal Policy Optimization (PPO) during the training process. Extensive experiments on three publicly benchmark datasets show that GAA-PPO yields significant performance advantages over the state-of-the-art method.",
        "DOI": "10.1016/j.neucom.2023.126707",
        "affiliation_name": "Jiangxi University of Science and Technology",
        "affiliation_city": "Ganzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Relay Hindsight Experience Replay: Self-guided continual reinforcement learning for sequential object manipulation tasks with sparse rewards",
        "paper_author": "Luo Y.",
        "publication": "Neurocomputing",
        "citied_by": "14",
        "cover_date": "2023-11-07",
        "Abstract": "Learning with sparse rewards remains a challenging problem in reinforcement learning (RL). In particular, for sequential object manipulation tasks, the RL agent generally only receives a reward upon successful completion of the entire task, leading to low exploration efficiency. To address this sample inefficiency, we propose a novel self-guided continual RL framework, named Relay Hindsight Experience Replay (RHER). RHER decomposes a sequential task into several subtasks with increasing complexity, allowing the agent to learn from the simplest subtask and gradually complete the task. It is crucial that a Self-Guided Exploration Strategy (SGES) is proposed to use the already-learned simpler subtask policy to guide the exploration of a more complex subtask. This strategy allows the agent to break the barriers of sparse reward sequential tasks and achieve efficient learning stage by stage. As a result, the proposed RHER method achieves state-of-the-art performance on the benchmark tasks (FetchPush and FetchPickAndPlace). Furthermore, the experimental results demonstrate the superiority and high efficiency of RHER on a variety of single-object and multi-object manipulation tasks (e.g., ObstaclePush, DrawerBox, TStack, etc.). Finally, the proposed RHER method can also learn a contact-rich task on a real robot from scratch within 250 episodes.",
        "DOI": "10.1016/j.neucom.2023.126620",
        "affiliation_name": "Institute of Intelligent Machines Chinese Academy of Sciences",
        "affiliation_city": "Hefei",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Russo-Ukrainian War: Prediction and explanation of Twitter suspension",
        "paper_author": "Shevtsov A.",
        "publication": "Proceedings of the 2023 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2023",
        "citied_by": "1",
        "cover_date": "2023-11-06",
        "Abstract": "On 24 February 2022, Russia invaded Ukraine, starting what is now known as the Russo-Ukrainian War, initiating an online discourse on SNs. Twitter one of the most popular SNs, with an open and democratic character, enables a transparent discussion among its large user base. Unfortunately, this often leads to Twitter's policy violations, propaganda, abusive actions, civil integrity violations, and consequently to user accounts' suspension and deletion. This study focuses on the Twitter suspension mechanism and the analysis of shared content and features leading to an accurate machine-learning suspension prediction. Toward this goal, we have obtained a dataset containing 107.7M tweets, originating from 9.8 million users, using Twitter API. We extract the categories of shared content of the suspended accounts and explain their characteristics, through the extraction of text embeddings in junction with cosine similarity clustering. Our results reveal scam campaigns taking advantage of trending topics regarding the Russia-Ukrainian conflict for Bitcoin and Ethereum fraud, spam, and advertisement campaigns. Additionally, we apply a ML methodology including a SHapley Additive explainability model to understand and explain how user accounts get suspended.",
        "DOI": "10.1145/3625007.3627317",
        "affiliation_name": "Institute of Computer Science",
        "affiliation_city": "Heraklion",
        "affiliation_country": "Greece"
    },
    {
        "paper_title": "PolicyClusterGCN: Identifying Efficient Clusters for Training Graph Convolutional Networks",
        "paper_author": "Gurukar S.",
        "publication": "Proceedings of the 2023 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2023",
        "citied_by": "0",
        "cover_date": "2023-11-06",
        "Abstract": "Graph convolutional networks (GCNs) have achieved huge success in several machine learning (ML) tasks on graph-structured data. Recently, several sampling techniques have been proposed for the efficient training of GCNs and to improve the performance of GCNs on ML tasks. Specifically, the subgraph-based sampling approaches such as ClusterGCN and GraphSAINT have achieved state-of-the-art performance on the node classification tasks. These subgraph-based sampling approaches rely on heuristics - such as graph partitioning via edge cuts - to identify clusters that are then treated as minibatches during GCN training. In this work, we hypothesize that rather than relying on such heuristics, one can learn a reinforcement learning (RL) policy to compute efficient clusters that lead to effective GCN performance. To that end, we propose PolicyClusterGCN, an online RL framework that can identify good clusters for GCN training. We develop a novel Markov Decision Process (MDP) formulation that allows the policy network to predict \"importance\"weights on the edges which are then utilized by a clustering algorithm (Graclus) to compute the clusters. We train the policy network using a standard policy gradient algorithm where the rewards are computed from the classification accuracies while training GCN using clusters given by the policy. Experiments on six real-world datasets and several synthetic datasets show that PolicyClusterGCN outperforms existing state-of-the-art models on node classification task.",
        "DOI": "10.1145/3625007.3627499",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Columbus",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Challenges of machine learning and AI (What is next?), Responsible and ethical AI",
        "paper_author": "Gkontra P.",
        "publication": "Clinical Applications of Artificial Intelligence in Real-World Data",
        "citied_by": "4",
        "cover_date": "2023-11-04",
        "Abstract": "Research in medical artificial intelligence (AI) is experiencing an explosive growth. This growth highlights the potential of AI to significantly improve healthcare across a wide spectrum of applications such as risk stratification, diagnosis, therapeutics, and resource management among others. However, despite the great promises of medical AI and recent technological advancements, a gap persists in translating and deploying AI solutions within clinical settings. This gap is attributed to the risks and challenges that these promising technologies entail. To bring AI one step closer to the real-word clinical practice, we identify and outline the principal clinical, ethical and socio-ethical risks associated with AI in healthcare, unravelling their potential sources. These risks include potential errors leading to patient harm, risk of bias causing exacerbated health disparities, lack of transparency and trust, as well as susceptibilities to hacking and data privacy breaches. Furthermore, we discuss approaches towards minimizing risks and developing tools that can be safely deployed and routinely used in the clinic. Moreover, we introduce a set of concrete recommendations aimed at mitigating risks and maximizing the advantages presented by medical AI. These recommendations include fostering multi-stakeholder engagement throughout the AI production lifecycle, increased transparency and traceability, exhaustive clinical validation of AI tools, and comprehensive AI training and education for both medical practitioners and the general public. The adoption of such policies stands to significantly influence the trajectory and deployment of AI within clinical practice.",
        "DOI": "10.1007/978-3-031-36678-9_17",
        "affiliation_name": "Imperial College Faculty of Medicine",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "A novel fractional Hausdorff grey system model and its applications",
        "paper_author": "Xie W.",
        "publication": "Journal of Intelligent and Fuzzy Systems",
        "citied_by": "1",
        "cover_date": "2023-11-04",
        "Abstract": "Grey system models have proven to be effective techniques in diverse fields and are crucial to global decision science. Amongst the various approaches of grey theory, the fractional-order grey model is fundamental and extends the cumulative generation method used in grey theory. Fractional-order cumulative generating operator offers numerous significant benefits, especially in educational funding that is often influenced by economic policies. However, their computational complexity complicates the generalization of fractional-order operators in real-world scenarios. In this paper, an enhanced fractional-order grey model is proposed based on a new fractional-order accumulated generating operator. The newly introduced model estimates parameters by utilizing the method of least squares and determines the order of the model through the implementation of metaheuristic algorithms. Our results show that, after conducting both Monte Carlo simulations and practical case analyses, the newly proposed model outperforms both existing grey prediction models and machine learning models in small sample environments, thus demonstrating superior forecast accuracy. Moreover, our experiments reveal that the proposed model has a simpler structure than previously developed grey models and achieves greater prediction accuracy.",
        "DOI": "10.3233/JIFS-230121",
        "affiliation_name": "Qufu Normal University",
        "affiliation_city": "Qufu",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Garbage in, garbage out: mitigating risks and maximizing benefits of AI in research",
        "paper_author": "Hanson B.",
        "publication": "Nature",
        "citied_by": "17",
        "cover_date": "2023-11-02",
        "Abstract": "Artificial-intelligence tools are transforming data-driven science — better ethical standards and more robust data curation are needed to fuel the boom and prevent a bust.",
        "DOI": "10.1038/d41586-023-03316-8",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Why the UK-led global AI summit is missing the point",
        "paper_author": "NA",
        "publication": "Nature",
        "citied_by": "1",
        "cover_date": "2023-11-02",
        "Abstract": "Robust regulation of AI technologies will be crucial to protecting against harms. Researchers’ voices must be heard. [Figure not available: see fulltext.]",
        "DOI": "10.1038/d41586-023-03333-7",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Prediction of Water Level in Lakes by RNN-Based Deep Learning Algorithms to Preserve Sustainability in Changing Climate and Relationship to Microcystin",
        "paper_author": "Ozdemir S.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "4",
        "cover_date": "2023-11-01",
        "Abstract": "In recent years, intensive water use combined with global climate change has increased fluctuations in freshwater lake levels, hydrological characteristics, water quality, and water ecosystem balance. To provide a sustainable management plan in the long term, deep learning models (DL) can provide fast and reliable predictions of lake water levels (LWLs) in challenging future scenarios. In this study, artificial neural networks (ANNs) and four recurrent neural network (RNN) algorithms were investigated to predict LWLs that were applied in time series such as one day, five days, ten days, twenty days, one month, two months, and four months ahead. The results show that the performance of the Long Short-Term Memory (LSTM) model with a prediction of 60 days is in the very good range and outperforms the benchmark, the Naïve Method, by 78% and the ANN at the significance level (p < 0.05) with an RMSE = 0.1762 compared to other DL algorithms. The RNN-based DL algorithms show better prediction performance, specifically, for long time horizons, 57.98% for 45 days, 78.55% for 60 days, and 58% for 120 days, and it is better to use a prediction period of at least 20 days with an 18.45% performance increase to take advantage of the gated RNN algorithms for predicting future water levels. Additionally, microcystin concentration was tightly correlated with temperature and was most elevated between 15 and 20 m water depths during the summer months. Evidence on LWL forecasting and microcystin concentrations in the context of climate change could help develop a sustainable water management plan and long-term policy for drinking water lakes.",
        "DOI": "10.3390/su152216008",
        "affiliation_name": "Middle East Technical University (METU)",
        "affiliation_city": "Ankara",
        "affiliation_country": "Turkey"
    },
    {
        "paper_title": "AI-Based Resource Allocation in E2E Network Slicing with Both Public and Non-Public Slices",
        "paper_author": "Wang Y.",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "1",
        "cover_date": "2023-11-01",
        "Abstract": "Network slicing is a key technology for 5G networks, which divides the traditional physical network into multiple independent logical networks to meet the diverse requirements of end-users. This paper focuses on the resource allocation problem in the scenario where public and non-public network slices coexist. There are two kinds of resources to be allocated: one is the resource blocks (RBs) allocated to the users in the radio access network, and the other is the server resources in the core network. We first formulate the above resource allocation problem as a nonlinear integer programming problem by maximizing the operator profit as the objective function. Then, a combination of deep reinforcement learning (DRL) and machine learning (ML) algorithms are used to solve this problem. DRL, more specifically, independent proximal policy optimization (IPPO), is employed to provide the RB allocation scheme that makes the objective function as large as possible. ML, more specifically, random forest (RF), assists DRL agents in receiving fast reward feedback by determining whether the allocation scheme is feasible. The simulation results show that the IPPO-RF algorithm has good performance, i.e., not only are all the constraints satisfied, but the requirements of the non-public network slices are ensured.",
        "DOI": "10.3390/app132212505",
        "affiliation_name": "Purple Mountain Laboratory",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Natural Language Processing Adoption in Governments and Future Research Directions: A Systematic Review",
        "paper_author": "Jiang Y.",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "6",
        "cover_date": "2023-11-01",
        "Abstract": "Natural language processing (NLP), which is known as an emerging technology creating considerable value in multiple areas, has recently shown its great potential in government operations and public administration applications. However, while the number of publications on NLP is increasing steadily, there is no comprehensive review for a holistic understanding of how NLP is being adopted by governments. In this regard, we present a systematic literature review on NLP applications in governments by following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) protocol. The review shows that the current literature comprises three levels of contribution: automation, extension, and transformation. The most-used NLP techniques reported in government-related research are sentiment analysis, machine learning, deep learning, classification, data extraction, data mining, topic modelling, opinion mining, chatbots, and question answering. Data classification, management, and decision-making are the most frequently reported reasons for using NLP. The salient research topics being discussed in the literature can be grouped into four categories: (1) governance and policy, (2) citizens and public opinion, (3) medical and healthcare, and (4) economy and environment. Future research directions should focus on (1) the potential of chatbots, (2) NLP applications in the post-pandemic era, and (3) empirical research for government work.",
        "DOI": "10.3390/app132212346",
        "affiliation_name": "The State University of New York, Korea",
        "affiliation_city": "Incheon",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "From Pixels to Sustainability: Trends and Collaborations in Remote Sensing for Advancing Sustainable Cities and Communities (SDG 11)",
        "paper_author": "Ekmen O.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "2",
        "cover_date": "2023-11-01",
        "Abstract": "Remote sensing data and methods have become indispensable for observing and modeling the Earth and have great potential for monitoring a substantial portion of the targets defined under the United Nations Sustainable Development Goals (SDGs). This study investigates remote sensing research on SDG 11 (sustainable cities and communities) from 2016 to 2023, highlighting the growing interest in the field. By evaluating a large number of selected articles (6820) using a specialized keyword selection strategy and various filters, a significant increase in publication frequency was observed. Remote Sensing and Sustainability were found to be the most relevant journals. A trend towards research addressing urban ecological quality, changes in land use patterns, and the impact of impervious surfaces was found in domain-specific citations. Semi-niche motor themes encompass deep learning, feature extraction, and semantic segmentation. Simultaneously, remote sensing, machine learning, and change detection serve as foundational motor themes, merging elements of both basic and motor themes. The introduction of new analytical methods (e.g., new indices), together with the use of open data and crowdsourcing, has gained great interest. While there has been a strong focus on land cover, urban expansion, and land surface temperature, the main gaps were identified in regional development, disaster, resilience, natural and cultural heritage, housing, and inclusiveness. The findings show the significance of remote sensing research and its practical applications for shaping urban policy, planning strategies, and sustainable urban development. By extracting research patterns using centrality and density analyses and identifying underexplored areas, valuable insights into relationships, significance, and developmental progress within SDG 11-related remote sensing research were gained and may contribute to future planning and informing policymaking decisions.",
        "DOI": "10.3390/su152216094",
        "affiliation_name": "Hacettepe Üniversitesi",
        "affiliation_city": "Ankara",
        "affiliation_country": "Turkey"
    },
    {
        "paper_title": "Healthcare Sustainability: Hospitalization Rate Forecasting with Transfer Learning and Location-Aware News Analysis",
        "paper_author": "Chen J.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "2",
        "cover_date": "2023-11-01",
        "Abstract": "Monitoring and forecasting hospitalization rates are of essential significance to public health systems in understanding and managing overall healthcare deliveries and strategizing long-term sustainability. Early-stage prediction of hospitalization rates is crucial to meet the medical needs of numerous patients during emerging epidemic diseases such as COVID-19. Nevertheless, this is a challenging task due to insufficient data and experience. In addition, relevant existing work neglects or fails to exploit the extensive contribution of external factors such as news, policies, and geolocations. In this paper, we demonstrate the significant relationship between hospitalization rates and COVID-19 infection cases. We then adapt a transfer learning architecture with dynamic location-aware sentiment and semantic analysis (TLSS) to a new application scenario: hospitalization rate prediction during COVID-19. This architecture learns and transfers general transmission patterns of existing epidemic diseases to predict hospitalization rates during COVID-19. We combine the learned knowledge with time series features and news sentiment and semantic features in a dynamic propagation process. We conduct extensive experiments to compare the proposed approach with several state-of-the-art machine learning methods with different lead times of ground truth. Our results show that TLSS exhibits outstanding predictive performance for hospitalization rates. Thus, it provides advanced artificial intelligence (AI) techniques for supporting decision-making in healthcare sustainability.",
        "DOI": "10.3390/su152215840",
        "affiliation_name": "Stevens Institute of Technology",
        "affiliation_city": "Hoboken",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Auto-Evaluation Model for the Prediction of Building Energy Consumption That Combines Modified Kalman Filtering and Long Short-Term Memory",
        "paper_author": "Yang F.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "4",
        "cover_date": "2023-11-01",
        "Abstract": "As the world grapples with the challenges posed by climate change and depleting energy resources, achieving sustainability in the construction and operation of buildings has become a paramount concern. The construction and operation of buildings account for a substantial portion of global energy consumption and carbon emissions. Hence, the accurate prediction of building energy consumption is indispensable for reducing energy waste, minimizing greenhouse gas emissions, and fostering sustainable urban development. The aspiration to achieve predicted outcomes with remarkable accuracy has emerged as a pivotal objective, coinciding with the burgeoning popularity of deep learning techniques. This paper presents an auto-evaluation model for building energy consumption prediction via Long Short-Term Memory with modified Kalman filtering (LSTM-MKF). Results gleaned from data validation activities evince a notable transformation—a reduction of the maximal prediction error from an initial 83% to a markedly ameliorated 24% through the intervention of the proposed model. The LSTM-MKF model, a pioneering contribution within this paper, clearly exhibits a distinct advantage over the other models in terms of predictive accuracy, as underscored by its superior performance in all three key metrics, including mean absolute error, root mean square error, and mean square error. The model presents excellent potential as a valuable tool for enhancing the precision of predictions of building energy consumption, a pivotal aspect in energy efficiency, smart city development, and the formulation of informed energy policy.",
        "DOI": "10.3390/su152215749",
        "affiliation_name": "The Hong Kong Polytechnic University",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong"
    },
    {
        "paper_title": "Optimal Charging Station Placement and Scheduling for Electric Vehicles in Smart Cities",
        "paper_author": "Alanazi F.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "6",
        "cover_date": "2023-11-01",
        "Abstract": "Electric vehicles (EVs) have emerged as a transformative solution for reducing carbon emissions and promoting environmental sustainability in the automotive industry. However, the widespread adoption of EVs in the United States faces challenges, including high costs and unequal access to charging infrastructure. To overcome these barriers and ensure equitable EV usage, a comprehensive understanding of the intricate interplay among social, economic, and environmental factors influencing the placement of charging stations is crucial. This study investigates the key variables that contribute to demographic disparities in the accessibility of EV charging stations (EVCSs). We analyze the impact of various factors, including EV percentage, geographic area, population density, available electric vehicle supply equipment (EVSE) ports, electricity sources, energy costs, per capita and average family income, traffic patterns, and climate, on the placement of EVCSs in nine selected US states. Furthermore, we employ predictive modeling techniques, such as linear regression and support vector machine, to explore unique nuances in EVCS installation. By leveraging real-world data from these states and the identified variables, we forecast the future distribution of EVCSs using machine learning. The linear regression model demonstrates exceptional effectiveness, achieving 90% accuracy, 94% precision, 89% recall, and a 91% F1 score. Both graphical analysis and machine learning converge on a significant finding: Texas emerges as the most favorable state for optimal EVCS placement among the studied areas. This research enhances our understanding of the multifaceted dynamics that govern the accessibility of EVCSs, thereby informing the development of policies and strategies to accelerate EV adoption, reduce emissions, and promote social inclusivity.",
        "DOI": "10.3390/su152216030",
        "affiliation_name": "Jouf University",
        "affiliation_city": "Sakakah",
        "affiliation_country": "Saudi Arabia"
    },
    {
        "paper_title": "A Prediction Hybrid Framework for Air Quality Integrated with W-BiLSTM(PSO)-GRU and XGBoost Methods",
        "paper_author": "Chang W.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "12",
        "cover_date": "2023-11-01",
        "Abstract": "Air quality issues are critical to daily life and public health. However, air quality data are characterized by complexity and nonlinearity due to multiple factors. Coupled with the exponentially growing data volume, this provides both opportunities and challenges for utilizing deep learning techniques to reveal complex relationships in massive knowledge from multiple sources for correct air quality prediction. This paper proposes a prediction hybrid framework for air quality integrated with W-BiLSTM(PSO)-GRU and XGBoost methods. Exploiting the potential of wavelet decomposition and PSO parameter optimization, the prediction accuracy, stability and robustness was improved. The results indicate that the R2 values of PM2.5, PM10, SO2, CO, NO2, and O3 predictions exceeded 0.94, and the MAE and RMSE values were lower than 0.02 and 0.03, respectively. By integrating the state-of-the-art XGBoost algorithm, meteorological data from neighboring monitoring stations were taken into account to predict air quality trends, resulting in a wider range of forecasts. This strategic merger not only enhanced the prediction accuracy, but also effectively solved the problem of sudden interruption of monitoring. Rigorous analysis and careful experiments showed that the proposed method is effective and has high application value in air quality prediction, building a solid framework for informed decision-making and sustainable development policy formulation.",
        "DOI": "10.3390/su152216064",
        "affiliation_name": "Beihang University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Research on Experience Replay of Off-policy Deep Reinforcement Learning: A Review",
        "paper_author": "Hu Z.J.",
        "publication": "Zidonghua Xuebao/Acta Automatica Sinica",
        "citied_by": "3",
        "cover_date": "2023-11-01",
        "Abstract": "As a machine learning method that does not need to obtain training data in advance, reinforcement learning (RL) is an important method to solve the sequential decision-making problem by finding the optimal strategy in the continuous interaction between the agent and the environment. Through the combination of deep learning (DL), deep reinforcement learning (DRL) has both powerful perception and decision-making capabilities, and is widely used in many fields to solve complex decision-making problems. Off-policy reinforcement learning separates exploration and utilization by storing and replaying interactive experience, making it easier to find the global optimal solution. How to make reasonable and efficient use of experience is the key to improve the efficiency of off-policy reinforcement learning methods. First, this paper introduces the basic theory of reinforcement learning. Then, the on-policy and off-policy reinforcement learning algorithms are briefly introduced. Next, two mainstream solutions of experience replay (ER) problem are introduced, including experience utilization and experience expansion. Finally, the relevant research work is summarized and prospected.",
        "DOI": "10.16383/j.aas.c220648",
        "affiliation_name": "Moscow Aviation Institute (National Research University)",
        "affiliation_city": "Moscow",
        "affiliation_country": "Russian Federation"
    },
    {
        "paper_title": "A Human-machine Collaborative Control Algorithm for Intelligent Vehicles Based on Model Prediction and Policy Learning",
        "paper_author": "Jiang Y.",
        "publication": "Binggong Xuebao/Acta Armamentarii",
        "citied_by": "0",
        "cover_date": "2023-11-01",
        "Abstract": "A human-machine collaborative control algorithm based on model prediction and policy learning is proposed for the optimal decision-making and high maneuvering motion control of intelligent vehicles in complex environments. The algorithm takes advantage of the human driver’s understanding of the environment and comprehensive processing ability to assist the machine in local trajectory planning at the decision planning level, including speed adjustment and dynamic path generation, to achieve the human-machine collaboration . For the timeliness of the online optimal planning and control of vehicles with high maneuverability, on the one hand, a long sampling interval and a simplified dynamics model are used to design a local trajectory planning method based on model predictive control at the local planning level in order to achieve efficient online trajectory optimization. On the other hand, a learning-based predictive control method based on rolling time-domain reinforcement learning is used to optimize the control strategy in the control layer in order to improve the computational efficiency and adaptability of online optimal control. In the driving simulation on the mountain highway with the driver in the loop, the proposed method not only complies with the driver’s acceleration and deceleration commands and steering commands to generate a safe and smooth planning trajectory for human-machine cooperation, but also can accurately control the vehicle to travel along the desired trajectory in real time. In the human-machine cooperative control mode, the time to complete the same driving task is reduced by 8. 3% on average and the steering operation load is reduced by 51. 1% compared with the manual driving by six ordinary drivers.",
        "DOI": "10.12382/bgxb.2022.0815",
        "affiliation_name": "National University of Defense Technology China",
        "affiliation_city": "Changsha",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Compound Rules and Reinforcement Learning Based Scheduling Method for Mixed Model Assembly Lines",
        "paper_author": "Guo J.",
        "publication": "Zhongguo Jixie Gongcheng/China Mechanical Engineering",
        "citied_by": "2",
        "cover_date": "2023-11-01",
        "Abstract": "A scheduling method was proposed based on compound rules and reinforcement learning for balancing and sequencing problems of mixed model assembly lines. A balancing rule set and a sequencing rule set were designed with the consideration of mathematical model, and a proximal policy optimization（P P O ） algorithm featured with Actor-Critic training procedure and preferential experience learning mechanism was employed to regulate weighted parameters of these rules, in order to generate reasonable balancing and sequencing solutions. In comparative experiments, the proposed scheduling method demonstrates the effectiveness over other methods including P P O algorithm with single rule, compound rules, and a genetic algorithm.",
        "DOI": "10.3969/j.issn.1004-132X.2023.21.009",
        "affiliation_name": "Nanjing University of Aeronautics and Astronautics",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Predicting health insurance uptake in Kenya using Random Forest: An analysis of socioeconomic and demographic factors",
        "paper_author": "Yego N.K.K.",
        "publication": "PLoS ONE",
        "citied_by": "0",
        "cover_date": "2023-11-01",
        "Abstract": "Universal Health Coverage (UHC) is a global objective aimed at providing equitable access to essential and cost-effective healthcare services, irrespective of individuals’ financial circumstances. Despite efforts to promote UHC through health insurance programs, the uptake in Kenya remains low. This study aimed to explore the factors influencing health insurance uptake and offer insights for effective policy development and outreach programs. The study utilized machine learning techniques on data from the 2021 FinAccess Survey. Among the models examined, the Random Forest model demonstrated the highest performance with notable metrics, including a high Kappa score of 0.9273, Recall score of 0.9640, F1 score of 0.9636, and Accuracy of 0.9636. The study identified several crucial predictors of health insurance uptake, ranked in ascending order of importance by the optimal model, including poverty vulnerability, social security usage, income, education, and marital status. The results suggest that affordability is a significant barrier to health insurance uptake. The study highlights the need to address affordability challenges and implement targeted interventions to improve health insurance uptake in Kenya, thereby advancing progress towards achieving Universal Health Coverage (UHC) and ensuring universal access to quality healthcare services.",
        "DOI": "10.1371/journal.pone.0294166",
        "affiliation_name": "University of Rwanda",
        "affiliation_city": "Butare",
        "affiliation_country": "Rwanda"
    },
    {
        "paper_title": "Bibliometric Insights into the Implications of Urban Built Environment on Travel Behavior",
        "paper_author": "Gao C.",
        "publication": "ISPRS International Journal of Geo-Information",
        "citied_by": "3",
        "cover_date": "2023-11-01",
        "Abstract": "With the rapid pace of global urbanization, understanding the impact of the urban built environment on travel behavior has become increasingly significant for developing sustainable and efficient transportation systems. This study conducts a bibliometric review of related research over the past two decades (1997–2023), utilizing 1745 publications from the Web of Science database through network analysis and content analysis. It provides a comprehensive quantitative analysis encompassing publication trends, national and institutional collaborations, and keyword evolution clustering perspectives. The results reveal that (1) academic interest in exploring the implications of the urban built environment on travel behavior has grown markedly, especially in the past decade, with emerging technological approaches and research perspectives; (2) the USA, P.R.CHINA, and the United Kingdom are major research forces in this field, with notable contributions from research institutions in P.R.CHINA and the USA; (3) the “Transportation Research Part” series journals demonstrate extensive influence both in terms of publication count and citation count; (4) through keyword co-occurrence network analysis, three development stages along with five major clusters were identified: travel behavior modeling and public health, active transportation and sustainable development, urban development and carbon emissions, land use and transportation integration, and urban transportation systems and machine learning. Overall, sustained research remains warranted within this field, particularly focusing on selecting new built environment metrics while integrating emerging technologies into travel behavior modeling frameworks. The insights from this study have implications for urban transportation planning and policy, offering guidance on future research directions and policymaking.",
        "DOI": "10.3390/ijgi12110453",
        "affiliation_name": "Anhui Jianzhu University",
        "affiliation_city": "Hefei",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Systematic Review of Agent-Based and System Dynamics Models for Social-Ecological System Case Studies",
        "paper_author": "Nugroho S.",
        "publication": "Systems",
        "citied_by": "10",
        "cover_date": "2023-11-01",
        "Abstract": "Social–ecological system (SES) modeling involves developing and/or applying models to investigate complex problems arising from the interactions between humans and natural systems. Among the different types, agent-based models (ABM) and system dynamics (SD) are prominent approaches in SES modeling. However, few SES models influence decision-making support and policymaking. The objectives of this study were to explore the application of ABM and SD in SES studies through a systematic review of published real-world case studies and determine the extent to which existing SES models inform policymaking processes. We identified 35 case studies using ABM, SD, or a hybrid of the two and found that each modeling approach shared commonalities that collectively contributed to the policymaking process, offering a comprehensive understanding of the intricate dynamics within SES, facilitating scenario exploration and policy testing, and fostering effective communication and stakeholder engagement. This study also suggests several improvements to chart a more effective trajectory for research in this field, including fostering interdisciplinary collaboration, developing hybrid models, adopting transparent model reporting, and implementing machine-learning algorithms.",
        "DOI": "10.3390/systems11110530",
        "affiliation_name": "Ritsumeikan University Osaka Ibaraki Campus",
        "affiliation_city": "Ibaraki",
        "affiliation_country": "Japan"
    },
    {
        "paper_title": "What Determinants Will Enhance or Constrain the Spatiality of Agricultural Products with Geographical Indications in Northeast China? An Interpretable Learning Approach",
        "paper_author": "Luo S.",
        "publication": "ISPRS International Journal of Geo-Information",
        "citied_by": "1",
        "cover_date": "2023-11-01",
        "Abstract": "Geographical indication (GI) offers a unique protection scheme to preserve high-quality agricultural products and support rural sustainability at the territorial level. However, not all the areas with traditional agricultural products are acknowledged with a GI. Quantifying the contribution of each factor to geographical indication agricultural products (GIAPs) can facilitate the formulation of effective policies to improve rural livelihoods. In this study, the random forest (RF) model was applied to investigate the contribution of multi-perspective factors, including nature, society, agriculture and market, on the distribution of GIAPs, and examined the driving causes using interpretable approaches. The empirical findings demonstrate that the RF model is able to accurately capture most of the important factors characterizing GIAPs and to make out-of-sample predictions of the study units which obtain GIs. This study revealed that natural conditions and market demand were contributing aspects to the disparity of GIAPs in Northeast China. The order of determinants was the category of online GIAPs (CatOn) > the number of online GIAPs (NumOn) > the area of black soil (BlaSoil) > the distance to offline stores selling GIAPs (DisOff). Of these, GIAPs was lower than (Formula presented.) in parts of districts of Jilin and Heilongjiang Provinces when the area of black soil (BlaSoil) gradually increased. When the category and number of online GIAPs (CatOn and NumOn) were less than 20 and 5, respectively, GIAPs were enhanced, especially for 40% of the districts in Liaoning Province. Deepening understanding of GIAPs helps to better target and tailor sustainable development policies.",
        "DOI": "10.3390/ijgi12110442",
        "affiliation_name": "University of Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Poverty improvement policies and household income: Evidence from China",
        "paper_author": "Zhang H.",
        "publication": "Heliyon",
        "citied_by": "8",
        "cover_date": "2023-11-01",
        "Abstract": "The purposes of this research is to analyzed the impact of various policies on household incomes and to obtain the influential factors according to their importance. Utilizing a survey data of poor households supported by poverty improvement policies, for individual income, this research implements the traditional random forest method. To analyze the influence of strategies on the overall household earnings, we propose a novel splitting method centered on copula entropy and construct a novel choice tree. Finally, we conclude that the policies are generally effective but that levels of effectiveness are varied for people with different levels of income. We also identified the policy arrangement that has the greatest impact on household income.",
        "DOI": "10.1016/j.heliyon.2023.e21442",
        "affiliation_name": "Nanjing University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Percutaneous Coronary Intervention Mortality, Cost, Complications, and Disparities after Radiation Therapy: Artificial Intelligence-Augmented, Cost Effectiveness, and Computational Ethical Analysis",
        "paper_author": "Monlezun D.J.",
        "publication": "Journal of Cardiovascular Development and Disease",
        "citied_by": "2",
        "cover_date": "2023-11-01",
        "Abstract": "The optimal cardio-oncology management of radiation therapy and its complications are unknown despite the high patient and societal costs. This study is the first known nationally representative, multi-year, artificial intelligence and propensity score-augmented causal clinical inference and computational ethical and policy analysis of percutaneous coronary intervention (PCI) mortality, cost, and disparities including by primary malignancy following radiation therapy. Bayesian Machine learning-augmented Propensity Score translational (BAM-PS) statistics were conducted in the 2016–2020 National Inpatient Sample. Of the 148,755,036 adult hospitalizations, 2,229,285 (1.50%) had a history of radiation therapy, of whom, 67,450 (3.00%) had an inpatient AMI, and of whom, 18,400 (28.69%) underwent PCI. Post-AMI mortality, costs, and complications were comparable with and without radiation across cancers in general and across the 30 primary malignancies tested, except for breast cancer, in which PCI significantly increased mortality (OR 3.70, 95%CI 1.10–12.43, p = 0.035). In addition to significant sex, race, and insurance disparities, significant regional disparities were associated with nearly 50 extra inpatient deaths and over USD 500 million lost. This large clinical, cost, and pluralistic ethical analysis suggests PCI when clinically indicated should be provided to patients regardless of sex, race, insurance, or region to generate significant improvements in population health, cost savings, and social equity.",
        "DOI": "10.3390/jcdd10110445",
        "affiliation_name": "The University of Texas MD Anderson Cancer Center",
        "affiliation_city": "Houston",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Understanding the implications of under-reporting, vaccine efficiency and social behavior on the post-pandemic spread using physics informed neural networks: A case study of China",
        "paper_author": "Ghosh S.",
        "publication": "PLoS ONE",
        "citied_by": "2",
        "cover_date": "2023-11-01",
        "Abstract": "In late 2019, the emergence of COVID-19 in Wuhan, China, led to the implementation of stringent measures forming the zero-COVID policy aimed at eliminating transmission. Zero-COVID policy basically aimed at completely eliminating the transmission of COVID-19. However, the relaxation of this policy in late 2022 reportedly resulted in a rapid surge of COVID-19 cases. The aim of this work is to investigate the factors contributing to this outbreak using a new SEIR-type epidemic model with time-dependent level of immunity. Our model incorporates a time-dependent level of immunity considering vaccine doses administered and time-post-vaccination dependent vaccine efficacy. We find that vaccine efficacy plays a significant role in determining the outbreak size and maximum number of daily infected. Additionally, our model considers under-reporting in daily cases and deaths, revealing their combined effects on the outbreak magnitude. We also introduce a novel Physics Informed Neural Networks (PINNs) approach which is extremely useful in estimating critical parameters and helps in evaluating the predictive capability of our model.",
        "DOI": "10.1371/journal.pone.0290368",
        "affiliation_name": "Texas A&amp;M University-Commerce",
        "affiliation_city": "Commerce",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Profiles of tobacco smokers and ex-smokers in a large-scale random sample survey across Wales: an unsupervised machine-learning cluster analysis",
        "paper_author": "Evans A.",
        "publication": "Lancet (London, England)",
        "citied_by": "2",
        "cover_date": "2023-11-01",
        "Abstract": "BACKGROUND: The Welsh government recently set a target to be smoke-free by 2030, which means reducing the prevalence of tobacco smoking in adults to 5% by then. The goal is to improve health and population life expectancy. To support this strategy, we identified profile groups with different sets of socioeconomic and demographic characteristics within the population of smokers. We compared these profiles to those identified in the ex-smoker population to provide a broader understanding of smokers and inform targeting of interventions and policy. METHODS: We did a cross-sectional study using data from the National Survey for Wales. This survey is a random sample telephone survey of individuals aged 16 years and older across Wales carried out from Sept 1, 2021 to Jan 31, 2022, weighted to be representative of the Welsh population. For the smoking subgroup, we did a weighted hierarchical cluster analysis with multiple imputation to impute missing data and repeated it for ex-smokers. In total, 63 survey variables were used in the analysis. These variables included smoking history, e-cigarette use, sociodemographics, lifestyle factors, individual-level deprivation, general health and long-term conditions, mental health, and wellbeing. FINDINGS: Among the 6407 respondents (weighted proportions: 49% male, 51% female; 28% aged 16-34 years, 46% aged 35-44 years, 26% aged ≥65 years; 95% white, 5% other ethnicity), 841 (13%) smoked and 2136 (33%) were ex-smokers. Four distinctive profiles of smokers were identified, the groups were of relatively comparable size and characterised by similarities described as (1) high-risk alcohol drinkers and without children; (2) single, mostly in social housing, and poor health and mental health; (3) mostly single, younger, tried e-cigarettes, and poor mental health; (4) older couples and poor health; when comparing the groups with each other. Cluster quality and validation statistics were considered fair: silhouette coefficient=0·09, Dunn index (Dunn2)=1·06. Generally, ex-smoker clusters differed from smoking clusters because of themes related to increased sickness, better affluence, employment, and older age (≥75 years). INTERPRETATION: This study suggests that not all smokers are the same, and they do not fall into one coherent group. Smoking cessation interventions to improve the health of ageing populations might need a different approach to consider a wider context or motivations to inform targeted quitting. It is acknowledged that smoking might be underreported because of perceived social unacceptability. FUNDING: Public Health Wales.",
        "DOI": "10.1016/S0140-6736(23)02070-6",
        "affiliation_name": "Public Health Wales",
        "affiliation_city": "Cardiff",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Time Series Analysis and Forecasting of Solar Generation in Spain Using eXtreme Gradient Boosting: A Machine Learning Approach",
        "paper_author": "Saigustia C.",
        "publication": "Energies",
        "citied_by": "5",
        "cover_date": "2023-11-01",
        "Abstract": "The rapid expansion of solar photovoltaic (PV) generation has established its pivotal role in the shift toward sustainable energy systems. This study conducts an in-depth analysis of solar generation data from 2015 to 2018 in Spain, with a specific emphasis on temporal patterns, excluding weather data. Employing the powerful eXtreme gradient boosting (XGBoost) algorithm for modeling and forecasting, our research underscores its exceptional efficacy in capturing solar generation trends, as evidenced by a remarkable root mean squared error (RMSE) of 11.042, a mean absolute error (MAE) of 5.621, an R-squared (R²) of 0.999, and a minimal mean absolute percentage error (MAPE) of 0.046. These insights hold substantial implications for grid management, energy planning, and policy development, reaffirming solar energy’s promise as a dependable and sustainable contributor to the electrical power system’s evolution. This research contributes to the growing body of knowledge aimed at optimizing renewable energy integration and enhancing energy sustainability for future generations.",
        "DOI": "10.3390/en16227618",
        "affiliation_name": "Politechnika Lubelska",
        "affiliation_city": "Lublin",
        "affiliation_country": "Poland"
    },
    {
        "paper_title": "Artificial Intelligence (AI) can change the way of doing policy modelling",
        "paper_author": "Estrada M.A.R.",
        "publication": "Journal of Policy Modeling",
        "citied_by": "2",
        "cover_date": "2023-11-01",
        "Abstract": "This paper seeks to assess the transformative potential of Artificial Intelligence (AI) in policy modeling. Rapid advancements in AI, encompassing algorithms, advanced programming software, robotics, metadata, sophisticated mathematical models, neural networks, and graphical models are ushering in innovative new research methods for analysing and resolving intricate socio-economic issues. Our focus lies in a comparative evaluation of Artificial Intelligence Response (AIR) versus Human Intelligence Response (HIR) in generating swift and potent solutions to various socio-economic challenges. To achieve this, we propose a fundamental model for appraising the effectiveness of policy modeling, known as the “Policy Modeling Response Evaluator (PMR-Evaluator).” Furthermore, we conducted an experiment to gauge the responsiveness and effectiveness of both AIR and HIR. This experiment revolved around addressing a specific socio-economic problem, namely controlling inflation. Initially, we scrutinized responses from an extensive database of papers published in the Journal of Policy Modeling (JPM) by Elsevier over the past forty-five years (1978–2023) to ascertain HIR's capacity to analyze and resolve inflation-related issues. Concurrently, we utilized ChatGPT, a powerful artificial intelligence application (AI-APP), to explore potential solutions for controlling inflation. Ultimately, we analyzed whether HIR or AIR proved more effective and precise.",
        "DOI": "10.1016/j.jpolmod.2023.11.005",
        "affiliation_name": "Akademia Ekonomiczno-Humanistyczna w Warszawie",
        "affiliation_city": "Warsaw",
        "affiliation_country": "Poland"
    },
    {
        "paper_title": "Assessing the Potential of Artificial Intelligence in Advancing Clean Energy Technologies in Europe: A Systematic Review",
        "paper_author": "Necula S.C.",
        "publication": "Energies",
        "citied_by": "5",
        "cover_date": "2023-11-01",
        "Abstract": "This systematic review investigates the role of artificial intelligence (AI) in advancing clean energy technologies within Europe, based on a literature survey from 2006 to 2023. The assessment reveals that AI, particularly through deep learning and neural networks, enhances the efficiency, optimization, and management of clean energy systems. Noteworthy is AI’s capacity to improve short-term energy forecasts, essential for smart cities and IoT applications. Our findings indicate that AI drives innovation in renewable energy, contributing to the development of smart grids and enabling collaborative energy-sharing models. While the research underscores AI’s substantial influence in Europe’s energy sector, it also identifies gaps, such as varied AI algorithm applications in different renewable energy sectors. The study emphasizes the need for integrating AI with emerging clean energy innovations, advocating for interdisciplinary research to navigate the socio-economic, environmental, and policy dimensions. This approach is crucial for guiding a sustainable and balanced advancement in the clean energy landscape, signifying AI’s pivotal role in Europe’s energy transition.",
        "DOI": "10.3390/en16227633",
        "affiliation_name": "Universitatea Alexandru Ioan Cuza",
        "affiliation_city": "Iasi",
        "affiliation_country": "Romania"
    },
    {
        "paper_title": "Data Resources: Milestones and Building Blocks",
        "paper_author": "Kahn C.E.",
        "publication": "Radiology: Artificial Intelligence",
        "citied_by": "2",
        "cover_date": "2023-11-01",
        "Abstract": "NA",
        "DOI": "10.1148/ryai.230418",
        "affiliation_name": "Penn Medicine",
        "affiliation_city": "Philadelphia",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Genetic newborn screening and digital technologies: A project protocol based on a dual approach to shorten the rare diseases diagnostic path in Europe",
        "paper_author": "Garnier N.",
        "publication": "PLoS ONE",
        "citied_by": "5",
        "cover_date": "2023-11-01",
        "Abstract": "Since 72% of rare diseases are genetic in origin and mostly paediatrics, genetic newborn screening represents a diagnostic “window of opportunity”. Therefore, many gNBS initiatives started in different European countries. Screen4Care is a research project, which resulted of a joint effort between the European Union Commission and the European Federation of Pharmaceutical Industries and Associations. It focuses on genetic newborn screening and artificial intelligence-based tools which will be applied to a large European population of about 25.000 infants. The neonatal screening strategy will be based on targeted sequencing, while whole genome sequencing will be offered to all enrolled infants who may show early symptoms but have resulted negative at the targeted sequencing-based newborn screening. We will leverage artificial intelligence-based algorithms to identify patients using Electronic Health Records (EHR) and to build a repository “symptom checkers” for patients and healthcare providers. S4C will design an equitable, ethical, and sustainable framework for genetic newborn screening and new digital tools, corroborated by a large workout where legal, ethical, and social complexities will be addressed with the intent of making the framework highly and flexibly translatable into the diverse European health systems.",
        "DOI": "10.1371/journal.pone.0293503",
        "affiliation_name": "sitem-insel AG",
        "affiliation_city": "Bern",
        "affiliation_country": "Switzerland"
    },
    {
        "paper_title": "Soil Data Cube and Artificial Intelligence Techniques for Generating National-Scale Topsoil Thematic Maps: A Case Study in Lithuanian Croplands",
        "paper_author": "Samarinas N.",
        "publication": "Remote Sensing",
        "citied_by": "6",
        "cover_date": "2023-11-01",
        "Abstract": "There is a growing realization among policymakers that in order to pave the way for the development of evidence-based conservation recommendations for policy, it is essential to improve the capacity for soil-health monitoring by adopting multidimensional and integrated approaches. However, the existing ready-to-use maps are characterized mainly by a coarse spatial resolution (>200 m) and information that is not up to date, making their use insufficient for the EU’s policy requirements, such as the common agricultural policy. This work, by utilizing the Soil Data Cube, which is a self-hosted custom tool, provides yearly estimations of soil thematic maps (e.g., exposed soil, soil organic carbon, clay content) covering all the agricultural area in Lithuania. The pipeline exploits various Earth observation data such as a time series of Sentinel-2 satellite imagery (2018–2022), the LUCAS (Land Use/Cover Area Frame Statistical Survey) topsoil database, the European Integrated Administration and Control System (IACS) and artificial intelligence (AI) architectures to improve the prediction accuracy as well as the spatial resolution (10 m), enabling discrimination at the parcel level. Five different prediction models were tested with the convolutional neural network (CNN) model to achieve the best accuracy for both targeted indicators (SOC and clay) related to the R (Formula presented.) metric (0.51 for SOC and 0.57 for clay). The model predictions supported by the prediction uncertainties based on the PIR formula (average PIR 0.48 for SOC and 0.61 for clay) provide valuable information on the model’s interpretation and stability. The model application and the final predictions of the soil indicators were carried out based on national bare-soil-reflectance composite layers, generated by employing a pixel-based composite approach to the overlaid annual bare-soil maps and by using a combination of a series of vegetation indices such as NDVI, NBR2, and SCL. The findings of this work provide new insights for the generation of soil thematic maps on a large scale, leading to more efficient and sustainable soil management, supporting policymakers and the agri-food private sector.",
        "DOI": "10.3390/rs15225304",
        "affiliation_name": "Aristotle University of Thessaloniki",
        "affiliation_city": "Thessaloniki",
        "affiliation_country": "Greece"
    },
    {
        "paper_title": "A comparative study of statistical and machine learning models on carbon dioxide emissions prediction of China",
        "paper_author": "Li X.",
        "publication": "Environmental Science and Pollution Research",
        "citied_by": "21",
        "cover_date": "2023-11-01",
        "Abstract": "The escalating levels of carbon dioxide (CO2) emissions represent the primary driver of global warming, and addressing them is of paramount importance. Timely and accurate prediction, as well as effective control of CO2 emissions, are pivotal for guiding mitigation measures. This paper aims to select the best prediction model for near-real-time daily CO2 emissions in China. The prediction models are based on univariate daily time-series data spanning January 1st, 2020, to September 30st, 2022. Six models are proposed, including three statistical models: grey prediction (GM(1,1)), autoregressive integrated moving average (ARIMA), and seasonal autoregressive integrated moving average with exogenous factors (SARIMAX), and three machine learning models: artificial neural network (ANN), random forest (RF), and long short-term memory (LSTM). The performance of these six models is evaluated using five criteria: mean squared error (MSE), root-mean squared error (RMSE), mean absolute error (MAE), mean absolute percentage error (MAPE), and coefficient of determination (R 2). Our findings reveal that the three machine learning models consistently outperform the three statistical models across all five criteria. Among them, the LSTM model demonstrates exceptional performance for daily CO2 emission prediction, boasting an impressively low MSE value of 3.5179e−04, an RMSE value of 0.0187, an MAE value of 0.0140, an MAPE value of 14.8291%, and a high R 2 value of 0.9844. This underscores the robustness of the LSTM model in capturing and predicting complex emission patterns, positioning it as the most suitable option for near-real-time daily CO2 emission prediction based on the provided daily time series data. Moreover, our study’s results provide valuable insights into emissions forecasting, enabling data-driven decision-making for policymakers and stakeholders. The accurate and timely predictions offered by the LSTM model can aid in the formulation of effective strategies to mitigate carbon emissions, contributing to a more sustainable future. Furthermore, the findings of this study can enhance our understanding of the dynamics of CO2 emissions, leading to more informed environmental policies and actions aimed at reducing carbon emissions.",
        "DOI": "10.1007/s11356-023-30428-5",
        "affiliation_name": "Beijing Wuzi University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Livelihood Changes, Spatial Anticontagion Policy Effects, and Structural Resilience of National Food Systems in a Sub-Saharan African Country Context: A Panel Machine Learning Approach",
        "paper_author": "Frimpong S.",
        "publication": "Social Sciences",
        "citied_by": "0",
        "cover_date": "2023-11-01",
        "Abstract": "The livelihood changes due to the COVID-19 policies in low-income and transitional economies serve as a lever for gauging the structural resilience of national food systems. Yet, few studies have addressed the cascading effects of the pandemic policies on the livelihood changes of farming system actors or modeled and provided coherent hypotheses about the transitory structural shifts at the micro-level. Other studies on the subject have either captured the early impacts of the pandemic on food systems with limited or no insight into the sub-Saharan African context or have used macro-level data, due to sparsely available micro-level data. These early insights are relevant for the design of early warning systems. However, an ongoing and deeper insight into the effects of pandemic policies is critical, since new and more comprehensive policies are needed to address the economic fallout and the extenuating effects of COVID-19 on food supply chain disruptions. The overriding questions are as follows: what are the effects of the pandemic policies on the livelihoods of food system actors and are there spatial-economic variations in the effects of the pandemic policies on the livelihoods of the farming system actors? Using 2019 and 2020 primary data from 836 farming system actors in Ghana, we offer fresh insights into the transitory micro-level livelihood changes caused by the COVID-19 anticontagion policies. We analyzed the data using the generalized additive, subset regression, classical linear, and logistic regression models in a machine learning framework. We show that the changes in the livelihood outcomes of the food system actors in Ghana coincide with the nature of pandemic mitigation policies adopted in the spatial units. We found that the lockdown policies had a negative and significant effect on the livelihoods of the farming system actors in the lockdown areas. The policies also negatively affected the livelihoods of the farming system actors in distant communities that shared no direct boundary with the lockdown areas. On the contrary, the lockdown policies positively affected the livelihoods of the farming system actors in the directly contiguous communities to the lockdown areas. We also document the shifts in the livelihood outcomes of the farming system actors, such as income, employment, food demand, and food security in the different spatial policy areas.",
        "DOI": "10.3390/socsci12110618",
        "affiliation_name": "FAO Regional Office for Africa, Ghana",
        "affiliation_city": "Accra",
        "affiliation_country": "Ghana"
    },
    {
        "paper_title": "Darkweb research: Past, present, and future trends and mapping to sustainable development goals",
        "paper_author": "Raman R.",
        "publication": "Heliyon",
        "citied_by": "23",
        "cover_date": "2023-11-01",
        "Abstract": "The Darkweb, part of the deep web, can be accessed only through specialized computer software and used for illegal activities such as cybercrime, drug trafficking, and exploitation. Technological advancements like Tor, bitcoin, and cryptocurrencies allow criminals to carry out these activities anonymously, leading to increased use of the Darkweb. At the same time, computers have become an integral part of our daily lives, shaping our behavior, and influencing how we interact with each other and the world. This work carries out the bibliometric study on the research conducted on Darkweb over the last decade. The findings illustrate that most research on Darkweb can be clustered into four areas based on keyword co-occurrence analysis: (i) network security, malware, and cyber-attacks, (ii) cybercrime, data privacy, and cryptography, (iii) machine learning, social media, and artificial intelligence, and (iv) drug trafficking, cryptomarket. National Science Foundation from the United States is the top funder. Darkweb activities interfere with the Sustainable Development Goals (SDG) laid forth by the United Nations to promote peace and sustainability for current and future generations. SDG 16 (Peace, Justice, and Strong Institutions) has the highest number of publications and citations but has an inverse relationship with Darkweb, as the latter undermines the former. This study highlights the need for further research in bitcoin, blockchain, IoT, NLP, cryptocurrencies, phishing and cybercrime, botnets and malware, digital forensics, and electronic crime countermeasures about the Darkweb. The study further elucidates the multi-dimensional nature of the Darkweb, emphasizing the intricate relationship between technology, psychology, and geopolitics. This comprehensive understanding serves as a cornerstone for evolving effective countermeasures and calls for an interdisciplinary research approach. The study also delves into the psychological motivations driving individuals towards illegal activities on the Darkweb, highlighting the urgency for targeted interventions to promote pro-social online behavior.",
        "DOI": "10.1016/j.heliyon.2023.e22269",
        "affiliation_name": "Amrita University, Amritapuri Campus",
        "affiliation_city": "Kollam",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Predicting incident cardiovascular disease among African-American adults: A deep learning approach to evaluate social determinants of health in the Jackson heart study",
        "paper_author": "Morris M.C.",
        "publication": "PLoS ONE",
        "citied_by": "1",
        "cover_date": "2023-11-01",
        "Abstract": "The present study sought to leverage machine learning approaches to determine whether social determinants of health improve prediction of incident cardiovascular disease (CVD). Participants in the Jackson Heart study with no history of CVD at baseline were followed over a 10-year period to determine first CVD events (i.e., coronary heart disease, stroke, heart failure). Three modeling algorithms (i.e., Deep Neural Network, Random Survival Forest, Penalized Cox Proportional Hazards) were used to evaluate three feature sets (i.e., demographics and standard/biobehavioral CVD risk factors [FS1], FS1 combined with psychosocial and socioeconomic CVD risk factors [FS2], and FS2 combined with environmental features [FS3]) as predictors of 10-year CVD risk. Contrary to hypothesis, overall predictive accuracy did not improve when adding social determinants of health. However, social determinants of health comprised eight of the top 15 predictors of first CVD events. The social determinates of health indicators included four socioeconomic factors (insurance status and types), one psychosocial factor (discrimination burden), and three environmental factors (density of outdoor physical activity resources, including instructional and water activities; modified retail food environment index excluding alcohol; and favorable food stores). Findings suggest that whereas understanding biological determinants may identify who is currently at risk for developing CVD and in need of secondary prevention, understanding upstream social determinants of CVD risk could guide primary prevention efforts by identifying where and how policy and community-level interventions could be targeted to facilitate changes in individual health behaviors.",
        "DOI": "10.1371/journal.pone.0294050",
        "affiliation_name": "North Carolina A&amp;T College of Engineering",
        "affiliation_city": "Greensboro",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "ASAP-CORPS: A Semi-Autonomous Platform for COntact-Rich Precision Surgery",
        "paper_author": "Balakuntala M.V.",
        "publication": "Military Medicine",
        "citied_by": "0",
        "cover_date": "2023-11-01",
        "Abstract": "Introduction: Remote military operations require rapid response times for effective relief and critical care. Yet, the military theater is under austere conditions, so communication links are unreliable and subject to physical and virtual attacks and degradation at unpredictable times. Immediate medical care at these austere locations requires semi-autonomous teleoperated systems, which enable the completion of medical procedures even under interrupted networks while isolating the medics from the dangers of the battlefield. However, to achieve autonomy for complex surgical and critical care procedures, robots require extensive programming or massive libraries of surgical skill demonstrations to learn effective policies using machine learning algorithms. Although such datasets are achievable for simple tasks, providing a large number of demonstrations for surgical maneuvers is not practical. This article presents a method for learning from demonstration, combining knowledge from demonstrations to eliminate reward shaping in reinforcement learning (RL). In addition to reducing the data required for training, the self-supervised nature of RL, in conjunction with expert knowledge-driven rewards, produces more generalizable policies tolerant to dynamic environment changes. A multimodal representation for interaction enables learning complex contact-rich surgical maneuvers. The effectiveness of the approach is shown using the cricothyroidotomy task, as it is a standard procedure seen in critical care to open the airway. In addition, we also provide a method for segmenting the teleoperator’s demonstration into subtasks and classifying the subtasks using sequence modeling. Materials and Methods: A database of demonstrations for the cricothyroidotomy task was collected, comprising six fundamental maneuvers referred to as surgemes. The dataset was collected by teleoperating a collaborative robotic platform—SuperBaxter, with modified surgical grippers. Then, two learning models are developed for processing the dataset—one for automatic segmentation of the task demonstrations into a sequence of surgemes and the second for classifying each segment into labeled surgemes. Finally, a multimodal off-policy RL with rewards learned from demonstrations was developed to learn the surgeme execution from these demonstrations. Results: The task segmentation model has an accuracy of 98.2%. The surgeme classification model using the proposed interaction features achieved a classification accuracy of 96.25% averaged across all surgemes compared to 87.08% without these features and 85.4% using a support vector machine classifier. Finally, the robot execution achieved a task success rate of 93.5% compared to baselines of behavioral cloning (78.3%) and a twin-delayed deep deterministic policy gradient with shaped rewards (82.6%). Conclusions: Results indicate that the proposed interaction features for the segmentation and classification of surgical tasks improve classification accuracy. The proposed method for learning surgemes from demonstrations exceeds popular methods for skill learning. The effectiveness of the proposed approach demonstrates the potential for future remote telemedicine on battlefields.",
        "DOI": "10.1093/milmed/usad175",
        "affiliation_name": "Purdue Polytechnic Institute",
        "affiliation_city": "West Lafayette",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Cloud-Based Machine Learning for Flood Policy Recommendations in Makassar City, Indonesia",
        "paper_author": "Rimba A.B.",
        "publication": "Water (Switzerland)",
        "citied_by": "1",
        "cover_date": "2023-11-01",
        "Abstract": "Makassar City frequently experiences monsoonal floods, typical of a tropical city in Indonesia. However, there is no high-accuracy flood map for flood inundation. Examining the flood inundation area would help to provide a suitable flood policy. Hence, the study utilizes multiple satellite data sources on a cloud-based platform, integrating the physical factors of a flood (i.e., land use data and digital elevation model—DEM—data) with the local government’s urban land use plan and existing drainage networks. The research aims to map the inundation area, identify the most vulnerable land cover, slope, and elevation, and assess the efficiency of Makassar’s drainage system and urban land use plan. The study reveals that an uncoordinated drainage system in the Tamalanrea, Biringkanaya, and Manggala sub-districts results in severe flooding, encompassing a total area of 35.28 km2. The most affected land use type is cultivation land, constituting approximately 43.5% of the flooded area. Furthermore, 82.26% of the urban land use plan, covering 29.02 km2, is submerged. It is imperative for the local government and stakeholders to prioritize the enhancement of drainage systems and urban land use plans, particularly in low-lying and densely populated regions.",
        "DOI": "10.3390/w15213783",
        "affiliation_name": "Badan Riset dan Inovasi Nasional",
        "affiliation_city": "Central Jakarta",
        "affiliation_country": "Indonesia"
    },
    {
        "paper_title": "Retraction notice to “Application of Machine Learning and Big Data in Doubly Fed Induction Generator based Stability Analysis of Multi Machine System using Substantial Transformative Optimization Algorithm” [Microprocessors and Microsystems 73 (2020) 102971] (Microprocessors and Microsystems (2020) 73, (S0141933119305344), (10.1016/j.micpro.2019.102971))",
        "paper_author": "Seethalakshmi V.S.",
        "publication": "Microprocessors and Microsystems",
        "citied_by": "0",
        "cover_date": "2023-11-01",
        "Abstract": "This article has been retracted: please see Elsevier Policy on Article Withdrawal (https://www.elsevier.com/about/policies/article-withdrawal). This article has been retracted at the request of the Editor-in-Chief. Subsequent to acceptance of these special issue papers by the responsible guest editors, Vyasa Sai, Anand Paul and Ramachandran Varatharajan, the integrity and rigor of the peer-review process of the Special Issue were investigated and confirmed to fall beneath the high standards expected by Microprocessors & Microsystems. Due to a configuration error in the editorial system, unfortunately neither the Editor in Chief nor the designated Handling Editors received these papers for approval as per the journal's standard workflow. The journal has attempted to contact the authors of this article to give them the opportunity to respond to the findings outlined in this notice. However, the journal has not received any substantive response.",
        "DOI": "10.1016/j.micpro.2023.104942",
        "affiliation_name": "PSNA College of Engineering and Technology",
        "affiliation_city": "Dindigul",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Energy Efficient Power Allocation in Massive MIMO Based on Parameterized Deep DQN",
        "paper_author": "Sharma S.",
        "publication": "Electronics (Switzerland)",
        "citied_by": "3",
        "cover_date": "2023-11-01",
        "Abstract": "Machine learning offers advanced tools for efficient management of radio resources in modern wireless networks. In this study, we leverage a multi-agent deep reinforcement learning (DRL) approach, specifically the Parameterized Deep Q-Network (DQN), to address the challenging problem of power allocation and user association in massive multiple-input multiple-output (M-MIMO) communication networks. Our approach tackles a multi-objective optimization problem aiming to maximize network utility while meeting stringent quality of service requirements in M-MIMO networks. To address the non-convex and nonlinear nature of this problem, we introduce a novel multi-agent DQN framework. This framework defines a large action space, state space, and reward functions, enabling us to learn a near-optimal policy. Simulation results demonstrate the superiority of our Parameterized Deep DQN (PD-DQN) approach when compared to traditional DQN and RL methods. Specifically, we show that our approach outperforms traditional DQN methods in terms of convergence speed and final performance. Additionally, our approach shows 72.2% and 108.5% improvement over DQN methods and the RL method, respectively, in handling large-scale multi-agent problems in M-MIMO networks.",
        "DOI": "10.3390/electronics12214517",
        "affiliation_name": "Ajou University",
        "affiliation_city": "Suwon",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "Construction of a Micro Model for CO<inf>2</inf> Emissions from CNG Taxi Based on Trajectory Data and Deep Learning Method and Evaluation of Carbon Reduction Benefits",
        "paper_author": "Liu Q.",
        "publication": "Journal of Geo-Information Science",
        "citied_by": "0",
        "cover_date": "2023-11-01",
        "Abstract": "Many large cities have been actively promoting the policy of \"replacing oil with gas\" for taxis. Taxis are converted from traditional gasoline consumption to Compressed Natural Gas (CNG) to achieve energy conservation and emission reduction goals. To accurately evaluate the carbon dioxide (CO2) emission reduction benefits of CNG taxis, taking Wuhan as an example, a vehicle microscopic CO2 emission model based on deep learning method and trajectory data was proposed to investigate the spatial- temporal characteristics of CO2 emissions of taxis under different fuel scenarios. Considering the driving feature sequence and fuel type of vehicles, the Portable Emission Measurement System (PEMS) was used to collect vehicle CO2 emission data in the road test experiment, then we constructed a vehicle microscopic CO2 emission model by the BiLSTM algorithm and further verified its accuracy. Based on the proposed CO2 emission model and the trajectory data of 15 752 Wuhan taxis, the CO2 emissions throughout the entire lifecycle of urban taxis by 92# gasoline and CNG were estimated respectively to quantify the CO2 emission reduction benefits of CNG taxis. The results show that the proposed model had a higher accuracy than common regression algorithms such as SVR and LSTM, and the predictions matched well with real vehicle CO2 emission changes, meeting the accuracy for a large- scale estimation of urban taxi CO2 emissions. In addition, the accuracy of taxi CO2 emission estimation based on deep learning methods was also higher than that of physical microscopic models such as IVE and CMEM. Especially, when using CNG as vehicle fuel, the physical models had significant computational errors due to not involving technical parameters. The empirical results show that, taxi CO2 emissions using CNG were reduced by 22.05% during the PTW process and by 49.45% during the WTP process, compared to emissions using 92 # gasoline. Our results reveal both the temporal and spatial patterns of taxi CO2 emission as well as the CO2 emission reduction benefits of CNG taxis. The outperformance of deep learning methods over other methods for estimating vehicle CO2 emissions provides new ideas for large- scale and high- precision estimation of vehicle emissions. The CO2 emission reduction benefits of using CNG as fuel in taxis are significant, which provides a reference for the government to formulate relevant energy-saving and CO2 emission reduction policies.",
        "DOI": "10.12082/dqxxkx.2023.230300",
        "affiliation_name": "Wuhan University",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Multi-objectives reinforcement federated learning blockchain enabled Internet of things and Fog-Cloud infrastructure for transport data",
        "paper_author": "Mohammed M.A.",
        "publication": "Heliyon",
        "citied_by": "13",
        "cover_date": "2023-11-01",
        "Abstract": "For the past decade, there has been a significant increase in customer usage of public transport applications in smart cities. These applications rely on various services, such as communication and computation, provided by additional nodes within the smart city environment. However, these services are delivered by a diverse range of cloud computing-based servers that are widely spread and heterogeneous, leading to cybersecurity becoming a crucial challenge among these servers. Numerous machine-learning approaches have been proposed in the literature to address the cybersecurity challenges in heterogeneous transport applications within smart cities. However, the centralized security and scheduling strategies suggested so far have yet to produce optimal results for transport applications. This work aims to present a secure decentralized infrastructure for transporting data in fog cloud networks. This paper introduces Multi-Objectives Reinforcement Federated Learning Blockchain (MORFLB) for Transport Infrastructure. MORFLB aims to minimize processing and transfer delays while maximizing long-term rewards by identifying known and unknown attacks on remote sensing data in-vehicle applications. MORFLB incorporates multi-agent policies, proof-of-work hashing validation, and decentralized deep neural network training to achieve minimal processing and transfer delays. It comprises vehicle applications, decentralized fog, and cloud nodes based on blockchain reinforcement federated learning, which improves rewards through trial and error. The study formulates a combinatorial problem that minimizes and maximizes various factors for vehicle applications. The experimental results demonstrate that MORFLB effectively reduces processing and transfer delays while maximizing rewards compared to existing studies. It provides a promising solution to address the cybersecurity challenges in intelligent transport applications within smart cities. In conclusion, this paper presents MORFLB, a combination of different schemes that ensure the execution of transport data under their constraints and achieve optimal results with the suggested decentralized infrastructure based on blockchain technology.",
        "DOI": "10.1016/j.heliyon.2023.e21639",
        "affiliation_name": "Al-Ayen Iraqi University, AUIQ",
        "affiliation_city": "An Nasiriyah",
        "affiliation_country": "Iraq"
    },
    {
        "paper_title": "Incorporating Russo-Ukrainian war in Brent crude oil price forecasting: A comparative analysis of ARIMA, TARMA and ENNReg models",
        "paper_author": "Mati S.",
        "publication": "Heliyon",
        "citied_by": "12",
        "cover_date": "2023-11-01",
        "Abstract": "This article investigates the performance of three models - Autoregressive Integrated Moving Average (ARIMA), Threshold Autoregressive Moving Average (TARMA) and Evidential Neural Network for Regression (ENNReg) - in forecasting the Brent crude oil price, a crucial economic variable with a significant impact on the global economy. With the increasing complexity of the price dynamics due to geopolitical factors such as the Russo-Ukrainian war, we examine the impact of incorporating information on the war on the forecasting accuracy of these models. Our analysis shows that incorporating the impact of the war can significantly improve the forecasting accuracy of the models, and the ENNReg model with the inclusion of the dummy variable outperforms the other models during the war period. Including the war variable has enhanced the forecasting accuracy of the ENNReg model by 0.11%. These results carry significant implications regarding policymakers, investors, and researchers interested in developing accurate forecasting models in the presence of geopolitical events such as the Russo-Ukrainian war. The results can be used by the governments of oil-exporting countries for budget policies.",
        "DOI": "10.1016/j.heliyon.2023.e21439",
        "affiliation_name": "Nigeria National Petroluem Corporation",
        "affiliation_city": "Abuja",
        "affiliation_country": "Nigeria"
    },
    {
        "paper_title": "Retraction notice to “Development of hybrid machine learning model for simulation of chemical reactors in water treatment applications: Absorption in amino acid” [Environ. Technol. Innov. 27 (2022) 102417] (Environmental Technology &amp; Innovation (2022) 27, (S2352186422000761), (10.1016/j.eti.2022.102417))",
        "paper_author": "Zhang Y.",
        "publication": "Environmental Technology and Innovation",
        "citied_by": "0",
        "cover_date": "2023-11-01",
        "Abstract": "This article has been retracted: please see Elsevier Policy on Article Withdrawal (https://www.elsevier.com/about/policies/article-withdrawal). This article has been retracted at the request of the Editors-in-Chief. After publication, concerns were raised about authorship and description of author contributions. The authors were requested to provide an explanation regarding the contributions, but the Editors-in-Chief found the response to be insufficient. The Editors-in-Chief therefore no longer have confidence in whether the contributions made in this Article can be attributed to the authors. The first author, Yanjie Zhang, disagrees with the retraction.",
        "DOI": "10.1016/j.eti.2023.103317",
        "affiliation_name": "Jeddah International College",
        "affiliation_city": "Jeddah",
        "affiliation_country": "Saudi Arabia"
    },
    {
        "paper_title": "Applying modified-data mining techniques to assess public transportation vulnerable urban and suburban city areas",
        "paper_author": "Oh D.",
        "publication": "Heliyon",
        "citied_by": "2",
        "cover_date": "2023-11-01",
        "Abstract": "To guarantee the right to move for residents in areas where public transportation is insufficient, research is needed to identify vulnerable areas and prepare measures. This paper defines the vulnerable regions of public transportation within various city types in Korea. In order to identify appropriate areas to apply the Demand Responsive Transit (DRT), the regions with vulnerability were compared with a specific city (Yangsan-si) which already the DRT system was successfully adopted. To collect monthly bus data, web-data crawling method was performed and processed with coordinating program by matching GPS coordinate. The public transportation demand was predicted for each grid cell size (100 m, 250 m, and 500 m) by different methodologies. Various data mining models based on regression were analyzed to predict bus demand of vulnerable areas. Among models, a modified model was suggested to combine Automated machine learning models for high prediction performance. The modified model outperformed other methods as 0.685 and prediction performance was appropriate at 100 m rectangle grid. Regional characters of DRT bus allocation areas were extracted by K-means clustering method and differentiate urban and suburban types. The findings of this study provide valuable insights into conditions that DRT bus stop can be installed. The urban bus stop areas located in metropolitan cities and the suburban bus stop allocation areas located in countryside. The study results can be used as policy data for the successful introduction to prevent social exclusion and improve resident welfare in the future.",
        "DOI": "10.1016/j.heliyon.2023.e21213",
        "affiliation_name": "Hanyang University ERICA Campus",
        "affiliation_city": "Ansan",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "If Our Aim Is to Build Morality into an Artificial Agent, How Might We Begin to Go about Doing So?",
        "paper_author": "Seeamber R.",
        "publication": "IEEE Intelligent Systems",
        "citied_by": "2",
        "cover_date": "2023-11-01",
        "Abstract": "As AI becomes pervasive in most fields, from health care to autonomous driving, it is essential that we find successful ways of building morality into our machines, especially for decision making. However, the question of what it means to be moral is still debated, particularly in the context of AI. In this article, we highlight the different aspects that should be considered when building moral agents, including the most relevant moral paradigms and challenges. We also discuss the top-down and bottom-up approaches to design and the role of emotion and sentience in morality. We then propose solutions, including a hybrid approach to design and a hierarchical approach to combining moral paradigms. We emphasize how governance and policy are becoming ever more critical in AI ethics and in ensuring that the tasks we set for moral agents are attainable, that ethical behavior is achieved, and that we obtain good AI.",
        "DOI": "10.1109/MIS.2023.3320875",
        "affiliation_name": "Imperial College London",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Rethinking Certification for Trustworthy Machine-Learning-Based Applications",
        "paper_author": "Anisetti M.",
        "publication": "IEEE Internet Computing",
        "citied_by": "7",
        "cover_date": "2023-11-01",
        "Abstract": "Machine learning (ML) is increasingly used to implement advanced applications with nondeterministic behavior, which operate on the cloudedge continuum. The pervasive adoption of ML is urgently calling for assurance solutions to assess applications nonfunctional properties (e.g., fairness, robustness, and privacy) with the aim of improving their trustworthiness. Certification has been clearly identified by policy makers, regulators, and industrial stakeholders as the preferred assurance technique to address this pressing need. Unfortunately, existing certification schemes are not immediately applicable to nondeterministic applications built on ML models. This article analyzes the challenges and deficiencies of current certification schemes, discusses open research issues, and proposes a first certification scheme for ML-based applications.",
        "DOI": "10.1109/MIC.2023.3322327",
        "affiliation_name": "Università degli Studi di Milano",
        "affiliation_city": "Milan",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Relatedness in the era of machine learning",
        "paper_author": "Tacchella A.",
        "publication": "Chaos, Solitons and Fractals",
        "citied_by": "6",
        "cover_date": "2023-11-01",
        "Abstract": "Relatedness is a quantification of how much two human activities are similar in terms of the inputs and contexts needed for their development. Under the idea that it is easier to move between related activities than towards unrelated ones, empirical approaches to quantify relatedness are currently used as predictive tools to inform policies and development strategies in governments, international organizations, and firms. Here we show that the standard, widespread approach of estimating Relatedness through the co-location of activities (e.g. Product Space) generates a measure of relatedness that performs worse than trivial auto-correlation prediction strategies. In this paper, working on data about countries’ trade, technologies, and scientific production, we show two main findings. First, we find that a shift from two-product correlations (network-density based) to many-product correlations (decision trees) can dramatically improve the quality of forecasts, allowing the possibility to assist policymakers in optimizing decisions to promote growth. Then, we propose a new methodology to empirically estimate Relatedness that we call Continuous Projection Space (CPS). CPS, which represents a general network embedding technique, vastly outperforms all the co-location, network-based approaches, while retaining similar interpretability in terms of pairwise distances. Depending on the dataset the best approach is always either CPS or machine learning algorithms based on decision trees.",
        "DOI": "10.1016/j.chaos.2023.114071",
        "affiliation_name": "Sapienza Università di Roma",
        "affiliation_city": "Rome",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Land cover and crop types mapping using different spatial resolution imagery in a Mediterranean irrigated area",
        "paper_author": "Acharki S.",
        "publication": "Environmental Monitoring and Assessment",
        "citied_by": "2",
        "cover_date": "2023-11-01",
        "Abstract": "Crop type identification is critical for agricultural sustainability policy development and environmental assessments. Therefore, it is important to obtain their spatial distribution via different approaches. Medium-, high- and very high-resolution optical satellite sensors are efficient tools for acquiring this information, particularly for challenging studies such as those conducted in heterogeneous agricultural fields. This research examined the ability of four multitemporal datasets (Sentinel-1-SAR (S1), Sentinel-2-MSI (S2), RapidEye (RE), and PlanetScope (PS)) to identify land cover and crop types (LCCT) in a Mediterranean irrigated area. To map LCCT distribution, a supervised pixel-based classification is adopted using Support Vector Machine with a radial basis function kernel (SVMRB) and Random Forest (RF). Thus, LCCT maps were generated into three levels, including six (Level I), ten (Level II), and fourteen (Level III) classes. Overall, the findings revealed high overall accuracies of >92%, >83%, and > 81% for Level I, Level II, and Level III, respectively, except for Sentinel-1. It was found that accuracy improves considerably when the number of classes decreases, especially when cropland or non-cropland classes are grouped into one. Furthermore, there was a similarity in performance between S2 alone and S1S2. PlanetScope LCCT classifications outperform other sensors. In addition, the present study demonstrated that SVM achieved better performances against RF and can thereby effectively extract LCCT information from high-resolution imagery as PlanetScope.",
        "DOI": "10.1007/s10661-023-11877-4",
        "affiliation_name": "Van Lang University",
        "affiliation_city": "Ho Chi Minh City",
        "affiliation_country": "Viet Nam"
    },
    {
        "paper_title": "Computerized analysis of hypomimia and hypokinetic dysarthria for improved diagnosis of Parkinson's disease",
        "paper_author": "Skibińska J.",
        "publication": "Heliyon",
        "citied_by": "7",
        "cover_date": "2023-11-01",
        "Abstract": "Background and Objective: An aging society requires easy-to-use approaches for diagnosis and monitoring of neurodegenerative disorders, such as Parkinson's disease (PD), so that clinicians can effectively adjust a treatment policy and improve patients' quality of life. Current methods of PD diagnosis and monitoring usually require the patients to come to a hospital, where they undergo several neurological and neuropsychological examinations. These examinations are usually time-consuming, expensive, and performed just a few times per year. Hence, this study explores the possibility of fusing computerized analysis of hypomimia and hypokinetic dysarthria (two motor symptoms manifested in the majority of PD patients) with the goal of proposing a new methodology of PD diagnosis that could be easily integrated into mHealth systems. Methods: We enrolled 73 PD patients and 46 age- and gender-matched healthy controls, who performed several speech/voice tasks while recorded by a microphone and a camera. Acoustic signals were parametrized in the fields of phonation, articulation and prosody. Video recordings of a face were analyzed in terms of facial landmarks movement. Both modalities were consequently modeled by the XGBoost algorithm. Results: The acoustic analysis enabled diagnosis of PD with 77% balanced accuracy, while in the case of the facial analysis, we observed 81% balanced accuracy. The fusion of both modalities increased the balanced accuracy to 83% (88% sensitivity and 78% specificity). The most informative speech exercise in the multimodality system turned out to be a tongue twister. Additionally, we identified muscle movements that are characteristic of hypomimia. Conclusions: The introduced methodology, which is based on the myriad of speech exercises likewise audio and video modality, allows for the detection of PD with an accuracy of up to 83%. The speech exercise - tongue twisters occurred to be the most valuable from the clinical point of view. Additionally, the clinical interpretation of the created models is illustrated. The presented computer-supported methodology could serve as an extra tool for neurologists in PD detection and the proposed potential solution of mHealth will facilitate the patient's and doctor's life.",
        "DOI": "10.1016/j.heliyon.2023.e21175",
        "affiliation_name": "Brno University of Technology, Faculty of Electrical Engineering and Communication",
        "affiliation_city": "Brno",
        "affiliation_country": "Czech Republic"
    },
    {
        "paper_title": "Near real-time flood inundation and hazard mapping of Baitarani River Basin using Google Earth Engine and SAR imagery",
        "paper_author": "Atchyuth B.A.S.",
        "publication": "Environmental Monitoring and Assessment",
        "citied_by": "3",
        "cover_date": "2023-11-01",
        "Abstract": "Flood inundation mapping and satellite imagery monitoring are critical and effective responses during flood events. Mapping of a flood using optical data is limited due to the unavailability of cloud-free images. Because of its capacity to penetrate clouds and operate in all kinds of weather, synthetic aperture radar is preferred for water inundation mapping. Flood mapping in Eastern India’s Baitarani River Basin for 2018, 2019, 2020, 2021, and 2022 was performed in this study using Sentinel-1 imagery and Google Earth Engine with Otsu’s algorithm. Different machine-learning algorithms were used to map the LULC of the study region. Dual polarizations VH and VV and their combinations VV×VH, VV+VH, VH−VV, VV−VH, VV/VH, and VH/VV were examined to identify non-water and water bodies. The normalized difference water index (NDWI) map derived from Sentinel-2 data validated the surface water inundation with 80% accuracy. The total inundated areas were identified as 440.3 km2 in 2018, 268.58 km2 in 2019, 178.40 km2 in 2020, 203.79 km2 in 2021, and 321.33 km2 in 2022, respectively. The overlap of flood maps on the LULC map indicated that flooding highly affected agriculture and urban areas in these years. The approach using the near-real-time Sentinel-1 SAR imagery and GEE platform can be operationalized for periodic flood mapping, helps develop flood control measures, and helps enhance flood management. The generated annual flood inundation maps are also useful for policy development, agriculture yield estimation, crop insurance framing, etc.",
        "DOI": "10.1007/s10661-023-11876-5",
        "affiliation_name": "Maine College of Engineering and Computing",
        "affiliation_city": "Orono",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Modeling habitat suitability for endemic Grizzled leaf monkey (Presbytis comata) using geospatial machine learning approach",
        "paper_author": "Santoso C.",
        "publication": "Remote Sensing Applications: Society and Environment",
        "citied_by": "3",
        "cover_date": "2023-11-01",
        "Abstract": "Global biodiversity, which plays a significant role in maintaining the sustainability of Earth's ecosystems, is threatened by increasingly destructive and uncontrolled human activities. Indonesia's Grizzled leaf monkey or Javan Surili (Presbytis comata) is an endemic primate whose population continues to decline. As the high potential for reducing animal habitats on Java has implications for endemic Surili primates, it is necessary to study the characteristics of suitable habitats that can be used as the basis for strategic policies. This study aimed to develop a habitat suitability index (HSI) for Surili with a study of forest areas in Java Island, Indonesia, using a multi-machine learning approach. The novelty of this study is the integration of three machine learning methods, random forest (RF), support vector machines (SVM), and maximum entropy (MaxEnt), and the use of four parameters, climate, topography, ecology, and anthropogenic factors. This is expected to increase the accuracy of assessing the suitability of the developed Surili habitat. Of the 116 Surili encounter points in the region, 7 (6%) were in the suitable HSI class, 38 (33%) in the high HSI class, 49 (42%) in the moderate HSI class, and 22 (19%) in the low and unsuitable HSI classes. Meanwhile, of the 13 release points, 12 (92%) were dominated by areas with high-to-suitable HSI classes. Based on the receiver operating characteristic curve value, the integrated multi machine learning algorithms value was 0.8578. This study could help formulate potential strategic policies in the restoration program that has been pursued to conserve Surili species in Indonesia.",
        "DOI": "10.1016/j.rsase.2023.101067",
        "affiliation_name": "Institut Teknologi Bandung",
        "affiliation_city": "Bandung",
        "affiliation_country": "Indonesia"
    },
    {
        "paper_title": "Monitoring the green transition in the power sector with the electricity generation emissions (EGE) tracker",
        "paper_author": "Portela J.",
        "publication": "Energy Strategy Reviews",
        "citied_by": "2",
        "cover_date": "2023-11-01",
        "Abstract": "This paper introduces the Electricity Generation Emissions (EGE) tracker as a new indicator for measuring the decarbonization process associated with the electricity generation mix. The EGE is a composite indicator calculated at the cross-country level on the basis of electricity production of different generation technologies weighted by their corresponding life cycle emission factors. In addition, a four-step methodology is proposed to monitor the energy transition rigorously. It combines index construction and decomposition with the application of machine learning and visualization techniques in a cross-country cluster analysis and temporal mapping. EGE tracker provides a benchmark for comparing countries' sustainability performance in the electricity generation process and quantifies the effectiveness of their climate policies. The design of the index offers a novel measurement to analyze the contribution of each technology to emission reduction. The application of EGE tracker and the proposed methodology reveals a highly heterogeneous emissions reduction trend across the OECD, indicating that the process of moving away from fossil fuels varies by country and evidences different effectiveness in their climate policies. Moreover, our study highlights the potential for better utilization of renewables and the optimization of sustainable energy mix combinations, paving the way for a cleaner, greener energy future.",
        "DOI": "10.1016/j.esr.2023.101236",
        "affiliation_name": "Universidad Pontificia Comillas",
        "affiliation_city": "Madrid",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "Automated detection, categorisation and developers’ experience with the violations of honesty in mobile apps",
        "paper_author": "Obie H.O.",
        "publication": "Empirical Software Engineering",
        "citied_by": "1",
        "cover_date": "2023-11-01",
        "Abstract": "Human values such as honesty, social responsibility, fairness, privacy, and the like are things considered important by individuals and society. Software systems, including mobile software applications (apps), may ignore or violate such values, leading to negative effects in various ways for individuals and society. While some works have investigated different aspects of human values in software engineering, this mixed-methods study focuses on honesty as a critical human value. In particular, we studied (i) how to detect honesty violations in mobile apps, (ii) the types of honesty violations in mobile apps, and (iii) the perspectives of app developers on these detected honesty violations. We first develop and evaluate 7 machine learning (ML) models to automatically detect violations of the value of honesty in app reviews from an end-user perspective. The most promising was a Deep Neural Network model with F1 score of 0.921. We then conducted a manual analysis of 401 reviews containing honesty violations and characterised honesty violations in mobile apps into 10 categories: unfair cancellation and refund policies; false advertisements; delusive subscriptions; cheating systems; inaccurate information; unfair fees; no service; deletion of reviews; impersonation; and fraudulent-looking apps. A developer survey and interview study with mobile developers then identified 7 key causes behind honesty violations in mobile apps and 8 strategies to avoid or fix such violations. The findings of our developer study also articulate the negative consequences that honesty violations might bring for businesses, developers, and users. Finally, the app developers’ feedback shows that our prototype ML-based models can have promising benefits in practice.",
        "DOI": "10.1007/s10664-023-10361-4",
        "affiliation_name": "Commonwealth Scientific and Industrial Research Organisation",
        "affiliation_city": "Canberra",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Towards a model of human-cyber–physical automata and a synthesis framework for control policies",
        "paper_author": "Tang X.",
        "publication": "Journal of Systems Architecture",
        "citied_by": "4",
        "cover_date": "2023-11-01",
        "Abstract": "Advances in research and increasing applications of Cyber–Physical Systems (CPSs) show the need to consider factors of humans in the loop. This has led to the growing research focus on Human-Cyber–Physical Systems (HCPSs). In general, humans in an HCPS interact with both the cyber and physical systems, as well as among the humans themselves. For a better understanding, correct design, development, operation, and maintenance of HCPSs, a computational theory based on a computational model is required. This paper presents our initial work towards a model of human-cyber–physical automata (HCPA). We consider an HCPS as a combination of a human-physical system (HPS) and a CPS in which the control switches between the humans and the machines. We define an HCPA by connecting the automaton of the HPS and the automaton of the CPS through a switch control automaton. The switch control automaton makes switching decision in some critical states shared by the HPS and the CPS. Our theorem shows that the control switching between the HPS and the CPS increases the probability of satisfying a given property. We model the behaviour of a human in specified applications or even in carry out specific tasks, instead of general human intelligence. Therefore, a human can make mistakes to decision making and thus it is a probabilistic automaton with learning ability. The switching between the human and the machine is modelled by an oracle. The oracle learns about the human behaviour, the machine behaviour, as well as the environment to make the control decisions. To generate the control policies of the human and the oracle, we propose a synthesis framework to maximize the probability of the satisfaction of a property specified in Linear Temporal Logic (LTL) by the HCPA. We present a prototype implementation of the framework by extending the model-free reinforcement learning (RL) algorithm and model-free deep-RL algorithm, and our experiment shows that our synthesis framework is effective in obtaining switch policies.",
        "DOI": "10.1016/j.sysarc.2023.102989",
        "affiliation_name": "Southwest University",
        "affiliation_city": "Chongqing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Deep reinforcement learning for layout planning – An MDP-based approach for the facility layout problem",
        "paper_author": "Heinbach B.",
        "publication": "Manufacturing Letters",
        "citied_by": "4",
        "cover_date": "2023-11-01",
        "Abstract": "Deep Reinforcement Learning (DRL) has demonstrated operational excellence in several production-related problems. This paper applies DRL to facility layout problems (FLP) using Proximal Policy Optimisation, Advantage Actor-Critic and Deep Q-Networks. We show that the proposed approach produces an improved arrangement of facilities. The contribution of this work is the proof of concept that DRL can optimise layouts with respect to material handling costs using only an image representation of the layout and a reward signal. The approach shows potential to generalise to new layouts without the need to model or train, thus significantly speeding up layout design procedures.",
        "DOI": "10.1016/j.mfglet.2023.09.007",
        "affiliation_name": "Universität Siegen",
        "affiliation_city": "Siegen",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Landscape of multiculturalism in Australia: Tracking ethnic diversity and its relation with neighbourhood features in 2001–2021",
        "paper_author": "Wang S.",
        "publication": "Applied Geography",
        "citied_by": "4",
        "cover_date": "2023-11-01",
        "Abstract": "Embracing multiculturalism has become an important initiative for enhancing cultural identity, social cohesion, and harmony in many migrant-receiving countries. While a large body of literature has examined ethnic diversity in relation to migration, less attention has been given to tracking changes in ethnic diversity over time and from a spatial perspective. Our study aims to contribute to this gap by examining nationwide ethnic diversity in Australia from 2001 to 2021 at various spatial scales and exploring its relationship with the neighbourhood features of ethnic settlement to provide practical implications for urban planning and design. To measure ethnic diversity, we draw on the five census datasets from 2001, 2006, 2011, 2016, and 2021 and apply Shannon's diversity index to the smallest census unit, as well as to the levels of capital cities, rural areas, and states. We then employ a machine learning method to model the relationship between ethnic diversity and neighbourhood features of migrants' settlement, including demographic and socioeconomic features and housing characteristics, and to reveal changes in this relationship over time. Our key findings suggest that the level of ethnic diversity dropped during 2006–2011, possibly due to changes in migration policy in Australia, and dropped again during 2016–2021, possibly due to the outbreak of COVID-19. Neighbourhoods with high ethnic diversity are associated with populations that speak English fluently and hold high education degrees, as well as easy access to public transit and affordable housing. Our study provides practical suggestions and implications for migration and settlement policy-making and promoting better social cohesion through urban planning, neighbourhood design, and housing regulation.",
        "DOI": "10.1016/j.apgeog.2023.103114",
        "affiliation_name": "The University of Queensland",
        "affiliation_city": "Brisbane",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Assessing the spatial pattern of supply-demand mismatches in ecosystem flood regulation service: A case study in Xiamen",
        "paper_author": "Luo Z.",
        "publication": "Applied Geography",
        "citied_by": "7",
        "cover_date": "2023-11-01",
        "Abstract": "Balancing the supply and demand of urban flood regulation services is crucial for refining flood management policies. Previous studies often employed coarse evaluation units and data limitations may constrained assessments of flood regulation service demand, while this study employs finer evaluation units and an integrated methodology using open data. Taking Xiamen as a case study, machine learning and a comprehensive multi-criteria evaluation model were used to assess flood regulation service demand, hydrological modeling was conducted to evaluate the service supply. Spatial supply-demand disparities were analyzed by the supply-demand ratio. Results reveal pronounced spatial mismatches between flood regulation service supply and demand in Xiamen, with supply deficient units primarily concentrated in intense constructed areas, specifically Jimei, Huli, and Siming districts. The transferability of the method is examined, the correlation between supply-demand relationships and land use is analyzed, demonstrating the applicability of the methods in supporting similar studies in poor-data areas. The methodology could thereby facilitate spatial planning optimization and policy adjustments.",
        "DOI": "10.1016/j.apgeog.2023.103113",
        "affiliation_name": "Tianjin University",
        "affiliation_city": "Tianjin",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Who supports liberal policies? A tale of two referendums in Italy",
        "paper_author": "Madio L.",
        "publication": "Economics Letters",
        "citied_by": "1",
        "cover_date": "2023-11-01",
        "Abstract": "We leverage a unique dataset at the municipality level in Italy to examine the factors that drive support for two separate referendum campaigns — one on the decriminalization of cannabis cultivation and the other on physician-assisted suicide. Using machine learning techniques, we identify key predictors of support for both referendums, including income, population density, and political leaning of the municipality. Our analysis also highlights that local economic conditions, such as the number of firms, and educational attainment, along with exposure to organized crime, are critical factors driving mobilization in favor of the cannabis referendum. In contrast, support for legalizing assisted suicide is more likely to be explained by religiosity.",
        "DOI": "10.1016/j.econlet.2023.111338",
        "affiliation_name": "Università degli Studi di Bergamo",
        "affiliation_city": "Bergamo",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Machine learning approaches to forecasting cryptocurrency volatility: Considering internal and external determinants",
        "paper_author": "Wang Y.",
        "publication": "International Review of Financial Analysis",
        "citied_by": "15",
        "cover_date": "2023-11-01",
        "Abstract": "Given the volatile nature of cryptocurrencies, accurately forecasting cryptocurrency volatility and understanding its determinants are crucial. This paper applies machine learning (ML) techniques to forecast cryptocurrency volatility using internal determinants (e.g., lagged volatility, previous trading information) and external determinants (e.g., technology, financial, and policy uncertainty factors). Both Random Forest and Long Short-Term Memory (LSTM) networks significantly outperform traditional volatility models such as GARCH. Furthermore, we explore two optimization models—Genetic Algorithm and Artificial Bee Colony—to tune the hyper-parameters of LSTM. Our results indicate that the application of these optimization models substantially improves forecasting performance. Moreover, using SHapley Additive exPlanations, an interpretation method, we find that internal determinants play the most important roles in volatility forecasts. Finally, our results show that models trained with determinants from multiple cryptocurrencies outperform those trained with determinants from a single cryptocurrency, suggesting that considering a broader range of determinants can capture the complex dynamics in the cryptocurrency market.",
        "DOI": "10.1016/j.irfa.2023.102914",
        "affiliation_name": "University of Edinburgh Business School",
        "affiliation_city": "Edinburgh",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "A novel hybrid machine learning model for prediction of CO2 using socio-economic and energy attributes for climate change monitoring and mitigation policies",
        "paper_author": "Kumar S.",
        "publication": "Ecological Informatics",
        "citied_by": "22",
        "cover_date": "2023-11-01",
        "Abstract": "Industrial development has contributed to carbon emissions majorly, resulting in high concentrations of greenhouse gases (GHGs) in the environment leading to climate change phenomenon. Climate change threatens people in multiple ways: threatening food security, water scarcity, frequent and extreme weather events, the spread of diseases, economic losses and migration etc. The World Health Organization (WHO) declared that climate change is the greatest threat to global health in the 21st century. Since 1970, CO2 emission has increased by about 90 per cent, making it 78 per cent of the total greenhouse gas emission. Climate change impact, carbon emission factors and social-economic attributes make the prediction of GHG emission a very complex research problem having dynamic scenarios due to a large number of factors and impacting variables. Accurate prediction of carbon emission in such a scenario makes it one of the most important and challenging research works. Artificial intelligence and machine learning approaches are increasingly being used to study complex, dynamic environmental phenomenons with high variability of time, space and other factors. The research paper proposes hybrid machine learning models for the prediction of CO2 emissions using energy and social-economic variables. The work uses energy and socioeconomic variables from 1960 to 2018 to collate them to provide a new perspective on the application of machine learning approaches in the modelling and prediction of GHG emissions. The proposed hybrid model of principal component analysis (PCA) and machine learning approaches is compared on accuracy and efficiency and performs better than other machine learning and deep learning approaches such as Linear regression variants, random forest regression (RFR), support vector regression (SVR), recurrent neural network (RNN), long short term Memory (LSTM) and Tabnet etc. The proposed hybrid model reports MAE equal to 0.0307, RMSE equal to 0.0346, MAPE equals to 5.1447 and SMAPE equal to 5.2267. In terms of efficiency of computation, the proposed hybrid machine learning model PCA+LER took 12.4 ms better than other models. This proposed research work of correlation analysis and prediction can help policymakers and governments in the mitigation and management of carbon emissions.",
        "DOI": "10.1016/j.ecoinf.2023.102253",
        "affiliation_name": "University of Delhi",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Load frequency control strategy for islanded multimicrogrids with V2G dependent on learning-based model predictive control",
        "paper_author": "Fan P.",
        "publication": "IET Generation, Transmission and Distribution",
        "citied_by": "2",
        "cover_date": "2023-11-01",
        "Abstract": "System reliability and stability can be significantly improved by the interconnected operation of multimicrogrids, and electric vehicles (EVs) provide a more flexible solution for frequency control, which also present challenges for frequency control. Therefore, a load frequency control (LFC) strategy for multimicrogrids with vehicle to grid (V2G) dependent on learning-based model predictive control (MPC) is proposed. First, a controller-interconnected multimicrogrid topology is proposed; thus, a multimicrogrid consisting of microturbines (MTs), distributed power sources, and EVs and their random power constraints is established. Second, a control parameter adaptive algorithm based on learning-based MPC is designed. The real-time frequency offset and EV station output power boundary are used as the state set, adjustable parameters of the MPC controller are used as the action set, and reward function is set with frequency deviation so that the adaptive adjustment of the weight parameters of the MPC controller is realised. Additionally, the improved MPC controller designed in this paper can transform the frequency control process into an optimization problem, which is well adapted to various random constraints in the control process. In addition, the deep deterministic policy gradient (DDPG)-MPC double-layer controller can prevent machine learning controller failure. Finally, the simulation results show that, compared with traditional control and MPC algorithms, the learning-based MPC controller applied to the controller interconnection structure can exchange information between submicrogrids. Moreover, based on the experience accumulated in the prelearning process, the controller parameters can be updated according to the environmental state in real time, thereby significantly improving the robustness and rapidity of the multimicrogrid frequency control process. Meanwhile, compared with a traditional DDPG controller, the proposed controller with double-layer coupling structure can better ensure the safe operation of the multimicrogrid system when the machine learning agent fails and cannot output actions normally.",
        "DOI": "10.1049/gtd2.12994",
        "affiliation_name": "Wuhan University",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Optimisation of thermal energy storage systems incorporated with phase change materials for sustainable energy supply: A systematic review",
        "paper_author": "Odoi-Yorke F.",
        "publication": "Energy Reports",
        "citied_by": "26",
        "cover_date": "2023-11-01",
        "Abstract": "Thermal energy storage systems, also known as thermal batteries integrated with phase change materials, have gained significant attention in recent years as a promising solution for sustainable energy supply. Thermal batteries can significantly promote a sustainable energy supply by boosting the efficiency and reliability of renewable energy systems, enhancing energy access in isolated regions, lowering greenhouse gas emissions, and enhancing energy security. However, there are still challenges to optimising these systems to maximise their efficiency and effectiveness. This study presents a systematic literature review of various thermal batteries for industrial, commercial, and domestic applications. The preferred reporting items for systematic reviews and meta-analyses guidelines were adopted for this review. The primary objective was to identify factors affecting thermal battery performance. Data collection was focused on research papers published from 2013–2023 extracted from the Scopus, Web of Science, and Google Scholar databases. The study findings highlight the importance of considering material thermophysical properties, design configurations, and operating conditions when optimising thermal batteries. Also, this study highlights the current state of knowledge in the field and suggests future research and development directions. In particular, artificial intelligence and machine learning are suggested to promote faster and more precise optimisation of thermal batteries. The findings of this study are useful to academia and industries promoting the adoption of sustainable energy solutions for a greener and more resilient future.",
        "DOI": "10.1016/j.egyr.2023.09.044",
        "affiliation_name": "Cape Coast Technical University",
        "affiliation_city": "Cape Coast",
        "affiliation_country": "Ghana"
    },
    {
        "paper_title": "Part-Guided 3D RL for Sim2Real Articulated Object Manipulation",
        "paper_author": "Xie P.",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "2",
        "cover_date": "2023-11-01",
        "Abstract": "Manipulating unseen articulated objects through visual feedback is a critical but challenging task for real robots. Existing learning-based solutions mainly focus on visual affordance learning or other pre-trained visual models to guide manipulation policies, which face challenges for novel instances in real-world scenarios. In this letter, we propose a novel part-guided 3D RL framework, which can learn to manipulate articulated objects without demonstrations. We combine the strengths of 2D segmentation and 3D RL to improve the efficiency of RL policy training. To improve the stability of the policy on real robots, we design a Frame-consistent Uncertainty-aware Sampling (FUS) strategy to get a condensed and hierarchical 3D representation. In addition, a single versatile RL policy can be trained on multiple articulated object manipulation tasks simultaneously in simulation and shows great generalizability to novel categories and instances. Experimental results demonstrate the effectiveness of our framework in both simulation and real-world settings.",
        "DOI": "10.1109/LRA.2023.3313063",
        "affiliation_name": "Shanghai Artificial Intelligence Laboratory",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Signal Temporal Logic Neural Predictive Control",
        "paper_author": "Meng Y.",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "9",
        "cover_date": "2023-11-01",
        "Abstract": "Ensuring safety and meeting temporal specifications are critical challenges for long-term robotic tasks. Signal temporal logic (STL) has been widely used to systematically and rigorously specify these requirements. However, traditional methods of finding the control policy under those STL requirements are computationally complex and not scalable to high-dimensional or systems with complex nonlinear dynamics. Reinforcement learning (RL) methods can learn the policy to satisfy the STL specifications via hand-crafted or STL-inspired rewards, but might encounter unexpected behaviors due to ambiguity and sparsity in the reward. In this letter, we propose a method to directly learn a neural network controller to satisfy the requirements specified in STL. Our controller learns to roll out trajectories to maximize the STL robustness score in training. In testing, similar to Model Predictive Control (MPC), the learned controller predicts a trajectory within a planning horizon to ensure the satisfaction of the STL requirement in deployment. A backup policy is designed to ensure safety when our controller fails. Our approach can adapt to various initial conditions and environmental parameters. We conduct experiments on six tasks, where our method with the backup policy outperforms the classical methods (MPC, STL-solver), model-free and model-based RL methods in STL satisfaction rate, especially on tasks with complex STL specifications while being 10X-100X faster than the classical methods.",
        "DOI": "10.1109/LRA.2023.3315536",
        "affiliation_name": "MIT School of Engineering",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Machine learning reveals the effects of drivers on PM<inf>2.5</inf> and CO<inf>2</inf> based on ensemble source apportionment method",
        "paper_author": "Xu H.",
        "publication": "Atmospheric Research",
        "citied_by": "8",
        "cover_date": "2023-11-01",
        "Abstract": "Air pollution and climate change are currently common global concern issues. Understanding the main pollution sources of fine particulate matter (PM2.5) and carbon dioxide (CO2), and the drivers that accelerate their concentrations can provide novel insights into the co-benefits of air-quality improvement and climate change mitigation. Hence, in this study, we conducted source apportionment of PM2.5 and CO2, and obtained impacts of common sources on PM2.5-CO2 using an ensemble source apportionment method. Then, machine learning (ML) methods were utilized to study the effects of different drivers (including main pollution sources and meteorological factors) on PM2.5 and CO2. The results demonstrate that coal combustion (40%), vehicle exhaust (37%), and industrial emissions (23%) were the main common sources of PM2.5 and CO2. Controlling the combustion of fossil fuels and long-term vehicle electrification still hold significant importance for reducing pollutions and carbon emissions. The results of data-driven ML models show that temperature had the highest effect among meteorological factors, reflecting the seasonal changes of CO2 and PM2.5. The study provides a feasible framework for exploring co-benefits of clean air policies on greenhouse gas emissions based on online observation dataset.",
        "DOI": "10.1016/j.atmosres.2023.107019",
        "affiliation_name": "Tianjin University",
        "affiliation_city": "Tianjin",
        "affiliation_country": "China"
    },
    {
        "paper_title": "The peer effect of digital transformation and corporate environmental performance: Empirical evidence from listed companies in China",
        "paper_author": "Ren X.",
        "publication": "Economic Modelling",
        "citied_by": "72",
        "cover_date": "2023-11-01",
        "Abstract": "Enhancing corporate environmental performance is crucial to addressing the global climate crisis. The extant research has explored the factors influencing environmental performance at the macroeconomic level. In contrast, this study focuses on micro-firms and selects a sample of Chinese-listed companies from 2011 to 2021 to investigate how the peer effect of digital transformation impact corporate environmental performance. Machine learning and textual analysis are adopted to measure digital transformation. Environmental performance is measured by corporate carbon emission reduction and the environmental score in the environmental, social, and governance (ESG) rating. This paper confirms that the industry and regional peer effects of digital transformation contribute to environmental performance. The industry peer effect of digital transformation can improve corporate environmental performance by promoting innovation, while the regional peer effect of digital transformation can alleviate corporate financing constraints, improving its environmental performance. This paper can serve as a reference for companies to explore sustainable development and for governments to formulate environmental protection policies.",
        "DOI": "10.1016/j.econmod.2023.106515",
        "affiliation_name": "Zhongnan University of Economics and Law",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Unraveling the complex interplay between soil characteristics and radon surface exhalation rates through machine learning models and multivariate analysis",
        "paper_author": "Al-Shboul K.F.",
        "publication": "Environmental Pollution",
        "citied_by": "5",
        "cover_date": "2023-11-01",
        "Abstract": "This research seeks to elucidate the intricate interplay between soil characteristics and the rates of radon surface exhalation rate. To achieve this aim, Light Gradient Boosting Machine (LightGBM) and eXtreme Gradient Boosting (XGBoost) machine learning (ML) algorithms are employed, supported by Multivariate Analysis (MA). An analysis was performed on a collection of soil samples, examining radon surface exhalation rates and other pertinent properties such as moisture content, particle size distributions, and the concentrations of Ra-226, Th-232, and K-40. The analysis revealed several key factors influencing radon exhalation rates, namely Ra-226 concentration, moisture content, and larger soil particles. To visualize the intricate relationships between these variables, contour plots of experimental and ML-generated data were created. These visual representations demonstrated that elevated soil moisture levels decrease radon exhalation rates. In contrast, higher concentrations of Ra-226 and a greater proportion of large soil particles led to an increase in exhalation rates. This endeavor presents these complex relationships in an accessible manner, furthering our understanding of the factors in radon surface exhalation. MA techniques, including Hierarchical Cluster Analysis (HCA) and Principal Component Analysis (PCA), were initially employed to investigate the complex interactions of soil attributes on radon exhalation. HCA identified three distinct clusters but faced limitations in detecting strong negative impacts. PCA successfully captured these inverse effects, indicating that the first two principal components accounted for approximately 80% of the total variance, primarily attributed to Ra-226 concentration, moisture content, and the percentage of large soil particles. However, neither technique could quantify the effects of soil attributes on radon exhalation rates. LightGBM outperformed XGBoost, but both successfully quantified the impacts of the studied soil characteristics on radon exhalation. Sensitivity analysis confirmed the robustness and accuracy of both models. This study highlights that XGBoost and LightGBM algorithms can effectively quantify radon exhalation rates based on soil characteristics, providing valuable insights for environmental policies, land use planning, and radon mitigation strategies.",
        "DOI": "10.1016/j.envpol.2023.122440",
        "affiliation_name": "Jordan University of Science and Technology",
        "affiliation_city": "Irbid",
        "affiliation_country": "Jordan"
    },
    {
        "paper_title": "CuraZone: The tool to care for populated areas[Formula presented]",
        "paper_author": "Jardim R.",
        "publication": "Software Impacts",
        "citied_by": "0",
        "cover_date": "2023-11-01",
        "Abstract": "The COVID-19 pandemic highlighted the inadequate readiness of numerous nations to address diseases that could potentially evolve into epidemics or pandemics, posing risks to health systems and supply chains. Statistical analysis and predictive models were developed to manage COVID-19 and other diseases that harm public health. However, few public-policy decision-support tools are documented in the literature, although several governments have created them. In line with the previous developments, this tool uses socioeconomic features to model the COVID-19 province's mortality rates. This paper presents a tool to predict the mortality rate of a province using supervised learning techniques, named CuraZone. This tool was validated using 196 provinces in Peru for training and considering 31 characteristics. The tool displays the dataset's most essential characteristics, shows the country's mean square error (MSE), and predicts a province's mortality rate. In addition, the tool contributes to the field of Explainable AI (XAI), as it shows the importance of each feature. Highlighted contributions of this work include the support for the decision-making of governments or stakeholders in epidemics, providing the source code in an open and reproducible way, and the estimated mortality rate for specific populations of a neighborhood, city, or country.",
        "DOI": "10.1016/j.simpa.2023.100581",
        "affiliation_name": "Universidad del Pacífico",
        "affiliation_city": "Lima",
        "affiliation_country": "Peru"
    },
    {
        "paper_title": "Q-learning-based unmanned aerial vehicle path planning with dynamic obstacle avoidance",
        "paper_author": "Sonny A.",
        "publication": "Applied Soft Computing",
        "citied_by": "29",
        "cover_date": "2023-11-01",
        "Abstract": "Recently, unmanned aerial vehicles (UAVs) have shown promising results for autonomous sensing. UAVs have been deployed for multiple applications that include surveillance, mapping, tracking, and search operations. Finding an efficient path between a source and a goal is a critical issue that has been the focus of recent exploration. Many path-planning algorithms are utilized to find an efficient path for a UAV to navigate from a source to a goal with obstacle avoidance. Despite the extensive literature and numerous research proposals for path planning, dynamic obstacle avoidance has not been addressed with machine learning. When the obstacles are dynamic, i.e., they can change their position over time, and the constraints of the path planning algorithm become more challenging. This in turn adds a layer of complexity to the path planning algorithm. To address this challenge, a Q-learning algorithm is proposed in this work to facilitate efficient path planning for UAVs with both static and dynamic obstacle avoidance. We introduced the Shortest Distance Prioritization policy in the learning process which marginally reduces the distance that the UAV has to travel to reach the goal. Further, the proposed Q-learning algorithm adopts a grid-graph-based method to solve the path-planning problem. It learns to maximize the reward based on the agent's behavior in the environment. Through results, the performance comparison between the proposed approach and state-of-the-art path planning approaches such as A-star, Dijkstra, and Sarsa algorithms are evaluated in terms of learning time and path length. We show through results that the proposed approach results in improved performance when compared to state-of-the-art approaches. Further, the effect of an increased number of obstacles are evaluated on the performance of the proposed approach.",
        "DOI": "10.1016/j.asoc.2023.110773",
        "affiliation_name": "Universitetet i Agder",
        "affiliation_city": "Kristiansand",
        "affiliation_country": "Norway"
    },
    {
        "paper_title": "Understanding German farmers’ intention to adopt drought insurance",
        "paper_author": "Nordmeyer E.F.",
        "publication": "Journal of Environmental Management",
        "citied_by": "4",
        "cover_date": "2023-11-01",
        "Abstract": "Climate risks, particularly droughts and heat waves, negatively affect agricultural incomes worldwide. Drought insurance is promising to mitigate resulting income losses at the farm level. As the proportion of German farmers insured against drought is low, policymakers and insurers aim to increase the appeal of drought insurance to farmers. However, to accelerate their efforts in this regard, more information regarding farmers' intention to adopt drought insurance beyond current adoption is needed. To obtain initial insights, we surveyed 127 German farmers in a risk management context and applied a modified transtheoretical model of behavioral change. This revealed detailed information on the gradual adoption of drought insurance. Given a heterogenous distribution among the gradual stages of adoption, a binomial logit model was estimated instead of an ordered logit to investigate farmers' current intention to adopt drought insurance. Furthermore, the machine learning technique of least absolute shrinkage and selection operator (LASSO) was applied to select the most relevant features to be used as explanatory variables in the estimation. The results show that farmers' gender and risk attitude, land tenure, how severely they were affected previously by weather risks, and the level of trust in index insurance products have a statistically significant effect on farmers' intention to adopt drought insurance. Additionally, this study provides insights into farmers' reasons against drought insurance. As such, the results are important to policymakers considering policy intervention, insurers interested in farmers' intention to insure and to researchers focusing on farmers’ adaptation to climate change.",
        "DOI": "10.1016/j.jenvman.2023.118866",
        "affiliation_name": "Georg-August-Universität Göttingen",
        "affiliation_city": "Gottingen",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "How climate change may shift power demand in Japan: Insights from data-driven analysis",
        "paper_author": "Gurriaran L.",
        "publication": "Journal of Environmental Management",
        "citied_by": "0",
        "cover_date": "2023-11-01",
        "Abstract": "The impact of climate change on power demand in Japan and its related CO2 emissions is a matter of concern for the Japanese authorities and power companies as it may have consequences on the power grid, but is also of global importance as Japan is a significant contributor to global greenhouse gas emissions. In this study, we trained random forest models against daily power data in ten Japanese regions and for different types of power generation to project changes in future power production and its carbon intensity. We used climate variables, heat stress indices, and one variable for the level of human activities. We then used the models trained from the present-day period to estimate the future power demand, carbon intensity, and pertaining CO2 emissions over the period 2020–2100 under three Shared Socioeconomic Pathways (SSPs) scenarios (SSP126, SSP370, and SSP585). The impact of climate change on CO2 emissions via power generation shows seasonal and regional disparities. In cold regions, a decrease in power demand during winter under future warming leads to an overall decrease in power demand over the year. In contrast, the decrease in winter power demand in hot regions can be overcompensated by an increase in summer power demand due to more frequent hot days, resulting in an overall annual increase. From our regional models, power demand is projected to increase the most in most Japanese regions in May, June, September, and October rather than in the middle of summer, as found in previous studies. This increase could result in regular power outages during those months as the power grid could become particularly tense. Overall, we observed that power demand in regions with extreme climates is more sensitive to global warming than in temperate regions. The impact of climate change on power demand induces a net annual decrease in CO2 emissions in all regions except for Okinawa, in which power demand strongly increases during the summer, resulting in a net annual increase in CO2 emissions. However, climate change's impact on carbon intensity may reverse the trend in some regions (Shikoku, Tohoku). Additionally, we assessed the relative impacts of socioeconomic factors such as population, GDP, and environmental policies on CO2 emissions. When combined with these factors, we found that the climate change effect is more important than when considered individually and significantly impacts total CO2 emissions under SSP585. The contrasting results observed in the warm and cold regions of Japan can offer valuable insight into the potential future variations in energy demand and resulting CO2 emissions on a global scale.",
        "DOI": "10.1016/j.jenvman.2023.118799",
        "affiliation_name": "Université Paris-Saclay",
        "affiliation_city": "Gif-sur-Yvette",
        "affiliation_country": "France"
    },
    {
        "paper_title": "Using random forest to find the discontinuity points for carbon efficiency during COVID-19",
        "paper_author": "Qu Y.",
        "publication": "Soft Computing",
        "citied_by": "1",
        "cover_date": "2023-11-01",
        "Abstract": "As there is a constant trade-off between carbon dioxide emissions against economic growth for every government, carbon efficiency is a key indicator to guide sustainable development. However, the energy crisis and COVID-19 recovery (declined cases of COVID-19 infection, flight recovery, manufacturing restart, and increasing import and export trading) could affect carbon efficiency. Therefore, this paper combines the fuzzy regression discontinuity and random forest algorithm (RF-FRD method) to estimate the discontinuity of the energy crisis and COVID-19 recovery on carbon efficiency. The findings show that discontinuity points in carbon efficiency were induced by the energy crisis and COVID-19 recovery. The positive treatment effect at the first discontinuity point proves that the “zero-tolerance” policies effectively promote carbon efficiency. Besides, the negative treatment effect at the second discontinuity point proves that electricity rationing has not always improved carbon efficiency during the energy crisis.",
        "DOI": "10.1007/s00500-023-09179-5",
        "affiliation_name": "Adam Smith Business School",
        "affiliation_city": "Glasgow",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "A comprehensive review of machine learning algorithms and their application in geriatric medicine: present and future",
        "paper_author": "Woodman R.J.",
        "publication": "Aging Clinical and Experimental Research",
        "citied_by": "38",
        "cover_date": "2023-11-01",
        "Abstract": "The increasing access to health data worldwide is driving a resurgence in machine learning research, including data-hungry deep learning algorithms. More computationally efficient algorithms now offer unique opportunities to enhance diagnosis, risk stratification, and individualised approaches to patient management. Such opportunities are particularly relevant for the management of older patients, a group that is characterised by complex multimorbidity patterns and significant interindividual variability in homeostatic capacity, organ function, and response to treatment. Clinical tools that utilise machine learning algorithms to determine the optimal choice of treatment are slowly gaining the necessary approval from governing bodies and being implemented into healthcare, with significant implications for virtually all medical disciplines during the next phase of digital medicine. Beyond obtaining regulatory approval, a crucial element in implementing these tools is the trust and support of the people that use them. In this context, an increased understanding by clinicians of artificial intelligence and machine learning algorithms provides an appreciation of the possible benefits, risks, and uncertainties, and improves the chances for successful adoption. This review provides a broad taxonomy of machine learning algorithms, followed by a more detailed description of each algorithm class, their purpose and capabilities, and examples of their applications, particularly in geriatric medicine. Additional focus is given on the clinical implications and challenges involved in relying on devices with reduced interpretability and the progress made in counteracting the latter via the development of explainable machine learning.",
        "DOI": "10.1007/s40520-023-02552-2",
        "affiliation_name": "College of Medicine and Public Health",
        "affiliation_city": "Adelaide",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Hybrid machine learning model for hourly ozone concentrations prediction and exposure risk assessment",
        "paper_author": "Lingxia W.",
        "publication": "Atmospheric Pollution Research",
        "citied_by": "3",
        "cover_date": "2023-11-01",
        "Abstract": "With the development and improvement of optimization algorithms, machine learning models have become more important in air quality prediction. These models can be used to address problems associated with atmospheric environmental pollutants like ozone and facilitate the development of appropriate control policies. Firstly, the data sets of meteorological and pollution in Nanjing and four nearby cities from 2018 to 2021 were divided into four sections to build mixed models. The extreme gradient boosting (XGBoost) algorithm was improved by the permutation importance method (PIM) and Pearson correlation coefficient to acquire the most important features of the seasonal model and analysis of the effect on ozone prediction. The results reflect that the NO2, PM2.5, and PM10 parameters have an important influence, accounting for an average of 23% of prediction, and the average influence of the meteorological parameters of the solar radiation exceeds 11%. Secondly, the particle swarm optimization (PSO), grey wolf optimizer (GWO), and ant lion optimizer (ALO) algorithms were used to optimize the SVR model parameters to predict hourly ozone concentration in January, April, July, and September 2022. The R2 values are 0.95, 0.92, 0.85, and 0.84, respectively, and the RMSE values are 5.2, 12.4, 18.0 and 16.6 μg/m3, respectively. Finally, the impact of hourly ozone concentration on population health was studied by a risk assessment of the population density-weighted pollution exposure. The result shows that the influence on humans gradually decreased yearly from 2018 to 2021, the average variation from 2.6% to −1.1%, and health risks during the evening rush hours are still prevalent and require further attention.",
        "DOI": "10.1016/j.apr.2023.101916",
        "affiliation_name": "Nanjing University of Information Science &amp; Technology",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Do municipal mergers reduce the cost of waste management? Evidence from Japan",
        "paper_author": "Li J.",
        "publication": "Regional Science and Urban Economics",
        "citied_by": "2",
        "cover_date": "2023-11-01",
        "Abstract": "This study investigates whether municipal mergers result in lower waste management costs. We develop a novel virtual merging method based on machine learning techniques to compile the data of the control group and estimate the effect of the large-scale consolidation in Japan on the various costs of managing municipal solid waste. We find that these mergers actually led to an increase in the total cost per ton. Estimation results also reveal that the construction cost increased in the post-merger period, which can be explained by the special bonds provided by the national government for new projects in merged municipalities. By contrast, the processing and management cost is little affected by the mergers, except for the absorption-type mergers and municipalities that never joined waste management associations. These results suggest that municipal mergers might not always bring a substantial scale economy in municipal solid waste management. Policymakers should be careful when promoting municipal mergers in the belief that a scale economy will prevail after the reform.",
        "DOI": "10.1016/j.regsciurbeco.2023.103939",
        "affiliation_name": "Graduate School of Global Environmental Studies",
        "affiliation_city": "Kyoto",
        "affiliation_country": "Japan"
    },
    {
        "paper_title": "Security and privacy problems in voice assistant applications: A survey",
        "paper_author": "Li J.",
        "publication": "Computers and Security",
        "citied_by": "8",
        "cover_date": "2023-11-01",
        "Abstract": "Voice assistant applications have become omniscient nowadays. Two models that provide the two most important functions for real-life applications (i.e., Google Home, Amazon Alexa, Siri, etc.) are Automatic Speech Recognition (ASR) models and Speaker Identification (SI) models. According to recent studies, security and privacy threats have also emerged with the rapid development of the Internet of Things (IoT). The security issues researched include attack techniques toward machine learning models and other hardware components widely used in voice assistant applications. The privacy issues include technical-wise information stealing and policy-wise privacy breaches. The voice assistant application takes a steadily growing market share every year, but their privacy and security issues never stopped causing huge economic losses and endangering users' personal sensitive information. Thus, it is important to have a comprehensive survey to outline the categorization of the current research regarding the security and privacy problems of voice assistant applications. This paper concludes and assesses five kinds of security attacks and three types of privacy threats in the papers published in the top-tier conferences of cyber security and voice domain.",
        "DOI": "10.1016/j.cose.2023.103448",
        "affiliation_name": "Swinburne University of Technology",
        "affiliation_city": "Hawthorn",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "A critical review of green growth indicators in G7 economies from 1990 to 2019",
        "paper_author": "Herman K.S.",
        "publication": "Sustainability Science",
        "citied_by": "8",
        "cover_date": "2023-11-01",
        "Abstract": "Green growth policies aim to address both climate change and economic growth and are now prevalent throughout many economies. While green growth is sufficiently assessed in qualitative, case-study-based literature, quantitative and cross-country analyses are still limited. In response to this research deficit, our aim is twofold: (1) to develop a classification framework to quantitatively analyse green growth and (2) to identify key policy inputs and techno-economic or environmental outputs for green growth through a novel taxonomy. We focus on the G7 countries, since they have, historically, tended to align their economic policies. We employ a machine-automated K-means clustering algorithm, as well as correlation analyses, to assess where green growth “win–wins,” or co-benefits to the economy and environment, might exist. Our findings suggest that enthusiasm should be tempered for public policy commitments for green growth; despite unified green growth policy in G7 countries—significant differences are observed for both policy inputs and environmental/economic outputs. As a result, we caution policymakers and researchers against drawing generalised conclusions about the effectiveness of green growth policies, even among highly developed economies. Finally, our research draws attention to data deficiencies which, evidently, reduce more robust assessment across countries and over time.",
        "DOI": "10.1007/s11625-023-01397-y",
        "affiliation_name": "University of Sussex Business School",
        "affiliation_city": "Brighton",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "An imbalanced classification approach for establishment of cause-effect relationship between Heart-Failure and Pulmonary Embolism using Deep Reinforcement Learning",
        "paper_author": "Firdous N.",
        "publication": "Engineering Applications of Artificial Intelligence",
        "citied_by": "4",
        "cover_date": "2023-11-01",
        "Abstract": "Pulmonary Embolism (PE) — a non-cardiac cause of cardiac arrest is a strenuous job to perform as it is non-specific in presentation and shares various overlapping features with various other clinical disorders like Myocardial Infarction, Pneumonia, etc. This paper delineates a method to identify Pulmonary Embolism (PE) as a reason for Heart Failure using Deep Reinforcement Learning (DRL) algorithm. The methodology has been partitioned into two phases. Phase I acts towards the creation of a novel dataset as there was no data available for this particular problem statement. Phase II deals with the scope and application of the DRL algorithm on the above-invented dataset. The dataset formed is imbalanced in its essence. To effectively tackle the imbalanced dataset challenge, a cutting-edge imbalanced classification algorithm rooted in the power of Deep Reinforcement Learning (DRL) has been embraced. The classification model has been drawn up as a Sequential Decision-Making procedure and is resolved by making use of the DRL algorithm viz Double Deep Q-Network (DDQN). A classification action is performed by an agent on a single sample at each time step. Classification actions are assessed by the environment, based on the assessment, a reward is given to an agent. In order to make the agent more responsive towards the minority class samples, rewards belonging to the minority class are higher. Under the supervision of this reward system, the agent finally learns the optimal classification policy for this imbalanced dataset problem. Comparative analysis of the DDQN algorithm with other Deep Reinforcement Learning algorithms, including Dueling DQN, and Proximal Policy Optimization (PPO) algorithms has been performed in order to arrive at the best learning technique for the dataset. Furthermore, the DDQN algorithm has been evaluated against a selection of Machine Learning (ML) algorithms. From the experimentation, it is demonstrated that the DDQN model outperformed other approaches with an accuracy of 99.99%, G-mean 0.9999, Recall 1.0000, and Specificity 0.9999.",
        "DOI": "10.1016/j.engappai.2023.107004",
        "affiliation_name": "Regional Institute of Management and Technology",
        "affiliation_city": "Mandi Gobindgarh",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Biofuels for a sustainable future: Examining the role of nano-additives, economics, policy, internet of things, artificial intelligence and machine learning technology in biodiesel production",
        "paper_author": "Shelare S.D.",
        "publication": "Energy",
        "citied_by": "61",
        "cover_date": "2023-11-01",
        "Abstract": "As the global population and economy grow, so does the energy demand. Over-reliance on non-renewable resources leads to depletion and price spikes, making renewable alternatives necessary. Biodiesel is an eco-friendly and non-toxic fuel that closely resembles traditional fossil fuels. It is produced from various sources, including animal fat, palm oil, and non-edible plant oil. Biodiesel releases fewer harmful air pollutants and greenhouse gases than fossil fuels and is simpler to manage. Despite these advantages, it cannot replace traditional diesel fuel on a large scale. This overview summarizes biodiesel production, explaining the different types of feedstock utilized and their benefits and drawbacks. Various biodiesel production methodologies are discussed. The primary objective of this article is to inform engineers, industrialists, and researchers involved in waste biodiesel, as well as to highlight waste biodiesel as a potential substitute for fossil fuels. This review article discusses the nano-additives in biodiesel and applications of internet of things, artificial intelligence, and machine learning in biofuel. This review shows that nano-additives can potentially improve biodiesel fuel properties, favorable economic and policy environments promoting biodiesel production, and internet of things, artificial intelligence, and machine learning technologies optimize the biodiesel production processes. These advances can help promote biodiesel as a cleaner, renewable energy source, lowering the consumption of fossil fuels. It also suggests further biofuel development by improving efficiency, expanding feedstock options, creating policy support, developing infrastructure, and increasing public awareness.",
        "DOI": "10.1016/j.energy.2023.128874",
        "affiliation_name": "Priyadarshini College of Engineering, Nagpur",
        "affiliation_city": "Nagpur",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Assessing the impact of urban form and urbanization process on tropospheric nitrogen dioxide pollution in the Yangtze River Delta, China",
        "paper_author": "Gao Y.",
        "publication": "Environmental Pollution",
        "citied_by": "11",
        "cover_date": "2023-11-01",
        "Abstract": "Optimizing urban form through urban planning and management policies can improve air quality and transition to demand-side control. Nitrogen dioxide (NO2) in the urban atmosphere, mainly emitted by anthropogenic sources such as industry and vehicles, is a key precursor of fine particles and ozone pollution. Both NO2 and its secondary pollutants pose health risks for humans. Here we assess the interactions between urban forms and airborne NO2 pollution in different cities with various stages of urbanization in the Yangtze River Delta (YRD) in China, by using the machine learning and geographical regression model. The results reveal a strong correlation between urban fragmentation and tropospheric NO2 vertical column density (TVCD) in YRD cities in 2020, particularly those with lower or higher levels of urbanization. The correlation coefficients (R2) between NO2 TVCD and the largest patch index (a metric of urban fragmentation) in different cities are greater than 0.8. For cities at other urbanization stages, population and road density are strongly correlated with NO2 TVCD, with an R2 larger than 0.61. This study highlights the interdependence among urbanization, urban forms, and air pollution, emphasizing the importance of customized urban landscape management strategies for mitigating urban air pollution.",
        "DOI": "10.1016/j.envpol.2023.122436",
        "affiliation_name": "Anhui Institute of Optics and Fine Mechanics",
        "affiliation_city": "Hefei",
        "affiliation_country": "China"
    },
    {
        "paper_title": "An innovative model fusion algorithm to improve the recall rate of peer-to-peer lending default customers",
        "paper_author": "Liu Z.",
        "publication": "Intelligent Systems with Applications",
        "citied_by": "9",
        "cover_date": "2023-11-01",
        "Abstract": "Peer-to-peer (P2P) lending is a fintech innovation that provides loans to individuals and businesses without the need for additional intermediaries. However, if the borrower fails to repay the loan on time, the bank suffers a financial loss due to the borrower's default. At present, many studies are trying to improve the accuracy of credit default risk prediction models to reduce the risk of financial institutions' loan business, but it is also a meaningful study to focus on improving the recall rate of model prediction results. In the related research field of credit default risk prediction, improving the recall rate is crucial for banks and other lending institutions. The recall rate refers to the proportion of all true positive examples that are correctly identified as positive examples. In credit default risk prediction, true positive cases refer to the cases where borrowers default, while being correctly identified as positive cases means that the model can accurately predict which borrowers may default. If banks' risk prediction models can improve recall rates, this can help them better assess risk, formulate appropriate lending policies, and minimize default losses. This study aims to further improve the recall and AUC metrics (area under the ROC curve) of P2P credit default prediction using the lending dataset from Lending Club using an improved machine learning model fusion algorithm. Our proposed algorithm consists of two machine learning algorithms. The improved LightGBM algorithm and the improved XGBoost algorithm are used for model fusion to obtain the LGB-XGB-Stacking model. By optimizing the evaluation metrics in the training phase of these two algorithms, we have achieved significant improvement in results, especially in the recall rate of defaulted customers and the overall AUC metrics. After comparing the predictive performance of the models, our proposed predictive model is improved in the following aspects. First, the recall of our proposed prediction model is significantly better than other models. Second, it also outperforms other machine learning models on the AUC metric. Among them, the recall rate of the positive sample (default customer) is 24.43% higher than that of the XGBoost model, and the overall AUC index is 6.71% higher. In the end, it was found that XGBoost, LightGBM, and CatBoost models performed very well in terms of accuracy rate improvement. The accuracy rates of these three models are very close, and they are all higher than other models. Therefore, it is found that machine learning models are still an effective method for credit default prediction research, especially tree models. Although the accuracy of our model is slightly lower than the above models, our proposed model outperforms the above models in identifying defaulting customers. Our model can more accurately identify defaulting customers and minimize the risk of bad debts for financial institutions.",
        "DOI": "10.1016/j.iswa.2023.200272",
        "affiliation_name": "Xinjiang University",
        "affiliation_city": "Urumqi",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Use of Real-World Evidence for Drug Regulatory Decisions in China: Current Status and Future Directions",
        "paper_author": "Li P.",
        "publication": "Therapeutic Innovation and Regulatory Science",
        "citied_by": "6",
        "cover_date": "2023-11-01",
        "Abstract": "Real-world data (RWD) and real-world evidence (RWE) have garnered great interest for supporting drug research and development (R&D) by medical researchers and regulators in recent years. The application and development of RWD/E in drug regulatory decision-making have been vigorously promoted in China. This study seeks to provide a broad overview of how RWE has been contributing to drug regulatory decisions in China. In this paper, we review the development of RWD and RWE, summarize key elements that promote application of RWE, introduce relevant methods and guidelines, elaborate on the opportunities and challenges of RWE in regulatory decision-making in China, and put forward suggestions to promote the application of RWE in China’s regulatory decision-making and to further facilitate innovative drug evaluation and regulation.",
        "DOI": "10.1007/s43441-023-00555-9",
        "affiliation_name": "Shenyang Pharmaceutical University",
        "affiliation_city": "Shenyang",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Artificial Intelligence text generators for overcoming language barriers in ecological research communication",
        "paper_author": "Zenni R.D.",
        "publication": "Austral Ecology",
        "citied_by": "2",
        "cover_date": "2023-11-01",
        "Abstract": "Language barriers can impede the dissemination of research findings, restrict collaboration and exclude non-English-speaking researchers from the global scientific community. To overcome this challenge, we explore the potential of Generative Artificial Intelligence (GenAI) text generators to assist non-anglophone researchers in producing high-quality academic texts for publication in scientific journals, with a focus on the field of ecological research. These tools can produce grammatically correct, coherent and contextually appropriate text, improving scientific communication quality. Improving scientific communication is vital in Ecology, where research findings can have important implications for the environment and public policy. GenAI text generators can generate summaries of research findings, abstracts and social media posts promoting research findings. Nonetheless, researchers must exercise caution and use these tools together with human review and editing to ensure accuracy and clarity. As natural language processing and machine learning continue to evolve, the use of GenAI text generators in scientific communication is poised to become increasingly important.",
        "DOI": "10.1111/aec.13417",
        "affiliation_name": "Universidade Federal de Lavras",
        "affiliation_city": "Lavras",
        "affiliation_country": "Brazil"
    },
    {
        "paper_title": "A survey on Evolutionary Reinforcement Learning algorithms",
        "paper_author": "Zhu Q.",
        "publication": "Neurocomputing",
        "citied_by": "24",
        "cover_date": "2023-11-01",
        "Abstract": "Reinforcement Learning (RL) has proven to be highly effective in various real-world applications. However, in certain scenarios, Evolutionary Algorithms (EAs) have been utilized as an alternative to RL algorithms. Recently, Evolutionary Reinforcement Learning algorithms (ERLs) have emerged as a promising solution that combines the advantages of both RL and EA. This paper presents a comprehensive survey that encompasses a majority of the studies in this exciting research area. We classify these ERLs according to the EA used in their frameworks and analyze the strengths and limitations of various EA components and combination schemes. Additionally, we conduct several experiments to evaluate the performance of some representative ERLs. By categorizing the different approaches and assessing their effectiveness, the paper can assist researchers and practitioners in selecting the most suitable method for their particular application.",
        "DOI": "10.1016/j.neucom.2023.126628",
        "affiliation_name": "Shenzhen University",
        "affiliation_city": "Shenzhen",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Modeling and forecasting electricity consumption amid the COVID-19 pandemic: Machine learning vs. nonlinear econometric time series models",
        "paper_author": "Charfeddine L.",
        "publication": "Sustainable Cities and Society",
        "citied_by": "21",
        "cover_date": "2023-11-01",
        "Abstract": "Accurately modeling and forecasting electricity consumption remains a challenging task due to the large number of the statistical properties that characterize this time series such as seasonality, trend, sudden changes, slow decay of autocorrelation function, among many others. This study contributes to this literature by using and comparing four advanced time series econometrics models, and four machine learning and deep learning models to analyze and forecast electricity consumption during COVID-19 pre-lockdown, lockdown, releasing-lockdown, and post-lockdown phases. Monthly data on Qatar's total electricity consumption has been used from January 2010 to December 2021. The empirical findings demonstrate that both econometric and machine learning models are able to capture most of the important statistical features characterizing electricity consumption. In particular, it is found that climate change based factors, e.g temperature, rainfall, mean sea-level pressure and wind speed, are key determinants of electricity consumption. In terms of forecasting, the results indicate that the autoregressive fractionally integrated moving average and the three state autoregressive Markov switching models with exogenous variables outperform all other models. Policy implications and energy-environmental recommendations are proposed and discussed.",
        "DOI": "10.1016/j.scs.2023.104860",
        "affiliation_name": "Department of Humanities, College of Arts and Sciences, Qatar University",
        "affiliation_city": "Doha",
        "affiliation_country": "Qatar"
    },
    {
        "paper_title": "Prediction of building energy performance using mathematical gene-expression programming for a selected region of dry-summer climate",
        "paper_author": "Alzara M.",
        "publication": "Engineering Applications of Artificial Intelligence",
        "citied_by": "17",
        "cover_date": "2023-11-01",
        "Abstract": "Developing energy-efficient buildings considering building design parameters can help reduce buildings' energy consumption. The energy efficiency of residential buildings is considered a top priority for the energy policies of a country. Thus, this study utilizes gene-expression programming (GEP) to estimate the energy performance of residential buildings. The energy consumption evaluations were carried out using the Etotect energy simulation software. Eight building parameters with 768 data points were considered to generate the database for the heating load (HL) and cooling load (CL), including relative compactness, surface area, wall area, roof area, overall building height, glazing orientation, glazing area, and distribution of glazing area. Different GEP predictive models with varying parameters for building HL and CL were developed, and the best-performing prediction model was selected. In addition, several statistical indices were utilized to measure the accuracy of the proposed GEP models. The results revealed that GEP14 gives the most robust prediction model for HL having R2-value greater than 0.9 for both the training and validation set. Likewise, R2-value >0.9 is achieved for best- CL (GEP11). Furthermore, the mean absolute error (MAE) values for both the predictive HL’ and CL’ by prediction models were relatively less for both the training and testing databases. The sensitivity and parametric analysis reveal that the overall height (Ho), roof area (Af), and glazing area (Ag) were the most influential parameters for both predictive models. Thus, GEP results demonstrate the robust performance in predicting the building energy.",
        "DOI": "10.1016/j.engappai.2023.106958",
        "affiliation_name": "Faculty of Engineering",
        "affiliation_city": "Mansoura",
        "affiliation_country": "Egypt"
    },
    {
        "paper_title": "A novel approach for the prediction and analysis of daily concentrations of particulate matter using machine learning",
        "paper_author": "Panneerselvam B.",
        "publication": "Science of the Total Environment",
        "citied_by": "4",
        "cover_date": "2023-11-01",
        "Abstract": "Traditional air quality analysis and prediction methods depend on the statistical and numerical analyses of historical air quality data with more information related to a specific region; therefore, the results are unsatisfactory. In particular, fine particulate matter (PM2.5, PM10) in the atmosphere is a major concern for human health. The modelling (analysis and prediction) of particulate matter concentrations remains unsatisfactory owing to the rapid increase in urbanization and industrialization. In the present study, we reconstructed a prediction model for both PM2.5 and PM10 with varying meteorological conditions (windspeed, temperature, precipitation, specific humidity, and air pressure) in a specific region. In this study, a prediction model was developed for the two observation stations in the study region. The analysis of particulate matter shows that seasonal variation is a primary factor that highly influences air pollutant concentrations in urban regions. Based on historical data, the maximum number of days (92 days in 2019) during the winter season exceeded the maximum permissible level of particulate matter (PM2.5 = 15 μg/m3) concentration in air. The prediction results showed better performance of the Gaussian process regression model, with comparatively larger R2 values and smaller errors than the other models. Based on the analysis and prediction, these novel methods may enhance the accuracy of particulate matter prediction and influence policy- and decision-makers among pollution control authorities to protect air quality.",
        "DOI": "10.1016/j.scitotenv.2023.166178",
        "affiliation_name": "Aryabhatta Research Institute of Observational Sciences",
        "affiliation_city": "Nainital",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Multimodal learning analytics—In-between student privacy and encroachment: A systematic review",
        "paper_author": "Prinsloo P.",
        "publication": "British Journal of Educational Technology",
        "citied_by": "9",
        "cover_date": "2023-11-01",
        "Abstract": "Abstract: Since the emergence of learning analytics (LA) in 2011 as a distinct field of research and practice, multimodal learning analytics (MMLA), shares an interdisciplinary approach to research and practice with LA in its use of technology (eg, low cost sensors, wearable technologies), the use of artificial intelligence (AI) and machine learning (ML), and the provision of automated, mostly real-time feedback to students and instructors. Much of MMLA takes place in experimental and laboratory settings, researching students' learning in in-between spaces—between research and classroom application, in-between students' learning in private and public spaces as researchers track students' learning both in their use of social media and connected devices, and through the use of context-aware and adaptive devices; and lastly, in-between respecting students' privacy while increasingly using intrusive technologies. This study seeks to establish what is known about MMLA in terms of rationale for applications, the nature and scope of data collected, the study contexts, evidence of commercial interests and/or downstream uses of students' data, and consideration of ethics, privacy, and the protection of student data. This systematic review analysed 192 articles using a search string consisting of various combinations of multimodal (data) and learning analytics. The main findings include, inter alia, that though MMLA provide insights into learning and teaching, there is little evidence of MMLA findings successfully being applied to classroom settings, at scale. Given that the nature of MMLA research often necessitates the use of a range of (intrusive) sensors and recording technologies and can include children in its samples; the encroachment of students' right to privacy is a huge concern that is not addressed. There is also a need to reconsider the rationale for collecting multimodal data, the conditions under which such data collection will be ethical and in service of students' wellness, and the boundaries that should protect their (multimodal) data. Practitioner notes What is already known about this topic Experimental educational research predates both multimodal learning analytics (MMLA) and educational data mining (EDM). MMLA has been an integral part of learning analytics as research focus and practice since its inception. The increased digitisation and datafication, advances in technology (eg, sensor technology, geo-tracking, etc.) and a growing normalisation of wearable technologies provides greater scope for collecting multimodal data. There is a vast body of published MMLA research providing a range of insight into students' and educators' behaviours aiming to increase the effectiveness of teaching. What this paper adds While there are several studies providing insights into the state of MMLA research, this study provides insight to a selected range of factors in MMLA research such as the type of research (empirical/conceptual); the nature and scope of data collected; the sample populations (pre-higher education, higher education, etc.); evidence of commercial interests or consideration of downstream uses; issues pertaining to consent, privacy, data protection and ethics; and evidence of how findings were used to improve teaching and learning. The vast majority of MMLA research targets higher education, is empirical in nature and is based on relatively small samples of participation in experimental settings. Confirms previous research that found the predominance of small samples, and a lack of replicability and, as a result, lacking scalability. That there is very little explicit discussion of the ethical and privacy implications and data protection, either at design stage or for future implementation. Similarly, that there is little consideration of potential commercial interests or downstream uses of data. Implications for practice and/or policy MMLA in its essence requires interdisciplinary approaches and teams. For MMLA to move beyond small-scale, experimental settings to application in real (classroom) settings, larger, replicable studies should be conducted together with ways to make study findings actionable for teachers and students. Ethical issues, commercial interests and downstream uses of collected data must be considered within the design and approval of MMLA research.",
        "DOI": "10.1111/bjet.13373",
        "affiliation_name": "Universitetet i Bergen",
        "affiliation_city": "Bergen",
        "affiliation_country": "Norway"
    },
    {
        "paper_title": "Rebuilding high-quality near-surface ozone data based on the combination of WRF-Chem model with a machine learning method to better estimate its impact on crop yields in the Beijing-Tianjin-Hebei region from 2014 to 2019",
        "paper_author": "Han T.",
        "publication": "Environmental Pollution",
        "citied_by": "7",
        "cover_date": "2023-11-01",
        "Abstract": "In recent years, the problem of surface ozone pollution in China has been of great concern. According to observation data from monitoring stations, the concentration of near-surface ozone (O3) in China has gradually increased in recent years, and ozone concentration often exceeds the contaminant limit standard, especially in the Beijing-Tianjin-Hebei (BTH) region. High O3 concentration pollution will adversely affect crop growth, which can cause crop yield losses. Therefore, it is urgent to recognize the situation of ozone pollution in the BTH region and quantitatively evaluate the crop yield losses caused by ozone pollution to develop more effective pollution prevention and control policies. However, the monitoring of ozone concentration in China started relatively late compared with some developed countries, and currently, long-time series data covering the BTH region cannot be obtained, which makes it difficult to evaluate the impact of ozone on crop yield. Therefore, a new method (WRFC-XGB) was proposed in this study to establish a high-precision near-surface O3 concentration dataset covering the whole BTH region from 2014 to 2019 by integrating the Weather Research and Forecasting with Chemistry (WRF-Chem) model with the extreme gradient boosting (XGBoost) machine learning algorithm. Through verification with ground observation station data, the results of WRFC-XGB are satisfactory, and R2 can reach 0.78–0.91. Compared with other algorithms, the accuracy of the near-surface ozone concentration dataset is greatly improved, which can be used to estimate the impact of surface ozone on crop yield. Based on this dataset, the yield loss of winter wheat, rice, and maize caused by O3 pollution was estimated by using the response equation of the relative yield and ozone dose index. The results showed that the total yield losses of winter wheat, rice and maize from 2014 to 2019 were 2659.21 million tons, 49.23 million tons and 1721.56 million tons due to ozone pollution in the BTH region, respectively, and the highest relative yield loss of crops caused by O3 pollution could be 29.37% during 2014–2019, which indicated that the impact of ozone pollution on crop yield cannot be ignored, and effective measures need to be developed to control ozone pollution, prevent crop production loss, and ensure people's food security.",
        "DOI": "10.1016/j.envpol.2023.122334",
        "affiliation_name": "China Meteorological Administration",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Mapping center pivot irrigation systems in global arid regions using instance segmentation and analyzing their spatial relationship with freshwater resources",
        "paper_author": "Chen F.",
        "publication": "Remote Sensing of Environment",
        "citied_by": "9",
        "cover_date": "2023-11-01",
        "Abstract": "Ensuring freshwater availability and supply sufficient for socio-economic development is important for human health and productivity. Globally, increasing industrial agriculture is driving crop productivity, with increasing surface water and groundwater being withdrawn for irrigation. This study mapped center pivot irrigation systems (CPIS), which are indicators of intensified industrial agriculture and large-scale investment in agriculture, with satellite data using a convolutional neural network in global arid areas. A cascade instance segmentation network was adopted, and both the convolutional block attention module technique and PointRend technique were used to increase the performance of off-the-shelf algorithms. Geospatial analysis methods were used to derive the relationships between CPIS and freshwater resources. Overall there were about 10.26 million hectares (Mha) of irrigated areas equipped with CPIS in global arid areas in 2021, and there were 4.41 Mha of CPIS-equipped irrigated fields added in the past twenty years at a relative increase of 75% compared with those at the beginning of this century. Although the largest extent of increased CPIS-equipped irrigated fields was in North America, the other five continents had higher relative increases, with Africa having the highest relative growth of 253%. The analysis of spatial relationships between CPIS and freshwater resources indicates that the stress on freshwater resources from industrial agriculture is intensifying in arid areas globally. Overall, the world's arid regions with the highest water stress from CPIS are mostly located in western North America, northern Africa, and the Arabian Peninsula. The results facilitate a better understanding of the current status of freshwater stress and ecosystem sustainability in relation to the development of agricultural intensification in global arid areas. The outcomes support governments to judge better the environmental consequences of agricultural modernization and further help them make balanced agriculture and water management policies.",
        "DOI": "10.1016/j.rse.2023.113760",
        "affiliation_name": "College of Science and Engineering",
        "affiliation_city": "Adelaide",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Role of National Conditions in Occupational Fatal Accidents in the Construction Industry Using Interpretable Machine Learning Approach",
        "paper_author": "Koc K.",
        "publication": "Journal of Management in Engineering",
        "citied_by": "8",
        "cover_date": "2023-11-01",
        "Abstract": "Current national occupational safety and health (OSH) initiatives follow reactive approaches, i.e., if it breaks, fix it. Existing accounts, however, failed to improve national OSH performances substantially, which imposes the need for an in-depth and proactive (fix it so it will not break) investigation of national occupational fatality risks. Despite many studies examining the fatality risk of workers based on project-, company-, and/or behavior-related factors, the role of national conditions on the countrywide fatality risk of workers has not been explored. The present research leverages the national statistics of Turkey to examine their influence on construction workers' fatality risk through a machine learning-based prediction model. Several widely used machine learning methods were adopted for determining whether the upcoming month poses a significant fatality risk for construction workers or not based on national statistics of the previous month. According to analysis results, the gradient boosting decision tree algorithm yielded the highest prediction performance in terms of f1-score. The recently developed game theory-based Shapley Additive Explanations (SHAP) algorithm was used to identify whether and how national conditions affect countrywide fatality risk of construction workers. Findings illustrate that the share of the construction sector in employment, market demand, and labor shortage are the most significant national factors in determining the fatality risk. SHAP summary and SHAP dependence plots are further presented to provide decision makers with a clearer understanding of hidden relationships between fatality risk and national conditions. In addition, a framework that can be practically used by policy makers and governmental authorities is developed to help minimize national occupational fatality risk. Overall, predicting national fatality risk in the industry and identifying the national precursors of occupational fatalities contribute to the development of macrolevel safety improvements based on country-specific conditions.",
        "DOI": "10.1061/JMENEA.MEENG-5516",
        "affiliation_name": "Yıldız Teknik Üniversitesi",
        "affiliation_city": "Istanbul",
        "affiliation_country": "Turkey"
    },
    {
        "paper_title": "Finite-Set Direct Torque Control via Edge-Computing-Assisted Safe Reinforcement Learning for a Permanent-Magnet Synchronous Motor",
        "paper_author": "Schenke M.",
        "publication": "IEEE Transactions on Power Electronics",
        "citied_by": "11",
        "cover_date": "2023-11-01",
        "Abstract": "Advances in the field of reinforcement learning (RL)-based drive control allow formulation of holistic optimization goals for the data-driven training phase. The resulting controllers feature efficient drive operation without the necessity of an a priori known plant model but, so far, conduction of the corresponding training phase in real-world drive systems has been applied only sparsely due to safety concerns. This contribution targets the challenging problem of self-learning torque control for a permanent-magnet synchronous motor assuming a finite control set, i.e., the direct selection of switching actions instead of a modulator-based setup. In order to allow a secure and effective online training with real-world drive systems, the RL controller is monitored by a safeguarding algorithm that prevents application of unsafe switching actions, e.g., such that result in overcurrent. The accruing amount of measurement data is handled with the use of an edge-computing pipeline to outsource the RL training from the embedded control hardware. The inference of the utilized artificial neural network in hard real time is realized with the use of a reconfigurable field-programmable gate array architecture. The resulting RL-based algorithm is able to learn a torque control policy in just 10 min, which has been validated during comprehensive real-world experiments.",
        "DOI": "10.1109/TPEL.2023.3303651",
        "affiliation_name": "Paderborn University",
        "affiliation_city": "Paderborn",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Analysis of spatial and temporal carbon emission efficiency in Yangtze River Delta city cluster — Based on nighttime lighting data and machine learning",
        "paper_author": "Sun Q.",
        "publication": "Environmental Impact Assessment Review",
        "citied_by": "18",
        "cover_date": "2023-11-01",
        "Abstract": "Improving carbon emission efficiency(CEE) is crucial to reducing CO2 emissions. Most studies on CO2 emission are conducted at national and industrial scales, and city-scale studies still need to be included. In order to collect more consistent city- and county-scale CO2 emission data, the sparrow search neural network is trained to fit the energy consumption CO2 emissions with nighttime light in this study. Additionally, using the SBM-DEA model and spatial econometric techniques, the CEE values of 27 cities in the Yangtze River Delta region (YRDR) from 2000 to 2020 were examined from the perspective of total factor inputs. The findings demonstrate that CEE's general trend is erratic and uneven. The CEE value of the YRDR decreases from 0.720 in 2000 to 0.628 by 2020, which means that the YRDR has redundant capital and labour inputs and insufficient economic output. The low value carbon efficiency areas are mainly concentrated in the western part of the YRDR, i.e. the Anhui Province region. Shanghai, Wuxi and Suzhou have high carbon efficiency values of 1.21, 2.08 and 1.00 respectively, and are exemplary cities in terms of carbon efficiency, while the rest of the cities have varying degrees of efficiency loss. Taking Chizhou-Jiaxing as the middle line, the CEE pattern in the YRDR presents a state of “low in the middle and high at each end,” and center of gravity for CEE generally shifts southward. Additionally, the cold-spot areas of CEE are concentrated in the southern part of Anhui Province, and develop a low-efficiency zone with Chizhou, Anqing, and Xuancheng as clusters and spreading outwards. Overall, this paper significantly narrows the spatial scale of carbon accounting studies and the findings can be applied to the formulation of customized carbon reduction policies.",
        "DOI": "10.1016/j.eiar.2023.107232",
        "affiliation_name": "China University of Mining and Technology",
        "affiliation_city": "Xuzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Reconstructing annual XCO<inf>2</inf> at a 1 km×1 km spatial resolution across China from 2012 to 2019 based on a spatial CatBoost method",
        "paper_author": "Wu C.",
        "publication": "Environmental Research",
        "citied_by": "15",
        "cover_date": "2023-11-01",
        "Abstract": "Long-time-series, high-resolution datasets of the column-averaged dry-air mole fraction of carbon dioxide (XCO2) have great practical importance for mitigating the greenhouse effect, assessing carbon emissions and implementing a low-carbon cycle. However, the mainstream XCO2 datasets obtained from satellite observations have coarse spatial resolutions and are inadequate for supporting research applications with different precision requirements. Here, we developed a new spatial machine learning model by fusing spatial information with CatBoost, called SCatBoost, to fill the above gap based on existing global land-mapped 1° XCO2 data (GLM-XCO2). The 1-km-spatial-resolution dataset containing XCO2 values in China from 2012 to 2019 reconstructed by SCatBoost has stronger and more stable predictive power (confirmed with a cross-validation (R2 = 0.88 and RSME = 0.20 ppm)) than other traditional models. According to the estimated dataset, the overall national XCO2 showed an increasing trend, with the annual mean concentration rising from 392.65 ppm to 410.36 ppm. In addition, the spatial distribution of XCO2 concentrations in China reflects significantly higher concentrations in the eastern coastal areas than in the western inland areas. The contributions of this study can be summarized as follows: (1) It proposes SCatBoost, integrating the advantages of machine learning methods and spatial characteristics with a high prediction accuracy; (2) It presents a dataset of fine-scale and high resolution XCO2 over China from 2012 to 2019 by the model of SCatBoost; (3) Based on the generated data, we identify the spatiotemporal trends of XCO2 in the scale of nation and city agglomeration. These long-term and high resolution XCO2 data help understand the spatiotemporal variations in XCO2, thereby improving policy decisions and planning about carbon reduction.",
        "DOI": "10.1016/j.envres.2023.116866",
        "affiliation_name": "Nanjing University of Information Science &amp; Technology",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Threshold effect of data amount and grid size on urban land use type identification using multi-source data fusion",
        "paper_author": "Lv H.",
        "publication": "Sustainable Cities and Society",
        "citied_by": "5",
        "cover_date": "2023-11-01",
        "Abstract": "Reliable urban land use maps are important for sustainable development and planning. Currently, the effects of different data source combinations and grid sizes on mapping results have rarely been studied. To reduce subjectivity in data selection, 10 collected multi-source spatial data were combined by traversal to create 1013 simulated combination schemes. Considering the size range of these data sources, 10 fusion grid sizes were selected. Then, a multi-source data learning model for urban land use classification (ULUC) was established by combining convolutional neural networks and long short-term memory. By taking Jinshui District (Zhengzhou, China) as an example, 10130 ULUC mappings were obtained. The maximum accuracy (82.9%) was achieved in the combination scheme D1D2D3D5D6D7D8D9D10 at a grid size of 30 m. The optimal solution among simulation 10130 schemes had an accuracy of 82.9%, a 14.7% improvement compared to the average accuracy of 67.6%. It is found that (1) The maximum accuracy showed a tendency to increase and then decrease with the increase in the variety of multi-source data combinations;(2) As the grid size decreases, the maximum accuracy also exhibited a tendency to increase and then decrease; (3) There was a significant threshold effect for both data combination types and grid sizes.",
        "DOI": "10.1016/j.scs.2023.104855",
        "affiliation_name": "Zhengzhou University",
        "affiliation_city": "Zhengzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Learn-to-supervise: Causal reinforcement learning for high-level control in industrial processes",
        "paper_author": "Nadim K.",
        "publication": "Engineering Applications of Artificial Intelligence",
        "citied_by": "2",
        "cover_date": "2023-11-01",
        "Abstract": "Possessing efficient supervisory control systems is crucial for maintaining the desired operational performance of complex industrial processes. Several challenges face the developers of these systems, such as requiring accurate physical models, dealing with the variability and uncertainty of process operating conditions and coordinating between local controllers to reach desired global performance. This paper proposes an intelligent supervisory control approach based on causal reinforcement learning (CRL) to effectively manipulate the controllers’ setpoints of the process in a way that optimizes its key performance indicators (KPIs), thereby improving the energy efficiency of the process. The approach adopts deep reinforcement learning (DRL) to develop an efficient control policy through interaction with a process simulation. The DRL training history is then exploited using interpretable machine learning and process mining to build a discrete event system (DES) model, in the form of a state-event graph. The DES model identifies causal relationships between events and provides interpretability to the control policy developed by the DRL method. The DES discovered is exploited as a Markov decision process to apply the Q-learning algorithm as a CRL supervisor. The supervisor incorporates causal knowledge into its training process, thus improving the DRL control policy developed and identifying the event paths that optimize the process's KPIs. The proposed approach is validated using two heat recovery systems in a pulp & paper mill. It successfully achieves a control policy that reduces energy consumption by up to 15.6% for the first system and 5.02% for the second, compared to the expert's baseline methods.",
        "DOI": "10.1016/j.engappai.2023.106853",
        "affiliation_name": "Natural Resources Canada",
        "affiliation_city": "Ottawa",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Which factor contributes more to the fuel consumption gap between in-laboratory vs. real-world driving conditions? An independent component analysis",
        "paper_author": "Fan P.",
        "publication": "Energy Policy",
        "citied_by": "7",
        "cover_date": "2023-11-01",
        "Abstract": "A widening vehicle fuel consumption gap has been found between in-laboratory and real-world driving conditions, which can undermine policy-making concerning energy saving and greenhouse gas reduction. Various factors have been identified as contributors to the gap; however, their contributions have not been independently assessed, which hinders the gap's narrowing. This study confirmed an average gap of 42% based on 0.95 billion records of second-by-second vehicle operating (speed, acceleration, etc.) and fuel consumption data collected from 395 light-duty vehicles through On-board Diagnostics (OBD) devices in Beijing. Contributions of fuel consumption rate (FCR)-related, engine load-related, and road grade discrepancies between the in-laboratory test procedure vs. real-world driving conditions to the gap were independently assessed. Results indicated that the FCR-related, engine load-related, and road grade discrepancies contributed 20.7%, 17.0%, and 3.2% to the gap, respectively (only 1.0% of the gap has not been explained). Replacing the test cycle from the New European Driving Cycle (NEDC) with the China Light-duty vehicle Test Cycle (CLTC-P) has the potential to reduce the engine load-related gap from 17.0% to 6.9% in Beijing. Policies should focus on developing more local test procedures or recording real-world fuel consumption through onboard devices to evaluate vehicle economy with higher reality representation.",
        "DOI": "10.1016/j.enpol.2023.113739",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Atlanta",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Price forecasts of ten steel products using Gaussian process regressions",
        "paper_author": "Xu X.",
        "publication": "Engineering Applications of Artificial Intelligence",
        "citied_by": "28",
        "cover_date": "2023-11-01",
        "Abstract": "Addressing price forecasting problems is an important exercise to policymakers and market participants in the resource business sector. In this work, we build Gaussian process regression models through cross validation and Bayesian optimizations over different kernels and basis functions for daily price index forecasting of ten major steel products in the Chinese market during July 20, 2011–April 15, 2021. This study aims to be the first attempt of exploring potential of Gaussian process regressions for price forecasting exercises with the coverage of all of the ten most important steel products that carry enormous economic significance in China as the largest steel consumer and producer in the world. The models offer accurate out-of-sample forecasts for the two-year period of April 16, 2019–April 15, 2021 with relative root mean square errors ranging from 0.07404% to 0.22379% across the ten price indices and correlation coefficients above 99.9%. They also lead to better forecast performance than some traditional econometric models and some other machine learning models as benchmarks. The models constructed here could be utilized by policymakers as part of policy design and implementations and by market participants as part of market assessments and decision making.",
        "DOI": "10.1016/j.engappai.2023.106870",
        "affiliation_name": "NC State University",
        "affiliation_city": "Raleigh",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "A deep reinforcement learning-based maintenance optimization for vacuum packaging machines considering product quality degradation",
        "paper_author": "Jiménez H.",
        "publication": "Journal of Food Process Engineering",
        "citied_by": "2",
        "cover_date": "2023-11-01",
        "Abstract": "Vacuum loss in packaged meats can lead to product defects resulting in significant economic losses and negative public health issues. Therefore, it is crucial to study the degradation of components that are critical for the provision of vacuum and package sealing to enhance system availability and process safety. Accordingly, this article proposes a condition-based maintenance policy that integrates quality information considering meat cuts that lack proper vacuum as defective items. A deep reinforcement learning algorithm is used to learn a set of adequate maintenance actions to be performed at each maintenance inspection while maximizing the system availability and/or minimizing the total maintenance cost including the cost of producing defectives items. A numerical case study and benchmarking were performed, demonstrating that the proposed model surpasses the corrective maintenance policy. It leads to a 2.2% increase in system reliability, a 91% reduction in maintenance costs, a 93% reduction in defects identified in production, and a 90% reduction in defects identified on supermarket shelves. Such results demonstrate that the model can (i) prescribe maintenance actions at each inspection according to critical degradation states; (ii) exploit maintenance opportunities that lead to economic savings; and (iii) reduce product reprocessing and propagation of defects to shelves. Practical applications: A new machine learning-based maintenance model promises to revolutionize the vacuum packaging industry by enhancing system reliability, reducing costs, and improving product quality. The model enables managers to make dynamic decisions based on the system state, avoiding inefficient maintenance planning and ensuring maximum productivity. By predicting quality performance through vacuum condition, the model allows for timely decision-making and reduces the need for costly laboratory analysis. The free-model's estimation of structural and economic relations of components enables managers to retrain and adapt maintenance policies for optimal system performance. With improved system reliability and reduced production of non-conforming items, the industry can reduce reprocessing costs, contamination risks, and protect brand image by ensuring better control over meat hygiene. This new approach to maintenance optimization has significant implications for process safety, efficiency of operations, and profits of the vacuum packaging industry, making it a potential game-changer in the field.",
        "DOI": "10.1111/jfpe.14429",
        "affiliation_name": "Université de Lorraine",
        "affiliation_city": "Nancy",
        "affiliation_country": "France"
    },
    {
        "paper_title": "Predicting urban rail transit safety via artificial neural networks",
        "paper_author": "Awad F.A.",
        "publication": "Safety Science",
        "citied_by": "6",
        "cover_date": "2023-11-01",
        "Abstract": "This paper studies the operational safety of urban rail transit (URT) systems through Artificial Neural Networks. While recent safety literature adopting systematic models of analysis consider the complexity of URT operations, they focus on single systems or single components of the operational process. Our study contributes to the URT safety literature by having a macro perspective, while considering that such complex socio-technical systems involve multiple non-linear interactions among their components. To our knowledge, we present the first cross-country analysis of URT safety through machine learning models in the literature, using a unique international dataset from 31 URT systems which comprises annual system-level data. Two models are estimated to predict the annual URT injuries. The first model includes safety-related incidents as inputs, while the second includes operational characteristics of the system. Additionally, a closed-form formula is presented to predict the annual number of injuries based on operational features of the URT system along with an illustrative example to demonstrate benchmarking applications. The results are promising and indicate good generalizability. The models proposed in this study could be useful for operators and policy makers as they aid in prioritizing improvements, predicting future safety performance based on changes in operational features, and as a benchmarking tool.",
        "DOI": "10.1016/j.ssci.2023.106282",
        "affiliation_name": "Al-Ahliyya Amman University",
        "affiliation_city": "Amman",
        "affiliation_country": "Jordan"
    },
    {
        "paper_title": "Robust federated deep reinforcement learning for optimal control in multiple virtual power plants with electric vehicles",
        "paper_author": "Feng B.",
        "publication": "Applied Energy",
        "citied_by": "21",
        "cover_date": "2023-11-01",
        "Abstract": "The deployment of virtual power plants (VPPs) with electric vehicles (EVs) is crucial for the successful integration of renewable energy sources and efficient management of EV charging and discharging while maintaining sustainability and cost-effectiveness. Deep reinforcement learning (DRL) is a highly promising method that uses historical data to learn optimal control strategies and adapts to a wide range of real-time scenarios. To address data privacy concerns in VPPs, federated DRL, which trains models across multiple VPPs, is necessary. However, existing federated DRL methods are prone to disturbance, which can severely impact system performance. This paper proposes a robust federated DRL method to ensure the robustness and reliability of VPP control strategies. Firstly, we formulate the control strategies of multiple VPPs as a Markov decision process that takes into account disturbances, aiming to achieve self-balance as much as possible. Secondly, we employ the stochastically controlled stochastic gradient method to increase training speed. Additionally, we introduce the robust gradient filter to develop a robust federated DRL method based on policy-based DRL. Finally, we validate the effectiveness and robustness of the proposed robust federated DRL method, which maintains balance in internal VPP power.",
        "DOI": "10.1016/j.apenergy.2023.121615",
        "affiliation_name": "College of Electrical Engineering, Zhejiang University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "HOMOCHAR: A novel adversarial attack framework for exposing the vulnerability of text based neural sentiment classifiers",
        "paper_author": "Bajaj A.",
        "publication": "Engineering Applications of Artificial Intelligence",
        "citied_by": "11",
        "cover_date": "2023-11-01",
        "Abstract": "State-of-the-art deep learning algorithms have demonstrated remarkable proficiency in the task of text classification. Despite the widespread use of deep learning-based language models, there remains much work to be done in order to improve the security of these models. This is particularly concerning for their growing use in sensitive applications, such as sentiment analysis. This study demonstrates that language models possess inherent susceptibility to textual adversarial attacks, wherein a small number of words or characters are modified to produce an adversarial text that deceives the machine into producing erroneous predictions while maintaining its true meaning for human readers. The current study offers HOMOCHAR, a novel textual adversarial attack that operates within a black box setting. The proposed method generates more robust adversarial examples by considering the task of perturbing a text input with transformations at the character level. The objective is to deceive a target NLP model while adhering to specific linguistic constraints in a way such that the perturbations are imperceptible to humans. Comprehensive experiments are performed to assess the effectiveness of the proposed attack method against several popular models, including Word-CNN, Word-LSTM along with five powerful transformer models on two benchmark datasets, i.e., MR & IMDB utilized for sentiment analysis task. Empirical findings indicate that the proposed attack model consistently attains significantly greater attack success rates (ASR) and generates high-quality adversarial examples when compared to conventional methods. The results indicate that text-based sentiment prediction techniques can be circumvented, leading to potential consequences for existing policy measures.",
        "DOI": "10.1016/j.engappai.2023.106815",
        "affiliation_name": "Delhi Technological University",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Explainable AI for paid-up risk management in life insurance products",
        "paper_author": "Bermúdez L.",
        "publication": "Finance Research Letters",
        "citied_by": "2",
        "cover_date": "2023-11-01",
        "Abstract": "Explainable artificial intelligence (xAI) provides a better understanding of the decision-making processes and results generated by black-box machine learning (ML) models. Here, we outline several xAI techniques in order to equip risk managers with more explainable ML methods. We illustrate this by describing an application for the more effective management of paid-up risk in insurance savings products. We draw on a database of real universal life policies to fit an initial logistic regression model and several tree-based models. We then use different xAI techniques, including a novel approach that leverages a Kohonen network of Shapley values, to offer valuable perspectives on tree-based models to the end-user. Based on these findings, we show how non-trivial ideas can emerge to improve paid-up risk management.",
        "DOI": "10.1016/j.frl.2023.104242",
        "affiliation_name": "Universitat de Barcelona",
        "affiliation_city": "Barcelona",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "A Prescriptive Machine Learning Approach to Mixed-Integer Convex Optimization",
        "paper_author": "Bertsimas D.",
        "publication": "INFORMS Journal on Computing",
        "citied_by": "4",
        "cover_date": "2023-11-01",
        "Abstract": "We introduce a prescriptive machine learning approach to speed up the process of solving mixed-integer convex optimization (MICO) problems. We solve multiple optimization instances and train a machine learning model in advance, which we use to solve new instances. Previous works have shown that the predictions of classification algorithms enable us to solve optimization problems much faster than commercial solvers. What distinguishes this paper from the previous work is that we use a prescriptive algorithm, Optimal Policy Trees (OPT), instead of classification algorithms. Whereas classification algorithms aim to predict the correct label and consider all other labels equally undesirable, a prescriptive approach takes into account all the available decision options and their counterfactuals. We first introduce an algorithm that is purely based on OPT and also its extension. We compare their performance with Optimal Classification Trees (OCT) on various MICO problems. Test problems include transportation optimization, portfolio optimization, facility location, and hybrid vehicle control. We also experiment on real-world instances taken from the Mixed Integer Programming Library. OPT-based methods have a significant edge on finding feasible solutions, whereas OCT-based methods have a slight edge on the degree of suboptimality. The proposed extension of the pure OPT algorithm improves on the suboptimality of the solutions the algorithm produces.",
        "DOI": "10.1287/ijoc.2022.0188",
        "affiliation_name": "Massachusetts Institute of Technology",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Margin notes from the COVID-19 pandemic for the future of healthcare innovation",
        "paper_author": "Bond A.",
        "publication": "Healthcare Management Forum",
        "citied_by": "2",
        "cover_date": "2023-11-01",
        "Abstract": "The COVID-19 pandemic has been characterized as a “big-event disruption” that fundamentally challenged the sustainability of existing healthcare business and service models and demanded innovation through “dual transformation” simultaneously to both core operations and the evolution of new strategic directions. The concept of disruptive innovation as applied to healthcare is reviewed and the strategies of distributed healthcare organizations supporting the most medically and socially complex communities during the COVID-19 pandemic are described as demonstrative of the promise of disruptive innovation in healthcare to bring about the necessary shift away from acute and facility-based care to integrated health and social care in the community. The place of new digital health technologies including “big data” analytics, digital platforms, and artificial intelligence/machine learning are identified as being integral to optimizing the scale and scope of impact of distributed community health and social care.",
        "DOI": "10.1177/08404704231185487",
        "affiliation_name": "Inner City Health Associates",
        "affiliation_city": "Toronto",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "How could Artificial Intelligence be used to increase the potential of biorefineries in the near future? A review",
        "paper_author": "Arias A.",
        "publication": "Environmental Technology and Innovation",
        "citied_by": "15",
        "cover_date": "2023-11-01",
        "Abstract": "Innovation in digitalization and low-carbon technologies are leading the way for the production sector. In the context of the bioeconomy, a path is opening up for the integration of bio-based processes into the value chain as alternative production schemes to fossil fuel-based production models, although process modeling and optimization is needed as this approach is at an early stage of design and development. The large number of variables in the biorefinery cascade scheme presents the inherent difficulty of the optimization strategy, considering conditions that allow for higher productivity and revenues in parallel with lower environmental burdens. The implementation of artificial intelligence (AI) through techniques such as machine learning and predictive modeling could be considered as an efficient tool for process optimization. Such techniques require large amounts of historical data to identify effects, synergies and clusters of parameters; detect production anomalies; develop models for predictive, prescriptive or root cause analysis; and provide autonomous control. In this sense, this critical review report aims to provide an overview of available reports that have considered AI for the evaluation of biorefinery production models, identifying its potentialities to enable better production strategies under the principles of sustainability and circular economy. This review aims to be useful for the development of further research on the implementation of digitalization of biorefinery processes. This work aims to identify the forefront innovations in the development of bio-based processes that meet efficiency and sustainability criteria in order to provide information of interest to policy makers, stakeholders and industry professionals.",
        "DOI": "10.1016/j.eti.2023.103277",
        "affiliation_name": "Centro de Investigación Interdisciplinar en Tecnoloxías Ambientais (CRETUS)",
        "affiliation_city": "Santiago de Compostela",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "An AI-Driven VM Threat Prediction Model for Multi-Risks Analysis-Based Cloud Cybersecurity",
        "paper_author": "Saxena D.",
        "publication": "IEEE Transactions on Systems, Man, and Cybernetics: Systems",
        "citied_by": "21",
        "cover_date": "2023-11-01",
        "Abstract": "Cloud virtualization technology, ingrained with physical resource sharing, prompts cybersecurity threats on users' virtual machines (VMs) due to the presence of inevitable vulnerabilities on the offsite servers. Contrary to the existing works which concentrated on reducing resource sharing and encryption/decryption of data before transfer for improving cybersecurity which raises computational cost overhead, the proposed model operates diversely for efficiently serving the same purpose. This article proposes a novel multiple risks analysis-based VM threat prediction model (MR-TPM) to secure computational data and minimize adversary breaches by proactively estimating the VMs threats. It considers multiple cybersecurity risk factors associated with the configuration and management of VMs, along with analysis of users' behavior. All these threat factors are quantified for the generation of respective risk score values and fed as input into a machine learning-based classifier to estimate the probability of threat for each VM. The performance of MR-TPM is evaluated using benchmark Google Cluster and OpenNebula VM threat traces. The experimental results demonstrate that the proposed model efficiently computes the cybersecurity risks and learns the VM threat patterns from historical and live data samples. The deployment of MR-TPM with existing VM allocation policies reduces cybersecurity threats up to 88.9%.",
        "DOI": "10.1109/TSMC.2023.3288081",
        "affiliation_name": "Indian Institute of Information Technology Bhopal",
        "affiliation_city": "Bhopal",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Dynamic Job-Shop Scheduling Problems Using Graph Neural Network and Deep Reinforcement Learning",
        "paper_author": "Liu C.L.",
        "publication": "IEEE Transactions on Systems, Man, and Cybernetics: Systems",
        "citied_by": "35",
        "cover_date": "2023-11-01",
        "Abstract": "The job-shop scheduling problem (JSSP) is one of the best-known combinatorial optimization problems and is also an essential task in various sectors. In most real-world environments, scheduling is complex, stochastic, and dynamic, with inevitable uncertainties. Therefore, this article proposes a novel framework based on graph neural networks (GNNs) and deep reinforcement learning (DRL) to deal with the dynamic JSSP (DJSSP) with stochastic job arrivals and random machine breakdowns by minimizing the makespan. In the proposed framework, JSSP is formulated as a Markov decision process (MDP) and is associated with a disjunctive graph to encode the information of jobs and machines as nodes and arcs. We propose a GNN architecture to perform representation learning by transforming graph states into node embeddings. Then, the agent takes actions using a parameterized policy in terms of policy learning. Operations are used as actions, and an effective reward is well designed to guide the agent. We train our proposed method using proximal policy optimization (PPO), which helps minimize the loss function while ensuring that the deviation is relatively small. Extensive experiments show that the proposed method can achieve excellent results considering different criteria: efficiency, effectiveness, robustness, and generalizability. Once the proposed method is trained, it can directly schedule new JSSPs of different sizes and distributions in static benchmark tests, showing its excellent generalizability and effectiveness compared to another DRL-based method. Furthermore, the proposed method simultaneously maintains the win rate (a quantitative metric) and the scheduling score (a qualitative metric) when scheduling in dynamic environments.",
        "DOI": "10.1109/TSMC.2023.3287655",
        "affiliation_name": "National Yang Ming Chiao Tung University",
        "affiliation_city": "Hsinchu",
        "affiliation_country": "Taiwan"
    },
    {
        "paper_title": "A time series based machine learning strategy for wastewater-based forecasting and nowcasting of COVID-19 dynamics",
        "paper_author": "Lai M.",
        "publication": "Science of the Total Environment",
        "citied_by": "13",
        "cover_date": "2023-11-01",
        "Abstract": "Monitoring COVID-19 infection cases has been a singular focus of many policy makers and communities. However, direct monitoring through testing has become more onerous for a number of reasons, such as costs, delays, and personal choices. Wastewater-based epidemiology (WBE) has emerged as a viable tool for monitoring disease prevalence and dynamics to supplement direct monitoring. The objective of this study is to intelligently incorporate WBE information to nowcast and forecast new weekly COVID-19 cases and to assess the efficacy of such WBE information for these tasks in an interpretable manner. The methodology consists of a time-series based machine learning (TSML) strategy that can extract deeper knowledge and insights from temporal structured WBE data in the presence of other relevant temporal variables, such as minimum ambient temperature and water temperature, to boost the capability for predicting new weekly COVID-19 case numbers. The results confirm that feature engineering and machine learning can be utilized to enhance the performance and interpretability of WBE for COVID-19 monitoring, along with identifying the different recommended features to be applied for short-term and long-term nowcasting and short-term and long-term forecasting. The conclusion of this research is that the proposed time-series ML methodology performs as well, and sometimes better, than simple predictions that assume available and accurate COVID-19 case numbers from extensive monitoring and testing. Overall, this paper provides an insight into the prospects of machine learning based WBE to the researchers and decision-makers as well as public health practitioners for predicting and preparing the next wave of COVID-19 or the next pandemic.",
        "DOI": "10.1016/j.scitotenv.2023.165105",
        "affiliation_name": "College of Engineering and Physical Sciences",
        "affiliation_city": "Laramie",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Governing the rural futures: Anxiety machine, anticipatory actions and rural affective politics",
        "paper_author": "Wang C.M.",
        "publication": "Environment and Planning C: Politics and Space",
        "citied_by": "1",
        "cover_date": "2023-11-01",
        "Abstract": "Regional revitalisation ideas are widely regarded as cures for socio-economic problems in rural areas in developed countries. Instead of relying on exogenous resources, the concept seeks to revalorise rural communities through cultural resources and self-responsibility. Such an approach to rural development has gained rapid popularity across East Asian regions over the past decade. Rather than merely focusing on the movement of people and ideas, a growing body of literature on policy mobility directs more attention to the power relations of the movement. However, less attention has been paid to how rural futures are anticipated and acted upon and how policies that have implemented specific futures elsewhere are justified. To address these theoretical gaps, this paper draws on the work of future geographies and develops the idea of the anxiety machine. I suggest that the study of policy mobility must seriously consider how rural futures are imagined and governed, and what rural affective politics emerged from the enactment of particular futures. With reference to a case study of rural revitalisation policy learning in Taiwan and Japan, this paper suggests that an emphasis on the circulation of global anticipatory knowledge advances the understanding of the geographies of rural policy-making.",
        "DOI": "10.1177/23996544231188828",
        "affiliation_name": "National Taiwan University",
        "affiliation_city": "Taipei",
        "affiliation_country": "Taiwan"
    },
    {
        "paper_title": "Marine plastics, circular economy, and artificial intelligence: A comprehensive review of challenges, solutions, and policies",
        "paper_author": "seyyedi S.r.",
        "publication": "Journal of Environmental Management",
        "citied_by": "32",
        "cover_date": "2023-11-01",
        "Abstract": "Global plastic production is rapidly increasing, resulting in significant amounts of plastic entering the marine environment. This makes marine litter one of the most critical environmental concerns. Determining the effects of this waste on marine animals, particularly endangered organisms, and the health of the oceans is now one of the top environmental priorities. This article reviews the sources of plastic production, its entry into the oceans and the food chain, the potential threat to aquatic animals and humans, the challenges of plastic waste in the oceans, the existing laws and regulations in this field, and strategies. Using conceptual models, this study looks at a circular economy framework for energy recovery from ocean plastic wastes. It does this by drawing on debates about AI-based systems for smart management. In the last sections of the present research, a novel soft sensor is designed for the prediction of accumulated ocean plastic waste based on social development features and the application of machine learning computations. Plus, the best scenario of ocean plastic waste management with a concentration on both energy consumption and greenhouse gas emissions is discussed using USEPA-WARM modeling. Finally, a circular economy concept and ocean plastic waste management policies are modeled based on the strategies of different countries. We deal with green chemistry and the replacement of plastics derived from fossil sources.",
        "DOI": "10.1016/j.jenvman.2023.118591",
        "affiliation_name": "National University of Singapore",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore"
    },
    {
        "paper_title": "Free nitrous acid prediction in ANAMMOX process using hybrid deep neural network model",
        "paper_author": "Li J.",
        "publication": "Journal of Environmental Management",
        "citied_by": "8",
        "cover_date": "2023-11-01",
        "Abstract": "Free nitrous acid (FNA) is a critical metric for stabilization of ANAMMOX but can not be directly and immediately measured by sensors or chemical measurement method, which hinders the effective management and operation for ANAMMOX. This study focuses on FNA prediction using hybrid model based on temporal convolutional network (TCN) combined with attention mechanism (AM) optimized by multiobjective tree-structured parzen estimator (MOTPE), called MOTPE-TCNA. A case study in an ANAMMOX reactor is carried out. Results show that nitrogen removal rate (NRR) is highly correlated with FNA concentration, indicating that it can forecast the operational status by predicting FNA. Then, MOTPE successfully optimizes the hyperparameters of TCN, helping TCN achieve a high prediction accuracy, and AM furtherly improves model accuracy. MOTPE-TCNA obtains the highest prediction accuracy, whose R2 value gets 0.992, increasing 1.71–11.80% compared to other models. As a deep neural network model, MOTPE-TCNA has more advantages than traditional machine learning methods in FNA prediction, which is beneficial to maintain the stable operation and easy control for ANAMMOX process.",
        "DOI": "10.1016/j.jenvman.2023.118566",
        "affiliation_name": "State Key Laboratory of Pulp and Paper Engineering",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Data-driven models for predicting community changes in freshwater ecosystems: A review",
        "paper_author": "Lee D.Y.",
        "publication": "Ecological Informatics",
        "citied_by": "12",
        "cover_date": "2023-11-01",
        "Abstract": "Freshwater ecosystems are sensitive to disturbances related to human activities, such as climate and land-use changes. To predict and understand the potential impacts of these disturbances, models can be employed. In this study, we reviewed data-based research employing models over the last three decades to predict the biological elements of freshwater ecosystems at different scales, with a focus on phytoplankton, macroinvertebrates, and fish. Specifically, we investigated existing research trends, evaluated the ability of current models to predict changes in freshwater organisms in response to environmental changes, and suggested future research directions. Among the three aquatic organisms, phytoplankton were the focus of studies related to water quality management, whereas most studies on macroinvertebrates and fish skewed toward modeling community composition changes and habitat suitability. Considering that many studies contained more than two study objects, there was a lack of research modeling future changes, such as climate change and subsequent changes in habitat conditions. Hybrid modeling methods using both correlative and mechanistic models have recently become more important, and are likely to improve modeling performance. Advanced models have the potential to significantly enhance the conservation and management of freshwater ecosystems, while also facilitating the development of effective policies that can better address the challenges faced by these ecosystems. Model uncertainty and sensitivity analysis, as well as the interpretable techniques of machine learning, also have the potential to improve model performance. This study provides valuable insights for modeling and general scientific research based on data-driven models.",
        "DOI": "10.1016/j.ecoinf.2023.102163",
        "affiliation_name": "National Institute of Environmental Research",
        "affiliation_city": "Icheon",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "Budgeting for SDGs: Quantitative methods to assess the potential impacts of public expenditure",
        "paper_author": "Guariso D.",
        "publication": "Development Engineering",
        "citied_by": "8",
        "cover_date": "2023-11-01",
        "Abstract": "Using a novel large-scale dataset that links thousands of expenditure programs to the Sustainable Development Goals for over a decade, we analyze the impact of public expenditure on more than 100 different development indicators. Contrary to the single-dimensional view of evaluating expenditure in terms of overall economic growth, we take a multi-dimensional approach. Then, we assess the effectiveness of three quantitative methods for capturing expenditure effects on development: (1) regression analysis, (2) machine learning techniques, and (3) agent computing. We find that, under the existing data and for this particular task, approaches (1) and (2) have difficulties disentangling sector-specific effects (i.e., target effects in the SDG semantics), which is consistent with results in previous empirical research. In contrast, by applying a micro-founded agent-computing model of policy prioritization, we can provide empirical evidence about potential impacts and bottlenecks across a high-dimensional policy space. Our findings suggest that, in the discussion of budgeting for SDGs, one should carefully evaluate the data available, the suitability of data-driven approaches, and consider alternative methods that are richer in terms of incorporating explicit causal mechanisms and scalable to a large set of indicators.",
        "DOI": "10.1016/j.deveng.2023.100113",
        "affiliation_name": "The Alan Turing Institute",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Trends and statistics of artificial intelligence and radiomics research in Radiology, Nuclear Medicine, and Medical Imaging: bibliometric analysis",
        "paper_author": "Kocak B.",
        "publication": "European Radiology",
        "citied_by": "28",
        "cover_date": "2023-11-01",
        "Abstract": "Objective: To conduct a comprehensive bibliometric analysis of artificial intelligence (AI) and its subfields as well as radiomics in Radiology, Nuclear Medicine, and Medical Imaging (RNMMI). Methods: Web of Science was queried for relevant publications in RNMMI and medicine along with their associated data from 2000 to 2021. Bibliometric techniques utilised were co-occurrence, co-authorship, citation burst, and thematic evolution analyses. Growth rate and doubling time were also estimated using log-linear regression analyses. Results: According to the number of publications, RNMMI (11,209; 19.8%) was the most prominent category in medicine (56,734). USA (44.6%) and China (23.1%) were the two most productive and collaborative countries. USA and Germany experienced the strongest citation bursts. Thematic evolution has recently exhibited a significant shift toward deep learning. In all analyses, the annual number of publications and citations demonstrated exponential growth, with deep learning-based publications exhibiting the most prominent growth pattern. Estimated continuous growth rate, annual growth rate, and doubling time of the AI and machine learning publications in RNMMI were 26.1% (95% confidence interval [CI], 12.0–40.2%), 29.8% (95% CI, 12.7–49.5%), and 2.7 years (95% CI, 1.7–5.8), respectively. In the sensitivity analysis using data from the last 5 and 10 years, these estimates ranged from 47.6 to 51.1%, 61.0 to 66.7%, and 1.4 to 1.5 years. Conclusion: This study provides an overview of AI and radiomics research conducted mainly in RNMMI. These results may assist researchers, practitioners, policymakers, and organisations in gaining a better understanding of both the evolution of these fields and the importance of supporting (e.g., financial) these research activities. Key Points: • In terms of the number of publications on AI and ML, Radiology, Nuclear Medicine, and Medical Imaging was the most prominent category compared to the other categories related to medicine (e.g., Health Policy & Services, Surgery). • All evaluated analyses (i.e., AI, its subfields, and radiomics), based on the annual number of publications and citations, demonstrated exponential growth, with decreasing doubling time, which indicates increasing interest from researchers, journals, and, in turn, the medical imaging community. • The most prominent growth pattern was observed in deep learning-based publications. However, the further thematic analysis demonstrated that deep learning has been underdeveloped but highly relevant to the medical imaging community.",
        "DOI": "10.1007/s00330-023-09772-0",
        "affiliation_name": "Dipartimento di Medicina, Chirurgia e Odontoiatria “Scuola Medica Salernitana\"",
        "affiliation_city": "Baronissi",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Dynamic Beam-Based Random Access Scheme for M2M Communications in Massive MIMO Systems",
        "paper_author": "Zheng K.",
        "publication": "IEEE Transactions on Vehicular Technology",
        "citied_by": "2",
        "cover_date": "2023-11-01",
        "Abstract": "Internet of things, supported by machine-To-machine (M2M) communications, is one of the most important applications for the 6th generation (6G) systems. A major challenge facing by 6G is enabling a massive number of M2M devices to access networks in a timely manner. Therefore, this article exploits the spatial selectivity of massive multi-input multi-output (MIMO) to reduce the collision issue when massive M2M devices initiate random access simultaneously. In particular, a beam-based random access protocol is first proposed to make efficient use of the limited uplink resources for massive M2M devices. To address the non-uniform distribution of M2M devices in the space and time dimensions, an Markov decision process (MDP) problem with the objective of minimizing the average access delay is then formulated. Next, we present a dynamic beam-based access scheme based on the double deep Q network (DDQN) algorithm to solve the optimal policy. Finally, simulations are conducted to demonstrate the effectiveness of the proposed scheme including the model training and random access performance.",
        "DOI": "10.1109/TVT.2023.3286660",
        "affiliation_name": "Ningbo University",
        "affiliation_city": "Ningbo",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Social vulnerability predictors of drug poisoning mortality: A machine learning analysis in the United States",
        "paper_author": "Tatar M.",
        "publication": "American Journal on Addictions",
        "citied_by": "1",
        "cover_date": "2023-11-01",
        "Abstract": "Background and Objectives: Drug poisoning is a leading cause of unintentional deaths in the United States. Despite the growing literature, there are a few recent analyses of a wide range of community-level social vulnerability features contributing to drug poisoning mortality. Current studies on this topic face three limitations: often studying a limited subset of vulnerability features, focusing on small sample sizes, or solely including local data. To address this gap, we conducted a national-level analysis to study the impacts of several social vulnerability features in predicting drug mortality rates in the United States. Methods: We used machine learning to investigate the role of 16 social vulnerability features in predicting drug mortality rates for US counties in 2014, 2016, and 2018—the most recent available data. We estimated each vulnerability feature's gain relative contribution in predicting drug poisoning mortality. Results: Among all social vulnerability features, the percentage of noninstitutionalized persons with a disability is the most influential predictor, with a gain relative contribution of 18.6%, followed by population density and the percentage of minority residents (13.3% and 13%, respectively). Percentages of households with no available vehicles, mobile homes, and persons without a high school diploma are the following features with gain relative contributions of 6.3%, 5.8%, and 5.1%, respectively. Conclusion and Scientific Significance: We identified social vulnerability features that are most predictive of drug poisoning mortality. Public health interventions and policies targeting vulnerable communities may increase the resilience of these communities and mitigate the overdose death and drug misuse crisis.",
        "DOI": "10.1111/ajad.13445",
        "affiliation_name": "Institute for Advanced Studies in Basic Sciences, Zanjan",
        "affiliation_city": "Zanjan",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "Exploiting circular economy enablers for SMEs to advance towards a more sustainable development: An empirical study in the post COVID-19 era",
        "paper_author": "Santolin R.B.",
        "publication": "Resources, Conservation and Recycling Advances",
        "citied_by": "11",
        "cover_date": "2023-11-01",
        "Abstract": "The transition towards a more circular development requires the activation of several key Circular Economy (CE) enablers. Their combination generates a complex system that aims to advance towards a more sustainable development. However, the COVID-19 pandemic has created many changes in this global economy, altering the interactions between nature, people, governments, and businesses. The effects were mainly felt by Small and Medium-sized Enterprises (SMEs). This study analyzes the present state of CE enablers for SMEs considering a sustainability assessment overview. Eight CE enablers that have changed because of the pandemic were especially identified through an exhaustive literature review and have been analyzed by the involvement of 29 scholars and practitioners. The results generated with the application of a fuzzy TOPSIS methodology, evidence that ‘digital technologies’, ‘green consumption’, and ‘circular entrepreneurship’ are the CE enablers with the greatest potential to contribute to a more circular and sustainable development in the post-pandemic.",
        "DOI": "10.1016/j.rcradv.2023.200164",
        "affiliation_name": "Instituto Federal de Educação, Ciȇncia e Tecnologia do Rio Grande do Sul",
        "affiliation_city": "Bento Goncalves",
        "affiliation_country": "Brazil"
    },
    {
        "paper_title": "Anomaly detection of policies in distributed firewalls using data log analysis",
        "paper_author": "Andalib A.",
        "publication": "Journal of Supercomputing",
        "citied_by": "6",
        "cover_date": "2023-11-01",
        "Abstract": "A distributed firewall is a security application that monitors and controls traffic on an organization’s network. While centralized firewalls are used against attacks coming from outside a network, distributed firewalls are considered for inside attacks from internal networks such as wireless access and VPN tunnel. Distributed firewalls use policies, which are stated by rules, to find anomalous packets. However, such static rules may be incomplete. In this case, by monitoring firewall logs, the anomalies can be detected. Such logs become big when networks have high traffic, but their hidden knowledge contains valuable information about existing anomalies. In this paper, to detect the anomalies, we extract patterns from big data logs of distributed firewalls using data mining and machine learning. The proposed method is applied to big logs from distributed firewalls in a real security environment, and results are analyzed.",
        "DOI": "10.1007/s11227-023-05417-7",
        "affiliation_name": "Islamic Azad University, Rasht Branch",
        "affiliation_city": "Rasht",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "Hierarchical Relational Graph Learning for Autonomous Multirobot Cooperative Navigation in Dynamic Environments",
        "paper_author": "Wang T.",
        "publication": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
        "citied_by": "3",
        "cover_date": "2023-11-01",
        "Abstract": "As a specific kind of cyber-physical systems (CPSs), autonomous robot clusters play an important role in various intelligent manufacturing fields. However, due to the increasing design complexity of robot clusters, it is becoming more and more challenging to guarantee the safety and efficiency for multirobot cooperative navigation in dynamic and complex environments. Although deep reinforcement learning (DRL) shows great potential in learning multirobot cooperative navigation policies, existing DRL-based approaches suffer from scalability issues and rarely consider the transferability of trained policies to new tasks. To address these problems, this article presents a novel DRL-based multirobot cooperative navigation approach named HRMR-Navi that equips each robot with both a two-layered hierarchical graph network model and an attention-based communication model. In our approach, the hierarchical graph network model can efficiently figure out hierarchical relations among all agents that either cooperate for efficiency or avoid obstacles for safety to derive more advanced strategies, and the communication model can accurately form a global view of the environment for a specific robot, thus, the multirobot cooperation efficiency can be further strengthened. Meanwhile, we propose an improved proximal policy optimization (PPO) algorithm based on the Maximum Entropy Reinforcement Learning, named MEPPO, to enhance the robot exploration ability. Comprehensive experimental results demonstrate that, compared with state-of-the-art approaches, HRMR-Navi can achieve more efficient cooperative navigation with less time cost, lower collision rate, higher scalability, and better knowledge transferability.",
        "DOI": "10.1109/TCAD.2023.3260710",
        "affiliation_name": "East China Normal University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Does one size fit all? Comparing the determinants of the FinTech market segments expansion",
        "paper_author": "Stolbov M.",
        "publication": "Journal of Finance and Data Science",
        "citied_by": "3",
        "cover_date": "2023-11-01",
        "Abstract": "The paper aims to indentify and compare the determinants of the overall FinTech market expansion and its major segments – cryptocurrency and peer-to-peer lending markets – in a dataset, which covers 64 countries and 51 potentially relevant factors. To this end, we apply a battery of state-of-the-art variable selection techniques from machine learning, comprising Bayesian model averaging (BMA), least absolute shrinkage and selection operator (LASSO), variable selection using random forests (VSURF) as well as spike-and-slab regression. We document substantial heterogeneity of the pivotal determinants across the FinTech market as a whole and its major segments. Thus, specific rather than general policy measures are needed to foster the development of standalone FinTech market segments. Moreover, our findings suggest that most countries don't need to seek a universal specialization in FinTech activities, concentrating on the segment where they have a competitive edge in terms of the pivotal determinants which drive its expansion.",
        "DOI": "10.1016/j.jfds.2023.01.002",
        "affiliation_name": "Moscow State Institute of International Relations (MGIMO)",
        "affiliation_city": "Moscow",
        "affiliation_country": "Russian Federation"
    },
    {
        "paper_title": "Iterative-AMC: a novel model compression and structure optimization method in mechanical system safety monitoring",
        "paper_author": "Ji M.",
        "publication": "Structural Health Monitoring",
        "citied_by": "0",
        "cover_date": "2023-11-01",
        "Abstract": "With the rapid development of artificial intelligence, various fault diagnosis methods based on the deep neural networks have made great advances in mechanical system safety monitoring. To get the high accuracy for the fault diagnosis, researchers tend to adopt the deep network layers and amount of neurons or kernels in each layer. This results in a large redundancy and the structure uncertainty of the fault diagnosis networks. Moreover, it is hard to deploy these networks on the embedded platforms because of the large scales of the network parameters. This brings huge challenges to the practical application of the intelligent diagnosis algorithms. To solve the above problems, an iterative automatic machine compression method, named Iterative-AMC, is proposed in this paper. The proposed method aims to automatically compress and optimize the structure of the large-scale neural networks. Experiments are carried out based on two test benches. With the proposed Iterative-AMC method, the problems of the parameter redundancy and the structure uncertainty can be addressed. The scale of the original network can be greatly compressed, and the compressed fault diagnosis network is successfully deployed on a small-scale FPGA chip.",
        "DOI": "10.1177/14759217231155486",
        "affiliation_name": "Harbin Institute of Technology",
        "affiliation_city": "Harbin",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Consumer-Centric Internet of Medical Things for Cyborg Applications Based on Federated Reinforcement Learning",
        "paper_author": "Tiwari P.",
        "publication": "IEEE Transactions on Consumer Electronics",
        "citied_by": "58",
        "cover_date": "2023-11-01",
        "Abstract": "The Internet of Medical Things (IoMT) is the new digital healthcare application paradigm that offers many healthcare services to users. IoMT-based emerging healthcare applications such as cyborgs, the combination of advanced artificial intelligence (AI) robots, and doctors performing surgical operations remotely from hospitals to patients in their homes. For instance, robot-based knee replacement procedures, and thigh medical care real-time performance monitoring systems are cyborg applications. The paper introduces the multi-agent federated reinforcement learning policy (MFRLP) indicated in mobile and fog agents based on the socket remote procedure call (RPC) paradigm. The goal is to design a consumer-centric cyborg-efficient training testing system that executes the overall application mechanism with minimum delays in the IoMT system. The study develops the RPC based on reinforcement learning and federated learning that adopts dynamic changes in the environment for cyborg applications. As a result, MFRLP minimized the training and testing in the mobile and fog environments by 50%, local processing time by 40%, and processing time by 50% compared to existing machine learning (ML) methods for cyborg applications. The code is publicly available at https://github.com/prayagtiwari/CIoMT.",
        "DOI": "10.1109/TCE.2023.3242375",
        "affiliation_name": "Kristiania University College",
        "affiliation_city": "Oslo",
        "affiliation_country": "Norway"
    },
    {
        "paper_title": "Using geographical random forest models to explore spatial patterns in the neighborhood determinants of hypertension prevalence across chicago, illinois, USA",
        "paper_author": "Lotfata A.",
        "publication": "Environment and Planning B: Urban Analytics and City Science",
        "citied_by": "15",
        "cover_date": "2023-11-01",
        "Abstract": "In the United States, the rise in hypertension prevalence has been connected to neighborhood characteristics. While various studies have found a link between neighborhood and health, they do not evaluate the relative dependence of each component in the growth of hypertension and, more significantly, how this value differs geographically (i.e., across different neighborhoods). This study ranks the contribution of ten socioeconomic neighborhood factors to hypertension prevalence in Chicago, Illinois, using multiple global and local machine learning models at the census tract level. First, we use Geographical Random Forest, a recently proposed non-linear machine learning regression method, to assess each predictive factor’s spatial variation and contribution to hypertension prevalence. Then we compare GRF performance to Geographically Weighted Regression (local model), Random Forest (global model), and OLS (global model). The results indicate that GRF outperforms all models and that the importance of variables varies by census tract. Household composition is the most important factor in the Chicago tracts, while on the other hand, Housing type and Transportation is the least important factor. While the household composition is the most important determinant around north Lake Michigan, the socioeconomic condition of the neighborhood in Chicago’s mid-north has the most importance on hypertension prevalence. Understanding how the importance of socioeconomic factors associated with hypertension prevalence varies spatially aids in the design and implementation of health policies based on the most critical factors identified at the local level (i.e., tract), rather than relying on broad city-level guidelines (i.e., for entire Chicago and other large cities).",
        "DOI": "10.1177/23998083231153401",
        "affiliation_name": "Queen's University Belfast",
        "affiliation_city": "Belfast",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Validated digital literacy measures for populations with low levels of internet experiences",
        "paper_author": "Ali A.",
        "publication": "Development Engineering",
        "citied_by": "18",
        "cover_date": "2023-11-01",
        "Abstract": "A growing body of evidence suggests that digital literacy is an important barrier constraining adoption and use of Internet and digital technologies in the developing world. By enabling people to effectively find valuable information online, digital literacy can play a crucial role in expanding economic opportunities, thereby leading to human development and poverty reduction. Unfortunately, there is a dearth of validated survey measures for capturing digital literacy of populations who have limited prior exposure to technology. We present a novel approach for measuring digital literacy of low literacy and new Internet users, an important segment of users in developing countries. Using a sample of 143 social media users in Pakistan, which includes a significant fraction of low literacy individuals, we measure digital literacy by observing the effectiveness of participants in completing a series of tasks and by recording a set of self-reported survey responses. We then use machine learning methods (e.g., Random Forest) to identify a parsimonious set of survey questions that are most predictive of ground truth digital literacy established through participant observation. Our approach is easily scalable in low-resource settings and can aid in tracking digital literacy as well as designing interventions and policies tailored to users with different levels of digital literacy.",
        "DOI": "10.1016/j.deveng.2023.100107",
        "affiliation_name": "Lahore University of Management Sciences",
        "affiliation_city": "Lahore",
        "affiliation_country": "Pakistan"
    },
    {
        "paper_title": "Distinctive Voices: Political Speech, Rhetoric, and the Substantive Representation of Women in European Parliaments",
        "paper_author": "Wäckerle J.",
        "publication": "Legislative Studies Quarterly",
        "citied_by": "13",
        "cover_date": "2023-11-01",
        "Abstract": "As the share of women in parliaments rises, increased attention is paid to how they substantively represent women. Meanwhile, the availability of parliamentary speech data has enabled researchers to dissect politicians’ rhetorical patterns. We combine these two literatures to ask whether rhetorical differences between men and women in parliament are connected to style, policy, and preferences of women voters. We apply machine-learning models to speeches from five West European parliaments (2000–18) to measure the femininity of the rhetoric used in each speech. Results show that women and men talk differently in parliament, and that this distinctiveness is due to both style and substance. Combining these results with public opinion surveys, we find that women MPs have the most distinctively “feminine” discourse on issues that are most salient to women in society. These findings showcase the direct connection between descriptive and substantive representation of women in contemporary democracies.",
        "DOI": "10.1111/lsq.12410",
        "affiliation_name": "Universität zu Köln",
        "affiliation_city": "Koln",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "A Novel Hybrid Approach for Intent Creation and Detection Using K-Means-Based Topic Clustering and Heuristic-Based Capsule Network",
        "paper_author": "Magoo C.",
        "publication": "International Journal of Information Technology and Decision Making",
        "citied_by": "3",
        "cover_date": "2023-11-01",
        "Abstract": "Background: The social media revolution has offered new facilities and opportunities to the online community to communicate their intentions, opinions, and views regarding products, services, policies, and events. The identification of intent focuses on the detection of intents from user reviews, that is, whether the specific review of the user includes intention or not. Intent mining is also named intent identification which helps business organizations to identify the purchase intentions of users. However, detecting user intentions encoded in text queries is a complicated task in several Natural Language Processing (NLP) applications such as robots, smart agents, personal assistants, and search engines. The existing research works have discovered the utilization of several machine learning techniques to detect the intents from queries of users. Most works consider intent detection as a classification problem, with utterances as predefined intents. Research question: Whether the researcher resolves the detection of user intentions encoded in text queries? How the researcher solves the existing challenges based on intent mining? Purpose: The main contribution of the research is to design and implement intent detection using topic clustering and deep learning. Methodology: Initially, the dataset related to diverse queries is gathered. Then, the label creation is performed by clustering. The clustering is performed by a k-means clustering model with a cosine similarity function. Once the clustering is performed for different queries, the label is created, which is used to train the network under the detection process. For the detection, this paper uses a Heuristic-based Capsule Network (H-CapNet) that could perform the intention for a new query. The hybrid meta-heuristic algorithm with Escaping Energy searched Grey-Harris Hawks Algorithm (EEG-HHA) is used for improving the capsule network. Validation: Experimental analysis shows that the developed method has superior performance in evaluating standard datasets with other approaches. Results: From the simulation results, the accuracy of the developed EEG-HHA-CapNet for dataset 1 is secured at 3%, 1.6%, 2%, and 1.1% increased than PSO-CapNet, WOA-CapNet, HHO-CapNet, and GWO-CapNet. Conclusion: Thus, the designed user intent detection models reveal their more advanced performance based on the diverse performance and error metrics for datasets 1 and 2.",
        "DOI": "10.1142/S0219622022500924",
        "affiliation_name": "J.C. Bose University of Science and Technology, YMCA",
        "affiliation_city": "Faridabad",
        "affiliation_country": "India"
    },
    {
        "paper_title": "AgentVisor: Agent-Based Network Hypervisor for Autonomic Network Virtualization",
        "paper_author": "Choi J.S.",
        "publication": "IEEE Network",
        "citied_by": "2",
        "cover_date": "2023-11-01",
        "Abstract": "A network hypervisor is a novel solution for network virtualization that allows the dynamic creation of virtual networks to facilitate independent business solutions to numerous tenants over a shared network infrastructure. This article proposes an agent-based network hypervisor (AgentVisor) architecture that virtualizes controller agents, separated from tenants' controllers, to support high levels of adaptation, automation and autonomy of virtual software-defined networking networks (vSDNs). The controller agents enable closed control loop automation of systematic data collection, data-driven decision-making and autonomic control actions for monitoring, configuration and fault handling of vSDNs without intervention of tenants' controllers in a cloud/edge environment. The controller agent also offers an extended northbound interface with tenants' controllers to support policy/intelligence management based on artificial intelligence and machine learning algorithms. Through controller agent virtualization, the proposed AgentVisor architecture enables a multiagent system of autonomic vSDNs that facilitate independent business solutions to different markets over a shared infrastructure for the vision of fifth generation (5G) and beyond.",
        "DOI": "10.1109/MNET.126.2200324",
        "affiliation_name": "Hanyang University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "Machine learning tool-based prediction and forecasting of municipal solid waste generation rate: a case study in Guwahati, Assam, India",
        "paper_author": "Singh T.",
        "publication": "International Journal of Environmental Science and Technology",
        "citied_by": "10",
        "cover_date": "2023-11-01",
        "Abstract": "Integrated large-scale solid waste management (SWM) policies are the need of the hour to design, develop and sustain SWM models. An accurate prediction and forecasting of municipal solid waste generation (MSWG) rate are essential for such advanced strategies. The primary objective of this study is to examine the criticality of demographic and socio-economic parameters for the fair prediction and forecasting of the MSWG rate. Machine learning (ML) models were formulated by mapping solid waste quantities at the municipal level with socio-economic and demographic variables of Guwahati city. Tree-based ML algorithms, namely decision tree (DT), random forest (RF) and gradient boosting (GB), were applied to build the models with 1936 data size. The moving average (MA) approaches were adapted for the forecasting of the MSWG rate. Model validation resulted in a root mean square error, RMSE (3.01), mean absolute error, MAE (2.86) and coefficient of determination, R2 (0.99) for the GB model and correlation coefficient (r) of 0.82 between observed and predicted values and thereby resulted in best performance in conjunction with DT and RF. With the exponential MA, the forecasted RMSE and R2 for GB, RF and DT were 2.12, 3.63 and 4.22; and 0.981, 0.972 and 0.967, respectively. However, with a model accuracy of 97%, the computation time for GB model (19.18 min) exhibited maximum due to its high complexity. The overall methodology involved developing effective tools to aid in regional SWM and planning through the integration of data sources in the public domain, pre-processing and modelling from diverse sources.",
        "DOI": "10.1007/s13762-022-04644-4",
        "affiliation_name": "Indian Institute of Technology Guwahati",
        "affiliation_city": "Guwahati",
        "affiliation_country": "India"
    },
    {
        "paper_title": "SWIPT-Empowered Sustainable Wireless Federated Learning: Paradigms, Challenges, and Solutions",
        "paper_author": "Wu Y.",
        "publication": "IEEE Network",
        "citied_by": "10",
        "cover_date": "2023-11-01",
        "Abstract": "Wireless federated learning (FL), which allows edge devices to perform local deep/machine learning (DL/ML) training and further aggregates the locally trained models from them via radio channels, establishes a promising framework for enabling various DL/ML-based services in future B5G/6G networks. Despite respecting the data privacy, periodically performing the local model training is not friendly to energy-constrained edge devices and degrades the sustainability and performance of FL services. In this article, motivated by the advanced simultaneous wireless information and power transfer (SWIPT), we propose a framework of SWIPT-empowered wireless FL that can provide over-the-air wireless power transfer in parallel with the transmission of global/local models. We present the key approaches of leveraging SWIPT for FL with their advantages illustrated. The practical challenging issues in reaping the benefits of integrating SWIPT are then discussed and we also provide the potential solutions to address these issues. A representative case study of FL via SWIPT is presented to validate the advantages of exploiting SWIPT. To this end, we present a joint design of SWIPT policy and the client-scheduling for FL, which is firstly formulated as a finite horizon dynamic optimization problem and then is solved by an actor-critic-based deep reinforcement learning algorithm. We finally articulate some potential open future directions regarding the SWIPT-empowered wireless FL.",
        "DOI": "10.1109/MNET.128.2200344",
        "affiliation_name": "State Key Laboratory of Internet of Things for Smart City",
        "affiliation_city": "Taipa",
        "affiliation_country": "Macao"
    },
    {
        "paper_title": "Mapping and characterising buildings for flood exposure analysis using open-source data and artificial intelligence",
        "paper_author": "Bhuyan K.",
        "publication": "Natural Hazards",
        "citied_by": "16",
        "cover_date": "2023-11-01",
        "Abstract": "The mapping and characterisation of building footprints is a challenging task due to inaccessibility and incompleteness of the required data, thus hindering the estimation of loss caused by natural and anthropogenic hazards. Major advancements have been made in the collaborative mapping of buildings with platforms like OpenStreetMap, however, many parts of the world still lack this information or the information is outdated. We created a semi-automated workflow for the development of elements-at-risk (EaR) databases of buildings by detecting building footprints using deep learning and characterising the footprints with building occupancy information using building morphological metrics and open-source auxiliary data. The deep learning model was used to detect building EaR footprints in a city in Kerala (India) with an F1 score of over 76%. The footprints were classified into 13 building occupancy types along with information such as average number of floors, total floor space area, building density, and percentage of built-up area. We analysed the transferability of the approach to a different city in Kerala and obtained an almost similar F1 score of 74%. We also examined the exposure of the buildings and the associated occupancies to floods using the 2018 flood susceptibility map of the respective cities. We notice certain shortcomings in our research particularly, the need for a local expert and good quality auxiliary data to obtain reasonable building occupancy information, however, our research contributes to developing a rapid method for generating a building EaR database in data-scarce regions with attributes of occupancy types, thus supporting regional risk assessment, disaster risk mitigation, risk reduction initiatives, and policy developments.",
        "DOI": "10.1007/s11069-022-05612-4",
        "affiliation_name": "Faculty of Geo-Information Science and Earth Observation – ITC",
        "affiliation_city": "Enschede",
        "affiliation_country": "Netherlands"
    },
    {
        "paper_title": "Factors affecting per capita ecological footprint in OECD countries: Evidence from machine learning techniques<sup> a</sup>",
        "paper_author": "Gorus M.S.",
        "publication": "Energy and Environment",
        "citied_by": "5",
        "cover_date": "2023-11-01",
        "Abstract": "For a few decades, factors affecting environmental deterioration have been at the center of much interest This paper examines the impact of income level, disaggregated energy consumption, types of globalization level, and urbanization on per capita ecological footprint by utilizing novel machine learning techniques (tree regression, boosting, bagging, and random forest) for 27 OECD countries during 1971–2016. It is found that the random forest algorithms best fit the dataset. The empirical results exhibit that oil product consumption, electricity consumption, and gross domestic product are the most significant variables for our model. Besides, the partial dependence plots results show that economic growth and especially fossil fuel energy consumption damage the environment. These findings have important implications for both developed and developing countries for designing proper energy and environmental policies. Especially, policymakers should focus on sustainable development instead of plain economic growth.",
        "DOI": "10.1177/0958305X221112913",
        "affiliation_name": "Ankara Yildirim Beyazit University",
        "affiliation_city": "Ankara",
        "affiliation_country": "Turkey"
    },
    {
        "paper_title": "Research on innovation features and optimization strategies of industrial clusters from the perspective of TLCN",
        "paper_author": "Luo Y.",
        "publication": "Kybernetes",
        "citied_by": "0",
        "cover_date": "2023-11-01",
        "Abstract": "Purpose: The focus of industrial cluster innovation lies in the cooperation between enterprises and universities/scientific research institutes to make a theoretical breakthrough in the system and mechanism of industrial cluster network. Under the theoretical framework of cluster network, industrial structure can be optimized and upgraded, and enterprise benefit can be improved. Facing the increasing proliferation and multi-structured enterprise data, how to obtain potential and high-quality innovation features will determine the ability of industrial cluster network innovation, as well as the paper aims to discuss these issues. Design/methodology/approach: Based on complex network theory and machine learning method, this paper constructs the structure of “three-layer coupling network” (TLCN), predicts the innovation features of industrial clusters and focuses on the theoretical basis of industrial cluster network innovation model. This paper comprehensively uses intelligent information processing technologies such as network parameters and neural network to predict and analyze the industrial cluster data. Findings: From the analysis of the experimental results, the authors obtain five innovative features (policy strength, cooperation, research and development investment, centrality and geographical position) that help to improve the ability of industrial clusters, and give corresponding optimization strategy suggestions according to the result analysis. Originality/value: Building a TLCN structure of industrial clusters. Exploring the innovation features of industrial clusters. Establishing the analysis paradigm of machine learning method to predict the innovation features of industrial clusters.",
        "DOI": "10.1108/K-01-2022-0055",
        "affiliation_name": "Business School of ZUCC",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Robust Actor-Critic With Relative Entropy Regulating Actor",
        "paper_author": "Cheng Y.",
        "publication": "IEEE Transactions on Neural Networks and Learning Systems",
        "citied_by": "8",
        "cover_date": "2023-11-01",
        "Abstract": "The accurate estimation of Q-function and the enhancement of agent's exploration ability have always been challenges of off-policy actor-critic algorithms. To address the two concerns, a novel robust actor-critic (RAC) is developed in this article. We first derive a robust policy improvement mechanism (RPIM) by using the local optimal policy about the current estimated Q-function to guide policy improvement. By constraining the relative entropy between the new policy and the previous one in policy improvement, the proposed RPIM can enhance the stability of the policy update process. The theoretical analysis shows that the incentive to increase the policy entropy is endowed when the policy is updated, which is conducive to enhancing the exploration ability of agents. Then, RAC is developed by applying the proposed RPIM to regulate the actor improvement process. The developed RAC is proven to be convergent. Finally, the proposed RAC is evaluated on some continuous-action control tasks in the MuJoCo platform and the experimental results show that RAC outperforms several state-of-the-art reinforcement learning algorithms.",
        "DOI": "10.1109/TNNLS.2022.3155483",
        "affiliation_name": "China University of Mining and Technology",
        "affiliation_city": "Xuzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "MULTI-AGENT PROXIMAL POLICY OPTIMIZATION FOR PORTFOLIO OPTIMIZATION",
        "paper_author": "Khemlichi F.",
        "publication": "Journal of Theoretical and Applied Information Technology",
        "citied_by": "0",
        "cover_date": "2023-10-31",
        "Abstract": "Deep reinforcement learning is a subfield of machine learning that combines the ideas of deep learning and reinforcement learning to enable agents to learn and make decisions in complex environments. It has been applied to a wide range of tasks, including gaming, robotics, and finance, among others. In finance, reinforcement learning (RL) has emerged as a promising technique for solving strategic decision-making problems in complex financial environments using reward-based approaches for optimal control. In this paper, we propose a novel algorithm that leverages the power of Multi-Agent Reinforcement Learning (MARL) coupled with Proximal Policy Optimization (PPO) to tackle the complex problem of portfolio optimization. What sets this approach apart is its utilization of MARL, which involves multiple agents learning and interacting within the same environment. This is in contrast to the traditional single-agent approaches commonly used in portfolio optimization. In portfolio optimization, MARL enables agents to learn from the interactions with other agents and the environment, leading to more realistic and robust investment strategies. The performance of the algorithm was assessed on the S&P 500 market using various numbers of agents and assets, and its performance was compared to several benchmarks. The performance metrics used for evaluation consisted of annual profit, annual volatility, Sharpe ratio, and Sortino ratio. The findings demonstrated that the algorithm outperformed the benchmarks in terms of all the performance metrics considered, regardless of the number of agents and assets involved.",
        "DOI": "NA",
        "affiliation_name": "Université Sidi Mohamed Ben Abdellah",
        "affiliation_city": "Fez",
        "affiliation_country": "Morocco"
    },
    {
        "paper_title": "Machine Learning Analysis of Raman Spectra To Quantify the Organic Constituents in Complex Organic-Mineral Mixtures",
        "paper_author": "Zarei M.",
        "publication": "Analytical Chemistry",
        "citied_by": "4",
        "cover_date": "2023-10-31",
        "Abstract": "Important decisions in local agricultural policy and practice often hinge on the soil’s chemical composition. Raman spectroscopy offers a rapid noninvasive means to quantify the constituents of complex organic systems. But the application of Raman spectroscopy to soils presents a multifaceted challenge due to organic/mineral compositional complexity and spectral interference arising from overwhelming fluorescence. The present work compares methodologies with the capacity to help overcome common obstacles that arise in the analysis of soils. We created conditions representative of these challenges by combining varying proportions of six amino acids commonly found in soils with fluorescent bentonite clay and coarse mineral components. Referring to an extensive data set of Raman spectra, we compare the performance of the convolutional neural network (CNN) and partial least-squares regression (PLSR) multivariate models for amino acid composition. Strategies employing volume-averaged spectral sampling and data preprocessing algorithms improve the predictive power of these models. Our average test R2 for PLSR models exceeds 0.89 and approaches 0.98, depending on the complexity of the matrix, whereas CNN yields an R2 range from 0.91 to 0.97, demonstrating that classic PLSR and CNN perform comparably, except in cases where the signal-to-noise ratio of the organic component is very low, whereupon CNN models outperform. Artificially isolating two of the most prevalent obstacles in evaluating the Raman spectra of soils, we have characterized the effect of each obstacle on the performance of machine learning models in the absence of other complexities. These results highlight important considerations and modeling strategies necessary to improve the Raman analysis of organic compounds in complex mixtures in the presence of mineral spectral components and significant fluorescence.",
        "DOI": "10.1021/acs.analchem.3c02348",
        "affiliation_name": "The University of British Columbia",
        "affiliation_city": "Vancouver",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "How Fresh is the Data? An Optimal Learning-Based End-to-End Pull-Based Forwarding Framework for NDNoTs",
        "paper_author": "Buchipalli T.",
        "publication": "MSWiM 2023 - Proceedings of the International ACM Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems",
        "citied_by": "0",
        "cover_date": "2023-10-30",
        "Abstract": "The Named Data Networks (NDNs) are considered as a suitable architectural paradigm for collecting the sensory data generated by the Internet of Things (IoTs). However, towards retrieving fresh ephemeral sensory data, we show that the naive integration of NDN and IoT/sensor, without using learning-based forwarding methods, is energy-demanding and sub-optimal in retrieving fresh data. To this end, a novel learning-based NDNoT data forwarding framework is proposed. We analytically show that a Reinforcement Learning (RL) based NDN forwarding strategy can optimally retrieve fresh sensory data from IoTs in an energy-efficient manner. The proposed NDN forwarding strategy is modelled as an optimal-stopping problem using Multi-Armed Bandit (MABs). The structural result of the MAB is investigated to yield optimal policy in terms of both energy efficiency and fresh data delivery. The resultant optimal policy implemented in the form of an NDNoT forwarding algorithm namely, FRESH is validated and compared against the state-of-the-art NDN forwarding strategies using extensive simulation.",
        "DOI": "10.1145/3616388.3617552",
        "affiliation_name": "Indian Institute of Technology Tirupati",
        "affiliation_city": "Tirupati",
        "affiliation_country": "India"
    },
    {
        "paper_title": "TMC: Near-Optimal Resource Allocation for Tiered-Memory Systems",
        "paper_author": "Ni Y.",
        "publication": "SoCC 2023 - Proceedings of the 2023 ACM Symposium on Cloud Computing",
        "citied_by": "2",
        "cover_date": "2023-10-30",
        "Abstract": "Main memory dominates data center server cost, and hence data center operators are exploring alternative technologies such as CXL-attached and persistent memory to improve cost without jeopardizing performance. Introducing multiple tiers of memory introduces new challenges, such as selecting the appropriate memory configuration for a given workload mix. In particular, we observe that inefficient configurations increase cost by up to 2.6× for clients, and resource stranding increases cost by 2.2× for cloud operators. To address this challenge, we introduce TMC, a system for recommending cloud configurations according to workload characteristics and the dynamic resource utilization of a cluster. Whereas prior work utilized extensive simulation or costly machine learning techniques, incurring significant search costs, our approach profiles applications to reveal internal properties that lead to fast and accurate performance estimations. Our novel configuration-selection algorithm incorporates a new heuristic, packing penalty, to ensure that recommended configurations will also achieve good resource efficiency. Our experiments demonstrate that TMC reduces the search cost by up to 4× over the state-of-the-art, while improving resource utilization by up to 17% as compared to a naive policy that requests optimal tiered memory allocations in isolation.",
        "DOI": "10.1145/3620678.3624667",
        "affiliation_name": "Alibaba Group, USA",
        "affiliation_city": "San Mateo",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Is Machine Learning Necessary for Cloud Resource Usage Forecasting?",
        "paper_author": "Christofidi G.",
        "publication": "SoCC 2023 - Proceedings of the 2023 ACM Symposium on Cloud Computing",
        "citied_by": "5",
        "cover_date": "2023-10-30",
        "Abstract": "Robust forecasts of future resource usage in cloud computing environments enable high efficiency in resource management solutions, such as autoscaling and overcommitment policies. Production-level systems use lightweight combinations of historical information to enable practical deployments. Recently, Machine Learning (ML) models, in particular Long Short Term Memory (LSTM) neural networks, have been proposed by various works, for their improved predictive capabilities. Following this trend, we train LSTM models and observe high levels of prediction accuracy, even on unseen data. Upon meticulous visual inspection of the results, we notice that although the predicted values seem highly accurate, they are nothing but versions of the original data shifted by one time step into the future. Yet, this clear shift seems to be enough to produce a robust forecast, because the values are highly correlated across time. We investigate time series data of various resource usage metrics (CPU, memory, network, disk I/O) across different cloud providers and levels, such as at the physical or virtual machine-level and at the application job-level. We observe that resource utilization displays very small variations in consecutive time steps. This insight can enable very simple solutions, such as data shifts, to be used for cloud resource forecasting and deliver highly accurate predictions. This is the reason why we ask whether complex machine learning models are even necessary to use. We envision that practical resource management systems need to first identify the extent to which simple solutions can be effective, and resort to using machine learning to the extent that enables its practical use.",
        "DOI": "10.1145/3620678.3624790",
        "affiliation_name": "IMDEA Software Institute",
        "affiliation_city": "Pozuelo de Alarcon",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "Power and Public Participation in AI",
        "paper_author": "Corbett E.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "10",
        "cover_date": "2023-10-30",
        "Abstract": "The rapid growth of AI in contemporary life has outpaced the public participation necessary for society to determine how these technologies should be used. As scholars respond to this challenge by exploring new modes of public participation in AI, we help advance these efforts by introducing influential work from public planning scholarship, the Ladder of Citizen Participation, as an analytical lens to help compare and contrast power in this work. We used the ladder to analyze participatory approaches to AI development in recent scholarship, finding that most of this work informs or consults rather than partners with or delegates control to participants. We also found that papers frequently reflect a writing style that makes it difficult to ascertain the degree of power afforded. We discuss implications from our work for powerholders (developers, researchers, practitioners) offering participatory approaches to AI and for people (specific communities, stakeholders, general public) participating in those processes.",
        "DOI": "10.1145/3617694.3623228",
        "affiliation_name": "University of Maryland, College Park",
        "affiliation_city": "College Park",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Triton: Software-Defined Threat Model for Secure Multi-Tenant ML Inference Accelerators",
        "paper_author": "Banerjee S.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-10-29",
        "Abstract": "Secure machine-learning inference is essential with the advent of multi-tenancy in machine learning-as-a-service (MLaaS) accelerators. Model owners demand the confidentiality of both model weights and architecture, while end users want to protect their personal data. Moreover, ML models used in mission-critical applications like autonomous vehicles or disease classification need integrity protection. While hardware trusted execution environments (TEE) [4, 41] provide data confidentiality and integrity, they face two challenges in the adoption for ML inference. First, TEEs are susceptible to numerous side channels, arising from resource sharing in multi-tenant systems. Second, the performance overhead of these TEEs is often proportional to the secret data size, making them unattractive for data-intensive real-time inference. The diverse deployment threats further complicate these challenges. For instance, compared to time-sharing execution, multi-tenant accelerators must assume a larger attack surface with adversaries monitoring or tampering with on-accelerator resources. Some inference process sensitive inputs while others compute on public inputs. As a result, existing TEE designs often adopt a single, perhaps the most restrictive threat model, which overburdens many secure ML inference deployments. To address the challenges in adopting TEEs for secure ML inference, we introduce the Triton TEE framework. Triton tailors threat models to each deployment with low overhead while mitigating side-channel leakages. Triton achieves this by offering an interface to define fine-grained secrets in an ML model or input, along with the attacker observation capabilities. Triton framework generates code for a custom threat model for each application based on its security requirements. The security policy of each secret is embedded in the instruction to convey the security guarantee to the hardware. The expressive threat model and secret declaration can reduce the secure ML inference overhead from to across different multi-tenant deployments.",
        "DOI": "10.1145/3623652.3623672",
        "affiliation_name": "Arm Research",
        "affiliation_city": null,
        "affiliation_country": null
    },
    {
        "paper_title": "NutritionVerse: Empirical Study of Various Dietary Intake Estimation Approaches",
        "paper_author": "Tai C.E.A.",
        "publication": "MADiMa 2023 - Proceedings of the 8th International Workshop on Multimedia Assisted Dietary Management, Co-located with: MM 2023",
        "citied_by": "1",
        "cover_date": "2023-10-29",
        "Abstract": "Accurate dietary intake estimation is critical for informing policies and programs to support healthy eating, as malnutrition has been directly linked to decreased quality of life. However self-reporting methods such as food diaries suffer from substantial bias. Other conventional dietary assessment techniques and emerging alternative approaches such as mobile applications incur high time costs and may necessitate trained personnel. Recent work has focused on using computer vision and machine learning to automatically estimate dietary intake from food images, but the lack of comprehensive datasets with diverse viewpoints, modalities and food annotations hinders the accuracy and realism of such methods. To address this limitation, we introduce NutritionVerse-Synth, the first large-scale dataset of 84,984 photorealistic synthetic 2D food images with associated dietary information and multimodal annotations (including depth images, instance masks, and semantic masks). Additionally, we collect a real image dataset, NutritionVerse-Real, containing 889 images of 251 dishes to evaluate realism. Leveraging these novel datasets, we develop and benchmark NutritionVerse, an empirical study of various dietary intake estimation approaches, including indirect segmentation-based and direct prediction networks. We further fine-tune models pretrained on synthetic data with real images to provide insights into the fusion of synthetic and real data. Finally, we release both datasets (NutritionVerse-Synth, NutritionVerse-Real) and the collection of models (NutritionVerse) on https://bit.ly/genai4good as part of an open initiative to accelerate machine learning for dietary sensing.",
        "DOI": "10.1145/3607828.3617799",
        "affiliation_name": "University of Waterloo",
        "affiliation_city": "Waterloo",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "WattWiser: Power &amp; Resource-Efficient Scheduling for Multi-Model Multi-GPU Inference Servers",
        "paper_author": "Jahanshahi A.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2023-10-28",
        "Abstract": "With the increasing integration of Machine Learning (ML) applications into cloud services, providing high throughput Machine Learning inference serving has become a major demand for cloud service providers. The inference requests need to respond with bounded latency for each request to maintain a consistent Service-Level Objective (SLO). To ensure SLO, inference servers are equipped with multiple GPUs to satisfy the computational requirements. However, multi-GPU systems are extremely power-hungry. To resolve this, it is ideal to consolidate the load to a sub-set of GPUs, and potentially share GPUs, in order to minimize power consumption, without violating SLO. By consolidating GPUs and potentially sharing GPUs we can reduce the power consumption of multi-GPU inference servers. However, multiple inference models typically share the same inference server, which adds significant challenges in multi-model multi-GPU inference server environments. In this paper, we explore the challenges that this brings in achieving power efficiency. We introduce WattWiser, a model management and scheduling policy that achieves power savings in multi-model environments where GPUs are shared. Our results show that WattWiser can reduce power consumption by 34% while serving multiple models and maintaining the SLO.",
        "DOI": "10.1145/3634769.3634807",
        "affiliation_name": "Marlan and Rosemary Bourns College of Engineering",
        "affiliation_city": "Riverside",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "A Railway Passenger Flow Prediction Model Based on Improved Prophet",
        "paper_author": "Chang J.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-10-27",
        "Abstract": "Railway passenger departments can benefit from predictive analysis of passenger flow data to inform operational policies. This paper presents an Improved Prophet method for predicting railway passenger flow, tailored to the unique features of this mode of transport. The data is extracted, constructed, and transformed to meet the requirements of the model. A nonlinear correlation analysis algorithm is used to determine the impact of external factors on passenger flow. The holiday factors are analyzed individually, resulting in an Improved Prophet model. The prediction outcomes are compared with Long Short-Term Memory Network (LSTM) and Differential Integrated Moving Average Autoregressive Model (ARIMA) models using RMSE, MAE, and MAPE evaluation metrics. The results demonstrate that the Improved Prophet method outperforms other baseline models in predicting railway passenger flow. This can provide valuable decision-making support to the railway passenger transport department",
        "DOI": "10.1145/3650215.3650354",
        "affiliation_name": "Lanzhou Jiaotong University",
        "affiliation_city": "Lanzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Grouped Federated Learning Algorithm Based on Non-IID Data",
        "paper_author": "Li Z.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-10-27",
        "Abstract": "Federated learning is a new machine learning paradigm in which multiple clients collaborate to train a machine learning model while protecting local data privacy. Client-side non-identically and non-independently distributed (Non-IID) data is one of the most important challenges for federated learning algorithms. The classification accuracy of the federated learning model drops drastically due to the weight dispersion caused by data heterogeneity. In this work, we adopt a way of dividing client data that is closer to real applications. We propose a federated learning algorithm based on rearranging all clients(FedRC). FedRC sets up a novel within-group training approach and a server-side pre-aggregation policy to address the impact of Non-IID on the model. In addition, we test FedRC on a simulated dataset and the experimental results demonstrate that FedRC outperforms the baseline algorithm and significantly improves the performance on Non-IID data.",
        "DOI": "10.1145/3650215.3650331",
        "affiliation_name": "Changchun University of Science and Technology",
        "affiliation_city": "Changchun",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Automated Machine Learning in Waste Classification: A Revolutionary Approach to Efficiency and Accuracy",
        "paper_author": "Lee Z.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-10-27",
        "Abstract": "In recent years, waste segregation, treatment, and recycling have become critical global issues, drawing significant attention worldwide. However, the efficiency of recycling processes can be influenced by numerous factors. Among these, waste classification plays a crucial role, and researchers have explored the application of machine learning to automate this step. Nonetheless, traditional machine learning approaches often require skilled professionals to invest substantial time in debugging, and achieving satisfactory accuracy can be challenging. To address this concern, we propose leveraging Automated Machine Learning (AutoML) to enhance the speed and accuracy of waste classification, thereby expediting the implementation of waste segregation policies. Our findings indicate that AutoML outperforms traditional machine learning models, requiring less time and energy, while achieving an impressive accuracy and precision rate of 95.1%.",
        "DOI": "10.1145/3633637.3633684",
        "affiliation_name": "Dongguan University of Technology",
        "affiliation_city": "Dongguan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Intervention targets for reducing mortality between mid-adolescence and mid-adulthood: A protocol for a machine-learning facilitated systematic umbrella review",
        "paper_author": "Kerr J.A.",
        "publication": "BMJ Open",
        "citied_by": "1",
        "cover_date": "2023-10-27",
        "Abstract": "Introduction A rise in premature mortality - defined here as death during the most productive years of life, between adolescence and middle adulthood (15-60 years) - is contributing to stalling life expectancy in high-income countries. Causes of mortality vary, but often include substance misuse, suicide, unintentional injury and non-communicable disease. The development of evidence-informed policy frameworks to guide new approaches to prevention require knowledge of early targets for intervention, and interactions between higher level drivers. Here, we aim to: (1) identify systematic reviews with or without meta-analyses focused on intervention targets for premature mortality (in which intervention targets are causes of mortality that can, at least hypothetically, be modified to reduce risk); (2) evaluate the review quality and risk of bias; (3) compare and evaluate each review's, and their relevant primary studies, findings to identify existing evidence gaps. Methods and analysis In May 2023, we searched electronic databases (MEDLINE, PubMed, Embase, Cochrane Library) for peer-reviewed papers published in the English language in the 12 years from 2012 to 2023 that examined intervention targets for mortality. Screening will narrow these papers to focus on systematic reviews with or without meta-analyses, and their primary papers. Our outcome is death between ages 15 and 60 years; with potential intervention targets measured prior to death. A MeaSurement Tool to Assess systematic Reviews (AMSTAR 2) will be used to assess quality and risk of bias within included systematic reviews. Results will be synthesised narratively due to anticipated heterogeneity between reviews and between primary studies contained within included reviews. Ethics and dissemination This review will synthesise findings from published systematic reviews and meta-analyses, and their primary reviewed studies, meaning ethics committee approval is not required. Our findings will inform cross-cohort consortium development, be published in a peer-reviewed journal, and be presented at national and international conferences. PROSPERO registration number CRD42022355861.",
        "DOI": "10.1136/bmjopen-2022-068733",
        "affiliation_name": "Children’s Health Queensland",
        "affiliation_city": "Brisbane",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Living guidelines for generative AI — why scientists must oversee its use",
        "paper_author": "Bockting C.L.",
        "publication": "Nature",
        "citied_by": "34",
        "cover_date": "2023-10-26",
        "Abstract": "NA",
        "DOI": "10.1038/d41586-023-03266-1",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Skill development for society 5.0: A focus on the new-age skilling process",
        "paper_author": "Shalender K.",
        "publication": "Innovations and Sustainability in Society 5.0",
        "citied_by": "2",
        "cover_date": "2023-10-24",
        "Abstract": "The purpose of the chapter is to highlight the key skills that are required to make a successful transition to society 5.0. After doing a comprehensive literature review, it has been found that among the prominent new-age skills, the role and importance of artificial intelligence (AI), machine learning (ML), automation, and data Analytics (DA) couldn't be overemphasized. The chapter highlights the importance of these skills in detail and then the following section details about the development of the conceptual framework required for imbibing these skills among professionals. The research concludes with a discussion of the important results and implications of these findings for stakeholders across the education, business and policy-making ecosystems. The study is unique in terms of its implications for society and by providing a conceptual framework for making a seamless transition towards society 5.0, it contributes to academia, corporate, and society at large.",
        "DOI": "NA",
        "affiliation_name": "Chitkara University, Punjab",
        "affiliation_city": "Rajpura",
        "affiliation_country": "India"
    },
    {
        "paper_title": "The theory of technological response and progress in chaos",
        "paper_author": "Ozcan S.",
        "publication": "Foresight",
        "citied_by": "2",
        "cover_date": "2023-10-24",
        "Abstract": "Purpose: This study aims to develop the first Theory of Technological Response and Progress in Chaos (TRPC) and examine the case of technological development during the COVID-19 pandemic. The research objectives of this study were to: identify the key technologies that act as a response mechanism during the chaos event, specifically in the case of COVID-19; examine how technologies evolve, develop and diffuse in an immediate crisis and a chaotic environment; theorise various types and periods of technological response and progress during the emergence of chaos and the stages that unfold; and develop policy-oriented recommendations and establish technological foundations to address subsequent chaos events. Design/methodology/approach: This study used the grounded theory as a methodology with a mixed-method approach that included quantitative and qualitative methods. The authors used the quantitative method to assist with the qualitative step to build the TRPC theory. Accordingly, this study integrated machine learning and text mining approaches to the qualitative data analysis following the steps of the grounded theory approach. Findings: As a result of the TRPC theory development process, the authors identified three types of technologies (survival, essential and enhancement technologies) and five types of periods (stable, initial, survival-dominant, essential-dominant and enhancement-dominant periods) that are specific to chaos-technology interactions. The policy implications of this study demonstrate that a required technological base and know-how must be established before a chaotic event emerges. Research limitations/implications: Concerning the limitations of this study, social media data has advantages over other data sources, such as the examination of dynamic areas and analyses of immediate responses to chaos. However, other researchers can examine publications and patent sources to augment the findings concerning scientific approaches and new inventions in relation to COVID-19 and other chaos-specific developments. The authors developed the TRPC theory by studying the COVID-19 pandemic, however, other researchers can utilise it to study other chaos-related conditions, such as chaotic events that are caused by natural disasters. Other scholars can investigate the technological response and progress pattern in other rapidly emerging chaotic events of an uncertain and complex nature to augment these findings. Practical implications: Following the indications of the OECD (2021a) and considering the study conducted by the European Parliamentary Research Service (Kritikos, 2020), the authors identified the key technologies that are significant for chaos and COVID-19 response using machine learning and text intelligence approach. Accordingly, the authors mapped all technological developments using clustering approaches, and examined the technological progress within the immediate chaos period using social media data. Social implications: The key policy implication of this study concerns the need for policymakers to develop policies that will help to establish the required technological base and know-how before chaos emerges. As a result, a rapid response can be implemented to mitigate the chaos and transform it into a competitive advantage. The authors also revealed that this recommendation overlaps with the model of dynamic capabilities in the literature (Teece and Pisano, 2003). Furthermore, this study recommends that nations and organisations establish a technological base that specifically includes technologies that bear 3A characteristics. These are the most crucial technologies for the survival- and essential-dominant stages. Moreover, the results of this study demonstrate that chaos accelerates technological progress through the rapid adoption and diffusion of technologies into different fields. Hence, nations and organisations should regard this rapid progress as an opportunity and establish the prior knowledge base and technologies before chaos emerges. Originality/value: The authors have contributed to the chaos studies and the relationship between chaos and technological development by establishing the first theoretical foundation using the grounded theory approach, hereafter referred to as the TRPC theory. As part of the TRPC theory, the authors present three periods of technological response in the following sequence: survival technology, essential technology and enhancement technology. Moreover, this study illustrates the evolving technological importance and priorities as the periods of technological progress proceed under rapidly developing chaos.",
        "DOI": "10.1108/FS-11-2022-0138",
        "affiliation_name": "Faculty of Business and Law",
        "affiliation_city": "Portsmouth",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "PRECISION: Decentralized Constrained Min-Max Learning with Low Communication and Sample Complexities",
        "paper_author": "Liu Z.",
        "publication": "Proceedings of the International Symposium on Mobile Ad Hoc Networking and Computing (MobiHoc)",
        "citied_by": "0",
        "cover_date": "2023-10-23",
        "Abstract": "Recently, min-max optimization problems have received increasing attention due to their wide range of applications in machine learning (ML). However, most existing min-max solution techniques are either single-machine or distributed algorithms coordinated by a central server. In this paper, we focus on the decentralized min-max optimization for learning with domain constraints, where multiple agents collectively solve a nonconvex-strongly-concave min-max saddle point problem without coordination from any server. Decentralized min-max optimization problems with domain constraints underpins many important ML applications, including multi-agent ML fairness assurance, and policy evaluations in multi-agent reinforcement learning. We propose an algorithm called PRECISION (proximal gradient-tracking and stochastic recursive variance reduction) that enjoys a convergence rate of O(1/T), where T is the maximum number of iterations. To further reduce sample complexity, we propose PRECISION+ with an adaptive batch size technique. We show that the fast O(1/T) convergence of PRECISION and PRECISION+ to an ϵ-stationary point imply O(ϵ-2) communication complexity and [EQUATION] sample complexity, where m is the number of agents and n is the size of dataset at each agent. To our knowledge, this is the first work that achieves O(ϵ-2) in both sample and communication complexities in decentralized min-max learning with domain constraints. Our experiments also corroborate the theoretical results.",
        "DOI": "10.1145/3565287.3610267",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Columbus",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Learning to Schedule in Non-Stationary Wireless Networks With Unknown Statistics",
        "paper_author": "Nguyen Q.M.",
        "publication": "Proceedings of the International Symposium on Mobile Ad Hoc Networking and Computing (MobiHoc)",
        "citied_by": "2",
        "cover_date": "2023-10-23",
        "Abstract": "The emergence of large-scale wireless networks with partially-observable and time-varying dynamics has imposed new challenges on the design of optimal control policies. This paper studies efficient scheduling algorithms for wireless networks subject to generalized interference constraint, where mean arrival and mean service rates are unknown and non-stationary. This model exemplifies realistic edge devices' characteristics of wireless communication in modern networks. We propose a novel algorithm termed MW-UCB for generalized wireless network scheduling, which is based on the Max-Weight policy and leverages the Sliding-Window Upper-Confidence Bound to learn the channels' statistics under non-stationarity. MW-UCB is provably throughput-optimal under mild assumptions on the variability of mean service rates. Specifically, as long as the total variation in mean service rates over any time period grows sub-linearly in time, we show that MW-UCB can achieve the stability region arbitrarily close to the stability region of the class of policies with full knowledge of the channel statistics. Extensive simulations validate our theoretical results and demonstrate the favorable performance of MW-UCB.",
        "DOI": "10.1145/3565287.3610258",
        "affiliation_name": "Massachusetts Institute of Technology",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Misinformation Concierge: A Proof-of-Concept with Curated Twitter Dataset on COVID-19 Vaccination",
        "paper_author": "Sharma S.",
        "publication": "International Conference on Information and Knowledge Management, Proceedings",
        "citied_by": "3",
        "cover_date": "2023-10-21",
        "Abstract": "We demonstrate the Misinformation Concierge, a proof-of-concept that provides actionable intelligence on misinformation prevalent in social media. Specifically, it uses language processing and machine learning tools to identify subtopics of discourse and discerns non/misleading posts; presents statistical reports for policy-makers to understand the big picture of prevalent misinformation in a timely manner; and recommends rebuttal messages for specific pieces of misinformation, identified from within the corpus of data - providing means to intervene and counter misinformation promptly. The Misinformation Concierge proof-of-concept using a curated dataset is accessible at: https://demo-frontend-uy34.onrender.com/.",
        "DOI": "10.1145/3583780.3614746",
        "affiliation_name": "School of Computer Science and Engineering",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore"
    },
    {
        "paper_title": "Amazon web services: The definitive guide for beginners and advanced users",
        "paper_author": "Dubey P.",
        "publication": "Amazon Web Services: The Definitive Guide for Beginners and Advanced Users",
        "citied_by": "3",
        "cover_date": "2023-10-19",
        "Abstract": "Amazon Web Services: A Comprehensive Guide for Beginners and Advanced Users is your go-to companion for learning and mastering AWS. It presents 10 easy-to-read chapters that build a foundation for cloud computing while also equipping readers with the skills necessary to use AWS for commercial projects. Readers will learn how to use AWS cloud computing services for seamless integrations, effective monitoring, and optimizing cloud-based web applications. What you will learn from this guide: 1. Identity and Access Management in AWS: Learn about IAM roles, security of the root account, and password policies, ensuring a robust foundation in access management. 2. Amazon EC2 Instance: Explore the different types of EC2 instances, pricing strategies, and hands-on experiences to launch, manage, and terminate EC2 instances effectively. This knowledge will help to make informed choices about pricing strategies. 3. Storage Options and Solutions: A detailed examination of storage options within Amazon EC2 instances. Understanding Amazon Elastic Block Store (EBS), Amazon Elastic File Storage (EFS), and more, will enhance your ability to handle data storage efficiently. 4. Load Balancing and Auto Scaling: Learn about different types of load balancers and how auto-scaling groups operate, to master the art of managing varying workloads effectively. 5. Amazon Simple Storage Service (S3): Understand S3 concepts such as buckets, objects, versioning, storage classes, and practical applications. 6. AWS Databases and Analytics: Gain insights into modern databases, AWS cloud databases, and analytics services such as Amazon Quicksight, AWS Glue, and Amazon Redshift. 7. Compute Services and Integrations: Understand the workings of Docker, virtual machines, and various compute services offered by AWS, including AWS Lambda and Amazon Lightsail, Amazon MQ and Amazon SQS. 8. Cloud Monitoring: Understand how to set up alarms, analyze metrics, and ensure the efficient monitoring of your cloud environment using Amazon Cloud Watch and Cloud Trail.",
        "DOI": "10.2174/97898151658211230101",
        "affiliation_name": "Dr. C.V. Raman University",
        "affiliation_city": "Bilaspur",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Predictive algorithms in the delivery of public employment services",
        "paper_author": "Körtner J.",
        "publication": "Handbook of Labour Market Policy in Advanced Democracies",
        "citied_by": "3",
        "cover_date": "2023-10-17",
        "Abstract": "With the growing availability of digital administrative data and the recent advances in machine learning, the use of predictive algorithms in the delivery of labour market policy is becoming more prevalent. In public employment services (PES), predictive algorithms are used to support the classification of jobseekers based on their risk of long-term unemployment (profiling), the selection of beneficial active labour market programmes (targeting), and the matching of jobseekers to suitable job opportunities (matching). In this chapter, we offer a conceptual introduction to the applications of predictive algorithms for the different functions PES have to fulfil and review the history of their use up to the current state of the practice. In addition, we discuss two issues that are inherent to the use of predictive algorithms: algorithmic fairness concerns and the importance of considering how caseworkers will interact with algorithmic systems and make decisions based on their predictions.",
        "DOI": "10.4337/9781800880887.00037",
        "affiliation_name": "Université de Lausanne (UNIL)",
        "affiliation_city": "Lausanne",
        "affiliation_country": "Switzerland"
    },
    {
        "paper_title": "Measuring Swing Voters with a Supervised Machine Learning Ensemble",
        "paper_author": "Hare C.",
        "publication": "Political Analysis",
        "citied_by": "6",
        "cover_date": "2023-10-17",
        "Abstract": "Theory has long suggested that swing voting is a response to cross-pressures arising from a mix of individual attributes and contextual factors. Unfortunately, existing regression-based approaches are ill-suited to explore the complex combinations of demographic, policy, and political factors that produce swing voters in American elections. This gap between theory and practice motivates our use of an ensemble of supervised machine learning methods to predict swing voters in the 2012, 2016, and 2020 U.S. presidential elections. The results from the learning ensemble substantiate the existence of swing voters in contemporary American elections. Specifically, we demonstrate that the learning ensemble produces well-calibrated and externally valid predictions of swing voter propensity in later elections and for related behaviors such as split-ticket voting. Although interpreting black-box models is more challenging, they can nonetheless provide meaningful substantive insights meriting further exploration. Here, we use flexible model-agnostic tools to perturb the ensemble and demonstrate that cross-pressures (particularly those involving ideological and policy-related considerations) are essential to accurately predict swing voters.",
        "DOI": "10.1017/pan.2022.24",
        "affiliation_name": "University of California, Davis",
        "affiliation_city": "Davis",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Mapping the spatial transmission risk and public spatial awareness in the use of personal protective equipment: COVID-19 pandemic in East Java, Indonesia",
        "paper_author": "Purwanto P.",
        "publication": "International Journal of Disaster Risk Reduction",
        "citied_by": "1",
        "cover_date": "2023-10-15",
        "Abstract": "This study aimed at developing a machine learning-based COVID-19 transmission risk spatial model and an analysis of the public spatial awareness in the use of personal protective equipment (PPE) in their spatial environment (i.e., a spatial-based COVID-19 transmission risk model). Random Forest model combined with Information Gain was used in this study. Twenty-three geospatial variables that passed a feature selection process were inputted to build a spatial-based COVID-19 transmission risk model. The validation outcome reveals that Random Forest (RF) model achieved excellent COVID-19 transmission risk prediction result (AUC value of 94.7%). Transmission risk modeling results show that the moderate-to-high class risk pattern was concentrated in residential areas with complex road infrastructure. The model showed that land use, road infrastructure, minimum temperature, and close to urban areas is important for spatial modeling of COVID-19 transmission risk. The results of the public spatial awareness analysis show variations in respondents' use of PPE based on their spatial environment. They tend to carry one PPE whose use increases significantly from low-risk to high-risk. While being indoor, the majority used hand sanitizer, and a face mask when being outdoor. Thus, developing a spatial-based COVID-19 transmission risk prediction linked with public spatial awareness can be used as a reference to establish health policies or regulations by decision-makers, health boards, and governments to control and simultaneously reduce the risk of transmission more effectively and efficiently.",
        "DOI": "10.1016/j.ijdrr.2023.104018",
        "affiliation_name": "Universitas Negeri Malang",
        "affiliation_city": "Malang",
        "affiliation_country": "Indonesia"
    },
    {
        "paper_title": "The use of machine-learning methods for post-earthquake building usability assessment: A predictive model for seismic-risk impact analyses",
        "paper_author": "Tocchi G.",
        "publication": "International Journal of Disaster Risk Reduction",
        "citied_by": "6",
        "cover_date": "2023-10-15",
        "Abstract": "The assessment of building usability in the aftermath of an earthquake is mostly aimed at post-event emergency management, but it is also valuable for the planning of risk-reduction policies. In the seismic risk assessment field, the development of suitable consequence functions that correlate physical damage to usability and serviceability of structures is crucial to evaluate the expected social and economic losses in a region of interest. Predictive models for usability classification generally are calibrated on empirical data and provide the probability of loss of usability as function of the intensity measure, the building type and the severity of damage attained by the structure. Exploiting the large amount of data available in Italy, a decision tree-based approach is proposed in this study to assess post-earthquake usability of ordinary buildings. Thanks to its high interpretability coupled with reasonable predictive capability _, the selected machine learning algorithm allows investigation of the structural parameters that have a significant impact on building usability, while also accounting for the traditionally neglected uncertainty of subjective decisions. Finally, to show the potential of the proposed usability consequence models, a large-scale risk analysis is carried out to evaluate the spatial distribution of expected building-usability losses over time.",
        "DOI": "10.1016/j.ijdrr.2023.104033",
        "affiliation_name": "George R. Brown School of Engineering and Computing",
        "affiliation_city": "Houston",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Prediction of groundwater quality index to assess suitability for drinking purpose using averaged neural network and geospatial analysis",
        "paper_author": "Ahn S.H.",
        "publication": "Ecotoxicology and Environmental Safety",
        "citied_by": "2",
        "cover_date": "2023-10-15",
        "Abstract": "Groundwater quality management is pivotal for ensuring public health and ecological resilience. However, the conventional water quality indices often face challenges related to parameter selection, geographic coverage, and scalability. The integration of machine learning and spatial analysis represents a promising methodological shift, allowing for high accuracy and adaptive management strategies. The Safe Groundwater Project in Unsupplied Areas (2017–2020) employed a comprehensive Groundwater Quality Index (GQI) to evaluate potable groundwater quality across South Korea, utilizing a large dataset comprising 28 water quality parameters and 3552 wells. This study revealed that over 50 % of the evaluated wells (Total 8326 wells) were inappropriate as sources of drinking water, indicating a pressing need for policy revision. The averaged neural network model achieved a high predictive accuracy of approximately 95 % for GQI grades, outperforming other classification models. The introduction of 2D spatial analysis in conjunction with machine learning algorithms notably increased the predictive accuracy for unevenly distributed groundwater samples. Moreover, this combined approach enabled the intuitive visualization of groundwater vulnerability across various regions, which can inform targeted interventions for effective resource allocation and management. This research represents a methodologically robust, interdisciplinary approach that holds significant implications for a framework for future groundwater quality management and vulnerability assessment.",
        "DOI": "10.1016/j.ecoenv.2023.115485",
        "affiliation_name": "Yonsei University Mirae Campus",
        "affiliation_city": "Wonju",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "Mapping historical forest biomass for stock-change assessments at parcel to landscape scales",
        "paper_author": "Johnson L.K.",
        "publication": "Forest Ecology and Management",
        "citied_by": "3",
        "cover_date": "2023-10-15",
        "Abstract": "Understanding historical forest dynamics, specifically changes in forest biomass and carbon stocks, has become critical for assessing current forest climate benefits and projecting future benefits under various policy, regulatory, and stewardship scenarios. Carbon accounting frameworks based exclusively on national forest inventories are limited to broad-scale estimates, but model-based approaches that combine these inventories with remotely sensed data can yield contiguous fine-resolution maps of forest biomass and carbon stocks across landscapes over time. Here we describe a fundamental step in building a map-based stock-change framework: mapping historical forest biomass at fine temporal and spatial resolution (annual, 30 m) across all of New York State (USA) from 1990 to 2019, using freely available data and open-source tools. Using Landsat imagery, US Forest Service Forest Inventory and Analysis (FIA) data, and off-the-shelf LiDAR collections we developed three modeling approaches for mapping historical forest aboveground biomass (AGB): training on FIA plot-level AGB estimates (direct), training on LiDAR-derived AGB maps (indirect), and an ensemble averaging predictions from the direct and indirect models. Model prediction surfaces (maps) were tested against FIA estimates at multiple scales. All three approaches produced viable outputs, yet tradeoffs were evident in terms of model complexity, map accuracy, saturation, and fine-scale pattern representation. The resulting map products can help identify where, when, and how forest carbon stocks are changing as a result of both anthropogenic and natural drivers alike. These products can thus serve as inputs to a wide range of applications including stock-change assessments, monitoring reporting and verification frameworks, and prioritizing parcels for protection or enrollment in improved management programs.",
        "DOI": "10.1016/j.foreco.2023.121348",
        "affiliation_name": "SUNY College of Environmental Science and Forestry",
        "affiliation_city": "Syracuse",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "A compound framework incorporating improved outlier detection and correction, VMD, weight-based stacked generalization with enhanced DESMA for multi-step short-term wind speed forecasting",
        "paper_author": "Fu W.",
        "publication": "Applied Energy",
        "citied_by": "29",
        "cover_date": "2023-10-15",
        "Abstract": "Precise wind speed forecasting contributes to wind power consumption and power grid schedule as well as promotes the implementation of global carbon neutrality policy. However, in existing research, the negative impact of outliers on forecasting models is ignored and the inherent shortcomings of the single predictors have not been taken seriously. Moreover, the intrinsic parameters of predictors are set by manual and empirical methods in some research, leading to difficulties in achieving optimal forecasting performance. To solve the shortcomings of existing research, a multi-step short-term wind speed forecasting framework is proposed by incorporating boxplot-medcouple (MC), variational mode decomposition (VMD), phase space reconstruction (PSR), weight-based stacked generalization with enhanced differential evolution slime mold algorithm (DESMA). Firstly, boxplot-MC is employed to achieve outlier detection and correction for preprocessing original wind speed data by analyzing values and trends. Then, the modified data is further adaptively decomposed into multiple subsequences by VMD, after which each subsequence is constructed into feature matrices through PSR. Subsequently, weight-based multi-model fusion strategy in layer-1 of stacked generalization is proposed to integrate the predicting values acquired by three primary learners, of which the weight coefficients are calculated with the error between actual values and predicting values. After that, kernel extreme learning machine (KELM) in layer-2 of stacked generalization is applied to predict the fusion result to obtain forecasting value corresponding to each subsequence. Meanwhile, an enhanced DESMA based on slime mold algorithm (SMA) and differential evolution (DE) is proposed to calibrate the parameters of KELM. Eventually, the final wind speed forecasting results are attained by summing the prediction values of all subsequences. Furthermore, comparative experiments from different aspects are undertaken on real datasets to ascertain the availability of the proposed framework. The experimental results are clarified as follows: (1) outlier detection and correction employing boxplot-MC is dedicated to analyzing values and trends effectively, with which the negative impact of outliers can be weakened while retaining valid data significantly; (2) VMD can prominently reduce the non-smoothness and volatility of wind speed data; (3) weight-based stacked generalization is conducive to exploiting the advantages of individual primary learners, contributing to compensating for instability; (4) DESMA enhances prediction accuracy by optimizing the parameters of KELM. Additionally, the code has been made available at https://github.com/fyc233/a-multi-step-short-term-wind-speed-forecasting-framework.git.",
        "DOI": "10.1016/j.apenergy.2023.121587",
        "affiliation_name": "China Three Gorges University",
        "affiliation_city": "Yichang",
        "affiliation_country": "China"
    },
    {
        "paper_title": "On the performance of different Deep Reinforcement Learning based controllers for the path-following of a ship",
        "paper_author": "Sivaraj S.",
        "publication": "Ocean Engineering",
        "citied_by": "11",
        "cover_date": "2023-10-15",
        "Abstract": "A set of continuous state-action space-based deep reinforcement learning algorithms are used for the path following of a ship in calm water and waves. The mathematical model of a KVLCC2 tanker represents the ship dynamics. The mathematical model includes the hull force, rudder force, propulsion force, and external wave forces. Look ahead distance-based guidance algorithm called Line of Sight (LOS) is used for computing the Cross Track Error (CTE) and Heading Error (HE). The reward function is designed based on HE and CTE. The created Environment is trained with four different Deep Reinforcement Learning (DRL) agents named Proximal Policy Optimization (PPO), Deep Deterministic Policy Gradients (DDPG), Twin-Delayed Deep Deterministic Policy Gradients (TD3), and Soft-Actor Critic (SAC). Common Neural Network architecture is used for all four agents. Yaw rate, HE, and CTE serve as input to the Neural Network, and the rudder deflection rate (δ°) corresponds to the action space (output). Computation time, average cross-track error, and rudder actuation are computed and compared for path-following scenarios. DDPG performs better with a minimum average CTE for all the simulated cases. However, SAC demands minimum rudder control effort to achieve the tasks. Finally, the trained agents are validated using Hardware In-Loop (HIL) simulation.",
        "DOI": "10.1016/j.oceaneng.2023.115607",
        "affiliation_name": "Indian Institute of Technology Madras",
        "affiliation_city": "Chennai",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Coupling multi-sensory earth observation datasets, in-situ measurements, and machine learning algorithms for total blue C stock estimation of an estuarine mangrove forest",
        "paper_author": "Datta D.",
        "publication": "Forest Ecology and Management",
        "citied_by": "7",
        "cover_date": "2023-10-15",
        "Abstract": "In recent years, research on blue carbon (C) has garnered substantial attention worldwide. Nevertheless, we observed a lack of holistic approach, in terms of measurement of total blue C (TBC) potentials. This study focuses on developing a novel approach toward blue C accounting by spatially explicit modelling and estimation of TBC stock in a mangrove wetland of eastern India. A hybrid methodology has been adopted incorporating destructive and non-destructive sampling, allometric and predictive modelling, laboratory-based elemental analysis, and multi-sensory remote sensing (RS) based datasets. Predicted TBC density has been mapped within the wetland influence zone (WIZ) of the study site. Point-specific sample data (n = 250) has been used for the determination of the soil organic C (SOC) prediction model. Spline interpolation, displaying highest R2 value (R2 = 0.74) has been chosen for spatially explicit modelling of total SOC stock. Above ground biomass (AGB) was determined using the relationship between remotely sensed data (ALOS PALSAR-2 and Pleiades-1B) and in-situ dendrometric variables (viz. wood density, tree height, and girth at breast height). Here, among the different parametric and nonparametric models to estimate AGB, the BP-ANN models, specifically model number 22 (adjusted R2 = 0.84, MSE = 1.28, AIC = 3.67, BIC = 1.60), has been identified as the best-fit one with higher adjusted R2 and lesser AIC and BIC values. Indirect allometric equations involving modelled AGB values had been used to generate spatially explicit community-specific below ground biomass values at per pixel basis (∼2 m). Above and below ground C were estimated from these raster data. Integrating all these datasets in a GIS platform, the overall TBC stock of the mangrove was recorded at 246710.91 Mg. The TBC density of mangrove WIZ had revealed considerable variations, ranging from 0.34 Mg ha−1 to 881.50 Mg ha−1. Cumulatively, the study attempted to amalgamate all facets of blue C pools with satisfactory accuracy. This holistic methodology may further aid in regional C stock inventorization, management, and policy formulation, thereby strengthening the socio-economic resilience of coastal communities through carbon trading, reduce emissions from ecosystem degradation as well as support ongoing conservation efforts.",
        "DOI": "10.1016/j.foreco.2023.121345",
        "affiliation_name": "Banwarilal Bhalotia College",
        "affiliation_city": "Asansol",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Predicting spatiotemporal soil organic carbon responses to management using EPIC-IIASA meta-models",
        "paper_author": "Ippolito T.",
        "publication": "Journal of Environmental Management",
        "citied_by": "2",
        "cover_date": "2023-10-15",
        "Abstract": "The management of Soil Organic Carbon (SOC) is a critical component of both nature-based solutions for climate change mitigation and global food security. Agriculture has contributed substantially to a reduction in global SOC through cultivation, thus there has been renewed focus on management practices which minimize SOC losses and increase SOC gain as pathways towards maintaining healthy soils and reducing net greenhouse gas emissions. Mechanistic models are frequently used to aid in identifying these pathways due to their scalability and cost-effectiveness. Yet, they are often computationally costly and rely on input data that are often only available at coarse spatial resolutions. Herein, we build statistical meta-models of a multifactorial crop model in order to both (a) obtain a simplified model response and (b) explore the biophysical determinants of SOC responses to management and the geospatial heterogeneity of SOC dynamics across Europe. Using 5600 unique simulations of crop growth from the gridded Environmental Policy Integrated Climate-based Gridded Agricultural Model (EPIC-IIASA GAM) covering 86,000 simulation units across Europe, we build multiple polynomial regression ensemble meta-models for unique combinations of climate and soil across Europe in order to predict SOC responses to varying management intensities. We find that our biophysically-explicit meta models are highly accurate (R2 = 0.97) representations of the full mechanistic model and can be used in lieu of the full EPIC-IIASA GAM model for the estimation of SOC responses to cropland management. Model stratification by means of climate and soil clustering improved the performance of the meta-models compared to the full EU-scale model. In regional and local validations of the meta-model predictions, we find that the meta-models largely capture broad SOC dynamics such as the linear nature of SOC responses to residue application, yet they often underestimate the magnitude of SOC responses to management. Furthermore, we find notable differences between the results from the biophysically-specific models throughout Europe, which point to spatially-distinct SOC responses to management choices such as nitrogen fertilizer application rates and residue retention that illustrate the potential for these models to be used for future management applications. While more accurate input data, calibration, and validation will be needed to accurately predict SOC change, we demonstrate the use of our meta-models for biophysical cluster and field study scale analyses of broad SOC dynamics with basically zero fine-tuning of the models needed. This work provides a framework for simplifying large-scale agricultural models and identifies the opportunities for using these meta-models for assessing SOC responses to management at a variety of scales.",
        "DOI": "10.1016/j.jenvman.2023.118532",
        "affiliation_name": "International Institute for Applied Systems Analysis, Laxenburg",
        "affiliation_city": "Laxenburg",
        "affiliation_country": "Austria"
    },
    {
        "paper_title": "Deforestation, certification, and transnational palm oil supply chains: Linking Guatemala to global consumer markets",
        "paper_author": "VanderWilde C.P.",
        "publication": "Journal of Environmental Management",
        "citied_by": "11",
        "cover_date": "2023-10-15",
        "Abstract": "Although causal links between tropical deforestation and palm oil are well established, linking this land use change to where the palm oil is actually consumed remains a distinct challenge and research gap. Supply chains are notoriously difficult to track back to their origin (i.e., the ‘first-mile’). This poses a conundrum for corporations and governments alike as they commit to deforestation-free sourcing and turn to instruments like certification to increase supply chain transparency and sustainability. The Roundtable on Sustainable Palm Oil (RSPO) offers the most influential certification system in the sector, but whether it actually reduces deforestation is still unclear. This study used remote sensing and spatial analysis to assess the deforestation (2009–2019) caused by oil palm plantation expansion in Guatemala, a major palm oil source for international consumer markets. Our results reveal that plantations are responsible for 28% of deforestation in the region and that more than 60% of these plantations encroach on Key Biodiversity Areas. RSPO-certified plantations, comprising 63% of the total cultivated area assessed, did not produce a statistically significant reduction in deforestation. Using trade statistics, the study linked this deforestation to the palm oil supply chains of three transnational conglomerates – Pepsico, Mondelēz International, and Grupo Bimbo – all of whom rely on RSPO-certified supplies. Addressing this deforestation and supply chain sustainability challenge hinges on three measures: 1) reform of RSPO policies and practices; 2) robust corporate tracking of supply chains; and 3) strengthening forest governance in Guatemala. This study offers a replicable methodology for a wide-range of investigations that seek to understand the translational linkages between environmental change (e.g. deforestation) and consumption.",
        "DOI": "10.1016/j.jenvman.2023.118505",
        "affiliation_name": "McGill University, Macdonald Campus",
        "affiliation_city": "Sainte-Anne-de-Bellevue",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Research on Port Throughput Prediction Based on Data Mining",
        "paper_author": "Fan Z.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-10-13",
        "Abstract": "This paper studies the prediction model of port throughput in China based on data mining, which has certain practical significance and theoretical value for guiding port planning, operation management and policy formulation. Based on the monthly throughput data of the top 40 ports in China from 2019 to 2022, time series clustering method is used to classify ports into four categories; For each category, the most representative ports are selected according to the distance from the centroid, and throughput prediction models are established using ARIMA, quadratic exponential smoothing, random forest, and BP neural network, and the prediction results of various ports are compared; Based on each group of representative ports, find the best prediction method and predict the throughput of the same type of port; To verify the effectiveness of the clustering and selecting representative ports to determine the prediction model used in this study, four models were used to predict all ports, and the root mean square error was calculated to evaluate the prediction effect. The results show that using different prediction models after classification is better than not classifying, and selecting representative ports based on clustering results to find the best prediction method is indeed the best prediction method for this type of performance.",
        "DOI": "10.1145/3644523.3644634",
        "affiliation_name": "Beijing Jiaotong University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Worldwide AI ethics: A review of 200 guidelines and recommendations for AI governance",
        "paper_author": "Corrêa N.K.",
        "publication": "Patterns",
        "citied_by": "36",
        "cover_date": "2023-10-13",
        "Abstract": "The utilization of artificial intelligence (AI) applications has experienced tremendous growth in recent years, bringing forth numerous benefits and conveniences. However, this expansion has also provoked ethical concerns, such as privacy breaches, algorithmic discrimination, security and reliability issues, transparency, and other unintended consequences. To determine whether a global consensus exists regarding the ethical principles that should govern AI applications and to contribute to the formation of future regulations, this paper conducts a meta-analysis of 200 governance policies and ethical guidelines for AI usage published by public bodies, academic institutions, private companies, and civil society organizations worldwide. We identified at least 17 resonating principles prevalent in the policies and guidelines of our dataset, released as an open source database and tool. We present the limitations of performing a global-scale analysis study paired with a critical analysis of our findings, presenting areas of consensus that should be incorporated into future regulatory efforts.",
        "DOI": "10.1016/j.patter.2023.100857",
        "affiliation_name": "Universität Bonn",
        "affiliation_city": "Bonn",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Mapping loneliness through social intelligence analysis: A step towards creating global loneliness map",
        "paper_author": "Shah H.A.",
        "publication": "BMJ Health and Care Informatics",
        "citied_by": "2",
        "cover_date": "2023-10-12",
        "Abstract": "Objectives Loneliness is a prevalent global public health concern with complex dynamics requiring further exploration. This study aims to enhance understanding of loneliness dynamics through building towards a global loneliness map using social intelligence analysis. Settings and design This paper presents a proof of concept for the global loneliness map, using data collected in October 2022. Twitter posts containing keywords such as € lonely', € loneliness', € alone', € solitude' and € isolation' were gathered, resulting in 841 796 tweets from the USA. City-specific data were extracted from these tweets to construct a loneliness map for the country. Sentiment analysis using the valence aware dictionary for sentiment reasoning tool was employed to differentiate metaphorical expressions from meaningful correlations between loneliness and socioeconomic and emotional factors. Measures and results The sentiment analysis encompassed the USA dataset and city-wise subsets, identifying negative sentiment tweets. Psychosocial linguistic features of these negative tweets were analysed to reveal significant connections between loneliness, socioeconomic aspects and emotional themes. Word clouds depicted topic variations between positively and negatively toned tweets. A frequency list of correlated topics within broader socioeconomic and emotional categories was generated from negative sentiment tweets. Additionally, a comprehensive table displayed top correlated topics for each city. Conclusions Leveraging social media data provide insights into the multifaceted nature of loneliness. Given its subjectivity, loneliness experiences exhibit variability. This study serves as a proof of concept for an extensive global loneliness map, holding implications for global public health strategies and policy development. Understanding loneliness dynamics on a larger scale can facilitate targeted interventions and support.",
        "DOI": "10.1136/bmjhci-2022-100728",
        "affiliation_name": "Hamad Bin Khalifa University, College of Science and Engineering",
        "affiliation_city": "Doha",
        "affiliation_country": "Qatar"
    },
    {
        "paper_title": "Characterization, typification and holistic consumer perception of welfare in laying poultry in Brazil: a machine learning approach",
        "paper_author": "Arno A.",
        "publication": "Journal of Agricultural Science",
        "citied_by": "2",
        "cover_date": "2023-10-11",
        "Abstract": "The intensification of production systems raises concerns about animal welfare. In egg production, the use of cages is the main reason for discussion. The current transition from the production system to cage-free systems raises questions about consumer perception. The objective of this study was to typify, characterize and differentiate the profile of Brazilian consumers regarding animal welfare in laying poultry. For this, a questionnaire with 28 questions, addressing questions about sociodemographic indicators (SOC), eating habits (HAB), knowledge about the production chain (CON), general perception of animal welfare in egg production (HPW) and about eggs with an animal welfare guarantee (PEAWG) was answered by 1415 consumers. Machine learning techniques were applied to characterize; typify and holistic perception. Three groups of consumers were defined: interested, emerging and indifferent. All indicators under study showed discriminatory power (P < 0.001). The indicators that showed the greatest importance for the classification of the three profiles were HPW < EHAB < COM < PEAWG < SOC. The results indicate the potential of interested and emerging groups to become consumers of cage-free eggs and also indicate the need to inform the population about animal welfare in egg production. The results reinforce the need to create specific public policies for the production chain, in order to value egg production and reaffirm interest in the area, especially in specific niches such as production in cage-free systems.",
        "DOI": "10.1017/S0021859623000552",
        "affiliation_name": "Universidade de São Paulo",
        "affiliation_city": "Sao Paulo",
        "affiliation_country": "Brazil"
    },
    {
        "paper_title": "The Role of Energy Consumption and Economic Growth on Carbon Emission: Application of Artificial Neural Network",
        "paper_author": "Sah H.K.",
        "publication": "International Journal of Energy Economics and Policy",
        "citied_by": "1",
        "cover_date": "2023-10-11",
        "Abstract": "This paper examines the influence of gross domestic product (GDP) and energy consumption (renewable energy and non-renewable energy) on carbon emissions in European Union (EU) Countries use of panel data from 2000 to 2020. By using Artificial Neural Network (ANN) machine learning computational technique, variables are categorized into input and output parameters. The result from the analyses shows that RMSE values of all variables are significant. Further, the normalized importance obtained from the multilayer perception ANN algorithm highlights the importance of variables and their association. The finding suggests that EU countries should adopt a clean energy strategy and policy for environmental protection without compromising economic growth.",
        "DOI": "10.32479/ijeep.14666",
        "affiliation_name": "Alliance University",
        "affiliation_city": "Bengaluru",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Knowledge Exploration: Teaching Cyber-Security Using Controlled Web-Based Laboratories",
        "paper_author": "Tapiawala K.S.",
        "publication": "SIGITE 2023 - Proceedings of the 24th Annual Conference on Information Technology Education",
        "citied_by": "0",
        "cover_date": "2023-10-11",
        "Abstract": "This project aims to provide a safe and controlled environment for students in a cyber-security class to practice website exploitation using multiple methods. The project proposes the utilization of BeEF-XSS and OWASP Juice Shop as tools to enable an educator to teach web-based exploitation while adhering to the security policies of an institution. The methods used are creating separate hands-on labs using BeEF-XSS and OWASP Juice Shop to provide an active learning environment and gain more comprehensive learning experience. The applications used in this project include Kali Linux virtual machine, BeEF-XSS software package, OWASP Juice Shop, and Burp Suite. The project also provides a set of environment requirements and setup instructions for the labs. Finally, the labs created are limited in difficulty, making them suitable for freshmen and sophomore students. Our work will provide a good reference for teaching web application security and cybersecurity in general.",
        "DOI": "10.1145/3585059.3611443",
        "affiliation_name": "Grand Valley State University",
        "affiliation_city": "Allendale",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Integrating remote sensing derived indices and machine learning algorithms for precise extraction of small surface water bodies in the lower Thoubal river watershed, India",
        "paper_author": "Hibjur Rahaman M.",
        "publication": "Journal of Cleaner Production",
        "citied_by": "22",
        "cover_date": "2023-10-10",
        "Abstract": "Surface water resources have witnessed various challenges namely shortage of water availability, deterioration in water quality and reduction of water bodies due to climate change and anthropogenic activities. Thus, extracting surface water bodies accurately and examining trend in their areal extent assumes greater significance. This study makes a novel attempt to utilize remote sensing derived indices and machine learning algorithms to extract and examine spatio-temporal changes in surface water bodies in the lower Thoubal river watershed. Spectral water indices were derived using Landsat data for the years 1989, 1997, 2005, 2013 and 2020 and were fed into machine learning algorithms for precise extraction of surface water bodies. The random forest model exhibited superior performance in terms of surface water extraction. The Mann-Kendall test was then used to examine the trend of surface water bodies during 1989–2020. The analysis revealed a significant declining trend in water extent over the past three decades. Future simulations using a Markov-Chain model also projected a decreasing trend in water bodies for the years 2030 and 2040 in both wet and dry months. The morphological pattern of the surface water bodies was assessed using morphological spatial pattern analysis (MSPA). It has also shown decrease in area under water bodies. These changes in the dynamics of surface water bodies may be attributed to human encroachment on wetlands for settlement and farming purposes, damming of streamflow, garbage dumping and climate change. Thus, a comprehensive approach encompassing water conservation, demand management, supply augmentation, legislation and long-term planning is essential for fostering sustainable water resource management. The findings of the study may serve as a valuable record for decision-making process and policy responses. We argue that the methodology applied in the study may help in protecting and conserving neglected small surface water bodies in other geographical regions.",
        "DOI": "10.1016/j.jclepro.2023.138563",
        "affiliation_name": "Jamia Millia Islamia",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Introducing an adaptive model for auto-scaling cloud computing based on workload classification",
        "paper_author": "Alidoost Alanagh Y.",
        "publication": "Concurrency and Computation: Practice and Experience",
        "citied_by": "2",
        "cover_date": "2023-10-10",
        "Abstract": "With the increasing expansion of cloud computing services, one of the main goals of researchers is to solve its major challenges. Cloud service providers must satisfy the service level agreement for customers and prevent resource wastage as much as possible. Without a precise, optimal, and dynamic policy, this is unattainable. The key idea is the ability to acquire resources as you need them and release resources when you no longer need them, named “Cloud Elasticity.” Elasticity is a trade-off between resource acquisition and release, and if this optimization is done best, the service level agreement will be fully achieved and the cloud provider will have the least waste of resources. The researchers used machine learning techniques to predict user workload and decide to scale up/out the resources. A challenging issue is the different characteristics of the users' workloads. The results show that each prediction algorithm works well on a class of users' workloads not all. Hence, in this study, a new architecture has been suggested to automatically classify the workloads based on their sequential statistical characteristics. First, the sequential statistical characteristics of the users' workload are extracted and then a trained neural network classifies the user's workload. The developed adaptive model chooses the best suitable algorithm among LR, SVM, and ARIMA to predict the workload. The results indicate a 10% improvement in forecast error.",
        "DOI": "10.1002/cpe.7720",
        "affiliation_name": "Qom University of Technology",
        "affiliation_city": "Qom",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "Using Artificial Intelligence for Predicting the Duration of Emergency Evacuation during Hospital Fire",
        "paper_author": "Sahebi A.",
        "publication": "Disaster Medicine and Public Health Preparedness",
        "citied_by": "8",
        "cover_date": "2023-10-10",
        "Abstract": "Objective: A danger threatening hospitals is fire. The most important action following a fire is to urgently evacuate the hospital during the shortest time possible. The aim of this study was to predict the duration of emergency evacuation following hospital fire using machine-learning algorithms. Methods: In this study, the real emergency evacuation duration of 190 patients admitted to a hospital was predicted in a simulation based on the following 8 factors: the number of hospital floors, patient preparation and transfer time, distance to the safe location, as well as patient's weight, age, sex, and movement capability. To design and validate the model, we used statistical models of machine learning, including Support Vector Machines Random Forest, Naive Bayes Classifier, and Artificial Neural Network. Results: Data analysis showed that based on the Area Under the Curve, precision, and sensitivity values of 99.5%, 92.4%, and 92.1%, respectively, the Random Forest model showed a better performance compared to other models for predicting the duration of hospital emergency evacuation during fire. Conclusion: Predicting evacuation duration can provide managers with accurate information and true analyses of these events. Therefore, health policy makers and managers can promote preparedness and responsiveness during fire by predicting evacuation duration and developing appropriate plans using machine learning models.",
        "DOI": "10.1017/dmp.2022.187",
        "affiliation_name": "Ilam University of Medical Sciences",
        "affiliation_city": "Ilam",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "QCMP: Load Balancing via In-Network Reinforcement Learning",
        "paper_author": "Zheng C.",
        "publication": "FIRA 2023 - Proceedings of the 2023 2nd ACM SIGCOMM Workshop on Future of Internet Routing and Addressing",
        "citied_by": "7",
        "cover_date": "2023-10-09",
        "Abstract": "Traffic load balancing is a long time networking challenge. The dynamism of traffic and the increasing number of different workloads that flow through the network exacerbate the problem. This work presents QCMP, a Reinforcement-Learning based load balancing solution. QCMP is implemented within the data plane, providing dynamic policy adjustment with quick response to changes in traffic. QCMP is implemented using P4 on a switch-ASIC and using BMv2 in a simulation environment. Our results show that QCMP requires negligible resources, runs at line rate, and adapts quickly to changes in traffic patterns.",
        "DOI": "10.1145/3607504.3609291",
        "affiliation_name": "University of Oxford",
        "affiliation_city": "Oxford",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Design of an energy efficient dynamic virtual machine consolidation model for smart cities in urban areas",
        "paper_author": "Biswas N.K.",
        "publication": "Intelligent Data Analysis",
        "citied_by": "1",
        "cover_date": "2023-10-06",
        "Abstract": "The growing smart cities in urban areas are becoming more intelligent day by day. Massive storage and high computational resources are required to provide smart services in urban areas. It can be provided through intelligence cloud computing. The establishment of large-scale cloud data centres is rapidly increasing to provide utility-based services in urban areas. Enormous energy consumption of data centres has a destructive effect on the environment. Due to the enormous energy consumption of data centres, a massive amount of greenhouse gases (GHG) are emitted into the environment. Virtual Machine (VM) consolidation can enable energy efficiency to reduce energy consumption of cloud data centres. The reduce energy consumption can increase the Service Level Agreement (SLA) violation. Therefore, in this research, an energy-efficient dynamic VM consolidation model has been proposed to reduce the energy consumption of cloud data centres and curb SLA violations. Novel algorithms have been proposed to accomplish the VM consolidation. A new status of any host called an almost overload host has been introduce, and determined by a novel algorithm based on the Naive Bayes Classifier Machine Learning (ML) model. A new algorithm based on the exponential binary search is proposed to perform the VM selection. Finally, a new Modified Power-Aware Best Fit Decreasing (MPABFD) VM allocation policy is proposed to allocate all VMs. The proposed model has been compared with certain well-known baseline algorithms. The comparison exhibits that the proposed model improves the energy consumption by 25% and SLA violation by 87%.",
        "DOI": "10.3233/IDA-220754",
        "affiliation_name": "Kalyani Government Engineering College",
        "affiliation_city": "Nadia",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Global Solar Radiation Forecasting with Artificial Neural Networks",
        "paper_author": "Sari M.",
        "publication": "Advances in Transdisciplinary Engineering",
        "citied_by": "1",
        "cover_date": "2023-10-05",
        "Abstract": "This study presents a detailed examination of using artificial neural networks for predicting global solar radiation. The research aims to develop an artificial neural network model using five years of solar radiation and meteorological variables (precipitation, wind speed, relative humidity, vapor pressure, cloudiness, current pressure, average temperature, number of sunny days, solar radiation, and daily average solar intensity) obtained from the central meteorological observation station of Kocaeli province between 2017 and 2021. The model aims to address the complexity of solar radiation as a phenomenon and the challenges associated with direct measurement. Artificial neural networks are considered an ideal tool for this purpose due to their ability to analyze complex data structures and identify relationships. The dataset used in this study includes detailed measurements of five years of solar radiation and meteorological variables collected from the meteorological observation station. These data encompass factors crucial for solar radiation prediction and provide information to enhance the accuracy of the model. The dataset is divided into training, validation, and testing phases, and relevant metrics are used to evaluate the performance of the artificial neural network model. The results demonstrate the successful prediction of global solar radiation by the developed artificial neural network model. The model undergoes a learning process to comprehend the complexity of solar radiation and make predictions by utilizing the relationships between various meteorological variables. This study emphasizes the importance of solar radiation prediction in areas such as solar energy projects, energy planning, and climate change research.",
        "DOI": "10.3233/ATDE230300",
        "affiliation_name": "Kocaeli Üniversitesi",
        "affiliation_city": "İzmit",
        "affiliation_country": "Turkey"
    },
    {
        "paper_title": "AI tools as science policy advisers? The potential and the pitfalls",
        "paper_author": "Tyler C.",
        "publication": "Nature",
        "citied_by": "14",
        "cover_date": "2023-10-05",
        "Abstract": "NA",
        "DOI": "10.1038/d41586-023-02999-3",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Applications of Natural Language Processing Tools in Orthopaedic Surgery: A Scoping Review",
        "paper_author": "Sasanelli F.",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "3",
        "cover_date": "2023-10-01",
        "Abstract": "The advent of many popular commercial forms of natural language processing tools has changed the way we can utilise digital technologies to tackle problems with big data. The objective of this review is to evaluate the current research and landscape of natural language processing tools and explore their potential use and impact in the field of orthopaedic surgery. In doing so, this review aims to answer the research question of how NLP tools can be utilised to streamline processes within orthopedic surgery. To do this, a scoping review was performed in accordance with the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) and Arksey and O’Malley framework for scoping reviews, as well as a computer-assisted literature search on the Medline, Embase and Google Scholar databases. Papers that evaluated the use of natural language processing tools in the field of orthopaedic surgery were included. Our literature search identified 24 studies that were eligible for inclusion. Our scoping review captured articles that highlighted multiple uses of NLP tools in orthopaedics. In particular, one study reported on the use of NLP for intraoperative monitoring, six for detection of adverse events, five for establishing orthopaedic diagnoses, two for assessing the patient experience, two as an informative resource for patients, one for predicting readmission, one for triaging, five for auditing and one for billing and coding. All studies assessed these various uses of NLP through its tremendous computational ability in extracting structured and unstructured text from the medical record, including operative notes, pathology and imaging reports, and progress notes, for use in orthopaedic surgery. Our review demonstrates that natural language processing tools are becoming increasingly studied for use and integration within various processes of orthopaedic surgery. These AI tools offer tremendous promise in improving efficiency, auditing and streamlining tasks through their immense computational ability and versatility. Despite this, further research to optimise and adapt these tools within the clinical environment, as well as the development of evidence-based policies, guidelines and frameworks are required before their wider integration within orthopaedics can be considered.",
        "DOI": "10.3390/app132011586",
        "affiliation_name": "Melbourne Medical School",
        "affiliation_city": "Melbourne",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Machine learning modeling for identifying predictors of unmet need for family planning among married/in-union women in Ethiopia: Evidence from performance monitoring and accountability (PMA) survey 2019 dataset",
        "paper_author": "Kebede S.D.",
        "publication": "PLOS Digital Health",
        "citied_by": "3",
        "cover_date": "2023-10-01",
        "Abstract": "Unmet need for contraceptives is a public health issue globally that affects maternal and child health. Reducing unmet need reduces the risk of abortion or childbearing by preventing unintended pregnancy. The unmet need for family planning is a frequently used indicator for monitoring family planning programs. This study aimed to identify predictors of unmet need for family planning using advanced machine learning modeling on recent PMA 2019 survey data. The study was conducted using secondary data from PMA Ethiopia 2019 cross-sectional household and female survey which was carried out from September 2019 to December 2019. Eight machine learning classifiers were employed on a total weighted sample of 5819 women and evaluated using performance metrics to predict and identify important predictors of unmet need of family planning with Python 3.10 version software. Data preparation techniques such as removing outliers, handling missing values, handling unbalanced categories, feature engineering, and data splitting were applied to smooth the data for further analysis. Finally, Shapley Additive exPlanations (SHAP) analysis was used to identify the top predictors of unmet need and explain the contribution of the predictors on the model's output. Random Forest was the best predictive model with a performance of 85% accuracy and 0.93 area under the curve on balanced training data through tenfold crossvalidation. The SHAP analysis based on random forest model revealed that husband/partner disapproval to use family planning, number of household members, women education being primary, being from Amhara region, and previously delivered in health facility were the top important predictors of unmet need for family planning in Ethiopia. Findings from this study suggest various sociocultural and economic factors might be considered while implementing health policies intended to decrease unmet needs for family planning in Ethiopia. In particular, the husband's/partner's involvement in family planning sessions should be emphasized as it has a significant impact on women's demand for contraceptives.",
        "DOI": "10.1371/journal.pdig.0000345",
        "affiliation_name": "Wollo University",
        "affiliation_city": "Dessie",
        "affiliation_country": "Ethiopia"
    },
    {
        "paper_title": "Unveiling the Food and Income Insecurity among Farm Households of Lucknow, Uttar Pradesh",
        "paper_author": "Ravi S.C.",
        "publication": "Indian Journal of Extension Education",
        "citied_by": "0",
        "cover_date": "2023-10-01",
        "Abstract": "The study was conducted during 2022-23 to assess the level of food insecurity and income status among farm households. Data from 474 farmers through personal interview method were collected. Agriculture was the primary occupation for most households followed by off-farm activities. Average per capita annual income (Rs. 1,00,073) was lower than the national average. The per capita annual income was Rs. 73,303, Rs. 93,256 and Rs. 1,44,456 for marginal, small, and medium farmers, respectively. About 47 per cent of the expenditure was made on consumption. A comparison of calorie intake to recommended calorie intake indicated that food insecurity was prevailing among 26 percent of the farmers. The major contribution to calorie intake was from cereals, the consumption of vegetables and fruits was low. A decision tree model using machine learning algorithms was used to identify the factors influencing food security. Per capita income, family size, consumption expenditure, social participation, and land holdings had significant importance in classifying the households as food secure and insecure. Diversifying farm activities and creating additional opportunities in rural areas, teaching households about balanced diets, promoting home gardening, and institutional policies to improve food security may be the strategic points.",
        "DOI": "10.48165/IJEE.2023.59420",
        "affiliation_name": "ICAR - Central Institute for Subtropical Horticulture, Lucknow",
        "affiliation_city": "Lucknow",
        "affiliation_country": "India"
    },
    {
        "paper_title": "AB-Gen: Antibody Library Design with Generative Pre-trained Transformer and Deep Reinforcement Learning",
        "paper_author": "Xu X.",
        "publication": "Genomics, Proteomics and Bioinformatics",
        "citied_by": "9",
        "cover_date": "2023-10-01",
        "Abstract": "Antibody leads must fulfill multiple desirable properties to be clinical candidates. Primarily due to the low throughput in the experimental procedure, the need for such multi-property optimization causes the bottleneck in preclinical antibody discovery and development, because addressing one issue usually causes another. We developed a reinforcement learning (RL) method, named AB-Gen, for antibody library design using a generative pre-trained transformer (GPT) as the policy network of the RL agent. We showed that this model can learn the antibody space of heavy chain complementarity determining region 3 (CDRH3) and generate sequences with similar property distributions. Besides, when using human epidermal growth factor receptor-2 (HER2) as the target, the agent model of AB-Gen was able to generate novel CDRH3 sequences that fulfill multi-property constraints. Totally, 509 generated sequences were able to pass all property filters, and three highly conserved residues were identified. The importance of these residues was further demonstrated by molecular dynamics simulations, consolidating that the agent model was capable of grasping important information in this complex optimization task. Overall, the AB-Gen method is able to design novel antibody sequences with an improved success rate than the traditional propose-then-filter approach. It has the potential to be used in practical antibody design, thus empowering the antibody discovery and development process. The source code of AB-Gen is freely available at Zenodo (https://doi.org/10.5281/zenodo.7657016) and BioCode (https://ngdc.cncb.ac.cn/biocode/tools/BT007341).",
        "DOI": "10.1016/j.gpb.2023.03.004",
        "affiliation_name": "King Abdullah University of Science and Technology",
        "affiliation_city": "Thuwal",
        "affiliation_country": "Saudi Arabia"
    },
    {
        "paper_title": "Twitter Propaganda Operations: Analyzing Sociopolitical Issues in Saudi Arabia",
        "paper_author": "Albert C.D.",
        "publication": "Social Media and Society",
        "citied_by": "1",
        "cover_date": "2023-10-01",
        "Abstract": "The purpose of this article is to explore Arabic-language Tweets based out of Saudi Arabia to investigate the social media landscape. Specifically, this article seeks to address the question, “What thematic issues concerning the U.S. socio-political landscape are present in Arabic-language Twitter postings?” And, “To what extent can these issues be described as propagandic in nature?” To do so, we propose a machine-learning and artificial intelligence span detection approach to identify propaganda Tweets in Middle Eastern Countries, with a focus on Saudi Arabia. As opposed to previous work, this article maps and investigates different propaganda categories using the BEND Social Cyber Security framework. This article then proceeds to a case study analysis of state-sponsored targeted propaganda from Saudi Arabia and briefly describes the categories of propaganda uncovered. We then relate those categories to the BEND Framework and conclude with policy recommendations and discussion.",
        "DOI": "10.1177/20563051231216964",
        "affiliation_name": "Augusta University",
        "affiliation_city": "Augusta",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Selected Topics From the State of the Science in Transfusion Medicine: Key Insights on Current Progress and Future Directions",
        "paper_author": "Roubinian N.H.",
        "publication": "Transfusion Medicine Reviews",
        "citied_by": "0",
        "cover_date": "2023-10-01",
        "Abstract": "NA",
        "DOI": "10.1016/j.tmrv.2023.150781",
        "affiliation_name": "University of California, San Francisco",
        "affiliation_city": "San Francisco",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "DISCRETE UNCERTAINTY QUANTIFICATION FOR OFFLINE REINFORCEMENT LEARNING",
        "paper_author": "Pérez J.L.",
        "publication": "Journal of Artificial Intelligence and Soft Computing Research",
        "citied_by": "1",
        "cover_date": "2023-10-01",
        "Abstract": "In many Reinforcement Learning (RL) tasks, the classical online interaction of the learning agent with the environment is impractical, either because such interaction is expensive or dangerous. In these cases, previous gathered data can be used, arising what is typically called Offline RL. However, this type of learning faces a large number of challenges, mostly derived from the fact that exploration/exploitation trade-off is overshadowed. In addition, the historical data is usually biased by the way it was obtained, typically, a sub-optimal controller, producing a distributional shift from historical data and the one required to learn the optimal policy. In this paper, we present a novel approach to deal with the uncertainty risen by the absence or sparse presence of some state-action pairs in the learning data. Our approach is based on shaping the reward perceived from the environment to ensure the task is solved. We present the approach and show that combining it with classic online RL methods make them perform as good as state of the art Offline RL algorithms such as CQL and BCQ. Finally, we show that using our method on top of established offline learning algorithms can improve them.",
        "DOI": "10.2478/jaiscr-2023-0019",
        "affiliation_name": "Universidade de Santiago de Compostela",
        "affiliation_city": "Santiago de Compostela",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "Random Forest Regression in Predicting Students’ Achievements and Fuzzy Grades",
        "paper_author": "Doz D.",
        "publication": "Mathematics",
        "citied_by": "6",
        "cover_date": "2023-10-01",
        "Abstract": "The use of fuzzy logic to assess students’ knowledge is not a completely new concept. However, despite dealing with a large quantity of data, traditional statistical methods have typically been the preferred approach. Many studies have argued that machine learning methods could offer a viable alternative for analyzing big data. Therefore, this study presents findings from a Random Forest (RF) regression analysis to understand the influence of demographic factors on students’ achievements, i.e., teacher-given grades, students’ outcomes on the national assessment, and fuzzy grades, which were obtained as a combination of the two. RF analysis showed that demographic factors have limited predictive power for teacher-assigned grades, unlike INVALSI scores and fuzzy grades. School type, macroregion, and ESCS are influential predictors, whereas gender and origin have a lesser impact. The study highlights regional and socio-economic disparities, influencing both student outcomes and fuzzy grades, underscoring the need for equitable education. Unexpectedly, gender’s impact on achievements is minor, possibly due to gender-focused policies. Although the study acknowledges limitations, its integration of fuzzy logic and machine learning sets the foundation for future research and policy recommendations, advocating for diversified assessment approaches and data-driven policymaking.",
        "DOI": "10.3390/math11194129",
        "affiliation_name": "University of Primorska",
        "affiliation_city": "Koper",
        "affiliation_country": "Slovenia"
    },
    {
        "paper_title": "Copper price prediction using LSTM recurrent neural network integrated simulated annealing algorithm",
        "paper_author": "Chen J.",
        "publication": "PLoS ONE",
        "citied_by": "0",
        "cover_date": "2023-10-01",
        "Abstract": "Copper is an important mineral and fluctuations in copper prices can affect the stable functioning of some countries’ economies. Policy makers, futures traders and individual investors are very concerned about copper prices. In a recent paper, we use an artificial intelligence model long short-term memory (LSTM) to predict copper prices. To improve the efficiency of long short-term memory (LSTM) model, we introduced a simulated annealing (SA) algorithm to find the best combination of hyperparameters. The feature engineering problem of the AI model is then solved by correlation analysis. Three economic indicators, West Texas Intermediate Oil Price, Gold Price and Silver Price, which are highly correlated with copper prices, were selected as inputs to be used in the training and forecasting model. Three different copper price time periods, namely 485, 363 and 242 days, were chosen for the model forecasts. The forecast errors are 0.00195, 0.0019 and 0.00097, respectively. Compared with the existing literature, the prediction results of this paper are more accurate and less error. The research in this paper provides a reliable reference for analyzing future copper price changes.",
        "DOI": "10.1371/journal.pone.0285631",
        "affiliation_name": "Hubei University Of Economics",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Waste Management in the Smart City: Current Practices and Future Directions",
        "paper_author": "Szpilko D.",
        "publication": "Resources",
        "citied_by": "34",
        "cover_date": "2023-10-01",
        "Abstract": "The discourse surrounding sustainability, particularly in the urban environment, has gained considerable momentum in recent years. The concept of a smart city epitomises the integration of innovative technological solutions with community-centred approaches, thereby laying the groundwork for a sustainable lifestyle. One of the crucial components of this integration is the effective and innovative management of waste. The aim of this article was to classify scientific research pertaining to waste management within the context of smart city issues, and to identify emerging directions for future research. A systematic literature review, based on a bibliometric analysis of articles included in the Scopus and Web of Science databases, was conducted for this study. The purpose of such a systematic review is to identify, integrate, and evaluate research on a selected topic, using clearly defined criteria. The research query included: TITLE-ABS-KEY (“smart city” AND (waste OR garbage OR trash OR rubbish)) in the case of Scopus, and TS = (“smart city” AND (waste OR garbage OR trash OR rubbish)) in the case of the Web of Science database. A total of 1768 publication records qualified for the analysis. This study presents an investigation into the current and forthcoming directions of waste management in smart cities, synthesising the latest advancements and methods. The findings outline specific future research directions encompassing technological advancement, special waste challenges, digitisation, energy recovery, transportation, community engagement, policy development, security, novel frameworks, economic and environmental impact assessment, and global implications. These insights reflect a multifaceted approach, advocating a technology-driven perspective that is integral to urban sustainability and quality of life. The study’s findings provide practical avenues for cities to enhance waste management through modern technologies, promoting efficient systems and contributing to sustainable urban living and the circular economy. The insights are vital for policymakers and industry leaders globally, supporting the creation of universal standards and policies, thereby fostering comprehensive waste management systems aligned with global sustainability objectives.",
        "DOI": "10.3390/resources12100115",
        "affiliation_name": "Universidad de Sevilla",
        "affiliation_city": "Sevilla",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "An Investigation of the Predictability of Uncertainty Indices on Bitcoin Returns",
        "paper_author": "Wang J.",
        "publication": "Journal of Risk and Financial Management",
        "citied_by": "2",
        "cover_date": "2023-10-01",
        "Abstract": "Policymakers and portfolio managers pay keen attention to sources of uncertainties that drive asset returns and volatility. The influence of uncertainty on Bitcoin has the potential to drive fluctuations in the entire cryptocurrency market. We investigate the predictability of thirteen economic policy uncertainty indices on Bitcoin returns. Using the Random Forest machine learning algorithm, we find that Singapore’s economic policy uncertainty (EPU) has the strongest predictive power on Bitcoin returns, followed by financial crisis (FC) uncertainty and world trade uncertainty (WTU). We further categorize these uncertainties into different groups. Interestingly, the predictability of uncertainty indices on Bitcoin returns within the international trade group is stronger compared to other uncertainty categories. Additionally, we observed that internet-based uncertainty measures have more predictive power of Bitcoin returns than newspaper- and report-based measures. These results are robust using various additional machine learning methods. We believe that these findings could be valuable for policymakers and portfolio managers when making decisions related to uncertainty drivers of cryptocurrency prices and returns.",
        "DOI": "10.3390/jrfm16100461",
        "affiliation_name": "Cameron School of Business",
        "affiliation_city": "Wilmington",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Prediction of PM<inf>2.5</inf> Concentration Using Spatiotemporal Data with Machine Learning Models",
        "paper_author": "Ma X.",
        "publication": "Atmosphere",
        "citied_by": "5",
        "cover_date": "2023-10-01",
        "Abstract": "Among the critical global crises curbing world development and sustainability, air quality degradation has been a long-lasting and increasingly urgent one and it has been sufficiently proven to pose severe threats to human health and social welfare. A higher level of model prediction accuracy can play a fundamental role in air quality assessment and enhancing human well-being. In this paper, four types of machine learning models—random forest model, ridge regression model, support vector machine model, extremely randomized trees model—were adopted to predict PM2.5 concentration in ten cities in the Jing-Jin-Ji region of north China based on multi-sources spatiotemporal data including air quality and meteorological data in time series. Data were fed into the model by using the rolling prediction method which is proven to improve prediction accuracy in our experiments. Lastly, the comparative experiments show that at the city level, RF and ExtraTrees models have better predictive results with lower mean absolute error (MAE), root mean square error (RMSE), and higher index of agreement (IA) compared to other selected models. For seasonality, level four models all have the best prediction performances in winter time and the worst in summer time, and RF models have the best prediction performance with the IA ranging from 0.93 to 0.98 with an MAE of 5.91 to 11.68 μg/m3. Consequently, the demonstration of how each model performs differently in each city and each season is expected to shed light on environmental policy implications.",
        "DOI": "10.3390/atmos14101517",
        "affiliation_name": "North China University of Water Resources and Electric Power",
        "affiliation_city": "Zhengzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A novel non-ferrous metal price hybrid forecasting model based on data preprocessing and error correction",
        "paper_author": "He Z.",
        "publication": "Resources Policy",
        "citied_by": "8",
        "cover_date": "2023-10-01",
        "Abstract": "Accurately forecasting the price of non-ferrous metals is of great significance for traders to avoid risks, enterprises to arrange production plans, and countries to formulate economic policies. In order to improve the forecasting accuracy of non-ferrous metal prices, this paper proposes a novel non-ferrous metal price hybrid forecasting model named IVWAIEE (IVMD-WPD-ARIMA-IELM-ECD). Firstly, the original price series is decomposed into several smoother IMFs using variational mode decomposition (VMD). Simultaneously, the improved sparrow search algorithm (IFASSA) is used to optimize the parameters of VMD to improve the adaptability of VMD. Secondly, wavelet packet decomposition (WPD) is used to decompose the residual sequence generated by VMD to further extract the information in the residual sequence. Then, the components generated by VMD and WPD are defined as high frequency components and low frequency components according to the zero-crossing rate. ARIMA is used to forecast the low frequency components with gentle fluctuations, and extreme learning machine optimized by IFASSA (IELM) is used to forecast the high frequency components with strong fluctuations. The forecasting results of each component are accumulated to obtain the initial forecasting results and error sequence of the non-ferrous metal price. Next, WPD is used to further decompose the error sequence, and the error subsequence is predicted by ARIMA and IELM to obtain the error prediction results. Finally, the error prediction results are used to correct the initial forecasting results, and the final forecasting results of non-ferrous metal prices are obtained. In order to verify the superiority of the proposed model, the copper, aluminum, and zinc futures prices of the London Metal Exchange (LME) are selected as empirical data to verify the model. The results show that the proposed IVWAIEE model has better prediction accuracy and robustness than other benchmark models. Its RMSE values in predicting copper, aluminum, and zinc futures prices are 0.2238, 0.1863, and 0.2137, respectively, and MAE values are 0.1696, 0.1171, and 0.1644, respectively, which are lower than those of other benchmark models; The proposed model not only enriches the application of secondary decomposition and error correction in the field of non-ferrous metal price forecasting, but also solves the problems of insufficient adaptability and underutilization of residual sequence in the traditional variational mode decomposition method; The research results of this paper can provide scientific and effective guidance for the investment, production, and decision-making of non-ferrous metal stakeholders.",
        "DOI": "10.1016/j.resourpol.2023.104189",
        "affiliation_name": "Fuzhou University",
        "affiliation_city": "Fuzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Identifying, Understanding, and Addressing Disparities in Glaucoma Care in the United States",
        "paper_author": "Davuluru S.S.",
        "publication": "Translational Vision Science and Technology",
        "citied_by": "11",
        "cover_date": "2023-10-01",
        "Abstract": "Glaucoma is the leading cause of irreversible blindness worldwide, currently affecting around 80 million people. Glaucoma prevalence is rapidly rising in the United States due to an aging population. Despite recent advances in the diagnosis and treatment of glaucoma, significant disparities persist in disease detection, management, and outcomes among the diverse patient populations of the United States. Research on disparities is critical to identifying, understanding, and addressing societal and healthcare inequalities. Disparities research is especially important and impactful in the context of irreversible diseases such as glaucoma, where earlier detection and intervention are the primary approach to improving patient outcomes. In this article, we first review recent studies identifying disparities in glaucoma care that affect patient populations based on race, age, and gender. We then review studies elucidating and furthering our understanding of modifiable factors that contribute to these inequities, including socioeconomic status (particularly age and education), insurance product, and geographic region. Finally, we present work proposing potential strategies addressing disparities in glaucoma care, including teleophthalmology and artificial intelligence. We also discuss the presence of non-modifiable factors that contribute to differences in glaucoma burden and can confound the detection of glaucoma disparities. Translational Relevance: By recognizing underlying causes and proposing potential solutions, healthcare providers, policymakers, and other stakeholders can work collab-oratively to reduce the burden of glaucoma and improve visual health and clinical outcomes in vulnerable patient populations.",
        "DOI": "10.1167/tvst.12.10.18",
        "affiliation_name": "Keck School of Medicine of USC",
        "affiliation_city": "Los Angeles",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Fine Resolution Mapping of Soil Organic Carbon in Croplands with Feature Selection and Machine Learning in Northeast Plain China",
        "paper_author": "Zhang X.",
        "publication": "Remote Sensing",
        "citied_by": "2",
        "cover_date": "2023-10-01",
        "Abstract": "Unsustainable human management has negative effects on cropland soil organic carbon (SOC), causing a decrease in soil health and the emission of greenhouse gas. Due to contiguous fields, large-scale mechanized operations are widely used in the Northeast China Plain, which greatly improves production efficiency while decreasing the soil quality, especially for SOC. Therefore, an up-to-date SOC map is needed to estimate soil health after long-term cultivation to inform better land management. Using Quantile Regression Forest, a total of 396 soil samples from 132 sampling sites at three soil depth intervals and 40 environmental covariates (e.g., Landsat 8 spectral indices, and WorldClim 2 and MODIS products) selected by the Boruta feature selection algorithm were used to map the spatial distribution of SOC in the cropland of the Northeast Plain at a 90 m spatial resolution. The results showed that SOC increased overall from the southern area to the northern area, with an average of 17.34 g kg−1 in the plough layer (PL) and 13.92 g kg−1 in the compacted layer (CL). At the vertical scale, SOC decreased, with depths getting deeper. The average decrease in SOC from PL to CL was 3.41 g kg−1. Climate (i.e., average temperature, daytime and nighttime land surface temperature, and mean temperature of driest quarter) was the dominant controlling factor, followed by position (i.e., oblique geographic coordinate at 105°), and organism (i.e., the average and variance of net primary productivity in the non-crop period). The average uncertainty was 1.04 in the PL and 1.07 in the CL. The high uncertainty appeared in the area with relatively scattered fields, high altitudes, and complex landforms. This study updated the 90 m resolution cropland SOC maps at spatial and vertical scales, which clarifies the influence of mechanized operations and provides a reference for soil conservation policy-making.",
        "DOI": "10.3390/rs15205033",
        "affiliation_name": "College of Environmental and Resource Sciences",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A New Multi-Objective Genetic Programming Model for Meteorological Drought Forecasting",
        "paper_author": "Reihanifar M.",
        "publication": "Water (Switzerland)",
        "citied_by": "10",
        "cover_date": "2023-10-01",
        "Abstract": "Drought forecasting is a vital task for sustainable development and water resource management. Emerging machine learning techniques could be used to develop precise drought forecasting models. However, they need to be explicit and simple enough to secure their implementation in practice. This article introduces a novel explicit model, called multi-objective multi-gene genetic programming (MOMGGP), for meteorological drought forecasting that addresses both the accuracy and simplicity of the model applied. The proposed model considers two objective functions: (i) root mean square error and (ii) expressional complexity during its evolution. While the former is used to increase the model accuracy at the training phase, the latter is assigned to decrease the model complexity and achieve parsimony conditions. The model evolution and verification procedure were demonstrated using the standardized precipitation index obtained for Burdur City, Turkey. The comparison with benchmark genetic programming (GP) and multi-gene genetic programming (MGGP) models showed that MOMGGP provides the same forecasting accuracy with more parsimony conditions. Thus, it is suggested to utilize the model for practical meteorological drought forecasting.",
        "DOI": "10.3390/w15203602",
        "affiliation_name": "Department of Civil and Environmental Engineering",
        "affiliation_city": "Berkeley",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Practical Attacks of Round-Reduced SIMON Based on Deep Learning",
        "paper_author": "Hou Z.",
        "publication": "Computer Journal",
        "citied_by": "1",
        "cover_date": "2023-10-01",
        "Abstract": "At CRYPTO'19, Gohr built a bridge between deep learning and cryptanalysis. Based on deep neural networks, he trained neural distinguishers of SPECK32/64. Besides, with the help of neural distinguishers, he attacked 11-round SPECK32/64 using Bayesian optimization. Compared with the traditional attack, its complexity was reduced. Although his work opened a new direction of machine learning aided cryptanalysis, there are still two research gaps that researchers are eager to fill in. (i) Can the attack using neural distinguishers be used to other block ciphers? (ii) Are there effective key recovery attacks on large-size block ciphers adopting neural distinguishers? In this paper, our core target is to propose an effective neural-aided key recovery policy to attack large-size block ciphers. For large-size block ciphers, it costs too much time in pre-computation, especially in wrong key response profile, which is the main reason why there are almost no neural aided attacks on large-size block ciphers. Fortunately, we find that there is a fatal flaw in the wrong key profile. In the some experiments of SIMON32/64 and SIMON48/96, there is a regular of change in response profiles, which implies that we can use partial response instead of the complete response. Based on this, we propose a generic key recovery attack scheme which can attack large-size block ciphers. As an application, we perform a key recovery attack on 13-round SIMON64/128, which is the first practical attack using neural distinguishers to large-size ciphers. In addition, we also attack 13-round SIMON32/64 and SIMON48/96, which also shows that the neural distinguishers can be used to other block ciphers.",
        "DOI": "10.1093/comjnl/bxac102",
        "affiliation_name": "Information Engineering University China",
        "affiliation_city": "Zhengzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Mid- and End-of-the-Century Estimation of Agricultural Suitability of California’s Specialty Crops",
        "paper_author": "Granco G.",
        "publication": "Land",
        "citied_by": "0",
        "cover_date": "2023-10-01",
        "Abstract": "Specialty crops with long economic life cycles have lower adaptability and flexibility to climate change, making long-term planning crucial. This study examines the impact of climate change on almond, citrus, pistachio, and walnut production in California, using a machine learning approach to estimate crop suitability under current and future environmental conditions. We used recent satellite-observed cropland data to generate an occurrence dataset for these crops. Ecological data including bioclimatic variables derived from global circulation models developed under the Coupled Model Intercomparison Project Phase 6 (CMIP6) and surface variables were used to model suitability. The bioclimatic variables relating to temperature and precipitation had the largest effect on each crop’s suitability estimation. The results indicate that suitable areas for almonds, citrus, and walnuts will change significantly within 20 years due to climatic change, and the change will be even greater by the end of the century, indicating a potential loss of 94% of the current suitable area. The results for pistachios indicate change in the spatial distribution of suitable area but the total area is predicted to remain near the current suitable area. Policymakers, researchers, and farmers must work together to develop proactive adaptation strategies to mitigate the negative effects of climate change on specialty crop production. The application of a species distribution model for agriculture suitability provides critical information for future work on adaptation to climate change, identifying areas to target for further analysis.",
        "DOI": "10.3390/land12101907",
        "affiliation_name": "California State Polytechnic University, Pomona",
        "affiliation_city": "Pomona",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "AIMS: An Automatic Semantic Machine Learning Microservice Framework to Support Biomedical and Bioengineering Research",
        "paper_author": "Yu H.Q.",
        "publication": "Bioengineering",
        "citied_by": "0",
        "cover_date": "2023-10-01",
        "Abstract": "The fusion of machine learning and biomedical research offers novel ways to understand, diagnose, and treat various health conditions. However, the complexities of biomedical data, coupled with the intricate process of developing and deploying machine learning solutions, often pose significant challenges to researchers in these fields. Our pivotal achievement in this research is the introduction of the Automatic Semantic Machine Learning Microservice (AIMS) framework. AIMS addresses these challenges by automating various stages of the machine learning pipeline, with a particular emphasis on the ontology of machine learning services tailored to the biomedical domain. This ontology encompasses everything from task representation, service modeling, and knowledge acquisition to knowledge reasoning and the establishment of a self-supervised learning policy. Our framework has been crafted to prioritize model interpretability, integrate domain knowledge effortlessly, and handle biomedical data with efficiency. Additionally, AIMS boasts a distinctive feature: it leverages self-supervised knowledge learning through reinforcement learning techniques, paired with an ontology-based policy recording schema. This enables it to autonomously generate, fine-tune, and continually adapt to machine learning models, especially when faced with new tasks and data. Our work has two standout contributions demonstrating that machine learning processes in the biomedical domain can be automated, while integrating a rich domain knowledge base and providing a way for machines to have self-learning ability, ensuring they handle new tasks effectively. To showcase AIMS in action, we have highlighted its prowess in three case studies of biomedical tasks. These examples emphasize how our framework can simplify research routines, uplift the caliber of scientific exploration, and set the stage for notable advances.",
        "DOI": "10.3390/bioengineering10101134",
        "affiliation_name": "University of Derby",
        "affiliation_city": "Derby",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Optimization of Task-Scheduling Strategy in Edge Kubernetes Clusters Based on Deep Reinforcement Learning",
        "paper_author": "Wang X.",
        "publication": "Mathematics",
        "citied_by": "3",
        "cover_date": "2023-10-01",
        "Abstract": "Kubernetes, known for its versatility in infrastructure management, rapid scalability, and ease of deployment, makes it an excellent platform for edge computing. However, its native scheduling algorithm struggles with load balancing, especially during peak task deployment in edge environments characterized by resource limitations and low latency demands. To address this issue, a proximal policy optimization with the least response time (PPO-LRT) algorithm was proposed in this paper. This deep reinforcement learning approach learns the pod-scheduling process, which can adaptively schedule edge tasks to the most suitable worker nodes with the shortest response time according to the current cluster load and pod state. To evaluate the effectiveness of the proposed algorithm, multiple virtual machines were created, and we built a heterogeneous node cluster. Additionally, we deployed k3s, a Kubernetes distribution suited for edge environments, on the cluster. The load balancing, high load resilience, and average response time during peak task deployment were tested by initiating numerous tasks within a limited time frame. The results validate that the PPO-LRT-based scheduler shows superior performance in cluster load balancing compared to the Kube scheduler. After the deployment of 500 random tasks, several cluster nodes become overwhelmed by using the Kube scheduler, whereas the PPO-LRT-based scheduler evenly allocates the workload across the cluster, reducing the average response time by approximately 31%.",
        "DOI": "10.3390/math11204269",
        "affiliation_name": "Hunan University of Technology",
        "affiliation_city": "Zhuzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Land Use and Land Cover (LULC) Assessment within the Batanes Protected Landscapes and Seascapes",
        "paper_author": "Doyog N.D.",
        "publication": "Philippine Journal of Science",
        "citied_by": "0",
        "cover_date": "2023-10-01",
        "Abstract": "Declared protected areas have ecologically important landscapes that must be conserved and protected. Status of protected areas could be monitored through land use and land cover (LULC) assessments. LULC offers baseline data for integrated land use planning and improvement of existing policies are therefore necessary to be conducted. This study was conducted to monitor the existing LULC of six islands within the Batanes Protected Landscapes and Seascapes (BPLS) through a machine learning (ML)-based random forest (RF) classifier using multi-sourced data such as Landsat imageries’ surface reflectance (SR), Landsat-derived land surface temperature (LST), and global ecosystem dynamic investigation (GEDI)-derived height (Ht) metrics and to determine the effects of the LST and Ht metrics to LULC classification. Four layer stacked images with different features were analyzed – including SR, SR-LST, SR-Ht, and SR-LST-Ht. The result of the LULC classification showed an accuracy based on Macro F1-score and Kappa (K) of 0.81 and 0.83, 0.83 and 0.86, 0.86 and 0.89, and 0.93 and 0.94, for SR, SR-LST, SR-Ht, and SR-LST-Ht, respectively. When compared to the existing global-scale LULC, this study has higher accuracy than the GLAD and ESRI products, which have Macro F1-scores and K-values of 0.73 and 0.71, and 0.59 and 0.64, respectively. To conclude, the inclusion of LST and Ht information in addition to SR data in LULC classification can improve the accuracy by up to 12% and 11% based on Macro F1-score and K, respectively. The result of this study can serve as a reference for achieving improved and reliable LULC information that is necessary for monitoring fluctuations of the global earth’s resources and comprehensive LULC planning. In addition, the technique used in this study can serve as a reference in generating reliable LULC information that can aid in the sustainable implementation of policies, rules, and regulations intended for declared protected areas like BPLS.",
        "DOI": "10.56899/152.05.18",
        "affiliation_name": "University of the Philippines Baguio",
        "affiliation_city": "Baguio City",
        "affiliation_country": "Philippines"
    },
    {
        "paper_title": "Machine Learning Solutions for Offshore Wind Farms: A Review of Applications and Impacts",
        "paper_author": "Masoumi M.",
        "publication": "Journal of Marine Science and Engineering",
        "citied_by": "11",
        "cover_date": "2023-10-01",
        "Abstract": "The continuous advancement within the offshore wind energy industry is propelled by the imperatives of renewable energy generation, climate change policies, and the zero-emission targets established by governments and communities. Increasing the dimensions of offshore wind turbines to augment energy production, enhancing the power generation efficiency of existing systems, mitigating the environmental impacts of these installations, venturing into deeper waters for turbine deployment in regions with optimal wind conditions, and the drive to develop floating offshore turbines stand out as significant challenges in the domains of development, installation, operation, and maintenance of these systems. This work specifically centers on providing a comprehensive review of the research undertaken to tackle several of these challenges using machine learning and artificial intelligence. These machine learning-based techniques have been effectively applied to structural health monitoring and maintenance, facilitating the more accurate identification of potential failures and enabling the implementation of precision maintenance strategies. Furthermore, machine learning has played a pivotal role in optimizing wind farm layouts, improving power production forecasting, and mitigating wake effects, thereby leading to heightened energy generation efficiency. Additionally, the integration of machine learning-driven control systems has showcased considerable potential for enhancing the operational strategies of offshore wind farms, thereby augmenting their overall performance and energy output. Climatic data prediction and environmental studies have also benefited from the predictive capabilities of machine learning, resulting in the optimization of power generation and the comprehensive assessment of environmental impacts. The scope of this review primarily includes published articles spanning from 2005 to March 2023.",
        "DOI": "10.3390/jmse11101855",
        "affiliation_name": "Manhattan College",
        "affiliation_city": "New York",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Sustainability of Forest Eco-Products: Comprehensive Analysis and Future Research Directions",
        "paper_author": "Wang J.",
        "publication": "Forests",
        "citied_by": "7",
        "cover_date": "2023-10-01",
        "Abstract": "Forest ecological products are closely related to ecological balance, and an in-depth understanding of the development dynamics of these products is crucial to the realization of sustainable development that integrates ecological, economic, and social benefits. Based on the Web of Science (WOS) and China National Knowledge Infrastructure (CNKI) databases, this study conducted a comprehensive econometric analysis of the number of articles, journals, research institutions, author collaborations, research hotspots, and research trends of forest ecological products globally during the period of 2003–2023 with the help of CiteSpace software (Philadelphia, PA, USA). The study’s results revealed the following insights: (1) The research on forest ecological products in recent years showed a general upward trend, but the research interest in foreign countries was higher than that in China. (2) The literature within the WOS database primarily focused on the field of ecology, whereas the literature in the CNKI database predominantly emphasized the field of forestry. (3) In both databases, the Chinese Academy of Sciences was the organization with the highest number of articles. Globally, Chinese institutions had the largest proportion of articles issued. The high percentage of articles issued by specialized agricultural and forestry schools in China showed clear domain relevance. (4) In both databases, author collaborations were relatively decentralized, and no significant core group of authors had been formed. (5) The research hotspots in foreign countries focused on the ecological regulation of forest ecological products, while the research hotspots in China focused on the realization of the economic value of forest ecological products. (6) “Machine learning”, “river basin”, and “health” are the future research frontiers in foreign countries, while “ecological function” and “forest ecosystem service” are the future research frontiers in China. The results of both databases indicate that the sustainability of forest ecological products is a research trend for the coming period. Finally, the outlook for future research on forest eco-products is presented in four aspects: promoting the establishment of a unified international standard certification system for forest eco-products, developing diversified products, strengthening the function of policy support and guidance, and establishing national partnerships.",
        "DOI": "10.3390/f14102008",
        "affiliation_name": "Northeast Forestry University",
        "affiliation_city": "Harbin",
        "affiliation_country": "China"
    },
    {
        "paper_title": "An open-source probabilistic record linkage process for records with family-level information: Simulation study and applied analysis",
        "paper_author": "Prindle J.",
        "publication": "PLoS ONE",
        "citied_by": "3",
        "cover_date": "2023-10-01",
        "Abstract": "Research with administrative records involves the challenge of limited information in any single data source to answer policy-related questions. Record linkage provides researchers with a tool to supplement administrative datasets with other information about the same people when identified in separate sources as matched pairs. Several solutions are available for undertaking record linkage, producing linkage keys for merging data sources for positively matched pairs of records. In the current manuscript, we demonstrate a new application of the Python RecordLinkage package to family-based record linkages with machine learning algorithms for probability scoring, which we call probabilistic record linkage for families (PRLF). First, a simulation of administrative records identifies PRLF accuracy with variations in match and data degradation percentages. Accuracy is largely influenced by degradation (e.g., missing data fields, mismatched values) compared to the percentage of simulated matches. Second, an application of data linkage is presented to compare regression model estimate performance across three record linkage solutions (PRLF, ChoiceMaker, and Link Plus). Our findings indicate that all three solutions, when optimized, provide similar results for researchers. Strengths of our process, such as the use of ensemble methods, to improve match accuracy are discussed. We then identify caveats of record linkage in the context of administrative data.",
        "DOI": "10.1371/journal.pone.0291581",
        "affiliation_name": "University of Southern California",
        "affiliation_city": "Los Angeles",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Doubly robust evaluation of high-dimensional surrogate markers",
        "paper_author": "Agniel D.",
        "publication": "Biostatistics",
        "citied_by": "1",
        "cover_date": "2023-10-01",
        "Abstract": "When evaluating the effectiveness of a treatment, policy, or intervention, the desired measure of efficacy may be expensive to collect, not routinely available, or may take a long time to occur. In these cases, it is sometimes possible to identify a surrogate outcome that can more easily, quickly, or cheaply capture the effect of interest. Theory and methods for evaluating the strength of surrogate markers have been well studied in the context of a single surrogate marker measured in the course of a randomized clinical study. However, methods are lacking for quantifying the utility of surrogate markers when the dimension of the surrogate grows. We propose a robust and efficient method for evaluating a set of surrogate markers that may be high-dimensional. Our method does not require treatment to be randomized and may be used in observational studies. Our approach draws on a connection between quantifying the utility of a surrogate marker and the most fundamental tools of causal inference-namely, methods for robust estimation of the average treatment effect. This connection facilitates the use of modern methods for estimating treatment effects, using machine learning to estimate nuisance functions and relaxing the dependence on model specification. We demonstrate that our proposed approach performs well, demonstrate connections between our approach and certain mediation effects, and illustrate it by evaluating whether gene expression can be used as a surrogate for immune activation in an Ebola study.",
        "DOI": "10.1093/biostatistics/kxac020",
        "affiliation_name": "Bordeaux Population Health Research Center (BPH)",
        "affiliation_city": "Bordeaux",
        "affiliation_country": "France"
    },
    {
        "paper_title": "Analyzing sales of the Korean restaurant franchise during the COVID-19 pandemic with the mixed-effects model approach",
        "paper_author": "Lee C.",
        "publication": "PLoS ONE",
        "citied_by": "1",
        "cover_date": "2023-10-01",
        "Abstract": "Using point-of-sales (POS) data, the sales trends of 48 member stores of a Korean restaurant franchise during the COVID-19 pandemic were analyzed. As daily sales are nested in each member store of a franchise, the hierarchical structure of POS data was fully and effectively utilized by employing a mixed-effects model. The results showed that although sales volumes in all member stores were negatively affected by the pandemic, the level of impact varied according to store location: sales at some stores were drastically reduced, while a few others even achieved a slight increase in sales during the pandemic. These findings suggest that the government support policy for small business owners should be designed in a locally optimized way, to take account of neighborhood characteristics and the degree of sales loss for individual business owners.",
        "DOI": "10.1371/journal.pone.0293147",
        "affiliation_name": "Kangwon National University",
        "affiliation_city": "Chuncheon",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "Artificial intelligence, machine learning and big data in natural resources management: A comprehensive bibliometric review of literature spanning 1975–2022",
        "paper_author": "Pandey D.K.",
        "publication": "Resources Policy",
        "citied_by": "24",
        "cover_date": "2023-10-01",
        "Abstract": "Applying artificial intelligence (AI), machine learning (ML), and big data to natural resource management (NRM) is revolutionizing how natural resources are managed. To gain more insights into the domain, we use 394 Scopus-indexed documents to explore the thematic evolution and explore future research directions. We found that the topics related to AI, ML, and big data for natural resource management have increased significantly since 2012. While “Remote Sensing” is the most productive journal, S. Alqadhi and J. Mallick are the most contributing authors, and the United States has been the most contributing country. While the keywords “sustainable development” and “remote sensing” have been growing steadily since 1975, “natural resource modeling” and “machine learning” have been more popular during the last few years. The thematic analysis reveals that the existing literature is concentrated around four clusters, and the content analysis of the clusters uncovers 15 future research agendas. These research agendas include the development of efficient strategies for NRM, understanding the role of AI and ML in natural resource management, leveraging data-driven methods for decision-making, and developing models for interdisciplinary and cross-sectoral approaches. The study provides important implications of using technology in NRM. These technologies help policymakers create effective policies, improves assessment and decision-making, and optimizes resource use. These advancements benefit society by increasing access to essential resources in a fair manner, and they have positive impacts on both the public and private sectors, enabling evidence-based policymaking and responsible resource extraction. Collaboration and investment in these technologies are crucial for achieving sustainable development and preserving natural resources for future generations.",
        "DOI": "10.1016/j.resourpol.2023.104250",
        "affiliation_name": "Shiv Nadar University Chennai",
        "affiliation_city": "Chennai",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Guidelines for Use of Large Language Models by Authors, Reviewers, and Editors: Considerations for Imaging Journals",
        "paper_author": "Moy L.",
        "publication": "Radiology",
        "citied_by": "9",
        "cover_date": "2023-10-01",
        "Abstract": "NA",
        "DOI": "10.1148/radiol.239024",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Evaluation of coupling coordination development between digital economy and green finance: Evidence from 30 provinces in China",
        "paper_author": "Liu Z.",
        "publication": "PLoS ONE",
        "citied_by": "4",
        "cover_date": "2023-10-01",
        "Abstract": "The convergence of China’s digital economy and green finance holds great significance for fostering a sustainable and high-quality developmental path. However, existing studies have not explored the coupling coordination development between these two crucial subsystems. To bridge this gap, this paper employs a modified coupling coordination degree (CCD) model to assess and affirm the coupling coordination degree between the digital economy and green finance across 30 provinces in China from 2015–2021. Based on degree results, provinces are classified into three clusters by using K-means and hierarchical clustering algorithm. Our findings unveil that the current level of coupling coordination development in China is at a primary coordination stage. Although regional disparities significantly exist, the overall level of coordination remains steadily increasing, with the eastern region outperforming the western region. Additionally, we determine that the COVID-19 pandemic’s disruption on the coupling coordination development of these systems has been limited. This research sheds light on the evolution of coupling systems and offers practical recommendations for strengthening the coordinated development of the digital economy and green finance.",
        "DOI": "10.1371/journal.pone.0291936",
        "affiliation_name": "Huainan Normal University",
        "affiliation_city": "Huainan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Fracking Twitter: Utilizing machine learning and natural language processing tools for identifying coalition and causal narratives",
        "paper_author": "Pattison A.",
        "publication": "Politics and Policy",
        "citied_by": "2",
        "cover_date": "2023-10-01",
        "Abstract": "The Narrative Policy Framework (NPF) has provided policy scholars with a valuable method to gain empirical insight into the power of narratives in the policy process. However, a significant limitation of the NPF has been its ability to deploy this framework on large N datasets due to the labor-intensive nature of collecting narrative data. In recent years, NPF scholars have turned to computational social science tools to address this challenge. This study builds upon this emerging body of literature and our previous work, which uses sentiment analysis, a natural language processing technique, to evaluate the use of the angel/devil shift across coalitions before and after a major policy change. We examined Tweets that included the terms “fracking” and “New York” before and after the introduction of a moratorium. While sentiment analysis allowed us to gain insight into the narrative structure of the fracking policy discourse space, the labor involved in hand-coding Twitter users into neutral-, pro-, or anti-fracking groups was onerous. This project aims to supplement our natural language processing method by employing supervised machine learning techniques to increase the universe of respondents. We hand-coded 500 Twitter users into neutral-, pro-, or anti-fracking groups and trained a much larger dataset using an extreme gradient boost algorithm to classify a broader corpus of Tweets. This enabled us to expand the number of Tweets used in the analyses. We then applied sentiment analysis on this newly classified larger dataset to reveal differences in the pro-fracking and anti-fracking advocacy coalitions. By using machine learning to classify pro and con Tweets, we gained the ability to achieve significantly greater insight into how these two subgroups employed different narrative and linguistic devices in their Twitter discussions about fracking. Related Articles: Merry, Melissa K. 2022. “Trump's Tweets as Policy Narratives: Constructing the Immigration Issue via Social Media.” Politics & Policy 50(4): 752–72. https://doi.org/10.1111/polp.12487. Robles, Pedro, and Daniel J. Mallinson. 2023. “Catching Up with AI: Pushing toward a Cohesive Governance Framework.” Politics & Policy 51(3): 355–72. https://doi.org/10.1111/polp.12529. Shanahan, Elizabeth A., Mark K. McBeth, and Paul L. Hathaway. 2011. “Narrative Policy Framework: The Influence of Media Policy Narratives on Public Opinion.” Politics & Policy 39(3): 373–400. https://doi.org/10.1111/j.1747-1346.2011.00295.x.",
        "DOI": "10.1111/polp.12555",
        "affiliation_name": "Colgate University",
        "affiliation_city": "Hamilton",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "A Comparative Assessment of Bridge Deck Wearing Surfaces: Performance, Deterioration, and Maintenance",
        "paper_author": "Kale A.",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "4",
        "cover_date": "2023-10-01",
        "Abstract": "Bridge decks deteriorate faster compared to other bridge components, primarily influenced by traffic volume, while previous studies have examined the effect of bridge-wearing surfaces on deterioration, further understanding of the relationship between bridge performance and maintenance is needed for policy-making and planning purposes. In this study, we focus on nine influential variables to unravel the intricate connections among performance, deterioration, and maintenance of six distinct bridge-wearing surfaces: Monolithic Concrete, Gravel, Wood or Timber, Bituminous, Low Surface Concrete, and Other. Statistical analyses were employed to determine associations between variables and concepts, exploring similarities and differences across various wearing surface types. In particular, machine learning algorithms were utilized to model the maintenance considering the performance and deterioration of the six diverse wearing surfaces. This approach allowed for an examination of interactions between those variables and concepts. We further applied a well-performing prediction model (which achieved an accuracy of 0.86 and an AUC score of approximately 0.83) to obtain interpretable insights regarding bridge deck surfaces. Analysis with interpretable methods such as SHAP (Shapley additive explanation) and PDP (partial dependency plot) revealed that deterioration, deck age, deck area, and overall performance were the most influential variables among average daily traffic, average daily truck traffic, and the number of spans significantly influenced the maintenance of bridge deck condition with different wearing surfaces. Notably, a strong relationship between performance and maintenance was observed in specific wearing surface types, such as Monolithic Concrete and Wood or Timber, while Other surface types exhibited different patterns. These findings highlight the need for tailored approaches when assessing bridge health, considering the distinct characteristics of different bridge deck types.",
        "DOI": "10.3390/app131910883",
        "affiliation_name": "University of Nebraska Omaha",
        "affiliation_city": "Omaha",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Long-Term Forecasting Framework for Renewable Energy Technologies’ Installed Capacity and Costs for 2050",
        "paper_author": "Rozon F.",
        "publication": "Energies",
        "citied_by": "9",
        "cover_date": "2023-10-01",
        "Abstract": "Published forecasts underestimate renewable energy capacity growth and potential cost reductions, creating uncertainty around investment decisions and slowing progress. Scenario-based projections diverge widely, driven by variations in modelling techniques and underlying assumptions, with policy-based models typically being overly conservative. With historical generation capacity and cost data readily available, this research demonstrates that data-driven approaches can be leveraged to improve long-term capacity and cost forecasts of solar, wind, and battery storage technologies. Unlike exponential growth models prevailing over shorter time scales, logistic curves requiring asymptotic limits, or machine learning algorithms dependent on extensive datasets, this analysis demonstrates that temporal quadratic regressions are a better starting point to represent capacity growth trends over two to three decades. When coupled with published learning rates, trend-based capacity forecasts provided tighter and lower capital and levelized cost of energy outlooks than most reviewed scenarios, with photovoltaics global average levelized cost of energy reducing from 0.057/kWh to below USD 0.03/kWh by 2030 and below USD 0.02/kWh by 2040. Greater transparency on manufacturing ecosystems is proposed so that more advanced analytical techniques can be utilized. This analysis indicates that without direct interventions to accelerate the growth in wind power generation, global renewable energy technology deployment will fall short of the generation capacities required to meet climate change objectives.",
        "DOI": "10.3390/en16196874",
        "affiliation_name": "Stellenbosch University",
        "affiliation_city": "Stellenbosch",
        "affiliation_country": "South Africa"
    },
    {
        "paper_title": "Classification of deep learning convolutional neural network feature extraction for student graduation prediction",
        "paper_author": "Salam A.",
        "publication": "Indonesian Journal of Electrical Engineering and Computer Science",
        "citied_by": "1",
        "cover_date": "2023-10-01",
        "Abstract": "One indicator of a university’s educational quality is the proportion of enrolled students who actually graduate within four years. This proportion is typically fewer than the number of students that enroll in a given year. A low graduation rate can have a negative impact on both the university’s reputation and its accreditation because it indicates that fewer students are completing their degrees. Student activity, economic, and other issues all play a role in why some students are unable to complete their degrees on time. As a result, stakeholders need a model that can predict whether or not students will graduate on time as a means of evaluating and giving a basis for policy actions. This research proposes a model for converting textual data into an image format using a deep learning convolutional neural network (CNN), and then classifying the extracted features using a variety of machine learning classification algorithms like the decision tree, random forest, Naive Bayes, support vector machine (SVM), and k-nearest neighbor (K-NN). The classification model trained on feature extraction data had a 96.1% accuracy rate, while the classification model trained on the original data achieved a 71.2% accuracy rate.",
        "DOI": "10.11591/ijeecs.v32.i1.pp335-341",
        "affiliation_name": "Universitas Dian Nuswantoro",
        "affiliation_city": "Semarang",
        "affiliation_country": "Indonesia"
    },
    {
        "paper_title": "Groundwater Quality Assessment Based on the Random Forest Water Quality Index—Taking Karamay City as an Example",
        "paper_author": "Xiong Y.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "9",
        "cover_date": "2023-10-01",
        "Abstract": "In the past few decades, global industrial development and population growth have led to a scarcity of water resources, making sustainable management of groundwater a global challenge. The Water Quality Index (WQI) serves as a comprehensive method for assessing water quality and can provide valuable recommendations at the water quality level, optimizing policies for groundwater management. However, the subjectivity and uncertainty of the traditional WQI have negative impacts on evaluation outcomes, particularly in determining indicator weights and selecting aggregation functions. The proposed water quality index for groundwater based on the random forest (RFWQI) model in this study addresses these issues. It selects water quality indicators based on the actual pollution situation in the study area, employs an advanced random forest model to rank water quality indicators, determines indicator weights using the rank centroid method, scores the indicators using a sub-index function designed for groundwater development, and compares the results of two commonly used aggregation functions to identify the optimal one. Based on the aggregated scores, the water quality at 137 monitoring sites is classified into five levels: “Excellent”, “Good”, “Medium”, “Poor”, or “Unacceptable”. Among the 11 water quality indicators (sodium, sulfate, chloride, bicarbonate, total dissolved solids, fluoride, boron, nitrate, pH, CODMn, and hardness), chloride was given the highest weight (0.236), followed by total dissolved solids (0.156), and sodium was given the lowest weight (0.008). The random forest model exhibits a good prediction capability before hyperparameter tuning (86% accuracy, RMSE of 0.378), and after grid search and five-fold cross-validation, the optimal hyperparameter combination is determined, further improving the performance of the random forest model (94% accuracy, F1-Score of 0.967, AUC of 0.91, RMSE of 0.232). For the newly developed groundwater sub-index function, interpolation is used to score each indicator, and after comparing two aggregation functions, the NSF aggregation function is selected as the most suitable for groundwater assessment. Overall, most of the groundwater in the study area was of poor quality (52.5% of low quality) and not suitable for drinking.",
        "DOI": "10.3390/su151914477",
        "affiliation_name": "Anyang Institute of Technology",
        "affiliation_city": "Anyang",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Influence of Social Overhead Capital Facilities on Housing Prices Using Machine Learning",
        "paper_author": "Paik J.",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "2",
        "cover_date": "2023-10-01",
        "Abstract": "Featured Application: Real Estate Price Prediction, Housing Price Prediction Applications, Establishment of a Stable Real Estate Policy. The South Korean residential real estate market is influenced by both the traditional dynamics of demand and supply and external factors such as housing policies and macroeconomic conditions. Considering the proportion of housing assets in individual wealth, market fluctuations can have significant implications. While previous studies have utilized variables such as GDP growth rate, patent issuance, and birth rate, and employed models such as LSTM and ARIMA for housing price predictions, many have overlooked the influence of local factors. In particular, there has been insufficient investigation into the impact of subway stations and living social overhead capital facilities on housing prices, especially in metropolitan areas. This study seeks to bridge this gap by analyzing the usage trends of subway stations, evaluating the impact of living social overhead capital facilities on housing values, and deriving the optimal machine learning model for price predictions near subway stations. We compared and analyzed a total of eight machine learning regression models, including Linear Regression, Decision Tree, Random Forest, LightGBM, Ridge, Lasso, Elastic Net, and XGBoost, all of which are popular regression models, especially in the context of machine learning and data science. Through comparative analysis of these machine learning techniques, we aim to provide insights for more rational housing price determinations, thereby promoting stability in the real estate market.",
        "DOI": "10.3390/app131910732",
        "affiliation_name": "Pyeongtaek University",
        "affiliation_city": "Pyeongtaek",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "Incorporating Social and Policy Drivers into Land-Use and Land-Cover Projection",
        "paper_author": "Abbasnezhad B.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "3",
        "cover_date": "2023-10-01",
        "Abstract": "Forestlands in the southeastern U.S. generate a great variety of ecosystem services that contribute to the well-being of humans and nonhumans alike. Despite their importance, forests continue to be lost to other land uses such as agricultural production and urban development. Advancements in remote sensing and machine learning techniques have facilitated land use/land cover (LULC) change projections, but many prior efforts have neglected to account for social and policy dimensions. We incorporated key socio-economic factors, conservation policies, societal preferences, and landscape biophysical features into LULC projection techniques under four different development scenarios. We applied this approach in the Upper Flint watershed, which flows south from the Atlanta, Georgia metropolitan area and is characterized by extensive urbanization and associated deforestation. Our results suggest that incorporating social and policy drivers in future LULC projection approaches leads to more realistic results with higher accuracy levels, offering decision-makers, development planners, and policymakers better opportunities to forecast the effects of anticipated changes on the availability of ESs in the future. Conservation organizations and public agencies can benefit from such analysis to identify regions requiring conservation interventions for prioritizing their conservation efforts. We used publicly available data for the conterminous U.S., hence our approach can be replicable in other study regions within the nation.",
        "DOI": "10.3390/su151914270",
        "affiliation_name": "University of Georgia",
        "affiliation_city": "Athens",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Comparison between Physics-Based Approaches and Neural Networks for the Energy Consumption Optimization of an Automotive Production Industrial Process",
        "paper_author": "Pelella F.",
        "publication": "Energies",
        "citied_by": "0",
        "cover_date": "2023-10-01",
        "Abstract": "The automotive production sector plays a significant role in the energy consumption of all the industrial sphere, which currently represents approximately 38% of the total global energy use. Especially in production sites with several manufacturing lines working in parallel, the occurrence of failures and anomalies or sudden changes in the production volume may require a re-scheduling of the entire production process. In this regard, a digital twin of each phase of the process would give several indications about the new re-scheduled manufacture in terms of energy consumption and the control strategy to adopt. Therefore, the main goal of this paper is to propose different modeling approaches to a degreasing tank process, which is a preliminary phase at automotive production sites before the application of paint to car bodies. In detail, two different approaches have been developed: the first is a physics-based thermodynamic approach, which relies on the mass and energy balances of the system analyzed, and the second is machine learning-based, with the calibration of several artificial neural networks (ANNs). All the investigated approaches were assessed and compared, and it was determined that, for this application and with the data at our disposal, the thermodynamic approach has better prediction accuracy, with an overall mean absolute error (MAE) of 1.30 °C. Moreover, the model can be used to optimize the heat source policy of the tank, for which it has demonstrated, with historical data, an energy saving potentiality of up to 30%, and to simulate future scenarios in which, due to company constraints, a re-scheduling of the production of more work shifts is required.",
        "DOI": "10.3390/en16196916",
        "affiliation_name": "Stellantis N.V.",
        "affiliation_city": "Lijnden",
        "affiliation_country": "Netherlands"
    },
    {
        "paper_title": "A Comprehensive Study of the Impact of Waste Fires on the Environment and Health",
        "paper_author": "Jakhar R.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "8",
        "cover_date": "2023-10-01",
        "Abstract": "The escalating crisis of environmental degradation, with waste fires acting as a potent accelerant, has reached a critical juncture that demands immediate attention. This crisis disproportionately affects developing and low-income nations, where unregulated disposal and incineration in open areas have become rampant. These open waste fires serve as hotbeds for many environmental hazards ranging from air and water pollution to soil degradation. In addition, they contribute to the growing threat of marine litter and are a significant source of greenhouse gas emissions, exacerbating global climate change. Beyond their environmental toll, waste fires present an immediate and long-term threat to human health, causing respiratory problems and skin conditions and potentially leading to more serious health outcomes, such as cancer. Their impacts are multidimensional, affecting not only the environment but also pose severe health risks to communities, especially those near waste-burning sites. In this technologically advanced era, the application of artificial intelligence (AI), Machine Learning (ML), and deep learning technologies has the potential to revolutionize waste fire management. These technologies can significantly improve the accuracy of identifying, monitoring, and ultimately mitigating waste fires, making them indispensable tools in the fight against this complex issue. This article offers a comprehensive and in-depth examination of the historical evolution of waste fires, with the aim of shedding light on the critical factors that contribute to their occurrence. We explore the scientific mechanisms by which waste fires lead to environmental pollution and public health crises, providing a holistic understanding of their far-reaching impacts. We present an overview of significant research initiatives, policy interventions, and technological solutions that have been proposed or implemented by authoritative bodies around the world. By synthesizing existing research and offering new insights, this paper aims to facilitate a deeper understanding of the intricacies of waste fires and spur innovative solutions for their sustainable management and eventual eradication. Therefore, this article focuses on environmental and human health problems while outlining the comprehensive approach and potential contributions to solving this critical issue.",
        "DOI": "10.3390/su151914241",
        "affiliation_name": "AGH University of Krakow",
        "affiliation_city": "Krakow",
        "affiliation_country": "Poland"
    },
    {
        "paper_title": "An Automated Machine Learning Approach towards Energy Saving Estimates in Public Buildings",
        "paper_author": "Biessmann F.",
        "publication": "Energies",
        "citied_by": "1",
        "cover_date": "2023-10-01",
        "Abstract": "Reducing the energy consumption of buildings in the public sector is an important component in our efforts towards reaching our sustainability goals. In this context, a decisive prerequisite for administrations and policy makers is a tool for estimating the effectiveness of measures to reduce energy consumption. Estimating the impact of planned investments in building technology at scale, however, remains challenging, mainly for two reasons. For one, accurate physical modeling requires detailed building data, which can be difficult to obtain. Second, adapting established building models to novel measures aiming at energy consumption reduction is difficult. Hence, modeling building consumption patterns after retrofitting is a non-trivial task, and more research is needed to improve modeling techniques as well as to assess their effectiveness across a wide range of application scenarios. Modeling tools need to be generic enough to enable modeling of a variety of building types, they should ideally require as few input features as possible and they should allow for a high degree of automation in the selection and calibration of building modeling tools. Here, we propose a novel machine learning approach that does not require detailed building data and can automatically adapt to retrofitting measures. We evaluate our method on a data set of 113 public buildings in 4 building categories in Berlin, Germany. The data set contains energy consumption data in the initial state and after implementation of a weather-predictive heating control system. Despite being fully automated and requiring only minimal information about the building, our model can reliably predict the energy consumption of large public buildings better than established methods. All code and data are publicly released.",
        "DOI": "10.3390/en16196799",
        "affiliation_name": "Technische Universität Berlin",
        "affiliation_city": "Berlin",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Optimizing Stage Construction and Level Balancing of Match-3 Puzzle Game with PPO Algorithm Machine Learning",
        "paper_author": "Kim B.",
        "publication": "Electronics (Switzerland)",
        "citied_by": "2",
        "cover_date": "2023-10-01",
        "Abstract": "Although the global market has witnessed a proliferation of diverse match-3 puzzle games, achieving success in this competitive market remains challenging. The crucial factors that determine the success of match-3 puzzle games are the creation of numerous engaging stages and precise level balancing. The purpose of this study is to propose a match-3 puzzle game system that aims at identifying the most effective algorithm for training artificial intelligence agents in stage construction and level balancing verification. To validate the systems’ usefulness, this paper conducted experiments with the Proximal Policy Optimization (PPO) algorithm and obtained cumulative reward and entropy value graphs. Consequently, it has been confirmed that the system can be employed to compare learning outcomes for each algorithm and identify the optimal algorithm suitable for match-3 puzzle games. The use of machine learning technology in match-3 puzzle games holds the promise of revolutionizing game development and leading to the creation of more captivating and rewarding gaming experiences for players.",
        "DOI": "10.3390/electronics12194098",
        "affiliation_name": "Gachon University",
        "affiliation_city": "Seongnam",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "Sentiment Analysis of Tweets on Menu Labeling Regulations in the US",
        "paper_author": "Yang Y.",
        "publication": "Nutrients",
        "citied_by": "1",
        "cover_date": "2023-10-01",
        "Abstract": "Menu labeling regulations in the United States mandate chain restaurants to display calorie information for standard menu items, intending to facilitate healthy dietary choices and address obesity concerns. For this study, we utilized machine learning techniques to conduct a novel sentiment analysis of public opinions regarding menu labeling regulations, drawing on Twitter data from 2008 to 2022. Tweets were collected through a systematic search strategy and annotated as positive, negative, neutral, or news. Our temporal analysis revealed that tweeting peaked around major policy announcements, with a majority categorized as neutral or news-related. The prevalence of news tweets declined after 2017, as neutral views became more common over time. Deep neural network models like RoBERTa achieved strong performance (92% accuracy) in classifying sentiments. Key predictors of tweet sentiments identified by the random forest model included the author’s followers and tweeting activity. Despite limitations such as Twitter’s demographic biases, our analysis provides unique insights into the evolution of perceptions on the regulations since their inception, including the recent rise in negative sentiment. It underscores social media’s utility for continuously monitoring public attitudes to inform health policy development, execution, and refinement.",
        "DOI": "10.3390/nu15194269",
        "affiliation_name": "Washington University in St. Louis, George Warren Brown School of Social Work",
        "affiliation_city": "St. Louis",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Predicting Unmet Healthcare Needs in Post-Disaster: A Machine Learning Approach",
        "paper_author": "Han H.J.",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "0",
        "cover_date": "2023-10-01",
        "Abstract": "Unmet healthcare needs in the aftermath of disasters can significantly impede recovery efforts and exacerbate health disparities among the affected communities. This study aims to assess and predict such needs, develop an accurate predictive model, and identify the key influencing factors. Data from the 2017 Long-term Survey on the Change of Life of Disaster Victims in South Korea were analyzed using machine learning techniques, including logistic regression, C5.0 tree-based model, and random forest. The features were selected based on Andersen’s health behavior model and disaster-related factors. Among 1659 participants, 31.5% experienced unmet healthcare needs after a disaster. The random forest algorithm exhibited the best performance in terms of precision, accuracy, Under the Receiver Operating Characteristic (AUC-ROC), and F-1 scores. Subjective health status, disaster-related diseases or injuries, and residential area have emerged as crucial factors predicting unmet healthcare needs. These findings emphasize the vulnerability of disaster-affected populations and highlight the value of machine learning in post-disaster management policies for decision-making.",
        "DOI": "10.3390/ijerph20196817",
        "affiliation_name": "KyungHee University College of Medicine",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "Artificial Intelligence Applications to Public Health Nutrition",
        "paper_author": "An R.",
        "publication": "Nutrients",
        "citied_by": "4",
        "cover_date": "2023-10-01",
        "Abstract": "NA",
        "DOI": "10.3390/nu15194285",
        "affiliation_name": "Washington University in St. Louis, George Warren Brown School of Social Work",
        "affiliation_city": "St. Louis",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Data-Sparse Prediction of High-Risk Schools for Lead Contamination in Drinking Water: Examples from Four U.S. States",
        "paper_author": "Shrivatsa S.",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "0",
        "cover_date": "2023-10-01",
        "Abstract": "Childhood lead exposure through drinking water has long-term effects on cognition and development, and is a significant public health concern. The comprehensive lead testing of public schools entails high expense and time. In prior work, random forest modeling was used successfully to predict the likelihood of lead contamination in the drinking water from schools in the states of California and Massachusetts. In those studies, data from 70% of the schools was used to predict the probability of unsafe water lead levels (WLLs) in the remaining 30%. This study explores how the model predictions degrade, as the training dataset forms a progressively smaller proportion of schools. The size of the training set was varied from 80% to 10% of the total samples in four US states: California, Massachusetts, New York, and New Hampshire. The models were evaluated using the precision-recall area under curve (PR AUC) and area under the receiver operating characteristic curve (ROC AUC). While some states required as few as 10% of the schools to be included in the training set for an acceptable ROC AUC, all four states performed within an acceptable ROC AUC range when at least 50% of the schools were included. The results in New York and New Hampshire were consistent with the prior work that found the most significant predictor in the modeling to be the Euclidean distance to the closest school in the training set demonstrating unsafe WLLs. This study further supports the efficacy of predictive modeling in identifying the schools at a high risk of lead contamination in their drinking water supply, even when the survey data is incomplete on WLLs in all schools.",
        "DOI": "10.3390/ijerph20196895",
        "affiliation_name": "Department of Civil and Environmental Engineering",
        "affiliation_city": "Berkeley",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Forecasting rare earth stock prices with machine learning",
        "paper_author": "Henriques I.",
        "publication": "Resources Policy",
        "citied_by": "10",
        "cover_date": "2023-10-01",
        "Abstract": "Rare earth elements (REEs) are indispensable for producing green technologies and electronics. Demand for REEs in clean energy technologies in 2040 are projected to be three to seven times higher than today and will be critical to the clean technology transition needed to stave off catastrophic climate change. Forecasting rare earth stock prices is critical for making well informed investment decisions concerning this important asset class. Despite the latter, the literature on forecasting rare earth stock prices is scarce. We use machine learning techniques to forecast daily rare earth stock price direction. The analysis reveals that random forests, extremely randomized trees, RNN, and support vector machine have higher prediction accuracy than Lasso or Naïve Bayes. We find that the 10- to 20-day forecasts using random forests, extremely randomized trees, and support vector machine achieve prediction accuracies greater than 85% with some prediction accuracy reaching 90%. Lasso prediction accuracy is higher than Naïve Bayes but never greater than 67%. The MA200, MA50, on balance volume, VIX, and WAD are the most important predictive features of rare earth stock price direction. A switching portfolio that uses trading signals from an Extra Trees model impressively outperforms a buy and hold portfolio. Our results reveal the high prediction accuracy of using machine learning methods in forecasting rare earth stock price direction which should be useful to investors, policy makers and venture capitalists.",
        "DOI": "10.1016/j.resourpol.2023.104248",
        "affiliation_name": "Schulich School of Business",
        "affiliation_city": "Toronto",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Research on land resource management integrated with support vector machine —Based on the perspective of green innovation",
        "paper_author": "Jin T.",
        "publication": "Resources Policy",
        "citied_by": "4",
        "cover_date": "2023-10-01",
        "Abstract": "Traditional methods of land resource management are no longer adequate to address the rapidly changing environmental and societal demands. By introducing data-driven approaches like SVM, this study offers a novel avenue for more precise analysis and prediction of land resource utilization trends, the green innovation serves to enhance the efficiency and ecological sustainability of land resource utilization through pioneering and technological means. there exists a close connection between land resource management, green innovation, and sustainable development. The experiment utilized a Support Vector Machine (SVM) as the foundational model and focused on land resources in Beijing, utilizing remote sensing imagery for prediction and planning. The experimental results demonstrate significant achievements in land resource classification prediction with the integrated SVM model, indicating a strong linear relationship between the prediction results and actual observation data. This suggests that the introduction of green innovation can enhance the effectiveness of land resource classification prediction, providing more efficient decision support for land resource management and sustainable development. These results offer important policy recommendations for the promotion of green space expansion and conservation policies, facilitation of data sharing and technology support policies, formulation of land resource management and integration of sustainable development goals.",
        "DOI": "10.1016/j.resourpol.2023.104180",
        "affiliation_name": "College of Engineering and Applied Sciences",
        "affiliation_city": "Stony Brook",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Translating Predictive Analytics for Public Health Practice: A Case Study of Overdose Prevention in Rhode Island",
        "paper_author": "Allen B.",
        "publication": "American Journal of Epidemiology",
        "citied_by": "2",
        "cover_date": "2023-10-01",
        "Abstract": "Prior applications of machine learning to population health have relied on conventional model assessment criteria, limiting the utility of models as decision support tools for public health practitioners. To facilitate practitioners' use of machine learning as a decision support tool for area-level intervention, we developed and applied 4 practice-based predictive model evaluation criteria (implementation capacity, preventive potential, health equity, and jurisdictional practicalities). We used a case study of overdose prevention in Rhode Island to illustrate how these criteria could inform public health practice and health equity promotion. We used Rhode Island overdose mortality records from January 2016-June 2020 (n = 1,408) and neighborhood-level US Census data. We employed 2 disparate machine learning models, Gaussian process and random forest, to illustrate the comparative utility of our criteria to guide interventions. Our models predicted 7.5%-36.4% of overdose deaths during the test period, illustrating the preventive potential of overdose interventions assuming 5%-20% statewide implementation capacities for neighborhood-level resource deployment. We describe the health equity implications of use of predictive modeling to guide interventions along the lines of urbanicity, racial/ethnic composition, and poverty. We then discuss considerations to complement predictive model evaluation criteria and inform the prevention and mitigation of spatially dynamic public health problems across the breadth of practice. This article is part of a Special Collection on Mental Health.",
        "DOI": "10.1093/aje/kwad119",
        "affiliation_name": "NYU CS department",
        "affiliation_city": "New York",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Severe Acute Respiratory Syndrome Coronavirus 2 Delta Variant Genomic Variation Associated with Breakthrough Infection in Northern California: A Retrospective Cohort Study",
        "paper_author": "Skarbinski J.",
        "publication": "Journal of Infectious Diseases",
        "citied_by": "1",
        "cover_date": "2023-10-01",
        "Abstract": "Background: The association between severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) genomic variation and breakthrough infection is not well defined among persons with Delta variant SARS-CoV-2 infection. Methods: In a retrospective cohort, we assessed whether individual nonlineage defining mutations and overall genomic variation (including low-frequency alleles) were associated with breakthrough infection, defined as SARS-CoV-2 infection after coronavirus disease 2019 primary vaccine series. We identified all nonsynonymous single-nucleotide polymorphisms, insertions, and deletions in SARS-CoV-2 genomes with ≥5% allelic frequency and population frequency of ≥5% and ≤95%. Using Poisson regression, we assessed the association with breakthrough infection for each individual mutation and a viral genomic risk score. Results: Thirty-six mutations met our inclusion criteria. Among 12 744 persons infected with Delta variant SARS-CoV-2, 5949 (47%) were vaccinated and 6795 (53%) were unvaccinated. Viruses with a viral genomic risk score in the highest quintile were 9% more likely to be associated with breakthrough infection than viruses in the lowest quintile, but including the risk score improved overall predictive model performance (measured by C statistic) by only +0.0006. Conclusions: Genomic variation within SARS-CoV-2 Delta variant was weakly associated with breakthrough infection, but several potential nonlineage defining mutations were identified that might contribute to immune evasion by SARS-CoV-2.",
        "DOI": "10.1093/infdis/jiad164",
        "affiliation_name": "Innovative Genomics Institute",
        "affiliation_city": "Berkeley",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "A water quality dataset of levels of metal, nutrient and anions in sample water points from sixteen selected urban and rural districts of Uganda",
        "paper_author": "Namatovu H.K.",
        "publication": "Data in Brief",
        "citied_by": "3",
        "cover_date": "2023-10-01",
        "Abstract": "This dataset highlights some of the water quality issues in Uganda. The rationale for collecting the water samples was to test and ascertain the level and source of contamination. A total of one hundred and eighty five samples were collected from sixteen districts. At each water point, a sample was collected using a sterile plastic container, which was pre-rinsed with the water to be sampled. Water samples were drawn from protected and unprotected springs, shallow wells, taps, rain tanks, water reservoirs, open and hand dug wells and boreholes and immediately transported on ice to the National Water Quality Reference Laboratory for analysis. At the laboratory, a BWB flame photometer, Ethylenediamine tetraacetic acid (EDTA)1 titration and gallery plus-thermos fisher discreet analyzer were used to analyze metal, nutrient and anion elements. On-site testing of dissolved oxygen, pH, electrical conductivity and turbidity was done using a water data sonde. This data can be used to draw comparative analyses of water quality issues in rural and urban districts and help in identifying the factors that influence water quality variations. The data can further be used for trend analysis and identifying long-term patterns whilst providing insights into pollution sources and the impact of environmental and climate change. Consequently, mathematical and machine learning models can use this data together with other parameters to predict the changes in water quality which information is essential for policy and decisions making. This data can be used by environmental scientists to draw insights into the health of the aquatic biodiversity; geospatial analysts to ascertain proximal water contaminants; public health specialists to analyze pathogens leading to water-borne diseases; water chemists to study the source and cause of water pollution; data scientists to perform predictive and descriptive analyses; and policy makers to formulate laws and regulations.",
        "DOI": "10.1016/j.dib.2023.109601",
        "affiliation_name": "Makerere University",
        "affiliation_city": "Kampala",
        "affiliation_country": "Uganda"
    },
    {
        "paper_title": "Severity of vehicle-to-vehicle accidents in the UAE: An exploratory analysis using machine learning algorithms",
        "paper_author": "Maghelal P.",
        "publication": "Heliyon",
        "citied_by": "2",
        "cover_date": "2023-10-01",
        "Abstract": "The World Health Organization (WHO) identifies road traffic injuries as a global health problem. The Eastern-Mediterranean region is particularly suffering from low traffic safety levels, recording the third highest death per capita ratio in the world. It is critical to evaluate and understand the causes of crashes and their severity levels as a first step to devising policies that aim to reduce these causes. Previous studies examining the frequency or severity of crashes present important limitations that motivate the need for the current work. While these studies have investigated the relation of contributing factors to severity of crashes, not until recently the importance of these factors are bring investigated. Even then, less research have explored various Machine Learning models and none in the middle-eastern region. This is critical because the WHO report concludes that the chances of dying in a traffic crash in this region are second only to Africa per 100000 population. This is a first study analyzing the severity of vehicle-to-vehicle crashes among drivers in the United Arab Emirates. Traffic Crash Data was obtained from the Abu Dhabi Police, which consisted of 11,400 observations during the period 2014–2017. Machine learning algorithms, including gradient boosting (GB), support vector machines (SVM), and random forest (RF), were trained and tested to predict crash severity and extract (using feature analysis) its determinants. The models were evaluated using two performance metrics: prediction accuracy and F1-scores. The RF model outperformed both GB and SVM, with the confusion matrix of RF reporting a better prediction for all four crash severity classes. The feature importance analysis indicates that the age of car, age of the injured, and the age of the initiator have the highest effect on severity, which is an important finding as the listed factors were rarely considered in previous studies. Vehicle and road characteristics such as vehicle class, crash type, and lighting are slightly associated with the severity. Consistent with other studies, gender was the least essential predictor of severity. Recommendations are finally provided to the Abu Dhabi Department of Municipalities and Transport (AD-DMT) authority to guide the development of road safety policies and countermeasures to mitigate the occurrence and severity of crashes.",
        "DOI": "10.1016/j.heliyon.2023.e20694",
        "affiliation_name": "Rabdan Academy",
        "affiliation_city": "Abu Dhabi",
        "affiliation_country": "United Arab Emirates"
    },
    {
        "paper_title": "The consumer price index prediction using machine learning approaches: Evidence from the United States",
        "paper_author": "Nguyen T.T.",
        "publication": "Heliyon",
        "citied_by": "6",
        "cover_date": "2023-10-01",
        "Abstract": "The consumer price index (CPI) is one of the most important macroeconomic indicators for determining inflation, and accurate predictions of CPI changes are important for a country's economic development. This study uses multivariate linear regression (MLR), support vector regression (SVR), autoregressive distributed lag (ARDL), and multivariate adaptive regression splines (MARS) to predict the CPI of the United States. Data from January 2017 to February 2022 were randomly selected and divided into two stages: 80 % for training and 20% for testing. The US CPI was modeled for the observed period and relied on a mix of elements, including crude oil price, world gold price, and federal fund effective rate. Evaluation metrics—mean absolute percentage value, mean absolute error, root mean square error, R-squared, and correlation of determination—were employed to estimate forecasted values. The MLR, SVR, ARDL, and MARS models attained high accuracy parameters, while the MARS algorithm generated higher accuracy in US CPI forecasts than the others in the testing phase. These outputs could support the US government in overseeing economic policies, sectors, and social security, thereby boosting national economic development.",
        "DOI": "10.1016/j.heliyon.2023.e20730",
        "affiliation_name": "National Kaohsiung University of Science and Technology",
        "affiliation_city": "Kaohsiung",
        "affiliation_country": "Taiwan"
    },
    {
        "paper_title": "Maximum Entropy Hierarchical Reinforcement Learning with Advantage-weighted Mutual Information Maximization",
        "paper_author": "Wu L.",
        "publication": "Jisuanji Xuebao/Chinese Journal of Computers",
        "citied_by": "2",
        "cover_date": "2023-10-01",
        "Abstract": "Reinforcement learning is a significant research area in machine learning. By interacting with the environment， agents can adapt to the dynamic environment. At the same time， this interactive learning approach allows an agent to progressively optimize its policy， which is promising for a wide range of applications. Deep reinforcement learning，a method that combines reinforcement learning with deep learning， plays a crucial role in artificial intelligence. This combination enables agents to learn and make autonomous decisions in complex and dynamic environments without complex supervised data. In recent years，deep reinforcement learning has achieved remarkable results in games and complex control tasks. For example，Deep Q Learning （DQN）algorithm uses a convolutional neural network to process the visual input from the game screen and continuously updates the policy through a Q-learning algorithm. In Atari 2600 games，the DQN can learn advanced game strategies autonomously by looking at the game screen pixel information，even without human expert guidance. However，DQN is only applicable to discrete action space tasks. To solve this problem， Deep Deterministic Policy Gradient （DDPG）combines deterministic policy gradient algorithms with DQN algorithms to achieve policy optimization and learning in continuous action spaces. Twin Delayed Deep Deterministic Policy Gradient（TD3）algorithm uses a clipped double Q network to prevent the value function from being overestimated. Moreover， it introduces delayed policy updates and targeted policy smoothing to improve policy learning stability and exploratory power. Soft Actor-Critic（SAC）algorithm achieves efficient learning over a continuous action space by simultaneously learning a policy network and a value function network， combined with entropy regularization. The algorithm provides a useful learning framework for solving large-scale problems. However，deep reinforcement learning is difficult to solve complex tasks quickly and stably due to the limited exploration capability. Hierarchical reinforcement learning is an essential branch of deep reinforcement learning that focuses on solving large-scale problems. It is an effective solution to the problem of performance degradation of deep reinforcement learning when dealing with large-scale problems through time abstraction. However， there are still challenges such as the unreasonable setting of a priori knowledge and the inability to balance exploration and exploitation effectively. To address the above problems， Maximum Entropy Hierarchical Reinforcement Learning with Advantage-weighted Mutual Information Maximization（HRL-AMIM）algorithm is proposed. The method solves the sample clustering problem induced by the policy by weighted importance sampling of the advantage function and maximizing the average mutual information，adding internal rewards to emphasize the diversity of Options. Meanwhile，rewards are introduced into the maximum entropy reinforcement learning goal，which makes the policy more exploratory and better stable. In addition，the Option number annealing method not only reduces the impact of prior knowledge on performance but also balances the exploration and exploitation of the algorithm， achieving higher sample efficiency and faster learning speed. The HRL-AMIM algorithm is applied to the Mujoco task，and the experiments show that the algorithm is superior to the traditional deep reinforcement learning algorithms and similar hierarchical reinforcement learning algorithms in terms of performance and stability. Furthermore， the robustness and effectiveness of the algorithm are verified by ablation experiments and hyperparameter sensitivity experiments.",
        "DOI": "10.11897/SP.J.1016.2023.02066",
        "affiliation_name": "Soochow University",
        "affiliation_city": "Suzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A Multi-platform Cache Partitioning Method Based on Machine Learning",
        "paper_author": "Qiu J.F.",
        "publication": "Jisuanji Xuebao/Chinese Journal of Computers",
        "citied_by": "0",
        "cover_date": "2023-10-01",
        "Abstract": "The Last Level Cache （LLC） is shared in a multi-level-cache and multi-core CPU. If multi-programming randomly access LCC， it incurs accessing conflicts， and results in severe performance degradation. Currently，researchers focus on cache partitioning to reduce LLC conflicts. Existing cache partitioning methods adopt heuristic algorithm which needs uncertain steps to optimal performance. Thus it may bring up excessive partitioning overheads or even failure to converge in the partitioning process. To this end， we propose MLPart， a machine learning-based multi-platform adaptive cache partitioning method. With a few of running parameters，MLPart employs the decision tree and sequence-to-sequence models respectively to predict the performance of the different partitioning policies， and finally find out the best-performance partitioning policy with certain partition steps. In addition， the experiments show that even the same program runs in different computing platforms and result in the totally different runtime parameters. It is hard to ensure that trained models have sufficient generalization ability to be deployed in different platforms. MLPart also adopt transfer learning and tuning techniques to optimize the decision tree and sequence-to-sequence models for reducing the training overheads on other platforms. The experimental results illustrate that MLPart only needs a few of partition steps to find the optimal cache partitioning policy on Intel Xeon processor. The overall performance improvements using MLPart are highest and most stable in the KPart， CLITE and C&A.",
        "DOI": "10.11897/SP.J.1016.2023.02097",
        "affiliation_name": "Zhejiang University of Technology",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Machine learning for individualized prediction of hepatocellular carcinoma development after the eradication of hepatitis C virus with antivirals",
        "paper_author": "Minami T.",
        "publication": "Journal of Hepatology",
        "citied_by": "19",
        "cover_date": "2023-10-01",
        "Abstract": "Background & Aims: Accurate risk stratification for hepatocellular carcinoma (HCC) following the achievement of a sustained virologic response (SVR) is necessary for optimal surveillance. We aimed to develop and validate a machine learning (ML) model to predict the risk of HCC after achievement of an SVR in individual patients. Methods: In this multicenter cohort study, 1,742 patients with chronic hepatitis C who achieved an SVR were enrolled. Five ML models were developed including DeepSurv, gradient boosting survival analysis, random survival forest (RSF), survival support vector machine, and a conventional Cox proportional hazard model. Model performance was evaluated using Harrel's c-index and was externally validated in an independent cohort (977 patients). Results: During the mean observation period of 5.4 years, 122 patients developed HCC (83 in the derivation cohort and 39 in the external validation cohort). An RSF model, based on seven parameters at the achievement of an SVR, showed the best discriminative ability, with a c-index of 0.839 in the external validation cohort and a high discriminative ability when patients were categorized into three risk groups (p <0.001). Furthermore, using an app that has been made available online, this RSF model (termed the SMART model) enabled the generation of an individualized predictive curve for HCC occurrence for each patient. Conclusions: We developed and externally validated an RSF model with good predictive performance for the risk of HCC after an SVR. This model can be used for risk stratification and, subject to validation and cost-effectiveness analysis, could be applied to personalized surveilance approaches in each country. Impact and implications: A novel prediction model for hepatocellular carcinoma (HCC) occurrence in patients after hepatitis C virus eradication was developed using machine learning algorithms. This model, using seven commonly measured parameters, has been shown to have a good predictive ability for HCC development and could be used as part of a personalized surveillance approach. Further studies will be required before this model can be considered in surveilance policies tailored to the medical situation in each country.",
        "DOI": "10.1016/j.jhep.2023.05.042",
        "affiliation_name": "Kanto Central Hospital",
        "affiliation_city": "Tokyo",
        "affiliation_country": "Japan"
    },
    {
        "paper_title": "Is ChatGPT a “Fire of Prometheus” for Non-Native English-Speaking Researchers in Academic Writing?",
        "paper_author": "Hwang S.I.",
        "publication": "Korean Journal of Radiology",
        "citied_by": "32",
        "cover_date": "2023-10-01",
        "Abstract": "Large language models (LLMs) such as ChatGPT have garnered considerable interest for their potential to aid non-native English-speaking researchers. These models can function as personal, round-the-clock English tutors, akin to how Prometheus in Greek mythology bestowed fire upon humans for their advancement. LLMs can be particularly helpful for non-native researchers in writing the Introduction and Discussion sections of manuscripts, where they often encounter challenges. However, using LLMs to generate text for research manuscripts entails concerns such as hallucination, plagiarism, and privacy issues; to mitigate these risks, authors should verify the accuracy of generated content, employ text similarity detectors, and avoid inputting sensitive information into their prompts. Consequently, it may be more prudent to utilize LLMs for editing and refining text rather than generating large portions of text. Journal policies concerning the use of LLMs vary, but transparency in disclosing artificial intelligence tool usage is emphasized. This paper aims to summarize how LLMs can lower the barrier to academic writing in English, enabling researchers to concentrate on domain-specific research, provided they are used responsibly and cautiously.",
        "DOI": "10.3348/kjr.2023.0773",
        "affiliation_name": "Faculty of Medicine",
        "affiliation_city": "Okayama",
        "affiliation_country": "Japan"
    },
    {
        "paper_title": "A machine-learning approach to a mobility policy proposal",
        "paper_author": "Shulajkovska M.",
        "publication": "Heliyon",
        "citied_by": "5",
        "cover_date": "2023-10-01",
        "Abstract": "The objective of the URBANITE project is to design an open-data, open-source, smart-city framework to enhance the decision-making processes in European cities. The framework's basis is a robust and user-friendly simulation tool that is supplemented with several innovative service modules. One of the modules, a multi-output, machine-learning unit, is deployed on the simulation results, enabling city officials to more effectively analyse vast quantities of data, discern patterns and trends, and so facilitate advanced policy decisions. The city's decision makers define potential city scenarios, key performance indicators, and a utility function, while the module assists in identifying the policy that is best aligned with the stipulated constraints and preferences. One of the main improvements is a speeding up of the policy testing for the decision makers, reducing the time needed for one policy verification from 3 hours to around 10 seconds. The system was evaluated for Bilbao's Moyua area, where it suggested strategies that could result in a decrease in emissions of more than 5% CO2, NOx, PM in the selected area and a broader part of the city with a machine-learning accuracy of 91%. The system was therefore able to provide valuable insights into effective policies for restricting private traffic in specific districts and identifying the most advantageous times for these restrictions.",
        "DOI": "10.1016/j.heliyon.2023.e20393",
        "affiliation_name": "Institut \"Jožef Stefan\"",
        "affiliation_city": "Ljubljana",
        "affiliation_country": "Slovenia"
    },
    {
        "paper_title": "Antibiotic prescribing in remote versus face-to-face consultations for acute respiratory infections in primary care in England: an observational study using target maximum likelihood estimation",
        "paper_author": "Vestesson E.",
        "publication": "eClinicalMedicine",
        "citied_by": "12",
        "cover_date": "2023-10-01",
        "Abstract": "Background: The COVID-19 pandemic has led to an ongoing increase in the use of remote consultations in general practice in England. Although the evidence is limited, there are concerns that the increase in remote consultations could lead to more antibiotic prescribing. Methods: In this cohort study, we used patient-level primary care data from the Clinical Practice Research Datalink to estimate the association between consultation mode (remote versus face-to-face) and antibiotic prescribing in England for acute respiratory infections (ARI) between April 2021 and March 2022. Eligibility criteria were applied at both practice-level and patient-level. 400 practices in England were sampled at random and then 600,000 patients were randomly sampled from the eligible patients (whose sex was recorded). Consultations for acute respiratory infections were identified. All antibiotic prescriptions were included, with the exception of antituberculosis drugs and antileprotic drugs, as identified through chapter 5.1 of the British National Formulary. The CPRD Aurum data was linked to the COVID-19 ONS infection survey by region. All analyses were done at the individual level. Repeated consultations from the same patient within 7 days were grouped together. We used targeted maximum likelihood estimation, a causal machine learning method with adjustment for infection type and patient-level, clinician-level and practice-level factors. Findings: There were 45,997 ARI consultations (34,555 unique patients) within the study period, of which 28,127 were remote and 17,870 were face-to-face. For children, 48% of consultations were remote and, for adults, 66% were remote. For children, 42% of remote and 43% of face-to-face consultations led to an antibiotic prescription; the equivalent values for adults were 52% and 42%, respectively. After adjustment with TMLE, adults with a remote consultation had 23% (odds ratio [OR] 1.23, 95% CI: 1.18–1.29) higher chance of being prescribed antibiotics than if they had been seen face-to-face. We found no significant association between consultation mode and antibiotic prescribing in children (OR 1.04 95% CI: 0.98–1.11). Interpretation: The higher rates of antibiotic prescribing in remote consultations for adults are cause for concern. We see no significant difference in antibiotic prescribing between consultation mode for children. These findings should inform antimicrobial stewardship activities for health-care professionals and policy makers. Future research should examine differences in guideline-compliance between remote and face-to-face consultations to understand the factors driving antibiotic prescribing in different consultation modes. Funding: None.",
        "DOI": "10.1016/j.eclinm.2023.102245",
        "affiliation_name": "UCL Great Ormond Street Institute of Child Health",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Application of machine learning to assess people's perception of household energy in the developing world: A case of Nepal",
        "paper_author": "Bhattarai U.",
        "publication": "Energy and AI",
        "citied_by": "5",
        "cover_date": "2023-10-01",
        "Abstract": "Research on social aspects of energy and those applying machine learning (ML) is limited compared to the ‘hard’ disciplines such as science and engineering. We aim to contribute to this niche through this multidisciplinary study integrating energy, social science and ML. Specifically, we aim: (i) to compare the applicability of different ML models in household (HH) energy; and (ii) to explain people's perception of HH energy using the most appropriate model. We carried out cross-sectional survey of 323 HHs in a developing country (Nepal) and extracted 14 predictor variables and one response variable. We tested the performance of seven ML models: K-Nearest Neighbors (KNN), Multi-Layer Perceptron (MLP), Extra Trees Classifier (ETC), Random Forest (RF), Ridge Classifier (RC), Multinomial Regression–Logit (MR-L) and Probit (MR-P) in classifying people's responses. The models were evaluated against six metrics (confusion matrix, precision, f1 score, recall, balanced accuracy and overall accuracy). In this study, ETC outperformed all other models demonstrating a balanced accuracy of 0.79, 0.95 and 0.68 respectively for the Agree, Neutral and Disagree response categories. Results showed that, compared to conventional statistical models, data driven ML models are better in classifying people's perceptions. It was seen that the majority of the surveyed people from rural (68%) and semi-urban areas (67%) tend to resist energy changes due to economic constraints and lack of awareness. Interestingly, most (73%) of the urban residents are open to changes, but still resort to fuel-stacking because of distrust in the state. These grass-root level responses have strong policy implications.",
        "DOI": "10.1016/j.egyai.2023.100303",
        "affiliation_name": "Institute of Environmental Science and Meteorology, College of Science, University of the Philippines, Diliman",
        "affiliation_city": "Quezon City",
        "affiliation_country": "Philippines"
    },
    {
        "paper_title": "Using machine learning to expound energy poverty in the global south: Understanding and predicting access to cooking with clean energy",
        "paper_author": "Mukelabai M.D.",
        "publication": "Energy and AI",
        "citied_by": "9",
        "cover_date": "2023-10-01",
        "Abstract": "Efforts towards achieving high access to cooking with clean energy have not been transformative due to a limited understanding of the clean-energy drivers and a lack of evidence-based clean-energy policy recommendations. This study addresses this gap by building a high-performing machine learning model to predict and understand the mechanisms driving energy poverty - specifically access to cooking with clean energy. In a first-of-a-kind, the estimated cost of US$14.5 trillion to enable universal access to cooking with clean energy encompasses all the intermediate inputs required to build self-sufficient ecosystems by creating value-addition sectors. Unlike previous studies, the data-driven clean-cooking transition pathways provide foundations for shaping policy and building energy models that can transform the complex energy and cooking landscape. Developing these pathways is necessary to increase people's financial resilience to tackle energy poverty. The findings also show the absence of a linear relationship between electricity access and clean cooking - evidencing the need for a rapid paradigm shift to address energy poverty. A new fundamental approach that focuses on improving and sustaining the financial capacity of households through a systems approach is required so that they can afford electricity or fuels for cooking.",
        "DOI": "10.1016/j.egyai.2023.100290",
        "affiliation_name": "Cranfield University",
        "affiliation_city": "Cranfield",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Prediction of electrical power consumption in the household: fresh evidence from machine learning approach",
        "paper_author": "Krishnan L.",
        "publication": "Energy Efficiency",
        "citied_by": "1",
        "cover_date": "2023-10-01",
        "Abstract": "Household power consumption assists the power supply department in determining how much energy people use and whether there are any unusual power consumption patterns. This study offers an extendable experimental analytical framework and analyzes the data in a visual manner using the household level electric power usage dataset as an example. Concerns about energy shortages and pollution have increased, highlighting the need to make full use of the limited electrical power accessible. The impact of multinomial regression, ridge regression, lasso regression, and polynomial regression on various features are compared in this experiment. In the experimental dataset, the effect of the polynomial regression model outperforms that of the multinomial regression, ridge regression, and lasso regression models. The necessity of establishing programmes to encourage the use of more thermally efficient locally available materials in building among other policy suggestions are provided. Household power consumption helps the power supply department understand the power consumption of residents and whether there will be some abnormal power consumption phenomena. Taking the individual household electric power consumption dataset as an example, this paper establishes an extensible experimental analysis framework and analyzes the data in a visual way. In the experiment, the effects of multinomial regression, ridge regression, polynomial regression, and lasso regression models on different characteristics are compared. The experiment shows that the effect of the polynomial regression model is better than the multinomial regression, ridge regression, and lasso regression model in the experimental dataset.",
        "DOI": "10.1007/s12053-023-10155-z",
        "affiliation_name": "Periyar University",
        "affiliation_city": "Salem",
        "affiliation_country": "India"
    },
    {
        "paper_title": "A systematic review of the evaluation of agricultural policies: Using prisma",
        "paper_author": "Bastidas-Orrego L.M.",
        "publication": "Heliyon",
        "citied_by": "10",
        "cover_date": "2023-10-01",
        "Abstract": "Background: The food security of each country depends on agricultural development, which is sensitive to the implementation of agricultural public policies. These must evolve as new ruralities arise, with new phenomena, such as climate change, ecosystem services, changes in consumer preferences, globalization, sustainability and ecological awareness. Hence, of ex-ante and ex-post evaluations of agricultural policies, are important because they provide timely information to government entities. There are different methodologies for policy evaluation, which have evolved over time. Aims: This systematic review aims to identify manuscript that systematically review methodologies, policies and variables evaluated during the last 50 years to determine whether a policy has been efficient. To assess the quality of the included manuscript and to describe the measures and domains identified. Methods: EBSCO, Dialnet, SciELO, Scopus, Science Direct, Dimensions and Web of Science were searched. A total of 154 manuscript were identified, the review was finalized by reviewing the title, and abstract and the review was finalized by reviewing the title, abstract and full text, resolving disagreements. Of these 154 manuscripts, 37 met the criteria and were included in the analysis. PRISMA checklists were used to evaluate the methodology. Outcomes and results: It were found that there are few studies on the design of evaluation methodologies for agricultural policies in the literature. Research shows that the latest policy evaluation proposals present more complex methodologies involving tools such as machine learning and agent-based modelling (ABM). On the other hand, the issue of sustainability as a policy is seen in the agri-environmental policy evaluation. Conclusions and implications: The evolution of agricultural policy methodologies can be observed at the beginning with the use of quantitative methodologies, such as matrices, statistics and econometrics. With the emergence of new variables, such as agri-environmental variables, citizen participation and market opening, methodologies have become more comprehensive, combining qualitative and quantitative variables. Methodologies were identified that evaluate robust agricultural policies and others that focus on the evaluation of one or two policies. These studies are important for research that focuses not only on the evaluation of agricultural policies but also on their design and implementation processes.",
        "DOI": "10.1016/j.heliyon.2023.e20292",
        "affiliation_name": "Fundación Universitaria María Cano",
        "affiliation_city": "Medellin",
        "affiliation_country": "Colombia"
    },
    {
        "paper_title": "Learning to Solve Decision Problems Over Two Timescales: An Application to 5G Puncturing",
        "paper_author": "Randall M.",
        "publication": "Wireless Personal Communications",
        "citied_by": "0",
        "cover_date": "2023-10-01",
        "Abstract": "One of the biggest innovations on 5G and beyond is the support of three different services with particular delay and bandwidth requirements, such as Massive Machine Type Communications (MMTC), enhanced Mobile Broad Band (eMBB) and Ultra-Reliable Low Latency Communications (URLLC). In order to achieve these multiple service requirements, all users have to share resources over the 5G Orthogonal Frequency-Division Multiple Access (OFDMA) frame. One of the strategies proposed by the 5G standard is puncturing, which allows the scheduler to assign eMBB services on a timescale, and on a shorter timescale to preemptively overwrite part of the eMBB assignment when a URLLC user arrives. The optimization of puncturing poses a challenging problem: the optimal allocation depends on traffic arriving over different timescales, which forces the scheduler to make allocation decisions without knowledge of future users’ demands, all while having to satisfy several strong constraints. This kind of multiple timescales optimization with restrictions is also to be found in many interesting problems, such as energy management. We propose a learning mechanism where the system learns offline the optimal allocation according to the network state. This learned estimation is then used online to determine the optimal allocation. Through simulations, we verify that the proposed learning strategy provides results close to the optimal policy, improving state of the art proposals for puncturing schemes.",
        "DOI": "10.1007/s11277-023-10735-3",
        "affiliation_name": "Universidad de la Republica",
        "affiliation_city": "Montevideo",
        "affiliation_country": "Uruguay"
    },
    {
        "paper_title": "Large-scale automatic extraction of agricultural greenhouses based on high-resolution remote sensing and deep learning technologies",
        "paper_author": "Chen W.",
        "publication": "Environmental Science and Pollution Research",
        "citied_by": "4",
        "cover_date": "2023-10-01",
        "Abstract": "Widely used agricultural greenhouses are critical in the development of facility agriculture because of not only their huge capacity in food and vegetable supplies, but also their environmental and climatic effects. Therefore, it is important to obtain the spatial distribution of agricultural greenhouses for agricultural production, policy making, and even environmental protection. Remote sensing technologies have been widely used in greenhouse extraction mainly in small or local regions, while large-scale and high-resolution (~ 1-m) greenhouse extraction is still lacking. In this study, agricultural greenhouses in an important agricultural province (Shandong, China) are extracted by the combination of high-resolution remote sensing images from Google Earth and deep learning algorithm with high accuracy (94.04% for mean intersection over union over test set). The results demonstrated that the agricultural greenhouses cover an area of 1755.3 km2, accounting for 1.11% of the total province and 2.31% of total cultivated land. The spatial density map of agricultural greenhouses also suggested that the facility agriculture in Shandong has obviously regional aggregation characteristics, which is vulnerable in both environment and economy. The results of this study are useful and meaningful for future agriculture planning and environmental management.",
        "DOI": "10.1007/s11356-023-29802-0",
        "affiliation_name": "China University of Mining &amp; Technology, Beijing",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Leveraging artificial intelligence and mutual authentication to optimize content caching in edge data centers",
        "paper_author": "Marwan M.",
        "publication": "Journal of King Saud University - Computer and Information Sciences",
        "citied_by": "0",
        "cover_date": "2023-10-01",
        "Abstract": "Available online Edge data centers are designed to meet the stringent QoE requirements of delay-sensitive and computationally intensive services in Content Delivery Network (CDN) and 5G networks. The primary purpose of this paper was to formulate and solve the problem of optimizing many control variables jointly: (i) what contents to store by taking into consideration edge capacity, and (ii) what contents to recommend to each Internet of Everything (IoE) item, based on identity and access management (IAM). In reactive caching policy, we proposed a new Two-Factor Authentication (2FA) scheme founded upon the Elliptic Curve Cryptography (ECC) and one-way hash function for access control. More interestingly, we use Non-negative Matrix Factorization (NMF), Fuzzy C-Means (FCM), Random Forest (RF) and Pearson Correlation (PC) to improve the accuracy and latency of traditional data filtering models. The intelligent recommendation engine we propose is designed to be implemented by cloud for caching and prefetching contents at the edge. The experimental results validate the theoretical guarantees of the proposed solution and its ability to achieve significant performance gains compared to common baseline models.",
        "DOI": "10.1016/j.jksuci.2023.101742",
        "affiliation_name": "Ecole Nationale Supérieure d'Informatique et d'Analyse des Systèmes",
        "affiliation_city": "Rabat",
        "affiliation_country": "Morocco"
    },
    {
        "paper_title": "MF^2: Model-free reinforcement learning for modeling-free building HVAC control with data-driven environment construction in a residential building",
        "paper_author": "Wang M.",
        "publication": "Building and Environment",
        "citied_by": "8",
        "cover_date": "2023-10-01",
        "Abstract": "Reinforcement Learning (RL) has advanced energy-efficient control of building Heating, Ventilation and Air Conditioning (HVAC) systems. Constructing a suitable RL environment for buildings is a crucial challenge. Compared to widely-used simulation-based environments, data-driven approaches offer higher training efficiency but face convergence difficulties due to influential factors, limiting their current application. To explore data-driven construction of RL environments for building HVAC systems, this study proposes two strategies for controlling room temperature setpoints in a residential building. XGBoost and Long Short-Term Memory Network (LSTM) are trained for energy consumption and room temperature prediction. One strategy predicts parameters for on-off states, while the other for power-on states. The XGBoost models are integrated into an OpenAI Gym environment. The first strategy achieves 0.8634 R2 and 0.2423 Root Mean Squared Error (RMSE) for energy consumption prediction. The R2 of room air temperature models are approximately 0.99 and the RMSE are lower than 0.31. The second strategy achieves 0.9181 R2 and 0.1042 RMSE for energy consumption prediction and similar performance for room temperature prediction. Deep Q-learning (DQN) and Deep Deterministic Policy Gradient (DDPG) algorithms are separately trained using these environments. Results show that the first strategy fails to induce the correct training of RL models, while the second strategy successfully induces a useable DDPG model for controlling building HVAC systems but fails to induce a useable DQN model. We analyze the reasons behind these observations. Compared to the original room temperature setpoint method, the DDPG-based HVAC control logic achieves a 10.06% energy-saving effect while ensuring comfort.",
        "DOI": "10.1016/j.buildenv.2023.110816",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Intelligent Model for Avoiding Road Accidents Using Artificial Neural Network",
        "paper_author": "Kushwaha M.",
        "publication": "International Journal of Computers, Communications and Control",
        "citied_by": "2",
        "cover_date": "2023-10-01",
        "Abstract": "Accidents typically occurred on roads, resulting in significant societal losses. Road accidents are a worldwide issue that result in the loss of precious human lives and property. The purpose of this paper is to create an intelligent system-based on Machine Learning model for avoiding road accidents, as well as a system that effectively reduces road accidents severities. The Artificial Neural Network (ANN) algorithm, along with others such as Logistic Regression (LR), Decision Tree (DT), K-Nearest Neighbor (KNN), Support Vector Machine (SVM), Nave Bayes (NB), Stochastic Gradient Descent (SGD), Random Forest (RF), Gradient Boosting (GB), and AdaBoost, is used to create an intelligence system. Many driving collaborator procedures, installed in a few vehicles, assist drivers in avoiding vehicle crashes by providing early cautioning messages. The intelligence road crash avoidance system model is built on dataset of 29 columns and 1048575 rows. Preprocessing, feature selection, and feature extraction performed with the help of heat map and correlation matrix are used to select features. Linear Discriminant Analysis (LDA) is used for feature extraction. The testing dataset revealed that the proposed ANN method outperforms other algorithms with an accuracy of 0.856. Intelligent systems aid in the prevention of traffic accidents, which aids police officers and researchers in developing new policies.",
        "DOI": "10.15837/ijccc.2023.5.5317",
        "affiliation_name": "SRM Institute of Science and Technology",
        "affiliation_city": "Kattankulathur",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Empirical Policy Optimization for n-Player Markov Games",
        "paper_author": "Zhu Y.",
        "publication": "IEEE Transactions on Cybernetics",
        "citied_by": "14",
        "cover_date": "2023-10-01",
        "Abstract": "In single-agent Markov decision processes, an agent can optimize its policy based on the interaction with the environment. In multiplayer Markov games (MGs), however, the interaction is nonstationary due to the behaviors of other players, so the agent has no fixed optimization objective. The challenge becomes finding equilibrium policies for all players. In this research, we treat the evolution of player policies as a dynamical process and propose a novel learning scheme for Nash equilibrium. The core is to evolve one's policy according to not just its current in-game performance, but an aggregation of its performance over history. We show that for a variety of MGs, players in our learning scheme will provably converge to a point that is an approximation to Nash equilibrium. Combined with neural networks, we develop an empirical policy optimization algorithm, which is implemented in a reinforcement-learning framework and runs in a distributed way, with each player optimizing its policy based on own observations. We use two numerical examples to validate the convergence property on small-scale MGs, and a pong example to show the potential on large games.",
        "DOI": "10.1109/TCYB.2022.3179775",
        "affiliation_name": "Huawei Noah's Ark Lab",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong"
    },
    {
        "paper_title": "Factor-bridging algorithm for the prediction of job satisfaction: Developing country perspective",
        "paper_author": "Khan M.A.",
        "publication": "Journal of King Saud University - Computer and Information Sciences",
        "citied_by": "2",
        "cover_date": "2023-10-01",
        "Abstract": "Job satisfaction is crucial for both job seekers and employers. To ensure positive job satisfaction, companies must implement policies that consider employees’ perceptions of their work. This study utilizes machine learning and factor analysis to predict job satisfaction. Although, factor analysis has limitations, such as data quality, small sample size, and difficulty interpreting factors, machine learning algorithms can overcome these challenges. This study predicts job satisfaction using field data by combining factor analysis with machine learning algorithms. Factor loading values significantly impact classification algorithms such as Logistic Regression, Decision Trees, Support Vector Machines, and Random Forest. Especially, the Management Support, Equity, Non-Financial Compensation, and Financial Compensation feature variables are highly effective. They are used to predict job satisfaction with factor-1, factor-2, and factor-3 values. The Random Forest and Support Vector Machines algorithms have shown the importance of these values. The enhanced precision has been demonstrated visually to highlight the contrast when compared to the factor loading analysis and their corresponding eigenvalues.",
        "DOI": "10.1016/j.jksuci.2023.101743",
        "affiliation_name": "International Islamic University Chittagong",
        "affiliation_city": "Dhaka",
        "affiliation_country": "Bangladesh"
    },
    {
        "paper_title": "Identification of high-frequency trading: A machine learning approach",
        "paper_author": "Goudarzi M.",
        "publication": "Research in International Business and Finance",
        "citied_by": "3",
        "cover_date": "2023-10-01",
        "Abstract": "This study aims to develop a probabilistic model using machine learning techniques to identify high-frequency trading (HFT) based on order book data. The model enables precise intraday identifications, addressing the lack of a widely accepted framework for HFT identification and the inconsistencies arising from proxy indicators. Leveraging academic data, the model offers improved consistency and reproducibility for future HFT research. By incorporating fuzzy logic, the probabilistic model allows policymakers greater flexibility in shaping policies. The study utilises data from the BEDOFIH database of the French capital market and develops a robust classification model capable of accurately distinguishing HFT. Additionally, reverse engineering enhances the model's interpretability by transforming it into an interpretable regression tree without compromising its predictability. This research contributes to advancing HFT research, providing valuable insights, and offering a transferable methodology for identifying HFT in diverse market contexts.",
        "DOI": "10.1016/j.ribaf.2023.102078",
        "affiliation_name": "Università di Trento",
        "affiliation_city": "Trento",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "LJIR: Learning Joint-Action Intrinsic Reward in cooperative multi-agent reinforcement learning",
        "paper_author": "Chen Z.",
        "publication": "Neural Networks",
        "citied_by": "10",
        "cover_date": "2023-10-01",
        "Abstract": "Effective exploration is the key to achieving high returns for reinforcement learning. Agents must explore jointly in multi-agent systems to find the optimal joint policy. Due to the exploration problem and the shared reward, the policy-based multi-agent reinforcement learning algorithms face policy overfitting, which may lead to the joint policy falling into a local optimum. This paper introduces a novel general framework called Learning Joint-Action Intrinsic Reward (LJIR) for improving multi-agent reinforcement learners’ joint exploration ability and performance. LJIR observes agents’ state and joint actions to learn to construct an intrinsic reward online that can guide effective joint exploration. With the novel combination of Transformer and random network distillation, LJIR selects the novel states to give more intrinsic rewards, which help agents find the best joint actions. LJIR can dynamically adjust the weight of exploration and exploitation during training and keep the policy invariance finally. To ensure LJIR seamlessly adopts existing MARL algorithms, we also provide a flexible combination method for intrinsic and external rewards. Empirical results on the SMAC benchmark show that the proposed method achieves state-of-the-art performance in challenging tasks.",
        "DOI": "10.1016/j.neunet.2023.08.016",
        "affiliation_name": "Central South University",
        "affiliation_city": "Changsha",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Solar panel adoption among Mexican small and medium-sized commercial and service businesses",
        "paper_author": "Hancevic P.I.",
        "publication": "Energy Economics",
        "citied_by": "5",
        "cover_date": "2023-10-01",
        "Abstract": "We analyze the determinants of adoption of distributed solar photovoltaic systems, focusing on small and medium-sized commercial and service firms. We use monthly billing data that are perfectly matched with data from a novel survey that gathers information on electricity consumption, stock of electric equipment, and a rich set of firm characteristics in the Metropolitan Area of Aguascalientes, Mexico. Using an econometric model, we find evidence that a set of explanatory variables such as business characteristics, the economic sector, ownership status, stock and usage of equipment and appliances, presence of other solar technologies, and views about the use of renewable energy are important determinants of the probability of adoption of solar panel systems. Furthermore, using machine learning methods to identify the best predictors of solar adoption, we indirectly validate the theory-driven empirical model by assessing a large set of explanatory variables and selecting a subset of these variables. In addition, we investigate relevant cases where a priori solar panel adoption seems to be cost-effective but structural adoption barriers and adoption gaps might coexist for certain groups of electricity users. We also calculate the social cost savings and the avoided CO2 emissions. Finally, based on our results, we provide several policy implications and recommendations.",
        "DOI": "10.1016/j.eneco.2023.106979",
        "affiliation_name": "Centro de Investigación y Docencia Económicas",
        "affiliation_city": "Mexico",
        "affiliation_country": "Mexico"
    },
    {
        "paper_title": "Reinforcement and deep reinforcement learning-based solutions for machine maintenance planning, scheduling policies, and optimization",
        "paper_author": "Ogunfowora O.",
        "publication": "Journal of Manufacturing Systems",
        "citied_by": "34",
        "cover_date": "2023-10-01",
        "Abstract": "Systems and machines undergo various failure modes that result in machine health degradation, so maintenance actions are required to restore them back to a state where they can perform their expected functions. Since maintenance tasks are inevitable, maintenance planning is essential to ensure the smooth operations of the production system and other industries at large. Maintenance planning is a decision-making problem that aims at developing optimum maintenance policies and plans that help reduces maintenance costs, extend asset life, maximize their availability, and ultimately ensure workplace safety. Reinforcement learning is a data-driven decision-making algorithm that has been increasingly applied to develop dynamic maintenance plans while leveraging the continuous information from condition monitoring of the system and machine states. By leveraging the condition monitoring data of systems and machines with reinforcement learning, smart maintenance planners can be developed, which is a precursor to achieving a smart factory. This paper presents a literature review on the applications of reinforcement and deep reinforcement learning for maintenance planning and optimization problems. To capture the common ideas without losing touch with the uniqueness of each publication, taxonomies used to categorize the systems were developed, and reviewed publications were highlighted, classified, and summarized based on these taxonomies. Adopted methodologies, findings, and well-defined interpretations of the reviewed studies were summarized in graphical and tabular representations to maximize the utility of the work for both researchers and practitioners. This work also highlights the research gaps, key insights from the literature, and areas for future work.",
        "DOI": "10.1016/j.jmsy.2023.07.014",
        "affiliation_name": "University of Victoria",
        "affiliation_city": "Victoria",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Predicting return to work after traumatic brain injury using machine learning and administrative data",
        "paper_author": "Van Deynse H.",
        "publication": "International Journal of Medical Informatics",
        "citied_by": "1",
        "cover_date": "2023-10-01",
        "Abstract": "Background: Accurate patient-specific predictions on return-to-work after traumatic brain injury (TBI) can support both clinical practice and policymaking. The use of machine learning on large administrative data provides interesting opportunities to create such prognostic models. Aim: The current study assesses whether return-to-work one year after TBI can be predicted accurately from administrative data. Additionally, this study explores how model performance and feature importance change depending on whether a distinction is made between mild and moderate-to-severe TBI. Methods: This study used a population-based dataset that combined discharge, claims and social security data of patients hospitalized with a TBI in Belgium during the year 2016. The prediction of TBI was attempted with three algorithms, elastic net logistic regression, random forest and gradient boosting and compared in their performance by their accuracy, sensitivity, specificity and area under the receiver operator curve (ROC AUC). Results: The distinct modelling algorithms resulted in similar results, with 83% accuracy (ROC AUC 85%) for a binary classification of employed vs. not employed and up to 76% (ROC AUC 82%) for a multiclass operationalization of employment outcome. Modelling mild and moderate-to-severe TBI separately did not result in considerable differences in model performance and feature importance. The features of main importance for return-to-work prediction were related to pre-injury employment. Discussion: While clearly offering some information beneficial for predicting return-to-work, administrative data needs to be supplemented with additional information to allow further improvement of patient-specific prognose.",
        "DOI": "10.1016/j.ijmedinf.2023.105201",
        "affiliation_name": "Universitair Ziekenhuis Brussel",
        "affiliation_city": "Jette",
        "affiliation_country": "Belgium"
    },
    {
        "paper_title": "A holistic AI-based approach for pharmacovigilance optimization from patients behavior on social media",
        "paper_author": "Roche V.",
        "publication": "Artificial Intelligence in Medicine",
        "citied_by": "1",
        "cover_date": "2023-10-01",
        "Abstract": "In this paper, we propose a holistic AI-based pharmacovigilance optimization approach using patient's social media data. Instead of focusing on the detection and identification of Adverse Drug Events (ADE) in social media posts in single time points, we propose a holistic approach that looks at the evolution of different user behavior indicators in time. We examine various NLP-based indicators such as word frequency, semantic similarity, Adverse Drug Reactions mentions, and sentiment analysis. We introduce a classification approach to identify normal vs. abnormal time periods based on patient comments. This approach, along with user behavior indicators, can optimize the pharmacovigilance process by flagging the need for immediate attention and further investigation. We specifically focus on the Levothyrox® case in France, which sparked media attention due to changes in the medication formula and affected patient behavior on medical forums. For classification, we propose a deep learning architecture called Word Cloud Convolutional Neural Network (WC-CNN), trained on word clouds from patient comments. We evaluate different temporal resolutions and NLP pre-processing techniques, finding that monthly resolution and the proposed indicators can effectively detect new safety signals, with an accuracy of 75%. We have made the code open source, available via github.",
        "DOI": "10.1016/j.artmed.2023.102638",
        "affiliation_name": "NYU Abu Dhabi",
        "affiliation_city": "Abu Dhabi",
        "affiliation_country": "United Arab Emirates"
    },
    {
        "paper_title": "Blood-based colorectal cancer screening: are we ready for the next frontier?",
        "paper_author": "Wang C.P.",
        "publication": "The Lancet Gastroenterology and Hepatology",
        "citied_by": "2",
        "cover_date": "2023-10-01",
        "Abstract": "NA",
        "DOI": "10.1016/S2468-1253(23)00188-7",
        "affiliation_name": "NYU Grossman School of Medicine",
        "affiliation_city": "New York",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "GIS-based landslide susceptibility mapping of Western Rwanda: an integrated artificial neural network, frequency ratio, and Shannon entropy approach",
        "paper_author": "Nwazelibe V.E.",
        "publication": "Environmental Earth Sciences",
        "citied_by": "19",
        "cover_date": "2023-10-01",
        "Abstract": "The May 2nd and 3rd, 2023 landslide in Rwanda’s Western Province caused a devastating natural disaster, resulting in the tragic loss of 95 lives. Ngororero, Rubavu, Nyabihu, and Karongi were the worst-hit areas, as reported by Rwanda Broadcasting Agency (RBA). Such recurring disasters have posed significant challenges to the affected communities, requiring strong measures like susceptibility mapping to address their impact in the future. The literature review indicates that statistic and machine-learning susceptibility mapping efforts have been applied in the study region. However, these studies have not focused explicitly on localized scale studies of the western province; instead, they have mainly concentrated on examining the entire country. Using artificial neural networks (ANN), Shannon entropy (SE), and frequency ratio (FR), this paper aims to fill some gaps in the Rwandan landslide literature by integrating localized studies of the landslide susceptibility mapping (LSM) of the western province of Rwanda using the available higher data resolution. The LSM studies took 1157 landslide inventory locations and considered a broader range of landslide-conditioning factors compared to the previous studies on the region (distance from the road, aspect, elevation, slope degree, stream power index, normalized differential vegetation index, plan curvature, distance from the river, topographic wetness index, geology, and rainfall). In model training, 70% (810 points) of the landslide points underwent utilization, while the remaining 30% (347 points) served the purpose of model testing. The obtained area under the curve (AUC) values from model validation and testing provided reliable accuracy measures for the three LSM methods: ANN (AUC = 0.929 and 0.924), FR (AUC = 0.895 and 0.889), and SE (AUC = 0.768 and 0.750). Despite varying data handling, the models show that Rutsiro, Ngororero, and Karongi in Rwanda's Western Province have the highest landslide concentration. The relative importance of conditioning factors indicates that geology, rainfall, distance to the road, slope, and NDVI factors played a crucial role in landslides in the studied area. The slope can be stabilized by enhancing drainage, modifying slope angles, and implementing structural fortifications. It is hoped that the findings of this study will aid Rwandan policymakers and global researchers mitigate landslides and their dynamics.",
        "DOI": "10.1007/s12665-023-11134-4",
        "affiliation_name": "Universität Freiburg",
        "affiliation_city": "Freiburg im Breisgau",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Neural nets for sustainability conversations: modeling discussion disciplines and their impacts",
        "paper_author": "Pugh K.",
        "publication": "Neural Computing and Applications",
        "citied_by": "2",
        "cover_date": "2023-10-01",
        "Abstract": "We live in the age polarization, where conversations on matters of sustainability more often produce acrimony or stalemate than productive action. Better understanding conversation features and their impacts may lead to better innovation, solution-design, and ongoing collaboration. We describe a study to test alternate machine learning models for classifying six “discussion disciplines”, which are conversation features associated with rhetorical intent. The model providing the best outcome used the Bi-directional Encoder Representations from Transformers (BERT) layered with a Residual Network (ResNet). The training data were 1135 utterances from Maine aquaculture town hall-like meetings and similar conversations, which had been hand-coded for the discussion disciplines. In addition, we generated 300 phrases corresponding to three conversation outcomes: Intent-to-Act, Options-Generation, and Relationship-Building. We then used the trained model and information retrieval to classify a large corpus of 591 open-source transcripts, containing over 21,000 utterances. A binary logistic regression analysis showed that two discussion disciplines, “Inclusion” and “Courtesy,” had positive, statistically significant, impacts on Intent-to-act: a 10 percentage point increase in the share of the Inclusion or Courtesy yielded a 45% or 34% increase, respectively, in the likelihood of Intent-to-Act. This study shows the applicability of neural networks in modeling conversations and identifying the dialog acts that can provide measurable and predictable impact on conversation outcomes. Conversational intelligence can support a variety of human interactions, such as town halls, policy-deliberations, private–public partnerships, and sustainability teamwork.",
        "DOI": "10.1007/s00521-023-08819-z",
        "affiliation_name": "Franklin W. Olin College of Engineering",
        "affiliation_city": "Needham",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Could climate change exacerbate droughts in Bangladesh in the future?",
        "paper_author": "Rahman M.",
        "publication": "Journal of Hydrology",
        "citied_by": "6",
        "cover_date": "2023-10-01",
        "Abstract": "Droughts are one of the most complex, common, and catastrophic natural disasters, causing severe damage to agriculture and the economy. However, drought susceptibility must be measured and predicted in a systematic way, especially in light of potential climate change scenarios. This study aimed to predict current and future drought susceptibility in Bangladesh using historical climate data (1991–2020) and coupled model intercomparison project 6 data for three seasons: pre-monsoon, monsoon, and post-monsoon. We applied an advanced machine-learning algorithm of artificial neural network (ANN) with a genetic algorithm (GA) optimizer to predict drought-prone areas. Nine hydrological parameters–rainfall, temperature, humidity, cloud coverage, wind speed, sunshine, potential evapotranspiration, and solar radiation–were used to develop drought susceptibility maps. Receiver operating characteristic curves and statistical metrics were used to validate the models. The results of a multilayer perceptron ANN coupled with a GA-based optimizer showed that the relevant statistical measures for training and testing datasets were the root mean square error (RMSE = 0.127 and 0.160) and coefficient of determination (R2 = 0.967 and 0.949) for the pre-monsoon season, monsoon season (RMSE = 0.023 and 0.035; R2 = 0.998 and 0.997), and post-monsoon season (RMSE = 0.083 and 0.142; R2 = 0.986 and 0.959), respectively. Further, drought-prone areas in the baseline drought period of 2020 for pre-monsoon season represented 23.86%, 14.24%, 12.85%, 29.92%, and 19.13% of the total area, respectively; similarly, for monsoon corresponding values were 1.83%, 44.18%, 4.99%, 8.76%, and 40.24%; and for post-monsoon drought they were 24.43%, 20.94%, 16.04%, 37.79%, and 0.80% of the total landmass of Bangladesh. These results can help reduce future drought impacts and be of value in assisting policy responses in the country.",
        "DOI": "10.1016/j.jhydrol.2023.130096",
        "affiliation_name": "Bangladesh University of Professionals",
        "affiliation_city": "Dhaka",
        "affiliation_country": "Bangladesh"
    },
    {
        "paper_title": "Learning and Leveraging Conventions in the Design of Haptic Shared Control Paradigms for Steering a Ground Vehicle",
        "paper_author": "Izadi V.",
        "publication": "International Journal of Control, Automation and Systems",
        "citied_by": "0",
        "cover_date": "2023-10-01",
        "Abstract": "The main objective of this paper is to establish a framework to study the co-adaptation between humans and automation systems in a haptic shared control framework. We specifically used this framework to design control transfer strategies between humans and automation systems to resolve a conflict when co-steering a semi-automated ground vehicle. The proposed framework contains three main parts. First, we defined a modular structure to separate partner-specific strategies from task-dependent representations and use this structure to learn different co-adaption strategies. In this structure, we assume the human and automation steering commands can be determined by optimizing cost functions. For each agent, the costs are defined as a combination of a set of hand-coded features and vectors of weights. The hand-coded features can be selected to describe task-dependent representations. On the other hand, the weight distributions over these features can be used as a proxy to determine the partner-specific conventions. Second, to leverage the learned co-adaptation strategies, we developed a map connecting different strategies to the outputs of human-automation interactions by employing a collaborative-competitive game concept. Finally, using the map, we designed an adaptable automation system capable of co-adapting to human driver’s strategies. Specifically, we designed an episode-based policy search using the deep deterministic policy gradients technique to determine the optimal weights vector distribution of automation’s cost function. The simulation results demonstrate that the handover strategies designed based on co-adaption between human and automation systems can successfully resolve a conflict and improve the performance of the human automation teaming.",
        "DOI": "10.1007/s12555-022-0509-6",
        "affiliation_name": "William States Lee College of Engineering",
        "affiliation_city": "Charlotte",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Assessing the potential impact of applying a higher sensitivity test to selected cattle populations for the control of bovine tuberculosis in England",
        "paper_author": "Romero M.P.",
        "publication": "Preventive Veterinary Medicine",
        "citied_by": "2",
        "cover_date": "2023-10-01",
        "Abstract": "Bovine tuberculosis (bTB) continues to be the costliest, most complex animal health problem in England. The effectiveness of the test-and-slaughter policy is hampered by the imperfect sensitivity of the surveillance tests. Up to half of recurrent incidents within 24 months of a previous one could have been due to undetected infected cattle not being removed. Improving diagnostic testing with more sensitive tests, like the interferon (IFN)-gamma test, is one of the government's top priorities. However, blanket deployment of such tests could result in more false positive results (due to imperfect specificity), together with logistical and cost-efficiency challenges. A targeted application of such tests in higher prevalence scenarios, such as a subpopulation of high-risk herds, could mitigate against these challenges. We developed classification machine learning algorithms (using 80% of 2012–2019 bTB surveillance data as the training set) to evaluate the deployment of IFN-gamma testing in high-risk herds (i.e. those at risk of an incident in England) in two testing data sets: i) the remaining 20% of 2012–19 data, and ii) 2020 bTB surveillance data. The resulting model, classification tree analysis, with an area under a receiver operating characteristic (ROC) curve (AUC) > 95, showed a 73% sensitivity and a 97% specificity in the 2012–2019 test dataset. Used on 2020 data, it predicted eight percent (3 510 of 41 493) of eligible active herds as at-risk of a bTB incident, the majority of them (66% or 2 328 herds) experiencing at least one. Whilst all predicted at-risk herds could have preventive measures applied, the additional application of IFN-gamma test in parallel interpretation to the statutory skin test, if the risk materialises, would have resulted in 8 585 additional IFN-gamma reactors detected (a 217% increase over the 2 710 IFN-gamma reactors already detected by tests carried out). Only 18% (330 of 1 819) of incidents in predicted high-risk herds had the IFN-gamma test applied in 2020. We therefore conclude that this methodology provides a better way of directing the application of the IFN-gamma test towards the high-risk subgroup of herds. Classification tree analysis ensured the systematic identification of high-risk herds to consistently apply additional measures in a targeted way. This could increase the detection of infected cattle more efficiently, preventing recurrence and accelerating efforts to achieve eradication by 2038. This methodology has wider application, like targeting improved biosecurity measures in avian influenza at-risk farms to limit damage to the industry in future outbreaks.",
        "DOI": "10.1016/j.prevetmed.2023.106004",
        "affiliation_name": "Animal and Plant Health Agency",
        "affiliation_city": "Addlestone",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "An integrated framework for residential layout designs: Combining parametric modeling, neural networks, and multi-objective optimization for outdoor activity space optimization",
        "paper_author": "Hu Z.",
        "publication": "Alexandria Engineering Journal",
        "citied_by": "5",
        "cover_date": "2023-10-01",
        "Abstract": "The quality of life of residents and community vitality is significantly enhanced by outdoor activity spaces within residential areas. Existing residential layout designs primarily focus on a restricted set of predefined layouts and most outdoor evaluation indices are limited to thermal comfort, thereby leading to an incomplete evaluation system. Furthermore, there is a scarcity of studies offering a comprehensive design process, from parameterized generation to performance evaluation. As such, there is a pressing need for an all-encompassing design framework that investigates a broad array of layout possibilities while providing multidimensional evaluations based on diverse metrics, thereby augmenting the accuracy and intelligence of residential layout designs. This research introduces a building layout generation framework that integrates parametric modeling (PD), neural network modeling (ANN), and multi-objective optimization (MOO) methods. The framework utilizes parametric design methods to construct a geometric model that adheres to building codes. Following this, it builds separate neural network models for three key metrics: sky view factor (SVF), sunshine duration, and noise, this approach is aimed at accelerating the evaluation process. This alternative model serves as the objective function for multi-objective optimization, identifying the optimal building layout that balances multiple metrics. Additionally, the framework utilizes the ideal point method to optimize the final Pareto solution set, thereby informing designers' and planners' program selections. Case studies of this framework indicate that the residential layouts generated can adhere to building codes while effectively balancing the three crucial metrics. Relative to the conventional modeling-evaluation process, this framework augments efficiency substantially. The design framework presented herein will offer comprehensive decision support for policy makers to optimize residential layout designs, further fostering the harmonious coexistence of people and cities, while advancing intelligent building design and sustainable urban development.",
        "DOI": "10.1016/j.aej.2023.08.049",
        "affiliation_name": "Sichuan Agricultural University",
        "affiliation_city": "Ya'an",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Dynamic modeling of topsoil organic carbon and its scenarios forecast in global Mollisols regions",
        "paper_author": "Bao Y.",
        "publication": "Journal of Cleaner Production",
        "citied_by": "5",
        "cover_date": "2023-10-01",
        "Abstract": "Global Mollisols regions play crucial roles in maintaining world food security, performing ecosystem services and global carbon cycling. Little is known about the spatial and temporal variability of soil organic carbon (SOC), future trends, and the driving forces of changes in each Mollisols region. The aim of this work is to trace the SOC dynamics during the 2000–2020 period and forecast its dynamics for 2020–2040 period (every 5 years) across the global Mollisols regions, highlighting the driving factors for these changes. We used bare soil images and environmental covariates to construct a robust prediction model of SOC using machine learning. The best model was used to forecast future changes in SOC content under different emission scenarios. Also, the role of different predictor variables in predicting and driving SOC changes is indicated with the help of the shapley value algorithm. Results show that SOC content in the current global Mollisols regions is 26.45 g kg−1 in Ukraine, 21.34 g kg−1 in Northeast China, 21.05 g kg−1 in the USA and 15.59 g kg−1 in Argentina. However, they have been decreasing since 2001 and showed a slowing trend, with the highest and lowest rates of 11.45% in Ukraine and 7.33% in the USA, respectively. The forecast under different climate scenarios shows a slight decrease in SOC content within a range of 0.46%–1.22%. Among them, SSP 245 is more consistent with the current trend of SOC content, while SSP585 indicates the greatest loss of SOC content. Results revealed that the driving factors for SOC prediction in descending order were meteorologic, parent material, terrain and vegetation. All soil and meteorology variables contributed by 71%, while soil texture fractions and temperature were the top 20% of these variables. The main factors driving SOC changes from high to low latitudes are temperature, precipitation, and vegetation. It can be concluded that the temporal transfer model is a powerful tool to trace and forecast the dynamics of SOC in Mollisols regions at the global scale, enabling a clear understanding and ranking the factors affecting this process under diverse climate conditions.",
        "DOI": "10.1016/j.jclepro.2023.138544",
        "affiliation_name": "Aerospace Information Research Institute",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Offline Deep Reinforcement Learning and Off-Policy Evaluation for Personalized Basal Insulin Control in Type 1 Diabetes",
        "paper_author": "Zhu T.",
        "publication": "IEEE Journal of Biomedical and Health Informatics",
        "citied_by": "7",
        "cover_date": "2023-10-01",
        "Abstract": "Recent advancements in hybrid closed-loop systems, also known as the artificial pancreas (AP), have been shown to optimize glucose control and reduce the self-management burdens for people living with type 1 diabetes (T1D). AP systems can adjust the basal infusion rates of insulin pumps, facilitated by real-time communication with continuous glucose monitoring. Deep reinforcement learning (DRL) has introduced new paradigms of basal insulin control algorithms. However, all the existing DRL-based AP controllers require extensive random online interactions between the agent and environment. While this can be validated in T1D simulators, it becomes impractical in real-world clinical settings. To this end, we propose an offline DRL framework that can develop and validate models for basal insulin control entirely offline. It comprises a DRL model based on the twin delayed deep deterministic policy gradient and behavior cloning, as well as off-policy evaluation (OPE) using fitted Q evaluation. We evaluated the proposed framework on an in silico dataset generated by the UVA/Padova T1D simulator, and the OhioT1DM dataset, a real clinical dataset. The performance on the in silico dataset shows that the offline DRL algorithm significantly increased time in range while reducing time below range and time above range for both adult and adolescent groups. Then, we used the OPE to estimate model performance on the clinical dataset, where a notable increase in policy values was observed for each subject. The results demonstrate that the proposed framework is a viable and safe method for improving personalized basal insulin control in T1D.",
        "DOI": "10.1109/JBHI.2023.3303367",
        "affiliation_name": "University College London",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Social media-based social–psychological community resilience analysis of five countries on COVID-19",
        "paper_author": "Valinejad J.",
        "publication": "Journal of Computational Social Science",
        "citied_by": "2",
        "cover_date": "2023-10-01",
        "Abstract": "Community resilience (CR) has been studied as an indicator to measure how well a given community copes with and recovers from a given disaster. Social–psychological community resilience (SPCR) has been used as a basis to determine public policy directions based on priority. Although the impact of the COVID-19 has been serious all over the world and interferes every aspect of our daily life, some countries have handled this disaster better than others due to their different disaster management policies and perceptions about the disaster. In this work, we are interested in measuring and analyzing SPCR through social media information in five different countries which can reflect different disaster management policies and perceptions toward the COVID-19. In the literature, measuring SPCR has been discussed, but the key attributes have not been agreed upon. We propose to use two attributes for measuring SPCR, i.e., community wellbeing (CW) and community capital (CC), because social and psychological resilience can be the firm basis for a community to be restored and reinvented into the so-called transformative community to ensure sustainability in the future generation. We use Tweeter data and investigate how each country shows different trends of SPCR in response to real and fake tweets generated during a COVID-19 period using machine learning and text-mining tools. We employ tweets generated in Australia (AUS), Singapore (SG), Republic of Korea (ROK), the United Kingdom (UK), and the United States (US), during March–November 2020 and measure the SPCR of each country and its associated attributes for analyzing the overall trends. Our results show that ROK among the five countries in our study has the highest level in CW, CC, and the resulting SPCR on real tweets reflecting reality, a result that matches well with the fact that ROK is resilient to COVID-19 during March–November 2020. Further, our results indicate that SPCR on real tweets is up to 80% higher than SPCR on fake tweets, suggesting that a much stronger community resilience may be achieved on real tweets. Finally, our results show that there is a negative correlation between SPCR values on fake and real tweets overall when considering all the tweets of the five countries to derive the overall trends. However, for each country, we observe a different correlation, either positive or negative, depending on each country. This implies that there should be further investigation of analyzing SPCR by considering unique cultural and national characteristics of each country.",
        "DOI": "10.1007/s42001-023-00220-z",
        "affiliation_name": "Virginia Tech College of Engineering",
        "affiliation_city": "Blacksburg",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Learning reward machines: A study in partially observable reinforcement learning",
        "paper_author": "Toro Icarte R.",
        "publication": "Artificial Intelligence",
        "citied_by": "6",
        "cover_date": "2023-10-01",
        "Abstract": "Reinforcement Learning (RL) is a machine learning paradigm wherein an artificial agent interacts with an environment with the purpose of learning behaviour that maximizes the expected cumulative reward it receives from the environment. Reward machines (RMs) provide a structured, automata-based representation of a reward function that enables an RL agent to decompose an RL problem into structured subproblems that can be efficiently learned via off-policy learning. Here we show that RMs can be learned from experience, instead of being specified by the user, and that the resulting problem decomposition can be used to effectively solve partially observable RL problems. We pose the task of learning RMs as a discrete optimization problem where the objective is to find an RM that decomposes the problem into a set of subproblems such that the combination of their optimal memoryless policies is an optimal policy for the original problem. We show the effectiveness of this approach on three partially observable domains, where it significantly outperforms A3C, PPO, and ACER, and discuss its advantages, limitations, and broader potential.1",
        "DOI": "10.1016/j.artint.2023.103989",
        "affiliation_name": "Centro Nacional de Inteligencia Artificial",
        "affiliation_city": "Santiago",
        "affiliation_country": "Chile"
    },
    {
        "paper_title": "Estimating city-wide hourly bicycle flow using a hybrid LSTM MDN",
        "paper_author": "Myhrmann M.S.",
        "publication": "Transportation Research Part A: Policy and Practice",
        "citied_by": "5",
        "cover_date": "2023-10-01",
        "Abstract": "Cycling can reduce greenhouse gas emissions and air pollution and increase public health. Hence, policymakers in cities worldwide seek to improve bicycle mode shares. Efforts to increase the bicycle's mode share involve many measures, one of them being the improvement of cycling safety often requiring an analysis of the factors surrounding accidents. However, meaningful analysis of cycling safety requires accurate bicycle flow data that are generally sparse or only available at the aggregate level. Therefore, safety engineers often rely on aggregated variables or calibration factors that fail to account for variations in the cycling traffic relevant to policymaking. This paper illustrates how machine learning can support policy analysis by delivering detailed bicycle flow predictions. The illustration applies a Deep Learning approach, the Long Short-Term Memory Mixture Density Network (LSTMMDN), to estimate hourly bicycle flow in Copenhagen, conditional on weather, temporal and road conditions at the segment level. The method addresses some shortcomings in the calibration factor method resulting in 66–77% more accurate bicycle traffic estimates. To quantify the impact of more accurate bicycle traffic estimates in cycling safety analysis, we test the effect of different flow estimates in a bicycle crash risk model, i.e. the models are identical except for the exposure variables. One model is estimated using the LSTMMDN estimates, one using the calibration-based estimates, and one using yearly mean traffic estimates. The results show that investing in more advanced methods for obtaining bicycle volume estimates can improve the quality of safety analyses and other performance measures.",
        "DOI": "10.1016/j.tra.2023.103783",
        "affiliation_name": "Technical University of Denmark",
        "affiliation_city": "Lyngby",
        "affiliation_country": "Denmark"
    },
    {
        "paper_title": "Prediction of suicidal ideation in children and adolescents using machine learning and deep learning algorithm: A case study in South Korea where suicide is the leading cause of death",
        "paper_author": "Shin S.",
        "publication": "Asian Journal of Psychiatry",
        "citied_by": "4",
        "cover_date": "2023-10-01",
        "Abstract": "Background: Korea has the highest suicide rate among Organisation for Economic Co-operation and Development (OECD) countries. Consequently, central and local governments and private organizations in Korea cooperate in promoting various suicide prevention projects to actively respond to suicide problems. Machine learning has been used to predict suicidal ideation in the fields of health and medicine but not from a social science perspective. Objective: Since suicidal ideation is a major predictor of suicide attempts, being able to anticipate and mitigate it helps prevent suicide. Therefore, this study presents a data-based analysis method for predicting suicidal thoughts quickly and effectively and suggests countermeasures against the causes of suicidal thoughts. Participants and methods: To predict early signs of suicidal ideation in children and adolescents, big data collected for approximately 4 years (from 2017 to 2020) from the Korea Youth Policy Institute (NYPI) were used. To accurately predict suicidal ideation, supervised ma- chine learning classification algorithms such as logistic regression, random forest, XGBoost, multilayer perceptron (MLP), and convolutional neural network (CNN) were used. Results: Using CNN, suicidal ideation was predicted with an accuracy of approximately 90 %. The logistic regression results showed that sadness and depression increased suicidal thoughts by more than 25 times, and anxiety, loneliness, and experience of abusive language increased suicidal thoughts by more than three times. Conclusions: Machine learning and deep learning approaches have the potential to predict and respond to suicidal thoughts in children, adolescents, and the general population, as well as help respond to the suicide crisis by preemptively identifying the cause.",
        "DOI": "10.1016/j.ajp.2023.103725",
        "affiliation_name": "Incheon National University",
        "affiliation_city": "Incheon",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "Correction to: Healthcare framework for identification of strokes based on versatile distributed computing and machine learning (Soft Computing, (2023), 27, 20, (15397-15405), 10.1007/s00500-023-09002-1)",
        "paper_author": "Xiao Y.",
        "publication": "Soft Computing",
        "citied_by": "0",
        "cover_date": "2023-10-01",
        "Abstract": "The affiliation of the authors Yineng Xiao and Zhao Liu as incorrectly published. The correct affiliation is copied below: Yineng Xiao: 1. The Global Intellectual Property Institute, Nanjing University, Suzhou, China 2. Advanced Institute of Information Technology, Peking University, Hangzhou, China Zhao Liu: School of Public Policy and Management, University of Chinese Academy of Sciences, Beijing, China The original article has been corrected.",
        "DOI": "10.1007/s00500-023-09095-8",
        "affiliation_name": "Nanjing University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Retraction notice to “Heart disease classification models from optical device-based electrocardiogram signals using machine learning algorithms” [Optik 271 (2022) 170176](S0030402622014346)(10.1016/j.ijleo.2022.170176)",
        "paper_author": "Goyal S.",
        "publication": "Optik",
        "citied_by": "0",
        "cover_date": "2023-10-01",
        "Abstract": "This article has been retracted: please see Elsevier Policy on Article Withdrawal (https://www.elsevier.com/locate/withdrawalpolicy). This article has been retracted at the request of the Editor-in-Chief. The journal was alerted to a PubPeer post, stating that the article contains several tortured phrases that make some passages hard to parse. The PubPeer post also stated that the article contains several references that are likely to be unreliable, as detailed here: PubPeer - Heart disease classification models from optical device-base. The journal requested the authors to provide an explanation of these concerns, as well as raw data and any evidence that the authors felt would be useful, but the corresponding author's response did not satisfactorily address the editor's concerns. The editor-in-chief reviewed the case and decided to retract the article.",
        "DOI": "10.1016/j.ijleo.2023.171227",
        "affiliation_name": "Bhagwan Parshuram Institute of Technology",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India"
    },
    {
        "paper_title": "NeuroCrossover: An intelligent genetic locus selection scheme for genetic algorithm using reinforcement learning",
        "paper_author": "Liu H.",
        "publication": "Applied Soft Computing",
        "citied_by": "11",
        "cover_date": "2023-10-01",
        "Abstract": "Researchers have been studying genetic algorithms (GAs) extensively in recent decades and employing them to address extremely challenging combinatorial optimization problems (COPs). Although GAs achieve superior performance, they are less efficient because most GAs are designed manually without intelligent parameter configuration to support scalable problem-solving strategies and learnable evolutionary operators. To address this issue, machine learning (ML) techniques have been integrated with GAs for operator and parameter selection, however, few studies have focused on intelligent genetic locus selection for influential operators in GAs. To fill this gap, this paper proposes an intelligent genetic locus selection algorithm that serves as the foundation of parameter configuration for critical operators. With the established framework, the Cross Information Synergistic Attention (CISA) model and the n-step proximal policy optimization (PPO) have been utilized to intelligently select the appropriate genetic locus for the most influential phase, i.e., crossover, during the evolutionary process. The proposed NeuroCrossover algorithm is validated on extensive COPs, including the traveling salesman problem, capacitated vehicle routing problem, and bin packing problem. The results demonstrate the efficiency and effectiveness of our algorithm, which outperforms other methods in terms of solution quality, convergence speed, and generalization. For instance, with the CISA, the average percentage gaps of our algorithm are 3.64% and 6.38% for instances in TSPLIB and CVRPLIB, respectively, obtaining gains of about 0.47% and 1.20% compared to those of GA. The proposed algorithm provides a novel solution to lead GAs to an efficient search and improve their scalability and learnability.",
        "DOI": "10.1016/j.asoc.2023.110680",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Enhanced open biomass burning detection: The BranTNet approach using UAV aerial imagery and deep learning for environmental protection and health preservation",
        "paper_author": "Wang H.",
        "publication": "Ecological Indicators",
        "citied_by": "10",
        "cover_date": "2023-10-01",
        "Abstract": "Open biomass burning (OBB) in agriculture presents a significant and well-documented challenge, posing severe consequences for both environmental and human health. OBB releases air pollutants that degrade air quality and contribute to climate change, leading to premature deaths in regions with high concentrations of open crop straw burning (OCSB) emissions. Although policies aimed at prohibiting OBB are in place, the efficacy of these regulations in mitigating OCSB emissions remains ambiguous. Consequently, early prevention and monitoring of open biomass combustion are imperative for environmental preservation. Traditional monitoring techniques, reliant on fixed-position cameras, are constrained by their location and monitoring intensity, making concealed fire recognition a complex problem. To address this limitation and monitor the human living environment more flexibly and accurately, we propose a new method to identify straw fires in UAV Aerial Image Using CNN Branch Reinforce Transformer which named BranTNet, enabling early detection and rapid response to crop straw fires. By integrating computer vision technology and deep learning algorithms, straw fires in UAV-acquired aerial survey images can be detected and categorized. In the realm of artificial intelligence algorithms, we skillfully merge convolution and attention mechanisms, harnessing the full potential of both methodologies. Moreover, we seamlessly incorporate transfer learning, skillfully unifying self-training convolution modules with pre-trained transformer modules. This strategic amalgamation not only minimizes time costs but also ensures optimal experimental outcomes. Regarding data, we meticulously collected a substantial number of authentic samples, ensuring the sufficiency of our experimental dataset. The experimental results demonstrate that our proposed method exhibits exceptional accuracy and robustness in detecting and identifying straw fires in UAV aerial survey images. Our approach outperforms the use of convolution or attention mechanisms alone. By integrating this approach with drone technology, we unlock the potential for developing more versatile and precise monitoring solutions, expanding the application of drones to diverse domains. This progress contributes significantly to the early detection and prevention of crop straw fires, fundamentally reducing environmental pollution, curbing carbon emissions, and advancing the cause of carbon neutrality. This innovative technique for monitoring and preventing OBB holds substantial promise in mitigating the adverse effects of OBB on the environment and human health.",
        "DOI": "10.1016/j.ecolind.2023.110788",
        "affiliation_name": "College of Medicine and Biological Information Engineering, Northeastern University",
        "affiliation_city": "Shenyang",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Market index price prediction using Deep Neural Networks with a Self-Similarity approach",
        "paper_author": "Mendoza C.",
        "publication": "Applied Soft Computing",
        "citied_by": "5",
        "cover_date": "2023-10-01",
        "Abstract": "Stock indexes are of vital importance to understand financial and economic markets of sectors and nations. Given the importance of market indexes, researchers, investors, and policy makers are continuously working to improve models to forecast market movements; small improvements in modeling have the potential for large financial gains. To this end, machine learning approaches have gained in popularity as both software and hardware performance have increased. In this work, we apply machine learning applications to the S&P 500, DAX, AEX and the SMI indexes to improve forecasting performance. In particular, we take advantage of the fractal and Self-Similarity behaviors that exist in the time series of these indexes using simple Recurrent Neural Network, Multilayer Perceptron, and Long-Short Term Memory architectures. We apply the architecture using 60-, 30-, and 15-min windows of daily series for each approach and index. The results indicate that for the S&P 500 the proposed self-similarity models outperform all base approaches. However, results are not uniform across all models and indexes. Overall, performance differences across models and indexes are presented. To check the difference between the forecasts of the self-similarity and base models, Model Confidence Set was used.",
        "DOI": "10.1016/j.asoc.2023.110700",
        "affiliation_name": "Robert Morris University",
        "affiliation_city": "Pittsburgh",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Investigating the impact of pretreatment strategies on photocatalyst for accurate CO<inf>2</inf>RR productivity quantification: A machine learning approach",
        "paper_author": "Liu Y.",
        "publication": "Chemical Engineering Journal",
        "citied_by": "8",
        "cover_date": "2023-10-01",
        "Abstract": "The photocatalytic carbon dioxide reduction reaction (CO2RR) process is one of the most attractive approaches to mitigate the energy crisis by producing a series of fuels such as methane, methanol, formic acid, acetic acid, etc. Unfortunately, these organic products sometimes happen to be the impurity residuals left on photocatalyst surfaces during large-scale industrial manufacture and artificial contamination during pretreatment processes, which will lead to incorrect quantification of CO2RR productivity or even give an inadequate deduction of the reaction mechanism. Through the machine learning approach (random forest), we further quantified the relative importance of each pretreatment in contributing to the clean TiO2 surfaces, where light irradiation gives the highest contribution (49.7%) among all considered approaches, in contrast to the ultrasound bath cleaning (8.7%). Some suggestions and notes were further provided to point out the limitations and possible improvements for specific techniques. Unexpectedly, even a moderate HCOOH production rate of 100 μmol h−1 g−1 over TiO2-based CO2RR catalysts is speculated to suffer from the non-negligible contamination interference, where a nearly 100% of HCOOH quantification bias is introduced to give the risk to precisely capture the CO2RR productivity feature.",
        "DOI": "10.1016/j.cej.2023.145255",
        "affiliation_name": "Tongji University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A machine learning approach to analyzing spatiotemporal impacts of mobility restriction policies on infection rates",
        "paper_author": "Young Song A.",
        "publication": "Transportation Research Part A: Policy and Practice",
        "citied_by": "0",
        "cover_date": "2023-10-01",
        "Abstract": "This study analyzed the impact of a range of policies that restrict travel accessibility and mobility on infection rates for the original strain of the virus during the first year of the COVID-19 crisis. We constructed a multidimensional dataset and developed an effective data-driven predictive model to investigate causality between a policy, mobility, and an infection, drawing upon spatiotemporal perspectives. The multidimensional dataset included daily infections, daily restriction policies, and daily and hourly multimodal travel patterns. We quantified and normalized the dataset in relation to pre-COVID-19 policies and travel activities. A machine learning framework that integrated principal component analysis (PCA) and a Gaussian process regression (GPR) was formulated to evaluate the effectiveness of mobility restriction policies and their optimal implementation time during the infancy stage of the pandemic. In a case study, we selected Seoul in South Korea and Sydney in Australia for model calibrations and validations. Both countries deployed comprehensive urban restriction policies during the worldwide pandemic. The proposed model produced better performance than diverse non-parametric and parametric models to estimate the daily number of infections in the two areas. Furthermore, we discovered effective restriction policies and the best times to implement them to minimize the number of acquired COVID-19 cases by analyzing coefficients in PCA and GPR kernel functions. Our finding has far-reaching policy implications. First, the proposed methods can be used for formulating restriction policies for other regions with diverse population densities as the chosen cities in this case study. Second, our finding contributes to evidence-based policymaking.",
        "DOI": "10.1016/j.tra.2023.103795",
        "affiliation_name": "Yonsei University Mirae Campus",
        "affiliation_city": "Wonju",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "Light-driven simultaneous water purification and green energy production by photocatalytic fuel cell: A comprehensive review on current status, challenges, and perspectives",
        "paper_author": "Ni J.",
        "publication": "Chemical Engineering Journal",
        "citied_by": "51",
        "cover_date": "2023-10-01",
        "Abstract": "Solar energy is a crucial source that sustains all life and activities on Earth. Photocatalysis based on solar energy is an exciting technology with great potential for solving complex problems, including environmental pollution, energy crisis, and global warming. Over the past few decades, significant efforts have been made in developing photocatalytic techniques. One particularly promising area is the use of photocatalytic fuel cells (PFCs) for light-driven wastewater remediation and energy generation, which has gained considerable attention due to its ability to simultaneously remove organic pollutants and generate electricity/hydrogen using sunlight alone. In this review, we comprehensively assess recent progress in the development of photoanode/photocathode materials, cathode materials, system configurations, and radical reaction processes. We also summarize five key strategies to improve system dynamics and charge transfer properties. By highlighting the significance of designing and implementing PFCs as alternatives to traditional technologies, we provide insights into future research directions necessary for the advancement of highly efficient PFCs. Furthermore, we extensively discuss the challenges, perspectives, and future studies for various PFC or hybrid systems, and also the cost analysis of PFC based wasterwater treatment technique. Addressing challenges related to catalyst design, charge carrier dynamics, mass transport, system integration, and scalability, along with exploring environmental applications, advanced characterization techniques, combining with machine learning, system optimization, and control, will pave the way for the successful implementation and widespread adoption of PFCs.",
        "DOI": "10.1016/j.cej.2023.145162",
        "affiliation_name": "Korea Institute of Energy Technology",
        "affiliation_city": "Naju",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "Assessing the Impacts of Birmingham’s Clean Air Zone on Air Quality: Estimates from a Machine Learning and Synthetic Control Approach",
        "paper_author": "Liu B.",
        "publication": "Environmental and Resource Economics",
        "citied_by": "4",
        "cover_date": "2023-10-01",
        "Abstract": "We apply a two-step data driven approach to determine the causal impact of the clean air zone (CAZ) policy on air quality in Birmingham, UK. Levels of NO2, NOx and PM2.5 before and after CAZ implementation were collected from automatic air quality monitoring sites both within and outside the CAZ. We apply a unique combination of two recent methods: (1) a random forest machine learning method to strip out the effects of meteorological conditions on air pollution levels, and then (2) the Augmented Synthetic Control Method (ASCM) on the de-weathered air pollution data to isolate the causal effect of the CAZ. We find that, during the first year following the formal policy implementation, the CAZ led to significant but modest reductions of NO2 and NOX levels measured at the roadside within (up to 3.4% and 5.4% of NO2 and NOX, respectively) and outside (up to 6.6% and 11.9%) the zone, with no detectable changes at the urban background site outside the CAZ. No significant impacts of the CAZ were found on concentrations of fine particulates (PM2.5). Our analysis demonstrates the short-term effectiveness of CAZ in reducing concentrations of NO2 and NOX.",
        "DOI": "10.1007/s10640-023-00794-2",
        "affiliation_name": "University of Birmingham",
        "affiliation_city": "Birmingham",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Local and utility-wide cost allocations for a more equitable wildfire-resilient distribution grid",
        "paper_author": "Wang Z.",
        "publication": "Nature Energy",
        "citied_by": "5",
        "cover_date": "2023-10-01",
        "Abstract": "Climate-induced extreme weather conditions make electricity infrastructure more vulnerable. They increase the risk of power-line-ignited wildfires which can, in turn, jeopardize electric power delivery. Here, leveraging machine learning, we show that lower-income communities in California not only have lower fractions of power distribution lines undergrounded, but overhead lines and poles in their neighbourhoods are also more vulnerable to wildfires. Should they bear the cost of undergrounding fire-prone lines themselves, they would have to pay a disproportionately higher cost per household. We propose a cost allocation scheme with an income threshold below which the cost is borne by utility-wide ratepayers and above which the cost is borne locally. This scheme can not only minimize the average of undergrounding costs per household as a share of income, but also homogenize such cost–income ratios across communities. Our research demonstrates the opportunity to appropriately integrate existing policies to make electricity infrastructure affordable, equitable and reliable amidst climate change.",
        "DOI": "10.1038/s41560-023-01306-8",
        "affiliation_name": "Stanford Woods Institute for the Environment",
        "affiliation_city": "Stanford",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Predicting building age from urban form at large scale",
        "paper_author": "Nachtigall F.",
        "publication": "Computers, Environment and Urban Systems",
        "citied_by": "7",
        "cover_date": "2023-10-01",
        "Abstract": "To stay within 1.5 °C of global warming, reducing energy-related emissions in the building sector is essential. Rather than generic climate recommendations, this requires tailored, low-carbon urban planning solutions and spatially explicit methods that can inform policy measures at urban, street and building scale. Here, we propose a scalable method that is able to predict building age information in different European countries using only open urban morphology data. We find that spatially cross-validated regression models are sufficiently robust to generalize and predict building age in unseen cities with a mean absolute error (MAE) between 15.3 years (Netherlands) and 19.9 years (Spain). Our experiments show that large-scale models improve generalization for predicting across cities, but are not needed to infer missing data within known cities. Filling data gaps within known cities is possible with a MAE between 9.6 years (Netherlands) and 16.7 years (Spain). Overall, our results demonstrate the feasibility of generating missing age data in different contexts across Europe and informing climate mitigation policies such as large-scale energy retrofits. For the French residential building stock, we find that using age predictions to target retrofit efforts can increase energy savings by more than 50% compared to missing age data. Finally, we highlight challenges posed by data inconsistencies and urban form differences between countries that need to be addressed for an actual roll-out of such methods.",
        "DOI": "10.1016/j.compenvurbsys.2023.102010",
        "affiliation_name": "Mercator Research Institute on Global Commons and Climate Change",
        "affiliation_city": "Berlin",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Comprehensive assessment of resilience of flood hazard villages using a modeling and field survey approach",
        "paper_author": "Avand M.",
        "publication": "International Journal of Disaster Risk Reduction",
        "citied_by": "5",
        "cover_date": "2023-10-01",
        "Abstract": "Watersheds have been heavily affected by natural and human stresses in recent years, and their ability to recover and adapt to changed conditions depends on the resilience of the watersheds. Flood is one of these tensions, and to reduce the damages caused by it, it is necessary to identify vulnerable areas. This study aims is to evaluate the resilience of flood-prone sub-watersheds in the Beshar basin the Kohgiluyeh-Boyerahmad province of Iran. For this purpose, flood risk areas were determined using three machine learning models (MLMs), including random forest (RF), generalized linear model (GLM), and artificial neural network (ANN). Three models were evaluated based on criteria such as the ROC curve and Kappa coefficient, and the most accurate model was selected to identify areas at risk and complete the resilience questionnaire by residents. Social, economic, policy, and infrastructure criteria and 24 important and influential items were used to measure resilience. Different statistical methods were used to analyze the questionnaires and determine the resilience of different sub-basins. The results showed that the RF model (AUC = 0.96) is more accurate than the other two models. The flood risk map also showed that the very low-risk class had the largest area (2722 km2, 86% of the total study area). Also, the resilience results showed a decrease in the mean resilience scores after 2006 compared to before 2006. The results of the spatial changes of the resilience of different sub-watersheds in these two periods showed that in the first period, 4, 9, 13, and 16 sub-watersheds are in the low resilience class and, 10 and 17 sub-watersheds are in the high resilience class. Also; after 2006, 3, 4, 9, and 21 sub-watersheds were placed in the low resilience class and 10, 13, and 14 sub-watersheds were in the high resilience class, which has had changes compared to the previous period.",
        "DOI": "10.1016/j.ijdrr.2023.103910",
        "affiliation_name": "Agricultural Research, Education &amp; Extension Organization, Iran",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "Artificial intelligence and the work–health interface: A research agenda for a technologically transforming world of work",
        "paper_author": "Jetha A.",
        "publication": "American Journal of Industrial Medicine",
        "citied_by": "8",
        "cover_date": "2023-10-01",
        "Abstract": "The labor market is undergoing a rapid artificial intelligence (AI) revolution. There is currently limited empirical scholarship that focuses on how AI adoption affects employment opportunities and work environments in ways that shape worker health, safety, well-being and equity. In this article, we present an agenda to guide research examining the implications of AI on the intersection between work and health. To build the agenda, a full day meeting was organized and attended by 50 participants including researchers from diverse disciplines and applied stakeholders. Facilitated meeting discussions aimed to set research priorities related to workplace AI applications and its impact on the health of workers, including critical research questions, methodological approaches, data needs, and resource requirements. Discussions also aimed to identify groups of workers and working contexts that may benefit from AI adoption as well as those that may be disadvantaged by AI. Discussions were synthesized into four research agenda areas: (1) examining the impact of stronger AI on human workers; (2) advancing responsible and healthy AI; (3) informing AI policy for worker health, safety, well-being, and equitable employment; and (4) understanding and addressing worker and employer knowledge needs regarding AI applications. The agenda provides a roadmap for researchers to build a critical evidence base on the impact of AI on workers and workplaces, and will ensure that worker health, safety, well-being, and equity are at the forefront of workplace AI system design and adoption.",
        "DOI": "10.1002/ajim.23517",
        "affiliation_name": "Vector Institute",
        "affiliation_city": "Toronto",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "The contingency impact of culture on health security capacities for pandemic preparedness: A moderated Bayesian inference analysis",
        "paper_author": "Messner W.",
        "publication": "Journal of International Management",
        "citied_by": "1",
        "cover_date": "2023-10-01",
        "Abstract": "Managing pandemics is an enduring societal problem because major health emergencies have historically led to substantial changes and developments. While extant research has examined cultural and institutional factors that have influenced how governments have responded to the COVID-19 pandemic, there has been far less exploration of the factors associated with differences in the provision of preventative collective services, such as building health security capacities. This article examines the contingency impact of national culture on the association between a country's economic development and its pandemic preparedness. Methodically, the study uses a moderated Bayesian inference analysis, which is a machine learning technique that has been called for in international business research. Unlike traditional frequentist linear regression analysis, which aims to identify a single set of best-fit coefficients for a specified set of variables, Bayesian regression analysis generates posterior distributions of coefficients based on priors for an average of multiple potential models using the Markov Chain Monte Carlo technique. The use of moderated Bayesian inference analysis provides a novel approach to analyzing complex data in international business research. The study's findings can support governments in their resource allocation and policy development to address shortcomings in their preparedness for infectious disease outbreaks.",
        "DOI": "10.1016/j.intman.2023.101056",
        "affiliation_name": "Darla Moore School of Business",
        "affiliation_city": "Columbia",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Multi-agent deep reinforcement learning based resource management in SWIPT enabled cellular networks with H2H/M2M co-existence",
        "paper_author": "Li X.",
        "publication": "Ad Hoc Networks",
        "citied_by": "2",
        "cover_date": "2023-10-01",
        "Abstract": "Machine-to-Machine (M2M) communication is crucial in developing Internet of Things (IoT). As it is well known that cellular networks have been considered as the primary infrastructure for M2M communications, there are several key issues to be addressed in order to deploy M2M communications over cellular networks. Notably, the rapid growth of M2M traffic dramatically increases energy consumption, as well as degrades the performance of existing Human-to-Human (H2H) traffic. Sustainable operation technology and resource management are efficacious ways for solving these issues. In this paper, we investigate a resource management problem in cellular networks with H2H/M2M coexistence. First, considering the energy-constrained nature of machine type communication devices (MTCDs), we propose a novel network model enabled by simultaneous wireless information and power transfer (SWIPT), which empowers MTCDs with the ability to simultaneously perform energy harvesting (EH) and information decoding. Given the diverse characteristics of IoT devices, we subdivide MTCDs into critical and tolerable types, further formulating the resource management problem as an energy efficiency (EE) maximization problem under divers Quality-of-Service (QoS) constraints. Then, we develop a multi-agent deep reinforcement learning (DRL) based scheme to solve this problem. It provides optimal spectrum, transmit power and power splitting (PS) ratio allocation policies, along with efficient model training under designed behavior-tracking based state space and common reward function. Finally, we verify that with a reasonable training mechanism, multiple M2M agents successfully work cooperatively in a distributed way, resulting in network performance that outperforms other intelligence approaches in terms of convergence speed and meeting the EE and QoS requirements.",
        "DOI": "10.1016/j.adhoc.2023.103256",
        "affiliation_name": "Beijing Information Science &amp; Technology University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A two-stage credit scoring model based on random forest: Evidence from Chinese small firms",
        "paper_author": "Zhou Y.",
        "publication": "International Review of Financial Analysis",
        "citied_by": "9",
        "cover_date": "2023-10-01",
        "Abstract": "Small firms are major contributors to most economies, often supported by government policies. However, the credit scoring of small firms is complicated and costly, making it a challenging field of research. Using loan data from 3045 small firms in China, we design a two-stage expert system for default prediction that quantifies the variables and thresholds that have a key impact. Firstly, we use SMOTE to deal with the imbalanced data and secondly, we employ random forest to build predictive credit features. Dominance analysis shows that, when making default assessments on Chinese small firms, it is important to consider not only financial factors, but also non-financial and macroeconomic factors. In particular, the net cash profit, the firm's legal disputes and the per capita disposable income of urban residents are key factors in credit scoring. Robustness tests show that our proposed methodology performs better than other machine learning models, and this result is robust with observations from other countries.",
        "DOI": "10.1016/j.irfa.2023.102755",
        "affiliation_name": "Dalian University of Technology",
        "affiliation_city": "Dalian",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Value Functions Factorization With Latent State Information Sharing in Decentralized Multi-Agent Policy Gradients",
        "paper_author": "Zhou H.",
        "publication": "IEEE Transactions on Emerging Topics in Computational Intelligence",
        "citied_by": "10",
        "cover_date": "2023-10-01",
        "Abstract": "The use of centralized training and decentralized execution for value function factorization demonstrates the potential for addressing cooperative multi-agent reinforcement tasks. QMIX, one of the methods in this field, has emerged as the leading approach and showed superior performance on the StarCraft II micromanagement benchmark. Nonetheless, its monotonic mixing method of combining per-agent estimates in QMIX has limitations in representing joint action Q-values and may not provide enough global state information for accurately estimating single-agent value function, which can lead to suboptimal results. To this end, we present LSF-SAC, a novel framework that features a variational inference-based information-sharing mechanism as extra state information to assist individual agents in the value function factorization. We demonstrate that such latent individual state information sharing can significantly expand the power of value function factorization, while fully decentralized execution can still be maintained in LSF-SAC through a soft-actor-critic design. We evaluate LSF-SAC on the StarCraft II micromanagement challenge and demonstrate that it outperforms several state-of-the-art methods in challenging collaborative tasks. We further set extensive ablation studies for locating the key factors accounting for its performance improvements. We believe that this new insight can lead to new local value estimation methods and variational deep learning algorithms.",
        "DOI": "10.1109/TETCI.2023.3293193",
        "affiliation_name": "School of Engineering and Applied Science",
        "affiliation_city": "Washington, D.C.",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "BigTech credit risk assessment for SMEs",
        "paper_author": "Huang Y.",
        "publication": "China Economic Review",
        "citied_by": "4",
        "cover_date": "2023-10-01",
        "Abstract": "Lending by big technology companies (BigTechs) is an important new financial innovation in the digital era. This paper attempts to evaluate robustness and special features of BigTech's credit risk assessment. Using 1.8 million loan transactions for online merchants of a leading Chinese virtue bank, we carry out a horse race analysis between the BigTech approach (i.e., big data and machine learning models) and the bank approach (i.e., traditional financial data and scorecard models) in predicting loan defaults. We show that the BigTech approach better predicts loan defaults, reflecting information and modeling advantages. Though bank approach do well for the firms which have records in credit registry, BigTech's proprietary information can complement or, where necessary, substitute for credit history in predicting defaults, especially for the unbanked borrowers. We further discuss inclusiveness feature of the BigTech approach and the implications for financial inclusion, financial intermediaries' businesses and regulators' policy.",
        "DOI": "10.1016/j.chieco.2023.102016",
        "affiliation_name": "Ant group",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Dive into the details of self-supervised learning for medical image analysis",
        "paper_author": "Zhang C.",
        "publication": "Medical Image Analysis",
        "citied_by": "28",
        "cover_date": "2023-10-01",
        "Abstract": "Self-supervised learning (SSL) has achieved remarkable performance in various medical imaging tasks by dint of priors from massive unlabeled data. However, regarding a specific downstream task, there is still a lack of an instruction book on how to select suitable pretext tasks and implementation details throughout the standard “pretrain-then-finetune” workflow. In this work, we focus on exploiting the capacity of SSL in terms of four realistic and significant issues: (1) the impact of SSL on imbalanced datasets, (2) the network architecture, (3) the applicability of upstream tasks to downstream tasks and (4) the stacking effect of SSL and common policies for deep learning. We provide a large-scale, in-depth and fine-grained study through extensive experiments on predictive, contrastive, generative and multi-SSL algorithms. Based on the results, we have uncovered several insights. Positively, SSL advances class-imbalanced learning mainly by boosting the performance of the rare class, which is of interest to clinical diagnosis. Unfortunately, SSL offers marginal or even negative returns in some cases, including severely imbalanced and relatively balanced data regimes, as well as combinations with common training policies. Our intriguing findings provide practical guidelines for the usage of SSL in the medical context and highlight the need for developing universal pretext tasks to accommodate diverse application scenarios. The code of this paper can be found at https://github.com/EndoluminalSurgicalVision-IMR/Medical-SSL.",
        "DOI": "10.1016/j.media.2023.102879",
        "affiliation_name": "Shanghai Jiao Tong University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Inappropriate nighttime light reduces living comfort",
        "paper_author": "Li C.",
        "publication": "Environmental Pollution",
        "citied_by": "3",
        "cover_date": "2023-10-01",
        "Abstract": "Living comfort is an important aspect of human well-being and a critical index of sustainable environments. Many environmental factors are associated with living comfort. Nighttime light (NTL) is remote sensing data that is widely used to reflect development level and economic status, and it also represents the lighting intensity in living environments. However, the relationship between NTL and living comfort is poorly understood. Here, we employ linear regression and a random forest model to investigate the direct impact of NTL on living comfort. Our results show that increased NTL is negatively associated with living comfort, but this relationship may be obscured by other factors, such as infrastructure. According to the nonlinear relationship, when the NTL is approximately 10 nW/cm2∙sr, there is a peak in living comfort. Hence, ensuring a reasonable level of lighting is a key to promoting sustainable development. Our research offers crucial insights that can aid in developing sustainable development policies to enhance livability.",
        "DOI": "10.1016/j.envpol.2023.122173",
        "affiliation_name": "Kyushu University",
        "affiliation_city": "Fukuoka",
        "affiliation_country": "Japan"
    },
    {
        "paper_title": "Autonomous intelligent control of earth pressure balance shield machine based on deep reinforcement learning",
        "paper_author": "Liu X.",
        "publication": "Engineering Applications of Artificial Intelligence",
        "citied_by": "8",
        "cover_date": "2023-10-01",
        "Abstract": "In order to reduce the construction risk caused by human operation error and improve the geological adaptive ability of the shield machine, an autonomous intelligent control method is proposed for shield machine within the framework of interaction–judgment–decision based on Deep Deterministic Policy Gradient (DDPG) deep reinforcement learning in this study. Due to the strong nonlinear relationship between the shield machine's tunneling parameters, this research builds a deep reinforcement learning environment using mechanism model of sealed cabin pressure. DDPG agent model of the shield machine is established to replace the shield machine to interact and train with the geological environment. By minimizing the difference between the target pressure setting value and the sealed cabin pressure value, the dynamic balance between the sealed cabin pressure and the pressure on the excavation surface is realized, and the best strategy is obtained. Through real-time interaction with the geological environment, the method in this paper can dynamically adjust the tunneling parameters, accurately control the sealed cabin pressure, and has a strong geological adaptive ability. By realizing the intelligent decision-making of the tunneling parameters, it greatly improves the independent decision-making ability of the shield machine system, reduces the inaccuracy of human operation, and provides an effective guarantee for the efficient and safe operation of the shield machine. This study applies deep reinforcement learning technology to the control field of earth pressure balance shield machine, promotes AI technology, and provides a new idea for the development of AI construction technology in engineering field.",
        "DOI": "10.1016/j.engappai.2023.106702",
        "affiliation_name": "Dalian University of Technology",
        "affiliation_city": "Dalian",
        "affiliation_country": "China"
    },
    {
        "paper_title": "IPv6 flood attack detection based on epsilon greedy optimized Q learning in single board computer",
        "paper_author": "Daru A.F.",
        "publication": "International Journal of Electrical and Computer Engineering",
        "citied_by": "2",
        "cover_date": "2023-10-01",
        "Abstract": "Internet of things is a technology that allows communication between devices within a network. Since this technology depends on a network to communicate, the vulnerability of the exposed devices increased significantly. Furthermore, the use of internet protocol version 6 (IPv6) as the successor to internet protocol version 4 (IPv4) as a communication protocol constituted a significant problem for the network. Hence, this protocol was exploitable for flooding attacks in the IPv6 network. As a countermeasure against the flood, this study designed an IPv6 flood attack detection by using epsilon greedy optimized Q learning algorithm. According to the evaluation, the agent with epsilon 0.1 could reach 98% of accuracy and 11,550 rewards compared to the other agents. When compared to control models, the agent is also the most accurate compared to other algorithms followed by neural network (NN), K-nearest neighbors (KNN), decision tree (DT), naive Bayes (NB), and support vector machine (SVM). Besides that, the agent used more than 99% of a single central processing unit (CPU). Hence, the agent will not hinder internet of things (IoT) devices with multiple processors. Thus, we concluded that the proposed agent has high accuracy and feasibility in a single board computer (SBC).",
        "DOI": "10.11591/ijece.v13i5.pp5782-5791",
        "affiliation_name": "Universitas Semarang",
        "affiliation_city": "Semarang",
        "affiliation_country": "Indonesia"
    },
    {
        "paper_title": "Causal reinforcement learning based on Bayesian networks applied to industrial settings",
        "paper_author": "Valverde G.",
        "publication": "Engineering Applications of Artificial Intelligence",
        "citied_by": "5",
        "cover_date": "2023-10-01",
        "Abstract": "The increasing amount of real-time data collected from sensors in industrial environments has accelerated the application of machine learning in decision-making. Reinforcement learning (RL) is a powerful tool to find optimal policies for achieving a given goal. However, RL's typical application is risky and insufficient in environments where actions can have irreversible consequences and require interpretability and fairness. While new trends in RL may provide guidance based on expert knowledge, they do not often consider uncertainty or include prior knowledge in the learning process. We propose a causal reinforcement learning alternative based on Bayesian networks (RLBNs) to address this challenge. The RLBN simultaneously models a policy and takes advantage of the joint distribution of the state and action space, reducing uncertainty in unknown situations. We propose a training algorithm for the network's parameters and structure based on the reward function and likelihood of the effects and measurements taken. Our experiment with the CartPole benchmark and industrial fouling using ordinary differential equations (ODEs) demonstrates that RLBNs are interpretable, secure, flexible, and more robust than their competitors. Our contributions include a novel method that incorporates expert knowledge into the decision-making engine. It uses Bayesian networks with a predefined structure as a causal graph and a hybrid learning strategy that considers both likelihood and reward. This would avoid losing the virtues of the Bayesian network.",
        "DOI": "10.1016/j.engappai.2023.106657",
        "affiliation_name": "Universidad Politécnica de Madrid",
        "affiliation_city": "Madrid",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "Twin-delayed deep deterministic policy gradient algorithm for the energy management of microgrids",
        "paper_author": "Domínguez-Barbero D.",
        "publication": "Engineering Applications of Artificial Intelligence",
        "citied_by": "15",
        "cover_date": "2023-10-01",
        "Abstract": "The microgrid market is growing significantly due to several drivers, such as the need to lower greenhouse gas emissions by integrating higher shares of distributed renewable energy sources, falling costs of microgrid components, the need for more reliable power supply infrastructures, and new off-grid solutions to foster electricity access in developing economies. Coordinated management of the microgrid components is crucial for their effectiveness, and this can be very challenging when hosting solar or wind generation. This paper studies the energy management problem of a microgrid based on reinforcement learning algorithms. The advantage of using these algorithms against other optimization and machine learning techniques is that they do not need past experiences to learn a strategy. The learning is based on trial and error experiences, which facilitates its easy implementation to other microgrids while demonstrating their facility to be applied in real cases. In particular, this paper proposes an implementation for an Energy Management System (EMS) in microgrids using the Twin-Delayed Deep Deterministic Policy Gradient (TD3) algorithm. Moreover, it compares the proposed algorithm with the Deep Q-Network (DQN). This comparison evaluates the improvement over exploiting the continuous nature of the decision variables against a discretization of the same since the DQN cannot make actions over a continuous space.",
        "DOI": "10.1016/j.engappai.2023.106693",
        "affiliation_name": "Universidad Pontificia Comillas, Escuela Técnica Superior de Ingeniería, Instituto de Investigación Tecnológica",
        "affiliation_city": "Madrid",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "Twenty years of energy policy in Europe: achievement of targets and lessons for the future",
        "paper_author": "Márquez-Sobrino P.",
        "publication": "Clean Technologies and Environmental Policy",
        "citied_by": "10",
        "cover_date": "2023-10-01",
        "Abstract": "The different energy transition efforts in the EU-27 countries are analysed, paying special attention to the achievement of set energy targets and the real influence on energy dependence and GHG reduction. Various methodologies were used, ranging from construction of timelines to geo-statistical analysis using Geographic Information Systems (GIS) and the implementation of machine learning techniques and models, using R. The results show how different modifications of the energy saving and efficiency targets, along with lower power consumption due to the COVID pandemic, resulted in that although most of the EU-27 countries have achieved their saving and efficiency targets, this has not been reflected in a real reduction in consumption (compared to 1990 levels). In addition, the fulfilment of the objectives has not resulted in a reduction in energy dependence, generating a false sense of security and satisfaction in the fulfilment of the targets. Concerning GHGs, almost all EU-27 countries decrease their GHG emissions per capita compared to 2000 (with the exception of Lithuania, Bulgaria, Croatia and Latvia), with this decrease being mainly related to the fulfilment of renewable energy targets in transport. The conclusion highlights the need to make greater efforts to achieve saving and efficiency in the near future; otherwise, higher power consumption via renewable energy sources, while helping meet future increases in energy demand, will not impact the reduction in energy dependence compared to current levels. Graphical abstract: [Figure not available: see fulltext.] Achievement of energy transition targets. Contribution to the reduction in greenhouse gases and energy dependence.",
        "DOI": "10.1007/s10098-023-02543-x",
        "affiliation_name": "Universidad de Sevilla",
        "affiliation_city": "Sevilla",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "Federated deep contrastive learning for mid-term natural gas demand forecasting",
        "paper_author": "Qin D.",
        "publication": "Applied Energy",
        "citied_by": "14",
        "cover_date": "2023-10-01",
        "Abstract": "Accurate mid-term gas demand forecasting plays a crucial role for gas companies and policymakers to achieve reliable gas supply plans, supply contracts management, and efficient operation to meet the increasing gas demand. However, mid-term gas demand forecasting faces the problems of data paucity caused by the low frequency of collecting monthly data and heterogeneous consumption patterns of various usage categories. This paper proposes a novel Federated Contrastive pretraining - Local Clustered Finetuning paradigm (FedCon-LCF) by integrating federated learning, deep contrastive learning, and clustering approaches. The proposed method can utilize data from multiple gas companies to overcome data paucity issues in a privacy-preserving way, and high-performance forecasting can be achieved by local clustered regression considering the heterogeneous patterns. An improved hierarchical contrastive loss and multi-scale regression loss are integrated to develop the Forecasting-Oriented Contrastive Learning model (FOCL), which can effectively extract information and generate fine-grained representations of time series for accurate forecasting. The proposed method is evaluated on a dataset collected from 11 gas companies in 11 different Chinese cities with a total of 17648 clients over 10 usage categories. The proposed method outperforms the benchmark LSTM model with an average improvement of 25.30% in MSE and 16.52% in MAE for 3-month-ahead, 6-month-ahead, 9-month-ahead, and 12-month-ahead gas demand forecasting.",
        "DOI": "10.1016/j.apenergy.2023.121503",
        "affiliation_name": "ENN Group Co., Ltd.",
        "affiliation_city": "Langfang",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Neural network based feedback optimal control for pinpoint landers under disturbances",
        "paper_author": "Mulekar O.S.",
        "publication": "Acta Astronautica",
        "citied_by": "2",
        "cover_date": "2023-10-01",
        "Abstract": "Imitation learning leverages example demonstrations to teach a policy to replicate a desired behavior. In this investigation, the example demonstrations consist of open-loop optimal trajectories calculated off-line. This paper introduces the use of imitation learning to demonstrate optimal feedback control of two different high-fidelity 6-degree-of-freedom (6DOF) lander models. A loss of optimality is shown when disturbances are applied to policies trained only with nominal trajectories. Methodologies such as multi-phase optimal control and triple-single-phase optimal control are applied to include disturbances in the optimal trajectory generation, and the policies are trained to mitigate loss of optimality when disturbances are applied. Monte Carlo simulations show that loss of optimality can be mitigated by including disturbances in the optimal trajectory generation and training data.",
        "DOI": "10.1016/j.actaastro.2023.06.033",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Daytona Beach",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Using smart card data to model public transport user profiles in light of the COVID-19 pandemic",
        "paper_author": "Lizana M.",
        "publication": "Travel Behaviour and Society",
        "citied_by": "10",
        "cover_date": "2023-10-01",
        "Abstract": "The COVID-19 pandemic caused an unprecedented impact on public transport demand. Even though several studies have investigated the change in the use of public transport during the pandemic, most existing studies where large passive datasets have been considered focus on the drop in ridership at the aggregate level. To address this gap, this research aims to identify and model profiles of passengers considering their public transport recovery after the long-term lockdown in Santiago, Chile, during the early stage of the pandemic. The methodology proposed a three-stage approach associated with the analysis of smart card records. First, cardholder residential areas were identified to enrich the available data by integrating demographic information from the census. Then, a clustering analysis was applied to recognise distinctive classes of users based on their public transport usage change between the pre-pandemic and the post-lockdown phase. Finally, two different models were implemented to uncover the relationships between class membership and travellers’ characteristics (i.e. travel history and demographic characteristics of their residential area). Results revealed a heterogeneous recovery of public transport usage among passengers, summarising them into two recognisable classes: those who mainly returned to their pre-pandemic patterns and those who adapted their mobility profiles. A statistically significant association of travel history with the mobility adaptation profile was found, as well as with aggregate socio-demographic attributes. These insights about the extent of heterogeneity and its drivers can help in the formulation of specific policies associated with public transport supply in the post-pandemic era.",
        "DOI": "10.1016/j.tbs.2023.100620",
        "affiliation_name": "University of Leeds",
        "affiliation_city": "Leeds",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Causal explanation for reinforcement learning: quantifying state and temporal importance",
        "paper_author": "Wang X.",
        "publication": "Applied Intelligence",
        "citied_by": "3",
        "cover_date": "2023-10-01",
        "Abstract": "Explainability plays an increasingly important role in machine learning. Because reinforcement learning (RL) involves interactions between states and actions over time, it’s more challenging to explain an RL policy than supervised learning. Furthermore, humans view the world through a causal lens and thus prefer causal explanations over associational ones. Therefore, in this paper, we develop a causal explanation mechanism that quantifies the causal importance of states on actions and such importance over time. We also demonstrate the advantages of our mechanism over state-of-the-art associational methods in terms of RL policy explanation through a series of simulation studies, including crop irrigation, Blackjack, collision avoidance, and lunar lander.",
        "DOI": "10.1007/s10489-023-04649-7",
        "affiliation_name": "Georgia Institute of Technology",
        "affiliation_city": "Atlanta",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Learning Modular Robot Control Policies",
        "paper_author": "Whitman J.",
        "publication": "IEEE Transactions on Robotics",
        "citied_by": "17",
        "cover_date": "2023-10-01",
        "Abstract": "Modular robots can be rearranged into a new design, perhaps each day, to handle a wide variety of tasks by forming a customized robot for each new task. However, reconfiguring just the mechanism is not sufficient: each design also requires its own unique control policy. One could craft a policy from scratch for each new design, but such an approach is not scalable, especially given the large number of designs that can be generated from even a small set of modules. Instead, we create a modular policy framework where the policy structure is conditioned on the hardware arrangement, and use just one training process to create a policy that controls a wide variety of designs. Our approach leverages the fact that the kinematics of a modular robot can be represented as a design graph, with nodes as modules and edges as connections between them. Given a robot, its design graph is used to create a policy graph with the same structure, where each node contains a deep neural network, and modules of the same type share knowledge via shared parameters (e.g., all legs on a hexapod share the same network parameters). We developed a model-based reinforcement learning algorithm, interleaving model learning and trajectory optimization to train the policy. We show the modular policy generalizes to a large number of designs that were not seen during training without any additional learning. Finally, we demonstrate the policy controlling a variety of designs to locomote with both simulated and real robots.",
        "DOI": "10.1109/TRO.2023.3284362",
        "affiliation_name": "The Robotics Institute",
        "affiliation_city": "Pittsburgh",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Collaborative optimization of multi-microgrids system with shared energy storage based on multi-agent stochastic game and reinforcement learning",
        "paper_author": "Wang Y.",
        "publication": "Energy",
        "citied_by": "20",
        "cover_date": "2023-10-01",
        "Abstract": "Achieving the economical and stable operation of Multi-microgrids (MMG) systems is vital. However, there are still some challenging problems to be solved. Firstly, from the perspective of stable operation, it is necessary to minimize the energy fluctuation of the main grid. Secondly, the characteristics of energy conversion equipment need to be considered. Finally, privacy protection while reducing the operating cost of an MMG system is crucial. To address these challenges, a Data-driven strategy for MMG systems with Shared Energy Storage (SES) is proposed. In this paper, the Mixed-Attention is applied to fit the conditions of the equipment, and Multi-Agent Soft Actor-Critic(MA-SAC), Multi-Agent Win or Learn Fast Policy Hill-Climbing (MA-WoLF-PHC) are proposed to solve the partially observable dynamic stochastic game problem. By testing the operation data of the MMG system in Northwest China, following conclusions are drawn: the R-Square (R2) values of results reach 0.999, indicating the neural network effectively models the nonlinear conditions. The proposed MMG system framework can reduce energy fluctuations in the main grid by 1746.5 kW in 24 h and achieve a cost reduction of 16.21% in the test. Finally, the superiority of the proposed algorithms is verified through their fast convergence speed and excellent optimization performance.",
        "DOI": "10.1016/j.energy.2023.128182",
        "affiliation_name": "Northeast Electric Power University",
        "affiliation_city": "Jilin",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Applying an interpretable machine learning framework to study mobility inequity in the recovery phase of COVID-19 pandemic",
        "paper_author": "Li Z.",
        "publication": "Travel Behaviour and Society",
        "citied_by": "5",
        "cover_date": "2023-10-01",
        "Abstract": "The COVID-19 pandemic is a public health crisis that also fuels the pervasive social inequity in the United States. Existing studies have extensively analyzed the inequity issues on mobility across different demographic groups during the lockdown phase. However, it is unclear whether the mobility inequity is perennial and will continue into the mobility recovery phase. This study utilizes ride-hailing data from Jan 1st, 2019, to Mar 31st, 2022, in Chicago to analyze the impact of various factors, such as demographic, land use, and transit connectivity, on mobility inequity in the different recovery phases. Instead of commonly used statistical methods, this study leverages advanced time-series clustering and an interpretable machine learning algorithm. The result demonstrates that inequity still exists in the mobility recovery phase of the COVID-19 pandemic, and the degree of mobility inequity in different recovery phases is varied. Furthermore, mobility inequity is more likely to exist in the census tract with more families without children, lower health insurance coverage, inflexible workstyle, more African Americans, higher poverty rate, fewer commercial land use, and higher Gini index. This study aims to further the understanding of the social inequity issue during the mobility recovery phase of the COVID-19 pandemic and help governments propose proper policies to tackle the unequal impact of the pandemic.",
        "DOI": "10.1016/j.tbs.2023.100621",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "College Station",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "From theory to practice: optimisation of available information for landslide hazard assessment in Rome relying on official, fragmented data sources",
        "paper_author": "Esposito C.",
        "publication": "Landslides",
        "citied_by": "12",
        "cover_date": "2023-10-01",
        "Abstract": "The definition of landslide hazard is a step-like procedure that encompasses the quantification of its spatial and temporal attributes, i.e., a reliable definition of landslide susceptibility and a detailed analysis of landslide recurrence. However, available information is often incomplete, fragmented and unsuitable for reliable quantitative analysis. Nevertheless, landslide hazard evaluation has a key role in the implementation of risk mitigation policies and an effort should be done to retrieve information and make it useful for this purpose. In this research, we go through this topic of optimising the information available in catalogues, starting from landslide inventory review and constitution of a boosted training dataset, propaedeutic for susceptibility analysis based on machine learning methods. The temporal recurrence of landslide events has been approached here either through the definitions of large-scale quantitative hazard descriptors or by analysis of historical rainfall (i.e., the main triggering factor for the considered shallow earth slope failures) databases through the definition of rainfall probability curves. Spatial and temporal attributes were integrated, selecting potential landslide source areas ranked in terms of hazard. Data integration was also pursued through persistent scatterer interferometry analysis which pointed out areas of interest within potential landslide source areas featured by ongoing ground movement. The consequential approach led to the definition of the first hazard product of the city of Rome at a local scale functional for advisory purposes or the statutory level, representing a thematic layer able to orient the risk managers and infrastructure stakeholders.",
        "DOI": "10.1007/s10346-023-02095-7",
        "affiliation_name": "Sapienza Università di Roma",
        "affiliation_city": "Rome",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Time series forecasting of COVID-19 infections and deaths in Alpha and Delta variants using LSTM networks",
        "paper_author": "Sheikhi F.",
        "publication": "PLoS ONE",
        "citied_by": "3",
        "cover_date": "2023-10-01",
        "Abstract": "Since the beginning of the rapidly spreading COVID-19 pandemic, several mutations have occurred in the genetic sequence of the virus, resulting in emerging different variants of concern. These variants vary in transmissibility, severity of infections, and mortality rate. Designing models that are capable of predicting the future behavior of these variants in the societies can help decision makers and the healthcare system to design efficient health policies, and to be prepared with the sufficient medical devices and an adequate number of personnel to fight against this virus and the similar ones. Among variants of COVID-19, Alpha and Delta variants differ noticeably in the virus structures. In this paper, we study these variants in the geographical regions with different size, population densities, and social life styles. These regions include the country of Iran, the continent of Asia, and the whole world. We propose four deep learning models based on Long Short-Term Memory (LSTM), and examine their predictive power in forecasting the number of infections and deaths for the next three, next five, and next seven days in each variant. These models include Encoder Decoder LSTM (ED-LSTM), Bidirectional LSTM (Bi-LSTM), Convolutional LSTM (Conv-LSTM), and Gated Recurrent Unit (GRU). Performance of these models in predictions are evaluated using the root mean square error, mean absolute error, and mean absolute percentage error. Then, the Friedman test is applied to find the leading model for predictions in all conditions. The results show that ED-LSTM is generally the leading model for predicting the number of infections and deaths for both variants of Alpha and Delta, with the ability to forecast long time intervals ahead.",
        "DOI": "10.1371/journal.pone.0282624",
        "affiliation_name": "K. N. Toosi University of Technology",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "Early warning of critical transitions in crude oil price",
        "paper_author": "An S.",
        "publication": "Energy",
        "citied_by": "3",
        "cover_date": "2023-10-01",
        "Abstract": "Critical transitions in the crude oil price are important in market management. Previous methods have focused on generic early warning signals in complex systems if a critical transition is approaching and ignored the specific feature of the oil price. This paper proposes a heteroscedastic network model in which early warnings of critical transitions are identified based on the community structure of the network representing the dynamic process of a time series. Using WTI crude oil price data, we detect early warning signals of critical transitions. Our findings indicate that major switches exist between early warnings and critical transitions in different periods, and the corresponding features can be analyzed based on the fundamentals and expectations of traders. Importantly, based on the network indicators associated with early warnings, the fundamental features may be similar during certain periods, and the changes in fundamentals and expectations before and after critical transitions are not random. A new complex system perspective is used to explore early warnings for critical transitions, and useful implications for energy-related market investors and policy-makers are provided.",
        "DOI": "10.1016/j.energy.2023.128089",
        "affiliation_name": "School of Economics and Management",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Differentiable Logic Policy for Interpretable Deep Reinforcement Learning: A Study From an Optimization Perspective",
        "paper_author": "Li X.",
        "publication": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "citied_by": "3",
        "cover_date": "2023-10-01",
        "Abstract": "The interpretability of policies remains an important challenge in Deep Reinforcement Learning (DRL). This paper explores interpretable DRL via representing policy by Differentiable Inductive Logic Programming (DILP) and provides a theoretical and empirical study of DILP-based policy learning from an optimization perspective. We first identified a fundamental fact that DILP-based policy learning should be solved as a constrained policy optimization problem. We then proposed to use Mirror Descent for policy optimization (MDPO) to deal with the constraints of DILP-based policies. We derived the closed-form regret bound of MDPO with function approximation, which is helpful to the design of DRL frameworks. Moreover, we studied the convexity of DILP-based policy to further verify the benefits gained from MDPO. Empirically, we experimented MDPO, its on-policy variant, and 3 mainstream policy learning methods, and the results verified our theoretical analysis.",
        "DOI": "10.1109/TPAMI.2023.3285634",
        "affiliation_name": "University of the Sunshine Coast",
        "affiliation_city": "Sippy Downs",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Dryland farming wheat yield prediction using the Lasso regression model and meteorological variables in dry and semi-dry region",
        "paper_author": "Didari S.",
        "publication": "Stochastic Environmental Research and Risk Assessment",
        "citied_by": "6",
        "cover_date": "2023-10-01",
        "Abstract": "The risk of climate change and international market fluctuations complicate crop production. Wheat is considered one of the most strategic products in food security. Dryland farming of wheat is prevalent in many parts of the world, and it occupies a large part of the cultivated land. However, its performance is highly dependent on weather conditions and changes. Yield prediction models could be used for planning purposes when dealing with changes in yield. This study analyzed the effectiveness of the least absolute shrinkage and selection operator (LASSO) model in selecting variables for predicting dryland wheat yield in southwestern Iran. The model was used with 45 weather-based variables across annual, seasonal, and monthly time frames. The results showed that temperature, evaporation, and extreme temperatures followed by radiation and precipitation variables categories, are effective meteorological variables in estimating dryland farming wheat yield in the study area. Monthly timescale could estimate yield with minimum error compared to other timescales. However, considering all selected variables regardless of their timescale (total) results, the best estimation in most districts with the model’s R2 and normalized root mean square error (NRMSE) varied between 57.98–99.50 and 1.46–21.94, respectively. Therefore, the LASSO regression could be used reliably for each district considering the most effective meteorological parameters in that region for accurate decision-makers policies.",
        "DOI": "10.1007/s00477-023-02490-5",
        "affiliation_name": "Fasa University",
        "affiliation_city": "Fasa",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "Development of predictive optimization model for autonomous rotary drilling system using machine learning approach",
        "paper_author": "Amadi K.",
        "publication": "Journal of Petroleum Exploration and Production Technology",
        "citied_by": "3",
        "cover_date": "2023-10-01",
        "Abstract": "The growing global energy demand and strict environmental policies motivate the use of technology and performance improvement techniques in drilling operations. In the traditional drilling method, the effort and time required to optimize drilling depend on the effectiveness of human driller in selecting the optimal set of parameters to improve system performance. Although existing work has identified the significance of upscaling from manual drilling to autonomous drilling system, little has been done to support this transition. In this paper, predictive optimization model is proposed for autonomous drilling systems. To evaluate optimized operating procedure, a comparative study of surface operating parameters using weight on bit (WOB), rotary speed (RPM) versus drilling mechanical specific energy (DMSE), and feed thrust (FET) is presented. The study used a data-driven approach that uses offset drilling data with machine learning model in finding a pair of input operating variables that serves as best tuning parameters for the topdrive and drawwork system. The results illustrate that derived variables (DMSE, FET) gave higher prediction accuracy with correlation coefficient (R 2) of 0.985, root mean square error (RMSE) of 7.6 and average absolute percentage error (AAPE) of 34, whilst using the surface operating parameters (WOB, RPM) delivered an R 2, RMSE and AAPE of 0.74, 28 and 106, respectively. Although previous researches have predicted ROP using ANN, this research considered the selection of tuning control variables and using it in predicting the system ROP for an autonomous system. The model output offers parameter optimization and adaptative control of autonomous drilling system.",
        "DOI": "10.1007/s13202-023-01656-9",
        "affiliation_name": "Australian University",
        "affiliation_city": "Safat",
        "affiliation_country": "Kuwait"
    },
    {
        "paper_title": "Predicting declining and growing occupations using supervised machine learning",
        "paper_author": "Khalaf C.",
        "publication": "Journal of Computational Social Science",
        "citied_by": "2",
        "cover_date": "2023-10-01",
        "Abstract": "In the United States (U.S.), structural changes in the economy remain varied, yet continuous, prompting the need for regular analyses of both declining and growing occupations. As automation, robotization, and digitization continues to accelerate and drive new patterns of economic change, so does the need for proactive programs and policies aimed at targeted workforce re-training. Applying machine learning (ML) to occupational data provides one potential approach to inform such workforce initiatives, specifically by helping to predict both declining and growing occupations with advanced accuracy. In this paper, we examine the extent to which occupational attributes are predictive of the declining and growing status of jobs in the State of Ohio (USA). In particular, we examine the results from five distinct supervised ML models (i.e., multinomial logistic regression, nearest neighbors, random forest, adaptive boosting, and gradient boosting), and data on the characteristics of occupations from O*NET, as well as information on employment changes from the U.S. Bureau of Labor Statistics. We found that the random forest and gradient boosting models perform the best, predicting declining and growing jobs in Ohio at roughly 92% accuracy in the test set. Moreover, our analysis revealed that the most important features in predicting declining occupations are physical (e.g., spending time making repetitive motions), while the most important features in predicting growing occupations are related to obtaining information and communication. Our method can be replicated at a local or regional level to help practitioners predict future occupational shifts, ultimately enhancing economic and workforce development efforts.",
        "DOI": "10.1007/s42001-023-00211-0",
        "affiliation_name": "University of Illinois at Chicago",
        "affiliation_city": "Chicago",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Using machine learning techniques to reduce uncertainty for outpatient appointment scheduling practices in outpatient clinics",
        "paper_author": "Golmohammadi D.",
        "publication": "Omega (United Kingdom)",
        "citied_by": "9",
        "cover_date": "2023-10-01",
        "Abstract": "Most outpatient clinics apply deterministic block scheduling policies to patient visits even though patients utilize varying amounts of time, leaving patients, operations managers, and clinicians frustrated because patients and physicians are kept waiting. This paper offers a decision-making model for schedulers so that the service time needed for a specific patient can be predicted to allow outpatient clinics to schedule more effectively. We employed an analytical approach, with a data driven methodology consisting of two phases. In phase one, machine learning algorithms are used to predict service time for outpatient clinics servicing patients with various characteristics. This study supports the understanding of factors that impact service time. A large dataset from an outpatient clinic is obtained and used in the analyses. Four dominant data mining models are developed to predict service time, and their performances are compared: neural networks (NNs), generalized linear model (GLM), linear regression (LR), and support vector regression (SVM). The NN models performed the best. The reason for visiting the doctor and patient type are identified as the primary characteristics to aid in predicting patient service time. We compare the proposed NN models with commonly used scheduling policies in practice in the second phase via simulation modeling and analysis. This paper contributes to the literature in four ways. First, we obtained a large dataset and extracted quality data to test the prediction accuracy of multiple models to determine which one improves scheduling the best. Second, patient characteristics are identified through machine learning modeling and sensitivity analysis to understand which ones are most important for service time prediction accuracy. Third, we analyzed the performance of standard scheduling policies used in clinics. Lastly, we provide clinical policy implications and recommendations that will provide insights and support appointment scheduling decisions.",
        "DOI": "10.1016/j.omega.2023.102907",
        "affiliation_name": "Rutgers Business School—Newark and New Brunswick",
        "affiliation_city": "Newark",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Multi-Agent Deep Reinforcement Learning Based Incentive Mechanism for Multi-Task Federated Edge Learning",
        "paper_author": "Zhao N.",
        "publication": "IEEE Transactions on Vehicular Technology",
        "citied_by": "10",
        "cover_date": "2023-10-01",
        "Abstract": "Federated edge learning (FEL) is capable of training large-scale machine learning models without exposing the raw data of edge devices (EDs). Considering that the learning performance heavily depends on the active participation of EDs, it is essential to motivate the resource-limited EDs to contribute their efforts to learning tasks. In this paper, a learning-based multi-task FEL mechanism is proposed to design the economic incentive and participation contribution strategy jointly. Specifically, the incentive-based interaction between the edge servers and EDs is formulated as a multi-leader multi-follower Stackelberg game. Then, the theoretical analysis is provided to prove the existence and uniqueness of the Stackelberg equilibrium. To obtain the equilibrium solution under the incomplete information, a Markov decision process is formulated for the two-stage Stackelberg game. Considering the high dimensionality of the continuous action space, a multi-agent double actors deep deterministic policy gradient algorithm is employed to achieve the optimal training-ratio of EDs and the payment policies of edge servers. Numerical results validate the effectiveness and efficiency of our proposed incentive mechanism.",
        "DOI": "10.1109/TVT.2023.3276898",
        "affiliation_name": "Hubei University of Technology",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "An Overview of Pulses Production in India: Retrospect and Prospects of the Future Food with an Application of Hybrid Models",
        "paper_author": "Mishra P.",
        "publication": "National Academy Science Letters",
        "citied_by": "11",
        "cover_date": "2023-10-01",
        "Abstract": "Forecasts are valuable to countries to make informed business decisions and develop data-driven strategies. The production of pulses is an integral part of agricultural diversification initiatives because it offers promising economic opportunities to reduce rural poverty and unemployment in developing countries. Pulses are the cheapest source of protein needed for human health. India's pulses production guidelines must be based on accurate and best forecast models. Comparing classical statistical and machine learning models based on different scientific data series is the subject of high-level research today. This study focused on the forecasting behaviour of pulses production for India, Karnataka, Madhya Pradesh, Maharashtra, Rajasthan and Uttar Pradesh. The data series was split into a training dataset (1950–2014) and a testing dataset (2015–2019) for model building and validation purposes, respectively. ARIMA, NNAR and hybrid models were used and compared on training and validation datasets based on goodness of fit (RMSE, MAE and MASE). This research demonstrates that due to the diverse agricultural conditions across different provinces in India, there is no single model that can accurately predict pulse production in all regions. This study’s highest accuracy model is ARIMA. ARIMA outperforms NNAR, a machine learning model. Pulse production in India, Rajasthan, and Madhya Pradesh will expand by 26.11%, 12.62%, and 0.51% from 2020 to 2030, whereas it would decline by − 6.5%, − 6.21%, and − 6.76 per cent in Karnataka, Maharashtra, and Uttar Pradesh, respectively. The current forecast results could allow policymakers to develop more aggressive food security and sustainability plans and better Indian pulses production policies in the future.",
        "DOI": "10.1007/s40009-023-01267-2",
        "affiliation_name": "Lovely Professional University",
        "affiliation_city": "Phagwara",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Executive-centered AI? Designing predictive systems for the public sector",
        "paper_author": "Henriksen A.",
        "publication": "Social Studies of Science",
        "citied_by": "8",
        "cover_date": "2023-10-01",
        "Abstract": "Recent policies and research articles call for turning AI into a form of IA (‘intelligence augmentation’), by envisioning systems that center on and enhance humans. Based on a field study at an AI company, this article studies how AI is performed as developers enact two predictive systems along with stakeholders in public sector accounting and public sector healthcare. Inspired by STS theories about values in design, we analyze our empirical data focusing especially on how objectives, structured performances, and divisions of labor are built into the two systems and at whose expense. Our findings reveal that the development of the two AI systems is informed by politically motivated managerial interests in cost-efficiency. This results in AI systems that are (1) designed as managerial tools meant to enable efficiency improvements and cost reductions, and (2) enforced on professionals on the ‘shop floor’ in a top-down manner. Based on our findings and a discussion drawing on literature on the original visions of human-centered systems design from the 1960s, we argue that turning AI into IA seems dubious, and ask what human-centered AI really means and whether it remains an ideal not easily realizable in practice. More work should be done to rethink human-machine relationships in the age of big data and AI, in this way making the call for ethical and responsible AI more genuine and trustworthy.",
        "DOI": "10.1177/03063127231163756",
        "affiliation_name": "Aarhus Universitet",
        "affiliation_city": "Aarhus",
        "affiliation_country": "Denmark"
    },
    {
        "paper_title": "Vaccine sentiment analysis using BERT + NBSVM and geo-spatial approaches",
        "paper_author": "Umair A.",
        "publication": "Journal of Supercomputing",
        "citied_by": "11",
        "cover_date": "2023-10-01",
        "Abstract": "Since the spread of the coronavirus flu in 2019 (hereafter referred to as COVID-19), millions of people worldwide have been affected by the pandemic, which has significantly impacted our habits in various ways. In order to eradicate the disease, a great help came from unprecedentedly fast vaccines development along with strict preventive measures adoption like lockdown. Thus, world wide provisioning of vaccines was crucial in order to achieve the maximum immunization of population. However, the fast development of vaccines, driven by the urge of limiting the pandemic caused skeptical reactions by a vast amount of population. More specifically, the people’s hesitancy in getting vaccinated was an additional obstacle in fighting COVID-19. To ameliorate this scenario, it is important to understand people’s sentiments about vaccines in order to take proper actions to better inform the population. As a matter of fact, people continuously update their feelings and sentiments on social media, thus a proper analysis of those opinions is an important challenge for providing proper information to avoid misinformation. More in detail, sentiment analysis (Wankhade et al. in Artif Intell Rev 55(7):5731–5780, 2022. https://doi.org/10.1007/s10462-022-10144-1) is a powerful technique in natural language processing that enables the identification and classification of people feelings (mainly) in text data. It involves the use of machine learning algorithms and other computational techniques to analyze large volumes of text and determine whether they express positive, negative or neutral sentiment. Sentiment analysis is widely used in industries such as marketing, customer service, and healthcare, among others, to gain actionable insights from customer feedback, social media posts, and other forms of unstructured textual data. In this paper, Sentiment Analysis will be used to elaborate on people reaction to COVID-19 vaccines in order to provide useful insights to improve the correct understanding of their correct usage and possible advantages. In this paper, a framework that leverages artificial intelligence (AI) methods is proposed for classifying tweets based on their polarity values. We analyzed Twitter data related to COVID-19 vaccines after the most appropriate pre-processing on them. More specifically, we identified the word-cloud of negative, positive, and neutral words using an artificial intelligence tool to determine the sentiment of tweets. After this pre-processing step, we performed classification using the BERT + NBSVM model to classify people’s sentiments about vaccines. The reason for choosing to combine bidirectional encoder representations from transformers (BERT) and Naive Bayes and support vector machine (NBSVM) can be understood by considering the limitation of BERT-based approaches, which only leverage encoder layers, resulting in lower performance on short texts like the ones used in our analysis. Such a limitation can be ameliorated by using Naive Bayes and Support Vector Machine approaches that are able to achieve higher performance in short text sentiment analysis. Thus, we took advantage of both BERT features and NBSVM features to define a flexible framework for our sentiment analysis goal related to vaccine sentiment identification. Moreover, we enrich our results with spatial analysis of the data by using geo-coding, visualization, and spatial correlation analysis to suggest the most suitable vaccination centers to users based on the sentiment analysis outcomes. In principle, we do not need to implement a distributed architecture to run our experiments as the available public data are not massive. However, we discuss a high-performance architecture that will be used if the collected data scales up dramatically. We compared our approach with the state-of-art methods by comparing most widely used metrics like Accuracy, Precision, Recall and F-measure. The proposed BERT + NBSVM outperformed alternative models by achieving 73% accuracy, 71% precision, 88% recall and 73% F-measure for classification of positive sentiments while 73% accuracy, 71% precision, 74% recall and 73% F-measure for classification of negative sentiments respectively. These promising results will be properly discussed in next sections. The use of artificial intelligence methods and social media analysis can lead to a better understanding of people’s reactions and opinions about any trending topic. However, in the case of health-related topics like COVID-19 vaccines, proper sentiment identification could be crucial for implementing public health policies. More in detail, the availability of useful findings on user opinions about vaccines can help policymakers design proper strategies and implement ad-hoc vaccination protocols according to people’s feelings, in order to provide better public service. To this end, we leveraged geospatial information to support effective recommendations for vaccination centers.",
        "DOI": "10.1007/s11227-023-05319-8",
        "affiliation_name": "Università degli Studi di Napoli Federico II",
        "affiliation_city": "Naples",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Topology-based radiomic features for prediction of parotid gland cancer malignancy grade in magnetic resonance images",
        "paper_author": "Ikushima K.",
        "publication": "Magnetic Resonance Materials in Physics, Biology and Medicine",
        "citied_by": "4",
        "cover_date": "2023-10-01",
        "Abstract": "Purpose: The malignancy grades of parotid gland cancer (PGC) have been assessed for a decision of treatment policies. Therefore, we have investigated the feasibility of topology-based radiomic features for the prediction of parotid gland cancer (PGC) malignancy grade in magnetic resonance (MR) images. Materials and methods: Two-dimensional T1- and T2-weighted MR images of 39 patients with PGC were selected for this study. Imaging properties of PGC can be quantified using the topology, which could be useful for assessing the number of the k-dimensional holes or heterogeneity in PGC regions using invariants of the Betti numbers. Radiomic signatures were constructed from 41,472 features obtained after a harmonization using an elastic net model. PGC patients were stratified using a logistic classification into low/intermediate- and high-grade malignancy groups. The training data were increased by four times to avoid the overfitting problem using a synthetic minority oversampling technique. The proposed approach was assessed using a 4-fold cross-validation test. Results: The highest accuracy of the proposed approach was 0.975 for the validation cases, whereas that of the conventional approach was 0.694. Conclusion: This study indicated that topology-based radiomic features could be feasible for the noninvasive prediction of the malignancy grade of PGCs.",
        "DOI": "10.1007/s10334-023-01084-0",
        "affiliation_name": "Graduate School of Medical Sciences",
        "affiliation_city": "Fukuoka",
        "affiliation_country": "Japan"
    },
    {
        "paper_title": "Combining deep reinforcement learning with technical analysis and trend monitoring on cryptocurrency markets",
        "paper_author": "Kochliaridis V.",
        "publication": "Neural Computing and Applications",
        "citied_by": "9",
        "cover_date": "2023-10-01",
        "Abstract": "Cryptocurrency markets experienced a significant increase in the popularity, which motivated many financial traders to seek high profits in cryptocurrency trading. The predominant tool that traders use to identify profitable opportunities is technical analysis. Some investors and researchers also combined technical analysis with machine learning, in order to forecast upcoming trends in the market. However, even with the use of these methods, developing successful trading strategies is still regarded as an extremely challenging task. Recently, deep reinforcement learning (DRL) algorithms demonstrated satisfying performance in solving complicated problems, including the formulation of profitable trading strategies. While some DRL techniques have been successful in increasing profit and loss (PNL) measures, these techniques are not much risk-aware and present difficulty in maximizing PNL and lowering trading risks simultaneously. This research proposes the combination of DRL approaches with rule-based safety mechanisms to both maximize PNL returns and minimize trading risk. First, a DRL agent is trained to maximize PNL returns, using a novel reward function. Then, during the exploitation phase, a rule-based mechanism is deployed to prevent uncertain actions from being executed. Finally, another novel safety mechanism is proposed, which considers the actions of a more conservatively trained agent, in order to identify high-risk trading periods and avoid trading. Our experiments on 5 popular cryptocurrencies show that the integration of these three methods achieves very promising results.",
        "DOI": "10.1007/s00521-023-08516-x",
        "affiliation_name": "Aristotle University of Thessaloniki",
        "affiliation_city": "Thessaloniki",
        "affiliation_country": "Greece"
    },
    {
        "paper_title": "Explainability-based Trust Algorithm for electricity price forecasting models",
        "paper_author": "Heistrene L.",
        "publication": "Energy and AI",
        "citied_by": "6",
        "cover_date": "2023-10-01",
        "Abstract": "Advanced machine learning (ML) algorithms have outperformed traditional approaches in various forecasting applications, especially electricity price forecasting (EPF). However, the prediction accuracy of ML reduces substantially if the input data is not similar to the ones seen by the model during training. This is often observed in EPF problems when market dynamics change owing to a rise in fuel prices, an increase in renewable penetration, a change in operational policies, etc. While the dip in model accuracy for unseen data is a cause for concern, what is more, challenging is not knowing when the ML model would respond in such a manner. Such uncertainty makes the power market participants, like bidding agents and retailers, vulnerable to substantial financial loss caused by the prediction errors of EPF models. Therefore, it becomes essential to identify whether or not the model prediction at a given instance is trustworthy. In this light, this paper proposes a trust algorithm for EPF users based on explainable artificial intelligence techniques. The suggested algorithm generates trust scores that reflect the model's prediction quality for each new input. These scores are formulated in two stages: in the first stage, the coarse version of the score is formed using correlations of local and global explanations, and in the second stage, the score is fine-tuned further by the Shapley additive explanations values of different features. Such score-based explanations are more straightforward than feature-based visual explanations for EPF users like asset managers and traders. A dataset from Italy's and ERCOT's electricity market validates the efficacy of the proposed algorithm. Results show that the algorithm has more than 85% accuracy in identifying good predictions when the data distribution is similar to the training dataset. In the case of distribution shift, the algorithm shows the same accuracy level in identifying bad predictions.",
        "DOI": "10.1016/j.egyai.2023.100259",
        "affiliation_name": "Pandit Deendayal Energy University",
        "affiliation_city": "Gandhinagar",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Hierarchical multi-robot navigation and formation in unknown environments via deep reinforcement learning and distributed optimization",
        "paper_author": "Chang L.",
        "publication": "Robotics and Computer-Integrated Manufacturing",
        "citied_by": "15",
        "cover_date": "2023-10-01",
        "Abstract": "Compared with a single robot, Multi-robot Systems (MRSs) can undertake more challenging tasks in complex scenarios benefiting from the increased transportation capacity and fault tolerance. This paper presents a hierarchical framework for multi-robot navigation and formation in unknown environments with static and dynamic obstacles, where the robots compute and maintain the optimized formation while making progress to the target together. In the proposed framework, each single robot is capable of navigating to the global target in unknown environments based on its local perception, and only limited communication among robots is required to obtain the optimal formation. Accordingly, three modules are included in this framework. Firstly, we design a learning network based on Deep Deterministic Policy Gradient (DDPG) to address the global navigation task for single robot, which derives end-to-end policies that map the robot's local perception into its velocity commands. To handle complex obstacle distributions (e.g. narrow/zigzag passage and local minimum) and stabilize the training process, strategies of Curriculum Learning (CL) and Reward Shaping (RS) are combined. Secondly, for an expected formation, its real-time configuration is optimized by a distributed optimization. This configuration considers surrounding obstacles and current formation status, and provides each robot with its formation target. Finally, a velocity adjustment method considering the robot kinematics is designed which adjusts the navigation velocity of each robot according to its formation target, making all the robots navigate to their targets while maintaining the expected formation. This framework allows for formation online reconfiguration and is scalable with the number of robots. Extensive simulations and 3-D evaluations verify that our method can navigate the MRS in unknown environments while maintaining the optimal formation.",
        "DOI": "10.1016/j.rcim.2023.102570",
        "affiliation_name": "Nanjing University of Information Science &amp; Technology",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Self-Punishment and Reward Backfill for Deep Q-Learning",
        "paper_author": "Bonyadi M.R.",
        "publication": "IEEE Transactions on Neural Networks and Learning Systems",
        "citied_by": "3",
        "cover_date": "2023-10-01",
        "Abstract": "Reinforcement learning (RL) agents learn by encouraging behaviors, which maximizes their total reward, usually provided by the environment. In many environments, however, the reward is provided after a series of actions rather than each single action, leading the agent to experience ambiguity in terms of whether those actions are effective, an issue known as the credit assignment problem. In this brief, we propose two strategies inspired by behavioral psychology to enable the agent to intrinsically estimate more informative reward values for actions with no reward. The first strategy, called self-punishment (SP), discourages the agent from making mistakes that lead to undesirable terminal states. The second strategy, called the reward backfill (RB), backpropagates the rewards between two rewarded actions. We prove that, under certain assumptions and regardless of the RL algorithm used, these two strategies maintain the order of policies in the space of all possible policies in terms of their total reward and, by extension, maintain the optimal policy. Hence, our proposed strategies integrate with any RL algorithm that learns a value or action-value function through experience. We incorporated these two strategies into three popular deep RL approaches and evaluated the results on 30 Atari games. After parameter tuning, our results indicate that the proposed strategies improve the tested methods in over 65% of tested games by up to over 25 times performance improvement.",
        "DOI": "10.1109/TNNLS.2021.3140042",
        "affiliation_name": "The University of Queensland",
        "affiliation_city": "Brisbane",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "ENHANCING DATA TRANSMISSION FOR INTELLIGENT INFORMATION SYSTEMS USING SDN TECHNIQUE AND MACHINE LEARNING ALGORITHM",
        "paper_author": "Yadam M.S.",
        "publication": "Journal of Theoretical and Applied Information Technology",
        "citied_by": "0",
        "cover_date": "2023-09-30",
        "Abstract": "Intelligent information systems, which have recently undergone development and complexity, are now indispensable to the entire world. The networking strategy has unquestionably altered based on machine learning principles to be programable and dynamically configurable with the greatest flexibility and simplicity of use. The term \"software-defined network\"(SDN) refers to networks that are managed using software applications and SDN controllers as opposed to the more traditional network management consoles and commands, which require a lot of administrative overhead. To centralize network control and administration, SDN changed the topology of network devices to be more flexible and programable. The software-defined network's uses protocols for interacting with and managing switches is called OpenFlow (OF). With this protocol, the switches learn the routing information from the controller and then pass data packets based on this information. One of the most important components of the SDN is the controller, which is the smartest component of the network such as the Ryu controller. Including the importance of the Ryu controller in SDN. This article discussed how to enhance data traffic transmission and classification in the SDN environment. This research shows how we can track all data packets and traffics and automatically identify all data types and classify them correctly, so we can apply a security policy, bandwidth, and quota for each type. The most different thing we used is using a real SDN network environment and also connected a real physical lambda server that makes daily continuous training for all data traffic and synchs this at the same time with the SDN controller that applies this instantly on the real live traffic. Using Machine Learning (ML) and Artificial Intelligence (AI) to enhance the SDN environments and identify data traffic types automatically. The controller (using ML and AI) takes the needed action automatically according to the data types. Enhance security, Data Transmission, and Data Availability in the software-defined networking and Intelligent Systems environment.",
        "DOI": "NA",
        "affiliation_name": "Faculty of Computer and Information",
        "affiliation_city": "Mansoura",
        "affiliation_country": "Egypt"
    },
    {
        "paper_title": "Future Innovations in Novel Detection for Atrial Fibrillation (FIND-AF): Pilot study of an electronic health record machine learning algorithm-guided intervention to identify undiagnosed atrial fibrillation",
        "paper_author": "Nadarajah R.",
        "publication": "Open Heart",
        "citied_by": "4",
        "cover_date": "2023-09-30",
        "Abstract": "Introduction Atrial fibrillation (AF) is associated with a fivefold increased risk of stroke. Oral anticoagulation reduces the risk of stroke, but AF is elusive. A machine learning algorithm (Future Innovations in Novel Detection of Atrial Fibrillation (FIND-AF)) developed to predict incident AF within 6 months using data in primary care electronic health records (EHRs) could be used to guide AF screening. The objectives of the FIND-AF pilot study are to determine yields of AF during ECG monitoring across AF risk estimates and establish rates of recruitment and protocol adherence in a remote AF screening pathway. Methods and analysis The FIND-AF Pilot is an interventional, non-randomised, single-arm, open-label study that will recruit 1955 participants aged 30 years or older, without a history of AF and eligible for oral anticoagulation, identified as higher risk and lower risk by the FIND-AF risk score from their primary care EHRs, to a period of remote ECG monitoring with a Zenicor-ECG device. The primary outcome is AF diagnosis during ECG monitoring, and secondary outcomes include recruitment rates, withdrawal rates, adherence to ECG monitoring and prescription of oral anticoagulation to participants diagnosed with AF during ECG monitoring. Ethics and dissemination The study has ethical approval (the North West - Greater Manchester South Research Ethics Committee reference 23/NW/0180). Findings will be announced at relevant conferences and published in peer-reviewed journals in line with the Funder's open access policy. Trial registration number NCT05898165.",
        "DOI": "10.1136/openhrt-2023-002447",
        "affiliation_name": "University of Leeds, School of Medicine",
        "affiliation_city": "Leeds",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Changing health policy practices and evaluation of specialist physicians towards city hospitals",
        "paper_author": "Saylan B.",
        "publication": "Applied Research Approaches to Technology, Healthcare, and Business",
        "citied_by": "0",
        "cover_date": "2023-09-29",
        "Abstract": "Strategic changes and policy implementation have a significant impact on health and health-related issues. The motivation of this study is to evaluate the opinions of specialist physicians towards city hospitals, which is a new and controversial policy action, and to analyze the findings obtained from these opinions by using various classification and machine learning methods. In order to evaluate their views on city hospitals, specialist physicians were divided into three groups using hierarchical clustering method in terms of health service quality and efficiency, coordination of care components, interdisciplinary care teams, and integration of health services dimensions. The differences between these groups were found to be statistically significant in terms of four dimensions (p < 0.0001). Naive Bayes (AUC=0.896, F1=0.757), one of the machine learning techniques used to predict clusters obtained from four dimensions obtained from the evaluations of specialist physicians, was found to be the best predictor of fourdimensional classroom evaluations.",
        "DOI": "10.4018/979-8-3693-1630-6.ch023",
        "affiliation_name": "Hacettepe Üniversitesi",
        "affiliation_city": "Ankara",
        "affiliation_country": "Turkey"
    },
    {
        "paper_title": "Cardsformer: Grounding Language to Learn a Generalizable Policy in Hearthstone",
        "paper_author": "Xia W.",
        "publication": "Frontiers in Artificial Intelligence and Applications",
        "citied_by": "1",
        "cover_date": "2023-09-28",
        "Abstract": "Hearthstone is a widely played collectible card game that challenges players to strategize using cards with various effects described in natural language. While human players can easily comprehend card descriptions and make informed decisions, artificial agents struggle to understand the game's inherent rules and are unable to generalize their policies through natural language. To address this issue, we propose Cardsformer, a method capable of acquiring linguistic knowledge and learning a generalizable policy in Hearthstone. Cardsformer consists of a Prediction Model trained with offline trajectories to predict state transitions based on card descriptions and a Policy Model capable of generalizing its policy on unseen cards. To our knowledge, this is the first work to consider language knowledge in a card game. Experiments show that our approach significantly improves data efficiency and outperforms the state-of-the-art in Hearthstone even when there are untrained cards in the deck, inspiring a new perspective of tackling problems as such with knowledge representation from large language models. As the game constantly releases new cards along with new descriptions and new effects, the challenge in Hearthstone remains. To encourage further research, we make our code publicly available and publish PyStone, the code base of Hearthstone on which we conducted our experiments, as an open benchmark.",
        "DOI": "10.3233/FAIA230581",
        "affiliation_name": "University of Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Governance: a Pugwash council for the digital age",
        "paper_author": "Marshall W.",
        "publication": "Nature",
        "citied_by": "1",
        "cover_date": "2023-09-28",
        "Abstract": "NA",
        "DOI": "10.1038/d41586-023-03018-1",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "A look into the ethics of autonomous vehicles systems (AVS)",
        "paper_author": "Feng J.H.",
        "publication": "Autonomous Vehicles and Systems: A Technological and Societal Perspective",
        "citied_by": "0",
        "cover_date": "2023-09-27",
        "Abstract": "What choices should autonomous vehicle systems (AVS) make when they are faced in moral dilemma situations? Various research in both academic and industry has been conducted to construct an ethical framework for these autonomous systems. However, there are still ongoing discussions on the ethics and concerns of such systems. This chapter aims to explore and discuss in-depth of the current and advances in ethical issues and framework, both from an internal and external perspective of the AVS.",
        "DOI": "NA",
        "affiliation_name": "Carnegie Mellon University",
        "affiliation_city": "Pittsburgh",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "FGT-SAMK-NN: impact of the right to be forgotten using a lazy algorithm in data stream learning",
        "paper_author": "Peniche E.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-09-26",
        "Abstract": "\"Right to Be Forgotten\"is guaranteed by new international regulations on personal management data. This means that individuals can request the erasure of their data from third-party tools and services. However, this poses a challenge for machine learning estimators, who will need to forget parts of their knowledge. This paper examines the impact of learning and forgetting policies in Data Stream Learning. Storing data or retraining learning models from scratch in data stream mining is usually not feasible due to the large volume of instances. Therefore, more efficient solutions are necessary to deal with the dynamic nature of online machine learning. To address this issue, we implemented FGT-SAMK-NN, an incremental version of one of the most knowledgeable algorithms in Data Stream lazy algorithms: The SAMK-NN classifier. FGT-SAMK-NN can erase its past data, and we investigate the impact of data forgetting on predictive performance. Our proposal is compared to the original SAMK-NN algorithm using four non-stationary stream datasets. Our results demonstrate that evaluation metrics did not undergo significant changes, which may support the idea that has a good architecture for adaptations of the proposed nature. However, it was also noted that the processing time is very high for cases involving more forgettings, which may indicate that the high complexity of the model creates conflicts if the pattern of data streams, where the algorithm is used, involves a high forgetfulness rate.",
        "DOI": "10.1145/3614321.3614322",
        "affiliation_name": "Universidade Federal Fluminense",
        "affiliation_city": "Niteroi",
        "affiliation_country": "Brazil"
    },
    {
        "paper_title": "TwiSP: a framework for exploring polarized issues in Twitter",
        "paper_author": "Diaz G.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2023-09-26",
        "Abstract": "Social and political polarization has become a dramatically intensifying force that is having a huge impact on political discourse, public policies and electoral outcomes in the 21st century. Twitter is a social media platform that mirrors to a large extent the sociological notion of public opinion, and has notably fueled these polarization dynamics worldwide. A proper understanding of how different issues become polarized in Twitter and their interrelationship is therefore crucial for the development of effective policies and governance strategies in our democracies. This paper introduces TwiSP, a framework for analyzing polarization on controversial topics in Twitter. TwiSP utilizes a combination of two cutting-edge machine learning techniques: stance detection for identifying attitudes and perspectives and BERTopic for topic modeling. The outcome of TwiSP is a visual tree-like representation of all tweets related to conflicting topics (rooted in a particular topic), contrasting their relationship using different colors to denote the degree of polarization. As a case study, we show how the TwiSP framework can be used for analyzing polarized issues in the context of the COVID-19 vaccine, exploring the resulting degree of polarization and the key topics driving it. The results reveal the diversity of opinions and the presence of highly polarized clusters in social media discussions. We contend that the TwiSP framework provides a novel and valuable tool for decision makers, helping them to recognize contentious issues behind the dynamics of polarization and ultimately identifying potential opportunities for bridging divides.",
        "DOI": "10.1145/3614321.3614324",
        "affiliation_name": "Consejo Nacional de Investigaciones Científicas y Técnicas",
        "affiliation_city": "Buenos Aires",
        "affiliation_country": "Argentina"
    },
    {
        "paper_title": "FaceWard: Face Anonymization in Group Photos",
        "paper_author": "Kilinç S.",
        "publication": "Proceedings of the 25th International Conference on Mobile Human-Computer Interaction, MobileHCI 2023 Companion",
        "citied_by": "1",
        "cover_date": "2023-09-26",
        "Abstract": "Sharing photos on social media and messaging services often result in a vast amount of personal data being made public online. As a result, it has become increasingly vital to devise measures that ensure privacy protection, especially for people who want to maintain social boundaries by hiding their faces in group photos. In this paper, we propose FaceWard, an automatic system for face anonymization of people different from a target person. FaceWard is based on a pattern matching algorithm and supports different anonymization policies such as blur or smiley overlays. Taken together, FaceWard yields a very efficient solution, eliminating the need for computationally expensive training of complex machine learning models, thus offering a practical trade-off between prediction accuracy and data availability. FaceWard is publicly available as open source software.",
        "DOI": "10.1145/3565066.3608249",
        "affiliation_name": "University of Luxembourg",
        "affiliation_city": "Esch-sur-Alzette",
        "affiliation_country": "Luxembourg"
    },
    {
        "paper_title": "Healthcare through data science - A transdisciplinary perspective from Latin America",
        "paper_author": "Chatterjee P.",
        "publication": "IEEE Technology and Engineering Management Society Body of Knowledge (TEMSBOK)",
        "citied_by": "0",
        "cover_date": "2023-09-25",
        "Abstract": "Research and innovation in the domain of technology have seen a strong transformation to transdisciplinary collaboration in recent years. Fields like Artificial Intelligence have amplified their scope, transcending through various disciplines of science and engineering. In the domain of healthcare in Latin America, digital transformation through data science has extended from the top to bottom, extending from digital administration and data-backed healthcare policies on one hand, smart eHealth devices and intelligent monitoring through Internet of Things and machine learning, on the other hand. The third sustainable development goal of the United Nations is \"good health and well-being.\" In this aspect, Artificial Intelligence plays a strong role in predictive, preventive, participatory, and personalized healthcare. This chapter focuses on a holistic view of the digital transformation of healthcare in Latin America through Artificial Intelligence and transdisciplinary cooperation. It is based on the following aspects - strategic collaboration between the medical and engineering domains like physics, electronics, statistics, biology, and computer science for seamless transfer of technology, harnessing data science tools of machine learning for accurate predictions of diseases, thus exercising preventive healthcare, and integration of education, research, and innovation through international academic and scientific collaborations. This work illustrates the focal goal of providing healthcare services following best practices through digital data-powered transformations and transdisciplinary exchange around medicine and bioengineering in a Latin American perspective.",
        "DOI": "10.1002/9781119987635.ch17",
        "affiliation_name": "Universidad de la Republica",
        "affiliation_city": "Montevideo",
        "affiliation_country": "Uruguay"
    },
    {
        "paper_title": "Artificial Intelligence, Climate Change and Innovative Democratic Governance",
        "paper_author": "Cortez F.",
        "publication": "European Journal of Risk Regulation",
        "citied_by": "1",
        "cover_date": "2023-09-25",
        "Abstract": "This policy-oriented article explores the sustainability dimension of digitalisation and artificial intelligence (AI). While AI can contribute to halting climate change via targeted applications in specific domains, AI technology in general could also have detrimental effects for climate policy goals. Moreover, digitalisation and AI can have an indirect effect on climate policy via their impact on political processes. It will be argued that, if certain conditions are fulfilled, AI-facilitated digital tools could help with setting up frameworks for bottom-up citizen participation that could generate the legitimacy and popular buy-in required for speedy transformations needed to reach net zero such as radically revamping the energy infrastructure among other crucial elements of the green transition. This could help with ameliorating a potential dilemma of voice versus speed regarding the green transition. The article will further address the nexus between digital applications such as AI and climate justice. Finally, the article will consider whether innovative governance methods could instil new dynamism into the multi-level global climate regime, such as by facilitating interlinkages and integration between different levels. Before implementing innovative governance arrangements, it is crucial to assess whether they do not exacerbate old or even generate new inequalities of access and participation.",
        "DOI": "10.1017/err.2023.60",
        "affiliation_name": "Ethicqual",
        "affiliation_city": "The Hague",
        "affiliation_country": "Netherlands"
    },
    {
        "paper_title": "Data security techniques in cloud computing based on machine learning algorithms and cryptographic algorithms: Lightweight algorithms and genetics algorithms",
        "paper_author": "Thabit F.",
        "publication": "Concurrency and Computation: Practice and Experience",
        "citied_by": "12",
        "cover_date": "2023-09-25",
        "Abstract": "Cloud computing (CC) refers to the on-demand availability of network resources, particularly data storage and processing power, without requiring special or direct administration by users. CC, which just made its debut as a collection of public and private data centers, provides clients with a unified platform throughout the Internet. Cloud computing has revolutionized the world, opening up new horizons with bright potential due to its performance, accessibility, low cost, and many other benefits. Due to the exponential rise of cloud computing, systems based on cloud computing now require an effective data security mechanism. Comprehensive security policies, corporate security culture, and cloud security solutions are used to ensure the level of cloud data security. Many techniques exist to protect data communication in the cloud environment, including encryption. Encryption algorithms play an important role in information security systems and various cloud computing-based systems. Current researchers have focused on lightweight cryptography, genetics-based cryptography, and machine learning (ML) algorithms for security in CC. This review study analyses CC security threats, problems, and solutions that use one or more algorithms. The work discusses several lightweight cryptographies, genetics-based cryptography and different ML algorithms that are used to overcome cloud security issues, including supervised, unsupervised, semi-supervised, and reinforcement learning. Moreover, we enlist future research directions to secure CC models.",
        "DOI": "10.1002/cpe.7691",
        "affiliation_name": "NYU Abu Dhabi",
        "affiliation_city": "Abu Dhabi",
        "affiliation_country": "United Arab Emirates"
    },
    {
        "paper_title": "A causal roadmap for generating high-quality real-world evidence",
        "paper_author": "Dang L.E.",
        "publication": "Journal of Clinical and Translational Science",
        "citied_by": "18",
        "cover_date": "2023-09-22",
        "Abstract": "Increasing emphasis on the use of real-world evidence (RWE) to support clinical policy and regulatory decision-making has led to a proliferation of guidance, advice, and frameworks from regulatory agencies, academia, professional societies, and industry. A broad spectrum of studies use real-world data (RWD) to produce RWE, ranging from randomized trials with outcomes assessed using RWD to fully observational studies. Yet, many proposals for generating RWE lack sufficient detail, and many analyses of RWD suffer from implausible assumptions, other methodological flaws, or inappropriate interpretations. The Causal Roadmap is an explicit, itemized, iterative process that guides investigators to prespecify study design and analysis plans; it addresses a wide range of guidance within a single framework. By supporting the transparent evaluation of causal assumptions and facilitating objective comparisons of design and analysis choices based on prespecified criteria, the Roadmap can help investigators to evaluate the quality of evidence that a given study is likely to produce, specify a study to generate high-quality RWE, and communicate effectively with regulatory agencies and other stakeholders. This paper aims to disseminate and extend the Causal Roadmap framework for use by clinical and translational researchers; three companion papers demonstrate applications of the Causal Roadmap for specific use cases.",
        "DOI": "10.1017/cts.2023.635",
        "affiliation_name": "Moderna Therapeutics",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Curiosity Creates Diversity in Policy Search",
        "paper_author": "Le Tolguenec P.A.",
        "publication": "ACM Transactions on Evolutionary Learning and Optimization",
        "citied_by": "2",
        "cover_date": "2023-09-20",
        "Abstract": "When searching for policies, reward-sparse environments often lack sufficient information about which behaviors to improve upon or avoid. In such environments, the policy search process is bound to blindly search for reward-yielding transitions and no early reward can bias this search in one direction or another. A way to overcome this is to use intrinsic motivation in order to explore new transitions until a reward is found. In this work, we use a recently proposed definition of intrinsic motivation, Curiosity, in an evolutionary policy search method. We propose Curiosity-ES,1 an evolutionary strategy adapted to use Curiosity as a fitness metric. We compare Curiosity-ES with other evolutionary algorithms intended for exploration, as well as with Curiosity-based reinforcement learning, and find that Curiosity-ES can generate higher diversity without the need for an explicit diversity criterion and leads to more policies which find reward.",
        "DOI": "10.1145/3605782",
        "affiliation_name": "Université de Toulouse",
        "affiliation_city": "Toulouse",
        "affiliation_country": "France"
    },
    {
        "paper_title": "Combining Evolution and Deep Reinforcement Learning for Policy Search: A Survey",
        "paper_author": "Sigaud O.",
        "publication": "ACM Transactions on Evolutionary Learning and Optimization",
        "citied_by": "16",
        "cover_date": "2023-09-20",
        "Abstract": "Deep neuroevolution and deep Reinforcement Learning have received a lot of attention over the past few years. Some works have compared them, highlighting their pros and cons, but an emerging trend combines them so as to benefit from the best of both worlds. In this article, we provide a survey of this emerging trend by organizing the literature into related groups of works and casting all the existing combinations in each group into a generic framework. We systematically cover all easily available papers irrespective of their publication status, focusing on the combination mechanisms rather than on the experimental results. In total, we cover 45 algorithms more recent than 2017. We hope this effort will favor the growth of the domain by facilitating the understanding of the relationships between the methods, leading to deeper analyses, outlining missing useful comparisons and suggesting new combinations of mechanisms.",
        "DOI": "10.1145/3569096",
        "affiliation_name": "Institut des Systèmes Intelligents et de Robotique",
        "affiliation_city": "Paris",
        "affiliation_country": "France"
    },
    {
        "paper_title": "Reaching the limit in autonomous racing: Optimal control versus reinforcement learning",
        "paper_author": "Song Y.",
        "publication": "Science Robotics",
        "citied_by": "68",
        "cover_date": "2023-09-20",
        "Abstract": "A central question in robotics is how to design a control system for an agile mobile robot. This paper studies this question systematically, focusing on a challenging setting: autonomous drone racing. We show that a neural network controller trained with reinforcement learning (RL) outperformed optimal control (OC) methods in this setting. We then investigated which fundamental factors have contributed to the success of RL or have limited OC. Our study indicates that the fundamental advantage of RL over OC is not that it optimizes its objective better but that it optimizes a better objective. OC decomposes the problem into planning and control with an explicit intermediate representation, such as a trajectory, that serves as an interface. This decomposition limits the range of behaviors that can be expressed by the controller, leading to inferior control performance when facing unmodeled effects. In contrast, RL can directly optimize a task-level objective and can leverage domain randomization to cope with model uncertainty, allowing the discovery of more robust control responses. Our findings allowed us to push an agile drone to its maximum performance, achieving a peak acceleration greater than 12 times the gravitational acceleration and a peak velocity of 108 kilometers per hour. Our policy achieved superhuman control within minutes of training on a standard workstation. This work presents a milestone in agile robotics and sheds light on the role of RL and OC in robot control.",
        "DOI": "10.1126/scirobotics.adg1462",
        "affiliation_name": "Intel Corporation",
        "affiliation_city": "Santa Clara",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Spatial assessment of topsoil zinc concentrations in Europe",
        "paper_author": "Van Eynde E.",
        "publication": "Science of the Total Environment",
        "citied_by": "23",
        "cover_date": "2023-09-20",
        "Abstract": "Zinc (Zn) is essential to sustain crop production and human health, while it can be toxic when present in excess. In this manuscript, we applied a machine learning model on 21,682 soil samples from the Land Use and Coverage Area frame Survey (LUCAS) topsoil database of 2009/2012 to assess the spatial distribution in Europe of topsoil Zn concentrations measured by aqua regia extraction, and to identify the influence of natural drivers and anthropogenic sources on topsoil Zn concentrations. As a result, a map was produced showing topsoil Zn concentrations in Europe at a resolution of 250 m. The mean predicted Zn concentration in Europe was 41 mg kg−1, with a root mean squared error of around 40 mg kg−1 calculated for independent soil samples. We identified clay content as the most important factor explaining the overall distribution of soil Zn in Europe, with lower Zn concentrations in coarser soils. Next to texture, low Zn concentrations were found in soils with low pH (e.g. Podzols), as well as in soils with pH above 8 (i.e., Calcisols). The presence of deposits and mining activities mainly explained the occurrence of relatively high Zn concentrations above 167 mg kg−1 (the one percentile highest concentrations) within 10 km from these sites. In addition, the relatively higher Zn levels found in grasslands in regions with high livestock density may point to manure as a significant source of Zn in these soils. The map developed in this study can be used as a reference to assess the eco-toxicological risks associated with soil Zn concentrations in Europe and areas with Zn deficiency. In addition, it can provide a baseline for future policies in the context of pollution, soil health, human health, and crop nutrition.",
        "DOI": "10.1016/j.scitotenv.2023.164512",
        "affiliation_name": "European Commission Joint Research Centre",
        "affiliation_city": "Brussels",
        "affiliation_country": "Belgium"
    },
    {
        "paper_title": "Data analytics for management, banking and finance: Theories and application",
        "paper_author": "Saâdaoui F.",
        "publication": "Data Analytics for Management, Banking and Finance: Theories and Application",
        "citied_by": "1",
        "cover_date": "2023-09-19",
        "Abstract": "This book is a practical guide on the use of various data analytics and visualization techniques and tools in the banking and financial sectors. It focuses on how combining expertise from interdisciplinary areas, such as machine learning and business analytics, can bring forward a shared vision on the benefits of data science from the research point of view to the evaluation of policies. It highlights how data science is reshaping the business sector. It includes examples of novel big data sources and some successful applications on the use of advanced machine learning, natural language processing, networks analysis, and time series analysis and forecasting, among others, in the banking and finance. It includes several case studies where innovative data science models is used to analyse, test or model some crucial phenomena in banking and finance. At the same time, the book is making an appeal for a further adoption of these novel applications in the field of economics and finance so that they can reach their full potential and support policy-makers and the related stakeholders in the transformational recovery of our societies. The book is for stakeholders involved in research and innovation in the banking and financial sectors, but also those in the fields of computing, IT and managerial information systems, helping through this new theory to better specify the new opportunities and challenges. The many real cases addressed in this book also provide a detailed guide allowing the reader to realize the latest methodological discoveries and the use of the different Machine Learning approaches (supervised, unsupervised, reinforcement, deep, etc.) and to learn how to use and evaluate performance of new data science tools and frameworks.",
        "DOI": "10.1007/978-3-031-36570-6",
        "affiliation_name": "Faculté des Sciences de Monastir",
        "affiliation_city": "Monastir",
        "affiliation_country": "Tunisia"
    },
    {
        "paper_title": "Emerging technologies for digital infrastructure development",
        "paper_author": "Rana M.E.",
        "publication": "Emerging Technologies for Digital Infrastructure Development",
        "citied_by": "0",
        "cover_date": "2023-09-18",
        "Abstract": "Emerging Technologies for Digital Infrastructure Development is a comprehensive and insightful book that reviews the transformative impact of cutting-edge technologies on the digital landscape. It presents 16 topics, from e-commerce consumer behavior to AI applications in healthcare and cybersecurity, this book offers a detailed overview of the role of technology in shaping the modern world. With a focus on bridging the digital divide in education, the book presents innovative solutions to contemporary challenges. The editors also emphasize the importance of privacy and security in an interconnected world by discussing cybersecurity measures and threat detection strategies. The book serves as a valuable resource for technology professionals, researchers, and academics, offering a deep dive into the latest trends and applications in digital infrastructure. It also caters to business leaders, policy makers, and students seeking to understand the transformative potential of emerging technologies. Key technologies highlighted in the book include machine learning and AI models, IoT, data analytics, recommendation systems and e-learning systems. Applications of these technologies are demonstrated for healthcare, e-commerce, cybersecurity, aviation and education sectors. Emerging Technologies for Digital Infrastructure Development offers insights and solutions that pave the way for a secure, efficient, and inclusive digital future.",
        "DOI": "10.2174/97898150809571230101",
        "affiliation_name": "Asia Pacific University of Technology and Innovation",
        "affiliation_city": "Kuala Lumpur",
        "affiliation_country": "Malaysia"
    },
    {
        "paper_title": "Dynamic scheduling with uncertain job types",
        "paper_author": "Shen Z.J.M.",
        "publication": "European Journal of Operational Research",
        "citied_by": "1",
        "cover_date": "2023-09-16",
        "Abstract": "Uncertain job types can arise as a result of predictive or diagnostic inaccuracy in healthcare or repair service systems and unknown preferences in matching service systems. In this paper, we study systems with multiple types of jobs, in which type information is imperfect and will be updated dynamically. Each job has a prior probability of belonging to a certain type which may be predicted by data, models, or experts. A job can only be processed by the right machine, and a job assigned to the wrong machine must be rescheduled. More information is learned from the mismatch, and job type probabilities are updated. The question is how to dynamically schedule all jobs so that they can be processed in a timely fashion. We use a novel coupling and inductive method to conduct optimality analysis. We obtain the near-optimal policy regarding completion time, named the less-uncertainty-first policy, when there are two types of jobs; the insights it yields are used to develop heuristic algorithms for more general cases. We also consider other objectives, including the number of mismatches and the total amount of time jobs spend in the system. In our numerical study, we examine the performance of the proposed heuristics when there are more than two types of jobs under two learning schemes: dedicated learning and exclusive learning. In the extension, we also analyze an online version of the problem in which jobs arrive sequentially to the system and must be assigned immediately and irrevocably without any knowledge of future jobs. We analyze the competitive ratios of different scheduling policies and find similar insights. It is essential that managers dynamically schedule services by leveraging predictive information and knowledge learned from mismatches. Our proposed less-uncertainty-first policy, which accounts for system dynamics to avoid mismatches and resource idling, can be used to improve system efficiency in various contexts.",
        "DOI": "10.1016/j.ejor.2023.02.013",
        "affiliation_name": "UC Berkeley’s Industrial Engineering and Operations Research Department",
        "affiliation_city": "Berkeley",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Intelligent Wargame Deduction Decision Method Based on Deep Reinforcement Learning",
        "paper_author": "Hu S.",
        "publication": "Jisuanji Gongcheng/Computer Engineering",
        "citied_by": "1",
        "cover_date": "2023-09-15",
        "Abstract": "Wargame deduction is an important method for cultivating modern military commanders. Introducing artificial intelligence technology in wargame deduction can simplify organizational processes and improve deduction efficiency. Owing to the complex situational information and incomplete inference information, intelligent wargame based on machine learning often reduces the sample efficiency of autonomous decision-making models. This paper proposes an intelligent wargame deduction decision-making method based on deep reinforcement learning. In response to the efficiency issue of intelligent wargame deduction and combat decision-making, a baseline is introduced into the strategy network, and the training of the policy network is accelerated. Subsequently, derivation and proof are presented, and a method for updating the parameters of the policy network after adding the baseline is proposed. The process of introducing the state-value function in the wargame deduction environment into the model is analyzed. Construct a Low Advantage Policy-Value Network(LAPVN) model and its training framework for wargame deduction under traditional policy-value networks, and construct the model using battlefield situational awareness methods. In a wargame combat experimental environment that approximately conforms to military operational rules, the traditional policy-value network and LAPVN are compared for training. In 400 self-game training sessions, the loss value of the LAPVN model decreases from 5.3 to 2.3, and the convergence is faster than that of the traditional policy-value network. The KL divergence of the LAPVN model is very close to zero during the training process.",
        "DOI": "10.19678/j.issn.1000-3428.0067067",
        "affiliation_name": "People's Liberation Army College of International Relations",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Machine learning approaches reveal highly heterogeneous air quality co-benefits of the energy transition",
        "paper_author": "Zhang D.",
        "publication": "iScience",
        "citied_by": "1",
        "cover_date": "2023-09-15",
        "Abstract": "Estimating health benefits of reducing fossil fuel use from improved air quality provides important rationales for carbon emissions abatement. Simulating pollution concentration is a crucial step of the estimation, but traditional approaches often rely on complicated chemical transport models that require extensive expertise and computational resources. In this study, we develop a machine learning framework that is able to provide precise and robust annual average fine particle (PM2.5) concentration estimations directly from a high-resolution fossil energy use dataset. Applications of the framework with Chinese data reveal highly heterogeneous health benefits of avoiding premature mortality by reducing fossil fuel use in different sectors and regions in China with a mean of $19/tCO2 and a standard deviation of $38/tCO2. Reducing rural and residential coal use offers the highest co-benefits with a mean of $151/tCO2. Our findings prompt careful policy designs to maximize cost-effectiveness in the transition toward a carbon-neutral energy system.",
        "DOI": "10.1016/j.isci.2023.107652",
        "affiliation_name": "MIT School of Engineering",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Role of tie channel on wetland hydrological security and sustenance",
        "paper_author": "Pal S.",
        "publication": "Journal of Cleaner Production",
        "citied_by": "4",
        "cover_date": "2023-09-15",
        "Abstract": "How far the de-linking and morphological and hydrological degradation of tie channels connecting river to the wetland are caused for the areal and hydrological transformation of a wetland was not received enough attention in previous literature. This paper tried to explain this about the confluence reach of the Dwarka and Brahmni rivers in Moribund deltaic India. In order to explain the linkages 16 tie channels’ change depicting variables and seven wetland area and hydrological transformation (consistency, hydro-period, water depth, water richness etc.) related variables were taken. Ordinary least square (OLS) regression was applied for explain the linkage. Machine learning approaches were applied for water richness mapping. Tie channel evolution was digitized from the Survey of India (SOI) toposheet (1974) and historical Google Earth images. The result revealed that, the tie channel witnessed morphological and hydrological degradations like channel constriction, channel clogging, flow lowering etc. and these were further identified as some vectors of areal shrinkage and growing hydrological insecurity in the linked wetlands. The total wetland area declined from 44.89 km2 to 16.86 km2 from 1991 to 2021. The rate of areal loss, shallowing WD, growing inconsistency of water presence, narrowing HP, and weakening WR was found high in recent times due to de-linking and degradations of tie channels. This approach of explanation of hydrological insecurity of wetland in relation to tie channel degradations, de-linking and findings are quite unique. So, re-linking and morphological correction of the tie channels would be a good policy for wetland restoration.",
        "DOI": "10.1016/j.jclepro.2023.138162",
        "affiliation_name": "University of Gour Banga",
        "affiliation_city": "Malda",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Machine learning-enabled design of metasurface based near-perfect daytime radiative cooler",
        "paper_author": "Su W.",
        "publication": "Solar Energy Materials and Solar Cells",
        "citied_by": "12",
        "cover_date": "2023-09-15",
        "Abstract": "Global warming and the energy crisis are major challenges facing the world today. Traditional structural design methods for radiative coolers are no longer sufficient to meet the increasingly demanding realities of these challenges. Simplifying the design process and optimizing structural parameters have become pressing issues. In this paper, a new optimization method for metasurface based daytime radiative cooler is proposed, which utilizes the K-nearest neighbor (KNN) algorithm in machine learning (ML) to construct regression models and optimize designs for two structures of quadrangular prismatic metasurface (QPM) and circular truncated cone metasurface (CTCM). The results show that the atmospheric window emissivity (mainly in the 8–13 μm range) of QPM and CTCM reaches 96.98% and 97.81%, respectively, while the absorptivity/emissivity in solar spectrum is only 9.34% and 7.21%. The mean absolute percentage error (MAPE) of the KNN regression model is only 0.74% and 0.29%, which is significantly better than other classical ML algorithms. Meanwhile, the designed structures achieve net cooling power of 63.48 W/m2 and 90.13 W/m2 at ambient temperature for both QPM and CTCM. This systematic study provides a novel approach for the design of daytime radiative coolers.",
        "DOI": "10.1016/j.solmat.2023.112488",
        "affiliation_name": "Hohai University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Identifying small decentralized solar systems in aerial images using deep learning",
        "paper_author": "Frimane Â.",
        "publication": "Solar Energy",
        "citied_by": "4",
        "cover_date": "2023-09-15",
        "Abstract": "Statistics on installed solar energy systems (SES) play a crucial role in the solar energy industry, providing valuable information for a wide range of stakeholders, such as policy makers, authorities, and financial evaluators. For example, grid operators rely on accurate data on photovoltaic penetration levels to ensure the quality and stability of the power supply. In this research, we present an automatic approach helping generate these statistics using deep learning and image processing techniques. Our proposed model is a machine learning approach that utilizes a specific architecture of convolutional neural networks (CNN) called the “U-net” to detect SES from aerial images. We experimented different network settings to enhance the SES identification performance. In this study, the model was evaluated using two datasets from different locations, one from Sweden and one from Germany. Additionally, the model was trained and tested on a combination of both datasets. The impact of image resolution was also examined. The experimental results show that this architecture performs better than many recent CNN models that have been proposed in the literature for the task of SES identification from aerial images. To make it easy for others to replicate our findings, We have shared all the scripts, software, and dependencies required for running the model in this paper, along with instructions on how to use it in Appendix A.",
        "DOI": "10.1016/j.solener.2023.111822",
        "affiliation_name": "Uppsala Universitet",
        "affiliation_city": "Uppsala",
        "affiliation_country": "Sweden"
    },
    {
        "paper_title": "MERLIN: Multi-agent offline and transfer learning for occupant-centric operation of grid-interactive communities",
        "paper_author": "Nweye K.",
        "publication": "Applied Energy",
        "citied_by": "21",
        "cover_date": "2023-09-15",
        "Abstract": "Building and power generation decarbonization present new challenges in electric grid reliability as a result of renewable energy source intermittency and increase in grid load caused by end-use electrification. To restore reliability, grid-interactive efficient buildings can provide grid flexibility services through demand response. Reinforcement learning is well-suited for energy management in grid-interactive efficient buildings as it is able to adapt to unique building characteristics compared to rule-based control and model predictive control. Yet, factors hindering the adoption of reinforcement learning in real-world applications include its sample inefficiency during training, control security and generalizability. Here we address these challenges by proposing the MERLIN framework for the training, evaluation, deployment and transfer of control policies for distributed energy resources in grid-interactive communities for different levels of data availability. We utilize a real-world community smart meter dataset to show that while independently trained battery control policies can learn unique occupant behavior and provide up to 60% performance improvement at the district level, transfer learning provides comparable building and district level performance while reducing training costs.",
        "DOI": "10.1016/j.apenergy.2023.121323",
        "affiliation_name": "Cockrell School of Engineering",
        "affiliation_city": "Austin",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Dynamic pricing for fast charging stations with deep reinforcement learning",
        "paper_author": "Cui L.",
        "publication": "Applied Energy",
        "citied_by": "12",
        "cover_date": "2023-09-15",
        "Abstract": "With the rapid development of electric vehicles (EVs) and charging infrastructures, the unbalanced utilization rate of fast charging stations (FCSTs) and the long waiting time for charging have aroused considerable attention. The incurred low operation profit of FCSTs and low satisfaction of EVs impose difficulties on the further development of EV industry. Existing literature ignored the influence of real-time charging price changes on traffic flow variation and EV charging determination during the dynamic price regulating process. This paper focuses on solving these crucial issues in the dynamic pricing for FCSTs with deep reinforcement learning (DRL). Firstly, considering the spatial–temporal interactions of different roads, a traffic flow prediction model is proposed based on the LSTM combined with the GNN-FiLM. Then, the Origin-Destination (OD) estimation is used to estimate the charging requirements of EVs based on the predicted traffic flow, and a charging demand prediction method for FCSTs is developed by converting the EV satisfaction into economic costs with different dimensions. Then, the vehicle–road learning environment is built with the Markov decision process (MDP), and a dynamic pricing strategy based on the Deep Deterministic Policy Gradient (DDPG) learning is proposed to achieve the optimal charging prices of FCSTs with maximum operation profit. Moreover, during the learning process, the real-time charging price is renewed based on the predicted charging demand, and the future charging demand is further predicted under the renewed charging price until the optimal price is achieved. Finally, simulation results validate that the proposed dynamic pricing strategy effectively improves the profit of FCSTs, alleviates the road congestion, and improves the users’ satisfaction.",
        "DOI": "10.1016/j.apenergy.2023.121334",
        "affiliation_name": "Nanjing Institute of Technology",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Predictive control of power demand peak regulation based on deep reinforcement learning",
        "paper_author": "Fu Q.",
        "publication": "Journal of Building Engineering",
        "citied_by": "14",
        "cover_date": "2023-09-15",
        "Abstract": "As urbanization continues to accelerate, effectively managing peak electricity demand becomes increasingly critical to avoid power outages and system overloads that can negatively impact both buildings and power systems. To tackle this challenge, we propose a novel model-free predictive control method called “Dynamic Dual Predictive Control-Deep Deterministic Policy Gradient (D2PC-DDPG)\" based on a deep reinforcement learning framework. Our method employs the Deep Forest-Deep Q-Network (DF-DQN) model to predict electricity demand across multiple buildings, and based on the output of the DF-DQN model, applies the Deep Deterministic Policy Gradient (DDPG) algorithm to optimize coordinated control of energy storage systems, including hot and chilled water storage tanks in multiple buildings. Experimental results show that our proposed DF-DQN model outperforms other traditional machine learning, deep learning, and reinforcement learning methods in terms of prediction accuracy, such as mean absolute error (MAE), mean absolute percentage error (MAPE), and root mean square error (RMSE). Moreover, our D2PC-DDPG method achieves superior control performance and peak load reduction compared to other reinforcement learning methods and an RBC-based control method. Specifically, our method successfully reduced peak load by 27.1% and 21.4% over a two-week period in the same regions. To demonstrate the generalizability of our D2PC-DDPG method, we tested it in five different regions and compared its performance with an RBC-based control method. The results showed that our method achieved an average reduction of 16.6%, 7%, 9.2%, and 11% for ramping, 1-load_factor, average_daily_peak, and peak_demand, respectively. These findings demonstrate the effectiveness and practicality of our proposed method in addressing critical energy management issues in various urban environments.",
        "DOI": "10.1016/j.jobe.2023.106992",
        "affiliation_name": "Suzhou University of Science and Technology",
        "affiliation_city": "Suzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "The role of socio-demographic and economic characteristics on energy-related occupant behavior",
        "paper_author": "Palani H.",
        "publication": "Journal of Building Engineering",
        "citied_by": "10",
        "cover_date": "2023-09-15",
        "Abstract": "Building-related sectors use more energy than all other energy consuming sectors. Energy consumption behavior of occupants and their socio-demographic profiles are some of the key factors affecting building energy consumption. This study aims to understand the relationships between the socio-demographic and economic characteristics of occupants and their energy-related behavior. To achieve this aim, six hypotheses are developed and tested over three steps: (1) identification of the changes in the occupants’ energy-related behavior before and after being exposed to energy-saving interventions, (2) measurement of the socio-demographic profiles of the occupants, and (3) use of several machine learning methods to capture the relationships and test the hypotheses. Using decision tree learning to interpret the results, we find that education level, income, and age have the largest impact to predict the energy consumption behavior of occupants. The results also show four occupant profiles prone to switch to lower energy use: (1) age between 20 and 39 years old, education level of high school degree or lower, and income below 20 k; (2) age of 30 years old or younger, education level of high school degree or lower, and income above 100 k; (3) age of 40 years old or older, education level of bachelor's degree or lower, and income below 20 k; and (4) age of 59 years old or younger, education level of Master's degree or higher, and income above 100 k. This study can help decision-makers (e.g., utility companies, policy makers) to tailor incentive programs and policies to ultimately lower global building energy consumption.",
        "DOI": "10.1016/j.jobe.2023.106875",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Chicago",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Exploration of the factors that influence total phosphorus in surface water and an evaluation of surface water vulnerability based on an advanced algorithm and traditional index method",
        "paper_author": "Zhang H.",
        "publication": "Journal of Environmental Management",
        "citied_by": "7",
        "cover_date": "2023-09-15",
        "Abstract": "Due to the continuous influence of human activities, phosphorus pollution in surface water has become a persistent problem that needs to be addressed since phosphorous entails certain risks and degrees of damage to ecosystems and humans. The presence and accumulation of total phosphorus (TP) concentrations in surface waters is the result of a combined effect of many natural and anthropogenic factors, and it is often difficult to intuitively identify the individual importance of each factor in regard to the pollution of the aquatic environment. Considering these issues, this study provides a new methodology to better understand the vulnerability of surface water to TP pollution and the factors that influence TP pollution through the application of two modeling approaches. This includes the boosted regression tree (BRT), an advanced machine learning method, and the traditional comprehensive index method (CIM). Different factors, such as natural variables (including slope, soil texture, normalized difference vegetation index (NDVI), precipitation, and drainage density) and point and nonpoint source anthropogenic factors were included to model the vulnerability of surface water to TP pollution. Two methods were used to produce a vulnerability map of surface water to TP pollution. Pearson correlation analysis was used to validate the two vulnerability assessment methods. The results showed that BRT was more strongly correlated than CIM. In addition, the importance ranking results showed that slope, precipitation, NDVI, decentralized livestock farming and soil texture had a greater influence on TP pollution. Industrial activities, scale livestock farming and population density, which are all contributing sources of pollution, were all relatively less important. The introduced methodology can be used to quickly identify the area most vulnerable to TP pollution and to develop problem specific adaptive policies and measures to reduce the damage from TP pollution.",
        "DOI": "10.1016/j.jenvman.2023.118155",
        "affiliation_name": "Chinese Research Academy of Environmental Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Probabilistic carbon price prediction with quantile temporal convolutional network considering uncertain factors",
        "paper_author": "Cao Y.",
        "publication": "Journal of Environmental Management",
        "citied_by": "34",
        "cover_date": "2023-09-15",
        "Abstract": "Accurate carbon price projections can serve as valuable investment guides and risk warnings for carbon trading participants. However, the escalation of uncertain factors has brought numerous new hurdles to existing carbon price forecast methods. In this paper, we develop a novel probabilistic forecast model called quantile temporal convolutional network (QTCN) that can precisely describe the uncertain fluctuation of carbon prices. We also investigate the impact of external factors on carbon market prices, including energy prices, economic status, international carbon markets, environmental conditions, public concerns, and especially uncertain factors. Taking China's Hubei carbon emissions exchange as a study case, we verify that our QTCN outperforms other classical benchmark models in terms of prediction errors and actual trading returns. Our findings suggest that coal prices and EU carbon prices have the most significant effect on Hubei carbon price forecasting, while air quality index appears to be the least important. Besides, we demonstrate the great contribution of geopolitical risk and economic policy uncertainty to carbon price projections. The effect of these uncertainties is more pronounced when the carbon price is at a high quantile level. This research can offer valuable guidelines for carbon market risk management and provide new insight into carbon price formation mechanisms in the era of global conflict.",
        "DOI": "10.1016/j.jenvman.2023.118137",
        "affiliation_name": "North China Electric Power University (Baoding)",
        "affiliation_city": "Baoding",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Revolutionizing municipal solid waste management (MSWM) with machine learning as a clean resource: Opportunities, challenges and solutions",
        "paper_author": "Munir M.T.",
        "publication": "Fuel",
        "citied_by": "31",
        "cover_date": "2023-09-15",
        "Abstract": "Effective municipal solid waste management is essential for public health, environmental protection, economic benefits, and clean energy generation for future commercial applications. However, challenges like real-time monitoring, automated sorting systems, optimized collection routes, predictive maintenance, and public education and engagement hinder efficiency. Machine learning can address these challenges through real-time monitoring, automated sorting, route optimization, predictive maintenance, and targeted public education. Supervised, unsupervised, and reinforcement learning can be applied to various waste management processes, enhancing energy extraction and clean fuel production for commercial sectors like the steel industry. Machine learning can effectively predict waste generation, design collection routes, classify waste materials, forecast real-time landfill filling rates, detect operational issues, prevent illicit dumping, and establish predictive maintenance systems. However, it must be integrated with other strategies, policies, and regulations for a sustainable waste management system. Additionally, cost-benefit analyses, scalability, and implementation feasibility should be considered before investing. In conclusion, machine learning can improve municipal solid waste management efficiency and effectiveness, but further research is needed. The present study offers vital knowledge for key stakeholders, including successful case studies and evaluations of societal technology and customer readiness levels.",
        "DOI": "10.1016/j.fuel.2023.128548",
        "affiliation_name": "American University of the Middle East",
        "affiliation_city": "Al Ahmadi",
        "affiliation_country": "Kuwait"
    },
    {
        "paper_title": "Toward Carbon-Neutral Edge Computing: Greening Edge AI by Harnessing Spot and Future Carbon Markets",
        "paper_author": "Ma H.",
        "publication": "IEEE Internet of Things Journal",
        "citied_by": "10",
        "cover_date": "2023-09-15",
        "Abstract": "Provisioning dynamic machine learning (ML) inference as a service for artificial intelligence (AI) applications of edge devices faces many challenges, including the tradeoff among accuracy loss, carbon emission, and unknown future costs. Besides, many governments are launching carbon emission rights (CERs) for operators to reduce carbon emissions further to reverse climate change. Facing these challenges, to achieve carbon-aware ML task offloading under limited CERs thus to achieve green edge AI, we establish a joint ML task offloading and CER purchasing problem, intending to minimize the accuracy loss under the long-term time-averaged cost budget of purchasing the required CER. However, considering the uncertainty of the resource prices, the CER purchasing prices, the carbon intensity of sites, and ML tasks' arrivals, it is hard to decide the optimal policy online over a long-running period time. To overcome this difficulty, we leverage the two-timescale Lyapunov optimization technique, of which the T -slot drift-plus-penalty methodology inspires us to propose an online algorithm that purchases CER in multiple timescales (on-preserved in carbon future market and on-demanded in the carbon spot market) and makes decisions about where to offload ML tasks. Considering the NP-hardness of the T -slot problems, we further propose the resource-restricted randomized dependent rounding algorithm to help to gain the near-optimal solution with no help of any future information. Our theoretical analysis and extensive simulation results driven by the real carbon intensity trace show the superior performance of the proposed algorithms.",
        "DOI": "10.1109/JIOT.2023.3268339",
        "affiliation_name": "Sun Yat-Sen University",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Interpretable machine learning learns complex interactions of urban features to understand socio-economic inequality",
        "paper_author": "Fan C.",
        "publication": "Computer-Aided Civil and Infrastructure Engineering",
        "citied_by": "15",
        "cover_date": "2023-09-15",
        "Abstract": "Inequality in cities is a phenomenon arising from the complex interactions among urban systems and population activities. Conventional statistics and mathematical models like multiple regression models require assumptions of feature interactions with specified mathematical forms that may fail to fully capture complex interactions of heterogeneous urban components, creating challenges in systematically assessing socio-economic inequality in cities. To overcome the limitations of these conventional mathematical models, in this work, we propose an interpretable machine learning model to capture the complex interactions of urban variables and the main interaction effects on socio-economic statuses. We extract urban features from high-resolution anonymized mobile phone data with billions of activity records related to people and facilities in 47 US metropolitan areas and predict the attributes of urban areas from six income and race groups. We show that socio-economic inequality in cities can be effectively measured by the predictability of trained machine learning models in controlled experiments. We also examine the tradeoff between spatial resolution, sample size, and model accuracy; test the presence of influential features; and measure the transferability of the trained models to identify the optimal values for controlled factors. The results show that metropolitan areas share similar patterns of inequality, which could be moderated by improved polycentric facility distribution and road density. The generality of associated factors and transferability of machine learning models can help bridge data gaps between cities and inform about inequality alleviation strategies. Despite similarities, 50% to 90% of variations among cities are still present, which shows the need for localized policies for inequality alleviation and mitigation. Our study shows that machine learning models could be an effective approach to examine inequality, which opens avenues for more data-centric and complexity-informed planning, design, policymaking, and engineering toward equitable cities.",
        "DOI": "10.1111/mice.12972",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "College Station",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Spatiotemporal modeling of air pollutant concentrations in Germany using machine learning",
        "paper_author": "Balamurugan V.",
        "publication": "Atmospheric Chemistry and Physics",
        "citied_by": "3",
        "cover_date": "2023-09-14",
        "Abstract": "Machine learning (ML) models are becoming a meaningful tool for modeling air pollutant concentrations. ML models are capable of learning and modeling complex nonlinear interactions between variables, and they require less computational effort than chemical transport models (CTMs). In this study, we used gradientboosted tree (GBT) and multi-layer perceptron (MLP; neural network) algorithms to model near-surface nitrogen dioxide (NO2) and ozone (O3) concentrations over Germany at 0.1° spatial resolution and daily intervals. We trained the ML models using TROPOspheric Monitoring Instrument (TROPOMI) satellite column measurements combined with information on emission sources, air pollutant precursors, and meteorology as feature variables. We found that the trained GBT model for NO2 and O3 explained a major portion of the observed concentrations (R2 D 0:68-0.88 and RMSE D 4:77-8.67 μgm-3; R2 D 0:74-0.92 and RMSE D 8:53-13.2 μgm-3, respectively). The trained MLP model performed worse than the trained GBT model for both NO2 and O3 (R2 D 0:46-0.82 and R2 D 0:42-0.9, respectively). Our NO2 GBT model outperforms the CAMS model, a data-assimilated CTM but slightly underperforms for O3. However, our NO2 and O3 ML models require less computational effort than CTM. Therefore, we can analyze people's exposure to near-surface NO2 and O3 with significantly less effort. During the study period (30 April 2018 and 1 July 2021), it was found that around 36% of people lived in locations where the World Health Organization (WHO) NO2 limit was exceeded for more than 25% of the days during the study period, while 90% of the population resided in areas where the WHO O3 limit was surpassed for over 25% of the study days. Although metropolitan areas had high NO2 concentrations, rural areas, particularly in southern Germany, had high O3 concentrations. Furthermore, our ML models can be used to evaluate the effectiveness of mitigation policies. Near-surface NO2 and O3 concentration changes during the 2020 COVID-19 lockdown period over Germany were indeed reproduced by the GBT model, with meteorology-normalized near-surface NO2 having significantly decreased (by 23±5:3 %) and meteorology-normalized near-surface O3 having slightly increased (by 1±4:6 %) over 10 major German metropolitan areas when compared to 2019. Finally, our O3 GBT model is highly transferable to neighboring countries and locations where no measurements are available (R2 D 0:87-0.94), whereas our NO2 GBT model is moderately transferable (R2 D 0:32-0.64).",
        "DOI": "10.5194/acp-23-10267-2023",
        "affiliation_name": "Harvard John A. Paulson School of Engineering and Applied Sciences",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Exploring COVID-19 vaccine hesitancy and uptake in Nairobi's urban informal settlements: An unsupervised machine learning analysis of a longitudinal prospective cohort study from 2021 to 2022",
        "paper_author": "Rajshekhar N.",
        "publication": "BMJ Open",
        "citied_by": "7",
        "cover_date": "2023-09-12",
        "Abstract": "Objectives To illustrate the utility of unsupervised machine learning compared with traditional methods of analysis by identifying archetypes within the population that may be more or less likely to get the COVID-19 vaccine. Design A longitudinal prospective cohort study (n=2009 households) with recurring phone surveys from 2020 to 2022 to assess COVID-19 knowledge, attitudes and practices. Vaccine questions were added in 2021 (n=1117) and 2022 (n=1121) rounds. Setting Five informal settlements in Nairobi, Kenya. Participants Individuals from 2009 households included. Outcome measures and analysis Respondents were asked about COVID-19 vaccine acceptance (February 2021) and vaccine uptake (March 2022). Three distinct clusters were estimated using K-Means clustering and analysed against vaccine acceptance and vaccine uptake outcomes using regression forest analysis. Results Despite higher educational attainment and fewer concerns regarding the pandemic, young adults (cluster 3) were less likely to intend to get the vaccine compared with cluster 1 (41.5% vs 55.3%, respectively; p<0.01). Despite believing certain COVID-19 myths, older adults with larger households and more fears regarding economic impacts of the pandemic (cluster 1) were more likely to ultimately to get vaccinated than cluster 3 (78% vs 66.4%; p<0.01), potentially due to employment requirements. Middle-aged women who are married or divorced and reported higher risk of gender-based violence in the home (cluster 2) were more likely than young adults (cluster 3) to report wanting to get the vaccine (50.5% vs 41.5%; p=0.014) but not more likely to have gotten it (69.3% vs 66.4%; p=0.41), indicating potential gaps in access and broader need for social support for this group. Conclusions Findings suggest this methodology can be a useful tool to characterise populations, with utility for improving targeted policy, programmes and behavioural messaging to promote uptake of healthy behaviours and ensure equitable distribution of prevention measures.",
        "DOI": "10.1136/bmjopen-2022-071032",
        "affiliation_name": "Centre for Geographic Medicine Research",
        "affiliation_city": "Kilifi",
        "affiliation_country": "Kenya"
    },
    {
        "paper_title": "Shared Decision-Making and Cardiovascular Health: A Scientific Statement from the American Heart Association",
        "paper_author": "Dennison Himmelfarb C.R.",
        "publication": "Circulation",
        "citied_by": "40",
        "cover_date": "2023-09-12",
        "Abstract": "Shared decision-making is increasingly embraced in health care and recommended in cardiovascular guidelines. Patient involvement in health care decisions, patient-clinician communication, and models of patient-centered care are critical to improve health outcomes and to promote equity, but formal models and evaluation in cardiovascular care are nascent. Shared decision-making promotes equity by involving clinicians and patients, sharing the best available evidence, and recognizing the needs, values, and experiences of individuals and their families when faced with the task of making decisions. Broad endorsement of shared decision-making as a critical component of high-quality, value-based care has raised our awareness, although uptake in clinical practice remains suboptimal for a range of patient, clinician, and system issues. Strategies effective in promoting shared decision-making include educating clinicians on communication techniques, engaging multidisciplinary medical teams, incorporating trained decision coaches, and using tools (ie, patient decision aids) at appropriate literacy and numeracy levels to support patients in their cardiovascular decisions. This scientific statement shines a light on the limited but growing body of evidence of the impact of shared decision-making on cardiovascular outcomes and the potential of shared decision-making as a driver of health equity so that everyone has just opportunities. Multilevel solutions must align to address challenges in policies and reimbursement, system-level leadership and infrastructure, clinician training, access to decision aids, and patient engagement to fully support patients and clinicians to engage in the shared decision-making process and to drive equity and improvement in cardiovascular outcomes.",
        "DOI": "10.1161/CIR.0000000000001162",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Darwin: Flexible Learning-based CDN Caching",
        "paper_author": "Chen J.",
        "publication": "SIGCOMM 2023 - Proceedings of the ACM SIGCOMM 2023 Conference",
        "citied_by": "3",
        "cover_date": "2023-09-10",
        "Abstract": "Cache management is critical for Content Delivery Networks (CDNs), impacting their performance and operational costs. Most production CDNs apply static, hand-tuned caching policy parameters at cache servers, such as admission frequency or size thresholds for the Hot Object Caches (HOC) of their system. However, these static policies fall short when a server is faced with unpredictable traffic pattern changes, even when policies employ multiple control parameters/knobs. Recent approaches have proposed learning-based solutions to dynamically adjust policy parameters, but they are limited in action space, caching objectives, or impose high overhead. We propose Darwin, a CDN cache management system that is robust to traffic pattern changes and can flexibly optimize different caching objectives with unrestricted action spaces. Darwin employs a three-stage pipeline involving traffic pattern feature collection, unsupervised clustering for classification, and neural bandit expert selection to choose the optimal caching policy. Through extensive simulations, experiments using an Apache Traffic Server (ATS)-based prototype, and theoretical analysis, we show that Darwin achieves significant performance gain w.r.t. different objectives such as maximizing object hit rates and minimizing disk writes, while simultaneously adapting to traffic pattern shifts. Darwin imposes negligible overhead and achieves high throughput compared to the state-of-the-art.",
        "DOI": "10.1145/3603269.3604863",
        "affiliation_name": "University of California, Berkeley",
        "affiliation_city": "Berkeley",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Uncertainty-aware Energy Harvest Prediction and Management for IoT Devices",
        "paper_author": "Yamin N.",
        "publication": "ACM Transactions on Design Automation of Electronic Systems",
        "citied_by": "4",
        "cover_date": "2023-09-09",
        "Abstract": "Internet of things (IoT) devices are popular in several high-impact applications such as mobile healthcare and digital agriculture. However, IoT devices have limited operating lifetime due to their small form factor. Harvesting energy from ambient sources is an effective method to supplement the battery. Energy harvesting necessitates development of energy management policies to manage the harvested energy. Designing optimal policies for energy management is challenging for two key reasons: (1) ambient energy sources are highly stochastic; therefore, energy management policies must consider the associated uncertainty; (2) energy management policies must consider future energy availability while making decisions to ensure that sufficient energy is available when there is no ambient energy. Prior approaches typically consider energy in the immediate future (e.g., 1 hour) and do not account for the uncertainty in future energy harvest. This article proposes novel machine learning and dynamic optimization-based approaches to handle the two challenges. Specifically, we first develop a novel set of features and use it in a low-power neural network architecture to predict future energy availability and uncertainty. The energy predictions and uncertainty are used in a dynamic optimization algorithm to optimally allocate the harvested energy. Experiments on solar energy data over 5 years from Golden, Colorado, show that the proposed energy prediction model achieves 3.4 J mean absolute error while having a coverage of 80%. Moreover, our energy management algorithm provides energy allocations that are within 2.5 J of an optimal Oracle with 2.65 mJ to 36.54 mJ of energy overhead.",
        "DOI": "10.1145/3606372",
        "affiliation_name": "Washington State University Pullman",
        "affiliation_city": "Pullman",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "DTRL: Decision Tree-based Multi-Objective Reinforcement Learning for Runtime Task Scheduling in Domain-Specific System-on-Chips",
        "paper_author": "Basaklar T.",
        "publication": "ACM Transactions on Embedded Computing Systems",
        "citied_by": "3",
        "cover_date": "2023-09-09",
        "Abstract": "Domain-specific systems-on-chip (DSSoCs) combine general-purpose processors and specialized hardware accelerators to improve performance and energy efficiency for a specific domain. The optimal allocation of tasks to processing elements (PEs) with minimal runtime overheads is crucial to achieving this potential. However, this problem remains challenging as prior approaches suffer from non-optimal scheduling decisions or significant runtime overheads. Moreover, existing techniques focus on a single optimization objective, such as maximizing performance. This work proposes DTRL, a decision-tree-based multi-objective reinforcement learning technique for runtime task scheduling in DSSoCs. DTRL trains a single global differentiable decision tree (DDT) policy that covers the entire objective space quantified by a preference vector. Our extensive experimental evaluations using our novel reinforcement learning environment demonstrate that DTRL captures the trade-off between execution time and power consumption, thereby generating a Pareto set of solutions using a single policy. Furthermore, comparison with state-of-the-art heuristic-, optimization-, and machine learning-based schedulers shows that DTRL achieves up to 9× higher performance and up to 3.08× reduction in energy consumption. The trained DDT policy achieves 120 ns inference latency on Xilinx Zynq ZCU102 FPGA at 1.2 GHz, resulting in negligible runtime overheads. Evaluation on the same hardware shows that DTRL achieves up to 16% higher performance than a state-of-the-art heuristic scheduler.",
        "DOI": "10.1145/3609108",
        "affiliation_name": "University of Wisconsin-Madison",
        "affiliation_city": "Madison",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Integrated Approach for Maintenance Planning and Scheduling in a Flow Shop Using Deep Reinforcement Learning",
        "paper_author": "Marchesano M.G.",
        "publication": "Frontiers in Artificial Intelligence and Applications",
        "citied_by": "1",
        "cover_date": "2023-09-08",
        "Abstract": "Maintenance scheduling is critical for many industries, and Deep Reinforcement Learning (DRL) has shown great potential in optimizing scheduling decisions in complex and dynamic environments. This proposal introduces an integrated simulation tool and DRL algorithm for effective maintenance event scheduling and planning in a Flow Shop production line. This comprehensive solution aims to optimize maintenance plans and maximize productivity by combining simulation capabilities with intelligent decision-making via DRL. The integrated simulation tool replicates the production line Flow Shop in a virtual environment, allowing for precise modeling and simulation of machine operations, job flows, and maintenance events. The tool evaluates different maintenance procedures and their impact on overall performance by capturing the system's dynamics and complexities. The novelty of the approach lies in the fact that the training phase is performed on a single machine, and the policy developed is tested on a Flow Shop line with machines with the same Weibull parameters (α and β) and with machines with different Weibull parameters. The proposed integrated simulation tool and DRL algorithm provide a powerful solution for the scheduling and planning of maintenance events in a production line Flow Shop. By combining simulation capabilities with intelligent decision-making through DRL, this approach offers a comprehensive solution to optimize maintenance strategies and enhance overall production performance in all experimental settings tested.",
        "DOI": "10.3233/FAIA230241",
        "affiliation_name": "Università degli Studi di Napoli Federico II",
        "affiliation_city": "Naples",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Dynamic Power Management in Large Manycore Systems: A Learning-to-Search Framework",
        "paper_author": "Narang G.",
        "publication": "ACM Transactions on Design Automation of Electronic Systems",
        "citied_by": "2",
        "cover_date": "2023-09-08",
        "Abstract": "The complexity of manycore System-on-chips (SoCs) is growing faster than our ability to manage them to reduce the overall energy consumption. Further, as SoC design moves toward three-dimensional (3D) architectures, the core's power density increases leading to unacceptable high peak chip temperatures. In this article, we consider the optimization problem of dynamic power management (DPM) in manycore SoCs for an allowable performance penalty (say, 5%) and admissible peak chip temperature. We employ a machine learning- (ML) based DPM policy, which selects the voltage/frequency levels for different cluster of cores as a function of the application workload features such as core computation and inter-core traffic, and so on. We propose a novel learning-to-search (L2S) framework to automatically identify an optimized sequence of DPM decisions from a large combinatorial space for joint energy-thermal optimization for one or more given applications. The optimized DPM decisions are given to a supervised learning algorithm to train a DPM policy, which mimics the corresponding decision-making behavior. Our experiments on two different manycore architectures designed using wireless interconnect and monolithic 3D demonstrate that principles behind the L2S framework are applicable for more than one configuration. Moreover, L2S-based DPM policies achieve up to 30% energy-delay product savings and reduce the peak chip temperature by up to 17 °C compared to the state-of-the-art ML methods for an allowable performance overhead of only 5%.",
        "DOI": "10.1145/3603501",
        "affiliation_name": "Intel Research Laboratories",
        "affiliation_city": "Santa Clara",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Embed equity throughout innovation",
        "paper_author": "Wailoo K.A.",
        "publication": "Science",
        "citied_by": "5",
        "cover_date": "2023-09-08",
        "Abstract": "NA",
        "DOI": "10.1126/science.adk6365",
        "affiliation_name": "University of California, San Francisco",
        "affiliation_city": "San Francisco",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Opportunities and challenges of AI/ML in finance",
        "paper_author": "Komal ",
        "publication": "The Impact of AI Innovation on Financial Sectors in the Era of Industry 5.0",
        "citied_by": "4",
        "cover_date": "2023-09-05",
        "Abstract": "The financial sector's rapid adoption of artificial intelligence (AI) and machine learning (ML) is examined in this chapter. It highlights the benefits of these technologies in terms of financial depth and efficiency, while also raising concerns about their potential to widen the digital divide between rich and developing nations. The chapter contributes to the topic of the impact of this technology by distilling and categorizing the distinct threats it may offer to the integrity and stability of the financial system, as well as policy issues & viable regulatory approaches. Because of the dynamic nature of this technology and its application in finance, its strengths and drawbacks remain unknown. In light of the possibility of unforeseen hazards, nations will need to strengthen prudential oversight.",
        "DOI": "10.4018/979-8-3693-0082-4.ch014",
        "affiliation_name": "SRM University Delhi-NCR, Sonepat",
        "affiliation_city": "Sonipat",
        "affiliation_country": "India"
    },
    {
        "paper_title": "NON HIERARCHICAL K-MEANS ANALYSIS TO CLUSTERING PRIORITY DISTRIBUTION OF FUEL SUBSIDIES IN INDONESIA",
        "paper_author": "Astuti A.B.",
        "publication": "Barekeng",
        "citied_by": "0",
        "cover_date": "2023-09-01",
        "Abstract": "The growth rate of inflation in Indonesia continues to increase from day to day. The inflation rate in Indonesia reached 1.17% in September 2022 which is the highest inflation rate in the last seven years. One of the causes of high inflation is caused by the increasing demand for motor vehicle fuel. Therefore, there is a need for appropriate action from the government in determining related policies. K-Means multivariate cluster analysis is a non-hierarchical cluster method that is popularly used, one of which is used in Machine Learning algorithms, especially Unsupervised Learning. The purpose of this research is to clustering that are priority distribution of subsidies in Indonesia based on the characteristics formed. The data in this study consist of the percentage of poverty, the percentage of total transportation, the percentage of transportation use, and the percentage of area. Data were analyzed using multivariate cluster analysis with the K-Means method. Based on the research results, information was obtained that the data fulfilled a representative sample with value of KMO >50%. In addition, there are 4 optimal clusters which are the results of the calculation of the Elbow and Silhoutte methods, so 4 provincial clusters are formed with their respective characteristics. Cluster 1 is a province that is highly prioritized to receive fuel subsidies, Cluster 2 is a province that is not highly prioritized for fuel subsidies, Cluster 3 is a province that is prioritized to receive fuel subsidies, and Cluster 4 is a province that is not prioritized to receive fuel subsidies.",
        "DOI": "10.30598/barekengvol17iss3pp1663-1672",
        "affiliation_name": "Brawijaya University",
        "affiliation_city": "Malang",
        "affiliation_country": "Indonesia"
    },
    {
        "paper_title": "A Retrospective Study of International Studies in Library and Information Science Using Automated Classification Technique (1945-2019)",
        "paper_author": "Nezhad F.G.",
        "publication": "Scientometrics Research Journal",
        "citied_by": "0",
        "cover_date": "2023-09-01",
        "Abstract": "Purpose: Analyzing the subject structure of research in this field can provide valuable information about the current state of the field, including its issues and needs, strengths, gaps, and shortcomings. The present study aims to analyze the global research topics in information science and library science, while also reviewing research topics from previous years and identifying thematic gaps. This analysis will provide a foundation for planning and policy research in this field. Methodology: The present study utilized an automated classification technique. Data analysis was performed on 85,403 articles indexed on the Web of Science website between 1945 and 2019 in the field of library and information science. The source and tool used for extracting research data was Web of Science. After preparing and pre-processing the data and formulating the classification plan for articles across 31 thematic categories, the logistic regression algorithm has been employed for the automatic classification of articles. This algorithm assigns articles to their respective thematic categories using the Cosine similarity criterion. The various stages of the research were performed using the Python 3.7 programming language and the Pandas, Numpy, NLTK, and Plotly software packages. Findings: Most of the global research in this field is dedicated to information technology, while the field of organizational management has the least amount of research. The growth trend of research in many thematic areas is increasing. However, there is further growth in areas such as web studies, information technology, information behavior, user studies, in formation retrieval, and search engines. After the 1990s, popular terms such as social media, big data, virtual library, IoT (Internet of Things), machine learning, deep learning, data mining, open data, artificial intelligence, natural language processing, cloud computing, virtual reality, augmented reality, and machine translation have emerged in research in this field. It appears that the utilization of keywords pertaining to traditional subjects in the field has declined, while the usage of concepts encompassing new topics related to technology and the web has increased. The rapid growth of fields such as web development, technology, artificial intelligence, and data mining has led to the evolution of research topics and increased connections with computer science and information technology, particularly in recent years. The changes in the subjects of this field can be attributed to the development of the web and advancements in technology. Additionally, the concepts and research topics in this field have had an impact on globalization, communication progress, and overall development in various economic, social, and cultural dimensions. Conclusion: The conclusion of the study highlights the thematic trends in library and information science research over different time periods. It also identifies the weaknesses in the scientific output of this field, while outlining the evolutionary process and the path of progress and development in the field of science. Due to the fact that scientific outputs are derived from the issues, needs, and priorities of a society at any given time, studying them can help clarify the most important issues and research needs in the field of library and information science. This, in turn, can provide a foundation for identifying future research needs and subjects in the field. In other words, having a clear understanding of the subject areas of library and information science will help researchers in this field identify the most important issues and needs. This, in turn, will enable them to take appropriate actions when selecting topics for future research. By conducting needs-based research, we can lay the groundwork for policy development and research planning in this field. This approach will also help address potential challenges and problems that may arise. This is despite the fact that if the needs and issues are ignored, research resources and credits will be spent on topics that do not have the necessary priority and importance. It cannot be expected that the findings from these topics will solve the problems and pave the way for the advancement of this field.",
        "DOI": "10.22070/rsci.2022.15372.1538",
        "affiliation_name": "Shahid Chamran University of Ahvaz",
        "affiliation_city": "Ahvaz",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "A novel financial trading system based on reinforcement learning and technical analysis applied on the Tehran securities exchange market",
        "paper_author": "Pourahmadi Z.",
        "publication": "Journal of Mathematics and Modeling in Finance",
        "citied_by": "1",
        "cover_date": "2023-09-01",
        "Abstract": "Stock trading is a significant decision-making problem in asset management.This study introduces a financial trading system (FTS) that leverages artificial intelligence (AI) techniques to automate buy and sell orders specifically in Iran’s stock market. Due to limited availability of labelled data in financial markets, the FTS utilizes reinforcement learning (RL), a subset of AI, for training. The model incorporates technical analysis and a constrained policy to enhance decisionmaking capabilities. The proposed algorithm is applied to the Tehran Securities Exchange, evaluating its efficiency across 45 periods using three different stock market indices. Performance comparisons are made against common strategies such as buy and hold, randomly selected actions, and maintaining the initial stock portfolio, with and without transaction costs. The results indicate that the FTS outperforms these methods, exhibiting excellent performance metrics including Sharp ratio, PP, PF, and MDD. Consequently, the findings suggest that the FTS serves as a valuable asset management tool in the Iranian financial market.",
        "DOI": "10.22054/jmmf.2023.74166.1088",
        "affiliation_name": "Yazd University",
        "affiliation_city": "Yazd",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "Applications and effectiveness of artificial intelligence and machine learning techniques in physiotherapy",
        "paper_author": "García M.B.P.",
        "publication": "Revista Habanera de Ciencias Medicas",
        "citied_by": "0",
        "cover_date": "2023-09-01",
        "Abstract": "Introduction: The convergence of artificial intelligence (AI), machine learning (ML), and physiotherapy are constantly evolving fields that have led to significant advancements in the diagnosis, treatment, and monitoring of patients. Objective: The objective of this systematic literature review (SLR) is to comprehensively analyze the scientific literature from the last 5 years to identify technological advances and approaches with trends towards the fields of both AI and physiotherapy, gathering valuable information for specialists. Material and Methods: The PRISMA methodology was employed to conduct a systematic analysis of 94 articles that met the inclusion and exclusion criteria defined by the authors, ensuring quality assessment based on predetermined criteria. Results: Developed countries lead research in the field, with India emerging as a prominent actor. Various techniques were identified, ranging from basic algorithms to deep learning, emphasizing continuous progress. The influence of AI and ML extends from radiological diagnosis to the simulation of clinical assessments, providing benefits in both clinical effectiveness and socio-economic aspects. The technology drives personalized therapies and remote monitoring, transforming physiotherapeutic practices. Conclusions: The findings of this review have significant implications for physiotherapy practices and policies, emphasizing the need for increased research in developing countries and the implementation of advanced technological approaches.",
        "DOI": "NA",
        "affiliation_name": "Universidad Nacional de Chimborazo",
        "affiliation_city": "Riobamba",
        "affiliation_country": "Ecuador"
    },
    {
        "paper_title": "Examining the Plausible Applications of Artificial Intelligence &amp; Machine Learning in Accounts Payable Improvement",
        "paper_author": "Kanaparthi V.K.",
        "publication": "FinTech",
        "citied_by": "11",
        "cover_date": "2023-09-01",
        "Abstract": "Accounts Payable (AP) is a time-consuming and labor-intensive process used by large corporations to compensate vendors on time for goods and services received. A comprehensive verification procedure is executed before disbursing funds to a supplier or vendor. After the successful conclusion of these validations, the invoice undergoes further processing by traversing multiple stages, including vendor identification; line-item matching; accounting code identification; tax code identification, ensuring proper calculation and remittance of taxes, verifying payment terms, approval routing, and compliance with internal control policies and procedures, for a comprehensive approach to invoice processing. At the moment, each of these processes is almost entirely manual and laborious, which makes the process time-consuming and prone to mistakes in the ongoing education of agents. It is difficult to accomplish the task of automatically processing these invoices for payment without any human involvement. To provide a solution, we implemented an automated invoicing system with modules based on artificial intelligence. This system processes invoices from beginning to finish. It takes very little work to configure it to meet the specific needs of each unique customer. Currently, the system has been put into production use for two customers. It has handled roughly 80 thousand invoices, of which 76 percent were automatically processed with little or no human interaction.",
        "DOI": "10.3390/fintech2030026",
        "affiliation_name": "Cloud Analytics AI",
        "affiliation_city": "McLean",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Modern Internet Search Analytics and Osseointegration: What are Patients Asking and Reading Online?",
        "paper_author": "Murphy E.P.",
        "publication": "Strategies in Trauma and Limb Reconstruction",
        "citied_by": "1",
        "cover_date": "2023-09-01",
        "Abstract": "Background: Osseointegration (OI) limb has been performed for over 30 years and is an example of an advance in technology and surgical technique which has led to improvements in patient mobility and quality of life. An increasing number of patients seek information about osseointegration. The aim of this study was to categorise the most frequently asked questions by patients using the Google search engine and the most frequently accessed websites with the highest return on answers. The secondary aims of this study were to assess the quality of the information provided on those websites and to stratify, by category, which websites provide the best quality information. Materials and methods: Ten permutations and conjugations of the word ‘osseointegration’ were entered into Google. The first fifty ‘People also ask’ and associated websites by Google’s machine learning and natural language processing engine were collected for each search term. The Rothwell classification system of questions by topic (Fact, Value, Policy) and websites by category was used (Commercial, Academic, Medical Practice, Single Surgeon Personal, Government, Social Media). Website quality was assessed using the Journal of the American Medical Association (JAMA) benchmark criteria (Likert-style rating 0-4). Pearson’s Chi-squared and Student’s t-tests were performed for statistical analysis as appropriate (significance, p < 0.05). Results: The 10 search terms generated 454 questions and referenced 408 websites. Of the 454 questions generated, the most common question categories were fact (70.8%), value (19.2%), and policy (10%). The most common website type was social media (37.4%). The most common question types were technical details (30.4%), specific activity (20.6%), and cost (14.1%). Only 1.6% of questions related to risks and complications. Generally, website quality was poor with 64.1% having a JAMA score of 0 or 1. Websites that were categorised as ‘Government’ had the highest overall JAMA scores: 71.4% had a score of 4. Conclusion: Based on Google search engine’s results, the most commonly asked questions about osteointegration related to technical details, specific activities and cost; only 1.6% related to risks and complications. Interestingly, social media websites represented the highest volume of search result referrals. Overall, the quality of websites was poor with the most factual information coming from governmental websites.",
        "DOI": "10.5005/jp-journals-10080-1603",
        "affiliation_name": "University Hospital Galway",
        "affiliation_city": "Galway",
        "affiliation_country": "Ireland"
    },
    {
        "paper_title": "Editorial",
        "paper_author": "NA",
        "publication": "Journal of Mental Health Policy and Economics",
        "citied_by": "0",
        "cover_date": "2023-09-01",
        "Abstract": "NA",
        "DOI": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Towards machines that understand people",
        "paper_author": "Howes A.",
        "publication": "AI Magazine",
        "citied_by": "10",
        "cover_date": "2023-09-01",
        "Abstract": "The ability to estimate the state of a human partner is an insufficient basis on which to build cooperative agents. Also needed is an ability to predict how people adapt their behavior in response to an agent's actions. We propose a new approach based on computational rationality, which models humans based on the idea that predictions can be derived by calculating policies that are approximately optimal given human-like bounds. Computational rationality brings together reinforcement learning and cognitive modeling in pursuit of this goal, facilitating machine understanding of humans.",
        "DOI": "10.1002/aaai.12116",
        "affiliation_name": "Aalto University",
        "affiliation_city": "Espoo",
        "affiliation_country": "Finland"
    },
    {
        "paper_title": "Development of a Forecasting Model of Teaching Effectiveness",
        "paper_author": "Borbon M.",
        "publication": "Journal of Institutional Research South East Asia",
        "citied_by": "0",
        "cover_date": "2023-09-01",
        "Abstract": "This research project aims to utilize Python programming and machine learning algorithms to design a predictive model for assessing faculty effectiveness. The model considers various factors such as teaching effectiveness, course management, course materials, class openness, and course management. By analyzing these factors and testing the various model's performance against standard metrics, the collected data is processed and analyzed using regression analysis and decision trees, enabling the development of a predictive model. This model may provide estimates of future performance, allowing for the identification of high-performing faculty members, areas for improvement, and optimal resource allocation. The study results demonstrate that Naive Bayes, Random Forest, and Decision Tree algorithms are particularly effective in predicting faculty performance based on the provided data. These findings promise to inform the development of strategies and policies that enhance faculty effectiveness and contribute to institutional excellence. By employing a data-driven approach, this study offers valuable insights into the utility of different machine learning algorithms and their predictive capabilities in assessing faculty performance within the context of higher education.",
        "DOI": "NA",
        "affiliation_name": "De La Salle-College Of Saint Benilde",
        "affiliation_city": "Manila",
        "affiliation_country": "Philippines"
    },
    {
        "paper_title": "Using Artificial Intelligence to Predict and Prevent Future Food Insecurity",
        "paper_author": "Villacis A.H.",
        "publication": "Georgetown Journal of International Affairs",
        "citied_by": "2",
        "cover_date": "2023-09-01",
        "Abstract": "The article explores the role and prospects of artificial intelligence (AI) in addressing global food insecurity. It provides an overview of machine learning (ML) techniques—the core learning component of AI—used to predict food security outcomes and discusses real-world examples as well as recent applications of ML. It further examines the challenges and limitations of ML, including concerns related to data quality and ethical con-siderations, followed by policy recommendations in crucial areas such as funding, cross-sector collabo-ration, education, and data standards. Finally, it underscores the importance of recognizing AI as a complementary tool, rather than a standalone solu-tion, in the pursuit of the ultimate goal of achieving a world without hunger.",
        "DOI": "10.1353/gia.2023.a913645",
        "affiliation_name": "W. P. Carey School of Business",
        "affiliation_city": "Tempe",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Protection and enhancement of the coastal area of the wilaya of El Tarf (Algeria): Automatic analysis using computer tools",
        "paper_author": "Belloulou B.",
        "publication": "Ekologia Bratislava",
        "citied_by": "0",
        "cover_date": "2023-09-01",
        "Abstract": "The coastline of El Tarf province, stretching for 90 km, presents a great variety of forms and complex ecosystems that need to be protected. The Coastal Law 02-02 was established to achieve this goal by using geographic information system (GIS) and remote sensing to assess the degree of implementation of this law on the ground. This evaluation highlighted a significant evolution of land use in the coastal zone of El Tarf province over a period of 32 years, from 1990 to 2022. In 1990, the area was predominantly forested (55.08%) with a proportion of agricultural land of 27.26% and a significant portion of wetland areas (17.26%). Over time, the forested area decreased to reach 48.58% in 2022, while agricultural land and urban areas increased. This evolution suggests increasing pressure on natural resources, with potential implications for the environment and biodiversity of the region. Despite the 2002 Coastal Law 02-02s which sets specific provisions for the protection and enhancement of the coastline, it is important to emphasize the importance of sustainable management of natural resources and land use in the region. It is essential to implement measures to protect the fragile ecosystems of the region and ensure the sustainability of natural resource use to preserve the environment and biodiversity of the region for future generations. This analysis could also eventually enable decision-makers to have supporting elements to evolve the law 02.02 with the aim of better preserving the coastal area.",
        "DOI": "10.2478/eko-2023-0031",
        "affiliation_name": "École Nationale Supérieure des Sciences de la Mer et de l'Aménagement du Littoral",
        "affiliation_city": "Algiers",
        "affiliation_country": "Algeria"
    },
    {
        "paper_title": "Digital twin in power system research and development: Principle, scope, and challenges",
        "paper_author": "Yassin M.A.M.",
        "publication": "Energy Reviews",
        "citied_by": "24",
        "cover_date": "2023-09-01",
        "Abstract": "In order to address the issues that arise in modern power systems, such as system dynamics, stability, control, efficiency, reliability, economy, planning and policy, and so on, efforts have been made to develop new tools and techniques, components, methodologies, and scientific innovations in a variety of fields. These efforts have been undertaken to address these issues. The term “digital twin” (DT) refers to one of the most reliable and rapidly developing technologies that have recently been incorporated into a variety of applications, platforms, and real-time projects. The authors of this study offered a scoping review of DT technologies with a primary emphasis on power systems. It has been established that the underlying notion behind this technology, as well as its operating principle, types, communication channels and protocols, and standards, have all been thoroughly examined. In addition, the possibility of integrating other technologies with DT has also been considered, along with the potential benefits of doing so and the potential difficulties that may arise. Based on the information gained from the current projects, the finished projects, the research publications, as well as the research and industry insights, a critical discussion has been made.",
        "DOI": "10.1016/j.enrev.2023.100039",
        "affiliation_name": "University of South-Eastern Norway",
        "affiliation_city": "Kongsberg",
        "affiliation_country": "Norway"
    },
    {
        "paper_title": "Recognizing chemical structures drawn by hand using deep learning algorithms and predict probable chemical structure",
        "paper_author": "Banik M.",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2023-09-01",
        "Abstract": "Chemists frequently use structure diagrams created by hand to express concepts regarding organic molecules. However, the simplicity of use, naturalness, and speed of drawing on paper is not present in the software used today to specify these structures to a computer because it relies on a conventional mouse and keyboard interface. As a result, we created a sketch-based system that can decipher manually drawn organic chemical diagrams. We are transforming a chemical molecule's graphical representation into its typical structural representation. The chemical structure recognition method should identify the graph's nodes and groups and the correct bond labels for each vertex. We offer an approach that builds on cutting-edge techniques to address the issue in the face of the additional challenges posed by hand-drawn molecules, allowing users to sketch out molecules on paper and upload a scanned picture of that drawing to recognize the chemicals. We apply fundamental corner detection techniques to determine the atoms and groups that make up the nodes of the chemical structure graph. We conclude that our corner detection method may be more effective than the commonly used line vectorization algorithms. The critical distinction in our policy is employing a neural network that operates supervised machine learning to categorize bonds according to numerous feature descriptors of bond cross-sections.",
        "DOI": "10.1063/5.0167028",
        "affiliation_name": "Guru Nanak Institute of Technology, Kolkata",
        "affiliation_city": "Kolkata",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Analysis of the Effects of Escrow Accounts in Russia Using Evidence-based Policy and Machine Learning Methods",
        "paper_author": "Komotskiy E.I.",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2023-09-01",
        "Abstract": "The article analyzes data on new construction projects throughout Russia for the period 2017-2019 in order to answer the question of what effect the introduction of escrow accounts had on the building industry in terms of changes in the construction time of comparable facilities. This change from an economic point of view is due to the fact that construction companies have to actively use credit money, and thus there is a motivation to complete the project as soon as possible in order to avoid additional costs for loans. To answer this question, methods of evidence-based policy are used, in particular, matching based on machine learning algorithms and methods of collecting and analyzing data from websites. The data set obtained in the course of work is unique in terms of analysis of the construction industry in Russia.",
        "DOI": "10.1063/5.0164546",
        "affiliation_name": "Ural Federal University",
        "affiliation_city": "Yekaterinburg",
        "affiliation_country": "Russian Federation"
    },
    {
        "paper_title": "Tariff Analysis in Automobile Insurance: Is It Time to Switch from Generalized Linear Models to Generalized Additive Models?",
        "paper_author": "Díaz Martínez Z.",
        "publication": "Mathematics",
        "citied_by": "2",
        "cover_date": "2023-09-01",
        "Abstract": "Generalized Linear Models (GLMs) are the standard tool used for pricing in the field of automobile insurance. Generalized Additive Models (GAMs) are more complex and computationally intensive but allow taking into account nonlinear effects without the need to discretize the explanatory variables. In addition, they fit perfectly into the mental framework shared by actuaries and are easier to use and interpret than machine learning models, such as trees or neural networks. This work compares both the GLM and GAM approaches, using a wide sample of policies to assess their differences in terms of quality of predictions, complexity of use, and time of execution. The results show that GAMs are a powerful alternative to GLMs, particularly when “big data” implementations of GAMs are used.",
        "DOI": "10.3390/math11183906",
        "affiliation_name": "Universidad Complutense de Madrid",
        "affiliation_city": "Madrid",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "Dynamic Regimes for Corporate Human Capital Development Used Reinforcement Learning Methods",
        "paper_author": "Orlova E.V.",
        "publication": "Mathematics",
        "citied_by": "3",
        "cover_date": "2023-09-01",
        "Abstract": "Corporate human capital is a critical driver of sustainable economic growth, which is becoming increasingly important in the changing nature of work. Due to the expansion of various areas of human activity, the employee’s profile becomes multifaceted. Therefore, the problem of human capital management based on the individual trajectories of professional development, aimed at increasing the labor efficiency and contributing to the growth of the corporate operational efficiency, is relevant, timely, socially, and economically significant. The paper proposes a methodology for the dynamic regimes for human capital development (DRHC) to design individual trajectories for the employee’s professional development, based on reinforcement learning methods. The DRHC develops an optimal management regime as a set of programs aimed at developing an employee in the professional field, taking into account their individual characteristics (health quality, major and interdisciplinary competencies, motivation, and social capital). The DRHC architecture consists of an environment—an employee model—as a Markov decision-making process and an agent—decision-making center of a company. The DRHC uses DDQN, SARSA, and PRO algorithms to maximize the agent’s utility function. The implementation of the proposed DRHC policy would improve the quality of corporate human capital, increase labor resource efficiency, and ensure the productivity growth of companies.",
        "DOI": "10.3390/math11183916",
        "affiliation_name": "Ufa University of Science and Technology",
        "affiliation_city": "Ufa",
        "affiliation_country": "Russian Federation"
    },
    {
        "paper_title": "Demonstrating Trustworthiness to Patients in Data-Driven Health Care",
        "paper_author": "Nong P.",
        "publication": "Hastings Center Report",
        "citied_by": "2",
        "cover_date": "2023-09-01",
        "Abstract": "Patient data is used to drive an ecosystem of advanced digital tools in health care, like predictive models or artificial intelligence-based decision support. Patients themselves, however, receive little information about these technologies or how they affect their care. This raises important questions about patient trust and continued engagement in a health care system that extracts their data but does not treat them as key stakeholders. This essay explores these tensions and provides steps forward for health systems as they design advanced health information-technology (IT) policies and practices. It centers patients, their concerns, and the ways they perceive trustworthiness to reframe advanced health IT in service of patient interests.",
        "DOI": "10.1002/hast.1526",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "A Study on the Effects of Digital Finance on Green Low-Carbon Circular Development Based on Machine Learning Models",
        "paper_author": "Zhang X.",
        "publication": "Mathematics",
        "citied_by": "5",
        "cover_date": "2023-09-01",
        "Abstract": "With technological transformations such as big data, blockchain, artificial intelligence, and cloud computing, digital techniques are infiltrating the field of finance. Digital finance (DF) is a resource-saving and environmentally friendly innovative financial service. It shows great green attributes and can drive the flow of financial resources towards environmentally-friendly enterprises, thereby promoting green low-carbon circular development (GLCD). However, few studies have explored the coupling mechanism between DF and GLCD. To fill this gap, this paper explores the effect of DF on GLCD, and established a mediating effect model to investigate the mechanism of DF in promoting GLCD. Additionally, this paper established a random forest model and a CatBoost model based on machine learning to examine the relative importance of DF and the factors affecting GLCD. The results show that DF has significant positive effects on GLCD, and technological innovation plays a key role in the effect of DF on GLCD; meanwhile, the effect of DF on GLCD shows nonlinear features with an increasing “marginal effect”; moreover, both DF and conventional factors have significant impacts on GLCD. Our study highlights the effect of DF on GLCD and underscores the importance of developing policies for DF and GLCD. This study provides an empirical basis and path reference for DF to achieve “carbon peak, carbon neutralization” in China.",
        "DOI": "10.3390/math11183903",
        "affiliation_name": "Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Performance evaluation of DHRR-RIS based HP design using machine learning algorithms",
        "paper_author": "Girish Kumar N.G.",
        "publication": "Intelligent and Converged Networks",
        "citied_by": "0",
        "cover_date": "2023-09-01",
        "Abstract": "Reconfigurable Intelligent Surfaces (RIS) have emerged as a promising technology for improving the reliability of massive MIMO communication networks. However, conventional RIS suffer from poor Spectral Efficiency (SE) and high energy consumption, leading to complex Hybrid Precoding (HP) designs. To address these issues, we propose a new low-complexity HP model, named Dynamic Hybrid Relay Reflecting RIS based Hybrid Precoding (DHRR-RIS-HP). Our approach combines active and passive elements to cancel out the downsides of both conventional designs. We first design a DHRR-RIS and optimize the pilot and Channel State Information (CSI) estimation using an adaptive threshold method and Adaptive Back Propagation Neural Network (ABPNN) algorithm, respectively, to reduce the Bit Error Rate (BER) and energy consumption. To optimize the data stream, we cluster them into private and public streams using Enhanced Fuzzy C-Means (EFCM) algorithm, and schedule them based on priority and emergency level. To maximize the sum rate and SE, we perform digital precoder optimization at the Base Station (BS) side using Deep Deterministic Policy Gradient (DDPG) algorithm and analog precoder optimization at the DHRR-RIS using Fire Hawk Optimization (FHO) algorithm. We implement our proposed work using MATLAB R2020a and compare it with existing works using several validation metrics. Our results show that our proposed work outperforms existing works in terms of SE, Weighted Sum Rate (WSR), and BER.",
        "DOI": "10.23919/ICN.2023.0019",
        "affiliation_name": "Bangalore Institute of Technology",
        "affiliation_city": "Bengaluru",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Artificial intelligence in medical writing and scientific papers authorship",
        "paper_author": "Gutierrez R.F.S.",
        "publication": "Angiologia",
        "citied_by": "3",
        "cover_date": "2023-09-01",
        "Abstract": "NA",
        "DOI": "10.20960/angiologia.00512",
        "affiliation_name": "Complejo Asistencial Universitario de León",
        "affiliation_city": "Leon",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "Forecasting Carbon Dioxide Emission in Thailand Using Machine Learning Techniques",
        "paper_author": "Chimphlee S.",
        "publication": "Indonesian Journal of Electrical Engineering and Informatics",
        "citied_by": "1",
        "cover_date": "2023-09-01",
        "Abstract": "Machine Learning (ML) models and the massive quantity of data accessible provide useful tools for analyzing the advancement of climate change trends and identifying major contributors. Random Forest (RF), Gradient Boosting Regression (GBR), XGBoost (XGB), Support Vector Machines (SVC), Decision Trees (DT), K-Nearest Neighbors (KNN), Principal Component Analysis (PCA), ensemble methods, and Genetic Algorithms (GA) are used in this study to predict CO2 emissions in Thailand. A variety of evaluation criteria are used to determine how well these models work, including R-squared (R2), mean absolute error (MAE), root mean squared error (RMSE), mean absolute percentage error (MAPE), and correctness. The results show that the RF and XGB algorithms function exceptionally well, with high R-squared values and low error rates. KNN, PCA, ensemble methods, and GA, on the other hand, outperform the top-performing models. Their lower R-squared values and higher error scores indicate that they are unable to accurately anticipate CO2 emissions. This paper contributes to the field of environmental modeling by comparing the effectiveness of various machine learning approaches in forecasting CO2 emissions. The findings can assist Thailand in promoting sustainable development and developing policies that are consistent with worldwide efforts to combat climate change.",
        "DOI": "10.52549/ijeei.v11i3.4892",
        "affiliation_name": "Suan Dusit University",
        "affiliation_city": "Bangkok",
        "affiliation_country": "Thailand"
    },
    {
        "paper_title": "Survey on Visual-Guided Adversarial Imitation Learning",
        "paper_author": "Cui M.",
        "publication": "Journal of Frontiers of Computer Science and Technology",
        "citied_by": "0",
        "cover_date": "2023-09-01",
        "Abstract": "The problem of optimal decision has a long history in the field of machine learning. Imitation learning, originating from reinforcement learning, is studied to reconstruct the expected policy from expert data and learn the optimal decision- making. In recent years, imitation learning has been successfully applied in both theoretical research and computer vision, as well as in various applications such as autonomous driving and robotics. The origin of imitation learning and the two traditional research methods, namely behavior cloning and inverse reinforcement learning, are introduced. With the development of adversarial training structures, generative adversarial imitation learning has become a key research direction, and its subsequent improvement work is collectively referred to as adversarial imitation learning. The research content of adversarial imitation learning combined with visual demonstrations is analyzed, along with summarizing common issues like suboptimal expert demonstrations, limited data, and low sample utilization efficiency, and the existing corresponding solutions. Then, the performance of different methods in addressing these problems is compared and analyzed based on experimental results. Finally, practical applications of adversarial visual imitation learning in scenarios such as autonomous driving and industrial robotics are discussed, and this paper is concluded by pointing out future research directions, as well as the potential prospects and challenges in applications.",
        "DOI": "10.3778/j.issn.1673-9418.2301067",
        "affiliation_name": "Suzhou University of Science and Technology",
        "affiliation_city": "Suzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A Trajectory Control for Bipedal Walking Robot Using Stochastic-Based Continuous Deep Reinforcement Learning",
        "paper_author": "Surriani A.",
        "publication": "Evergreen",
        "citied_by": "3",
        "cover_date": "2023-09-01",
        "Abstract": "The bipedal walking robot is an advanced anthropomorphic robot that can mimic the human ability to walk. Controlling the bipedal walking robot is difficult due to its nonlinearity and complexity. To solve this problem, recent studies have applied various machine learning algorithms based on reinforcement learning approaches, however most of them rely on deterministic-policy-based strategy. This research proposes Soft Actor Critic (SAC), which has stochastic policy strategy for controlling the bipedal walking robot. The option thought deterministic and stochastic policy affects the exploration of DRL algorithm. The SAC is a Deep Reinforcement Learning (DRL) based algorithm whose improvement obtained through the augmented entropy-based expected return allows the SAC algorithm to learn faster, gain exploration ability, and still ensure convergence. The SAC algorithm's performance is validated with a bipedal robot to walk towards the straight-line trajectory. The number of the reward and the cumulative reward during the training is used as the algorithm's performance evaluation. The SAC algorithm controls the bipedal walking robot well with a total reward of 384,752.8.",
        "DOI": "10.5109/7151701",
        "affiliation_name": "Universitas Gadjah Mada",
        "affiliation_city": "Yogyakarta",
        "affiliation_country": "Indonesia"
    },
    {
        "paper_title": "Robustness of Proactive Intensive Care Unit Transfer Policies",
        "paper_author": "Grand-Clément J.",
        "publication": "Operations Research",
        "citied_by": "5",
        "cover_date": "2023-09-01",
        "Abstract": "Patients whose transfer to the intensive care unit (ICU) is unplanned are prone to higher mortality rates and longer length of stay. Recent advances in machine learning to predict patient deterioration have introduced the possibility of proactive transfer from the ward to the ICU. In this work, we study the problem of finding robust patient transfer policies that account for the important problem of uncertainty in statistical estimates because of data limitations when optimizing to improve overall patient care. We propose a Markov decision process model to capture the evolution of patient health, where the states represent a measure of patient severity. Under fairly general assumptions, we show that an optimal transfer policy has a threshold structure (i.e., that it transfers all patients above a certain severity level to the ICU (subject to available capacity)). As model parameters are typically determined based on statistical estimations from real-world data, they are inherently subject to misspecification and estimation errors. This is an important issue, which can lead to choosing significantly suboptimal policies. We account for this parameter uncertainty by deriving a robust policy that optimizes the worst-case reward across all plausible values of the model parameters. We are able to show that the robust policy also has a threshold structure under fairly general assumptions and that it is more aggressive in transferring patients than the optimal nominal policy, which does not take into account parameter uncertainty. We present computational experiments using a data set of hospitalizations at 21 Kaiser Permanente Northern California hospitals and present empirical evidence of the sensitivity of various hospital metrics (mortality, length of stay, and average ICU occupancy) to small changes in the parameters. Although threshold policies are a simplification of the actual complex sequence of decisions leading (or not) to a transfer to the ICU, our work provides useful insights into the impact of parameter uncertainty on deriving simple policies for proactive ICU transfer that have strong empirical performance and theoretical guarantees.",
        "DOI": "10.1287/opre.2022.2403",
        "affiliation_name": "École des hautes études commerciales de Paris",
        "affiliation_city": "Jouy-en-Josas",
        "affiliation_country": "France"
    },
    {
        "paper_title": "Predict-Then-Optimise Strategies for Water Flow Control",
        "paper_author": "Vaz V.B.",
        "publication": "Leibniz International Proceedings in Informatics, LIPIcs",
        "citied_by": "0",
        "cover_date": "2023-09-01",
        "Abstract": "A pressure sewer system is a network of pump stations used to collect and manage sewage from individual properties that cannot be directly connected to the gravity driven sewer network due to the topography of the terrain. We consider a common scenario for a pressure sewer system, where individual sites collect sewage in a local tank, and then pump it into the gravity fed sewage network. Standard control systems simply wait until the local tank reaches (near) capacity and begin pumping out. Unfortunately such simple control usually leads to peaks in sewage flow in the morning and evening, corresponding to peak water usage in the properties. High peak flows require equalization basins or overflow systems, or larger capacity sewage treatment plants. In this paper we investigate combining prediction and optimisation to better manage peak sewage flows. We use simple prediction methods to generate realistic possible future scenarios, and then develop optimisation models to generate pumping plans that try to smooth out flows into the network. The solutions of these models create a policy for pumping out that is specialized to individual properties and which overall is able to substantially reduce peak flows.",
        "DOI": "10.4230/LIPIcs.CP.2023.42",
        "affiliation_name": "University of Melbourne",
        "affiliation_city": "Melbourne",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Parallax-based Imitation Learning for Insertion Tasks with Uncertainties of Hole Position and Wire Pose",
        "paper_author": "Niwa Y.",
        "publication": "IEEJ Transactions on Electronics, Information and Systems",
        "citied_by": "0",
        "cover_date": "2023-09-01",
        "Abstract": "Standard insertion machines require pre-determined component position and posture. If they change every time, we must solve this problem. Most conventional methods attempted to solve this task by identifying the position and posture. However, these methods require a multi-step strategy following the handmade rule. This paper proposes an imitation learning method to automate the wire insertion task with uncertainties in position and posture. The proposed model learns the motion policy through human demonstrations and maps image data to the robot's action in a single step. Moreover, the model considers the parallax of the stereo images for accurate insertion. In addition, the model outputs the insertion action and recovery action to recover from insertion failures. However, the standard data collection method cannot collect recovery actions, and manual labeling of action classes is essential. This paper proposes a novel data collection method called \"Labeling with Human Intervention (LHI)\"to tackle this problem. This method automatically generates action labels and collects recovery action with human intervention. We conducted real-space insertion tests and found that our approach achieved 97.2% (35/36).",
        "DOI": "10.1541/ieejeiss.143.862",
        "affiliation_name": "Gifu University",
        "affiliation_city": "Gifu",
        "affiliation_country": "Japan"
    },
    {
        "paper_title": "A weakly supervised framework for high-resolution crop yield forecasts",
        "paper_author": "Paudel D.",
        "publication": "Environmental Research Letters",
        "citied_by": "5",
        "cover_date": "2023-09-01",
        "Abstract": "Predictor inputs and labels (e.g. yield data) for crop yield forecasting are not always available at the same spatial resolution. Common statistical and machine learning methods require inputs and labels at the same resolution. Therefore, they cannot produce high resolution (HR) yield forecasts in the absence of HR yield data. We propose a weakly supervised (WS) deep learning framework that uses HR inputs and low resolution (LR) labels (crop areas and yields) to produce HR forecasts. The forecasting model was calibrated by aggregating HR forecasts and comparing with LR crop area and yield statistics. The framework was evaluated by disaggregating yields from parent statistical regions to sub-regions for five countries and two crops in Europe. Similarly, corn yields were disaggregated from counties to 10 km grids in the US. The performance of WS models was compared with naive disaggregation (ND) models, which assigned LR forecasts for a region or county to all HR sub-units, and strongly supervised models trained with HR yield labels. In Europe, all models (ND, WS and strongly supervised) were statistically similar, mainly due to the effect of yield trend. In the US, the WS models performed even better than the strongly supervised models. Based on Kendall’s rank correlation coefficient, the WS model forecasts captured significant amounts of HR yield variability. Combining information from WS with Trend model (using LR yield trend) and WS No Trend model (not using yield trend) provided good estimates of yields as well as spatial variability among sub-regions or grids. High resolution crop yield forecasts are useful to policymakers and other stakeholders for local analysis and monitoring. Our weakly supervised framework produces such forecasts even in the absence of high resolution yield data.",
        "DOI": "10.1088/1748-9326/acf50e",
        "affiliation_name": "Université de Montpellier",
        "affiliation_city": "Montpellier",
        "affiliation_country": "France"
    },
    {
        "paper_title": "Machine Learning Ensemble Modelling for Predicting Unemployment Duration",
        "paper_author": "Gabrikova B.",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "5",
        "cover_date": "2023-09-01",
        "Abstract": "Predictions of the unemployment duration of the economically active population play a crucial assisting role for policymakers and employment agencies in the well-organised allocation of resources (tied to solving problems of the unemployed, whether on the labour supply or demand side) and providing targeted support to jobseekers in their job search. This study aimed to develop an ensemble model that can serve as a reliable tool for predicting unemployment duration among jobseekers in Slovakia. The ensemble model was developed using real data from the database of jobseekers (those registered as unemployed and actively searching for a job through the Local Labour Office, Social Affairs, and Family) using the stacking method, incorporating predictions from three individual models: CART, CHAID, and discriminant analysis. The final meta-model was created using logistic regression and indicates an overall accuracy of the prediction of unemployment duration of almost 78%. This model demonstrated high accuracy and precision in identifying jobseekers at risk of long-term unemployment exceeding 12 months. The presented model, working with real data of a robust nature, represents an operational tool that can be used to check the functionality of the current labour market policy and to solve the problem of long-term unemployed individuals in Slovakia, as well as in the creation of future government measures aimed at solving the problem of unemployment. The measures from the state are financed from budget funds, and by applying the appropriate model, it is possible to arrive at the rationalization of the financing of these measures, or to specifically determine the means intended to solve the problem of long-term unemployment in Slovakia (this, together with the regional disproportion of unemployment, is considered one of the most prominent problems in the labour market in Slovakia). The model also has the potential to be adapted in other economies, taking into account country-specific conditions and variables, which is possible due to the data-mining approach used.",
        "DOI": "10.3390/app131810146",
        "affiliation_name": "University of Žilina",
        "affiliation_city": "Zilina",
        "affiliation_country": "Slovakia"
    },
    {
        "paper_title": "Optimizing User Engagement Through Adaptive Ad Sequencing",
        "paper_author": "Rafieian O.",
        "publication": "Marketing Science",
        "citied_by": "4",
        "cover_date": "2023-09-01",
        "Abstract": "In this paper, we propose a unified dynamic framework for adaptive ad sequencing that optimizes user engagement with ads. Our framework comprises three components: (1) a Markov decision process that incorporates intertemporal tradeoffs in ad interventions, (2) an empirical framework that combines machine learning methods with insights from causal inference to achieve personalization, counterfactual validity, and scal-ability, and (3) a robust policy evaluation method. We apply our framework to large-scale data from the leading in-app ad network of an Asian country. We find that the dynamic policy generated by our framework improves the current practice in the industry by 5.76%. This improvement almost entirely comes from the increased average ad response to each impression instead of the increased usage by each user. We further document a U-shaped pattern in improvements across the length of the user’s history, with high values when the user is new or when enough data are available for the user. Next, we show that ad diversity is higher under our policy and explore the reason behind it. We conclude by discussing the implications and broad applicability of our framework to settings where a platform wants to sequence content to optimize user engagement.",
        "DOI": "10.1287/mksc.2022.1423",
        "affiliation_name": "Cornell SC Johnson College of Business",
        "affiliation_city": "Ithaca",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Quantitative Analysis of the Driving Factors of Water Quality Variations in the Minjiang River in Southwestern China",
        "paper_author": "Liu C.",
        "publication": "Water (Switzerland)",
        "citied_by": "1",
        "cover_date": "2023-09-01",
        "Abstract": "The Minjiang River is an important first-level tributary of the Yangtze River. Understanding the driving factors of water quality variations in the Minjiang River is crucial for future policy planning of watershed ecology protection of the Yangtze River. The water quality of the Minjiang River is impacted by both meteorological factors and anthropogenic factors. By using wavelet analysis, machine learning, and Shapley analysis approaches, the impacts of meteorological factors and anthropogenic factors on the permanganate index (CODMn) and ammonia nitrogen (NH3-N) concentrations at the outlet of the Minjiang River Basin were quantified. The observed CODMn and NH3-N concentration data in the Minjiang River from 2016 to 2020 were decomposed into long-term trend signals and periodic signals. The long-term trends in water qualities showed that anthropogenic factors were the major driving factors, accounting for 98.38% of the impact on CODMn concentrations and 98.18% of the impact on NH3-N concentrations. The periodic fluctuations in water qualities in the Minjiang River Basin were mainly controlled by meteorological factors, with an impact of 68.89% on CODMn concentrations and 63.94% on NH3-N concentrations. Compared to anthropogenic factors, meteorological factors have a greater impact on water quality in the Minjiang River Basin during both the high-temperature and rainy seasons from July to September and during the winter from December to February. The separate quantification of impacts of driving factors on the varying water quality signals contributed to the originality in this work, providing more intuitive insights for the assessment of the influences of policies and the climate change on the water quality.",
        "DOI": "10.3390/w15183299",
        "affiliation_name": "Chinese Research Academy of Environmental Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Research on Ecological Driving Following Strategy Based on Deep Reinforcement Learning",
        "paper_author": "Zhou W.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "0",
        "cover_date": "2023-09-01",
        "Abstract": "Traditional car-following models usually prioritize minimizing inter-vehicle distance error when tracking the preceding vehicle, often neglecting crucial factors like driving economy and passenger ride comfort. To address this limitation, this paper integrates the concept of eco-driving and formulates a multi-objective function that encompasses economy, comfort, and safety. A novel eco-driving car-following strategy based on the deep deterministic policy gradient (DDPG) is proposed, employing the vehicle’s state, including data from the preceding vehicle and the ego vehicle, as the state space, and the desired time headway from the intelligent driver model (IDM) as the action space. The DDPG agent is trained to dynamically adjust the following vehicle’s speed in real-time, striking a balance between driving economy, comfort, and safety. The results reveal that the proposed DDPG-based IDM model significantly enhances comfort, safety, and economy when compared to the fixed-time headway IDM model, achieving an economy improvement of 2.66% along with enhanced comfort. Moreover, the proposed approach maintains a relatively stable following distance under medium-speed conditions, ensuring driving safety. Additionally, the comprehensive performance of the proposed method is analyzed under three typical scenarios, confirming its generalization capability. The DDPG-enhanced IDM car-following model aligns with eco-driving principles, offering novel insights for advancing IDM-based car-following models.",
        "DOI": "10.3390/su151813325",
        "affiliation_name": "Jiangsu University",
        "affiliation_city": "Zhenjiang",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Fraud Detection in Healthcare Insurance Claims Using Machine Learning",
        "paper_author": "Nabrawi E.",
        "publication": "Risks",
        "citied_by": "16",
        "cover_date": "2023-09-01",
        "Abstract": "Healthcare fraud is intentionally submitting false claims or producing misinterpretation of facts to obtain entitlement payments. Thus, it wastes healthcare financial resources and increases healthcare costs. Subsequently, fraud poses a substantial financial challenge. Therefore, supervised machine and deep learning analytics such as random forest, logistic regression, and artificial neural networks are successfully used to detect healthcare insurance fraud. This study aims to develop a health model that automatically detects fraud from health insurance claims in Saudi Arabia. The model indicates the greatest contributing factor to fraud with optimal accuracy. The labeled imbalanced dataset used three supervised deep and machine learning methods. The dataset was obtained from three healthcare providers in Saudi Arabia. The applied models were random forest, logistic regression, and artificial neural networks. The SMOT technique was used to balance the dataset. Boruta object feature selection was applied to exclude insignificant features. Validation metrics were accuracy, precision, recall, specificity, F1 score, and area under the curve (AUC). Random forest classifiers indicated policy type, education, and age as the most significant features with an accuracy of 98.21%, 98.08% precision, 100% recall, an F1 score of 99.03%, specificity of 80%, and an AUC of 90.00%. Logistic regression resulted in an accuracy of 80.36%, 97.62% precision, 80.39% recall, an F1 score of 88.17%, specificity of 80%, and an AUC of 80.20%. ANN revealed an accuracy of 94.64%, 98.00% precision, 96.08% recall, an F1 score of 97.03%, a specificity of 80%, and an AUC of 88.04%. This predictive analytics study applied three successful models, each of which yielded acceptable accuracy and validation metrics; however, further research on a larger dataset is advised.",
        "DOI": "10.3390/risks11090160",
        "affiliation_name": "King Saud bin Abdulaziz University for Health Sciences",
        "affiliation_city": "Riyadh",
        "affiliation_country": "Saudi Arabia"
    },
    {
        "paper_title": "Understanding and predicting systemic corporate distress: a machine-learning approach",
        "paper_author": "Hacibedel B.",
        "publication": "Journal of Credit Risk",
        "citied_by": "1",
        "cover_date": "2023-09-01",
        "Abstract": "In this paper we study systemic nonfinancial corporate sector distress using firm-level probabilities of default, covering 55 economies and spanning the last three decades. Systemic corporate distress is identified by elevated probabilities of default across a large portion of the firms in an economy. A machine-learning-based early-warning system is constructed to predict the risk of systemic distress in one year’s time. Our results show that credit expansion, monetary policy tightening, overvalued stock prices and debt-linked balance-sheet weaknesses predict corporate distress. We also find that systemic corporate distress events are associated with contractions in gross domestic product. Their impacts are milder than those of financial crises.",
        "DOI": "10.21314/JCR.2023.006",
        "affiliation_name": "International Monetary Fund",
        "affiliation_city": "Washington, D.C.",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Improving the Performance of Autonomous Driving through Deep Reinforcement Learning",
        "paper_author": "Tammewar A.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "4",
        "cover_date": "2023-09-01",
        "Abstract": "Reinforcement learning (RL) is revolutionizing the artificial intelligence (AI) domain and significantly aiding in building autonomous systems with a higher level comprehension of the world as we observe it. Deep learning (DL) facilitates RL to scale and resolve previously intractable problems, for instance, allowing supervision principles designed for robots to be acquired directly from visual data, developing video game proficiency from pixel-level information, etc. Recent research shows that RL algorithms help represent problems dealing with high-dimensional, unprocessed data input and can have successful applications in computer vision, pattern identification, natural language analysis, and speech parsing. This research paper focuses on training a simulation model of a car to navigate autonomously on a racetrack using RL. The study explores several fundamental algorithms in Deep RL, namely Proximal Policy Optimization (PPO), Deep Q-network (DQN), and Deep Deterministic Policy Gradient (DDPG). The paper documents a comparative analysis of these three prominent algorithms—based on their speed, accuracy, and overall performance. After a thorough evaluation, the research indicates that the DQN surpassed the other existing algorithms. This study further examined the performance of the DQN with and without ε-decay and observed that the DQN with ε-decay is better suited for our objective and is significantly more stable than its non ε-decay counterpart. The findings from this research could assist in improving the performance and stability of autonomous vehicles using the DQN with ε -decay. It concludes by discussing the fine-tuning of the model for future real-world applications and the potential research areas within the field of autonomous driving.",
        "DOI": "10.3390/su151813799",
        "affiliation_name": "Symbiosis Institute of Technology",
        "affiliation_city": "Pune",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Impact of an ML-Based Demand Response Mechanism on the Electrical Distribution Network: A Case Study in Terni",
        "paper_author": "Bucarelli M.A.",
        "publication": "Electronics (Switzerland)",
        "citied_by": "0",
        "cover_date": "2023-09-01",
        "Abstract": "The development of smart grids requires the active participation of end users through demand response mechanisms to provide technical benefits to the distribution network and receive economic savings. Integrating advanced machine learning tools makes it possible to optimise the network and manage the mechanism to maximise the benefits. This paper proceeds by forecasting consumption for the next 24 h using a recurrent neural network and by processing these data using a reinforcement learning-based optimisation model to identify the best demand response policy. The model is tested in a real environment: a portion of the Terni electrical distribution network. Several scenarios were identified, considering users’ participation at different levels and limiting the potential with various constraints.",
        "DOI": "10.3390/electronics12183948",
        "affiliation_name": "Atos Research and Innovation",
        "affiliation_city": "Madrid",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "Analysis of Factors Associated with Highway Personal Car and Truck Run-Off-Road Crashes: Decision Tree and Mixed Logit Model with Heterogeneity in Means and Variances Approaches",
        "paper_author": "Champahom T.",
        "publication": "Informatics",
        "citied_by": "1",
        "cover_date": "2023-09-01",
        "Abstract": "Among several approaches to analyzing crash research, the use of machine learning and econometric analysis has found potential in the analysis. This study aims to empirically examine factors influencing the single-vehicle crash for personal cars and trucks using decision trees (DT) and mixed binary logit with heterogeneity in means and variances (RPBLHMV) and compare model accuracy. The data in this study were obtained from the Department of Highway during 2011–2017, and the results indicated that the RPBLHMV was superior due to its higher overall prediction accuracy, sensitivity, and specificity values when compared to the DT model. According to the RPBLHMV results, car models showed that injury severity was associated with driver gender, seat belt, mount the island, defect equipment, and safety equipment. For the truck model, it was found that crashes located at intersections or medians, mounts on the island, and safety equipment have a significant influence on injury severity. DT results also showed that running off-road and hitting safety equipment can reduce the risk of death for car and truck drivers. This finding can illustrate the difference causing the dependent variable in each model. The RPBLHMV showed the ability to capture random parameters and unobserved heterogeneity. But DT can be easily used to provide variable importance and show which factor has the most significance by sequencing. Each model has advantages and disadvantages. The study findings can give relevant authorities choices for measures and policy improvement based on two analysis methods in accordance with their policy design. Therefore, whether advocating road safety or improving policy measures, the use of appropriate methods can increase operational efficiency.",
        "DOI": "10.3390/informatics10030066",
        "affiliation_name": "Rajamangala University of Technology Isan",
        "affiliation_city": "Nakhon Ratchasima",
        "affiliation_country": "Thailand"
    },
    {
        "paper_title": "Analysis of the Effectiveness of Public Health Measures on COVID-19 Transmission",
        "paper_author": "Silva T.C.",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "2",
        "cover_date": "2023-09-01",
        "Abstract": "In this study, we investigate the COVID-19 epidemics in Brazilian cities, using early-time approximations of the SIR model in networks and combining the VAR (vector autoregressive) model with machine learning techniques. Different from other works, the underlying network was constructed by inputting real-world data on local COVID-19 cases reported by Brazilian cities into a regularized VAR model. This model estimates directional COVID-19 transmission channels (connections or links between nodes) of each pair of cities (vertices or nodes) using spectral network analysis. Despite the simple epidemiological model, our predictions align well with the real COVID-19 dynamics across Brazilian municipalities, using data only up until May 2020. Given the rising number of infectious people in Brazil—a possible indicator of a second wave—these early-time approximations could be valuable in gauging the magnitude of the next contagion peak. We further examine the effect of public health policies, including social isolation and mask usage, by creating counterfactual scenarios to quantify the human impact of these public health measures in reducing peak COVID-19 cases. We discover that the effectiveness of social isolation and mask usage varies significantly across cities. We hope our study will support the development of future public health measures.",
        "DOI": "10.3390/ijerph20186758",
        "affiliation_name": "Fundacao Getulio Vargas",
        "affiliation_city": "Rio de Janeiro",
        "affiliation_country": "Brazil"
    },
    {
        "paper_title": "Economic and Policy Research Interests Highlighted in the 25<sup>th</sup> NIMH-Sponsored Mental Health Services Research Conference",
        "paper_author": "Humensky J.L.",
        "publication": "Journal of Mental Health Policy and Economics",
        "citied_by": "1",
        "cover_date": "2023-09-01",
        "Abstract": "Background: The National Institute of Mental Health (NIMH) remains committed to addressing real-world challenges with delivering high quality mental health care to people in need by advancing a services research agenda to improve access, continuity, quality, equity, and value of mental healthcare nationwide, and to improve outcomes for people with serious mental illnesses (SMI). The NIMH-Sponsored Mental Health Services Research Conference (MHSR) is a highly productive venue for discussing topics of interest to NIMH audiences and disseminating NIMH’s latest research findings directly to mental health clinicians, policy makers, administrators, advocates, consumers, and scientists who attend. Aims: This Perspective summarizes and provides highlights from the 25th MHSR. It also reviews three papers presented at the 25th MSHR and subsequently published in the June 2023 special issue of The Journal of Mental Health Policy and Economics (JMHPE). Methods: The authors review three papers published in the June 2023 special issue of JMHPE, identifying common themes across the papers and illustrating how the papers’ findings promote key areas of NIMH research interests. Results: Three important areas are highlighted in this review: (i) service user engagement in the research enterprise, (ii) financing the implementation of the 988 Suicide & Crisis Lifeline, and (iii) methods to predict mental health workforce turnover. Discussion: These three papers illustrate key areas in which policy research can help to promote quality mental health care. One notable common theme across the papers is that of the role that end users play in the research enterprise. The papers focus on (i) service users and the value they bring to informing the practice of research, (ii) policy makers and the information they need to make evidence-informed decisions, and (iii) provider organization leadership, by using an innovative machine learning process to help organizations predict and address staff turnover. Implications for Health Care: NIMH encourages and often requires strong research-practice partnerships to help ensure findings will be of value to end users and make their way into the practice setting. The three papers reviewed in this perspective are exemplars of how necessary stakeholder partnerships are to improve care for those with mental illness. Implications for Health Policies: The highlighted papers (i) provide recommendations for structural changes to research institutions to increase service user engagement in all aspects of the research enterprise, (ii) identify policy solutions to improve fiscal readiness to address increased demand of 988, and (iii) pilot a novel data-driven approach to predict mental health workforce turnover, a significant problem in community mental health clinics, offering health system leaders and policy makers an opportunity to proactively intervene to help maintain continuity of staffing. Implications for Further Research: Consistent with NIMH’s Strategic Plan for Research and current funding announcements, there remains an urgent need to (i) develop strategies to better implement, scale, and sustain existing evidence-supported treatments and services, particularly in historically underserved communities, and (ii) develop, test, and evaluate new solutions to improve access, continuity, quality, equity, and value of care.",
        "DOI": "NA",
        "affiliation_name": "National Institute of Mental Health",
        "affiliation_city": "Bethesda",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Changing Land Use and Urban Dynamics around an Industrial Zone in Bangladesh: A Remote Sensing Analysis",
        "paper_author": "Basak P.",
        "publication": "Land",
        "citied_by": "2",
        "cover_date": "2023-09-01",
        "Abstract": "This article examines the adverse effect of rapid industrialization around Bangladesh’s Dhaka Export Processing Zone (DEPZ) by analyzing Landsat satellite images captured between 1989 and 2019. Image classification was performed to separate built-up areas with machine learning algorithms in Google Earth Engine. Image analysis was conducted using ArcMap and ArcGIS Pro. Field observations, interviews, and the literature review provided information for explanations about the phenomenon observed from satellite image analyses. The findings reveal that when DEPZ started its operation in 1993, there was hardly any built-up area in the vicinity. Within three decades, over 25% of the land within a 5 km radius of DEPZ has been converted into a built-up area, triggering an almost seven-fold increase in population. Industrial and urban growth in the DEPZ area has caused significant soil and water pollution in the broader region. As a result, the quantity and quality of agricultural land has degraded. In the long run, the planned industrial development initiative has contributed to unsustainable urban growth and environmental consequences. Insights drawn from this article can guide policymakers to re-evaluate their policy for rapid and large-scale industrialization.",
        "DOI": "10.3390/land12091753",
        "affiliation_name": "University of Liberal Arts Bangladesh",
        "affiliation_city": "Dhaka",
        "affiliation_country": "Bangladesh"
    },
    {
        "paper_title": "Energy Consumption and Human Well-Being: A Systematic Review",
        "paper_author": "tho Pesch G.",
        "publication": "Energies",
        "citied_by": "9",
        "cover_date": "2023-09-01",
        "Abstract": "Understanding the relationship between energy use and well-being is crucial for designing holistic energy policy. The latter has to both effectively mitigate climate change driven by current fossil-based energy systems as well as promote human development, which requires energy. While a significant body of research investigates this relationship, study designs differ significantly, so findings cannot be easily generalized. This machine learning-aided review provides an overview of the current state of the literature examining this relationship. We highlight and discuss methodological differences between the studies, including their perspective (top-down or bottom-up), spatial scope, and the respective energy and well-being indicators used. The review reveals that most research takes a top-down perspective, analyzing country-level data across multiple countries. These studies typically find a positive relationship between energy use and well-being, and most confirm the existence of a saturation effect. We reveal that countries in the Global South are underrepresented in current studies. Bottom-up studies focus on specific countries or country groups using household-level data, yielding more nuanced findings that can be further disaggregated by consumption domain. We find that energy and well-being indicators differ substantially across studies, yet the implications of this choice are not always sufficiently discussed. The review shows and discusses the current shift from production- to consumption-based energy indicators.",
        "DOI": "10.3390/en16186494",
        "affiliation_name": "Haskoli Islands",
        "affiliation_city": "Reykjavik",
        "affiliation_country": "Iceland"
    },
    {
        "paper_title": "Leveraging large language models to monitor climate technology innovation",
        "paper_author": "Toetzke M.",
        "publication": "Environmental Research Letters",
        "citied_by": "4",
        "cover_date": "2023-09-01",
        "Abstract": "To achieve net-zero emissions, public policy needs to foster rapid innovation of climate technologies. However, there is a scarcity of comprehensive and up-to-date evidence to guide policymaking by monitoring climate innovation systems. This is notable, especially at the center of the innovation process, where nascent inventions transition into profitable and scalable market solutions. Here, we discuss the potential of large language models (LLMs) to monitor climate technology innovation. By analyzing large pools of unstructured text data sources, such as company reports and social media, LLMs can automate information retrieval processes and thereby improve existing monitoring in terms of cost-effectiveness, timeliness, and comprehensiveness. In this perspective, we show how LLMs can play a crucial role in informing innovation policy for the energy transition by highlighting promising use cases and prevailing challenges for research and policy.",
        "DOI": "10.1088/1748-9326/acf233",
        "affiliation_name": "Munich Center for Machine Learning",
        "affiliation_city": "Munich",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Graph-Based Deep Learning Model for Forecasting Chloride Concentration in Urban Streams to Protect Salt-Vulnerable Areas",
        "paper_author": "Oliveira Santos V.",
        "publication": "Environments - MDPI",
        "citied_by": "7",
        "cover_date": "2023-09-01",
        "Abstract": "In cold-climate regions, road salt is used as a deicer for winter road maintenance. The applied road salt melts ice and snow on roads and can be washed off through storm sewer systems into nearby urban streams, harming the freshwater ecosystem. Therefore, aiming to develop a precise and accurate model to determine future chloride concentration in the Credit River in Ontario, Canada, the present work makes use of a “Graph Neural Network”–“Sample and Aggregate” (GNN-SAGE). The proposed GNN-SAGE is compared to other models, including a Deep Neural Network-based transformer (DNN-Transformer) and a benchmarking persistence model for a 6 h forecasting horizon. The proposed GNN-SAGE surpassed both the benchmarking persistence model and the DNN-Transformer model, achieving RMSE and R2 values of 51.16 ppb and 0.88, respectively. Additionally, a SHAP analysis provides insight into the variables that influence the model’s forecasting, showing the impact of the spatiotemporal neighboring data from the network and the seasonality variables on the model’s result. The GNN-SAGE model shows potential for use in the real-time forecasting of water quality in urban streams, aiding in the development of regulatory policies to protect vulnerable freshwater ecosystems in urban areas.",
        "DOI": "10.3390/environments10090157",
        "affiliation_name": "Universidade Federal do Ceará",
        "affiliation_city": "Fortaleza",
        "affiliation_country": "Brazil"
    },
    {
        "paper_title": "Performance Assessment of Irrigation Projects in Nepal by Integrating Landsat Images and Local Data",
        "paper_author": "Neupane A.",
        "publication": "Remote Sensing",
        "citied_by": "0",
        "cover_date": "2023-09-01",
        "Abstract": "With growing global concern for food and water insecurity, an efficient method to monitor irrigation projects is essential, especially in the developing world where irrigation performance is often suboptimal. In Nepal, the irrigated area has not been objectively recorded, although their assessment has substantial implications for national policy, project’s annual budgets, and donor funding. Here, we present the application of Landsat images to measure irrigated areas in Nepal for the past 17 years to contribute to the assessment of the irrigation performance. Landsat 5 TM (2006–2011) and Landsat 8 OLI (2013–2022) images were used to develop a machine learning model, which classifies irrigated and non-irrigated areas in the study areas. The random forest classification achieved an overall accuracy of 82.2% and kappa statistics of 0.72. For the class of irrigation areas, the producer’s accuracy and consumer’s accuracy were 79% and 96%, respectively. Our regionally trained machine learning model outperforms the existing global cropland map, highlighting the need for such models for local irrigation project evaluations. We assess irrigation project performance and its drivers by combining long-term changes in satellite-derived irrigated areas with local data related to irrigation performance, such as annual budget, irrigation service fee, crop yield, precipitation, and main canal discharge.",
        "DOI": "10.3390/rs15184633",
        "affiliation_name": "The University of Tokyo",
        "affiliation_city": "Tokyo",
        "affiliation_country": "Japan"
    },
    {
        "paper_title": "Ethical Implications of Chatbot Utilization in Nephrology",
        "paper_author": "Garcia Valencia O.A.",
        "publication": "Journal of Personalized Medicine",
        "citied_by": "35",
        "cover_date": "2023-09-01",
        "Abstract": "This comprehensive critical review critically examines the ethical implications associated with integrating chatbots into nephrology, aiming to identify concerns, propose policies, and offer potential solutions. Acknowledging the transformative potential of chatbots in healthcare, responsible implementation guided by ethical considerations is of the utmost importance. The review underscores the significance of establishing robust guidelines for data collection, storage, and sharing to safeguard privacy and ensure data security. Future research should prioritize defining appropriate levels of data access, exploring anonymization techniques, and implementing encryption methods. Transparent data usage practices and obtaining informed consent are fundamental ethical considerations. Effective security measures, including encryption technologies and secure data transmission protocols, are indispensable for maintaining the confidentiality and integrity of patient data. To address potential biases and discrimination, the review suggests regular algorithm reviews, diversity strategies, and ongoing monitoring. Enhancing the clarity of chatbot capabilities, developing user-friendly interfaces, and establishing explicit consent procedures are essential for informed consent. Striking a balance between automation and human intervention is vital to preserve the doctor–patient relationship. Cultural sensitivity and multilingual support should be considered through chatbot training. To ensure ethical chatbot utilization in nephrology, it is imperative to prioritize the development of comprehensive ethical frameworks encompassing data handling, security, bias mitigation, informed consent, and collaboration. Continuous research and innovation in this field are crucial for maximizing the potential of chatbot technology and ultimately improving patient outcomes.",
        "DOI": "10.3390/jpm13091363",
        "affiliation_name": "Faculty of Medicine Ramathibodi Hospital, Mahidol University",
        "affiliation_city": "Bangkok",
        "affiliation_country": "Thailand"
    },
    {
        "paper_title": "Nonlinear Hierarchical Effects of Housing Prices and Built Environment Based on Multiscale Life Circle—A Case Study of Chengdu",
        "paper_author": "Song Y.",
        "publication": "ISPRS International Journal of Geo-Information",
        "citied_by": "4",
        "cover_date": "2023-09-01",
        "Abstract": "Determining the optimal planning scale for urban life circles and analyzing the associated built environment factors are crucial for comprehending and regulating residential differentiation. This study aims to bridge the current research void concerning the nonlinear hierarchical relationships between the built environment and residential differentiation under the multiscale effect. Specifically, six indicators were derived from urban crowdsourcing data: diversity of built environment function (DBEF1), density of built environment function (DBEF2), blue–green environment (BGE), traffic accessibility (TA), population vitality (PV), and shopping vitality (SV). Then, a gradient boosting decision tree (GBDT) was applied to derive the analysis of these indicators. Finally, the interpretability of machine learning was leveraged to quantify the relative importance and nonlinear relationships between built environment indicators and housing prices. The results indicate a hierarchical structure and inflection point effect of the built environment on residential premiums. Notably, the impact trend of the built environment on housing prices within a 15 min life circle remains stable. The effect of crowd behavior, as depicted by PV and SV, on housing prices emerges as the most significant factor. Furthermore, this study also categorizes housing into common and high-end residences, thereby unveiling that distinct residential neighborhoods exhibit varying degrees of dependence on the built environment. The built environment exerts a scale effect on the formation of residential differentiation, with housing prices exhibiting increased sensitivity to the built environment at a smaller life circle scale. Conversely, the effect of the built environment on housing prices is amplified at a larger life circle scale. Under the dual influence of the scale and hierarchical effect, this framework can dynamically adapt to the uncertainty of changes in life circle planning policies and residential markets. This provides strong theoretical support for exploring the optimal life circle scale, alleviating residential differentiation, and promoting group fairness.",
        "DOI": "10.3390/ijgi12090371",
        "affiliation_name": "Sichuan Normal University",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China"
    },
    {
        "paper_title": "The Real-Time Optimal Attitude Control of Tunnel Boring Machine Based on Reinforcement Learning",
        "paper_author": "Jia G.",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "2",
        "cover_date": "2023-09-01",
        "Abstract": "Efficient control of tunnel boring machine (TBM) tunneling along the designed tunnel axis in an unknown variable geological environment is a difficult and significant task. At present, the TBM attitude during tunneling is mostly manually controlled based on the deviation between the tunneling axis and the designed tunnel axis and their experiences. The tunneling axis from manual control is often the snakelike motion around the designed tunnel axis, even exceeding the deviation limit, for which this paper analyzed three reasons, the unknown geological environment, the hysteresis of TBM position response, and the unsolved overall optimization of tunneling axis. For these reasons, this paper proposed a real-time optimal control framework of TBM attitude based on reinforcement learning, which contains the geological information predictive model, TBM attitude and position (TBMAP) predictive model, and optimal attitude control policy (OACP). This framework can predict the current geological information in real-time and provide the corresponding real-time optimal attitude control that simultaneously considers the hysteresis of TBM position response and the overall optimization of the tunneling axis. This attitude control framework can be directly deployed to TBM without increasing costs and excessive modifications to the equipment. To verify the effectiveness of this attitude control framework, the Xinjiang Yiner Water Supply Phase II Project, using the TBM method, was adopted as a case study. The results revealed that the accuracy of geological environment recognition reached 94%, and OACP can significantly reduce the accumulated deviation of the tunneling axis from the designed tunnel axis by over 80% compared with the manual control and easily provide real-time decision support for attitude control in actual engineering.",
        "DOI": "10.3390/app131810026",
        "affiliation_name": "Dalian University of Technology",
        "affiliation_city": "Dalian",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Towards a Universal Privacy Model for Electronic Health Record Systems: An Ontology and Machine Learning Approach",
        "paper_author": "Nowrozy R.",
        "publication": "Informatics",
        "citied_by": "13",
        "cover_date": "2023-09-01",
        "Abstract": "This paper proposed a novel privacy model for Electronic Health Records (EHR) systems utilizing a conceptual privacy ontology and Machine Learning (ML) methodologies. It underscores the challenges currently faced by EHR systems such as balancing privacy and accessibility, user-friendliness, and legal compliance. To address these challenges, the study developed a universal privacy model designed to efficiently manage and share patients’ personal and sensitive data across different platforms, such as MHR and NHS systems. The research employed various BERT techniques to differentiate between legitimate and illegitimate privacy policies. Among them, Distil BERT emerged as the most accurate, demonstrating the potential of our ML-based approach to effectively identify inadequate privacy policies. This paper outlines future research directions, emphasizing the need for comprehensive evaluations, testing in real-world case studies, the investigation of adaptive frameworks, ethical implications, and fostering stakeholder collaboration. This research offers a pioneering approach towards enhancing healthcare information privacy, providing an innovative foundation for future work in this field.",
        "DOI": "10.3390/informatics10030060",
        "affiliation_name": "La Trobe University",
        "affiliation_city": "Melbourne",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Modelling the Unidentified Abortion Burden from Four Infectious Pathogenic Microorganisms (Leptospira interrogans, Brucella abortus, Brucella ovis, and Chlamydia abortus) in Ewes Based on Artificial Neural Networks Approach: The Epidemiological Basis for a Control Policy",
        "paper_author": "Arteaga-Troncoso G.",
        "publication": "Animals",
        "citied_by": "2",
        "cover_date": "2023-09-01",
        "Abstract": "Unidentified abortion, of which leptospirosis, brucellosis, and ovine enzootic abortion are important factors, is the main cause of disease spread between animals and humans in all agricultural systems in most developing countries. Although there are well-defined risk factors for these diseases, these characteristics do not represent the prevalence of the disease in different regions. This study predicts the unidentified abortion burden from multi-microorganisms in ewes based on an artificial neural networks approach and the GLM. Methods: A two-stage cluster survey design was conducted to estimate the seroprevalence of abortifacient microorganisms and to identify putative factors of infectious abortion. Results: The overall seroprevalence of Brucella was 70.7%, while Leptospira spp. was 55.2%, C. abortus was 21.9%, and B. ovis was 7.4%. Serological detection with four abortion-causing microorganisms was determined only in 0.87% of sheep sampled. The best GLM is integrated via serological detection of serovar Hardjo and Brucella ovis in animals of the slopes with elevation between 2600 and 2800 meters above sea level from the municipality of Xalatlaco. Other covariates included in the GLM, such as the sheep pen built with materials of metal grids and untreated wood, dirt and concrete floors, bed of straw, and the well water supply were also remained independently associated with infectious abortion. Approximately 80% of those respondents did not wear gloves or masks to prevent the transmission of the abortifacient zoonotic microorganisms. Conclusions: Sensitizing stakeholders on good agricultural practices could improve public health surveillance. Further studies on the effect of animal–human transmission in such a setting is worthwhile to further support the One Health initiative.",
        "DOI": "10.3390/ani13182955",
        "affiliation_name": "Instituto Nacional de Perinatologia",
        "affiliation_city": "Mexico",
        "affiliation_country": "Mexico"
    },
    {
        "paper_title": "Short-Term Forecasting of Electric Vehicle Load Using Time Series, Machine Learning, and Deep Learning Techniques",
        "paper_author": "Vishnu G.",
        "publication": "World Electric Vehicle Journal",
        "citied_by": "14",
        "cover_date": "2023-09-01",
        "Abstract": "Electric vehicles (EVs) are inducing revolutionary developments to the transportation and power sectors. Their innumerable benefits are forcing nations to adopt this sustainable mode of transport. Governments are framing and implementing various green energy policies. Nonetheless, there exist several critical challenges and concerns to be resolved in order to reap the complete benefits of E-mobility. The impacts of unplanned EV charging are a major concern. Accurate EV load forecasting followed by an efficient charge scheduling system could, to a large extent, solve this problem. This work focuses on short-term EV demand forecasting using three learning frameworks, which were applied to real-time adaptive charging network (ACN) data, and performance was analyzed. Auto-regressive (AR) forecasting, support vector regression (SVR), and long short-term memory (LSTM) frameworks demonstrated good performance in EV charging demand forecasting. Among these, LSTM showed the best performance with a mean absolute error (MAE) of 4 kW and a root-mean-squared error (RMSE) of 5.9 kW.",
        "DOI": "10.3390/wevj14090266",
        "affiliation_name": "Amrita University, Amritapuri Campus",
        "affiliation_city": "Kollam",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Big Data Analytics with the Multivariate Adaptive Regression Splines to Analyze Key Factors Influencing Accident Severity in Industrial Zones of Thailand: A Study on Truck and Non-Truck Collisions",
        "paper_author": "Seefong M.",
        "publication": "Big Data and Cognitive Computing",
        "citied_by": "3",
        "cover_date": "2023-09-01",
        "Abstract": "Machine learning currently holds a vital position in predicting collision severity. Identifying factors associated with heightened risks of injury and fatalities aids in enhancing road safety measures and management. Presently, Thailand faces considerable challenges with respect to road traffic accidents. These challenges are particularly acute in industrial zones, where they contribute to a rise in injuries and fatalities. The mixture of heavy traffic, comprising both trucks and non-trucks, significantly amplifies the risk of accidents. This situation, hence, generates profound concerns for road safety in Thailand. Consequently, discerning the factors that influence the severity of injuries and fatalities becomes pivotal for formulating effective road safety policies and measures. This study is specifically aimed at predicting the factors contributing to the severity of accidents involving truck and non-truck collisions in industrial zones. It considers a variety of aspects, including roadway characteristics, underlying assumptions of cause, crash characteristics, and weather conditions. Due to the fact that accident data is big data with specific characteristics and complexity, with the employment of machine learning in tandem with the Multi-variate Adaptive Regression Splines technique, we can make precise predictions to identify the factors influencing the severity of collision outcomes. The analysis demonstrates that various factors augment the severity of accidents involving trucks. These include darting in front of a vehicle, head-on collisions, and pedestrian collisions. Conversely, for non-truck related collisions, the significant factors that heighten severity are tailgating, running signs/signals, angle collisions, head-on collisions, overtaking collisions, pedestrian collisions, obstruction collisions, and collisions during overcast conditions. These findings illuminate the significant factors influencing the severity of accidents involving trucks and non-trucks. Such insights provide invaluable information for developing targeted road safety measures and policies, thereby contributing to the mitigation of injuries and fatalities.",
        "DOI": "10.3390/bdcc7030156",
        "affiliation_name": "Rajamangala University of Technology Isan",
        "affiliation_city": "Nakhon Ratchasima",
        "affiliation_country": "Thailand"
    },
    {
        "paper_title": "The Influence of Transportation Accessibility on Traffic Volumes in South Korea: An Extreme Gradient Boosting Approach",
        "paper_author": "Lee S.",
        "publication": "Urban Science",
        "citied_by": "0",
        "cover_date": "2023-09-01",
        "Abstract": "This study explored how transportation accessibility and traffic volumes for automobiles, buses, and trucks are related. This study employed machine learning techniques, specifically the extreme gradient boosting decision tree model (XGB) and Shapley Values (SHAP), with national data sources in South Korea collected from the Korea Transport Institute, Statistics Korea, and National Spatial Data Infrastructure Portal. Several key findings of feature importance and plots in non-linear relationships are as follows: First, accessibility indicators exhibited around 5 to 10% of feature importance except for Mart (around 50%). Second, better accessibility to public transportation infrastructures, such as bus stops and transit stations, was associated with higher annual average daily traffic (AADT), particularly in metropolitan areas including Seoul and Busan. Third, access to large-scale markets may have unintended effects on traffic volumes for both vehicles and automobiles. Fourth, it was shown that lower rates of AADT were associated with higher accessibility to elementary schools for all three modes of transportation. This study contributes to (1) understanding complex relationships between the variables, (2) emphasizing the role of transportation accessibility in transportation plans and policies, and (3) offering relevant policy implications.",
        "DOI": "10.3390/urbansci7030091",
        "affiliation_name": "Chungnam National University",
        "affiliation_city": "Daejeon",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "A Comprehensive Survey on Knowledge-Defined Networking",
        "paper_author": "Wijesekara P.A.D.S.N.",
        "publication": "Telecom",
        "citied_by": "11",
        "cover_date": "2023-09-01",
        "Abstract": "Traditional networking is hardware-based, having the control plane coupled with the data plane. Software-Defined Networking (SDN), which has a logically centralized control plane, has been introduced to increase the programmability and flexibility of networks. Knowledge-Defined Networking (KDN) is an advanced version of SDN that takes one step forward by decoupling the management plane from control logic and introducing a new plane, called a knowledge plane, decoupled from control logic for generating knowledge based on data collected from the network. KDN is the next-generation architecture for self-learning, self-organizing, and self-evolving networks with high automation and intelligence. Even though KDN was introduced about two decades ago, it had not gained much attention among researchers until recently. The reasons for delayed recognition could be due to the technology gap and difficulty in direct transformation from traditional networks to KDN. Communication networks around the globe have already begun to transform from SDNs into KDNs. Machine learning models are typically used to generate knowledge using the data collected from network devices and sensors, where the generated knowledge may be further composed to create knowledge ontologies that can be used in generating rules, where rules and/or knowledge can be provided to the control, management, and application planes for use in decision-making processes, for network monitoring and configuration, and for dynamic adjustment of network policies, respectively. Among the numerous advantages that KDN brings compared to SDN, enhanced automation and intelligence, higher flexibility, and improved security stand tall. However, KDN also has a set of challenges, such as reliance on large quantities of high-quality data, difficulty in integration with legacy networks, the high cost of upgrading to KDN, etc. In this survey, we first present an overview of the KDN architecture and then discuss each plane of the KDN in detail, such as sub-planes and interfaces, functions of each plane, existing standards and protocols, different models of the planes, etc., with respect to examples from the existing literature. Existing works are qualitatively reviewed and assessed by grouping them into categories and assessing the individual performance of the literature where possible. We further compare and contrast traditional networks and SDN against KDN. Finally, we discuss the benefits, challenges, design guidelines, and ongoing research of KDNs. Design guidelines and recommendations are provided so that identified challenges can be mitigated. Therefore, this survey is a comprehensive review of architecture, operation, applications, and existing works of knowledge-defined networks.",
        "DOI": "10.3390/telecom4030025",
        "affiliation_name": "University of Ruhuna",
        "affiliation_city": "Galle",
        "affiliation_country": "Sri Lanka"
    },
    {
        "paper_title": "Interactive medical image segmentation with self-adaptive confidence calibration",
        "paper_author": "Shen C.",
        "publication": "Frontiers of Information Technology and Electronic Engineering",
        "citied_by": "3",
        "cover_date": "2023-09-01",
        "Abstract": "Interactive medical image segmentation based on human-in-the-loop machine learning is a novel paradigm that draws on human expert knowledge to assist medical image segmentation. However, existing methods often fall into what we call interactive misunderstanding, the essence of which is the dilemma in trading off short- and long-term interaction information. To better use the interaction information at various timescales, we propose an interactive segmentation framework, called interactive MEdical image segmentation with self-adaptive Confidence CAlibration (MECCA), which combines action-based confidence learning and multi-agent reinforcement learning. A novel confidence network is learned by predicting the alignment level of the action with short-term interaction information. A confidence-based reward-shaping mechanism is then proposed to explicitly incorporate confidence in the policy gradient calculation, thus directly correcting the model’s interactive misunderstanding. MECCA also enables user-friendly interactions by reducing the interaction intensity and difficulty via label generation and interaction guidance, respectively. Numerical experiments on different segmentation tasks show that MECCA can significantly improve short- and long-term interaction information utilization efficiency with remarkably fewer labeled samples. The demo video is available at https://bit.ly/mecca-demo-video .",
        "DOI": "10.1631/FITEE.2200299",
        "affiliation_name": "East China Normal University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Donor activity is associated with US legislators’ attention to political issues",
        "paper_author": "Goel P.",
        "publication": "PLoS ONE",
        "citied_by": "0",
        "cover_date": "2023-09-01",
        "Abstract": "Campaign contributions are a staple of congressional life. Yet, the search for tangible effects of congressional donations often focuses on the association between contributions and votes on congressional bills. We present an alternative approach by considering the relationship between money and legislators’ speech. Floor speeches are an important component of congressional behavior, and reflect a legislator’s policy priorities and positions in a way that voting cannot. Our research provides the first comprehensive analysis of the association between a legislator’s campaign donors and the policy issues they prioritize with congressional speech. Ultimately, we find a robust relationship between donors and speech, indicating a more pervasive role of money in politics than previously assumed. We use a machine learning framework on a new dataset that brings together legislator metadata for all representatives in the US House between 1995 and 2018, including committee assignments, legislative speech, donation records, and information about Political Action Committees. We compare information about donations against other potential explanatory variables, such as party affiliation, home state, and committee assignments, and find that donors consistently have the strongest association with legislators’ issue-attention. We further contribute a procedure for identifying speech and donation events that occur in close proximity to one another and share meaningful connections, identifying the proverbial needles in the haystack of speech and donation activity in Congress which may be cases of interest for investigative journalism. Taken together, our framework, data, and findings can help increase the transparency of the role of money in politics.",
        "DOI": "10.1371/journal.pone.0291169",
        "affiliation_name": "Department of Computer Science",
        "affiliation_city": "College Park",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Africa's readiness for artificial intelligence in clinical radiotherapy delivery: Medical physicists to lead the way",
        "paper_author": "Manson E.N.",
        "publication": "Physica Medica",
        "citied_by": "6",
        "cover_date": "2023-09-01",
        "Abstract": "Background: There have been several proposals by researchers for the introduction of Artificial Intelligence (AI) technology due to its promising role in radiotherapy practice. However, prior to the introduction of the technology, there are certain general recommendations that must be achieved. Also, the current challenges of AI must be addressed. In this review, we assess how Africa is prepared for the integration of AI technology into radiotherapy service delivery. Methods: To assess the readiness of Africa for integration of AI in radiotherapy services delivery, a narrative review of the available literature from PubMed, Science Direct, Google Scholar, and Scopus was conducted in the English language using search terms such as Artificial Intelligence, Radiotherapy in Africa, Machine Learning, Deep Learning, and Quality Assurance. Results: We identified a number of issues that could limit the successful integration of AI technology into radiotherapy practice. The major issues include insufficient data for training and validation of AI models, lack of educational curriculum for AI radiotherapy-related courses, no/limited AI teaching professionals, funding, and lack of AI technology and resources. Solutions identified to facilitate smooth implementation of the technology into radiotherapy practices within the region include: creating an accessible national data bank, integrating AI radiotherapy training programs into Africa's educational curriculum, investing in AI technology and resources such as electronic health records and cloud storage, and creation of legal laws and policies to support the use of the technology. These identified solutions need to be implemented on the background of creating awareness among health workers within the radiotherapy space. Conclusion: The challenges identified in this review are common among all the geographical regions in the African continent. Therefore, all institutions offering radiotherapy education and training programs, management of the medical centers for radiotherapy and oncology, national and regional professional bodies for medical physics, ministries of health, governments, and relevant stakeholders must take keen interest and work together to achieve this goal.",
        "DOI": "10.1016/j.ejmp.2023.102653",
        "affiliation_name": "University for Development Studies Ghana",
        "affiliation_city": "Tamale",
        "affiliation_country": "Ghana"
    },
    {
        "paper_title": "Exploring patient medication adherence and data mining methods in clinical big data: A contemporary review",
        "paper_author": "Xu Y.",
        "publication": "Journal of Evidence-Based Medicine",
        "citied_by": "20",
        "cover_date": "2023-09-01",
        "Abstract": "Background: Increasingly, patient medication adherence data are being consolidated from claims databases and electronic health records (EHRs). Such databases offer an indirect avenue to gauge medication adherence in our data-rich healthcare milieu. The surge in data accessibility, coupled with the pressing need for its conversion to actionable insights, has spotlighted data mining, with machine learning (ML) emerging as a pivotal technique. Nonadherence poses heightened health risks and escalates medical costs. This paper elucidates the synergistic interaction between medical database mining for medication adherence and the role of ML in fostering knowledge discovery. Methods: We conducted a comprehensive review of EHR applications in the realm of medication adherence, leveraging ML techniques. We expounded on the evolution and structure of medical databases pertinent to medication adherence and harnessed both supervised and unsupervised ML paradigms to delve into adherence and its ramifications. Results: Our study underscores the applications of medical databases and ML, encompassing both supervised and unsupervised learning, for medication adherence in clinical big data. Databases like SEER and NHANES, often underutilized due to their intricacies, have gained prominence. Employing ML to excavate patient medication logs from these databases facilitates adherence analysis. Such findings are pivotal for clinical decision-making, risk stratification, and scholarly pursuits, aiming to elevate healthcare quality. Conclusion: Advanced data mining in the era of big data has revolutionized medication adherence research, thereby enhancing patient care. Emphasizing bespoke interventions and research could herald transformative shifts in therapeutic modalities.",
        "DOI": "10.1111/jebm.12548",
        "affiliation_name": "First Affiliated Hospital of Jinan University",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Learning Multiple-Gait Quadrupedal Locomotion via Hierarchical Reinforcement Learning",
        "paper_author": "Wei L.",
        "publication": "International Journal of Precision Engineering and Manufacturing",
        "citied_by": "2",
        "cover_date": "2023-09-01",
        "Abstract": "Over long periods of evolution, legged animals have developed the capability to use a variety of gaits to move efficiently and flexibly at different speeds. To enable quadruped robots to acquire this ability, this study proposes a two-stage training hierarchical framework that can have quadruped robots generate energy-efficient multiple-gait locomotion, consisting of a gait selection policy module and a react controller module. The parameters of both modules are optimized using reinforcement learning. The experimental results in the simulation demonstrate that the proposed method can generate energy-efficient multiple-gait quadrupedal locomotion compared to previous methods. To validate the robustness and effectiveness of the method, we constructed a closed-chain quadruped robot and deployed the controller trained by the method to the robot. The experimental results in the real world suggest that the controller can enable the robot to move stably and efficiently in different gaits. The main contribution of this paper is that the authors propose a novel hierarchical framework, which makes quadruped robots use an optimal gait at a specific speed and smoothly switch to another one after getting a different speed command. These behaviors are automatically produced through simulation training, eliminating the need for the tedious work of designing gaits and modulating controllers. Experimental results showcase that the proposed method has significant advantages compared to previous methods.",
        "DOI": "10.1007/s12541-023-00885-6",
        "affiliation_name": "Wuhan University",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Automated gadget discovery in the quantum domain",
        "paper_author": "Trenkwalder L.M.",
        "publication": "Machine Learning: Science and Technology",
        "citied_by": "0",
        "cover_date": "2023-09-01",
        "Abstract": "In recent years, reinforcement learning (RL) has become increasingly successful in its application to the quantum domain and the process of scientific discovery in general. However, while RL algorithms learn to solve increasingly complex problems, interpreting the solutions they provide becomes ever more challenging. In this work, we gain insights into an RL agent’s learned behavior through a post-hoc analysis based on sequence mining and clustering. Specifically, frequent and compact subroutines, used by the agent to solve a given task, are distilled as gadgets and then grouped by various metrics. This process of gadget discovery develops in three stages: First, we use an RL agent to generate data, then, we employ a mining algorithm to extract gadgets and finally, the obtained gadgets are grouped by a density-based clustering algorithm. We demonstrate our method by applying it to two quantum-inspired RL environments. First, we consider simulated quantum optics experiments for the design of high-dimensional multipartite entangled states where the algorithm finds gadgets that correspond to modern interferometer setups. Second, we consider a circuit-based quantum computing environment where the algorithm discovers various gadgets for quantum information processing, such as quantum teleportation. This approach for analyzing the policy of a learned agent is agent and environment agnostic and can yield interesting insights into any agent’s policy.",
        "DOI": "10.1088/2632-2153/acf098",
        "affiliation_name": "Universität Konstanz",
        "affiliation_city": "Konstanz",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Apriori Algorithm Based Approach for Improving QoS and SLA Guarantee in IaaS Clouds Using Pattern-Based Service-Oriented Architecture",
        "paper_author": "Godhrawala H.",
        "publication": "SN Computer Science",
        "citied_by": "3",
        "cover_date": "2023-09-01",
        "Abstract": "Cloud computing is a modern computing technology. The integration of fog computing, edge computing, mist computing, IoT, 5G, etc. with cloud computing adds immense power to cloud computing. However, this integration leads to two problems. The first problem is to define an interoperable cloud architecture and the second problem is resource allocation to optimize performance. To address both problems, we propose a SOA for cloud. We implement a machine learning based Apriori algorithm, to find association rules between QoS to define a stronger SLA. This approach also helps in defining better and simpler resource management policies and for developer it becomes easier to develop and deploy cloud applications. When cloud manages resources in a well-defined manner, revenue is optimized easily with maintaining required QoS levels. Experiments and detailed discussion show that this approach successfully helps in defining better SLA and implement a simpler resource management policy, makes easier to handle QoS, reduces costs and optimizes revenues.",
        "DOI": "10.1007/s42979-023-02079-3",
        "affiliation_name": "Marwadi University",
        "affiliation_city": "Rajkot",
        "affiliation_country": "India"
    },
    {
        "paper_title": "The framing of initial COVID-19 communication: Using unsupervised machine learning on press releases",
        "paper_author": "Tomasi S.",
        "publication": "Business and Society Review",
        "citied_by": "1",
        "cover_date": "2023-09-01",
        "Abstract": "The COVID-19 pandemic was a global health crisis that required US residents to understand the phenomenon, interpret the cues, and make sense within their environment. Therefore, how the communication of COVID-19 was framed to stakeholders during the early stages of the pandemic became important to guide them through specific actions in their state and subsequently with the sensemaking process. The present study examines which frames were emphasized in the states' press releases on policies and other COVID information to influence stakeholders on what to focus on to help with sensemaking during the crisis. We conducted content analysis on 602 press releases from 50 US states using an unsupervised machine learning approach called Latent Dirichlet Allocation (LDA). The results show that health communication using press releases to help the public make sense of the crisis were framed to include health frames as well as economic frames. Health communication messages are typically framed with health and safety measures; however, this study shows that economic frames were emphasized more than public health frames in the government's health communication for COVID-19, which forced both large and small businesses to engage in specific socially responsible activities that were previously voluntary to support public health safety.",
        "DOI": "10.1111/basr.12323",
        "affiliation_name": "George Mason University",
        "affiliation_city": "Fairfax",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Application of deep learning and machine learning models to improve healthcare in sub-Saharan Africa: Emerging opportunities, trends and implications",
        "paper_author": "Mbunge E.",
        "publication": "Telematics and Informatics Reports",
        "citied_by": "17",
        "cover_date": "2023-09-01",
        "Abstract": "Deep learning and machine learning techniques present unmatched opportunities to improve healthcare in sub-Saharan Africa (SSA). However, there is a paucity of literature on AI-based applications deployed to improve care in SSA, which makes it challenging to organise the research contributions in the present and to highlight obstacles and emerging research areas that need to be explored in the future. This study applied the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analysis) model to conduct a comprehensive review of deep learning and machine learning models deployed in SSA to improve access to care while exploring emerging opportunities, trends and implications for integrating AI-based models in SSA healthcare. This study reveals that AI models can analyse and derive inferences from massive health data for early detection, diagnosis, monitoring for chronic disorders, prediction of diseases, monitoring large-scale public health patterns and help limit exposure in contagious environments. AI can facilitate the development of targeted health interventions and improve patient outcomes in all stages of diagnosis, treatment, drug development and monitoring, personalised medicine, patient control and care. Integrating AI models with health applications can tremendously assist health professionals and policymakers in disease diagnosis and making informed decisions. AI algorithms bias, poor access to health data and formats, and lack of policies and frameworks supporting the integration of data-driven AI-based solutions into health systems hinder the integration of AI-based models into health systems. There is a need for transparency and ethical use of AI and crafting policies that support the use of AI in SSA health systems. Utilising AI-based models in healthcare can also assist researchers and healthcare workers to move towards smart care and better comprehend future research needs of AI in smart care.",
        "DOI": "10.1016/j.teler.2023.100097",
        "affiliation_name": "University of Eswatini",
        "affiliation_city": "Mbabane",
        "affiliation_country": "Swaziland"
    },
    {
        "paper_title": "A Comparative Study on Deep Learning Models for COVID-19 Forecast",
        "paper_author": "Guo Z.",
        "publication": "Healthcare (Switzerland)",
        "citied_by": "1",
        "cover_date": "2023-09-01",
        "Abstract": "The COVID-19 pandemic has led to a global health crisis with significant morbidity, mortality, and socioeconomic disruptions. Understanding and predicting the dynamics of COVID-19 are crucial for public health interventions, resource allocation, and policy decisions. By developing accurate models, informed public health strategies can be devised, resource allocation can be optimized, and virus transmission can be reduced. Various mathematical and computational models have been developed to estimate transmission dynamics and forecast the pandemic’s trajectories. However, the evolving nature of COVID-19 demands innovative approaches to enhance prediction accuracy. The machine learning technique, particularly the deep neural networks (DNNs), offers promising solutions by leveraging diverse data sources to improve prevalence predictions. In this study, three typical DNNs, including the Long Short-Term Memory (LSTM) network, Physics-informed Neural Network (PINN), and Deep Operator Network (DeepONet), are employed to model and forecast COVID-19 spread. The training and testing data used in this work are the global COVID-19 cases in the year of 2021 from the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University. A seven-day moving average as well as the normalization techniques are employed to stabilize the training of deep learning models. We systematically investigate the effect of the number of training data on the predicted accuracy as well as the capability of long-term forecast in each model. Based on the relative (Formula presented.) errors between the predictions from deep learning models and the reference solutions, the DeepONet, which is capable of learning hidden physics given the training data, outperforms the other two approaches in all test cases, making it a reliable tool for accurate forecasting the dynamics of COVID-19.",
        "DOI": "10.3390/healthcare11172400",
        "affiliation_name": "Central South University Xiangya School of Medicine",
        "affiliation_city": "Changsha",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Data-Driven Integrated Decision Model for Analysing Energetic Behaviour of Innovative Construction Materials Capable of Hybrid Energy Storage",
        "paper_author": "Politi C.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "3",
        "cover_date": "2023-09-01",
        "Abstract": "Aligning the European Union goals for climate neutrality by 2050 and the ambition for carbon equivalent emissions reduction to almost half by 2030 demands the exploration of alternative decarbonisation pathways. Energy consumption across all sectors is identified as a crucial contributor to this challenge, with a number of legislative and regulatory frameworks and commitments to be introduced every year. In response to these trends, the concept of exploiting a building’s thermal mass through the integration of phase change materials (PCMs) enhances the ability of building elements to reserve and deliver large amounts of energy during phase transitions. However, the incorporation of PCMs into building elements requires the thorough understanding of their thermal behaviour. This study evaluates and predicts the thermophysical properties of mineral particles carrying PCMs and coated with a cementitious layer able to be utilised as fillers in construction applications. By employing deep learning and predictive modelling techniques, the numerical data-driven model developed in this paper enhances accuracy and efficiency in property estimation and prediction, facilitating material selection, system design, and optimisation. A model in a MATLAB simulation environment is presented, evaluating and predicting the thermophysical properties of semi-organic particles able to enhance building envelope thermal mass as a hybrid energy storage solution. These findings show the time needed for a building block to undergo cooling, demonstrating a clear upgrade in the thermal discharge of the walls. Substituting traditional EP with PCM-enhanced EP leads to a minimum reduction of 1 °C per hour in the discharge rate, thereby extending the comfort duration of indoor spaces without necessitating additional space heating. These models offer the potential for assessing diverse material compositions and usage scenarios, offering valuable insights to aid decisions in optimizing building energy efficiency.",
        "DOI": "10.3390/su151712863",
        "affiliation_name": "National Technical University of Athens (NTUA)",
        "affiliation_city": "Athens",
        "affiliation_country": "Greece"
    },
    {
        "paper_title": "Monitoring Agricultural Land and Land Cover Change from 2001–2021 of the Chi River Basin, Thailand Using Multi-Temporal Landsat Data Based on Google Earth Engine",
        "paper_author": "Suwanlee S.R.",
        "publication": "Remote Sensing",
        "citied_by": "7",
        "cover_date": "2023-09-01",
        "Abstract": "In recent years, climate change has greatly affected agricultural activity, sustainability and production, making it difficult to conduct crop management and food security assessment. As a consequence, significant changes in agricultural land and land cover (LC) have occurred, mostly due to the introduction of new agricultural practices, techniques and crops. Earth Observation (EO) data, cloud-computing platforms and powerful machine learning methods can certainly support analysis within the agricultural context. Therefore, accurate and updated agricultural land and LC maps can be useful to derive valuable information for land change monitoring, trend planning, decision-making and sustainable land management. In this context, this study aims at monitoring temporal and spatial changes between 2001 and 2021 (with a four 5-year periods) within the Chi River Basin (NE–Thailand). Specifically, all available Landsat archives and the random forest (RF) classifier were jointly involved within the Google Earth Engine (GEE) platform in order to: (i) generate five different crop type maps (focusing on rice, cassava, para rubber and sugarcane classes), and (ii) monitoring the agricultural land transitions over time. For each crop map, a confusion matrix and the correspondent accuracy were computed and tested according to a validation dataset. In particular, an overall accuracy > 88% was found in all of the resulting five crop maps (for the years 2001, 2006, 2011, 2016 and 2021). Subsequently the agricultural land transitions were analyzed, and a total of 18,957 km2 were found as changed (54.5% of the area) within the 20 years (2001–2021). In particular, an increase in cassava and para rubber areas were found at the disadvantage of rice fields, probably due to two different key drivers taken over time: the agricultural policy and staple price. Finally, it is worth highlighting that such results turn out to be decisive in a challenging agricultural environment such as the Thai one. In particular, the high accuracy of the five derived crop type maps can be useful to provide spatial consistency and reliable information to support local sustainable agriculture land management, decisions of policymakers and many stakeholders.",
        "DOI": "10.3390/rs15174339",
        "affiliation_name": "Geo-Informatics and Space Technology Development Agency",
        "affiliation_city": "Bangkok",
        "affiliation_country": "Thailand"
    },
    {
        "paper_title": "Improving the Accuracy of Random Forest Classifier for Identifying Burned Areas in the Tangier-Tetouan-Al Hoceima Region Using Google Earth Engine",
        "paper_author": "Badda H.",
        "publication": "Remote Sensing",
        "citied_by": "6",
        "cover_date": "2023-09-01",
        "Abstract": "Forest fires have become a major concern in the northern parts of Morocco, particularly in the Tangier-Tetouan-Al Hoceima (TTA) region, causing significant damage to the environment and human lives. To address this pressing issue, this study proposes an approach that utilizes remote sensing (RS) and machine learning (ML) techniques to detect burned areas in the TTA region within the Google Earth Engine platform. The study focuses on burned areas resulting from forest fires in three specific locations in the TTA region that have experienced such fires in recent years, namely Tangier-Assilah in 2017, M’diq Fnideq in 2020, and Chefchaouen in 2021. In our study, we extensively explored multiple combinations of spectral indices, such as normalized burn ratio (dNBR), normalized difference vegetation index (dNDVI), soil-adjusted vegetation index (dSAVI), and burned area index (dBAI), in conjunction with Sentinel-2 (S2) satellite images. These combinations were employed within the Random Forest (RF) algorithm, allowing us to draw important conclusions. Initially, we assess the individual effectiveness of the dNBR index, which yields accuracy rates of 83%, 90%, and 82% for Tangier-Assilah, Chefchaouen, and M’diq Fnideq, respectively. Recognizing the need for improved outcomes, we expand our analysis by incorporating spectral indices and S2 bands. However, the results obtained from this expanded combination lack consistency and stability across different locations. While Tangier-Assilah and M’diq Fnideq experience accuracy improvements, reaching 95% and 88%, respectively, the inclusion of Sentinel bands has an adverse effect on Chefchaouen, resulting in a decreased accuracy of 87%. To achieve optimal accuracy, our focus shifted towards the combination of dNBR and the other spectral indices. The results were truly remarkable, with accuracy rates of 96%, 97%, and 97% achieved for Tangier-Assilah, Chefchaouen, and M’diq Fnideq, respectively. Our decision to prioritize the spectral indices was based on the feature importance method, which highlights the significance of each feature in the classification process. The practical implications of our study extend to fire management and prevention in the TTA region. The insights gained from our analysis can inform the development of effective policies and strategies to mitigate the impact of forest fires. By harnessing the potential of RS and ML techniques, along with the utilization of spectral indices, we pave the way for enhanced fire monitoring and response capabilities in the region.",
        "DOI": "10.3390/rs15174226",
        "affiliation_name": "Instituto de Sistemas e Robótica",
        "affiliation_city": "Lisbon",
        "affiliation_country": "Portugal"
    },
    {
        "paper_title": "The Spatiotemporal Distribution of NO<inf>2</inf> in China Based on Refined 2DCNN-LSTM Model Retrieval and Factor Interpretability Analysis",
        "paper_author": "Chen R.",
        "publication": "Remote Sensing",
        "citied_by": "4",
        "cover_date": "2023-09-01",
        "Abstract": "With the advancement of urbanization in China, effective control of pollutant emissions and air quality have become important goals in current environmental management. Nitrogen dioxide (NO2), as a precursor of tropospheric ozone and fine particulate matter, plays a significant role in atmospheric chemistry research and air pollution control. However, the uneven ground monitoring stations and low temporal resolution of polar-orbiting satellites set challenges for accurately assessing near-surface NO2 concentrations. To address this issue, a spatiotemporal refined NO2 retrieval model was established for China using the geostationary satellite Himawari-8. The spatiotemporal characteristics of NO2 were analyzed and its contribution factors were explored. Firstly, seven Himawari-8 channels sensitive to NO2 were selected by using the forward feature selection based on information entropy. Subsequently, a 2DCNN-LSTM network model was constructed, incorporating the selected channels and meteorological variables as retrieval factors to estimate hourly NO2 in China from March 2018 to February 2020 (with a resolution of 0.05°, per hour). The performance evaluation demonstrates that the full-channel 2DCNN-LSTM model has good fitting capability and robustness (R2 = 0.74, RMSE = 10.93), and further improvements were achieved after channel selection (R2 = 0.87, RMSE = 6.84). The 10-fold cross-validation results indicate that the R2 between retrieval and measured values was above 0.85, the MAE was within 5.60, and the RMSE iwas within 7.90. R2 varied between 0.85 and 0.90, showing better validation at mid-day (R2 = 0.89) and in spring and fall transition seasons (R2 = 0.88 and R2 = 0.90). To investigate the cooperative effect of meteorological factors and other air pollutants on NO2, statistical methods (beta coefficients) were used to test the factor interpretability. Meteorological factors as well as other pollutants were analyzed. From a statistical perspective, PM2.5, boundary layer height, and O3 were found to have the largest impacts on near-surface NO2 concentrations, with each standard deviation change in these factors leading to 0.28, 0.24, and 0.23 in standard deviations of near-surface NO2, respectively. The findings of this study contribute to a comprehensive understanding of the spatiotemporal distribution of NO2 and provide a scientific basis for formulating targeted air pollution policies.",
        "DOI": "10.3390/rs15174261",
        "affiliation_name": "Lanzhou University",
        "affiliation_city": "Lanzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Task Offloading Decision-Making Algorithm for Vehicular Edge Computing: A Deep-Reinforcement-Learning-Based Approach",
        "paper_author": "Shi W.",
        "publication": "Sensors",
        "citied_by": "6",
        "cover_date": "2023-09-01",
        "Abstract": "Efficient task offloading decision is a crucial technology in vehicular edge computing, which aims to fulfill the computational performance demands of complex vehicular tasks with respect to delay and energy consumption while minimizing network resource competition and consumption. Conventional distributed task offloading decisions rely solely on the local state of the vehicle, failing to optimize the utilization of the server’s resources to its fullest potential. In addition, the mobility aspect of vehicles is often neglected in these decisions. In this paper, a cloud-edge-vehicle three-tier vehicular edge computing (VEC) system is proposed, where vehicles partially offload their computing tasks to edge or cloud servers while keeping the remaining tasks local to the vehicle terminals. Under the restrictions of vehicle mobility and discrete variables, task scheduling and task offloading proportion are jointly optimized with the objective of minimizing the total system cost. Considering the non-convexity, high-dimensional complex state and continuous action space requirements of the optimization problem, we propose a task offloading decision-making algorithm based on deep deterministic policy gradient (TODM_DDPG). TODM_DDPG algorithm adopts the actor–critic framework in which the actor network outputs floating point numbers to represent deterministic policy, while the critic network evaluates the action output by the actor network, and adjusts the network evaluation policy according to the rewards with the environment to maximize the long-term reward. To explore the algorithm performance, this conduct parameter setting experiments to correct the algorithm core hyper-parameters and select the optimal combination of parameters. In addition, in order to verify algorithm performance, we also carry out a series of comparative experiments with baseline algorithms. The results demonstrate that in terms of reducing system costs, the proposed algorithm outperforms the compared baseline algorithm, such as the deep Q network (DQN) and the actor–critic (AC), and the performance is improved by about 13% on average.",
        "DOI": "10.3390/s23177595",
        "affiliation_name": "Key Laboratory of Computer Network and Information Integration, Ministry of Education",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Rapid and Green Classification Method of Bacteria Using Machine Learning and NIR Spectroscopy",
        "paper_author": "Farias L.R.",
        "publication": "Sensors",
        "citied_by": "2",
        "cover_date": "2023-09-01",
        "Abstract": "Green Chemistry is a vital and crucial instrument in achieving pollution control, and it plays an important role in helping society reach the Sustainable Development Goals (SDGs). NIR (near-infrared spectroscopy) has been utilized as an alternate technique for molecular identification, making the process faster and less expensive. Near-infrared diffuse reflectance spectroscopy and Machine Learning (ML) algorithms were utilized in this study to construct identification and classification models of bacteria such as Escherichia coli, Salmonella enteritidis, Enterococcus faecalis and Listeria monocytogenes. Furthermore, divide these bacteria into Gram-negative and Gram-positive groups. The green and quick approach was created by combining NIR spectroscopy with a diffuse reflectance accessory. Using infrared spectral data and ML techniques such as principal component analysis (PCA), hierarchical cluster analysis (HCA) and K-Nearest Neighbor (KNN), It was feasible to accomplish the identification and classification of four bacteria and classify these bacteria into two groups: Gram-positive and Gram-negative, with 100% accuracy. We may conclude that our study has a high potential for bacterial identification and classification, as well as being consistent with global policies of sustainable development and green analytical chemistry.",
        "DOI": "10.3390/s23177336",
        "affiliation_name": "Instituto Federal de Educação, Ciência e Tecnologia de Roraima – IFRR",
        "affiliation_city": "Boa Vista",
        "affiliation_country": "Brazil"
    },
    {
        "paper_title": "A systematic review of clinical health conditions predicted by machine learning diagnostic and prognostic models trained or validated using real-world primary health care data",
        "paper_author": "Abdulazeem H.",
        "publication": "PLoS ONE",
        "citied_by": "9",
        "cover_date": "2023-09-01",
        "Abstract": "With the advances in technology and data science, machine learning (ML) is being rapidly adopted by the health care sector. However, there is a lack of literature addressing the health conditions targeted by the ML prediction models within primary health care (PHC) to date. To fill this gap in knowledge, we conducted a systematic review following the PRISMA guidelines to identify health conditions targeted by ML in PHC. We searched the Cochrane Library, Web of Science, PubMed, Elsevier, BioRxiv, Association of Computing Machinery (ACM), and IEEE Xplore databases for studies published from January 1990 to January 2022. We included primary studies addressing ML diagnostic or prognostic predictive models that were supplied completely or partially by real-world PHC data. Studies selection, data extraction, and risk of bias assessment using the prediction model study risk of bias assessment tool were performed by two investigators. Health conditions were categorized according to international classification of diseases (ICD-10). Extracted data were analyzed quantitatively. We identified 106 studies investigating 42 health conditions. These studies included 207 ML prediction models supplied by the PHC data of 24.2 million participants from 19 countries. We found that 92.4% of the studies were retrospective and 77.3% of the studies reported diagnostic predictive ML models. A majority (76.4%) of all the studies were for models’ development without conducting external validation. Risk of bias assessment revealed that 90.8% of the studies were of high or unclear risk of bias. The most frequently reported health conditions were diabetes mellitus (19.8%) and Alzheimer’s disease (11.3%). Our study provides a summary on the presently available ML prediction models within PHC. We draw the attention of digital health policy makers, ML models developer, and health care professionals for more future interdisciplinary research collaboration in this regard.",
        "DOI": "10.1371/journal.pone.0274276",
        "affiliation_name": "McGill Faculty of Medicine and Health Sciences",
        "affiliation_city": "Montreal",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Applying a Social Determinants of Health Framework to Guide Digital Innovations That Reduce Disparities in Chronic Disease",
        "paper_author": "Goldstein S.P.",
        "publication": "Psychosomatic Medicine",
        "citied_by": "3",
        "cover_date": "2023-09-01",
        "Abstract": "Chronic diseases are among the top causes of global death, disability, and health care expenditure. Digital health interventions (e.g., patient support delivered via technologies such as smartphones, wearables, videoconferencing, social media, and virtual reality) may prevent and mitigate chronic disease by facilitating accessible, personalized care. Although these tools have promise to reach historically marginalized groups, who are disproportionately affected by chronic disease, evidence suggests that digital health interventions could unintentionally exacerbate health inequities. This commentary outlines opportunities to harness recent advancements in technology and research design to drive equitable digital health intervention development and implementation. We apply \"calls to action\"from the World Health Organization Commission on Social Determinants of Health conceptual framework to the development of new, and refinement of existing, digital health interventions that aim to prevent or treat chronic disease by targeting intermediary, social, and/or structural determinants of health. Three mirrored \"calls to action\"are thus proposed for digital health research: a) develop, implement, and evaluate multilevel, context-specific digital health interventions; b) engage in intersectoral partnerships to advance digital health equity and social equity more broadly; and c) include and empower historically marginalized groups to develop, implement, and access digital health interventions. Using these \"action items,\"we review several technological and methodological innovations for designing, evaluating, and implementing digital health interventions that have greater potential to reduce health inequities. We also enumerate possible challenges to conducting this work, including leading interdisciplinary collaborations, diversifying the scientific workforce, building trustworthy community relationships, and evolving health care and digital infrastructures.",
        "DOI": "10.1097/PSY.0000000000001176",
        "affiliation_name": "Miriam Hospital",
        "affiliation_city": "Providence",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Estimating Energy Consumption of Battery Electric Vehicles Using Vehicle Sensor Data and Machine Learning Approaches",
        "paper_author": "Achariyaviriya W.",
        "publication": "Energies",
        "citied_by": "17",
        "cover_date": "2023-09-01",
        "Abstract": "Transport electrification, which entails replacing fossil fuel-powered engines with electric drivetrains through the use of electric vehicles (EVs), has been identified as a potential strategy for reducing emissions in the transportation sector. As the adoption of EVs increases, there is a growing need to understand their performance and characteristics, particularly the factors that influence energy consumption under actual driving conditions. This study sought to investigate the actual energy consumption of commercial battery electric vehicles (BEVs) in Thailand by conducting real-world driving tests under various route conditions, including urban and rural route modes. Data collection was performed through the use of onboard diagnostics and global positioning system devices. The result shows that the average energy consumption of the BEVs in this study was 148.03 Wh/km. Moreover, several machine learning (ML) techniques were utilized to analyze the collected dataset to predict energy consumption and identify the key factors influencing energy consumption. A comprehensive investigation of factor significance was carried out by employing a specific algorithm in conjunction with the SHapley Additive exPlanations (SHAP) approach. This investigation provided insights into the influence of battery current and vehicle speed on the energy consumption of BEVs, particularly in the context of urban route conditions. The results of this study provide valuable insights into the energy consumption of BEVs and the factors affecting it, which can aid in improving energy efficiency and informing policy decisions related to transport electrification.",
        "DOI": "10.3390/en16176351",
        "affiliation_name": "Chiang Mai University",
        "affiliation_city": "Chiang Mai",
        "affiliation_country": "Thailand"
    },
    {
        "paper_title": "Integrating climate change adaptation policies in spatial development planning in hyperarid regions of Kerman province, Iran",
        "paper_author": "Karami H.",
        "publication": "Heliyon",
        "citied_by": "2",
        "cover_date": "2023-09-01",
        "Abstract": "In recent years, lifestyle changes and urbanization of societies, as well as macro-environmental changes, i.e. climate changes (CCs), have caused changes in the land spatial structure and the transfer of resources between different economic sectors of the land. The development of long-term spatial development plans (SDPs) needs to be compatible with CCs, especially in hyperarid areas with low supplies and high demands. In this research, machine learning methods; including Cellular Automata (CA), Random Forest (RF) and regression models through PLUS model were used to simulate the amount of supplies and demands based on land cover (LC) maps during the years 2000, 2010 and 2020 in the hyperarid areas of Kerman, Iran. Then, the best predicted model (Kappa = 0.94, overall accuracy = 0.98) was used to simulate changes in LC classes under climate change scenarios (CCSs) for 2050. The results showed the efficiency of machine learning in simulating land cover changes (LCCs) under CCSs. Findings revealed that SDPs of these areas are not compatible under any possible consideration of CCSs. The modeling results showed that spatial development plans under CCSs is not environmentally efficient and there is no compatibility between supplies, based on agricultural lands, and demands, based on increased population, by 2050. Overall, under the scenario of RCP 8.5, man-made, agriculture and natural LC classes with 106.9, 2.9, and 18.6% changes, respectively, showed the greatest changes compared to 2020. Population control, adjustment of infrastructures, and changes in LC plans can reduce socio-economical and socio-environmental problems in the future of hyperarid areas to some extent.",
        "DOI": "10.1016/j.heliyon.2023.e19785",
        "affiliation_name": "Shahid Beheshti University",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "Closed-Loop Medication Management with an Electronic Health Record System in U.S. and Finnish Hospitals",
        "paper_author": "Shermock S.B.",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "7",
        "cover_date": "2023-09-01",
        "Abstract": "Many medication errors in the hospital setting are due to manual, error-prone processes in the medication management system. Closed-loop Electronic Medication Management Systems (EMMSs) use technology to prevent medication errors by replacing manual steps with automated, electronic ones. As Finnish Helsinki University Hospital (HUS) establishes its first closed-loop EMMS with the new Epic-based Electronic Health Record system (APOTTI), it is helpful to consider the history of a more mature system: that of the United States. The U.S. approach evolved over time under unique policy, economic, and legal circumstances. Closed-loop EMMSs have arrived in many U.S. hospital locations, with myriad market-by-market manifestations typical of the U.S. healthcare system. This review describes and compares U.S. and Finnish hospitals’ EMMS approaches and their impact on medication workflows and safety. Specifically, commonalities and nuanced differences in closed-loop EMMSs are explored from the perspectives of the care/nursing unit and hospital pharmacy operations perspectives. As the technologies are now fully implemented and destined for evolution in both countries, perhaps closed-loop EMMSs can be a topic of continued collaboration between the two countries. This review can also be used for benchmarking in other countries developing closed-loop EMMSs.",
        "DOI": "10.3390/ijerph20176680",
        "affiliation_name": "Helsinki University Hospital",
        "affiliation_city": "Helsinki",
        "affiliation_country": "Finland"
    },
    {
        "paper_title": "Capturing low-rate DDoS attack based on MQTT protocol in software Defined-IoT environment",
        "paper_author": "Al-Fayoumi M.",
        "publication": "Array",
        "citied_by": "11",
        "cover_date": "2023-09-01",
        "Abstract": "The MQTT (Message Queue Telemetry Transport) protocol has recently been standardized to provide a lightweight open messaging service over low-bandwidth and resource-constrained communication environments. Hence, it is the primary messaging protocol used by Internet of Things (IoT) devices to disseminate telemetry data in a machine-to-machine approach. Despite its advantages in providing reliable, scalable, and timely delivery, the MQTT protocol is widely vulnerable to flooding and denial of service attacks, specifically, the low-rate distributed denial of services (LR-DDoS). Unlike conventional DDoS, the LR-DDoS attack tends to appear as normal traffic at a very slow rate, which makes it difficult to differentiate from legitimate packets, allowing the packets to move undetected by traditional detection policies. This paper presents an intelligent lightweight detection scheme that can capture LR-DDoS attacks based on MQTT protocol in a software-defined IoT environment. The proposed scheme examines the performance of four machine learning models on a modern dataset (LRDDoS-MQTT-2022) with a minimum feature set (i.e., two features only) and a balanced dataset, namely: decision tree classifier (DTC), multilayer perceptron (MLP), artificial neural networks (ANN), and naïve Bayes classifier (NBC). Our exploratory assessment demonstrates the arrogance of the DTC detection scheme achieving an accuracy of 99.5% with peak detection speed. Eventually, our best outcomes outdo existing models with higher prediction rates.",
        "DOI": "10.1016/j.array.2023.100316",
        "affiliation_name": "Princess Sumaya University",
        "affiliation_city": "Amman",
        "affiliation_country": "Jordan"
    },
    {
        "paper_title": "Investigating machine learning for simulating urban transport patterns: A comparison with traditional macro-models",
        "paper_author": "Parishwad O.",
        "publication": "Multimodal Transportation",
        "citied_by": "24",
        "cover_date": "2023-09-01",
        "Abstract": "Predicting passenger flow within a city is crucial for intelligent transportation management systems, especially in the context of urban development, post-pandemic policy changes, and infrastructure improvements. Traditional macro models have limitations in accurately capturing the complex structure of real traffic flows, and recent advancements in machine learning offer promising approaches for improving transportation simulations. This research aims to compare the effectiveness of traditional simulation models with a selective machine learning (ML) model for traffic flow prediction in Oslo, Norway. Sensitivity and scenario analyses are conducted to examine the models’ parameters and derive the city's characteristics. Results substantiate that the traditional Spatial Interaction model (SIM), although interpretable and requiring fewer parameters, has limitations in accurately capturing real flow structures and exhibits greater variability compared to the ML model. Statistical analyses support these findings and raise questions about the validity of the ML model's results over the SIM. The research highlights the potential of ML models to identify trends in passenger flows and simulate traffic flows in different scenarios related to city development. Overall, the research presents a decision support system for planners and policymakers to predict traffic flow accurately and efficiently. It highlights the benefits and drawbacks of both the traditional SIM and ML models, contributing to the ongoing discussion of the role of machine learning in transportation modeling.",
        "DOI": "10.1016/j.multra.2023.100085",
        "affiliation_name": "Chalmers University of Technology",
        "affiliation_city": "Gothenburg",
        "affiliation_country": "Sweden"
    },
    {
        "paper_title": "Towards facing uncertainties in biofuel supply chain networks: a systematic literature review",
        "paper_author": "Habibi F.",
        "publication": "Environmental Science and Pollution Research",
        "citied_by": "5",
        "cover_date": "2023-09-01",
        "Abstract": "Biofuel supply chains (BSCs) face diverse uncertainties that pose serious challenges. This has led to an expanding body of research focused on studying these challenges. Hence, there is a growing need for a comprehensive review that summarizes the current studies, identifies their limitations, and provides essential advancements to support scholars in the field. To overcome these limitations, this research aims to provide insights into managing uncertainties in BSCs. The review utilizes the Systematic Reviews and Meta-Analyses (PRISMA) method, identifying 205 papers for analysis. This study encompasses three key tasks: first, it analyses the general information of the shortlisted papers. Second, it discusses existing methodologies and their limitations in addressing uncertainties. Lastly, it identifies critical research gaps and potential future directions. One notable gap involves the underutilization of machine learning techniques, which show potential for risk identification, resilient planning, demand prediction, and parameter estimations in BSCs but have received limited attention. Another area for investigation is the potential of agent-based simulation, which can contribute to analysing resilient policies, evaluating resilience, predicting parameters, and assessing the impact of emerging technologies on BSC resilience in the twenty-first century. Additionally, the study identifies the omission of various realistic assumptions, such as backward flow, lateral transshipments, and ripple effects in BSC. This study highlights the complexity of managing uncertainties in BSCs and emphasizes the need for further research and attention. It contributes to policymakers’ understanding of uncertain sources and suitable approaches while inspiring researchers to address limitations and generate breakthrough ideas in managing BSC uncertainties.",
        "DOI": "10.1007/s11356-023-29331-w",
        "affiliation_name": "University of New South Wales at Australian Defence Force Academy",
        "affiliation_city": "Canberra",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Deep Q-learning recommender algorithm with update policy for a real steam turbine system",
        "paper_author": "Modirrousta M.H.",
        "publication": "IET Collaborative Intelligent Manufacturing",
        "citied_by": "8",
        "cover_date": "2023-09-01",
        "Abstract": "In modern industrial systems, diagnosing faults in time and using the best methods becomes increasingly crucial. It is possible to fail a system or to waste resources if faults are not detected or are detected late. Machine learning and deep learning (DL) have proposed various methods for data-based fault diagnosis, and the authors are looking for the most reliable and practical ones. A framework based on DL and reinforcement learning (RL) is developed for fault detection. The authors have utilised two algorithms in their work: Q-Learning and Soft Q-Learning. Reinforcement learning frameworks frequently include efficient algorithms for policy updates, including Q-learning. These algorithms optimise the policy based on the predictions and rewards, resulting in more efficient updates and quicker convergence. The authors can increase accuracy, overcome data imbalance, and better predict future defects by updating the RL policy when new data is received. By applying their method, an increase of 3%–4% in all evaluation metrics by updating policy, an improvement in prediction speed, and an increase of 3%–6% in all evaluation metrics compared to a typical backpropagation multi-layer neural network prediction with comparable parameters is observed. In addition, the Soft Q-learning algorithm yields better outcomes compared to Q-learning.",
        "DOI": "10.1049/cim2.12081",
        "affiliation_name": "K. N. Toosi University of Technology",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "Review of technological progress in carbon dioxide capture, storage, and utilization",
        "paper_author": "Davoodi S.",
        "publication": "Gas Science and Engineering",
        "citied_by": "111",
        "cover_date": "2023-09-01",
        "Abstract": "Emissions of substantial amounts of greenhouse gases (GHG) accumulating in the atmosphere have caused climate alterations and increased global temperatures. Several techniques have been developed to mitigate the release of carbon dioxide (CO2) and tackle this concern. Carbon capture, utilization, and storage (CCUS) is now being adopted as a promising approach among various techniques. This review considers CCUS and its role in reducing the effects of CO2 on the climate. It discusses the amount of CCUS necessary to achieve this goal and existing projects and commitments from governments and corporations to build CCUS infrastructure. CO2 capture and storage conditions are examined, considering different technologies, transportation, and storage alternatives available, including CO2 mineralization. Requirements and methods for long-term CO2 sequestration are addressed, including the injection of supercritical CO2 into subsurface geological sites. The economic feasibility of captured CO2 utilization is considered, such as integrating CCUS with power generation infrastructure and utilizing CO2 for biofuel production. Machine learning is useful for assessing CCUS criteria and its expanding role is discussed. Political policies, fiscal hurdles, and incentives related to CCUS are considered in the context of the outlook for CCUS until 2050. Cost, environmental, economic, and safety issues have an important bearing on the ability of CCUS projects to raise sufficient funding and secure government support.",
        "DOI": "10.1016/j.jgsce.2023.205070",
        "affiliation_name": "DWA Energy Ltd.",
        "affiliation_city": "Lincoln",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Digital Health Applications (DiHA): Approaches to develop a reimbursement process for the statutory health insurance in Austria",
        "paper_author": "Goetz G.",
        "publication": "Health Policy and Technology",
        "citied_by": "6",
        "cover_date": "2023-09-01",
        "Abstract": "Purpose: To elaborate a concept for implementing digital health applications (DiHA), including prioritisation criteria (PC) for the Austrian context and an overview of available prioritised DiHAs. Methods: Based on European DiHA-listings and input by Austrian experts, a categorised meta-directory of DiHAs was created. PC were developed to reflect, inter alia, the provisions of the Austrian General Insurance Act, and were applied to the meta-directory to identify DiHAs potentially relevant for the Austrian statutory health insurance. An iterative process with expert involvement was used to tailor an existing reimbursement framework to the Austrian setting. Results: The meta-directory comprised 132 DiHAs. Developed PC focused on plausibility (German language) and legal aspects (treatment/monitoring of chronic conditions), while other criteria (e.g. interoperability standards) were considered optional. After applying the PC, 38 DiHAs were potentially relevant in the Austrian setting. Of these, only seven supported current health record integration. Most of the prioritised DiHAs reported on CE marking (29/38) and data protection (35/38), while reporting on risk class (10/38) and technical algorithms (0/38) was sparse. For DiHA reimbursement, a four-step process is proposed: identification (ideally based on needs assessment); filtering based on PC; review of technical, regulatory and evidentiary requirements; and health technology assessment. Conclusion: The proposed concept can offer guidance for policy makers (e.g., on prioritising available DiHAs) and may further foster scientific debate with regard to DiHA implementation. Further discussion on how to fully incorporate regulatory, technical, and evidentiary criteria is needed. Attention should be given to national implementation requirements, re-assessment criteria, and appropriate remuneration schemes.",
        "DOI": "10.1016/j.hlpt.2023.100780",
        "affiliation_name": "Technische Universität Berlin",
        "affiliation_city": "Berlin",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "The future of the technology-based manufacturing in the European Union",
        "paper_author": "Mallik A.K.",
        "publication": "Results in Engineering",
        "citied_by": "15",
        "cover_date": "2023-09-01",
        "Abstract": "The manufacturing industry tries to innovate always to cater to customer-oriented products. The digitization of manufacturing is revolutionizing the future of this industry. The Industrial Internet of Things (IIoT) is bringing down the labor cost, reducing the machine downtime, and overall increasing the production speed. A new technological trend like Machine Learning (ML) is a subset of Artificial Intelligence (AI) that uses computer algorithms based on available data and can improve or decide further, automatically, based on experience, without the need for prior programming commands. ML can improve daily processes for identifying bottlenecks, developing products, controlling quality, providing security to the industry, and using AI robotics in place of humans. On one hand, technologies like ML, AI, IIoT, new materials, photonics, and rapid prototyping are driving the manufacturing sector in adopting a future version of Industry 4.0, and on the other hand, the European Commission (EC) has defined a roadmap until 2050 or more, in achieving the sustainable goal of carbon-neutrality and complete digitalization with resilience across the European continent. However, it is challenging to match the planned and actual roadmaps to the future of the technology-based manufacturing industry. There are uncertainties about how the future will be shaped by technologies in the EU manufacturing industry, in the changing political, environmental and social world environment. Recognizing these difficulties, the current article consults the available literature on this topic to determine the factors that will characterize the future of the manufacturing industry across EU countries. The relevant information about the EU manufacturing sectors has first been collected from various sources like Eurostat data, the EC policy documents, manufacturing company's annual reports, research reviews, journal articles, EU Industry Days annual event, etc. Then the collected data were analyzed to gain insight into the future of the technology-based EU manufacturing industry in the context of the European Commission's outlined policies. Variable factors from different manufacturing sectors are presented from different EU member states and scenario analysis was used for understanding the possible future. It is concluded that the future does not lie in adapting to the changing environment but in creating the future by EU companies themselves - revolution must be met by revolution. Their early experiences and path dependency can be seen as stubbornness, which may act as formidable barriers to building new capabilities. Therefore, companies must step-wise integrate resources to create a new process, and new structure, with personnel motivation, that fits with the broader European context in the coming decades.",
        "DOI": "10.1016/j.rineng.2023.101356",
        "affiliation_name": "Vrije Universiteit Brussel",
        "affiliation_city": "Brussels",
        "affiliation_country": "Belgium"
    },
    {
        "paper_title": "A narrative review of recent tools and innovations toward automating living systematic reviews and evidence syntheses",
        "paper_author": "Schmidt L.",
        "publication": "Zeitschrift fur Evidenz, Fortbildung und Qualitat im Gesundheitswesen",
        "citied_by": "8",
        "cover_date": "2023-09-01",
        "Abstract": "Living reviews are an increasingly popular research paradigm. The purpose of a ‘living’ approach is to allow rapid collation, appraisal and synthesis of evolving evidence on an important research topic, enabling timely influence on patient care and public health policy. However, living reviews are time- and resource-intensive. The accumulation of new evidence and the possibility of developments within the review's research topic can introduce unique challenges into the living review workflow. To investigate the potential of software tools to support living systematic or rapid reviews, we present a narrative review informed by an examination of tools contained on the Systematic Review Toolbox website. We identified 11 tools with relevant functionalities and discuss the important features of these tools with respect to different steps of the living review workflow. Four tools (NestedKnowledge, SWIFT-ActiveScreener, DistillerSR, EPPI-Reviewer) covered multiple, successive steps of the review process, and the remaining tools addressed specific components of the workflow, including scoping and protocol formulation, reference retrieval, automated data extraction, write-up and dissemination of data. We identify several ways in which living reviews can be made more efficient and practical. Most of these focus on general workflow management, or automation through artificial intelligence and machine-learning, in the screening process. More sophisticated uses of automation mostly target living rapid reviews to increase the speed of production or evidence maps to broaden the scope of the map. We use a case study to highlight some of the barriers and challenges to incorporating tools into the living review workflow and processes. These include increased workload, the need for organisation, ensuring timely dissemination and challenges related to the development of bespoke automation tools to facilitate the review process. We describe how current end-user tools address these challenges, and which knowledge gaps remain that could be addressed by future tool development. Dedicated web presences for automatic dissemination of in-progress evidence updates, rather than solely relying on peer-reviewed journal publications, help to make the effort of a living evidence synthesis worthwhile. Despite offering basic living review functionalities, existing end-user tools could be further developed to be interoperable with other tools to support multiple workflow steps seamlessly, to address broader automatic evidence retrieval from a larger variety of sources, and to improve dissemination of evidence between review updates.",
        "DOI": "10.1016/j.zefq.2023.06.007",
        "affiliation_name": "Sciome LLC",
        "affiliation_city": "Research Triangle Park",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Machine Learning on Medicare Claims Poorly Predicts the Individual Risk of 30-Day Unplanned Readmission After Total Joint Arthroplasty, Yet Uncovers Interesting Population-level Associations With Annual Procedure Volumes",
        "paper_author": "Kunze K.N.",
        "publication": "Clinical Orthopaedics and Related Research",
        "citied_by": "9",
        "cover_date": "2023-09-01",
        "Abstract": "Background Unplanned hospital readmissions after total joint arthroplasty (TJA) represent potentially serious adverse events and remain a critical measure of hospital quality. Predicting the risk of readmission after TJA may provide patients and clinicians with valuable information for preoperative decision-making.Questions/purposes(1) Can nonlinear machine-learning models integrating preoperatively available patient, surgeon, hospital, and county-level information predict 30-day unplanned hospital readmissions in a large cohort of nationwide Medicare beneficiaries undergoing TJA? (2) Which predictors are the most important in predicting 30-day unplanned hospital readmissions? (3) What specific information regarding population-level associations can we obtain from interpreting partial dependency plots (plots describing, given our modeling choice, the potentially nonlinear shape of associations between predictors and readmissions) of the most important predictors of 30-day readmission?MethodsNational Medicare claims data (chosen because this database represents a large proportion of patients undergoing TJA annually) were analyzed for patients undergoing inpatient TJA between October 2016 and September 2018. A total of 679,041 TJAs (239,391 THAs [61.3% women, 91.9% White, 52.6% between 70 and 79 years old] and 439,650 TKAs [63.3% women, 90% White, 55.2% between 70 and 79 years old]) were included. Model features included demographics, county-level social determinants of health, prior-year (365-day) hospital and surgeon TJA procedure volumes, and clinical classification software-refined diagnosis and procedure categories summarizing each patient's Medicare claims 365 days before TJA. Machine-learning models, namely generalized additive models with pairwise interactions (prediction models consisting of both univariate predictions and pairwise interaction terms that allow for nonlinear effects), were trained and evaluated for predictive performance using area under the receiver operating characteristic (AUROC; 1.0 = perfect discrimination, 0.5 = no better than random chance) and precision-recall curves (AUPRC; equivalent to the average positive predictive value, which does not give credit for guessing \"no readmission\"when this is true most of the time, interpretable relative to the base rate of readmissions) on two holdout samples. All admissions (except the last 2 months' worth) were collected and split randomly 80%/20%. The training cohort was formed with the random 80% sample, which was downsampled (so it included all readmissions and a random, equal number of nonreadmissions). The random 20% sample served as the first test cohort (\"random holdout\"). The last 2 months of admissions (originally held aside) served as the second test cohort (\"2-month holdout\"). Finally, feature importances (the degree to which each variable contributed to the predictions) and partial dependency plots were investigated to answer the second and third research questions.ResultsFor the random holdout sample, model performance values in terms of AUROC and AUPRC were 0.65 and 0.087, respectively, for THA and 0.66 and 0.077, respectively, for TKA. For the 2-month holdout sample, these numbers were 0.66 and 0.087 and 0.65 and 0.075. Thus, our nonlinear models incorporating a wide variety of preoperative features from Medicare claims data could not well-predict the individual likelihood of readmissions (that is, the models performed poorly and are not appropriate for clinical use). The most predictive features (in terms of mean absolute scores) and their partial dependency graphs still confer information about population-level associations with increased risk of readmission, namely with older patient age, low prior 365-day surgeon and hospital TJA procedure volumes, being a man, patient history of cardiac diagnoses and lack of oncologic diagnoses, and higher county-level rates of hospitalizations for ambulatory-care sensitive conditions. Further inspection of partial dependency plots revealed nonlinear population-level associations specifically for surgeon and hospital procedure volumes. The readmission risk for THA and TKA decreased as surgeons performed more procedures in the prior 365 days, up to approximately 75 TJAs (odds ratio [OR] = 1.2 for TKA and 1.3 for THA), but no further risk reduction was observed for higher annual surgeon procedure volumes. For THA, the readmission risk decreased as hospitals performed more procedures, up to approximately 600 TJAs (OR = 1.2), but no further risk reduction was observed for higher annual hospital procedure volumes.ConclusionA large dataset of Medicare claims and machine learning were inadequate to provide a clinically useful individual prediction model for 30-day unplanned readmissions after TKA or THA, suggesting that other factors that are not routinely collected in claims databases are needed for predicting readmissions. Nonlinear population-level associations between low surgeon and hospital procedure volumes and increased readmission risk were identified, including specific volume thresholds above which the readmission risk no longer decreases, which may still be indirectly clinically useful in guiding policy as well as patient decision-making when selecting a hospital or surgeon for treatment.Level of EvidenceLevel III, therapeutic study.",
        "DOI": "10.1097/CORR.0000000000002705",
        "affiliation_name": "Hospital for Special Surgery - New York",
        "affiliation_city": "New York",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Understanding the land use intensity of residential buildings in Brazil: An ensemble machine learning approach",
        "paper_author": "Belmiro C.",
        "publication": "Habitat International",
        "citied_by": "5",
        "cover_date": "2023-09-01",
        "Abstract": "The verticalization of cities impacts the quality of urban life. The empirical investigation of the determinants of the floor-area ratio (FAR) of lots using the traditional econometric approaches, however, has little explanatory power, and research about it using machine learning (ML) is almost nonexistent. This study applies two ensemble machine learning strategies, random forest (RF) and extreme gradient boosting (XGBoost), to investigate the determinants of the FAR of all formally registered multifamily residential lots in the city of Recife, Brazil. Taking into account a collection of key determinants influencing the floor area ratio (FAR), which encompass structural, accessibility, environmental, amenity, and policy variables, the findings reveal that the ensemble random forest approach significantly enhances the explanatory ability of these determinants when compared to conventional strategies like ordinary least squares (OLS) or locally weighted regression (LWR). Although generally in line with traditional urban economic arguments, the evidence also reveals important non-linearities in the effects of the variables on the FAR that are useful for urban planning and public housing policy.",
        "DOI": "10.1016/j.habitatint.2023.102896",
        "affiliation_name": "Universidade Federal de Pernambuco",
        "affiliation_city": "Recife",
        "affiliation_country": "Brazil"
    },
    {
        "paper_title": "Effect of Telemedicine Use on Medical Spending and Health Care Utilization: A Machine Learning Approach",
        "paper_author": "Jamal A.",
        "publication": "AJPM Focus",
        "citied_by": "2",
        "cover_date": "2023-09-01",
        "Abstract": "Introduction: This study analyzes the effect of telemedicine use on healthcare utilization and medical spending for patients with chronic mental illness. Methods: Using the IBM MarketScan Research database from 2009 to 2018, this study examined the timing of users’ first telemedicine use and identified similar periods for non-users by using random forest and random forest proximity matching. A difference-in-differences approach, which tests whether there are differences in the study outcomes before and after the actual/predicted first use among the treated group (users) compared with the control group (non-users), was then used to assess the impact of telemedicine. Analyses were done in 2021. Results: Comparing users with non-users after matching suggested that telemedicine use both increases the number of overall outpatient visits (0.461; 95% CI=0.280, 0.642; p<0.001) related to psychotherapy and evaluation and management services, and decreases the number of in-person visits (0.280; 95% CI= −0.446, −0.114; p=0.001) for patients with chronic mental health diagnoses. Total medical spending was not significantly affected. Additionally, no evidence was found of telemedicine use being associated with an increased probability of an emergency department visit or hospitalization. Conclusions: The study findings suggest that telemedicine use is associated with an increase in outpatient care utilization for patients with chronic mental health diagnoses. No substantive changes in medical spending, the probability of an emergency department visit, or the probability of hospitalization were noted. Results provide insights into the effect of telemedicine use on spending and healthcare utilization for patients with chronic mental illness. These findings may inform research to guide future telemedicine policies and interventions.",
        "DOI": "10.1016/j.focus.2023.100127",
        "affiliation_name": "Murray State University",
        "affiliation_city": "Murray",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Implementation frameworks for end-to-end clinical AI: Derivation of the SALIENT framework",
        "paper_author": "Van Der Vegt A.H.",
        "publication": "Journal of the American Medical Informatics Association",
        "citied_by": "16",
        "cover_date": "2023-09-01",
        "Abstract": "Objective: To derive a comprehensive implementation framework for clinical AI models within hospitals informed by existing AI frameworks and integrated with reporting standards for clinical AI research. Materials and Methods: (1) Derive a provisional implementation framework based on the taxonomy of Stead et al and integrated with current reporting standards for AI research: TRIPOD, DECIDE-AI, CONSORT-AI. (2) Undertake a scoping review of published clinical AI implementation frameworks and identify key themes and stages. (3) Perform a gap analysis and refine the framework by incorporating missing items. Results: The provisional AI implementation framework, called SALIENT, was mapped to 5 stages common to both the taxonomy and the reporting standards. A scoping review retrieved 20 studies and 247 themes, stages, and subelements were identified. A gap analysis identified 5 new cross-stage themes and 16 new tasks. The final framework comprised 5 stages, 7 elements, and 4 components, including the AI system, data pipeline, human-computer interface, and clinical workflow. Discussion: This pragmatic framework resolves gaps in existing stage- and theme-based clinical AI implementation guidance by comprehensively addressing the what (components), when (stages), and how (tasks) of AI implementation, as well as the who (organization) and why (policy domains). By integrating research reporting standards into SALIENT, the framework is grounded in rigorous evaluation methodologies. The framework requires validation as being applicable to real-world studies of deployed AI models. Conclusions: A novel end-to-end framework has been developed for implementing AI within hospital clinical practice that builds on previous AI implementation frameworks and research reporting standards.",
        "DOI": "10.1093/jamia/ocad088",
        "affiliation_name": "The University of Queensland",
        "affiliation_city": "Brisbane",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Machine learning–couched treatment algorithms tailored to individualized profile of patients with primary anterior chamber angle closure predisposed to the glaucomatous optic neuropathy",
        "paper_author": "Kurysheva N.I.",
        "publication": "EPMA Journal",
        "citied_by": "12",
        "cover_date": "2023-09-01",
        "Abstract": "Background: Primary angle closure glaucoma (PACG) is still one of the leading causes of irreversible blindness, with a trend towards an increase in the number of patients to 32.04 million by 2040, an increase of 58.4% compared with 2013. Health risk assessment based on multi-level diagnostics and machine learning–couched treatment algorithms tailored to individualized profile of patients with primary anterior chamber angle closure are considered essential tools to reverse the trend and protect vulnerable subpopulations against health-to-disease progression. Aim: To develop a methodology for personalized choice of an effective method of primary angle closure (PAC) treatment based on comparing the prognosis of intraocular pressure (IOP) changes due to laser peripheral iridotomy (LPI) or lens extraction (LE). Methods: The multi-parametric data analysis was used to develop models predicting individual outcomes of the primary angle closure (PAC) treatment with LPI and LE. For doing this, we suggested a positive dynamics in the intraocular pressure (IOP) after treatment, as the objective measure of a successful treatment. Thirty-seven anatomical parameters have been considered by applying artificial intelligence to the prospective study on 30 (LE) + 30 (LPI) patients with PAC. Results and data interpretation in the framework of 3P medicine: Based on the anatomical and topographic features of the patients with PAC, mathematical models have been developed that provide a personalized choice of LE or LPI in the treatment. Multi-level diagnostics is the key tool in the overall advanced approach. To this end, for the future application of AI in the area, it is strongly recommended to consider the following: 1.Clinically relevant phenotyping applicable to advanced population screening2.Systemic effects causing suboptimal health conditions considered in order to cost-effectively protect affected individuals against health-to-disease transition3.Clinically relevant health risk assessment utilizing health/disease-specific molecular patterns detectable in body fluids with high predictive power such as a comprehensive tear fluid analysis.",
        "DOI": "10.1007/s13167-023-00337-1",
        "affiliation_name": "Federal Biomedical Agency Russia",
        "affiliation_city": "Moscow",
        "affiliation_country": "Russian Federation"
    },
    {
        "paper_title": "Algorithmic bias in artificial intelligence is a problem—And the root issue is power",
        "paper_author": "Walker R.",
        "publication": "Nursing Outlook",
        "citied_by": "14",
        "cover_date": "2023-09-01",
        "Abstract": "Background: Artificial intelligence (AI) in health care continues to expand at a rapid rate, impacting both nurses and communities we accompany in care. Purpose: We argue algorithmic bias is but a symptom of a more systemic and longstanding problem: power imbalances related to the creation, development, and use of health care technologies. Methods: This commentary responds to Drs. O'Connor and Booth's 2022 article, “Algorithmic bias in health care: Opportunities for nurses to improve equality in the age of artificial intelligence.” Discussion: Nurses need not ‘reinvent the wheel’ when it comes to AI policy, curricula, or ethics. We can and should follow the lead of communities already working ‘from the margins’ who provide ample guidance. Conclusion: Its neither feasible nor just to expect individual nurses to counter systemic injustice in health care through individual actions, more technocentric curricula, or industry partnerships. We need disciplinary supports for collective action to renegotiate power for AI tech.",
        "DOI": "10.1016/j.outlook.2023.102023",
        "affiliation_name": "University of Massachusetts Amherst",
        "affiliation_city": "Amherst",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Mindset × Context: Schools, Classrooms, and the Unequal Translation of Expectations into Math Achievement",
        "paper_author": "Carroll J.M.",
        "publication": "Monographs of the Society for Research in Child Development",
        "citied_by": "5",
        "cover_date": "2023-09-01",
        "Abstract": "When do adolescents' dreams of promising journeys through high school translate into academic success? This monograph reports the results of a collaborative effort among sociologists and psychologists to systematically examine the role of schools and classrooms in disrupting or facilitating the link between adolescents' expectations for success in math and their subsequent progress in the early high school math curriculum. Our primary focus was on gendered patterns of socioeconomic inequality in math and how they are tethered to the school's peer culture and to students' perceptions of gender stereotyping in the classroom. To do this, this monograph advances Mindset × Context Theory. This orients research on educational equity to the reciprocal influence between students' psychological motivations and their school-based opportunities to enact those motivations. Mindset × Context Theory predicts that a student's mindset will be more strongly linked to developmental outcomes among groups of students who are at risk for poor outcomes, but only in a school or classroom context where there is sufficient need and support for the mindset. Our application of this theory centers on expectations for success in high school math as a foundational belief for students' math progress early in high school. We examine how this mindset varies across interpersonal and cultural dynamics in schools and classrooms. Following this perspective, we ask: 1. Which gender and socioeconomic identity groups showed the weakest or strongest links between expectations for success in math and progress through the math curriculum? 2. How did the school's peer culture shape the links between student expectations for success in math and math progress across gender and socioeconomic identity groups? 3. How did perceptions of classroom gender stereotyping shape the links between student expectations for success in math and math progress across gender and socioeconomic identity groups?. We used nationally representative data from about 10,000 U.S. public school 9th graders in the National Study of Learning Mindsets (NSLM) collected in 2015–2016—the most recent, national, longitudinal study of adolescents' mindsets in U.S. public schools. The sample was representative with respect to a large number of observable characteristics, such as gender, race, ethnicity, English Language Learners (ELLs), free or reduced price lunch, poverty, food stamps, neighborhood income and labor market participation, and school curricular opportunities. This allowed for generalization to the U.S. public school population and for the systematic investigation of school- and classroom-level contextual factors. The NSLM's complete sampling of students within schools also allowed for a comparison of students from different gender and socioeconomic groups with the same expectations in the same educational contexts. To analyze these data, we used the Bayesian Causal Forest (BCF) algorithm, a best-in-class machine-learning method for discovering complex, replicable interaction effects. Chapter IV examined the interplay of expectations, gender, and socioeconomic status (SES; operationalized with maternal educational attainment). Adolescents' expectations for success in math were meaningful predictors of their early math progress, even when controlling for other psychological factors, prior achievement in math, and racial and ethnic identities. Boys from low-SES families were the most vulnerable identity group. They were over three times more likely to not make adequate progress in math from 9th to 10th grade relative to girls from high-SES families. Boys from low-SES families also benefited the most from their expectations for success in math. Overall, these results were consistent with Mindset × Context Theory's predictions. Chapters V and VI examined the moderating role of school-level and classroom-level factors in the patterns reported in Chapter IV. Expectations were least predictive of math progress in the highest-achieving schools and schools with the most academically oriented peer norms, that is, schools with the most formal and informal resources. School resources appeared to compensate for lower levels of expectations. Conversely, expectations most strongly predicted math progress in the low/medium-achieving schools with less academically oriented peers, especially for boys from low-SES families. This chapter aligns with aspects of Mindset × Context Theory. A context that was not already optimally supporting student success was where outcomes for vulnerable students depended the most on student expectations. Finally, perceptions of classroom stereotyping mattered. Perceptions of gender stereotyping predicted less progress in math, but expectations for success in math more strongly predicted progress in classrooms with high perceived stereotyping. Gender stereotyping interactions emerged for all sociodemographic groups except for boys from high-SES families. The findings across these three analytical chapters demonstrate the value of integrating psychological and sociological perspectives to capture multiple levels of schooling. It also drew on the contextual variability afforded by representative sampling and explored the interplay of lab-tested psychological processes (expectations) with field-developed levers of policy intervention (school contexts). This monograph also leverages developmental and ecological insights to identify which groups of students might profit from different efforts to improve educational equity, such as interventions to increase expectations for success in math, or school programs that improve the school or classroom cultures.",
        "DOI": "10.1111/mono.12471",
        "affiliation_name": "New York University",
        "affiliation_city": "New York",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "An explainable artificial intelligence approach to understanding drivers of economic energy consumption and sustainability",
        "paper_author": "Srivastava P.R.",
        "publication": "Energy Economics",
        "citied_by": "16",
        "cover_date": "2023-09-01",
        "Abstract": "This study aims to optimize energy consumption to achieve sustainability, as there is a necessary foray into the green energy market by switching to net-zero carbon-emitting fuel alternatives. There is a need to decide when to switch to these net-zero fuels from conventional energy sources, which calls for a deeper investigation into the driving factors that lead to harmful energy emissions. This investigation will help monitor and curb such indicators to minimize harmful emissions or determine the opportune time to switch to green energy alternatives if the indicator levels cannot be controlled. This research is motivated by recent studies that consider factors such as per capita carbon intensity and other sector-specific factors such as per dwelling carbon intensity and per value-added carbon intensity. However, these studies do not scientifically quantify the extent to which each element contributes to the final conventional energy consumption. Furthermore, sector-specific consumption indicators also need to be estimated to undertake significant design modifications for developing energy-efficient systems. The consumption is predicted as “total consumption” for the four sectors considered: residential, industry, services, and transportation. Advanced machine learning algorithms such as random forest, gradient boosting, and deep neural network are used for this purpose. The essential factors/drivers for each sector are derived through the explainable artificial intelligence Shapley framework, which scientifically measures the contribution of each factor to energy consumption. Consequently, the major key indicators for each of the four sectors under consideration are identified. Alternatively, the insights gained from the study may prompt a complete switch to alternative energy-efficient sources. The study findings provide valuable insights for both communities and businesses in achieving the goals of net-zero and energy conservation. Additionally, some examples are highlighted to demonstrate how practitioners can implement customized designs under each sector to reduce harmful emissions and promote energy efficiency. Suitable steps for stakeholder engagement and community participation in this regard are also suggested.",
        "DOI": "10.1016/j.eneco.2023.106868",
        "affiliation_name": "Indian Institute of Management Bodh Gaya",
        "affiliation_city": "Gaya",
        "affiliation_country": "India"
    },
    {
        "paper_title": "A site selection framework for urban power substation at micro-scale using spatial optimization strategy and geospatial big data",
        "paper_author": "Yao Y.",
        "publication": "Transactions in GIS",
        "citied_by": "4",
        "cover_date": "2023-09-01",
        "Abstract": "The world is facing more energy crises due to extreme weather and the rapidly growing demand for electricity. Siting new substations and optimizing the location of existing ones are necessary to address the energy crisis. The current site selection lacks consideration of spatial and temporal heterogeneity in urban power demand, which results in unreasonable energy transfer and waste, leading to power outages in some areas. Aiming to maximize the grid coverage and transformer utilization, we propose a multi-scene micro-scale urban substation siting framework (UrbanPS): (1) The framework uses multi-source big data and the machine learning model to estimate fine-scale power consumption for different scenarios; (2) the region growing algorithm is used to divide the power supply area of substations; and the (3) location set coverage problem and genetic algorithm are introduced to optimize the substation location. The UrbanPS was used to perform siting optimization of 110 kV terminal substations in Pingxiang City, Jiangxi Province. Results show that the coverage and utilization rate of the optimization results under different power consumption scenarios are close to 99%. We also found that the power can be saved by dynamic regulation of substation operation.",
        "DOI": "10.1111/tgis.13093",
        "affiliation_name": "University of Oxford",
        "affiliation_city": "Oxford",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Hardware-in-the-Loop Soft Robotic Testing Framework Using an Actor-Critic Deep Reinforcement Learning Algorithm",
        "paper_author": "Marquez J.",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "4",
        "cover_date": "2023-09-01",
        "Abstract": "Polymer-based soft robots are difficult to characterize due to their non-linear nature. This difficulty is compounded by multiple additional degrees of movement freedom which adds complexity to any control strategy proposed. The following work proposes and demonstrates a modular framework to test, debug and characterize soft robots using the robot operating system (ROS), to enable modeless deep reinforcement learning control strategies through hardware-in-the-loop system training. The framework is demonstrated using an actor-critic algorithm to learn a locomotion policy for a two-actuator pneu-net soft robot with integrated resistive flex sensors. The result of convergent locomotion studies was an 89.5% increase in the likelihood of reaching the end of frame design goal versus random oracle actuation vectors.",
        "DOI": "10.1109/LRA.2023.3301215",
        "affiliation_name": "University of Texas at El Paso College of Engineering",
        "affiliation_city": "El Paso",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Design, Implementation, and Observer-Based Output Control of a Super-Coiled Polymer-Driven Two Degree-of-Freedom Robotic Eye",
        "paper_author": "Rajendran S.K.",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "3",
        "cover_date": "2023-09-01",
        "Abstract": "The prevalence of ineffective corrective surgeries for ocular motor disorders calls for a robotic eye platform in aiding ophthalmologists to better understand the biomechanisms of human eye movement. This letter presents the first hardware design and implementation of a 2-DOF robotic eye driven by super-coiled polymer (SCP) artificial muscles. While our previous work designed and simulated a deep deterministic policy gradient (DDPG) learning-based controller that requires full-state feedback of the SCP-driven robotic eye, measuring the temperature states of the slender SCPs is generally impractical for the ubiquitously aimed robot. To address this predicament, this letter proposes a reduced-order state observer to estimate the temperature of SCPs given the kinematic measurements. Combining the designed observer and the learning-based controller, the closed-loop output feedback control is implemented on the robotic eye prototype to examine its performance on three classical types of eye movements: visual fixation, saccadic pursuit, and smooth pursuit. The experimental results are presented which successfully validate the observer-based output control of the SCP-driven robotic eye.",
        "DOI": "10.1109/LRA.2023.3301296",
        "affiliation_name": "College of Engineering and Computing",
        "affiliation_city": "Fairfax",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "The amenity value of natural views",
        "paper_author": "Hamilton T.L.",
        "publication": "Real Estate Economics",
        "citied_by": "2",
        "cover_date": "2023-09-01",
        "Abstract": "We estimate nonmarket values for natural views in an urban setting. These views contain the aesthetics of natural areas commonly found in public parks and open space, and offer an aspect of property valuation that previous research is unable to disentangle from proximity to parks and open space. We incorporate machine learning techniques on Google Street View images to identify natural views in an urban setting. We find positive capitalization rates associated with household views of park-like properties. Estimates are robust to a variety of specifications, including models that are identified off of new developments on neighboring properties and falsification tests that help to rule out the effect of a broader neighborhood environment. From a policy perspective, our results inform as to the optimal size, location, and shape of open space. Furthermore, machine learning methods used in the construction of our view variable provide a potentially powerful tool for other nonmarket valuation studies.",
        "DOI": "10.1111/1540-6229.12451",
        "affiliation_name": "The University of Alabama",
        "affiliation_city": "Tuscaloosa",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Regional economic integration and machine learning: Policy insights from the review of literature",
        "paper_author": "De Lombaerde P.",
        "publication": "Journal of Policy Modeling",
        "citied_by": "5",
        "cover_date": "2023-09-01",
        "Abstract": "Due to its focus on prediction rather than causal inference, machine learning has long been treated somewhat neglectfully in the economic literature. For several reasons, however, interest in machine learning has surged recently and is slowly finding its way into the econometric toolbox. Within the economic literature, regional integration has been one of the research areas at the forefront of this development, with various studies experimenting with different machine learning techniques to shed light on the complex dynamics governing regional integration processes. This paper provides the first systematic review of the literature that uses machine learning to study regional economic integration. The focus is twofold, first analysing studies along various thematic and methodological features (and the links between them), and then discussing the scope and nature of policy insights derived from the surveyed body of literature.",
        "DOI": "10.1016/j.jpolmod.2023.07.001",
        "affiliation_name": "Thu Dau Mot University",
        "affiliation_city": "Thu Dau Mot",
        "affiliation_country": "Viet Nam"
    },
    {
        "paper_title": "Optimal discharge of patients from intensive care via a data-driven policy learning framework",
        "paper_author": "Lejarza F.",
        "publication": "Operations Research for Health Care",
        "citied_by": "2",
        "cover_date": "2023-09-01",
        "Abstract": "Clinical decision support tools rooted in machine learning and optimization can provide significant value to healthcare providers through better management of intensive care units. In particular, it is important that intensive care unit patient discharge decisions account for the nuanced trade-off between decreasing the length of stay and the risk of readmission or death after discharge of a patient. This work introduces a comprehensive framework (i.e., not geared towards any particular disease or condition) for capturing this trade-off and to recommend optimal discharge timing decisions given the electronic health records of a patient. A data-driven approach is used to derive a parsimonious, discrete state space representation to represent the physiological condition of a given patient. Based on this model and a given cost function, an infinite-horizon discounted Markov decision process is formulated and solved numerically to compute an optimal discharge policy, whose performance is assessed using off-policy evaluation strategies. Extensive numerical experiments are performed to validate the proposed framework using real-life intensive care unit patient data.",
        "DOI": "10.1016/j.orhc.2023.100400",
        "affiliation_name": "Cockrell School of Engineering",
        "affiliation_city": "Austin",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Phenology-assisted supervised paddy rice mapping with the Landsat imagery on Google Earth Engine: Experiments in Heilongjiang Province of China from 1990 to 2020",
        "paper_author": "Zhang C.",
        "publication": "Computers and Electronics in Agriculture",
        "citied_by": "50",
        "cover_date": "2023-09-01",
        "Abstract": "Accurate spatial distribution maps of paddy rice played crucial roles in food security and market stability. Decades-spanning Landsat images were useful for long-term paddy rice mapping. However, it still remained challenging to achieve consistent paddy rice mapping using the Landsat series images due to many factors such as sparse observations, frequent weather contamination, and shortage of training samples. To address these challenges, this study proposed a flexible Phenology-assisted Supervised Paddy Rice (PSPR) mapping framework on Google Earth Engine (GEE). This was achieved by utilizing the automation of the phenological methods, training data generation and purification, and the all-season classification capacity of the machine learning methods. We demonstrated the method by generating high-resolution 30-m paddy rice maps of Heilongjiang Province of China from 1990 to 2020. The derived rice maps were validated using abundant reference samples, four existing paddy rice products, and available agricultural statistics. The result showed that improved performance was verified in comparison to previous studies and a high linear relationship was observed with an average R2 of 0.993. Based on the spatiotemporal analysis, it was discovered that the rice planting in Heilongjiang has significantly shifted northward in the last three decades and this northward shift surprisingly appeared earlier than the previous studies, which was to our best knowledge first to be revealed in related studies. The multi-year dataset is useful for rice monitoring, water management, and policy making. All data and codes used in this study can be accessed on GitHub (https://github.com/MKGenesis/PSPR-rice-HLJ).",
        "DOI": "10.1016/j.compag.2023.108105",
        "affiliation_name": "Zhejiang Lab",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Discovering agents",
        "paper_author": "Kenton Z.",
        "publication": "Artificial Intelligence",
        "citied_by": "7",
        "cover_date": "2023-09-01",
        "Abstract": "Causal models of agents have been used to analyse the safety aspects of machine learning systems. But identifying agents is non-trivial – often the causal model is just assumed by the modeller without much justification – and modelling failures can lead to mistakes in the safety analysis. This paper proposes the first formal causal definition of agents – roughly that agents are systems that would adapt their policy if their actions influenced the world in a different way. From this we derive the first causal discovery algorithm for discovering the presence of agents from empirical data, given a set of variables and under certain assumptions. We also provide algorithms for translating between causal models and game-theoretic influence diagrams. We demonstrate our approach by resolving some previous confusions caused by incorrect causal modelling of agents.",
        "DOI": "10.1016/j.artint.2023.103963",
        "affiliation_name": "Imperial College London",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Carbon prices forecasting based on the singular spectrum analysis, feature selection, and deep learning: Toward a unified view",
        "paper_author": "Zhang C.",
        "publication": "Process Safety and Environmental Protection",
        "citied_by": "5",
        "cover_date": "2023-09-01",
        "Abstract": "An accurate carbon price prediction is vital for governments to formulate emission reduction policies and for corporate managers to carry out sustainable management. Although many models have been proposed to meet this purpose, there is still a lack of comprehensive comparisons among the key competing models in a single study. This study conducted a comprehensive evaluation of six feature selection methods and six machine learning models for carbon price prediction, and then proposed a novel data-driven hybrid model that integrates singular spectrum analysis, random forest, and long short-term memory neural network. The results show that random forest and long short-term memory neural networks are the most competitive feature selection and prediction models, respectively. It is more reasonable to construct the model's input by comprehensively considering variables’ short-, medium-, and long-term features. When the recombination value is approximately half the length of the embedding window, the singular spectrum analysis is most conducive to improving the prediction accuracy. The hybrid model proposed is always significantly superior to other benchmark models. Our work contributes to guiding carbon price modeling to help regulators and managers accurately grasp carbon price signals.",
        "DOI": "10.1016/j.psep.2023.07.015",
        "affiliation_name": "Xiamen University",
        "affiliation_city": "Xiamen",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Protecting patient safety and privacy in the era of artificial intelligence",
        "paper_author": "Alonso A.",
        "publication": "Seminars in Vascular Surgery",
        "citied_by": "16",
        "cover_date": "2023-09-01",
        "Abstract": "The promise of artificial intelligence (AI) in health care has propelled a significant uptrend in the number of clinical trials in AI and global market spending in this novel technology. In vascular surgery, this technology has the ability to diagnose disease, predict disease outcomes, and assist with image-guided surgery. As we enter an era of rapid change, it is critical to evaluate the ethical concerns of AI, particularly as it may impact patient safety and privacy. This is particularly important to discuss in the early stages of AI, as technology frequently outpaces the policies and ethical guidelines regulating it. Issues at the forefront include patient privacy and confidentiality, protection of patient autonomy and informed consent, accuracy and applicability of this technology, and propagation of health care disparities. Vascular surgeons should be equipped to work with AI, as well as discuss its novel risks to patient safety and privacy.",
        "DOI": "10.1053/j.semvascsurg.2023.06.002",
        "affiliation_name": "Boston Medical Center",
        "affiliation_city": "Boston",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Understanding and modeling willingness-to-pay for public policies to enhance road safety: A perspective from Pakistan",
        "paper_author": "Subhan F.",
        "publication": "Transport Policy",
        "citied_by": "1",
        "cover_date": "2023-09-01",
        "Abstract": "Evaluating road safety improvements becomes important because it can assist policymakers in allocating economic resources to improve safety and implementing effective policy interventions. As such, this study aims to estimate the value of road safety risk measures using a new modeling approach for willingness-to-pay (WTP). Specifically, this study integrates a machine learning technique (decision tree) with a correlated random parameters Tobit with heterogeneity-in-means model. The decision tree identifies a priori relationships for higher-order interactions, while the model captures unobserved heterogeneity and the correlation between random parameters. The proposed modeling framework examines the determinants of public WTP for improving road safety using a sample of car drivers from Peshawar, Pakistan. WTP for fatal and severe injury risk reductions is estimated and used to calculate the values of corresponding risk reductions, which can be used for monetizing the cost of road traffic crashes in the country. Modeling results reveal that most respondents are willing to contribute to road safety improvement policies. Further, the model also uncovers significant heterogeneity in WTP corresponding to the safer perception of the overall road infrastructure and perceived risk of accident involvement. Systematic preference heterogeneity is also found in the model by including higher-order interactions, providing additional insights into the complex relationship of WTP with its determinants. Further, the marginal effects of explanatory variables indicate different sensitivities toward WTP, which can help to quantify the impacts of these variables on both the probability and magnitude of WTP. Overall, the proposed modeling framework has a twofold contribution. First, the modeling framework provides valuable insights into the determinants of public WTP, mainly when the heterogeneous effects of variables are interactive. Second, its implementation and consequent findings shall help prioritize different road safety policies/projects by better understanding public sensitivity to WTP.",
        "DOI": "10.1016/j.tranpol.2023.07.016",
        "affiliation_name": "Faculteit Techniek, Bestuur en Management, TU Delft",
        "affiliation_city": "Delft",
        "affiliation_country": "Netherlands"
    },
    {
        "paper_title": "Evolution strategies-based optimized graph reinforcement learning for solving dynamic job shop scheduling problem",
        "paper_author": "Su C.",
        "publication": "Applied Soft Computing",
        "citied_by": "17",
        "cover_date": "2023-09-01",
        "Abstract": "The job shop scheduling problem (JSSP) with dynamic events and uncertainty is a strongly NP-hard combinatorial optimization problem (COP) with extensive applications in the manufacturing system. Recently, growing interest has been aroused in utilizing machine learning techniques to solve the JSSP. However, most prior arts cannot handle dynamic events and barely consider uncertainties. To close this gap, this paper proposes a framework to solve a dynamic JSSP (DJSP) with machine breakdown and stochastic processing time based on Graph Neural Network (GNN) and deep reinforcement learning (DRL). To this end, we first formulate the DJSP as a Markov Decision Process (MDP), where disjunctive graph represent the states. Secondly, we propose a GNN-based model to effectively extract the embeddings of the state by considering the features of the dynamic events and the stochasticity of the problem, e.g., the machine breakdown and stochastic processing time. Then, the model constructs solutions by dispatching optimal operations to machines based on the learned embeddings. Notably, we propose to use the evolution strategies (ES) to find optimal policies that are more stable and robust than conventional DRL algorithms. The extensive experiments show that our method substantially outperforms existing reinforcement learning-based and traditional methods on multiple classic benchmarks.",
        "DOI": "10.1016/j.asoc.2023.110596",
        "affiliation_name": "School of Computer Science and Engineering",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore"
    },
    {
        "paper_title": "Cause of Death estimation from Verbal Autopsies: Is the Open Response redundant or synergistic?",
        "paper_author": "Cejudo A.",
        "publication": "Artificial Intelligence in Medicine",
        "citied_by": "0",
        "cover_date": "2023-09-01",
        "Abstract": "Civil registration and vital statistics systems capture birth and death events to compile vital statistics and to provide legal rights to citizens. Vital statistics are a key factor in promoting public health policies and the health of the population. Medical certification of cause of death is the preferred source of cause of death information. However, two thirds of all deaths worldwide are not captured in routine mortality information systems and their cause of death is unknown. Verbal autopsy is an interim solution for estimating the cause of death distribution at the population level in the absence of medical certification. A Verbal Autopsy (VA) consists of an interview with the relative or the caregiver of the deceased. The VA includes both Closed Questions (CQs) with structured answer options, and an Open Response (OR) consisting of a free narrative of the events expressed in natural language and without any pre-determined structure. There are a number of automated systems to analyze the CQs to obtain cause specific mortality fractions with limited performance. We hypothesize that the incorporation of the text provided by the OR might convey relevant information to discern the CoD. The experimental layout compares existing Computer Coding Verbal Autopsy methods such as Tariff 2.0 with other approaches well suited to the processing of structured inputs as is the case of the CQs. Next, alternative approaches based on language models are employed to analyze the OR. Finally, we propose a new method with a bi-modal input that combines the CQs and the OR. Empirical results corroborated that the CoD prediction capability of the Tariff 2.0 algorithm is outperformed by our method taking into account the valuable information conveyed by the OR. As an added value, with this work we made available the software to enable the reproducibility of the results attained with a version implemented in R to make the comparison with Tariff 2.0 evident.",
        "DOI": "10.1016/j.artmed.2023.102622",
        "affiliation_name": "Universidad del Pais Vasco",
        "affiliation_city": "Leioa",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "Sentiment Analysis of Tweets on Soda Taxes",
        "paper_author": "An R.",
        "publication": "Journal of Public Health Management and Practice",
        "citied_by": "6",
        "cover_date": "2023-09-01",
        "Abstract": "Context: As a primary source of added sugars, sugar-sweetened beverage (SSB) consumption may contribute to the obesity epidemic. A soda tax is an excise tax charged on selling SSBs to reduce consumption. Currently, 8 cities/counties in the United States have imposed soda taxes. Objective: This study assessed people's sentiments toward soda taxes in the United States based on social media posts on Twitter. Design: We designed a search algorithm to systematically identify and collect soda tax-related tweets posted on Twitter. We built deep neural network models to classify tweets by sentiments. Setting: Computer modeling. Participants: Approximately 370 000 soda tax-related tweets posted on Twitter from January 1, 2015, to April 16, 2022. Main Outcome Measure: Sentiment associated with a tweet. Results: Public attention paid to soda taxes, indicated by the number of tweets posted annually, peaked in 2016, but has declined considerably ever since. The decreasing prevalence of tweets quoting soda tax-related news without revealing sentiments coincided with the rapid increase in tweets expressing a neutral sentiment toward soda taxes. The prevalence of tweets expressing a negative sentiment rose steadily from 2015 to 2019 and then slightly leveled off, whereas that of tweets expressing a positive sentiment remained unchanged. Excluding news-quoting tweets, tweets with neutral, negative, and positive sentiments occupied roughly 56%, 29%, and 15%, respectively, during 2015-2022. The authors' total number of tweets posted, followers, and retweets predicted tweet sentiment. The finalized neural network model achieved an accuracy of 88% and an F1 score of 0.87 in predicting tweet sentiments in the test set. Conclusions: Despite its potential to shape public opinion and catalyze social changes, social media remains an underutilized source of information to inform government decision making. Social media sentiment analysis may inform the design, implementation, and modification of soda tax policies to gain social support while minimizing confusion and misinterpretation.",
        "DOI": "10.1097/PHH.0000000000001721",
        "affiliation_name": "Washington University in St. Louis, George Warren Brown School of Social Work",
        "affiliation_city": "St. Louis",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Learning Adaptive Policies for Autonomous Excavation under Various Soil Conditions by Adversarial Domain Sampling",
        "paper_author": "Osa T.",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "2",
        "cover_date": "2023-09-01",
        "Abstract": "Excavation is a frequent task in construction. In this context, automation is expected to reduce hazard risks and labor-intensive work. To this end, recent studies have investigated using reinforcement learning (RL) to automate construction machines. One of the challenges in applying RL to excavation tasks concerns obtaining skills adaptable to various conditions. When the conditions of soils differ, the optimal plans for efficiently excavating the target area will significantly differ. In existing meta-learning methods, the domain parameters are often uniformly sampled; this implicitly assumes that the difficulty of the task does not change significantly for different domain parameters. In this study, we empirically show that uniformly sampling the domain parameters is insufficient when the task difficulty varies according to the task parameters. Correspondingly, we develop a framework for learning a policy that can be generalized to various domain parameters in excavation tasks. We propose two techniques for improving the performance of an RL method in our problem setting: adversarial domain sampling and domain parameter estimation with a sensitivity-aware importance weight. In the proposed adversarial domain sampling technique, the domain parameters leading to low expected Q-values are actively sampled during the training phase. In addition, we propose a technique for training a domain parameter estimator based on the sensitivity of the Q-function to the domain parameter. The proposed techniques improve the performance of the RL method for our excavation task. We empirically show that our approach outperforms existing meta-learning and domain adaptation methods for excavation tasks.",
        "DOI": "10.1109/LRA.2023.3296933",
        "affiliation_name": "RIKEN Center for Advanced Intelligence Project",
        "affiliation_city": "Tokyo",
        "affiliation_country": "Japan"
    },
    {
        "paper_title": "The use of ICTs and income distribution in Brazil: A machine learning explanation using SHAP values",
        "paper_author": "Herrera G.P.",
        "publication": "Telecommunications Policy",
        "citied_by": "13",
        "cover_date": "2023-09-01",
        "Abstract": "This study explores the complex relationship between information and communication technologies (ICTs) and socioeconomic characteristics. We employ a cutting-edge explainable machine learning approach, known as SHAP values, to interpret an XGBoost and neural network model, as well as benchmark traditional econometric methods. The application of machine learning algorithms combined with the SHAP methodology reveals complex nonlinear relationships in the data and important insights to guide tailored policy-making. Our results suggest that there is an interaction between education and ICTs that contributes to income prediction. Furthermore, level of education and age are found to be positively associated with income, while gender presents a negative relationship; that is, women earn less than men on average. This study highlights the need for more efficient public policies to fight gender inequality in Brazil. It is also important to introduce policies that promote quality education and the teaching of skills related to technology and digitalization to prepare individuals for changes in the job market and avoid the digital divide and increasing social inequality.",
        "DOI": "10.1016/j.telpol.2023.102598",
        "affiliation_name": "Griffith Business School",
        "affiliation_city": "Brisbane",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Integrating machine learning and environmental variables to constrain uncertainty in crop yield change projections under climate change",
        "paper_author": "Li L.",
        "publication": "European Journal of Agronomy",
        "citied_by": "15",
        "cover_date": "2023-09-01",
        "Abstract": "Robust crop yield projections under future climates are fundamental prerequisites for reliable policy formation. Both process-based crop models and statistical models are commonly used for this purpose. Process-based models tend to simplify processes, minimize the effects of extreme events, and ignore biotic pressures, while statistical models cannot deterministically capture intricate biological and physiological processes underpinning crop growth. We attempted to integrate and overcome shortcomings in both modelling frameworks by integrating the dynamic linear model (DLM) and random forest machine learning model (RF) with nine global gridded crop models (GGCM), respectively, in order to improve projections and reduce uncertainties of maize (Zea mays L.) and soybean (Glycine max [L.] Merrill) yield projections. Our results demonstrated substantial improvements in model performance accuracy by using RF in concert with GGCM across China's maize and soybean belt. This improvement surpasses that achieved using DLM. For maize, the GGCM+RF models increased the r values from 0.15 to 0.61–0.64–0.77 and decreased nRMSE from approximately 0.20 to 0.50–0.13–0.17 compared with using GGCM alone. For soybean, the models increased r from 0.37 to 0.70–0.54–0.70 and decreased nRMSE from 0.17 to 0.35–0.17–0.20 compared with using GGCM alone. The main factors influencing maize yield changes included chilling days (CD), crop pests and diseases (CPDs), and drought, while for soybean the primary influencing factors included CPD, tropical days (based on exceeding a maximum temperature), and drought. Our approach decreased uncertainties by 33–78% for maize and by 56–68% for soybean. The main source of uncertainty for GGCM was the crop model. For GGCM+RF, the main source of uncertainty for the 2040–2069 period was the global climate model, while the main source of uncertainty for the 2070–2099 period was the climate scenario. Our results provide a novel, robust, and pragmatic framework to constrain uncertainties in order to accurately assess the impact of future climate change on crop yields. These results could be used to interpret future ensemble studies by accounting for uncertainty in crop and climate models, as well as to assess future emissions scenarios.",
        "DOI": "10.1016/j.eja.2023.126917",
        "affiliation_name": "Tasmanian Institute of Agriculture",
        "affiliation_city": "Hobart",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Variational eligibility trace meta-reinforcement recurrent network for residual life prediction of space rolling bearings",
        "paper_author": "Li F.",
        "publication": "Applied Soft Computing",
        "citied_by": "2",
        "cover_date": "2023-09-01",
        "Abstract": "Traditional sequence recurrent neural networks (SRNNs) have the defect of long time dependence in the prediction of time series, resulting in their poor generalization ability. Moreover, it is required to traverse the whole training data set to realize supervised learning by SRNNs, which increases the time complexity and leads to their low prediction accuracy and high computation cost in the residual life prediction of space rolling bearings in the ground simulated space environment. In view of this, a novel SRNN named variational eligibility trace meta-reinforcement recurrent network (VETMRRN) is proposed for achieving higher residual life prediction accuracy and lower computation cost. In the proposed VETMRRN, a new sequence recurrent network structure is constructed to increase the memory amount of historical information, thus improving the long-term memory capacity of VETMRRN. Then, a hyperparameter self-initialization meta-learning network with an oracle gate mechanism is designed to self-initialize the hyperparameters of VETMRRN for fast determination of the optimal review sequence length. Hence, VETMRRN can adapt to different input sequence lengths and avoid the defect of long time dependence of traditional SRNNs. Furthermore, a variational auto-encoding meta policy gradient learning algorithm with an eligibility trace operator is designed to improve the training speed and enhance the global optimization effect for VETMRRN parameters. Based on the above advantages of VETMRRN, a new residual life prediction method of space rolling bearings in the ground simulated space environment is proposed. Firstly, the time-frequency fusion features are extracted by Shapely-value feature fusion from the vibration acceleration data of space rolling bearing as the performance degradation features. Then, the performance degradation features are input into VETMRRN to predict the performance degradation feature trends of space rolling bearings. Finally, a Weibull-distribution reliability model is established based on the performance degradation feature trend values to predict the residual life of space rolling bearings. The effectiveness of the proposed VETMRRN-based prediction method is verified by the vibration acceleration data collected from the self-built vibration monitoring platform of space rolling bearings in the ground simulated space environment. The results indicate that compared to traditional SRNNs, deep sparse auto-encoding neural network (DSAE-NN), and multi-kernel least-square support vector machine (MK-LSSVM), the proposed method can improve the prediction accuracy and reduce the computation cost in the residual life prediction of space rolling bearings. In the future, the generalization performance of VETMRRN still needs to be further improved.",
        "DOI": "10.1016/j.asoc.2023.110582",
        "affiliation_name": "Chongqing University",
        "affiliation_city": "Chongqing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A content analysis of the Central Bank's press releases in Colombia",
        "paper_author": "Arango L.E.",
        "publication": "Latin American Journal of Central Banking",
        "citied_by": "1",
        "cover_date": "2023-09-01",
        "Abstract": "The Central Bank uses press releases after board meetings for at least three purposes: first, to justify policy measures based on the economic situation; second, to provide some forward guidance signals to agents; and third, to supply some further (latent) information to the markets. This article involves a reading analysis of press releases based on a machine-learning technique to show, first, the coherence between communications and the changes of the interest rate and, second, the capacity of communications to alter inflation expectations. We find that, following the official mandate of the Central Bank, inflation and inflation expectations as well as economic activity were significant topics in the adoption of policy measures between September 2004 and March 2016, with more emphasis on the former. Our indicators of forward guidance are not significant in the adoption of contemporary policy measures. Finally, with the help of latent semantic analysis, we extract the underlying factors that are then used in structural VAR models to identify and measure the impact of press releases’ shocks on inflation expectations. Our results indicate that Colombia's Central Bank uses communications as a monetary policy tool and that this strategy influences market inflation expectations.",
        "DOI": "10.1016/j.latcb.2023.100097",
        "affiliation_name": "Banco de la República Colombia",
        "affiliation_city": "Bogota",
        "affiliation_country": "Colombia"
    },
    {
        "paper_title": "Building wildland–urban interface zone resilience through performance-based wildfire engineering. A holistic theoretical framework",
        "paper_author": "Tampekis S.",
        "publication": "Euro-Mediterranean Journal for Environmental Integration",
        "citied_by": "22",
        "cover_date": "2023-09-01",
        "Abstract": "In recent years, a worldwide expansion in the frequency of large, uncontrolled, and catastrophic wildfire events has occurred, creating drastic social, economic, and environmental damage, especially in wildland–urban interface (WUI) zones. This damage includes losses of life, infrastructure, and ecosystem services. The impacts of wildfires at the WUI derive from the complicated and multidimensional interconnected relationships present in the Anthropocene. To enforce resilience of the environment and human communities against wildfires, it is critical to comprehend the local social-ecological systems holistically. In this paper, we present a theoretical framework approach, built on performance-based wildfire engineering, that is envisioned to be a stepping stone towards WUI resilience. To attain this objective, performance benchmarking and design is disaggregated into explicit components of a rigorous mathematical framework. They are linked to a causal inference chain, providing an integrated picture and enabling decision analysis to identify the optimal management strategies based on quantitative parameters. The proposed framework is developed from the total probability theorem and divides the risk assessment into single parts, in particular (1) hazard (wildfire) analysis, (2) social-ecological impact characterization, (3) social-ecological interaction analysis, (4) social-ecological impact analysis, (5) damage analysis, and (6) loss analysis. Therefore, the proposed framework can be applied by emergency agencies directly to assess the performance of society and ecosystem recovery after a wildfire, making emergency management and resilience policy-making more effective.",
        "DOI": "10.1007/s41207-023-00385-z",
        "affiliation_name": "Democritus University of Thrace",
        "affiliation_city": "Komotini",
        "affiliation_country": "Greece"
    },
    {
        "paper_title": "Assessing water scarcity narratives in Brazil – Challenges for urban governance",
        "paper_author": "Lazaro L.L.B.",
        "publication": "Environmental Development",
        "citied_by": "7",
        "cover_date": "2023-09-01",
        "Abstract": "Analyzing water scarcity narratives is important for formulating policies. However, narratives and discourse are not neutral but are deeply embedded in societal contexts. They have the power to influence decision-making processes by legitimizing some solutions and disregarding others, as well as by including or excluding the needs of specific social groups. In this study, we examine water scarcity narratives in Brazil, identify those that perpetuate and support strategic viewpoints and determine which types of narratives possess the most persuasive and coordinative power. Furthermore, we aimed to understand how framing the issue impacts local-level solutions, policies, governance, and water management, specifically emphasizing the state of Sao Paulo. We analyzed a large textual dataset from Brazilian newspaper articles from 2010 to 2021, utilizing machine-learning text classification tools, an essential method in Natural Language Processing. Our findings indicate that the water scarcity narrative in Brazil is multifaceted and encompass various narrative typologies that offer insights into different aspects of the issue. These typologies include narratives on institutional, management/mismanagement, escalating deforestation, and causal factors such as reduced rainfall. These narratives tend to downplay the responsibility of local governments for effective water management and align with climate-centric perspectives. Furthermore, various groups have politically appropriated some narratives, including those promoting denial narratives, which undermine the severity of the water scarcity problem and emphasize the abundance of this resource. On the other hand, water justice narratives bring attention to the challenges of limited access to water and inadequate infrastructure. The COVID-19 pandemic has brought these issues to the forefront revealing the urgent need for equitable water distribution and improved infrastructure.",
        "DOI": "10.1016/j.envdev.2023.100885",
        "affiliation_name": "Faculty of Social Sciences &amp; Health",
        "affiliation_city": "Durham",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Role of Shapley Additive Explanations and Resampling Algorithms for Contract Failure Prediction of Public-Private Partnership Projects",
        "paper_author": "Koc K.",
        "publication": "Journal of Management in Engineering",
        "citied_by": "6",
        "cover_date": "2023-09-01",
        "Abstract": "A public-private partnership (PPP) is a common procurement model implemented worldwide as a catalyst for economic growth and improved public infrastructure. However, due to their inherent characteristics, the risk of failure in some PPP projects is high, causing heavy losses to both entities. Despite distinctive progress being made in PPP projects to reduce their failure probability, there is no proper and effective framework to predict PPP project failure in advance in either developing or in developed countries. The present study aims to develop a machine learning (ML) model to predict the failure of PPP projects to prosper in adverse conditions. This research addresses two critical issues, i.e., class imbalance and interpretability of ML models, that differentiate the current study from data-driven studies to date. First, existing studies usually focused on comparing and selecting the most adequate ML methods, but this study distinctively compared the performances of nine data resampling algorithms. Besides, in order to enhance the interpretability and visibility of the proposed model, a game theory-based feature investigation algorithm, Shapley additive explanations (SHAP), was used to identify not only the most significant features, but also the conditions of the features that cause failure or success in PPP projects. The findings illustrate that the proposed model yielded the highest prediction performance once the data set was resampled with the support vector machine-synthetic minority oversampling technique (SVM-SMOTE). SHAP analysis further shows that unsolicited proposals, domestic credit to the private sector, and project type/subtype have significant impacts on the prediction rationale. Overall, this study contributes to theory through incorporating resampling methods and SHAP algorithm into ML models as well as to practice with an advanced and reliable model to predict the status of PPP projects. The data-driven model and findings are expected to respond to current policy and industry needs by proposing a robust decision-making input for detecting risky PPP projects, allocating resources more effectively based on the most critical failure factors, and promoting the transparency of PPP project outcomes.",
        "DOI": "10.1061/JMENEA.MEENG-5492",
        "affiliation_name": "Yıldız Teknik Üniversitesi",
        "affiliation_city": "Istanbul",
        "affiliation_country": "Turkey"
    },
    {
        "paper_title": "Mass customization with reinforcement learning: Automatic reconfiguration of a production line",
        "paper_author": "Deng J.",
        "publication": "Applied Soft Computing",
        "citied_by": "3",
        "cover_date": "2023-09-01",
        "Abstract": "This paper addresses the problem of efficient automation system configuration for mass customization in industrial manufacturing. Due to the various demands from customers, production lines need to adjust the process parameters of the machines based on specific quality parameters. Reinforcement learning, which learns from samples, can tackle the problem more efficiently than the currently used methods. Based on the proximal policy optimization and centralized training with decentralized execution, a multi-agent reinforcement learning method (MARL) is proposed to reconfigure process parameters of machines based on the changed specifications. The proposed method has the actor of each agent observing only its own state, the agents are made to collaborate by a centralized critic which observes all the states. To evaluate the method, a steel strip rolling line with six collaborating mills is studied. Simulation results show that the proposed method outperforms the existing methods and state-of-the-art multi-agent reinforcement learning methods in terms of accuracy and computing costs.",
        "DOI": "10.1016/j.asoc.2023.110547",
        "affiliation_name": "The State Key Laboratory of Rolling and Automation, Northeastern University",
        "affiliation_city": "Shenyang",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Towards green machine learning for resource allocation in beyond 5G RAN slicing",
        "paper_author": "Oliveira A.",
        "publication": "Computer Networks",
        "citied_by": "1",
        "cover_date": "2023-09-01",
        "Abstract": "The new generation of mobile communications, 5G, has been rolled out. While basic features were successively implemented, more complex ones have been left aside for a new release. Such is the case of RAN slicing, which enables the division of a radio infrastructure into software-controlled logical networks. Among the technical difficulties is the radio resource allocation since slices can be attached to contract agreements with network performance targets, and the number of resources required to support the performance varies with the signal quality of the connected devices. Machine learning solutions can predict the number of radio resources needed based on the current state of the network. Our previous work, KPI-Converter, addressed this issue with densely connected neural networks. However, amid the current global energy crisis, we should focus on more green machine learning solutions that can achieve similar performance with much lower usage of computational resources. In this work, we present KPIC-Lite, a solution for resource allocation for RAN slicing that consumes 700 to 1000 times fewer resources than our previous work while performing similarly in most tested scenarios. We introduce a new asymmetric loss function that significantly boosts convergence compared to a-OMC, the state-of-the-art loss function for operator monetary costs. Another bonus compared to a-OMC is its ability to use second-order optimisers efficiently. The second-order optimiser used in our work reduced the computational resources of the solution.",
        "DOI": "10.1016/j.comnet.2023.109877",
        "affiliation_name": "Instituto de Engenharia de Sistemas e Computadores: Investigação e Desenvolvimento em Lisboa",
        "affiliation_city": "Lisbon",
        "affiliation_country": "Portugal"
    },
    {
        "paper_title": "Evaluating global intelligence innovation: An index based on machine learning methods",
        "paper_author": "Ma X.",
        "publication": "Technological Forecasting and Social Change",
        "citied_by": "5",
        "cover_date": "2023-09-01",
        "Abstract": "This study investigates national intelligence innovation through machine learning methods. We propose a global intelligence innovation index (GIII) to evaluate the global landscape of intelligence innovation of 101 countries around the world. First, we develop a conceptual framework of national intelligence innovation based on the innovation ecosystem theory to construct GIII. Second, we measure GIII based on machine learning methods, including the k-means clustering algorithm and the random forest model. Finally, we evaluate the national intelligence innovation using GIII and provide theoretical and practical insights. The results show that global intelligence innovation development presents a convoluted situation, as high income doesn't necessarily promote intelligence innovation. Furthermore, intelligence innovation shows interesting relationships with unemployment, aging, and shares of economic sectors. GIII provides a reference to the level of intelligence innovation in various countries around the world and helps decision-makers better formulate policies to facilitate intelligence innovation development.",
        "DOI": "10.1016/j.techfore.2023.122736",
        "affiliation_name": "Beijing Foreign Studies University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Cultural, creative, and complex: A computational foundation of culture-driven urban governance",
        "paper_author": "Grossi E.",
        "publication": "Cities",
        "citied_by": "6",
        "cover_date": "2023-09-01",
        "Abstract": "Culture and creative production have an important but somewhat elusive role in urban development. None of the many conceptual paradigms that have been proposed so far to explain it has turned out entirely satisfactory. We argue that the main reason behind this failure is the implicit linear thinking that informs all these approaches: namely, the idea that a few, major drivers explain urban development through a direct, clearly readable systemic impact. Cities are complex socio-environmental systems whose functioning depends on the concurrent interaction of many different factors which cannot be reduced to the action of a few, simple causal forces. Failing to understand such complexity easily leads to dysfunctional urban governance approaches. In this paper, we analyze a database of 144 European cities as described by 58 variables belonging to different domains, as designed by a preliminary stage of the Cultural and Creative City Monitor (CCCM). Our analysis builds on innovative machine learning techniques (PST) and on the Minimum Spanning Tree (MST) representation to map the structural interdependencies between the cultural and non-cultural sectors in cities with a strong cultural policy orientation. This toolbox carries considerable potential for precision cultural policies and data-driven urban governance strategies of the future.",
        "DOI": "10.1016/j.cities.2023.104437",
        "affiliation_name": "Consiglio Nazionale delle Ricerche",
        "affiliation_city": "Rome",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Exploring treatment effect heterogeneity of a PROMs alert intervention in knee and hip arthroplasty patients: A causal forest application",
        "paper_author": "Langenberger B.",
        "publication": "Computers in Biology and Medicine",
        "citied_by": "3",
        "cover_date": "2023-09-01",
        "Abstract": "Patient reported outcome measures (PROMs) experience an uptake in use for hip (HA) and knee arthroplasty (KA) patients. As they may be used for patient monitoring interventions, it remains unclear whether their use in HA/KA patients is effective, and which patient groups benefit the most. Nonetheless, knowledge about treatment effect heterogeneity is crucial for decision makers to target interventions towards specific subgroups that benefit to a greater extend. Therefore, we evaluate the treatment effect heterogeneity of a remote PROM monitoring intervention that includes ∼8000 HA/KA patients from a randomized controlled trial conducted in nine German hospitals. The study setting gave us the unique opportunity to apply a causal forest, a recently developed machine learning method, to explore treatment effect heterogeneity of the intervention. We found that among both HA and KA patients, the intervention was especially effective for patients that were female, >65 years of age, had a blood pressure disease, were not working, reported no backpain and were adherent. When transferring the study design into standard care, policy makers should make use of the knowledge obtained in this study and allocate the treatment towards subgroups for which the treatment is especially effective.",
        "DOI": "10.1016/j.compbiomed.2023.107118",
        "affiliation_name": "University of St. Gallen",
        "affiliation_city": "St Gallen",
        "affiliation_country": "Switzerland"
    },
    {
        "paper_title": "Reinforcement learning for energy-efficient control of parallel and identical machines",
        "paper_author": "Loffredo A.",
        "publication": "CIRP Journal of Manufacturing Science and Technology",
        "citied_by": "3",
        "cover_date": "2023-09-01",
        "Abstract": "Nowadays, the growing interest in industry for enhancing the sustainability of manufacturing processes is becoming a major trend. Energy consumption can be lowered by controlling machine states with energy-efficient control policies that switch off/on the device. Recent studies have shown that Reinforcement Learning algorithms can effectively control manufacturing systems without the requirement of prior knowledge about system parameters. This is a significant factor since full information on system dynamics is difficult to obtain in real-world applications. This work proposes a new Reinforcement Learning-based algorithm to apply energy-efficient control strategies to a single workstation consisting of identical parallel machines. The model goal is to achieve the optimum trade-off between system productivity and energy demand without relying on full knowledge of the system dynamics. Numerical experiments confirm effectiveness, applicability, and generality of the proposed approach, even when applied to a real-world industrial system from the automotive sector.",
        "DOI": "10.1016/j.cirpj.2023.05.007",
        "affiliation_name": "Karlsruher Institut für Technologie",
        "affiliation_city": "Karlsruhe",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Active control of flexible rotors using deep reinforcement learning with application of multi-actor-critic deep deterministic policy gradient",
        "paper_author": "Ahmed M.H.",
        "publication": "Engineering Applications of Artificial Intelligence",
        "citied_by": "4",
        "cover_date": "2023-09-01",
        "Abstract": "Vibration in rotating machinery is one of the main causes of machine failure. Active and passive control methods have been developed in order to reduce vibration levels and extend the operating speeds of machines. These controllers are either manually tuned or tuned during operation using adaptive control techniques and are tailored to a single vibration source. In the field of artificial intelligence, deep reinforcement learning has greatly impacted the field of continuous control, from mastering simple games to controlling multiple actuators in a robot doing complex tasks. Deep reinforcement learning agents are capable of finding optimal control policies without a model of the underlying system. This work proposes the multi-actor-critic deep deterministic policy gradient (MAC-DDPG) algorithm by integrating multiple criteria to train concurrent actors in a periodic system. Using the frequency footprint of each type of vibration, a cost function is designed to train the critics and actors. The proposed controller is evaluated on a test rig supported by two patented Smart Electro-Magnetic Actuator Journal Integrated Bearings (SEMAJIB). The proposed controller is capable of finding optimal control policies for reducing the synchronous vibration caused by the rotor's unbalance and stabilizing a system with oil whip vibration. A derivative controller actor and a harmonic actor are used concurrently for controlling the vibrations. The proposed controller is able to reach unbalance vibration reduction up to 93%. In addition, the proposed controller is successful in completely eliminating oil whip instability with up to 99% reduction.",
        "DOI": "10.1016/j.engappai.2023.106593",
        "affiliation_name": "Faculty of Engineering",
        "affiliation_city": "Cairo",
        "affiliation_country": "Egypt"
    },
    {
        "paper_title": "Grassland mowing event detection using combined optical, SAR, and weather time series",
        "paper_author": "Holtgrave A.K.",
        "publication": "Remote Sensing of Environment",
        "citied_by": "11",
        "cover_date": "2023-09-01",
        "Abstract": "The European Union's Common Agricultural Policy (CAP) and the Habitats Directive aim to improve biodiversity in agricultural landscapes. Both policies require enormous monitoring, which can be facilitated by remote sensing. Use intensity, measured by mowing frequency is an important indicator of biodiversity in permanent grasslands. The frequency and timing of mowing can be determined using satellite remote sensing because photosynthetically active biomass changes rapidly in response to mowing. However, the rapid regrowth of grasses requires very dense satellite time series for reliable detection. Radar time series can complement optical time series and fill in cloud-related gaps to overcome this problem. Additional weather data can support the detection of grassland mowing events, as mowing events are associated with specific meteorological conditions. However, previous studies have not fully exploited both potentials or different machine learning approaches for mowing event detection. This study presents a new transferable two-step approach to detect grassland mowing events using combined optical and SAR data and additional weather data. First, we filled cloud-related gaps in optical time series using a supervised machine learning regression with optical and SAR data. We then classified time series sequences of optical, SAR and weather data into mown and unmown using four different machine learning algorithms. We used time series of NDVI and EVI (combined Sentinel-2 and Landsat 8), SAR backscatter, six-day interferometric coherence, backscatter radar vegetation index, backscatter cross-ratio (Sentinel-1), and temperature and precipitation sums. Our test sites are distributed across Germany and cover the entire gradient of grassland use intensities. Mowing events could be detected with F1 values of up to 89%, first cut with up to 94%. Our results show no structural advantage of infilling time series with machine learning over linearly interpolated time series. The combined Sentinel-2 and Landsat-8 time series provided dense time series with mostly median gaps less than 20 days, which proved sufficient to reliably detect mowing events. SAR data were not essential for mowing event detection in our study, but weather data improved classification results for models trained on all areas and years. However, when the model was transferred to unknown years or areas that were not used for training, SAR data improved detection accuracy, whereas weather data degrade it. Models trained on all years but not all study sites detected mowing events with an accuracy of up to F1 = 76%. Models trained with all regions but not all years detected mowing events in untrained years with F1 up to 80%.",
        "DOI": "10.1016/j.rse.2023.113680",
        "affiliation_name": "Technische Universität Berlin",
        "affiliation_city": "Berlin",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "PLSNet: Position-aware GCN-based autism spectrum disorder diagnosis via FC learning and ROIs sifting",
        "paper_author": "Wang Y.",
        "publication": "Computers in Biology and Medicine",
        "citied_by": "11",
        "cover_date": "2023-09-01",
        "Abstract": "Brain function connectivity, derived from functional magnetic resonance imaging (fMRI), has enjoyed high popularity in the studies of Autism Spectrum Disorder (ASD) diagnosis. Albeit rapid progress has been made, most studies still suffer from several knotty issues: (1) the hardship of modeling the sophisticated brain neuronal connectivity; (2) the mismatch of identically graph node setup to the variations of different brain regions; (3) the dimensionality explosion resulted from excessive voxels in each fMRI sample; (4) the poor interpretability giving rise to unpersuasive diagnosis. To ameliorate these issues, we propose a position-aware graph-convolution-network-based model, namely PLSNet, with superior accuracy and compelling built-in interpretability for ASD diagnosis. Specifically, a time-series encoder is designed for context-rich feature extraction, followed by a function connectivity generator to model the correlation with long range dependencies. In addition, to discriminate the brain nodes with different locations, the position embedding technique is adopted, giving a unique identity to each graph region. We then embed a rarefying method to sift the salient nodes during message diffusion, which would also benefit the reduction of the dimensionality complexity. Extensive experiments conducted on Autism Brain Imaging Data Exchange demonstrate that our PLSNet achieves state-of-the-art performance. Notably, on CC200 atlas, PLSNet reaches an accuracy of 76.4% and a specificity of 78.6%, overwhelming the previous state-of-the-art with 2.5% and 6.5% under five-fold cross-validation policy. Moreover, the most salient brain regions predicted by PLSNet are closely consistent with the theoretical knowledge in the medical domain, providing potential biomarkers for ASD clinical diagnosis. Our code is available at https://github.com/CodeGoat24/PLSNet.",
        "DOI": "10.1016/j.compbiomed.2023.107184",
        "affiliation_name": "Shandong First Medical University &amp; Shandong Academy of Medical Sciences",
        "affiliation_city": "Jinan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Dealing with Changes: Resilient Routing via Graph Neural Networks and Multi-Agent Deep Reinforcement Learning",
        "paper_author": "Bhavanasi S.S.",
        "publication": "IEEE Transactions on Network and Service Management",
        "citied_by": "15",
        "cover_date": "2023-09-01",
        "Abstract": "The computer networking community has been steadily increasing investigations into machine learning to help solve tasks such as routing, traffic prediction, and resource management. The traditional best-effort nature of Internet connections allows a single link to be shared among multiple flows competing for network resources, often without consideration of in-network states. In particular, due to the recent successes in other applications, Reinforcement Learning has seen steady growth in network management and, more recently, routing. However, if there are changes in the network topology, retraining is often required to avoid significant performance losses. This restriction has chiefly prevented the deployment of Reinforcement Learning-based routing in real environments. In this paper, we approach routing as a reinforcement learning problem with two novel twists: minimize flow set collisions, and construct a reinforcement learning policy capable of routing in dynamic network conditions without retraining. We compare this approach to other routing protocols, including multi-agent learning, with respect to various Quality-of-Service metrics, and we report our lesson learned.",
        "DOI": "10.1109/TNSM.2023.3287936",
        "affiliation_name": "Saint Louis University School of Science and Engineering",
        "affiliation_city": "St. Louis",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Applications of Reinforcement Learning for maintenance of engineering systems: A review",
        "paper_author": "Marugán A.P.",
        "publication": "Advances in Engineering Software",
        "citied_by": "19",
        "cover_date": "2023-09-01",
        "Abstract": "Nowadays, modern engineering systems require sophisticated maintenance strategies to ensure their correct performance. Maintenance has become one of the most important tasks of the systems lifecycle. This paper presents a literature review of the application of Reinforcement Learning algorithms for the maintenance of engineering systems. Reinforcement Learning-based maintenance has been classified regarding four types of system: transportation systems, manufacturing and production systems, civil infrastructures, power and energy systems, and other systems. Based on the literature review, this paper includes an overall analysis of the current state and a discussion of main limitations, challenges, and future trends in this field. A summary table is provided to present clearly the most important references. This research work demonstrates that Reinforcement Learning algorithms have a great potential for generating maintenance policies, outperforming most conventional strategies.",
        "DOI": "10.1016/j.advengsoft.2023.103487",
        "affiliation_name": "CUNEF University",
        "affiliation_city": "Madrid",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "Factors affecting phase change in coupling coordination between population, crop yield, and soil erosion in China's 281 cities",
        "paper_author": "Wang J.",
        "publication": "Land Use Policy",
        "citied_by": "13",
        "cover_date": "2023-09-01",
        "Abstract": "Soil erosion in cropland areas is mainly influenced by agricultural activities and natural conditions. Previous studies have largely focused on the biophysical processes or economic drivers of soil erosion. There have been few attempts to balance the impacts of population, agricultural production, and soil erosion to address the global socioecological predicament facing cropland. We combined the coupling coordination degree model (CCDM) with the Shapley additive explanations (SHAP) method to evaluate the coupling coordination level between population demand, agricultural production, and soil erosion as well as the influence of socioeconomic factors in 281 Chinese cities for the period from 1995 to 2010. Coupling between population, crop yield, and soil erosion was generally moderate across China during 1995–2010. Cities with a GDP in the range of 4.42–241.54 billion could fall into different coupling coordination phases that were identified by K-means clustering. The SHAP results showed that GDP and population density were the most important factors influencing the coordination level, while industrial structure was the key determinant that distinguished the different phases in cities with a similar economic status. Building on research on the evolutionary aspects of system coupling coordination, our study reveals for the first time the probable causes of changes in system coupling coordination via machine learning algorithms, providing a reference for future investigations. Our findings also provide a basis for developing policy recommendations to balance social demands, agriculture, and environmental protection.",
        "DOI": "10.1016/j.landusepol.2023.106761",
        "affiliation_name": "Huazhong Agricultural University",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Dynamic nowcast of the New Zealand greenhouse gas inventory",
        "paper_author": "Jones M.",
        "publication": "Environmental Modelling and Software",
        "citied_by": "0",
        "cover_date": "2023-09-01",
        "Abstract": "As efforts to mitigate the effects of climate change grow, reliable and thorough reporting of greenhouse gas emissions are essential for measuring progress towards international and domestic emissions reductions targets. New Zealand's national emissions inventories are currently reported between 15 to 27 months out-of-date. We present a machine learning approach to nowcast (dynamically estimate) national greenhouse gas emissions in New Zealand in advance of the national emissions inventory's release, with just a two month latency due to current data availability. Key findings include an estimated 0.2% decrease in national gross emissions since 2020 (as at July 2022). Our study highlights the predictive power of a dynamic view of emissions intensive activities. This methodology is a proof of concept that a machine learning approach can make sub-annual estimates of national greenhouse gas emissions by sector with a relatively low error that could be of value for policy makers.",
        "DOI": "10.1016/j.envsoft.2023.105745",
        "affiliation_name": "Level One",
        "affiliation_city": "Wellington Central",
        "affiliation_country": "New Zealand"
    },
    {
        "paper_title": "AUV 3D docking control using deep reinforcement learning",
        "paper_author": "Zhang T.",
        "publication": "Ocean Engineering",
        "citied_by": "12",
        "cover_date": "2023-09-01",
        "Abstract": "Autonomous docking can enable AUV to have long endurance, so it is necessary to consider the issue of robust docking control under current and wave disturbances. In this work, based on the proximal policy optimization (PPO) algorithm, we developed a model-free docking controller to complete three-dimensional docking tasks under disturbances. To improve the performance of PPO, two mechanisms are proposed, including adaptive rollback clipping and self-generated demonstration replay. A simulation environment is constructed, including fuzzy hydrodynamic parameters, ocean current and wave disturbance model. Simulation results demonstrate that our proposed method has faster learning speed, higher robustness, and can control AUV to achieve 3D docking tasks in complex environments with a high success rate.",
        "DOI": "10.1016/j.oceaneng.2023.115021",
        "affiliation_name": "Ludong University",
        "affiliation_city": "Yantai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Gaussian Differential Privacy Integrated Machine Learning Model for Industrial Internet of Things",
        "paper_author": "Lazar A.J.P.",
        "publication": "SN Computer Science",
        "citied_by": "3",
        "cover_date": "2023-09-01",
        "Abstract": "Agriculture, energy, mining, healthcare, and transportation are a few of the top industries transformed by the industrial internet of things (IIoT). Industry 4.0 mainly relies on machine learning (ML) to use the vast interconnectedness and large amounts of IIoT data that IIoT primarily drives. ML approaches are trained on confidential data generated by the IIoT environment often exposes privacy to adversarial assaults. Blockchain-based ML is established in the proposed secured IIoT research work to safeguard and enhance privacy. This research proposes a Gaussian differential privacy-integrated machine learning model (GDPIMLM), created for a scalable and controlled IIoT system, fully connected (FC) layer, implemented blockchain benefits to create a privacy-preserving mechanism while considering other limitations and reasonable time. A probability test using two altered Gaussian distributions (GD) forms the basis for defining Gaussian differential privacy (GDP). Due to a central restriction theorem for differential privacy, GDP is the primary privacy term within the family of Federal differential privacy (f-DP) policies. Ethereum is used to implement experimental evaluations. Data collections show that the recommended method enhances digital data privacy with industry-leading security without reducing performance. The performance analysis of the proposed model of secured industrial internet of things (IIoT) is studied using the accuracy of comparison with existing methods and for diverse distributed entities (DE). The proposed secured IIoT attains a minimal time consumption (TC) of 790 Sec—the highest accuracy of 92%, outperforming the existing method.",
        "DOI": "10.1007/s42979-023-01820-2",
        "affiliation_name": "CMR Institute of Technology, Hyderabad",
        "affiliation_city": "Hyderabad",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Addressing structural hurdles for metadata extraction from environmental impact statements",
        "paper_author": "Laparra E.",
        "publication": "Journal of the Association for Information Science and Technology",
        "citied_by": "0",
        "cover_date": "2023-09-01",
        "Abstract": "Natural language processing techniques can be used to analyze the linguistic content of a document to extract missing pieces of metadata. However, accurate metadata extraction may not depend solely on the linguistics, but also on structural problems such as extremely large documents, unordered multi-file documents, and inconsistency in manually labeled metadata. In this work, we start from two standard machine learning solutions to extract pieces of metadata from Environmental Impact Statements, environmental policy documents that are regularly produced under the US National Environmental Policy Act of 1969. We present a series of experiments where we evaluate how these standard approaches are affected by different issues derived from real-world data. We find that metadata extraction can be strongly influenced by nonlinguistic factors such as document length and volume ordering and that the standard machine learning solutions often do not scale well to long documents. We demonstrate how such solutions can be better adapted to these scenarios, and conclude with suggestions for other NLP practitioners cataloging large document collections.",
        "DOI": "10.1002/asi.24809",
        "affiliation_name": "The University of Arizona",
        "affiliation_city": "Tucson",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Certified reinforcement learning with logic guidance",
        "paper_author": "Hasanbeig H.",
        "publication": "Artificial Intelligence",
        "citied_by": "5",
        "cover_date": "2023-09-01",
        "Abstract": "Reinforcement Learning (RL) is a widely employed machine learning architecture that has been applied to a variety of control problems. However, applications in safety-critical domains require a systematic and formal approach to specifying requirements as tasks or goals. We propose a model-free RL algorithm that enables the use of Linear Temporal Logic (LTL) to formulate a goal for unknown continuous-state/action Markov Decision Processes (MDPs). The given LTL property is translated into a Limit-Deterministic Generalised Büchi Automaton (LDGBA), which is then used to shape a synchronous reward function on-the-fly. Under certain assumptions, the algorithm is guaranteed to synthesise a control policy whose traces satisfy the LTL specification with maximal probability.",
        "DOI": "10.1016/j.artint.2023.103949",
        "affiliation_name": "Magdalen College",
        "affiliation_city": "Oxford",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Policy ensemble gradient for continuous control problems in deep reinforcement learning",
        "paper_author": "Liu G.",
        "publication": "Neurocomputing",
        "citied_by": "4",
        "cover_date": "2023-09-01",
        "Abstract": "Policy gradient algorithms for reinforcement learning (RL) have successfully tackled a broad range of high-dimensional continuous RL problems, including many challenging robotic control problems. These algorithms can be largely divided into two categories, i.e., on-policy algorithms and off-policy algorithms. Off-policy deep RL (DRL) algorithms enjoy better sample efficiency than and often outperform on-policy algorithms. However, cutting-edge off-policy algorithms still suffer from the low-quality estimation of policy gradients, resulting in compromised learning performance and high sensitivity to hyper-parameter settings. To address this issue, we propose a new concept of robust policy gradient (RPG). Driven by RPG, this paper further develops a new policy ensemble gradient (PEG) algorithm for DRL, inspired by the recent success of several ensemble DRL algorithms. PEG efficiently and effectively estimates RPG by using multiple policy gradients obtained respectively from several off-policy base learners in an ensemble. The estimated RPG is then utilized for training all base learners simultaneously. Comprehensive experiments have been performed on six Mujoco benchmark problems. Compared to four state-of-the-art off-policy algorithms and four cutting-edge ensemble policy gradient algorithms, our new PEG algorithm achieved highly competitive stability, performance and sample efficiency. Further analysis shows that PEG is insensitive to varied hyper-parameter settings, confirming the positive role of RPG in building reliable and effective off-policy DRL algorithms.",
        "DOI": "10.1016/j.neucom.2023.126381",
        "affiliation_name": "National Institute of Water and Atmospheric Research",
        "affiliation_city": "Auckland",
        "affiliation_country": "New Zealand"
    },
    {
        "paper_title": "Short-term prediction of wave height based on a deep learning autoregressive integrated moving average model",
        "paper_author": "Ban W.",
        "publication": "Earth Science Informatics",
        "citied_by": "6",
        "cover_date": "2023-09-01",
        "Abstract": "Effective wave height prediction is crucial for ocean development, marine planning, and other ocean-related projects in coastal areas. A novel hybrid ARIMA-LSTM model is proposed, combining the strengths of Autoregressive Integrated Moving Average (ARIMA) in modeling linear relationships and Long Short-Term Memory (LSTM) in capturing non-linear components within time series data. Applied to Hangzhou Bay and Zhoushan Lianghengshan area data, the ARIMA-LSTM model outperforms traditional ARIMA, Support Vector Machine (SVM), LSTM, and Backpropagation (BP) neural network models across different stations, time periods, and typhoon scenarios. This innovative approach provides valuable technical support for accurate short-term effective wave height predictions.",
        "DOI": "10.1007/s12145-023-01023-6",
        "affiliation_name": "Zhejiang Ocean University",
        "affiliation_city": "Zhoushan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Digital health, digital medicine, and digital therapeutics in cardiology: current evidence and future perspective in Japan",
        "paper_author": "Nomura A.",
        "publication": "Hypertension Research",
        "citied_by": "14",
        "cover_date": "2023-09-01",
        "Abstract": "Ten years passed since Japan set out the Action Plan of Growth Strategy that declared the initiatives of digitalization for medicine, nursing care, and healthcare to achieve the world’s most advanced medical care. The initiatives formed the foundation of the Japanese national strategy and have been continuously refined, resulting in the current environment of digital health and digital medicine. Digital health–related terminologies are organized, such as “digital health,” “digital medicine,” and “digital therapeutics” (DTx), as well as several common digital technologies, including artificial intelligence, machine learning, and mobile health (mHealth). DTx is included in mHealth and is a novel disease treatment option. Also, this article thoroughly describes DTx in Japan and compares it with those in the US and Germany, the leading countries in digital health–related policies, regulations, and their development status. In Japan, two of three DTx applications that have been approved and reimbursed by the Ministry of Health, Labor, and Welfare are explained in detail in relation to cardiovascular medicine. When added to a standard smoking cessation program, the DTx system for nicotine dependence significantly improved the continuous abstinence rate. Moreover, the DTx for hypertension together with the guideline-based hypertension management was effective in patients aged 65 years or younger who were diagnosed with essential hypertension without antihypertensive agents, and it was also found to be cost-effective. DTx in cardiovascular medicine, with consideration on safety, efficacy, and cost-effectiveness, could be widely used not only through basic experiments and clinical studies but also through social implementation. [Figure not available: see fulltext.].",
        "DOI": "10.1038/s41440-023-01317-8",
        "affiliation_name": "Kanazawa University Graduate School of Medical Sciences",
        "affiliation_city": "Kanazawa",
        "affiliation_country": "Japan"
    },
    {
        "paper_title": "Oil futures volatility prediction: Bagging or combination?",
        "paper_author": "Lyu Z.",
        "publication": "International Review of Economics and Finance",
        "citied_by": "2",
        "cover_date": "2023-09-01",
        "Abstract": "This paper compares the predictive performance of the bagging method and traditional combination models for forecasting oil futures volatility, using economic policy uncertainty (EPU) indices and macroeconomic variables as predictors. Our empirical findings indicate that the bagging method outperforms the conventional combination models, demonstrating the effectiveness of machine learning combination models. These results are confirmed by different evaluation methods, alternative forecasting methods, and alternative oil futures, and hold up during the COVID-19 pandemic and various business cycles. Furthermore, we show that EPU indices are more useful than macroeconomic variables for forecasting oil volatility during the COVID-19 pandemic. Thus, our analysis provides new insights into combination forecasts.",
        "DOI": "10.1016/j.iref.2023.05.007",
        "affiliation_name": "Southwest Jiaotong University",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Stock market volatility prediction: Evidence from a new bagging model",
        "paper_author": "Luo Q.",
        "publication": "International Review of Economics and Finance",
        "citied_by": "1",
        "cover_date": "2023-09-01",
        "Abstract": "The purpose of this study is to investigate which model can improve the precision of the categorical economic policy uncertainty indices in predicting volatility in the U.S. stock market. In this study, a new model is constructed by combining autoregressive model and bagging method. The empirical outcomes indicate that machine learning models outperform traditional forecasting models and that the new model constructed in this study has the best forecasting ability. We perform robustness tests using an alternative stock index, alternative forecasting windows, and different economic cycles. The results show that these findings are robust. We hope to provide new insights into the application of the bagging method in stock market volatility forecasting.",
        "DOI": "10.1016/j.iref.2023.05.008",
        "affiliation_name": "Nanjing University of Finance and Economics",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Unraveling the O<inf>3</inf>-NO<inf>X</inf>-VOCs relationships induced by anomalous ozone in industrial regions during COVID-19 in Shanghai",
        "paper_author": "Lu B.",
        "publication": "Atmospheric Environment",
        "citied_by": "13",
        "cover_date": "2023-09-01",
        "Abstract": "The COVID-19 pandemic promoted strict restrictions to human activities in China, which led to an unexpected increase in ozone (O3) regarding to nitrogen oxides (NOx) and volatile organic compounds (VOCs) co-abatement in urban China. However, providing a quantitative assessment of the photochemistry that leads to O3 increase is still challenging. Here, we evaluated changes in O3 arising from photochemical production with precursors (NOX and VOCS) in industrial regions in Shanghai during the COVID-19 lockdowns by using machine learning models and box models. The changes of air pollutants (O3, NOX, VOCs) during the COVID-19 lockdowns were analyzed by deweathering and detrending machine learning models with regard to meteorological and emission effects. After accounting for effects of meteorological variability, we find increase in O3 concentration (49.5%). Except for meteorological effects, model results of detrending the business-as-usual changes indicate much smaller reduction (−0.6%), highlighting the O3 increase attributable to complex photochemistry mechanism and the upward trends of O3 due to clear air policy in Shanghai. We then used box models to assess the photochemistry mechanism and identify key factors that control O3 production during lockdowns. It was found that empirical evidence for a link between efficient radical propagation and the optimized O3 production efficiency of NOX under the VOC-limited conditions. Simulations with box models also indicate that priority should be given to controlling industrial emissions and vehicle exhaust while the VOCs and NOX should be managed at a proper ratio in order to control O3 in winter. While lockdown is not a condition that could ever be continued indefinitely, findings of this study offer theoretical support for formulating refined O3 management in industrial regions in Shanghai, especially in winter.",
        "DOI": "10.1016/j.atmosenv.2023.119864",
        "affiliation_name": "Shandong University",
        "affiliation_city": "Jinan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "State-level politicization of crisis communication on Twitter during COVID-19: Conceptualization, measurement, and impacts",
        "paper_author": "Hu Q.",
        "publication": "Public Administration Review",
        "citied_by": "4",
        "cover_date": "2023-09-01",
        "Abstract": "The political dimension of crisis communication remains understudied in public administration. We defined the politicization of government crisis communication as the employment of politics-oriented communication strategies in crisis messaging. We further examined the state-level politicization occurring during COVID-19 and its influence on public engagement and policy compliance. We applied machine learning algorithms to analyze 43,642 Twitter messages posted by fifty US state governors, assessing the extent to which these governors politicized crisis communication. We compiled data from multiple sources to explore the influence of communication politicization on public engagement and compliance behaviors. While most governors showed major concerns regarding reputation and blame, their level of politicization and selection of communication strategies varied. Increased levels of communication politicization discouraged the public's online engagement and policy compliance. Excessive levels of political consideration could undermine the legitimacy and effectiveness of government crisis communication, and thus an examination of their relationship was essential.",
        "DOI": "10.1111/puar.13653",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Deep Robot Sketching: An application of Deep Q-Learning Networks for human-like sketching",
        "paper_author": "Fernandez-Fernandez R.",
        "publication": "Cognitive Systems Research",
        "citied_by": "1",
        "cover_date": "2023-09-01",
        "Abstract": "The current success of Reinforcement Learning algorithms for its performance in complex environments has inspired many recent theoretical approaches to cognitive science. Artistic environments are studied within the cognitive science community as rich, natural, multi-sensory, multi-cultural environments. In this work, we propose the introduction of Reinforcement Learning for improving the control of artistic robot applications. Deep Q-learning Neural Networks (DQN) is one of the most successful algorithms for the implementation of Reinforcement Learning in robotics. DQN methods generate complex control policies for the execution of complex robot applications in a wide set of environments. Current art painting robot applications use simple control laws that limits the adaptability of the frameworks to a set of simple environments. In this work, the introduction of DQN within an art painting robot application is proposed. The goal is to study how the introduction of a complex control policy impacts the performance of a basic art painting robot application. The main expected contribution of this work is to serve as a first baseline for future works introducing DQN methods for complex art painting robot frameworks. Experiments consist of real world executions of human drawn sketches using the DQN generated policy and TEO, the humanoid robot. Results are compared in terms of similarity and obtained reward with respect to the reference inputs.",
        "DOI": "10.1016/j.cogsys.2023.05.004",
        "affiliation_name": "Universidad Complutense de Madrid",
        "affiliation_city": "Madrid",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "A Robot Motion Learning Method Using Broad Learning System Verified by Small-Scale Fish-Like Robot",
        "paper_author": "Xu S.",
        "publication": "IEEE Transactions on Cybernetics",
        "citied_by": "19",
        "cover_date": "2023-09-01",
        "Abstract": "The widespread application of learning-based methods in robotics has allowed significant simplifications to controller design and parameter adjustment. In this article, robot motion is controlled with learning-based methods. A control policy using a broad learning system (BLS) for robot point-reaching motion is developed. A sample application based on a magnetic small-scale robotic system is designed without detailed mathematical modeling of the dynamic systems. The parameter constraints of the nodes in the BLS-based controller are derived based on Lyapunov theory. The design and control training processes for a small-scale magnetic fish motion are presented. Finally, the effectiveness of the proposed method is demonstrated by convergence of the artificial magnetic fish motion to the targeted area with the BLS trajectory, successfully avoiding obstacles.",
        "DOI": "10.1109/TCYB.2023.3269773",
        "affiliation_name": "Shenzhen Institute of Advanced Technology",
        "affiliation_city": "Shenzhen",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Investigation of energy management strategy for a novel electric-hydraulic hybrid vehicle: Self-adaptive electric-hydraulic ratio",
        "paper_author": "Hong J.",
        "publication": "Energy",
        "citied_by": "17",
        "cover_date": "2023-09-01",
        "Abstract": "The innovation and development of energy management strategies attract more and more attention as a key technology in hybrid electric vehicles. This paper focuses on a novel type of electric-hydraulic hybrid vehicle with multiple working modes and zero emissions, which offers a deeper potential for energy efficiency. Steady-state simulation with a rule-based mode switching strategy verifies that the electric-hydraulic ratio in the hybrid driving mode can interfere with energy management performance. Committed to filling the literature gap on the electric-hydraulic ratio, this paper proposes the idea of combining deep reinforcement learning with a rule-based control strategy and employs the Twin Delayed Deep Deterministic Policy Gradient to control the electric-hydraulic ratio. Thereafter, an energy management strategy framework based on the self-adaptive electric-hydraulic ratio was developed. Offline training and online test can demonstrate that the proposed energy management strategy enables the self-adaptive electric-hydraulic ratio under various driving cycles and a significant reduction in the energy consumption rate. The research findings in this paper are the crystallization of traditional control strategy and advanced algorithm, which has stronger practical value and development prospects. This is the first time that the self-adaptive electric-hydraulic ratio is applied to the establishment of energy management strategies.",
        "DOI": "10.1016/j.energy.2023.127582",
        "affiliation_name": "Qingdao University",
        "affiliation_city": "Qingdao",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A systematic method of remaining useful life estimation based on physics-informed graph neural networks with multisensor data",
        "paper_author": "He Y.",
        "publication": "Reliability Engineering and System Safety",
        "citied_by": "20",
        "cover_date": "2023-09-01",
        "Abstract": "Data-driven models, especially deep learning models, are proposed for remaining useful life (RUL) estimation with multisensor signals. Various treatments to reduce data sensitivity, addressing the difficulty of learning dynamic topologies, and coping with the lack of engineering physics guidance for model training limit the performance of these models and their use. This study proposes a systematic method to estimate RUL with multisensory data under dynamic operating conditions and multiple failure modes. Firstly, ARMA regression is introduced into the graph convolutional network(GCN) model. This allows the information loss in the GCN model following training to be lifted with low computational complexity. Secondly, the physics equations of balancing for economy and security in preventive maintenance policies is introduced in the loss function for training. This involves in a way to impose a higher penalty on delayed predictions, so to focus the neural network training on the control of high-risk situations. Finally, the method is validated on the popular C-MAPSS dataset. Compared with other cutting-edge methods, the proposed method can ensure high-fitting accuracy with strong security. In practice, the controllability and flexibility of deep learning models are enhanced, ensuring the reduction of high-risk, uncertain situations while sacrificing as little accuracy as possible.",
        "DOI": "10.1016/j.ress.2023.109333",
        "affiliation_name": "Dongguan University of Technology",
        "affiliation_city": "Dongguan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "MARCEL: Mobile active rover chassis for enhanced locomotion",
        "paper_author": "Bouton A.",
        "publication": "Journal of Field Robotics",
        "citied_by": "4",
        "cover_date": "2023-09-01",
        "Abstract": "To extend planetary exploration beyond the current limitations of wheeled vehicles while preserving reliability, simplicity, and efficiency, actuation can be judiciously incorporated into the locomotion system. Based on a static analysis, we propose a new four-wheeled chassis concept for planetary rovers that can traverse more challenging terrain with the help of two internal active joints. These joints are arranged as follows: a vertical pivot articulates the chassis around its center while a bogie allows the rear wheels to rotate around the longitudinal axis of the vehicle. We also introduce a control method that uses a two-stage procedure to produce an interpretable controller based on a policy devised by reinforcement learning. This way, we eliminate the black box made of a neural network and facilitate the transfer from simulation to reality. The resulting controller efficiently harnesses the internal mobility of the chassis to climb over obstacles in a sequenced manner while relying only on proprioceptive data provided by the chassis. A rover prototype named MARCEL has been built and tested experimentally. Contrary to any state-of-the-art six-wheeled passive chassis, the proposed locomotion system and its associated control has proven to be able to overcome solid step obstacles as tall as the diameter of the wheels with a (Figure presented.) edge and a friction coefficient as low as 0.5. This simple but capable design will enable future missions to explore more challenging areas while providing better guarantees in the face of unforeseen difficulties that could arise.",
        "DOI": "10.1002/rob.22188",
        "affiliation_name": "University of Surrey",
        "affiliation_city": "Guildford",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Predicting Choice of Filtering of Motorized Two Wheelers in Urban Mixed Traffic",
        "paper_author": "Damani J.",
        "publication": "Transportation Research Record",
        "citied_by": "5",
        "cover_date": "2023-09-01",
        "Abstract": "Moving through lateral gaps defined by slower vehicles, or “filtering,” is a common characteristic of motorized two wheelers (MTWs) in mixed traffic conditions. Despite its potential benefits such as reduced journey time and emissions, filtering is a critical maneuver, owing to the lesser conspicuity of MTWs and their almost non-existent physical protection from crashes. An in-depth knowledge of filtering behavior is necessary for various theoretical and practical applications. In an attempt to model this distinct driving characteristic of MTWs, this study investigates the filtering interactions of MTWs in urban mixed traffic conditions. Specifically, the behavioral differences between following and filtering interactions were investigated while considering the conditions found in heterogeneous traffic. The choice of filtering was modeled based on multiple factors including spatial parameters and classification parameters. This paper employs a utility-based binary logit model (BLM) and two machine learning (ML) based models: random forest (RF); and adaptive neuro fuzzy inference system (ANFIS) to predict the filtering choice of MTWs in urban mixed traffic. A comparative analysis revealed that, generally if not always, the ML based models (prediction accuracy of RF and ANFIS was 90.07% and 96.02%, respectively) performed better than utility-based models (prediction accuracy of BLM was 80.05%). Owing to the better performance measures of the ANFIS technique, it can be considered a powerful tool for predicting following and filtering choices of MTWs, useful for policy makers designing intelligent transportation systems and microsimulation models.",
        "DOI": "10.1177/03611981231158320",
        "affiliation_name": "Indian Institute of Technology Bombay",
        "affiliation_city": "Mumbai",
        "affiliation_country": "India"
    },
    {
        "paper_title": "A Deep Reinforcement Learning approach for Vertical Stabilization of tokamak plasmas",
        "paper_author": "Dubbioso S.",
        "publication": "Fusion Engineering and Design",
        "citied_by": "9",
        "cover_date": "2023-09-01",
        "Abstract": "Reinforcement Learning has emerged as a promising approach to implement efficient data-driven controllers for a variety of applications. In this paper, a Deep Deterministic Policy Gradient (DDPG) algorithm is used to train a Vertical Stabilization agent, to be considered as a possible alternative to the model-based solutions usually adopted in existing machines. The agent is trained and validated considering the ITER tokamak magnetic control as case study environment. The tuning of the DDPG algorithm's hyper-parameters is motivated through a sensitivity analysis.",
        "DOI": "10.1016/j.fusengdes.2023.113725",
        "affiliation_name": "Università degli Studi della Tuscia Viterbo",
        "affiliation_city": "Viterbo",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Emergency Department Pediatric Readiness among US Trauma Centers: A Machine Learning Analysis of Components Associated with Survival",
        "paper_author": "Newgard C.D.",
        "publication": "Annals of Surgery",
        "citied_by": "5",
        "cover_date": "2023-09-01",
        "Abstract": "Objective: We used machine learning to identify the highest impact components of emergency department (ED) pediatric readiness for predicting in-hospital survival among children cared for in US trauma centers. Background: ED pediatric readiness is associated with improved short-term and long-term survival among injured children and part of the national verification criteria for US trauma centers. However, the components of ED pediatric readiness most predictive of survival are unknown. Methods: This was a retrospective cohort study of injured children below 18 years treated in 458 trauma centers from January 1, 2012, through December 31, 2017, matched to the 2013 National ED Pediatric Readiness Assessment and the American Hospital Association survey. We used machine learning to analyze 265 potential predictors of survival, including 152 ED readiness variables, 29 patient variables, and 84 ED-level and hospital-level variables. The primary outcome was in-hospital survival. Results: There were 274,756 injured children, including 4585 (1.7%) who died. Nine ED pediatric readiness components were associated with the greatest increase in survival: policy for mental health care (+8.8% change in survival), policy for patient assessment (+7.5%), specific respiratory equipment (+7.2%), policy for reduced-dose radiation imaging (+7.0%), physician competency evaluations (+4.9%), recording weight in kilograms (+3.2%), life support courses for nursing (+1.0%-2.5%), and policy on pediatric triage (+2.5%). There was a 268% improvement in survival when the 5 highest impact components were present. Conclusions: ED pediatric readiness components related to specific policies, personnel, and equipment were the strongest predictors of pediatric survival and worked synergistically when combined.",
        "DOI": "10.1097/SLA.0000000000005741",
        "affiliation_name": "OHSU-PSU School of Public Health",
        "affiliation_city": "Portland",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "AI-Based Resource Allocation in End-to-End Network Slicing under Demand and CSI Uncertainties",
        "paper_author": "Gharehgoli A.",
        "publication": "IEEE Transactions on Network and Service Management",
        "citied_by": "19",
        "cover_date": "2023-09-01",
        "Abstract": "Network slicing (NwS) is one of the main technologies in the fifth-generation of mobile communication and beyond (5G+). One of the important challenges in the NwS is information uncertainty which mainly involves demand and channel state information (CSI). Demand uncertainty is divided into three types: number of users requests, amount of bandwidth, and requested virtual network functions workloads. Moreover, the CSI uncertainty is modeled by three methods: worst-case, probabilistic, and hybrid. In this paper, our goal is to maximize the utility of the infrastructure provider by exploiting deep reinforcement learning (DRL) algorithms in end-to-end NwS resource allocation under demand and CSI uncertainties. Enhanced mobile broadband (eMBB) requires high data rates. The uncertainties we argued above have a direct negative impact on the data rate and our objective function. Therefore, we focus primarily on eMBB. Additionally, we also consider ultra-reliable low latency communications (uRLLC) and massive machine-type communication (mMTC). The proposed formulation is a non-convex mixed-integer non-linear programming problem. To perform resource allocation in problems that involve uncertainty, we need a history of previous information. To this end, we use a recurrent deterministic policy gradient (RDPG) algorithm, a recurrent and memory-based approach in DRL. Then, we compare the RDPG method in different scenarios with soft actor-critic (SAC), deep deterministic policy gradient (DDPG), distributed, and greedy algorithms. The simulation results show that the SAC method is better than the DDPG, distributed, and greedy methods, respectively. Moreover, the RDPG method out performs the SAC approach on average by 70%.",
        "DOI": "10.1109/TNSM.2023.3243837",
        "affiliation_name": "Shahrood University of Technology",
        "affiliation_city": "Shahrood",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "Decoupling Optimization for Complex PDN Structures Using Deep Reinforcement Learning",
        "paper_author": "Zhang L.",
        "publication": "IEEE Transactions on Microwave Theory and Techniques",
        "citied_by": "18",
        "cover_date": "2023-09-01",
        "Abstract": "This article presents a new optimization method for complex power distribution networks (PDNs) with irregular shapes and multilayer structures using deep reinforcement learning (DRL), which has not been considered before. A fast boundary integration method is applied to compute the impedance matrix of a PDN structure. Subsequently, a new DRL algorithm based on proximal policy optimization (PPO) is proposed to optimize the decoupling capacitor (decap) placement by minimizing the number of decaps while satisfying the desired target impedance. In the proposed approach, the PDN structure information is encoded into matrices and serves as the input of the DRL algorithm, which increases the flexibility of the method to be extended and generalized to different PDN configurations. Also, the output of the algorithm determines the selection of decap types and locations collaboratively, making it easier to find the optimal solution in a huge search space. The proposed method is compared with the state-of-the-art approaches and shows consistent advantages in reducing the number of decaps in different testing cases.",
        "DOI": "10.1109/TMTT.2023.3248237",
        "affiliation_name": "College of Engineering and Computing",
        "affiliation_city": "Rolla",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Safe deep reinforcement learning in diesel engine emission control",
        "paper_author": "Norouzi A.",
        "publication": "Proceedings of the Institution of Mechanical Engineers. Part I: Journal of Systems and Control Engineering",
        "citied_by": "12",
        "cover_date": "2023-09-01",
        "Abstract": "A deep reinforcement learning application is investigated to control the emissions of a compression ignition diesel engine. The main purpose of this study is to reduce the engine-out nitrogen oxide (Formula presented.) emissions and to minimize fuel consumption while tracking a reference engine load. First, a physics-based engine simulation model is developed in GT-Power and calibrated using experimental data. Using this model and a GT-Power/Simulink co-simulation, a deep deterministic policy gradient is developed. To reduce the risk of an unwanted output, a safety filter is added to the deep reinforcement learning. Based on the simulation results, this filter has no effect on the final trained deep reinforcement learning; however, during the training process, it is crucial to enforce constraints on the controller output. The developed safe reinforcement learning is then compared with an iterative learning controller and a deep neural network–based nonlinear model predictive controller. This comparison shows that the safe reinforcement learning is capable of accurately tracking an arbitrary reference input while the iterative learning controller is limited to a repetitive reference. The comparison between the nonlinear model predictive control and reinforcement learning indicates that for this case reinforcement learning is able to learn the optimal control output directly from the experiment without the need for a model. However, to enforce output constraint for safe learning reinforcement learning, a simple model of system is required. In this work, reinforcement learning was able to reduce (Formula presented.) emissions more than the nonlinear model predictive control; however, it suffered from slightly higher error in load tracking and a higher fuel consumption.",
        "DOI": "10.1177/09596518231153445",
        "affiliation_name": "University of Alberta",
        "affiliation_city": "Edmonton",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "HGNAS++: Efficient Architecture Search for Heterogeneous Graph Neural Networks",
        "paper_author": "Gao Y.",
        "publication": "IEEE Transactions on Knowledge and Data Engineering",
        "citied_by": "23",
        "cover_date": "2023-09-01",
        "Abstract": "Heterogeneous graphs are commonly used to describe networked data with multiple types of nodes and edges. Heterogeneous Graph Neural Networks (HGNNs) are powerful tools for analyzing heterogeneous graphs. However, designing neural architectures of HGNNs requires extensive domain knowledge and time-consuming manual work. Recently, neural architecture search algorithms have become popular in automatically designing neural architectures for homogeneous graph neural networks. In this paper, we present a Heterogeneous Graph Neural Architecture Search algorithm (HGNAS for short) which allows the automatic design of heterogeneous graph neural architectures. Specifically, HGNAS first designs a new search space based on existing popular HGNNs. Then, HGNAS uses a policy network as the controller to sample and find the best neural architecture from the designed search space by maximizing the expected accuracy of the selected architectures on a given validation dataset. Moreover, we design a new method HGNAS++ to improve the efficiency of HGNAS by training the RNN controller within a generative adversarial learning framework. The basic idea of HGNAS++ is to embed a pairwise ranker into the reinforcement learning based architecture search algorithm. The pairwise ranker can be taken as a discriminator which selects more accurate architectures between pairs of candidate architectures. Then, the RNN controller can be updated more efficiently by only using a relatively small number of candidate architectures selected by the pairwise ranker. Experiments on real-world heterogeneous graph datasets show that HGNAS is capable of designing novel HGNNs that beat the best human-invented HGNNs. On the benchmark datasets, HGNAS++ improves HGNAS in terms of evaluation cost, with a reduction of 50% of the evaluated candidate architectures and a decrease of 24% in search time on average. As a byproduct, HGNAS++ can find sparse yet powerful neural architectures for HGNNs.",
        "DOI": "10.1109/TKDE.2023.3239842",
        "affiliation_name": "Institute of Information Engineering",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Blockchain-Based Computing Resource Trading in Autonomous Multi-Access Edge Network Slicing: A Dueling Double Deep Q-Learning Approach",
        "paper_author": "Kwantwi T.",
        "publication": "IEEE Transactions on Network and Service Management",
        "citied_by": "12",
        "cover_date": "2023-09-01",
        "Abstract": "We investigate the computing resource allocation in multi-access edge network slicing (NS) in the context of revenue and multi-access edge computing (MEC) resource management. The significant variety of slice resource utilization levels across slice tenants (i.e., Mobile Virtual Network Operators (MVNOs)) challenges MEC resource management in NS with MEC, leading to virtual machine resource (VMR) (i.e., computing resource) wastage or scarcity. As a result, for efficient MEC resource management, the infrastructure provider (InP) encourages dynamic resource sharing and trading (DRST) of unutilized slice VMR quotas. Nevertheless, cellular network security and privacy issues deter MVNOs from collaborating on effective DRST. The security characteristics inherent in blockchain have recently gained much interest for secure resource trading. Thus, this paper proposes a unique hierarchical blockchain-based inter-slice computing resource trading (ISCRT) scheme for peer-to-peer (P2P) MVNOs in an autonomous multi-sliced MEC-based 5G network. For secure ISCRT transactions, a consortium blockchain network with hyperledger smart contracts (SC) is designed. We model the demand and pricing problems of buyer and seller MVNOs for the unutilized VMRs using a two-stage Stackelberg game. Then, to obtain the Stackelberg equilibrium (SE), an enhanced dueling double deep Q-network (D3QN) algorithm is proposed, which intelligently determines the optimal demand and pricing policies of MVNOs for the unutilized VMRs during ISCRT transactions at negotiation intervals. Simulation analysis shows that the proposed enhanced D3QN algorithm outperforms benchmark schemes in terms of the MVNO slice-level satisfaction and VMR utilization while reducing double-spending attacks in ISCRT settings by 16% and increasing both players' utility.",
        "DOI": "10.1109/TNSM.2023.3240301",
        "affiliation_name": "Southwestern University of Finance and Economics",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Knowledge-Engineered Multi-Cloud Resource Brokering for Application Workflow Optimization",
        "paper_author": "Pandey A.",
        "publication": "IEEE Transactions on Network and Service Management",
        "citied_by": "3",
        "cover_date": "2023-09-01",
        "Abstract": "Data-intensive application workflows benefit by leveraging cloud services to decrease execution times and increase data sharing. Cloud service providers (CSPs) have distinct capabilities and policies, and performance/cost of the cloud services are amongst the prime factors for CSP selection. However, workflow users who need brokering of cloud resources often lack expert guidance to handle the problem of overwhelming choice in CSP selection, and optimization to compensate for service dynamics. In this paper, we address the optimal resource selection problem using a multi-cloud resource broker viz., OnTimeURB that uses knowledge-engineering of user requirements and service capabilities across multiple CSPs. OnTimeURB is powered by integer linear programming and a Naive Bayes classifier to recommend optimal cloud template solutions by weighting performance, agility, cost, and security (PACS) factors. We evaluate the OnTimeURB recommendations with a catalog of bioinformatics application workflows using four CSP resources featuring more than 300 different instance configurations. Our evaluation results show the efficacy of OnTimeURB in creating consistently cost-effective and agile solutions compared to a state-of-the-art k-nearest neighbors (k-NN) approach. We also show that OnTimeURB has 91% success rate improvement in workflow execution times via cloud template recommendations over approaches that do not use knowledge-engineered multi-CSP resource brokering.",
        "DOI": "10.1109/TNSM.2022.3227767",
        "affiliation_name": "University of Missouri",
        "affiliation_city": "Columbia",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Linkages between smart, lean, and resilient manufacturing for sustainable development",
        "paper_author": "Benkhati I.",
        "publication": "Business Strategy and the Environment",
        "citied_by": "3",
        "cover_date": "2023-09-01",
        "Abstract": "In a highly competitive and changing environment, manufacturers are called to adopt the paradigms of smart, sustainable, resilient, and lean manufacturing (SSRL). However, despite the growing interest in the topic and the potential of smart manufacturing features to enable sustainable development, little is known about these interactions in the system of SSRL. Therefore, to address this gap, this study investigates the moderating effect of SM on lean manufacturing and resilient manufacturing for SP while examining the existing relationship between lean and resilience. Drawing upon a hybrid methodology including the partial least squares-structural equation modeling (PLS-SEM) and machine learning algorithms, this research paper develops and tests the hypothetical model using collected data from 399 organizations in China, Africa, and Europe. The findings reveal important implications regarding the fostering effect of SM on the components of the SSRL system and how firms can be both lean and resilient and achieve high sustainable performance simultaneously.",
        "DOI": "10.1002/bse.3322",
        "affiliation_name": "International University of Rabat",
        "affiliation_city": "Sale",
        "affiliation_country": "Morocco"
    },
    {
        "paper_title": "Detecting Imperfect Substitution between Comparably Skilled Immigrants and Natives: A Machine Learning Approach",
        "paper_author": "Lu Y.",
        "publication": "International Migration Review",
        "citied_by": "0",
        "cover_date": "2023-09-01",
        "Abstract": "Immigration economists often disagree about whether comparably skilled immigrants and natives are perfect substitutes in the United States and other developed countries, leading these scholars to different assessments of the labor market impacts of immigration and policy recommendations. This article attempts to provide theoretical bases for understanding the immigrant-native substitution and to introduce machine learning techniques to resolve the empirical debate. Using the male subsample from the US Census and American Community Survey, it shows that the difference in covariate selection explains substantial disagreements in estimating immigrant-native substitution. Given the difficulties in providing compelling theoretical justifications for covariates selected, this article proposes estimating via the Lasso-type (least absolute shrinkage and selection operator) estimators. My Lasso-based estimation rejects perfect substitution, but it also implies easier substitution than that preferred by Ottaviano and Peri, suggesting more direct immigrant-native competition. By extending the sample to women, I find similar immigrant-native substitution across gender. Therefore, this article casts doubt on previous immigration impact assessments. Indeed, my simulation suggests considerable precision gains concerning the immigration's wage impacts on immigrants themselves. Furthermore, this article identifies immigrant segregation as a critical source of the national-level imperfect substitution, which decreases within progressively smaller regions and almost disappears in the same city. By introducing the Lasso-type estimators into migration studies, this article makes solid progress toward evaluating and understanding imperfect immigrant-native substitution and its socioeconomic consequences.",
        "DOI": "10.1177/01979183221126467",
        "affiliation_name": "Shandong University of Finance and Economics",
        "affiliation_city": "Jinan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "The effect of environmental, social and governance score on operating performance after mergers and acquisitions",
        "paper_author": "Teti E.",
        "publication": "Business Strategy and the Environment",
        "citied_by": "6",
        "cover_date": "2023-09-01",
        "Abstract": "This paper examines how the corporate social responsibility performance of the acquirer firm, measured with the environmental, social and governance score, is related to postmerger operating performance, by analysing 796 merger operations that took place between 2011 and 2018. The analysis was carried out by first considering the full sample and then dividing the sample into three subsamples: acquirer companies with an environmental, social and governance score below the median, acquirer companies with a rating above the median and finally those companies considered to have a very high score (over 80). To support the results obtained from this analysis, a machine learning technique was subsequently applied to the data. The results obtained from the analysis demonstrated that acquiring companies with a high environmental, social and governance rating manage to generate a significant improvement in operating performance postdeal after the merger, whereas this is not the case for companies with a low score or for companies with a score of above 80. These results seem to demonstrate that although a high environmental, social and governance score can have a positive impact on postmerger operating performance, this only true up to a point. A possible explanation for this could be that the costs involved in integrating and aligning the culture of the two merging companies increase when there is the need to maintain a high score.",
        "DOI": "10.1002/bse.3293",
        "affiliation_name": "Università di Pisa",
        "affiliation_city": "Pisa",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Using Machine Learning to Capture Heterogeneity in Trade Agreements",
        "paper_author": "Baier S.L.",
        "publication": "Open Economies Review",
        "citied_by": "5",
        "cover_date": "2023-09-01",
        "Abstract": "This paper uses machine learning techniques to capture heterogeneity in free trade agreements. The tools of machine learning allow us to quantify several features of trade agreements, including volume, comprehensiveness, and legal enforceability. Combining machine learning results with gravity analysis of trade, we find that more comprehensive agreements result in larger estimates of the impact of trade agreements. In addition, we identify the policy provisions that have the most substantial effect on creating trade flows. In particular, legally binding provisions on antidumping, capital mobility, competition, customs harmonization, dispute settlement mechanism, e-commerce, environment, export and import restrictions, freedom of transit, investment, investor-state dispute settlement, labor, public procurement, sanitary and phytosanitary measures, services, technical barriers to trade, telecommunications, and transparency tend to have the largest trade creation effects.",
        "DOI": "10.1007/s11079-022-09685-3",
        "affiliation_name": "Wilbur O. and Ann Powers College of Business",
        "affiliation_city": "Clemson",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Mining Online Discourse Related to Transgender Exclusive Policies in Interscholastic Sport: an Exploratory Natural Language Processing Study",
        "paper_author": "Pickett A.C.",
        "publication": "Sexuality Research and Social Policy",
        "citied_by": "7",
        "cover_date": "2023-09-01",
        "Abstract": "Introduction: In recent years, many US states have introduced legislation to restrict the access of transgender people to the interscholastic sport. Methods: We compiled a corpus of Twitter posts (n = 17,182) between October 2021 and March 2022 related to trans-exclusionary policies in sport. We performed a cluster analysis on these tweets to explore salient topics in the data. Machine learning algorithms were used to identify automated bots in each topic as well as to measure sentiment in each tweet. Results: Four major clusters of tweets were present in the corpus. Two were primarily driven by human-authored and focused on fairness, both supportive and exclusive of trans athletes. The others were largely driven by bot accounts and included news sharing and vitriolic media sharing. Much of the conversation was driven by automated bot accounts. Conclusions: Primary arguments surrounding trans athletes focus on fairness. However, at least on Twitter, the issue has been given outsized salience through the use of automated bot accounts. This increased issue of salience has influenced a spate of exclusionary legislation, despite the known benefits of participating in sports and physical activity. Policy Implications: Social media has the potential to influence policy-making and legislators but is increasingly polarized. With respect to trans sport, much of the conversation was driven by bot accounts, which do not reflect the true opinions of individual users. Policymakers should be cautious in the use of social media as a tool for generalizing public opinion on contentious issues.",
        "DOI": "10.1007/s13178-022-00768-x",
        "affiliation_name": "Indiana University Bloomington",
        "affiliation_city": "Bloomington",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Big data analytics and the effects of government restrictions and prohibitions in the COVID-19 pandemic on emergency department sustainable operations",
        "paper_author": "Sariyer G.",
        "publication": "Annals of Operations Research",
        "citied_by": "9",
        "cover_date": "2023-09-01",
        "Abstract": "Grounded in dynamic capabilities, this study mainly aims to model emergency departments' (EDs) sustainable operations in the current situation caused by the COVID-19 pandemic by using emerging big data analytics (BDA) technologies. Since government may impose some restrictions and prohibitions in coping with emergencies to protect the functioning of EDs, it also aims to investigate how such policies affect ED operations. The proposed model is designed by collecting big data from multiple sources and implementing BDA to transform it into action for providing efficient responses to emergencies. The model is validated in modeling the daily number of patients, the average daily length of stay (LOS), and daily numbers of laboratory tests and radiologic imaging tests ordered. It is applied in a case study representing a large-scale ED. The data set covers a seven-month period which collectively means the periods before COVID-19 and during COVID-19, and includes data from 238,152 patients. Comparing statistics on daily patient volumes, average LOS, and resource usage, both before and during the COVID-19 pandemic, we found that patient characteristics and demographics changed in COVID-19. While 18.92% and 27.22% of the patients required laboratory and radiologic imaging tests before-COVID-19 study period, these percentages were increased to 31.52% and 39.46% during-COVID-19 study period. By analyzing the effects of policy-based variables in the model, we concluded that policies might cause sharp decreases in patient volumes. While the total number of patients arriving before-COVID-19 was 158,347, it decreased to 79,805 during-COVID-19. On the other hand, while the average daily LOS was 117.53 min before-COVID-19, this value was calculated to be 165,03 min during-COVID-19 study period. We finally showed that the model had a prediction accuracy of between 80 to 95%. While proposing an efficient model for sustainable operations management in EDs for dynamically changing environments caused by emergencies, it empirically investigates the impact of different policies on ED operations.",
        "DOI": "10.1007/s10479-022-04955-2",
        "affiliation_name": "Izmir Bakircay University",
        "affiliation_city": "Izmir",
        "affiliation_country": "Turkey"
    },
    {
        "paper_title": "Unemployment in Rural Europe: A Machine Learning Perspective",
        "paper_author": "Celbiş M.G.",
        "publication": "Applied Spatial Analysis and Policy",
        "citied_by": "6",
        "cover_date": "2023-09-01",
        "Abstract": "This paper aims to provide policy-relevant findings that can contribute to the resilience of rural regions by discovering the main individual-level factors related to unemployment in those areas through the use of a set of machine learning techniques. Unemployment status is predicted using tree-based classification models: namely, classification tree, bootstrap aggregation, random forest, gradient boosting, and stochastic gradient boosting. The results are further analyzed using inferential techniques such as SHAP value analysis. Results suggest that access to training programmes can mitigate the labor market inequalities caused by differences in education levels, gender, age, alongside with parental education levels. The results also show how such inequalities are even larger for various subgroups detected by the employed algorithms.",
        "DOI": "10.1007/s12061-022-09464-0",
        "affiliation_name": "United Nations University – Maastricht Economic and Social Research Institute on Innovation and Technology",
        "affiliation_city": "Maastricht",
        "affiliation_country": "Netherlands"
    },
    {
        "paper_title": "Trajectory Design and Resource Allocation for Multi-UAV Networks: Deep Reinforcement Learning Approaches",
        "paper_author": "Chang Z.",
        "publication": "IEEE Transactions on Network Science and Engineering",
        "citied_by": "46",
        "cover_date": "2023-09-01",
        "Abstract": "The future mobile communication system is expected to provide ubiquitous connectivity and unprecedented services over billions of devices. The unmanned aerial vehicle (UAV), which is prominent in its flexibility and low cost, emerges as a significant network entity to realize such ambitious targets. In this work, novel machine learning-based trajectory design and resource allocation schemes are presented for a multi-UAV communications system. In the considered system, the UAVs act as aerial Base Stations (BSs) and provide ubiquitous coverage. In particular, with the objective to maximize the system utility over all served users, a joint user association, power allocation and trajectory design problem is presented. To solve the problem caused by high dimensionality in state space, we first propose a machine learning-based strategic resource allocation algorithm which comprises of reinforcement learning and deep learning to design the optimal policy of all the UAVs. Then, we also present a multi-agent deep reinforcement learning scheme for distributed implementation without knowing a priori knowledge of the dynamic nature of networks. Extensive simulation studies are conducted and illustrated to evaluate the advantages of the proposed scheme.",
        "DOI": "10.1109/TNSE.2022.3171600",
        "affiliation_name": "Purple Mountain Laboratory",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Local Learning Enabled Iterative Linear Quadratic Regulator for Constrained Trajectory Planning",
        "paper_author": "Ma J.",
        "publication": "IEEE Transactions on Neural Networks and Learning Systems",
        "citied_by": "18",
        "cover_date": "2023-09-01",
        "Abstract": "Trajectory planning is one of the indispensable and critical components in robotics and autonomous systems. As an efficient indirect method to deal with the nonlinear system dynamics in trajectory planning tasks over the unconstrained state and control space, the iterative linear quadratic regulator (iLQR) has demonstrated noteworthy outcomes. In this article, a local-learning-enabled constrained iLQR algorithm is herein presented for trajectory planning based on hybrid dynamic optimization and machine learning. Rather importantly, this algorithm attains the key advantage of circumventing the requirement of system identification, and the trajectory planning task is achieved with a simultaneous refinement of the optimal policy and the neural network system in an iterative framework. The neural network can be designed to represent the local system model with a simple architecture, and thus it leads to a sample-efficient training pipeline. In addition, in this learning paradigm, the constraints of the general form that are typically encountered in trajectory planning tasks are preserved. Several illustrative examples on trajectory planning are scheduled as part of the test itinerary to demonstrate the effectiveness and significance of this work.",
        "DOI": "10.1109/TNNLS.2022.3165846",
        "affiliation_name": "The Hong Kong University of Science and Technology (Guangzhou)",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Forecasting of Engine Performance for Gasoline-Ethanol Blends using Machine Learning",
        "paper_author": "Sonawane S.",
        "publication": "Journal of Engineering and Technological Sciences",
        "citied_by": "3",
        "cover_date": "2023-08-31",
        "Abstract": "The incorporation of alternative fuels in the automotive domain has brought a new paradigm to tackle the environmental and energy crises. Therefore, it is of interest to test and forecast engine performance with blended fuels. This paper presents an experimental study on gasoline-ethanol blends to test and forecast engine behavior due to changes in the fuel. This study employed a machine learning (ML) technique called TOPSIS to forecast the performance of a slightly higher blend fuelled engine based on experimental data obtained from the same engine running on 0% ethanol blend (E0) and E10 fuels under full load conditions. The engine performance predictions of this ML model were validated for 15% ethanol blend (E15) and further used to predict the engine performance of 20% ethanol blend fuel. The prediction R2 score for the ML model was found to be greater than 0.95 and the MAPE range was 1% to 5% for all observed engine performance attributes. Thus, this paper presents the potential of TOPSIS methodology-based ML predictions on blended fuel engine performance to shorten the testing efforts of blended fuel engines. This methodology may help to faster incorporate higher blended fuels in the automotive sector.",
        "DOI": "10.5614/j.eng.technol.sci.2023.55.3.10",
        "affiliation_name": "Symbiosis Institute of Technology",
        "affiliation_city": "Pune",
        "affiliation_country": "India"
    },
    {
        "paper_title": "MODERNIZATION OF THE NATIONAL ACCOUNTING AND AUDITING SYSTEM USING DIGITAL TRANSFORMATION TOOLS",
        "paper_author": "Shapovalova A.",
        "publication": "Financial and Credit Activity: Problems of Theory and Practice",
        "citied_by": "9",
        "cover_date": "2023-08-31",
        "Abstract": "This study aims to develop a concept for modernizing the national accounting policy, considering global trends and technological advancements in the digital economy within the Accounting 4.0 paradigm. The study employed various methods: analytical method (data analysis and collection), documentary analysis (study of regulatory requirements), expert method (study of expert conclusions), scientometric method (assessment of scientific support), comparative analysis (correlation of expert and scientific findings), and synthesis method (formation of modernization concept). The optimal digital transformation tools for the national accounting policy, determined through comparative analysis of expert and scientometric evaluations, include Cloud Computing, Blockchain Tech-nology, Big Data, Artificial Intelligence (AI), Machine Learning (ML), and the Internet of Things (IoT). These technologies enable flexible, secure, and efficient processing of large data volumes, automation of processes, enhanced accuracy and transparency in accounting reporting, and improved decision-making. The study highlights the lack of research on instrumental support for the national accounting and audit system despite the ongoing implementation of the adapted Ukrainian International Financial Reporting Standards eXtensible Business Reporting Language (UA IFRS XBRL). Implementing the modernization concept aligned with the Accounting 4.0 paradigm has the potential to enhance efficiency and quality in accounting and auditing, foster digital economy devel-opment, and increase international competitiveness. This can be achieved by introducing modern digital technologies that automate processes, enhance data analytics, and ensure reliability and transparency in accounting and auditing. Such changes will boost productivity, mitigate risks, and enhance confidence in financial reporting.",
        "DOI": "10.55643/fcaptp.4.51.2023.4102",
        "affiliation_name": "Lviv National Environmental University",
        "affiliation_city": "Dubliany",
        "affiliation_country": "Ukraine"
    },
    {
        "paper_title": "Carbon offsets aren’t helping the planet — four ways to fix them",
        "paper_author": "Boyd P.W.",
        "publication": "Nature",
        "citied_by": "10",
        "cover_date": "2023-08-31",
        "Abstract": "NA",
        "DOI": "10.1038/d41586-023-02649-8",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Selecting Minimum Acceptable Student’s Mark to Participate in Bologna Final Exam Using Machine Learning Approach",
        "paper_author": "Qattan G.A.",
        "publication": "Zanco Journal of Pure and Applied Sciences",
        "citied_by": "0",
        "cover_date": "2023-08-30",
        "Abstract": "In the higher education sector, the minimum acceptable mark for participation in the final examination in the Bologna system can vary depending on the particular institution, program, and country. In general, most institutions require students to have a minimum overall score of “pass” to be eligible to take final exams. However, some institutions may have to change the minimum acceptable mark to be in line with the approved system and their examination policy. This research sheds light on the possibility of accepting extra students to participate in the final exam, if their scores are slightly lower than the general admission score, and predicting their success based on the student’s grades in previous years. Linear and Polynomial regression (supervised machine learning analysis) were used to give promising results when applied to previous actual records of students in the College of Engineering and hundreds of random marks to increase the accuracy of estimating students' acceptance mark rate for entering the final exams and passing them by considering the new mark rate less than the standard and traditional one. This research will help weak grades students and allow them to participate in the final exam with the possibility of success.",
        "DOI": "10.21271/ZJPAS.35.4.09",
        "affiliation_name": "Salahaddin University-Erbil",
        "affiliation_city": "Erbil",
        "affiliation_country": "Iraq"
    },
    {
        "paper_title": "Analysis of Sentiments and Emotions Attributes of COVID-19-related tweets in the Philippines Using time-Series Analysis",
        "paper_author": "Balan A.K.D.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2023-08-29",
        "Abstract": "Twitter has become a host of individuals' expressions of sentiments and emotion given the effect and consequences of the COVID-19 pandemic. Supported by the appraisal theory of emotion [1], this descriptive cross-sectional (Type 2) study described the pattern of sentiments and emotions over the course of COVID-19 outbreak in accordance to gender. In the following analysis, Gupta et al.'s (2020) COVID-19 twitter dataset and the additional scraped data from September 2021 - December 2021 was used in the present study. The methodologies heavily relied on topic modeling techniques and pre-trained machine learning-based emotion analytic algorithms. Results showed that the negative sentiments surrounding the media is caused by using keywords like covid19, coronavirus, pandemic, and cases but, it appeared to be in a joyful emotion as tweets are expressed in prayers and social solidarity. Furthermore, an increase in the number of reported COVID-19 cases also increases the intensity of fear in females. Implementation of lockdown, quarantines and strict measures lead to higher anger for males. Lastly, with the continuous vaccine rollouts, positivity and optimism are being more expressed across threads. Given the rise in COVID-19 cases, these findings provide sentiment analysis in a population which can be used to improve COVID-19 management through policies and projects in supporting the well-being of the society.",
        "DOI": "10.1145/3625704.3625715",
        "affiliation_name": "Mapúa University",
        "affiliation_city": "Manila",
        "affiliation_country": "Philippines"
    },
    {
        "paper_title": "Evaluation of rural tourism competitiveness based on multi-source data and machine learning: A case study of Lin’an District in Hangzhou, China",
        "paper_author": "Zhao Q.",
        "publication": "Progress in Geography",
        "citied_by": "3",
        "cover_date": "2023-08-28",
        "Abstract": "Evaluating tourism competitiveness is important for ensuring the sustainable development of rural tourism. In the digital information era, multi-source data and machine learning methods can efficiently reveal the characteristics of relevant elements from a geospatial perspective, providing a new method for scientific evaluation of rural tourism competitiveness. Based on multi-source remote sensing and Internet data at the village level from 2019 to 2022, this study identified the rural tourism competitiveness in Lin'an District of Hangzhou City using four machine learning models, including logistic regression (LR), support vector machine (SVM), random forest (RF), and extreme gradient boosting tree (XGB), and the optimal model was selected to reveal the spatial pattern of competitiveness and analyze the critical indicators of identification. The results show that: 1) The accuracy of the rural tourism competitiveness evaluation using the random forest (RF) model is better than the other three machine learning models. 2) Tourism resources, service facilities, accessibility, and policy conditions are the main factors affecting the rural tourism competitiveness. 3) Villages in the high tourism competitiveness category are distributed in strips in the northern and western areas of Lin'an District, with superior development conditions. The medium competitiveness villages are distributed in clumps in the eastern and central-western areas of the district, which have lower quality of tourism resources and service facilities. Low-competitiveness villages are distributed in patches in the central and western areas of the district, with superior ecological environment and land endowment, but lacking resource development and policy support. The study results may provide some policy references and technical supports for promoting the sustainable development of rural tourism.",
        "DOI": "10.18306/dlkxjz.2023.08.008",
        "affiliation_name": "School of Earth Sciences, Zhejiang University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Federated Analysis for Privacy-Preserving Data Sharing: A Technical and Legal Primer",
        "paper_author": "Casaletto J.",
        "publication": "Annual Review of Genomics and Human Genetics",
        "citied_by": "8",
        "cover_date": "2023-08-25",
        "Abstract": "Continued advances in precision medicine rely on the widespread sharing of data that relate human genetic variation to disease. However, data sharing is severely limited by legal, regulatory, and ethical restrictions that safeguard patient privacy. Federated analysis addresses this problem by transferring the code to the data-providing the technical and legal capability to analyze the data within their secure home environment rather than transferring the data to another institution for analysis. This allows researchers to gain new insights from data that cannot be moved, while respecting patient privacy and the data stewards' legal obligations. Because federated analysis is a technical solution to the legal challenges inherent in data sharing, the technology and policy implications must be evaluated together. Here, we summarize the technical approaches to federated analysis and provide a legal analysis of their policy implications.",
        "DOI": "10.1146/annurev-genom-110122-084756",
        "affiliation_name": "Baskin School of Engineering",
        "affiliation_city": "Santa Cruz",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Guidance for Authors, Peer Reviewers, and Editors on Use of AI, Language Models, and Chatbots",
        "paper_author": "Flanagin A.",
        "publication": "JAMA",
        "citied_by": "50",
        "cover_date": "2023-08-22",
        "Abstract": "NA",
        "DOI": "10.1001/jama.2023.12500",
        "affiliation_name": "Journal of American Medical Association",
        "affiliation_city": "Chicago",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Overviewing the emerging methods for predicting urban Sprawl features",
        "paper_author": "Bélinga A.G.",
        "publication": "E3S Web of Conferences",
        "citied_by": "1",
        "cover_date": "2023-08-18",
        "Abstract": "Urban sprawl, a common phenomenon characterized by uncontrolled urban growth, has far-reaching socio-economic and environmental implications. It’s a complex phenomenon, and finding a better way to tackle it is essential. Accurate simulation and prediction of urban sprawl features would facilitate decision-making in urban planning and the formulation of city growth policies. This article provides an overview of the techniques used to this end. Initially, it highlights the use of a certain category of so-called traditional methods, such as statistical models or classical machine learning methods. It then focuses particularly on the intersection of deep learning and urban sprawl modelling, examining how deep learning methods are being exploited to simulate and predict urban sprawl. I finally studies hybrid approaches that combine deep learning with agent-based models, cellular automata, or other techniques offer a synergistic way to leverage the strengths of different methodologies for urban sprawl modelling.",
        "DOI": "10.1051/e3sconf/202341803008",
        "affiliation_name": "Rabat Information Technology Center",
        "affiliation_city": "Rabat",
        "affiliation_country": "Morocco"
    },
    {
        "paper_title": "A Quantitative Approach to Road Safety in Morocco: Reducing Accidents through Predictive Modeling",
        "paper_author": "Bel-Lahcen M.",
        "publication": "E3S Web of Conferences",
        "citied_by": "0",
        "cover_date": "2023-08-18",
        "Abstract": "This paper uses machine learning to predict road accidents in Morocco, a country marked by high annual accident rates. Our model employs data such as weather, time of day, and road conditions, derived from historical accidents and environmental records. Findings suggest that such predictive modeling can enable traffic authorities to anticipate high-risk situations and enact pre-emptive safety measures, contributing to significant reductions in road accidents. This study provides a data-driven approach towards policy implementation for road safety, with insights applicable to global road safety initiatives.",
        "DOI": "10.1051/e3sconf/202341802004",
        "affiliation_name": "Mohammed VI Polytechnic University",
        "affiliation_city": "Ben Guerir",
        "affiliation_country": "Morocco"
    },
    {
        "paper_title": "Unveiling the Environmental Implications of Automatic Text Generation and the Role of Detection Systems",
        "paper_author": "Al Karkouri A.",
        "publication": "E3S Web of Conferences",
        "citied_by": "1",
        "cover_date": "2023-08-17",
        "Abstract": "The emergence of artificial intelligence (AI) and natural language processing (NLP) technologies has led to the proliferation of automated systems capable of generating text. While these advancements have enhanced various fields, such as language translation and content generation, they have also given rise to concerns regarding the potential misuse of generated texts, particularly in the context of environmental preservation. This scientific article investigates the intricate relationship between automatic detection of generated texts and the environment. We examine the impact of generated texts on environmental awareness, misinformation propagation, and the role of automated detection systems in mitigating the risks associated with generated content. Our findings highlight the crucial need for robust detection mechanisms to preserve the integrity of environmental discourse and ensure sustainable decision-making.",
        "DOI": "10.1051/e3sconf/202341201102",
        "affiliation_name": "Université de Poitiers",
        "affiliation_city": "Poitiers",
        "affiliation_country": "France"
    },
    {
        "paper_title": "Measuring the Emission Changes and Meteorological Dependence of Source-Specific BC Aerosol Using Factor Analysis Coupled With Machine Learning",
        "paper_author": "Dai T.",
        "publication": "Journal of Geophysical Research: Atmospheres",
        "citied_by": "8",
        "cover_date": "2023-08-16",
        "Abstract": "Reducing ambient black carbon (BC) relies on the targeted control of anthropogenic emissions. Measuring emission changes in source-specific BC aerosol is essential to assess the effectiveness of regulatory policies but is difficult due to the presence of meteorology and multiple co-existing emissions. Herein, we propose a data-driven approach, combining dispersion-normalized factor analysis (DN-PMF) with a machine learning weather adjustment (deweathering) technique, to decompose ambient BC into source emissions and meteorological drivers. Six refined BC sources were extracted from the factor analysis of aethalometer multi-wavelength BC and concurrent observational datasets. In addition to the widely reported dominant sources, such as vehicular emissions (VE) and coal/biomass burning (BB), a discernible port and shipping emission source were identified with potential impacts on coastal air quality. The source-specific BC showed abrupt changes in response to interventions (e.g., holidays) after separating weather-related confounders. Significant reductions in deweathered coal and BB, VE, and local dust verified the effectiveness of policies, such as clean winter-heating and support for the Clean Air Actions. As revealed by a post-hoc model explanation technique, the evolution of the boundary layer was the predominant meteorological driver exerting the opposite impact on local sources with respect to distant regional-wide sources, that is, the port and shipping emissions.",
        "DOI": "10.1029/2023JD038696",
        "affiliation_name": "China Meteorological Administration",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Research on early identification of disruptive technologies based on heterogeneous patents",
        "paper_author": "Wang K.",
        "publication": "Studies in Science of Science",
        "citied_by": "4",
        "cover_date": "2023-08-15",
        "Abstract": "Disruptive technologies are regarded as a revolutionary force to \" change the rules of the game\" and \" reshape the future pattern\" . All previous industrial revolutions have shown that whoever has mastered disruptive technologies first will take priority to occupy the technological commanding heights. With the prominence of the leading characteristics of disruptive technologies, they have increasingly risen to the fields of science and technology, national defense and military, and gradually developed to the height of national strategic leadership. Identifying disruptive technologies in advance is of great significance to the cultivation of disruptive technologies and policy formulation. Disruptive technologies are a kind of technology that \" blazes a new trail\", and \" blazes a new trail\" means differences from mainstream technologies, so identifying disruptive technologies from the perspective of heterogeneity is more consistent with the law of technological development. Similarity technologies tend to optimize existing technologies and extend the technology track and technology life cycle; Long distance and heterogeneous technologies are more likely to realize technological trajectory transition and produce disruptive technologies. Heterogeneous technology can lead to the variation of technological structure and promote technological innovation and development. From the perspective of heterogeneous patents, this paper proposes an early identification method of potentially disruptive technologies based on machine learning. Firstly, the BERT semantic vector and IPC weight vector are combined to obtain patent features; then a variety of anomaly detection algorithms are used to identify heterogeneous patents, and the patent influence is calculated through the expanded patent co-citation network; then construct heterogeneous patent indicators and technical influence relationship data; Finally, use machine learning algorithms to predict potential disruptive technologies from a large number of heterogeneous patents, and predict the future direction of technology research and development. This paper uses patents in the field of 3D printing (Additive Manufacturing, AM) to verify the effectiveness of this method. The research conclusions mainly include: (1) There are a lot of heterogeneous technologies in the field of 3D printing, but there are few heterogeneous technologies that really have an important impact on this field. It is unrealistic to use traditional expert evaluation methods (2) Among machine learning algorithms, random forest algorithm has the best performance in identifying potentially subversive technologies in 3D printing field. (3) The research and development directions of potential disruptive technologies in 3D printing field in the future are: ① metal 3D printing and preparation of alloy, concrete and ceramic materials; ② The application of 3D printing technology in the fields of medicine, electronic instruments, electrical engineering and electronic energy, as well as the research and development of polymer chemistry, polymers and other materials; ③ Computer, control and communication technology; ④ Chemical engineering, biotechnology and biomaterial analysis; ⑤ The application of 3D printing technology in engines, pumps, turbines, textile and paper machines, mechanical parts, civil engineering, furniture, games, transportation, other consumer goods and other fields. The potential subversive technologies identified in the 3D printing field are in line with the reality and have reference value for policy formulation, enterprise research and development and subsequent academic research.",
        "DOI": "NA",
        "affiliation_name": "Dalian University of Technology",
        "affiliation_city": "Dalian",
        "affiliation_country": "China"
    },
    {
        "paper_title": "In-Network Caching for ICN-Based IoT (ICN-IoT): A Comprehensive Survey",
        "paper_author": "Zhang Z.",
        "publication": "IEEE Internet of Things Journal",
        "citied_by": "45",
        "cover_date": "2023-08-15",
        "Abstract": "The Internet of Things (IoT) has already emerged as one of the most popular directions in today's information and communication technology (ICT) domain. With its advancement over different application areas, such as smart home, smart healthcare, industry 4.0, etc., a huge amount of data has been generated by billions of IoT devices, which aggravates the shortcomings of the network layer (IP)-based networks, such as limited expressiveness of IP addressing, inefficient support for mobility, and in-network caching. Building IoT on top of information-centric networking (ICN) is believed to be a promising solution to tackle the above challenge, especially the in-network caching of ICN can significantly benefit IoT in terms of reducing data and saving IoT devices' energy. However, caching IoT data is more challenging than caching traditional Internet content, e.g., video, because IoT data are usually valid within a certain period of time, and IoT devices are typically constrained with battery. Hence, in this survey, we first review the current implementation proposals of ICN-based IoT (ICN-IoT). Next, we present the conventional caching decision policies and replacement policies which could be adopted to mitigate the aforementioned challenges, e.g., reducing IoT traffic, saving energy, and reducing data retrieval latency. Further, since leveraging machine learning (ML) techniques have the potential to further improve the caching efficiency by dealing with uncertainties, e.g., predicting unknown information, adaptively interacting with the environment, we also demonstrate the recently proposed ML-based caching schemes for ICN-IoT. In addition, we outline the open research issues and point out the future opportunities of caching in ICN-IoT.",
        "DOI": "10.1109/JIOT.2023.3274653",
        "affiliation_name": "School of Communications and Information Engineering",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Extracting principal building variables from automatically collected urban scale façade images for energy conservation through deep transfer learning",
        "paper_author": "Yu X.",
        "publication": "Applied Energy",
        "citied_by": "4",
        "cover_date": "2023-08-15",
        "Abstract": "Buildings account for 40% of the energy consumption and 13% of the greenhouse gas (GHG) emissions in the U.S. To improve building energy efficiency, cities around the U.S. issued energy policies, such as the energy performance disclosure requirement in New York City, to encourage building owners to make informed retrofitting decisions. However, complying with these policies is expensive and time-consuming for government agencies and building owners, especially for old buildings where detailed building information is not readily available. In this work, we propose an automatic, non-intrusive, and scalable framework to capture energy-essential building variables through reasoning building façade images - FaçadeReasoner. Specifically, we first build a comprehensive building information dataset and identify the most impactful (i.e., principal) variables in relation to building energy performance and GHG emissions using the state-of-the-art feature attribution model. Next, we propose a method to automatically collect an urban scale building image dataset with more than 10,000 façade images and extract principal-building-variables from these images using deep transfer learning. Results show that FacadeReasoner has the capability to predict principal building variables, namely “building type” (accuracy 0.77), “year built” (accuracy 0.62), “building height” (R2 0.80), and rough estimates of “building area” (R2 0.46) from façade images. This study is unique as it marks the first attempt to enable an automated end-to-end framework for urban scale principal-building-variables extraction, providing an efficient and economical alternative for large building portfolio owners and managers (e.g., municipalities) to comprehend urban scale energy-related building information for informed decision-making, directly contributing to Net-Zero 2050.",
        "DOI": "10.1016/j.apenergy.2023.121228",
        "affiliation_name": "NYU Tandon School of Engineering",
        "affiliation_city": "New York",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Modeling and energy dynamic control for a ZEH via hybrid model-based deep reinforcement learning",
        "paper_author": "Li Y.",
        "publication": "Energy",
        "citied_by": "27",
        "cover_date": "2023-08-15",
        "Abstract": "Efficient and flexible energy management strategy can play an important role in energy conservation in building sector. The model-free reinforcement learning control of building energy systems generally requires an enormous amount of training data and low learning efficiency creates an obstacle to practice. This work proposes a hybrid model-based reinforcement learning framework to optimize the indoor thermal comfort and energy cost-saving performances of a ZEH (zero energy house) space heating system using relatively short-period monitored data. The reward function is designed regarding energy cost, PV self-consumption and thermal discomfort, proposed agents can interact with the reduced-order thermodynamic model and an uncertain environment, and makes optimal control policies through the learning process. Simulation results demonstrate that proposed agents achieve efficient convergence, D3QN presents a superiority of convergence performance. To evaluate the performances of proposed algorithms, the trained agents are tested using monitored data. With learned policies, the self-learning agents could balance the needs of thermal comfort, energy cost saving and increasing on-site PV consumption compared with the baselines. The comparative analysis shows that D3QN achieved over 30% cost savings compared with measurement results. D3QN outperforms DQN and Double DQN agents in test scenarios maintaining more stable temperatures under various outside conditions.",
        "DOI": "10.1016/j.energy.2023.127627",
        "affiliation_name": "Tongji Architectural Design (Group) Co. Ltd",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Consultative engagement of stakeholders toward a roadmap for African language technologies",
        "paper_author": "Siminyu K.",
        "publication": "Patterns",
        "citied_by": "2",
        "cover_date": "2023-08-11",
        "Abstract": "There has been a rise in natural language processing (NLP) communities across the African continent (Masakhane, AfricaNLP workshops). With this momentum noted, and given the existing power asymmetries that plague the African continent, there is an urgent need to ensure that these technologies move toward shared goals between organizations and stakeholders, not only to improve the representation of African languages in cutting-edge NLP research but also to ensure that NLP research enables technological advances toward human dignity, well-being, and equity for those who speak African languages. This study investigates the motivations, focus, and challenges faced by various stakeholders who are at the core of the NLP process. We perform structured stakeholder identification to identify core stakeholders in the NLP process. Interviews with representatives of these stakeholder groups are performed and are collated into relevant themes. Finally, a set of recommendations are proposed for use by policy and artificial intelligence (AI) researchers.",
        "DOI": "10.1016/j.patter.2023.100820",
        "affiliation_name": "Mozilla Foundation",
        "affiliation_city": "Mountain View",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Streaming Machine Learning for Supporting Data Prefetching in Modern Data Storage Systems",
        "paper_author": "Lucas Filho E.R.",
        "publication": "AI4Sys 2023 - Proceedings of the 1st Workshop on AI for Systems",
        "citied_by": "0",
        "cover_date": "2023-08-10",
        "Abstract": "Modern data storage systems optimize data access by distributing data across multiple storage tiers and caches, based on numerous tiering and caching policies. The policies' decisions, and in particular the ones related to data prefetching, can severely impact the performance of the entire storage system. In recent years, various machine learning algorithms have been employed to model access patterns in complex data storage workloads. Even though data storage systems handle a constantly changing stream of file requests, current approaches continue to train their models offline in a batch-based approach. In this paper, we investigate the use of streaming machine learning to support data prefetching decisions in data storage systems as it introduces various advantages such as high training efficiency, high prediction accuracy, and high adaptability to changing workload patterns. After extracting a representative set of features in an online fashion, streaming machine learning models can be trained and tested while the system is running. To validate our methodology, we present one streaming classification model to predict the next file offset to be read in a file. We assess the model's performance using production traces provided by Huawei Technologies and demonstrate that streaming machine learning is a feasible approach with low memory consumption and minimal training delay, facilitating accurate predictions in real-time.",
        "DOI": "10.1145/3588982.3603608",
        "affiliation_name": "Huawei Technologies Co., Ltd.",
        "affiliation_city": "Shenzhen",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Application of artificial intelligence in libraries and information centers services: prospects and challenges",
        "paper_author": "Jha S.K.",
        "publication": "Library Hi Tech News",
        "citied_by": "24",
        "cover_date": "2023-08-10",
        "Abstract": "Purpose: Artificial intelligence (AI) is one of the emerging technologies of this time. AI is a widely used technology in library services that can transform the best services in the age of information technology. This paper aims to highlight the use of AI in library operations. Several research has been undertaken on this subject, but that only address a few applications. AI and libraries have a substantial nexus; nevertheless, the use and awareness of AI in library services are still creating question marks addressed in this paper. This study will help the policy stakeholder, librarians and scholars in the field to address these issues before the deployment of AI in library services. Design/methodology/approach: This study is based on a qualitative method using content analysis techniques. An extensive review of literature on “artificial intelligence”, “smart libraries” was carried to ascertain the emerging technologies in the smart library domain. Literature was searched against various keywords like artificial intelligence, smart technologies, Internet of Things, electronic resource management, data mining and ambient intelligence. This study highlights the pros and cons of AI in library services and its possible solutions. Findings: The findings of this study show that AI is a vibrant technology that can be used in library services; however, some obstacles like adequate funds, the attitude of librarians and technical skills are a few obstacles that hamper AI in library operations. The findings also reveal that using AI in library operations will accelerate libraries in the right direction. Furthermore, this study highlights various applications that can be deployed without spending costs. Practical implications: This paper may be of interest to academic, librarians, policymakers, researchers and the government to have a perspective on initiatives in the country on application of technology in library services. This study can introduce the current status and potential of this technology to bring the technology revolution in library and information center services. Social implications: This study will motivate library professionals to take advantage of AI in library services and further accelerate library operations in the right direction. Originality/value: This study covers the understanding of AI in library services that will help the librarian’s and information professionals leverage AI in library scenarios. Furthermore, the practical implication of AI in library services will bring positive change in implementing AI.",
        "DOI": "10.1108/LHTN-06-2023-0102",
        "affiliation_name": "O.P. Jindal Global University",
        "affiliation_city": "Sonipat",
        "affiliation_country": "India"
    },
    {
        "paper_title": "A Multi-Temporal Analysis of Archaeological Site Destruction using Landsat Satellite Data and Machine Learning, Moche Valley, Peru",
        "paper_author": "Payntar N.D.",
        "publication": "Journal on Computing and Cultural Heritage",
        "citied_by": "2",
        "cover_date": "2023-08-09",
        "Abstract": "The destruction of archaeological sites and the loss of archaeological landscapes remains a global concern as populations and urban areas continue to expand. Archaeological sites are not only significant to local communities, national identities, and modern tourist economies but also provide critical knowledge of past sociocultural interactions, settlement patterns, human-environment relationships, and risk mitigation strategies. While archaeological landscapes and site destruction have remained outside of traditional land use land cover change (LULCC) studies, they are a form of urban and agricultural land use. By conceptualizing archaeological site destruction within land change science, this study provides an innovative approach for assessing \"what's left\" of historically surveyed archaeological landscapes. Using a Random Forest algorithm and Landsat satellite data, this study quantifies archaeological site destruction attributed to LULCC in Peru's lower Moche Valley between 1985 and 2020. More than 400 archaeological sites previously recorded during the Chan Chan-Moche Valley Project (CCMVP, 1969-1974) are analyzed. Results indicate that less than a quarter of the original CCMVP sites remain on the landscape. The primary drivers of LULCC in the lower Moche Valley include population growth, migration, and government policies, while secondary drivers include heritage values. Positioning archaeological survey data within land change science and integrating machine learning techniques can benefit historic survey reassessments globally and provides significant knowledge of archaeological site destruction and the socioeconomic conditions that underly dynamic landscape changes.",
        "DOI": "10.1145/3586079",
        "affiliation_name": "The University of Texas at Austin",
        "affiliation_city": "Austin",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Framing Immigrants at the Intersection of Education and Immigration Enforcement in United States Newspapers",
        "paper_author": "Colbern A.",
        "publication": "New Trends in Qualitative Research",
        "citied_by": "0",
        "cover_date": "2023-08-08",
        "Abstract": "The United States Supreme Court ruled in Plyler v. Doe (1982) that all children regardless of their legal immigration status have a fundamental right to K-12 education. Despite education being a fundamental right, it remains fragile and contested for undocumented immigrants. Federal courts have had to reaffirm Plyler multiple times, including issuing a permanent injunction over California’s Proposition 187 (1994), which banned undocumented children from K–12 public schools and required officials and teachers to report anyone they suspected of being undocumented to federal immigration authorities. More recently, the courts blocked Alabama’s HB 56 (2012) provision that required schools to check and report newly enrolled K–12 students’ immigration status. The entanglement between postsecondary education and immigration, which falls outside of Plyler’s protection, has grown more pronounced over the past two decades. Federal immigration law in 1996 opened the door for states to actively regulate undocumented immigrants’ right to postsecondary education, including banning admissions or creating discriminatory hardships by denying in-state tuition or financial aid. While a rich scholarship covers these policies and their effects, no systematic study exists on the news framing of the intersection between education and immigration. This article examines 40,469 news articles published from 1980 to 2022 in six national and state news sources in the United States to explore the (dis)connections between education (K-12 and postsecondary) and immigration. Combining machine learning techniques and social network analysis with qualitative coding, we show that reporters’ use of a range of experts creates a deep conflation of education with immigration enforcement and illegality framing. Despite quests for journalistic neutrality, we argue that the use of experts by reporters prevents immigrant education from being a topic on its own or a topic where immigrants are framed primarily in a positive and inclusive way.",
        "DOI": "10.36367/ntqr.16.2023.e797",
        "affiliation_name": "Arizona State University",
        "affiliation_city": "Tempe",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "\"Fairness Toolkits, A Checkbox Culture?\" On the Factors that Fragment Developer Practices in Handling Algorithmic Harms",
        "paper_author": "Balayn A.",
        "publication": "AIES 2023 - Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",
        "citied_by": "11",
        "cover_date": "2023-08-08",
        "Abstract": "Fairness toolkits are developed to support machine learning (ML) practitioners in using algorithmic fairness metrics and mitigation methods. Past studies have investigated practical challenges for toolkit usage, which are crucial to understanding how to support practitioners. However, the extent to which fairness toolkits impact practitioners' practices and enable reflexivity around algorithmic harms remains unclear (i.e., distributive unfairness beyond algorithmic fairness, and harms that are not related to the outputs of ML systems). Little is currently understood about the root factors that fragment practices when using fairness toolkits and how practitioners reflect on algorithmic harms. Yet, a deeper understanding of these facets is essential to enable the design of support tools for practitioners. To investigate the impact of toolkits on practices and identify factors that shape these practices, we carried out a qualitative study with 30 ML practitioners with varying backgrounds. Through a mixed within and between-subjects design, we tasked the practitioners with developing an ML model, and analyzed their reported practices to surface potential factors that lead to differences in practices. Interestingly, we found that fairness toolkits act as double-edge swords - with potentially positive and negative impacts on practices. Our findings showcase a plethora of human and organizational factors that play a key role in the way toolkits are envisioned and employed. These results bear implications for the design of future toolkits and educational training for practitioners and call for the creation of new policies to handle the organizational constraints faced by practitioners.",
        "DOI": "10.1145/3600211.3604674",
        "affiliation_name": "Delft University of Technology",
        "affiliation_city": "Delft",
        "affiliation_country": "Netherlands"
    },
    {
        "paper_title": "Self-Destructing Models: Increasing the Costs of Harmful Dual Uses of Foundation Models",
        "paper_author": "Henderson P.",
        "publication": "AIES 2023 - Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",
        "citied_by": "11",
        "cover_date": "2023-08-08",
        "Abstract": "A growing ecosystem of large, open-source foundation models has reduced the labeled data and technical expertise necessary to apply machine learning to many new problems. Yet foundation models pose a clear dual-use risk, indiscriminately reducing the costs of building both harmful and beneficial machine learning systems. Policy tools such as restricted model access and export controls are the primary methods currently used to mitigate such dual-use risks. In this work, we review potential safe-release strategies and argue that both policymakers and AI researchers would benefit from fundamentally new technologies enabling more precise control over the downstream usage of open-source foundation models. We propose one such approach: the task blocking paradigm, in which foundation models are trained with an additional mechanism to impede adaptation to harmful tasks without sacrificing performance on desirable tasks. We call the resulting models self-destructing models, inspired by mechanisms that prevent adversaries from using tools for harmful purposes. We present an algorithm for training self-destructing models leveraging techniques from meta-learning and adversarial learning, which we call meta-learned adversarial censoring (MLAC). In a small-scale experiment, we show MLAC can largely prevent a BERT-style model from being re-purposed to perform gender identification without harming the model's ability to perform profession classification.",
        "DOI": "10.1145/3600211.3604690",
        "affiliation_name": "Stanford University",
        "affiliation_city": "Stanford",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "The Bureaucratic Challenge to AI Governance: An Empirical Assessment of Implementation at U.S. Federal Agencies",
        "paper_author": "Lawrence C.",
        "publication": "AIES 2023 - Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",
        "citied_by": "5",
        "cover_date": "2023-08-08",
        "Abstract": "Can government govern artificial intelligence (AI)? One of the central questions of AI governance surrounds state capacity, namely whether government has the ability to accomplish its policy goals. We study this question by assessing how well the U.S. federal government has implemented three binding laws around AI governance: two executive orders - concerning trustworthy AI in the public sector (E.O. 13,960) and AI leadership (E.O. 13,859) - and the AI in Government Act. We conduct the first systematic empirical assessment of the implementation status of these three laws, which have each been described as central to US AI innovation. First, we track, through extensive research, line-level adoption of each mandated action. Based on publicly available information, we find that fewer than 40 percent of 45 legal requirements could be verified as having been implemented. Second, we research the specific implementation of transparency requirements at up to 220 federal agencies. We find that nearly half of agencies failed to publicly issue AI use case inventories - even when these agencies have demonstrable use cases of machine learning. Even when agencies have complied with these requirements, efforts are inconsistent. Our work highlights the weakness of U.S. state capacity to carry out AI governance mandates and we discuss implications for how to address bureaucratic capacity challenges.",
        "DOI": "10.1145/3600211.3604701",
        "affiliation_name": "Stanford University",
        "affiliation_city": "Stanford",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Machine Learning practices and infrastructures",
        "paper_author": "Berman G.",
        "publication": "AIES 2023 - Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",
        "citied_by": "5",
        "cover_date": "2023-08-08",
        "Abstract": "Machine Learning (ML) systems, particularly when deployed in high-stakes domains, are deeply consequential. They can exacerbate existing inequities, create new modes of discrimination, and reify outdated social constructs. Accordingly, the social context (i.e. organisations, teams, cultures) in which ML systems are developed is a site of active research for the field of AI ethics, and intervention for policymakers. This paper focuses on one aspect of social context that is often overlooked: interactions between practitioners and the tools they rely on, and the role these interactions play in shaping ML practices and the development of ML systems. In particular, through an empirical study of questions asked on the Stack Exchange forums, the use of interactive computing platforms (e.g. Jupyter Notebook and Google Colab) in ML practices is explored. I find that interactive computing platforms are used in a host of learning and coordination practices, which constitutes an infrastructural relationship between interactive computing platforms and ML practitioners. I describe how ML practices are co-evolving alongside the development of interactive computing platforms, and highlight how this risks making invisible aspects of the ML life cycle that AI ethics researchers' have demonstrated to be particularly salient for the societal impact of deployed ML systems.",
        "DOI": "10.1145/3600211.3604689",
        "affiliation_name": "The Australian National University",
        "affiliation_city": "Canberra",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Conflux: Exploiting Persistent Memory and RDMA Bandwidth via Adaptive I/O Mode Selection",
        "paper_author": "Qi Z.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-08-07",
        "Abstract": "Persistent Memory (PM) and Remote Direct Memory Access (RDMA) technologies have significantly improved the storage and network performance in data centers and spawned a slew of distributed file system (DFS) designs. Existing DFSs often consider remote storage a performance constraint, assuming it delivers lower bandwidth and higher latency than local storage devices. However, the advances in RDMA technology provide an opportunity to bridge the performance gap between local and remote access, enabling DFSs to leverage both local and remote PM bandwidth and achieve higher overall throughput. We propose Conflux, a new DFS architecture that leverages the aggregated bandwidth of PM and RDMA networks. Conflux dynamically steers I/O requests to local and remote PM to fully utilize PM and RDMA bandwidth under heavy workloads. To adaptively decide the I/O run-time path, we propose SEED, a learning-based policy engine predicting Conflux I/O latency and making decisions in a real-time system. Furthermore, Conflux adopts a fine-grained concurrency control approach to improve its scalability. Experimental results show that Conflux achieves up to 4.7× throughput compared to existing DFSs on multi-threaded workloads.",
        "DOI": "10.1145/3605573.3605574",
        "affiliation_name": "Shanghai Jiao Tong University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Integration of Digital Twin and Federated Learning for Securing Vehicular Internet of Things",
        "paper_author": "Gupta D.",
        "publication": "2023 Research in Adaptive and Convergent Systems RACS 2023",
        "citied_by": "9",
        "cover_date": "2023-08-06",
        "Abstract": "In the present era of advanced technology, the Internet of Things (IoT) plays a crucial role in enabling smart connected environments. This includes various domains such as smart homes, smart healthcare, smart cities, smart vehicles, and many others. The IoT facilitates the integration and interconnection of devices, enabling them to communicate, share data, and work together to create intelligent and efficient systems. With ubiquitous smart connected devices and systems, a large amount of data associated with them is at a prime risk from malicious entities (e.g., users, devices, applications) in these systems. Innovative technologies, including cloud computing, Machine Learning (ML), and data analytics, support the development of anomaly detection models for the Vehicular Internet of Things (V-IoT), which encompasses collaborative automatic driving and enhanced transportation systems. However, traditional centralized anomaly detection models fail to provide better services for connected vehicles due to issues such as high latency, privacy leakage, performance overhead, and model drift. Recently, Federated Learning (FL) has gained significant recognition for its ability to address data privacy concerns in the IoT domain. In the context of V-IoT, which involves autonomous vehicles and intelligent transportation systems with connected vehicles communicating with various sensors and devices, FL is used to develop an anomaly detection model. Current technology, the Digital Twin (DT), proves beneficial in addressing uncertain crises and data security issues by creating a virtual replica that simulates various factors, including traffic trajectories, city policies, and vehicle utilization. This enables the system to facilitate efficient and inclusive decision-making. However, the effectiveness of a V-IoT DT system heavily relies on the collection of long-term and high-quality data to make appropriate decisions. Consequently, its advantages may be limited when confronted with urgent crises like the COVID-19 pandemic. This paper introduces a Hierarchical Federated Learning (HFL) based anomaly detection model for V-IoT, aiming to enhance the accuracy of the model. Our proposed model integrates both DT and HFL approaches to create a comprehensive system for detecting malicious activities using an anomaly detection model. Additionally, real-world V-IoT use case scenarios are presented to demonstrate the application of the proposed model.",
        "DOI": "10.1145/3599957.3606250",
        "affiliation_name": "Embry-Riddle Aeronautical University",
        "affiliation_city": "Daytona Beach",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Autodidactic and coachable neural architectures",
        "paper_author": "Michael L.",
        "publication": "Compendium of Neurosymbolic Artificial Intelligence",
        "citied_by": "1",
        "cover_date": "2023-08-04",
        "Abstract": "The prediction made by a learned model is rarely the end outcome of interest to a given agent. In most real-life scenarios, a certain policy is applied on the model's prediction and on some relevant context to reach a decision. It is the (possibly temporally distant) effects of this decision that bring value to the agent. Moreover, it is those effects, and not the model's prediction, that need to be evaluated as far as the agent's satisfaction is concerned. The formalization of such scenarios naturally raises certain questions: How should a learned model be integrated with a policy to reach decisions? How should the learned model be trained and evaluated in the presence of such a policy? How is the training affected in terms of the type of access that one has on the policy? How can the policy be represented and updated in a way that is cognitively compatible with a human, so that it offers an explainable layer of reasoning on top of the learned model? This chapter offers a high-level overview of past work on the integration of modular reasoning with autodidactic learning and with user-driven coaching, as it applies on neural-symbolic architectures that combine sequentially a neural module with an arbitrary symbolically represented (and possibly non-differentiable) policy. In this context, the chapter offers responses to the questions above when the policy can be reasoned with only in a deductive manner, or in a deductive and an abductive manner. It further discusses how the policy can be learned / updated in an elaboration-tolerant and cognitivelylight manner through machine coaching, and highlights the connections of the dialectical coaching process with the central role that argumentation plays in human reasoning.",
        "DOI": "10.3233/FAIA230143",
        "affiliation_name": "Open University of Cyprus",
        "affiliation_city": "Nicosia",
        "affiliation_country": "Cyprus"
    },
    {
        "paper_title": "Certified Edge Unlearning for Graph Neural Networks",
        "paper_author": "Wu K.",
        "publication": "Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "citied_by": "10",
        "cover_date": "2023-08-04",
        "Abstract": "The emergence of evolving data privacy policies and regulations has sparked a growing interest in the concept of \"machine unlearning\", which involves enabling machine learning models to forget specific data instances. In this paper, we specifically focus on edge unlearning in Graph Neural Networks (GNNs), which entails training a new GNN model as if certain specified edges never existed in the original training graph. Unlike conventional unlearning scenarios where data samples are treated as independent entities, edges in graphs exhibit correlation. Failing to carefully account for this data dependency would result in the incomplete removal of the requested data from the model. While retraining the model from scratch by excluding the specific edges can eliminate their influence, this approach incurs a high computational cost. To overcome this challenge, we introduce CEU, a Certified Edge Unlearning framework. CEU expedites the unlearning process by updating the parameters of the pre-trained GNN model in a single step, ensuring that the update removes the influence of the removed edges from the model. We formally prove that CEU offers a rigorous theoretical guarantee under the assumption of convexity on the loss function. Our empirical analysis further demonstrates the effectiveness and efficiency of CEU for both linear and deep GNNs - it achieves significant speedup gains compared to retraining and existing unlearning methods while maintaining comparable model accuracy to retraining from scratch.",
        "DOI": "10.1145/3580305.3599271",
        "affiliation_name": "Stevens Institute of Technology",
        "affiliation_city": "Hoboken",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "FedSkill: Privacy Preserved Interpretable Skill Learning via Imitation",
        "paper_author": "Jiang Y.",
        "publication": "Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "citied_by": "2",
        "cover_date": "2023-08-04",
        "Abstract": "Imitation learning that replicates experts' skills via their demonstrations has shown significant success in various decision-making tasks. However, two critical challenges still hinder the deployment of imitation learning techniques in real-world application scenarios. First, existing methods lack the intrinsic interpretability to explicitly explain the underlying rationale of the learned skill and thus making learned policy untrustworthy. Second, due to the scarcity of expert demonstrations from each end user (client), learning a policy based on different data silos is necessary but challenging in privacy-sensitive applications such as finance and healthcare. To this end, we present a privacy-preserved interpretable skill learning framework (FedSkill) that enables global policy learning to incorporate data from different sources and provides explainable interpretations to each local user without violating privacy and data sovereignty. Specifically, our proposed interpretable skill learning model can capture the varying patterns in the trajectories of expert demonstrations, and extract prototypical information as skills that provide implicit guidance for policy learning and explicit explanations in the reasoning process. Moreover, we design a novel aggregation mechanism coupled with the based skill learning model to preserve global information utilization and maintain local interpretability under the federated framework. Thoroughly experiments on three datasets and empirical studies demonstrate that our proposed FedSkill framework not only outperforms state-of-the-art imitation learning methods but also exhibits good interpretability under a federated setting. Our proposed FedSkill framework is the first attempt to bridge the gaps among federated learning, interpretable machine learning, and imitation learning.",
        "DOI": "10.1145/3580305.3599349",
        "affiliation_name": "University of Connecticut",
        "affiliation_city": "Storrs",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "A Dual-Agent Scheduler for Distributed Deep Learning Jobs on Public Cloud via Reinforcement Learning",
        "paper_author": "Xing M.",
        "publication": "Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "citied_by": "5",
        "cover_date": "2023-08-04",
        "Abstract": "Public cloud GPU clusters are becoming emerging platforms for training distributed deep learning jobs. Under this training paradigm, the job scheduler is a crucial component to improve user experiences, i.e., reducing training fees and job completion time, which can also save power costs for service providers. However, the scheduling problem is known to be NP-hard. Most existing work divides it into two easier sub-tasks, i.e., ordering task and placement task, which are responsible for deciding the scheduling orders of jobs and placement orders of GPU machines, respectively. Due to the superior adaptation ability, learning-based policies can generally perform better than traditional heuristic-based methods. Nevertheless, there are still two main challenges that have not been well-solved. First, most learning-based methods only focus on ordering or placement policy independently, while ignoring their cooperation. Second, the unbalanced machine performances and resource contention impose huge overhead and uncertainty on job duration, but rarely be considered in existing work. To tackle these issues, this paper presents a dual-agent scheduler framework abstracted from the two sub-tasks to jointly learn the ordering and placement policies and make better-informed scheduling decisions. Specifically, we design an ordering agent with a scalable squeeze-and-communicate strategy for better cooperation; for the placement agent, we propose a novel Random Walk Gaussian Process to learn the performance similarities of GPU machines while being aware of the uncertain performance fluctuation. Finally, the dual-agent is jointly optimized with multi-agent reinforcement learning. Extensive experiments conducted on the real-world production cluster trace demonstrate the superiority of our model.",
        "DOI": "10.1145/3580305.3599241",
        "affiliation_name": "ByteDance Ltd.",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A Novel Model based Energy Management Strategy for Plug-in Hybrid Electric Vehicles using Deep Reinforcement Learning",
        "paper_author": "Ghode S.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2023-08-03",
        "Abstract": "Over the last few years, Hybrid Electric Vehicles (HEVs) have become increasingly popular due to their potential to simplify fuel consumption and greenhouse gas emissions. The energy management of HEVs is a critical task that involves controlling the power split between the Internal Combustion Engine (ICE) and electric motor based on the vehicle's state and driving conditions. Traditional rule-based strategies for HEV energy management may not be able to adapt to varying driving conditions or optimize the vehicle's performance in real-time. To address this, researchers have explored the potential of advanced machine learning techniques, such as Deep Reinforcement Learning (DRL), as a more effective approach for HEV energy management. DRL is a subfield of machine learning that combines deep neural networks with reinforcement learning to learn an optimal control policy. Among various DRL algorithms, Deep Dyna-Q learning is a hybrid approach that combines model-based and model-free learning. Our paper introduces an innovative strategy for energy management for HEVs using Deep Dyna-Q learning that optimizes the power split in real-time based on the vehicle's state and driving conditions. We evaluate the proposed strategy on two driving cycles and compare it with Deep Q-Learning (DQL). The findings indicate that the energy management approach presented in this paper surpasses DQL in terms of vehicle performance and fuel efficiency for both driving cycles.",
        "DOI": "10.1145/3607947.3608004",
        "affiliation_name": "Council of Indian Institutes of Information Technology",
        "affiliation_city": "Gwalior",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Improving mental health care in depression: A call for action",
        "paper_author": "Eder J.",
        "publication": "European Psychiatry",
        "citied_by": "6",
        "cover_date": "2023-08-03",
        "Abstract": "Depressive disorders have one of the highest disability-adjusted life years (DALYs) of all medical conditions, which led the European Psychiatric Association to propose a policy paper, pinpointing their unmet health care and research needs. The first part focuses on what can be currently done to improve the care of patients with depression, and then discuss future trends for research and healthcare. Through the narration of clinical cases, the different points are illustrated. The necessary political framework is formulated, to implement such changes to fundamentally improve psychiatric care. The group of European Psychiatrist Association (EPA) experts insist on the need for (1) increased awareness of mental illness in primary care settings, (2) the development of novel (biological) markers, (3) the rapid implementation of machine learning (supporting diagnostics, prognostics, and therapeutics), (4) the generalized use of electronic devices and apps into everyday treatment, (5) the development of the new generation of treatment options, such as plasticity-promoting agents, and (6) the importance of comprehensive recovery approach. At a political level, the group also proposed four priorities, the need to (1) increase the use of open science, (2) implement reasonable data protection laws, (3) establish ethical electronic health records, and (4) enable better healthcare research and saving resources.",
        "DOI": "10.1192/j.eurpsy.2023.2434",
        "affiliation_name": "Norfolk and Suffolk NHS Foundation Trust",
        "affiliation_city": "Norwich",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Implementation of information technologies in the international accounting system of fuel and energy sector enterprises",
        "paper_author": "Kuzmenko H.",
        "publication": "E3S Web of Conferences",
        "citied_by": "0",
        "cover_date": "2023-08-02",
        "Abstract": "The study aims the analysis the introduction of information technologies into the international accounting system of organisations in the fuel and energy sector, in particular, to establish the optimal nomenclature composition of digital transformation tools for accounting and auditing tools for the economic activities of organisations in the fuel and energy sector, as well as to establish practical steps to implement these tools in the accounting policies of the studied enterprises. The use of two principles for determining the appropriate range of technological solutions of Accounting 4.0, namely, the expert principle (based on the analysis of 10 professional resources) and the scientometric one (based on the analysis of 500 relevant publications) with their subsequent correlative and analytical comparison, allows providing an independent and high-quality solution to the issue of identifying solutions and tools for digital transformation and modernisation of accounting processes in the economic and economic activities of fuel and energy companies. It is established today, among information technologies, the following digital tools of Accounting 4.0 are the highest priority for integration into the international accounting policy of fuel and energy companies: Cloud Computing, Blockchain Technology, Big Data, Artificial intelligence (AI), AI-based automation, Machine learning and Internet of things.",
        "DOI": "10.1051/e3sconf/202340801022",
        "affiliation_name": "Central Ukrainian National Technical University",
        "affiliation_city": "Kropyvnytskyi",
        "affiliation_country": "Ukraine"
    },
    {
        "paper_title": "Key predictors for climate policy support and political mobilization: The role of beliefs and preferences",
        "paper_author": "Simon M.",
        "publication": "PLOS Climate",
        "citied_by": "3",
        "cover_date": "2023-08-01",
        "Abstract": "Public support and political mobilization are two crucial factors for the adoption of ambitious climate policies in line with the international greenhouse gas reduction targets of the Paris Agreement. Despite their compound importance, they are mainly studied separately. Using a random forest machine-learning model, this article investigates the relative predictive power of key established explanations for public support and mobilization for climate policies. Predictive models may shape future research priorities and contribute to theoretical advancement by showing which predictors are the most and least important. The analysis is based on a pre-election conjoint survey experiment on the Swiss CO2 Act in 2021. Results indicate that beliefs (such as the perceived effectiveness of policies) and policy design preferences (such as for subsidies or tax-related policies) are the most important predictors while other established explanations, such as socio-demographics, issue salience (the relative importance of issues) or political variables (such as the party affiliation) have relatively weak predictive power. Thus, beliefs are an essential factor to consider in addition to explanations that emphasize issue salience and preferences driven by voters’ cost-benefit considerations.",
        "DOI": "10.1371/journal.pclm.0000145",
        "affiliation_name": "University of Bern",
        "affiliation_city": "Bern",
        "affiliation_country": "Switzerland"
    },
    {
        "paper_title": "A Hybrid Supervised Machine Learning Model for the Prediction of Insider Threats",
        "paper_author": "Adun I.J.",
        "publication": "NIPES - Journal of Science and Technology Research",
        "citied_by": "1",
        "cover_date": "2023-08-01",
        "Abstract": "The quest and sensitivity of organizational resources has permeated need for information confidentiality while ensuring availability and integrity are met, if organizations are to thrive and survive. To fend off malicious insider, organizations have implemented strategies, policies and techniques to manage malicious insider attacks. Machine Learning (ML) algorithms are implemented as learning paradigms, having the ability to learn from prior instances. ML present intelligent implicitly designed models having the capability of predicting possible outcomes from machine learning dataset based on perceived features which might be computationally explored. Hybrid Supervised Machine Learning Model for the Prediction of Insider Threats (HSMLM-IT) has been designed, simulated and validated utilizing Support Vector Machine (SVM) for label classification and Adaptive Neuro Fuzzy Inference System (ANFSI) for predictive learning. The SVM blocks provides a classification accuracy of 92% and precision of 93% while the ANFIS training blocks provides an ANFIS accuracy of 91% and ANFIS error of 9%.",
        "DOI": "10.5281/zenodo.8313125",
        "affiliation_name": "University of Benin",
        "affiliation_city": "Benin",
        "affiliation_country": "Nigeria"
    },
    {
        "paper_title": "Research Progress of Cathode Materials for Solid Oxide Fuel Cells",
        "paper_author": "Chang C.",
        "publication": "Xiyou Jinshu/Chinese Journal of Rare Metals",
        "citied_by": "9",
        "cover_date": "2023-08-01",
        "Abstract": "Nowadays，energy crisis and environmental pollution are two major problems affecting the development of society. Human beings are imperatively looking for efficient and environmentally friendly energy utilization methods. It is urgent to develop clean，envi⁃ ronmentally friendly，stable and reliable new energy to meet the country´s major needs in energy structure adjustment，efficient and clean use of fossil fuels and the realization of carbon peaking and carbon neutrality goals. Fuel cells have become the most promising new generation of power generation technology because of their high energy conversion rate，low pollution emissions and wide fuel sources. There are five mature fuel cell power generation systems classification by electrolyte，including phosphoric acid fuel cell （PAFC），alkaline fuel cell（AFC），molten carbonate fuel cell（MCFC），proton exchange membrane fuel cell（PEMFC）and solid ox⁃ ide fuel cell（SOFC）. Among them，SOFC is an all-solid，green，economical，pollution-free and fuel-selective power generation de⁃ vice，which has great development prospects in efficiency conversion and environment-friendly energy applications. The studies have shown that SOFC had a series of problems such as slow start，high cost，components diffusion and aging，high decay rate of perfor⁃ mance，sealing and connection difficulties working at high-temperature（800~1000 ℃）. In contrast，medium and low temperature SOFC has good stability，low cost，fast start，and wide selection of sealing materials. Therefore，it is necessary to carry out technology to develop and research of medium and low temperature SOFC. SOFC is assembled with cathode，anode，electrolyte and connecting material. The electrolyte ohmic loss and electrode polarization loss will be increased sharply when operating in the medium and at 600~ 800 ℃. At present，electrolyte ohmic loss has been effectively solved by electrolyte thin film technology. However，the slow dynam⁃ ics，instability and polarization loss of oxygen reduction reaction（ORR）in the cathode are the main challenges restricting the develop⁃ ment of SOFC technology. The mismatch between the thermal expansion coefficient of cathode materials and electrolyte materials，the problems of Cr poisoning，CO2 poisoning and SO2 poisoning also have a great impact on the low temperature process in SOFC. Develop⁃ ing high performance cathodes is the key to lower the working temperature of SOFC. SOFC with excellent performance was closely relat⁃ ed to the performance of cathode materials. It is obligatory for researchers to develop cathode materials with excellent performance，good structural stability and high efficiency that can meet the needs of medium and low temperature SOFC. This work reviewed the re⁃ cent research progress of cathode materials with different composition and microstructure. The cathode materials could be divided into perovskite（ABO3）structure，double perovskite（A2B2O6）structure，perovskite-like（An+1BnO3n+1）structure，spinel structure（AB2O4），other structural cathode materials and composite cathode materials according to the structural characteristics. Perovskite based oxides were widely favored by researchers due to their special crystal structure，high ionic conductivity，good oxygen permeability and excel⁃ lent catalytic activity. ABO3 perovskite oxide was a typical ion-electron hybrid conductor and had been widely used as a cathode materi⁃ al for SOFC. According to whether it contained cobalt elements，it could be divided into cobalt-based cathode materials and cobalt-free cathode materials. Perovskite structure cathode could not satisfy the demands of high-performance cell. Compared with the perovskite structure，double perovskite oxides had many advantages. Cathode with double perovskite structure showed higher ionic electron con⁃ ductivity，better oxygen ion diffusion and surface exchange dynamics than the perovskite oxide with the same elements in the medium and low temperature region，and had a great development prospect. Perovskite-like structural cathode materials had the advantages of stable structure，low thermal expansion coefficient，high diffusion coefficient，oxygen exchange capacity and strong surface adsorption force，etc. They had been successfully applied in medium and low temperature solid oxide fuel cells，and had become a research hot⁃ spot of SOFC cathode materials in recent years. Spinel oxide and other structural materials with some fantastic properties had become a good choice for preparing new cathode materials. In addition，the composite cathode material prepared by complex processes had fan⁃ tastic performance than the single cathode material，which could promote the progress of medium and low temperature SOFC technolo⁃ gy，and it was also another research direction for the development and preparation of SOFC cathode materials in the future. However，there were still some structural and performance defects of these several cathode materials，which prevented them from being widely used in SOFC，thus limiting the further large-scale development of SOFC. Therefore，it was still an urgent mission to study the cathode materials in-depth for medium-low temperature SOFC，so as to promote the commercialization of SOFC application. Finally，this re⁃ view looked forward to the strategies and methods for optimization cathode performance and development new cathode materials of solid oxide fuel cells in the future. In order to improve the electrochemical properties of cathode materials，it could be considered from the following aspects：（1）Modify the existing cathode materials.（2）Accelerate the development of new cathode materials.（3）Using a series of processes to prepare composite cathode materials.（4）Structural design and performance prediction of cathode materials by using first-principle calculation and machine learning methods.",
        "DOI": "10.13373/j.cnki.cjrm.XY23020016",
        "affiliation_name": "Beijing Key Laboratory of Materials Utilization of Nonmetallic Minerals and Solid Wastes",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Health Data Sharing towards Knowledge Creation",
        "paper_author": "Elvas L.B.",
        "publication": "Systems",
        "citied_by": "5",
        "cover_date": "2023-08-01",
        "Abstract": "Data sharing and service reuse in the health sector pose significant privacy and security challenges. The European Commission recognizes health data as a unique and cost-effective resource for research, while the OECD emphasizes the need for privacy-protecting data governance systems. In this paper, we propose a novel approach to health data access in a hospital environment, leveraging homomorphic encryption to ensure privacy and secure sharing of medical data among healthcare entities. Our framework establishes a secure environment that enforces GDPR adoption. We present an Information Sharing Infrastructure (ISI) framework that seamlessly integrates artificial intelligence (AI) capabilities for data analysis. Through our implementation, we demonstrate the ease of applying AI algorithms to treated health data within the ISI environment. Evaluating machine learning models, we achieve high accuracies of 96.88% with logistic regression and 97.62% with random forest. To address privacy concerns, our framework incorporates Data Sharing Agreements (DSAs). Data producers and consumers (prosumers) have the flexibility to express their prefearences for sharing and analytics operations. Data-centric policy enforcement mechanisms ensure compliance and privacy preservation. In summary, our comprehensive framework combines homomorphic encryption, secure data sharing, and AI-driven analytics. By fostering collaboration and knowledge creation in a secure environment, our approach contributes to the advancement of medical research and improves healthcare outcomes. A real case application was implemented between Portuguese hospitals and universities for this data sharing.",
        "DOI": "10.3390/systems11080435",
        "affiliation_name": "Iscte – Instituto Universitário de Lisboa",
        "affiliation_city": "Lisbon",
        "affiliation_country": "Portugal"
    },
    {
        "paper_title": "Enhancing Zero-Energy Building Operations for ESG: Accurate Solar Power Prediction through Automatic Machine Learning",
        "paper_author": "Lee S.",
        "publication": "Buildings",
        "citied_by": "3",
        "cover_date": "2023-08-01",
        "Abstract": "Solar power systems, such as photovoltaic (PV) systems, have become a necessary feature of zero-energy buildings because efficient building design and construction materials alone are not sufficient to meet the building’s energy consumption needs. However, solar power generation is subject to fluctuations based on weather conditions, and these fluctuations are higher than other renewable energy sources. This phenomenon has emphasized the importance of predicting solar power generation through weather forecasting. In this paper, an Automatic Machine Learning (AML)-based method is proposed to create multiple prediction models based on solar power generation and weather data. Then, the best model to predict daily solar power generation is selected from these models. The solar power generation data used in this study was obtained from an actual solar system installed in a zero-energy building, while the weather data was obtained from open data provided by the Korea Meteorological Administration. In addition, To verify the validity of the proposed method, an ideal data model with high accuracy but difficult to apply to the actual system and a comparison model with a relatively low accuracy but suitable for application to the actual system were created. The performance was compared with the model created by the proposed method. Based on the validation process, the proposed approach shows 5–10% higher prediction accuracies compared to the comparison model.",
        "DOI": "10.3390/buildings13082050",
        "affiliation_name": "Chung-Ang University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "Applying Machine Learning in Cloud Service Price Prediction: The Case of Amazon IaaS",
        "paper_author": "Fragiadakis G.",
        "publication": "Future Internet",
        "citied_by": "4",
        "cover_date": "2023-08-01",
        "Abstract": "When exploring alternative cloud solution designs, it is important to also consider cost. Thus, having a comprehensive view of the cloud market and future price evolution allows well-informed decisions to choose between alternatives. Cloud providers offer various service types with different pricing policies. Currently, infrastructure-as-a-Service (IaaS) is considered the most mature cloud service, while reserved instances, where virtual machines are reserved for a fixed period of time, have the largest market share. In this work, we employ a machine-learning approach based on the CatBoost algorithm to explore a price-prediction model for the reserve instance market. The analysis is based on historical data provided by Amazon Web Services from 2016 to 2022. Early results demonstrate the machine-learning model’s ability to capture the underlying evolution patterns and predict future trends. Findings suggest that prediction accuracy is not improved by integrating data from older time periods.",
        "DOI": "10.3390/fi15080277",
        "affiliation_name": "Harokopio University of Athens",
        "affiliation_city": "Athens",
        "affiliation_country": "Greece"
    },
    {
        "paper_title": "Assessing Sustainable Impacts of Green Energy Projects for the Development of Renewable Energy Technologies: A Triple Bottom Line Approach",
        "paper_author": "Liao Z.",
        "publication": "Processes",
        "citied_by": "7",
        "cover_date": "2023-08-01",
        "Abstract": "The escalating global concern for sustainable development necessitates an in-depth understanding of the role of renewable energy projects. Evaluating their impact on economic, environmental, and social sustainability is of significant importance. In this study, the impact of green energy projects on economic, environmental, and social sustainability across APEC countries from 2010 to 2021 is comprehensively assessed using machine learning models. The employed machine learning models revealed associations between key variables and sustainability implications of green energy projects. Renewable energy consumption emerged as a significant contributor to economic performance, scoring a compelling importance score of 0.34. Concurrently, fossil fuel energy consumption and urban population were identified as key influencers on environmental outcomes and social impacts, respectively, with importance scores of 0.36 and 0.42. The empirical evidence presented in this research underscores the pivotal role of renewable energy projects in driving economic development, counteracting environmental harm, and facilitating urban electricity access, while also noting the counteracting effect of fossil fuel consumption. The study’s outcomes are intended to guide future research directions and inform policy formulations, contributing significantly to global sustainability discourse.",
        "DOI": "10.3390/pr11082228",
        "affiliation_name": "Hanjiang Normal University",
        "affiliation_city": "Shiyan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Preventing Multidrug-Resistant Bacterial Transmission in the Intensive Care Unit with a Comprehensive Approach: A Policymaking Manual",
        "paper_author": "Schinas G.",
        "publication": "Antibiotics",
        "citied_by": "10",
        "cover_date": "2023-08-01",
        "Abstract": "Patients referred to intensive care units (ICU) commonly contract infections caused by multidrug-resistant (MDR) bacteria, which are typically linked to complications and high mortality. There are numerous independent factors that are associated with the transmission of these pathogens in the ICU. Preventive multilevel measures that target these factors are of great importance in order to break the chain of transmission. In this review, we aim to provide essential guidance for the development of robust prevention strategies, ultimately ensuring the safety and well-being of patients and healthcare workers in the ICU. We discuss the role of ICU personnel in cross-contamination, existing preventative measures, novel technologies, and strategies employed, along with antimicrobial surveillance and stewardship (AMSS) programs, to construct effective and thoroughly described policy recommendations. By adopting a multifaceted approach that combines targeted interventions with broader preventive strategies, healthcare facilities can create a more coherent line of defense against the spread of MDR pathogens. These recommendations are evidence-based, practical, and aligned with the needs and realities of the ICU setting. In conclusion, this comprehensive review offers a blueprint for mitigating the risk of MDR bacterial transmission in the ICU, advocating for an evidence-based, multifaceted approach.",
        "DOI": "10.3390/antibiotics12081255",
        "affiliation_name": "University General Hospital of Patras",
        "affiliation_city": "Rio",
        "affiliation_country": "Greece"
    },
    {
        "paper_title": "Machine-Learning Techniques for Predicting Phishing Attacks in Blockchain Networks: A Comparative Study",
        "paper_author": "Joshi K.",
        "publication": "Algorithms",
        "citied_by": "13",
        "cover_date": "2023-08-01",
        "Abstract": "Security in the blockchain has become a topic of concern because of the recent developments in the field. One of the most common cyberattacks is the so-called phishing attack, wherein the attacker tricks the miner into adding a malicious block to the chain under genuine conditions to avoid detection and potentially destroy the entire blockchain. The current attempts at detection include the consensus protocol; however, it fails when a genuine miner tries to add a new block to the blockchain. Zero-trust policies have started making the rounds in the field as they ensure the complete detection of phishing attempts; however, they are still in the process of deployment, which may take a significant amount of time. A more accurate measure of phishing detection involves machine-learning models that use specific features to automate the entire process of classifying an attempt as either a phishing attempt or a safe attempt. This paper highlights several models that may give safe results and help eradicate blockchain phishing attempts.",
        "DOI": "10.3390/a16080366",
        "affiliation_name": "AIR Institute, Spain",
        "affiliation_city": "Carbajosa de la Sagrada",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "Forecasting Turning Points of Carbon Emissions in Beijing Based on Interpretable Machine Learning",
        "paper_author": "Yao T.",
        "publication": "Atmosphere",
        "citied_by": "0",
        "cover_date": "2023-08-01",
        "Abstract": "For curbing the global climate crisis, China has set an ambitious target of peak carbon emissions by 2030. Beijing, the capital of China, has implemented a carbon reduction policy since 2012. Using the reduced and generalized forms of the Environmental Kuznets Curve (EKC), we deduce that both the cubic EKC and the genetic algorithm-based EKC have an N-shape. The first turning point of the three-order EKC occurs around 2011, demonstrating the effectiveness of the carbon reduction policy. However, the time series model predicts that Beijing will reach the second turning point around 2026, when the gross domestic product (GDP) is about CNY 5000 billion and carbon emissions will begin to increase again. Interpretable machine learning is proposed to explore the socio-economic drivers in carbon emissions, indicating that total energy consumption and GDP contribute the most. Therefore, we should accelerate the upgrading of energy consumption and adjust the industrial structure, thus facilitating Beijing to its peak carbon emissions and achieving carbon neutrality.",
        "DOI": "10.3390/atmos14081288",
        "affiliation_name": "Peking University Health Science Center",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Land Use and Land Cover Classification in the Northern Region of Mozambique Based on Landsat Time Series and Machine Learning",
        "paper_author": "Macarringue L.S.",
        "publication": "ISPRS International Journal of Geo-Information",
        "citied_by": "2",
        "cover_date": "2023-08-01",
        "Abstract": "Accurate land use and land cover (LULC) mapping is essential for scientific and decision-making purposes. The objective of this paper was to map LULC classes in the northern region of Mozambique between 2011 and 2020 based on Landsat time series processed by the Random Forest classifier in the Google Earth Engine platform. The feature selection method was used to reduce redundant data. The final maps comprised five LULC classes (non-vegetated areas, built-up areas, croplands, open evergreen and deciduous forests, and dense vegetation) with an overall accuracy ranging from 80.5% to 88.7%. LULC change detection between 2011 and 2020 revealed that non-vegetated areas had increased by 0.7%, built-up by 2.0%, and dense vegetation by 1.3%. On the other hand, open evergreen and deciduous forests had decreased by 4.1% and croplands by 0.01%. The approach used in this paper improves the current systematic mapping approach in Mozambique by minimizing the methodological gaps and reducing the temporal amplitude, thus supporting regional territorial development policies.",
        "DOI": "10.3390/ijgi12080342",
        "affiliation_name": "Embrapa Agricultura Digital",
        "affiliation_city": "Campinas",
        "affiliation_country": "Brazil"
    },
    {
        "paper_title": "Impact of Green Energy Transportation Systems on Urban Air Quality: A Predictive Analysis Using Spatiotemporal Deep Learning Techniques",
        "paper_author": "Mumtaz R.",
        "publication": "Energies",
        "citied_by": "4",
        "cover_date": "2023-08-01",
        "Abstract": "Transitioning to green energy transport systems, notably electric vehicles, is crucial to both combat climate change and enhance urban air quality in developing nations. Urban air quality is pivotal, given its impact on health, necessitating accurate pollutant forecasting and emission reduction strategies to ensure overall well-being. This study forecasts the influence of green energy transport systems on the air quality in Lahore and Islamabad, Pakistan, while noting the projected surge in electric vehicle adoption from less than 1% to 10% within three years. Predicting the impact of this change involves analyzing data before, during, and after the COVID-19 pandemic. The lockdown led to minimal fossil fuel vehicle usage, resembling a green energy transportation scenario. The novelty of this work is twofold. Firstly, remote sensing data from the Sentinel-5P satellite were utilized to predict air quality index (AQI) trends before, during, and after COVID-19. Secondly, deep learning models, including long short-term memory (LSTM) and bidirectional LSTM, and machine learning models, including decision tree and random forest regression, were utilized to forecast the levels of NO (Formula presented.), SO (Formula presented.), and CO in the atmosphere. Our results demonstrate that implementing green energy transportation systems in urban centers of developing countries can enhance air quality by approximately 98%. Notably, the bidirectional LSTM model outperformed others in predicting NO (Formula presented.) and SO (Formula presented.) concentrations, while the LSTM model excelled in forecasting CO concentration. These results offer valuable insights into predicting air pollution levels and guiding green energy policies to mitigate the adverse health effects of air pollution.",
        "DOI": "10.3390/en16166087",
        "affiliation_name": "NDSU College of Engineering",
        "affiliation_city": "Fargo",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Deep Reinforcement Learning Based Robust Communication for Internet of Vehicles",
        "paper_author": "Rim Gasmi ",
        "publication": "Automatic Control and Computer Sciences",
        "citied_by": "1",
        "cover_date": "2023-08-01",
        "Abstract": "Abstract: The high number of connected nodes in Internet of Vehicles (IoVs) drives to high data exchange between nodes, which increases the network overhead. Moreover, the recurrent change in vehicle mobility in Internet of Vehicles (IoVs) drives to frequent changes in network topology which in turn causes frequent link disconnections. Therefore, the most addressed issues in IoVs are to manage the high quantity of packets sent by the huge number of vehicles connected with IoT devices, to reduce communication delays and guarantee the longest communication stability. Clustering techniques have been utilized to reduce network overhead in IoVs networks. Classical clustering algorithms have been proposed to enhance network performances. However, IoVs environment is characterized by the high dynamicity of nodes, therefore, the optimization methods already proposed cannot perfectly deal with the characteristics of IoVs. Reinforcement learning (RL) is a machine learning algorithm, where the agent learns from its environment and tries to enhance its policies to obtain the best reward. In this paper, we propose to use deep reinforcement learning (DRL) to select the best cluster heads based on node’s degree, node’s buffer size, and signal strength. In the proposed work, the vehicle can perfectly select the cluster heads by choosing the best state-action values taking in consideration the high dynamicity of the network.",
        "DOI": "10.3103/S014641162304003X",
        "affiliation_name": "Centre Universitaire Illizi",
        "affiliation_city": "Illizi",
        "affiliation_country": "Algeria"
    },
    {
        "paper_title": "A Multi-Task Fusion Strategy-Based Decision-Making and Planning Method for Autonomous Driving Vehicles",
        "paper_author": "Liu W.",
        "publication": "Sensors",
        "citied_by": "7",
        "cover_date": "2023-08-01",
        "Abstract": "The autonomous driving technology based on deep reinforcement learning (DRL) has been confirmed as one of the most cutting-edge research fields worldwide. The agent is enabled to achieve the goal of making independent decisions by interacting with the environment and learning driving strategies based on the feedback from the environment. This technology has been widely used in end-to-end driving tasks. However, this field faces several challenges. First, developing real vehicles is expensive, time-consuming, and risky. To further expedite the testing, verification, and iteration of end-to-end deep reinforcement learning algorithms, a joint simulation development and validation platform was designed and implemented in this study based on VTD–CarSim and the Tensorflow deep learning framework, and research work was conducted based on this platform. Second, sparse reward signals can cause problems (e.g., a low-sample learning rate). It is imperative for the agent to be capable of navigating in an unfamiliar environment and driving safely under a wide variety of weather or lighting conditions. To address the problem of poor generalization ability of the agent to unknown scenarios, a deep deterministic policy gradient (DDPG) decision-making and planning method was proposed in this study in accordance with a multi-task fusion strategy. The main task based on DRL decision-making planning and the auxiliary task based on image semantic segmentation were cross-fused, and part of the network was shared with the main task to reduce the possibility of model overfitting and improve the generalization ability. As indicated by the experimental results, first, the joint simulation development and validation platform built in this study exhibited prominent versatility. Users were enabled to easily substitute any default module with customized algorithms and verify the effectiveness of new functions in enhancing overall performance using other default modules of the platform. Second, the deep reinforcement learning strategy based on multi-task fusion proposed in this study was competitive. Its performance was better than other DRL algorithms in certain tasks, which improved the generalization ability of the vehicle decision-making planning algorithm.",
        "DOI": "10.3390/s23167021",
        "affiliation_name": "Zhejiang University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Model-Based Predictive Control and Reinforcement Learning for Planning Vehicle-Parking Trajectories for Vertical Parking Spaces",
        "paper_author": "Shi J.",
        "publication": "Sensors",
        "citied_by": "8",
        "cover_date": "2023-08-01",
        "Abstract": "This paper proposes a vehicle-parking trajectory planning method that addresses the issues of a long trajectory planning time and difficult training convergence during automatic parking. The process involves two stages: finding a parking space and parking planning. The first stage uses model predictive control (MPC) for trajectory tracking from the initial position of the vehicle to the starting point of the parking operation. The second stage employs the proximal policy optimization (PPO) algorithm to transform the parking behavior into a reinforcement learning process. A four-dimensional reward function is set to evaluate the strategy based on a formal reward, guiding the adjustment of neural network parameters and reducing the exploration of invalid actions. Finally, a simulation environment is built for the parking scene, and a network framework is designed. The proposed method is compared with the deep deterministic policy gradient and double-delay deep deterministic policy gradient algorithms in the same scene. Results confirm that the MPC controller accurately performs trajectory-tracking control with minimal steering wheel angle changes and smooth, continuous movement. The PPO-based reinforcement learning method achieves shorter learning times, totaling only 30% and 37.5% of the deep deterministic policy gradient (DDPG) and twin-delayed deep deterministic policy gradient (TD3), and the number of iterations to reach convergence for the PPO algorithm with the introduction of the four-dimensional evaluation metrics is 75% and 68% shorter compared to the DDPG and TD3 algorithms, respectively. This study demonstrates the effectiveness of the proposed method in addressing a slow convergence and long training times in parking trajectory planning, improving parking timeliness.",
        "DOI": "10.3390/s23167124",
        "affiliation_name": "Chongqing University of Posts and Telecommunications",
        "affiliation_city": "Chongqing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Evaluation of the Use of the 12 Bands vs. NDVI from Sentinel-2 Images for Crop Identification",
        "paper_author": "Lozano-Tello A.",
        "publication": "Sensors",
        "citied_by": "2",
        "cover_date": "2023-08-01",
        "Abstract": "Today, machine learning applied to remote sensing data is used for crop detection. This makes it possible to not only monitor crops but also to detect pests, a lack of irrigation, or other problems. For systems that require high accuracy in crop identification, a large amount of data is required to generate reliable models. The more plots of and data on crop evolution used over time, the more reliable the models. Here, a study has been carried out to analyse neural network models trained with the Sentinel satellite’s 12 bands, compared to models that only use the NDVI, in order to choose the most suitable model in terms of the amount of storage, calculation time, accuracy, and precision. This study achieved a training time gain of 59.35% for NDVI models compared with 12-band models; however, models based on 12-band values are 1.96% more accurate than those trained with the NDVI alone when it comes to making predictions. The findings of this study could be of great interest to administrations, businesses, land managers, and researchers who use satellite image data mining techniques and wish to design an efficient system, particularly one with limited storage capacity and response times.",
        "DOI": "10.3390/s23167132",
        "affiliation_name": "Universidad de Extremadura",
        "affiliation_city": "Badajoz",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "Evolution of research on climate risk insurance: A bibliometric analysis from 1975 to 2022",
        "paper_author": "Lin Y.H.",
        "publication": "Advances in Climate Change Research",
        "citied_by": "4",
        "cover_date": "2023-08-01",
        "Abstract": "Insurance against climate risk is essential for mitigating the adverse effects of climate change. However, theoretical consensus regarding climate risk insurance remains elusive, and the implementation of climate insurance policies varies markedly between countries owing to various challenges. This study conducted bibliometric analysis of 1082 relevant publications (1975–2022) to determine the theoretical basis, evolution of research hotspots, and methodologies associated with climate risk insurance. Climate insurance publications are growing at an average annual rate of 8.9%, with more than 2333 authors from 1103 organizations in 78 countries publishing on the subject. On the basis of milestones of global climate change assessment, i.e., the publication of the Fourth and Fifth Assessment Reports of the Intergovernmental Panel on Climate Change, climate insurance research can be divided into three major phases. In the start-up phase (1975–2007), research schemes examined the feasibilities and potentials of the National Flood Insurance Program in the United States, and the socioeconomic implications of transferring climate risk through reinsurance. The methodologies used in these studies were relatively simple owing to lack of comprehensive data. Research on flood insurance increased rapidly during the development phase (2008–2014), with increasing emphasis on the possibility of developing a flood insurance market in the Netherlands. Studies utilized catastrophe modeling and probabilistic approaches to estimate natural disaster losses and financial impacts. The boom phase (2015–2022) involved more research on the affordability of climate risk insurance given income inequality. The topic of climate insurance and the scope of its impact have developed global and interdisciplinary characteristics in terms of journal, sector, and disciplinary base. In the future, a trend might develop whereby big data will be combined with artificial intelligence and machine learning to design and implement index insurance.",
        "DOI": "10.1016/j.accre.2023.08.003",
        "affiliation_name": "Renmin University of China",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Prehospital prediction of hospital admission for emergent acuity patients transported by paramedics: A population-based cohort study using machine learning",
        "paper_author": "Strum R.P.",
        "publication": "PLoS ONE",
        "citied_by": "0",
        "cover_date": "2023-08-01",
        "Abstract": "Introduction The closest emergency department (ED) may not always be the optimal hospital for certain stable high acuity patients if further distanced ED’s can provide specialized care or are less overcrowded. Machine learning (ML) predictions may support paramedic decision-making to transport a subgroup of emergent patients to a more suitable, albeit more distanced, ED if hospital admission is unlikely. We examined whether characteristics known to paramedics in the prehospital setting were predictive of hospital admission in emergent acuity patients. Materials and methods We conducted a population-level cohort study using four ML algorithms to analyze ED visits of the National Ambulatory Care Reporting System from January 1, 2018 to December 31, 2019 in Ontario, Canada. We included all adult patients (≥18 years) transported to the ED by paramedics with an emergent Canadian Triage Acuity Scale score. We included eight characteristic classes as model predictors that are recorded at ED triage. All ML algorithms were trained and assessed using 10-fold cross-validation to predict hospital admission from the ED. Predictive model performance was determined using the area under curve (AUC) with 95% confidence intervals and probabilistic accuracy using the Brier Scaled score. Variable importance scores were computed to determine the top 10 predictors of hospital admission. Results All machine learning algorithms demonstrated acceptable accuracy in predicting hospital admission (AUC 0.77–0.78, Brier Scaled 0.22–0.24). The characteristics most predictive of admission were age between 65 to 105 years, referral source from a residential care facility, presenting with a respiratory complaint, and receiving home care. Discussion Hospital admission was accurately predicted based on patient characteristics known prehospital to paramedics prior to arrival. Our results support consideration of policy modification to permit certain emergent acuity patients to be transported to a further distanced ED. Additionally, this study demonstrates the utility of ML in paramedic and prehospital research.",
        "DOI": "10.1371/journal.pone.0289429",
        "affiliation_name": "McMaster University",
        "affiliation_city": "Hamilton",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Traffic Management in IoT Backbone Networks Using GNN and MAB with SDN Orchestration",
        "paper_author": "Guo Y.",
        "publication": "Sensors",
        "citied_by": "12",
        "cover_date": "2023-08-01",
        "Abstract": "Traffic management is a critical task in software-defined IoT networks (SDN-IoTs) to efficiently manage network resources and ensure Quality of Service (QoS) for end-users. However, traditional traffic management approaches based on queuing theory or static policies may not be effective due to the dynamic and unpredictable nature of network traffic. In this paper, we propose a novel approach that leverages Graph Neural Networks (GNNs) and multi-arm bandit algorithms to dynamically optimize traffic management policies based on real-time network traffic patterns. Specifically, our approach uses a GNN model to learn and predict network traffic patterns and a multi-arm bandit algorithm to optimize traffic management policies based on these predictions. We evaluate the proposed approach on three different datasets, including a simulated corporate network (KDD Cup 1999), a collection of network traffic traces (CAIDA), and a simulated network environment with both normal and malicious traffic (NSL-KDD). The results demonstrate that our approach outperforms other state-of-the-art traffic management methods, achieving higher throughput, lower packet loss, and lower delay, while effectively detecting anomalous traffic patterns. The proposed approach offers a promising solution to traffic management in SDNs, enabling efficient resource management and QoS assurance.",
        "DOI": "10.3390/s23167091",
        "affiliation_name": "Hanyang University ERICA Campus",
        "affiliation_city": "Ansan",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "Using PPO Models to Predict the Value of the BNB Cryptocurrency",
        "paper_author": "Firsov D.V.",
        "publication": "Emerging Science Journal",
        "citied_by": "2",
        "cover_date": "2023-08-01",
        "Abstract": "This paper identifies hidden patterns between trading volumes and the market value of an asset. Based on open market data, we try to improve the existing corpus of research using new, innovative neural network training methods. Dividing into two independent models, we conducted a comparative analysis between two methods of training Proximal Policy Optimization (PPO) models. The primary difference between the two PPO models is the data. To showcase the drastic differences the PPO model makes in market conditions, one model uses historical data from Binance trading history as a data sample and the trading pair BNB/USDT as a predicted asset. Another model, apart from purely price fluctuations, also draws data on trading volume. That way, we can clearly illustrate what the difference can be if we add additional markers for model training. Using PPO models, the authors conduct a comparative analysis of prediction accuracy, taking the sequence of BNB token values and trading volumes on 15-minute candles as variables. The main research question of this paper is to identify an increase in the accuracy of the PPO model when adding additional variables. The primary research gap that we explore is whether PPO models specifically trained on highly volatile assets can be improved by adding additional markers that are closely linked. In our study, we identified the closest marker, which is a trading volume. The study results show that including additional parameters in the form of trading volume significantly reduces the model's accuracy. The scientific contribution of this research is that it shows in practice that the PPO model does not require additional parameters to form accurately predicting models within the framework of market forecasting.",
        "DOI": "10.28991/ESJ-2023-07-04-012",
        "affiliation_name": "Financial University under the Government of the Russian Federation",
        "affiliation_city": "Moscow",
        "affiliation_country": "Russian Federation"
    },
    {
        "paper_title": "A novel online multi-task learning for COVID-19 multi-output spatio-temporal prediction",
        "paper_author": "Wu Z.",
        "publication": "Heliyon",
        "citied_by": "1",
        "cover_date": "2023-08-01",
        "Abstract": "In light of the ongoing COVID-19 pandemic, predicting its trend would significantly impact decision-making. However, this is not a straightforward task due to three main difficulties: temporal autocorrelation, spatial dependency, and concept drift caused by virus mutations and lockdown policies. Although machine learning has been extensively used in related work, no previous research has successfully addressed all three challenges simultaneously. To overcome this challenge, we developed a novel online multi-task regression algorithm that incorporates a chain structure to capture spatial dependency, the ADWIN drift detector to adapt to concept drift, and the lag time series feature to capture temporal autocorrelation. We conducted several comparative experiments based on the number of daily confirmed cases in 20 areas in California and affiliated cities. The results from our experiments demonstrate that our proposed model is superior in adapting to concept drift in COVID-19 data and capturing spatial dependencies across various regions. This leads to a significant improvement in prediction accuracy when compared to existing state-of-the-art batch machine learning methods, such as N-Beats, DeepAR, TCN, and LSTM.",
        "DOI": "10.1016/j.heliyon.2023.e18771",
        "affiliation_name": "Universiti Malaya",
        "affiliation_city": "Kuala Lumpur",
        "affiliation_country": "Malaysia"
    },
    {
        "paper_title": "Leading consumption patterns of psychoactive substances in Colombia: A deep neural network-based clustering-oriented embedding approach",
        "paper_author": "Palomino K.",
        "publication": "PLoS ONE",
        "citied_by": "2",
        "cover_date": "2023-08-01",
        "Abstract": "The number of health-related incidents caused using illegal and legal psychoactive substances (PAS) has dramatically increased over two decades worldwide. In Colombia, the use of illicit substances has increased up to 10.3%, while the consumption alcohol and tobacco has increased to 84% and 12%, respectively. It is well-known that identifying drug consumption patterns in the general population is essential in reducing overall drug consumption. However, existing approaches do not incorporate Machine Learning and/or Deep Data Mining methods in combination with spatial techniques. To enhance our understanding of mental health issues related to PAS and assist in the development of national policies, here we present a novel Deep Neural Network-based Clustering-oriented Embedding Algorithm that incorporates an autoencoder and spatial techniques. The primary goal of our model is to identify general and spatial patterns of drug consumption and abuse, while also extracting relevant features from the input data and identifying clusters during the learning process. As a test case, we used the largest publicly available database of legal and illegal PAS consumption comprising 49,600 Colombian households. We estimated and geographically represented the prevalence of consumption and/or abuse of both PAS and non-PAS, while achieving statistically significant goodness-of-fit values. Our results indicate that region, sex, housing type, socioeconomic status, age, and variables related to household finances contribute to explaining the patterns of consumption and/or abuse of PAS. Additionally, we identified three distinct patterns of PAS consumption and/or abuse. At the spatial level, these patterns indicate concentrations of drug consumption in specific regions of the country, which are closely related to specific geographic locations and the prevailing social and environmental contexts. These findings can provide valuable insights to facilitate decision-making and develop national policies targeting specific groups given their cultural, geographic, and social conditions.",
        "DOI": "10.1371/journal.pone.0290098",
        "affiliation_name": "Universidad del Norte",
        "affiliation_city": "Barranquilla",
        "affiliation_country": "Colombia"
    },
    {
        "paper_title": "How to understand high global food price? Using SHAP to interpret machine learning algorithm",
        "paper_author": "Han X.",
        "publication": "PLoS ONE",
        "citied_by": "4",
        "cover_date": "2023-08-01",
        "Abstract": "The global food prices have surged to historical highs, and there is no consensus on the reasons behind this round of price increases in academia. Based on theoretical analysis, this study uses monthly data from January 2000 to May 2022 and machine learning models to examine the root causes of that period’s global food price surge and global food security situation. The results show that: Firstly, the increase in the supply of US dollars and the rise in oil prices during pandemic are the two most important variables affecting food prices. The unlimited quantitative easing monetary policy of the US dollar is the primary factor driving the global food price surge, and the alternating impact of oil prices and excessive US dollar liquidity are key features of the surge. Secondly, in the context of the global food shortage, the impact of food production reduction and demand growth expectations on food prices will further increase. Thirdly, attention should be paid to potential agricultural import supply chain risks arising from international uncertainty factors such as the ongoing Russia-Ukraine conflict. The Russian-Ukrainian conflict has profoundly impacted the global agricultural supply chain, and crude oil and fertilizers have gradually become the main driving force behind the rise in food prices.",
        "DOI": "10.1371/journal.pone.0290120",
        "affiliation_name": "Ministry of Agriculture of the People's Republic of China",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Evaluating the determinants of support for police militarization among officers",
        "paper_author": "Welch R.M.",
        "publication": "Politics and Policy",
        "citied_by": "1",
        "cover_date": "2023-08-01",
        "Abstract": "We evaluate the determinants of officer support for their agency participating in the 1033 Program: a program that facilitates the flow of military hardware to local law enforcement agencies. In doing so, we provide insight into why officers demand such equipment, which, in turn, may partially explain patterns of program participation and equipment usage. We utilize a series of random forest models to examine survey data collected from officers in a large police department, finding that being White and exhibiting animus toward minority communities are highly predictive of officer support across models. Our findings validate long-held public concerns regarding the distributional patterns and consequences of 1033 transfers: concerns that have led to a number of proposed policy changes at the state and federal levels meant to restrict program usage (e.g., EO-13688, HR-1694, MO HB-330). Policy makers should consider how out-group animus may drive distributional patterns and usage when considering policy reform.",
        "DOI": "10.1111/polp.12549",
        "affiliation_name": "Arkansas State University",
        "affiliation_city": "Jonesboro",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Investigating the Effect of Students’ Knowledge, Beliefs, and Digital Citizenship Skills on the Prevention of Cybercrime",
        "paper_author": "Althibyani H.A.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "7",
        "cover_date": "2023-08-01",
        "Abstract": "The growing prevalence of cybercrime, particularly among young adults, necessitates the promotion of digital citizenship to educate students about responsible online behavior and to equip them with the skills to mitigate cyber risks. The specific objective of this study was to investigate the effect of digital citizenship skills on the prevention of cybercrime among higher education students. A mixed-method approach, including surveys and interviews, was employed to collect data from 652 students in Saudi Arabia. This study found that digital citizenship generally has a significant impact on students’ awareness and prevention of cybercrime through the development of responsible online behavior. Knowledge of digital law came first, followed by beliefs about digital manners. Digital communication skills came third, followed by digital rights, knowledge, and duties in fourth place. Then, digital commerce skills and digital health beliefs came fifth and sixth, respectively. This was followed by digital access skills, then digital security, and finally digital culture. The results also revealed a negative statistical relationship between digital citizenship and cybercrimes’ various forms including national, financial, banking, social, immoral, insulting, slanderous, defaming, threatening, and harassment in virtual learning environments. These findings have significant implications for the understanding of how higher education institutions can promote digital citizenship and prevent cybercrime by integrating digital citizenship education into their curriculum, providing training for educators, and establishing clear policies and guidelines for responsible online behavior.",
        "DOI": "10.3390/su151511512",
        "affiliation_name": "University of Jeddah",
        "affiliation_city": "Jeddah",
        "affiliation_country": "Saudi Arabia"
    },
    {
        "paper_title": "Spatiotemporal Characteristics Prediction and Driving Factors Analysis of NPP in Shanxi Province Covering the Period 2001–2020",
        "paper_author": "Ba W.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "3",
        "cover_date": "2023-08-01",
        "Abstract": "The advent of a range of high-precision NPP products, including MODIS NPP, MOD17 NPP, and GIMMS NPP, has sparked growing interest in the study of Earth’s ecosystems. In order to enhance comprehension of ecosystem health, in order to facilitate the development of rational resource management and environmental conservation policies, this investigation employs the MOD17A3 dataset to analyze historical variations in Net Primary Productivity (NPP) within Shanxi Province from 2001 to 2020, while also exploring future trends. The Theil–Sen median trend analysis and Mann–Kendall test are commonly used methods for analyzing time series data, employed to study the spatiotemporal trends and variations in NPP. The Grey Wolf Optimization–Support Vector Machine (GWO–SVM) model combines optimization algorithms and machine learning methods, enhancing the predictive capacity of the model for future NPP time series changes. Conversely, the Hurst exponent utilizes historical NPP trends to assess the persistence characteristics of NPP and predict future spatial variations in NPP. This study additionally investigates the natural driving factors of NPP using the Geographic Detector approach. The key findings of this study are as follows. (1) Overall, NPP in Shanxi Province exhibits a fluctuating upward trend from 2001 to 2020, with an average value of 206.278 gCm−2a−1. Spatially, NPP exhibits a northwest–low and southeast–high pattern, with significant spatial heterogeneity and considerable variability. (2) The average Hurst exponent is 0.86, indicating a characteristic of strong persistence in growth in future NPP. Regions with strong or higher persistent growth account for 95.54% of the total area. (3) According to the CMIP6 climate scenarios, NPP is projected to gradually increase from 2025 to 2030. (4) The interactive effects between natural factors contribute more to NPP variations than individual factors, with the rainfall–elevation interaction having the highest contribution percentage.",
        "DOI": "10.3390/su151512070",
        "affiliation_name": "Beijing Normal University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "The Effect of Local Government Environmental Concern on Corporate Environmental Investment: Evidence from China",
        "paper_author": "Yu D.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "7",
        "cover_date": "2023-08-01",
        "Abstract": "This paper uses machine learning tools to construct local government environmental concern indicators and empirically examines the impact of local government environmental concern on corporate environmental investment. From the resource endowment perspective, corporate resources’ moderating role is also verified. The major findings are as follows: (1) local government environmental concern has a significant positive effect on the environmental investment of corporations in their jurisdictions; (2) corporations with fewer financial and political resources will pay more attention to the local government’s intention when making environmental investment decisions, and the promotion effect of local government environmental concern on the environmental investment of such corporations is more prominent. Further analysis shows that this promotion effect is more significant in regions with a high intensity of environmental regulation and high levels of economic development, and is more effective for key regulated corporations. This paper verifies the effect of local government on micro-corporations in environmental governance from the perspective of environmental concern, broadens the boundary of research on the relationship between government and corporate environmental responsibility fulfillment, and enriches the study of factors influencing corporate environmental investment behavior. It also provides important empirical evidence for central and local governments to implement green development and build a government–business collaborative environmental governance system.",
        "DOI": "10.3390/su151511604",
        "affiliation_name": "Zhejiang Agriculture and Forestry University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Forecasting and Inventory Planning: An Empirical Investigation of Classical and Machine Learning Approaches for Svanehøj’s Future Software Consolidation",
        "paper_author": "Wahedi H.J.",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "4",
        "cover_date": "2023-08-01",
        "Abstract": "Challenges related to effective supply and demand planning and inventory management impose critical planning issues for many small and medium-sized enterprises (SMEs). In recent years, data-driven methods in machine learning (ML) algorithms have provided beneficial results for many large-scale enterprises (LSE). However, ML applications have not yet been tested in SMEs, leaving a technological gap. Limited recourse capabilities and financial constraints expose the risk of implementing an insufficient enterprise resource planning (ERP) setup, which amplifies the need for additional support systems for data-driven decision-making. We found the forecasts and determination of inventory management policies in SMEs are often based on subjective decisions, which might fail to capture the complexity of achieving performance goals. Our research aims to utilize the leverage of ML models for SMEs within demand and inventory management by considering various key performance indicators (KPI). The research is based on collaboration with a Danish SME that faced issues related to forecasting and inventory planning. We implemented the following ML models: Artificial Neural Network (ANN), Long Short-Term Memory (LSTM), Support Vector Regression (SVR), Random Forest (RF), Wavelet-ANN (W-ANN), and Wavelet-LSTM (W-LSTM) for forecasting purposes and reinforcement learning approaches, namely Q-learning and Deep Q Network (DQN) for inventory management. Results demonstrate that predictive ML models perform superior concerning the statistical forecasting approaches, but not always if we focus on industrial KPIs. However, when ML models are solely considered, the results indicate careful consideration must be regarded, given that model evaluation can be perceived from an academic and managerial perspective. Secondly, Q-learning is found to yield preferable economic results in terms of inventory planning. The proposed models can serve as an extension to modern ERP systems by offering a data-driven approach to demand and supply planning decision-making.",
        "DOI": "10.3390/app13158581",
        "affiliation_name": "Aalborg University",
        "affiliation_city": "Aalborg",
        "affiliation_country": "Denmark"
    },
    {
        "paper_title": "Identification and Prediction Network Analysis Based on Multivariate Data of Urban Form: A Case Study of Shenzhen, China",
        "paper_author": "Yu Z.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "1",
        "cover_date": "2023-08-01",
        "Abstract": "The rapid growth of urban populations has resulted in a scarcity of land, thus making sustainable urban development an urgent matter. Although Shenzhen has implemented land policies and optimized its functional layouts, these measures have inadvertently contributed to a shortage of available land for development. The city’s exponential population growth and expansive urban expansion have outpaced the supply of land. This study endeavors to identify urban commercial patterns by employing multiple data sources and applying machine learning and network analysis to predict future commercial areas. The results demonstrated that the identification of commercial points of interest and analysis of land surface temperature distributions made Futian district the primary area for ongoing commercial development, while also revealing a positive correlation between these two datasets. By leveraging network analysis to thoroughly examine this data, Bao’an district was highlighted as the future focal point for Shenzhen’s commercial sector, with 22 core nodes identified in total. Finally, by assessing the network centrality within the spatial networks, and utilizing clustering algorithms to categorize nodes into groups, the economic clustering pattern was determined as the predominant model for Shenzhen’s commercial growth. This research represents a significant contribution to the realm of sustainable urban development and presents a valuable framework for other cities to adopt.",
        "DOI": "10.3390/su151511857",
        "affiliation_name": "Huazhong University of Science and Technology",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Knowledge Mapping with CiteSpace, VOSviewer, and SciMAT on Intelligent Connected Vehicles: Road Safety Issue",
        "paper_author": "Ji W.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "12",
        "cover_date": "2023-08-01",
        "Abstract": "The rapid development of the Intelligent connected vehicle (ICV) industry has stimulated technological innovation in energy and communication while also highlighting the need for effective policies and road safety measures. Understanding and addressing road safety issues in the context of ICVs can contribute to ICV development and safe driving. This paper employs a knowledge mapping approach to scientifically and intuitively demonstrate research on the road safety issues of ICV over the last decade. By utilizing bibliometric tools such as CiteSpace, VOSviewer, and SciMAT, a total of 3661 original articles from the Web of Science are examined to explore three aspects. Firstly, the study investigates the collaborative relationships among authors and institutions within the industry. Secondly, it summarizes major research topics by analyzing and clustering keywords. Lastly, the paper identifies research hotspots and predicts future research directions. The findings reveal a dynamic field characterized by close collaboration among diverse institutions, with China and the United States emerging as the most active countries and mathematics and computer science journals becoming mainstream. According to three bibliometric tools, the research topics primarily revolve around three areas: Vehicular ad hoc Networks (VANET), intelligent transportation systems (ITS), and network security. Machine learning and V2X communication are predicted to be essential research topics in the next stage. Research on traffic accidents still has potential as the number of ICVs increases.",
        "DOI": "10.3390/su151512003",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Quantitative Model Construction for Sustainable Security Patterns in Social–Ecological Links Using Remote Sensing and Machine Learning",
        "paper_author": "Liu L.",
        "publication": "Remote Sensing",
        "citied_by": "15",
        "cover_date": "2023-08-01",
        "Abstract": "With the global issues of extreme climate and urbanization, the ecological security patterns (ESPs) in the Qinling Mountains are facing prominent challenges. As a crucial ecological barrier in China, understanding the characteristics of ESPs in the Qinling Mountains is vital for achieving sustainable development. This study focuses on Yangxian and employs methods such as machine learning (ML), remote sensing (RS), geographic information systems (GISs), analytic hierarchy process and principal component analysis (AHP–PCA), and the minimum cumulative resistance (MCR) model to construct an ecological security network based on multi-factor ecological sensitivity (ES) and conduct quantitative spatial analysis. The results demonstrate that the AHP–PCA method based on ML overcomes the limitations of the single-weighting method. The ESPs of Yangxian were established, consisting of 21 main and secondary ecological sources with an area of 592.81 km2 (18.55%), 41 main and secondary ecological corridors with a length of 738.85 km, and 33 ecological nodes. A coupling relationship among three dimensions was observed: comprehensive ecological sensitivity, ESPs, and administrative districts (ADs). Huangjinxia Town (1.43 in C5) and Huayang Town (7.28 in C4) likely have significant areas of ecological vulnerability, while Machang Town and Maoping Town are important in the ESPs. ADs focus on protection and management. The second corridor indicated high-quality construction, necessitating the implementation of strict protection policies in the study area. The innovation lies in the utilization of quantitative analysis methods, such as ML and RS technologies, to construct an ecological spatial pattern planning model and propose a new perspective for the quantitative analysis of ecological space. This study provides a quantitative foundation for urban and rural ecological spatial planning in Yangxian and will help facilitate the sustainable development of ecological planning in the Qinling region.",
        "DOI": "10.3390/rs15153837",
        "affiliation_name": "Xinjiang Institute of Ecology and Geography Chinese Academy of Sciences",
        "affiliation_city": "Urumqi",
        "affiliation_country": "China"
    },
    {
        "paper_title": "HealthLock: Blockchain-Based Privacy Preservation Using Homomorphic Encryption in Internet of Things Healthcare Applications",
        "paper_author": "Ali A.",
        "publication": "Sensors",
        "citied_by": "49",
        "cover_date": "2023-08-01",
        "Abstract": "The swift advancement of the Internet of Things (IoT), coupled with the growing application of healthcare software in this area, has given rise to significant worries about the protection and confidentiality of critical health data. To address these challenges, blockchain technology has emerged as a promising solution, providing decentralized and immutable data storage and transparent transaction records. However, traditional blockchain systems still face limitations in terms of preserving data privacy. This paper proposes a novel approach to enhancing privacy preservation in IoT-based healthcare applications using homomorphic encryption techniques combined with blockchain technology. Homomorphic encryption facilitates the performance of calculations on encrypted data without requiring decryption, thus safeguarding the data’s privacy throughout the computational process. The encrypted data can be processed and analyzed by authorized parties without revealing the actual contents, thereby protecting patient privacy. Furthermore, our approach incorporates smart contracts within the blockchain network to enforce access control and to define data-sharing policies. These smart contracts provide fine-grained permission settings, which ensure that only authorized entities can access and utilize the encrypted data. These settings protect the data from being viewed by unauthorized parties. In addition, our system generates an audit record of all data transactions, which improves both accountability and transparency. We have provided a comparative evaluation with the standard models, taking into account factors such as communication expense, transaction volume, and security. The findings of our experiments suggest that our strategy protects the confidentiality of the data while at the same time enabling effective data processing and analysis. In conclusion, the combination of homomorphic encryption and blockchain technology presents a solution that is both resilient and protective of users’ privacy for healthcare applications integrated with IoT. This strategy offers a safe and open setting for the management and exchange of sensitive patient medical data, while simultaneously preserving the confidentiality of the patients involved.",
        "DOI": "10.3390/s23156762",
        "affiliation_name": "UNITAR International University",
        "affiliation_city": "Petaling Jaya",
        "affiliation_country": "Malaysia"
    },
    {
        "paper_title": "Machine learning in run-time control of multicore processor systems",
        "paper_author": "Maurer F.",
        "publication": "IT - Information Technology",
        "citied_by": "1",
        "cover_date": "2023-08-01",
        "Abstract": "Modern embedded and cyber-physical applications consist of critical and non-critical tasks co-located on multiprocessor systems on chip (MPSoCs). Co-location of tasks results in contention for shared resources, resulting in interference on interconnect, processing units, storage, etc. Hence, machine learning-based resource managers must operate even non-critical tasks within certain constraints to ensure proper execution of critical tasks. In this paper we demonstrate and evaluate countermeasures based on backup policies to enhance rule-based reinforcement learning to enforce constraints. Detailed experiments reveal the CPUs' performance degradation caused by different designs, as well as their effectiveness in preventing constraint violations. Further, we exploit the interpretability of our approach to further improve the resource manager's operation by adding designers' experience into the rule set.",
        "DOI": "10.1515/itit-2023-0056",
        "affiliation_name": "Department of Computer Science",
        "affiliation_city": "San Diego",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "A Normalized Global Economic Policy Uncertainty Index from Unsupervised Machine Learning",
        "paper_author": "Xu W.",
        "publication": "Mathematics",
        "citied_by": "1",
        "cover_date": "2023-08-01",
        "Abstract": "In this work, we integrate the conventional unsupervised machine learning algorithm—the Principal Component Analysis (PCA) with the Random Matrix Theory to propose a novel global economic policy uncertainty (GPEU) index that accommodates global economic policy fluctuations. An application of the Random Matrix Analysis illustrates the majority of the PCA components of EPU’s mirror random patterns that lack substantial economic information, while the only exception—the dominant component—is non-random and serves as a fitting candidate for the GEPU index. Compared to the prevalent GEPU index, which amalgamates each economy’s EPU weighted by its GDP value, the new index works equally well in identifying typical global events. Most notably, the new index eliminates the requirement of extra economic data, thereby avoiding potential endogeneity in empirical studies. To demonstrate this, we study the correlation between gold future volatility and GEPU using the GARCH-MIDAS model, and show that the newly proposed GEPU index outperforms the previous version. Additionally, we employ complex network methodologies to present a topological characterization of the GEPU indices. This research not only contributes to the advancement of unsupervised machine learning algorithms in the economic field but also proposes a robust and effective GEPU index that outperforms existing models.",
        "DOI": "10.3390/math11153268",
        "affiliation_name": "China Academy for Rural Development, Zhejiang University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Monthly precipitation prediction at regional scale using deep convolutional neural networks",
        "paper_author": "Ni L.",
        "publication": "Hydrological Processes",
        "citied_by": "5",
        "cover_date": "2023-08-01",
        "Abstract": "Variations in monthly precipitation are associated with climate extremes having significant socio-economic and eco-environmental impacts. Knowledge of monthly precipitation information is therefore valuable for policy making. Extensive research has been conducted on dynamic prediction using state-of-the-art coupled climate models. However, the skilful prediction of monthly precipitation with dynamical models remains a challenge. With the development of machine learning tools, statistical predictions show comparable performance with dynamic models, but they are limited to at-site monthly prediction, due to the lack of ability for processing spatially connected geophysical data. To improve monthly precipitation forecasting and provide regional forecasts, we propose a model termed UNet-RegPre based on convolutional neural network and U-net architecture. The model shows comparable prediction skills with recurrent state-of-the-art dynamic forecasts (CFSv2), and has the capability to capture spatiotemporal patterns of precipitation and reproduce the main process of representative droughts. The key precursors for rainfall development identified by UNet-RegPre shows that the constructed model can detect the climatic connection between rainfall in eastern China and summer monsoon, showing the potential to advance hydrometeorological understanding with a deep learning-based model. These results suggest that the proposed model can be a potential tool for precipitation prediction.",
        "DOI": "10.1002/hyp.14954",
        "affiliation_name": "Department of Biological and Agricultural Engineering",
        "affiliation_city": "College Station",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Challenges and solutions in clinical research during the COVID-19 pandemic: A narrative review",
        "paper_author": "Nomali M.",
        "publication": "Health Science Reports",
        "citied_by": "13",
        "cover_date": "2023-08-01",
        "Abstract": "Background and Aims: The COVID-19 pandemic has presented significant challenges to clinical research, necessitating the adoption of innovative and remote methods to conduct studies. This study aimed to investigate these challenges and propose solutions for conducting clinical research during the pandemic. Methods: A narrative review was conducted (approval ID: IR.AMS.REC.1401.029), utilizing keyword searches in PubMed and Web of Science (WOS) citation index expanded (SCI-EXPANDED) from January 2020 to January 2023. Keywords included COVID-19, clinical research, barriers, obstacles, facilitators and enablers. Results: Out of 2508 records retrieved, 43 studies were reviewed, providing valuable insights into the challenges and corresponding solutions for conducting clinical research during the COVID-19 pandemic. The identified challenges were categorized into four main groups: issues related to researchers or investigators, issues related to participants and ethical concerns, administrative issues, and issues related to research implementation. To address these challenges, multiple strategies were proposed, including remote monitoring through phone or video visits, online data collection and interviews to minimize in-person contact, development of virtual platforms for participant interaction and questionnaire completion, consideration of financial incentives, adherence to essential criteria such as inclusion and exclusion parameters, participant compensation, and risk assessment for vulnerable patients. Conclusion: The COVID-19 pandemic has significantly impacted clinical research, requiring the adaptation and enhancement of existing research structures. Although remote methods and electronic equipment have limitations, they hold promise as effective solutions during this challenging period.",
        "DOI": "10.1002/hsr2.1482",
        "affiliation_name": "School of Nursing Midwifery",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "Movement Health: The Current Status",
        "paper_author": "Shaw J.",
        "publication": "Current Sports Medicine Reports",
        "citied_by": "0",
        "cover_date": "2023-08-01",
        "Abstract": "NA",
        "DOI": "10.1249/JSR.0000000000001088",
        "affiliation_name": "Oregon Health &amp; Science University",
        "affiliation_city": "Portland",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Introducing AHA’s New President: Joseph C. Wu, MD, PhD, FAHA",
        "paper_author": "Kuehn B.M.",
        "publication": "Journal of the American Heart Association",
        "citied_by": "1",
        "cover_date": "2023-08-01",
        "Abstract": "NA",
        "DOI": "10.1161/JAHA.123.031618",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Exploring the application of machine learning to expert evaluation of research impact",
        "paper_author": "Williams K.",
        "publication": "PLoS ONE",
        "citied_by": "5",
        "cover_date": "2023-08-01",
        "Abstract": "The objective of this study is to investigate the application of machine learning techniques to the large-scale human expert evaluation of the impact of academic research. Using publicly available impact case study data from the UK's Research Excellence Framework (2014), we trained five machine learning models on a range of qualitative and quantitative features, including institution, discipline, narrative style (explicit and implicit), and bibliometric and policy indicators. Our work makes two key contributions. Based on the accuracy metric in predicting high- and low-scoring impact case studies, it shows that machine learning models are able to process information to make decisions that resemble those of expert evaluators. It also provides insights into the characteristics of impact case studies that would be favoured if a machine learning approach was applied for their automated assessment. The results of the experiments showed strong influence of institutional context, selected metrics of narrative style, as well as the uptake of research by policy and academic audiences. Overall, the study demonstrates promise for a shift from descriptive to predictive analysis, but suggests caution around the use of machine learning for the assessment of impact case studies.",
        "DOI": "10.1371/journal.pone.0288469",
        "affiliation_name": "School of Social and Political Sciences",
        "affiliation_city": "Melbourne",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Examining the Prevailing Negative Sentiments Surrounding Measles Vaccination: Unsupervised Deep Learning of Twitter Posts from 2017 to 2022",
        "paper_author": "Ng Q.X.",
        "publication": "Cyberpsychology, Behavior, and Social Networking",
        "citied_by": "3",
        "cover_date": "2023-08-01",
        "Abstract": "Despite the proven safety and clinical efficacy of the Measles vaccine, many countries are seeing new heights of vaccine hesitancy or refusal, and are experiencing a resurgence of measles infections as a consequence. With the use of novel machine learning tools, we investigated the prevailing negative sentiments related to Measles vaccination through an analysis of public Twitter posts over a 5-year period. We extracted original tweets using the search terms related to \"measles\"and \"vaccine,\"and posted in English from January 1, 2017, to December 15, 2022. Of these, 155,363 tweets were identified to be negative sentiment tweets from unique individuals, through the use of Bidirectional Encoder Representations from Transformers (BERT) Named Entity Recognition and SieBERT, a pretrained sentiment in English analysis model. This was followed by topic modeling and qualitative thematic analysis performed inductively by the study investigators. A total of 11 topics were generated after applying BERTopic. To facilitate a global discussion of results, the topics were grouped into four different themes through iterative thematic analysis. These include (a) the rejection of \"anti-vaxxers\"or antivaccine sentiments, (b) misbeliefs and misinformation regarding Measles vaccination, (c) negative transference due to COVID-19 related policies, and (d) public reactions to contemporary Measles outbreaks. Theme 1 highlights that the current public discourse may further alienate those who are vaccine hesitant because of the disparaging language often used, while Themes 2 and 3 highlight the typology of misperceptions and misinformation underlying the negative sentiments related to Measles vaccination and the psychological tendency of disconfirmation bias. Nonetheless, the analysis was based solely on Twitter and only tweets in English were included; hence, the findings may not necessarily generalize to non-Western communities. It is important to further understand the thinking and feeling of those who are vaccine hesitant to address the issues at hand.",
        "DOI": "10.1089/cyber.2023.0025",
        "affiliation_name": "MOH Holdings Pte Ltd",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore"
    },
    {
        "paper_title": "Spatiotemporal analysis of built environment restrained traffic carbon emissions and policy implications",
        "paper_author": "Wu J.",
        "publication": "Transportation Research Part D: Transport and Environment",
        "citied_by": "17",
        "cover_date": "2023-08-01",
        "Abstract": "Urban environmental policies need to be rectified considering the spatioemporal variations of traffic emissions. However, knowledge to support such a decision-making process is insufficient. This study analyzes the spatiotemporal distributions of traffic emissions in the built environment and their potential nonlinear associations. Considering the recent innovations in machine learning, a tree-boosting algorithm combined with Gaussian process and random effects models (GPBoost) is applied using the big GPS taxi data from Dalian, China. The nonlinear relationships between built environment variables and traffic carbon (CO2) emissions are interpreted using the SHapley Additive ExPlanation (SHAP). It is found that the proposed GPBoost model that considers spatial heterogeneity enhances the overall predictive power compared to traditional machine learning models. Most of the built environment variables have a nonlinear relationship with traffic carbon emissions and the threshold effects vary over time, indicating the necessity of dynamic urban management.",
        "DOI": "10.1016/j.trd.2023.103839",
        "affiliation_name": "Technische Universiteit Eindhoven",
        "affiliation_city": "Eindhoven",
        "affiliation_country": "Netherlands"
    },
    {
        "paper_title": "Optimal feature selection on Serial Cascaded deep learning for predictive maintenance system in automotive industry with fused optimization algorithm",
        "paper_author": "Chinta V.S.",
        "publication": "Advanced Engineering Informatics",
        "citied_by": "8",
        "cover_date": "2023-08-01",
        "Abstract": "Machines can make an appropriate operating and maintenance choice when defects are accurately and promptly predicted. Researchers choose data-driven predictive maintenance techniques for making the prediction more quickly and economically than alternative methods. To maintain secure and dependable manufacturing operations, the production equipment needs predictive maintenance. It is crucial to have an efficient predictive maintenance model for preventing unexpected shutdown by defects during production. The majority of related research focuses on early fault warnings but ignores how different issues differ in severity. Making the right maintenance policy is extremely important for a manufacturing organization because maintenance affects employee safety, economy, reliability, and availability. To solve these difficulties in the automotive sector, an efficient predictive maintenance strategy with deep structured architecture is offered to predict faults in the machines and increase the lifespan of the equipment. Firstly, the required data is collected from external sources for the predictive maintenance system. The garnered data is given to the pre-processing section and the transformation stage. Secondly, the transformed data is subjected to the autoencoder for selecting the optimal encoded vectors with the Henry Gas Solubility Search and Rescue Optimization (HGSSRO). The selected features are given to deep hybrid learning with Serial Cascaded Deep Learning (SCDL) to predict the occurrence of faults. Here, the autoencoder obtains the encoded vectors from the optimally selected features. Then, the optimal encoded vectors are subjected to the LSTM to acquire the features. The extracted features are given to DNN for the final prediction of failures. In this hybrid SCDL, the variables are optimized by the same HGSSRO to improve the prediction performance. The efficacy of the developed deep learning-aided predictive system is validated ed by comparing the conventional prediction models.",
        "DOI": "10.1016/j.aei.2023.102105",
        "affiliation_name": "Chaitanya Bharathi Institute of Technology",
        "affiliation_city": "Hyderabad",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Use of Generative Artificial Intelligence, Including Large Language Models Such as ChatGPT, in Scientific Publications: Policies of KJR and Prominent Authorities",
        "paper_author": "Park S.H.",
        "publication": "Korean Journal of Radiology",
        "citied_by": "22",
        "cover_date": "2023-08-01",
        "Abstract": "NA",
        "DOI": "10.3348/kjr.2023.0643",
        "affiliation_name": "Asan Medical Center",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "Developing and validating a natural language processing algorithm to extract preoperative cannabis use status documentation from unstructured narrative clinical notes",
        "paper_author": "Sajdeya R.",
        "publication": "Journal of the American Medical Informatics Association",
        "citied_by": "7",
        "cover_date": "2023-08-01",
        "Abstract": "Objective: This study aimed to develop a natural language processing algorithm (NLP) using machine learning (ML) techniques to identify and classify documentation of preoperative cannabis use status. Materials and Methods: We developed and applied a keyword search strategy to identify documentation of preoperative cannabis use status in clinical documentation within 60 days of surgery. We manually reviewed matching notes to classify each documentation into 8 different categories based on context, time, and certainty of cannabis use documentation. We applied 2 conventional ML and 3 deep learning models against manual annotation. We externally validated our model using the MIMIC-III dataset. Results: The tested classifiers achieved classification results close to human performance with up to 93% and 94% precision and 95% recall of preoperative cannabis use status documentation. External validation showed consistent results with up to 94% precision and recall. Discussion: Our NLP model successfully replicated human annotation of preoperative cannabis use documentation, providing a baseline framework for identifying and classifying documentation of cannabis use. We add to NLP methods applied in healthcare for clinical concept extraction and classification, mainly concerning social determinants of health and substance use. Our systematically developed lexicon provides a comprehensive knowledge-based resource covering a wide range of cannabis-related concepts for future NLP applications. Conclusion: We demonstrated that documentation of preoperative cannabis use status could be accurately identified using an NLP algorithm. This approach can be employed to identify comparison groups based on cannabis exposure for growing research efforts aiming to guide cannabis-related clinical practices and policies.",
        "DOI": "10.1093/jamia/ocad080",
        "affiliation_name": "University of Florida",
        "affiliation_city": "Gainesville",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Applying topic modelling and qualitative content analysis to identify and characterise ENDS product promotion and sales on Instagram",
        "paper_author": "Shah N.",
        "publication": "Tobacco Control",
        "citied_by": "12",
        "cover_date": "2023-08-01",
        "Abstract": "Background Increased public health and regulatory scrutiny concerning the youth vaping epidemic has led to greater attention to promotion and sales of vaping products on social media platforms. Objectives We used unsupervised machine learning to identify and characterise sale offers of electronic nicotine delivery systems (ENDS) and associated products on Instagram. We examined types of sellers, geographic ENDS location and use of age verification. Methods Our methodology was composed of three phases: data collection, topic modelling and content analysis. We used data mining approaches to query hashtags related to ENDS product use among young adults to collect Instagram posts. For topic modelling, we applied an unsupervised machine learning approach to thematically categorise and identify topic clusters associated with selling activity. Content analysis was then used to characterise offers for sale of ENDS products. Results From 70 725 posts, we identified 3331 engaged in sale of ENDS products. Posts originated from 20 different countries and were roughly split between individual (46.3%) and retail sellers (43.4%), with linked online sellers (8.8%) representing a smaller volume. ENDS products most frequently offered for sale were flavoured e-liquids (53.0%) and vaping devices (20.5%). Online sellers offering flavoured e-liquids were less likely to use age verification at point of purchase (29% vs 64%) compared with other products. Conclusions Instagram is a global venue for unregulated ENDS sales, including flavoured products, and access to websites lacking age verification. Such posts may violate Instagram’s policies and US federal and state law, necessitating more robust review and enforcement to prevent ENDS uptake and access.",
        "DOI": "10.1136/tobaccocontrol-2021-056937",
        "affiliation_name": "Department of Anesthesiology",
        "affiliation_city": "San Diego",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Self-play reinforcement learning guides protein engineering",
        "paper_author": "Wang Y.",
        "publication": "Nature Machine Intelligence",
        "citied_by": "27",
        "cover_date": "2023-08-01",
        "Abstract": "Designing protein sequences towards desired properties is a fundamental goal of protein engineering, with applications in drug discovery and enzymatic engineering. Machine learning-guided directed evolution has shown success in expediting the optimization cycle and reducing experimental burden. However, efficient sampling in the vast design space remains a challenge. To address this, we propose EvoPlay, a self-play reinforcement learning framework based on the single-player version of AlphaZero. In this work, we mutate a single-site residue as an action to optimize protein sequences, analogous to playing pieces on a chessboard. A policy-value neural network reciprocally interacts with look-ahead Monte Carlo tree search to guide the optimization agent with breadth and depth. We extensively evaluate EvoPlay on a suite of in silico directed evolution tasks over full-length sequences or combinatorial sites using functional surrogates. EvoPlay also supports AlphaFold2 as a structural surrogate to design peptide binders with high affinities, validated by binding assays. Moreover, we harness EvoPlay to prospectively engineer luciferase, resulting in the discovery of variants with 7.8-fold bioluminescence improvement beyond wild type. In sum, EvoPlay holds great promise for facilitating protein design to tackle unmet academic, industrial and clinical needs.",
        "DOI": "10.1038/s42256-023-00691-9",
        "affiliation_name": "Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Crude oil price shocks, volatility spillovers, and global systemic financial risk transmission mechanisms: Evidence from the stock and foreign exchange markets",
        "paper_author": "Chen J.",
        "publication": "Resources Policy",
        "citied_by": "14",
        "cover_date": "2023-08-01",
        "Abstract": "Crude oil, as one of the most important international bulk commodities, has both financial and geopolitical attributes. As such, its price fluctuations are bound to have profound impacts on the international financial markets. We decomposed crude oil price shocks into supply, demand and risk shocks using a structural vector autoregressive (SVAR) model. We then established a network of volatility spillovers and selected four typical time periods to examine the spillover effects between the three price shocks, the global stock market, and the foreign exchange market. Based on this data, we constructed the DCC-GARCH and asymmetric BEKK-GARCH models to study the dynamic interconnections between markets, risk spillover effects, and cross-impact relationships. Finally, we established an early warning model for the risk of oil price fluctuations using the machine learning method of the long short-term memory (LSTM).Based on the empirical research, this paper draws the following conclusions: (a) the risk spillovers of crude oil price shocks exhibit typical time-variant characteristics in different periods, with demand shocks having the strongest spillover effects, while the effects of supply shocks are the weakest; (b) in the vast majority of cases, crude oil-importing countries are the recipients of these risks; and (c) due to the close linkage between global stock markets and foreign exchange markets, the possibility of oil price shocks exacerbating the spread of global systemic financial risks is greatly enhanced. The findings provide policymakers and investors with a reference for the regulation of market operations, as well as risk prevention and avoidance.",
        "DOI": "10.1016/j.resourpol.2023.103875",
        "affiliation_name": "Nanjing Audit University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Quantifying the impacts of emissions and meteorology on the interannual variations of air pollutants in major Chinese cities from 2015 to 2021",
        "paper_author": "Dai Q.",
        "publication": "Science China Earth Sciences",
        "citied_by": "21",
        "cover_date": "2023-08-01",
        "Abstract": "Air pollutant concentration is a function of emission rates and meteorology. To accurately evaluate the effect of control measures, the meteorological covariate must be corrected from the observations. This study quantified the impacts of emission abatement and meteorological condition on the interannual variations of SO2, NO2, CO, O3, PM10 and PM2.5 concentrations in 31 major Chinese cities using an optimized machine learning-based meteorological normalization technique. Overall, the annual average concentrations of SO2, NO2, CO, PM10 and PM2.5 were reduced by 86%, 51%, 99%, 86% and 88% from 2015 to 2020, respectively, in the studied cities, attributable to their emission reductions. However, the concentration of O3 was found with no significant decrease with the reduction of precursors. Emission abatement notably improved air quality between 2015 and 2018. Such a decline in emissions tended to progressively slow down since 2018. Overall, the meteorological conditions in 2016–2017 and 2018–2019 were unfavorable for a better air quality, while it became favorable in 2020–2021. Specifically, emission abatement in 2021 further lowered the concentrations of SO2, NO2, CO, and PM2.5, while the emission of PM10 increased. And changes in precursors emissions worsened O3 air quality. To meet the demand of improving air quality, more aggressive abatement measures need to be formulated to synergistically reduce NOx, volatile organic compounds, and coarse particles.",
        "DOI": "10.1007/s11430-022-1128-1",
        "affiliation_name": "China Meteorological Administration",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Spatiotemporal analysis of fine particulate matter for India (1980–2021) from MERRA-2 using ensemble machine learning",
        "paper_author": "Kumar V.",
        "publication": "Atmospheric Pollution Research",
        "citied_by": "7",
        "cover_date": "2023-08-01",
        "Abstract": "Particle exposure affects more humans globally than any other air pollutant. However, due to expensive instruments and infrastructural deficiency, a high spatiotemporal network of monitoring stations is not possible, leading to data-scarce regions. Satellite and reanalysis datasets can be implemented to estimate particulate matter, but they do not provide surface concentration and needs to be reconstructed from the components. In this study, a machine learning (ML) framework is implemented to reconstruct PM2.5 from MERRA-2 data components, namely black carbon (BC), organic carbon (OC), dust (DUST), sea salt (SS), and sulfate (SO4) mass concentration. The ground-level data were collected from India's 335 continuous ambient air quality monitoring stations (CAAQMS) and respective MERRA-2 data for 2017–2021 at hourly resolution. Random forest (RF) performs better with train and test scores (R2) of 0.84 and 0.73, respectively, while the empirical equation provides an R2 of only 0.26 on test data. The estimated PM2.5 for Indian states from 1980 to 2021 indicates a significant increase in most cases. However, states in the Indo-Gangetic plain such as Delhi, Punjab, Haryana, and Uttar Pradesh, are the most polluted regions of India. The major shift in concentration is from 2000 onwards, which can be seen as a direct result of the economic liberalization policies implemented in 1991. The results provide evidence for the limitations of the broad application of the empirical equation and the feasibility of ML algorithms as a potential reconstruction technique for developing robust and accurate region-specific models from MERRA-2 data.",
        "DOI": "10.1016/j.apr.2023.101834",
        "affiliation_name": "Environmental Science and Engineering Department",
        "affiliation_city": "Mumbai",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Using machine learning to study the association of sociodemographic indicators, biomarkers, and oral condition in older adults in Colombia",
        "paper_author": "Botero J.E.",
        "publication": "Journal of the American Dental Association",
        "citied_by": "1",
        "cover_date": "2023-08-01",
        "Abstract": "Background: Chronic health conditions and socioeconomic problems that affect the well-being and life expectancy of older adults are common. The objective of this cross-sectional study was to analyze the association between sociodemographic variables, oral conditions, and general health and the biomarkers of older adults using machine learning (ML). Methods: A total of 15,068 surveys from the national study of Health, Well-Being and Aging (Salud, Bienestar y Envejecimiento) data set were used for this secondary analysis. Of these, 3,128 people provided blood samples for the analysis of blood biomarkers. Sociodemographic, oral health, and general health variables were analyzed using ML and logistic regression. Results: The results of clustering analysis showed that dyslipidemia was associated with poor oral condition, lower socioeconomic status, being female, and low education. The self-perception of oral health in older adults was not associated with the presence of teeth, blood biomarkers, or socioeconomic variables. However, the necessity of replacing a dental prosthesis was associated with the lowest self-perception of oral health. Edentulism was associated with being female, increased age, and smoking. Conclusions: Socioeconomic and educational disparities, sex, and smoking are important factors for tooth loss and suboptimal blood biomarkers in older adults. ML is a powerful tool for identifying potential variables that may aid in the prevention of systemic and oral diseases in older adults, which would improve geriatric dentistry. Practical Implications: These findings can help the academic community identify critical sociodemographic and clinical factors that influence the process of healthy aging and serve as a useful guide to enhance health care policies and geriatric oral health care services.",
        "DOI": "10.1016/j.adaj.2023.04.017",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Green and sustainable biomass supply chain for environmental, social and economic benefits",
        "paper_author": "Hiloidhari M.",
        "publication": "Biomass and Bioenergy",
        "citied_by": "49",
        "cover_date": "2023-08-01",
        "Abstract": "Bioenergy is a clean and renewable source of energy that can reduce global depency on fossil fuel, and it is a sustainable, economically viable, and socially acceptable. Bioenergy production aligns with several United Nations Sustainable Development Goals (SDGs) directly or indirectly. Bioenergy feedstocks are spatio-temporally distributed and, therefore, design of a green and sustainable Biomass Supply Chain (BSC) is pivotal for effective commercialization of bioenergy. The BSC starts with biomass harvest and includes collection, processing, storage, and transportation as intermediate processes and ends with biomass delivery at the conversion facilities. All these processes are spatially interlinked. Cost-effective bioenergy generation requires an effective and efficient BSC model. The absence of such model is the major cause of failure of bioenergy plants. With this backdrop, this paper reviewed literature related to BSC, and its elements. The elements are then linked with emissions, economy, and socio-cultural aspects to draw a wider picture of bioenergy for sustainable development. The challenges associated with bioenergy are elucidated with in-depth discussion. The analysis shows that green and sustainable BSC can be a major tool to achieve UN SDGs in many ways. On the contrary, present situation of BSC is challenging from multiple perspectives: environmental, socio-cultural, economic, policy, institutional as well as technological challenges. To achieve global deployment of bioenergy with net zero emissions target, use of advanced and emerging tools and techniques like artificial intelligence, machine learning, remote sensing & GIS, Life Cycle Assessment (LCA) and Bioenergy with Carbon Capture and Storage (BECCS) is recommended.",
        "DOI": "10.1016/j.biombioe.2023.106893",
        "affiliation_name": "NDSU College of Engineering",
        "affiliation_city": "Fargo",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Preoperative Prediction and Risk Factor Identification of Hospital Length of Stay for Total Joint Arthroplasty Patients Using Machine Learning",
        "paper_author": "Park J.",
        "publication": "Arthroplasty Today",
        "citied_by": "2",
        "cover_date": "2023-08-01",
        "Abstract": "Background: The aim of this study was to improve understanding of hospital length of stay (LOS) in patients undergoing total joint arthroplasty (TJA) in a high-efficiency, hospital-based pathway. Methods: We retrospectively reviewed 1401 consecutive primary and revision TJA patients across 67 patient and preoperative care characteristics from 2016 to 2019 from the institutional electronic health records. A machine learning approach, testing multiple models, was used to assess predictors of LOS. Results: The median LOS was 1 day; outpatients accounted for 16.5%, 1-day inpatient stays for 38.0%, 2-day stays for 26.4%, and 3-days or more for 19.1%. Patients characteristically fell into 1 of 3 broad categories that contained relatively similar characteristics: outpatient (0-day LOS), short stay (1- to 2-day LOS), and prolonged stay (3 days or greater). The random forest models suggested that a lower Risk Assessment and Prediction Tool score, unplanned admission or hospital transfer, and a medical history of cardiovascular disease were associated with an increased LOS. Documented narcotic use for surgery preparation prior to hospitalization and preoperative corticosteroid use were factors independently associated with a decreased LOS. Conclusions: After TJA, most patients have either an outpatient or short-stay hospital episode. Patients who stay 2 days do not differ substantially from patients who stay 1 day, while there is a distinct group that requires prolonged admission. Our machine learning models support a better understanding of the patient factors associated with different hospital LOS categories for TJA, demonstrating the potential for improved health policy decisions and risk stratification for centers caring for complex patients.",
        "DOI": "10.1016/j.artd.2023.101166",
        "affiliation_name": "Herbert Wertheim College of Engineering",
        "affiliation_city": "Gainesville",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "IGBT reliability analysis of photovoltaic inverter with reactive power output capability",
        "paper_author": "Zhang B.",
        "publication": "Microelectronics Reliability",
        "citied_by": "15",
        "cover_date": "2023-08-01",
        "Abstract": "When the PV power supply participates in reactive power regulation of distribution network, its output reactive power will affect the reliability of IGBT in the PV inverter. Aiming at this problem, this paper first qualitatively analyzed the influence of photovoltaic power supply participating in reactive power regulation of distribution network on the reliability of photovoltaic power supply. Then, a quantitative evaluation method of IGBT reliability based on data-driven was proposed. This method uses LightGBM machine learning model to replace the traditional thermoelectric coupling model, which effectively improves the calculation efficiency of IGBT junction temperature and reduces the dependence of IGBT reliability evaluation results on IGBT model parameters. Through this method, the reliability of core power electronic devices in photovoltaic inverters is quantitatively evaluated according to active power, reactive power, solar irradiance and ambient temperature. Finally, based on the IEEE 33 node distribution system, the reliability of IGBT in PV inverters participating in reactive power regulation of the distribution network was evaluated.",
        "DOI": "10.1016/j.microrel.2023.115073",
        "affiliation_name": "North China Electric Power University (Baoding)",
        "affiliation_city": "Baoding",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Identifying major climate extreme indices driver of stream flow discharge variability using machine learning and SHaply Additive Explanation",
        "paper_author": "Isa Z.",
        "publication": "Sustainable Water Resources Management",
        "citied_by": "1",
        "cover_date": "2023-08-01",
        "Abstract": "This study identifies major climate extreme indices as drivers of stream flow discharge variability using machine learning and the SHaply Additive Explanation. The homogenized and bias-corrected downscaled rainfall and temperatures were used to compute fifteen climate extreme indices using RClimdex. The data set was partitioned into 70% and 30% for Auto Machine Learning (AutoML) training and testing of the 38 machine learning models. The coefficient of determinant (R 2), mean square error (MSE), and root squared mean error (RSME) were used to evaluate the models. The variability and trend of the stream flow discharge was assessed using Mann Kendall and Sen’s slope. The findings revealed that there is high variability with an insignificant negative trend (Z: – 0.90, P > 0.05) in inter-annual discharge. In addition, among the 38 ML, the extra trees regression proved to perform better with a coefficient of determinant (R 2 > 0.9) and minimal MSE (less than 7 m3) and RMSE (less than 4 m3) for the training and testing. In addition, the result revealed that the maximum temperature (TMAX), total precipitation (PRCPTOT), and monthly maximum consecutive 5-day precipitation (RX5DAY) are the major climate indices that influence the stream flow discharge variability of the Kaduna River catchment area. From the findings, it was concluded that climate indices have a greater impact on water resources than average daily precipitation and temperature. As such, it is recommended that the combination of machine learning and SHaply Additive Explanation value makes it a great tool for exploring the impact of climate on water resources for policy making and sustainable water resource development and management.",
        "DOI": "10.1007/s40899-023-00897-0",
        "affiliation_name": "Kaduna State University",
        "affiliation_city": "Kaduna",
        "affiliation_country": "Nigeria"
    },
    {
        "paper_title": "A comprehensive review of water quality indices for lotic and lentic ecosystems",
        "paper_author": "Mogane L.K.",
        "publication": "Environmental Monitoring and Assessment",
        "citied_by": "18",
        "cover_date": "2023-08-01",
        "Abstract": "Freshwater resources play a pivotal role in sustaining life and meeting various domestic, agricultural, economic, and industrial demands. As such, there is a significant need to monitor the water quality of these resources. Water quality index (WQI) models have gradually gained popularity since their maiden introduction in the 1960s for evaluating and classifying the water quality of aquatic ecosystems. WQIs transform complex water quality data into a single dimensionless number to enable accessible communication of the water quality status of water resource ecosystems. To screen relevant articles, the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) method was employed to include or exclude articles. A total of 17 peer-reviewed articles were used in the final paper synthesis. Among the reviewed WQIs, only the Canadian Council for Ministers of the Environment (CCME) index, Irish water quality index (IEWQI) and Hahn index were used to assess both lotic and lentic ecosystems. Furthermore, the CCME index is the only exception from rigidity because it does not specify parameters to select. Except for the West-Java WQI and the IEWQI, none of the reviewed WQI performed sensitivity and uncertainty analysis to improve the acceptability and reliability of the WQI. It has been proven that all stages of WQI development have a level of uncertainty which can be determined using statistical and machine learning tools. Extreme gradient boosting (XGB) has been reported as an effective machine learning tool to deal with uncertainties during parameter selection, the establishment of parameter weights, and determining accurate classification schemes. Considering the IEWQI model architecture and its effectiveness in coastal and transitional waters, this review recommends that future research in lotic or lentic ecosystems focus on addressing the underlying uncertainty issues associated with the WQI model in addition to the use of machine learning techniques to improve the predictive accuracy and robustness and increase the domain of application.",
        "DOI": "10.1007/s10661-023-11512-2",
        "affiliation_name": "University of Pretoria",
        "affiliation_city": "Pretoria",
        "affiliation_country": "South Africa"
    },
    {
        "paper_title": "Distributional generative adversarial imitation learning with reproducing kernel generalization",
        "paper_author": "Zhou Y.",
        "publication": "Neural Networks",
        "citied_by": "2",
        "cover_date": "2023-08-01",
        "Abstract": "Generative adversarial imitation learning (GAIL) regards imitation learning (IL) as a distribution matching problem between the state–action distributions of the expert policy and the learned policy. In this paper, we focus on the generalization and computational properties of policy classes. We prove that the generalization can be guaranteed in GAIL when the class of policies is well controlled. With the capability of policy generalization, we introduce distributional reinforcement learning (RL) into GAIL and propose the greedy distributional soft gradient (GDSG) algorithm to solve GAIL. The main advantages of GDSG can be summarized as: (1) Q-value overestimation, a crucial factor leading to the instability of GAIL with off-policy training, can be alleviated by distributional RL. (2) By considering the maximum entropy objective, the policy can be improved in terms of performance and sample efficiency through sufficient exploration. Moreover, GDSG attains a sublinear convergence rate to a stationary solution. Comprehensive experimental verification in MuJoCo environments shows that GDSG can mimic expert demonstrations better than previous GAIL variants.",
        "DOI": "10.1016/j.neunet.2023.05.027",
        "affiliation_name": "Shanghai University College of Sciences",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A partially observable multi-ship collision avoidance decision-making model based on deep reinforcement learning",
        "paper_author": "Zheng K.",
        "publication": "Ocean and Coastal Management",
        "citied_by": "22",
        "cover_date": "2023-08-01",
        "Abstract": "Unmanned ships have drawn widespread attention for their potential to enhance navigational safety, minimize human errors, and improve shipping efficiency. Nevertheless, the complexity and uncertainty of mixed obstacle environments present significant challenges to developing unmanned ships, particularly in collision avoidance decision-making. This paper proposes a new model using the Partially Observable Markov Decision Process (POMDP) to construct a collision avoidance decision-making model in mixed obstacle environments for autonomous ships, which can address the environment's complexity and uncertainty and improve decision accuracy. An image-state observation method is proposed as images can provide more accurate, rich, and reliable information. A dense reward function is designed to address the issue of sparse rewards in fitting the algorithm. The Proximal Policy Optimization (PPO) algorithm is utilized for model training. Based on this, a route guidance method called the PPO for POMDP with guidelines under dense reward (G-IPOMDP-PPO) is proposed, which can improve training efficiency. Simulations are conducted in various mixed obstacle environments and compared with conventional algorithms. The results show that the proposed model can safely and efficiently make collision avoidance decisions in complex and uncertain environments. This research provides a new solution and theoretical foundation for developing autonomous ships and can be extended to achieving dynamic interactive collision avoidance in mixed obstacle environments.",
        "DOI": "10.1016/j.ocecoaman.2023.106689",
        "affiliation_name": "Aalto University",
        "affiliation_city": "Espoo",
        "affiliation_country": "Finland"
    },
    {
        "paper_title": "Learning Robust and Agile Legged Locomotion Using Adversarial Motion Priors",
        "paper_author": "Wu J.",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "28",
        "cover_date": "2023-08-01",
        "Abstract": "Developing both robust and agile locomotion skills for legged robots is non-trivial. In this work, we present the first blind locomotion system capable of traversing challenging terrains robustly while moving rapidly over natural terrains. Our approach incorporates the Adversarial Motion Priors (AMP) in locomotion policy training and demonstrates zero-shot generalization from the motion dataset on flat terrains to challenging terrains in the real world. We show this result on a quadruped robot Go1 using only proprioceptive sensors consisting of the IMU and joint encoders. Experiments on the Go1 demonstrate the robust and natural motion generated by the proposed method for traversing challenging terrains while moving rapidly over natural terrains.",
        "DOI": "10.1109/LRA.2023.3290509",
        "affiliation_name": "Shanghai Jiao Tong University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Deep reinforcement learning for optimal planning of assembly line maintenance",
        "paper_author": "Geurtsen M.",
        "publication": "Journal of Manufacturing Systems",
        "citied_by": "9",
        "cover_date": "2023-08-01",
        "Abstract": "Discovering the optimal maintenance planning strategy can have a substantial impact on production efficiency, yet this aspect is often overlooked in favor of production planning. This is a missed opportunity as maintenance and production activities are deeply intertwined. Our study sheds light on the significance of maintenance planning, particularly in the dynamic setting of an assembly line. By maximizing the average production rate and incorporating flexible planning windows, buffer content, and machine production states, a unique problem is addressed in which a policy for planning maintenance on the final machine of a serial assembly line is developed. To achieve this, novel average-reward deep reinforcement learning techniques are employed and pitted against generic dispatching methods. Using a digital twin with real-world data, experiments demonstrate the immense potential of this new deep reinforcement learning technique, producing policies that outperform generic dispatching strategies and practitioner policies.",
        "DOI": "10.1016/j.jmsy.2023.05.011",
        "affiliation_name": "Nexperia B.V.",
        "affiliation_city": "Nijmegen",
        "affiliation_country": "Netherlands"
    },
    {
        "paper_title": "Predictive hierarchical reinforcement learning for path-efficient mapless navigation with moving target",
        "paper_author": "Li H.",
        "publication": "Neural Networks",
        "citied_by": "7",
        "cover_date": "2023-08-01",
        "Abstract": "Deep reinforcement learning (DRL) has been proven as a powerful approach for robot navigation over the past few years. DRL-based navigation does not require the pre-construction of a map, instead, high-performance navigation skills can be learned from trial-and-error experiences. However, recent DRL-based approaches mostly focus on a fixed navigation target. It is noted that when navigating to a moving target without maps, the performance of the standard RL structure drops dramatically on both the success rate and path efficiency. To address the mapless navigation problem with moving target, the predictive hierarchical DRL (pH-DRL) framework is proposed by integrating the long-term trajectory prediction to provide a cost-effective solution. In the proposed framework, the lower-level policy of the RL agent learns robot control actions to a specified goal, and the higher-level policy learns to make long-range planning of shorter navigation routes by sufficiently exploiting the predicted trajectories. By means of making decisions over two level of policies, the pH-DRL framework is robust to the unavoidable errors in long-term predictions. With the application of deep deterministic policy gradient (DDPG) for policy optimization, the pH-DDPG algorithm is developed based on the pH-DRL structure. Finally, through comparative experiments on the Gazebo simulator with several variants of the DDPG algorithm, the results demonstrate that the pH-DDPG outperforms other algorithms and achieves a high success rate and efficiency even though the target moves fast and randomly.",
        "DOI": "10.1016/j.neunet.2023.06.007",
        "affiliation_name": "Zhejiang Lab",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Global estimates of gap-free and fine-scale CO<inf>2</inf> concentrations during 2014–2020 from satellite and reanalysis data",
        "paper_author": "Zhang L.",
        "publication": "Environment International",
        "citied_by": "8",
        "cover_date": "2023-08-01",
        "Abstract": "Carbon dioxide (CO2) is a crucial greenhouse gas with substantial effects on climate change. Satellite-based remote sensing is a commonly used approach to detect CO2 with high precision but often suffers from extensive spatial gaps. Thus, the limited availability of data makes global carbon stocktaking challenging. In this paper, a global gap-free column-averaged dry-air mole fraction of CO2 (XCO2) dataset with a high spatial resolution of 0.1° from 2014 to 2020 is generated by the deep learning-based multisource data fusion, including satellite and reanalyzed XCO2 products, satellite vegetation index data, and meteorological data. Results indicate a high accuracy for 10-fold cross-validation (R2 = 0.959 and RMSE = 1.068 ppm) and ground-based validation (R2 = 0.964 and RMSE = 1.010 ppm). Our dataset has the advantages of high accuracy and fine spatial resolution compared with the XCO2 reanalysis data as well as that generated from other studies. Based on the dataset, our analysis reveals interesting findings regarding the spatiotemporal pattern of CO2 over the globe and the national-level growth rates of CO2. This gap-free and fine-scale dataset has the potential to provide support for understanding the global carbon cycle and making carbon reduction policy, and it can be freely accessed at https://doi.org/10.5281/zenodo.7721945.",
        "DOI": "10.1016/j.envint.2023.108057",
        "affiliation_name": "Ministry of Natural Resources of the People's Republic of China",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Zero-shot learning has the potential to revolutionise research on exposure to alcohol and other drugs in digital media",
        "paper_author": "Kuntsche E.",
        "publication": "International Journal of Drug Policy",
        "citied_by": "3",
        "cover_date": "2023-08-01",
        "Abstract": "NA",
        "DOI": "10.1016/j.drugpo.2023.104098",
        "affiliation_name": "La Trobe University",
        "affiliation_city": "Melbourne",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Spatial–temporal recurrent reinforcement learning for autonomous ships",
        "paper_author": "Waltz M.",
        "publication": "Neural Networks",
        "citied_by": "4",
        "cover_date": "2023-08-01",
        "Abstract": "This paper proposes a spatial–temporal recurrent neural network architecture for deep Q-networks that can be used to steer an autonomous ship. The network design makes it possible to handle an arbitrary number of surrounding target ships while offering robustness to partial observability. Furthermore, a state-of-the-art collision risk metric is proposed to enable an easier assessment of different situations by the agent. The COLREG rules of maritime traffic are explicitly considered in the design of the reward function. The final policy is validated on a custom set of newly created single-ship encounters called ‘Around the Clock’ problems and the commonly used Imazu (1987) problems, which include 18 multi-ship scenarios. Performance comparisons with artificial potential field and velocity obstacle methods demonstrate the potential of the proposed approach for maritime path planning. Furthermore, the new architecture exhibits robustness when it is deployed in multi-agent scenarios and it is compatible with other deep reinforcement learning algorithms, including actor-critic frameworks.",
        "DOI": "10.1016/j.neunet.2023.06.015",
        "affiliation_name": "Technische Universität Dresden",
        "affiliation_city": "Dresden",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Congestion in cities: Can road capacity expansions provide a solution?",
        "paper_author": "Anupriya ",
        "publication": "Transportation Research Part A: Policy and Practice",
        "citied_by": "12",
        "cover_date": "2023-08-01",
        "Abstract": "Road network congestion; a traffic state characterised by slower speeds, longer trip times, and increased vehicular queuing; is a major issue in most urban areas around the globe. Building more roads is a commonly employed policy intervention to reduce congestion. This strategy, however, is controversial because under certain conditions road capacity expansions may induce growth in traffic volumes. A crucial precursor to understanding whether road capacity expansions provide a solution to congestion is to quantify the technology driving congestion in urban road networks. This congestion technology describes the variation in performance of the network, often represented by traffic flow through the road network, over its intensity of use given by the number of vehicles in the network. However, obtaining empirical estimates of congestion technology from data on traffic variables is challenging due to statistical biases that emerge via the complex interactions between traffic flow, traffic controls, and capacity. To adjust for such biases, this paper presents an approach based on causal statistical modelling to quantify the nature and form of congestion technology in road networks in twenty-four cities worldwide. Our results suggest that increasing network capacity is in general not an efficient solution to manage congestion, in the sense that the average travel speed in the network does not increase substantially with an increase in capacity. This result and our congestion technology estimates have important implications for optimal urban transportation strategies.",
        "DOI": "10.1016/j.tra.2023.103726",
        "affiliation_name": "National University of Singapore",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore"
    },
    {
        "paper_title": "Machine learning application to assess deforestation and wildfire levels in protected areas with tourism management",
        "paper_author": "Silva F.R.d.",
        "publication": "Journal for Nature Conservation",
        "citied_by": "5",
        "cover_date": "2023-08-01",
        "Abstract": "This study aims to identify the influence of management plans, management boards and tourism management on the relationship between performance indicators in the management of protected areas and degradation processes. To understand these relationships, 283 protected areas (PAs) in Brazil were analyzed. The first stage of the research used classification models based on machine learning algorithms, which revealed that predictive variables were a promising way to assess PAs vulnerability to deforestation and wildfire, giving decision-makers an 87.5% and 72.8% chance, respectively, of correctly identifying the PAs more susceptible to these threats. The predictive variables more relevant to deforestation were biome, area, and tourism management, while for wildfire, governance and PA type were the most relevant. Predictive variables were also a promising way to assess PAs management, giving decision-makers a 79.7% and 78.1% chance, respectively, to correctly identify the PAs with higher levels of effectiveness and governance. In addition, in the second stage, to empirically reinforce the models, multivariate analyses were performed, through which it was possible to confirm that deforestation levels are significantly higher in areas of sustainable use than in fully protected areas and determine how the positive interaction with tourism management contributes to the reduction in deforestation records and improves effectiveness. Therefore, it is understood that tourism management can strongly influence the sustainability of natural resources, and it is of utmost importance to generate tourism management policies with potential value generation for natural spaces.",
        "DOI": "10.1016/j.jnc.2023.126435",
        "affiliation_name": "Leibniz Center for Tropical Marine Research",
        "affiliation_city": "Bremen",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Green finance and the socio-politico-economic factors’ impact on the future oil prices: Evidence from machine learning",
        "paper_author": "Mohsin M.",
        "publication": "Resources Policy",
        "citied_by": "22",
        "cover_date": "2023-08-01",
        "Abstract": "This paper suggests an innovative method of estimating crude oil prices based on multiple socio-politico-economic factors in context of green finance using the Least Absolute Shrinkage and Selection Operator (LASSO) model. This work also examines the relevance of six factors (commodities market factors, geopolitical factors, supply, demand, and financial market factors), in addition to green finace, in evaluating several forecasting models and identifying statistically essential factors for predicting future oil prices. We implement state of the art LASSO model on a data set of the abovementioned factors (26 variables). The proposed model is evaluated against four bench mark models (traditional statistical models (OLS, GARCH), EIA and artificial neural networks) at different time steps (1 step, 3 steps, 6 steps, and 9 steps). Statistical analysis of outcomes shows that the LASSO technique produces better predictions than other benchmark models. The results also deliver detailed insights into the temporal association between numerous socio-politico-economic factors, green finance and crude oil. Our findings indicate that the global output of steel, the Kilian index, the Institute for Supply Management index, green finance index, the value of the dollar, and the frequency of terrorist strikes in Central East and Northern Africa are important demand drivers. These elements, taken as a whole, are more significant than supply and speculation. We also find that no variable from the supply factor is essential in determining the future oil value. Our results are significant for government and policy makers to gain insight into future oil prices in the context of various social, economic, and political factors.",
        "DOI": "10.1016/j.resourpol.2023.103780",
        "affiliation_name": "Taif University",
        "affiliation_city": "Taif",
        "affiliation_country": "Saudi Arabia"
    },
    {
        "paper_title": "Modeling limit order trading with a continuous action policy for deep reinforcement learning",
        "paper_author": "Tsantekidis A.",
        "publication": "Neural Networks",
        "citied_by": "4",
        "cover_date": "2023-08-01",
        "Abstract": "Limit Orders allow buyers and sellers to set a “limit price” they are willing to accept in a trade. On the other hand, market orders allow for immediate execution at any price. Thus, market orders are susceptible to slippage, which is the additional cost incurred due to the unfavorable execution of a trade order. As a result, limit orders are often preferred, since they protect traders from excessive slippage costs due to larger than expected price fluctuations. Despite the price guarantees of limit orders, they are more complex compared to market orders. Orders with overly optimistic limit prices might never be executed, which increases the risk of employing limit orders in Machine Learning (ML)-based trading systems. Indeed, the current ML literature for trading almost exclusively relies on market orders. To overcome this limitation, a Deep Reinforcement Learning (DRL) approach is proposed to model trading agents that use limit orders. The proposed method (a) uses a framework that employs a continuous probability distribution to model limit prices, while (b) provides the ability to place market orders when the risk of no execution is more significant than the cost of slippage. Extensive experiments are conducted with multiple currency pairs, using hourly price intervals, validating the effectiveness of the proposed method and paving the way for introducing limit order modeling in DRL-based trading.",
        "DOI": "10.1016/j.neunet.2023.05.051",
        "affiliation_name": "Aristotle University of Thessaloniki",
        "affiliation_city": "Thessaloniki",
        "affiliation_country": "Greece"
    },
    {
        "paper_title": "A machine learning approach to design a DPSIR model: A real case implementation of evidence-based policy creation using AI",
        "paper_author": "Penate-Sanchez A.",
        "publication": "Advanced Engineering Informatics",
        "citied_by": "1",
        "cover_date": "2023-08-01",
        "Abstract": "In this paper a method to learn a similarity metric from expert assessments via questionnaires is presented. The approach employed provides a solution to the modelling of a DPSIR sustainability approach where budgetary resources are limited and thus there is a need to select the most informative variables from the identified possibilities. This paper also shows the proposed approach already implemented by the local council of Las Palmas of Gran Canaria as part of the work to create a sustainability system to better control the impact of human pressure in the local region. The metric is learned using a weakly supervised approach and the expert assessments are modelled through variable triplets. The employment of machine learning approaches in the creation of sustainability models is fairly recent and rare but presents a great opportunity to contribute to one of the main challenges that human societies have to face nowadays.",
        "DOI": "10.1016/j.aei.2023.102042",
        "affiliation_name": "Universidad de Las Palmas de Gran Canaria",
        "affiliation_city": "Las Palmas de Gran Canaria",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "Forecasting and what-if analysis of new positive COVID-19 cases during the first three waves in Italy",
        "paper_author": "De Ruvo S.",
        "publication": "Medical and Biological Engineering and Computing",
        "citied_by": "2",
        "cover_date": "2023-08-01",
        "Abstract": "Abstract: The joint exploitation of data related to epidemiological, mobility, and restriction aspects of COVID-19 with machine learning algorithms can support the development of predictive models that can be used to forecast new positive cases and study the impact of more or less severe restrictions. In this work, we integrate heterogeneous data from several sources and solve a multivariate time series forecasting task, specifically targeting the Italian case at both national and regional levels, during the first three waves of the pandemic. The goal is to build a robust predictive model to predict the number of new cases over a given time horizon so that any restrictive actions can be better planned. In addition, we perform a what-if analysis based on the best-identified predictive models to evaluate the impact of specific restrictions on the trend of positive cases. Our focus on the first three waves is motivated by the fact that it represents a typical emergency scenario (when no stable cure or vaccine is available) that may occur when a new pandemic spreads. Our experimental results prove that exploiting the considered heterogeneous data leads to accurate predictive models, reaching a WAPE of 5.75% at the national level. Furthermore, in the subsequent what-if analysis, we observed that strong all-in-one initiatives, such as total lockdowns, may not be adequate, while more specific and targeted solutions should be adopted. The developed models can help policy and decision-makers better plan intervention strategies and retrospectively analyze the effects of the decisions made at different scales. Graphical abstract: Joint exploitation of data on epidemiological, mobility, and restriction aspects of COVID-19 with machine learning algorithms to learn predictive models to forecast new positive cases. [Figure not available: see fulltext.]",
        "DOI": "10.1007/s11517-023-02831-0",
        "affiliation_name": "Università degli studi di Bari Aldo Moro",
        "affiliation_city": "Bari",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Giant pandas are losing their edge: Population trend and distribution dynamic drivers of the giant panda",
        "paper_author": "Li Y.",
        "publication": "Global Change Biology",
        "citied_by": "12",
        "cover_date": "2023-08-01",
        "Abstract": "Comprehending the population trend and understanding the distribution range dynamics of species are necessary for global species protection. Recognizing what causes dynamic distribution change is crucial for identifying species' environmental preferences and formulating protection policies. Here, we studied the rear-edge population of the flagship species, giant pandas (Ailuropoda melanoleuca), to (1) assess their population trend using their distribution patterns, (2) evaluate their distribution dynamics change from the second (1988) to the third (2001) survey (2–3 Interval) and third to the fourth (2013) survey (3–4 Interval) using a machine learning algorithm (eXtremely Gradient Boosting), and (3) decode model results to identify driver factors in the first known use of SHapley Additive exPlanations. Our results showed that the population trends in Liangshan Mountains were worst in the second survey (k = 1.050), improved by the third survey (k = 0.97), but deteriorated by the fourth survey (k = 0.996), which indicates a worrying population future. We found that precipitation had the most significant influence on distribution dynamics among several potential environmental factors, showing a negative correlation between precipitation and giant panda expansion. We recommend that further research is needed to understand the microenvironment and animal distribution dynamics. We provide a fresh perspective on the dynamics of giant panda distribution, highlighting novel focal points for ecological research on this species. Our study offers theoretical underpinnings that could inform the formulation of more effective conservation policies. Also, we emphasize the uniqueness and importance of the Liangshan Mountains giant pandas as the rear-edge population, which is at a high risk of population extinction.",
        "DOI": "10.1111/gcb.16805",
        "affiliation_name": "Sichuan University",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Thermochemical upcycling of food waste into engineered biochar for energy and environmental applications: A critical review",
        "paper_author": "Yuan X.",
        "publication": "Chemical Engineering Journal",
        "citied_by": "26",
        "cover_date": "2023-08-01",
        "Abstract": "Environmental issues caused by food waste are important concerns for human well-being and ecosystem health. Valorization of food waste into energy and carbon materials has been extensively investigated. Here, we reviewed the most recent advancements in the thermochemical conversion of food waste into engineered biochar. Synthesis routes and practical applications of the food waste-derived biochar was succinctly reviewed. Engineered biochar is a promising alternative for mitigating environmental pollution and alleviating energy crisis. The underlying relationships between engineered biochar properties and specific applications are still unclear, therefore, machine learning-aided engineered biochar design and process optimization was proposed. Moreover, before any industrial scale implementation, detailed assessments of the environmental benefits and economic feasibility must be conducted. In the context of carbon neutrality, thermochemical upcycling of food waste into engineered biochar for energy and environmental applications can significantly contribute to attaining sustainable food waste management, mitigating environmental pollution, and addressing the energy shortage crisis, and thus will eventually facilitate the fulfillment of United Nations Sustainable Development Goals (SDGs). Furthermore, the existing challenges in the practical valorization of food waste into engineered biochar are comprehensively discussed, and outlooks are proposed.",
        "DOI": "10.1016/j.cej.2023.143783",
        "affiliation_name": "Key Lab of Energy Thermal Conversion and Control, Ministry of Education",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Reinforcement learning policy recommendation for interbank network stability",
        "paper_author": "Brini A.",
        "publication": "Journal of Financial Stability",
        "citied_by": "2",
        "cover_date": "2023-08-01",
        "Abstract": "In this paper, we analyze the effect of a policy recommendation on the performance of an artificial interbank market. Financial institutions stipulate lending agreements following a public recommendation and their individual information. The former is modeled by a reinforcement learning optimal policy that maximizes the system's fitness and gathers information on the economic environment. The policy recommendation directs economic actors to create credit relationships through the optimal choice between a low interest rate or a high liquidity supply. The latter, based on the agents’ balance sheet, allows determining the liquidity supply and interest rate that the banks optimally offer their clients within the market. Thanks to the combination between the public and the private signal, financial institutions create or cut their credit connections over time via a preferential attachment evolving procedure able to generate a dynamic network. Our results show that the emergence of a core–periphery interbank network, combined with a certain level of homogeneity in the size of lenders and borrowers, is essential to ensure the system's resilience. Moreover, the optimal policy recommendation obtained through reinforcement learning is crucial in mitigating systemic risk.",
        "DOI": "10.1016/j.jfs.2023.101139",
        "affiliation_name": "Alma Mater Studiorum Università di Bologna",
        "affiliation_city": "Bologna",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Dynamic trajectory planning for ships in dense environment using collision grid with deep reinforcement learning",
        "paper_author": "Teitgen R.",
        "publication": "Ocean Engineering",
        "citied_by": "11",
        "cover_date": "2023-08-01",
        "Abstract": "In areas with high maritime traffic, ship safety is of utmost importance when validating autonomous navigation models. While exact methods exist for specific situations, they are inadequate in a global context. This study employs an approximate deep reinforcement learning method to solve the navigation problem in a dense environment with numerous static and moving obstacles. Our model prioritizes ship integrity by enabling the agent to dynamically adapt its kinematics to its surroundings to reach a designated goal without colliding with obstacles. To achieve this, we incorporate collision grids in the form of danger zones as input to our model and train it using the proximal policy optimization algorithm. Additionally, we propose implementing the International Regulations for Preventing Collisions at Sea (COLREGs) in the collision grid as these navigation rules are necessary to obtain realistic behavior of an autonomous agent. The agent's performance is evaluated on a set of randomly generated scenarios operating in an environment complexity similar to the one used during training. These tests demonstrate that this type of data structure allows a trained agent to navigate in a dense environment while adhering to the COLREGs with a success rate of 94.69%.",
        "DOI": "10.1016/j.oceaneng.2023.114807",
        "affiliation_name": "École Nationale Supérieure de Techniques Avancées",
        "affiliation_city": "Palaiseau",
        "affiliation_country": "France"
    },
    {
        "paper_title": "Investigating the risk factors of motorcycle crash injury severity in Pakistan: Insights and policy recommendations",
        "paper_author": "Mansoor U.",
        "publication": "Transport Policy",
        "citied_by": "13",
        "cover_date": "2023-08-01",
        "Abstract": "A large number of fatalities and severe injuries are caused by motorcycle crashes worldwide, particularly in developing countries. More than 50% of crashes in Pakistan involve motorcycles. To analyze motorcycle crash severity, various models, including both statistical and machine learning methods, have been applied. Researchers have widely acknowledged that machine learning methods provide superior prediction performance but have weaker interpretability power. However, no study has investigated the consistency of risk factors identified by the two streams of models. The consistency of the findings between these two kinds of methods is vital to improve the interpretability power of machine learning methods for policymaking in an era with more and more applications in the area of traffic safety. This study aims to narrow this research gap by comparing the consistency of crash severity risk factors identified by statistical models and machine learning methods. The study analyzes motorcycle crashes in Rawalpindi city of Pakistan. Multinomial logit model (MNL) and three machine learning models, i.e., the random forest (RF), naive Bayes, and gradient-boosted trees methods, are used to analyze the prediction performance and identify risk factors. The results show that the RF model, with an overall accuracy of 86.7%, outperformed other models. The SHapley Additive exPlanations (SHAP) method was adopted to explore the interpretability of machine learning methods. It was found that the contributing factors to crash injury severity identified by the RF method, such as distracted driving, collisions involving pedestrians, collisions involving a truck, and female riders, are consistent with those determined by the MNL model. These results have clear implications for developing cost-effective safety countermeasures to improve motorcycle safety in Pakistan.",
        "DOI": "10.1016/j.tranpol.2023.05.013",
        "affiliation_name": "Imam Abdulrahman Bin Faisal University",
        "affiliation_city": "Dammam",
        "affiliation_country": "Saudi Arabia"
    },
    {
        "paper_title": "Discovering latent topics and trends in autonomous vehicle-related research: A structural topic modelling approach",
        "paper_author": "Tamakloe R.",
        "publication": "Transport Policy",
        "citied_by": "13",
        "cover_date": "2023-08-01",
        "Abstract": "Autonomous Vehicle (AV) technology is a disruptive transportation technology that promises to revolutionize how people travel. Due to their potential mobility benefits and associated impacts on public health and the environment, they have received massive attention from transportation experts. To date, there have been numerous academic research conducted regarding AVs, and many attempts have been made to summarize these research documents by manually reviewing them. Nevertheless, due to the vast nature of the existing literature, it is challenging to synthesize the themes in AV-related research and to explore their trends to inform future research direction. This study aims to utilize an advanced natural language processing technique to provide a comprehensive overview of the recent developments in AV-related research. The abstracts of 3292 articles published in transportation-based journals from January 2016 to October 2022 were collected and used for this study. A Structural Topic Model (STM) was employed to explore the dominant research themes hidden in the extant literature on AV research, examine their evolution over time, and determine the topics highly associated with developing and developed economies. Overall, the study highlighted that the least common themes in the literature are in the areas of the development of complex vehicle designs, safety, environmental benefits, and the impact of AVs on transportation infrastructure. Nevertheless, themes concerning AV safety and the environment are emerging as hot topics since late 2021. Topics related to investigating user perceptions, policies/approaches to increase user demand, and AV control/stability are highly studied. Besides, they predominantly originated from developed countries. The research themes, their associated trends, and the key policy suggestions drawn from the study are potentially useful for making well-informed research funding decisions, setting research priorities for countries, and guiding the future research endeavours of researchers.",
        "DOI": "10.1016/j.tranpol.2023.06.001",
        "affiliation_name": "University of Seoul",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "Boosted Regression Trees for Small-Area Population Forecasting",
        "paper_author": "Baker J.",
        "publication": "Population Research and Policy Review",
        "citied_by": "2",
        "cover_date": "2023-08-01",
        "Abstract": "Small-area population forecasting, such as the forecasting of age/gender groupings at the level of US Census Tracts, is challenged by thorny issues including (1) small population sizes, (2) frequent and sometimes directionally opposing shifts in population dynamics between censuses, (3) data availability, and (4) the ongoing evolution of the US census geographies. It is, therefore, not surprising that evaluation studies suggest wide-ranging forecast errors. Estimates vary between lows between 10% and 20% and highs sometimes exceeding 100% within any given age/gender interval. Despite its successes, only recently have population forecasters begun to explore the possibilities presented by machine learning. Using 1990 and 2000 census data, we develop 10-year age/gender-structured 2010 population forecasts for 50,965 census tracts in the U.S. using a well-known machine learning technique: boosted regression trees. Using standard ex post facto measures of forecast error (MAPE, MALPE, and MAPE-R), we demonstrate that forecasts based on “out-of-the-box” boosted regression trees have greater accuracy and produce fewer and less extreme outliers than comparison forecasts produced by the Hamilton-Perry method (reported in Baker et al. in Population Res Policy Rev 40:1341–1354, 2021. https://doi.org/10.1007/s11113-020-09601-y).",
        "DOI": "10.1007/s11113-023-09795-x",
        "affiliation_name": "University of California, San Diego",
        "affiliation_city": "La Jolla",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Multimodal biometric authentication method by federated learning",
        "paper_author": "Coelho K.K.",
        "publication": "Biomedical Signal Processing and Control",
        "citied_by": "18",
        "cover_date": "2023-08-01",
        "Abstract": "The Internet of Health Things requires rigid security policies to control access to sensitive data. However, nowadays, classic methods for user authentication may not meet the requirements for protection against unauthorized users during the collection, storage, and transmission of data. Therefore, there is a need for the evolution of technologies that allows the authentication of users based on unique personal identifiers (biometric characteristics). This work presents a security management approach for authentication that stands out for using two combined convolutional neural networks (CNN) for the biometric identification of users. The new approach relies on Federated Learning (FL) which is a Machine Learning paradigm that can support data management and privacy by training decentralized models collaboratively without effective data exchange. The new approach also combines Photoplethysmography and Electrocardiogram signals which improves identification accuracy and establishes a multimodal authentication. In sum, the new security management approach achieves high Accuracy, and a low false acceptance rate, guaranteeing protection against unauthorized access attempts.",
        "DOI": "10.1016/j.bspc.2023.105022",
        "affiliation_name": "Universidade Federal de Minas Gerais",
        "affiliation_city": "Belo Horizonte",
        "affiliation_country": "Brazil"
    },
    {
        "paper_title": "Integrated learning self-triggered control for model-free continuous-time systems with convergence guarantees",
        "paper_author": "Wan H.",
        "publication": "Engineering Applications of Artificial Intelligence",
        "citied_by": "3",
        "cover_date": "2023-08-01",
        "Abstract": "This paper presents an integrated self-triggered control strategy with convergence guarantees for model-free continuous-time systems using reinforcement learning. To consider the control cost and triggering consumption in the self-triggered scheme simultaneously, an integrated cost function is proposed. With this integrated cost function, the trade-off between the triggering occupation and control performance could be adjusted according to different requirements. Then, the actor-critic framework of reinforcement learning is employed to learn the control inputs and triggering intervals by minimizing the corresponding integrated cost function. Considering the divergent characteristics between the control inputs and triggering intervals, two different actors are utilized to learn the triggering strategy and control policy, respectively. Also, the convergence of the developed model-free self-triggered control learning algorithm is proved to ensure the limited learning duration of both the control policy and triggering strategy. The proposed framework can be used to design self-triggered controllers for a wide range of engineering systems with unknow dynamics, including control of aircraft, robots, chemical processes, and other automated systems. Finally, the effectiveness and superiorities of the proposed method are verified by an illustrative example.",
        "DOI": "10.1016/j.engappai.2023.106462",
        "affiliation_name": "Politecnico di Milano",
        "affiliation_city": "Milan",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Land use land cover change detection and urban sprawl prediction for Kuwait metropolitan region, using multi-layer perceptron neural networks (MLPNN)",
        "paper_author": "Al-Dousari A.E.",
        "publication": "Egyptian Journal of Remote Sensing and Space Science",
        "citied_by": "33",
        "cover_date": "2023-08-01",
        "Abstract": "With the rapid expansion of cities, monitoring urban sprawl is recognized as a vital tool by many researchers who use this information in several applications like urban planning, microclimate modelling, policy development, etc. However, accurate land cover (LC) prediction is still challenging, even with technological advancements. Machine learning (ML) and artificial intelligence (AI) have gained a reputation amongst diverse science applications, including their popularity in monitoring land cover. Therefore, the present study investigates the performance of the ML-based classification algorithm random forest (RF) in monitoring LC classes for 2016 and 2021 for the metropolitan region of Kuwait City, Kuwait. The accuracy assessment for the derived land use maps achieved an overall accuracy of 93.6% and 95.3% and kappa coefficient values of 0.86 and 0.93 for 2016 and 2021, respectively. The results show an increase in built-up cover by ∼11 %. The land use maps for 2016 and 2021 were further used to predict the urban built-up for 2026 using an artificial neural network (ANN) based on multi-layer perceptron neural networks (MLPNNs). It was predicted with an overall accuracy of 83.6%. The built-up was predicted to increase by 15% in 2021–2026, and mostly expansion was observed on the western and southern sides. The outcomes exhibit that MLPNN techniques combined with Remote sensing and Geographic Information Systems (RS and GIS) can be adopted to derive the land cover and predict the urban sprawl with fair accuracy and precision. Such studies would prove valuable to city governments and urban planners to improve future sustainable development strategies.",
        "DOI": "10.1016/j.ejrs.2023.05.003",
        "affiliation_name": "College of Engineering Roorkee",
        "affiliation_city": "Roorkee",
        "affiliation_country": "India"
    },
    {
        "paper_title": "An innovative methodology for the determination of wind farms installation location characteristics using GIS and Delaunay Triangulation",
        "paper_author": "Xenitidis K.",
        "publication": "Energy for Sustainable Development",
        "citied_by": "9",
        "cover_date": "2023-08-01",
        "Abstract": "Renewable energy development and more specifically Wind Farm (WF) installation has been increased during the last years by most countries. A discipline that has been studied thoroughly is the impact of wind turbines on installation locations, during energy production or at the end of their life. Citizens’ attitudes, social and environmental factors are among the criteria affecting the selection of WFs installation locations. Various machine learning tools have been used in order to analyze the allocation of the Renewable Energy Systems (RES). In this paper a methodology based on computational geometry and Geographical Information Systems (GIS) is proposed in order to cluster the locations of Wind Turbines (WT) and define the boundaries of the areas that are affected by them. Delaunay Triangulation is selected as the main algorithm, since it is appropriate for handling geospatial data and it uses a geometric approach. Furthermore, algorithms based on graph theory are proposed in order to improve the efficiency of the methodology. The proposed methodology was implemented in locations of WTs in Greece and was compared to other known clustering algorithms. It was found that the locations of WTs in Greece are separated into 158 clusters. Some characteristics of the areas defined by the clusters, such as the distance from the protected areas, the distance from the closest airport and the number of residential areas that are within them, were computed and it was shown that these areas can be separated into 4 groups. The application aims to help policy makers to improve the effectiveness of their decisions by understanding in a better way which areas are more impaired or beneficial by the installation of WFs.",
        "DOI": "10.1016/j.esd.2023.05.006",
        "affiliation_name": "Forest Research Institute",
        "affiliation_city": "Thessaloniki",
        "affiliation_country": "Greece"
    },
    {
        "paper_title": "Classification of smart grid stability prediction using cascade machine learning methods and the internet of things in smart grid",
        "paper_author": "Önder M.",
        "publication": "Neural Computing and Applications",
        "citied_by": "18",
        "cover_date": "2023-08-01",
        "Abstract": "In a smart grid, the main goals are to provide grid stability, improve power system performance and security, and reduce operations, system maintenance, and planning costs. The prediction stability of smart grid (SG) systems is essential in terms of power loss minimization and the importance of adequate energy policies. SG systems must accurately predict the energy demand and ensure the right amount of energy is available at the right time. If the prediction is inaccurate, it can lead to costly energy production or usage errors and create considerable inefficiencies in the power grid. Due to this, this manuscript offers five different cascade methods to detect the stability of SG systems. Detecting the stability of SG systems enables the grid to respond quickly to changes in demand and supply, improves system reliability, reduces power outages, and increases the overall efficiency of the grid. The present work proposed five different cascade methods with pre-processing, training and testing division, and the classification stages of the classification procedure for estimating SG stability. In the first pre-processing stage, the SG dataset is pre-proceeded with the feature selection (Relief, Fast Correlation-Based Filter (FCBF), and supervised attribute filter). The resampling (the bootstrapping), the Fuzzy C-Means Clustering-Based Feature Weighting (FCMFW), the resampling then feature selection (supervised attribute filter), and the feature selection (supervised attribute filter), then FCMFW. In the second stage, the training and testing division stage, the SG dataset was separated into three test and training data methods before the classification algorithm: The 5 Fold Cross Validation (FVC), 10 FVC, and hold-out (50–50%). In the third stage, the classification stage, five different classification algorithms, including Naive Kernel Bayes, Linear Support Vector Machine (SVM), Weighted K-Nearest Neighbors, Begged Trees, and Narrow Neural Network classifying algorithms, are used to classify the SG dataset. The simulation results of this study demonstrated that the suggested cascade ML system had achieved significant accuracy in predicting SG stability. The best cascade method is the feature selection (supervised attribute filter) + FCMFW + 10 FCV and then performing the bagged trees algorithm; thus, the new approach affords an accuracy of 99.9%. Furthermore, due to the rapid growth of ML techniques, sensors, and smart meters technologies, with Machine to Machine communication via the internet of things (IoT), the real-time identification process is made practical with higher accuracy. For this reason, our future research will focus on an IoT-based SG system, an E-stability determination system. Thanks to the proposed cascade method, the SG dataset can be classified easily, quickly, and reliably. E-stability determination systems can help to fast detect, predict, and respond, which is an important application of IoT on the grid systems.",
        "DOI": "10.1007/s00521-023-08605-x",
        "affiliation_name": "Bolu Abant İzzet Baysal Üniversitesi",
        "affiliation_city": "Bolu",
        "affiliation_country": "Türkiye"
    },
    {
        "paper_title": "Reinforcement Learning Model for Managing Noninvasive Ventilation Switching Policy",
        "paper_author": "Feng X.",
        "publication": "IEEE Journal of Biomedical and Health Informatics",
        "citied_by": "0",
        "cover_date": "2023-08-01",
        "Abstract": "Noninvasive ventilation (NIV) has been recognized as a first-line treatment for respiratory failure in patients with chronic obstructive pulmonary disease (COPD) and hypercapnia respiratory failure, which can reduce mortality and burden of intubation. However, during the long-term NIV process, failure to respond to NIV may cause overtreatment or delayed intubation, which is associated with increased mortality or costs. Optimal strategies for switching regime in the course of NIV treatment remain to be explored.For the goal of reducing 28-day mortality of the patients undergoing NIV, Double Dueling Deep Q Network (D3QN) of offline-reinforcement learning algorithm was adopted to develop an optimal regime model for making treatment decisions of discontinuing ventilation, continuing NIV, or intubation. The model was trained and tested using the data from Multi-Parameter Intelligent Monitoring in Intensive Care III (MIMIC-III) and evaluated by the practical strategies. Furthermore, the applicability of the model in majority disease subgroups (Catalogued by International Classification of Diseases, ICD) was investigated. Compared with physician's strategies, the proposed model achieved a higher expected return score (4.25 vs. 2.68) and its recommended treatments reduced the expected mortality from 27.82% to 25.44% in all NIV cases. In particular, for these patients finally received intubation in practice, if the model also supported the regime, it would warn of switching to intubation 13.36 hours earlier than clinicians (8.64 vs. 22 hours after the NIV treatment), granting a 21.7% reduction in estimated mortality. In addition, the model was applicable across various disease groups with distinguished achievement in dealing with respiratory disorders. The proposed model is promising to dynamically provide personalized optimal NIV switching regime for patients undergoing NIV with the potential of improving treatment outcomes.",
        "DOI": "10.1109/JBHI.2023.3274568",
        "affiliation_name": "Zhejiang Lab",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Estimating the common agricultural policy milestones and targets by neural networks",
        "paper_author": "Bonfiglio A.",
        "publication": "Evaluation and Program Planning",
        "citied_by": "5",
        "cover_date": "2023-08-01",
        "Abstract": "The New Delivery Model, introduced by the 2023–2027 Common Agricultural Policy, shifts the focus of policy programming and design from a compliance-based approach to one based on performance. The objectives indicated in the national strategic plans are monitored through the definition of a set of milestones and targets. This makes it necessary to define realistic and financially consistent target values. The aim of this paper is to outline a methodology to quantify robust target values for result indicators. As the main method, a machine learning model based on multilayer feedforward neural networks is put forward. This method is chosen for its ability to model possible non-linearities in the monitoring data and estimate multiple outputs. The proposed methodology is applied to the Italian case, more specifically to estimate target values for the result indicator related to enhancing performance through knowledge and innovation for 21 regional managing authorities. The related performance is then compared with that of traditional methods adopted to estimate target values. Results demonstrate the superiority of neural networks and suggest that this methodology might be used as a tool to help all Member States fulfill the key task of setting coherent and realistic targets for all result indicators.",
        "DOI": "10.1016/j.evalprogplan.2023.102296",
        "affiliation_name": "Council for Agricultural Research and Economics",
        "affiliation_city": "Rome",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "The effect of consumption and production policies on circular economy business models: A machine learning approach",
        "paper_author": "Arranz C.F.A.",
        "publication": "Journal of Industrial Ecology",
        "citied_by": "6",
        "cover_date": "2023-08-01",
        "Abstract": "The circular economy (CE) is attracting increasing interest, as it can bring environmental, social, and economic benefits. However, policymakers and scholars appear to concentrate more on the production side of CE, while consumption, and particularly policies that affect consumption have received less attention and their effect is ambiguous. This paper investigates the effect of CE consumption policies on circular economy business models (CEBMs) in firms, but also examines the interplay this type of policies have with CE production policies to have a broader picture of the circular economy policy framework and the relevance of each type of policy on firms. While previous studies assume rational and passive consumer behavior, this paper borrows from a natural resource-based view and stakeholder theory, arguing that consumers have a proactive attitude toward the consumption of environmentally friendly products. Moreover, we use institutional theory as an analytical framework for modeling the effects of a particular policy framework on the CEBM. Our analysis combines classical econometric methods with machine learning approaches, employing data from the EU. The results show that CE policies aimed at promoting consumption have a direct and positive effect on CEBMs. This paper also confirms that a wide portfolio of CE policies on production and consumption has a greater effect on the development of CEBMs, due to the complementarity of CE consumption and production policies. Moreover, we show that in interaction with CE production policies, CE policies on consumption have an even greater effect on CEBMs in firms than would have been anticipated.",
        "DOI": "10.1111/jiec.13397",
        "affiliation_name": "Essex Business School",
        "affiliation_city": "Colchester",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Development of lignocellulosic biorefineries for the sustainable production of biofuels: Towards circular bioeconomy",
        "paper_author": "Yadav A.",
        "publication": "Bioresource Technology",
        "citied_by": "63",
        "cover_date": "2023-08-01",
        "Abstract": "The idea of environment friendly and affordable renewable energy resources has prompted the industry to focus on the set up of biorefineries for sustainable bioeconomy. Lignocellulosic biomass (LCB) is considered as an abundantly available renewable feedstock for the production of biofuels which can potentially reduce the dependence on petrochemical refineries. By utilizing various conversion technologies, an integrated biorefinery platform of LCB can be created, embracing the idea of the ‘circular bioeconomy’. The development of effective pretreatment methods and biocatalytic systems by various bioengineering and machine learning approaches could reduce the bioprocessing costs, thereby making biomass-based biorefinery more sustainable. This review summarizes the development and advances in the lignocellulosic biorefineries from the LCB to the final product stage using various different state-of-the-art approaches for the progress of circular bioeconomy. The life cycle assessment which generates knowledge on the environmental impacts related to biofuel production chains is also summarized.",
        "DOI": "10.1016/j.biortech.2023.129145",
        "affiliation_name": "National Kaohsiung University of Science and Technology",
        "affiliation_city": "Kaohsiung",
        "affiliation_country": "Taiwan"
    },
    {
        "paper_title": "Winter-autumn air pollution control plan in North China modified the PM<inf>2.5</inf> compositions and sources in Central China",
        "paper_author": "Jiang S.",
        "publication": "Atmospheric Environment",
        "citied_by": "2",
        "cover_date": "2023-08-01",
        "Abstract": "The additional impact of emission-reduction measures in North China (NC) during autumn and winter on the air quality of downwind regions is an interesting but less addressed topic. The mass concentrations of routine air pollutants, the chemical compositions, and sources of fine particles (PM2.5) for January 2018, 2019, and 2020 at a megacity of Central China were identified, and meteorology-isolated by a machine-learning technique. Their variations were classified according to air mass direction. An unexpectedly sharp increase in emission-related PM2.5 by 22.7% (18.0 μg m−3) and 25.7% (19.4 μg m−3) for air masses from local and NC in 2019 was observed compared to those of 2018. Organic materials exhibited the highest increase in PM2.5 compositions by 6.90 μg m−3 and 6.23 μg m−3 for the air masses from local and NC. PM2.5 source contributions related to emission showed an upsurge from 1.39 μg m−3 (biomass burning) to 24.9 μg m−3 (secondary inorganic aerosol) in 2019 except for industrial processes, while all reduced in 2020. From 2018 to 2020, the emission-related contribution of coal combustion to PM2.5 increased from 10.0% to 19.0% for air masses from the local area. To support the priority natural gas quotas in northern China, additional coal in cities of southern China was consumed, raising related emissions from transportation activities and road dust in urban regions, as well as additional biofuel consumption in suburban or rural regions. All these activities could explain the increased primary PM2.5 and related precursor NO2. This study gave substantial evidence of air pollution control measures impacting the downwind regions and promote the necessity of air pollution joint control across the administration.",
        "DOI": "10.1016/j.atmosenv.2023.119827",
        "affiliation_name": "School of Earth Sciences, Zhejiang University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "The impacts of ambient ozone pollution on China's wheat yield and forest production from 2010 to 2021",
        "paper_author": "Wang Y.",
        "publication": "Environmental Pollution",
        "citied_by": "16",
        "cover_date": "2023-08-01",
        "Abstract": "Near-surface ozone causes damages on both crop and forest but their long-term spatiotemporal changes in China have been insufficiently explored, preventing comprehensive policy making with food security and climate targets. Moreover, limitation exists in the current metrics for long-term regional ozone risk assessment, AOT40 (the accumulated hourly ozone over a threshold of 40 ppbv) and PODY (phytotoxic ozone dose over a threshold of Y nmol ozone m−2 PLA s−1), with ignorance of meteorological influence for the former and complicated data collection and calculation procedures for the latter. Here, we developed a new metric for ozone-induced risk on winter wheat, O3MET, which can be easily derived based on ozone concentrations and meteorological variables, and is suitable for long-term assessment of ozone-induced wheat loss at the regional scale. Combining with existing metric for forest (O3RH), we comprehensively quantified the ozone damages on winter wheat yield and forest gross primary production (GPP) for mainland China during 2010–2021, the period with fast growth of ozone level across the country. The annual average losses of wheat yield and forest GPP were estimated at 26.5 Mt and 552.6 TgC, accounting for 17% and 4% of the total yield and GPP without ozone impact, respectively. Heavy dual ozone-induced damages on both wheat and forest were presented in East and South China. The ozone-induced wheat yield loss and forest GPP loss were estimated to increase at a rate of 1.8 Mt/yr and 13.9 TgC/yr for the entire country, respectively, driven mainly by the enhanced ambient ozone level within the research period. Besides ecological impact, the ozone pollution in the developed eastern China resulted in serious health burden as well, thus effective actions on ozone pollution alleviation in the region is crucial for reducing its ecological and health risks simultaneously.",
        "DOI": "10.1016/j.envpol.2023.121726",
        "affiliation_name": "Nanjing University of Information Science &amp; Technology",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Automatic document classification via transformers for regulations compliance management in large utility companies",
        "paper_author": "Dimlioglu T.",
        "publication": "Neural Computing and Applications",
        "citied_by": "4",
        "cover_date": "2023-08-01",
        "Abstract": "The operation of large utility companies such as Consolidated Edison Company of New York, Inc. (Con Edison) typically rely on large quantities of regulation documents from external institutions which inform the company of upcoming or ongoing policy changes or new requirements the company might need to comply with if deemed applicable. As a concrete example, if a recent regulatory publication mentions that the timeframe for the Company to respond to a reported system emergency in its service territory changes from within X time to within Y time—then the affected operating groups will be notified, and internal Company operating procedures may need to be reviewed and updated accordingly to comply with the new regulatory requirement. Each such regulation document needs to be reviewed manually by an expert to determine if the document is relevant to the company and, if so, which department it is relevant to. In order to help enterprises improve the efficiency of their operation, we propose an automatic document classification pipeline that determines whether a document is important for the company or not, and if deemed important it forwards those documents to the departments within the company for further review. Binary classification task of determining the importance of a document is done via ensembling the Naive Bayes (NB), support vector machine (SVM), random forest (RF), and artificial neural network (ANN) together for the final prediction, whereas the multi-label classification problem of identifying the relevant departments for a document is executed by the transformer-based DocBERT model. We apply our pipeline to a large corpus of tens of thousands of text data provided by Con Edison and achieve an accuracy score over 80 % . Compared with existing solutions for document classification which rely on a single classifier, our paper i) ensemble multiple classifiers for better accuracy results and escaping from the problem of overfitting, ii) utilize pretrained transformer-based DocBERT model to achieve ideal performance for multi-label classification task and iii) introduce a bi-level structure to improve the performance of the whole pipeline where the binary classification module works as a rough filter before finally distributing the text to corresponding departments through the multi-label classification module.",
        "DOI": "10.1007/s00521-023-08555-4",
        "affiliation_name": "NYU Tandon School of Engineering",
        "affiliation_city": "New York",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Predicting total household energy expenditures using ensemble learning methods",
        "paper_author": "Kesriklioğlu E.",
        "publication": "Energy",
        "citied_by": "5",
        "cover_date": "2023-08-01",
        "Abstract": "Total household energy expenditures are a complex topic because so many behavioral, technological, environmental, and policy variables can affect expenditures. This study aimed to develop a high-performance ensemble learning (EL) model to classify total household energy expenditures. For this purpose, household consumption data from 11,521 households were examined using the Household Budget Survey 2019 data set that the Turkish Statistical Institute (TURKSTAT) published. In addition to the variables directly related to household energy expenditures, new variables were created within the framework of the literature and under the guidance of expert opinion. The prepared data were passed through data preprocessing, modeling, prediction, and performance evaluation stages using the open source RapidMiner software program. Classification performances of machine learning and EL methods were compared. Aside from k-nearest neighbor, decision tree, naive Bayes, random forest, gradient boosted trees, and DFNN classifiers, the study used bagging, boosting, voting, and stacking EL methods. The stacking EL method in the ALL model and bagging EL method in the deep feed forward neural network (DFNN) classifiers achieved the highest performance among EL methods. The accuracy value of the stacking and bagging methods was 0.984. The results indicate that EL methods can enhance individual machine learning methods significantly.",
        "DOI": "10.1016/j.energy.2023.127581",
        "affiliation_name": "Atatürk Üniversitesi",
        "affiliation_city": "Erzurum",
        "affiliation_country": "Turkey"
    },
    {
        "paper_title": "Susceptible exposed infectious recovered-machine learning for COVID-19 prediction in Saudi Arabia",
        "paper_author": "Alsmadi M.K.",
        "publication": "International Journal of Electrical and Computer Engineering",
        "citied_by": "4",
        "cover_date": "2023-08-01",
        "Abstract": "Susceptible exposed infectious recovered (SEIR) is among the epidemiological models used in forecasting the spread of disease in large populations. SEIR is a fitting model for coronavirus disease (COVID-19) spread prediction. Somehow, in its original form, SEIR could not measure the impact of lockdowns. So, in the SEIR equations system utilized in this study, a variable was included to evaluate the impact of varying levels of social distance on the transmission of COVID-19. Additionally, we applied artificial intelligence utilizing the deep neural network machine learning (ML) technique. On the initial spread data for Saudi Arabia that were available up to June 25th, 2021, this improved SEIR model was used. The study shows possible infection to around 3.1 million persons without lockdown in Saudi Arabia at the peak of spread, which lasts for about 3 months beginning from the lockdown date (March 21st). On the other hand, the Kingdom's current partial lockdown policy was estimated to cut the estimated number of infections to 0.5 million over nine months. The data shows that stricter lockdowns may successfully flatten the COVID-19 graph curve in Saudi Arabia. We successfully predicted the COVID-19 epidemic's peaks and sizes using our modified deep neural network (DNN) and SEIR model.",
        "DOI": "10.11591/ijece.v13i4.pp4761-4776",
        "affiliation_name": "Imam Abdulrahman Bin Faisal University",
        "affiliation_city": "Dammam",
        "affiliation_country": "Saudi Arabia"
    },
    {
        "paper_title": "Joint optimization of maintenance and quality inspection for manufacturing networks based on deep reinforcement learning",
        "paper_author": "Ye Z.",
        "publication": "Reliability Engineering and System Safety",
        "citied_by": "16",
        "cover_date": "2023-08-01",
        "Abstract": "Most existing studies on joint optimization of manufacturing systems (MS) focus on small-scale systems with simple structures, such as the single-machine, simple serial, or parallel MS. Simultaneously, traditional algorithms utilized in small-scale MS always show an insufficiency in solving large-scale dynamic MS with complex structures, such as manufacturing networks. Therefore, considering the effectiveness of reinforcement learning on the infinite-horizon Markov Decision Process (MDP), this paper presents a joint optimization problem of preventive maintenance and work-in-process quality inspection for manufacturing networks with reliability-quality interactions. First, dynamic reliability and quality models are proposed at the machine level to cope with complex interactions in manufacturing networks. Second, based on the MDP-based optimization model, the proposed Deep Deterministic Policy Gradient (DDPG) algorithm realizes the optimal reliability-quality joint control in manufacturing networks. Besides, it also offers a novel mixed action space containing discrete maintenance and continuous quality inspection, which could satisfy the action diversity in actual production. At last, training and experiments imply our algorithm is more adaptable to diverse manufacturing scenarios than traditional ones. Also, it is proved that more-frequent state observations for learning cannot help the constructed reinforcement learning model get a better control policy because of the information redundancy.",
        "DOI": "10.1016/j.ress.2023.109290",
        "affiliation_name": "Penn State College of Engineering",
        "affiliation_city": "University Park",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Energy intensity, economic structure, and capital goods imports in upper-middle income countries: Insights from HDBSCAN clustering",
        "paper_author": "Carrasco C.A.",
        "publication": "Journal of Environmental Management",
        "citied_by": "1",
        "cover_date": "2023-08-01",
        "Abstract": "In recent years, there have been substantial efforts to improve the efficiency of production resources, including energy use, to reduce the human footprint from economic activities. Increasing production capacity and incorporating new technologies that improve energy efficiency in the production process are two primary challenges faced by developing countries, where capital goods imports could play a key role in addressing both challenges. This paper contributes to the empirical literature by examining the relationship between energy intensity, economic structure, and capital goods imports in a set of 36 upper-middle income economies in the period 2000–2019. The empirical strategy recognizes the existing heterogeneity among the broad group of countries in the sample by implementing the Hierarchical Density-Based Spatial Clustering of Applications with Noise algorithm, a state-of-the-art unsupervised machine learning technique which allows identification of clusters of countries and years. The results show the existence of ten clusters, where energy intensity has the most relevant positive associations with industry share, trade openness, and merchandise imports. Improvements in regulatory quality are associated with lower energy intensity. The direction and strength of the relationship between energy intensity and capital goods imports depend on the cluster; nonetheless, it is usually a weak relationship. Policy implications are discussed.",
        "DOI": "10.1016/j.jenvman.2023.117840",
        "affiliation_name": "Instituto Politécnico Nacional",
        "affiliation_city": "Mexico",
        "affiliation_country": "Mexico"
    },
    {
        "paper_title": "Unpacking the sustainable development goals (SDGs) interlinkages: A semantic network analysis of the SDGs targets",
        "paper_author": "Song J.",
        "publication": "Sustainable Development",
        "citied_by": "21",
        "cover_date": "2023-08-01",
        "Abstract": "Understanding sustainable development goal (SDG) targets and their interlinkages is crucial for achieving national and local SDGs since policies must be designed and implemented at the target level rather than at the macro goal level. However, due to their extensive nature, it remains challenging to fully determine their interlinkages. This study aims to identify the interlinkages between the SDG targets, employing a semantic network analysis with text-mining and Word2Vec machine-learning methodology. The network analysis of the entire SDG target network reveals that each community of closely connected targets comprises targets from multiple SDG goals, while targets within the same SDG goal also belong to different communities. The findings indicate that the current framework of 17 SDG goals may not be a suitable basis for developing SDG policies. Instead, intersectoral strategies focusing on a community of interconnected targets that necessitate coordination based on their actual interlinkages are required.",
        "DOI": "10.1002/sd.2547",
        "affiliation_name": "Seoul National University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "A combination-based machine learning algorithm estimating impacts of social, economic, and environmental on resident health—on China's provincial panel data",
        "paper_author": "Wen L.",
        "publication": "Engineering Applications of Artificial Intelligence",
        "citied_by": "4",
        "cover_date": "2023-08-01",
        "Abstract": "The factors influencing residents health have become complex and intertwined with the development of economy and society. Traditional research with a single factor on health will not provide an accurate picture of the situation. This paper collects data on economic, environmental and social factors to estimate their impact on regional health. Considering the data is multi-source and complex, this paper proposes a combined feature importance algorithm, which weighted the feature importance of RF, XGB and SOIL. The algorithm does not depend on the data and adaptively approximates the true results. The results show that economic factors have a significant and direct impact on health, environmental factors have a lag correlation with health level, and social factors have a more complicated effect on health. Finally, we provide policy suggestions for health on economic, environmental, and social development.",
        "DOI": "10.1016/j.engappai.2023.106135",
        "affiliation_name": "Wuhan University",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Social welfare in the light of topic modelling",
        "paper_author": "Baranowski M.",
        "publication": "Sociology Compass",
        "citied_by": "8",
        "cover_date": "2023-08-01",
        "Abstract": "With an increased focus on social well-being in response to a burgeoning global economy exposing the weaknesses of social welfare policies, research output in the field has grown exponentially. Keeping track of the evolving research themes proves difficult due to the steady rise in the number of studies published in the interdisciplinary field of social welfare. Therefore, researchers need a comprehensive overview to confirm the current shape of the field based on the published research. Using a latent Dirichlet allocation algorithm as a topic modelling technique, this study identified 12 prominent themes from more than 10,000 research outputs on social welfare published from 2000 to 2020 in Scopus-indexed journals. Such an exploratory text-mining approach to literature review provides broad insights into the diversity of research and may serve as a foundation for further in-depth studies. Identifying these 12 thematic areas and their sub-themes allows us to articulate the complexity and diversity of social welfare issues, which go far beyond the field of well-established welfare economics or social work. The study shows that the topic of ‘social welfare’ has not only evolved over time but has significantly broadened its meaning. It can no longer be solely synonymous with institutional social security. We contend that research in this area needs to take into account a broader and more systematic range of determinants constituting the dynamic character of social welfare.",
        "DOI": "10.1111/soc4.13086",
        "affiliation_name": "Uniwersytet im. Adama Mickiewicza w Poznaniu",
        "affiliation_city": "Poznan",
        "affiliation_country": "Poland"
    },
    {
        "paper_title": "REFER: Randomized Online Factor Selection Framework for portfolio Management",
        "paper_author": "Li Y.",
        "publication": "Expert Systems with Applications",
        "citied_by": "3",
        "cover_date": "2023-08-01",
        "Abstract": "Portfolio management is a critical problem in both machine learning and finance communities. To predict the returns of assets, existing studies have been leveraging side information to mine price-sensitive indicators, i.e., factors. However, with the brisk expansion of factor collection, existing factor selection methods face two main issues, i.e., high-cost and low-precision. In this paper, we first formalize the task of online factor selection as an online learning problem where a learner selects a set of factors in each round and aims to minimize the long-term regret. Then, we propose a Randomized onlinE Factor sElection fRamework, named REFER, which not only is particularly devised to address the above two issues, but also can widely serve for any existing multifactor models. Specifically, by studying the regret of existing factor-selecting policies, we propose two randomized policies along with their bandit variants that achieve sublinear regrets. The bandit variants further improve computational efficiency, and achieve a balanced trade-off between cost and precision. Finally, both theoretical analysis and extensive experiments on the real-world dataset demonstrate the effectiveness of our proposed framework and policies in terms of comprehensive evaluation criteria.",
        "DOI": "10.1016/j.eswa.2023.119837",
        "affiliation_name": "College of Computer Science and Technology, Zhejiang University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Explainable reinforcement learning for broad-XAI: a conceptual framework and survey",
        "paper_author": "Dazeley R.",
        "publication": "Neural Computing and Applications",
        "citied_by": "20",
        "cover_date": "2023-08-01",
        "Abstract": "Broad-XAI moves away from interpreting individual decisions based on a single datum and aims to provide integrated explanations from multiple machine learning algorithms into a coherent explanation of an agent’s behaviour that is aligned to the communication needs of the explainee. Reinforcement Learning (RL) methods, we propose, provide a potential backbone for the cognitive model required for the development of Broad-XAI. RL represents a suite of approaches that have had increasing success in solving a range of sequential decision-making problems. However, these algorithms operate as black-box problem solvers, where they obfuscate their decision-making policy through a complex array of values and functions. EXplainable RL (XRL) aims to develop techniques to extract concepts from the agent’s: perception of the environment; intrinsic/extrinsic motivations/beliefs; Q-values, goals and objectives. This paper aims to introduce the Causal XRL Framework (CXF), that unifies the current XRL research and uses RL as a backbone to the development of Broad-XAI. CXF is designed to incorporate many standard RL extensions and integrated with external ontologies and communication facilities so that the agent can answer questions that explain outcomes its decisions. This paper aims to: establish XRL as a distinct branch of XAI; introduce a conceptual framework for XRL; review existing approaches explaining agent behaviour; and identify opportunities for future research. Finally, this paper discusses how additional information can be extracted and ultimately integrated into models of communication, facilitating the development of Broad-XAI.",
        "DOI": "10.1007/s00521-023-08423-1",
        "affiliation_name": "Universidad Central de Chile",
        "affiliation_city": "Santiago",
        "affiliation_country": "Chile"
    },
    {
        "paper_title": "Reinforcement learning based approach for the optimization of mechanical properties of additively manufactured specimens",
        "paper_author": "Mishra A.",
        "publication": "International Journal on Interactive Design and Manufacturing",
        "citied_by": "6",
        "cover_date": "2023-08-01",
        "Abstract": "Reinforcement learning (RL) is a subfield of machine learning that has shown significant promise in improving the efficiency and quality of additive manufacturing (AM). Additive manufacturing, also known as 3D printing, is a process of building objects layer by layer using digital models. RL can help optimize the manufacturing process by allowing machines to learn from their experiences and make decisions accordingly. In AM, this can be particularly useful for controlling the printing parameters such as temperature, speed, and material flow rate. By using RL algorithms to optimize these parameters, manufacturers can achieve higher levels of precision, accuracy, and speed, while minimizing waste and reducing costs. The goal of the present research work is to find the optimal parameters of an additively manufactured specimens’ experimental dataset that contains four input parameters i.e. Infill percentage, Layer height, Print speed and Extrusion temperature and three output parameters i.e. Tensile Strength, Flexural Strength and Impact Strength using Q-learning. Q-learning is a popular reinforcement learning algorithm that is used to find the optimal policy for an agent in a given environment. In the present work, Q-learning algorithm is used to optimize the parameters of the dataset in order to achieve the highest possible performance. The results showed that the obtained Tensile Strength, Flexural Strength and Impact strength have error of 0.66%, 4.37%, and 10.2% in compared to the experimental results.",
        "DOI": "10.1007/s12008-023-01257-0",
        "affiliation_name": "Symbiosis Institute of Technology",
        "affiliation_city": "Pune",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Forecasting air quality index considering socioeconomic indicators and meteorological factors: A data granularity perspective",
        "paper_author": "Wang C.H.",
        "publication": "Journal of Forecasting",
        "citied_by": "6",
        "cover_date": "2023-08-01",
        "Abstract": "Forecasting air quality index (AQI) is critically important to provide a basis for government policy makers, especially in public health, smart transportation, energy management, economic development, and sustainable environments. In reality, AQI consists of various components, such as PM2.5, PM10, CO, NO2, and SO2. Although numerous methods have been presented, few studies concurrently considered the causalities of socioeconomic indicators and meteorological factors and different data granularities. The aggregate AQI of Taiwan comprises five representative cities: Taipei, Hsinchu, Taichung, Tainan, and Kaohsiung. Research findings identify seasonal factors, carbon power generation, steel and metal production, highway cargo load, the number of registered cars, and retail and manufacturing employment population as the key indicators to predict the monthly AQI of Taiwan. For the daily AQI of Hsinchu and the hourly AQI of Kaohsiung, PM2.5, PM10, O3, ambient temperature, humidity, wind speed, wind direction, and pollutants (CO, NO2, and SO2) are recognized. Deep learning significantly outperforms machine learning in the hourly AQI while it performs slightly better in the daily AQI. With the presented framework, governments can balance the trade-offs between economic development and environmental sustainability.",
        "DOI": "10.1002/for.2962",
        "affiliation_name": "National Yang Ming Chiao Tung University",
        "affiliation_city": "Hsinchu",
        "affiliation_country": "Taiwan"
    },
    {
        "paper_title": "A methodology to design, develop, and evaluate machine learning models for predicting dropout in school systems: the case of Chile",
        "paper_author": "Rodríguez P.",
        "publication": "Education and Information Technologies",
        "citied_by": "17",
        "cover_date": "2023-08-01",
        "Abstract": "School dropout is a structural problem which permanently penalizes students and society in areas such as low qualification jobs, higher poverty levels and lower life expectancy, lower pensions, and higher economic burden for governments. Given these high consequences and the surge of the problem due to COVID-19 pandemic, in this paper we propose a methodology to design, develop, and evaluate a machine learning model for predicting dropout in school systems. In this methodology, we introduce necessary steps to develop a robust model to estimate the individual risk of each student to drop out of school. As advancement from previous research, this proposal focuses on analyzing individual trajectories of students, incorporating the student situation at school, family, among other levels, changes, and accumulation of events to predict dropout. Following the methodology, we create a model for the Chilean case based on data available mostly through administrative data from the educational system, and according to known factors associated with school dropout. Our results are better than those from previous research with a relevant sample size, with a predictive capability 20% higher for the actual dropout cases. Also, in contrast to previous work, the including non-individual dimensions results in a substantive contribution to the prediction of leaving school. We also illustrate applications of the model for Chilean case to support public policy decision making such as profiling schools for qualitative studies of pedagogic practices, profiling students’ dropout trajectories and simulating scenarios.",
        "DOI": "10.1007/s10639-022-11515-5",
        "affiliation_name": "Universidad de Chile",
        "affiliation_city": "Santiago",
        "affiliation_country": "Chile"
    },
    {
        "paper_title": "Predicting academic success of autistic students in higher education",
        "paper_author": "Bakker T.",
        "publication": "Autism",
        "citied_by": "6",
        "cover_date": "2023-08-01",
        "Abstract": "Individuals with autism increasingly enroll in universities, but little is known about predictors for their success. This study developed predictive models for the academic success of autistic bachelor students (N = 101) in comparison to students with other health conditions (N = 2465) and students with no health conditions (N = 25,077). We applied propensity score weighting to balance outcomes. The research showed that autistic students’ academic success was predictable, and these predictions were more accurate than predictions of their peers’ success. For first-year success, study choice issues were the most important predictors (parallel program and application timing). Issues with participation in pre-education (missingness of grades in pre-educational records) and delays at the beginning of autistic students’ studies (reflected in age) were the most influential predictors for the second-year success and delays in the second and final year of their bachelor’s program. In addition, academic performance (average grades) was the strongest predictor for degree completion in 3 years. These insights can enable universities to develop tailored support for autistic students. Using early warning signals from administrative data, institutions can lower dropout risk and increase degree completion for autistic students. Laymen Summary: What is already known about the topic? Autistic youths increasingly enter universities. We know from existing research that autistic students are at risk of dropping out or studying delays. Using machine learning and historical information of students, researchers can predict the academic success of bachelor students. However, we know little about what kind of information can predict whether autistic students will succeed in their studies and how accurate these predictions will be. What does this article add? In this research, we developed predictive models for the academic success of 101 autistic bachelor students. We compared these models to 2,465 students with other health conditions and 25,077 students without health conditions. The research showed that the academic success of autistic students was predictable. Moreover, these predictions were more precise than predictions of the success of students without autism. For the success of the first bachelor year, concerns with aptitude and study choice were the most important predictors. Participation in pre-education and delays at the beginning of autistic students’ studies were the most influential predictors for second-year success and delays in the second and final year of their bachelor’s program. In addition, academic performance in high school was the strongest predictor for degree completion in 3 years. Implications for practice, research, or policy These insights can enable universities to develop tailored support for autistic students. Using early warning signals from administrative data, institutions can lower dropout risk and increase degree completion for autistic students.",
        "DOI": "10.1177/13623613221146439",
        "affiliation_name": "Vrije Universiteit Amsterdam",
        "affiliation_city": "Amsterdam",
        "affiliation_country": "Netherlands"
    },
    {
        "paper_title": "Migration narratives in Northern Triangle, Mexican and US media from 1999 to 2019",
        "paper_author": "Hinck R.S.",
        "publication": "International Migration",
        "citied_by": "2",
        "cover_date": "2023-08-01",
        "Abstract": "This study conceptualizes the mediated discussions of migration in Central America as narratives, arguing for the need to examine the broader contours of policy-related migration reporting across time. Using machine learning and text mining analyses, combined with qualitative narrative analysis, the study examines 53,441 news articles from 17 US, Mexican and Northern Triangle media outlets from 1999 to 2019, tracing and critiquing the shifts in coverage. Findings suggest that all three media systems generally align in their depiction of the scene, key agents and acts regarding migration; however, US narratives increasingly diverge from Northern Triangle and Mexican narratives regarding the purpose and instruments by which migration occurs, with US value claims narrowing over time emphasizing border security. This narrative trajectory within US media ignores migrants' determination and underlining rationales for migration, pushing them to take increasingly dangerous means to migrate to the USA and exacerbating the situation for all parties.",
        "DOI": "10.1111/imig.13110",
        "affiliation_name": "United States Coast Guard Academy",
        "affiliation_city": "New London",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Extracting Decision Tree From Trained Deep Reinforcement Learning in Traffic Signal Control",
        "paper_author": "Zhu Y.",
        "publication": "IEEE Transactions on Computational Social Systems",
        "citied_by": "10",
        "cover_date": "2023-08-01",
        "Abstract": "Deep reinforcement learning (DRL) has achieved impressive success in traffic signal control systems (TSCS). However, since a key component of many DRL models is the complex deep neural networks (DNNs), it hinders humans or experts from understanding and explaining the learned policy. Recently, many works have focused on developing interpretable techniques to compress or distill complex DNNs into smaller, faster, or more understandable models. The decision trees (DTs) are viewed as the de facto technique for interpretable and transparent machine learning, and can provide an easy-understanding decision path from the root to the leaf node. In this work, we utilize modified DTs to extract models with simpler hierarchical structures from premium policy achieved by DRL methods. First, we use a DRL algorithm to learn a premium policy for traffic signal control. Then, we collect a dataset with the learned premium policy by interacting with the environment. Finally, we extract the DTs with the collected dataset of state-actions pairs. We evaluate our method on a Simulation of Urban Mobility simulator in a simulation way. Simulation results show that the extracted DTs can generate human-understandable decision processes and provide explicit knowledge from the DNNs reference of the DRL.",
        "DOI": "10.1109/TCSS.2022.3225362",
        "affiliation_name": "Nanjing University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Example-guided learning of stochastic human driving policies using deep reinforcement learning",
        "paper_author": "Emuna R.",
        "publication": "Neural Computing and Applications",
        "citied_by": "3",
        "cover_date": "2023-08-01",
        "Abstract": "Deep reinforcement learning has been successfully applied to the generation of goal-directed behavior in artificial agents. However, existing algorithms are often not designed to reproduce human-like behavior, which may be desired in many environments, such as human–robot collaborations, social robotics and autonomous vehicles. Here we introduce a model-free and easy-to-implement deep reinforcement learning approach to mimic the stochastic behavior of a human expert by learning distributions of task variables from examples. As tractable use-cases, we study static and dynamic obstacle avoidance tasks for an autonomous vehicle on a highway road in simulation (Unity). Our control algorithm receives a feedback signal from two sources: a deterministic (handcrafted) part encoding basic task goals and a stochastic (data-driven) part that incorporates human expert knowledge. Gaussian processes are used to model human state distributions and to assess the similarity between machine and human behavior. Using this generic approach, we demonstrate that the learning agent acquires human-like driving skills and can generalize to new roads and obstacle distributions unseen during training.",
        "DOI": "10.1007/s00521-022-07947-2",
        "affiliation_name": "Ben-Gurion University of the Negev",
        "affiliation_city": "Beer-Sheva",
        "affiliation_country": "Israel"
    },
    {
        "paper_title": "The public-facing policy agenda of state legislatures: The communication of public policy via twitter",
        "paper_author": "Peterson D.A.M.",
        "publication": "Policy Studies Journal",
        "citied_by": "2",
        "cover_date": "2023-08-01",
        "Abstract": "How political actors choose which politics to focus on helps shape the outcome of the policy process. While the policy agenda of the federal government has received widespread attention, there is much less known about the policy agendas of the U.S. states. In this paper, we describe how and why states choose to have similar agendas. We rely on the Twitter activity of every state legislator in America to measure the attention that states pay to the categories developed in the Policy Agenda Project (PAP). We develop machine learning tools to measure the proportion of tweets from every state legislature from 2017 in each of the PAP policy topics. Our results show that states that the public-facing policy agenda of a state legislature is correlated with the level of legislative professionalism and the partisan and ideological politics of the state. These results further our understanding of state policymaking and agenda setting.",
        "DOI": "10.1111/psj.12485",
        "affiliation_name": "Department of Computer Science",
        "affiliation_city": "Ames",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "A Blockchain and ML-Based Framework for Fast and Cost-Effective Health Insurance Industry Operations",
        "paper_author": "Elhence A.",
        "publication": "IEEE Transactions on Computational Social Systems",
        "citied_by": "9",
        "cover_date": "2023-08-01",
        "Abstract": "Health insurance is crucial for each person, bearing in mind the increasing medical costs. COVID-19 has been an eye-opener as to how important it is to have health insurance. Medical emergencies can have a severe emotional and financial impact. Thus, a health insurance policy can help mitigate financial risks in unpredictable circumstances. However, the current insurance system is very expensive, as thousands of people pay the premiums, and very few take the claims. Furthermore, the claim settlement process is excruciatingly long and tiresome. In this article, we focus on establishing a rapid and cost-effective framework for the health insurance market, based on machine learning and blockchain technology. By developing a smart contract, blockchain may eliminate any third-party organizations and make the complete process safer, easier, and more efficient. The contract pays the claim based on the claimant's documentation. We optimized the premiums using a regression model based on the net amount claimed during the current policy tenure and various other criteria. For anticipating risk, a random forest classifier is used, which aids in the risk-rated premium rebate computation for policyholders for their next term of insurance.",
        "DOI": "10.1109/TCSS.2022.3219256",
        "affiliation_name": "Jaypee Institute of Information Technology",
        "affiliation_city": "Noida",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Assessment of China’s forest fire occurrence with deep learning, geographic information and multisource data",
        "paper_author": "Shao Y.",
        "publication": "Journal of Forestry Research",
        "citied_by": "32",
        "cover_date": "2023-08-01",
        "Abstract": "Considerable economic losses and ecological damage can be caused by forest fires, and compared to suppression, prevention is a much smarter strategy. Accordingly, this study focuses on developing a novel framework to assess forest fire risks and policy decisions on forest fire management in China. This framework integrated deep learning algorithms, geographic information, and multisource data. Compared to conventional approaches, our framework featured timesaving, easy implementation, and importantly, the use of deep learning that vividly integrates various factors from the environment and human activities. Information on 96,594 forest fire points from 2001 to 2019 was collected on Moderate Resolution Imaging Spectroradiometer (MODIS) fire hotspots from 2001 to 2019 from NASA's Fire Information Resource Management System. The information was classified into factors such as topography, climate, vegetation, and society. The prediction of forest fire risk was generated using a fully connected network model, and spatial autocorrelation used to analyze the spatial aggregation correlation of active fire hotspots in the whole area of China. The results show that high accuracy prediction of fire risks was achieved (accuracy 87.4%, positive predictive value 87.1%, sensitivity 88.9%, area under curve (AUC) 94.1%). Based on this, it was found that Chinese forest fire risk shows significant autocorrelation and agglomeration both in seasons and regions. For example, forest fire risk usually raises dramatically in spring and winter, and decreases in autumn and summer. Compared to the national average, Yunnan Province, Guangdong Province, and the Greater Hinggan Mountains region of Heilongjiang Province have higher fire risks. In contrast, a large region in central China has been recognized as having a long-term, low risk of forest fires. All forest risks in each region were recorded into the database and could contribute to the forest fire prevention. The successful assessment of forest fire risks in this study provides a comprehensive knowledge of fire risks in China over the last 20 years. Deep learning showed its advantage in integrating multiple factors in predicting forest fire risks. This technical framework is expected to be a feasible evaluation tool for the occurrence of forest fires in China.",
        "DOI": "10.1007/s11676-022-01559-1",
        "affiliation_name": "Hainan University",
        "affiliation_city": "Haikou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Reinforcement learning-based control to suppress the transient vibration of semi-active structures subjected to unknown harmonic excitation",
        "paper_author": "Pisarski D.",
        "publication": "Computer-Aided Civil and Infrastructure Engineering",
        "citied_by": "12",
        "cover_date": "2023-08-01",
        "Abstract": "The problem of adaptive semi-active control of transient structural vibration induced by unknown harmonic excitation is studied. The controller adaptation is attained by using a specially designed reinforcement learning algorithm that adjusts the parameters of a switching control policy to guarantee efficient dissipation of the structural energy. This algorithm relies on an efficient gradient-based sequence that accelerates the learning protocol and results in suboptimal control. The performance of this method is examined through numerical experiments for a span structure that is equipped with a semi-active device of controlled stiffness and damping parameters. The experiments cover a selection of control learning scenarios and comparisons to optimal open-loop and heuristic state-feedback control strategies. This study has confirmed that the developed method has high stabilizing performance, and the relatively low computational burden of the incorporated iterative learning algorithm facilitates its application to multi–degree-of-freedom structures.",
        "DOI": "10.1111/mice.12920",
        "affiliation_name": "Institute of Fundamental Technological Research of the Polish Academy of Sciences",
        "affiliation_city": "Warsaw",
        "affiliation_country": "Poland"
    },
    {
        "paper_title": "Automated Design of Metaheuristics Using Reinforcement Learning Within a Novel General Search Framework",
        "paper_author": "Yi W.",
        "publication": "IEEE Transactions on Evolutionary Computation",
        "citied_by": "25",
        "cover_date": "2023-08-01",
        "Abstract": "Metaheuristic algorithms have been investigated intensively to address highly complex combinatorial optimization problems. However, most metaheuristic algorithms have been designed manually by researchers of different expertise without a consistent framework. This article proposes a general search framework (GSF) to formulate in a unified way a range of different metaheuristics. With generic algorithmic components, including selection heuristics and evolution operators, the unified GSF aims to serve as the basis of analyzing algorithmic components for automated algorithm design. With the established new GSF, two reinforcement learning (RL)-based methods, deep Q-network based and proximal policy optimization-based methods, have been developed to automatically design a new general population-based algorithm. The proposed RL-based methods are able to intelligently select and combine appropriate algorithmic components during different stages of the optimization process. The effectiveness and generalization of the proposed RL-based methods are validated comprehensively across different benchmark instances of the capacitated vehicle routing problem with time windows. This study contributes to making a key step toward automated algorithm design with a general framework supporting fundamental analysis by effective machine learning.",
        "DOI": "10.1109/TEVC.2022.3197298",
        "affiliation_name": "Xidian University",
        "affiliation_city": "Xi'an",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Evaluation of regional industrial cluster innovation capability based on particle swarm clustering algorithm and multi-objective optimization",
        "paper_author": "Yan Y.",
        "publication": "Complex and Intelligent Systems",
        "citied_by": "7",
        "cover_date": "2023-08-01",
        "Abstract": "With the progress of the times and the development of science, industrial clusters have been regarded by all countries in the world as one of the important ways to enhance regional competitiveness, and become an inevitable trend of industrial development. The research on the innovation ability of industrial clusters can not only maintain sustainable development of industrial clusters and obtain sustained competitive advantages, but also provide reference for the government's policy formulation of industrial clusters. This paper aims to study the evaluation of regional industrial clusters' innovation capability based on particle swarm clustering and multi-objective optimization. This paper uses the theory of industrial cluster innovation and takes regional industrial system as the empirical research object to establish a regional industrial system capability evaluation system, which is based on the selection of indicators, combined with analytic hierarchy process and factor analysis to evaluate industrial innovation capability. On this basis, the particle swarm clustering theory is used to verify the innovation ability and evaluation index system of industrial clusters, and provide a reference for the evaluation of the innovation ability of industrial clusters. This paper divides the regional cluster innovation capability into four aspects: innovation input capability, environment support capability, self-development capability and innovation output capability, and systematically analyzes the key elements and in the composition of innovation elements and their relationships. It then constructs the evaluation index system of regional cluster innovation capability. At the same time, this paper introduces clustering analysis algorithm and swarm intelligence algorithm into regional innovation evaluation, combines particle swarm optimization algorithm and K-means clustering algorithm, and optimizes particle swarm clustering algorithm by adjusting adaptive parameters and adding fitness variance. The experimental results of this paper show that from the results of the tested innovation potential of the three industrial clusters, industrial cluster F has the strongest innovation ability, with an evaluation coefficient of 0.851, followed by industrial cluster F, which has a value of 0.623. This result is consistent with the actual innovation status of the selected industry. From this point of view, the established particle swarm clustering model for evaluating the innovation capability of regional industrial clusters is reliable and can be used to evaluate the innovation capability of different industrial clusters.",
        "DOI": "10.1007/s40747-021-00521-8",
        "affiliation_name": "Renmin University of China",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Applying machine learning to enhance the cache performance using reuse distance",
        "paper_author": "Jose J.",
        "publication": "Evolutionary Intelligence",
        "citied_by": "0",
        "cover_date": "2023-08-01",
        "Abstract": "The motivation for the search of efficient cache management policies is due to the drawback of the large number of CPU cycles taken to fetch a block from the main memory to the last level cache. Recently, Researchers are using most recent advancements in machine learning (ML) to optimise the performance of cache. They designed different models using ML techniques to improve the performance of cache. Unfortunately, the schemes suffer from several drawbacks like storage complexity incurred while storing the extra bits required for prediction, excess time required for prediction, increase in difficulty of reuse prediction when workloads are increased, increase in energy consumption during the prediction process and lack in achieving expected accuracy. In order to overcome these drawbacks, cache reuse prediction is analysed using classification problem, which is actual view of the learning problem. The vital parameters used are the Frequency of access and reuse distance, in which the latter is modelled as stack distance. As an innovative idea, reuse distance is calculated based on status of each block into account. In addition, numerical nature of the dataset used in this article, helps in exploration of the ensemble-based ML, which is less frequently experimented into the context of cache memories rather than the deep learning approach. By applying suitably placed models, the memory references with high reuse capabilities are fetched into the cache. They are predicted with the aid of classification and cross-validation on the feature sets, which improve the accuracy and sensitivity by 6.2% and 5.46% respectively, when compared to the existing works.",
        "DOI": "10.1007/s12065-022-00730-1",
        "affiliation_name": "National Institute of Technology Tiruchirappalli",
        "affiliation_city": "Tiruchirappalli",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Personalized federated recommendation system with historical parameter clustering",
        "paper_author": "Jie Z.",
        "publication": "Journal of Ambient Intelligence and Humanized Computing",
        "citied_by": "18",
        "cover_date": "2023-08-01",
        "Abstract": "As an information filtering tool, recommendation system can present interesting contents to specific users through utilizing community users’ information. Due to the increasingly strict collection of user privacy information and improvement of related policies, data are scattered in different organizations as data islands, making it difficult to train a reliable recommendation system model. As for federated learning, an emerging machine learning approach, it enables clients to co-train the model by uploading gradients, which avoids the server to collect sensitive data from clients. To address the problem of not independent and identically distribution in federated learning, we propose a federated recommendation system based on the clustering of historical parameters. The clients perform a weighted average of the historical learning parameters with the global parameters sent by the server through using the time decay factor. The server performs parameter aggregation and clustering on the received parameters. The system performs iterative training based on the users’ historical learning parameters. In addition, when it comes to the problem that the server lacks raw data and cannot provide personalized recommendations for users in the federated recommendation system, we propose a recommendation system model based on user embedding features. The server can use user embedding features for personalized recommendations and it cannot get users’ data through user embedding features. The clients use original data for the local personalized recommendations. We conduct experiments on the real dataset MovieLens-1M. The experimental results show that the proposed federated learning approach is better than the traditional federated learning approach.",
        "DOI": "10.1007/s12652-022-03709-z",
        "affiliation_name": "University of Lahore",
        "affiliation_city": "Lahore",
        "affiliation_country": "Pakistan"
    },
    {
        "paper_title": "Exploration with Task Information for Meta Reinforcement Learning",
        "paper_author": "Jiang P.",
        "publication": "IEEE Transactions on Neural Networks and Learning Systems",
        "citied_by": "4",
        "cover_date": "2023-08-01",
        "Abstract": "Meta reinforcement learning (meta-RL) is a promising technique for fast task adaptation by leveraging prior knowledge from previous tasks. Recently, context-based meta-RL has been proposed to improve data efficiency by applying a principled framework, dividing the learning procedure into task inference and task execution. However, the task information is not adequately leveraged in this approach, thus leading to inefficient exploration. To address this problem, we propose a novel context-based meta-RL framework with an improved exploration mechanism. For the existing exploration and execution problem in context-based meta-RL, we propose a novel objective that employs two exploration terms to encourage better exploration in action and task embedding space, respectively. The first term pushes for improving the diversity of task inference, while the second term, named action information, works as sharing or hiding task information in different exploration stages. We divide the meta-training procedure into task-independent exploration and task-relevant exploration stages according to the utilization of action information. By decoupling task inference and task execution and proposing the respective optimization objectives in the two exploration stages, we can efficiently learn policy and task inference networks. We compare our algorithm with several popular meta-RL methods on MuJoco benchmarks with both dense and sparse reward settings. The empirical results show that our method significantly outperforms baselines on the benchmarks in terms of sample efficiency and task performance.",
        "DOI": "10.1109/TNNLS.2021.3121432",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Deep Deterministic Policy Gradient with Compatible Critic Network",
        "paper_author": "Wang D.",
        "publication": "IEEE Transactions on Neural Networks and Learning Systems",
        "citied_by": "26",
        "cover_date": "2023-08-01",
        "Abstract": "Deep deterministic policy gradient (DDPG) is a powerful reinforcement learning algorithm for large-scale continuous controls. DDPG runs the back-propagation from the state-action value function to the actor network's parameters directly, which raises a big challenge for the compatibility of the critic network. This compatibility emphasizes that the policy evaluation is compatible with the policy improvement. As proved in deterministic policy gradient, the compatible function guarantees the convergence ability but restricts the form of the critic network tightly. The complexities and limitations of the compatible function impede its development in DDPG. This article introduces neural networks' similarity indices with gradients to measure the compatibility concretely. Represented as kernel matrices, we consider the actor network's and the critic network's training dataset, trained parameters, and gradients. With the sketching trick, the calculation time of the similarity index decreases hugely. The centered kernel alignment index and the normalized Bures similarity index provide us with consistent compatibility scores empirically. Moreover, we demonstrate the necessity of the compatible critic network in DDPG from three aspects: 1) analyzing the policy improvement/evaluation steps; 2) conducting the theoretic analysis; and 3) showing the experimental results. Following our research, we remodel the compatible function with an energy function model, enabling it suitable to the sizeable state-action space problem. The critic network has higher compatibility scores and better performance by introducing the policy change information into the critic-network optimization process. Besides, based on our experiment observations, we propose a light-computation overestimation solution. To prove our algorithm's performance and validate the compatibility of the critic network, we compare our algorithm with six state-of-the-art algorithms using seven PyBullet robotics environments.",
        "DOI": "10.1109/TNNLS.2021.3117790",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Chicago",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "SSD: A Real Time CNN-Based Face Mask Detection System Using Single Shot Bounding Boxes Detector and Mobile Net V2",
        "paper_author": "Choudhary P.",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2023-07-27",
        "Abstract": "COVID-19 has continued to be the cause of dilemma for billions or millions of lives even in 2021. A wave of emotion among all individuals plans to get a normal state of life. Therefore, wearing a mask is so important for reducing the flow of viral transmission to provide a way of protection. Though an implementation of this policy is not much achievable manually. By grasping technology key, our propose system is capable of extracting faces who wear mask or who wear not. This system helps to promote face mask utilization and ensure to give secure and safe environment. The paper presents all the tasks were need very systematic approach, starting from the collection of the data to the implementation of the solution and till evaluation of the System. All these tasks have been completed successfully and results are mentioned according to expectations. The most challenging task was the domain knowledge, to understand the language. It is one of the major areas and really need very fundamental and conceptual knowledge of Machine Learning, Deep learning and Python.",
        "DOI": "10.1063/5.0154120",
        "affiliation_name": "GLA University, Mathura",
        "affiliation_city": "Mathura",
        "affiliation_country": "India"
    },
    {
        "paper_title": "A Hybrid Data-Knowledge Driven Method for Technology Development Risk Evaluation",
        "paper_author": "Zhao X.",
        "publication": "Advances in Transdisciplinary Engineering",
        "citied_by": "0",
        "cover_date": "2023-07-27",
        "Abstract": "As technology competition between countries and companies becomes increasingly intense, various commercial and policy means are being used to hinder the technology development of competitors. The risk of technology development is thus much higher than before and must be accurately assessed so that timely preventive measures can be taken to ensure the safety of technology development. In this paper, a hybrid data-knowledge driven method is proposed for constructing a technology risk evaluation indicator system that can accurately identify the high-risk technologies. Expert knowledge is effectively leveraged through a data-driven model developed from a novel double-layer bagging machine learning method, which is able to learn the indicator aggregation rules in the indicator system automatically. A case study on the risk assessment of smart chip-related technologies for China is provided. As a result, the high-risk areas such as 'field programmable gate array' and 'central processing unit' are accurately identified, manifesting the effectiveness and accuracy of the proposed method.",
        "DOI": "10.3233/ATDE230048",
        "affiliation_name": "Beijing Institute of Technology",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Reinforcement Learning for Quality-Oriented Production Process Parameter Optimization Based on Predictive Models",
        "paper_author": "Paranjape A.",
        "publication": "Advances in Transdisciplinary Engineering",
        "citied_by": "0",
        "cover_date": "2023-07-27",
        "Abstract": "Production of low-quality or faulty products is costly for manufacturing companies since it wastes a lot of resources, human effort, and time. Avoiding such waste requires the correct set of process control parameters, which depends on the dynamic situation in the production processes. Research so far mainly focused on optimizing specific processes using traditional optimization algorithms, mainly evolutionary algorithms. To develop a framework that enables real-time optimization based on a predictive model for an arbitrary production process, this paper explores the application of reinforcement learning (RL) in the field of process parameter optimization. Inspired by the literature review on both, production process parameter optimization, and RL, a model based on maximum a posteriori policy optimization that can handle both numerical and categorical parameters is proposed. A validation study conducted on data sets from production fields compares the trained model to state-of-the-art traditional optimization algorithms and shows that RL can find optima of similar quality while requiring significantly less time.",
        "DOI": "10.3233/ATDE230059",
        "affiliation_name": "Rheinisch-Westfälische Technische Hochschule Aachen",
        "affiliation_city": "Aachen",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Impact of Clean Air Policy on Criteria Air Pollutants and Health Risks Across China During 2013–2021",
        "paper_author": "Li R.",
        "publication": "Journal of Geophysical Research: Atmospheres",
        "citied_by": "8",
        "cover_date": "2023-07-27",
        "Abstract": "The impact of clean air policy enacted in China since 2013 on air quality and human health remained poorly understood. For the first time, we developed a full-coverage air quality data set including six criteria air pollutants (PM2.5, PM10, NO2, SO2, CO, and 8-hr O3) using a three-stage model. Then, the LightGBM model was applied to further quantify the respective contributions of emission and meteorology to air quality. The results suggested that the deweathered concentrations of PM2.5, PM10, NO2, SO2, and CO in China decreased by 46.2%, 43.1%, 24.7%, 55.5%, and 37.2%, respectively. However, the 8-hr O3 concentration in China increased by 19.4%. The variation trends of deweathered PM2.5 and NO2 were much weaker than the absolute concentrations of these species, indicating more strict emission control measures should be imposed in the future. Based on the comparison of the effectiveness during 2013–2017 and 2018–2021, we found that nearly all of the regions suffered from immense difficulty in reducing PM2.5 emissions in recent years. However, the CO emission control has become more efficient since 2018. The increasing rate of deweathered 8-hr O3 concentration was more dramatic during 2018–2021 compared with 2013–2017, which was in contrast to the absolute 8-hr O3 concentration. It suggested that the rapid increase of ambient O3 level might be masked by favorable meteorological conditions since 2018. Although the large reduction of PM2.5 and NO2 saved more health benefits (625,362 cases), the increase of O3 exposure aggravated the health costs (20,419 cases) across China. Therefore, collaborative control of PM2.5, NO2, and O3 pollution are still imperative in the future work.",
        "DOI": "10.1029/2023JD038939",
        "affiliation_name": "Key Laboratory of Geographic Information Science, Ministry of Education",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Synthesizing Physical Character-Scene Interactions",
        "paper_author": "Hassan M.",
        "publication": "Proceedings - SIGGRAPH 2023 Conference Papers",
        "citied_by": "27",
        "cover_date": "2023-07-23",
        "Abstract": "Movement is how people interact with and affect their environment. For realistic character animation, it is necessary to synthesize such interactions between virtual characters and their surroundings. Despite recent progress in character animation using machine learning, most systems focus on controlling an agent's movements in fairly simple and homogeneous environments, with limited interactions with other objects. Furthermore, many previous approaches that synthesize human-scene interactions require significant manual labeling of the training data. In contrast, we present a system that uses adversarial imitation learning and reinforcement learning to train physically-simulated characters that perform scene interaction tasks in a natural and life-like manner. Our method learns scene interaction behaviors from large unstructured motion datasets, without manual annotation of the motion data. These scene interactions are learned using an adversarial discriminator that evaluates the realism of a motion within the context of a scene. The key novelty involves conditioning both the discriminator and the policy networks on scene context. We demonstrate the effectiveness of our approach through three challenging scene interaction tasks: carrying, sitting, and lying down, which require coordination of a character's movements in relation to objects in the environment. Our policies learn to seamlessly transition between different behaviors like idling, walking, and sitting. By randomizing the properties of the objects and their placements during training, our method is able to generalize beyond the objects and scenarios depicted in the training dataset, producing natural character-scene interactions for a wide variety of object shapes and placements. The approach takes physics-based character motion generation a step closer to broad applicability. Please see our supplementary video for more results.",
        "DOI": "10.1145/3588432.3591525",
        "affiliation_name": "Electronic Arts Inc",
        "affiliation_city": "Redwood City",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Autodidactic and coachable neural architectures",
        "paper_author": "Michael L.",
        "publication": "Frontiers in Artificial Intelligence and Applications",
        "citied_by": "0",
        "cover_date": "2023-07-21",
        "Abstract": "The prediction made by a learned model is rarely the end outcome of interest to a given agent. In most real-life scenarios, a certain policy is applied on the model's prediction and on some relevant context to reach a decision. It is the (possibly temporally distant) effects of this decision that bring value to the agent. Moreover, it is those effects, and not the model's prediction, that need to be evaluated as far as the agent's satisfaction is concerned. The formalization of such scenarios naturally raises certain questions: How should a learned model be integrated with a policy to reach decisions? How should the learned model be trained and evaluated in the presence of such a policy? How is the training affected in terms of the type of access that one has on the policy? How can the policy be represented and updated in a way that is cognitively compatible with a human, so that it offers an explainable layer of reasoning on top of the learned model? This chapter offers a high-level overview of past work on the integration of modular reasoning with autodidactic learning and with user-driven coaching, as it applies on neural-symbolic architectures that combine sequentially a neural module with an arbitrary symbolically represented (and possibly non-differentiable) policy. In this context, the chapter offers responses to the questions above when the policy can be reasoned with only in a deductive manner, or in a deductive and an abductive manner. It further discusses how the policy can be learned / updated in an elaboration-Tolerant and cognitively-light manner through machine coaching, and highlights the connections of the dialectical coaching process with the central role that argumentation plays in human reasoning.",
        "DOI": "10.3233/FAIA230143",
        "affiliation_name": "Open University of Cyprus",
        "affiliation_city": "Nicosia",
        "affiliation_country": "Cyprus"
    },
    {
        "paper_title": "Predicting party switching through machine learning and open data",
        "paper_author": "Meneghetti N.",
        "publication": "iScience",
        "citied_by": "1",
        "cover_date": "2023-07-21",
        "Abstract": "Parliament dynamics might seem erratic at times. Predicting future voting patterns could support policy design based on the simulation of voting scenarios. The availability of open data on legislative activities and machine learning tools might enable such prediction. In our paper, we provide evidence for this statement by developing an algorithm able to predict party switching in the Italian Parliament with over 70% accuracy up to two months in advance. The analysis was based on voting data from the XVII (2013–2018) and XVIII (2018–2022) Italian legislature. We found party switchers exhibited higher participation in secret ballots and showed a progressive decrease in coherence with their party's majority votes up to two months before the actual switch. These results show how machine learning combined with political open data can support predicting and understanding political dynamics.",
        "DOI": "10.1016/j.isci.2023.107098",
        "affiliation_name": "Università degli Studi della Tuscia Viterbo",
        "affiliation_city": "Viterbo",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Recent Advances in Geological Storage: Trapping Mechanisms, Storage Sites, Projects, and Application of Machine Learning",
        "paper_author": "Li N.",
        "publication": "Energy and Fuels",
        "citied_by": "21",
        "cover_date": "2023-07-20",
        "Abstract": "A significant amount of carbon dioxide (CO2) is being released into the atmosphere as a result of the acceleration of industrialization and increased energy consumption, which are causing a rise in world temperatures. Despite the fact that nations all over the world have actively participated in CO2 storage projects, there is still not enough to moderate global temperatures. Therefore, there is an urgent need for the development of CO2 sequestration technologies. The main geological storage trapping mechanisms are discussed in this work along with an analysis of the major influencing variables. Additionally, the benefits and drawbacks of significant storage locations and current research hotspots are explored. Storage project development across the globe is outlined. The use of machine learning for CO2 sequestration is reviewed toward the end. This extensive review reveals that the main variables affecting CO2 capture capacity are hydrodynamic and geochemical. The sequestration capability of various storage sites is significantly influenced by the reservoir characteristics and implementation methodologies. The successful implementation of the storage project is determined by local policies and public support. The development of machine learning technologies makes storage projects safer and more dependable.",
        "DOI": "10.1021/acs.energyfuels.3c01433",
        "affiliation_name": "Southwest Petroleum University China",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Activity and Selectivity Roadmap for C-N Electro-Coupling on MXenes",
        "paper_author": "Jiao Y.",
        "publication": "Journal of the American Chemical Society",
        "citied_by": "47",
        "cover_date": "2023-07-19",
        "Abstract": "Electrochemical coupling between carbon and nitrogen species to generate high-value C-N products, including urea, presents significant economic and environmental potentials for addressing the energy crisis. However, this electrocatalysis process still suffers from limited mechanism understanding due to the complex reaction networks, which restricts the development of electrocatalysts beyond trial-and-error practices. In this work, we aim to improve the understanding of the C-N coupling mechanism. This goal was achieved by constructing the activity and selectivity landscape on 54 MXene surfaces by density functional theory (DFT) calculations. Our results show that the activity of the C-N coupling step is largely determined by the *CO adsorption strength (Ead-CO), while the selectivity relies more on the co-adsorption strength of *N and *CO (Ead-CO and Ead-N). Based on these findings, we propose that an ideal C-N coupling MXene catalyst should satisfy moderate *CO and stable *N adsorption. Through the machine learning-based approach, data-driven formulas for describing the relationship between Ead-CO and Ead-N with atomic physical chemistry features were further identified. Based on the identified formula, 162 MXene materials were screened without time-consuming DFT calculations. Several potential catalysts were predicted with good C-N coupling performance, such as Ta2W2C3. The candidate was then verified by DFT calculations. This study has incorporated machine learning methods for the first time to provide an efficient high-throughput screening method for selective C-N coupling electrocatalysts, which could be extended to a wider range of electrocatalytic reactions to facilitate green chemical production.",
        "DOI": "10.1021/jacs.3c05171",
        "affiliation_name": "The University of Adelaide",
        "affiliation_city": "Adelaide",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Multi-Objective Optimization of Vehicle-Following Control for Connected Electric Vehicles Based on Deep Deterministic Policy Gradient",
        "paper_author": "Zhang Y.",
        "publication": "SAE International Journal of Electrified Vehicles",
        "citied_by": "0",
        "cover_date": "2023-07-17",
        "Abstract": "Eco-driving plays an increasingly important role in intelligent transportation systems, where the vehicle-following economy and safety are receiving increasing attention in recent years. In this context, this article proposes a novel deep deterministic policy gradient (DDPG)-based driving control strategy for connected electric vehicles (CEVs) under vehicle-following scenarios. Three original contributions make this article distinctive from existing studies. First, a multi-objective optimization problem including driving safety, passenger comfort, and the driving economy for the following vehicle is established, in which the battery capacity degradation cost is first considered in the vehicle-following problem. Second, a DDPG-based driving control strategy is proposed where a penalty is introduced into the multi-objective optimization reward function to accelerate the convergence process. Third, the coupling relationship of the three objectives is carefully studied. Different weighting factors are tested and analyzed to balance the three objectives. Detailed discussion and comparison under different driving cycles validate the superiority of the proposed method, e.g., a 16-31% reduction of battery capacity degradation cost with better safety and comfort, compared with existing vehicle-following strategies. This work makes a potential contribution to the artificial intelligence application of intelligent transportation systems.",
        "DOI": "10.4271/14-13-01-0005",
        "affiliation_name": "Central South University",
        "affiliation_city": "Changsha",
        "affiliation_country": "China"
    },
    {
        "paper_title": "CRACS: Compaction of Rules in Anticipatory Classifier Systems",
        "paper_author": "Orhand R.",
        "publication": "GECCO 2023 Companion - Proceedings of the 2023 Genetic and Evolutionary Computation Conference Companion",
        "citied_by": "1",
        "cover_date": "2023-07-15",
        "Abstract": "Rule Compaction of populations of Learning Classifier Systems (LCS) has always been a topic of interest to get more insights into the discovered underlying patterns from the data or to remove useless classifiers from the populations. However, these techniques have neither been used nor adapted to Anticipatory Learning Classifier Systems (ALCS). ALCS differ from other LCS in that they build models of their environments from which decision policies to solve their learning tasks are learned. We thus propose CRACS (Compaction of Rules in Anticipatory Classifier Systems), a compaction algorithm for ALCS that aims to reduce the size of their environmental models without impairing these models or the ability of these systems to solve their tasks. CRACS relies on filters applied to classifiers and subsumption principles. The capabilities of our compaction algorithm have been studied with three different ALCS on a thorough benchmark of 23 mazes of various levels of environmental uncertainty. The results show that CRACS reduces the size of populations of classifiers while the learned models of environments and the ability of ALCS to solve their tasks are preserved.",
        "DOI": "10.1145/3583133.3596352",
        "affiliation_name": "EPITA",
        "affiliation_city": "Le Kremlin-Bicetre",
        "affiliation_country": "France"
    },
    {
        "paper_title": "Neuroevolutionary Compiler Control for Code Optimization",
        "paper_author": "Heckel K.",
        "publication": "GECCO 2023 Companion - Proceedings of the 2023 Genetic and Evolutionary Computation Conference Companion",
        "citied_by": "0",
        "cover_date": "2023-07-15",
        "Abstract": "The optimization performed by compilers when generating executable programs is critical for software performance yet tuning this process to maximize efficiency is difficult due to the large number of possible modifications and the almost limitless number of potential input programs. To promote the application of artificial intelligence and machine learning to this challenge, Facebook Research released Compiler Gym[1], a reinforcement learning environment to allow the training of agents to perform compiler optimization control on real C/C++ programs. Whereas previously published approaches use techniques such as Proximal Policy Optimization or Deep Q Networks, this work utilizes neuroevolution and achieves competitive performance on the cBench-v1[2] program set while demonstrating the highly adaptive properties of the neuroevolution approach.",
        "DOI": "10.1145/3583133.3596380",
        "affiliation_name": "University of Sussex",
        "affiliation_city": "Brighton",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Quantum Enhancements for AlphaZero",
        "paper_author": "Chao J.",
        "publication": "GECCO 2023 Companion - Proceedings of the 2023 Genetic and Evolutionary Computation Conference Companion",
        "citied_by": "2",
        "cover_date": "2023-07-15",
        "Abstract": "Reinforcement learning algorithms including AlphaZero are powerful artificial intelligence (AI) algorithms, but are known to be resource intensive and unable to train within a reasonable budget. Speeding up learning would be valuable to further expand application areas for real world problems. In this paper, we investigate two quantum computing methods to enhance AlphaZero, with the goal of speeding up training. We evaluate the results by playing the board game Othello, an adversarial multi-agent turn based game similar to the game Go. First, parameterized quantum circuits (PQC) have been shown to train hybrid quantum-classical reinforcement learning agents in standard benchmark environments. With this inspiration, we replace the classical neural network with a PQC quantum neural network (QNN) in the AlphaZero architecture. Second, tensor-network quantum circuits have been used to extract important features for convolutional neural networks (CNNs) in image classification tasks. Using this as inspiration, we use a tree tensor network (TTN) to extract features from the Othello game board, generating a new set of feature vectors for a classical neural network to estimate the policy and value. Results show both novel methods converge to master the game and achieve the same level of play compared to the classical AlphaZero agent.",
        "DOI": "10.1145/3583133.3596302",
        "affiliation_name": "Naval Information Warfare Center Pacific",
        "affiliation_city": "San Diego",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Synthesizing Effective Diagnostic Models from Small Samples using Structural Machine Learning: a Case Study in Automating COVID-19 Diagnosis",
        "paper_author": "Kaszuba P.",
        "publication": "GECCO 2023 Companion - Proceedings of the 2023 Genetic and Evolutionary Computation Conference Companion",
        "citied_by": "0",
        "cover_date": "2023-07-15",
        "Abstract": "The global COVID-19 pandemic has demonstrated the urgent need for diagnostic tools that can be both readily applied and dynamically calibrated by non-specialists, in terms of a sensitivity/specificity tradeoff that complies with relevant healthcare policies and procedures. This article describes the design and deployment of a novel machine learning algorithm, Structural Machine Learning (SML), that combines memetic grammar-guided program synthesis with self-supervised learning in order to learn effectively from small data sets while remaining relatively resistant to overfitting. SML is used to construct a signal processing pipeline for audio time-series, which then serves as the diagnostic mechanism for a wide-spectrum, infrasound-to-ultrasound e-stethoscope. In blind trials supervised by a third party, SML is shown to be superior to Deep Learning approaches in terms of the area under the ROC curve, while allowing for transparent interpretation of the decision-making process.",
        "DOI": "10.1145/3583133.3590598",
        "affiliation_name": "University of York",
        "affiliation_city": "York",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Rethinking Population-Assisted Off-policy Reinforcement Learning",
        "paper_author": "Zheng B.",
        "publication": "GECCO 2023 - Proceedings of the 2023 Genetic and Evolutionary Computation Conference",
        "citied_by": "4",
        "cover_date": "2023-07-15",
        "Abstract": "While off-policy reinforcement learning (RL) algorithms are sample efficient due to gradient-based updates and data reuse in the replay buffer, they struggle with convergence to local optima due to limited exploration. On the other hand, population-based algorithms offer a natural exploration strategy, but their heuristic black-box operators are inefficient. Recent algorithms have integrated these two methods, connecting them through a shared replay buffer. However, the effect of using diverse data from population optimization iterations on off-policy RL algorithms has not been thoroughly investigated. In this paper, we first analyze the use of off-policy RL algorithms in combination with population-based algorithms, showing that the use of population data could introduce an overlooked error and harm performance. To test this, we propose a uniform and scalable training design and conduct experiments on our tailored framework in robot locomotion tasks from the OpenAI gym. Our results substantiate that using population data in off-policy RL can cause instability during training and even degrade performance. To remedy this issue, we further propose a double replay buffer design that provides more on-policy data and show its effectiveness through experiments. Our results offer practical insights for training these hybrid methods.",
        "DOI": "10.1145/3583131.3590512",
        "affiliation_name": "Southern University of Science and Technology",
        "affiliation_city": "Shenzhen",
        "affiliation_country": "China"
    },
    {
        "paper_title": "STRUCTURE TRANSFORMATION OF INFORMATION ORGANIZATION SUPPORT OF INNOVATIVE PROCESSES AT HIGH-TECH ENTERPRISES ON THE BASIS OF ADAPTATION OF MULTILEVEL METHODS OF SIMULATION MODELING IN THE ENVIRONMENT OF THE DIGITAL ECONOMY",
        "paper_author": "Novikov S.",
        "publication": "Journal of Theoretical and Applied Information Technology",
        "citied_by": "0",
        "cover_date": "2023-07-15",
        "Abstract": "The process of innovative development of knowledge-intensive industries should be implemented within a triangle: government, business and education/science, using various end-to-end innovation and digital solutions and project approach necessary to effectively organize the management process. The aim of the research conducted in the article is the qualitative development of methodological tools for the mechanisms of information support of various process groups in the field of innovation, through the use of actual methods of simulation modeling, which include components of machine learning, which as a result will improve the effectiveness of innovative project groups in high-tech enterprises. The scientific hypothesis of the study is the assumption that the effectiveness of various groups of processes associated with information support of high-tech enterprises in the field of innovation, can act as a qualitative basis for a consistent increase in the level of their competitiveness and reliability in the long-term strategic perspective. Scientific novelty consists in theoretical substantiation and development of methodological tools for competent and effective information support of practical activity of the enterprise, within the framework of its innovation policy, with the purpose of subsequent increase in groups of indicators characterizing economic efficiency of innovation and digital projects on the basis of application of simulation modeling methods, including components of machine learning. Theoretical and methodological basis of the study is based on scientific works devoted to the consideration of problems in the field of organization of information support in the field of innovation, scientific developments of profile experts/specialists in the field of artificial intelligence, economic and mathematical modeling. The authors made a multicomponent conceptual model of the organization system of information support for the life cycle of products of innovative type. A dynamic information model has been developed, which will allow the management of high-tech enterprise making optimal management decisions considering the turbulent economic environment. The authors propose an updated mechanism of organizational and economic management of innovative process groups at high-tech enterprises.",
        "DOI": "NA",
        "affiliation_name": "Moscow Aviation Institute (National Research University)",
        "affiliation_city": "Moscow",
        "affiliation_country": "Russian Federation"
    },
    {
        "paper_title": "DRL-based target interception strategy design for an underactuated USV without obstacle collision",
        "paper_author": "Zhang C.",
        "publication": "Ocean Engineering",
        "citied_by": "5",
        "cover_date": "2023-07-15",
        "Abstract": "This paper proposes a guidance strategy and a controller based on end-to-end deep reinforcement learning (DRL) to address the problem of unmanned surface vehicles (USVs) interception and obstacle avoidance. A deep deterministic policy gradient (DDPG) algorithm is introduced to generate the interception strategy, a reward function with multiple objectives is designed. The artificial potential field (APF) method is used to obtain an evasion strategy for target USVs, enabling the target USVs to perform evasive actions in reaction to the interception. To find a trade-off between the time consumption and the safety of obstacle avoidance, a multi-objective equilibrium method is proposed. An incremental proportion regulator based on prior knowledge is used to dynamically modify the reward function. Besides, a virtual-reality 3D simulator based on ROS and Gazebo is constructed to present the process of interception. To demonstrate the effectiveness of the proposed method, simulation results are presented. Compared to the original DDPG algorithm, the proposed multi-objective equilibrium method shortens the interception path while ensuring the safety of obstacle avoidance. Compared to the traditional model-based approach, the DRL-based controller shows better performance in interception missions, and the robustness of the proposed controller for dynamic obstacles are shown in multiple tests.",
        "DOI": "10.1016/j.oceaneng.2023.114443",
        "affiliation_name": "Shanghai Jiao Tong University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Neural network inspired differential evolution based task scheduling for cloud infrastructure",
        "paper_author": "Gupta P.",
        "publication": "Alexandria Engineering Journal",
        "citied_by": "8",
        "cover_date": "2023-07-15",
        "Abstract": "In recent years, cloud computing has become an essential technology for businesses and individuals alike. Task scheduling is a critical aspect of cloud computing that affects the performance and efficiency of cloud infrastructure. During this pandemic where most of the healthcare services like COVID-19 sampling, vaccination process, patient management and other services are dependent on cloud infrastructure. These services come with huge clients and server load in a small instance of time. These task loads can only be managed at cloud infrastructure where an efficient resource management algorithm plays an important role. The optimal utilization of cloud infrastructure and optimization algorithms plays a vital role. The cloud resources rely on the allocation policy of the tasks on cloud resources. Simple static, dynamic, and meta-heuristic techniques provide a solution but not the optimal solution. In such a scenario machine learning and evolutionary algorithms are only the solution. In this work, a hybrid model based on meta-heuristic technique and neural network is proposed. The presented neural network inspired differential evolution hybrid technique provides an optimal assignment of the tasks on cloud infrastructure. The performance of the DE-ANN hybrid approach is performed using performance metrics, average start time(ms), average finish time(ms), average execution time(ms), total completion time(ms), simulation time(ms), and average resource utilization respectively. The proposed DE-ANN approach is validated against BB-BC, and Genetic approaches. It outperforms the existing meta-heuristic techniques i.e. Genetic approach, and Big-Bang Big-Crunch. The performance is evaluated using two configuration scenarios using 5 virtual machines and 10 virtual machines with varying tasks from 1000 to 4500. Experimental results show that the DE-ANN technique significantly improves task scheduling performance compared to other traditional techniques. The technique achieves an average improvement of 19.15% in total completion time(ms), 32.23% in average finish time(ms), 51.95% in average execution time(ms), and 33.24% in average resource utilization respectively. The DE-ANN technique is also effective in handling dynamic and uncertain environments, making it suitable for real-world cloud infrastructures.",
        "DOI": "10.1016/j.aej.2023.04.032",
        "affiliation_name": "DIT University",
        "affiliation_city": "Dehradun",
        "affiliation_country": "India"
    },
    {
        "paper_title": "An apprenticeship-reinforcement learning scheme based on expert demonstrations for energy management strategy of hybrid electric vehicles",
        "paper_author": "Hu D.",
        "publication": "Applied Energy",
        "citied_by": "15",
        "cover_date": "2023-07-15",
        "Abstract": "Deep reinforcement learning (DRL) is a potential solution to develop efficient energy management strategies (EMS) for hybrid electric vehicles (HEV) that can adapt to the changing topology of electrified powertrains and the uncertainty of various driving scenarios. However, traditional DRL has many disadvantages, such as low efficiency and poor stability. This study proposes an apprenticeship-reinforcement learning (A-RL) framework based on expert demonstration (ED) model embedding to improve DRL. First, the demonstration data, calculated by dynamic programming (DP), were collected, and domain adaptive meta-learning (DAML) was used to train the ED model with the adaptive capability of working conditions. Then combined apprenticeship learning (AL) with DRL, and the ED model was used to guide the DRL to output action. The method was validated on three HEV models, and the results show that the training convergence rate increases significantly under the framework. The average increase that the apprenticeship-deep deterministic policy gradient (A-DDPG) based method applied to three HEVs achieved was 34.9 %. Apprenticeship-twin delayed twin delayed deep deterministic policy gradient (A-TD3) achieved 23 % acceleration in the power-split HEV. Because A-DDPG's EMS is more forward-looking and can mimic ED to some extent, the frequency of engine operation in the high-efficiency range has increased. Therefore, A-DDPG can improve the fuel economy of the series hybrid electric bus (HEB) by 0.2–2.7 %, and improvements averaged to about 9.6 % in the series–parallel HEV while maintaining the final SOC. This study aims to improve the sampling efficiency and optimal performance of EMS-based DRL and provide a basis for the design and development of vehicle energy saving and emission reduction.",
        "DOI": "10.1016/j.apenergy.2023.121227",
        "affiliation_name": "Tianjin University",
        "affiliation_city": "Tianjin",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Multi-Agent attention-based deep reinforcement learning for demand response in grid-responsive buildings",
        "paper_author": "Xie J.",
        "publication": "Applied Energy",
        "citied_by": "41",
        "cover_date": "2023-07-15",
        "Abstract": "Integrating renewable energy resources and deploying energy management devices offer great opportunities to develop autonomous energy management systems in grid-responsive buildings. Demand response can promote enhancing demand flexibility and energy efficiency while reducing consumer costs. In this work, we propose a novel multi-agent deep reinforcement learning (MADRL) based approach with an agent assigned to individual buildings to facilitate demand response programs with diverse loads, including space heating/cooling and electrical equipment. Achieving real-time autonomous demand response in networks of buildings is challenging due to uncertain system parameters, the dynamic market price, and complex coupled operational constraints. To develop a scalable approach for automated demand response in networks of interconnected buildings, coordination between buildings is necessary to ensure demand flexibility and the grid's stability. We propose a MADRL technique that utilizes an actor-critic algorithm incorporating shared attention mechanism to enable effective and scalable real-time coordinated demand response in grid-responsive buildings. The presented case studies demonstrate the ability of the proposed approach to obtain decentralized cooperative policies for electricity costs minimization and efficient load shaping without knowledge of building energy systems. The viability of the proposed control approach is also demonstrated by a reduction of over 6% net load demand compared to standard reinforcement learning approaches, deep deterministic policy gradient, and soft actor-critic algorithm, as well as a tailored MADRL approach for demand response.",
        "DOI": "10.1016/j.apenergy.2023.121162",
        "affiliation_name": "Cornell Ann S. Bowers College of Computing and Information Science",
        "affiliation_city": "Ithaca",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "A comprehensive technical, economic, and environmental evaluation for optimal planning of renewable energy resources to supply water desalination units: Kuwait case study",
        "paper_author": "AlHajri I.",
        "publication": "Energy",
        "citied_by": "6",
        "cover_date": "2023-07-15",
        "Abstract": "Over the past decades, the penetration of renewable energies has been increasing in many countries. Due to the natural intermittency of renewable energy sources, finding the optimal capacity and location of these energy resources is the most important issue in smart sustainable cities. In this regard, a mathematical strategy is proposed in this paper to find the optimal capacity and location of wind turbines and photovoltaic panels to minimize the total operation cost of a water and energy nexus. In addition, to increase the system efficiency and maximize the benefit of renewable energy sources, water desalination units have been implemented in the proposed methodology, and both electric and water networks are optimized together. In order to present a comprehensive model, the technical, environmental, and economic aspects of the problem are taken into account as a mixed-integer linear programming, where, a machine learning approach based on long-short term memory networks has been utilized for uncertainty modeling of the stochastic parameters including wind and solar generation power, electricity load demand, water demand, and electricity price. Finally, to evaluate the efficiency of the proposed method three different scenarios on a water and energy nexus have been studied considering Kuwait data. The numerical results show that by optimal planning of renewable energy sources, the total cost has decreased about $11950 that is verified the effectiveness of the proposed method.",
        "DOI": "10.1016/j.energy.2023.127416",
        "affiliation_name": "University of Bonab",
        "affiliation_city": "Bonab",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "Re-visiting resource curse hypothesis in China through the lens of human capital and globalization",
        "paper_author": "Ozcan B.",
        "publication": "Journal of Environmental Management",
        "citied_by": "19",
        "cover_date": "2023-07-15",
        "Abstract": "The resource curse hypothesis has recently become an important research topic in environmental economics. However, there still needs to be consensus in the literature on whether natural resource rents (NRRs) support economic growth. Previous studies on China have mainly analyzed the resource curse hypothesis based on local or regional data. However, this study examines the issue based on national-level data using globalization and human capital as control variables. The dynamic Auto-Regressive Distributive Lag (DARDL) Simulations and the Kernel-based Regularized Least Squares (KRLS) techniques are employed for policy formulation for 1980–2019. The empirical assessments indicate that NRRs escalate economic growth, i.e., China's resource curse hypothesis is invalid. Further, empirical outcomes reveal that human capital and globalization encourage China's economic growth. The KRLS, a machine learning algorithm, also supports the findings of the DARDL approach. Finally, based on the empirical outcomes, several policy recommendations can be developed, such as more investment in the education sector and the use of NRRs for productive sectors of the economy.",
        "DOI": "10.1016/j.jenvman.2023.117685",
        "affiliation_name": "Guangdong University of Foreign Studies",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Data-driven agent-based modelling of incentives for carbon sequestration: The case of sown biodiverse pastures in Portugal",
        "paper_author": "Ravaioli G.",
        "publication": "Journal of Environmental Management",
        "citied_by": "5",
        "cover_date": "2023-07-15",
        "Abstract": "Sown biodiverse permanent pastures rich in legumes (SBP) offset animal farming emissions due to their potential to sequester carbon. From 2009 to 2014 Portugal implemented a programme that provided payments to incentivize the adoption of SBP. However, no proper evaluation of its outcome was conducted. To address this gap, we develop an agent-based model (ABM) at the municipality level to study the adoption of SBP in Portugal and assess the outcome of the programme. We applied the first pure data-driven approach in agricultural land-use ABM, which relies on machine learning algorithms to define the agents’ behavioural rules and capture their interaction with biophysical conditions. The ABM confirms that the program effectively expanded the adoption of SBP. However, our estimates indicate that the adoption rate in the absence of payments would have been higher than originally predicted. Furthermore, the existence of the program decreased the adoption rate after its conclusion. These findings underscore the importance of using reliable models and considering residual effects to properly design land use policies. The ABM developed in this study provides a basis for future research aimed at supporting the development of new policies to further promote the adoption of SBP.",
        "DOI": "10.1016/j.jenvman.2023.117834",
        "affiliation_name": "Instituto Superior Técnico",
        "affiliation_city": "Lisbon",
        "affiliation_country": "Portugal"
    },
    {
        "paper_title": "The impact of climate change and sustainability standards on the insurance market",
        "paper_author": "Sood K.",
        "publication": "The Impact of Climate Change and Sustainability Standards on the Insurance Market",
        "citied_by": "6",
        "cover_date": "2023-07-14",
        "Abstract": "This book delves into the physical and logical impacts, both direct and indirect, on the insurance industry. New tech, such as big data, artificial intelligence, and machine learning, in the growth of sustainable economics with foreign direct investments (FDIs), trustworthiness and ethics are discussed. Related use cases of data science for claim processing, fraud detection and prevention, policy administration, pricing and underwriting are discussed along with cyber security issues, data protection, and big data regulatory reforms. In order to promote ESG sustainability, the insurance industry plays a critical and significant role. Climate-related risks are being factored into underwriting and investing strategies. Through their own operations and business activities, insurers may promote the ESG agenda and move towards sustainability. Promoting diversity and inclusion, lowering greenhouse gas (GHG) emissions, resolving gender inequality, and helping communities through charitable work will all improve the company's brand, reputation, and true ESG credentials. The book is structured in such a way as to provide an overall understanding of the current practices, trends, and future technologies in the insurance market.",
        "DOI": "10.1002/9781394167944",
        "affiliation_name": "Chitkara University, Punjab",
        "affiliation_city": "Rajpura",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Experience sampling methods for the personalised prediction of mental health problems in Spanish university students: protocol for a survey-based observational study within the PROMES-U project",
        "paper_author": "Portillo-Van Diest A.",
        "publication": "BMJ Open",
        "citied_by": "2",
        "cover_date": "2023-07-14",
        "Abstract": "Introduction There is a high prevalence of mental health problems among university students. Better prediction and treatment access for this population is needed. In recent years, short-term dynamic factors, which can be assessed using experience sampling methods (ESM), have presented promising results for predicting mental health problems. Methods and analysis Undergraduate students from five public universities in Spain are recruited to participate in two web-based surveys (at baseline and at 12-month follow-up). A subgroup of baseline participants is recruited through quota sampling to participate in a 15-day ESM study. The baseline survey collects information regarding distal risk factors, while the ESM study collects short-term dynamic factors such as affect, company or environment. Risk factors will be identified at an individual and population level using logistic regressions and population attributable risk proportions, respectively. Machine learning techniques will be used to develop predictive models for mental health problems. Dynamic structural equation modelling and multilevel mixed-effects models will be considered to develop a series of explanatory models for the occurrence of mental health problems. Ethics and dissemination The project complies with national and international regulations, including the Declaration of Helsinki and the Code of Ethics, and has been approved by the IRB Parc de Salut Mar (2020/9198/I) and corresponding IRBs of all participating universities. All respondents are given information regarding access mental health services within their university and region. Individuals with positive responses on suicide items receive a specific alert with indications for consulting with a health professional. Participants are asked to provide informed consent separately for the web-based surveys and for the ESM study. Dissemination of results will include peer-reviewed scientific articles and participation in scientific congresses, reports with recommendations for universities' mental health policy makers, as well as a well-balanced communication strategy to the general public. Study registration osf.io/p7csq.",
        "DOI": "10.1136/bmjopen-2023-072641",
        "affiliation_name": "Fundació Institut d'Investigació Sanitaria Illes Balears",
        "affiliation_city": "Palma",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "Health-related cues on the packages of processed and ultra-processed products: Prevalence and policy implications",
        "paper_author": "Alcaire F.",
        "publication": "British Journal of Nutrition",
        "citied_by": "7",
        "cover_date": "2023-07-14",
        "Abstract": "The information included on food packages has a crucial role in influencing consumer product associations and purchase decisions. In particular, visual and textual cues on processed and ultra-processed products can convey health-related associations that influence consumer healthiness perception and purchase decisions. In this context, the present work aimed to explore the use of health-related cues on the packages of processed and ultra-processed products sold in Uruguay to provide insights for policy making. A total of 3813 products from thirty-four different food categories found in four of the most important supermarket chains in Uruguay were surveyed. The textual and visual information included on the packages as well as the nutritional composition of the products were analysed. Results showed that 67 % of the products included at least one health-related cue. Pictures of culinary ingredients, natural and minimally processed foods were the most frequent health-related cue, followed by references to naturalness and claims related to critical nutrients. The prevalence of health-related cues largely differed across product categories, ranging from 100 to 17 %. The relationship between the presence of health-related cues on the packages and the excessive content of nutrients associated with non-communicable diseases was assessed using a gradient boosting model, which showed limited predictive ability. This suggests that the inclusion of health-related cues on food packages was not strongly related to the nutritional composition of products and therefore cannot be regarded as a healthiness indicator. These results stress the need to develop stricter labelling regulations to protect consumers from misleading information.",
        "DOI": "10.1017/S000711452200318X",
        "affiliation_name": "Universidad de la República Facultad de Química",
        "affiliation_city": "Montevideo",
        "affiliation_country": "Uruguay"
    },
    {
        "paper_title": "Large language model is a flagship for Japan",
        "paper_author": "Kinoshita S.",
        "publication": "Nature",
        "citied_by": "0",
        "cover_date": "2023-07-13",
        "Abstract": "NA",
        "DOI": "10.1038/d41586-023-02230-3",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Environmental Regulation, Smart Meter Adoption, and Carbon Emission: An Interpretable Machine Learning Approach",
        "paper_author": "Gao Y.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "2",
        "cover_date": "2023-07-11",
        "Abstract": "Information as a governance instrument has received increasing attention from e-government research on sustainable development. The implementation of advanced digital technology, such as smart meters, along with environmental regulations, plays an important role in curbing carbon emissions and creating a more sustainable future. In this paper, by combining decision tree and linear spline regression methods, we find a positive connection between smart meter adoption and reduced carbon emissions, and a negative relationship between state environmental regulatory stringency and carbon emissions. Our findings further indicate the impact of smart meter adoption on carbon emissions varies over different smart meter adoptions rate. The impact is stronger when the adoption rate reaches a certain threshold, and it becomes weaker when market saturation happens. These findings have important implications for the development and execution of environmental regulations and public policies for the adoption of smart meters in the United States.",
        "DOI": "10.1145/3598469.3598531",
        "affiliation_name": "School of Management at Clark University",
        "affiliation_city": "Worcester",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Trustworthy artificial intelligence and machine learning: Implications on users' security and privacy perceptions",
        "paper_author": "Do Espírito Santo Faria R.M.",
        "publication": "Confronting Security and Privacy Challenges in Digital Marketing",
        "citied_by": "2",
        "cover_date": "2023-07-10",
        "Abstract": "Artificial intelligence (AI) has altered our world in numerous ways. Although its application has benefits, the underlying issues surrounding privacy and security in AI need to be understood, not only by the organizations that use it but also by the users that are susceptible to its vulnerabilities. To better understand the impact of privacy and security in AI, this chapter reviews the current literature on artificial intelligence, trustworthiness, and privacy and security concepts and uses bibliometric techniques to understand and identify current trends in the field. Finally, the authors highlight the challenges facing AI and machine learning and discuss the results obtained from the bibliometric analysis, which provides insight into the several implications for managers and contributions to future research and policy.",
        "DOI": "10.4018/978-1-6684-8958-1.ch004",
        "affiliation_name": "Universidade de Aveiro",
        "affiliation_city": "Aveiro",
        "affiliation_country": "Portugal"
    },
    {
        "paper_title": "Handbook of research on decision-making capabilities improvement with serious games",
        "paper_author": "Correia A.",
        "publication": "Handbook of Research on Decision-Making Capabilities Improvement With Serious Games",
        "citied_by": "1",
        "cover_date": "2023-07-10",
        "Abstract": "How can a group be empowered to improve their ability to make decisions while also reinforcing the group's intended values, beliefs, and behaviors? Like positive reinforcement, which introduces a desirable or pleasant stimulus after a behavior has been completed and has been found to be effective for reinforcing such behavior, serious games introduce the behavior as a pleasant experience through engagement and entertainment. Where positive reinforcement relies heavily on the willpower of the subject to complete the behavior on their own, serious games introduce a motivational factor from the beginning of the behavior. Serious games are designed for purposes other than entertainment, such as training, learning, creating awareness, or behavior transformation through the introduction of content, topics, narratives, rules, and goals. They are immersive, engaging, and enjoyable, which enhances motivation and learning. The development of serious games is grounded in theoretical backgrounds, such as motivation, constructivism, flow experience, problem-based learning, and learning by doing. This method has been used in a variety of industries, including education, healthcare, military, policy analysis, and business functions such as marketing or financial purposes. They facilitate problem solving through challenges and rewards and use entertainment and engagement components. Serious games can address specific skills for many domains, foster collaboration, provide risk-free environments, and be used as analytical tools for educational research. They reinforce intended values, beliefs, and behaviors of players while conveying knowledge, skills, and attitudes, providing an integrated and effective approach to the transformation of an individual, group, or organization. The Handbook of Research on Decision-Making Capabilities Improvement With Serious Games discusses the use of advanced technologies including extended and immersive reality, digital twins, augmented reality (AR), virtual reality (VR), mixed reality (MR), and IoT sensors to improve decision-making skills and learning through serious games. This book discusses user engagement, game adaptation, content adaptation, and sensor technology. It showcases how to increase decision-making skills in individuals and organizations and incorporates the latest developments in artificial intelligence and machine learning. Led by experts with over 20 years of experience and covering topics such as serious game design, intelligent content adaptation, and machine learning algorithms. This book is designed for professionals in education, instructional designers, curriculum developers, program developers, administrators, educational software developers, policymakers, researchers, training professionals, privacy practitioners, government officials, consultants, IT researchers, academicians, and students.",
        "DOI": "10.4018/978-1-6684-9166-9",
        "affiliation_name": "CINAV",
        "affiliation_city": null,
        "affiliation_country": "Portugal"
    },
    {
        "paper_title": "Single-Leg Revenue Management with Advice",
        "paper_author": "Balseiro S.",
        "publication": "EC 2023 - Proceedings of the 24th ACM Conference on Economics and Computation",
        "citied_by": "3",
        "cover_date": "2023-07-09",
        "Abstract": "Single-leg revenue management is a foundational problem of revenue management that has been particularly impactful in the airline and hotel industry: Given n units of a resource, e.g. flight seats, and a stream of sequentially-arriving customers segmented by fares, what is the optimal online policy for allocating the resource. Previous work focused on designing algorithms when forecasts are available, which are not robust to inaccuracies in the forecast, or online algorithms with worst-case performance guarantees, which can be too conservative in practice. In this work, we look at the single-leg revenue management problem through the lens of the algorithms-with-advice framework, which attempts to harness the increasing prediction accuracy of machine learning methods by optimally incorporating advice about the future into online algorithms. In particular, we develop online algorithms which optimally trade-off consistency (performance when advice is accurate) and competitiveness (performance when advice is inaccurate) for every advice. Our results extend to other unit-cost online allocations problems such as the display advertising and the multiple secretary problem together with more general variable-cost problems such as the online knapsack problem.",
        "DOI": "10.1145/3580507.3597704",
        "affiliation_name": "Columbia University",
        "affiliation_city": "New York",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Integration of machine learning with wearable technologies",
        "paper_author": "Nahavandi D.",
        "publication": "Handbook of Human-Machine Systems",
        "citied_by": "2",
        "cover_date": "2023-07-07",
        "Abstract": "Wearable devices are changing our lives faster than expected. From healthcare services to industrial applications, wearable technologies are taking over. The main idea behind wearable devices is collection of data from their users and processing them to achieve specific objectives. Huge amount of data generated by wearables poses a new challenge for machine learning community. In this chapter, recent advancements in the field of wearable devices with special focus on applications of machine learning methods are reviewed. The chapter starts with a short history of wearable devices followed by a brief review of required machine learning approaches. Technologies such as cloud, fog, and edge computing which are related to storage and processing of data collected by wearable devices are introduced next. Moreover, state-of-the-art applications of machine learning for wearable devices data processing are reviewed. Finally, future research directions and challenges are discussed.",
        "DOI": "10.1002/9781119863663.ch31",
        "affiliation_name": "Deakin University",
        "affiliation_city": "Geelong",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Exploring COVID-19 vaccine hesitancy and behavioral themes using social media big-data: a text mining approach",
        "paper_author": "Yadav H.",
        "publication": "Kybernetes",
        "citied_by": "8",
        "cover_date": "2023-07-07",
        "Abstract": "Purpose: India has the biggest number of active users on social media platforms, particularly Twitter. The purpose of this paper is to examine public sentiment on COVID-19 vaccines and COVID Appropriate Behaviour (CAB) by text mining (topic modeling) and network analysis supported by thematic modeling. Design/methodology/approach: A sample dataset of 115,000 tweets from the Twitter platform was used to examine the perception of the COVID-19 vaccination and CAB from January 2021 to August 2021. The research applied a machine-learning algorithm and network analysis to extract hidden and latent patterns in unstructured data to identify the most prevalent themes. The COVID-19 Vaccine Hesitancy Amplification Model was formulated, which included five key topics based on sample big data from social media. Findings: The identified themes are Social Media Adaptivity, Lack of Knowledge Providing Mechanism, Perception of Vaccine Safety Measures, Health Care Infrastructure Capabilities and Fear of Coronavirus (Coronaphobia). The study implication assists communication strategists and stakeholders design effective communication strategies using digital platforms. The study reveals CAB themes as with Mask Wearing Issues and Employment Issues as relevant themes discussed on digital channels. Research limitations/implications: The themes extracted in the present study provide a roadmap for policy-makers and communication experts to utilize social media platforms for communicating and understanding the perception of preventive measures of vaccination and CAB. As evidenced by the increased engagement on social media platforms during the COVID-19-induced lockdown, digital platforms are indeed valuable from the communication perspective to be proactive in the event of a similar situation. Moreover, significant themes, including social media adaptivity, absence of knowledge-providing mechanism and perception of safety measures of the vaccine, are the critical parameters leading to an amplified effect on vaccine hesitancy. Practical implications: The COVID-19 Vaccine Hesitancy Amplification Themes (CVHAT) equips stakeholders and government strategists with a preconfigured paradigm to tackle dedicated communication campaigns and assess digital community behavior during health emergencies COVID-19. Social implications: The increased acceptance of vaccines and the following of CAB decrease the advocacy of mutation of the virus and promote the healthy being of the people. As CAB has been mentioned as a preventive strategy against the COVID-19 pandemic, the research preposition promotes communication intervention which helps to mitigate future such pandemics. As developing, economies require effective communication strategies for vaccine acceptance and CAB, this study contributes to filling the gap using a digital environment. Originality/value: Chan et al. (2020) recommended using social media platforms for public knowledge dissemination. The study observed that the value of a communication strategy is increased when communication happens using highly trusted and accessible channels such as Twitter and Facebook. With the preceding context, the present study is a novel approach to contribute toward digital communication strategies related to vaccination and CAB.",
        "DOI": "10.1108/K-06-2022-0810",
        "affiliation_name": "Indian Institute of Technology Delhi",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India"
    },
    {
        "paper_title": "A sustainable economic revival plan for post-COVID-19 using machine learning approach – a case study in developing economy context",
        "paper_author": "Bhanot N.",
        "publication": "Benchmarking",
        "citied_by": "5",
        "cover_date": "2023-07-06",
        "Abstract": "Purpose: The impact of COVID-19 has caused a recession in economies all over the world. In this context, the current study aims to analyze the prevailing economic scenario using a machine learning approach and suggest sustainable measures to recover the global economy taking the case of Make in India (MII) initiative of developing the economy as a base for the study. Design/methodology/approach: A well-known topic modeling technique – Latent Dirichlet allocation (LDA) algorithm has been employed to extract useful information characterizing the existing state of selected sectors under the MII initiative alongside catalytic policies that have been implemented for the same. The textual data acts as the base of the study upon which suggestions are provided. Findings: The findings obtained suggest that digital transformation will play a key role in concerned sectors to optimize the performance of manufacturing organizations. Additionally, inter-relationship between Key Performance Indicators for the economy's revival is crucial for effective utilization of foreign direct investment resources. Practical implications: The novel efforts to utilize MII initiative as a case present crucial information which can be used by policy makers and various other stakeholders across the globe to enhance decision-making and draft legislation across different sectors to empower the economy. Originality/value: The study presents a novel approach to utilize the MII initiative by identifying important measures for crucial sectors and associated policies that have been presented by employing a text mining approach which in itself makes it unique in its contribution to research literature.",
        "DOI": "10.1108/BIJ-09-2021-0564",
        "affiliation_name": "BML Munjal University",
        "affiliation_city": "Gurugram",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Applications of big data and artificial intelligence in smart energy systems",
        "paper_author": "Nagpal N.",
        "publication": "Applications of Big Data and Artificial Intelligence in Smart Energy Systems",
        "citied_by": "0",
        "cover_date": "2023-07-03",
        "Abstract": "This book covers smart grid applications of various big data analytics, artificial intelligence, and machine learning technologies for demand prediction, decision-making processes, policy, and energy management. It delves into the new technologies such as the Internet of Things, blockchain, etc. for smart home solutions, and smart city solutions in depth in the context of the modern power systems. In the era of propelling traditional energy systems to evolve towards smart energy systems, systems, including power generation energy storage systems, and electricity consumption have become more dynamic. The quality and reliability of power supply are impacted by the sporadic and rising use of electric vehicles, and domestic and industrial loads. Similarly, with the integration of solid-state devices, renewable sources, and distributed generation, power generation processes are evolving in a variety of ways. Several cutting-edge technologies are necessary for the safe and secure operation of power systems in such a dynamic setting, including load distribution automation, energy regulation and control, and energy trading. Technical topics discussed in the book include: • Hybrid smart energy system technologies • Energy demand forecasting • Use of different protocols and communication in smart energy systems • Power quality and allied issues and mitigation using AI • Intelligent transportation • Virtual power plants • AI business models.",
        "DOI": "NA",
        "affiliation_name": "VIT Bhopal University",
        "affiliation_city": "Sehore",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Reinforcement Learning Based Aircraft Controller Enhanced By Gaussian Process Trim Finding",
        "paper_author": "Benyamen H.",
        "publication": "ASME Letters in Dynamic Systems and Control",
        "citied_by": "2",
        "cover_date": "2023-07-01",
        "Abstract": "This work presents mathematical and practical frameworks for designing deep deterministic policy gradient (DDPG) flight controllers for fixed-wing aircraft. The aim is to design reinforcement learning (RL) flight controllers and accelerate training by substituting the six degrees-of-freedom aircraft models with linear time-invariant (LTI) dynamic models. The initial validation flight tests of the DDPG RL flight controller exhibited poor performance. Post-flight test investigation revealed that the unsatisfactory performance of the RL flight controller could be attributed to the high reliance of the LTI model on accurate control trim values and the substantial errors observed in the predicted trim values generated by the engineering-level dynamic analysis software. A complementary real-time learning Gaussian process (GP) regression was designed to mitigate this critical shortcoming of the LTI-based RL flight controller. The GP estimates and updates the trim control surfaces using observed flight data. The GP regression method incorporates real-time corrections to the trim control surfaces to enhance the performance of the flight controller. Flight test validation was repeated, and the results show that the RL controller, bolstered by the GP trim-finding algorithm, can successfully control the aircraft with excellent tracking performance.",
        "DOI": "10.1115/1.4063605",
        "affiliation_name": "KU School of Engineering",
        "affiliation_city": "Lawrence",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "GCP-HOLO: Generating High-Order Linkage Graphs for Path Synthesis",
        "paper_author": "Fogelson M.B.",
        "publication": "Journal of Mechanical Design",
        "citied_by": "6",
        "cover_date": "2023-07-01",
        "Abstract": "One degrees-of-freedom (1DOF) linkages are persistent in mechanical systems. However, designing linkages to follow a desired path, known as path synthesis, is challenging due to non-linearities, combinatorial nature, and strict geometric constraints. Current state-of-the-art algorithms cannot scale well to linkages with higher-order linkage graphs, which are required to satisfy more complicated paths for new mechanical systems, such as hopping and flying robots. One reason for this is that state-of-the-art algorithms spend the majority of the time exploring constraint-violating designs. This work uses an Assur group 0DOF linkage as a graph grammar rule to modify both linkage graph and spatial parameters, ensuring all designs are valid 1DOF linkages. Using this graph grammar, this paper formulates linkage path synthesis as a tree search and uses a deep reinforcement learning (DRL) agent to search the space of kinematically feasible planar 1DOF linkages. This paper introduces a method using a graph convolution policy for high-order linkage graph optimization (GCP-HOLO). An anytime algorithm, GCP-HOLO outputs linkages with 1-8 loops (4-16 bars) efficiently. When comparing the GCP-HOLO formulation to a recent state-of-the-art paper that solves a mixed integer conic program, GCP-HOLO generates sets of solutions of varying linkage complexities to eight test trajectories in a quarter of the time. Extending GCP-HOLO with a global node optimization, such as covariance matrix adaptation evolutionary strategy, the results quickly converge to finding better solutions for 4/8 tests, with the whole pipeline capable of a 13X speed increase.",
        "DOI": "10.1115/1.4062147",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Pittsburgh",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Analysis of lung cancer risk factors from medical records in Ethiopia using machine learning",
        "paper_author": "Endalie D.",
        "publication": "PLOS Digital Health",
        "citied_by": "3",
        "cover_date": "2023-07-01",
        "Abstract": "Cancer is a broad term that refers to a wide range of diseases that can affect any part of the human body. To minimize the number of cancer deaths and to prepare an appropriate health policy on cancer spread mitigation, scientifically supported knowledge of cancer causes is critical. As a result, in this study, we analyzed lung cancer risk factors that lead to a highly severe cancer case using a decision tree-based ranking algorithm. This feature relevance ranking algorithm computes the weight of each feature of the dataset by using split points to improve detection accuracy, and each risk factor is weighted based on the number of observations that occur for it on the decision tree. Coughing of blood, air pollution, and obesity are the most severe lung cancer risk factors out of nine, with a weight of 39%, 21%, and 14%, respectively. We also proposed a machine learning model that uses Extreme Gradient Boosting (XGBoost) to detect lung cancer severity levels in lung cancer patients. We used a dataset of 1000 lung cancer patients and 465 individuals free from lung cancer from Tikur Ambesa (Black Lion) Hospital in Addis Ababa, Ethiopia, to assess the performance of the proposed model. The proposed cancer severity level detection model achieved 98.9%, 99%, and 98.9% accuracy, precision, and recall, respectively, for the testing dataset. The findings can assist governments and non-governmental organizations in making lung cancer-related policy decisions.",
        "DOI": "10.1371/journal.pdig.0000308",
        "affiliation_name": "Jimma Institute of Technology",
        "affiliation_city": "Jimma",
        "affiliation_country": "Ethiopia"
    },
    {
        "paper_title": "DNN-Approximations of Control Laws for Nonlinear Systems with Polytopic Constraints",
        "paper_author": "Markolf L.",
        "publication": "IFAC-PapersOnLine",
        "citied_by": "1",
        "cover_date": "2023-07-01",
        "Abstract": "This work focuses on deep artificial feed-forward neural networks as parametric function approximators in optimal control of discrete-time nonlinear but affine-in-control systems subject to polytopic constraints on the continuous states and control inputs. The neural networks are either considered as approximators of optimal control laws or as approximators of the corresponding cost functions. In both cases, an approach is developed for determining the approximated optimal control inputs without violating the constraints. A simple approximate policy iteration algorithm exploiting both approaches is finally presented and illustrated for a numerical example.",
        "DOI": "10.1016/j.ifacol.2023.10.1847",
        "affiliation_name": "Universität Kassel",
        "affiliation_city": "Kassel",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Strategically revealing capabilities in General Lotto games",
        "paper_author": "Paarporn K.",
        "publication": "IFAC-PapersOnLine",
        "citied_by": "1",
        "cover_date": "2023-07-01",
        "Abstract": "Can revealing one's competitive capabilities to an opponent offer strategic benefits? In this paper, we address this question in the context of General Lotto games, a class of two-player competitive resource allocation models. We consider an asymmetric information setting where the opponent is uncertain about the resource budget of the other player, and holds a prior belief on its value. We assume the other player, called the signaler, is able to send a noisy signal about its budget to the opponent. With its updated belief, the opponent then must decide to invest in costly resources that it will deploy against the signaler's resource budget in a General Lotto game. We derive the subgame perfect equilibrium to this extensive-form game. In particular, we identify necessary and sufficient conditions for which a signaling policy improves the signaler's resulting performance in comparison to the scenario where it does not send any signal. Moreover, we provide the optimal signaling policy when these conditions are met. Notably we find that for some scenarios, the signaler can effectively double its performance.",
        "DOI": "10.1016/j.ifacol.2023.10.687",
        "affiliation_name": "University of Colorado at Colorado Springs",
        "affiliation_city": "Colorado Springs",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Decentralized Learning of Finite-Memory Policies in Dec-POMDPs",
        "paper_author": "Mao W.",
        "publication": "IFAC-PapersOnLine",
        "citied_by": "0",
        "cover_date": "2023-07-01",
        "Abstract": "Multi-agent reinforcement learning (MARL) under partial observability is notoriously challenging as the agents only have asymmetric partial observations of the system. In this paper, we study MARL in decentralized partially observable Markov decision processes (Dec-POMDPs) with partial history sharing. In search of decentralized and tractable MARL solutions, we identify the appropriate conditions under which we can adopt the common information approach to naturally extend existing single-agent policy learners to Dec-POMDPs. In particular, under the conditions of bounded local memories and an efficient representation of the common information, we present a MARL algorithm that learns a near-optimal finite-memory policy in Dec-POMDPs. We establish the iteration complexity of the algorithm, which depends only linearly on the number of agents. Simulations on classic Dec-POMDP tasks show that our approach significantly outperforms existing decentralized solutions, and nearly matches the centralized ones that require stronger informational assumptions.",
        "DOI": "10.1016/j.ifacol.2023.10.1346",
        "affiliation_name": "University of Maryland, College Park",
        "affiliation_city": "College Park",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Learning Minimax-Optimal Terminal State Estimators and Smoothers",
        "paper_author": "Zhang X.",
        "publication": "IFAC-PapersOnLine",
        "citied_by": "0",
        "cover_date": "2023-07-01",
        "Abstract": "We develop the first model-free policy gradient (PG) algorithm for the minimax state estimation of discrete-time linear dynamical systems, where adversarial disturbances could corrupt both dynamics and measurements. Specifically, the proposed algorithm learns a minimax-optimal solution for three fundamental tasks in robust (minimax) estimation, namely terminal state filtering, terminal state prediction, and smoothing, in a unified fashion. We further establish convergence and finite sample complexity guarantees for the proposed PG algorithm. Additionally, we propose a model-free algorithm to evaluate the attenuation (robustness) level of any estimator or smoother, which serves as a model-free solution to identify the maximum size of the disturbance under which the estimator will still be robust. We demonstrate the effectiveness of the proposed algorithms through extensive numerical experiments.",
        "DOI": "10.1016/j.ifacol.2023.10.447",
        "affiliation_name": "The Grainger College of Engineering",
        "affiliation_city": "Urbana",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "ML<sup>2</sup>-enabled Condition-based Demand, Production, Inventory, and Maintenance Planning",
        "paper_author": "Wesendrup K.",
        "publication": "IFAC-PapersOnLine",
        "citied_by": "1",
        "cover_date": "2023-07-01",
        "Abstract": "Production planning and control is pivotal to meeting customer demand and maximizing profit. At the same time, machine breakdowns compromise these goals, which can be tackled with a good maintenance strategy. Here, advances in condition-based maintenance and prognostics and health management allow predicting the health state of production machines through sensor data and prescribing optimal demand, production, inventory, and maintenance plans. Here, machine learning (ML) is promising for accurate health predictions using sensor data and decision-making in complex, highly dynamic production environments. Thus, in this work, two ML algorithms are applied. First, a data-driven regression algorithm predicts the health of a machine. This forecast is forwarded to a reinforcement learning algorithm (i.e. proximal policy optimization, recently made famous by its application within ChatGPT) to optimize demand, production, inventory, and maintenance plans. A computational study shows excellent performances of the ML-based health prediction and planning algorithms, which surpass traditional maintenance strategies.",
        "DOI": "10.1016/j.ifacol.2023.10.358",
        "affiliation_name": "University of Münster",
        "affiliation_city": "Munster",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "A Spectral Method of Moments for Hierarchical Imitation Learning",
        "paper_author": "Nguyen N.",
        "publication": "IFAC-PapersOnLine",
        "citied_by": "0",
        "cover_date": "2023-07-01",
        "Abstract": "Recent empirical success has led to a rise in popularity of the options framework for Hierarchical Reinforcement Learning (HRL). This framework tackles the scalability problem in Reinforcement Learning (RL) by introducing a layer of abstraction (i.e. high-level options) over the (low-level) decision process. Hierarchical Imitation Learning (HIL) is the problem of learning low-level and high-level policies within HRL from expert demonstrations consisting only of the low-level actions and states, with the high-level options being hidden (or latent). Due to the latent options, recent work on HIL has focused on the development of Expectation-Maximization (EM) algorithms inspired by approaches such as the celebrated Baum-Welch algorithm for hidden Markov models (HMMs). In this work, we take a different approach and derive a new HIL framework inspired by the spectral method of moments for HMMs. The method of moments offers global and consistent convergence under mild regulatory conditions, whilst only requiring one sweep through the data set of state and action pairs, giving it a competitive run time.",
        "DOI": "10.1016/j.ifacol.2023.10.881",
        "affiliation_name": "Boston University College of Engineering",
        "affiliation_city": "Boston",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Neural motion planning in dynamic environments",
        "paper_author": "Wullt B.",
        "publication": "IFAC-PapersOnLine",
        "citied_by": "0",
        "cover_date": "2023-07-01",
        "Abstract": "Motion planning is a mature field within robotics with many successful solutions. Despite this, current state-of-the-art planners are still computationally heavy. To address this, recent work have employed ideas from machine learning, which have drastically reduced the computational cost once a planner has been trained. It is mainly static environments that have been studied in this way. We continue along the same research direction but expand the problem to include dynamic environments, hence increasing the difficulty of the problem. Analogously to previous work, we use imitation learning, where a planning policy is learnt from an expert planner in a supervised manner. Our main contribution is a planner mimicking an expert that considers the future movement of all the obstacles in the environment, which is key in order to learn a successful policy in dynamic environments. We illustrate this by evaluating our approach in a dynamic environment and by comparing our planner with a conventional planner that re-plans at every iteration, which is a common approach in dynamic motion planning. We observe that our approach yields a higher success rate, while also taking less time and accumulating less distance to reach the goal.",
        "DOI": "10.1016/j.ifacol.2023.10.885",
        "affiliation_name": "Uppsala Universitet",
        "affiliation_city": "Uppsala",
        "affiliation_country": "Sweden"
    },
    {
        "paper_title": "Multi-Agent Reinforcement Learning with Information-sharing Constrained Policy Optimization for Global Cost Environment",
        "paper_author": "Okawa Y.",
        "publication": "IFAC-PapersOnLine",
        "citied_by": "3",
        "cover_date": "2023-07-01",
        "Abstract": "Multi-agent Reinforcement Learning (MARL) is a machine learning method that solves problems by using multiple learning agents in a data-driven manner. Because of the advantage of utilizing multiple agents simultaneously, MARL has become an efficient solution to large-scale problems in a wide range of fields. However, as with general single-agent reinforcement learning, MARL requires trial and error to acquire the appropriate policies for each agent in the learning process. Therefore, how to guarantee performance and constraint satisfaction in MARL is a critical issue for application to real-world problems. In this study, we propose an Information-sharing Constrained Policy Optimization (IsCPO) method for MARL that guarantees constraint satisfaction during learning. In detail, IsCPO sequentially updates the policies of multiple agents in random order while sharing information of the surrogate costs and KL-divergence for evaluating the current and updated policies to the next agent. In addition, if there are no candidates of policies to be updated in accordance with the shared information, IsCPO skips updating the policies of the rest of the agents until the next iteration. As a result, IsCPO makes it possible to acquire the individual suboptimal policies of agents, satisfying constraints on global costs related to the state of the environment and the actions from multiple agents. We also introduce a practical algorithm for IsCPO that simplifies its implementation by adopting several mathematical approximations. Finally, we show the validity and effectiveness through simulation results on a multiple cart-pole problem and base station sleep control problem in a mobile network.",
        "DOI": "10.1016/j.ifacol.2023.10.1854",
        "affiliation_name": "Fujitsu Limited",
        "affiliation_city": "Minato",
        "affiliation_country": "Japan"
    },
    {
        "paper_title": "Mixed H<inf>2</inf>/H<inf>∞</inf>-Policy Learning Synthesis",
        "paper_author": "Molu L.",
        "publication": "IFAC-PapersOnLine",
        "citied_by": "2",
        "cover_date": "2023-07-01",
        "Abstract": "A robustly stabilizing optimal control policy in a model-free mixed H2/H∞-control setting is here put forward for counterbalancing the slow convergence and non-robustness of traditional high-variance policy optimization (and by extension policy gradient) algorithms. Leveraging Itô's stochastic differential calculus, we iteratively solve the system's continuous-time (closed-loop) generalized algebraic Riccati equation(GARE) whilst updating its admissible controllers in a two-player, zero-sum differential game setting. Our new results are illustrated by learning-enabled control systems which gather previously disseminated results in this field in one holistic data-driven presentation with greater simplification, improvement, and clarity.",
        "DOI": "10.1016/j.ifacol.2023.10.148",
        "affiliation_name": "Microsoft Research",
        "affiliation_city": "Redmond",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Q-MPC: Stable and efficient reinforcement learning using model predictive control",
        "paper_author": "Oh T.H.",
        "publication": "IFAC-PapersOnLine",
        "citied_by": "0",
        "cover_date": "2023-07-01",
        "Abstract": "There is a growing interest in developing an efficient data-driven control method that can be implemented into digitized manufacturing processes. Model-free reinforcement learning (RL) is a machine learning method that can directly learn the optimal control policy from the process data. However, the model-free RL shows higher cost variance than the model-based method and may require an infeasible amount of data to learn the optimal control policy. Motivated by the fact that the system identification to linear model shows high data efficiency and stable performance, this paper proposes combining the linear model predictive control (MPC) with Q-learning. This combined scheme, Q-MPC, can improve the control performance more stably and safely. For the case study, linear MPC, Q-MPC, DDPG, TD3, and SAC methods are applied to the nonlinear benchmark system, mainly focusing on the learning speed and cost variance.",
        "DOI": "10.1016/j.ifacol.2023.10.1369",
        "affiliation_name": "Department of Chemical Engineering",
        "affiliation_city": "Kyoto",
        "affiliation_country": "Japan"
    },
    {
        "paper_title": "Resilient Consensus in Opinion Dynamics Under Adversarial Epidemics",
        "paper_author": "Masada T.",
        "publication": "IFAC-PapersOnLine",
        "citied_by": "0",
        "cover_date": "2023-07-01",
        "Abstract": "In this paper, we discuss the opinion dynamics with bounded confidence in multi-agent systems under an infection spreading environment. The dynamics of the infection spreading processes follows the so-called susceptible-infected-recovered (SIR) model. Here, the infection induces faulty behaviors in the agents whose opinions may deviate from their true opinions. Cooperating with infection suppression policies and the resilient algorithm based on the mean sub-sequence reduced (MSR) approach, resilient consensus can be attained by the regular agents within a safe region. In particular, we establish sufficient conditions for resilient consensus of opinion dynamics with large bounded confidence. A numerical example is provided to verify the effectiveness of our proposed approach.",
        "DOI": "10.1016/j.ifacol.2023.10.1899",
        "affiliation_name": "Institute of Science Tokyo",
        "affiliation_city": "Tokyo",
        "affiliation_country": "Japan"
    },
    {
        "paper_title": "Informed Random Forest to Model Associations of Epidemiological Priors, Government Policies, and Public Mobility",
        "paper_author": "Thapelo T.S.",
        "publication": "MDM Policy and Practice",
        "citied_by": "2",
        "cover_date": "2023-07-01",
        "Abstract": "Background. Infectious diseases constitute a significant concern worldwide due to their increasing prevalence, associated health risks, and the socioeconomic costs. Machine learning (ML) models and epidemic models formulated using deterministic differential equations are the most dominant tools for analyzing and modeling the transmission of infectious diseases. However, ML models can be inconsistent in extracting the dynamics of a disease in the presence of data drifts. Likewise, the capability of epidemic models is constrained to parameter dimensions and estimation. We aimed at creating a framework of informed ML that integrates a random forest (RF) with an adapted susceptible infectious recovered (SIR) model to account for accuracy and consistency in stochasticity within the dynamics of coronavirus disease 2019 (COVID-19). Methods. An adapted SIR model was used to inform a default RF on predicting new COVID-19 cases (NCCs) at given intervals. We validated the performance of the informed RF (IRF) using real data. We used Botswana’s pharmaceutical interventions (PIs) and non-PIs (NPIs) adopted between February 2020 and August 2022. The discrepancy between predictions and observations is modeled using loss functions, which are minimized, interpreted, and used to assess the IRF. Results. The findings on the real data have revealed the effectiveness of the default RF in modeling and predicting NCCs. The use of the effective reproductive rate to inform the RF yielded an excellent predictive power (84%) compared with 75% by the default RF. Conclusion. This research has potential to inform policy and decision makers in developing systems to evaluate interventions for infectious diseases. This framework is initiated by incorporating model outputs from an epidemic model to a machine learning model. An informed random forest (RF) is instantiated to model government and public responses to the COVID-19 pandemic. This framework does not require data transformations, and the epidemic model is shown to boost the RF’s performance. This is a baseline knowledge-informed learning framework for assessing public health interventions in Botswana.",
        "DOI": "10.1177/23814683231218716",
        "affiliation_name": "Botswana International University of Science and Technology",
        "affiliation_city": "Palapye",
        "affiliation_country": "Botswana"
    },
    {
        "paper_title": "Freezing out: Legacy media's shaping of AI as a cold controversy",
        "paper_author": "Dandurand G.",
        "publication": "Big Data and Society",
        "citied_by": "9",
        "cover_date": "2023-07-01",
        "Abstract": "Mainstream coverage of artificial intelligence often appears to emphasise the technologies’ benefit and economic potential over its growing downsides. How does a technology poised to be so disruptive become so uncritically embraced? Why is it, simply put, that artificial intelligence's representations in legacy media do not normally convey the controversialities otherwise found in research or policy debates? We introduce the concept of ‘freezing out’ to describe processes of translation that cool down debates over the merits of technology. Freezing out looks at the other side of controversy studies to study the production of uncontroversies or cold controversies rather than hot topics and debates. We use the coverage of artificial intelligence in Canadian national news outlets to analyse how controversiality becomes ‘frozen out’. Since Canadian academics won the prestigious ImageNet prize in 2012 introducing the modern turn toward machine learning approaches, Canada has promoted itself as a global leader. Using in-depth interviews with Francophone and Anglophone journalists as well as topic modelling on data collected from five major newspapers, we find that routine news making processes between journalists, experts, entrepreneurs, and governments build, maintain, and promote Canada's artificial intelligence ecosystem. Freezing out contributes to a broader interest in how heterogeneous actors traverse their domain of expertise across policy, media, and research circles to cool down artificial intelligence controversies.",
        "DOI": "10.1177/20539517231219242",
        "affiliation_name": "Centre Urbanisation Culture Société",
        "affiliation_city": "Montreal",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Turbocharger Control for Emission Reduction Based on Deep Reinforcement Learning",
        "paper_author": "Picerno M.",
        "publication": "IFAC-PapersOnLine",
        "citied_by": "1",
        "cover_date": "2023-07-01",
        "Abstract": "The development of embedded systems is a time-consuming process that relies on the calibration of the control systems by experienced engineers and the availability of prototype vehicles. Modern embedded systems must operate with non-linear functions in a wide range of applications. As a result, even experts are unable to fully comprehend all system interconnections, and optimal performance is rarely achieved. Machine Learning (ML)-based techniques can redefine the conventional approach to decision-making problems in this field. Because Reinforcement Learning (RL) algorithms are self-adaptive, near-optimal solutions can be developed with minimal human intervention, providing significant potential for cost and time savings. In this paper, it is demonstrated how RL can be used to design a control function that competes with and outperforms a state-of-the-art controller, while ensuring the stability of the system. In the Model-in-the-Loop (MiL) framework, a method of accelerated training for the control of the turbocharger is presented that considers both performance and emissions criteria. Data efficiency and training robustness were tested for the Proximal Policy Optimization (PPO) algorithm. Selected policies have been evaluated under transient conditions and are shown to enhance the overall boost pressure accuracy by up to 23%, while reducing cumulative NOx and soot emissions by 4% and 10%, respectively, compared to the reference controller. This work demonstrates another step towards the application of the PPO algorithm in the development of embedded systems for complex control problems. Due to its proven robustness and stability, the method is suitable for transfer to real hardware applications.",
        "DOI": "10.1016/j.ifacol.2023.10.1012",
        "affiliation_name": "Teaching and Research Area Mechatronics in Mobile Propulsion",
        "affiliation_city": "Aachen",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Unraveling the complex web: Heart disease and stroke",
        "paper_author": "Li F.",
        "publication": "Heart and Mind",
        "citied_by": "0",
        "cover_date": "2023-07-01",
        "Abstract": "NA",
        "DOI": "10.4103/hm.HM-D-23-00036",
        "affiliation_name": "Beijing Luhe Hospital, Capital Medical University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "ON THE USE OF MACHINE LEARNING TECHNIQUES AND DISCRETE CHOICE MODELS IN MODE CHOICE ANALYSIS",
        "paper_author": "Benjdiya O.",
        "publication": "Logforum",
        "citied_by": "3",
        "cover_date": "2023-07-01",
        "Abstract": "Background: The mode choice stage is a critical aspect that transportation experts rely on to develop a robust transportation system for a particular region. Various techniques are utilized to model mode choice behavior, including Discrete Choice Models (DCMs) and Machine Learning (ML) techniques. However, existing reviews typically focus on either DCMs or ML techniques, and reviews that cover both categories often concentrate on one category while merely mentioning some techniques from the other. This paper aims to address this gap by examining the principal DCMs and ML techniques published over the past four years, differentiating between models based on the granularity level, namely aggregate and disaggregate models. Additionally, a comprehensive discussion is conducted on the accuracy of the different models used in the reviewed articles. Methods: This paper provides a thorough and enhanced analysis of travel mode choice models and analysis techniques used in articles published on \"ScienceDirect\" from 2020 to 2023. To ensure a comprehensive coverage of the subject, a meticulous search strategy was employed, utilizing targeted keywords. As a result, a total of 38 articles were carefully selected for detailed examination and analysis. Results: The findings of this study highlight the suitability of different modeling approaches for varying levels of analysis. Discrete Choice Models demonstrate effectiveness in aggregate-level analyses, whereas Machine Learning Techniques prove more appropriate for disaggregate-level analyses. Moreover, the study suggests that employing hybrid models can potentially yield a promising solution to attain enhanced prediction accuracy without compromising interpretability. Conclusions: The examination of selected articles revealed several key points. Firstly, there is a concentration of studies on travel mode choice in European countries, China, and the USA, indicating a need for more research in developing countries. Secondly, the reviewed articles often lack in-depth analysis of individual behavior and fail to consider external factors like weather or seasons when employing disaggregate models. Thus, future studies should leverage technological advancements and explore new factors influencing mode choice behavior. Additionally, there is a need for further research on hybrid models that combine Discrete Choice Models (DCMs) with Machine Learning (ML) techniques or deep learning approaches. This research can provide guidance for practitioners unfamiliar with these methods and aid in the design of effective transportation policies. Lastly, considering the variety of models available, it is crucial to understand the extent to which these models can be generalized to different contexts, emphasizing the importance of studying model applicability and generalizability in diverse settings.",
        "DOI": "10.17270/J.LOG.2023.845",
        "affiliation_name": "Université Euro-Méditerranéenne de Fès",
        "affiliation_city": "Fez",
        "affiliation_country": "Morocco"
    },
    {
        "paper_title": "Visual Analytics Using Machine Learning for Transparency Requirements",
        "paper_author": "Fadloun S.",
        "publication": "Mathematics",
        "citied_by": "0",
        "cover_date": "2023-07-01",
        "Abstract": "Problem solving applications require users to exercise caution in their data usage practices. Prior to installing these applications, users are encouraged to read and comprehend the terms of service, which address important aspects such as data privacy, processes, and policies (referred to as information elements). However, these terms are often lengthy and complex, making it challenging for users to fully grasp their content. Additionally, existing transparency analytics tools typically rely on the manual extraction of information elements, resulting in a time-consuming process. To address these challenges, this paper proposes a novel approach that combines information visualization and machine learning analyses to automate the retrieval of information elements. The methodology involves the creation and labeling of a dataset derived from multiple software terms of use. Machine learning models, including naïve Bayes, BART, and LSTM, are utilized for the classification of information elements and text summarization. Furthermore, the proposed approach is integrated into our existing visualization tool TranspVis to enable the automatic detection and display of software information elements. The system is thoroughly evaluated using a database-connected tool, incorporating various metrics and expert opinions. The results of our study demonstrate the promising potential of our approach, serving as an initial step in this field. Our solution not only addresses the challenge of extracting information elements from complex terms of service but also provides a foundation for future research in this area.",
        "DOI": "10.3390/math11143091",
        "affiliation_name": "Princess Nourah Bint Abdulrahman University",
        "affiliation_city": "Riyadh",
        "affiliation_country": "Saudi Arabia"
    },
    {
        "paper_title": "Estimating Marketing Component Effects: Double Machine Learning from Targeted Digital Promotions",
        "paper_author": "Ellickson P.B.",
        "publication": "Marketing Science",
        "citied_by": "12",
        "cover_date": "2023-07-01",
        "Abstract": "We estimate the causal effects of different targeted email promotions on the opening and purchase decisions of the consumerswho receive them. To do so,we synthesize and extend recent advances in causal machine learning techniques to capture heterogeneity in the content of the email subject line itself as well as heterogeneous consumer responses to the promotional offers and semantic choices contained therein. We find that content and framing are important for driving performance. We identify precise causal estimates of the effects of individual deal components, personalized content, and various semantic choices on consumer outcomes all theway down the conversion funnel. The decompositional nature of our methodology allows us to show how different combinations of key words and promotional inducements produce significantly different outcomes, both within a given stage and across all stages of the funnel. Notably, discounts framed as clearance events sharply outperform those tied to particular products. We also find components that drive engagement at the top of the funnel don’t always lead to conversion at the bottom: their efficacy, across the funnel, is significantly moderated by the engagement levels of the consumers who receive them. Finally, leveraging both aspects of heterogeneity,we use off-policy evaluation to demonstrate the potential for significant gains fromimproved targeting.",
        "DOI": "10.1287/mksc.2022.1401",
        "affiliation_name": "Simon Business School",
        "affiliation_city": "Rochester",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "A generator reactive power optimization method based on XGBOOST-PSO to improve the voltage transient stability of a receiving terminal network",
        "paper_author": "Guo P.",
        "publication": "Dianli Xitong Baohu yu Kongzhi/Power System Protection and Control",
        "citied_by": "5",
        "cover_date": "2023-07-01",
        "Abstract": "Considering the effect of generator steady-state output reactive power on the ability to support transient voltage recovery stability, preventive control through optimizing that power to improve the ability of grid maintaining transient voltage stability under severe fault disturbance is studied based on machine learning. The method uses a weighted multi-binary table transient stability margin index to quantify the comprehensive stability margin of bus transient voltages to different expected fault disturbances. Then it determines the bus with weak stability margin based on the index ranking. At the same time, based on the comprehensive action sensitivity ranking of generator reactive power regulation on the voltage transient stability margin of each weak bus under the expected fault, the action sensitive generator can be determined. Then an XGBoost classification model for predicting transient voltage stability based on the system steady-state characteristics vector is developed. This considers the constraint of grid voltage maintaining transient stability under expected severe fault. It has the objective of reducing the impact on grid active power loss caused by generator reactive power. A regulation method of using power flow calculation to optimize the steady-state reactive power output of the sensitive generator is proposed. Finally, the validity of the proposed method is verified based on the PSASP calculation model for Yahu DC transmission fed-in Jiangxi power grid.",
        "DOI": "10.19783/j.cnki.pspc.221743",
        "affiliation_name": "Wuhan University",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Semantic temporality analysis: A computational approach to time in English and German texts",
        "paper_author": "Watanabe K.",
        "publication": "Research and Politics",
        "citied_by": "2",
        "cover_date": "2023-07-01",
        "Abstract": "Temporality is an important aspect of political discourse. Politicians and policymakers attempt to construct the past and the future to gain power, legitimize their policies, claim success for themselves and blame others. To make computational analysis of temporality more accessible, we develop a new methodology using a semisupervised machine-learning algorithm called Latent Semantic Scaling. Only with a set of common verbs in the past perfect and future tense as seed words, the algorithm estimates the temporality of all other words. We demonstrate that it can identify temporal orientation of English and German sentences from election manifestos around 60–70% accurately, which is comparable to the results from a recent study based on supervised machine-learning algorithms. We also apply it to Twitter posts by German political parties to reveal temporal orientation of policy issues.",
        "DOI": "10.1177/20531680231197456",
        "affiliation_name": "Waseda Institute for Advanced Study",
        "affiliation_city": "Tokyo",
        "affiliation_country": "Japan"
    },
    {
        "paper_title": "Robust Dynamic Pricing with Demand Learning in the Presence of Outlier Customers",
        "paper_author": "Chen X.",
        "publication": "Operations Research",
        "citied_by": "2",
        "cover_date": "2023-07-01",
        "Abstract": "This paper studies a dynamic pricing problem undermodel misspecification. To characterize model misspecification, we adopt the ϵ-contamination model-the most fundamental model in robust statistics and machine learning. In particular, for a selling horizon of length T, the online ϵ-contamination model assumes that demands are realized according to a typical unknown demand function only for (1-ϵ)T periods. For the rest of ϵT periods, an outlier purchase can happen with arbitrary demand functions. The challenges brought by the presence of outlier customers are mainly due to the fact that arrivals of outliers and their exhibited demand behaviors are completely arbitrary, therefore calling for robust estimation and exploration strategies that can handle any outlier arrival and demand patterns.We first consider unconstrained dynamic pricing without any inventory constraint. In this case, we adopt the Follow-the-Regularized-Leader algorithm to hedge against outlier purchase behavior. Then, we introduce inventory constraints. When the inventory is insufficient, we study a robust bisection-search algorithm to identify the clearance price-that is, the price at which the initial inventory is expected to clear at the end of T periods. Finally, we study the general dynamic pricing case, where a retailer has no clue whether the inventory is sufficient or not. In this case, we design a meta-algorithm that combines the previous two policies. All algorithms are fully adaptive, without requiring prior knowledge of the outlier proportion parameter ϵ. Simulation study shows that our policy outperforms existing policies in the literature.",
        "DOI": "10.1287/opre.2022.2280",
        "affiliation_name": "The Naveen Jindal School of Management",
        "affiliation_city": "Richardson",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Quantum policy gradient algorithms",
        "paper_author": "Jerbi S.",
        "publication": "Leibniz International Proceedings in Informatics, LIPIcs",
        "citied_by": "0",
        "cover_date": "2023-07-01",
        "Abstract": "Understanding the power and limitations of quantum access to data in machine learning tasks is primordial to assess the potential of quantum computing in artificial intelligence. Previous works have already shown that speed-ups in learning are possible when given quantum access to reinforcement learning environments. Yet, the applicability of quantum algorithms in this setting remains very limited, notably in environments with large state and action spaces. In this work, we design quantum algorithms to train state-of-The-Art reinforcement learning policies by exploiting quantum interactions with an environment. However, these algorithms only offer full quadratic speed-ups in sample complexity over their classical analogs when the trained policies satisfy some regularity conditions. Interestingly, we find that reinforcement learning policies derived from parametrized quantum circuits are well-behaved with respect to these conditions, which showcases the benefit of a fully-quantum reinforcement learning framework.",
        "DOI": "10.4230/LIPIcs.TQC.2023.13",
        "affiliation_name": "Universiteit Leiden",
        "affiliation_city": "Leiden",
        "affiliation_country": "Netherlands"
    },
    {
        "paper_title": "A transmission design in dynamic heterogeneous V2V networks through multi-agent deep reinforcement learning",
        "paper_author": "Qu N.",
        "publication": "China Communications",
        "citied_by": "2",
        "cover_date": "2023-07-01",
        "Abstract": "In highly dynamic and heterogeneous vehicular communication networks, it is challenging to efficiently utilize network resources and ensure demanding performance requirements of safety-related applications. This paper investigates machine-learning-assisted transmission design in a typical multi-user vehicle-to-vehicle (V2V) communication scenario. The transmission process proceeds sequentially along the discrete time steps, where several source nodes intend to deliver multiple different types of messages to their respective destinations within the same spectrum. Due to rapid movement of vehicles, real-time acquirement of channel knowledge and central coordination of all transmission actions are in general hard to realize. We consider applying multi-agent deep reinforcement learning (MADRL) to handle this issue. By transforming the transmission design problem into a stochastic game, a multi-agent proximal policy optimization (MAPPO) algorithm under a centralized training and decentralized execution framework is proposed such that each source decides its own transmission message type, power level, and data rate, based on local observations of the environment and feedback, to maximize its energy efficiency. Via simulations we show that our method achieves better performance over conventional methods.",
        "DOI": "10.23919/JCC.fa.2021-0825.202307",
        "affiliation_name": "Tongji University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Machine Learning Implementation in Membrane Bioreactor Systems: Progress, Challenges, and Future Perspectives: A Review",
        "paper_author": "Frontistis Z.",
        "publication": "Environments - MDPI",
        "citied_by": "11",
        "cover_date": "2023-07-01",
        "Abstract": "This study offers a review of machine learning (ML) applications in membrane bioreactor (MBR) systems, an emerging technology in advanced wastewater treatment. The review focuses on implementing ML algorithms to enhance the prediction of membrane fouling, control and optimize the system, and predict faults early, thereby enabling the development of novel cleaning strategies. Key ML algorithms such as artificial neural networks (ANNs), support vector machines (SVMs), random forest, and reinforcement learning (RL) are briefly introduced, with an emphasis on their potential and limitations in advanced wastewater applications. The main challenges obstructing the implementation, namely data quality, interpretability, and transferability of ML, are identified. Finally, future research trends are proposed, including ML integration with big data, the Internet of Things (IoT), and hybrid model development. The review also underscores the need for interdisciplinary collaboration and investment in data management, along with the implementation of new policies addressing data privacy and security. By addressing these challenges, the integration of ML into MBRs has the potential to significantly enhance performance and reduce the energy footprint, providing a sustainable solution for advanced wastewater treatment.",
        "DOI": "10.3390/environments10070127",
        "affiliation_name": "University of Western Macedonia",
        "affiliation_city": "Kozani",
        "affiliation_country": "Greece"
    },
    {
        "paper_title": "Unveiling the Comorbidities of Chronic Diseases in Serbia Using ML Algorithms and Kohonen Self-Organizing Maps for Personalized Healthcare Frameworks",
        "paper_author": "Rankovic N.",
        "publication": "Journal of Personalized Medicine",
        "citied_by": "5",
        "cover_date": "2023-07-01",
        "Abstract": "In previous years, significant attempts have been made to enhance computer-aided diagnosis and prediction applications. This paper presents the results obtained using different machine learning (ML) algorithms and a special type of a neural network map to uncover previously unknown comorbidities associated with chronic diseases, allowing for fast, accurate, and precise predictions. Furthermore, we are presenting a comparative study on different artificial intelligence (AI) tools like the Kohonen self-organizing map (SOM) neural network, random forest, and decision tree for predicting 17 different chronic non-communicable diseases such as asthma, chronic lung diseases, myocardial infarction, coronary heart disease, hypertension, stroke, arthrosis, lower back diseases, cervical spine diseases, diabetes mellitus, allergies, liver cirrhosis, urinary tract diseases, kidney diseases, depression, high cholesterol, and cancer. The research was developed as an observational cross-sectional study through the support of the European Union project, with the data collected from the largest Institute of Public Health “Dr. Milan Jovanovic Batut” in Serbia. The study found that hypertension is the most prevalent disease in Sumadija and western Serbia region, affecting 9.8% of the population, and it is particularly prominent in the age group of 65 to 74 years, with a prevalence rate of 33.2%. The use of Random Forest algorithms can also aid in identifying comorbidities associated with hypertension, with the highest number of comorbidities established as 11. These findings highlight the potential for ML algorithms to provide accurate and personalized diagnoses, identify risk factors and interventions, and ultimately improve patient outcomes while reducing healthcare costs. Moreover, they will be utilized to develop targeted public health interventions and policies for future healthcare frameworks to reduce the burden of chronic diseases in Serbia.",
        "DOI": "10.3390/jpm13071032",
        "affiliation_name": "Univerzitet Union Nikola Tesla",
        "affiliation_city": "Belgrade",
        "affiliation_country": "Serbia"
    },
    {
        "paper_title": "The Forecasting of a Leading Country’s Government Expenditure Using a Recurrent Neural Network with a Gated Recurrent Unit",
        "paper_author": "Yang C.H.",
        "publication": "Mathematics",
        "citied_by": "1",
        "cover_date": "2023-07-01",
        "Abstract": "Economic forecasting is crucial in determining a country’s economic growth or decline. Productivity and the labor force must be increased to achieve economic growth, which leads to the growth of gross domestic product (GDP) and income. Machine learning has been used to provide accurate economic forecasts, which are essential to sound economic policy. This study formulated a gated recurrent unit (GRU) neural network model to predict government expenditure, an essential component of gross domestic product. The GRU model was evaluated against autoregressive integrated moving average, support vector regression, exponential smoothing, extreme gradient boosting, convolutional neural network, and long short-term memory models using World Bank data regarding government expenditure from 1990 to 2020. The mean absolute error, root mean square error, and mean absolute percentage error were used as performance metrics. The GRU model demonstrates superior performance compared to all other models in terms of MAE, RMSE, and MAPE (with an average MAPE of 2.774%) when forecasting government spending using data from the world’s 15 largest economies from 1990 to 2020. The results indicate that the GRU can be used to provide accurate economic forecasts.",
        "DOI": "10.3390/math11143085",
        "affiliation_name": "National Kaohsiung University of Science and Technology",
        "affiliation_city": "Kaohsiung",
        "affiliation_country": "Taiwan"
    },
    {
        "paper_title": "Exploring Influential Factors with Structural Equation Modeling–Artificial Neural Network to Involve Medicine Users in Home Medicine Waste Management and Preventing Pharmacopollution",
        "paper_author": "Silva W.D.O.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "2",
        "cover_date": "2023-07-01",
        "Abstract": "The appropriate management of home medical waste is of paramount importance due to the adverse consequences that arise from improper handling. Incorrect disposal practices can lead to pharmacopollution, which poses significant risks to environmental integrity and human well-being. Involving medicine users in waste management empowers them to take responsibility for their waste and make informed decisions to safeguard the environment and public health. The objective of this research was to contribute to the prevention of pharmacopollution by identifying influential factors that promote responsible disposal practices among medicine users. Factors such as attitude, marketing campaigns, collection points, safe handling, medical prescription, package contents, and public policies and laws were examined. To analyze the complex relationships and interactions among these factors, a dual-staged approach was employed, utilizing advanced statistical modeling techniques and deep learning artificial neural network algorithms. Data were collected from 952 respondents in Pernambuco, a state in northeastern Brazil known for high rates of pharmacopollution resulting from improper disposal of household medical waste. The results of the study indicated that the propositions related to safety in handling and medical prescription were statistically rejected in the structural equation modeling (SEM) model. However, in the artificial neural network (ANN) model, these two propositions were found to be important predictors of cooperative behavior, highlighting the ANN’s ability to capture complex, non-linear relationships between variables. The findings emphasize the significance of user cooperation and provide insights for the development of effective strategies and policies to address pharmacopollution.",
        "DOI": "10.3390/su151410898",
        "affiliation_name": "Universidade Federal de Pernambuco",
        "affiliation_city": "Recife",
        "affiliation_country": "Brazil"
    },
    {
        "paper_title": "Estimation of the Extent of the Vulnerability of Agriculture to Climate Change Using Analytical and Deep-Learning Methods: A Case Study in Jammu, Kashmir, and Ladakh",
        "paper_author": "Malik I.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "16",
        "cover_date": "2023-07-01",
        "Abstract": "Climate stress poses a threat to the agricultural sector, which is vital for both the economy and livelihoods in general. Quantifying its risk to food security, livelihoods, and sustainability is crucial. This study proposes a framework to estimate the impact climate stress on agriculture in terms of three objectives: assessing the regional vulnerability (exposure, sensitivity, and adaptive capacity), analysing the climate variability, and measuring agricultural performance under climatic stress. The vulnerability of twenty-two sub-regions in Jammu, Kashmir, and Ladakh is assessed using indicators to determine the collective susceptibility of the agricultural framework to climate change. An index-based approach with min–max normalization is employed, ranking the districts based on their relative performances across vulnerability indicators. This work assesses the impact of socio-economic and climatic indicators on the performance of agricultural growth using the benchmark Ricardian approach. The parameters of the agricultural growth function are estimated using a linear combination of socio-economic and exposure variables. Lastly, the forecasted trends of climatic variables are examined using a long short-term memory (LSTM)-based recurrent neural network, providing an annual estimate of climate variability. The results indicate a negative impact of annual minimum temperature and decreasing land holdings on agricultural GDP, while cropping intensity, rural literacy, and credit facilities have positive effects. Budgam, Ganderbal, and Bandipora districts exhibit higher vulnerability due to factors such as low literacy rates, high population density, and extensive rice cultivation. Conversely, Kargil, Rajouri, and Poonch districts show lower vulnerability due to the low population density and lower level of institutional development. We observe an increasing trend of minimum temperature across the region. The proposed LSTM synthesizes a predictive estimate across five essential climate variables with an average overall root mean squared error (RMSE) of 0.91, outperforming the benchmark ARIMA and exponential-smoothing models by 32–48%. These findings can guide policymakers and stakeholders in developing strategies to mitigate climate stress on agriculture and enhance resilience.",
        "DOI": "10.3390/su151411465",
        "affiliation_name": "University of Sindh",
        "affiliation_city": "Jamshoro",
        "affiliation_country": "Pakistan"
    },
    {
        "paper_title": "Estimation of Methane Gas Production in Turkey Using Machine Learning Methods",
        "paper_author": "Ünal Uyar G.F.",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "8",
        "cover_date": "2023-07-01",
        "Abstract": "Methane gas emission into the atmosphere is rising due to the use of fossil-based resources in post-industrial energy use, as well as the increase in food demand and organic wastes that comes with an increasing human population. For this reason, methane gas, which is among the greenhouse gases, is seen as an important cause of climate change along with carbon dioxide. The aim of this study was to predict, using machine learning, the emission of methane gas, which has a greater effect on the warming of the atmosphere than other greenhouse gases. Methane gas estimation in Turkey was carried out using machine learning methods. The R2 metric was calculated as logistic regression (LR) 94.9%, artificial neural networks (ANNs) 93.6%, and support vector regression (SVR) 92.3%. All three machine learning methods used in the study were close to ideal statistical criteria. LR had the least error and highest prediction success, followed by ANNs and then SVR. The models provided successful results, which will be useful in the formulation of policies in terms of animal production (especially cattle production) and the disposal of organic human wastes, which are thought to be the main causes of methane gas emission.",
        "DOI": "10.3390/app13148442",
        "affiliation_name": "Aydin Adnan Menderes University",
        "affiliation_city": "Aydin",
        "affiliation_country": "Turkey"
    },
    {
        "paper_title": "Artificial Intelligence and Mathematical Models of Power Grids Driven by Renewable Energy Sources: A Survey",
        "paper_author": "Srinivasan S.",
        "publication": "Energies",
        "citied_by": "14",
        "cover_date": "2023-07-01",
        "Abstract": "To face the impact of climate change in all dimensions of our society in the near future, the European Union (EU) has established an ambitious target. Until 2050, the share of renewable power shall increase up to 75% of all power injected into nowadays’ power grids. While being clean and having become significantly cheaper, renewable energy sources (RES) still present an important disadvantage compared to conventional sources. They show strong fluctuations, which introduce significant uncertainties when predicting the global power outcome and confound the causes and mechanisms underlying the phenomena in the grid, such as blackouts, extreme events, and amplitude death. To properly understand the nature of these fluctuations and model them is one of the key challenges in future energy research worldwide. This review collects some of the most important and recent approaches to model and assess the behavior of power grids driven by renewable energy sources. The goal of this survey is to draw a map to facilitate the different stakeholders and power grid researchers to navigate through some of the most recent advances in this field. We present some of the main research questions underlying power grid functioning and monitoring, as well as the main modeling approaches. These models can be classified as AI- or mathematically inspired models and include dynamical systems, Bayesian inference, stochastic differential equations, machine learning methods, deep learning, reinforcement learning, and reservoir computing. The content is aimed at the broad audience potentially interested in this topic, including academic researchers, engineers, public policy, and decision-makers. Additionally, we also provide an overview of the main repositories and open sources of power grid data and related data sets, including wind speed measurements and other geophysical data.",
        "DOI": "10.3390/en16145383",
        "affiliation_name": "Chennai Institute of Technology",
        "affiliation_city": "Chennai",
        "affiliation_country": "India"
    },
    {
        "paper_title": "An Intelligent Algorithm for Solving Unit Commitments Based on Deep Reinforcement Learning",
        "paper_author": "Huang G.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "0",
        "cover_date": "2023-07-01",
        "Abstract": "With the reform of energy structures, the high proportion of volatile new energy access makes the existing unit commitment (UC) theory unable to satisfy the development demands of day-ahead market decision-making in the new power system. Therefore, this paper proposes an intelligent algorithm for solving UC, based on deep reinforcement learning (DRL) technology. Firstly, the DRL algorithm is used to model the Markov decision process of the UC problem, and the corresponding state space, transfer function, action space and reward function are proposed. Then, the policy gradient (PG) algorithm is used to solve the problem. On this basis, Lambda iteration is used to solve the output scheme of the unit in the start–stop state, and finally a DRL-based UC intelligent solution algorithm is proposed. The applicability and effectiveness of this method are verified based on simulation examples.",
        "DOI": "10.3390/su151411084",
        "affiliation_name": "China Southern Power Grid",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Implementation of a Sequence-to-Sequence Stacked Sparse Long Short-Term Memory Autoencoder for Anomaly Detection on Multivariate Timeseries Data of Industrial Blower Ball Bearing Units",
        "paper_author": "Karapalidou E.",
        "publication": "Sensors",
        "citied_by": "5",
        "cover_date": "2023-07-01",
        "Abstract": "The advent of Industry 4.0 introduced new ways for businesses to evolve by implementing maintenance policies leading to advancements in terms of productivity, efficiency, and financial performance. In line with the growing emphasis on sustainability, industries implement predictive techniques based on Artificial Intelligence for the purpose of mitigating machine and equipment failures by predicting anomalies during their production process. In this work, a new dataset that was made publicly available, collected from an industrial blower, is presented, analyzed and modeled using a Sequence-to-Sequence Stacked Sparse Long Short-Term Memory Autoencoder. Specifically the right and left mounted ball bearing units were measured during several months of normal operational condition as well as during an encumbered operational state. An anomaly detection model was developed for the purpose of analyzing the operational behavior of the two bearing units. A stacked sparse Long Short-Term Memory Autoencoder was successfully trained on the data obtained from the left unit under normal operating conditions, learning the underlying patterns and statistical connections of the data. The model was evaluated by means of the Mean Squared Error using data from the unit’s encumbered state, as well as using data collected from the right unit. The model performed satisfactorily throughout its evaluation on all collected datasets. Also, the model proved its capability for generalization along with adaptability on assessing the behavior of equipment similar to the one it was trained on.",
        "DOI": "10.3390/s23146502",
        "affiliation_name": "International Hellenic University",
        "affiliation_city": "Thessaloniki",
        "affiliation_country": "Greece"
    },
    {
        "paper_title": "‘Good farmers’ and ‘real vets’: social identities, behaviour change and the future of bovine tuberculosis eradication",
        "paper_author": "Enticott G.",
        "publication": "Irish Veterinary Journal",
        "citied_by": "3",
        "cover_date": "2023-07-01",
        "Abstract": "This paper considers the role of social research and human behaviour in attempts to eradicate bTB. Future attempts to eradicate bTB are likely to involve an increasing range of sophisticated technologies. However, the acceptance and use of these technologies is likely to depend on a range of behavioural incentives. The use of appropriate behavioural nudges may facilitate bTB eradication, but the paper contends that of more value are socio-cultural approaches to understanding behaviour. Specifically, the concepts of the ‘good farmer’ and ‘real vets’ are discussed to show how bTB eradication is dependent on social identities. In conclusion, the paper outlines four key roles for social research in assisting with future bTB eradication policies.",
        "DOI": "10.1186/s13620-023-00245-w",
        "affiliation_name": "College of Arts, Humanities and Social Sciences",
        "affiliation_city": "Cardiff",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Benchmarking Biologically-Inspired Automatic Machine Learning for Economic Tasks",
        "paper_author": "Lazebnik T.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "9",
        "cover_date": "2023-07-01",
        "Abstract": "Data-driven economic tasks have gained significant attention in economics, allowing researchers and policymakers to make better decisions and design efficient policies. Recently, with the advancement of machine learning (ML) and other artificial intelligence (AI) methods, researchers can now solve complex economic tasks with previously unseen performance and ease. However, to use such methods, one is required to have a non-trivial level of expertise in ML or AI, which currently is not standard knowledge in economics. In order to bridge this gap, automatic machine learning (AutoML) models have been developed, allowing non-experts to efficiently use advanced ML models with their data. Nonetheless, not all AutoML models are created equal in general, particularly for the unique properties associated with economic data. In this paper, we present a benchmarking study of biologically inspired and other AutoML techniques for economic tasks. We evaluate four different AutoML models alongside two baseline methods using a set of 50 diverse economic tasks. Our results show that biologically inspired AutoML models (slightly) outperformed non-biological AutoML in economic tasks, while all AutoML models outperformed the traditional methods. Based on our results, we conclude that biologically inspired AutoML has the potential to improve our economic understanding while shifting a large portion of the analysis burden from the economist to a computer.",
        "DOI": "10.3390/su151411232",
        "affiliation_name": "Shalvata Mental Health Center",
        "affiliation_city": "Hod HaSharon",
        "affiliation_country": "Israel"
    },
    {
        "paper_title": "Observation-Constrained Projection of Flood Risks and Socioeconomic Exposure in China",
        "paper_author": "Kang S.",
        "publication": "Earth's Future",
        "citied_by": "16",
        "cover_date": "2023-07-01",
        "Abstract": "As the planet warms, the atmosphere's water vapor holding capacity rises, leading to more intense precipitation extremes. River floods with high peak discharge or long duration can increase the likelihood of infrastructure failure and enhance ecosystem vulnerability. However, changes in the peak and duration of floods and corresponding socioeconomic exposure under climate change are still poorly understood. This study employs a bivariate framework to quantify changes in flood risks and their socioeconomic impacts in China between the past (1985–2014) and future (2071–2100) in 204 catchments. Future daily river streamflow is projected by using a cascade modeling chain based on the outputs of five bias-corrected global climate models (GCMs) under three shared socioeconomic CMIP6 pathways (SSP1-26, SSP3-70, and SSP5-85), a machine learning model and four hydrological models. We also utilize the copula function to build the joint distribution of flood peak and duration, and calculate the joint return periods of the bivariate flood hazard. Finally, the exposure of population and regional gross domestic product to floods are investigated at the national scale. Our results indicate that flood peak and duration are likely to increase in the majority of catchments by 25%–100% by the late 21st century depending on the shared socioeconomic pathway. China is projected to experience a significant increase in bivariate flood risks even under the lowest emission pathway, with 24.0 million dollars/km2 and 608 people/km2 exposed under a moderate emissions scenario (SSP3-70). These findings have direct implications for hazard mitigation and climate adaptation policies in China.",
        "DOI": "10.1029/2022EF003308",
        "affiliation_name": "Wuhan University",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Clustering energy support beliefs to reveal unique sub-populations using self-organizing maps",
        "paper_author": "Bedle H.",
        "publication": "Heliyon",
        "citied_by": "1",
        "cover_date": "2023-07-01",
        "Abstract": "Americans’ support for energy sources is quite complex and dependent on a range of socio-demographic characteristics. In a novel approach to investigate energy opinions on renewables and fossil fuels, self-organizing maps are employed to cluster individuals solely on their energy beliefs using social survey data collected from Pew Research in the Spring of 2021. These energy preference clusters are then used in regression models to examine attitudes regarding energy policy in the United States. Results from the self-organizing map (SOM) analysis reveal four distinct clusters: energy traditionalists who oppose renewable sources due to partisan ideologies; energy renewers who strongly prefer investment in only renewable energy sources; energy universalists who universally support all forms of energy; and the aberrant cluster, individuals who prefer solar power greatly over wind energy but demonstrate no other energy preference patterns. Results from regression analyses reveal that SOM clusters are highly predictive of attitudes regarding energy policy. Taken together, these results reveal the unique capability of machine learning to categorize human attitudes – which should be of particular interest to energy policymakers when considering the opinions of the electorate.",
        "DOI": "10.1016/j.heliyon.2023.e18351",
        "affiliation_name": "The University of Oklahoma",
        "affiliation_city": "Norman",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Sustainable Traffic Management for Smart Cities Using Internet-of-Things-Oriented Intelligent Transportation Systems (ITS): Challenges and Recommendations",
        "paper_author": "Musa A.A.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "42",
        "cover_date": "2023-07-01",
        "Abstract": "The emergence of smart cities has addressed many critical challenges associated with conventional urbanization worldwide. However, sustainable traffic management in smart cities has received less attention from researchers due to its complex and heterogeneous nature, which directly affects smart cities’ transportation systems. The study aimed at addressing traffic-related issues in smart cities by focusing on establishing a sustainable framework based on the Internet of Things (IoT) and Intelligent Transportation System (ITS) applications. To sustain the management of traffic in smart cities, which is composed of a hybridized stream of human-driven vehicles (HDV) and connected automated vehicles (CAV), a dual approach was employed by considering traffic as either modeling- and analysis-based, or/and the decision-making issues of previous research works. Moreover, the two techniques utilized real-time traffic data, and collected vehicle and road users’ information using AI sensors and ITS-based devices. These data can be processed and transmitted using machine learning algorithms and cloud computing for traffic management, traffic decision-making policies, and documentation for future use. The proposed framework suggests that deploying such systems in smart cities’ transportation could play a significant role in predicting traffic outcomes, traffic forecasting, traffic decongestion, minimizing road users’ lost hours, suggesting alternative routes, and simplifying urban transportation activities for urban dwellers. Also, the proposed integrated framework adopted can address issues related to pollution in smart cities by promoting public transportation and advocating low-carbon emission zones. By implementing these solutions, smart cities can achieve sustainable traffic management and reduce their carbon footprint, making them livable and environmentally friendly.",
        "DOI": "10.3390/su15139859",
        "affiliation_name": "Mewar University",
        "affiliation_city": "Chittorgarh",
        "affiliation_country": "India"
    },
    {
        "paper_title": "An Estimation of Daily PM2.5 Concentration in Thailand Using Satellite Data at 1-Kilometer Resolution",
        "paper_author": "Buya S.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "11",
        "cover_date": "2023-07-01",
        "Abstract": "This study addresses the limited coverage of regulatory monitoring for particulate matter 2.5 microns or less in diameter (PM2.5) in Thailand due to the lack of ground station data by developing a model to estimate daily PM2.5 concentrations in small regions of Thailand using satellite data at a 1-km resolution. The study employs multiple linear regression and three machine learning models and finds that the random forest model performs the best for PM2.5 estimation over the period of 2011–2020. The model incorporates several factors such as Aerosol Optical Depth (AOD), Land Surface Temperature (LST), Normalized Difference Vegetation Index (NDVI), Elevation (EV), Week of the year (WOY), and year and applies them to the entire region of Thailand without relying on monitoring station data. Model performance is evaluated using the coefficient of determination (R2) and root mean square error (RMSE), and the results indicate high accuracy for training (R2: 0.95, RMSE: 5.58 μg/m3), validation (R2: 0.78, RMSE: 11.18 μg/m3), and testing (R2: 0.71, RMSE: 8.79 μg/m3) data. These PM2.5 data can be used to analyze the short- and long-term effects of PM2.5 on population health and inform government policy decisions and effective mitigation strategies.",
        "DOI": "10.3390/su151310024",
        "affiliation_name": "Thailand National Electronics and Computer Technology Center",
        "affiliation_city": "Pathum Thani",
        "affiliation_country": "Thailand"
    },
    {
        "paper_title": "Distributed Dynamic Pricing Strategy Based on Deep Reinforcement Learning Approach in a Presale Mechanism",
        "paper_author": "Liang Y.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "1",
        "cover_date": "2023-07-01",
        "Abstract": "Despite the emergence of a presale mechanism that reduces manufacturing and ordering risks for retailers, optimizing the real-time pricing strategy in this mechanism and unknown demand environment remains an unsolved issue. Consequently, we propose an automatic real-time pricing system for e-retailers under the inventory backlog impact in the presale mode, using deep reinforcement learning technology based on the Dueling DQN algorithm. This system models the multicycle pricing problem with a finite sales horizon as a Markov decision process (MDP) to cope with the uncertain environment. We train and evaluate the proposed environment and agent in a simulation environment and compare it with two tabular reinforcement learning algorithms (Q-learning and SARSA). The computational results demonstrate that our proposed real-time pricing learning framework for joint inventory impact can effectively maximize retailers’ profits and has universal applicability to a wide range of presale models. Furthermore, according to a series of experiments, we find that retailers should not neglect the impact of the presale or previous prices on consumers’ purchase behavior. If consumers pay more attention to past prices, the retailer must decrease the current price. When the cost of inventory backlog increases, they need to offer deeper discounts in the early selling period. Additionally, introducing blockchain technology can improve the transparency of commodity traceability information, thus increasing consumer demand for purchase.",
        "DOI": "10.3390/su151310480",
        "affiliation_name": "Guangdong University of Finance &amp; Economics",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Anomaly Detection in Endemic Disease Surveillance Data Using Machine Learning Techniques",
        "paper_author": "Eze P.U.",
        "publication": "Healthcare (Switzerland)",
        "citied_by": "9",
        "cover_date": "2023-07-01",
        "Abstract": "Disease surveillance is used to monitor ongoing control activities, detect early outbreaks, and inform intervention priorities and policies. However, data from disease surveillance that could be used to support real-time decisionmaking remain largely underutilised. Using the Brazilian Amazon malaria surveillance dataset as a case study, in this paper we explore the potential for unsupervised anomaly detection machine learning techniques to discover signals of epidemiological interest. We found that our models were able to provide an early indication of outbreak onset, outbreak peaks, and change points in the proportion of positive malaria cases. Specifically, the sustained rise in malaria in the Brazilian Amazon in 2016 was flagged by several models. We found that no single model detected all anomalies across all health regions. Because of this, we provide the minimum number of machine learning models top-k models) to maximise the number of anomalies detected across different health regions. We discovered that the top three models that maximise the coverage of the number and types of anomalies detected across the thirteen health regions are principal component analysis, stochastic outlier selection, and the minimum covariance determinant. Anomaly detection is a potentially valuable approach to discovering patterns of epidemiological importance when confronted with a large volume of data across space and time. Our exploratory approach can be replicated for other diseases and locations to inform monitoring, timely interventions, and actions towards the goal of controlling endemic disease.",
        "DOI": "10.3390/healthcare11131896",
        "affiliation_name": "School of Computing and Information Systems",
        "affiliation_city": "Melbourne",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Torque Ripple Minimization of Variable Reluctance Motor Using Reinforcement Dual NNs Learning Architecture",
        "paper_author": "Alharkan H.",
        "publication": "Energies",
        "citied_by": "4",
        "cover_date": "2023-07-01",
        "Abstract": "The torque ripples in a switched reluctance motor (SRM) are minimized via an optimal adaptive dynamic regulator that is presented in this research. A novel reinforcement neural network learning approach based on machine learning is adopted to find the best solution for the tracking problem of the SRM drive in real time. The reference signal model which minimizes the torque pulsations is combined with tracking error to construct the augmented structure of the SRM drive. A discounted cost function for the augmented SRM model is described to assess the tracking performance of the signal. In order to track the optimal trajectory, a neural network (NN)-based RL approach has been developed. This method achieves the optimal tracking response to the Hamilton–Jacobi–Bellman (HJB) equation for a nonlinear tracking system. To do so, two neural networks (NNs) have been trained online individually to acquire the best control policy to allow tracking performance for the motor. Simulation findings have been undertaken for SRM to confirm the viability of the suggested control strategy.",
        "DOI": "10.3390/en16134839",
        "affiliation_name": "Qassim University",
        "affiliation_city": "Al-Mulida",
        "affiliation_country": "Saudi Arabia"
    },
    {
        "paper_title": "A Supply Chain Model with Carbon Emissions and Preservation Technology for Deteriorating Items under Trade Credit Policy and Learning in Fuzzy",
        "paper_author": "Alamri O.A.",
        "publication": "Mathematics",
        "citied_by": "0",
        "cover_date": "2023-07-01",
        "Abstract": "In this study, a supply chain model is proposed with preservation technology under learning fuzzy theory for deteriorating items where the demand rate depends on the selling price and also treats as a triangular fuzzy number. The deterioration rate of any item cannot be eliminated due to its natural process, but it can be controlled with the help of preservation technology. Some harmful gases are emitted during the preservation process due to deteriorating items that harm the environment. In general, it can be easily seen that most of the sellers offer a trade credit policy to their regular buyers. In this paper, the retailer’s inventory stock reduces due to demand and deterioration. It is also assumed that some units are defective due to machine defects or delivery inefficiency. The retailer accepted the policy of trade credit offered by the seller. The aim of this paper is to enhance the profit of the supply chain partners. We proposed a theorem to get the optimal values of the selling price and cycle length. The retailer’s total profit is a function of selling price and cycle length, and the retailer’s total profit is optimized with respect to selling price and cycle length under trade-credit. Numerical examples are also presented for the validation of the present study, and sensitivity analysis is also discussed to know the robustness of the supply chain model. Managerial insight and observation have been given in the sensitivity section. Limitations and future work of this paper have been presented in the conclusion section.",
        "DOI": "10.3390/math11132946",
        "affiliation_name": "University of Tabuk",
        "affiliation_city": "Tabuk",
        "affiliation_country": "Saudi Arabia"
    },
    {
        "paper_title": "Integration of High-Accuracy Geospatial Data and Machine Learning Approaches for Soil Erosion Susceptibility Mapping in the Mediterranean Region: A Case Study of the Macta Basin, Algeria",
        "paper_author": "Bouguerra H.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "7",
        "cover_date": "2023-07-01",
        "Abstract": "Erosion can have a negative impact on the agricultural sustainability and grazing lands in the Mediterranean area, especially in northern Algeria. It is useful to map the spatial occurrence of erosion and identify susceptible erodible areas on large scale. The main objective of this research was to compare the performance of four machine learning techniques: Categorical boosting, Adaptive boosting, Convolutional Neural Network, and stacking ensemble models to predict the occurrence of erosion in the Macta basin, northwestern Algeria. Several climatologic, morphologic, hydrological, and geological factors based on multi-sources data were elaborated in GIS environment to determine the erosion factors in the studied area. The conditioning factors encompassing rainfall erosivity, slope, aspect, elevation, LULC, topographic wetness index, distance from river, distance from roads, clay mineral ratio, lithology, and geology were derived via the integration of topographic attributes and remote sensing data including Landsat 8 and Sentinel 2 within a GIS framework. The inventory map of soil erosion was created by integrating data from the global positioning system to locate erosion sites, conducting extensive field surveys, and analyzing satellite images obtained from Google Earth through visual interpretation. The dataset was divided randomly into two sets with 60% for training and calibrating and 40% for testing the models. Statistical metrics including sensitivity, specificity, accuracy, and the area under the receiver operating characteristic curve (ROC) were used to assess the validity of the proposed models. The results revealed that machine learning and deep learning, as well stacking ensemble techniques, showed outstanding performance with accuracy over 98% with sensitivity 0.98 and specificity 0.98. Policy makers and local authorities can utilize the predicted erosion susceptibility maps to promote sustainable use of water and soil conservation and safeguard agricultural activities against potential damage.",
        "DOI": "10.3390/su151310388",
        "affiliation_name": "Sveučilište u Zagrebu, Gradevinski Fakultet",
        "affiliation_city": "Zagreb",
        "affiliation_country": "Croatia"
    },
    {
        "paper_title": "Developing an Optimal Ensemble Model to Estimate Building Demolition Waste Generation Rate",
        "paper_author": "Cha G.W.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "5",
        "cover_date": "2023-07-01",
        "Abstract": "Smart management of construction and demolition (C&D) waste is imperative, and researchers have implemented machine learning for estimating waste generation. In Korea, the management of demolition waste (DW) is important due to old buildings, and it is necessary to predict the amount of DW to manage it. Thus, this study employed decision tree (DT)-based ensemble models (i.e., random forest—RF, extremely randomized trees—ET, gradient boosting machine—GBM), and extreme gradient boost—XGboost) based on data characteristics (i.e., small datasets with categorical inputs) to predict the demolition waste generation rate (DWGR) of buildings in urban redevelopment areas. As a result of the study, the RF and GBM algorithms showed better prediction performance than the ET and XGboost algorithms. Especially, RF (6 features, 450 estimators; mean, 1169.94 kg·m−2) and GBM (4 features, 300 estimators; mean, 1166.25 kg·m−2) yielded the top predictive performances. In addition, feature importance affecting DWGR was found to have a significant impact on the order of gross floor area (GFA) > location > roof material > wall material. The straightforward collection of features used here can facilitate benchmarking as a decision-making tool in demolition waste management plans for industry stakeholders and policy makers. Therefore, in the future, it is required to improve the predictive performance of the model by updating additional data and building a reliable dataset.",
        "DOI": "10.3390/su151310163",
        "affiliation_name": "Dongguk University, Gyeongju",
        "affiliation_city": "Gyeongju",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "Content Caching and Distribution Policies for Vehicular Ad-Hoc Networks (VANETs): Modeling and Simulation",
        "paper_author": "Kilanioti I.",
        "publication": "Electronics (Switzerland)",
        "citied_by": "0",
        "cover_date": "2023-07-01",
        "Abstract": "The paper studies the application of various content distribution policies for vehicular ad hoc networks (VANETs) and compares their effectiveness under various simulation scenarios. Our implementation augments the existing Veins tool, an open source framework for vehicular network simulations based on the discrete event simulator OMNET++ and SUMO, a tool that simulates traffic on road networks. The proposed solution integrates various additional features into the pre-existing Veins realizations and expands them to include the modeling and implementation of proposed caching and content distribution policies and the measurement of respective metrics. Moreover, we integrate machine learning algorithms for distribution policies into the simulation framework in order to efficiently study distribution of content to the network nodes. These algorithms are pre-trained neural network models adapted for VANETs. Using these new functions, we can specify the simulation parameters, run a plethora of experiments and proceed to evaluate metrics and policies for content distribution.",
        "DOI": "10.3390/electronics12132901",
        "affiliation_name": "National Technical University of Athens (NTUA)",
        "affiliation_city": "Athens",
        "affiliation_country": "Greece"
    },
    {
        "paper_title": "Characterisation of Youth Entrepreneurship in Medellín-Colombia Using Machine Learning",
        "paper_author": "Ojeda-Beltrán A.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "2",
        "cover_date": "2023-07-01",
        "Abstract": "The aim of this paper is to identify profiles of young Colombian entrepreneurs based on data from the “Youth Entrepreneurship” survey developed by the Colombian Youth Secretariat. Our research results show five profiles of entrepreneurs, mainly differentiated by age and entrepreneurial motives, as well as the identification of relevant skills, capacities, and capabilities for entrepreneurship, such as creativity, learning, and leadership. The sample consists of 633 young people aged between 14 and 28 years in Medellín. The data treatment was approached through cluster analysis using the K-means algorithm to obtain information about the underlying nature and structure of the data. These data analysis techniques provide valuable information that can help to better understand the behaviour of Colombian entrepreneurs. They also reveal hidden information in the data. Therefore, one of the advantages of using statistical and artificial intelligence techniques in this type of study is to extract valuable information that might otherwise go unnoticed. The clusters generated show correlations with profiles that can support the design of policies in Colombia to promote an entrepreneurial ecosystem and the creation and development of new businesses through business regulation.",
        "DOI": "10.3390/su151310297",
        "affiliation_name": "Universidad Nacional Abierta y a Distancia",
        "affiliation_city": "Bogota",
        "affiliation_country": "Colombia"
    },
    {
        "paper_title": "Spatiotemporal Thermal Variations in Moroccan Cities: A Comparative Analysis",
        "paper_author": "Derdouri A.",
        "publication": "Sensors",
        "citied_by": "10",
        "cover_date": "2023-07-01",
        "Abstract": "This study examines the Land Surface Temperature (LST) trends in eight key Moroccan cities from 1990 to 2020, emphasizing the influential factors and disparities between coastal and inland areas. Geographically weighted regression (GWR), machine learning (ML) algorithms, namely XGBoost and LightGBM, and SHapley Additive exPlanations (SHAP) methods are utilized. The study observes that urban areas are often cooler due to the presence of urban heat sinks (UHSs), more noticeably in coastal cities. However, LST is seen to increase across all cities due to urbanization and the degradation of vegetation cover. The increase in LST is more pronounced in inland cities surrounded by barren landscapes. Interestingly, XGBoost frequently outperforms LightGBM in the analyses. ML models and SHAP demonstrate efficacy in deciphering urban heat dynamics despite data quality and model tuning challenges. The study’s results highlight the crucial role of ongoing urbanization, topography, and the existence of water bodies and vegetation in driving LST dynamics. These findings underscore the importance of sustainable urban planning and vegetation cover in mitigating urban heat, thus having significant policy implications. Despite its contributions, this study acknowledges certain limitations, primarily the use of data from only four discrete years, thereby overlooking inter-annual, seasonal, and diurnal variations in LST dynamics.",
        "DOI": "10.3390/s23136229",
        "affiliation_name": "University of Tsukuba",
        "affiliation_city": "Tsukuba",
        "affiliation_country": "Japan"
    },
    {
        "paper_title": "Exploiting Digitalization of Solar PV Plants Using Machine Learning: Digital Twin Concept for Operation",
        "paper_author": "Yalçin T.",
        "publication": "Energies",
        "citied_by": "16",
        "cover_date": "2023-07-01",
        "Abstract": "The rapid development of digital technologies and solutions is disrupting the energy sector. In this regard, digitalization is a facilitator and enabler for integrating renewable energies, management and operation. Among these, advanced monitoring techniques and artificial intelligence may be applied in solar PV plants to improve their operation and efficiency and detect potential malfunctions at an early stage. This paper proposes a Digital Twin DT concept, mainly focused on O&M, to obtain more information about the system by using several artificial intelligence boxes. Furthermore, it includes the development of several machine learning (ML) algorithms capable of reproducing the expected behavior of the solar PV plant and detecting the malfunctioning of different components. In this regard, this allows for reducing downtime and optimizing asset management. In this paper, different ML techniques are used and compared to optimize the selected methods for enhanced response. The paper presents all stages of the developed Digital Twin, including ML model development with an accuracy of 98.3% of the whole DT, and finally, a communication and visualization platform. The different responses and comparisons have been made using a model based on MATLAB/Simulink using different cases and system conditions.",
        "DOI": "10.3390/en16135044",
        "affiliation_name": "Adana Alparslan Türkeş Science and Technology University",
        "affiliation_city": "Adana",
        "affiliation_country": "Turkey"
    },
    {
        "paper_title": "Carbon Peak Scenario Simulation of Manufacturing Carbon Emissions in Northeast China: Perspective of Structure Optimization",
        "paper_author": "Xu C.",
        "publication": "Energies",
        "citied_by": "5",
        "cover_date": "2023-07-01",
        "Abstract": "The manufacturing industry is the pillar industry of China’s economy and a major carbon emitter, and its carbon emission reduction efforts directly determine whether the country’s carbon emission reduction target can be successfully met. In the context of the goals of the carbon peak and carbon neutrality policy, we examine the impact of manufacturing structure optimization on carbon emissions from 2003 to 2020 through a spatial econometric model, taking the old industrial centers in Northeast China as an example. We then apply a machine learning model to simulate manufacturing carbon emissions during the carbon peak stage and identify the optimal path for carbon emission reduction, which is important for promoting manufacturing carbon emission reduction in Northeast China. Since the goal of low-carbon economic development has gradually replaced the goal of maximizing economic efficiency in recent years, manufacturing structure optimization has come to focus on energy saving and emission reduction. Therefore, we define manufacturing structure optimization from the dual perspective of technology and energy consumption to broaden the existing research perspective. The results show the following: (1) The overall trend in manufacturing structure optimization in Northeast China is steadily improving, and the level of manufacturing structure optimization from the technology perspective is higher than that from the energy consumption perspective. (2) Manufacturing structure optimization and manufacturing carbon emissions in Northeast China both show a positive spatial correlation. Manufacturing structure optimization in Northeast China can effectively promote carbon emission reduction, and it also has a spatial spillover effect. (3) The carbon emission reduction effect of manufacturing structure optimization from the energy consumption perspective is better than that from the technology perspective, and the carbon emission reduction effect under the institutional innovation scenario is better than that under the baseline scenario and the technological innovation scenario. Focusing on manufacturing structure optimization from both technology and energy consumption perspectives, as well as continuously improving technological innovation and institutional innovation, can help to achieve manufacturing carbon emission reduction in Northeast China.",
        "DOI": "10.3390/en16135227",
        "affiliation_name": "Northeast Normal University",
        "affiliation_city": "Changchun",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Debiased Machine Learning for Estimating the Causal Effect of Urban Traffic on Pedestrian Crossing Behavior",
        "paper_author": "Kamal K.",
        "publication": "Transportation Research Record",
        "citied_by": "2",
        "cover_date": "2023-07-01",
        "Abstract": "Before the transition of automated vehicles (AVs) to urban roads and subsequently unprecedented changes in traffic conditions, the evaluation of transportation policies and futuristic road design related to pedestrian crossing behavior is of vital importance. Recent studies analyzed the non-causal impact of various variables on pedestrian waiting time in the presence of AVs. However, we mainly investigate the causal effect of traffic density on pedestrian waiting time. We develop a double/debiased machine learning (DML) model in which the impact of the confounders variable influencing both a policy and an outcome of interest is addressed, resulting in unbiased policy evaluation. Furthermore, we try to analyze the effect of traffic density by developing a copula-based joint model of the two main components of pedestrian crossing behavior, pedestrian stress level and waiting time. The copula approach has been widely used in the literature for addressing self-selection problems, which can be classified as a causality analysis in travel behavior modeling. The results obtained from copula approach and DML are compared based on the effect of traffic density. In the DML model structure, the standard error term of the density parameter is lower than that of the copula approach and the confidence interval is considerably more reliable. In addition, despite the similar sign of effect, the copula approach estimates the effect of traffic density lower than DML, because of the spurious effect of the confounders. In short, the DML model structure can flexibly adjust the impact of confounders by using machine learning algorithms and is more reliable for planning future policies.",
        "DOI": "10.1177/03611981231152246",
        "affiliation_name": "Toronto Metropolitan University",
        "affiliation_city": "Toronto",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "A large scale randomized controlled trial on herding in peer-review discussions",
        "paper_author": "Stelmakh I.",
        "publication": "PLoS ONE",
        "citied_by": "6",
        "cover_date": "2023-07-01",
        "Abstract": "Peer review is the backbone of academia and humans constitute a cornerstone of this process, being responsible for reviewing submissions and making the final acceptance/rejection decisions. Given that human decision-making is known to be susceptible to various cognitive biases, it is important to understand which (if any) biases are present in the peer-review process, and design the pipeline such that the impact of these biases is minimized. In this work, we focus on the dynamics of discussions between reviewers and investigate the presence of herding behaviour therein. Specifically, we aim to understand whether reviewers and discussion chairs get disproportionately influenced by the first argument presented in the discussion when (in case of reviewers) they form an independent opinion about the paper before discussing it with others. In conjunction with the review process of a large, top tier machine learning conference, we design and execute a randomized controlled trial that involves 1,544 papers and 2,797 reviewers with the goal of testing for the conditional causal effect of the discussion initiator’s opinion on the outcome of a paper. Our experiment reveals no evidence of herding in peer-review discussions. This observation is in contrast with past work that has documented an undue influence of the first piece of information on the final decision (e.g., anchoring effect) and analyzed herding behaviour in other applications (e.g., financial markets). Regarding policy implications, the absence of the herding effect suggests that the current status quo of the absence of a unified policy towards discussion initiation does not result in an increased arbitrariness of the resulting decisions.",
        "DOI": "10.1371/journal.pone.0287443",
        "affiliation_name": "Department of Computer Science",
        "affiliation_city": "College Park",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Hidden hazards and screening policy: Predicting undetected lead exposure in Illinois",
        "paper_author": "Abbasi A.",
        "publication": "Journal of Health Economics",
        "citied_by": "1",
        "cover_date": "2023-07-01",
        "Abstract": "Lead exposure still threatens children's health despite policies aiming to identify lead exposure sources. Some US states require de jure universal screening while others target screening, but little research examines the relative benefits of these approaches. We link lead tests for children born in Illinois between 2010 and 2014 to geocoded birth records and potential exposure sources. We train a random forest regression model that predicts children's blood lead levels (BLLs) to estimate the geographic distribution of undetected lead poisoning. We use these estimates to compare de jure universal screening against targeted screening. Because no policy achieves perfect compliance, we analyze different incremental screening expansions. We estimate that 5,819 untested children had a BLL ≥5 μg/dL, in addition to the 18,101 detected cases. 80% of these undetected cases should have been screened under the current policy. Model-based targeted screening can improve upon both the status quo and expanded universal screening.",
        "DOI": "10.1016/j.jhealeco.2023.102783",
        "affiliation_name": "University of Oxford",
        "affiliation_city": "Oxford",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Explainable machine learning models to analyse maternal health",
        "paper_author": "Patel S.S.",
        "publication": "Data and Knowledge Engineering",
        "citied_by": "10",
        "cover_date": "2023-07-01",
        "Abstract": "Maternal health is a significant public health concern for globe and many developing countries. A country like India (with large population), there are considerable disparities in maternal health service utilisation and maternal mortality within and across states. A more than a general healthcare operational policy would suffice, but a precision healthcare strategy would be needed. This article focused on explainable machine learning models that can precisely advise health care intervention policy and medical treatment to an administrative unit rather than a generic policy suggestion for improving maternal health. This study presents an exhaustive list of factors associated with Maternal Mortality Rate (MMR) and a series of explainable AI models. One of models uses CART heuristics to categorise districts (administrative boundaries) into lower and higher MMR classes. Another explainable model, Shapley Additive Explanations (SHAP), used SVM, ANN, boosting, and random forest machine learning models to investigate higher and lower MMR regions. Further, an Explainable Boosting Machine (EBM) also used, and the results are compared for policy suggestions. Some of the ignored features from general social science studies, such as topography and agro-climatic zone characteristics of a particular district, may be crucial in the analysis. Moreover, health infrastructure, insurance, and other factors also influence policymaking. This predictive and explainable work has significant implications for precision healthcare policy design to improve maternal health compared to a broader policy approach.",
        "DOI": "10.1016/j.datak.2023.102198",
        "affiliation_name": "Indian Institute of Management, Visakhapatnam",
        "affiliation_city": "Visakhapatnam",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Mortality Prediction in Severe Traumatic Brain Injury Using Traditional and Machine Learning Algorithms",
        "paper_author": "Wu X.",
        "publication": "Journal of Neurotrauma",
        "citied_by": "17",
        "cover_date": "2023-07-01",
        "Abstract": "Prognostic prediction of traumatic brain injury (TBI) in patients is crucial in clinical decision and health care policy making. This study aimed to develop and validate prediction models for in-hospital mortality after severe traumatic brain injury (sTBI). We developed and validated logistic regression (LR), LASSO regression, and machine learning (ML) algorithms including support vector machines (SVM) and XGBoost models. Fifty-four candidate predictors were included. Model performance was expressed in terms of discrimination (C-statistic) and calibration (intercept and slope). For model development, 2804 patients with sTBI in the Collaborative European NeuroTrauma Effectiveness Research in TBI (CENTER-TBI) China Registry study were included. External validation was performed in 1113 patients with sTBI in the CENTER-TBI European Registry study. XGBoost achieved high discrimination in mortality prediction, and it outperformed logistic and LASSO regression. The XGBoost model established in this study also outperformed prediction models currently available, including the International Mission for Prognosis and Analysis of Clinical Trials (IMPACT) core and International Mission for Prognosis and Analysis of Clinical Trials (CRASH) basic models. When including 54 variables, XGBoost and SVM reached C-statistics of 0.87 (95% confidence interval [CI]: 0.81-0.92) and 0.85 (95% CI: 0.79-0.90) at internal validation, and 0.88 (95% CI: 0.87-0.88) and 0.86 (95% CI: 0.85-0.87) at external validation, respectively. A simplified version of XGBoost and SVM using 26 variables selected by recursive feature elimination (RFE) reached C-statistics of 0.87 (95% CI: 0.82-0.92) and 0.86 (95% CI: 0.80-0.91) at internal validation, and 0.87 (95% CI: 0.87-0.88) and 0.87 (95% CI: 0.86-0.87) at external validation, respectively. However, when the number of variables included decreased, the difference between ML and LR diminished. All the prediction models can be accessed via a web-based calculator. Glasgow Coma Scale (GCS) score, age, pupillary light reflex, Injury Severity Score (ISS) for brain region, and the presence of acute subdural hematoma were the five strongest predictors for mortality prediction. The study showed that ML techniques such as XGBoost may capture information hidden in demographic and clinical predictors of patients with sTBI and yield more precise predictions compared with LR approaches.",
        "DOI": "10.1089/neu.2022.0221",
        "affiliation_name": "Faculty of Medicine, Dentistry and Health",
        "affiliation_city": "Sheffield",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "AI and IP: Theory to Policy and Back Again – Policy and Research Recommendations at the Intersection of Artificial Intelligence and Intellectual Property",
        "paper_author": "Picht P.G.",
        "publication": "IIC International Review of Intellectual Property and Competition Law",
        "citied_by": "12",
        "cover_date": "2023-07-01",
        "Abstract": "The interaction between artificial intelligence and intellectual property rights (IPRs) is one of the key areas of development in intellectual property law. After much, albeit selective, debate, it seems to be gaining increasing practical relevance through intense AI-related market activity, an initial set of case law on the matter, and policy initiatives by international organizations and lawmakers. Against this background, Zurich University’s Center for Intellectual Property and Competition Law is conducting, together with the Swiss Intellectual Property Institute, a research and policy project that explores the future of intellectual property law in an AI context. This paper briefly describes the AI/IP Research Project and presents an initial set of policy recommendations for the development of IP law with a view to AI. The recommendations address topics such as AI inventorship in patent law; AI authorship in copyright law; the need for sui generis rights to protect innovative AI output; rules for the allocation of AI-related IPRs; IP protection carve-outs in order to facilitate AI system development, training, and testing; the use of AI tools by IP offices; and suitable software protection and data usage regimes.",
        "DOI": "10.1007/s40319-023-01344-5",
        "affiliation_name": "Universität Zürich",
        "affiliation_city": "Zurich",
        "affiliation_country": "Switzerland"
    },
    {
        "paper_title": "Rapid opioid overdose response system technologies",
        "paper_author": "Tay Wee Teck J.",
        "publication": "Current Opinion in Psychiatry",
        "citied_by": "7",
        "cover_date": "2023-07-01",
        "Abstract": "Purpose of reviewOpioid overdose events are a time sensitive medical emergency, which is often reversible with naloxone administration if detected in time. Many countries are facing rising opioid overdose deaths and have been implementing rapid opioid overdose response Systems (ROORS). We describe how technology is increasingly being used in ROORS design, implementation and delivery.Recent findingsTechnology can contribute in significant ways to ROORS design, implementation, and delivery. Artificial intelligence-based modelling and simulations alongside wastewater-based epidemiology can be used to inform policy decisions around naloxone access laws and effective naloxone distribution strategies. Data linkage and machine learning projects can support service delivery organizations to mobilize and distribute community resources in support of ROORS. Digital phenotyping is an advancement in data linkage and machine learning projects, potentially leading to precision overdose responses. At the coalface, opioid overdose detection devices through fixed location or wearable sensors, improved connectivity, smartphone applications and drone-based emergency naloxone delivery all have a role in improving outcomes from opioid overdose. Data driven technologies also have an important role in empowering community responses to opioid overdose.SummaryThis review highlights the importance of technology applied to every aspect of ROORS. Key areas of development include the need to protect marginalized groups from algorithmic bias, a better understanding of individual overdose trajectories and new reversal agents and improved drug delivery methods.",
        "DOI": "10.1097/YCO.0000000000000870",
        "affiliation_name": "School of Medicine",
        "affiliation_city": "St Andrews",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "A machine learning approach for mapping surface urban heat island using environmental and socioeconomic variables: a case study in a medium-sized Brazilian city",
        "paper_author": "Furuya M.T.G.",
        "publication": "Environmental Earth Sciences",
        "citied_by": "10",
        "cover_date": "2023-07-01",
        "Abstract": "Smart cities must deal with climate change and find solutions to mitigate phenomena such as urban heat islands (UHI). The land surface temperature (LST) extracted from thermal images is a primary source of information to study UHI, characterizing the surface urban heat islands (SUHI). In addition to LST, environmental and socioeconomic variables have been adopted to explain the SUHI phenomenon. Although machine learning algorithms have potential in several areas, their application in the study of the contribution of these variables in the prediction of LST to characterize SUHI is still unknown. Therefore, the work proposes a machine learning approach to fill this gap. The LST was extracted from 15 Landsat 8 images from 2019 to 2021. Data on socioeconomic variables were obtained from the official demographic census, and environmental variables were extracted from Sentinel-2 and Planet images. Six algorithms were tested to assess the ability to estimate the LST based on the above-mentioned variables. The results showed that the Decision Tree algorithm had the best performance (r = 0.96, MAE = 1.49 °C and RMSE = 1.88 °C), followed by Random Forest. In addition, the inclusion of all seasons of the year and socioeconomic variables was shown to be relevant to the results. The main contribution of this work is to verify if the algorithms can optimize the SUHI characterization process, analyzing the influence exerted by the studied variables. In the social sphere, the information produced can help urban planning in the construction of smart cities.",
        "DOI": "10.1007/s12665-023-11017-8",
        "affiliation_name": "Universidade de Brasília",
        "affiliation_city": "Brasilia",
        "affiliation_country": "Brazil"
    },
    {
        "paper_title": "Meteorological data source comparison—a case study in geospatial modeling of potential environmental exposure to abandoned uranium mine sites in the Navajo Nation",
        "paper_author": "Girlamo C.",
        "publication": "Environmental Monitoring and Assessment",
        "citied_by": "1",
        "cover_date": "2023-07-01",
        "Abstract": "Meteorological (MET) data is a crucial input for environmental exposure models. While modeling exposure potential using geospatial technology is a common practice, existing studies infrequently evaluate the impact of input MET data on the level of uncertainty on output results. The objective of this study is to determine the effect of various MET data sources on the potential exposure susceptibility predictions. Three sources of wind data are compared: The North American Regional Reanalysis (NARR) database, meteorological aerodrome reports (METARs) from regional airports, and data from local MET weather stations. These data sources are used as inputs into a machine learning (ML) driven GIS Multi-Criteria Decision Analysis (GIS-MCDA) geospatial model to predict potential exposure to abandoned uranium mine sites in the Navajo Nation. Results indicate significant variations in results derived from different wind data sources. After validating the results from each source using the National Uranium Resource Evaluation (NURE) database in a geographically weighted regression (GWR), METARs data combined with the local MET weather station data showed the highest accuracy, with an average R 2 of 0.74. We conclude that local direct measurement-based data (METARs and MET data) produce a more accurate prediction than the other sources evaluated in the study. This study has the potential to inform future data collection methods, leading to more accurate predictions and better-informed policy decisions surrounding environmental exposure susceptibility and risk assessment.",
        "DOI": "10.1007/s10661-023-11283-w",
        "affiliation_name": "University of New Mexico School of Engineering",
        "affiliation_city": "Albuquerque",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Heavy metal pollution in Mongolian-Manchurian grassland soil and effect of long-range dust transport by wind",
        "paper_author": "Qin M.",
        "publication": "Environment International",
        "citied_by": "28",
        "cover_date": "2023-07-01",
        "Abstract": "Grasslands provide a range of valuable ecosystem services, but they are also particularly fragile ecosystems easily threatened by human activities, such as long-term open-pit mining and related industrial activities. In grassland area, dust containing heavy metal(loid)s generated by mines may further migrate to remote places, but few studies have focused on the long-range transport of contaminants as an important pollution source. In the present study, one of the largest and most intact grassland ecosystems, the Mongolian-Manchurian steppe, was selected to investigate its pollution status and track potential sources. A total of 150 soil samples were collected to explore reginal distribution of nine heavy metal(loid)s that has potential risk in grassland. We conducted a combined multi-variant analysis of positive matrix factorization (PMF) and machine learning, which foregrounded the source of long-range transport of contaminants and inspired the hypothesis of a novel stochastic model to describe contaminants distribution. Results showed four different sources accounting for 44.44% (parent material), 20.28% (atmospheric deposition), 20.39% (farming), and 14.89% (transportation) of the total concentration, respectively. Factor 2 indicated that coal surface mining lead to a significant enrichment of As and Se with their concentration far above the global average level, which was different from other reported grassland areas. Machine learning results further confirmed that atmospheric and topographic features were their contamination controlling factors. The model results proposed that As, Se and Cu released by surface mining will be transported over long distance under prevailing monsoon, until finally deposited in the windward slope of mountain due to terrain obstruction. The long-range transport by wind and deposition of contaminants may be a prevailing phenomenon in temperate grassland, making it a pollution source that cannot be ignored. Evidence from this study reveals the urgency of precautions for fragile grassland ecosystems around industrial areas and provides a basis for its management and risk control policies.",
        "DOI": "10.1016/j.envint.2023.108019",
        "affiliation_name": "Ministry of Natural Resources of the People's Republic of China",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Random forest regression on joint role of meteorological variables, demographic factors, and policy response measures in COVID-19 daily cases: global analysis in different climate zones",
        "paper_author": "Lyu Y.",
        "publication": "Environmental Science and Pollution Research",
        "citied_by": "4",
        "cover_date": "2023-07-01",
        "Abstract": "Different sources of factors in environment can affect the spread of COVID-19 by influencing the diffusion of the virus transmission, but the collective influence of which has hardly been considered. This study aimed to utilize a machine learning algorithm to assess the joint effects of meteorological variables, demographic factors, and government response measures on COVID-19 daily cases globally at city level. Random forest regression models showed that population density was the most crucial determinant for COVID-19 transmission, followed by meteorological variables and response measures. Ultraviolet radiation and temperature dominated meteorological factors, but the associations with daily cases varied across different climate zones. Policy response measures have lag effect in containing the epidemic development, and the pandemic was more effectively contained with stricter response measures implemented, but the generalized measures might not be applicable to all climate conditions. This study explored the roles of demographic factors, meteorological variables, and policy response measures in the transmission of COVID-19, and provided evidence for policymakers that the design of appropriate policies for prevention and preparedness of future pandemics should be based on local climate conditions, population characteristics, and social activity characteristics. Future work should focus on discerning the interactions between numerous factors affecting COVID-19 transmission.",
        "DOI": "10.1007/s11356-023-27320-7",
        "affiliation_name": "Chinese Center for Disease Control and Prevention",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Comparing and contrasting choice model and machine learning techniques in the context of vehicle ownership decisions",
        "paper_author": "Ali A.",
        "publication": "Transportation Research Part A: Policy and Practice",
        "citied_by": "9",
        "cover_date": "2023-07-01",
        "Abstract": "In recent years, planners have started considering Machine Learning (ML) techniques as an alternative to discrete choice models (CM). ML techniques are primarily data-driven and typically achieve better prediction accuracy compared to CM. However, it is hypothesized that since the ML techniques do not have the strong grounding to economic theory as the CMs, they may not perform well in contexts that are radically different from the ‘training’ scenario. It is also hypothesized that the relative prediction performance may be affected by the metrics used for comparing the models. This research aims to test these two hypotheses empirically by modelling vehicle ownership choices using household survey data from Dhaka, Bangladesh collected in 2004, 2010 and 2019. The performances of CM (multinomial logit) and ML techniques (neural networks and gradient boosting trees) have been compared using log-likelihood and mean absolute percentage error of market shares. The results indicate that the multinomial logit model (MNL) with a piecewise linear transformation of the household income, has the best performance in terms of log-likelihood and mean absolute percentage error of market shares. This is followed by Neural Networks (NN) and Gradient Boosting Trees (GBT). The results thus provide empirical evidence that the ML techniques do not consistently outperform CM. Moreover, the difference in the performance of the models further increases if the prediction scenario is substantially different. This reinforces the hypothesis that CMs, with their behavioural underpinning, are better suited for long-term forecasting than data-driven ML approaches, especially if the population and network attributes are expected to change substantially. These findings will be useful for planners and policy makers in the selection of the appropriate tool for forecasting travel demand.",
        "DOI": "10.1016/j.tra.2023.103727",
        "affiliation_name": "University of Leeds",
        "affiliation_city": "Leeds",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Carbon emissions predicting and decoupling analysis based on the PSO-ELM combined prediction model: evidence from Chongqing Municipality, China",
        "paper_author": "Liu B.",
        "publication": "Environmental Science and Pollution Research",
        "citied_by": "9",
        "cover_date": "2023-07-01",
        "Abstract": "The “14th Five-Year Plan” period is a crucial phase for China to achieve the goal of carbon peaking and carbon neutrality (referred to as the “double carbon”). Thus, it is very important to analyze the main factors affecting carbon emissions and accurately predict the change of carbon emissions to achieve the goal of double carbon. For the slow data updates and the low accuracy of traditional prediction models about the carbon emissions, the key factors of carbon emissions change selected by gray correlation method and the consumption of coal, oil, and natural gas were input into four single prediction models: gray prediction model GM(1,1), ridge regression, BP neural network, and WOA-BP neural network to obtain the fitted and predicted values of carbon emissions, which serve as input to the particle swarm optimization–extreme learning machine (PSO-ELM) model together. Based on the PSO-ELM combined prediction method above and the scenario prediction indicators constructed according to relevant policy documents of Chongqing Municipality, the carbon emission values of Chongqing Municipality during the 14th Five-Year Plan period are predicted in this paper. The empirical results show that carbon emissions of Chongqing Municipality still maintain an upward trend, but the growth rate slow down compared with 1998 to 2018. In general, the carbon emission and GDP of Chongqing Municipality showed a weak decoupling state during 1998 to 2025. By calculation, the PSO-ELM combined prediction model is superior to the above four single prediction models in carbon emission prediction and has good property by the robust testing. The research results can enrich the combined prediction method about the carbon emissions and provide policy suggestions for Chongqing’s low-carbon development during the 14th Five-Year Plan period.",
        "DOI": "10.1007/s11356-023-28022-w",
        "affiliation_name": "Chengdu University of Technology",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Resource Re-orchestration and firm survival in crisis periods: The role of business models of technology MNEs during COVID-19",
        "paper_author": "Attah-Boakye R.",
        "publication": "Technovation",
        "citied_by": "11",
        "cover_date": "2023-07-01",
        "Abstract": "Using data from world-leading digital-driven/technology multinational enterprises (DTMNEs), we draw from the resource orchestration theory to investigate the associations between business model (BM) drivers and firm performance during crisis periods. Drawing on data from the COVID-19 pandemic period, we deploy diverse analytical approaches including multivariate linear regressions and aggregated composite index statistical methods in examining how the BMs of our sampled DTMNEs drive firm performance. Our study highlights six methodological approaches that can be utilised by decision-makers in examining which variables in their BM drive better firm performance. Our findings revealed that the principal component analysis and multicriteria decision analysis (PROMETHEE methods) that espouse the use of aggregate composite index can provide significant and consistent predictive results in comparison to the traditional linear methods when examining the association between BM and firm performance during crisis periods. The paper provides policy and managerial implications on how firms and decision-makers can bolster business continuity, resilience, and plasticity by using analytical lenses that identify optimum resource orchestration during crises.",
        "DOI": "10.1016/j.technovation.2023.102769",
        "affiliation_name": "University of Huddersfield",
        "affiliation_city": "Huddersfield",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Learning Complex Motor Skills for Legged Robot Fall Recovery",
        "paper_author": "Yang C.",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "6",
        "cover_date": "2023-07-01",
        "Abstract": "Falling is inevitable for legged robots in challenging real-world scenarios, where environments are unstructured and situations are unpredictable, such as uneven terrain in the wild. Hence, to recover from falls and achieve all-terrain traversability, it is essential for intelligent robots to possess the complex motor skills required to resume operation. To go beyond the limitation of handcrafted control, we investigated a deep reinforcement learning approach to learn generalized feedback-control policies for fall recovery that are robust to external disturbances. We proposed a design guideline for selecting key states for initialization, including a comparison to the random state initialization. The proposed learning-based pipeline is applicable to different robot models and their corner cases, including both small-/large-size bipeds and quadrupeds. Further, we show that the learned fall recovery policies are hardware-feasible and can be implemented on real robots.",
        "DOI": "10.1109/LRA.2023.3281290",
        "affiliation_name": "UCL Engineering",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Customer retention and churn prediction in the telecommunication industry: a case study on a Danish university",
        "paper_author": "Saleh S.",
        "publication": "SN Applied Sciences",
        "citied_by": "14",
        "cover_date": "2023-07-01",
        "Abstract": "In this study, we explore the possible factors affecting churn in the Danish telecommunication industry and how those factors connect with retention strategies. The Danish telecommunication industry is experiencing a saturated market regarding the number of customers, but the number of service providers has increased significantly in recent years. Due to the high costs of acquiring new customers, the telecommunication industry put great emphasis on retaining customers in such an intensely competitive industry. We employ five machine learning algorithms: random forest, AdaBoost, logistic regression, extreme gradient boosting classifier, and decision tree classifier on four datasets from two geographical regions, Denmark and the USA. The first three datasets are from online repositories, and the last one contains responses from 311 students from Aalborg University collected through a survey. We identify key features extracted by the best-performing algorithms based on five performance measures. Based on that, we aggregate all the features that appear important for each dataset. The results demonstrate that customers’ preferences are not aligned. Among the prominent drivers, we find that service quality, customer satisfaction, offering subscription plan upgrades, and network coverage are unique to the Danish student population. Telecommunication companies need to integrate the sociohistoric milieu of the Nordic countries to tailor their retention policies to different consumer cultures.",
        "DOI": "10.1007/s42452-023-05389-6",
        "affiliation_name": "Aalborg University",
        "affiliation_city": "Aalborg",
        "affiliation_country": "Denmark"
    },
    {
        "paper_title": "Exploring key spatio-temporal features of crash risk hot spots on urban road network: A machine learning approach",
        "paper_author": "Wu P.",
        "publication": "Transportation Research Part A: Policy and Practice",
        "citied_by": "15",
        "cover_date": "2023-07-01",
        "Abstract": "Traffic safety is a critical factor that has always been considered in policy making for urban transportation planning and management. Accurately predicting crash risk hot spots allows urban transportation agencies to better implement countermeasures towards enhancing traffic safety. Considerable efforts have been devoted to investigate crash risk hot spots in many previous studies. We hereby identify three research gaps that remain to be resolved: first, the effects of spatio-temporal features surrounding hot spots are often ignored; second, false discovery rates tend to be higher when applying local spatial indices to identify hot spots; and third, the spatio-temporal correlations and heterogeneity of crash-related features in a subject spot and its neighboring spots have not been well captured in most crash risk prediction models. To fill these gaps, we propose an urban crash risk identification model by integrating space-time cubes and machine learning techniques. The spatio-temporal correlations and heterogeneity of crash-related features are represented by using statistical descriptions of neighboring cubes. Three space-time cube risk datasets collected from Manhattan in New York City are used to validate the proposed model in the case study. The eXtreme Gradient Boosting (XGBoost) classifier is employed to predict the risk patterns (hot spots, normal spots, and cold spots) of each cube due to its satisfactory prediction performance. The validation results suggest that our proposed model attains lower false discovery rates and higher crash risk prediction accuracy as compared to conventional methods. As the results of the feature selection are empowered by machine learning, we found that most key features are inherent to the features of spatial neighboring cubes, which manifests the importance of considering the features of neighboring spots when identifying crash risk hot spots. Moreover, SHapley Additive exPlanations (SHAP) are employed to interpret the effects of key features on hot spots, upon which the contributions of the features related to urban facilities, public transit, and land use are discussed. Based on the feature interpretation, several policy recommendations could be made to enhance urban road traffic safety in the future.",
        "DOI": "10.1016/j.tra.2023.103717",
        "affiliation_name": "School of Civil and Environmental Engineering",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore"
    },
    {
        "paper_title": "Using machine learning method to predict food waste in catering industry under high resolution: a case in Dongguan",
        "paper_author": "Tang J.",
        "publication": "Journal of Material Cycles and Waste Management",
        "citied_by": "3",
        "cover_date": "2023-07-01",
        "Abstract": "Waste classification is comprehensively carried out in China as an important national-level policy, and the large amount and the wide range of food waste generation (FWG) cause problems in the collection, transportation, and treatment. This study has conducted the prediction of FWG from the catering industry under high resolution, and provided suggestions and insights for food waste management and treatment. Taking Dongguan as an example, a Back Propagation Network (BPN) model is used to predict FWG under different operation data, and based on the acquired theoretical FWG numerical distribution, the intervals used to divide FWG values are determined. Then a Random Forest (RF) model is applied to predict the FWG intervals of the restaurants in the Point of Interest (POI) data. FWG of 96,303 restaurants is predicted, and the predicted FWG from the catering industry is about 3,106 t per day. Variation of FWG in different categories of restaurants, the material flow of FWG at the restaurant level, patterns of FWG at the restaurant level, and spatial patterns of FWG at the city level are also investigated. Suggestions for improvement of food waste collection standard and source reduction of FWG, and insights into food waste collection and distributed treatment system are raised. Graphical abstract: [Figure not available: see fulltext.]",
        "DOI": "10.1007/s10163-023-01706-8",
        "affiliation_name": "South China University of Technology",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "PoEMS: Policy Network-Based Early Warning Monitoring System for Sepsis in Intensive Care Units",
        "paper_author": "Dai H.",
        "publication": "IEEE Journal of Biomedical and Health Informatics",
        "citied_by": "1",
        "cover_date": "2023-07-01",
        "Abstract": "Sepsis is among the leading causes of morbidity and mortality in modern intensive care units (ICU). Due to accurate and early warning, the in-time antibiotic treatment of sepsis is critical for improving sepsis outcomes, contributing to saving lives, and reducing medical costs. However, the earlier prediction of sepsis onset is made, the fewer monitoring measurements can be processed, causing a lower prediction accuracy. In contrast, a more accurate prediction can be expected by analyzing more data but leading to the delayed warning associated with life-threatening events. In this study, we propose a novel deep reinforcement learning framework for solving early prediction of sepsis, called the Policy Network-based Early Warning Monitoring System (PoEMS). The proposed PoEMS provides accurate and early prediction results for sepsis onset based on analyzing varied-length electronic medical records (EMR). Furthermore, the system serves by monitoring the patient's health status consistently and provides an early warning only when a high risk of sepsis is detected. Additionally, a controlling parameter is designed for users to adjust the trade-off between earliness and accuracy, providing the adaptability of the model to meet various medical requirements in practical scenarios. Through a series of experiments on real-world medical data, the results demonstrate that our proposed PoEMS achieves a high AUROC result of more than 91% for early prediction, and predicts sepsis onset earlier and more accurately compared to other state-of-the-art competing methods.",
        "DOI": "10.1109/JBHI.2023.3272486",
        "affiliation_name": "National Yang Ming Chiao Tung University",
        "affiliation_city": "Hsinchu",
        "affiliation_country": "Taiwan"
    },
    {
        "paper_title": "The application of machine learning to rural population migration research",
        "paper_author": "Baggen H.S.",
        "publication": "Population, Space and Place",
        "citied_by": "0",
        "cover_date": "2023-07-01",
        "Abstract": "Many rural areas experience population stagnation or decline from out-migration with corresponding economic downturns. This is the case for the Northern Territory in Australia, a vast and sparsely populated jurisdiction. Its government has long sought to encourage stronger population growth but its population is young and highly transient, leading to high staff turn-overs and challenges for industries and government to attract families and skilled workers. Place-based factors such as job opportunities, access to essential services or environmental amenities influence satisfaction and migration decisions. The aim of this study was to understand why people might stay or move away through analysing responses to two open-text questions on the best and worst aspect of living in the Northern Territory. Over 3500 valid responses were analysed using machine learning-based unsupervised topic modelling which uncovered latent clusters. Forty-four percent of positive aspects were clustered into lifestyle factors, while negative aspects clustered around high living costs and crime. Some aspects, such as the weather and distance to other places were discussed as both positive and negative aspects. Topics discussed by respondents could be directly related to their intention to leave the Northern Territory, and also to specific individual's demographic characteristics providing insights for policies focused on attracting and retaining population. The use of unsupervised text mining in population research is rare and this study verifies its use to deliver objective and nuanced results generated from a large qualitative data set.",
        "DOI": "10.1002/psp.2664",
        "affiliation_name": "Charles Darwin University",
        "affiliation_city": "Darwin",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "An accelerated end-to-end method for solving routing problems",
        "paper_author": "Zhu T.",
        "publication": "Neural Networks",
        "citied_by": "7",
        "cover_date": "2023-07-01",
        "Abstract": "The application of neural network models to solve combinatorial optimization has recently drawn much attention and shown promising results in dealing with similar problems, like Travelling Salesman Problem. The neural network allows to learn solutions based on given problem instances, using reinforcement learning or supervised learning. In this paper, we present a novel end-to-end method to solve routing problems. In specific, we propose a gated cosine-based attention model (GCAM) to train policies, which accelerates the training process and the convergence of policy. Extensive experiments on different scale of routing problems show that the proposed method can achieve faster convergence of the training process than the state-of-the-art deep learning models while achieving solutions of the same quality.",
        "DOI": "10.1016/j.neunet.2023.05.003",
        "affiliation_name": "Yonsei University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "A Search-Based Testing Approach for Deep Reinforcement Learning Agents",
        "paper_author": "Zolfagharian A.",
        "publication": "IEEE Transactions on Software Engineering",
        "citied_by": "19",
        "cover_date": "2023-07-01",
        "Abstract": "Deep Reinforcement Learning (DRL) algorithms have been increasingly employed during the last decade to solve various decision-making problems such as autonomous driving, trading decisions, and robotics. However, these algorithms have faced great challenges when deployed in safety-critical environments since they often exhibit erroneous behaviors that can lead to potentially critical errors. One of the ways to assess the safety of DRL agents is to test them to detect possible faults leading to critical failures during their execution. This raises the question of how we can efficiently test DRL policies to ensure their correctness and adherence to safety requirements. Most existing works on testing DRL agents use adversarial attacks that perturb states or actions of the agent. However, such attacks often lead to unrealistic states of the environment. Furthermore, their main goal is to test the robustness of DRL agents rather than testing the compliance of the agents' policies with respect to requirements. Due to the huge state space of DRL environments, the high cost of test execution, and the black-box nature of DRL algorithms, exhaustive testing of DRL agents is impossible. In this paper, we propose a Search-based Testing Approach of Reinforcement Learning Agents (STARLA) to test the policy of a DRL agent by effectively searching for failing executions of the agent within a limited testing budget. We rely on machine learning models and a dedicated genetic algorithm to narrow the search toward faulty episodes (i.e., sequences of states and actions produced by the DRL agent). We apply STARLA on Deep-Q-Learning agents trained on two different RL problems widely used as benchmarks and show that STARLA significantly outperforms Random Testing by detecting more faults related to the agent's policy. We also investigate how to extract rules that characterize faulty episodes of the DRL agent using our search results. Such rules can be used to understand the conditions under which the agent fails and thus assess the risks of deploying it.",
        "DOI": "10.1109/TSE.2023.3269804",
        "affiliation_name": "University of Luxembourg",
        "affiliation_city": "Esch-sur-Alzette",
        "affiliation_country": "Luxembourg"
    },
    {
        "paper_title": "A<sup>3</sup>DCT: A cubic acceleration TCP for data center networks",
        "paper_author": "Lu Y.",
        "publication": "Journal of Network and Computer Applications",
        "citied_by": "3",
        "cover_date": "2023-07-01",
        "Abstract": "In recent years, online data-intensive (OLDI) applications have become particularly common in data center networks (DCNs), such as web search, advertisement systems, and distributed machine learning. OLDI applications have strict latency requirements for short flows and must operate under soft-real-time constraints (e.g., 300 ms latency). On the other hand, there exist long flows that are unaware of flow latency but are sensitive to throughput. Thus, a special transmission protocol that can satisfy different demands for both long flows and short flows is immediately needed. L2DCT is one of the most representative differentiated transmission protocols, which aim at the reduction in completion time for short flows. However, the performance of L2DCT becomes less significant in the scenario where short flows account for the majority (e.g., data mining). For the purposes of minimizing the flow completion time (FCT) of short flows, achieving the minimum average flow completion time (AFCT), and guaranteeing the deadline constraints upon flow transmission times, we propose a cubic acceleration TCP for DCNS, which is referred to as A3DCT. A3DCT employs the Shortest Remaining Process Time (SRPT) scheduling policy to adjust its congestion window according to the remaining bytes of the flow. Moreover, to improve the priority of short flows without affecting the throughput of long flows, A3DCT leverages a cubic function to indicate the urgency of short flows. Finally, A3DCT accelerates the recovery speed, in a flexible manner, depending on the urgency of different flows in a non-congestion state to request the bandwidth to finish transmission. We perform simulations of different scales by using NS-3 simulator to evaluate the performance of A3DCT. The evaluation results justify that A3DCT can not only achieve a low flow completion time, especially for short flows, but also guarantee the high throughput of long flows.",
        "DOI": "10.1016/j.jnca.2023.103654",
        "affiliation_name": "Nanjing University of Science and Technology",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Covariance in policy diffusion: Evidence from the adoption of hyperlocal air quality monitoring programs by US cities",
        "paper_author": "Qamar F.",
        "publication": "Cities",
        "citied_by": "4",
        "cover_date": "2023-07-01",
        "Abstract": "The diffusion of urban policy may occur through multiple mechanisms with complex interactions that obscure the interpretation of independent effects. In this work, we present a case study to explore the potential impact of covariance on the identification of the independent effects of the explanatory variables. We explore the influence of multiple diffusion mechanisms: learning, imitation, and coercion; policy mobility factors; and internal determinants on the likelihood of hyperlocal air quality monitoring program (HAMP) adoption by large cities in the US with a population >300,000 over the past decade. In general, results imply the adoption of HAMPs over the past decade has been motivated by sociopolitical rather than environmental goals. However, comparing the outcomes of a common, but limited, method to detect covariance and those from an expanded systematic variable selection exploration, shows that a third of the variables have covariance effects that can alter their impact and level of significance. The absence of coercion is found to lead to half of the variables exhibiting different results. These results highlight the importance of variable selection methods in determining a variable's significance and impact on the likelihood of policy adoption.",
        "DOI": "10.1016/j.cities.2023.104363",
        "affiliation_name": "University of Delaware",
        "affiliation_city": "Newark",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "A reinforcement learning algorithm acquires demonstration from the training agent by dividing the task space",
        "paper_author": "Zu L.",
        "publication": "Neural Networks",
        "citied_by": "2",
        "cover_date": "2023-07-01",
        "Abstract": "Although reinforcement learning (RL) has made numerous breakthroughs in recent years, addressing reward-sparse environments remains challenging and requires further exploration. Many studies improve the performance of the agents by introducing the state-action pairs experienced by an expert. However, such kinds of strategies almost depend on the quality of the demonstration by the expert, which is rarely optimal in a real-world environment, and struggle with learning from sub-optimal demonstrations. In this paper, a self-imitation learning algorithm based on the task space division is proposed to realize an efficient high-quality demonstration acquire while the training process. To determine the quality of the trajectory, some well-designed criteria are defined in the task space for finding a better demonstration. The results show that the proposed algorithm will improve the success rate of robot control and achieve a high mean Q value per step. The algorithm framework proposed in this paper has illustrated a great potential to learn from a demonstration generated by using self-policy in sparse environments and can be used in reward-sparse environments where the task space can be divided.",
        "DOI": "10.1016/j.neunet.2023.04.042",
        "affiliation_name": "University of Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Internet financial forecasting and digital economy development by using machine learning algorithm in the new consumption environment",
        "paper_author": "Huang P.",
        "publication": "Soft Computing",
        "citied_by": "3",
        "cover_date": "2023-07-01",
        "Abstract": "Data plane development kit (DPDK) provides an effective solution for the rapid processing of general multi-core processor platforms. Research based on DPDK on a general-purpose multi-core processor platform can use CPU cache to improve packet forwarding and processor core performance, as well as network function virtualization performance. In the parallel development of finance and network, people find that complex financial problems can be well-solved through big data network. The rapidly changing financial market is affected by economic and political factors, and the generation of financial data is also highly uncertain. However, financial forecast analysis and forecast modeling are of great significance. As a whole, this trend could help inform policy; from a meso-level, it can be insightful. The changes in financial data are conducive to the planning of business strategies for enterprises and gaining market competitive advantages; from a micro-level, individual investors will pay attention to historical data changes and the development of industries or companies before making investment decisions in order to obtain benefits. Internet technology can strengthen the accuracy of financial forecasting, so as to effectively avoid risks, but also can promote the development of digital economy, reduce labor costs, improve the breadth and depth of financial services in digital economy.",
        "DOI": "10.1007/s00500-023-08309-3",
        "affiliation_name": "Guangzhou College of Technology and Business",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Mental Illness as a Sentencing Determinant: A Comparative Case Law Analysis Based on a Machine Learning Approach",
        "paper_author": "Thomaidou M.A.",
        "publication": "Criminal Justice and Behavior",
        "citied_by": "4",
        "cover_date": "2023-07-01",
        "Abstract": "This study identifies factors that contribute to sentencing outcomes for criminally sentenced individuals experiencing mental disorders, in two U.S. states with divergent sociopolitical ideologies. Recent case law (n = 130) from appellate courts in New York and Kansas (from 2020 to 2021) was analyzed using regression and machine learning to predict sentence severity for individuals experiencing mental disorders. Across both states, trauma-related and personality disorders led to the most severe sentences, while paraphilia, addiction, and mood disorders had the lowest probability of imprisonment. Sentencing outcomes in Kansas were significantly more severe as compared with New York. A classification analysis identified important patterns of sentencing determinants that predicted which mental disorders were more likely to lead to incarceration. Findings and implications are discussed in relation to punishment disparities as well as the potentials and pitfalls regarding the use of machine learning approaches in criminal justice research and policy.",
        "DOI": "10.1177/00938548231170801",
        "affiliation_name": "Rutgers University",
        "affiliation_city": "East Orange",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Using machine learning to identify urban forest crown bounding boxes (CBB): Exploring a new method to develop urban forest policy",
        "paper_author": "Amati M.",
        "publication": "Urban Forestry and Urban Greening",
        "citied_by": "6",
        "cover_date": "2023-07-01",
        "Abstract": "Collecting and managing individual tree data is a critical activity for green sustainability strategies. Local governments are able to easily collect detailed public tree inventories, however data on trees located on private land are much more challenging and costly to collect. This means that new regulations to limit the removal of trees on private land go untested prior to their implementation, or fail to pass regulatory review processes. Without knowledge of the location of trees or the range of their different sizes, Local Government Authorities (LGAs) are unable to predict where a new policy to prohibit the removal of trees of a certain size is likely to have the greatest effect, where enforcement should be concentrated, or to convince government, the development sector and local communities of the need for action to preserve trees. The aim of this study was to explore the potential of a supervised machine learning algorithm as a cost-efficient method to understand tree sizes and locations on private land and to discuss how this information could be used for sustainable urban greening. We conclude by discussing some of the affordances of this approach to better target native vegetation protection and protect large trees; and report on the precision and recall of the detection of the urban forest.",
        "DOI": "10.1016/j.ufug.2023.127943",
        "affiliation_name": "RMIT University",
        "affiliation_city": "Melbourne",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Two-Stage and Three-Party Transboundary Watershed Management Based on Valuation Adjustment Mechanism (VAM) Agreement",
        "paper_author": "Gao J.",
        "publication": "Water Resources Management",
        "citied_by": "0",
        "cover_date": "2023-07-01",
        "Abstract": "The severe water shortage and pollution problems have become the bottleneck restricting the sustainable development of the economy and society. River basin ecological compensation is an important way to solve transboundary water pollution. To increase the cooperation willingness between governments and enterprises, and clarify the operation mechanism, this paper built a two-stage river basin ecological compensation mechanism in horizontal and vertical directions under VAM agreement. The results show that the externalization of environmental protection costs by the free-riding behavior of governments is the fundamental reason for the failure of the autonomous evolutionary game. The VAM agreement can reduce the uncertainty of upstream and downstream governments in environmental protection expenditure through contract pricing based on water quality, significantly improve free-riding behavior, and make the strategy of maximizing social benefits possible. After signing the VAM agreement, the upstream governments and enterprises become the main players in the second stage of the game, and the game results directly affect the final ownership of the downstream water quality and the ownership of the gambling amount. When the upstream government and enterprises adopt different strategies, by adjusting \"environmental protection funds and fines\", \"sewage treatment costs\", \"upstream government governance costs\" and \"gambling amount\", the negative side's strategic choices can be improved. However, when both sides adopt negative strategies, the adjustment of a single variable cannot achieve the optimal stability strategy of maximizing social benefits, and a more comprehensive strategy combination is needed. The research results are expected to provide a reference for the government to formulate environmental policies and promote coordinated basin governance.",
        "DOI": "10.1007/s11269-023-03505-0",
        "affiliation_name": "Southwest University",
        "affiliation_city": "Chongqing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Prediction of severity of aviation landing accidents using support vector machine models",
        "paper_author": "Silagyi D.V.",
        "publication": "Accident Analysis and Prevention",
        "citied_by": "10",
        "cover_date": "2023-07-01",
        "Abstract": "The purpose of this study was to apply support vector machine (SVM) models to predict the severity of aircraft damage and the severity of personal injury during an aircraft approach and landing accident and to evaluate and rank the importance of 14 accident factors across 39 sub-categorical factors. Three new factors were introduced using the theory of inattentional blindness: The presence of visual area surface penetrations for a runway, the Federal Aviation Administration's (FAA) visual area surface penetration policy timeframe, and the type of runway approach lighting. The study comprised 1,297 aircraft approach and landing accidents at airports within the United States with at least one instrument approach procedure. Support vector machine models were developed in using the linear, polynomial, radial basis function (RBF), and sigmoid kernels for the severity of aircraft damage and additional SVM models were developed for the severity of personal injury. The SVM models using the RBF kernel produced the best machine learning models with a 96% accuracy for predicting the severity of aircraft damage (0.94 precision, 0.95 recall, and 0.95 F1-score) and a 98% accuracy for predicting the severity of personal injury (0.99 precision, 0.98 recall, and 0.99 F1-score). The top predictors across both models were the pilot's total flight hours, time of the accident, pilot's age, crosswind component, landing runway number, single-engine land certificate, and any obstacle penetration. This study demonstrates the benefit of SVM modeling using the RBF kernel for accident prediction and for datasets with categorical factors.",
        "DOI": "10.1016/j.aap.2023.107043",
        "affiliation_name": "Embry-Riddle Aeronautical University",
        "affiliation_city": "Daytona Beach",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "GOSAFEOPT: Scalable safe exploration for global optimization of dynamical systems",
        "paper_author": "Sukhija B.",
        "publication": "Artificial Intelligence",
        "citied_by": "6",
        "cover_date": "2023-07-01",
        "Abstract": "Learning optimal control policies directly on physical systems is challenging. Even a single failure can lead to costly hardware damage. Most existing model-free learning methods that guarantee safety, i.e., no failures, during exploration are limited to local optima. This work proposes GOSAFEOPT as the first provably safe and optimal algorithm that can safely discover globally optimal policies for systems with high-dimensional state space. We demonstrate the superiority of GOSAFEOPT over competing model-free safe learning methods in simulation and hardware experiments on a robot arm.",
        "DOI": "10.1016/j.artint.2023.103922",
        "affiliation_name": "Aalto University",
        "affiliation_city": "Espoo",
        "affiliation_country": "Finland"
    },
    {
        "paper_title": "A data analytics framework for anomaly detection in flight operations",
        "paper_author": "Coelho e Silva L.",
        "publication": "Journal of Air Transport Management",
        "citied_by": "10",
        "cover_date": "2023-07-01",
        "Abstract": "In the air transport system, there has been a continuous effort to develop policies, tools, and methodologies that increase and standardize safety levels across the entire commercial aviation market, while also enhancing operational efficiency. Furthermore, there is a current focus on proactive approaches for aviation performance management. Within this context, data mining initiatives such as anomaly detection have become more prominent. Dealing with anomalies is a natural step for achieving the goals regarding operational safety and efficiency, as anomalies are often related to hazardous and inefficient operations. In this work, we propose a systematic flight data analytics framework for anomaly detection in flight operations in order to provide a comprehensive and reusable pipeline for model building, application, and explanation. The solution is designed to be applicable to both online and offline regimes and at multiple scales, while also building on domain-expert analysis. We demonstrate the framework applicability in two scenarios of routine flight operations monitoring considering both airline and air traffic management perspectives. In the first one, the framework is applied to aircraft performance data within an unsupervised learning setting with a density-based clustering approach for anomaly detection in landing operations at Minneapolis–Saint Paul International Airport (KMSP). The results are compared with those obtained with exceedance-based methods used in the current practice, revealing the detection of operationally significant anomalies beyond the benchmark. In the second case study, we apply the framework on flight tracking data within a supervised learning setting with the development of an autoencoder classifier for offline anomaly detection in terminal airspace arrival operations at Sao Paulo/Guarulhos International Airport (SBGR). Additionally, supervised learning models are developed for anomaly explanation. The autoencoder classifier was able to detect operationally significant anomalies, while the explanatory models provided novel insights about contributing factors to the anomalies identified. For instance, we learned that anomalous arrival trajectories are more likely to be associated with landing operations on runway 27 under wind scenarios, with an increase in the odds ratio of 62% and 58% for tailwinds and headwinds, respectively. In addition, we also observed a positive association between anomalies and wind gusts situations.",
        "DOI": "10.1016/j.jairtraman.2023.102409",
        "affiliation_name": "Instituto Tecnologico de Aeronautica",
        "affiliation_city": "Sao Jose dos Campos",
        "affiliation_country": "Brazil"
    },
    {
        "paper_title": "Energy optimization of wind turbines via a neural control policy based on reinforcement learning Markov chain Monte Carlo algorithm",
        "paper_author": "Tavakol Aghaei V.",
        "publication": "Applied Energy",
        "citied_by": "14",
        "cover_date": "2023-07-01",
        "Abstract": "This study focuses on the numerical analysis and optimal control of vertical-axis wind turbines (VAWT) using Bayesian reinforcement learning (RL). We specifically address small-scale wind turbines, which are well-suited to local and compact production of electrical energy on a small scale, such as urban and rural infrastructure installations. Existing literature concentrates on large scale wind turbines which run in unobstructed, mostly constant wind profiles. However urban installations generally must cope with rapidly changing wind patterns. To bridge this gap, we formulate and implement an RL strategy using the Markov chain Monte Carlo (MCMC) algorithm to optimize the long-term energy output of a wind turbine. Our MCMC-based RL algorithm is a model-free and gradient-free algorithm, in which the designer does not have to know the precise dynamics of the plant and its uncertainties. Our method addresses the uncertainties by using a multiplicative reward structure, in contrast with additive reward used in conventional RL approaches. We have shown numerically that the method specifically overcomes the shortcomings typically associated with conventional solutions, including, but not limited to, component aging, modeling errors, and inaccuracies in the estimation of wind speed patterns. Our results show that the proposed method is especially successful in capturing power from wind transients; by modulating the generator load and hence the rotor torque load, so that the rotor tip speed quickly reaches the optimum value for the anticipated wind speed. This ratio of rotor tip speed to wind speed is known to be critical in wind power applications. The wind to load energy efficiency of the proposed method was shown to be superior to two other methods; the classical maximum power point tracking method and a generator controlled by deep deterministic policy gradient (DDPG) method.",
        "DOI": "10.1016/j.apenergy.2023.121108",
        "affiliation_name": "İstinye Üniversitesi",
        "affiliation_city": "Istanbul",
        "affiliation_country": "Türkiye"
    },
    {
        "paper_title": "Prediction of global temperature anomaly by machine learning based techniques",
        "paper_author": "Sen D.",
        "publication": "Neural Computing and Applications",
        "citied_by": "8",
        "cover_date": "2023-07-01",
        "Abstract": "In this work, anthropogenic and natural factors were used to evaluate and forecast climate change on a global scale by using a variety of machine-learning techniques. First, significance analysis using the Shapley method was conducted to compare the importance of each variable. Accordingly, it was determined that the equivalent CO2 concentration in the atmosphere was the most important variable, which was proposed as further evidence of climate change due to fossil fuel-based energy generation. Following that, a variety of machine learning approaches were utilized to simulate and forecast the temperature anomaly until 2100 based on six distinct scenarios. Compared to the preindustrial period, the temperature anomaly for the best-case scenario was found to increase a mean value of 1.23 °C and 1.11 °C for the mid and end of the century respectively. On the other hand, the anomaly was estimated for the worst-case scenario to reach to a mean value of 2.52 °C and 4.97 °C for the same periods. It was then concluded that machine learning approaches can assist researchers in predicting climate change and developing policies for national governments, such as committing firmly to renewable energy regulations.",
        "DOI": "10.1007/s00521-023-08580-3",
        "affiliation_name": "University of Kyrenia",
        "affiliation_city": "Kyrenia",
        "affiliation_country": "Cyprus"
    },
    {
        "paper_title": "VCAs as partners or servants? The effects of information sensitivity and anthropomorphism roles on privacy concerns",
        "paper_author": "Sun Z.",
        "publication": "Technological Forecasting and Social Change",
        "citied_by": "9",
        "cover_date": "2023-07-01",
        "Abstract": "Advances in machine learning and natural language processing have driven the growing popularity of virtual conversational agents (VCAs). This anthropomorphic communication approach relies on user information sharing and real-time feedback from VCAs, and has raised privacy concerns while affecting various social interactions and relationships. Previous research on reducing user privacy concerns has mainly focused on user information mining, sensitive user information requests and privacy policies, while little is known about the anthropomorphic roles of partners and servants at the human-machine social hierarchy level. Therefore, this study, based on human-computer interaction (service) anthropomorphism at social level, develops a framework to investigate the impact of information sensitivity and VCAs' anthropomorphic roles, including partner and servant, on users' privacy concerns, as well as the mediating effects of competence- and integrity-based trust. The results show that when highly sensitive information is requested, user privacy concerns are greater for a partner VCA than a servant VCA, and vice-versa. Meanwhile, when a VCA requests highly sensitive information, integrity-based trust mediates the relationship between servant VCAs and privacy concerns, and when a VCA requests low-sensitivity information, competence-based trust mediates the same relationship. These insights provide actionable implications for managers.",
        "DOI": "10.1016/j.techfore.2023.122560",
        "affiliation_name": "China University of Petroleum (East China)",
        "affiliation_city": "Qingdao",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Efficient hyperparameters optimization through model-based reinforcement learning with experience exploiting and meta-learning",
        "paper_author": "Liu X.",
        "publication": "Soft Computing",
        "citied_by": "7",
        "cover_date": "2023-07-01",
        "Abstract": "Hyperparameter optimization plays a significant role in the overall performance of machine learning algorithms. However, the computational cost of algorithm evaluation can be extremely high for complex algorithm or large dataset. In this paper, we propose a model-based reinforcement learning with experience variable and meta-learning optimization method to speed up the training process of hyperparameter optimization. Specifically, an RL agent is employed to select hyperparameters and treat the k-fold cross-validation result as a reward signal to update the agent. To guide the agent’s policy update, we design an embedding representation called “experience variable” and dynamically update it during the training process. Besides, we employ a predictive model to predict the performance of machine learning algorithm with the selected hyperparameters and limit the model rollout in short horizon to reduce the impact of the inaccuracy of the model. Finally, we use the meta-learning technique to pre-train the model for fast adapting to a new task. To prove the advantages of our method, we conduct experiments on 25 real HPO tasks and the experimental results show that with the limited computational resources, the proposed method outperforms the state-of-the-art Bayesian methods and evolution method.",
        "DOI": "10.1007/s00500-023-08050-x",
        "affiliation_name": "University of Electronic Science and Technology of China",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Understanding the influence of news on society decision making: application to economic policy uncertainty",
        "paper_author": "Trust P.",
        "publication": "Neural Computing and Applications",
        "citied_by": "5",
        "cover_date": "2023-07-01",
        "Abstract": "The abundance of digital documents offers a valuable chance to gain insights into public opinion, social structure, and dynamics. However, the scale and volume of these digital collections makes manual analysis approaches extremely costly and not scalable. In this paper, we study the potential of using automated methods from natural language processing and machine learning, in particular weak supervision strategies, to understand how news influence decision making in society. Besides proposing a weak supervision solution for the task, which replaces manual labeling to a certain extent, we propose an improvement of a recently published economic index. This index is known as economic policy uncertainty (EPU) index and has been shown to correlate to indicators such as firm investment, employment, and excess market returns. In summary, in this paper, we present an automated data efficient approach based on weak supervision and deep learning (BERT + WS) for identification of news articles about economical uncertainty and adapt the calculation of EPU to the proposed strategy. Experimental results reveal that our approach (BERT + WS) improves over the baseline method centered in keyword search, which is currently used to construct the EPU index. The improvement is over 20 points in precision, reducing the false positive rate typical to the use of keywords.",
        "DOI": "10.1007/s00521-023-08438-8",
        "affiliation_name": "University College Cork",
        "affiliation_city": "Cork",
        "affiliation_country": "Ireland"
    },
    {
        "paper_title": "Wind design of solar panels for resilient and green communities: CFD with machine learning",
        "paper_author": "Aly A.M.",
        "publication": "Sustainable Cities and Society",
        "citied_by": "21",
        "cover_date": "2023-07-01",
        "Abstract": "Climate change mitigation and adaptation in urban environments call for more reliance on clean energy sources. Large photovoltaic (PV) systems have been enjoying renewed interest in clean and renewable energy. However, designing resilient PV systems faces an increased risk due to windstorms. Whether wind loads on PV systems are well understood, properly accounted for, and the damage is mitigated are crucial questions. While computational fluid dynamics (CFD) is proven effective for quantifying wind loads on structures, accurate and affordable computations are challenging. In this paper, we employ CFD approaches and machine learning (ML) to obtain the design wind loads on solar panels. We validate the CFD simulations using experimental data and compare the results with the standard practice. Our findings suggest that experimentally validated CFD simulations can yield different results from the standard practice. Additionally, we recommend stowing solar panels at a -15° angle during wind events to reduce damage. CFD simulations are then employed to train an ML model to predict velocity and pressure distributions around a solar panel. The study demonstrates that integrating ML and CFD can significantly speed up simulations (up to 10,000 times faster) without sacrificing accuracy. Efficient designs can shape the future of PV systems and contribute to climate change adaptation and mitigation for improved disaster resilience and circular economy policies.",
        "DOI": "10.1016/j.scs.2023.104529",
        "affiliation_name": "LSU College of Engineering",
        "affiliation_city": "Baton Rouge",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "GDP responses to supply chain disruptions in a post-pandemic era: Combination of DL and ANN outputs based on Google Trends",
        "paper_author": "Shahzad U.",
        "publication": "Technological Forecasting and Social Change",
        "citied_by": "14",
        "cover_date": "2023-07-01",
        "Abstract": "With the recent Russian-Ukraine conflict, the frequency and intensity of disruptive shocks on major supply chains have risen, causing increasing food and energy security concerns for regulators. That is, the combination of newly available sophisticated deep learning tools with real-time series data may represent a fruitful policy direction because machines can identify patterns without being pre-conditioned calibration thanks to experimental data training. This paper employs Deep Learning (DL) and Artificial Neural Network (ANN) algorithms and aimed predicts GDP responses to supply chain disruptions, energy prices, economic policy uncertainty, and google trend in the US. Sampled data from 2008 to 2022 are monthly wrangled and embed different recession episodes connected to the subprime crisis of 2008, the COVID-19 pandemic, the recent invasion of Ukraine by Russia, and the current economic recession in the US. Both DL and ANN outputs empirically (and unanimously) demonstrated how sensitive monthly GDP variations are to dynamic changes in supply chain performances. Findings identify the substantial role of google trends in delivering a consistent fit to predicted GDP values, which has implications While a comparative discussion over the larger forecasting performance of DL compared to ANN experiments is offered, implications for global policy, decision-makers and firm managers are finally provided.",
        "DOI": "10.1016/j.techfore.2023.122512",
        "affiliation_name": "Anhui University of Finance and Economics",
        "affiliation_city": "Bengbu",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Guided probabilistic reinforcement learning for sampling-efficient maintenance scheduling of multi-component system",
        "paper_author": "Zhang Y.",
        "publication": "Applied Mathematical Modelling",
        "citied_by": "5",
        "cover_date": "2023-07-01",
        "Abstract": "In recent years, multi-agent deep reinforcement learning has progressed rapidly as reflected by its increasing adoptions in industrial applications. This paper proposes a Guided Probabilistic Reinforcement Learning (Guided-PRL) model to tackle maintenance scheduling of multi-component systems in the presence of uncertainty with the goal of minimizing the overall life-cycle cost. The proposed Guided-PRL is deeply rooted in the Actor-Critic (AC) scheme. Since traditional AC falls short in sampling efficiency and suffers from getting stuck in local minima in the context of multi-agent reinforcement learning, it is thus challenging for the actor network to converge to a solution of desirable quality even when the critic network is properly configured. To address these issues, we develop a generic framework to facilitate effective training of the actor network, and the framework consists of environmental reward modeling, degradation formulation, state representation, and policy optimization. The convergence speed of the actor network is significantly improved with a guided sampling scheme for environment exploration by exploiting rules-based domain expert policies. To handle data scarcity, the environmental modeling and policy optimization are approximated with Bayesian models for effective uncertainty quantification. The Guided-PRL model is evaluated using the simulations of a 12-component system as well as GE90 and CFM56 engines. Compared with four alternative deep reinforcement learning schemes, the Guided-PRL lowers life-cycle cost by 34.92% to 88.07%. In comparison with rules-based expert policies, the Guided-PRL decreases the life-cycle cost by 23.26% to 51.36%.",
        "DOI": "10.1016/j.apm.2023.03.025",
        "affiliation_name": "State Key Laboratory of Fluid Power and Mechatronic Systems",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Nature dependent tourism – Combining big data and local knowledge",
        "paper_author": "Spalding M.D.",
        "publication": "Journal of Environmental Management",
        "citied_by": "8",
        "cover_date": "2023-07-01",
        "Abstract": "The ability to quantify nature's value for tourism has significant implications for natural resource management and sustainable development policy. This is especially true in the Eastern Caribbean, where many countries are embracing the concept of the Blue Economy. The utilization of user-generated content (UGC) to understand tourist activities and preferences, including the use of artificial intelligence and machine learning approaches, remains at the early stages of development and application. This work describes a new effort which has modelled and mapped multiple nature dependent sectors of the tourism industry across five small island nations. It makes broad use of UGC, while acknowledging the challenges and strengthening the approach with substantive input, correction, and modification from local experts. Our approach to measuring the nature-dependency of tourism is practical and scalable, producing data, maps and statistics of sufficient detail and veracity to support sustainable resource management, marine spatial planning, and the wider promotion of the Blue Economy framework.",
        "DOI": "10.1016/j.jenvman.2023.117696",
        "affiliation_name": "University of Cambridge",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "A conditional machine learning classification approach for spatio-temporal risk assessment of crime data",
        "paper_author": "Rodrigues A.",
        "publication": "Stochastic Environmental Research and Risk Assessment",
        "citied_by": "3",
        "cover_date": "2023-07-01",
        "Abstract": "Crime data analysis is an essential source of information to aid social and political decisions makers regarding the allocation of public security resources. Computer-aided dispatch systems and technological advances in geographic information systems have made analysing and visualising historical spatial and temporal records of crimes a vital part of police operations and strategy. We look at our motivating crime problem as a spatio-temporal point pattern. Using a conditional approach based on properties of Poisson point processes, we transform the spatio-temporal point process prediction problem into a classification problem. We create spatio-temporal handcrafted features to link future and past events and use machine learning algorithms to learn behavioural patterns from the data. The fitted model is then used to carry out the reverse transformation, i.e. to perform spatio-temporal risk predictions based on the outcomes of the classification problem. Our procedure has theoretical formalism from point process theory and gains flexibility and computational efficiency inherited from the machine learning field. We show its performance under some simulated scenarios and a real application to spatio-temporal prediction and risk assessment of homicides in Bogota, Colombia.",
        "DOI": "10.1007/s00477-023-02420-5",
        "affiliation_name": "Universidade Federal do Espírito Santo",
        "affiliation_city": "Vitoria",
        "affiliation_country": "Brazil"
    },
    {
        "paper_title": "Deep reinforcement learning based scheduling strategy for federated learning in sensor-cloud systems",
        "paper_author": "Zhang T.",
        "publication": "Future Generation Computer Systems",
        "citied_by": "5",
        "cover_date": "2023-07-01",
        "Abstract": "Sensor-cloud systems (SCSs) aim to provide flexible configurable platforms for monitoring and controlling the IoT-enabled applications. By integrating sensors, wireless networks and cloud for managing sensors, collecting data, and automating decision-making, the collected sensing data are typically used for machine learning purposes. With increasing emphasis in privacy protection, Federated Learning (FL) is widely adopted for enhancing privacy preservation. FL enables sharing of data for machine learning while preserving the privacy of the data owners. In SCSs, FL involves a large number of edge nodes in order to ensure a sufficient amount of data for model training. However, FL inevitably incurs prohibitive overheads if it simply gathers data from all the nodes, hence making it desirable to adopt some scheduling strategy so that data are collected only from a selected subset of nodes. This paper proposes a scheduling strategy based on deep reinforcement learning (DRL) for improving the performance and efficiency of FL in SCSs. The DRL environment, such as state space, action space, and reward function, is carefully designed. Proximal policy optimization is employed to train the DRL agent. Experimental results demonstrated that the proposed method outperforms other baselines on both independent and identically distributed (IID) and non-IID datasets.",
        "DOI": "10.1016/j.future.2023.03.009",
        "affiliation_name": "School of Computer Science and Engineering",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore"
    },
    {
        "paper_title": "A data mining framework for reporting trends in the predictive contribution of factors related to educational achievement",
        "paper_author": "Silva Filho R.L.C.",
        "publication": "Expert Systems with Applications",
        "citied_by": "4",
        "cover_date": "2023-07-01",
        "Abstract": "Large-scale assessment (LSA) data analysis is a relevant mechanism used worldwide for assessing educational achievement. Despite the leading role played by traditional statistics, data mining (DM) and machine learning (ML) have become widespread in this direction. Understanding what drives ML models to predict LSA outcomes may provide reliable information for education practitioners and policymakers. Although this understanding may become even more insightful over time, it has nonetheless received little attention. This paper presents a new framework to reveal trends in the relevance of contextual features for predicting educational outcomes. Within this framework, score-based feature contributions were defined as standards for improving human interpretability in a multivariate temporal analysis. Due to the limitations of existing scores in reporting the effects of highly correlated variables, such as socioeconomics, a new set of score-based metrics has been introduced. The metrics are inspired by the accumulation of local effects and are highly informative in reporting the direction and size of the contribution on the same scale as the predicted outcome. The proposed metrics have been assessed in both synthetic and real data, while the whole framework was applied to a Brazilian LSA (ENEM) on a yearly basis, between 2009 and 2019. The results have demonstrated that the proposal may be used to suggest new hypotheses for educational achievement and to confirm the predictive power of previously recognized hypotheses. For example, the strong contribution of well-known determinants of school performance, such as income, race, and parent́s education, were clearly highlighted. Furthermore, their trends and the behavior of features related to the school infrastructure and faculty, such as student́s computer, faculty education, faculty adequate training, and faculty work overload, may leverage new insights for further in-depth investigations on educational policies.",
        "DOI": "10.1016/j.eswa.2023.119729",
        "affiliation_name": "Instituto Federal do Norte de Minas Gerais",
        "affiliation_city": "Montes Claros",
        "affiliation_country": "Brazil"
    },
    {
        "paper_title": "Classifying habitat characteristics of wetlands using a self-organizing map",
        "paper_author": "Kim S.H.",
        "publication": "Ecological Informatics",
        "citied_by": "7",
        "cover_date": "2023-07-01",
        "Abstract": "Wetlands are nutrient-rich and biodiverse ecosystems that provide habitats for various animals and plants and protect against flooding. Classification of wetlands provides information to conservation planners and resource managers for ecosystem service determination. Many ecological case studies illuminate the self-organizing map (SOM) as a robust and powerful data classification and visualization tool. In this study, we use the SOM to analyze the habitat characteristics of inland wetlands in South Korea. We surveyed the plants, benthic macroinvertebrates, and bird species inhabiting 530 nationwide wetlands for four years from 2016 to 2019. Nine environmental features, including the proportion of urban area, farmland, grassland, a forest within a 1 km buffer zone, distance from the river and nearest wetland, area, perimeter, and average slope of wetland polygons, were used to train the SOM and examine the habitat characteristics of the surveyed living components. A map size of 10 × 11 pixels was considered for SOM training, and the output data were classified into eight clusters. Based on the occurrence frequency of the surveyed species group, most species were distributed in all clusters, whereas some dominated in specific clusters. We believe that our study contributes significantly to the literature because it highlights the significance of the SOM approach to cluster wetlands with dependent habitats and provides ecological information to build sustainable wetland conservation policies.",
        "DOI": "10.1016/j.ecoinf.2023.102048",
        "affiliation_name": "Yeungnam University",
        "affiliation_city": "Gyeongsan",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "Optimal Design of Wireless Charging Electric Buses-Based Machine Learning: A Case Study of Nguyen-Dupuis Network",
        "paper_author": "Fathollahi A.",
        "publication": "IEEE Transactions on Vehicular Technology",
        "citied_by": "17",
        "cover_date": "2023-07-01",
        "Abstract": "The Wireless Charging Electric Transit Bus (WCETB) is an innovative electric transportation technology that receives power wirelessly from underground power transmitters. Since the battery in the bus can be wirelessly charged during the moving, the need to stop for the vehicle at the charging station is eliminated, and recharging time is remarkably decreased. One of the best methodologies to commercialize the WCETB technology is to economically allocate the power tracks on considered routes while optimally determining the battery size of the vehicle at the same time. In this paper, the planning of the power transmitters and the capacity of the battery size for a WCETB with a multi-route model is optimally designed by Deep Deterministic Policy Gradient (DDPG). In particular, the DDPG algorithm is adopted in this model, an appropriate reward function is defined for the multiple route problem, and the optimal problem is solved by adopting the training ability of deep neural networks (DNNs), i.e., Actor and Critic neural networks. A complex Nguyen-Dupuis (N-D) traffic network with multiple routes is considered a case study to evaluate the validity and performance of the proposed deep learning scheme from a systematic point of view. Numerical analysis along with sensitivity examination confirms the efficiency of the optimal design and solution process.",
        "DOI": "10.1109/TVT.2023.3247838",
        "affiliation_name": "Aarhus Universitet",
        "affiliation_city": "Aarhus",
        "affiliation_country": "Denmark"
    },
    {
        "paper_title": "Anti-intellectualism amid the COVID-19 pandemic: The discursive elements and sources of anti-Fauci tweets",
        "paper_author": "Chen Y.",
        "publication": "Public Understanding of Science",
        "citied_by": "4",
        "cover_date": "2023-07-01",
        "Abstract": "Anti-intellectualism (resentment, hostility, and mistrust of experts) has become a growing concern during the pandemic. Using topic modeling and supervised machine learning, this study examines the elements and sources of anti-Fauci tweets as a case of anti-intellectual discourse on social media. Based on the theoretical framework of science-related populism, we identified three anti-intellectual discursive elements in anti-Fauci tweets: people-scientist antagonism, delegitimizing the motivation of scientists, and delegitimizing the knowledge of scientists. Delegitimizing the motivation of scientists appeared the most in anti-Fauci tweets. Politicians, conservative news media, and non-institutional actors (e.g. individuals and grassroots advocacy organizations) co-constructed the production and circulation of anti-intellectual discourses on Twitter. Anti-intellectual discourses resurged even under Twitter’s content moderation mechanism. We discuss theoretical and practical implications for building public trust in scientists, effective science communication, and content moderation policies on social media.",
        "DOI": "10.1177/09636625221146269",
        "affiliation_name": "Renmin University of China",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Reinforcement Learning-Based Adaptive Feature Boosting for Smart Grid Intrusion Detection",
        "paper_author": "Hu C.",
        "publication": "IEEE Transactions on Smart Grid",
        "citied_by": "11",
        "cover_date": "2023-07-01",
        "Abstract": "Intrusion detection systems (IDSs) are crucial in the security monitoring for the smart grid with increasing machine-to-machine communications and cyber threats thereafter. However, the multi-sourced, correlated, and heterogeneous smart grid data pose significant challenges to the accurate attack detection by IDSs. To improve the attack detection, this paper proposes Reinforcement Learning-based Adaptive Feature Boosting, which aims to leverage a series of AutoEncoders (AEs) to capture critical features from the multi-sourced smart grid data for the classification of normal, fault, and attack events. Multiple AEs are utilized to extract representative features from different feature sets that are automatically generated through a weighted feature sampling process; each AE-extracted feature set is then applied to build a Random Forest (RF) base classifier. In the feature sampling process, Deep Deterministic Policy Gradient (DDPG) is introduced to dynamically determine the feature sampling probability based on the classification accuracy. The critical features that improve the classification accuracy are assigned larger sampling probabilities and increasingly participate in the training of next AE. The presence of critical features is increased in the event classification over the multi-sourced smart grid data. Considering potential different alarms among base classifiers, an ensemble classifier is further built to distinguish normal, fault, and attack events. Our proposed approach is evaluated on the two realistic datasets collected from Hardware-In-the-Loop (HIL) and WUSTIL-IIOT-2021 security testbeds, respectively. The evaluation on the HIL security dataset shows that our proposed approach achieves the classification accuracy with 97.28%, an effective 5.5% increase over the vanilla Adaptive Feature Boosting. Moreover, the proposed approach not only accurately and stably selects critical features on the WUSTIL-IIOT-2021 dataset based on the significant difference of feature sampling probabilities between critical and uncritical features, i.e., the probabilities greater than 0.08 and less than 0.01, but also outperforms the other best-performing approaches with the increasing Matthew Correlation Coefficient (MCC) of 8.03%.",
        "DOI": "10.1109/TSG.2022.3230730",
        "affiliation_name": "Concordia Institute for Information Systems Engineering",
        "affiliation_city": "Montreal",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "On predicting school dropouts in Egypt: A machine learning approach",
        "paper_author": "Selim K.S.",
        "publication": "Education and Information Technologies",
        "citied_by": "15",
        "cover_date": "2023-07-01",
        "Abstract": "Compulsory school-dropout is a serious problem affecting not only the education systems, but also the developmental progress of any country as a whole. Identifying the risk of dropping out, and characterizing its main determinants, could help the decision-makers to draw eradicating policies for this persisting problem and reducing its social and economic negativities over time. Based on a substantially imbalanced Egyptian survey dataset, this paper aims to develop a Logistic classifier capable of early predicting students at-risk of dropping out. Training any classifier with an imbalanced dataset, usually weaken its performance especially when it comes to false negative classification. Due to this fact, an extensive comparative analysis is conducted to investigate a variety of resampling techniques. More specifically, based on eight under-sampling techniques and four over-sampling ones, and their mutually exclusive mixed pairs, forty-five resampling experiments on the dataset are conducted to build the best possible Logistic classifier. The main contribution of this paper is to provide an explicit predictive model for school dropouts in Egypt which could be employed for identifying vulnerable students who are continuously feeding this chronic problem. The key factors of vulnerability the suggested classifier identified are student chronic diseases, co-educational, parents' illiteracy, educational performance, and teacher caring. These factors are matching with those found by many of the research previously conducted in similar countries. Accordingly, educational authorities could confidently monitor these factors and tailor suitable actions for early intervention.",
        "DOI": "10.1007/s10639-022-11571-x",
        "affiliation_name": "Faculty of Economics and Political Science",
        "affiliation_city": "Giza",
        "affiliation_country": "Egypt"
    },
    {
        "paper_title": "Optimization of homogeneous charge compression ignition combustion in a light-duty diesel engine operated using ethyl acetate-gasoline blends",
        "paper_author": "Kale A.V.",
        "publication": "International Journal of Engine Research",
        "citied_by": "6",
        "cover_date": "2023-07-01",
        "Abstract": "The low temperature combustion mode of homogeneous charge compression ignition eliminates particulate matter and oxides of nitrogen (NOx) emissions trade-off that prevails in high-temperature, diffusion-controlled conventional diesel combustion (CDC). In the present research, the significant challenge of narrow operating load range that hinders the commercial implementation of light-duty HCCI engines was overcome by employing ethyl acetate-gasoline blends. The gasoline concentration in test fuels was reduced in 10% decrements, from 84% to 24%, to replace it with ethyl acetate. The use of ethyl acetate, a renewable fuel, can help solve the energy crisis due to the rapid depletion of fossil fuels. An ignition improver was blended in the test fuels in a predetermined amount of 6% so that combustion stability was not hampered at lower loads. Parametric investigations were conducted to study the effect of progressively increasing ethyl acetate in test fuels on HCCI combustion, performance, and emissions. The machine learning tool of artificial neural network was implemented to learn the behavior of the test engine, considering load and fuel composition as input variables. The feedforward artificial neural network models were developed to predict the start of combustion, combustion phasing, indicated thermal efficiency, and emissions of carbon monoxide, soot, NOx, and unburned hydrocarbon (HC). A multi-objective optimization was performed to arrive at the best operating condition by integrating artificial neural network models with the genetic algorithm. All the developed artificial neural network models could predict responses with acceptable accuracy. The genetic algorithm indicated that the optimum point of the operation was at 80% load and 65% ethyl acetate in the test fuels. Experiments were conducted to validate the optimal HCCI conditions that resulted in 27% higher indicated thermal efficiency, 54% lower HC+NOx, and 99% lower soot emissions than CDC. Overall, the present study demonstrated the benefits of considering ethyl acetate as a fuel to improve HCCI engine metrics of off-road diesel engines.",
        "DOI": "10.1177/14680874221138126",
        "affiliation_name": "Indian Institute of Technology Madras",
        "affiliation_city": "Chennai",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Modeling adaptive platoon and reservation-based intersection control for connected and autonomous vehicles employing deep reinforcement learning",
        "paper_author": "Li D.",
        "publication": "Computer-Aided Civil and Infrastructure Engineering",
        "citied_by": "17",
        "cover_date": "2023-07-01",
        "Abstract": "As a cutting-edge strategy to reduce travel delay and fuel consumption, platooning of connected and autonomous vehicles (CAVs) at signal-free intersections has become increasingly popular in academia. However, when determining optimal platoon size, few studies have attempted to comprehensively consider the relations between the size of a CAV platoon and traffic conditions around an intersection. To this end, this study develops an adaptive platoon-based autonomous intersection control model, named INTEL-PLT, which adopts deep reinforcement learning technique to realize the optimization of multiple dynamic objectives (e.g., efficiency, fairness, and energy saving). The framework of INTEL-PLT has a two-level structure: The first level employs a reservation-based policy integrated with a nonconflicting lane selection mechanism to determine the lanes’ releasing priorities; and the second level uses a deep Q-network algorithm to identify the optimal platoon size based on real-time traffic conditions (e.g., traffic density, vehicle movement, etc.) of an intersection. The model is validated and examined on the simulator Simulation of Urban Mobility. It is found that the proposed model exhibits superior performances on both travel efficiency and fuel conservation as compared with state-of-the-art methods in three typical traffic conditions. Moreover, several in-depth insights learned from the simulations are provided in this paper, which could better explain the relation between platoon size and traffic condition.",
        "DOI": "10.1111/mice.12956",
        "affiliation_name": "School of Civil and Environmental Engineering",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore"
    },
    {
        "paper_title": "Populism and De Facto Central Bank Independence",
        "paper_author": "Gavin M.",
        "publication": "Comparative Political Studies",
        "citied_by": "7",
        "cover_date": "2023-07-01",
        "Abstract": "Although central bank independence is a core tenet of monetary policy-making, it remains politically contested: In many emerging markets, populist governments are in frequent public conflict with the central bank. At other times, the same governments profess to respect the monetary authority’s independence. We model this conflict drawing on the crisis bargaining literature. Our model predicts that populist politicians will often bring a nominally independent central bank to heel without having to change its legal status. To provide evidence, we build a new data set of public pressure on central banks by classifying over 9000 analyst reports using machine learning. We find that populist politicians are more likely than non-populists to exert public pressure on the central bank, unless checked by financial markets, and also more likely to obtain interest rate concessions. Our findings underscore that de jure does not equal de facto central bank independence in the face of populist pressures.",
        "DOI": "10.1177/00104140221139513",
        "affiliation_name": "University of Ottawa",
        "affiliation_city": "Ottawa",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Incorporating networks in semantic understanding of streetscapes: Contextualising active mobility decisions",
        "paper_author": "Yap W.",
        "publication": "Environment and Planning B: Urban Analytics and City Science",
        "citied_by": "18",
        "cover_date": "2023-07-01",
        "Abstract": "Planning for active mobility satisfies many fundamental tenets of good urban design and planning. However, planning for active mobility is a complex endeavour due to numerous local, place-based factors that influence active mobility decisions. Recent advancements in urban data research have demonstrated the effectiveness of deep learning methods in evaluating active mobility potential for urban environments. At present, the incorporation of semantic information from deep learning models and street view imagery into spatio-temporal contexts remains a challenge. In particular, knowledge extraction from deep learning models remains an open question for urban planning and decision-making. Towards this issue, we propose a functional deep learning and network science workflow that employs open data from OpenStreetMap and Mapillary to assess factors affecting active mobility decisions and route planning. We demonstrate the generalisability of our analytical workflow through two case studies focusing on urban greenery in Nerima city (Japan) and urban visual complexity in Pasir Ris town (Singapore). Our results reveal clear patterns of heterogeneity in urban streetscapes and identify unevenness in street infrastructure provision based on destination types. Using this information, we propose specific areas for design intervention to improve active mobility planning. Our workflow is applicable for a diverse range of use cases making it relevant to a wide range of stakeholders, not limited to, urban researchers, policy makers and urban planners.",
        "DOI": "10.1177/23998083221138832",
        "affiliation_name": "National University of Singapore",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore"
    },
    {
        "paper_title": "Imbalanced Malware Family Classification Using Multimodal Fusion and Weight Self-Learning",
        "paper_author": "Li S.",
        "publication": "IEEE Transactions on Intelligent Transportation Systems",
        "citied_by": "16",
        "cover_date": "2023-07-01",
        "Abstract": "In recent years, the increasing prevalence of Intelligent Transportation Systems with advanced technologies has led to the emergence of many targeted forms of malware such as ransomware, Trojans, viruses, and malicious mining programs. And malware authors use policies like category disguise or family obfuscation in malware components to evade detection, which poses a great security threat to enterprises, government agencies, and Internet users. In this paper, we propose a malware family classification approach based on multimodal fusion and weight self-learning. Firstly, multiple modalities of malware such as byte, format, statistic, and semantic are fused in various ways to generate effective features. And then, we creatively add a weight self-learning mechanism of malware families into the classification model, which works by continuously calculating log-loss based on the family label and the probabilities predicted by each feature. The approach proves to achieve excellent classification performance on highly imbalanced malware family datasets with high efficiency and small resource overhead, which helps to identify and classify malware families and enhance the efficiency of massive malware analysis in Intelligent Transportation Systems.",
        "DOI": "10.1109/TITS.2022.3208891",
        "affiliation_name": "Taif University",
        "affiliation_city": "Taif",
        "affiliation_country": "Saudi Arabia"
    },
    {
        "paper_title": "Global economic policy uncertainty aligned: An informative predictor for crude oil market volatility",
        "paper_author": "Zhang Y.",
        "publication": "International Journal of Forecasting",
        "citied_by": "37",
        "cover_date": "2023-07-01",
        "Abstract": "This paper constructs an aligned global economic policy uncertainty (GEPU) index based on a modified machine learning approach. We find that the aligned GEPU index is an informative predictor for forecasting crude oil market volatility both in- and out-of-sample. Compared to general GEPU indices without supervised learning, well-recognized economic variables, and other popular uncertainty indicators, the aligned GEPU index is rather powerful and can provide preponderant or complementary information. The trading strategy based on the aligned GEPU index can also generate sizable economic gains. The statistical source of the aligned GEPU index's predictive power is that it can learn both the magnitude and sign of national EPU variables’ predictive ability and thus yields reasonable and informative loadings. On the other hand, the economic driving force probably stems from the ability for forecasting the shocks of oil-related fundamentals.",
        "DOI": "10.1016/j.ijforecast.2022.07.002",
        "affiliation_name": "Southwest Jiaotong University",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Integrating Reinforcement Learning and Learning From Demonstrations to Learn Nonprehensile Manipulation",
        "paper_author": "Sun X.",
        "publication": "IEEE Transactions on Automation Science and Engineering",
        "citied_by": "7",
        "cover_date": "2023-07-01",
        "Abstract": "Motor skills are essential for robots to accomplish complicated and dexterous manipulation tasks, which are difficult to be mastered through traditional controller designs. Currently, robots learning from demonstrations enable them to learn control policies automatically from human motor demonstrations. However, the nonlinearity and instantaneousness of the demonstrated forces prohibit robots from fully mastering the motor skill features by simply exploiting force examples. Therefore, a self-improvement learning scheme is required to refine the control policy further until satisfactory motor skills are acquired. Hence, this paper combines learning from demonstrations and reinforcement learning to learn a controller for complex motor skills. The proposed method is validated on an IIWA KUKA robot, performing a specified nonprehensile manipulation task. Note to Practitioners - The motivation of this paper originates from the requirement to develop an efficient and fast learning algorithm that improves the robot skill learning efficiency. Specifically, our research focuses on the nonprehensile manipulation task, easily subject to environmental changes. Therefore, the robot must continuously interact with the environment to master the skill. To accelerate the skill learning process, learning from demonstrations initializes the control policies, and then the robot starts to practice the demonstrated skill. After each practice round, the robot receives a reward from the environment, and based on the reinforcement learning algorithm limited up to 100 trials, the robot masters the nonprehensile manipulation skill.",
        "DOI": "10.1109/TASE.2022.3185071",
        "affiliation_name": "Shenzhen Institute of Advanced Technology",
        "affiliation_city": "Shenzhen",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Forecasting CPI inflation components with Hierarchical Recurrent Neural Networks",
        "paper_author": "Barkan O.",
        "publication": "International Journal of Forecasting",
        "citied_by": "32",
        "cover_date": "2023-07-01",
        "Abstract": "We present a hierarchical architecture based on recurrent neural networks for predicting disaggregated inflation components of the Consumer Price Index (CPI). While the majority of existing research is focused on predicting headline inflation, many economic and financial institutions are interested in its partial disaggregated components. To this end, we developed the novel Hierarchical Recurrent Neural Network (HRNN) model, which utilizes information from higher levels in the CPI hierarchy to improve predictions at the more volatile lower levels. Based on a large dataset from the US CPI-U index, our evaluations indicate that the HRNN model significantly outperforms a vast array of well-known inflation prediction baselines. Our methodology and results provide additional forecasting measures and possibilities to policy and market makers on sectoral and component-specific price changes.",
        "DOI": "10.1016/j.ijforecast.2022.04.009",
        "affiliation_name": "Bank of Israel",
        "affiliation_city": "Jerusalem",
        "affiliation_country": "Israel"
    },
    {
        "paper_title": "Deep Reinforcement Learning on Autonomous Driving Policy With Auxiliary Critic Network",
        "paper_author": "Wu Y.",
        "publication": "IEEE Transactions on Neural Networks and Learning Systems",
        "citied_by": "38",
        "cover_date": "2023-07-01",
        "Abstract": "Deep reinforcement learning (DRL) is a machine learning method based on rewards, which can be extended to solve some complex and realistic decision-making problems. Autonomous driving needs to deal with a variety of complex and changeable traffic scenarios, so the application of DRL in autonomous driving presents a broad application prospect. In this article, an end-to-end autonomous driving policy learning method based on DRL is proposed. On the basis of proximal policy optimization (PPO), we combine a curiosity-driven method called recurrent neural network (RNN) to generate an intrinsic reward signal to encounter the agent to explore its environment, which improves the efficiency of exploration. We introduce an auxiliary critic network on the original actor-critic framework and choose the lower estimate which is predicted by the dual critic network when the network update to avoid the overestimation bias. We test our method on the lane- keeping task and overtaking task in the open racing car simulator (TORCS) driving simulator and compare with other DRL methods, experimental results show that our proposed method can improve the training efficiency and control performance in driving tasks.",
        "DOI": "10.1109/TNNLS.2021.3116063",
        "affiliation_name": "Dongguan University of Technology",
        "affiliation_city": "Dongguan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Yield Prediction Models in Guangxi Sugarcane Planting Regions Based on Machine Learning Methods",
        "paper_author": "Shi J.",
        "publication": "Smart Agriculture",
        "citied_by": "3",
        "cover_date": "2023-06-30",
        "Abstract": "Accurate prediction of changes in sugarcane yield in Guangxi can provide important reference for the formulation of relevant policies by the government and provide decision-making basis for farmers to guide sugarcane planting, thereby improving sugarcane yield and quality and promoting the development of the sugarcane industry. This research was conducted to provide scientific data support for sugar factories and related management departments, explore the relationship between sugarcane yield and meteorological factors in the main sugarcane producing areas of Guangxi Zhuang Autonomous Region. [Methods] The study area included five sugarcane planting regions which laid in five different counties in Guangxi, China. The average yields per hectare of each planting regions were provided by Guangxi Sugar Industry Group which controls the sugar refineries of each planting region. The daily meteorological data including 14 meteorological factors from 2002 to 2019 were acquired from National Data Center for Meteorological Sciences to analyze their influences placed on sugarcane yield. Since meteorological factors could pose different influences on sugarcane growth during different time spans, a new kind of factor which includes meteorological factors and time spans was defined, such as the average precipitation in August, the average temperature from February to April, etc. And then the inter-correlation of all the meteorological factors of different time spans and their correlations with yields were analyzed to screen out the key meteorological factors of sensitive time spans. After that, four algorithms of BP neural network (BPNN), support vector machine (SVM), random forest (RF), and long short-term memory (LSTM) were employed to establish sugarcane apparent yield prediction models for each planting region. Their corresponding reference models based on the annual meteorological factors were also built. Additionally, the meteorological yields of every planting region were extracted by HP filtering, and a general meteorological yield prediction model was built based on the data of all the five planting regions by using RF, SVM BPNN, and LSTM, respectively. [Results and Discussions] The correlation analysis showed that different planting regions have different sensitive meteorological factors and key time spans. The highly representative meteorological factors mainly included sunshine hours, precipitation, and atmospheric pressure. According to the results of correlation analysis, in Region 1, the highest negative correlation coefficient with yield was observed at the sunshine hours during October and November, while the highest positive correlation coefficient was found at the minimum relative humidity in November. In Region 2, the maximum positive correlation coefficient with yield was observed at the average vapor pressure during February and March, whereas the maximum negative correlation coefficient was associated with the precipitation in August and September. In Region 3, the maximum positive correlation coefficient with yield was found at the 20-20 precipitation during August and September, while the maximum negative correlation coefficient was related to sunshine hours in the same period. In Region 4, the maximum positive correlation coefficient with yield was observed at the 20-20 precipitation from March to December, whereas the maximum negative correlation coefficient was associated with the highest atmospheric pressure from August to December. In Region 5, the maximum positive correlation coefficient with yield was found at the average vapor pressure from June and to August, whereas the maximum negative correlation coefficient as related to the lowest atmospheric pressure in February and March. For each specific planting region, the accuracy of apparent yield prediction model based on sensitive meteorological factors during key time spans was obviously better than that based on the annual average meteorological values. The LSTM model performed significantly better than the widely used classic BPNN, SVM, and RF models for both kinds of meteorological factors (under sensitive time spans or annually). The overall root mean square error (RMSE) and mean absolute percentage error (MAPE) of the LSTM model under key time spans were 10.34 t/ha and 6.85%, respectively, with a coefficient of determination Rv2 of 0.8489 between the predicted values and true values. For the general prediction models of the meteorological yield to multiple the sugarcane planting regions, the RF, SVM, and BPNN models achieved good results, and the best prediction performance went to BPNN model, with an RMSE of 0.98 t/ha, MAPE of 9.59%, and Rv2 of 0.965. The RMSE and MAPE of the LSTM model were 0.25 t/ha and 39.99%, respectively, and the Rv2 was 0.77. [Conclusions] Sensitive meteorological factors under key time spans were found to be more significantly correlated with the yields than the annual average meteorological factors. LSTM model shows better performances on apparent yield prediction for specific planting region than the classic BPNN, SVM, and RF models, but BPNN model showed better results than other models in predicting meteorological yield over multiple sugarcane planting regions.",
        "DOI": "10.12133/j.smartag.SA202304004",
        "affiliation_name": "Guangxi Academy of Agricultural Sciences",
        "affiliation_city": "Nanning",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Deep Learning-based Near-real-time Monitoring of Autumn Irrigation Extent at Sub-pixel Scale in a large Irrigation Districth",
        "paper_author": "Qian X.",
        "publication": "Agricultural Water Management",
        "citied_by": "7",
        "cover_date": "2023-06-30",
        "Abstract": "Flood irrigation is widely applied in harvested croplands of arid regions and can be classified as autumn/winter irrigation (AI) depending on the time of application. Due to its high water consumption, real-time monitoring of the AI extent is crucial to improve its scheduling. We proposed a remote sensing-based long short-term memory (LSTM) model for near-real-time monitoring of AI extent at sub-pixel scale. The model loosely coupled MODIS data with Sentinel-2 data to solve the mixed pixel issue of MODIS data, and calibrated Sentinel-2 thresholds for AI identification by a random forest (RF) module to extract large-scale reference data with high temporal frequencies. The variable importance estimated by RF is used as a reference for feature screening in the LSTM model. As Sentinel-2 images are not available daily, LSTM models trained with incomplete sequences were validated using multiple validation approaches. The model was applied to the Hetao Irrigation District, the largest irrigation district in arid region of China, and delivered reasonable performance with coefficient of determination of over 0.82 and mean absolute error of around 10.7%. Classification of irrigation patterns using simulated time series of irrigation area fractions revealed eight different irrigation patterns from 2010 to 2020 in the study region. Results indicate that the maximum fraction of AI area upstream is closely related to the cropland distribution pattern. The AI patterns changed significantly over the 11 years, with a more pronounced reduction in irrigation duration for individual pixels in the downstream area. These changes are associated with two important land policies implemented in many regions of China, land consolidation and land transfer. The proposed model demonstrates the potential for near-real-time monitoring of autumn irrigation extent within large irrigation districts, which can aid in AI scheduling and provide insight into irrigation patterns and practices.",
        "DOI": "10.1016/j.agwat.2023.108335",
        "affiliation_name": "State Key Laboratory of Hydro Science and Engineering",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Generation AI: Participatory Machine Learning Co-Design Projects with K-9 Students in Finland",
        "paper_author": "Tedre M.",
        "publication": "Annual Conference on Innovation and Technology in Computer Science Education, ITiCSE",
        "citied_by": "1",
        "cover_date": "2023-06-29",
        "Abstract": "In this poster, we present the results from the co-design school projects on machine learning. We address social and educational challenges in artificial intelligence including security, privacy and education. We employ the participatory co-design approach, which facilitates children's right to be heard, and positions them as active partners, advisers, and designers in research and development work on technology and socio-technological practices.",
        "DOI": "10.1145/3587103.3594171",
        "affiliation_name": "Itä-Suomen yliopisto",
        "affiliation_city": "Kuopio",
        "affiliation_country": "Finland"
    },
    {
        "paper_title": "Intelligent Intrusion Detection System using Enhanced Arithmetic Optimization Algorithm with Deep Learning Model",
        "paper_author": "Kavitha S.",
        "publication": "Tehnicki Vjesnik",
        "citied_by": "13",
        "cover_date": "2023-06-28",
        "Abstract": "The widespread use of interoperability and interconnectivity of computing systems is becoming indispensable for enhancing our day-to-day actions. The susceptibilities deem cyber-security systems necessary for assuming communication interchanges. Secure transmission needs security measures for combating the threats and required developments to security measures that counter evolving security risks. Though firewalls were devised to secure networks, in real-time they cannot detect intrusions. Hence, destructive cyber-attacks put forward severe security complexities, requiring reliable and adaptable intrusion detection systems (IDS) that could monitor unauthorized access, policy violations, and malicious activity practically. Conventional machine learning (ML) techniques were revealed for identifying data patterns and detecting cyber-attacks IDSs successfully. Currently, deep learning (DL) methods are useful for designing accurate and effective IDS methods. In this aspect, this study develops an intelligent IDS using enhanced arithmetic optimization algorithm with deep learning (IIDS-EAOADL) method. The presented IIDS-EAOADL model performs data standardization process to normalize the input data. Besides, equilibrium optimizer based feature selection (EOFS) approach is developed to elect an optimal subset of features. For intrusion detection, deep wavelet autoencoder (DWAE) classifier is applied. Since the proper tuning of parameters of the DWNN is highly important, EAOA algorithm is used to tune them. For assuring the simulation results of the IIDS-EAOADL technique, a widespread simulation analysis takes place using a benchmark dataset. The experimentation outcomes demonstrate the improvements of the IIDS-EAOADL model over other existing techniques",
        "DOI": "10.17559/TV-20221128071759",
        "affiliation_name": "Velammal College of Engineering and Technology",
        "affiliation_city": "Madurai",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Fairness in Contextual Resource Allocation Systems: Metrics and Incompatibility Results",
        "paper_author": "Jo N.",
        "publication": "Proceedings of the 37th AAAI Conference on Artificial Intelligence, AAAI 2023",
        "citied_by": "2",
        "cover_date": "2023-06-27",
        "Abstract": "We study critical systems that allocate scarce resources to satisfy basic needs, such as homeless services that provide housing. These systems often support communities disproportionately affected by systemic racial, gender, or other injustices, so it is crucial to design these systems with fairness considerations in mind. To address this problem, we propose a framework for evaluating fairness in contextual resource allocation systems that is inspired by fairness metrics in machine learning. This framework can be applied to evaluate the fairness properties of a historical policy, as well as to impose constraints in the design of new (counterfactual) allocation policies. Our work culminates with a set of incompatibility results that investigate the interplay between the different fairness metrics we propose. Notably, we demonstrate that: 1) fairness in allocation and fairness in outcomes are usually incompatible; 2) policies that prioritize based on a vulnerability score will usually result in unequal outcomes across groups, even if the score is perfectly calibrated; 3) policies using contextual information beyond what is needed to characterize baseline risk and treatment effects can be fairer in their outcomes than those using just baseline risk and treatment effects; and 4) policies using group status in addition to baseline risk and treatment effects are as fair as possible given all available information. Our framework can help guide the discussion among stakeholders in deciding which fairness metrics to impose when allocating scarce resources.",
        "DOI": "10.1609/aaai.v37i10.2636126397",
        "affiliation_name": "University of Southern California",
        "affiliation_city": "Los Angeles",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Learning to Play General-Sum Games against Multiple Boundedly Rational Agents",
        "paper_author": "Zhao E.",
        "publication": "Proceedings of the 37th AAAI Conference on Artificial Intelligence, AAAI 2023",
        "citied_by": "0",
        "cover_date": "2023-06-27",
        "Abstract": "We study the problem of training a principal in a multi-agent general-sum game using reinforcement learning (RL). Learning a robust principal policy requires anticipating the worst possible strategic responses of other agents, which is generally NP-hard. However, we show that no-regret dynamics can identify these worst-case responses in poly-time in smooth games. We propose a framework that uses this policy evaluation method for efficiently learning a robust principal policy using RL. This framework can be extended to provide robustness to boundedly rational agents too. Our motivating application is automated mechanism design: we empirically demonstrate our framework learns robust mechanisms in both matrix games and complex spatiotemporal games. In particular, we learn a dynamic tax policy that improves the welfare of a simulated trade-and-barter economy by 15%, even when facing previously unseen boundedly rational RL taxpayers.",
        "DOI": "10.1609/aaai.v37i10.2636126391",
        "affiliation_name": "Salesforce.com, Inc.",
        "affiliation_city": "San Francisco",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Provable Detection of Propagating Sampling Bias in Prediction Models",
        "paper_author": "Ravishankar P.",
        "publication": "Proceedings of the 37th AAAI Conference on Artificial Intelligence, AAAI 2023",
        "citied_by": "2",
        "cover_date": "2023-06-27",
        "Abstract": "With an increased focus on incorporating fairness in machine learning models, it becomes imperative not only to assess and mitigate bias at each stage of the machine learning pipeline but also to understand the downstream impacts of bias across stages. Here we consider a general, but realistic, scenario in which a predictive model is learned from (potentially biased) training data, and model predictions are assessed post-hoc for fairness by some auditing method. We provide a theoretical analysis of how a specific form of data bias, differential sampling bias, propagates from the data stage to the prediction stage. Unlike prior work, we evaluate the downstream impacts of data biases quantitatively rather than qualitatively and prove theoretical guarantees for detection. Under reasonable assumptions, we quantify how the amount of bias in the model predictions varies as a function of the amount of differential sampling bias in the data, and at what point this bias becomes provably detectable by the auditor. Through experiments on two criminal justice datasets- the well-known COMPAS dataset and historical data from NYPD's stop and frisk policy- we demonstrate that the theoretical results hold in practice even when our assumptions are relaxed.",
        "DOI": "10.1609/aaai.v37i8.26144",
        "affiliation_name": "New York University",
        "affiliation_city": "New York",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Local Justice and Machine Learning: Modeling and Inferring Dynamic Ethical Preferences toward Allocations",
        "paper_author": "Chen V.",
        "publication": "Proceedings of the 37th AAAI Conference on Artificial Intelligence, AAAI 2023",
        "citied_by": "1",
        "cover_date": "2023-06-27",
        "Abstract": "We consider a setting in which a social planner has to make a sequence of decisions to allocate scarce resources in a high-stakes domain. Our goal is to understand stakeholders' dynamic moral preferences toward such allocational policies. In particular, we evaluate the sensitivity of moral preferences to the history of allocations and their perceived future impact on various socially salient groups. We propose a mathematical model to capture and infer such dynamic moral preferences. We illustrate our model through small-scale human-subject experiments focused on the allocation of scarce medical resource distributions during a hypothetical viral epidemic. We observe that participants' preferences are indeed history- and impact-dependent. Additionally, our preliminary experimental results reveal intriguing patterns specific to medical resources-a topic that is particularly salient against the backdrop of the global covid-19 pandemic.",
        "DOI": "10.1609/aaai.v37i5.25737",
        "affiliation_name": "Carnegie Mellon University",
        "affiliation_city": "Pittsburgh",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Reinforcement Learning for Branch-and-Bound Optimisation Using Retrospective Trajectories",
        "paper_author": "Parsonson C.W.F.",
        "publication": "Proceedings of the 37th AAAI Conference on Artificial Intelligence, AAAI 2023",
        "citied_by": "10",
        "cover_date": "2023-06-27",
        "Abstract": "Combinatorial optimisation problems framed as mixed integer linear programmes (MILPs) are ubiquitous across a range of real-world applications. The canonical branch-and-bound algorithm seeks to exactly solve MILPs by constructing a search tree of increasingly constrained sub-problems. In practice, its solving time performance is dependent on heuristics, such as the choice of the next variable to constrain (‘branching’). Recently, machine learning (ML) has emerged as a promising paradigm for branching. However, prior works have struggled to apply reinforcement learning (RL), citing sparse rewards, difficult exploration, and partial observability as significant challenges. Instead, leading ML methodologies resort to approximating high quality handcrafted heuristics with imitation learning (IL), which precludes the discovery of novel policies and requires expensive data labelling. In this work, we propose retro branching; a simple yet effective approach to RL for branching. By retrospectively deconstructing the search tree into multiple paths each contained within a sub-tree, we enable the agent to learn from shorter trajectories with more predictable next states. In experiments on four combinatorial tasks, our approach enables learning-to-branch without any expert guidance or pre-training. We outperform the current state-of-the-art RL branching algorithm by 3-5× and come within 20% of the best IL method’s performance on MILPs with 500 constraints and 1000 variables, with ablations verifying that our retrospectively constructed trajectories are essential to achieving these results.",
        "DOI": "10.1609/aaai.v37i4.25521",
        "affiliation_name": "InstaDeep",
        "affiliation_city": null,
        "affiliation_country": null
    },
    {
        "paper_title": "Learning Control Policies for Stochastic Systems with Reach-Avoid Guarantees",
        "paper_author": "Žikelić Đ.",
        "publication": "Proceedings of the 37th AAAI Conference on Artificial Intelligence, AAAI 2023",
        "citied_by": "10",
        "cover_date": "2023-06-27",
        "Abstract": "We study the problem of learning controllers for discrete-time non-linear stochastic dynamical systems with formal reach-avoid guarantees. This work presents the first method for providing formal reach-avoid guarantees, which combine and generalize stability and safety guarantees, with a tolerable probability threshold p ∈ [0, 1] over the infinite time horizon in general Lipschitz continuous systems. Our method leverages advances in machine learning literature and it represents formal certificates as neural networks. In particular, we learn a certificate in the form of a reach-avoid supermartingale (RASM), a novel notion that we introduce in this work. Our RASMs provide reachability and avoidance guarantees by imposing constraints on what can be viewed as a stochastic extension of level sets of Lyapunov functions for deterministic systems. Our approach solves several important problems – it can be used to learn a control policy from scratch, to verify a reach-avoid specification for a fixed control policy, or to fine-tune a pre-trained policy if it does not satisfy the reach-avoid specification. We validate our approach on 3 stochastic non-linear reinforcement learning tasks.",
        "DOI": "10.1609/aaai.v37i10.26407",
        "affiliation_name": "Institute of Science and Technology Austria (ISTA)",
        "affiliation_city": "Klosterneuburg",
        "affiliation_country": "Austria"
    },
    {
        "paper_title": "Spatial Patterns of Nonlinear Effects of Built Environment on Beijing Subway Ridership",
        "paper_author": "He P.",
        "publication": "Jiaotong Yunshu Xitong Gongcheng Yu Xinxi/Journal of Transportation Systems Engineering and Information Technology",
        "citied_by": "4",
        "cover_date": "2023-06-25",
        "Abstract": "Machine learning is a nonlinear approach to examine the effects of built environment on station-level ridership of urban rail transit. But existing interpretation approach are not able to analyze the spatial patterns of results derived from machine learning models. This paper first utilizes multi-source location-based big data to quantify the indicators of built environment of urban rail transit, and then uses the κ indictor to analyze the spatial distribution of nonlinear effect of built environment on urban rail transit passenger flow based on the extreme gradient boosting incorporating local regression technique. The case study of Beijing subway shows that the nonlinear effects of built environment factors on egress flow of urban rail transit is significantly different. The top three indictors of built environment, including employment density, public accessibility, and mixed land use, account for 42.51% of total importance of variables. The spatial distributions of κ indictor of these three built environment factors show significant spatial heterogeneity, indicating the spatial non-stationary relationship between the passenger flow and built environment. The nonlinear spatial patterns indicate that the development of urban rail transit stations should not only adopt differentiated policies and strategies in different areas, but should also determine reasonable lower limit of resource to activate threshold effect of built environment, which would help to increase the ridership of urban rail transit.",
        "DOI": "10.16097/j.cnki.1009-6744.2023.03.020",
        "affiliation_name": "Beijing Jiaotong University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Mapping urban heritage images with social media data and artificial intelligence, a case study in testaccio, Rome",
        "paper_author": "Bai N.",
        "publication": "International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",
        "citied_by": "8",
        "cover_date": "2023-06-24",
        "Abstract": "The UNESCO 2011 Recommendation on the Historic Urban Landscape promotes to map cultural significance of urban heritage from the perspectives of the general public in pursuit of social inclusion in heritage management. The user-generated information already available on social media platforms in the form of images, comments, and ratings can be considered a rich source for collecting data concerning the tourists' image of destinations and their collective perception of urban cultural heritage. Considering the large amount of unstructured data, artificial intelligence (AI) can construct structured feature vectors therefrom and significantly aid the analysis and collation processes compared to the traditional manual approach for mapping public perception of cultural heritage. This paper presents an exploratory case study conducted in the area of Testaccio, Rome, showcasing the use of AI to map the perceived and narrated urban heritage images using social media data. An image-sharing platform, Flickr, is used to collect thousands of posts containing images and comments in the area, which are further analysed with pre-Trained image recognition, natural language processing, and dimensionality reduction algorithms. Results as the urban heritage images are visualised, showing the most significant elements from a public perspective. Such a methodology provides an alternative perspective of viewing the urban heritage attributes as a collection of depicted and posted content. It can contribute as a tool for the documentation of collective attention for inclusive heritage management and local development planning during the designing and policy-making processes.",
        "DOI": "10.5194/isprs-Archives-XLVIII-M-2-2023-139-2023",
        "affiliation_name": "Universiteit Twente",
        "affiliation_city": "Enschede",
        "affiliation_country": "Netherlands"
    },
    {
        "paper_title": "A Study of Global COVID-19 Outbreak",
        "paper_author": "Subashini N.",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2023-06-21",
        "Abstract": "The first confirmed detection of COVID-19 was in December, 2019 with symptomatic fever, cough, chest pain, bilateral pneumonia, and acute respiratory distress syndrome [1]. It was observed among the RT-PCR tested patients in Wuhan, Hubei Province, China. Later due to community transmission worldwide, COVID-19 was designated as a global pandemic by the WHO in February, 2020. This virus builds mutation as it transmits forming variants of concern. The first appeared Alpha variant popped up in the United Kingdom in September 2020. While in India, Delta variant is spreading rapidly for which the developed vaccines could offer protection against serious hospitalization. [2] There has been numerous researches to build a measurable (statistical/machine learning) model which can possibly predict the course of this pandemic to some extent. Time-Series forecasting is a widely deployed method adopted in studying breeding of pathogens, their resultant infections and possible epidemics. The Schalekamp et al[3], 2020 developed a risk model to forecast deaths using clinical reports. The Malki et al., 2020 [4] research formed a machine learning model using weather elements to evaluate transmission. In other parts of the scientific community, Hao, Xu, Hu, Wang generated a model[5] using Long Short-Term Memory (LSTM), Support Vector Machines (SVM) and Elman Neural Network. Ji et al., 2020[6] presented papers which generated powerful estimation techniques by means of Multivariate COX Regression. Nonetheless, the modified model built using susceptible-exposed-infectious-recovered (SIER) and Runge-Kutta methods is appraised to be remarkable.[7] All of these evaluate the effectiveness of policies and guidelines of a country. The objective is to detect such trends in COVID-19 cases, deaths and vaccinations and further analyze them using publicly available dataset distributed by Our World in Data [8] where each regional data is collected, merged and uploaded in comma-separated value (CSV) file. This dataset covers figures reported by governments and ministries of health from countries around the world from December 13, 2020 to the present day.",
        "DOI": "10.1063/5.0114355",
        "affiliation_name": "Ethiraj College For Women",
        "affiliation_city": "Chennai",
        "affiliation_country": "India"
    },
    {
        "paper_title": "A denoising model based on multi-agent reinforcement learning with data transformation for digital tomosynthesis",
        "paper_author": "Nam K.",
        "publication": "Physics in Medicine and Biology",
        "citied_by": "3",
        "cover_date": "2023-06-21",
        "Abstract": "Objective. Denoising models based on the supervised learning have been proposed for medical imaging. However, its clinical availability in digital tomosynthesis (DT) imaging is limited due to the necessity of a large amount of training data for providing acceptable image quality and the difficulty in minimizing a loss. Reinforcement learning (RL) can provide the optimal pollicy, which maximizes a reward, with a small amount of training data for implementing a task. In this study, we presented a denoising model based on the multi-agent RL for DT imaging in order to improve the performance of the machine learning-based denoising model. Approach. The proposed multi-agent RL network consisted of shared sub-network, value sub-network with a reward map convolution (RMC) technique and policy sub-network with a convolutional gated recurrent unit (convGRU). Each sub-network was designed for implementing feature extraction, reward calculation and action execution, respectively. The agents of the proposed network were assigned to each image pixel. The wavelet and Anscombe transformations were applied to DT images for delivering precise noise features during network training. The network training was implemented with the DT images obtained from the three-dimensional digital chest phantoms, which were constructed by using clinical CT images. The performance of the proposed denoising model was evaluated in terms of signal-to-noise ratio (SNR), structural similarity (SSIM) and peak signal-to-noise ratio (PSNR). Main results. Comparing the supervised learning, the proposed denoising model improved the SNRs of the output DT images by 20.64% while maintaining the similar SSIMs and PSNRs. In addition, the SNRs of the output DT images with the wavelet and Anscombe transformations were 25.88 and 42.95% higher than that for the supervised learning, respectively. Significance. The denoising model based on the multi-agent RL can provide high-quality DT images, and the proposed method enables the performance improvement of machine learning-based denoising models.",
        "DOI": "10.1088/1361-6560/acd615",
        "affiliation_name": "Konyang University",
        "affiliation_city": "Nonsan",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "A Distributed-GPU Deep Reinforcement Learning System for Solving Large Graph Optimization Problems",
        "paper_author": "Zheng W.",
        "publication": "ACM Transactions on Parallel Computing",
        "citied_by": "1",
        "cover_date": "2023-06-20",
        "Abstract": "Graph optimization problems (such as minimum vertex cover, maximum cut, traveling salesman problems) appear in many fields including social sciences, power systems, chemistry, and bioinformatics. Recently, deep reinforcement learning (DRL) has shown success in automatically learning good heuristics to solve graph optimization problems. However, the existing RL systems either do not support graph RL environments or do not support multiple or many GPUs in a distributed setting. This has compromised the ability of reinforcement learning in solving large-scale graph optimization problems due to lack of parallelization and high scalability. To address the challenges of parallelization and scalability, we develop RL4GO, a high-performance distributed-GPU DRL framework for solving graph optimization problems. RL4GO focuses on a class of computationally demanding RL problems, where both the RL environment and policy model are highly computation intensive. Traditional reinforcement learning systems often assume either the RL environment is of low time complexity or the policy model is small. In this work, we distribute large-scale graphs across distributed GPUs and use the spatial parallelism and data parallelism to achieve scalable performance. We compare and analyze the performance of the spatial parallelism and data parallelism and show their differences. To support graph neural network (GNN) layers that take as input data samples partitioned across distributed GPUs, we design parallel mathematical kernels to perform operations on distributed 3D sparse and 3D dense tensors. To handle costly RL environments, we design a parallel graph environment to scale up all RL-environment-related operations. By combining the scalable GNN layers with the scalable RL environment, we are able to develop high-performance RL4GO training and inference algorithms in parallel. Furthermore, we propose two optimization techniques - replay buffer on-the-fly graph generation and adaptive multiple-node selection - to minimize the spatial cost and accelerate reinforcement learning. This work also conducts in-depth analyses of parallel efficiency and memory cost and shows that the designed RL4GO algorithms are scalable on numerous distributed GPUs. Evaluations on large-scale graphs show that (1) RL4GO training and inference can achieve good parallel efficiency on 192 GPUs, (2) its training time can be 18 times faster than the state-of-the-art Gorila distributed RL framework [34], and (3) its inference performance achieves a 26 times improvement over Gorila.",
        "DOI": "10.1145/3589188",
        "affiliation_name": "Indiana University-Purdue University Indianapolis",
        "affiliation_city": "Indianapolis",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Quantifying the potential persuasive returns to political microtargeting",
        "paper_author": "Tappin B.M.",
        "publication": "Proceedings of the National Academy of Sciences of the United States of America",
        "citied_by": "26",
        "cover_date": "2023-06-20",
        "Abstract": "Much concern has been raised about the power of political microtargeting to sway voters' opinions, influence elections, and undermine democracy. Yet little research has directly estimated the persuasive advantage of microtargeting over alternative campaign strategies. Here, we do so using two studies focused on U.S. policy issue advertising. To implement a microtargeting strategy, we combined machine learning with message pretesting to determine which advertisements to show to which individuals to maximize persuasive impact. Using survey experiments, we then compared the performance of this microtargeting strategy against two other messaging strategies. Overall, we estimate that our microtargeting strategy outperformed these strategies by an average of 70% or more in a context where all of the messages aimed to influence the same policy attitude (Study 1). Notably, however, we found no evidence that targeting messages by more than one covariate yielded additional persuasive gains, and the performance advantage of microtargeting was primarily visible for one of the two policy issues under study. Moreover, when microtargeting was used instead to identify which policy attitudes to target with messaging (Study 2), its advantage was more limited. Taken together, these results suggest that the use of microtargeting - combining message pretesting with machine learning - can potentially increase campaigns' persuasive influence and may not require the collection of vast amounts of personal data to uncover complex interactions between audience characteristics and political messaging. However, the extent to which this approach confers a persuasive advantage over alternative strategies likely depends heavily on context.",
        "DOI": "10.1073/pnas.2216261120",
        "affiliation_name": "Massachusetts Institute of Technology",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Towards sustainable environment in Somalia: The role of conflicts, urbanization, and globalization on environmental degradation and emissions",
        "paper_author": "Warsame A.A.",
        "publication": "Journal of Cleaner Production",
        "citied_by": "45",
        "cover_date": "2023-06-20",
        "Abstract": "Climate change is a global phenomenon in the 21st century. Hence, achieving environmental sustainability has become a global initiative to tackle the repercussions of climate change. Fossil fuel energy consumption and economic growth remain critical amidst environmental degradation and emissions. Contrary to the previous attempts, this study examines the impacts of conflicts – internal and external –, urbanization, and globalization on environmental degradation and emissions in Somalia. The autoregressive distributed lag (ARDL) model, kernelized regularized least squares (KRLS) machine learning method, and vector error correction modeling (VECM) method are utilized with annual time series data spanning 1985–2016. The empirical results show that external conflict, globalization, and urbanization increase environmental degradation in the long run but not in the short run, except globalization which has a constructive role in enhancing environmental quality in the short-run. Notably, internal conflict is inconsequential both in the short- and long-run. The results of the study are robust for various analysis methods and environmental pollution indicators. In contrast, the VECM results indicate that urbanization, economic growth, and internal and external conflicts Granger cause environmental degradation both in the short and long-run, whereas globalization causes environmental degradation in the short run only. Notably, there is bidirectional causality between urbanization and environmental degradation in the short run only. A striking result is that both internal and external conflicts are neither caused by environmental degradation nor other regressors in the short- and long-run. Hence, relevant policy implications are suggested based on the empirical findings of the study.",
        "DOI": "10.1016/j.jclepro.2023.136856",
        "affiliation_name": "SIMAD University",
        "affiliation_city": "Mogadishu",
        "affiliation_country": "Somalia"
    },
    {
        "paper_title": "A deep learning-based novel approach to generate continuous daily stream nitrate concentration for nitrate data-sparse watersheds",
        "paper_author": "Saha G.K.",
        "publication": "Science of the Total Environment",
        "citied_by": "23",
        "cover_date": "2023-06-20",
        "Abstract": "High-frequency stream nitrate concentration provides critical insights into nutrient dynamics and can help to improve the effectiveness of management decisions to maintain a sustainable ecosystem. However, nitrate monitoring is conventionally conducted through lab analysis using in situ water samples and is typically at coarse temporal resolution. In the last decade, many agencies started collecting high-frequency (5–60 min intervals) nitrate data using optical sensors. The hypothesis of the study is that the data-driven models can learn the trend and temporal variability in nitrate concentration from high-frequency sensor-based nitrate data in the region and generate continuous nitrate data for unavailable data periods and data-limited locations. A Long Short-Term Memory (LSTM) model-based framework was developed to estimate continuous daily stream nitrate for dozens of gauge locations in Iowa, USA. The promising results supported the hypothesis; the LSTM model demonstrated median test-period Nash-Sutcliffe efficiency (NSE) = 0.75 and RMSE = 1.53 mg/L for estimating continuous daily nitrate concentration in 42 sites, which are unprecedented performance levels. Twenty-one sites (50 % of all sites) and thirty-four sites (76 % of all sites) demonstrated NSE > 0.75 and 0.50, respectively. The average nitrate concentration of neighboring sites was identified as a crucial determinant of continuous daily nitrate concentration. Seasonal model performance evaluation showed that the model performed effectively in the summer and fall seasons. About 26 sites showed correlations >0.60 between estimated nitrate concentration and discharge. The concentration-discharge (c-Q) relationship analysis showed that the study watersheds had four dominant nitrate transport patterns from landscapes to streams with increasing discharge, including the flushing pattern being the most dominant one. Stream nitrate estimation impedes due to data inadequacy. The modeling framework can be used to generate temporally continuous nitrate at nitrate data-limited regions with a nearby sensor-based nitrate gauge. Watershed planners and policymakers could utilize the continuous nitrate data to gain more information on the regional nitrate status and design conservation practices accordingly.",
        "DOI": "10.1016/j.scitotenv.2023.162930",
        "affiliation_name": "Department of Agricultural and Biological Engineering",
        "affiliation_city": "University Park",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Detecting and Measuring Aggressive Location Harvesting in Mobile Apps via Data-flow Path Embedding",
        "paper_author": "Lu H.",
        "publication": "Performance Evaluation Review",
        "citied_by": "0",
        "cover_date": "2023-06-19",
        "Abstract": "Today, location-based services have become prevalent in the mobile platform, where mobile apps provide specific services to a user based on his or her location. Unfortunately, mobile apps can aggressively harvest location data with much higher accuracy and frequency than they need because the coarse-grained access control mechanism currently implemented in mobile operating systems (e.g., Android) cannot regulate such behavior. This unnecessary data collection violates the data minimization policy, yet no previous studies have investigated privacy violations from this perspective, and existing techniques are insufficient to address this violation. To fill this knowledge gap, we take the first step toward detecting and measuring this privacy risk in mobile apps at scale. Particularly, we annotate and release the first dataset to characterize those aggressive location harvesting apps and understand the challenges of automatic detection and classification. Next, we present a novel system, LocationScope, to address these challenges by (i) uncovering how an app collects locations and how to use such data through a fine-tuned value set analysis technique, (ii) recognizing the fine-grained location-based services an app provides via embedding data-flow paths, which is a combination of program analysis and machine learning techniques, extracted from its location data usages, and (iii) identifying aggressive apps with an outlier detection technique achieving a precision of 97% in aggressive app detection. Our technique has further been applied to millions of free Android apps from Google Play as of 2019 and 2021. Highlights of our measurements on detected aggressive apps include their growing trend from 2019 to 2021 and the app generators' significant contribution of aggressive location harvesting apps.",
        "DOI": "10.1145/3606376.3593535",
        "affiliation_name": "Indiana University Bloomington",
        "affiliation_city": "Bloomington",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Detecting and Measuring Aggressive Location Harvesting in Mobile Apps via Data-flow Path Embedding",
        "paper_author": "Lu H.",
        "publication": "SIGMETRICS 2023 - Abstract Proceedings of the 2023 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems",
        "citied_by": "1",
        "cover_date": "2023-06-19",
        "Abstract": "Today, location-based services have become prevalent in the mobile platform, where mobile apps provide specific services to a user based on his or her location. Unfortunately, mobile apps can aggressively harvest location data with much higher accuracy and frequency than they need because the coarse-grained access control mechanism currently implemented in mobile operating systems (e.g., Android) cannot regulate such behavior. This unnecessary data collection violates the data minimization policy, yet no previous studies have investigated privacy violations from this perspective, and existing techniques are insufficient to address this violation. To fill this knowledge gap, we take the first step toward detecting and measuring this privacy risk in mobile apps at scale. Particularly, we annotate and release the first dataset to characterize those aggressive location harvesting apps and understand the challenges of automatic detection and classification. Next, we present a novel system, LocationScope, to address these challenges by (i) uncovering how an app collects locations and how to use such data through a fine-Tuned value set analysis technique, (ii) recognizing the fine-grained location-based services an app provides via embedding data-flow paths, which is a combination of program analysis and machine learning techniques, extracted from its location data usages, and (iii) identifying aggressive apps with an outlier detection technique achieving a precision of 97% in aggressive app detection. Our technique has further been applied to millions of free Android apps from Google Play as of 2019 and 2021. Highlights of our measurements on detected aggressive apps include their growing trend from 2019 to 2021 and the app generators' significant contribution of aggressive location harvesting apps.",
        "DOI": "10.1145/3578338.3593535",
        "affiliation_name": "Indiana University Bloomington",
        "affiliation_city": "Bloomington",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "GPU Job Scheduling based on Deep Reinforcement Learning",
        "paper_author": "Chen K.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-06-17",
        "Abstract": "The development of GPU and machine learning workloads has been profound; however, resource management and allocation have become difficult problems. The methods used nowadays are not suitable for handling numerous data. Inspired by recent advancements in reinforcement learning, which have shown success in solving real AI problems, this paper explores the use of Policy Gradient, Deep Q-Network, and Double Deep Q-Network approaches to optimize GPU job scheduling. The paper introduces a new GPU job scheduling model called DeepGJS (deep GPU job scheduling), which enables efficient resource management by allocating jobs to servers in an optimized manner. DeepGJS aims to minimize slowdown and completion time during job allocation and scheduling. Initial results and analysis indicate that DeepGJS can adapt to various conditions, converge quickly, and outperform baseline methods such as short job first and packer, as well as the DeepRM [11] model. The observed differences between these methods help determine the most suitable approach for different scenarios.",
        "DOI": "10.1145/3606043.3606049",
        "affiliation_name": "Beijing Normal University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Resource allocation with edge computing in IoT networks via machine learning and deep learning",
        "paper_author": "Sahu H.",
        "publication": "AIP Conference Proceedings",
        "citied_by": "1",
        "cover_date": "2023-06-16",
        "Abstract": "In this paper, we have prepared an optimal offloading scheme for resource allocation in IOT networks with edge computing. We have explored various machine learning and deep learning models for creating an optimal offloading scheme. ML and DL models outperforms most of the traditional methods of deciding the offloading scheme in a very cost effective manner. Edge computing is a very sensational invention in today's era. It has improved the functionality of countless devices. The computing ability of IOT devices have also improved with the latest discoveries. To provide the IOT user a good quality of service, an optimal offloading scheme is needed to decide whether to run a task locally or it should be offloaded to the edge server. We have made clusters of the tasks on the basis of priority scores given to the tasks according to various factors. We have used k-means clustering for creating the clusters and then a SVM machine learning model is used to classify the tasks. The classified groups are then sent to a Deep Q-Network where the tasks in each group are further classified by learning the optimal policy using Q-function in Q-learning. It also improved the efficiency of the model. Our offloading scheme is cost efficient as well as it also ensures a quality of service.",
        "DOI": "10.1063/5.0148082",
        "affiliation_name": "National Institute of Technology Raipur",
        "affiliation_city": "Raipur",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Accelerated Discovery of Advanced Thermoelectric Materials via Transfer Learning",
        "paper_author": "Li M.",
        "publication": "Advanced Energy Materials",
        "citied_by": "5",
        "cover_date": "2023-06-16",
        "Abstract": "Thermoelectric (TE) technology can realize direct conversion of widely distributed heat into useful electricity, which provides a promising route to solve the global energy crisis that is increasingly severe. However, it is extremely complex and time-consuming to discover advanced TE materials via conventional trial-and-error approaches. In this work, using a pre-trained neural network architecture for the electronic bandgap, a transfer learning (TL) strategy that allows ready and accurate prediction on the ZT values of any TE materials at arbitrary temperature is proposed. Compared with direct machine learning algorithms, the TL-driven model exhibits significantly enhanced predictive power beyond the initial dataset, as characterized by improved Pearson correlation coefficient (reduced mean absolute error) from 23% to 95% (0.35 to 0.07) for the p-type systems, and 46% to 94% (0.23 to 0.06) for the n-type systems. By screening 6353 possible candidates in the AFLOW repository that having relatively smaller gaps, 925 p-type and 788 n-type systems are quickly identified to exhibit ZT exceeding 2.0. Equally importantly, the established TL model is highly adaptable to ZT prediction in an even larger search space, where the constituent atoms and/or stoichiometric compositions of the screened systems may be variously tuned to further optimize their TE performance.",
        "DOI": "10.1002/aenm.202300049",
        "affiliation_name": "Wuhan University",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Using machine learning algorithms to identify predictors of social vulnerability in the event of a hazard: Istanbul case study",
        "paper_author": "Kalaycloǧlu O.",
        "publication": "Natural Hazards and Earth System Sciences",
        "citied_by": "2",
        "cover_date": "2023-06-15",
        "Abstract": "To what extent an individual or group will be affected by the damage of a hazard depends not just on their exposure to the event but on their social vulnerability - that is, how well they are able to anticipate, cope with, resist, and recover from the impact of a hazard. Therefore, for mitigating disaster risk effectively and building a disaster-resilient society to natural hazards, it is essential that policy makers develop an understanding of social vulnerability. This study aims to propose an optimal predictive model that allows decision makers to identify households with high social vulnerability by using a number of easily accessible household variables. In order to develop such a model, we rely on a large dataset comprising a household survey (n = 41 093) that was conducted to generate a social vulnerability index (SoVI) in Istanbul, Türkiye. In this study, we assessed the predictive ability of socio-economic, socio-demographic, and housing conditions on the household-level social vulnerability through machine learning models. We used classification and regression tree (CART), random forest (RF), support vector machine (SVM), naïve Bayes (NB), artificial neural network (ANN), k-nearest neighbours (KNNs), and logistic regression to classify households with respect to their social vulnerability level, which was used as the outcome of these models. Due to the disparity of class size outcome variables, subsampling strategies were applied for dealing with imbalanced data. Among these models, ANN was found to have the optimal predictive performance for discriminating households with low and high social vulnerability when random-majority under sampling was applied (area under the curve (AUC): 0.813). The results from the ANN method indicated that lack of social security, living in a squatter house, and job insecurity were among the most important predictors of social vulnerability to hazards. Additionally, the level of education, the ratio of elderly persons in the household, owning a property, household size, ratio of income earners, and savings of the household were found to be associated with social vulnerability. An open-access R Shiny web application was developed to visually display the performance of machine learning (ML) methods, important variables for the classification of households with high and low social vulnerability, and the spatial distribution of the variables across Istanbul neighbourhoods. The machine learning methodology and the findings that we present in this paper can guide decision makers in identifying social vulnerability effectively and hence let them prioritise actions towards vulnerable groups in terms of needs prior to an event of a hazard.",
        "DOI": "10.5194/nhess-23-2133-2023",
        "affiliation_name": "University College London",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "What Artificial Intelligence Chatbots Mean for Editors, Authors, and Readers of Peer-Reviewed Ophthalmic Literature",
        "paper_author": "Bressler N.M.",
        "publication": "JAMA Ophthalmology",
        "citied_by": "11",
        "cover_date": "2023-06-15",
        "Abstract": "NA",
        "DOI": "10.1001/jamaophthalmol.2023.1370",
        "affiliation_name": "Wilmer Eye Institute",
        "affiliation_city": "Baltimore",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Overview of ICARUS─A Curated, Open Access, Online Repository for Atmospheric Simulation Chamber Data",
        "paper_author": "Nguyen T.B.",
        "publication": "ACS Earth and Space Chemistry",
        "citied_by": "2",
        "cover_date": "2023-06-15",
        "Abstract": "Atmospheric simulation chambers continue to be indispensable tools for research in the atmospheric sciences. Insights from chamber studies are integrated into atmospheric chemical transport models, which are used for science-informed policy decisions. However, a centralized data management and access infrastructure for their scientific products had not been available in the United States and many parts of the world. ICARUS (Integrated Chamber Atmospheric data Repository for Unified Science) is an open access, searchable, web-based infrastructure for storing, sharing, discovering, and utilizing atmospheric chamber data [https://icarus.ucdavis.edu]. ICARUS has two parts: a data intake portal and a search and discovery portal. Data in ICARUS are curated, uniform, interactive, indexed on popular search engines, mirrored by other repositories, version-tracked, vocabulary-controlled, and citable. ICARUS hosts both legacy data and new data in compliance with open access data mandates. Targeted data discovery is available based on key experimental parameters, including organic reactants and mixtures that are managed using the PubChem chemical database, oxidant information, nitrogen oxide (NOx) content, alkylperoxy radical (RO2) fate, seed particle information, environmental conditions, and reaction categories. A discipline-specific repository such as ICARUS with high amounts of metadata works to support the evaluation and revision of atmospheric model mechanisms, intercomparison of data and models, and the development of new model frameworks that can have more predictive power in the current and future atmosphere. The open accessibility and interactive nature of ICARUS data may also be useful for teaching, data mining, and training machine learning models.",
        "DOI": "10.1021/acsearthspacechem.3c00043",
        "affiliation_name": "Cockrell School of Engineering",
        "affiliation_city": "Austin",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Countrywide mapping and assessment of organic carbon saturation in the topsoil using machine learning-based pedotransfer function with uncertainty propagation",
        "paper_author": "Szatmári G.",
        "publication": "Catena",
        "citied_by": "13",
        "cover_date": "2023-06-15",
        "Abstract": "Stakeholders and policymakers have been becoming more and more interested not just in the potential organic carbon (SOC) saturation level of soils but also in spatially explicit information on the degree of SOC deficit, which can support future policy and sustainable management strategies, and carbon sequestration-associated spatial planning. Thus the objective of our study was to develop a cubist-based pedotransfer function (PTF) for predicting and mapping the saturated SOC content of the topsoils (0–30 cm) in Hungary and then compare the resulting map with the actual SOC map to determine and assess the degree of SOC deficit. It was assumed that topsoils covered by permanent forests can be practically considered as saturated in SOC. Using the monitoring points of the Hungarian Soil Information and Monitoring System located in forests as reference soil profiles, we developed a cubist-based PTF. The transparent model structure provided by cubist allowed to show that not just the physicochemical properties of soils (e.g., texture, and pH) but also environmental conditions, such as topography (e.g., slope, altitude, and topographical position) and climate (e.g., long-term mean annual temperature, and evaporation), characterizing landscape are important factors in predicting the level of SOC saturation. Our results also pointed out that there is SOC deficit on large part of the country (∼80%) showing high spatial variability. It was also revealed that the most considerable potential for additional SOC sequestration can be found related to soils with medium to high actual SOC content.",
        "DOI": "10.1016/j.catena.2023.107086",
        "affiliation_name": "Geographical Institute",
        "affiliation_city": "Budapest",
        "affiliation_country": "Hungary"
    },
    {
        "paper_title": "How good are learning-based control v.s. model-based control for load shifting? Investigations on a single zone building energy system",
        "paper_author": "Fu Y.",
        "publication": "Energy",
        "citied_by": "16",
        "cover_date": "2023-06-15",
        "Abstract": "Both model predictive control (MPC) and deep reinforcement learning control (DRL) have been presented as a way to approximate the true optimality of a dynamic programming problem, and these two have shown significant operational cost saving potentials for building energy systems. However, there is still a lack of in-depth quantitative studies on their approximation levels to the true optimality, especially in the building energy domain. To fill in the gap, this paper provides a numerical framework that enables the evaluation of the optimality levels of different controllers for building energy systems. This framework is then used to comprehensively compare the optimal control performance of both MPC and DRL controllers with given computation budgets for a single zone fan coil unit system. Note the optimality is estimated based on a user-specific selection of trade-off weights among energy costs, thermal comfort and control slew rates. Compared with the best optimality we can find through expensive optimization simulations, the best DRL agent can maximally approximate the optimality by 96.54%, which outperforms the best MPC whose optimality level is 90.11%. However, due to the stochasticity, the DRL agent is only expected to approximate the optimality by 90.42%, which is almost equivalent to the best MPC. Except for Proximal Policy Optimization (PPO), all DRL agents can have a better approximation to the optimality than the best MPC, and are expected to have better approximation than the MPC with a prediction horizon of 32 steps (15 min per step). In terms of reducing energy cost and thermal discomfort, MPC can outperform the rule-based control (RBC) by 18.47%–25.44%. DRL can be expected to outperform RBC by 18.95%–25.65%, and the best DRL control policy can outperform RBC by 20.29%–29.72%. Although the comparison of the optimality level is performed in a perfect setting, e.g., MPC assumes perfect models, and DRL assumes a perfect offline training process and online deployment process, this can shed insight on their capabilities of approximating to the original dynamic programming problem.",
        "DOI": "10.1016/j.energy.2023.127073",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "College Station",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Predicting the energy consumption in buildings using the optimized support vector regression model",
        "paper_author": "Cai W.",
        "publication": "Energy",
        "citied_by": "47",
        "cover_date": "2023-06-15",
        "Abstract": "One of the most significant axes of regional, national, and worldwide energy policy is energy efficiency in building design. In particular, the energy efficiency of HVAC systems is of paramount importance. They provide energy for both residential and commercial sectors. As a result, assessing building energy usage is a critical step in optimizing building energy consumption. The impact of eight input factors on the two output variables, heating and cooling loads, for residential structures was explored in this study. For this purpose, the SVR-supervised machine learning algorithm was used. Despite its advantages, such as predictive accuracy and robustness, this method suffers from the fact that there is no specific rule for fitting its parameters. Therefore, six meta-heuristic optimization algorithms were investigated, and the strongest algorithm for optimal parameter fitting for the SVR model was presented. The correlation and error parameters analysis showed that the hybrid model SVR-AEO has the best performance in simulating residential buildings' heating and cooling loads. According to the obtained results, the value of R2 for the cooling and heating loads prediction in the training data is obtained to be 0.9975 and 0.99955, respectively.",
        "DOI": "10.1016/j.energy.2023.127188",
        "affiliation_name": "Ningbo University of Technology",
        "affiliation_city": "Ningbo",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Impact of leachate and landfill gas on the ecosystem and health: Research trends and the way forward towards sustainability",
        "paper_author": "Ghosh A.",
        "publication": "Journal of Environmental Management",
        "citied_by": "39",
        "cover_date": "2023-06-15",
        "Abstract": "Globally, a whopping increase in solid waste (SW) generation and the risks posed by climate change are major concerns. A wide spread practice for disposal of municipal solid waste (MSW) is landfill, which swells with population and urbanization. Waste, if treated properly, can be used to produce renewable energy. The recent global event COP 27 mainly stressed on production of renewable energy to achieve the Net Zero target. The MSW landfill is the most significant anthropogenic source of methane (CH4) emission. On one side, CH4 is a greenhouse gas (GHG), and on the other it is a main component of biogas. Wastewater that collects due to rainwater percolation in landfills creates landfill leachate. There is a need to understand global landfill management practices thoroughly for implementation of better practices and policies related to this threat. This study critically reviews recent publications on leachate and landfill gas. The review discusses leachate treatment and landfill gas emissions, focusing on the possible reduction technology of CH4 emission and its impact on the environment. Mixed leachate will benefit from the combinational therapy method because of its intricate combination. Implementation of circular material management, entrepreneurship ideas, blockchain, machine learning, LCA usage in waste management, and economic benefits from CH4 production have been emphasized. Bibliometric analysis of 908 articles from the last 37 years revealed that industrialized nations dominate this research domain, with the United States having the highest number of citations.",
        "DOI": "10.1016/j.jenvman.2023.117708",
        "affiliation_name": "Indian Institute of Management Sirmaur",
        "affiliation_city": "Paonta Sahib",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Machine learning-based approach for ballistic coefficient estimation of resident space objects in LEO",
        "paper_author": "Cimmino N.",
        "publication": "Advances in Space Research",
        "citied_by": "9",
        "cover_date": "2023-06-15",
        "Abstract": "The increasing number of Resident Space Objects poses a serious threat to the safe operation of satellites. Alongside with mitigation policies, it is fundamental to predict the trajectories of such objects which requires the accurate estimation of the physical characteristics that influence their orbits. An important role is played by the area-to-mass ratio, i.e., the ratio between the area exposed to the atmosphere/the Sun and the mass of the space object for the LEO/MEO and GEO region. Current literature proposes several approaches for the estimation of the area-to-mass ratio, ranging from semi-analytical to numerical methods. As regards the latter category, recent studies have focused on classification or regression algorithms for specific types of orbits (e.g., sun-synchronous, geostationary). In this context, this paper proposes a machine learning-based regression approach for the estimation of the ballistic coefficient (i.e., the product between the drag coefficient and the area-to-mass ratio) in Low Earth Orbit, covering a wide set of orbital parameters. Using a synthetic space catalogue, the performance of different types of machine learning techniques is evaluated and compared. A sensitivity analysis is carried out to analyse the effect on performance of the number of trainings, the propagation time, the number of objects of the training data set, and the frequency of the measurements. The applicability of the presented approach is tested and discussed using both real (i.e., based on publicly available catalogues of Two-Line Elements) and synthetic datasets.",
        "DOI": "10.1016/j.asr.2023.02.007",
        "affiliation_name": "Università degli Studi di Napoli Federico II",
        "affiliation_city": "Naples",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Content Popularity Prediction via Federated Learning in Cache-Enabled Wireless Networks",
        "paper_author": "Yan Y.",
        "publication": "ZTE Communications",
        "citied_by": "0",
        "cover_date": "2023-06-13",
        "Abstract": "With the rapid development of networks, users are increasingly seeking richer and high-quality content experience, and there is an urgent need to develop efficient content caching strategies to improve the content distribution efficiency of caching. Therefore, it will be an effective solution to combine content popularity prediction based on machine learning (ML) and content caching to enable the network to predict and analyze popular content. However, the data sets which contain users’private data cause the risk of privacy leakage. In this paper, to address this challenge, we propose a privacy-preserving algorithm based on federated learning (FL) and long short-term memory (LSTM), which is referred to as FL-LSTM, to predict content popularity. Simulation results demonstrate that the performance of the proposed algorithm is close to the centralized LSTM and better than other benchmark algorithms in terms of privacy protection. Meanwhile, the caching policy in this paper raises about 14.3% of the content hit rate.",
        "DOI": "10.12142/ZTECOM.202302004",
        "affiliation_name": "Northwestern Polytechnical University",
        "affiliation_city": "Xi'an",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Stone masonry design automation via reinforcement learning",
        "paper_author": "Kang S.K.",
        "publication": "Artificial Intelligence for Engineering Design, Analysis and Manufacturing: AIEDAM",
        "citied_by": "3",
        "cover_date": "2023-06-13",
        "Abstract": "The use of local natural and recycled feedstock is promising for sustainable construction. However, unlike versatile engineered bricks, natural and recycled feedstock involves design challenges due to their stochastic, sequential, and heterogeneous nature. For example, the practical use of stone masonry is limited, as it still relies on human experts with holistic domain knowledge to determine the sequential organization of natural stones with different sizes/shapes. Reinforcement learning (RL) is expected to address such design challenges, as it allows artificial intelligence (AI) agents to autonomously learn design policy, that is, identifying the best design decision at each time step. As a proof-of-concept RL framework for design automation involving heterogeneous feedstock, a stone masonry design framework is presented. The proposed framework is founded upon a virtual design environment, MasonTris, inspired by the analogy between stone masonry and Tetris. MasonTris provides a Tetris-like virtual environment combined with a finite element analysis (FEA), where AI agents learn effective design policies without human intervention. Also, a new data collection policy, almost-greedy policy, is designed to address the sparsity of feasible designs for faster/stable learning. As computation bottleneck occurs when parallel agents evaluate designs with different complexities, a modification of the RL framework is proposed that FEA is held until training data are retrieved for training. The feasibility and adaptability of the proposed framework are demonstrated by continuously improving stone masonry design policy in simplified design problems. The framework can be generalizable to different natural and recycled feedstock by incorporating more realistic assumptions, opening opportunities in design automation for sustainability.",
        "DOI": "10.1017/S0890060423000100",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Boston",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Optimization's Neglected Normative Commitments",
        "paper_author": "Laufer B.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "4",
        "cover_date": "2023-06-12",
        "Abstract": "Optimization is offered as an objective approach to resolving complex, real-world decisions involving uncertainty and conflicting interests. It drives business strategies as well as public policies and, increasingly, lies at the heart of sophisticated machine learning systems. A paradigm used to approach potentially high-stakes decisions, optimization relies on abstracting the real world to a set of decision(s), objective(s) and constraint(s). Drawing from the modeling process and a range of actual cases, this paper describes the normative choices and assumptions that are necessarily part of using optimization. It then identifies six emergent problems that may be neglected: 1) Misspecified values can yield optimizations that omit certain imperatives altogether or incorporate them incorrectly as a constraint or as part of the objective, 2) Problematic decision boundaries can lead to faulty modularity assumptions and feedback loops, 3) Failing to account for multiple agents' divergent goals and decisions can lead to policies that serve only certain narrow interests, 4) Mislabeling and mismeasurement can introduce bias and imprecision, 5) Faulty use of relaxation and approximation methods, unaccompanied by formal characterizations and guarantees, can severely impede applicability, and 6) Treating optimization as a justification for action, without specifying the necessary contextual information, can lead to ethically dubious or faulty decisions. Suggestions are given to further understand and curb the harms that can arise when optimization is used wrongfully.",
        "DOI": "10.1145/3593013.3593976",
        "affiliation_name": "Cornell Tech",
        "affiliation_city": "New York",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "In her Shoes: Gendered Labelling in Crowdsourced Safety Perceptions Data from India",
        "paper_author": "Sengupta N.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2023-06-12",
        "Abstract": "In recent years, a proliferation of women's safety mobile applications have emerged in India that crowdsource street safety perceptions to generate 'safety maps' used by policy makers for urban design and academics for studying mobility patterns. Men and women's differential access to information and communication technologies (ICTs), however, and the distinctions between their social and cultural subjective experiences may mitigate the value of crowdsourced safety perceptions data and the predictive ability of machine learning (ML) models utilizing such data. We explore this by collecting and analyzing primary data on safety perceptions from New Delhi, India. Our curated dataset consists of streetviews covering a wide range of neighborhoods for which we obtain subjective safety ratings from both female and male respondents. Simulation experiments where varying the proportion of ratings from each gender are assumed missing demonstrate that the predictive ability of standard ML techniques relies crucially on the distribution of data producers. We find that obtaining large amounts of crowdsourced safety labels from male respondents for predicting female safety perceptions is inefficient in a number of scenarios and even undesirable in others. Detailed comparisons between female and male respondents' data demonstrate significant gender differences in safety perceptions and associated vocabularies. Our results have important implications for the design of platforms relying on crowdsourced data and the insights generated from them.",
        "DOI": "10.1145/3593013.3593987",
        "affiliation_name": "Indian Institute of Technology Delhi",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India"
    },
    {
        "paper_title": "The Possibility of Fairness: Revisiting the Impossibility Theorem in Practice",
        "paper_author": "Bell A.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "8",
        "cover_date": "2023-06-12",
        "Abstract": "The \"impossibility theorem\"- which is considered foundational in algorithmic fairness literature - asserts that there must be trade-offs between common notions of fairness and performance when fitting statistical models, except in two special cases: when the prevalence of the outcome being predicted is equal across groups, or when a perfectly accurate predictor is used. However, theory does not always translate to practice. In this work, we challenge the implications of the impossibility theorem in practical settings. First, we show analytically that, by slightly relaxing the impossibility theorem (to accommodate a practitioner's perspective of fairness), it becomes possible to identify abundant sets of models that satisfy seemingly incompatible fairness constraints. Second, we demonstrate the existence of these models through extensive experiments on five real-world datasets. We conclude by offering tools and guidance for practitioners to understand when - and to what degree - fairness along multiple criteria can be achieved. This work has an important implication for the community: achieving fairness along multiple metrics for multiple groups (and their intersections) is much more possible than was previously believed.",
        "DOI": "10.1145/3593013.3594007",
        "affiliation_name": "Ukrainian Catholic University",
        "affiliation_city": "Lviv",
        "affiliation_country": "Ukraine"
    },
    {
        "paper_title": "What's fair is ⋯ fair? Presenting JustEFAB, an ethical framework for operationalizing medical ethics and social justice in the integration of clinical machine learning",
        "paper_author": "Mccradden M.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "11",
        "cover_date": "2023-06-12",
        "Abstract": "The problem of algorithmic bias represents an ethical threat to the fair treatment of patients when their care involves machine learning (ML) models informing clinical decision-making. The design, development, testing, and integration of ML models therefore require a lifecycle approach to bias identification and mitigation efforts. Presently, most work focuses on the ML tool alone, neglecting the larger sociotechnical context in which these models operate. Moreover, the narrow focus on technical definitions of fairness must be integrated within the larger context of medical ethics in order to facilitate equitable care with ML. Drawing from principles of medical ethics, research ethics, feminist philosophy of science, and justice-based theories, we describe the Justice, Equity, Fairness, and Anti-Bias (JustEFAB) guideline intended to support the design, testing, validation, and clinical evaluation of ML models with respect to algorithmic fairness. This paper describes JustEFAB's development and vetting through multiple advisory groups and the lifecycle approach to addressing fairness in clinical ML tools. We present an ethical decision-making framework to support design and development, adjudication between ethical values as design choices, silent trial evaluation, and prospective clinical evaluation guided by medical ethics and social justice principles. We provide some preliminary considerations for oversight and safety to support ongoing attention to fairness issues. We envision this guideline as useful to many stakeholders, including ML developers, healthcare decision-makers, research ethics committees, regulators, and other parties who have interest in the fair and judicious use of clinical ML tools.",
        "DOI": "10.1145/3593013.3594096",
        "affiliation_name": "The Hospital for Sick Children",
        "affiliation_city": "Toronto",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Using Supervised Learning to Estimate Inequality in the Size and Persistence of Income Shocks",
        "paper_author": "Bruns-Smith D.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-06-12",
        "Abstract": "Household responses to income shocks are important drivers of financial fragility, the evolution of wealth inequality, and the effectiveness of fiscal and monetary policy. Traditional approaches to measuring the size and persistence of income shocks are based on restrictive econometric models that impose strong homogeneity across households and over time. In this paper, we propose a more flexible, machine learning framework for estimating income shocks that allows for variation across all observable features and time horizons. First, we propose non-parametric estimands for shocks and shock persistence. We then show how to estimate these quantities by using off-the-shelf supervised learning tools to approximate the conditional expectation of future income given present information. We solve this income prediction problem in a large Icelandic administrative dataset, and then use the estimated shocks to document several features of labor income risk in Iceland that are not captured by standard economic income models.",
        "DOI": "10.1145/3593013.3594113",
        "affiliation_name": "Central Bank of Iceland",
        "affiliation_city": "Reykjavik",
        "affiliation_country": "Iceland"
    },
    {
        "paper_title": "Public sector innovation outcome-driven sustainable development in Bangladesh: Applying the dynamic autoregressive distributed lag simulations and Kernel-based regularised least square machine learning algorithm approaches",
        "paper_author": "Islam M.M.",
        "publication": "Journal of Public Policy",
        "citied_by": "4",
        "cover_date": "2023-06-12",
        "Abstract": "This research investigates the role of public sector innovation outcomes, e.g. trademark innovation, information and communication technology (ICT), renewable energy, and governance, in the sustainable development of Bangladesh during 1980-2019. Utilising the dynamic autoregressive distributed lag (DARDL) simulation approach, this study divulges a favourable long-term influencing profile of public sector innovation outcomes, i.e. trademark innovation, ICT, and renewable energy on sustainable development, while governance has a heterogeneous impact. Besides, the findings from the DARDL simulations area plots display 10% counterfactual shocks to the public sector innovation outcomes on sustainable development. Furthermore, the Kernel-based regularised least square machine learning algorithm approach used in the study examines the marginal effects of the public sector innovation outcomes on sustainable development for robust findings. Therefore, the policy suggestions are solely concerned with the public sector's adoption of more innovation dynamics through appropriate policy formulation.",
        "DOI": "10.1017/S0143814X22000368",
        "affiliation_name": "Ural Federal University",
        "affiliation_city": "Yekaterinburg",
        "affiliation_country": "Russian Federation"
    },
    {
        "paper_title": "Investigation of relationship between polycyclic aromatic hydrocarbons and human activities in urban soils of China using machine learning methods",
        "paper_author": "Xu X.",
        "publication": "Journal of Cleaner Production",
        "citied_by": "16",
        "cover_date": "2023-06-10",
        "Abstract": "Clarifying the impact of human activities on polycyclic aromatic hydrocarbons (PAHs) in soil can provide a scientific basis to reduce PAHs pollution and formulate control measures to manage soil PAHs pollution, which are important to reduce global warming and support China's implementation of carbon neutrality goals. Here, we characterize the PAHs in 1055 samples of urban soil collected in China from 2000 to 2020. The total concentration of PAHs ranged from 2.75 to 38,865 ng g−1. PAHs concentrations in the Yangtze River Delta (YangtzeD) and Pearl River Delta exhibited an inverted U-shaped curve over time, and PAHs concentrations in Beijing have decreased. Air pollution prevention policies and vehicle emission standards in these regions reduced PAHs in the soil. We measured PAHs in soils of 18 major cities in YangtzeD, representing a typical area. Total concentration of PAHs was 3.88–2153 ng g−1 in YangtzeD, with 40.5%, 36.3%, and 23.2% of PAHs coming from industry and transportation, coal combustion, and biomass combustion, respectively by Positive matrix factorization (PMF) model. To explore how human activities affect the PAH concentrations, we screened three machine learning models and selected SHapley Additive explanation-extreme gradient boosting (XGB-SHAP) as the best model. The results of XGB-SHAP (R2 = 0.64) show that the main human activities affecting PAHs were carbon emissions, population size, and economic development (MAS = 709.1) and industrial waste gas emissions (MAS = 577.1), and these were positively correlated with PAHs. Therefore, effective ways to reduce PAHs may largely rely on industrial co-control. With the prevention and control of air pollution, clean energy, and green and low-carbon development policies constantly being implemented, PAH concentrations will continue to be reduced in the future.",
        "DOI": "10.1016/j.jclepro.2023.136839",
        "affiliation_name": "University of Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "TotalDefMeme: A Multi-Attribute Meme dataset on Total Defence in Singapore",
        "paper_author": "Prakash N.",
        "publication": "MMSys 2023 - Proceedings of the 14th ACM Multimedia Systems Conference",
        "citied_by": "7",
        "cover_date": "2023-06-07",
        "Abstract": "Total Defence is a defence policy combining and extending the concept of military defence and civil defence. While several countries have adopted total defence as their defence policy, very few studies have investigated its effectiveness. With the rapid proliferation of social media and digitalisation, many social studies have been focused on investigating policy effectiveness through specially curated surveys and questionnaires either through digital media or traditional forms. However, such references may not truly reflect the underlying sentiments about the target policies or initiatives of interest. People are more likely to express their sentiment using communication mediums such as starting topic thread on forums or sharing memes on social media. Using Singapore as a case reference, this study aims to address this research gap by proposing TotalDefMeme, a large-scale multi-modal and multi-Attribute meme dataset that captures public sentiments toward Singapore's Total Defence policy. Besides supporting social informatics and public policy analysis of the Total Defence policy, TotalDefMeme can also support many downstream multi-modal machine learning tasks, such as aspect-based stance classification and multi-modal meme clustering. We perform baseline machine learning experiments on TotalDefMeme and evaluate its technical validity, and present possible future interdisciplinary research directions and application scenarios using the dataset as a baseline.",
        "DOI": "10.1145/3587819.3592545",
        "affiliation_name": "Singapore University of Technology and Design",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore"
    },
    {
        "paper_title": "Mapping of Spatial Distribution of Street Vendors Based on Street-View Images and Deep-Learning Technology",
        "paper_author": "Liu Y.",
        "publication": "Tropical Geography",
        "citied_by": "0",
        "cover_date": "2023-06-05",
        "Abstract": "Street vendors are an indispensable part of the urban social ecosystem, but due to a lack of comprehensive understanding, many cities have adopted simple eviction policies, resulting in the gradual marginalization and stigmatization of the street economy. The efficient governance of street vendors requires the comprehensive investigation of their business scale and spatial distribution information. However, traditional methods have limitations in terms of automatically surveying large-scale street vendor information, particularly spatial distribution. This paper proposes a method for the automatic investigation spatial distribution of street vendors based on street-view images and a deep-learning object recognition model. Street-view images were collected at fixed intervals according to the urban road network, and 1, 957 images containing one or more vendors were selected through human-machine interaction to establish street vendor label data. To achieve high recognition model accuracy, the category labels were subdivided into four categories: ground stalls, table stalls, tricycle stalls, and small truck stalls, based on the goods carriers used by street vendors. A deep neural-network-based image object detection model based on YOLO v4 was constructed to identify street vendors in the street-view image library, with an average F1 value of 0.77 and an mAP of 0.67. The accuracy of the model was satisfactory for investigating the number and location of street vendors covering the main roads in the city and then applying a kernel density distribution model to evaluate the spatial distribution pattern of street vendors. Using street vendors in Guangzhou as a case study, the proposed automatic investigation model identified 26,119 street vendors from 3,339,062 street-view images. The results showed that the street vendors were distributed in a multicenter aggregation pattern in the central urban area, mainly concentrated in areas with high pedestrian traffic, such as subway stations and urban villages; their numbers increased as road grades decreases. Street vendors were mainly distributed in areas with medium rents. The proposed method is helpful for performing the efficient, low-cost, and city-scale mapping of street vendors; the results obtained provide suggestions for formulating and implementing spatial governance policies for the informal economy and further provide suggestions for improving and implementing spatial governance policies for open and diverse urban street-view images. The results can be used as a reference for the location preference analysis of practitioners, the exploration of NIMBY syndrome, and the determination of the formalization zone. Although street-view images have an insufficient spatiotemporal coverage, using them to perform street vendor investigations is a low-cost and efficient method compared with the use of traditional investigation methods and data sources. In addition, the method proposed in this article can be coupled with multitask deep learning algorithms to investigate additional dimensions of street vendor information, such as the sex, age, and type of business of street vendors. Relevant research needs to be conducted in the future.",
        "DOI": "10.13284/j.cnki.rddl.003691",
        "affiliation_name": "Ministry of Natural Resources of the People's Republic of China",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Precise chirp control with model-based reinforcement learning for broadband frequency-swept laser of LiDAR",
        "paper_author": "Zhao H.",
        "publication": "Optics Express",
        "citied_by": "1",
        "cover_date": "2023-06-05",
        "Abstract": "Artificial intelligence (AI) has been widely used in various fields of physics and engineering in recent decades. In this work, we introduce model-based reinforcement learning (MBRL), which is an important branch of machine learning in the AI domain, to the broadband frequency-swept laser control for frequency modulated continuous wave (FMCW) light detection and ranging (LiDAR). With the concern of the direct interaction between the optical system and the MBRL agent, we establish the frequency measurement system model on the basis of the experimental data and the nonlinearity property of the system. In light of the difficulty of this challenging high-dimensional control task, we propose a twin critic network on the basis of the Actor-Critic structure to better learn the complex dynamic characteristics of the frequency-swept process. Furthermore, the proposed MBRL structure would stabilize the optimization process greatly. In the training process of the neural network, we apply a delaying strategy to the policy update and introduce a smoothing regularization strategy to the target policy to further enhance the network stability. With the well-trained control policy, the agent generates the excellent and regularly updated modulation signals to control the laser chirp precisely and an excellent detection resolution is obtained eventually. Our proposed work demonstrates that the integration of data-driven reinforcement learning (RL) and optical system control gives an opportunity to reduce the system complexity and accelerate the investigation and optimization of control systems.",
        "DOI": "10.1364/OE.488283",
        "affiliation_name": "Quzhou University",
        "affiliation_city": "Quzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Lessons learned from the hospital to home community care program in Singapore and the supporting AI multiple readmissions prediction model",
        "paper_author": "Abisheganaden J.",
        "publication": "Health Care Science",
        "citied_by": "0",
        "cover_date": "2023-06-01",
        "Abstract": "In a prior practice and policy article published in Healthcare Science, we introduced the deployed application of an artificial intelligence (AI) model to predict longer-term inpatient readmissions to guide community care interventions for patients with complex conditions in the context of Singapore's Hospital to Home (H2H) program that has been operating since 2017. In this follow on practice and policy article, we further elaborate on Singapore's H2H program and care model, and its supporting AI model for multiple readmission prediction, in the following ways: (1) by providing updates on the AI and supporting information systems, (2) by reporting on customer engagement and related service delivery outcomes including staff-related time savings and patient benefits in terms of bed days saved, (3) by sharing lessons learned with respect to (i) analytics challenges encountered due to the high degree of heterogeneity and resulting variability of the data set associated with the population of program participants, (ii) balancing competing needs for simpler and stable predictive models versus continuing to further enhance models and add yet more predictive variables, and (iii) the complications of continuing to make model changes when the AI part of the system is highly interlinked with supporting clinical information systems, (4) by highlighting how this H2H effort supported broader Covid-19 response efforts across Singapore's public healthcare system, and finally (5) by commenting on how the experiences and related capabilities acquired from running this H2H program and related community care model and supporting AI prediction model are expected to contribute to the next wave of Singapore's public healthcare efforts from 2023 onwards. For the convenience of the reader, some content that introduces the H2H program and the multiple readmissions AI prediction model that previously appeared in the prior Healthcare Science publication is repeated at the beginning of this article.",
        "DOI": "10.1002/hcs2.44",
        "affiliation_name": "Singapore Health Services",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore"
    },
    {
        "paper_title": "ORTHOGONAL STATISTICAL LEARNING",
        "paper_author": "Foster D.J.",
        "publication": "Annals of Statistics",
        "citied_by": "20",
        "cover_date": "2023-06-01",
        "Abstract": "We provide nonasymptotic excess risk guarantees for statistical learning in a setting where the population risk with respect to which we evaluate the target parameter depends on an unknown nuisance parameter that must be estimated from data. We analyze a two-stage sample splitting meta-algorithm that takes as input arbitrary estimation algorithms for the target parameter and nuisance parameter. We show that if the population risk satisfies a condition called Neyman orthogonality, the impact of the nuisance estimation error on the excess risk bound achieved by the meta-algorithm is of second order. Our theorem is agnostic to the particular algorithms used for the target and nuisance and only makes an assumption on their individual performance. This enables the use of a plethora of existing results from machine learning to give new guarantees for learning with a nuisance component. Moreover, by focusing on excess risk rather than parameter estimation, we can provide rates under weaker assumptions than in previous works and accommodate settings in which the target parameter belongs to a complex nonparametric class. We provide conditions on the metric entropy of the nuisance and target classes such that oracle rates of the same order, as if we knew the nuisance parameter, are achieved.",
        "DOI": "10.1214/23-AOS2258",
        "affiliation_name": "Stanford Engineering",
        "affiliation_city": "Stanford",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "A Safety-Critical Decision-Making and Control Framework Combining Machine-Learning-Based and Rule-Based Algorithms",
        "paper_author": "Aksjonov A.",
        "publication": "SAE International Journal of Vehicle Dynamics, Stability, and NVH",
        "citied_by": "20",
        "cover_date": "2023-06-01",
        "Abstract": "While machine-learning-based methods suffer from a lack of transparency, rule-based (RB) methods dominate safety-critical systems. Yet the RB approaches cannot compete with the first ones in robustness to multiple system requirements, for instance, simultaneously addressing safety, comfort, and efficiency. Hence, this article proposes a decision-making and control framework which profits from the advantages of both the RB and machine-learning-based techniques while compensating for their disadvantages. The proposed method embodies two controllers operating in parallel, called Safety and Learned. An RB switching logic selects one of the actions transmitted from both controllers. The Safety controller is prioritized whenever the Learned one does not meet the safety constraint, and also directly participates in the Learned controller training. Decision-making and control in autonomous driving are chosen as the system case study, where an autonomous vehicle (AV) learns a multitask policy to safely execute an unprotected left turn. Multiple requirements (i.e., safety, efficiency, and comfort) are set to vehicle motion. A numerical simulation is performed for the proposed framework validation, where its ability to satisfy the requirements and robustness to changing environments is successfully demonstrated.",
        "DOI": "10.4271/10-07-03-0018",
        "affiliation_name": "Aalto University",
        "affiliation_city": "Espoo",
        "affiliation_country": "Finland"
    },
    {
        "paper_title": "Research on Data-driven Real-time Scheduling Method of Smart Workshop",
        "paper_author": "Gu W.",
        "publication": "Jixie Gongcheng Xuebao/Journal of Mechanical Engineering",
        "citied_by": "0",
        "cover_date": "2023-06-01",
        "Abstract": "The intelligent manufacturing system adopts a large number of advanced information technologies such as the Internet of things, so that the workshop has accumulated a large amount of real-time production data. At the same time, the complex manufacturing system is prone to a series of disturbance events during the operation process, which puts forward higher requirements for the workshop’s real-time response capability. Therefore, in the manufacturing environment supported by industrial big data, a deep-reinforcement-learning-based real-time scheduling method is proposed for the hybrid flow shop scheduling problem with sequence-dependent setup times and blocking (HFSP-SDST-B), so as to realize the reasonable allocation of manufacturing resources and minimization of makespan. As a sequential decision-making problem, HFSP-SDST-B can be modeled as a markov decision process. At each scheduling point, the agent selects the corresponding scheduling rule according to the current production state, so as to perform the reasonable job sorting and machine allocation. In order to realize the real-time scheduling method driven by production data, the scheduling point considering the blocking, general production state characteristics, heuristic rules based on genetic programming and reward function are designed. Then a training method based on proximal policy optimization algorithm is proposed, so that the agent can build an effective mapping between state and rule. Finally, the experimental results show that compared with the existing dynamic scheduling methods, this method has superiority and generality, and can effectively deal with the unknown situation of stochastic disturbance time and new order insertion through learning.",
        "DOI": "10.3901/JME.2023.12.047",
        "affiliation_name": "Hohai University Changzhou",
        "affiliation_city": "Changzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "EU Smart Cities: Towards a New Framework of Urban Digital Transformation",
        "paper_author": "Shulajkovska M.",
        "publication": "Informatica (Slovenia)",
        "citied_by": "4",
        "cover_date": "2023-06-01",
        "Abstract": "The URBANITE H2020 project aims to address urban mobility challenges caused by growth and new transportation methods. It develops a decision support system for policymakers, incorporating simulation, evaluation of key performance indicators, a recommendation/decision support system, and machine learning capabilities. The system helps identify and improve key performance indicators, proposes effective policies, and enhances urban digital transformation for sustainable and efficient mobility.",
        "DOI": "10.31449/inf.v47i2.4904",
        "affiliation_name": "Institut \"Jožef Stefan\"",
        "affiliation_city": "Ljubljana",
        "affiliation_country": "Slovenia"
    },
    {
        "paper_title": "Algebraically explainable controllers: decision trees and support vector machines join forces",
        "paper_author": "Jüngermann F.",
        "publication": "International Journal on Software Tools for Technology Transfer",
        "citied_by": "3",
        "cover_date": "2023-06-01",
        "Abstract": "Recently, decision trees (DT) have been used as an explainable representation of controllers (a.k.a. strategies, policies, schedulers). Although they are often very efficient and produce small and understandable controllers for discrete systems, complex continuous dynamics still pose a challenge. In particular, when the relationships between variables take more complex forms, such as polynomials, they cannot be obtained using the available DT learning procedures. In contrast, support vector machines provide a more powerful representation, capable of discovering many such relationships, but not in an explainable form. Therefore, we suggest to combine the two frameworks to obtain an understandable representation over richer, domain-relevant algebraic predicates. We demonstrate and evaluate the proposed method experimentally on established benchmarks.",
        "DOI": "10.1007/s10009-023-00716-z",
        "affiliation_name": "Masaryk University",
        "affiliation_city": "Brno",
        "affiliation_country": "Czech Republic"
    },
    {
        "paper_title": "Deep Reinforcement Learning for Portfolio Management",
        "paper_author": "Ma Y.",
        "publication": "International Journal of Computers and their Applications",
        "citied_by": "1",
        "cover_date": "2023-06-01",
        "Abstract": "This paper discusses how to build deep reinforcement learning (DRL) agents to determine the allocation of money for assets in a portfolio so that the maximum return can be gained. The policy gradient method from reinforcement learning and convolutional neural network/recurrent neural network/convolutional neural network concatenated with the recurrent neural network from deep learning are combined to build the agents. With the proposed models, three types of portfolios are tested: stocks portfolio which has a positive influence due to the Covid-19; stocks portfolio which has a negative influence due to the Covid-19; and stocks, cryptocurrency combined portfolio which are randomly selected. The performance of our DRL agents is compared with that of equal-weighted agent and all the money fully invested in one-stock agents. All of our DRL agents showed the best performance on the randomly selected portfolio, which has an overall stable increasing trend. In addition, the performance of a linear regression model is also tested with the random selected portfolio, and it shows a poor result compared to other agents.",
        "DOI": "NA",
        "affiliation_name": "Southeast Missouri State University",
        "affiliation_city": "Cape Girardeau",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Decentralized Machine Learning based Energy Efficient Routing and Intrusion Detection in Unmanned Aerial Network (UAV)",
        "paper_author": "Srinivas C.",
        "publication": "International Journal on Recent and Innovation Trends in Computing and Communication",
        "citied_by": "1",
        "cover_date": "2023-06-01",
        "Abstract": "Decentralized machine learning (FL) is a system that uses federated learning (FL). Without disclosing locally stored sensitive information, FL enables multiple clients to work together to solve conventional distributed ML problems coordinated by a central server. In order to classify FLs, this research relies heavily on machine learning and deep learning techniques. The next generation of wireless networks is anticipated to incorporate unmanned aerial vehicles (UAVs) like drones into both civilian and military applications. The use of artificial intelligence (AI), and more specifically machine learning (ML) methods, to enhance the intelligence of UAV networks is desirable and necessary for the aforementioned uses. Unfortunately, most existing FL paradigms are still centralized, with a singular entity accountable for network-wide ML model aggregation and fusion. This is inappropriate for UAV networks, which frequently feature unreliable nodes and connections, and provides a possible single point of failure. There are many challenges by using high mobility of UAVs, of loss of packet frequent and difficulties in the UAV between the weak links, which affect the reliability while delivering data. An earlier UAV failure is happened by the unbalanced conception of energy and lifetime of the network is decreased; this will accelerate consequently in the overall network. In this paper, we focused mainly on the technique of security while maintaining UAV network in surveillance context, all information collected from different kinds of sources. The trust policies are based on peer-to-peer information which is confirmed by UAV network. A preshared UAV list or used by asymmetric encryption security in the proposal system. The wrong information can be identified when the UAV the network is hijacked physically by using this proposed technique. To provide secure routing path by using Secure Location with Intrusion Detection System (SLIDS) and conservation of energy-based prediction of link breakage done by location-based energy efficient routing (LEER) for discovering path of degree connectivity. Thus, the proposed novel architecture is named as Decentralized Federate Learning- Secure Location with Intrusion Detection System (DFL-SLIDS), which achieves 98% of routing overhead, 93% of end-to-end delay, 92% of energy efficiency, 86.4% of PDR and 97% of throughput.",
        "DOI": "10.17762/ijritcc.v11i6s.6960",
        "affiliation_name": "Kakatiya Institute of Technology &amp; Science",
        "affiliation_city": "Hanamkonda",
        "affiliation_country": "India"
    },
    {
        "paper_title": "FISCAL RULES’ COMPLIANCE AND SOCIAL WELFARE",
        "paper_author": "Baret K.",
        "publication": "Annals of Economics and Statistics",
        "citied_by": "1",
        "cover_date": "2023-06-01",
        "Abstract": "The post-pandemic economic reality seeks deep economic policies adjustment, including redesigning fiscal rules. This paper contributes to feeds the debate by investigating the side-effects of fiscal rules’ compliance on social welfare. It considers national Budget Balance Rules’ (BBR) compliance effects on social welfare proxies and the channel through which it operates between 2004 and 2015. Instead of fiscal rules strength or fiscal rules presence effectiveness, the study focuses on fiscal rules’ compliance to assess the impact of fiscal rules’ performance on social welfare, using a so-called “Double/Debiased Machine Learning (DML)” approach (Chernozhukov et al. (2018)). The results show that governments seem to reallocate their spending to achieve both BBR’s compliance and economic objectives, with negative consequences on social expenditure. We also conclude to an increasing effect on social inequalities, suggesting that governments face a trade-off between fiscal rules’ compliance and social objectives. Thanks to the DML methodology, we also identify the key determinants of national BBR’s compliance, taking care of voter preferences by computing a new proxy variable through a Latent Factor Analysis. We conclude that voter preferences appear as a significant factor of BBR’s compliance, supporting that the Wyplosz (2012)’s bias may matter when assessing fiscal rules’ performance.",
        "DOI": "10.2307/48731468",
        "affiliation_name": "Université de Strasbourg",
        "affiliation_city": "Strasbourg",
        "affiliation_country": "France"
    },
    {
        "paper_title": "Decomposition of Inequality of Opportunity in India: An Application of Data-Driven Machine Learning Approach",
        "paper_author": "Mehta B.S.",
        "publication": "Indian Journal of Labour Economics",
        "citied_by": "0",
        "cover_date": "2023-06-01",
        "Abstract": "This paper introduces a novel measure of inequality of opportunity (IOp) in India, by comparing both ex-ante and ex-post results, which aligns with Roemer’s (1998) equality of opportunity, theory. The study utilizes data-driven machine learning algorithms, namely conditional inference tree and conditional inference forest, to measure ex-ante IOp, and a transformation tree to estimate ex-post IOp. The findings indicate that, according to the ex-ante approach, approximately 58–61 percent of the overall income inequality can be attributed to variations in circumstances, while around 46 percent of the overall income inequality is explained by differences in the degree of efforts. The results from the tree-based analysis reveal that parents’ occupation, sector (rural–urban areas), and geographical regions are the primary circumstances contributing to IOp, which is further confirmed by the Shapley decomposition exercise. Specifically, individuals residing in rural areas in the eastern and central parts of the country, whose parents are employed in low-skilled and unskilled occupations, and have below secondary and no formal education, and who belong to marginalized social groups, exhibit significantly lower average income. Consequently, it is crucial to implement regional-level development policies that specifically target marginalized groups in order to foster a more equitable society and mitigate overall income inequality.",
        "DOI": "10.1007/s41027-023-00446-5",
        "affiliation_name": "Institute for Human Development",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India"
    },
    {
        "paper_title": "A novel adaptation of spatial interpolation methods to map health attitudes related to COVID-19",
        "paper_author": "Behal R.",
        "publication": "BMC Proceedings",
        "citied_by": "1",
        "cover_date": "2023-06-01",
        "Abstract": "Background: The COVID-19 pandemic presented substantial challenges to public health stakeholders working to vaccinate populations against the disease, particularly among vaccine hesitant individuals in low- and middle-income countries. Data on the determinants of vaccine hesitancy are scarce, and often available only at the national level. In this paper, our goal is to inform programmatic decision making in support of local vaccine uptake. Our analytical objectives to support this goal are to (1) reliably estimate attitudinal data at the hyperlocal level, and (2) estimate the loss of data heterogeneity among these attitudinal indicators at higher levels of aggregation. With hyperlocal attitudinal data on the determinants of vaccine hesitancy, public health stakeholders can better tailor interventions aimed at increasing uptake sub-nationally, and even down to the individual vaccination site or neighborhood. Methods: We estimated attitudinal data on the determinants of vaccine hesitancy as framed by the WHO’s Confidence, Complacency, and Convenience (“3Cs”) Model of Vaccine Hesitancy using a nationally and regionally representative household survey of 4,922 adults aged 18 and above, collected in February 2022. This custom survey was designed to collect information on attitudes towards COVID-19 and concerns about the COVID-19 vaccine. A machine learning (ML) framework was used to spatially interpolate metrics representative of the 3Cs at a one square kilometer (1km2) resolution using approximately 130 spatial covariates from high-resolution satellite imagery, and 24 covariates from the 2018 Nigeria Demographic and Health Survey (DHS). Results: Spatial interpolated hyperlocal estimates of the 3Cs captured significant information on attitudes towards COVID-19 and COVID-19 vaccines. The interpolated estimates held increased heterogeneity within each subsequent level of disaggregation, with most variation at the 1km2 level. Conclusions: Our findings demonstrate that a) attitudinal data can be successfully estimated at the hyperlocal level, and b) the determinants of COVID-19 vaccine hesitancy have large spatial variance that cannot be captured through national surveys alone. Access to community level attitudes toward vaccine safety and efficacy; vaccination access, time, and financial burden; and COVID-19 beliefs and infection concerns presents novel implications for public health practitioners and policymakers seeking to increase COVID-19 vaccine uptake through more customized community-level interventions.",
        "DOI": "10.1186/s12919-023-00264-z",
        "affiliation_name": "Johnson &amp; Johnson",
        "affiliation_city": "New Brunswick",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Editorial",
        "paper_author": "NA",
        "publication": "Journal of Mental Health Policy and Economics",
        "citied_by": "0",
        "cover_date": "2023-06-01",
        "Abstract": "NA",
        "DOI": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Design and Evaluation of Optimal Free Trials",
        "paper_author": "Yoganarasimhan H.",
        "publication": "Management Science",
        "citied_by": "18",
        "cover_date": "2023-06-01",
        "Abstract": "Free trial promotions are a commonly used customer acquisition strategy in the Software as a Service industry. We use data from a large-scale field experiment to study the effect of trial length on customer-level outcomes. We find that, on average, shorter trial lengths (surprisingly) maximize customer acquisition, retention, and profitability. Next, we examine the mechanism through which trial length affects conversions and rule out the demand cannibalization theory, find support for the consumer learning hypothesis, and show that long stretches of inactivity at the end of the trial are associated with lower conversions. We then develop a personalized targeting policy that allocates the optimal treatment to each user based on individual-level predictions of the outcome of interest (e.g., subscriptions) using a lasso model. We evaluate this policy using the inverse propensity score reward estimator and show that it leads to 6.8% improvement in subscription compared with a uniform 30-days for-all policy. It also performs well on long-term customer retention and revenues in our setting. Further analysis of this policy suggests that skilled and experienced users are more likely to benefit from longer trials, whereas beginners are more responsive to shorter trials. Finally, we show that personalized policies do not always outperform uniform policies, and we should be careful when designing and evaluating personalized policies. In our setting, personalized policies based on other methods (e.g., causal forests, random forests) perform worse than a simple uniform policy that assigns a short trial length to all users.",
        "DOI": "10.1287/mnsc.2022.4507",
        "affiliation_name": "Foster School of Business",
        "affiliation_city": "Seattle",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Accelerating Reinforcement Learning via Predictive Policy Transfer in 6G RAN Slicing",
        "paper_author": "Nagib A.M.",
        "publication": "IEEE Transactions on Network and Service Management",
        "citied_by": "11",
        "cover_date": "2023-06-01",
        "Abstract": "Reinforcement Learning (RL) algorithms have recently been proposed to solve dynamic radio resource management (RRM) problems in beyond 5G networks. However, RL-based solutions are still not widely adopted in commercial cellular networks. One of the primary reasons for this is the slow convergence of RL agents when they are deployed in a live network and when the network's context changes significantly. Concurrently, the open radio access network (O-RAN) paradigm promises to give mobile network operators (MNOs) more control over their networks, furthering the need for intelligent and RL-based network management. O-RAN's standardized interfaces will allow MNOs to make real-time custom changes to intelligently control various RRM functionalities. We consider a RAN slicing scenario in which MNOs can modify the weights of the RL reward function. This enables MNOs to change the priorities of fulfilling the service level agreements of the slices. However, this results in a practical challenge since the RL agent needs to adapt promptly to the changes made by the MNO. This challenge is addressed in this paper, where we first present and discuss the results from an exhaustive experiment to examine the efficiency of using transfer learning (TL) to accelerate the convergence of RL-based RAN slicing in the considered scenario. We then propose a novel predictive approach to enhance the TL-based acceleration by selecting the best-saved policy for reuse. By adopting the proposed policy transfer approach, RL agents are able to converge up to 14000 learning steps faster than their non-accelerated counterparts. The proposed machine learning (ML)-based predictive approach also shows up to a 96.5% accuracy in selecting the best expert policy to reuse for acceleration.",
        "DOI": "10.1109/TNSM.2023.3258692",
        "affiliation_name": "Faculty of Computers and Artificial Intelligence",
        "affiliation_city": "Giza",
        "affiliation_country": "Egypt"
    },
    {
        "paper_title": "Applying Transfer Learning Approaches for Intrusion Detection in Software-Defined Networking",
        "paper_author": "Chuang H.M.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "11",
        "cover_date": "2023-06-01",
        "Abstract": "In traditional network management, the configuration of routing policies and associated settings on individual routers and switches was performed manually, incurring a considerable cost. By centralizing network management, software-defined networking (SDN) technology has reduced hardware construction costs and increased flexibility. However, this centralized architecture renders information security vulnerable to network attacks, making intrusion detection in the SDN environment crucial. Machine-learning approaches have been widely used for intrusion detection recently. However, critical issues such as unknown attacks, insufficient data, and class imbalance may significantly affect the performance of typical machine learning. We addressed these problems and proposed a transfer-learning method based on the SDN environment. The following experimental results showed that our method outperforms typical machine learning methods. (1) our model achieved a F1-score of 0.71 for anomaly detection for unknown attacks; (2) for small samples, our model achieved a F1-score of 0.98 for anomaly detection and a F1-score of 0.51 for attack types identification; (3) for class imbalance, our model achieved an F1-score of 1.00 for anomaly detection and 0.91 for attack type identification. In addition, our model required 15,230 seconds (4 h 13 m 50 s) for training, ranking second among the six models when considering both performance and efficiency. In future studies, we plan to combine sampling techniques with few-shot learning to improve the performance of minority classes in class imbalance scenarios.",
        "DOI": "10.3390/su15129395",
        "affiliation_name": "National Defense University Taiwan",
        "affiliation_city": "Taoyuan",
        "affiliation_country": "Taiwan"
    },
    {
        "paper_title": "Machine Learning Techniques to Map the Impact of Urban Heat Island: Investigating the City of Jeddah",
        "paper_author": "Addas A.",
        "publication": "Land",
        "citied_by": "13",
        "cover_date": "2023-06-01",
        "Abstract": "Over the last decades, most agricultural land has been converted into residential colonies to accommodate the rapid population expansion. Population growth and urbanization result in negative consequences on the environment. Such land has experienced various environmental issues due to rapid urbanization and population increases. Such expansion in urbanization has a big impact on worsening the residences soon and in the long term, as the population is projected to increase more and more. One such issue is the urban heat island (UHI), which is computed based on land surface temperature (LST). The UHI effect has fundamental anthropogenic impacts on local areas, particularly in rapidly growing cities. This is due to the unplanned shifts in land use and land cover (LUALC) at the local level, which results in climate condition variations. Therefore, proper planning based on concrete information is the best policy in the long run to remedy these issues. In this study, we attempt to map out UHI phenomena using machine learning (ML) algorithms, including bagging and random subspace. The proposed research also fulfills the sustainable development goals (SDGs) requirement. We exploit the correlation and regression methods to understand the relationship between biophysical composition and the UHI effect. Our findings indicate that in the megacity of Jeddah, Saudi Arabia, from 2000 to 2021, the urban area enlarged by about 80%, while the UHI increased overall. Impervious surfaces significantly impact the UHI effect, while vegetation and water bodies have negative implications for the UHI effect. More than 80% of the total parts in Jeddah have been classified by extremely high UHI conditions, as determined by the bagging and random subspace models. In particular, the megacity’s south, north, and central-east parts were categorized by very high UHI conditions. This research is not only expected to assist in understanding the spatial patterns of the UHI in Jeddah, but to assist planners and policymakers in spatial planning. It will help to ensure sustainable urban management and improve life quality.",
        "DOI": "10.3390/land12061159",
        "affiliation_name": "Prince Sattam Bin Abdulaziz University",
        "affiliation_city": "Al Kharj",
        "affiliation_country": "Saudi Arabia"
    },
    {
        "paper_title": "Effect of Flexible Operation on Residual Life of High-Temperature Components of Power Plants",
        "paper_author": "Heo J.",
        "publication": "Processes",
        "citied_by": "0",
        "cover_date": "2023-06-01",
        "Abstract": "Electricity generation from renewable energy sources is emerging as a result of global carbon emission reduction policies. However, most renewable energy sources are non-dispatchable and cannot be adjusted to meet the fluctuating electricity demands of society. A flexible operation process has been proposed as an effective solution to compensate for the unstable nature of renewable energy sources. Thermal load fluctuations during flexible operation may cause creep–fatigue damage to the high-temperature components of thermal power plants, as they are designed with a focus on creep damage under a constant power level. This study investigated the residual life of high-temperature components, such as a superheater tube and a reheater header, to failure under flexible operation conditions using finite element analysis and empirical models. First, we determined an analytical solution for the straightened superheater tube under thermal conditions and compared it with the numerical solution to verify the numerical models. Through the verified finite element model, the creep–fatigue life of the reheater header was estimated by considering flexible operation factors and employing the Coffin–Manson and Larson–Miller models. Although fatigue damage increases with decreasing minimum load and ramp rate, we confirmed that creep damage significantly affects the residual life during flexible operation. In addition, a surrogate model was proposed to evaluate the residual life of the reheater as a function of the flexible operation factors using the machine learning methodology, based on the results of finite element methods. It can be used to predict its residual life without performing complex thermo-structural analysis and relying on empirical models for fatigue and creep life. We expect our findings to contribute to the efficient operation of thermal power plants by optimizing the flexible operation factors.",
        "DOI": "10.3390/pr11061679",
        "affiliation_name": "Myongji University",
        "affiliation_city": "Yongin",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "A Rapid and Easy Way for National Forest Heights Retrieval in China Using ICESat-2/ATL08 in 2019",
        "paper_author": "Gao S.",
        "publication": "Forests",
        "citied_by": "3",
        "cover_date": "2023-06-01",
        "Abstract": "Continuous and extensive monitoring of forest height is essential for estimating forest above-ground biomass and predicting the ability of forests to absorb CO2. In particular, forest height at the national scale is an important indicator reflecting the national forestry economic construction, environmental governance, and ecological balance. However, the lack of inventory data restricts large-scale monitoring of forest height to some extent. Conducting manual surveys of forest height for large-scale areas would be labor-intensive and time-consuming. The successful launch of the new generation of spaceborne light detection and ranging (LiDAR) (The Ice, Cloud, and Land Elevation Satellite-2/the Advanced Topographic Laser Altimeter System, ICESat-2/ATLAS) has brought new opportunities for national-scale forestry resource surveys. This paper explores a method to survey national forest canopy height from the new generation of ICESat-2/ATLAS data. In view of the sparse sampling and little overlap between repeated spaceborne LiDAR data, a strategy for assessing the overall change of canopy height for large scales is provided. Some spatially continuous ancillary data were used to assist ICESat-2/ATLAS data to generate a wall-to-wall (spatially continuous) forest canopy height map in China by using the machine learning approach and then quantifying the analysis of forest canopy height in various provinces. The results show that there is a good correlation between the model forest height and the verification data, with a root mean squared error (RMSE) of 3.30 m and a coefficient of determination (R2) of 0.87. This indicates that the method for retrieving national forest canopy height is reliable. There are some limitations in areas with lower vegetation coverage or complex topography which need additional filtering or terrain correction to achieve higher accuracy in measuring forest canopy height. Our analysis suggests that ICESat-2/ATLAS data can achieve the retrieval of national forest height at an overall level, and it would be feasible to use ICESAT-2/ATLAS products to estimate forest canopy height change for large-scale areas.",
        "DOI": "10.3390/f14061270",
        "affiliation_name": "Central South University",
        "affiliation_city": "Changsha",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Enhanced Adaptable and Distributed Access Control Decision Making Model Based on Machine Learning for Policy Conflict Resolution in BYOD Environment",
        "paper_author": "Ayedh M A.T.",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "2",
        "cover_date": "2023-06-01",
        "Abstract": "Organisations are adopting new IT strategies such as “Bring Your Own Device” (BYOD) and remote working. These trends are highly beneficial both for enterprise owners and employees in terms of increased productivity and reduced costs. However, security issues such as unauthorised access as well as privacy concerns pose significant obstacles. These can be overcome by adopting access control techniques and a dynamic security and privacy policy that governs these issues where they arise. Policy decision points in traditional access control systems, such as role-based access control (RBAC), attribute-based access control (ABAC), or relationship-based access control (ReBAC), may be limited because the status of access control can vary in response to minor changes in user and resource properties. As a result, system administrators rely on a solution for constructing complex rules with many conditions and permissions for decision control. This results in access control issues, including policy conflicts, decision-making bottlenecks, delayed access response times and mediocre performance. This paper proposes a policy decision-making and access control-based supervised learning algorithm. The algorithm enhances policy decision points (PDPs). This is achieved by transforming the PDP’s problem into a binary classification for security access control that either grants or denies access requests. Also, a vector decision classifier based on the supervised machine learning algorithm is developed to generate an accurate, effective, distributed and dynamic policy decision point (PDP). Performance was evaluated using the Kaggle-Amazon access control policy dataset, which compared the effectiveness of the proposed mechanism to previous research benchmarks in terms of performance, time and flexibility. The proposed solution obtains a high level of privacy for access control policies because the PDP does not communicate directly with the policy administration point (PAP). In conclusion, PDP-based ML generates accurate decisions and can simultaneously fulfill multiple massive policies and huge access requests with 95% Accuracy in a short response time of around 0.15 s without policy conflicts. Access control security is improved by making it dynamic, adaptable, flexible and distributed.",
        "DOI": "10.3390/app13127102",
        "affiliation_name": "Shaqra University",
        "affiliation_city": "Shaqra",
        "affiliation_country": "Saudi Arabia"
    },
    {
        "paper_title": "Attention Mechanism-Combined LSTM for Grain Yield Prediction in China Using Multi-Source Satellite Imagery",
        "paper_author": "Liu F.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "12",
        "cover_date": "2023-06-01",
        "Abstract": "Grain yield prediction affects policy making in various aspects such as agricultural production planning, food security assurance, and adjustment of foreign trade. Accurately predicting grain yield is of great significance in ensuring global food security. This paper is based on the MODIS remote sensing image data products from 2010 to 2020, and adds band information such as vegetation index and temperature to form composite remote sensing data as a dataset. Aiming at the lack of models for large-scale forecasting and the need for human intervention in traditional models, this paper proposes a grain production estimation model based on deep learning. First, image cropping and yield mapping techniques are used to process the data to generate training samples. Then the channel and spatial attention mechanism (convolutional block attention module, CBAM) is added to extract spatial information in different remote sensing bands to improve the efficiency of the model. Long short-term memory (LSTM) neural networks are added to obtain feature information in the time dimension. Finally, a national-scale grain yield prediction model is constructed. After the study, it was found that the LSTM model using a combination of multi-source satellite images and an attention mechanism can effectively predict grain yield in China. Furthermore, the proposed model was tested on data from 2018 to 2020 showing an average (Formula presented.) of 0.940 and an average RMSE of 80,020 tons, indicating that it can predict Chinese grain yield better. The model proposed in this paper extracts grain yield information directly from the composite remote sensing data, and solves the problem of small-scale research and imprecise yield prediction in an end-to-end manner.",
        "DOI": "10.3390/su15129210",
        "affiliation_name": "Central South University of Forestry and Technology",
        "affiliation_city": "Changsha",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Metaheuristic for Optimal Dynamic K-Coloring Application on Band Sharing for Automotive Radars",
        "paper_author": "Roudiere S.",
        "publication": "Sensors",
        "citied_by": "1",
        "cover_date": "2023-06-01",
        "Abstract": "The number of vehicles equipped with radars on the road has been increasing for years and is expected to reach 50% of cars by 2030. This rapid rise in radars will likely increase the risk of harmful interference, especially since radar specifications from standardization bodies (e.g., ETSI) provide requirements in terms of maximum transmit power but do no mandate specific radar waveform parameters nor channel access scheme policies. Techniques for interference mitigation are thus becoming very important to ensure the long-term correct operation of radars and upper-layer ADAS systems that depend on them in this complex environment. In our previous work, we have shown that organizing the radar band into time-frequency resources that do not interfere with each other vastly reduces the amount of interference by facilitating band sharing. In this paper, a metaheuristic is presented to find the optimal resource sharing between radars, knowing their relative positions and thereby the line-of-sight and non-line-of-sight interference risks during a realistic scenario. The metaheuristic aims at optimally minimizing interference while minimizing the number of resource changes that radars have to make. It is a centralized approach where everything about the system is known (e.g., the past and future positions of the vehicles). This and the high computational load induce that this algorithm is not meant to be used in real-time. However, the metaheuristic approach can be extremely useful for finding near optimal solutions in simulations, allowing for the extraction of efficient patterns, or as data generation for machine learning.",
        "DOI": "10.3390/s23125765",
        "affiliation_name": "Institut de Mathématiques de Toulouse",
        "affiliation_city": "Toulouse",
        "affiliation_country": "France"
    },
    {
        "paper_title": "Data-Driven Modeling of Air Traffic Controllers’ Policy to Resolve Conflicts",
        "paper_author": "Bastas A.",
        "publication": "Aerospace",
        "citied_by": "0",
        "cover_date": "2023-06-01",
        "Abstract": "With the aim to enhance automation in conflict detection and resolution (CD&R) tasks in the air traffic management (ATM) domain, this article studies the use of artificial intelligence and machine learning (AI/ML) methods to learn air traffic controllers’ (ATCOs) policy in resolving conflicts among aircraft assessed to violate separation minimum constraints during the en route phase of flights, in the tactical phase of operations. The objective is to model (Formula presented.) conflicts are being resolved by ATCOs. Towards this goal, the article formulates the ATCO policy learning problem for conflict resolution, addresses the challenging issue of an inherent lack of information in real-world data, and presents AI/ML methods that learn models of ATCOs’ behavior. The methods are evaluated using real-world datasets. The results show that AI/ML methods can achieve good accuracy on predicting ATCOs’ actions given specific conflicts, revealing the preferences of ATCOs for resolution actions in specific circumstances. However, the high accuracy of predictions is hindered by real-world data-inherent limitations.",
        "DOI": "10.3390/aerospace10060557",
        "affiliation_name": "University of Piraeus",
        "affiliation_city": "Piraeus",
        "affiliation_country": "Greece"
    },
    {
        "paper_title": "Policy-Based Spam Detection of Tweets Dataset",
        "paper_author": "Dar M.",
        "publication": "Electronics (Switzerland)",
        "citied_by": "7",
        "cover_date": "2023-06-01",
        "Abstract": "Spam communications from spam ads and social media platforms such as Facebook, Twitter, and Instagram are increasing, making spam detection more popular. Many languages are used for spam review identification, including Chinese, Urdu, Roman Urdu, English, Turkish, etc.; however, there are fewer high-quality datasets available for Urdu. This is mainly because Urdu is less extensively used on social media networks such as Twitter, making it harder to collect huge volumes of relevant data. This paper investigates policy-based Urdu tweet spam detection. This study aims to collect over 1,100,000 real-time tweets from multiple users. The dataset is carefully filtered to comply with Twitter’s 100-tweet-per-hour limit. For data collection, the snscrape library is utilized, which is equipped with an API for accessing various attributes such as username, URL, and tweet content. Then, a machine learning pipeline consisting of TF-IDF, Count Vectorizer, and the following machine learning classifiers: multinomial naïve Bayes, support vector classifier RBF, logical regression, and BERT, are developed. Based on Twitter policy standards, feature extraction is performed, and the dataset is separated into training and testing sets for spam analysis. Experimental results show that the logistic regression classifier has achieved the highest accuracy, with an F1-score of 0.70 and an accuracy of 99.55%. The findings of the study show the effectiveness of policy-based spam detection in Urdu tweets using machine learning and BERT layer models and contribute to the development of a robust Urdu language social media spam detection method.",
        "DOI": "10.3390/electronics12122662",
        "affiliation_name": "University of Engineering and Technology, Lahore",
        "affiliation_city": "Lahore",
        "affiliation_country": "Pakistan"
    },
    {
        "paper_title": "Coupling Machine and Deep Learning with Explainable Artificial Intelligence for Improving Prediction of Groundwater Quality and Decision-Making in Arid Region, Saudi Arabia",
        "paper_author": "Alshehri F.",
        "publication": "Water (Switzerland)",
        "citied_by": "23",
        "cover_date": "2023-06-01",
        "Abstract": "Recently, machine learning (ML) and deep learning (DL) models based on artificial intelligence (AI) have emerged as fast and reliable tools for predicting water quality index (WQI) in various regions worldwide. In this study, we propose a novel stacking framework based on DL models for WQI prediction, employing a convolutional neural network (CNN) model. Additionally, we introduce explainable AI (XAI) through XGBoost-based SHAP (SHapley Additive exPlanations) values to gain valuable insights that can enhance decision-making strategies in water management. Our findings demonstrate that the stacking model achieves the highest accuracy in WQI prediction (R2: 0.99, MAPE: 15.99%), outperforming the CNN model (R2: 0.90, MAPE: 58.97%). Although the CNN model shows a relatively high R2 value, other statistical measures indicate that it is actually the worst-performing model among the five tested. This discrepancy may be attributed to the limited training data available for the CNN model. Furthermore, the application of explainable AI (XAI) techniques, specifically XGBoost-based SHAP values, allows us to gain deep insights into the models and extract valuable information for water management purposes. The SHAP values and interaction plot reveal that elevated levels of total dissolved solids (TDS), zinc, and electrical conductivity (EC) are the primary drivers of poor water quality. These parameters exhibit a nonlinear relationship with the water quality index, implying that even minor increases in their concentrations can significantly impact water quality. Overall, this study presents a comprehensive and integrated approach to water management, emphasizing the need for collaborative efforts among all stakeholders to mitigate pollution levels and uphold water quality. By leveraging AI and XAI, our proposed framework not only provides a powerful tool for accurate WQI prediction but also offers deep insights into the models, enabling informed decision-making in water management strategies.",
        "DOI": "10.3390/w15122298",
        "affiliation_name": "Jamia Millia Islamia",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India"
    },
    {
        "paper_title": "UAV Air Game Maneuver Decision-Making Using Dueling Double Deep Q Network with Expert Experience Storage Mechanism",
        "paper_author": "Zhang J.",
        "publication": "Drones",
        "citied_by": "3",
        "cover_date": "2023-06-01",
        "Abstract": "Deep reinforcement learning technology applied to three-dimensional Unmanned Aerial Vehicle (UAV) air game maneuver decision-making often results in low utilization efficiency of training data and algorithm convergence difficulties. To address these issues, this study proposes an expert experience storage mechanism that improves the algorithm’s performance with less experience replay time. Based on this mechanism, a maneuver decision algorithm using the Dueling Double Deep Q Network is introduced. Simulation experiments demonstrate that the proposed mechanism significantly enhances the algorithm’s performance by reducing the experience by 81.3% compared to the prioritized experience replay mechanism, enabling the UAV agent to achieve a higher maximum average reward value. The experimental results suggest that the proposed expert experience storage mechanism improves the algorithm’s performance with less experience replay time. Additionally, the proposed maneuver decision algorithm identifies the optimal policy for attacking target UAVs using different fixed strategies.",
        "DOI": "10.3390/drones7060385",
        "affiliation_name": "Beihang University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Can Bike-Sharing Reduce Car Use in Alexandroupolis? An Exploration through the Comparison of Discrete Choice and Machine Learning Models",
        "paper_author": "Narayanan S.",
        "publication": "Smart Cities",
        "citied_by": "5",
        "cover_date": "2023-06-01",
        "Abstract": "The implementation of bike-sharing systems (BSSs) is expected to lead to modifications in the travel habits of transport users, one of which is the choice of travel mode. Therefore, this research focuses on the identification of factors influencing the shift of private car users to BSSs based on stated preference survey data from the city of Alexandroupolis, Greece. A binary logit model is employed for this purpose. The estimation results indicate the impacts of gender, income, travel time, travel cost and safety-related aspects on the mode shift, through which behavioural insights are derived. For example, car users are found to be twice as sensitive to the cost of BSSs than to that of car. Similarly, they are highly sensitive to BSS travel time. Based on the behavioural findings, policy measures are suggested under the following categories: (i) finance, (ii) regulation, (iii) infrastructure, (iv) campaigns and (v) customer targeting. In addition, a secondary objective of this research is to obtain insights from the comparison of the specified logit model with a machine learning approach, as the latter is slowly gaining prominence in the field of transport. For the comparison, a random forest classifier is also developed. This comparison shows a coherence between the two approaches, although a discrepancy in the feature importance for gender and travel time is observed. A deeper exploration of this discrepancy highlights the hurdles that often occur when using mathematically more powerful models, such as the random forest classifier.",
        "DOI": "10.3390/smartcities6030060",
        "affiliation_name": "Centre for Research and Technology-Hellas",
        "affiliation_city": "Thessaloniki",
        "affiliation_country": "Greece"
    },
    {
        "paper_title": "Secure Adaptive Context-Aware ABE for Smart Environments",
        "paper_author": "Inshi S.",
        "publication": "IoT",
        "citied_by": "2",
        "cover_date": "2023-06-01",
        "Abstract": "Predicting context-aware activities using machine-learning techniques is evolving to become more readily available as a major driver of the growth of IoT applications to match the needs of the future smart autonomous environments. However, with today’s increasing security risks in the emerging cloud technologies, which share massive data capabilities and impose regulation requirements on privacy, as well as the emergence of new multiuser, multiprofile, and multidevice technologies, there is a growing need for new approaches to address the new challenges of autonomous context awareness and its fine-grained security-enforcement models. The solutions proposed in this work aim to extend our previous LCA-ABE work to provide an intelligent, dynamic creation of context-aware policies, which has been achieved through deploying smart-learning techniques. It also provides data consent, automated access control, and secure end-to-end communications by leveraging attribute-based encryption (ABE). Moreover, our policy-driven orchestration model is able to achieve an efficient, real-time enforcement of authentication and authorization (AA) as well as federation services between users, service providers, and connected devices by aggregating, modelling, and reasoning context information and then updating consent accordingly in autonomous ways. Furthermore, our framework ensures that the accuracy of our algorithms is above 90% and their precision is around 85%, which is considerably high compared to the other reviewed approaches. Finally, the solution fulfills the newly imposed privacy regulations and leverages the full power of IoT smart environments.",
        "DOI": "10.3390/iot4020007",
        "affiliation_name": "École de Technologie Supérieure",
        "affiliation_city": "Montreal",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Artificial Intelligence-Based Prediction of Spanish Energy Pricing and Its Impact on Electric Consumption",
        "paper_author": "Hernández Rodríguez M.",
        "publication": "Machine Learning and Knowledge Extraction",
        "citied_by": "4",
        "cover_date": "2023-06-01",
        "Abstract": "The energy supply sector faces significant challenges, such as the ongoing COVID-19 pandemic and the ongoing conflict in Ukraine, which affect the stability and efficiency of the energy system. In this study, we highlight the importance of electricity pricing and the need for accurate models to estimate electricity consumption and prices, with a focus on Spain. Using hourly data, we implemented various machine learning models, including linear regression, random forest, XGBoost, LSTM, and GRU, to forecast electricity consumption and prices. Our findings have important policy implications. Firstly, our study demonstrates the potential of using advanced analytics to enhance the accuracy of electricity price and consumption forecasts, helping policymakers anticipate changes in energy demand and supply and ensure grid stability. Secondly, we emphasize the importance of having access to high-quality data for electricity demand and price modeling. Finally, we provide insights into the strengths and weaknesses of different machine learning algorithms for electricity price and consumption modeling. Our results show that the LSTM and GRU artificial neural networks are the best models for price and consumption modeling with no significant difference.",
        "DOI": "10.3390/make5020026",
        "affiliation_name": "Universidad de Granada",
        "affiliation_city": "Granada",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "Examining the Negative Sentiments Related to Influenza Vaccination from 2017 to 2022: An Unsupervised Deep Learning Analysis of 261,613 Twitter Posts",
        "paper_author": "Ng Q.X.",
        "publication": "Vaccines",
        "citied_by": "7",
        "cover_date": "2023-06-01",
        "Abstract": "Several countries are witnessing significant increases in influenza cases and severity. Despite the availability, effectiveness and safety of influenza vaccination, vaccination coverage remains suboptimal globally. In this study, we examined the prevailing negative sentiments related to influenza vaccination via a deep learning analysis of public Twitter posts over the past five years. We extracted original tweets containing the terms ‘flu jab’, ‘#flujab’, ‘flu vaccine’, ‘#fluvaccine’, ‘influenza vaccine’, ‘#influenzavaccine’, ‘influenza jab’, or ‘#influenzajab’, and posted in English from 1 January 2017 to 1 November 2022. We then identified tweets with negative sentiment from individuals, and this was followed by topic modelling using machine learning models and qualitative thematic analysis performed independently by the study investigators. A total of 261,613 tweets were analyzed. Topic modelling and thematic analysis produced five topics grouped under two major themes: (1) criticisms of governmental policies related to influenza vaccination and (2) misinformation related to influenza vaccination. A significant majority of the tweets were centered around perceived influenza vaccine mandates or coercion to vaccinate. Our analysis of temporal trends also showed an increase in the prevalence of negative sentiments related to influenza vaccination from the year 2020 onwards, which possibly coincides with misinformation related to COVID-19 policies and vaccination. There was a typology of misperceptions and misinformation underlying the negative sentiments related to influenza vaccination. Public health communications should be mindful of these findings.",
        "DOI": "10.3390/vaccines11061018",
        "affiliation_name": "School of Medicine, Dentistry &amp; Nursing",
        "affiliation_city": "Glasgow",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "A Review of Social Media Data Utilization for the Prediction of Disease Outbreaks and Understanding Public Perception",
        "paper_author": "Wang A.",
        "publication": "Big Data and Cognitive Computing",
        "citied_by": "6",
        "cover_date": "2023-06-01",
        "Abstract": "Infectious diseases take a large toll on the global population, not only through risks of illness but also through economic burdens and lifestyle changes. With both emerging and re-emerging infectious diseases increasing in number, mitigating the consequences of these diseases is a growing concern. The following review discusses how social media data, with a focus on textual Twitter data, can be collected and processed to perform disease surveillance and understand the public’s attitude toward policies around the control of emerging infectious diseases. In this paper, we review machine learning tools and approaches that were used to determine the correlation between social media activity in disease trends within regions, understand the public’s opinion, or public health leaders’ approaches to disease presentation. While recent models migrated toward popular deep learning methods, neural networks and algorithms that optimized existing models were also explored as new standards for social media data analysis in disease prediction and monitoring. As adherence to public health policies can be improved by understanding and responding to major concerns identified by sentiment analyses, the advancements and challenges in understanding text sentiment are also discussed. Recent sentiment classifiers include more complex classifications and can even recognize epidemiological considerations that affect the spread of outbreaks. The comprehensive integration of locational and epidemiological considerations with advanced modeling capabilities and sentiment analysis will produce robust models and more precision for both disease monitoring and prediction. Accurate real-time disease outbreak prediction models will provide health organizations with the capability to address public concerns and to initiate outbreak responses proactively rather than reactively.",
        "DOI": "10.3390/bdcc7020072",
        "affiliation_name": "University of Guelph",
        "affiliation_city": "Guelph",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "A Reinforcement Learning Approach for Scheduling Problems with Improved Generalization through Order Swapping",
        "paper_author": "Vivekanandan D.",
        "publication": "Machine Learning and Knowledge Extraction",
        "citied_by": "5",
        "cover_date": "2023-06-01",
        "Abstract": "The scheduling of production resources (such as associating jobs to machines) plays a vital role for the manufacturing industry not only for saving energy, but also for increasing the overall efficiency. Among the different job scheduling problems, the Job Shop Scheduling Problem (JSSP) is addressed in this work. JSSP falls into the category of NP-hard Combinatorial Optimization Problem (COP), in which solving the problem through exhaustive search becomes unfeasible. Simple heuristics such as First-In, First-Out, Largest Processing Time First and metaheuristics such as taboo search are often adopted to solve the problem by truncating the search space. The viability of the methods becomes inefficient for large problem sizes as it is either far from the optimum or time consuming. In recent years, the research towards using Deep Reinforcement Learning (DRL) to solve COPs has gained interest and has shown promising results in terms of solution quality and computational efficiency. In this work, we provide an novel approach to solve the JSSP examining the objectives generalization and solution effectiveness using DRL. In particular, we employ the Proximal Policy Optimization (PPO) algorithm that adopts the policy-gradient paradigm that is found to perform well in the constrained dispatching of jobs. We incorporated a new method called Order Swapping Mechanism (OSM) in the environment to achieve better generalized learning of the problem. The performance of the presented approach is analyzed in depth by using a set of available benchmark instances and comparing our results with the work of other groups.",
        "DOI": "10.3390/make5020025",
        "affiliation_name": "Technische Hochschule Rosenheim",
        "affiliation_city": "Rosenheim",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Sentiment analysis of tweets and government translations: Assessing China’s post-COVID-19 landscape for signs of withering or booming",
        "paper_author": "Wang H.",
        "publication": "Global Media and China",
        "citied_by": "3",
        "cover_date": "2023-06-01",
        "Abstract": "This article aims to gain insights into the prevailing public sentiment during the policy relaxation period by examining whether the post-COVID-19 landscape reflects signs of withering or booming conditions. Employing methods from natural language processing (NLP) and machine learning (ML), the analysis reveals a predominance of positive sentiment from December 7, 2022 to May 17, 2023, indicative of an optimistic perspective and a potentially flourishing environment. A predictive model based on logistic regression emerges as a notably effective tool for sentiment prediction, suggesting potential utility in predicting future public health crises. A comparison of sentiments in translations by the government aligns with previous research, revealing a less favorable depiction of translated texts compared to the source texts. Furthermore, the commonality index, a measure of group consensus value, surpasses the typical range, while the certainty index, a measure of confidence, slightly falls below the norm. These findings offer valuable insights for policy considerations while highlighting areas for international communication and understanding improvement.",
        "DOI": "10.1177/20594364231181745",
        "affiliation_name": "Beijing Language and Culture University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Smart Policy Control for Securing Federated Learning Management System",
        "paper_author": "Kalapaaking A.P.",
        "publication": "IEEE Transactions on Network and Service Management",
        "citied_by": "12",
        "cover_date": "2023-06-01",
        "Abstract": "The widespread adoption of Internet of Things (IoT) devices in smart cities, intelligent healthcare systems, and various real-world applications have resulted in the generation of vast amounts of data, often analyzed using different Machine Learning (ML) models. Federated learning (FL) has been acknowledged as a privacy-preserving machine learning technology, where multiple parties cooperatively train ML models without exchanging raw data. However, the current FL architecture does not allow for an audit of the training process due to the various data-protection policies implemented by each FL participant. Furthermore, there is no global model verifiability available in the current architecture. This paper proposes a smart contract-based policy control for securing the Federated Learning (FL) management system. First, we develop and deploy a smart contract-based local training policy control on the FL participants' side. This policy control is used to verify the training process, ensuring that the evaluation process follows the same rules for all FL participants. We then enforce a smart contract-based aggregation policy to manage the global model aggregation process. Upon completion, the aggregated model and policy are stored on blockchain-based storage. Subsequently, we distribute the aggregated global model and the smart contract to all FL participants. Our proposed method uses smart policy control to manage access and verify the integrity of machine learning models. We conducted multiple experiments with various machine learning architectures and datasets to evaluate our proposed framework, such as MNIST and CIFAR-10.",
        "DOI": "10.1109/TNSM.2023.3276594",
        "affiliation_name": "RMIT University",
        "affiliation_city": "Melbourne",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Predicting firm creation in rural Texas: A multi-model machine learning approach to a complex policy problem",
        "paper_author": "Hand M.C.",
        "publication": "PLoS ONE",
        "citied_by": "2",
        "cover_date": "2023-06-01",
        "Abstract": "Rural and urban America have becoming increasingly divided, both politically and economically. Entrepreneurship can help rural communities catch back up by jumpstarting economic growth, creating jobs, and building resilience to economic shocks. However, less is known about firm creation in rural areas compared to urban areas. To that end, in this paper we ask: What factors predict firm creation in rural America? Our analysis, based on a comparative framework involving multiple machine learning modeling techniques, helps addresses three gaps in academic literature on rural firm creation. First, entrepreneurship research stretches across disciplines, often using econometric methods to identify the effect of a specific variable, rather than comparing the predictive importance of multiple variables. Second, research on firm creation centers on high-tech, urban firms. Third, modern machine learning techniques have not yet been applied in an integrated way to address rural entrepreneurship, a complex economic and policy problem that defies simple, monocausal claims. In this paper, we apply four machine learning methods (subset selection, lasso, random forest, and extreme gradient boosting) to a novel dataset to examine what social and economic factors are predictive of firm growth in rural Texas counties from 2008–2018. Our results suggest that some factors commonly discussed as promoting entrepreneurship (e.g., access to broadband and patents) may not be as predictive as socioeconomic ones (age distribution, ethnic diversity, social capital, and immigration). We also find that the strength of specific industries (oil, wind, healthcare, and elder/childcare) predicts firm growth, as does the number of local banks. Most factors predictive of firm growth in rural counties are distinct from those in urban counties, supporting the argument that rural entrepreneurship is a distinct phenomenon worthy of distinct focus. More broadly, this multi-model approach can offer initial, focusing guidance to policymakers seeking to address similarly complex policy problems.",
        "DOI": "10.1371/journal.pone.0287217",
        "affiliation_name": "The University of Texas at Austin",
        "affiliation_city": "Austin",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "GFlowNets for AI-driven scientific discovery",
        "paper_author": "Jain M.",
        "publication": "Digital Discovery",
        "citied_by": "9",
        "cover_date": "2023-06-01",
        "Abstract": "Tackling the most pressing problems for humanity, such as the climate crisis and the threat of global pandemics, requires accelerating the pace of scientific discovery. While science has traditionally relied on trial and error and even serendipity to a large extent, the last few decades have seen a surge of data-driven scientific discoveries. However, in order to truly leverage large-scale data sets and high-throughput experimental setups, machine learning methods will need to be further improved and better integrated in the scientific discovery pipeline. A key challenge for current machine learning methods in this context is the efficient exploration of very large search spaces, which requires techniques for estimating reducible (epistemic) uncertainty and generating sets of diverse and informative experiments to perform. This motivated a new probabilistic machine learning framework called GFlowNets, which can be applied in the modeling, hypotheses generation and experimental design stages of the experimental science loop. GFlowNets learn to sample from a distribution given indirectly by a reward function corresponding to an unnormalized probability, which enables sampling diverse, high-reward candidates. GFlowNets can also be used to form efficient and amortized Bayesian posterior estimators for causal models conditioned on the already acquired experimental data. Having such posterior models can then provide estimators of epistemic uncertainty and information gain that can drive an experimental design policy. Altogether, here we will argue that GFlowNets can become a valuable tool for AI-driven scientific discovery, especially in scenarios of very large candidate spaces where we have access to cheap but inaccurate measurements or too expensive but accurate measurements. This is a common setting in the context of drug and material discovery, which we use as examples throughout the paper.",
        "DOI": "10.1039/d3dd00002h",
        "affiliation_name": "Montreal Institute for Learning Algorithms",
        "affiliation_city": "Montreal",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Built environment interventions for emission mitigation: A machine learning analysis of travel-related CO<inf>2</inf> in a developing city",
        "paper_author": "Shao Q.",
        "publication": "Journal of Transport Geography",
        "citied_by": "18",
        "cover_date": "2023-06-01",
        "Abstract": "The transport sector accounts for more than one-fifth of global CO2 emissions. Reducing fossil fuel consumption and travel-related CO2 emissions (TCE) is a major approach to mitigating global climate change. Urban planners worldwide propose to promote low-carbon travel by changing the built environment. Therefore, understanding the relationships between built environment variables and TCE is key to the development of land use and transportation policies. Using 2019 regional household travel data from Zhongshan, a polycentric urban area in China, this study developed a gradient boosting decision trees model to estimate the relative importance of built environment variables in predicting TCE and their nonlinear associations with TCE. Built environment variables collectively contribute nearly half of the predictive power to predicting TCE, suggesting the potential of built environment interventions. Among them, location accessibility to city-level and township-level centers and population density are the top-three important features in predicting TCE. Furthermore, most built environment variables show threshold relationships with TCE. The results suggest that polycentric development, intensification of town centers, and densification of street networks are conducive to TCE mitigation. These findings inform planners of effective ranges of built environment variables to promote low-carbon travel.",
        "DOI": "10.1016/j.jtrangeo.2023.103632",
        "affiliation_name": "University of Minnesota Twin Cities",
        "affiliation_city": "Minneapolis",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Energy impact assessment of electric vehicle insertion in the Brazilian scenario, 2020 – 2050: a machine learning approach to fleet projection",
        "paper_author": "de Carvalho E.N.",
        "publication": "e-Prime - Advances in Electrical Engineering, Electronics and Energy",
        "citied_by": "8",
        "cover_date": "2023-06-01",
        "Abstract": "The transportation sector stands out on the world stage as one of the largest consumers of energy and the trend towards electrification of the light vehicle fleet is proving to be challenging, given the impacts generated by growing demand of energy and mainly in the generation of electricity. In Brazil, the electricity generation mix comprises around 83% of renewable sources, basically composed of hydraulic, wind and biomass sources, making it reasonable to consider the electrification of the transportation sector as one of the measures to be adopted in the promotion of sector sustainability. This article contributes to energy planning through the long-term projection of the Brazilian fleet of light vehicles, and the simulation of their impacts on energy demand, in three scenarios of insertion of plug-in electric vehicles. In this sense, statistical methodologies applied to historical data series and supervised Machine Learning algorithms for curve fitting were used, as well as the well-to-wheels approach in the fuel life cycle to estimate the energy demand. The projections included three scenarios for the insertion of electric vehicles with annual increases in demand for electricity generation which would correspond to 0.6%, 0.9% and 4.2% of additional electricity generation demand in the year 2050, with reference to the year 2020. The electrification of the fleet also proved to be advantageous in terms of reducing the use of fossil fuels and promising in terms of sustainability in the transport sector. The results of these studies will provide fundamental information for energy planning, as well as for public policy decision-making.",
        "DOI": "10.1016/j.prime.2023.100184",
        "affiliation_name": "Universidade de Brasília",
        "affiliation_city": "Brasilia",
        "affiliation_country": "Brazil"
    },
    {
        "paper_title": "Sustainable Road Planning for Trucks in Urbanized Areas of Chinese Cities Using Deep Learning Approaches",
        "paper_author": "Wang H.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "5",
        "cover_date": "2023-06-01",
        "Abstract": "Nowadays, urban areas are experiencing heavy traffic, and governments are implementing various policies to manage it. For example, in China, trucks are prohibited from entering urban areas during the daytime to reduce traffic congestion. However, we have found that this policy is not cost-efficient for logistics, which includes gas fees, air pollution fees, and wear and tear expenses, as it cannot adjust to real-time traffic conditions. To minimize logistics costs in real-time, we propose DeepPlan, a deep-learning-based model that optimizes urban planning. Our model calculates the optimal route for each truck based on real-time traffic data in urban areas. We learned the optimal route from the trace data of taxi drivers who are experienced in minimizing logistics costs. Our experimental results show that DeepPlan outperforms existing urban plans by 25% and works well in various circumstances, including different weather and unexpected events.",
        "DOI": "10.3390/su15118763",
        "affiliation_name": "China University of Petroleum (East China)",
        "affiliation_city": "Qingdao",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Impact of Regional Mobility on Air Quality during COVID-19 Lockdown in Mississippi, USA Using Machine Learning",
        "paper_author": "Tuluri F.",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "0",
        "cover_date": "2023-06-01",
        "Abstract": "Social distancing measures and shelter-in-place orders to limit mobility and transportation were among the strategic measures taken to control the rapid spreading of COVID-19. In major metropolitan areas, there was an estimated decrease of 50 to 90 percent in transit use. The secondary effect of the COVID-19 lockdown was expected to improve air quality, leading to a decrease in respiratory diseases. The present study examines the impact of mobility on air quality during the COVID-19 lockdown in the state of Mississippi (MS), USA. The study region is selected because of its non-metropolitan and non-industrial settings. Concentrations of air pollutants—particulate matter 2.5 (PM2.5), particulate matter 10 (PM10), ozone (O3), nitrogen oxide (NO2), sulfur dioxide (SO2), and carbon monoxide (CO)—were collected from the Environmental Protection Agency, USA from 2011 to 2020. Because of limitations in the data availability, the air quality data of Jackson, MS were assumed to be representative of the entire region of the state. Weather data (temperature, humidity, pressure, precipitation, wind speed, and wind direction) were collected from the National Oceanic and Atmospheric Administration, USA. Traffic-related data (transit) were taken from Google for the year 2020. The statistical and machine learning tools of R Studio were used on the data to study the changes in air quality, if any, during the lockdown period. Weather-normalized machine learning modeling simulating business-as-scenario (BAU) predicted a significant difference in the means of the observed and predicted values for NO2, O3, and CO (p < 0.05). Due to the lockdown, the mean concentrations decreased for NO2 and CO by −4.1 ppb and −0.088 ppm, respectively, while it increased for O3 by 0.002 ppm. The observed and predicted air quality results agree with the observed decrease in transit by −50.5% as a percentage change of the baseline, and the observed decrease in the prevalence rate of asthma in MS during the lockdown. This study demonstrates the validity and use of simple, easy, and versatile analytical tools to assist policymakers with estimating changes in air quality in situations of a pandemic or natural hazards, and to take measures for mitigating if the deterioration of air quality is detected.",
        "DOI": "10.3390/ijerph20116022",
        "affiliation_name": "College of Science, Engineering and Technology",
        "affiliation_city": "Jackson",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Predicting Eastern Mediterranean Flash Floods Using Support Vector Machines with Precipitable Water Vapor, Pressure, and Lightning Data",
        "paper_author": "Asaly S.",
        "publication": "Remote Sensing",
        "citied_by": "5",
        "cover_date": "2023-06-01",
        "Abstract": "Flash floods in the Eastern Mediterranean (EM) region are considered among the most destructive natural hazards, which pose a significant challenge to model due to their high complexity. Machine learning (ML) methods have made a significant contribution to the advancement of flash flood prediction systems by providing cost-effective solutions with improved performance, enabling the modeling of the complex mathematical expressions underlying physical processes of flash floods. Thus, the development of ML methods for flash flood prediction holds the potential to mitigate risks, inform policy recommendations, minimize loss of human life, and reduce property damage caused by flash floods. Here, we present a novel approach for improving flash flood predictions in the EM region using Support Vector Machines (SVMs) with a combination of precipitable water vapor (PWV) data, derived from ground-based global navigation satellite system (GNSS) receivers, along with surface pressure measurements, and nearby lightning occurrence data to predict flash floods in an arid region of the EM. The SVM model was trained on historical data from 2004 to 2019 and was used to forecast the likelihood of flash floods in the region. The study found that integrating nearby lightning data with the other variables significantly improved the accuracy of flash flood prediction compared to using only PWV and surface pressure measurements. The results of the SVM model were validated using observed flash flood events, and the model was found to have a high predictive accuracy with an area under the receiver operating characteristic curve of 0.93 for the test set. The study provides valuable insights into the potential of utilizing a combination of meteorological and lightning data for improving flash flood forecasting in the Eastern Mediterranean region.",
        "DOI": "10.3390/rs15112916",
        "affiliation_name": "Ariel University",
        "affiliation_city": "Ariel",
        "affiliation_country": "Israel"
    },
    {
        "paper_title": "Mapping Maize Cropland and Land Cover in Semi-Arid Region in Northern Nigeria Using Machine Learning and Google Earth Engine",
        "paper_author": "Abubakar G.A.",
        "publication": "Remote Sensing",
        "citied_by": "10",
        "cover_date": "2023-06-01",
        "Abstract": "The monitoring of crop quantity and quality is vital for global food security. National food security has recently been at the forefront of local and regional research, and has become a vital priority for most developing countries. Therefore, ensuring reliable classification of cropland and other land cover is crucial for sustainable agricultural development and ensuring national food security. A good understanding of the Nigerian agricultural sector is essential to making better decisions and managing operations more efficiently. Scientists, practitioners, and policymakers must exchange reliable information to develop and support agricultural programs and policies. It is essential to develop and implement novel methods for mapping maize cropland and other land cover types. Thus, Seasonal Crop Inventory (SCI) is a valuable tool for farmers, researchers, and policymakers, as it provides critical information on crop production. It informs decisions related to land management, food security, and agricultural policy. In this study, Sentinel-1 and Sentinel-2 images have been combined to map maize cropland and other land covers in northern Nigeria during the 2016–2019 growing season. We employed a technologically advanced space-based remote sensing technique. As a pioneer study that obtained detailed information on northern Nigeria’s cropland, the research utilized platforms such as Google Earth Engine (GEE), a cloud-computing engine using various classification techniques that include Random Forest (RF), Support Vector Machine (SVM), and Classification Regression Trees (CART) algorithms to produce a pixel-based Seasonal Crop Inventory of the study area. The outcome demonstrated a reliable GEE-based mapping of the region’s cropland with satisfactory classification accuracy. It revealed the overall accuracy values and the Kappa coefficients to be above 97% during the different time nodes under study. It also indicated a 98% and 93% producer and user accuracy for the cropland. The research further revealed that the Random Forest performed the best among the three machine-learning models tested in this study for mapping the maize cropland and other land cover classes. Therefore, the study’s findings and the derived crop mapping would greatly help provide valuable information that helps farmers, policymakers, and other stakeholders make more informed decisions about agricultural production, land use planning, and resource management.",
        "DOI": "10.3390/rs15112835",
        "affiliation_name": "College of Civil Engineering and Architecture Zhejiang University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Adoption of machine learning algorithm for predicting the length of stay of patients (construction workers) during COVID pandemic",
        "paper_author": "Samy S.S.",
        "publication": "International Journal of Information Technology (Singapore)",
        "citied_by": "10",
        "cover_date": "2023-06-01",
        "Abstract": "The construction sector in a rapidly developing country like India is a very unorganized sector. A large number of workers were affected and hospitalized during the pandemic. This situation is costing the sector heavily in several respects. This research study was conducted as part of using machine learning algorithms to improve construction company health and safety policies. LOS (length of stay) is used to predict how long a patient will stay in a hospital. Predicting LOS is very useful not only for hospitals, but also for construction companies to measure resources and reduce costs. Predicting LOS has become an important step in most hospitals before admitting patients. In this post, we used the Medical Information Mart for Intensive Care(MIMIC III) dataset and applied four different machine learning algorithms: decision tree classifier, random forest, Artificial Neural Network (ANN), and logistic regression. First, I performed data pre-processing to clean up the dataset. In the next step, we performed function selection using the Select Best algorithm with an evaluation function of chi2 to perform hot coding. We then performed a split between training and testing and applied a machine learning algorithm. The metric used for comparison was accuracy. After implementing the algorithms, the accuracy was compared. Random forest was found to perform best at 89%. Afterwards, we performed hyperparameter tuning using a grid search algorithm on a random forest to obtain higher accuracy. The final accuracy is 90%. This kind of research can help improve health security policies by introducing modern computational techniques, and can also help optimize resources.",
        "DOI": "10.1007/s41870-023-01296-6",
        "affiliation_name": "NICMAR University, Pune",
        "affiliation_city": "Pune",
        "affiliation_country": "India"
    },
    {
        "paper_title": "A Multi-Scaling Reinforcement Learning Trading System Based on Multi-Scaling Convolutional Neural Networks",
        "paper_author": "Huang Y.",
        "publication": "Mathematics",
        "citied_by": "6",
        "cover_date": "2023-06-01",
        "Abstract": "Advancements in machine learning have led to an increased interest in applying deep reinforcement learning techniques to investment decision-making problems. Despite this, existing approaches often rely solely on single-scaling daily data, neglecting the importance of multi-scaling information, such as weekly or monthly data, in decision-making processes. To address this limitation, a multi-scaling convolutional neural network for reinforcement learning-based stock trading, termed multi-scaling convolutional neural network SARSA (state, action, reward, state, action), is proposed. Our method utilizes a multi-scaling convolutional neural network to obtain multi-scaling features of daily and weekly financial data automatically. This involves using a convolutional neural network with several filter sizes to perform a multi-scaling extraction of temporal features. Multiple-scaling feature mining allows agents to operate over longer time scaling, identifying low stock positions on the weekly line and avoiding daily fluctuations during continuous declines. This mimics the human approach of considering information at varying temporal and spatial scaling during stock trading. We further enhance the network’s robustness by adding an average pooling layer to the backbone convolutional neural network, reducing overfitting. State, action, reward, state, action, as an on-policy reinforcement learning method, generates dynamic trading strategies that combine multi-scaling information across different time scaling, while avoiding dangerous strategies. We evaluate the effectiveness of our proposed method on four real-world datasets (Dow Jones, NASDAQ, General Electric, and AAPLE) spanning from 1 January 2007 to 31 December 2020, and demonstrate its superior profits compared to several baseline methods. In addition, we perform various comparative and ablation tests in order to demonstrate the superiority of the proposed network architecture. Through these experiments, our proposed multi-scaling module yields better results compared to the single-scaling module.",
        "DOI": "10.3390/math11112467",
        "affiliation_name": "Macau University of Science and Technology",
        "affiliation_city": "Taipa",
        "affiliation_country": "Macao"
    },
    {
        "paper_title": "Does Anyone Suffer From Teenage Motherhood? Mental Health Effects of Teen Motherhood in Great Britain Are Small and Homogeneous",
        "paper_author": "O’flaherty M.",
        "publication": "Demography",
        "citied_by": "1",
        "cover_date": "2023-06-01",
        "Abstract": "Teen mothers experience disadvantage across a wide range of outcomes. However, previous research is equivocal with respect to possible long-term mental health consequences of teen motherhood and has not adequately considered the pos-si bility that effects on mental health may be heterogeneous. Drawing on data from the 1970 British Birth Cohort Study, this article applies a novel statistical machine-learning approach—Bayesian Additive Regression Trees—to estimate the effects of teen mother hood on mental health outcomes at ages 30, 34, and 42. We extend previous work by esti­mat­ing not only sam­ple-aver­age effects but also indi­vid­ual-spe­cific esti­ma­tes. Our results show that sample-average mental health effects of teen motherhood are sub-stan­tively small at all­time points, apart from age 30 com­par­i­sons to women who first became moth­ers at age 25‒30. Moreover, we find that these effects are largely homo-ge neous for all women in the sample—indicating that there are no subgroups in the data who experience important detrimental mental health consequences. We conclude that there are likely no men­tal health ben­e­fits to pol­icy and inter­ven­tions that aim to prevent teen motherhood.",
        "DOI": "10.1215/00703370-10788364",
        "affiliation_name": "Umeå Universitet",
        "affiliation_city": "Umea",
        "affiliation_country": "Sweden"
    },
    {
        "paper_title": "Exposome: from definition to future challenges",
        "paper_author": "D’Errico A.",
        "publication": "Recenti Progressi in Medicina",
        "citied_by": "0",
        "cover_date": "2023-06-01",
        "Abstract": "The exposome concept arises from the need to integrate different disciplines of public health and environmental sciences, mainly including environmental epidemiology, exposure science, and toxicology. The role of the exposome is to understand how the totality of an individual’s exposures throughout the lifetime can impact human health. The etiology of a health condition is rarely explained by a single exposure. Therefore, examining the human exposome as a whole becomes relevant to simultaneously consider multiple risk factors and more accurately estimate concurrent causes of different health outcomes. Generally, the exposome is explained through three domains: general external exposome, specific external exposome, and internal exposome. The general external exposome includes measurable population-level exposures such as air pollution or meteorological factors. The specific external exposome includes information on individual exposures, such as lifestyle factors, typically obtained from questionnaires. Meanwhile, the internal exposome encompasses multiple biological responses to external factors, detected through molecular and omics analyses. Additionally, in recent decades, the socio-exposome theory has emerged, where all exposures are studied as a phenomenon dependent on the interaction between socioeconomic factors that vary depending on the context, allowing the identification of mechanisms that lead to health inequalities. The considerable production of data in exposome studies has led researchers to face new methodological and statistical challenges, introducing various approaches to estimate the effect of the exposome on health. Among the most common are regression models (Exposome-Wide Association Study - ExWAS), dimensionality reduction and exposure grouping techniques, and machine learning methods. The significant conceptual and methodological innovation of the exposome for a more holistic evaluation of the risks associated with human health is continuously expanding and will require further investigations related to the application of information obtained from studies into prevention and public health policies.",
        "DOI": "10.1701/4042.40227",
        "affiliation_name": "Università degli Studi di Torino",
        "affiliation_city": "Turin",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Evidence of supply security and sustainability challenges in Nigeria's power sector",
        "paper_author": "Magazzino C.",
        "publication": "Utilities Policy",
        "citied_by": "9",
        "cover_date": "2023-06-01",
        "Abstract": "The increasing mismatch between the demand and supply of power in Nigeria raises concerns about the ability of this country to meet its vital energy security and sustainability targets in a demography-growing environment. This paper assesses how these three factors comove over the long run. While Nigeria provides an illustrative case, a multivariate framework including population dynamics, the demand for electricity, and CO2 emissions from the power and heating sector is set with actual time-series data spanning the last five decades. Two independent estimation strategies are conducted: a time-series analysis (i.e., Least Squares with breaks regression) is complemented with Machine Learning experiments (i.e., ML Clustering method). In general, both methodologies’ outputs stress the engine role of the population in driving the demand for power over the long run.",
        "DOI": "10.1016/j.jup.2023.101576",
        "affiliation_name": "Niccolò Cusano University",
        "affiliation_city": "Rome",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Self-attention reinforcement learning for multi-beam combining in mmWave 3D-MIMO systems",
        "paper_author": "Huang Y.",
        "publication": "Science China Information Sciences",
        "citied_by": "5",
        "cover_date": "2023-06-01",
        "Abstract": "Machine learning (ML) has been empowering all aspects of the wireless communication system design, among which, the reinforcement learning (RL)-based approaches have attracted a lot of research attention since they can interact with the environment directly and learn from the collected experiences efficiently. In this paper, we propose a novel and efficient RL-based multi-beam combining scheme for future millimeter-wave (mmWave) three-dimensional (3D) multi-input multi-output (MIMO) communication systems. The proposed scheme does not require perfect channel state information (CSI) or precise user location information which both are generally difficult to obtain in practice, and well addresses the crucial challenge of computational complexity incurred by the extremely huge state and action spaces associated with multiple users, multiple paths, and multiple 3D beams. In particular, a self-attention deep deterministic policy gradient (DDPG)-based beam selection and combination framework is proposed to learn the 3D beamforming pattern without CSI adaptively. We aim to maximize the sum-rate of the mmWave 3D-MIMO system by optimizing the serving beam set and the corresponding combining weights for each user. To this end, the transformer is incorporated into the DDPG to obtain the global information of the input elements and capture the signal directions precisely, which leads to a near-optimal beamformer design. Simulation results verify the superiority of the proposed self-attention DDPG over conventional ML-based beamforming schemes in terms of sum-rate under various scenarios.",
        "DOI": "10.1007/s11432-022-3542-6",
        "affiliation_name": "College of Information Science and Electronic Engineering, Zhejiang University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Underexplored reciprocity between genome-wide methylation status and long non-coding RNA expression reflected in breast cancer research: potential impacts for the disease management in the framework of 3P medicine",
        "paper_author": "Kapinova A.",
        "publication": "EPMA Journal",
        "citied_by": "3",
        "cover_date": "2023-06-01",
        "Abstract": "Breast cancer (BC) is the most common female malignancy reaching a pandemic scale worldwide. A comprehensive interplay between genetic alterations and shifted epigenetic regions synergistically leads to disease development and progression into metastatic BC. DNA and histones methylations, as the most studied epigenetic modifications, represent frequent and early events in the process of carcinogenesis. To this end, long non-coding RNAs (lncRNAs) are recognized as potent epigenetic modulators in pathomechanisms of BC by contributing to the regulation of DNA, RNA, and histones’ methylation. In turn, the methylation status of DNA, RNA, and histones can affect the level of lncRNAs expression demonstrating the reciprocity of mechanisms involved. Furthermore, lncRNAs might undergo methylation in response to actual medical conditions such as tumor development and treated malignancies. The reciprocity between genome-wide methylation status and long non-coding RNA expression levels in BC remains largely unexplored. Since the bio/medical research in the area is, per evidence, strongly fragmented, the relevance of this reciprocity for BC development and progression has not yet been systematically analyzed. Contextually, the article aims at: consolidating the accumulated knowledge on both—the genome-wide methylation status and corresponding lncRNA expression patterns in BC andhighlighting the potential benefits of this consolidated multi-professional approach for advanced BC management. Based on a big data analysis and machine learning for individualized data interpretation, the proposed approach demonstrates a great potential to promote predictive diagnostics and targeted prevention in the cost-effective primary healthcare (sub-optimal health conditions and protection against the health-to-disease transition) as well as advanced treatment algorithms tailored to the individualized patient profiles in secondary BC care (effective protection against metastatic disease). Clinically relevant examples are provided, including mitochondrial health control and epigenetic regulatory mechanisms involved.",
        "DOI": "10.1007/s13167-023-00323-7",
        "affiliation_name": "Qatar Foundation",
        "affiliation_city": "Doha",
        "affiliation_country": "Qatar"
    },
    {
        "paper_title": "Community- and data-driven homelessness prevention and service delivery: optimizing for equity",
        "paper_author": "Kube A.R.",
        "publication": "Journal of the American Medical Informatics Association",
        "citied_by": "4",
        "cover_date": "2023-06-01",
        "Abstract": "Objective: The study tests a community- and data-driven approach to homelessness prevention. Federal policies call for efficient and equitable local responses to homelessness. However, the overwhelming demand for limited homeless assistance is challenging without empirically supported decision-making tools and raises questions of whom to serve with scarce resources. Materials and Methods: System-wide administrative records capture the delivery of an array of homeless services (prevention, shelter, short-term housing, supportive housing) and whether households reenter the system within 2 years. Counterfactual machine learning identifies which service most likely prevents reentry for each household. Based on community input, predictions are aggregated for subpopulations of interest (race/ethnicity, gender, families, youth, and health conditions) to generate transparent prioritization rules for whom to serve first. Simulations of households entering the system during the study period evaluate whether reallocating services based on prioritization rules compared with services-as-usual. Results: Homelessness prevention benefited households who could access it, while differential effects exist for homeless households that partially align with community interests. Households with comorbid health conditions avoid homelessness most when provided longer-term supportive housing, and families with children fare best in short-term rentals. No additional differential effects existed for intersectional subgroups. Prioritization rules reduce community-wide homelessness in simulations. Moreover, prioritization mitigated observed reentry disparities for female and unaccompanied youth without excluding Black and families with children. Discussion: Leveraging administrative records with machine learning supplements local decision-making and enables ongoing evaluation of data- and equity-driven homeless services. Conclusions: Community- and data-driven prioritization rules more equitably target scarce homeless resources.",
        "DOI": "10.1093/jamia/ocad052",
        "affiliation_name": "College of Engineering and Computing",
        "affiliation_city": "Fairfax",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Off-Policy Evaluation With Online Adaptation for Robot Exploration in Challenging Environments",
        "paper_author": "Hu Y.",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "6",
        "cover_date": "2023-06-01",
        "Abstract": "Autonomous exploration has many important applications. However, classic information gain-based or frontier-based exploration only relies on the robot current state to determine the immediate exploration goal, which lacks the capability of predicting the value of future states and thus leads to inefficient exploration decisions. This letter presents a method to learn how 'good' states are, measured by the state value function, to provide a guidance for robot exploration in real-world challenging environments. We formulate our work as an off-policy evaluation (OPE) problem for robot exploration (OPERE). It consists of offline Monte-Carlo training on real-world data and performs Temporal Difference (TD) online adaptation to optimize the trained value estimator. We also design an intrinsic reward function based on sensor information coverage to enable the robot to gain more information with sparse extrinsic rewards. Results show that our method enables the robot to predict the value of future states so as to better guide robot exploration. The proposed algorithm achieves better prediction and exploration performance compared with the state-of-the-arts. To the best of our knowledge, this work for the first time demonstrates value function prediction on real-world dataset for robot exploration in challenging subterranean and urban environments.",
        "DOI": "10.1109/LRA.2023.3271520",
        "affiliation_name": "School of Engineering and Applied Sciences",
        "affiliation_city": "Buffalo",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Active Classification of Moving Targets With Learned Control Policies",
        "paper_author": "Serra-Gomez A.",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "2",
        "cover_date": "2023-06-01",
        "Abstract": "In this paper, we consider the problem where a drone has to collect semantic information to classify multiple moving targets. In particular, we address the challenge of computing control inputs that move the drone to informative viewpoints, position and orientation, when the information is extracted using a 'black-box' classifier, e.g., a deep learning neural network. These algorithms typically lack of analytical relationships between the viewpoints and their associated outputs, preventing their use in information-gathering schemes. To fill this gap, we propose a novel attention-based architecture, trained via Reinforcement Learning (RL), that outputs the next viewpoint for the drone favoring the acquisition of evidence from as many unclassified targets as possible while reasoning about their movement, orientation, and occlusions. Then, we use a low-level MPC controller to move the drone to the desired viewpoint taking into account its actual dynamics. We show that our approach not only outperforms a variety of baselines but also generalizes to scenarios unseen during training. Additionally, we show that the network scales to large numbers of targets and generalizes well to different movement dynamics of the targets.",
        "DOI": "10.1109/LRA.2023.3271508",
        "affiliation_name": "Department of Cognitive Robotics, TU Delft",
        "affiliation_city": "Delft",
        "affiliation_country": "Netherlands"
    },
    {
        "paper_title": "Orbit: A Unified Simulation Framework for Interactive Robot Learning Environments",
        "paper_author": "Mittal M.",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "45",
        "cover_date": "2023-06-01",
        "Abstract": "We present Orbit, a unified and modular framework for robot learning powered by Nvidia Isaac Sim. It offers a modular design to easily and efficiently create robotic environments with photo-realistic scenes and high-fidelity rigid and deformable body simulation. With Orbit, we provide a suite of benchmark tasks of varying difficulty- from single-stage cabinet opening and cloth folding to multi-stage tasks such as room reorganization. To support working with diverse observations and action spaces, we include fixed-arm and mobile manipulators with different physically-based sensors and motion generators. Orbit allows training reinforcement learning policies and collecting large demonstration datasets from hand-crafted or expert solutions in a matter of minutes by leveraging GPU-based parallelization. In summary, we offer an open-sourced framework that readily comes with 16 robotic platforms, 4 sensor modalities, 10 motion generators, more than 20 benchmark tasks, and wrappers to 4 learning libraries. With this framework, we aim to support various research areas, including representation learning, reinforcement learning, imitation learning, and task and motion planning. We hope it helps establish interdisciplinary collaborations in these communities, and its modularity makes it easily extensible for more tasks and applications in the future.",
        "DOI": "10.1109/LRA.2023.3270034",
        "affiliation_name": "NVIDIA",
        "affiliation_city": "Santa Clara",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Global Warming Status in the African Continent: Sources, Challenges, Policies, and Future Direction",
        "paper_author": "Bedair H.",
        "publication": "International Journal of Environmental Research",
        "citied_by": "29",
        "cover_date": "2023-06-01",
        "Abstract": "Africa is the second largest continent after Asia, having a larger than 30 million km2 area. Doubtlessly, one of the biggest ecological and societal problems of the twenty-first century is climate change. Since the early 1970s, it has been clear that Africa is already experiencing the effects of climate change, and it has given rise to a wide range of new and unusual phenomena, such as rising temperatures, poor agricultural output, extreme different weather scenarios, and the spread of disease, among other things. Therefore, the current review aims at screening the impact of climate change on agricultural sector, human health and food security in Africa compared to the other continents, evaluating the change projections in future and highlighting the role of African leaders in mitigating and adapting to these effects. Artificial intelligence, remote sensing, and high-tech algorithms were applied to analyze these effects. Historical data were downloaded in near real-time from January 2009 to the present from the FAO Water Productivity Open-access portal WaPOR and Terra Climate datasets on Earth Engine platform. Assessment process was performed using Google Earth Engine, whereas future data were downloaded from WorldClim 2.1. We used 2021–2040 timelines and two scenarios: SSP245 and SSP585. For the SSP and timeline, we downloaded four versions, based on four different global circulation models (GCMs): IPSL-CM6A-LR (France), MRI-ESM2-0 (Japan), CanESM5 (Canadian), and BCC-CSM2-MR (China), to reflect the uncertainty among GCMs. We averaged future projection of each variable and SSP across four GCMs to decrease the uncertainty connected with a particular GCM. We presented the averaged results as maps. Annual precipitation totals were significantly above average in Central and East Africa, while under SSP 245 scenario, Madagascar would experience high rainfall. The highest temperature anomalies were seen in parts of the Greater Horn of Africa, western equatorial regions, and the north-western part of the continent. Minimum and Maximum temperature predictions showed that Africa would experience harsh temperatures than previously recorded in the historical years. A high average maximum temperature is predicted across the sub-Sahara Africa, South Africa, Somalia, and Madagascar under SSP 245 and SSP 585. The MCD64A1 dataset tagged in Earth Engine was used to classify forest fire risk in Africa. Analysis revealed that the highest fire risk was recorded in Savannah in tropical and subtropical Africa. Further, changes in rainfall and increased temperature leading to increased evaporation would directly reduce runoff levels and recharge groundwater which in turn will have negative effects on biodiversity, agriculture, and food security. Notably, African leaders have played positive role in the recent climate negotiations and bright climate initiatives have been emerged. Hopefully they will solve the climate crisis across the continent.",
        "DOI": "10.1007/s41742-023-00534-w",
        "affiliation_name": "Libyan Authority for Scientific Research",
        "affiliation_city": "Tripoli",
        "affiliation_country": "Libya"
    },
    {
        "paper_title": "NCATS’s plan to grow the rare disease pipeline",
        "paper_author": "Mullard A.",
        "publication": "Nature Reviews Drug Discovery",
        "citied_by": "1",
        "cover_date": "2023-06-01",
        "Abstract": "Joni Rutter, director of the National Center for Advancing Translational Sciences (NCATS), discusses the centre’s audacious goal to have 25% of diseases covered by the experimental drug pipeline by 2032. [Figure not available: see fulltext.]",
        "DOI": "10.1038/d41573-023-00082-0",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Electric vehicles as a sustainable energy Technology: Observations from travel survey data and evaluation of adoption with Machine learning method",
        "paper_author": "Dai Z.",
        "publication": "Sustainable Energy Technologies and Assessments",
        "citied_by": "5",
        "cover_date": "2023-06-01",
        "Abstract": "Governments worldwide are promoting Electric Vehicles (EVs) to achieve the energy conservation and emissions reduction goal, but low penetration of EVs means that it still has far to go before stepping into the sustainable energy future. In addition to technological breakthroughs to enhance the appeal of EVs, how to locate demand and make targeted promotions is also vital in increasing the share of EVs. This study explored the household characteristics related to EV adoption with the household travel survey data and proposed a LightGBM-based prediction modelling framework with high accuracy and explainable results. During this process, the details of sampling techniques to overcome data imbalance were discussed with the aim of improving the model performance. Furthermore, through constructing the model with high interpretability, we identified important factors regarding EV adoptions through both statistical significance and feature importance analysis. The research findings can not only assist EV manufacturers in targeting potential buyers but also help policymakers understand families’ EV purchasing decisions and develop more targeted and equitable policies and incentive programs. Such two-pronged efforts have the potential to advance the sustainability transition towards greener transportation systems.",
        "DOI": "10.1016/j.seta.2023.103267",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Atlanta",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Substance use disorders: a comprehensive update of classification, epidemiology, neurobiology, clinical aspects, treatment and prevention",
        "paper_author": "Volkow N.D.",
        "publication": "World Psychiatry",
        "citied_by": "82",
        "cover_date": "2023-06-01",
        "Abstract": "Substance use disorders (SUDs) are highly prevalent and exact a large toll on individuals’ health, well-being, and social functioning. Long-lasting changes in brain networks involved in reward, executive function, stress reactivity, mood, and self-awareness underlie the intense drive to consume substances and the inability to control this urge in a person who suffers from addiction (moderate or severe SUD). Biological (including genetics and developmental life stages) and social (including adverse childhood experiences) determinants of health are recognized factors that contribute to vulnerability for or resilience against developing a SUD. Consequently, prevention strategies that target social risk factors can improve outcomes and, when deployed in childhood and adolescence, can decrease the risk for these disorders. SUDs are treatable, and evidence of clinically significant benefit exists for medications (in opioid, nicotine and alcohol use disorders), behavioral therapies (in all SUDs), and neuromodulation (in nicotine use disorder). Treatment of SUDs should be considered within the context of a Chronic Care Model, with the intensity of intervention adjusted to the severity of the disorder and with the concomitant treatment of comorbid psychiatric and physical conditions. Involvement of health care providers in detection and management of SUDs, including referral of severe cases to specialized care, offers sustainable models of care that can be further expanded with the use of telehealth. Despite advances in our understanding and management of SUDs, individuals with these conditions continue to be stigmatized and, in some countries, incarcerated, highlighting the need to dismantle policies that perpetuate their criminalization and instead develop policies to ensure support and access to prevention and treatment.",
        "DOI": "10.1002/wps.21073",
        "affiliation_name": "National Institute on Drug Abuse (NIDA)",
        "affiliation_city": "Bethesda",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Neural co-processors for restoring brain function: results from a cortical model of grasping",
        "paper_author": "Bryan M.J.",
        "publication": "Journal of Neural Engineering",
        "citied_by": "2",
        "cover_date": "2023-06-01",
        "Abstract": "Objective. A major challenge in designing closed-loop brain-computer interfaces is finding optimal stimulation patterns as a function of ongoing neural activity for different subjects and different objectives. Traditional approaches, such as those currently used for deep brain stimulation, have largely followed a manual trial-and-error strategy to search for effective open-loop stimulation parameters, a strategy that is inefficient and does not generalize to closed-loop activity-dependent stimulation. Approach. To achieve goal-directed closed-loop neurostimulation, we propose the use of brain co-processors, devices which exploit artificial intelligence to shape neural activity and bridge injured neural circuits for targeted repair and restoration of function. Here we investigate a specific type of co-processor called a ‘neural co-processor’ which uses artificial neural networks and deep learning to learn optimal closed-loop stimulation policies. The co-processor adapts the stimulation policy as the biological circuit itself adapts to the stimulation, achieving a form of brain-device co-adaptation. Here we use simulations to lay the groundwork for future in vivo tests of neural co-processors. We leverage a previously published cortical model of grasping, to which we applied various forms of simulated lesions. We used our simulations to develop the critical learning algorithms and study adaptations to non-stationarity in preparation for future in vivo tests. Main results. Our simulations show the ability of a neural co-processor to learn a stimulation policy using a supervised learning approach, and to adapt that policy as the underlying brain and sensors change. Our co-processor successfully co-adapted with the simulated brain to accomplish the reach-and-grasp task after a variety of lesions were applied, achieving recovery towards healthy function in the range 75%-90%. Significance. Our results provide the first proof-of-concept demonstration, using computer simulations, of a neural co-processor for adaptive activity-dependent closed-loop neurostimulation for optimizing a rehabilitation goal after injury. While a significant gap remains between simulations and in vivo applications, our results provide insights on how such co-processors may eventually be developed for learning complex adaptive stimulation policies for a variety of neural rehabilitation and neuroprosthetic applications.",
        "DOI": "10.1088/1741-2552/accaa9",
        "affiliation_name": "UW College of Engineering",
        "affiliation_city": "Seattle",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Ambient ozone exposure and depression among middle-aged and older adults: Nationwide longitudinal evidence in China",
        "paper_author": "Yuan Y.",
        "publication": "International Journal of Hygiene and Environmental Health",
        "citied_by": "9",
        "cover_date": "2023-06-01",
        "Abstract": "Background: Epidemiological studies have linked long-term ozone (O3) exposure with depression in developed countries. However, available literature is sparse and exists great heterogeneities. We aimed to investigate the association of long-term O3 exposure with depression among Chinese middle-aged and older adults. Methods: We designed a repeated measurement study based on longitudinal data from four waves (2011, 2013, 2015, and 2018) of the China Health and Retirement Longitudinal Study (CHARLS). Annual mean O3 concentrations assessed through machine learning–based spatiotemporal models were assigned to each participant at city level. Depression score was measured using the 10-item Center for Epidemiologic Studies Depression scale (CES-D-10), with scores above the cut-off point of ten defined as depressive symptom. Mixed-effects models were used to evaluate the impact of O3 on depression score and depressive symptom, and quantify the concentration-response (C-R) relationships. Subgroup analyses were performed to examine the potential effect modifications. Results: A total of 19,582 participants with 60,125 visits were included in our analysis, with mean depression score of 8.1 (standard deviation: 6.3). Multivariable-adjusted mixed-effects model estimated a 6.34% (95% confidence interval [CI]: 3.34%, 9.43%) increase in depression score and an odds ratio (OR) of 1.29 (95% CI: 1.16, 1.45) for depressive symptom associated with per 10-μg/m3 rise in annual mean O3 exposure. Significantly elevated risks were identified only at high concentrations (approximately ≥90 μg/m3). Participants who suffered from chronic diseases had a significant increased risk of depression (% Change in depression score: 8.42% [95% CI: 4.79%, 12.17%], and OR: 1.42 [95% CI: 1.24, 1.62]), and an evident effect modification was identified for depressive symptom (P = 0.01). Findings: Our study provided novel evidence that long-term O3 exposure could be a risk factor for depression among Chinese middle-aged and older adults. Our findings may have significant implications for formulating policies in reducing disease burden of depression by controlling air pollution.",
        "DOI": "10.1016/j.ijheh.2023.114185",
        "affiliation_name": "Jiangsu Provincial Center for Disease Prevention and Control",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "The system of autono‑mobility: computer vision and urban complexity—reflections on artificial intelligence at urban scale",
        "paper_author": "Iapaolo F.",
        "publication": "AI and Society",
        "citied_by": "4",
        "cover_date": "2023-06-01",
        "Abstract": "Focused on city-scale automation, and using self-driving cars (SDCs) as a case study, this article reflects on the role of AI—and in particular, computer vision systems used for mapping and navigation—as a catalyst for urban transformation. Urban research commonly presents AI and cities as having a one-way cause-and-effect relationship, giving undue weight to AI’s impact on cities and overlooking the role of cities in shaping AI. Working at the intersection of data science and social research, this paper aims to counter this trend by exploring the reverse perspective: how do cities affect the development, and expose the present limits, of SDCs? The contribution of this paper is threefold. First, by comparing urban and nonurban environments and thoroughly examining the relationship between computer vision and city-specific sociality and form, it defines machine autonomy/automation as a function of the sociotechnical milieu in which an AI system operates. Second, and related, the paper problematizes the notion of SDCs as autonomous technologies and the role it plays in envisioning contending policy arrangements and technical solutions for achieving full driving automation. Finally, the article offers insight into a materialist and spatialized understanding of AI—namely, not as an abstract quality susceptible to replication within discrete machines, but rather as a distributed property emerging through embodied interactions among a multiplicity of agents (human, non-human, and technological) within/with their environments.",
        "DOI": "10.1007/s00146-022-01590-0",
        "affiliation_name": "Oxford Brookes University",
        "affiliation_city": "Oxford",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Key predictors of greenhouse gas emissions for cities committing to mitigate and adapt to climate change",
        "paper_author": "Franco C.",
        "publication": "Cities",
        "citied_by": "12",
        "cover_date": "2023-06-01",
        "Abstract": "It is central for climate policy to understand, monitor and forecast greenhouse gas (GHG) emissions, generating insights on the key factors and actions with a greater impact to mitigate and adapt to global warming. Focusing on the leading role of cities, it is relevant to build robust and reliable GHG emissions prospects for every city committing to mitigate and adapt to climate change. In this paper, a novel methodology is proposed to build those prospects, presenting a case study with 6231 EU-27 cities and local municipalities taken from the Global Covenant of Mayors (GCoM). Key GHG emissions predictors for the target years of 2020 and 2030 refer to the baseline energy consumption and associated GHG emissions, as well as the population and the national emissions per capita trend. Additionally, other informative predictors are the baseline inventory year, the reduction target, the disaggregated baseline emissions by type of fuel (fossils or renewables), the heating and cooling degree days and the NUTS urban/rural categorization. The proposed methodology allows assessing the cities' achievements based on their expected performance, and it could be further implemented to support new cities willing to commit to a significant reduction of their GHG emissions.",
        "DOI": "10.1016/j.cities.2023.104342",
        "affiliation_name": "European Commission Joint Research Centre",
        "affiliation_city": "Brussels",
        "affiliation_country": "Belgium"
    },
    {
        "paper_title": "Strategies for classifying water quality in the Cauvery River using a federated learning technique",
        "paper_author": "J V.",
        "publication": "International Journal of Cognitive Computing in Engineering",
        "citied_by": "12",
        "cover_date": "2023-06-01",
        "Abstract": "Artificial intelligence methods are emerging techniques used in the field of environmental protection, especially in the analysis of air, water, and soil quality. AI analyzes vast amounts of environmental data to predict pollution and provide decision-makers with the information they need to develop efficient policies. One of the most important problems in environmental analysis is data security, and many organizations are actively working to ensure the secure collection, storage, and utilization of sensitive environmental data. In addition, organizations are focusing on developing strategies to protect their data from malicious attacks, such as cyber-attacks, as well as from accidental misuses, like unauthorized access. For this purpose, we have introduced a novel water quality prediction using the Federated Learning Technique. Federated learning enables multiple parties to collaborate and train a model on their local data without sharing it with others, thereby preserving data privacy. The proposed method is applied to a Cauvery River dataset of water quality parameters, and the results demonstrate that the PSO-optimized federated learning process achieves better prediction accuracy of 87%, a precision of 85%, a recall of 93%, and an 89% F1 score.",
        "DOI": "10.1016/j.ijcce.2023.04.004",
        "affiliation_name": "Vellore Institute of Technology",
        "affiliation_city": "Vellore",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Reconstructed Jing River streamflow from western China: A 399-year perspective for hydrological changes in the Loess Plateau",
        "paper_author": "Zhao X.",
        "publication": "Journal of Hydrology",
        "citied_by": "22",
        "cover_date": "2023-06-01",
        "Abstract": "The Jing River is a secondary tributary of the Yellow River, which flows through the middle of the Loess Plateau in China. Severe water scarcity and soil erosion in the basin have threatened sustainable social and economic development. To assess and solve the region's water resource problems, it is important to understand its historical hydrological climate change. Accordingly, we used five machine learning models and simple linear regression to reconstruct the January-June streamflow of the Jing River based on the tree ring width of Pinus tabulaeformis and Pinus armandii. By combining six models into an ensemble streamflow reconstruction, we obtained a more accurate reconstruction and streamflow variability information than with a single model. Over the past nearly four centuries, the Jing River has experienced seven high streamflow periods and ten low streamflow periods. The main atmospheric forcing factors driving the streamflow variability are the Pacific Decadal Oscillation and the El Niño-Southern Oscillation, which regulate the climate and hydrology of the region by affecting water vapor fluxes and the Asian monsoon. The different climate scenarios revealed the continued reduction in the future Jing River streamflow and a worsening water resource situation. This new streamflow reconstruction can serve as a valuable reference for analyzing regional hydrology and informing water resource management and policy formulations.",
        "DOI": "10.1016/j.jhydrol.2023.129573",
        "affiliation_name": "China Meteorological Administration",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "AcroMonk: A Minimalist Underactuated Brachiating Robot",
        "paper_author": "Javadi M.",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "14",
        "cover_date": "2023-06-01",
        "Abstract": "Brachiation is a dynamic, coordinated swinging maneuver of body and arms used by monkeys and apes to move between branches. As a unique underactuated mode of locomotion, it is interesting to study from a robotics perspective since it can broaden the deployment scenarios for humanoids and animaloids. While several brachiating robots of varying complexity have been proposed in the past, this letter presents the simplest possible prototype of a brachiation robot, using only a single actuator and unactuated grippers. The novel passive gripper design allows it to snap on and release from monkey bars, while guaranteeing well defined start and end poses of the swing. The brachiation behavior is realized in three different ways, using trajectory optimization via direct collocation and stabilization by a model-based time-varying linear quadratic regulator (TVLQR) or model-free proportional derivative (PD) control, as well as by a reinforcement learning (RL) based control policy. The three control schemes are compared in terms of robustness to disturbances, mass uncertainty, and energy consumption. The system design and controllers have been open-sourced. Due to its minimal and open design, the system can serve as a canonical underactuated platform for education and research.",
        "DOI": "10.1109/LRA.2023.3269296",
        "affiliation_name": "Stanford Engineering",
        "affiliation_city": "Stanford",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Towards machine learning for moral choice analysis in health economics: A literature review and research agenda",
        "paper_author": "Smeele N.V.R.",
        "publication": "Social Science and Medicine",
        "citied_by": "1",
        "cover_date": "2023-06-01",
        "Abstract": "Background: Discrete choice models (DCMs) for moral choice analysis will likely lead to erroneous model outcomes and misguided policy recommendations, as only some characteristics of moral decision-making are considered. Machine learning (ML) is recently gaining interest in the field of discrete choice modelling. This paper explores the potential of combining DCMs and ML to study moral decision-making more accurately and better inform policy decisions in healthcare. Methods: An interdisciplinary literature search across four databases – PubMed, Scopus, Web of Science, and Arxiv – was conducted to gather papers. Based on the Preferred Reporting Items for Systematic and Meta-analyses (PRISMA) guideline, studies were screened for eligibility on inclusion criteria and extracted attributes from eligible papers. Of the 6285 articles, we included 277 studies. Results: DCMs have shortcomings in studying moral decision-making. Whilst the DCMs' mathematical elegance and behavioural appeal hold clear interpretations, the models do not account for the ‘moral’ cost and benefit in an individual's utility calculation. The literature showed that ML obtains higher predictive power, model flexibility, and ability to handle large and unstructured datasets. Combining the strengths of ML methods with DCMs has the potential for studying moral decision-making. Conclusions: By providing a research agenda, this paper highlights that ML has clear potential to i) find and deepen the utility specification of DCMs, and ii) enrich the insights extracted from DCMs by considering the intrapersonal determinants of moral decision-making.",
        "DOI": "10.1016/j.socscimed.2023.115910",
        "affiliation_name": "Erasmus School of Health Policy &amp; Management",
        "affiliation_city": "Rotterdam",
        "affiliation_country": "Netherlands"
    },
    {
        "paper_title": "Hospitalization status and gender recognition over the arboviral medical records using shallow and RNN-based deep models",
        "paper_author": "Gorur K.",
        "publication": "Results in Engineering",
        "citied_by": "5",
        "cover_date": "2023-06-01",
        "Abstract": "In global health systems, clinicians have a challenging decision of a triage patient exposed to arbovirus infections to determine they should be hospitalized. Diagnosing symptoms and molecular testing can be uncertain and costly, especially in resource-limited settings. However, machine learning approaches have a high potential to determine through medical record examination whom to hospitalize. The purpose of this study is to determine hospitalized or outpatient individuals correctly by implementing shallow machine learning algorithms on SISA (Severity Index for Suspected Arbovirus) and SISAL (Severity Index for Suspected Arbovirus with Laboratory) datasets. Feed Forward Neural Network (FFNN), Probabilistic Neural Network (PNN), and Decision Tree (DecT) algorithm with three splitting criterions were used to process the SISA and SISAL datasets. The results of classification performances demonstrated that improved area under the curve scores (0.973) and accuracy (reaching up to 98.73% with FFNN) were obtained when compared with previous research study related to machine learning and arbovirus. Moreover, this study also aims to investigate gender recognition over arboviral infection medical records using recurrent neural network-based deep models. Hence vector control policy in the health system reflects the gender roles to control the spread of arboviral infection. Overall the outcomes are potentially very promising.",
        "DOI": "10.1016/j.rineng.2023.101109",
        "affiliation_name": "Bandırma Onyedi Eylül University",
        "affiliation_city": "Bandirma",
        "affiliation_country": "Turkey"
    },
    {
        "paper_title": "Dap-FL: Federated Learning Flourishes by Adaptive Tuning and Secure Aggregation",
        "paper_author": "Chen Q.",
        "publication": "IEEE Transactions on Parallel and Distributed Systems",
        "citied_by": "13",
        "cover_date": "2023-06-01",
        "Abstract": "Federated learning (FL), an attractive and promising distributed machine learning paradigm, has sparked extensive interest in exploiting tremendous data stored on ubiquitous mobile devices. However, conventional FL suffers severely from resource heterogeneity, as clients with weak computational and communication capabilities may be unable to complete local training using the same local training hyper-parameters. In this article, we propose Dap-FL, a deep deterministic policy gradient (DDPG)-assisted adaptive FL system, in which local learning rates and local training epochs are adaptively adjusted by all resource-heterogeneous clients through locally deployed DDPG-assisted adaptive hyper-parameter selection schemes. Particularly, the rationality of the proposed hyper-parameter selection scheme is confirmed through rigorous mathematical proof. Besides, due to the thoughtlessness of security consideration of adaptive FL systems in previous studies, we introduce the Paillier cryptosystem to aggregate local models in a secure and privacy-preserving manner. Rigorous analyses show that the proposed Dap-FL system could protect clients' private local models against chosen-plaintext attacks and chosen-message attacks in a widely used honest-but-curious participants and active adversaries security model. More importantly, through ingenious and extensive experiments, the proposed Dap-FL achieves higher model prediction accuracy than two state-of-the-art RL-assisted FL methods, i.e., 6.03% higher than DDPG-based FL and 7.85% higher than DQN-based FL. In addition, experimental results also show that the proposed Dap-FL achieves higher global model prediction accuracy and faster convergence rates than conventional FL, and the comprehensiveness of the adjusted local training hyper-parameters is validated.",
        "DOI": "10.1109/TPDS.2023.3267897",
        "affiliation_name": "Xidian University",
        "affiliation_city": "Xi'an",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Federated Ensemble Model-Based Reinforcement Learning in Edge Computing",
        "paper_author": "Wang J.",
        "publication": "IEEE Transactions on Parallel and Distributed Systems",
        "citied_by": "19",
        "cover_date": "2023-06-01",
        "Abstract": "Federated learning (FL) is a privacy-preserving distributed machine learning paradigm that enables collaborative training among geographically distributed and heterogeneous devices without gathering their data. Extending FL beyond the supervised learning models, federated reinforcement learning (FRL) was proposed to handle sequential decision-making problems in edge computing systems. However, the existing FRL algorithms directly combine model-free RL with FL, thus often leading to high sample complexity and lacking theoretical guarantees. To address the challenges, we propose a novel FRL algorithm that effectively incorporates model-based RL and ensemble knowledge distillation into FL for the first time. Specifically, we utilise FL and knowledge distillation to create an ensemble of dynamics models for clients, and then train the policy by solely using the ensemble model without interacting with the environment. Furthermore, we theoretically prove that the monotonic improvement of the proposed algorithm is guaranteed. The extensive experimental results demonstrate that our algorithm obtains much higher sample efficiency compared to classic model-free FRL algorithms in the challenging continuous control benchmark environments under edge computing settings. The results also highlight the significant impact of heterogeneous client data and local model update steps on the performance of FRL, validating the insights obtained from our theoretical analysis.",
        "DOI": "10.1109/TPDS.2023.3264480",
        "affiliation_name": "British Telecommunications plc",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "A machine learning approach to understand how accessibility influences alluvial gold mining expansion in the Peruvian Amazon",
        "paper_author": "Larrea-Gallegos G.",
        "publication": "Case Studies in Chemical and Environmental Engineering",
        "citied_by": "5",
        "cover_date": "2023-06-01",
        "Abstract": "Alluvial small-scale gold mining (ASGM) mining in the Amazon is expanding fiercely, generating severe environmental degradation, which includes the fast disappearance of primary forests in a highly biodiverse area of the world. Different factors motivate the growth of mining in the areas and understanding this expansion is important to safeguard protected areas or implement strategies to mitigate the related social and environmental impacts. Thus, the goal of this study is to apply machine learning techniques to explore gold mining expansion in Madre de Dios, in the Peruvian Amazon, and to identify possible future hotspots of these activities. Using an unsupervised learning algorithm and a random forest classification model, past expansion trends were analyzed and an explicit geo-spatial model was built. Results demonstrate that proximity to infrastructure is not always indicative of high mining probability. In fact, when analyzing the spatial distribution of model accuracy, it is observed that model performance decreases in clusters where accessibility and mining activity showed opposite trends. In contrast, the models yield accuracies greater than 0.9 when accessibility-related variables stand out as the most important. The model, which is flexible and reproducible, demonstrates to be useful to enhance decision making when implementing geo-spatial policies to address the problem of ASGM expansion in the Amazon.",
        "DOI": "10.1016/j.cscee.2023.100353",
        "affiliation_name": "Pontificia Universidad Catolica del Peru",
        "affiliation_city": "Lima",
        "affiliation_country": "Peru"
    },
    {
        "paper_title": "Cost-Effectiveness of Screening to Identify Patients With Atrial Fibrillation: A Systematic Review",
        "paper_author": "Halahakone U.",
        "publication": "Heart Lung and Circulation",
        "citied_by": "1",
        "cover_date": "2023-06-01",
        "Abstract": "Background: Screening for Atrial Fibrillation (AF) is recommended for people aged above 65 years. Screening for AF in asymptomatic individuals can be beneficial by enabling earlier diagnosis and the commencement of interventions to reduce the risk of early events, thus improving patient outcomes. This study systematically reviews the literature about the cost-effectiveness of various screening methods for previously undiagnosed AF. Methods: Four databases were searched to identify articles that are cost-effectiveness studies conducted on screening for AF published from January 2000 to August 2022. The Consolidated Health Economic Evaluation Reporting Standards 2022 checklist was used to assess the quality of the selected studies. A previously published approach was used to assess the usefulness of each study for health policy makers. Results: The database search yielded 799 results, with 26 articles meeting the inclusion criteria. Articles were categorised into four subgroups: (i) population screening, (ii) opportunistic screening, (iii) targeted, and (iv) mixed methods of screening. Most of the studies screened adults ≥65 years of age. Most studies were performed from a ‘health care payer perspective’ and almost all studies used ‘not screening’ as a comparator. Almost all screening methods assessed were found to be cost-effective in comparison to ‘not screening’. The reporting quality varied between 58% to 89%. The majority of the studies were found to be of limited usefulness for health policy makers, as none of the studies made any clear statements about policy change or implementation direction. Conclusion: All approaches of AF screening were found to be cost-effective compared with no screening, while opportunistic screening was found to be the optimal approach in some studies. However, screening for AF in asymptomatic individuals is context specific and likely to be cost-effective depending on the population screened, screening approach, frequency, and the duration of screening.",
        "DOI": "10.1016/j.hlc.2023.03.014",
        "affiliation_name": "Royal Brisbane and Women's Hospital",
        "affiliation_city": "Brisbane",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Incentive-based demand response modeling in a day-ahead wholesale electricity market in Japan, considering the impact of customer satisfaction on social welfare and profitability",
        "paper_author": "Malehmirchegini L.",
        "publication": "Sustainable Energy, Grids and Networks",
        "citied_by": "12",
        "cover_date": "2023-06-01",
        "Abstract": "Incentive-based demand response programs (IBDRPs) are voluntary programs that encourage customers (CUs) to cut their electricity usage during peak periods, in return for receiving incentives from the service provider (SP). This research addresses a comprehensive multi-objective modeling approach, based on maximizing the social welfare function of both SPs and CUs involved in the IBDRPs, considering both the economic drivers and the social factors affecting the market actors. The social aspect of the CUs’ participation in the IBDRPs is evaluated with respect to their satisfaction with the intangible benefits, which cannot be measured in monetary terms. To this aim, a Kano model for customer satisfaction is developed to estimate the level of CUs’ satisfaction with participation in the IBDRPs, taking into account the impact of the four attributes of comfort, flexibility, energy security, and environmental protection. The model then analyzes the proposed IBDRP implemented in the Japan Electric Power Exchange (JEPX) market, using real-time wholesale electricity prices and demand loads in the Tokyo residential areas. The results revealed that, the environmental protection attribute could more positively impact the CUs’ satisfaction level; consequently, their welfare is higher with more reduction in electricity from incentive incomes.",
        "DOI": "10.1016/j.segan.2023.101044",
        "affiliation_name": "Kyushu University",
        "affiliation_city": "Fukuoka",
        "affiliation_country": "Japan"
    },
    {
        "paper_title": "Air cargo transport demand forecasting using ConvLSTM2D, an artificial neural network architecture approach",
        "paper_author": "Gerardo Muros Anguita J.",
        "publication": "Case Studies on Transport Policy",
        "citied_by": "9",
        "cover_date": "2023-06-01",
        "Abstract": "The prediction of air traffic demand (passengers and cargo) in a regional/national air transport system is essential. Knowing the behavior of future demand helps, on the one hand, the design and execution of air transport public policies, which, for example, help to focus, guide and prioritize investment (public and private) for the expansion / modernization of airport infrastructures (or development of new airports), act on tariff policies, implement changes in regulatory policy, etc.; on the other hand, it helps airport managers to plan the airport. Therefore, in this paper, a short-term forecast (5 years) of the demand for air cargo transport was carried out, applied to a specific case study (Colombia), taking into account the most severe pandemic period (the year 2020). To perform the forecast, an approach based on Machine Learning/Deep Learning (ML/DL) method comprising a hybrid of convolutional and recurrent memory neural networks (that allow space-temporal non-linear analysis, such as multi-variable spaces and temporal multi-steps), is presented. The analysis developed here establishes the optimal length of the prediction period; on the other hand, the proposed methodology allows the identification of the most relevant socioeconomic features in the prediction of air cargo demand (domestic and international), i.e., interpreting the ML/DL results obtained through the variational analysis of different combinations of features. The results show that international air cargo demand is strongly dependent on Gross Domestic Product (GDP) and PCG (Per Capita GDP), while domestic air cargo demand is significantly dependent on PCG. Finally, the results show, for the case study country, very rapid recovery of air cargo demand at pre-pandemic rates (behavior already found in other recent studies and research).",
        "DOI": "10.1016/j.cstp.2023.101009",
        "affiliation_name": "Universidad Santo Tomás",
        "affiliation_city": "Bogota",
        "affiliation_country": "Colombia"
    },
    {
        "paper_title": "Policy gradients using variational quantum circuits",
        "paper_author": "Sequeira A.",
        "publication": "Quantum Machine Intelligence",
        "citied_by": "8",
        "cover_date": "2023-06-01",
        "Abstract": "Variational quantum circuits are being used as versatile quantum machine learning models. Some empirical results exhibit an advantage in supervised and generative learning tasks. However, when applied to reinforcement learning, less is known. In this work, we considered a variational quantum circuit composed of a low-depth hardware-efficient ansatz as the parameterized policy of a reinforcement learning agent. We show that an &#120598;-approximation of the policy gradient can be obtained using a logarithmic number of samples concerning the total number of parameters. We empirically verify that such quantum models behave similarly to typical classical neural networks used in standard benchmarking environments and quantum control, using only a fraction of the parameters. Moreover, we study the barren plateau phenomenon in quantum policy gradients using the Fisher information matrix spectrum.",
        "DOI": "10.1007/s42484-023-00101-8",
        "affiliation_name": "International Iberian Nanotechnology Laboratory",
        "affiliation_city": "Braga",
        "affiliation_country": "Portugal"
    },
    {
        "paper_title": "Corporate vulnerability in the US and China during COVID-19: A machine learning approach",
        "paper_author": "Khan M.A.",
        "publication": "Journal of Economic Asymmetries",
        "citied_by": "7",
        "cover_date": "2023-06-01",
        "Abstract": "The impact of COVID-19 on stock market dynamics and other macroeconomic indicators has been extensively researched. However, the question of how it affects corporate vulnerability has received less attention. This article aims to fill this gap by examining the implications of COVID-19 on corporate vulnerability in the United States (US) and China, using daily data from January 2020 to December 2021. The empirical results of cointegration analysis demonstrate that COVID-19 considerably worsen corporate vulnerabilities in the long-term in the US and in the short-term in China. Additionally, non-linear results demonstrate long-run asymmetries in the US and short-run asymmetries in China, confirming the accuracy of error prediction and suggesting that US corporations are more exposed to COVID-19-induced risks. The channels through which COVID-19 may affect corporate vulnerability include changes in consumer behavior and demand, disruptions in supply chains, financial stress, government policies and regulations, and changes in the competitive landscape. This study sheds light on the effects of the COVID-19 pandemic on corporate vulnerability in the US and China, revealing regulatory implications that may necessitate greater government involvement, managerial implications that emphasize risk management and contingency planning, and social implications that highlight the importance of prioritizing stakeholder welfare and embracing digital transformation.",
        "DOI": "10.1016/j.jeca.2023.e00302",
        "affiliation_name": "UBD School of Business and Economics",
        "affiliation_city": "Bandar Seri Begawan",
        "affiliation_country": "Brunei Darussalam"
    },
    {
        "paper_title": "Forecasting vapor pressure deficit for agricultural water management using machine learning in semi-arid environments",
        "paper_author": "Elbeltagi A.",
        "publication": "Agricultural Water Management",
        "citied_by": "28",
        "cover_date": "2023-06-01",
        "Abstract": "Precise evapotranspiration (ET) estimation is critical for agricultural water management, particularly in water-stressed developing countries. Vapor Pressure Deficit is one of the ET parameters that has a significant impact on its calculation (VPD). This paper forecasts VPD using ensemble learning-based modeling in eight different regions (Dakahliyah, Gharbiyah, Kafr Elsheikh, Dumyat, Port Said, Ismailia, Sharqiyah, and Qalubiyah) in Egypt. In this study, six machine learning algorithms were used: Linear Regression (LR), Additive regression trees (ART), Random SubSpace (RSS), Random Forest (RF), Reduced Error Pruning Tree (REPTree), and Quinlan's M5 algorithm (M5P). Monthly vapor pressure data were obtained from the Japanese 55-year Reanalysis JRA-55 from 1958 to 2021. The dateset has been divided into two segments: the training stage (1958–2005) and the testing stage (2006–2021). Five statistical measures were used to evaluate the model performances: Correlation Coefficient (CC), Mean Absolute Error (MAE), Root Mean Square Error (RMSE), Relative absolute error (RAE), and Root Relative Squared Error (RRSE), across both training and testing stages. RF model outperformed the rest of the models [CC = 0.9694; MAE = 0.0967; RMSE = 0.1252; RAE (%) = 21.7297 and RRSE (%) = 24.0356], followed closely by REPTree and RSS models. On the other hand, M5P model performance remained moderate and both LR and AR model were the worst. During the testing stage, RF outperformed the rest of the models in terms of (which statistic), followed closely by REPTree and RSS models. On the other hand, M5P performance remained moderate and both LR and AR models were the worst. This study recommended using the RF model for future hydro-climatological studies in general, and vapor pressure deficit modeling and prediction in particular. This study enables future magnitudes to be predicted, alerting the authorities and administrators involved to focus their policy-making on more specific pathways toward climate adaptation.",
        "DOI": "10.1016/j.agwat.2023.108302",
        "affiliation_name": "Faculty of Engineering",
        "affiliation_city": "Minya",
        "affiliation_country": "Egypt"
    },
    {
        "paper_title": "Turbulence control in plane Couette flow using low-dimensional neural ODE-based models and deep reinforcement learning",
        "paper_author": "Linot A.J.",
        "publication": "International Journal of Heat and Fluid Flow",
        "citied_by": "14",
        "cover_date": "2023-06-01",
        "Abstract": "The high dimensionality and complex dynamics of turbulent flows remain an obstacle to the discovery and implementation of control strategies. Deep reinforcement learning (RL) is a promising avenue for overcoming these obstacles, but requires a training phase in which the RL agent iteratively interacts with the flow environment to learn a control policy, which can be prohibitively expensive when the environment involves slow experiments or large-scale simulations. We overcome this challenge using a framework we call “DManD-RL” (data-driven manifold dynamics-RL), which generates a data-driven low-dimensional model of our system that we use for RL training. With this approach, we seek to minimize drag in a direct numerical simulation (DNS) of a turbulent minimal flow unit of plane Couette flow at Re=400 using two slot jets on one wall. We obtain, from DNS data with O(105) degrees of freedom, a 25-dimensional DManD model of the dynamics by combining an autoencoder and neural ordinary differential equation. Using this model as the environment, we train an RL control agent, yielding a 440-fold speedup over training on the DNS, with equivalent control performance. The agent learns a policy that laminarizes 84% of unseen DNS test trajectories within 900 time units, significantly outperforming classical opposition control (58%), despite the actuation authority being much more restricted. The agent often achieves laminarization through a counterintuitive strategy that drives the formation of two low-speed streaks, with a spanwise wavelength that is too small to be self-sustaining. The agent demonstrates the same performance when we limit observations to wall shear rate.",
        "DOI": "10.1016/j.ijheatfluidflow.2023.109139",
        "affiliation_name": "UW-Madison College of Engineering",
        "affiliation_city": "Madison",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Land use and land cover dynamics: Implications for thermal stress and energy demands",
        "paper_author": "Adeyeri O.E.",
        "publication": "Renewable and Sustainable Energy Reviews",
        "citied_by": "10",
        "cover_date": "2023-06-01",
        "Abstract": "This study examined the interaction between land use and land cover (LULC) dynamics, trend and thermal stress distribution using the universal thermal comfort index (UTCI) and different LULC classifications under two Coupled Model Intercomparison Project Phase 6 (CMIP6) Shared Socioeconomic Pathways (i.e., SSP 370 and 585) climate and land use scenarios for the historical (1959–2014) and future period (2045–2100). The moderate to strong cold stress in the annual and winter climatology in the midlatitudes was replaced by no thermal stress in the summer, while the summertime ranged from moderate to strong heat stress. A negative correlation was observed between thermal stress and southern hemispheric primary forests. Perennial croplands had the most dynamic changes in intensity during the historical period. Primary and secondary forests had an active influence on global thermal stress. Areas in the tropics recording moderate heat stress coincided with secondary nonforest, pastureland, and annual cropland expansions. The conversion of forest to range land and croplands and the subsequent negative forest trends increased the severity of thermal stress. The future projection showed intense thermal stress; however, the SSP-585 signals were more potent. As a result, cooling demands will rise, and heating demands will decline, yet, improved thermal comfort necessitates a higher cooling capacity, especially in the summer. Thermal stress may make it difficult for many cooling systems to meet people's energy demands. These could be a driving factor in shaping better land use policies, improving energy demand preparedness, and elucidating the potentially severe impacts of thermal stress.",
        "DOI": "10.1016/j.rser.2023.113274",
        "affiliation_name": "Karlsruher Institut für Technologie",
        "affiliation_city": "Karlsruhe",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Sentiment analysis of Twitter data regarding the agnipath scheme of the defense forces",
        "paper_author": "Sajwan V.",
        "publication": "Indonesian Journal of Electrical Engineering and Computer Science",
        "citied_by": "1",
        "cover_date": "2023-06-01",
        "Abstract": "Due to the popularity of social media today, people frequently share such criticism on Facebook, Twitter, Instagram, and other platforms. Therefore needs to know how your input from users of social media is generated in order to ascertain the public reaction to the policy that has been enacted. However, because of the comments, it is challenging to tell how many people have responded positive or negative. The objective of sentiment analysis of tweets is to provide insight into people's attitudes and perceptions regarding an event. This study illustrates the role of Twitter in the announcement of a new army vacancy through the \"agnipath scheme\" dubbed \"agniveer\". The result of this study can be used by the defense forces and government for decision making or policies related to the agnipath scheme. The study studied 4,000 English-language Twitter posts from July 3, 2022 to July 9, 2022. Manual text analysis revealed seven basic groups of tweet sentiments. The tweets' positive, negative, and neutral emotions were shown using orange data mining software, a powerful machine learning, data mining, and data visualization toolset. Result shows that agnipath scheme is mostly accepted by the people.",
        "DOI": "10.11591/ijeecs.v30.i3.pp1643-1650",
        "affiliation_name": "IIMT University, Meerut",
        "affiliation_city": "Meerut",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Hourly forecasting of the photovoltaic electricity at any latitude using a network of artificial neural networks",
        "paper_author": "Matera N.",
        "publication": "Sustainable Energy Technologies and Assessments",
        "citied_by": "27",
        "cover_date": "2023-06-01",
        "Abstract": "Nowadays, special attention is paid to the importance of using photovoltaic (PV) systems to tackle the problem of climate change and the energy crisis. Artificial intelligence is currently used in different science fields for its great potential and accuracy in forecasting problems. In this work, a network of artificial neural networks (ANNs) was trained and validated to forecast the hourly worldwide electrical power produced by various PV modules, with different electrical characteristics. Each ANN describes the worldwide performance of each PV module on the optimal inclination angle. The training data consists of the hourly air temperature, horizontal total solar radiation as input data and electrical power produced as output. The power is obtained from the hourly simulation of PV modules with an electrical circuit model in 24 localities at very different latitudes. The validation and generalization of the network were obtained by considering the six PV modules in further 24 localities and by considering two further PV modules in all 48 localities considered. The excellent results in terms of accuracy metrics confirmed that the network of ANNs is a reliable, simple and accurate tool that can be used to predict the hourly performance of any PV module in any location worldwide.",
        "DOI": "10.1016/j.seta.2023.103197",
        "affiliation_name": "Università del Salento",
        "affiliation_city": "Lecce",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Renewing the resource-based view: New contexts, new concepts, and new methods",
        "paper_author": "Helfat C.E.",
        "publication": "Strategic Management Journal",
        "citied_by": "75",
        "cover_date": "2023-06-01",
        "Abstract": "The resource-based view is an enduring and impactful mainstay of research within strategic management and beyond. This editors' introduction to the special issue on “new directions for the resource-based view” accomplishes two main tasks. First, we describe the contributions offered by the seven articles contained in the special issue. Second, we explain the potential value to research of incorporating into resource-based inquiry new contexts (artificial intelligence and digitization, distributed organizations, and stakeholders and sustainability); new concepts (resource redeployment, market shaping through resources and capabilities); and new methods (text analysis and machine learning, formal models, policy capturing). The overall aim of this introduction is to help invigorate the resource-based view by spotlighting a series of promising new directions.",
        "DOI": "10.1002/smj.3500",
        "affiliation_name": "David Eccles School of Business",
        "affiliation_city": "Salt Lake City",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Impact of extreme weather in production economics: Extracting evidence from user-generated content",
        "paper_author": "Saura J.R.",
        "publication": "International Journal of Production Economics",
        "citied_by": "30",
        "cover_date": "2023-06-01",
        "Abstract": "The last decade has witnessed an increase in the number of extreme weather events globally. In addition, the economic output around the world is at all-time high in terms of production and profitability. However, global warming and extreme weather are modifying the natural ecosystem and the human social system, leading to the appearance of extreme climate events that have an adverse impact on the world economy. To address this challenge, the present study identifies the main impacts of extreme weather on production economics based on the analysis of user-generated content (UGC) on the social network Twitter. Methodologically, a sentiment analysis with machine learning is developed and applied to analyze a sample of 1.4 m tweets; in addition, computing experiments to calculate the accuracy with Support Vector Classifier, Multinomial Naïve Bayes, Logistic Regression, and Random Forest Classifier are conducted. Second, a topic modeling known as latent Dirichlet allocation is applied to divide sentiment-classified tweets into topics. To complement these approaches, we also use the technique of textual analysis. These approaches are used under the framework of computer-aided test analysis system and natural language processing. The results are discussed and linked to appraisal theory. A total of 7 topics are identified, including positive (Sustainable energies and Green Entrepreneurs), neutral (Climate economy, Producer's productivity and Stock market), and negative (Economy and policy and Climate emergence). Finally, the present study discusses how the recent trend of an increase in extreme weather conditions has significantly impacted international markets, leading companies to adapt their business models and production systems accordingly. The results show that the climate economy and policy, producers' productivity, and the stock market are all heavily influenced by extreme weather and can have significant effects on the global economy.",
        "DOI": "10.1016/j.ijpe.2023.108861",
        "affiliation_name": "ESIC University",
        "affiliation_city": "Madrid",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "Dynamic production scheduling towards self-organizing mass personalization: A multi-agent dueling deep reinforcement learning approach",
        "paper_author": "Qin Z.",
        "publication": "Journal of Manufacturing Systems",
        "citied_by": "34",
        "cover_date": "2023-06-01",
        "Abstract": "Mass personalization is rapidly approaching. In response, manufacturing systems should be capable of autonomously changing production plans, configurations and schedules under dynamic manufacturing environments for producing personalized products. Self-organizing manufacturing network is a promising paradigm for mass personalization. The backbone of a self-organizing manufacturing network is an adaptive production scheduling method to dynamically allocate and sequence manufacturing jobs under dynamic settings, such as stochastic processing time or unplanned machine breakdown. However, existing production scheduling methods (i.e., heuristic rules, meta-heuristic algorithms, and existing reinforcement learning models) fail to automatically optimize production schedules while maintaining stable manufacturing performance, under dynamic settings. In this paper, we designed a reinforcement learning-based static-training-dynamic-execution approach for dynamic job shop scheduling problems. The scheduling policies are learned from static scheduling instances by a multi-agent dueling deep reinforcement learning approach. Under this approach, we proposed new representations of observation, action, reward, and cooperation mechanisms between agents. The learned scheduling policies are then deployed to a dynamic scheduling system where stochastic processing time and unplanned machine breakdown randomly occur. Extensive simulation experiments demonstrated that our approach outperforms heuristic rules on makespan under two dynamic manufacturing settings.",
        "DOI": "10.1016/j.jmsy.2023.03.003",
        "affiliation_name": "The University of Auckland",
        "affiliation_city": "Auckland",
        "affiliation_country": "New Zealand"
    },
    {
        "paper_title": "Meta attention for Off-Policy Actor-Critic",
        "paper_author": "Huang J.",
        "publication": "Neural Networks",
        "citied_by": "6",
        "cover_date": "2023-06-01",
        "Abstract": "Off-Policy Actor-Critic methods can effectively exploit past experiences and thus they have achieved great success in various reinforcement learning tasks. In many image-based and multi-agent tasks, attention mechanism has been employed in Actor-Critic methods to improve their sampling efficiency. In this paper, we propose a meta attention method for state-based reinforcement learning tasks, which combines attention mechanism and meta-learning based on the Off-Policy Actor-Critic framework. Unlike previous attention-based work, our meta attention method introduces attention in the Actor and the Critic of the typical Actor-Critic framework, rather than in multiple pixels of an image or multiple information sources in specific image-based control tasks or multi-agent systems. In contrast to existing meta-learning methods, the proposed meta-attention approach is able to function in both the gradient-based training phase and the agent's decision-making process. The experimental results demonstrate the superiority of our meta-attention method in various continuous control tasks, which are based on the Off-Policy Actor-Critic methods including DDPG and TD3.",
        "DOI": "10.1016/j.neunet.2023.03.024",
        "affiliation_name": "National University of Defense Technology China",
        "affiliation_city": "Changsha",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A Deep Reinforcement Learning Based Real-Time Solution Policy for the Traveling Salesman Problem",
        "paper_author": "Ling Z.",
        "publication": "IEEE Transactions on Intelligent Transportation Systems",
        "citied_by": "10",
        "cover_date": "2023-06-01",
        "Abstract": "The rapid development of logistics and navigation has led to increasing demand for solving route optimization problems in real-time. The traveling salesman problem (TSP) tends to require fast and reliable online solutions, which may not be met by traditional iterative optimization algorithms. In this work, a real-time solution policy is proposed for TSP. The idea is to build a mapping between city information and optimal solutions using deep neural networks. Therefore, when given a new set of city coordinates, the optimal route can be directly and quickly calculated without iteration. Considering the recent advancement in computer vision with deep convolutional neural networks (DCNNs), an image representation is proposed to convert TSP to a computer vision problem. A problem decomposition method is introduced to reduce the mapping complexity. Taking advantage of the powerful fitting capabilities of DCNN, a deep reinforcement learning method is designed without any labeling requirement. The proposed method is superior for real-time applications compared with other algorithms.",
        "DOI": "10.1109/TITS.2023.3256563",
        "affiliation_name": "State Key Laboratory of Industrial Control Technology",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A Strategy for Traffic Safety of Vehicular Platoons Under Connection Loss and Time-Delay",
        "paper_author": "Godinho D.A.",
        "publication": "IEEE Transactions on Intelligent Transportation Systems",
        "citied_by": "9",
        "cover_date": "2023-06-01",
        "Abstract": "Autonomous platoons are good alternatives for cargo transportation, with many approaches to safety and efficiency issues. In the last decades, much effort has been employed in this context, with solutions ranging from low-level control laws to learning strategies. Although some papers have concentrated on methods robust to disturbances or resilient to disruptions, most of the current literature assumes that the platoon starts and remains connected all the time, even when subjecting vehicles to limited communication ranges and time-delay. Therefore, in this paper, we address the problem of connectivity maintenance and stability of platoons under the complete disconnection of vehicles. The main goal is to increase the tolerance to agent exits, external elements (such as traffic lights and human-driven vehicles), and non-ideal initial conditions, improving traffic safety in mixed traffic scenarios. By modeling the network connection as a Directed Acyclic Graph, we use a state-machine policy to recover connectivity and regulate the spacing distance to other vehicles under heterogeneous time-delays. We demonstrate that this state alteration can be interpreted as a reference tracking problem and propose a design procedure that provides stability and zero spacing error in steady-state. Our control protocol allows vehicles to reach a consensus with the team, even when they start disconnected from other ones. Results with agent-based and nonlinear simulations illustrate the effectiveness of our approach.",
        "DOI": "10.1109/TITS.2023.3258633",
        "affiliation_name": "Universidade Federal de Minas Gerais",
        "affiliation_city": "Belo Horizonte",
        "affiliation_country": "Brazil"
    },
    {
        "paper_title": "Assessment of solar radiation resource and photovoltaic power potential across China based on optimized interpretable machine learning model and GIS-based approaches",
        "paper_author": "Song Z.",
        "publication": "Applied Energy",
        "citied_by": "40",
        "cover_date": "2023-06-01",
        "Abstract": "In light of the rapidly expanding solar photovoltaic (PV) sector, it is important to provide a deeper understanding of solar energy resources to successfully implement solar energy projects. In this study, an interpretable machine learning model based on extreme gradient boosting (XGBoost) optimized by particle swarm optimization (PSO) algorithms was developed to estimate global solar radiation. The results show that the proposed PSO-XGBoost model possesses the most superior accuracy and stability, with the coefficient of determination (R2), root mean square error (RMSE), mean absolute error (MAE), and mean absolute percentage error (MAPE) of 0.953, 1.597 MJ·m−2·day−1, 1.138 MJ·m−2·day−1, and 10.500%, respectively. With the geographic information system (GIS) -based approaches, a 50 km by 50 km spatial resolution map of long-term national average solar radiation resources was generated based on the reconstructed solar radiation dataset, as well as the PV power potential map. The findings reveal that the nationwide annual mean solar radiation resources were decreasing at an estimated attenuation of −0.83 W·m−2·decade−1, with a downward trend of the greatest magnitude of −1.83 W·m−2·decade−1 for summer. China's long-term average yearly PV power potential reached 285.00 kWh·m−2, indicating a spatial pattern of higher potentials in the northwestern and northern provinces, while lower values in the southeastern provinces. Moreover, the PV power potential in China decreased by 1.69 kWh·m−2·decade−1 from 1961 to 2016, with an attenuation of above 5 kWh·m−2·decade−1 in heavily polluted regions. During the 2010s, 30 out of the 31 provinces experienced a reduction in the PV power potential between 0.25% and 10.27%, with an average national reduction of 2.88%, compared to the 1960s scenario. Also, policy recommendations for long-term PV project deployment were given regarding the regional mismatch between PV power potential and installed capacity in China.",
        "DOI": "10.1016/j.apenergy.2023.121005",
        "affiliation_name": "The Hong Kong Polytechnic University",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong"
    },
    {
        "paper_title": "Forecasting labor needs for digitalization: A bi-partite graph machine learning approach",
        "paper_author": "Percia David D.",
        "publication": "World Patent Information",
        "citied_by": "2",
        "cover_date": "2023-06-01",
        "Abstract": "We use a unique database of digital, and cybersecurity hires from Swiss organizations and develop a method based on a temporal bi-partite network, which combines local and global indices through a Support Vector Machine. We predict the appearance and disappearance of job openings from one to six months horizons. We show that global indices yield the highest predictive power, although the local network does contribute to long-term forecasts. At the one-month horizon, the “area under the curve” and the “average precision” are 0.984 and 0.905, respectively. At the six-month horizon, they reach 0.864 and 0.543, respectively. Our study highlights the link between the skilled workforce and the digital revolution and the policy implications regarding intellectual property and technology forecasting.",
        "DOI": "10.1016/j.wpi.2023.102193",
        "affiliation_name": "Faculté des HEC",
        "affiliation_city": "Lausanne",
        "affiliation_country": "Switzerland"
    },
    {
        "paper_title": "A practical Reinforcement Learning implementation approach for continuous process control",
        "paper_author": "Patel K.M.",
        "publication": "Computers and Chemical Engineering",
        "citied_by": "12",
        "cover_date": "2023-06-01",
        "Abstract": "Industrial process control using model-based technologies is well established. These technologies are typically non-adaptive and so have limitations. Reinforcement Learning (RL) provides a model-free adaptive alternative. RL is a type of machine learning (ML) where models or data sets of the environment are not necessary before learning can start. It generates data, by exploring the environment and then learn the behavior from it. Though RL has been successfully applied for learning and playing various games such as Go, Chess, Atari; its application to continuous process control problems is not trivial. There is a need for online RL implementation to be safe, fast learning and explainable when applied to industrial control problems. Rather than adding to the extensive research on augmenting existing RL algorithms, the paper presents a unique systematic method of formulating the RL problem incorporating domain-specific knowledge about process constraints and objectives, resulting in reduced dimensionality, along with modifications to the exploration process, applicable to any model free RL algorithm supporting continuous states and actions, to enhance safety, speed and explainability of online RL implementation without requiring a simulation model. The approach is successfully implemented on two multivariable processes: a simulated distillation column and a temperature control lab setup using the Deep Deterministic Policy Gradient (DDPG) algorithm. The work demonstrates that the presented method is applicable to multivariable, noisy, non-linear processes with disturbances. It will further the potential of introducing the advances in Artificial Intelligence and Machine Learning algorithms for intelligent process control capable of enabling autonomous operation in the process industry.",
        "DOI": "10.1016/j.compchemeng.2023.108232",
        "affiliation_name": "Saudi Arabian Oil Company",
        "affiliation_city": "Dhahran",
        "affiliation_country": "Saudi Arabia"
    },
    {
        "paper_title": "Deep Reinforcement Learning-Based Control of Stewart Platform With Parametric Simulation in ROS and Gazebo",
        "paper_author": "Yadavari H.",
        "publication": "Journal of Mechanisms and Robotics",
        "citied_by": "9",
        "cover_date": "2023-06-01",
        "Abstract": "The Stewart platform is an entirely parallel robot with mechanical differences from typical serial robotic manipulators, which has a wide application area ranging from flight and driving simulators to structural test platforms. This work concentrates on learning to control a complex model of the Stewart platform using stateof- the-art deep reinforcement learning (DRL) algorithms. In this regard, to enhance the reliability of the learning performance and to have a test bed capable of mimicking the behavior of the system completely, a precisely designed simulation environment is presented. Therefore, we first design a parametric representation for the kinematics of the Stewart platform in Gazebo and robot operating system (ROS) and integrate it with a Python class to conveniently generate the structures in simulation description format (SDF). Then, to control the system, we benefit from three DRL algorithms: the asynchronous advantage actor-critic (A3C), the deep deterministic policy gradient (DDPG), and the proximal policy optimization (PPO) to learn the control gains of a proportional integral derivative (PID) controller for a given reaching task. We chose to apply these algorithms due to the Stewart platform's continuous action and state spaces, making them well-suited for our problem, where exact controller tuning is a crucial task. The simulation results show that the DRL algorithms can successfully learn the controller gains, resulting in satisfactory control performance.",
        "DOI": "10.1115/1.4056971",
        "affiliation_name": "İstinye Üniversitesi",
        "affiliation_city": "Istanbul",
        "affiliation_country": "Türkiye"
    },
    {
        "paper_title": "Removal notice to “Machine learning in health condition check-up: An approach using Breiman's random forest algorithm” (Measurement: Sensors (2022) 23, (S266591742200040X), (10.1016/j.measen.2022.100406))",
        "paper_author": "Abd Algani Y.M.",
        "publication": "Measurement: Sensors",
        "citied_by": "0",
        "cover_date": "2023-06-01",
        "Abstract": "This article has been removed: please see Elsevier Policy on Article Withdrawal (https://www.elsevier.com/about/policies/article-withdrawal). This article has been removed at the request of the Authors, due to incomplete authorisation for the publication of the article from one of the author's institutions. The authors sincerely apologize for the inconvenience.",
        "DOI": "10.1016/j.measen.2023.100748",
        "affiliation_name": "College of Sakhnin for Teacher Education",
        "affiliation_city": "Sakhnin",
        "affiliation_country": "Israel"
    },
    {
        "paper_title": "Comparison of machine learning methods for automatic bucket filling: An imitation learning approach",
        "paper_author": "Eriksson D.",
        "publication": "Automation in Construction",
        "citied_by": "7",
        "cover_date": "2023-06-01",
        "Abstract": "This paper addresses the problem of generating controllers for automatic bucket filling using data collected during normal operations by professional operators on worksites. We introduce methods to annotate different phases of bucket fillings from long time series data and synthesize five machine learning models. The methods learn a policy that maps input signals to control commands using imitation learning from the training data. We use four different datasets as training data and first compare the model's accuracy on predicting operator commands. We then deploy and evaluate the efficacy and robustness of the policies on a real machine. The experiments shows that the Multilayer Perceptron and the Convolutional Neural Network models had the best overall performance and robustness and could achieve human level performance. Furthermore, models with high prediction accuracy are not necessarily suitable for feedback control, and using data from real worksites increased the controller's robustness.",
        "DOI": "10.1016/j.autcon.2023.104843",
        "affiliation_name": "Tampere University",
        "affiliation_city": "Tampere",
        "affiliation_country": "Finland"
    },
    {
        "paper_title": "ChatGPT: The balance of future, honesty, and integrity",
        "paper_author": "Nuryana Z.",
        "publication": "Asian Journal of Psychiatry",
        "citied_by": "4",
        "cover_date": "2023-06-01",
        "Abstract": "NA",
        "DOI": "10.1016/j.ajp.2023.103571",
        "affiliation_name": "Universitas Ahmad Dahlan",
        "affiliation_city": "Yogyakarta",
        "affiliation_country": "Indonesia"
    },
    {
        "paper_title": "Estimation of causality in economic growth and expansionary policies using uplift modeling",
        "paper_author": "Bermeo C.",
        "publication": "Neural Computing and Applications",
        "citied_by": "6",
        "cover_date": "2023-06-01",
        "Abstract": "Uplift modeling corresponds to an area of machine learning focused on capturing causal relationships on various observational and experimental data. Currently it has several applications, particularly in the marketing area, focused on customer segmentation and the establishment of advertising campaigns. This research proposes an novel economic uplift approach, using branching causal algorithms to estimate the individual treatment effect on real GDP growth and changes in expansionary economic policy on a quarterly basis from an OECD dataset. The developed framework reveals positive causal effects on economic growth driven by expansionary policies, generalized for all countries under study. In addition, lagged causal effects on these policies, exerted by the economic cycle, are captured. The results obtained not only show a performance similar to that of the literature, but also conform to the theoretical and actual macroeconomic behavior.",
        "DOI": "10.1007/s00521-023-08397-0",
        "affiliation_name": "Universidad Técnica Federico Santa María",
        "affiliation_city": "Valparaiso",
        "affiliation_country": "Chile"
    },
    {
        "paper_title": "Magnitude and efficiency of straw return in building up soil organic carbon: A global synthesis integrating the impacts of agricultural managements and environmental conditions",
        "paper_author": "Li B.",
        "publication": "Science of the Total Environment",
        "citied_by": "12",
        "cover_date": "2023-06-01",
        "Abstract": "Enhancing soil organic carbon (SOC) through straw return (SR) has been widely recommended as a promising practice of climate-smart agriculture. Many studies have investigated the relative effect of straw return on SOC content, while the magnitude and efficiency of straw return in building up SOC stock remain uncertain. Here, we present an integrative synthesis of the magnitude and efficiency of SR-induced SOC changes, using a database comprising 327 observations at 115 sites globally. Straw return increased SOC by 3.68 ± 0.69 (95 % Confidence Interval, CI) Mg C ha−1, with a corresponding C efficiency of 20.51 ± 9.58 % (95 % CI), of which <30 % was contributed directly by straw-C input. The magnitude of SR-induced SOC changes increased (P < 0.05) with increasing straw-C input and experiment duration. However, the C efficiency decreased significantly (P < 0.01) with these two explanatory factors. No-tillage and crop rotation were found to enhance the SR-induced SOC increase, in both magnitude and efficiency. Straw return sequestrated larger amount of C in acidic and organic-rich soils than in alkaline and organic-poor soils. A machine learning random forest (RF) algorithm showed that the amount of straw-C input was the most important single factor governing the magnitude and efficiency of straw return. However, local agricultural managements and environmental conditions were together the dominant explanatory factors determining the spatial differences in SR-induced SOC stock changes. This entails that by optimizing agricultural managements in regions with favorable environmental conditions the farmer can accumulate more C with minor negative impacts. By clarifying the significance and relative importance of multiple local factors, our findings may aid the development of tailored region-specific straw return policies integrating the SOC increment and its environmental side costs.",
        "DOI": "10.1016/j.scitotenv.2023.162670",
        "affiliation_name": "Weifang University of Science and Technology",
        "affiliation_city": "Shouguang",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Impact of R&amp;D and innovation in Chinese road transportation sustainability performance: A novel trigonometric envelopment analysis for ideal solutions (TEA-IS)",
        "paper_author": "Antunes J.",
        "publication": "Socio-Economic Planning Sciences",
        "citied_by": "16",
        "cover_date": "2023-06-01",
        "Abstract": "Previous road transportation sustainability studies not only neglected the epistemic uncertainty that surrounds the impact of innovation and Research and Development (R&D) expenditure on pollutant emissions performance but also failed in designing and simultaneously exploring the strengths of alternative MCDM (Multiple-Criteria Decision-Making) approaches to better discriminate performance scores. This paper focuses on these two gaps by presenting a road transportation sustainability performance assessment of 29 Chinese provinces for a 14-year period in light of relevant socioeconomic and demographic variables. First, a novel TEA-IS model is developed to assess road transportation sustainability performance. Besides possessing the beneficial features of each model, this hybrid DEA-TOPSIS can analyze the sustainability performance from the perspective of the synergistic effects among the criteria. From the socioeconomic and demographic perspectives, we use machine learning techniques for predicting high-low performance and synergistic Chinese provinces. Results suggest that the discriminatory power of TEA-IS is good and there is high synergy in Chinese provinces in terms of sustainable road transportation. We further find that there is a high level of heterogeneity in road transportation sustainability among different geographical locations in China. The results further suggest that higher levels of synergy are strongly associated with medium and high-performing provinces. Finally, we find that innovation index, foreign direct investment, and non-coastal cities have both higher performance and synergy levels. We recommend that Chinese government should further provide favorable policies to enhance innovation and efforts should also be made to provide better environment for attracting more foreign direct investment. Finally, instead of road transportation, other modes of transportation, such as air and sea, are recommended to be further developed and utilized.",
        "DOI": "10.1016/j.seps.2023.101544",
        "affiliation_name": "University of Bradford School of Management",
        "affiliation_city": "Bradford",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Statistical arbitrage trading across electricity markets using advantage actor–critic methods",
        "paper_author": "Demir S.",
        "publication": "Sustainable Energy, Grids and Networks",
        "citied_by": "4",
        "cover_date": "2023-06-01",
        "Abstract": "In this paper, risk-constrained arbitrage trading strategies that exploit price differences arising across short-term electricity markets, namely day-ahead (DAM), continuous intraday (CID) and balancing (BAL) markets, are developed and evaluated. To open initial DAM positions, a rule-based trading policy using DAM and CID price forecasts is proposed. DAM prices are predicted using both technical indicator features and data augmentation methods, such as autoencoders and generative adversarial networks. Meanwhile, CID prices are predicted using novel features that are engineered from the limit order book. Using the forecasts, the direction of price movements is correctly predicted the majority of the time. To manage open DAM positions while optimising the risk-reward ratio, deep reinforcement learning agents trained using the advantage actor–critic algorithm (A2C) are employed. Evaluated across Dutch short-term markets, A2C yields profits surpassing those obtained using A3C and other benchmarks. We expect our study to benefit electricity traders and researchers who seek to develop state-of-art intelligent trading strategies.",
        "DOI": "10.1016/j.segan.2023.101023",
        "affiliation_name": "Technische Universiteit Eindhoven",
        "affiliation_city": "Eindhoven",
        "affiliation_country": "Netherlands"
    },
    {
        "paper_title": "Impact of 3D modeling behavior patterns on the creativity of sustainable building design through process mining",
        "paper_author": "Gao W.",
        "publication": "Automation in Construction",
        "citied_by": "10",
        "cover_date": "2023-06-01",
        "Abstract": "Study area of process mining exhibits great potential in explaining the design performance and behavior pattern in architecture, engineering and construction industries based on event data. However, research into creativity-related behavior pattern in sustainable building design through process mining remains blank. With the global climate change and energy crisis, creativity-driven problem-solving design process is becoming crucial. Based on >41-million lines of event data from 115 participants from a green building design competition, this study explored four dimensions on creative design behavioral pattern: 1) frequently used commands; 2) frequently used objects; 3) duration of used commands and 4) experience with the tool. Secondly, this study establishes and trains artificial neural network models according to the four dimensions to evaluate level of creativity of design works. Training results prove the effectiveness of the discoveries and realize a preliminary screening on creativity for large amount of data in sustainable building design.",
        "DOI": "10.1016/j.autcon.2023.104804",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Indian citizens sentiment classification on Citizenship Amendment Act 2019",
        "paper_author": "Kaur P.",
        "publication": "OPSEARCH",
        "citied_by": "0",
        "cover_date": "2023-06-01",
        "Abstract": "Social media platforms (SMPs) have become a popular avenue worldwide for the general public to socialize. They exchange their views or experiences on various SMPs about any product or policies via social media posts. Considering that the SMPs being a rich source of data for understanding public opinion on a specific topic, this paper aims to present a classification model that can perform sentiment analysis using Twitter’s real-time data in the case of Citizenship Amendment Act (CAA). Public opinion on CAA is captured using Twitter API which is restricted to India. These tweets are processed using basic preprocessing techniques. Further, preprocessed data fed to various machine learning (ML) classification models for identifying the sentiment of the tweets. Each tweet is classified as positive, negative, and neutral based on its sentiment. The obtained results and analysis empowered us to understand Indian public opinion towards CAA over the period. A comparative analysis of various ML techniques is performed and Stochastic Gradient Descent has outperformed other ML techniques in terms of accuracy.",
        "DOI": "10.1007/s12597-023-00626-3",
        "affiliation_name": "GLA University, Mathura",
        "affiliation_city": "Mathura",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Smart curbs: Measuring street activities in real-time using computer vision",
        "paper_author": "Miranda A.S.",
        "publication": "Landscape and Urban Planning",
        "citied_by": "16",
        "cover_date": "2023-06-01",
        "Abstract": "Streets are conduits of human activity. Despite their importance, studying street activity has been obscured by a lack of data on how people use them, with most approaches limited to studying a single point in time or small geographic areas. This paper proposes a new framework to measure street activity in real-time. Our framework leverages machine learning and computer vision to classify pedestrian activities and transportation modes using images collected from moving vehicles. We apply our methodology to measure street activity in Paris for five weeks. We produce activity maps for this period showing that streets vary dramatically in their capacity to support pedestrian activity and that these differences are highly persistent. Our proposed framework can be used to measure street activities in other contexts and cities, providing urban researchers with an approach to guide planning interventions, identify infrastructural deficiencies, and inform design policies that foster active streets.",
        "DOI": "10.1016/j.landurbplan.2023.104715",
        "affiliation_name": "Massachusetts Institute of Technology",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Detecting network attacks model based on a convolutional neural network",
        "paper_author": "Ali T.A.J.",
        "publication": "International Journal of Electrical and Computer Engineering",
        "citied_by": "7",
        "cover_date": "2023-06-01",
        "Abstract": "Due to the increasing use of networks at present, Internet systems have raised many security problems, and statistics indicate that the rate of attacks or intrusions has increased excessively annually, and in the event of any malicious attack on network vulnerabilities or information systems, it may lead to serious disasters, violating policies on network security, i.e., “confidentiality, integrity, and availability” (CIA). Therefore, many detection systems, such as the intrusion detection system, appeared. In this paper, we built a system that detects network attacks using the latest machine learning algorithms and a convolutional neural network based on a dataset of the CSE-CIC-IDS2018. It is a recent dataset that contains a set of common and recent attacks. The detection rate is 99.7%, distinguishing between aggressive attacks and natural assertiveness.",
        "DOI": "10.11591/ijece.v13i3.pp3072-3078",
        "affiliation_name": "University of Mosul",
        "affiliation_city": "Mosul",
        "affiliation_country": "Iraq"
    },
    {
        "paper_title": "Traffic Crash Severity: Comparing the Predictive Performance of Popular Statistical and Machine Learning Models Using the Glasgow Coma Scale",
        "paper_author": "Nazir M.",
        "publication": "Journal of The Institution of Engineers (India): Series A",
        "citied_by": "4",
        "cover_date": "2023-06-01",
        "Abstract": "Crash severity analysis and prediction is a promising field in traffic safety. Various statistical methods have been used to model the severity of road crashes. However, machine learning algorithms have gained popularity in recent years. This study compares the predictive performance of various machine learning and statistical models, including prediction accuracy, and determines the influence of various variables on crash severity. The crash severity data were collected from a Hospital in Kashmir (India), an area with mixed topography. The crash severity levels (CSLs) were represented in the Glasgow Coma Scale (GCS). For estimations, the two statistical models, logistic regression (LR) and decision tree (DT), and four machine learning models, including random forest (RF), support vector machine (SVM), gradient boosted tree (GBT), and extreme gradient boosting (XG BOOST), have been used. The results show that the machine learning models have higher prediction accuracy than the statistical models. Among all, the GBT model has the best overall prediction accuracy, particularly in the prediction of individual CSLs while LR was found to have the least accuracy. The influence of variables on CSL was found from DT and GBT. Both models have indicated that ‘time’ as a variable was the most influencing, followed by the casualty class of pedestrians over the CSLs. The results also show that the variable influences over CSL were different from different models. Based on the influence of variables, certain policy implications are suggested, which might aid the transportation department, and other concerned departments to reduce the severity and number of road traffic crashes (RTCs).",
        "DOI": "10.1007/s40030-023-00710-3",
        "affiliation_name": "National Institute of Technology Srinagar",
        "affiliation_city": "Srinagar",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Multi-scenario simulation of land use and land cover based on shared socioeconomic pathways: The case of coastal special economic zones in China",
        "paper_author": "Yang D.",
        "publication": "Journal of Environmental Management",
        "citied_by": "20",
        "cover_date": "2023-06-01",
        "Abstract": "Urban land-use change simulations without considering the sustainable planning policies, especially in special economic park highly concerned by planners, might lack the reliability and availability. Thus, this study proposes a novel planning support systems integrating the Cellular Automata Markov chain model and Shared Socioeconomic Pathways (CA-Markov-SSPs) for predicting the changing of land use and land cover (LULC) at the local and system level by using a novel machine learning-driven, multi-source spatial data modelling framework. Using multi-source satellite data of coastal special economic zones from 2000 to 2020 as a sample, calibration validation based on the kappa indicates a highest average reliability above 0.96 from 2015 to 2020, and the cultivated land and built-up land classes of LULC is the most significant changes in 2030 by using the transition matrix of probabilities, the other classes except water bodies continue to increase. And the non-sustainable development scenario can be prevented by the multiple level collaboration of socio-economic factors. This research intended to help decision makers to confine irrational urban expansion and achieve the sustainable development.",
        "DOI": "10.1016/j.jenvman.2023.117536",
        "affiliation_name": "Dalian Maritime University",
        "affiliation_city": "Dalian",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Solar Energy Forecasting Using Machine Learning and Deep Learning Techniques",
        "paper_author": "Rajasundrapandiyanleebanon T.",
        "publication": "Archives of Computational Methods in Engineering",
        "citied_by": "16",
        "cover_date": "2023-06-01",
        "Abstract": "Renewable energy sources are present copiously in the nature and are good for environmental conservation as they restore themselves and thus have considerable potential in the near future. It is hence important to concentrate on the forecast of these energy sources in order to make effective use of them as soon as possible. This paper is focused primarily on solar energy. There are many approaches that could be applied for the prediction of global solar radiation (GSR). In the field of artificial intelligence (AI), the forecasting of solar resources has moved from conventional mathematical approaches to the use of intelligent techniques. The extent to which data based decisions are made for planning such as judicious and functional for the solar energy sector has been increased to a large extent by this giant step. In modelling challenging and unpredictable connections in between a set of input data and output data along with specific patterns that occur between datasets, AI techniques have demonstrated increasing reliability. In this regard, purpose of this paper is to provide a synopsis of solar energy forecasting methods utilizing machine learning and deep learning approaches to the best of our understanding.",
        "DOI": "10.1007/s11831-023-09893-1",
        "affiliation_name": "Karunya Institute of Technology and Sciences",
        "affiliation_city": "Coimbatore",
        "affiliation_country": "India"
    },
    {
        "paper_title": "An expert system for redesigning software for cloud applications",
        "paper_author": "Yedida R.",
        "publication": "Expert Systems with Applications",
        "citied_by": "4",
        "cover_date": "2023-06-01",
        "Abstract": "Cloud-based software has many advantages. When services are divided into many independent components, they are easier to update. Also, during peak demand, it is easier to scale cloud services (just hire more CPUs). Hence, many organizations are partitioning their monolithic enterprise applications into cloud-based microservices. Recently there has been much work using machine learning to simplify this partitioning task. Despite much research, no single partitioning method can be recommended as generally useful. More specifically, those prior solutions are “brittle”; i.e. if they work well for one kind of goal in one dataset, then they can be sub-optimal if applied to many datasets and multiple goals. This work extends prior work and proposes DEEPLY to fix the brittleness problem. Specifically, we use (a) hyper-parameter optimization to sample from the Pareto frontier of configurations (b) a weighted loss to choose optimally from this Pareto frontier (c) the 1cycle learning rate policy to avoid local minima with Adam and (d) spectral clustering over k-means. Our work shows that DEEPLY outperforms other algorithms in this space across different metrics. Moreover, our ablation study reveals that of the changes, the weighted loss is the most important, followed by hyper-parameter optimization (contrary to prior belief). To enable the reuse of this research, DEEPLY is available on-line at.",
        "DOI": "10.1016/j.eswa.2023.119673",
        "affiliation_name": "NC State College of Engineering",
        "affiliation_city": "Raleigh",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "A Machine Learning Approach for an HPC Use Case: the Jobs Queuing Time Prediction",
        "paper_author": "Vercellino C.",
        "publication": "Future Generation Computer Systems",
        "citied_by": "11",
        "cover_date": "2023-06-01",
        "Abstract": "High-Performance Computing (HPC) domain provided the necessary tools to support the scientific and industrial advancements we all have seen during the last decades. HPC is a broad domain targeting to provide both software and hardware solutions as well as envisioning methodologies that allow achieving goals of interest, such as system performance and energy efficiency. In this context, supercomputers have been the vehicle for developing and testing the most advanced technologies since their first appearance. Unlike cloud computing resources that are provided to the end-users in an on-demand fashion in the form of virtualized resources (i.e., virtual machines and containers), supercomputers’ resources are generally served through State-of-the-Art batch schedulers (e.g., SLURM, PBS, LSF, HTCondor). As such, the users submit their computational jobs to the system, which manages their execution with the support of queues. In this regard, predicting the behaviour of the jobs in the batch scheduler queues becomes worth it. Indeed, there are many cases where a deeper knowledge of the time experienced by a job in a queue (e.g., the submission of check-pointed jobs or the submission of jobs with execution dependencies) allows exploring more effective workflow orchestration policies. In this work, we focused on applying machine learning (ML) techniques to learn from the historical data collected from the queuing system of real supercomputers, aiming at predicting the time spent on a queue by a given job. Specifically, we applied both unsupervised learning (UL) and supervised learning (SL) techniques to define the most effective features for the prediction task and the actual prediction of the queue waiting time. For this purpose, two approaches have been explored: on one side, the prediction of ranges on jobs’ queuing times (classification approach) and, on the other side, the prediction of the waiting time at the minutes level (regression approach). Experimental results highlight the strong relationship between the SL models’ performances and the way the dataset is split. At the end of the prediction step, we present the uncertainty quantification approach, i.e., a tool to associate the predictions with reliability metrics, based on variance estimation.",
        "DOI": "10.1016/j.future.2023.01.020",
        "affiliation_name": "Università degli Studi di Torino",
        "affiliation_city": "Turin",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Smart cataract detection system with bidirectional LSTM",
        "paper_author": "Kalyani B.J.D.",
        "publication": "Soft Computing",
        "citied_by": "4",
        "cover_date": "2023-06-01",
        "Abstract": "Traditional IT with advent of convolution neural networks and deep learning can lead a trendsetter for healthcare sector, diseases identification and prediction. An essential upsurge during pandemic is virtualization of hospital functional policies and care models. Virtual models are providing solutions for disease detection without consulting a doctor in turn help to enhance patient care and performance. These architectural and operational policies are significant for healthcare domain to enable strategic decision-making for intricate and sensitive environment. The machine learning models will reveal information from the historical, optimize the present and even predict the future performance of the different areas analysed. This paper reveals the challenges of healthcare with automated Cataract Detection and Grading System. The proposed system uses an efficient deep learning model with CNN and LSTM for detecting and classifying healthy eye from cataract eye. The proposed system produced an accuracy of 98.5 for the custom dataset.",
        "DOI": "10.1007/s00500-023-07879-6",
        "affiliation_name": "Erode Sengunthar Engineering College",
        "affiliation_city": "Erode",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Let the data speak about the cut-off values for multidimensional index: Classification of human development index with machine learning",
        "paper_author": "Wang H.",
        "publication": "Socio-Economic Planning Sciences",
        "citied_by": "11",
        "cover_date": "2023-06-01",
        "Abstract": "The Human Development Index (HDI) classification is essential as it relates to international aid policies and business strategies. Although the existing literature has criticized the arbitrariness of cut-off values of the HDI, few proposed an ideal approach to overcome this drawback. This paper first employs the unsupervised machine learning techniques, the K-means clustering and Partitioning Around Medoids algorithms, to cluster the HDI and offers more reasonable cut-off values for classifying countries in combination with the current HDI calculation method. The results indicate that we can group the countries worldwide into three clusters, given the 2018 HDI dataset. We suggest cut-off values of 0.65 and 0.85 to classify low, medium, and high human development countries. This paper provides a new perspective to classifying the HDI based on the similarity of countries’ development but not subjective judgments.",
        "DOI": "10.1016/j.seps.2023.101523",
        "affiliation_name": "Southwest University",
        "affiliation_city": "Chongqing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Multi-Input Autonomous Driving Based on Deep Reinforcement Learning With Double Bias Experience Replay",
        "paper_author": "Cui J.",
        "publication": "IEEE Sensors Journal",
        "citied_by": "15",
        "cover_date": "2023-06-01",
        "Abstract": "It is still a challenge to realize safe and fast autonomous driving through deep reinforcement learning (DRL). Most autonomous driving reinforcement learning models are subject to a single experience replay approach for training agents and how to improve the driving speed and safety of agents has become the focus of research. Therefore, we present an improved double-bias experience replay (DBER) approach, which enables the agent to choose its own driving learning tendency. A new loss function is proposed to ameliorate the relationship between negative loss and positive loss. The proposed approach has been applied to three algorithms to verify: deep Q network (DQN), dueling double DQN (DD-DQN), and quantile regression DQN (QR-DQN). Compared with the existing approaches, the proposed approach show better performance and robustness of driving policy on the driving simulator, which is implemented by the unity machine learning (ML) agents. The approach makes the vehicle agent obtain better performance, such as higher reward, faster driving speed, less lane changing, and more in the same training time.",
        "DOI": "10.1109/JSEN.2023.3237206",
        "affiliation_name": "Beijing University of Chemical Technology",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A Novel Active Disturbance Rejection Control of PMSM Based on Deep Reinforcement Learning for More Electric Aircraft",
        "paper_author": "Wang Y.",
        "publication": "IEEE Transactions on Energy Conversion",
        "citied_by": "14",
        "cover_date": "2023-06-01",
        "Abstract": "In this article, a novel active disturbance rejection control (ADRC) based on deep reinforcement learning (DRL) is proposed to improve the performance of permanent magnet synchronous motor (PMSM) for more electric aircraft (MEA). MEA motors have the requirements of safety and stability so that a new ADRC method is put forward based on the limitation of nonlinear error attenuation function of traditional ADRC. The flux weakening control model of PMSM is firstly established. Then the ADRC model is built and applied to the speed loop of the control system. In order to reduce the number of control parameters and the jitter of the control law, deep neural network is employed to replace the traditional control law. Markov decision process is integrated into the novel ADRC to establish DRL model. A method based on twin delayed deep deterministic (TD3) policy gradient algorithm is proposed to train the neural network and optimize the DRL model. Model predictive control and traditional ADRC are used as comparison algorithms. Simulation and experiments show the effectiveness and superiority of the proposed method.",
        "DOI": "10.1109/TEC.2023.3235927",
        "affiliation_name": "Southeast University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Synthetic Learner: Model-free inference on treatments over time",
        "paper_author": "Viviano D.",
        "publication": "Journal of Econometrics",
        "citied_by": "2",
        "cover_date": "2023-06-01",
        "Abstract": "Understanding the effect of a particular treatment or a policy pertains to many areas of interest, ranging from political economics, marketing to healthcare. In this paper, we develop a non-parametric algorithm for detecting the effects of treatment over time in the context of Synthetic Controls. The method builds on counterfactual predictions from many algorithms without necessarily assuming that the algorithms correctly capture the model. We introduce an inferential procedure to detect treatment effects and show that the testing procedure controls size asymptotically for stationary, beta mixing processes without imposing any restriction on the set of base algorithms under consideration. We discuss consistency guarantees for average treatment effect estimates and derive regret bounds for the proposed methodology. The class of algorithms may include Random Forest, Lasso, or any other machine-learning estimator. Numerical studies and an application illustrate the advantages of the method.",
        "DOI": "10.1016/j.jeconom.2022.07.006",
        "affiliation_name": "Department of Mathematics",
        "affiliation_city": "La Jolla",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Machine learning for optimal test admission in the presence of resource constraints",
        "paper_author": "Elitzur R.",
        "publication": "Health Care Management Science",
        "citied_by": "5",
        "cover_date": "2023-06-01",
        "Abstract": "Developing rapid tools for early detection of viral infection is crucial for pandemic containment. This is particularly crucial when testing resources are constrained and/or there are significant delays until the test results are available – as was quite common in the early days of Covid-19 pandemic. We show how predictive analytics methods using machine learning algorithms can be combined with optimal pre-test screening mechanisms, greatly increasing test efficiency (i.e., rate of true positives identified per test), as well as to allow doctors to initiate treatment before the test results are available. Our optimal test admission policies account for imperfect accuracy of both the medical test and the model prediction mechanism. We derive the accuracy required for the optimized admission policies to be effective. We also show how our policies can be extended to re-testing high-risk patients, as well as combined with pool testing approaches. We illustrate our techniques by applying them to a large data reported by the Israeli Ministry of Health for RT-PCR tests from March to September 2020. Our results demonstrate that in the context of the Covid-19 pandemic a pre-test probability screening tool with conventional RT-PCR testing could have potentially increased efficiency by several times, compared to random admission control.",
        "DOI": "10.1007/s10729-022-09624-1",
        "affiliation_name": "Rotman School of Management",
        "affiliation_city": "Toronto",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Simple random forest classification algorithms for predicting occurrences and sizes of wildfires",
        "paper_author": "Makowski D.",
        "publication": "Extremes",
        "citied_by": "6",
        "cover_date": "2023-06-01",
        "Abstract": "In order to formulate effective fire-mitigation policies, it is important to understand the spatial and temporal distribution of different types of wildfires and to be able to predict their occurrence taking the main influencing factors into account. The objective of this short communication is to assess the capability of a fast and easy-to-implement random forest algorithm to estimate cumulative probabilities fire frequency and burned area using a large dataset collected in the USA. The input variables of the algorithm are voluntary restricted to climate and land use factors, which are easy to obtain in practice. No input related to fire frequency, burned area, or to any other fire characteristic is used. After model selection and training, the performance of random forest is assessed using an independent dataset including 80,000 observations of fire occurrence and burned area. Results show that the score of our simple random forest algorithm is 9% higher than the score of the winner of the data challenge of Opitz (Extreme, 2022) revealing that, although this model has a good performance, it is not the best. However, the approach proposed here can be implemented using standard packages, does not require any fire monitoring system after training, and requires little specialized knowledge in machine learning, which makes it usable by a large diversity of stakeholders. The results of this study suggest that random forest should be part of the toolbox of engineers and scientists involved in wildfire prediction.",
        "DOI": "10.1007/s10687-022-00458-2",
        "affiliation_name": "Université Paris-Saclay",
        "affiliation_city": "Gif-sur-Yvette",
        "affiliation_country": "France"
    },
    {
        "paper_title": "Economic policy statements, social media, and stock market uncertainty: An analysis of Donald Trump’s tweets",
        "paper_author": "Ortiz D.P.",
        "publication": "Journal of Economics and Finance",
        "citied_by": "2",
        "cover_date": "2023-06-01",
        "Abstract": "This paper investigates the impact of economic policy communication via social media on stock market uncertainty. It uses a sample of Donald Trump’s tweets to identify and cluster policy-related tweets using a double machine learning approach based on natural language processing. The response of uncertainty to these tweets is then estimated using an event-study design. Tweets about foreign policy and trade, monetary policy, and immigration policy significantly increase market uncertainty as measured by the VIX. Independent of their content, also the frequency of tweets and the intensity of tweet sharing matter for stock market uncertainty. Most of the effects are transitory, reaching their peaks around two hours after the publication of tweets.",
        "DOI": "10.1007/s12197-022-09608-5",
        "affiliation_name": "Friedrich-Alexander-Universität Erlangen-Nürnberg",
        "affiliation_city": "Erlangen",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Machine Learning-Based Non-Intrusive Digital Forensic Service for Smart Homes",
        "paper_author": "Liu X.",
        "publication": "IEEE Transactions on Network and Service Management",
        "citied_by": "4",
        "cover_date": "2023-06-01",
        "Abstract": "Security and privacy concerns keep growing with the successful development of Internet of Things (IoT) and the booming deployment of smart homes. IoT devices are utilized cooperatively to enable the interactions between home surroundings and users' daily lives, containing forensically-valuable information about what happens in smart homes, which can help introduce digital forensics into smart homes to alleviate the growing concerns. However, current IoT devices, apps, and platforms usually do not provide built-in capabilities for digital forensics. To overcome this limitation, we propose a non-intrusive solution (i.e., bringing no modification to IoT devices, apps, and platforms) of digital forensic service to provide Forensics-as-a-Service (FaaS) for smart homes. First, it leverages side-channel analysis on sniffed network traffic to monitor commands, actions, and states of IoT devices. Then, it introduces provenance graphs (i.e., causal graphs) for smart home modeling to provide a holistic and overall explanation of smart homes. Machine learning (ML) techniques are applied to overcome the deficiency of a non-intrusive solution as it suffers from challenges in data collection and smart home modeling. Finally, it conducts forensic analysis based on scalable, reusable policies that are designed for graph-based smart home modeling. We implement a prototype of our forensic service and evaluate it in a real-world smart home. The evaluation results show that our forensic service can effectively collect forensic data for smart home modeling and conduct forensic analysis to explain security risks in smart homes.",
        "DOI": "10.1109/TNSM.2022.3224863",
        "affiliation_name": "Mohamed Bin Zayed University of Artificial Intelligence",
        "affiliation_city": "Abu Dhabi",
        "affiliation_country": "United Arab Emirates"
    },
    {
        "paper_title": "Effective machine learning, Meta-heuristic algorithms and multi-criteria decision making to minimizing human resource turnover",
        "paper_author": "Pourkhodabakhsh N.",
        "publication": "Applied Intelligence",
        "citied_by": "20",
        "cover_date": "2023-06-01",
        "Abstract": "Employee turnover is one of the most important issues in human resource management, which is a combination of soft and hard skills. This makes it difficult for managers to make decisions. In order to make better decisions, this article has been devoted to identifying factors affecting employee turnover using feature selection approaches such as Recursive Feature Elimination algorithm and Mutual Information and Meta-heuristic algorithms such as Gray Wolf Optimizer and Genetic Algorithm. The use of Multi-Criteria Decision-Making techniques is one of the other approaches used to identify the factors affecting the employee turnover in this article. Our expert has used the Best-Worst Method to evaluate each of these variables. In order to check the performance of each of the above methods and to identify the most significant factors on employee turnover, the results are used in some machine learning algorithms to check their accuracy in predicting the employee turnover. These three methods have been implemented on the human resources dataset of a company and the results show that the factors identified by the Mutual Information algorithm can show better results in predicting the employee turnover. Also, the results confirm that managers need a support tool to make decisions because the possibility of making mistakes in their decisions is high. This approach can be used as a decision support tool by managers and help managers and organizations to have a correct insight into the departure of their employees and adopt policies to retain and optimize their employees.",
        "DOI": "10.1007/s10489-022-04294-6",
        "affiliation_name": "Islamic Azad University",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "Blockchain acceptance rate prediction in the resilient supply chain with hybrid system dynamics and machine learning approach",
        "paper_author": "Roozkhosh P.",
        "publication": "Operations Management Research",
        "citied_by": "27",
        "cover_date": "2023-06-01",
        "Abstract": "In today’s era, the importance and implementation of blockchain networks have become feasible as it improves the resilience of the supply chain network at all levels by clarifying information and creating security in the network, improving the speed of response, and gaining the trust of customers. This paper aims to investigate the behavior of the blockchain acceptance rate (BAR) in the home appliances flexible supply chain in Iran using. system dynamics (SD), which is used to better define the relationships between the variables of the model that are non-linearly connected. Through simulating the behavior of the BAR in the long term in the supply chain, whilst conducting sensitivity analysis, policy design, and validation, this model will be implemented for the years 2020 to 2030. Additionally, post-simulation, blockchain acceptance behavior will be assessed by having simulated data considered as input for studied Multi-Layer Perceptron (MLP) and Vector Regression (SVR) (data that have the highest correlation with BAR). The acceptance rate behavior is predicted with the help of machine learning methods to have the best behavior and prediction for the data of 2020-2022 since the prediction function is compared to daily real data obtained these years. The results show that in 2030, the BAR will be around 0.6 if the COVID-19 outbreak impact is medium, and if the considered policy designs are implemented, this rate will reach a maximum of 0.8. So paying attention to the creation and design of policies can achieve positive implications for increasing the resilience of the supply chain in the long run. Findings suggest that the SD-MLP method is better than the SD-SVR method as it has less error and can predict the better behavior of the BAR.",
        "DOI": "10.1007/s12063-022-00336-x",
        "affiliation_name": "University of Technology Sydney",
        "affiliation_city": "Sydney",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Machine translation in higher education: Perceptions, policy, and pedagogy",
        "paper_author": "Paterson K.",
        "publication": "TESOL Journal",
        "citied_by": "16",
        "cover_date": "2023-06-01",
        "Abstract": "Multiple studies have shown that language learners and other students undertaking postsecondary studies in an additional language (L2) consult digital translation tools to complete course-related work despite general disapproval of their use by instructors. Significant improvements in the accuracy of machine translation (MT) along with their widespread use among students present ethical and pedagogical implications that have yet to be coherently addressed by instructors and institutions at the tertiary level. Recognizing MT as inextricable from L2 users' academic realities, this article reviews the current research on perceptions and purposes of its use in higher education institutions, discusses MT at the policy level (e.g., gaps in legislation related to academic integrity and, more broadly, inconsistencies between the aims of internationalization and the continued delegitimization of marginalized varieties of English), outlines various ways that MT can be harnessed to support learning (e.g., for vocabulary acquisition, writing, metalinguistic awareness, learner autonomy), and suggests ways forward in education, research, and theory on the intersection of MT and learning.",
        "DOI": "10.1002/tesj.690",
        "affiliation_name": "Western University",
        "affiliation_city": "London",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "From deterministic to stochastic: an interpretable stochastic model-free reinforcement learning framework for portfolio optimization",
        "paper_author": "Song Z.",
        "publication": "Applied Intelligence",
        "citied_by": "6",
        "cover_date": "2023-06-01",
        "Abstract": "As a fundamental problem in algorithmic trading, portfolio optimization aims to maximize the cumulative return by continuously investing in various financial derivatives within a given time period. Recent years have witnessed the transformation from traditional machine learning trading algorithms to reinforcement learning algorithms due to their superior nature of sequential decision making. However, the exponential growth of the imperfect and noisy financial data that is supposedly leveraged by the deterministic strategy in reinforcement learning, makes it increasingly challenging for one to continuously obtain a profitable portfolio. Thus, in this work, we first reconstruct several deterministic and stochastic reinforcement algorithms as benchmarks. On this basis, we introduce a risk-aware reward function to balance the risk and return. Importantly, we propose a novel interpretable stochastic reinforcement learning framework which tailors a stochastic policy parameterized by Gaussian Mixtures and a distributional critic realized by quantiles for the problem of portfolio optimization. In our experiment, the proposed algorithm demonstrates its superior performance on U.S. market stocks with a 63.1% annual rate of return while at the same time reducing the market value max drawdown by 10% when back-testing during the stock market crash around March 2020.",
        "DOI": "10.1007/s10489-022-04217-5",
        "affiliation_name": "Xi'an Jiaotong-Liverpool University",
        "affiliation_city": "Suzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Ascent Similarity Caching With Approximate Indexes",
        "paper_author": "Si Salem T.",
        "publication": "IEEE/ACM Transactions on Networking",
        "citied_by": "2",
        "cover_date": "2023-06-01",
        "Abstract": "Similarity search is a key operation in multimedia retrieval systems and recommender systems, and it will play an important role also for future machine learning and augmented reality applications. When these systems need to serve large objects with tight delay constraints, edge servers close to the end-user can operate as similarity caches to speed up the retrieval. In this paper we present AÇAI, a new similarity caching policy which improves on the state of the art by using (i) an (approximate) index for the whole catalog to decide which objects to serve locally and which to retrieve from the remote server, and (ii) a mirror ascent algorithm to update the set of local objects with strong guarantees even when the request process does not exhibit any statistical regularity.",
        "DOI": "10.1109/TNET.2022.3217012",
        "affiliation_name": "Centre Inria d'Université Côte d'Azur",
        "affiliation_city": "Sophia Antipolis",
        "affiliation_country": "France"
    },
    {
        "paper_title": "Mjolnir: A framework agnostic auto-tuning system with deep reinforcement learning",
        "paper_author": "Ben Slimane N.",
        "publication": "Applied Intelligence",
        "citied_by": "3",
        "cover_date": "2023-06-01",
        "Abstract": "Choosing the right setting for big data frameworks is an important yet difficult task. These frameworks come with a complex set of parameters that need to be tuned to achieve the best performance in terms of throughput and latency. Learning-based auto-tuning methods using traditional machine learning models might not be effective for the task because they require huge amounts of high-quality training data, which is time-consuming and very expensive. A good alternative would be to consider reinforcement learning methods to train an intelligent agent through trial and error. In this context, we propose a framework-agnostic auto-tuning system implementing an actor-critic algorithm namely TD3 (Twin Delayed Deep Deterministic Policy Gradient). We show that the agent can find an optimal configuration in a continuous high-dimensional search space with a limited number of steps. We conducted extensive experiments on Apache Spark, under different workloads from the HiBench, TPC-DS and TPC-H benchmarking tools. In this paper, we give a detailed representation of the reinforcement learning environment and show the best design through experiments. Results showed that our approach outperforms the state-of-the-art tuning methods and can improve the performance of spark workloads over the default configurations by up to ∼ 77 % with an average of ∼ 45 %. It also showed a promising adaptation behaviour to workload variation during evaluation.",
        "DOI": "10.1007/s10489-022-03956-9",
        "affiliation_name": "RandD. Eura Nova",
        "affiliation_city": "Tunis",
        "affiliation_country": "Tunisia"
    },
    {
        "paper_title": "Cooperative Buffer Management With Fine-Grained Data Migrations for Hybrid Memory Systems",
        "paper_author": "Wang X.",
        "publication": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
        "citied_by": "1",
        "cover_date": "2023-06-01",
        "Abstract": "Hybrid memory composed of DRAM and persistent memory (PM) offers a promising way to realize large-capacity main memory supporting in-memory data storage and computing. However, traditional buffer management schemes focus on improving the hit ratio but lack awareness of the limitations of PM, e.g., slower write time and lower write endurance than DRAM. Therefore, developing new buffer management policies that can reduce costly write-backs of PM blocks while maintaining high performance for the hybrid buffer, is of paramount importance. Existing approaches mainly use a page-grained buffering policy, which will cause unnecessary data migrations between DRAM and PM, leading to a high number of disk I/Os and PM writes. Aiming to reduce I/O costs and PM writes, we propose a new buffer manager named HiBuffer for DRAM/PM-based hybrid memory systems. HiBuffer presents several novel ideas. First, it adopts multigrained data layouts to manage the hybrid buffer cooperatively. In addition to the page granularity, we introduce Lines for the DRAM buffer and Sectors to the PM buffer, forming a buffer with three granularities, including Line, Sector, and Page. We prove that the multigrained cooperative buffer management can deliver higher performance than existing page-grained schemes. Second, we propose a sector-grained method to migrate data from DRAM to PM, which can avoid unnecessary data movements and reduce PM writes. Third, we use an out-of-place updating mechanism to absorb updates in DRAM, which can further reduce the writes to PM. We compare HiBuffer with three existing schemes, including LRU, CLOCK-DWF, and MiniPage, on five synthetic workloads and the YCSB benchmark using real Intel Optane DC PM. The results in terms of various metrics, including running time, PM writes, hit ratio, and disk I/Os, suggest the efficiency of HiBuffer. In particular, HiBuffer reduces the running time by up to 37.8% and the writes to PM by up to 83% compared to the competitors when evaluated on the YCSB benchmark.",
        "DOI": "10.1109/TCAD.2022.3210201",
        "affiliation_name": "ByteDance Ltd.",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Converging Game Theory and Reinforcement Learning For Industrial Internet of Things",
        "paper_author": "Ho T.M.",
        "publication": "IEEE Transactions on Network and Service Management",
        "citied_by": "2",
        "cover_date": "2023-06-01",
        "Abstract": "The fifth-generation (5G) wireless network provides high-rate, ultra-low latency, and high-reliability connections that can meet the Industrial Internet of Things (IIoT) requirements in factory automation, especially for robot motion control. In this paper, we address 5G service provisioning in an automated warehouse scenario, where swarm robotics is controlled by an industrial controller that provides routing and job instructions over the 5G network. Leveraging the coordinated multipoint (CoMP), we formulate a time-varying joint CoMP clustering and 5G ultra-reliable low-latency communication (URLLC) beamforming design problem to control the robots that move around the automated warehouse for goods storage with the planned reference tracks. Traditional iterative optimization approaches are impractical in such a dynamic wireless environment due to high computational time. We propose a game-theoretic CoMP clustering algorithm combined with the Proximal Policy Optimization method to obtain a stationary solution closed to that of the exhaustive search algorithm considered as the global optimal solution.",
        "DOI": "10.1109/TNSM.2022.3202168",
        "affiliation_name": "École de Technologie Supérieure",
        "affiliation_city": "Montreal",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Optimizing Multidocument Summarization by Blending Reinforcement Learning Policies",
        "paper_author": "Su D.J.",
        "publication": "IEEE Transactions on Artificial Intelligence",
        "citied_by": "5",
        "cover_date": "2023-06-01",
        "Abstract": "We consider extractive summarization within a cluster of related texts (multidocument summarization). Unlike single-document summarization, redundancy is particularly important because sentences across related documents might convey overlapping information. Thus, sentence extraction in such a setting is difficult because one will need to determine which pieces of information are relevant while avoiding unnecessary repetitiveness. To solve this difficult problem, we propose a novel reinforcement learning-based method Policy Blending with maximal marginal relevance and Reinforcement Learning (PoBRL) for solving multidocument summarization. PoBRL jointly optimizes over the following objectives necessary for a high-quality summary: importance, relevance, and length. Our strategy decouples this multiobjective optimization into different subproblems that can be solved individually by reinforcement learning. Utilizing PoBRL, we then blend each learned policies to produce a summary that is a concise and a complete representation of the original input. Our empirical analysis shows high performance on several multidocument datasets. Human evaluation also shows that our method produces high-quality output.",
        "DOI": "10.1109/TAI.2022.3201807",
        "affiliation_name": "School of Engineering and Applied Science",
        "affiliation_city": "Princeton",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Representative truck activity patterns from anonymous mobile sensor data",
        "paper_author": "Akter T.",
        "publication": "International Journal of Transportation Science and Technology",
        "citied_by": "6",
        "cover_date": "2023-06-01",
        "Abstract": "With new sources of big data, it is increasingly possible to practically implement advanced freight forecasting models including activity-based and truck touring models. Such models improve upon traditional trip-based approaches by capturing freight behaviors sensitive to transportation policy and infrastructure changes. A persistent challenge with the use of big data in this context is the ability to generalize a set of representative behaviors to serve as the basis for model calibration and validation from anonymized data depicting the complex behaviors of the population. To address this challenge, we present a two-stage methodology to extract unique and representative freight activity patterns from passively collected truck Global Positioning System (GPS) data. The first stage involved a heuristic-based approach to derive a set of stop and trip characteristics from large-streams of GPS pings. The second stage employed data mining and machine learning techniques to discern common freight activity patterns from the set of defined features. The resulting activity pattern profiles, defined as chains of activities and their trajectories over time and space, allow us to maintain the anonymity of the trucks included in the GPS dataset while providing high-resolution travel profiles- a necessary condition for most data sharing agreements between public agencies and private data providers. These activity patterns serve as the critical, and currently missing, data needed to calibrate and validate advanced freight forecasting models. With more advanced forecasting models reflective of observed freight behaviors, we will be able to evaluate a wider spectrum of policy and infrastructure scenarios more accurately.",
        "DOI": "10.1016/j.ijtst.2022.05.002",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Fayetteville",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Finite-Sample Analysis of Two-Time-Scale Natural Actor-Critic Algorithm",
        "paper_author": "Khodadadian S.",
        "publication": "IEEE Transactions on Automatic Control",
        "citied_by": "5",
        "cover_date": "2023-06-01",
        "Abstract": "Actor-critic style two-time-scale algorithms are one of the most popular methods in reinforcement learning, and have seen great empirical success. However, their performance is not completely understood theoretically. In this article, we characterize the global convergence of an online natural actor-critic algorithm in the tabular setting using a single trajectory of samples. Our analysis applies to very general settings, as we only assume ergodicity of the underlying Markov decision process. In order to ensure enough exploration, we employ an ϵ-greedy sampling of the trajectory. For a fixed and small enough exploration parameter ϵ, we show that the two-time-scale natural actor-critic algorithm has a rate of convergence of O(1/T1/4), where T is the number of samples, and this leads to a sample complexity of O(1/δ8) samples to find a policy that is within an error of δ from the global optimum. Moreover, by carefully decreasing the exploration parameter ϵ as the iterations proceed, we present an improved sample complexity of O(1/δ6) for convergence to the global optimum.",
        "DOI": "10.1109/TAC.2022.3190032",
        "affiliation_name": "Virginia Tech College of Engineering",
        "affiliation_city": "Blacksburg",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Worker-Centric Model Allocation for Federated Learning in Mobile Edge Computing",
        "paper_author": "Huang H.",
        "publication": "IEEE Transactions on Green Communications and Networking",
        "citied_by": "3",
        "cover_date": "2023-06-01",
        "Abstract": "Federated Learning (FL) is believed as a promising manner of distributed machine learning for 5G and future 6G networks in the context of mobile edge computing (MEC). From the worker's viewpoint, a problem is that no incentive drives them to participate in FL. Thus, different from the conventional server-centric model allocation methods, we study the worker-centric strategy in this paper. Because communicating with the FL server and training an FL model locally are energy-hungry, a dilemma for each worker is how to make the tradeoff between participating in FL training and the volume restriction of its battery. To address this issue, we formulate a WorkerFirst problem. Its NP-hardness is also proved. Next, we devise a DDQN algorithm and a DQL algorithm to strive for near-optimal decisions for each worker. These two algorithms are proposed by leveraging the deep reinforcement learning framework, while considering the energy consumption, training timespan, and the communication overheads of workers, simultaneously. The benefit of adopting such a DRL-based decision-making framework is that workers can execute the algorithms locally and dynamically adapt to the varying MEC environment. The evaluation results show that the proposed DDQN and DQL algorithms can learn good policies without knowing any prior knowledge of network conditions.",
        "DOI": "10.1109/TGCN.2022.3187335",
        "affiliation_name": "Sun Yat-Sen University",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Multi-Objective Reinforcement Learning Based Healthcare Expansion Planning Considering Pandemic Events",
        "paper_author": "Shuvo S.S.",
        "publication": "IEEE Journal of Biomedical and Health Informatics",
        "citied_by": "8",
        "cover_date": "2023-06-01",
        "Abstract": "Hospital capacity expansion planning is critical for a healthcare authority, especially in regions with a growing diverse population. Policymaking to this end often requires satisfying two conflicting objectives, minimizing capacity expansion cost and minimizing the number of denial of service (DoS) for patients seeking hospital admission. The uncertainty in hospital demand, especially considering a pandemic event, makes expansion planning even more challenging. This work presents a multi-objective reinforcement learning (MORL) based solution for healthcare expansion planning to optimize expansion cost and DoS simultaneously for pandemic and non-pandemic scenarios. Importantly, our model provides a simple and intuitive way to set the balance between these two objectives by only determining their priority percentages, making it suitable across policymakers with different capabilities, preferences, and needs. Specifically, we propose a multi-objective adaptation of the popular Advantage Actor-Critic (A2C) algorithm to avoid forced conversion of DoS discomfort cost to a monetary cost. Our case study for the state of Florida illustrates the success of our MORL based approach compared to the existing benchmark policies, including a state-of-the-art deep RL policy that converts DoS to economic cost to optimize a single objective.",
        "DOI": "10.1109/JBHI.2022.3187950",
        "affiliation_name": "University of South Florida, Tampa",
        "affiliation_city": "Tampa",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "An Automated Deep Reinforcement Learning Pipeline for Dynamic Pricing",
        "paper_author": "Afshar R.R.",
        "publication": "IEEE Transactions on Artificial Intelligence",
        "citied_by": "7",
        "cover_date": "2023-06-01",
        "Abstract": "A dynamic pricing problem is difficult due to the highly dynamic environment and unknown demand distributions. In this article, we propose a deep reinforcement learning (DRL) framework, which is a pipeline that automatically defines the DRL components for solving a dynamic pricing problem. The automated DRL pipeline is necessary because the DRL framework can be designed in numerous ways, and manually finding optimal configurations is tedious. The levels of automation make nonexperts capable of using DRL for dynamic pricing. Our DRL pipeline contains three steps of DRL design, including Markov decision process modeling, algorithm selection, and hyperparameter optimization. It starts with transforming available information to state representation and defining reward function using a reward shaping approach. Then, the hyperparameters are tuned using a novel hyperparameter optimization method that integrates Bayesian optimization and the selection operator of the genetic algorithm. We employ our DRL pipeline on reserve price optimization problems in online advertising as a case study. We show that using the DRL configuration obtained by our DRL pipeline, a pricing policy is obtained whose revenue is significantly higher than the benchmark methods. The evaluation is performed by developing a simulation for the real-time bidding environment that makes exploration possible for the reinforcement learning agent.",
        "DOI": "10.1109/TAI.2022.3186292",
        "affiliation_name": "Technische Universiteit Eindhoven",
        "affiliation_city": "Eindhoven",
        "affiliation_country": "Netherlands"
    },
    {
        "paper_title": "The Invisible Primary in an Agent-Based Model: Ideology, Strategy, and Competitive Dynamics",
        "paper_author": "Nwokora Z.",
        "publication": "Political Research Quarterly",
        "citied_by": "0",
        "cover_date": "2023-06-01",
        "Abstract": "Historical accounts of American presidential nominating contests suggest that candidates jockey over ideology and policy in ways that shape the outcomes of these races. Yet this aspect of competition has been difficult to analyze with the formal and statistical methods that dominate this research agenda. To address this gap, this article presents a computational agent-based model (ABM) of candidates’ ideological maneuvering during the invisible primary. We extend the framework developed by Michael Laver to study dynamic party competition in Europe, but recast it for the different context and to enable model fit to be more rigorously determined. Our analysis of data from the 2012 Republican invisible primary suggests the importance of ideological jockeying in this contest. Moreover, its dynamics can be well-explained by a basic version of the ABM in which candidates select between three strategies (aggregator, hunter or sticker) and then maintain that strategy over time. The fit of this model, particularly in the short run, can be improved by introducing a “momentum effect” that allows the candidates’ standing in the race to rise or fall without any accompanying ideological change.",
        "DOI": "10.1177/10659129221107567",
        "affiliation_name": "Stanford Engineering",
        "affiliation_city": "Stanford",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Responsive Policy Decisions for Improving the Accuracy of Medical Data Analysis in Healthcare-Based Human-Machine Interaction Systems",
        "paper_author": "Altameem A.",
        "publication": "International Journal of Humanoid Robotics",
        "citied_by": "1",
        "cover_date": "2023-06-01",
        "Abstract": "Human-computer interaction (HCI) is deployed in various real-time applications, including healthcare, for automated patient response. In such applications, robot-assisted interactive scenarios are modeled to handle patient queries and provide precise information. Timely query sensing and accurate data analysis are required to achieve accurate patient responses. In this study, responsive policy decision (RPD) using manifold mediator learning (MML) is introduced to improve data detection accuracy and accuracy in robot-assisted HCI applications. The initial decision-making process in data analytics is based on interaction stages and medical data detection. After identifying the most appropriate policy, respondents are provided with time-based responses based on the patient's queries. When it comes to improving the accuracy of data analysis decisions, machine learning uses policies based on interaction stages and previous state efficiency of HCI responses. The experimental analysis proves the reliability of the proposed method by improving the accuracy of data analysis and reducing its complexity and response time for the varying queries and time intervals.",
        "DOI": "10.1142/S0219843622400072",
        "affiliation_name": "Faculty of Engineering Helwan",
        "affiliation_city": "Helwan",
        "affiliation_country": "Egypt"
    },
    {
        "paper_title": "Coking coal futures price index forecasting with the neural network",
        "paper_author": "Xu X.",
        "publication": "Mineral Economics",
        "citied_by": "40",
        "cover_date": "2023-06-01",
        "Abstract": "Coking coal price forecasting is a significant issue for investors and policy makers. This study explores usefulness of the nonlinear autoregressive neural network for this forecasting problem in a dataset of daily closing prices of the coking coal futures traded in China Dalian Commodity Exchange during January 4, 2016–December 31, 2020. Through examining various model settings across the algorithm, delay, hidden neuron, and data splitting ratio, the model leading to generally accurate and stable performance is reached. Particularly, the model’s inputs are the lagged coking coal futures prices and output is the 1-day ahead price forecast. The model is based on the two-layer feedforward network with six delays and two hidden neurons, which is trained through the Levenberg-Marquardt algorithm, and leads to relative root mean square errors of 1.84%, 1.85%, and 1.84% for the training, validation, and testing phases, respectively. Usefulness of the machine learning technique for the price forecasting problem of the coking coal price is illustrated. Results here might be used on a standalone basis as technical forecasts or combined with fundamental forecasts to form perspectives of price trends and perform policy analysis.",
        "DOI": "10.1007/s13563-022-00311-9",
        "affiliation_name": "NC State University",
        "affiliation_city": "Raleigh",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "The fiscal response to revenue shocks",
        "paper_author": "Berset S.",
        "publication": "International Tax and Public Finance",
        "citied_by": "4",
        "cover_date": "2023-06-01",
        "Abstract": "We study the impact of fiscal revenue shocks on local fiscal policy. We focus on the very volatile revenues from the immovable property gains tax in the canton of Zurich, Switzerland, and analyze fiscal behavior following large and rare positive and negative transitory revenue shocks. We apply causal machine learning strategies and implement the post-double-selection LASSO method to identify the effect of revenue shocks on public finances. We find that local policymakers predominantly smooth transitory fiscal shocks.",
        "DOI": "10.1007/s10797-022-09727-z",
        "affiliation_name": "University of Fribourg",
        "affiliation_city": "Fribourg",
        "affiliation_country": "Switzerland"
    },
    {
        "paper_title": "Scalable Scheduling of Semiconductor Packaging Facilities Using Deep Reinforcement Learning",
        "paper_author": "Park I.B.",
        "publication": "IEEE Transactions on Cybernetics",
        "citied_by": "33",
        "cover_date": "2023-06-01",
        "Abstract": "Reinforcement learning (RL) has emerged as a promising approach for scheduling semiconductor operations. Yet, it is still challenging to solve large-scale scheduling problems based on an RL method since learning complexity grows fast as the size of shop floor increases. This challenge becomes more apparent when solving the scheduling problems with a diverse number of job types, which leads to the difficulties in exploration and function approximation in RL. This article presents a scheduling method for semiconductor packaging facilities using deep RL in which an agent allocates a job to one of machines in a centralized manner. Specifically, a novel state representation is introduced to effectively accommodate the variations in the number of available machines and the production requirements. Furthermore, we propose a continuous representation of an action to maintain the size of the action space even when the numbers of jobs, machines, and operation types are subject to change. Extensive experiments on large-scale datasets demonstrate that the proposed method mostly outperforms the metaheuristics and rule-based methods, as well as the other RL approaches considered in terms of makespan while requiring much less computation time than the metaheuristics.",
        "DOI": "10.1109/TCYB.2021.3128075",
        "affiliation_name": "Institute for Industrial Systems Innovation",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "Reinforcement Learning in Economics and Finance",
        "paper_author": "Charpentier A.",
        "publication": "Computational Economics",
        "citied_by": "34",
        "cover_date": "2023-06-01",
        "Abstract": "Reinforcement learning algorithms describe how an agent can learn an optimal action policy in a sequential decision process, through repeated experience. In a given environment, the agent policy provides him some running and terminal rewards. As in online learning, the agent learns sequentially. As in multi-armed bandit problems, when an agent picks an action, he can not infer ex-post the rewards induced by other action choices. In reinforcement learning, his actions have consequences: they influence not only rewards, but also future states of the world. The goal of reinforcement learning is to find an optimal policy – a mapping from the states of the world to the set of actions, in order to maximize cumulative reward, which is a long term strategy. Exploring might be sub-optimal on a short-term horizon but could lead to optimal long-term ones. Many problems of optimal control, popular in economics for more than forty years, can be expressed in the reinforcement learning framework, and recent advances in computational science, provided in particular by deep learning algorithms, can be used by economists in order to solve complex behavioral problems. In this article, we propose a state-of-the-art of reinforcement learning techniques, and present applications in economics, game theory, operation research and finance.",
        "DOI": "10.1007/s10614-021-10119-4",
        "affiliation_name": "Laboratoire d’Analyse et de Mathématiques Appliquées",
        "affiliation_city": "Marne-la-Vallee",
        "affiliation_country": "France"
    },
    {
        "paper_title": "EFFICIENT IOT-BASED CLOUD COMPUTING FRAMEWORK FOR SECURE DATA STORAGE USING MACHINE LEARNING ALGORITHM",
        "paper_author": "Patil R.S.",
        "publication": "Journal of Theoretical and Applied Information Technology",
        "citied_by": "1",
        "cover_date": "2023-05-31",
        "Abstract": "Cloud computing is a widely used technology that has changed the way people and organizations store and access information. This technology is versatile, and extensive amounts of data can be stored in the cloud. However, with the development of cloud computing, it is also faced with many difficulties, cloud computing security has become the leading cause of impeding its development. Cloud computing security has become a hot topic in industry and academic research. As a consequence, the security of data stored in the cloud serves as a key concern for cloud consumers due to ongoing hacking incidents in the cloud. This work used encryption with access management because authenticities, anonymity, and security over accessibility are mandatory. Accordingly, the article proposed a machine learning-based method for secure data storage in the cloud. Initially, the data is compressed using the Huffman algorithm, which minimizes text data size and storage, resource use, or transmission power. Accordingly, the compressed data are encrypted using a novel cryptographic technique. This method encrypts the data before uploading it onto the cloud. Subsequently, the malicious intention in the cloud platform is identified by proposing a Weighted Chimp Algorithm optimized Gaussian Kernel Radial Basis Function Neural Network. This malicious code can be spread through infrastructures in the cloud platforms and pose a great threat to users and enterprises. The proposed method accurately detects malicious code in the cloud. The proposed work is implemented using Python software. The proposed method is compared with the other existing methods like Fully Homomorphic Encryption (FHE), Ciphertext Policy-Attribute based Encryption (CP-ABE), and Quasi Modified Levy Flight Distribution Reversed Sheamir Algorithm (QMLFD-RSA). Accordingly, the proposed method outperforms these existing methods. The result revealed that the deduplication rate, throughput, cipher text and encryption time of the proposed method produce higher performance than the existing methods, ie) the deduplication rate for the proposed method is 94% and the outcome of the work proved that the proposed work produces better security than the other existing research respectively. This hybrid technique provides the user to get an advantage from retrieved information in a protected manner.",
        "DOI": "NA",
        "affiliation_name": "Bharati Vidyapeeth’s College of Engineering, Navi Mumbai",
        "affiliation_city": "Navi Mumbai",
        "affiliation_country": "India"
    },
    {
        "paper_title": "A natural language processing approach to categorise contributing factors from patient safety event reports",
        "paper_author": "Tabaie A.",
        "publication": "BMJ Health and Care Informatics",
        "citied_by": "6",
        "cover_date": "2023-05-31",
        "Abstract": "Objectives The objective of this study was to explore the use of natural language processing (NLP) algorithm to categorise contributing factors from patient safety event (PSE). Contributing factors are elements in the healthcare process (eg, communication failures) that instigate an event or allow an event to occur. Contributing factors can be used to further investigate why safety events occurred. Methods We used 10 years of self-reported PSE reports from a multihospital healthcare system in the USA. Reports were first selected by event date. We calculated χ 2 values for each ngram in the bag-of-words then selected N ngrams with the highest χ 2 values. Then, PSE reports were filtered to only include the sentences containing the selected ngrams. Such sentences were called information-rich sentences. We compared two feature extraction techniques from free-text data: (1) baseline bag-of-words features and (2) features from information-rich sentences. Three machine learning algorithms were used to categorise five contributing factors representing sociotechnical errors: communication/hand-off failure, technology issue, policy/procedure issue, distractions/interruptions and lapse/slip. We trained 15 binary classifiers (five contributing factors ∗ three machine learning models). The models' performances were evaluated according to the area under the precision-recall curve (AUPRC), precision, recall, and F1-score. Results Applying the information-rich sentence selection algorithm boosted the contributing factor categorisation performance. Comparing the AUPRCs, the proposed NLP approach improved the categorisation performance of two and achieved comparable results with baseline in categorising three contributing factors. Conclusions Information-rich sentence selection can be incorporated to extract the sentences in free-text event narratives in which the contributing factor information is embedded.",
        "DOI": "10.1136/bmjhci-2022-100731",
        "affiliation_name": "NC State University",
        "affiliation_city": "Raleigh",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Modeling, Replicating, and Predicting Human Behavior: A Survey",
        "paper_author": "Fuchs A.",
        "publication": "ACM Transactions on Autonomous and Adaptive Systems",
        "citied_by": "12",
        "cover_date": "2023-05-29",
        "Abstract": "Given the popular presupposition of human reasoning as the standard for learning and decision making, there have been significant efforts and a growing trend in research to replicate these innate human abilities in artificial systems. As such, topics including Game Theory, Theory of Mind, and Machine Learning, among others, integrate concepts that are assumed components of human reasoning. These serve as techniques to replicate and understand the behaviors of humans. In addition, next-generation autonomous and adaptive systems will largely include AI agents and humans working together as teams. To make this possible, autonomous agents will require the ability to embed practical models of human behavior, allowing them not only to replicate human models as a technique to \"learn\"but also to understand the actions of users and anticipate their behavior, so as to truly operate in symbiosis with them. The main objective of this article is to provide a succinct yet systematic review of important approaches in two areas dealing with quantitative models of human behaviors. Specifically, we focus on (i) techniques that learn a model or policy of behavior through exploration and feedback, such as Reinforcement Learning, and (ii) directly model mechanisms of human reasoning, such as beliefs and bias, without necessarily learning via trial and error.",
        "DOI": "10.1145/3580492",
        "affiliation_name": "Università di Pisa",
        "affiliation_city": "Pisa",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Integrating GIS Data with Machine Learning Approaches for Improved Flood Hazard Prediction",
        "paper_author": "Moutaouakil W.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-05-24",
        "Abstract": "Flooding is a significant natural hazard that can cause important loss of life and property damage. The accurate prediction of flood hazards is crucial for emergency management and disaster risk reduction. In the last few years, the use of Artificial Intelligent techniques has witnessed increasing rates, specifically in the domains of hydrology and water resources management, including flood hazard prediction. This research proposes a support to policy suggestion in order to mitigate the rate of human mortality and, to reduce property destruction caused by this severe natural phenomenon. Indeed, this study aims to evaluate and discover more efficient prediction algorithms of ML to evaluate the probability of potential flood hazards using ML algorithms, namely Ensemble Learning (EL), Logistic Model Tree (LMT), Random Forest (RF), K nearest neighbor (KNN), and Logistic Regression (LR). The study comprises monthly rainfall data over a period of 113 years, random partitioning was employed in order to allocate 80% of the dataset for training, and 20%, reserved for testing. The finding revealed that Logistic Regression achieves the best accuracy of 99.15% among other models employed for flood forecasting.",
        "DOI": "10.1145/3607720.3607774",
        "affiliation_name": "Hassan II University of Casablanca",
        "affiliation_city": "Casablanca",
        "affiliation_country": "Morocco"
    },
    {
        "paper_title": "Qualitative Intention-aware Attribute-based Access Control Policy Refinement",
        "paper_author": "Mitani S.",
        "publication": "Proceedings of ACM Symposium on Access Control Models and Technologies, SACMAT",
        "citied_by": "1",
        "cover_date": "2023-05-24",
        "Abstract": "Designing access control policies is often expensive and tedious due to the heterogeneous systems, services, and diverse user demands. Although ABAC policy and decision engine creation methods based on machine learning have been proposed, they cannot make good access decisions for applications and situations not envisioned by the decision-makers who provide training examples. It results in over-and under-permissiveness. In this paper, we propose a framework that refines pre-developed policies. It creates a decision engine that makes better decisions than those policies. Inspired by multiple criteria decision theory, our method uses the policy manager's qualitative intentions behind their judgments to guide access decisions so that more benefits are expected. In the evaluation, we prepare a coarse and relatively elaborate policy. We refine the coarse policy to obtain a decision engine that is compared for the similarity in access decisions with the elaborate policy using AUC as a measure. The results show that our method improves the coarse policy by a difference of 12-26% in AUC and outperforms the conventional machine learning methods by a difference of 3-11% in AUC.",
        "DOI": "10.1145/3589608.3593841",
        "affiliation_name": "NEC Corporation",
        "affiliation_city": "Tokyo",
        "affiliation_country": "Japan"
    },
    {
        "paper_title": "SAFE-PASS: Stewardship, Advocacy, Fairness and Empowerment in Privacy, Accountability, Security, and Safety for Vulnerable Groups",
        "paper_author": "Ray I.",
        "publication": "Proceedings of ACM Symposium on Access Control Models and Technologies, SACMAT",
        "citied_by": "0",
        "cover_date": "2023-05-24",
        "Abstract": "Our vision is to achieve societally responsible secure and trustworthy cyberspace that puts algorithmic and technological checks and balances on the indiscriminate sharing and analysis of data. We achieve this vision in a holistic manner by framing research directions with four major considerations: (i) Expanding knowledge and understanding of security and privacy perceptions and expectations in vulnerable groups, which significantly contribute to their unwillingness to share data, and use that knowledge to drive research in (a) mitigating missing/imbalanced data problems, (b) understanding and modeling security and privacy risks of data sharing, and (c) modeling utility of data sharing. (ii) Developing a risk-adaptive, policy model capable of capturing and articulating security and privacy expectations of users that are relevant in a particular context and develops associated technology to ensure provenance and accountability. (iii) Developing robust AI/ML algorithms that are transparent and explainable with respect to fairness and bias to reduce/eliminate discrimination, misuse, privacy violations, or other cyber-crimes. (iv) Developing models and techniques for a nuanced, contextually adaptive, and graded privacy paradigm that allows trade-offs between privacy and utility. Towards this, in this paper we present the SAFE-PASS framework to provide Stewardship, Advocacy, Fairness and Empowerment in Privacy, Accountability, Security, and Safety for Vulnerable Groups.",
        "DOI": "10.1145/3589608.3593830",
        "affiliation_name": "University of California, San Diego",
        "affiliation_city": "La Jolla",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Applications of big data and artificial intelligence in smart energy systems: Smart energy system: Design and its state-of-the art technologies",
        "paper_author": "Nagpal N.",
        "publication": "Applications of Big Data and Artificial Intelligence in Smart Energy Systems: Smart Energy System: Design and its State-of-The Art Technologies",
        "citied_by": "0",
        "cover_date": "2023-05-23",
        "Abstract": "This book covers smart grid applications of various big data analytics, artificial intelligence, and machine learning technologies for demand prediction, decision-making processes, policy, and energy management. The book delves into new technologies such as the Internet of Things, BlockChain for smart home solutions, and smart city solutions in depth in the context of modern power systems. In the era of propelling traditional energy systems to evolve towards smart energy systems, systems, including power generation energy storage systems, and electricity consumption have become more dynamic. The quality and reliability of power supply are impacted by the sporadic and rising use of electric vehicles, and domestic and industrial loads. Similarly, with the integration of solid state devices, renewable sources, and distributed generation, power generation processes are evolving in a variety of ways. Several cutting-edge technologies are necessary for the safe and secure operation of power systems in such a dynamic setting, including load distribution automation, energy regulation and control, and energy trading. Technical topics discussed in the book include: • Hybrid smart energy system technologies • Smart meters • Energy demand forecasting • Use of different protocols and communication in smart energy systems • Power quality and allied issues and mitigation using AI • Intelligent transportation • Virtual power plants • AI based smart energy business models • Smart home solutions • Blockchain solutions for smart grids.",
        "DOI": "NA",
        "affiliation_name": "VIT Bhopal University",
        "affiliation_city": "Sehore",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Releasing global forests from human management: How much more carbon could be stored?",
        "paper_author": "Roebroek C.T.J.",
        "publication": "Science",
        "citied_by": "56",
        "cover_date": "2023-05-19",
        "Abstract": "Carbon storage in forests is a cornerstone of policy-making to prevent global warming from exceeding 1.5°C. However, the global impact of management (for example, harvesting) on the carbon budget of forests remains poorly quantified. We integrated global maps of forest biomass and management with machine learning to show that by removing human intervention, under current climatic conditions and carbon dioxide (CO2) concentration, existing global forests could increase their aboveground biomass by up to 44.1 (error range: 21.0 to 63.0) petagrams of carbon. This is an increase of 15 to 16% over current levels, equating to about 4 years of current anthropogenic CO2 emissions. Therefore, without strong reductions in emissions, this strategy holds low mitigation potential, and the forest sink should be preserved to offset residual carbon emissions rather than to compensate for present emissions levels.",
        "DOI": "10.1126/SCIENCE.ADD5878",
        "affiliation_name": "European Commission Joint Research Centre",
        "affiliation_city": "Brussels",
        "affiliation_country": "Belgium"
    },
    {
        "paper_title": "AI-based big data algorithms and machine learning techniques for managing data in e-governance",
        "paper_author": "Samuel P.",
        "publication": "AI, IoT, and Blockchain Breakthroughs in E-Governance",
        "citied_by": "13",
        "cover_date": "2023-05-18",
        "Abstract": "E-government is the effective and efficient delivery of high-quality information and governmental services to citizens and/or other governmental and non-government entities. Both the service provider and the citizens using the services will gain from big data analysis. Market Research indicates that 75% of government agencies are using big data to enhance the standard of living for individuals. Big data analytics gives a perspective into the effectiveness of government programmes and policies. In order to gauge public sentiment and comprehend citizens ' thoughts and attitudes about government programmes, forecasting and prescriptive analysis suggests the optimal course of action. This chapter will provide a brief overview of key topics to help readers fully comprehend how big data is used across the government department.",
        "DOI": "10.4018/978-1-6684-7697-0.ch002",
        "affiliation_name": "SRM Institute of Science and Technology",
        "affiliation_city": "Kattankulathur",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Create an IPCC-like body to harness benefits and combat harms of digital tech",
        "paper_author": "Bak-Coleman J.",
        "publication": "Nature",
        "citied_by": "6",
        "cover_date": "2023-05-18",
        "Abstract": "Emerging information technologies, including ChatGPT, require proper stewardship. An intergovernmental panel to synthesize the evidence offers the best path forward.",
        "DOI": "10.1038/d41586-023-01606-9",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Audit AI search tools now, before they skew research",
        "paper_author": "Gusenbauer M.",
        "publication": "Nature",
        "citied_by": "7",
        "cover_date": "2023-05-18",
        "Abstract": "NA",
        "DOI": "10.1038/d41586-023-01613-w",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Machine Learning Based Techniques for Paddy Yield Prediction for the State of Andhra Pradesh",
        "paper_author": "Rohini S.",
        "publication": "International Journal of Intelligent Systems and Applications in Engineering",
        "citied_by": "2",
        "cover_date": "2023-05-17",
        "Abstract": "Timely and accurate crop yield prediction serves as a pillar for the country’s food security and frames the strategic policies for the government. In this study, we endeavoured to assess the effectiveness of three various machine learning-based methods to predict paddy yield for the Indian state of Andhra Pradesh. The models were developed using historical yield data for the years 2001 to 2020 along with the long-term derived satellite variables evapotranspiration (ET), leaf area index (LAI), land surface temperature (LST), normalised difference vegetation index (NDVI), and rainfall (RF). Multiple linear regression (MLR), support vector regression (SVR), and random forest regression (RFR) models were three different machine learning models that were assessed for performance. A correlation was established between these variables and crop yield. The highly correlated features model was built and the features with the least correlation were discarded. The performance of all three models was found to be satisfactory. The RFR model was found to have higher accuracy with an R2 value of 0.61 and an RMSE of 0.55 t ha-1. Whereas MLR and SVR were found to have R2 0.51 and 0.59, RMSE 0.59 t ha-1, and 0.54 t ha-1. The results from the current study have shown the capability of machine learning algorithms with limited datasets.",
        "DOI": "NA",
        "affiliation_name": "Sri Venkateswara University College Of Engineering",
        "affiliation_city": "Tirupati",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Analyse the consumer preferences and purchases of electric vehicles purchases and developments",
        "paper_author": "Kumar V.",
        "publication": "Contemporary Studies of Risks in Emerging Technology: Part B",
        "citied_by": "2",
        "cover_date": "2023-05-15",
        "Abstract": "Aim/Purpose: This chapter supplies an overview of extant literature investigating consumer preferences and purchase behaviour towards Electric vehicles in India. The present research applied the predictive and prescriptive analysis methods to understand the consumer purchase intentions evolved with the advancement of technologies. The study considers the security and privacy of data for companies and consumers. Design/Methodology/Approach: The conceptual model is analysed via path analysis using interviews with a set of people, and online survey data is collected from Indian users and prospects. Secondary data research methods are used to make interpretations. Findings: The research found that consumer preferences and purchase behaviour about electric vehicles have a wide scope in marketing decision-making. The findings support linking social cognitive perception and attitude intent with mitigating socio-demographic variables and mediating attitudes towards Battery Electric Vehicles (BEV). Research limitations/implications: The present chapter provides implications for future interdisciplinary research addressing consumer behaviour with mentioned technologies. This study proposes some theoretical and policy implications and supplies guidance to industry-specific consumers, suppliers, policymakers and business professionals to encourage the adoption of BEVs in the emerging transportation industry. The research explores the future possibility and extent to which the consumer prefers and buys EVs. Originality/Value: This study is based on the 'perception-attitude-intention' socio-psychological linkage framework. Earlier studies overlooked the effects of socio-psychological characteristics and socio-demographic moderators when considering the adoption of BEVs.",
        "DOI": "10.1108/978-1-80455-566-820231004",
        "affiliation_name": "Lovely Professional University",
        "affiliation_city": "Phagwara",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Forecasting carbon price in China using a novel hybrid model based on secondary decomposition, multi-complexity and error correction",
        "paper_author": "Yang H.",
        "publication": "Journal of Cleaner Production",
        "citied_by": "42",
        "cover_date": "2023-05-15",
        "Abstract": "As global warming intensifies, the reduction of carbon emissions is imminent. Carbon price is directly related to whether carbon can be effectively reduced. Therefore, accurately forecasting carbon price has important practical significance. Aiming at the nonstationary and nonlinear characteristics of carbon price, this paper proposes a novel hybrid model for forecasting carbon price, which is based on improved complete ensemble empirical mode decomposition with adaptive noise (ICEEMDAN), multiscale fuzzy entropy (MFE), complete ensemble empirical mode decomposition (CEEMD), improved random forest by salp swarm algorithm (SSARF), improved back propagation by cuckoo search (CSBP), improved extreme learning machine by whale optimization algorithm (WOAELM) and error correction (EC), named ICEEMDAN-MFE-CEEMD-SSARF-CSBP-WOAELM-EC. Firstly, carbon price is decomposed by ICEEMDAN, divided into high-, medium-, and low-complexity components by MFE. Secondly, high-complexity components are merged and secondarily decomposed by CEEMD, which are still recorded as high-complexity components. Then, SSARF, CSBP and WOAELM are used to forecast high-, medium-, and low-complexity components, respectively, and forecasting results are reconstructed. Finally, EC is carried out using an extreme learning machine to obtain the final forecasting results, and the Diebold-Mariano test is introduced for a comprehensive evaluation of the model. Taking carbon price in the pilot cities of Shenzhen and Hubei as examples, after 6 aspects and 20 comparative experiments, the results show that the proposed model has higher forecast accuracy, with MAPE, MAE and RMSE up to 0.03131, 0.00089 and 4.02e-06 in Hubei, and its forecasting ability is better than other commonly used international carbon financial price forecasting models, providing a theoretical and data basis for carbon pricing and formulating carbon reduction policies in China. The main contributions of this paper are the improved primary decomposition, the use of secondary decomposition, and the innovative combination of three optimal models to forecast carbon price, but it still needs to be optimized for practice.",
        "DOI": "10.1016/j.jclepro.2023.136701",
        "affiliation_name": "Xi'an Institute of Posts and Telecommunications",
        "affiliation_city": "Xi'an",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Conditional feature disentanglement learning for anomaly detection in machines operating under time-varying conditions",
        "paper_author": "Zhou H.",
        "publication": "Mechanical Systems and Signal Processing",
        "citied_by": "17",
        "cover_date": "2023-05-15",
        "Abstract": "Anomaly detection(AD) is an important task of machines’ condition monitoring(CM). Data-driven policies can be used in a more intelligent way to achieve anomaly detection and effectively avoid the introduction of expert experience, thus having a broader scope of application. However, Machines like wind turbines often work under time-varying operating conditions(TVOCs), and the performance of traditional data-driven AD methods is significantly degraded because TVOCs can lead to “false alarms” and “missed alarms” in the implementation due to the monitoring data shifting caused by variation of operating conditions(OCs). To address this problem, this paper proposes a novel conditional feature disentanglement learning framework to solve the disturbance in AD on account of entanglement between OCs and health states. The proposed approach performs conditional self-supervised AD by utilizing the variational autoencoder(VAE) and OCs information. Then, a feature disentanglement conditional VAE(FDCVAE) network is developed to realize the disentanglement of OCs and health states. Subsequently, An anomaly indicator(ANI) is constructed by the dimension reduction of the disentangled health state-related feature and combined with the statistics anomaly threshold for AD. Experiments on accelerated fatigue degradation of bearings under TVOCs validate the effectiveness of the proposed method and further demonstrate the superiority of the constructed ANI in eliminating TVOC interference, compared with common fault mechanisms-based and data-driven ANI. The proposed method not only achieves anomaly detection under TVOCs but also provides a new way for representation learning under variable working conditions in machine health management applications in the foreseeable future.",
        "DOI": "10.1016/j.ymssp.2023.110139",
        "affiliation_name": "Centre de Recherche sur les Risques et les Crises",
        "affiliation_city": "Sophia Antipolis",
        "affiliation_country": "France"
    },
    {
        "paper_title": "Recursive forecasting for vaccination demand with hybrid feature LSTM considering heterogeneous policies and risk perception",
        "paper_author": "Luo Q.",
        "publication": "Expert Systems with Applications",
        "citied_by": "5",
        "cover_date": "2023-05-15",
        "Abstract": "Vaccination demand changes rapidly during the pandemic, as the public health policy evolves and the pandemic develops among different countries and regions. Preprocessing the original multi-source daily data frames, we characterize the temporal utilities of heterogeneous vaccination policies and the epidemic risk perception, which lead to a boost to our recursive predictive practice. Beyond leveraging these two primary predictors, factors of non-pharmaceutical interventions (NPIs) and fundamental temporal dynamics are fully used to make fine-grained recursive predictions for vaccination demand. We conduct a series of experiments on real-world vaccination-related datasets among different countries and regions and the experimental results show that the proposed hybrid feature recursive model gets almost 94% predictive accuracy in mainland China, which gains an evident advantage over ones fed with features excluding policy-related and perception-related predictors with 7.42% improvement rate (IR) in terms of accuracy. It is far better than the baseline methods with 31.84% absolute IR on average and gets 92.57% average accuracy among different regions. The tailored Policy-specific Risk-aware Long Short-Term Memory (PsRa-LSTM) approach outperforms the state-of-the-art competing benchmark methods in terms of valid evaluation metrics including RMSE, MAE, and MAPE. Utilizing local and global machine learning model explainers, we carry out innovative quantitative contribution analysis to capture reasonable causality between preceding predictors and subsequent prediction targets. Policymakers can optimize the allocation and schedule of supplies and personnel with kinds of vaccines, formulate public health policy including sequential booster vaccination, or provide guidance to public opinion through media campaigns for vaccination.",
        "DOI": "10.1016/j.eswa.2023.119545",
        "affiliation_name": "Huazhong University of Science and Technology",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Export- and import-based economic models for predicting global trade using deep learning",
        "paper_author": "Yang C.H.",
        "publication": "Expert Systems with Applications",
        "citied_by": "18",
        "cover_date": "2023-05-15",
        "Abstract": "Forecasting global foreign trade is essential for developing government trade policies and management strategies for multinational corporations. However, achieving an accurate trade forecast is challenging because of the complex structural relationships between exports, imports and other economic variables. Many traditional forecasting models, such as time series, econometric, and machine learning, provide less accurate forecasts for trade data. This paper proposes an ensemble learning approach to improve forecasting performance by hybridizing the structural relationships between trade and deep learning models to predict foreign trade for ten major countries. The proposed method first establishes a cointegration relationship between exports and imports and their structural variables. The cointegrated models are then used to predict the future of trade, which is used as a benchmark model for comparison. A hybrid deep learning algorithm uses the cointegrated variables as input variables to predict trade data, and then are compared with time-series forecasts and economic structural models. The experimental results reveal that the ensemble learning method can achieve excellent forecasting performance for the tested periods of trade data. In most cases, the root means square error and mean absolute percentage error values are smaller than the time series and economic structural models.",
        "DOI": "10.1016/j.eswa.2023.119590",
        "affiliation_name": "National Kaohsiung University of Science and Technology",
        "affiliation_city": "Kaohsiung",
        "affiliation_country": "Taiwan"
    },
    {
        "paper_title": "A hierarchical framework for improving ride comfort of autonomous vehicles via deep reinforcement learning with external knowledge",
        "paper_author": "Du Y.",
        "publication": "Computer-Aided Civil and Infrastructure Engineering",
        "citied_by": "49",
        "cover_date": "2023-05-15",
        "Abstract": "Ride comfort plays an important role in determining the public acceptance of autonomous vehicles (AVs). Many factors, such as road profile, driving speed, and suspension system, influence the ride comfort of AVs. This study proposes a hierarchical framework for improving ride comfort by integrating speed planning and suspension control in a vehicle-to-everything environment. Based on safe, comfortable, and efficient speed planning via dynamic programming, a deep reinforcement learning-based suspension control is proposed to adapt to the changing pavement conditions. Specifically, a deep deterministic policy gradient with external knowledge (EK-DDPG) algorithm is designed for the efficient self-adaptation of suspension control strategies. The external knowledge of action selection and value estimation from other AVs are combined into the loss functions of the DDPG algorithm. In numerical experiments, real-world pavements detected in 11 districts of Shanghai, China, are applied to verify the proposed method. Experimental results demonstrate that the EK-DDPG-based suspension control improves ride comfort on untrained rough pavements by 27.95% and 3.32%, compared to a model predictive control (MPC) baseline and a DDPG baseline, respectively. Meanwhile, the EK-DDPG-based suspension control improves computational efficiency by 22.97%, compared to the MPC baseline, and performs at the same level as the DDPD baseline. This study provides a generalized and computationally efficient approach for improving the ride comfort of AVs.",
        "DOI": "10.1111/mice.12934",
        "affiliation_name": "Key Laboratory of Road and Traffic Engineering of the State Ministry of Education",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Privacy Policies across the Ages: Content of Privacy Policies 1996-2021",
        "paper_author": "Wagner I.",
        "publication": "ACM Transactions on Privacy and Security",
        "citied_by": "11",
        "cover_date": "2023-05-13",
        "Abstract": "It is well known that most users do not read privacy policies but almost always tick the box to agree with them. While the length and readability of privacy policies have been well studied and many approaches for policy analysis based on natural language processing have been proposed, existing studies are limited in their depth and scope, often focusing on a small number of data practices at single point in time. In this article, we fill this gap by analyzing the 25-year history of privacy policies using machine learning and natural language processing and presenting a comprehensive analysis of policy contents. Specifically, we collect a large-scale longitudinal corpus of privacy policies from 1996 to 2021 and analyze their content in terms of the data practices they describe, the rights they grant to users, and the rights they reserve for their organizations. We pay particular attention to changes in response to recent privacy regulations such as the GDPR and CCPA. We observe some positive changes, such as reductions in data collection post-GDPR, but also a range of concerning data practices, such as widespread implicit data collection for which users have no meaningful choices or access rights. Our work is an important step toward making privacy policies machine readable on the user side, which would help users match their privacy preferences against the policies offered by web services.",
        "DOI": "10.1145/3590152",
        "affiliation_name": "Universität Basel",
        "affiliation_city": "Basel",
        "affiliation_country": "Switzerland"
    },
    {
        "paper_title": "Obesity Prediction using Bayesian Optimized Gradient Boosted Trees",
        "paper_author": "Hao E.C.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-05-12",
        "Abstract": "Obesity is a growing concern worldwide, and its prediction is crucial for preventing and managing related health problems. In this study, we aim to develop a reliable and accurate model for predicting obesity based on various demographic and lifestyle factors. Prior studies have been trying to combat obesity by understanding the underlying causes and developing interventions to treat and prevent the disease. The model considers data such as age, gender, aspects of physical activity, and dietary habits from the publicly available dataset in the University of California Irvine (UCI) Machine Learning Repository to make predictions. We used different machine learning algorithms to analyze the data and evaluate the performance of the model. Our results showed that the model had a high accuracy rate of 99.84% from the LightGBM model, which indicates its potential for practical use. Nested Stratified Cross-Validation (CV) was used to confirm the results of the model prediction. Furthermore, the model provides valuable insights into the factors that contribute to obesity, which can be used to inform public health policies and interventions. In conclusion, this study has important implications for the prevention and management of obesity and highlights the importance of considering demographic and lifestyle factors in obesity prediction.",
        "DOI": "10.1145/3608298.3608308",
        "affiliation_name": "De La Salle University",
        "affiliation_city": "Manila",
        "affiliation_country": "Philippines"
    },
    {
        "paper_title": "Machine Learning on Insurance Premium Prediction",
        "paper_author": "Jesus R.M.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-05-10",
        "Abstract": "The insurance field is going through a phase of great transformation due to the growth of new technologies and techniques that are causing a change in the way data is handled and analyzed. The main perpetrator of this phenomenon is the introduction of Machine Learning (ML) in financial decision-making due to their efficiency and productivity. However, there is a new intervenient in the room, which will automate and support all steps of ML system development. Machine Learning Operations (MLOPs) will reduce technical friction, so that the model may move from an idea into production, in the shortest amount of time, and subsequently to market with the least possible risk. In this paper, a detailed review of the impacts of ML on insurance premium forecasting and the influence that MLOPs can have on forecasting outcomes is provided. Furthermore, a comprehensive summary is presented of crucial principles in the insurance industry, which are essential for comprehending the role that MLOPs will play in tailoring and individualizing insurance policies and premiums.",
        "DOI": "10.1145/3605423.3605450",
        "affiliation_name": "Universidade do Minho",
        "affiliation_city": "Braga",
        "affiliation_country": "Portugal"
    },
    {
        "paper_title": "Deep learning technique for forecasting the price of cauliflower",
        "paper_author": "Paul R.K.",
        "publication": "Current Science",
        "citied_by": "9",
        "cover_date": "2023-05-10",
        "Abstract": "Vegetables are the staple food in our diets. Vegetable prices are difficult to forecast because they are influen-ced by a variety of factors, including weather, demand and supply chain, Government policies, etc. and exhibit volatile fluctuations. Marketing of vegetables is complex, especially because of their perishability, seasonality and bulkiness. An accurate and timely forecast of vegeta-bles is essential to help its stakeholders. Previous studies observed that traditional statistical models are unable to capture the complex behaviour of vegetable markets. In this study, a comparative assessment has been carried out among the traditional time-series model, machine learning and deep learning techniques in order to find the best-suited model. For empirical illustration, cauli-flower markets have been chosen as it is one of India’s most important and popular winter. In order to identify the complexity in the price of cauliflower, the machine learning technique, i.e. artificial neural network and deep learning technique, i.e. long short-term memory model have been implemented. In addition, the traditio-nal stochastic time-series model, i.e. autoregressive inte-grated moving average model, was used to compare the prediction accuracy of the above models. To this end, the moving window forecast approach was also implemented to evaluate the sensitivity of these models with respect to forecast length. It can be concluded that the deep learning model outperforms the traditional time-series model and the machine learning technique for both short and long-term forecasting.",
        "DOI": "10.18520/cs/v124/i9/1065-1073",
        "affiliation_name": "ICAR - Indian Agricultural Statistics Research Institute, New Delhi",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Socio-economic development drives solid waste management performance in cities: A global analysis using machine learning",
        "paper_author": "Velis C.A.",
        "publication": "Science of the Total Environment",
        "citied_by": "30",
        "cover_date": "2023-05-10",
        "Abstract": "Mismanaged municipal solid waste (MSW), the major source of plastics pollution and a key contributor to climate forcing, in Global South cities poses public health and environmental problems. This study analyses the first consistent and quality assured dataset available for cities distributed worldwide, featuring a comprehensive set of solid waste management performance indicators (Wasteaware Cities Benchmark Indicators – WABI). Machine learning (multivariate random forest) and univariate non-linear regression are applied, identifying best-fit converging models for a broad range of explanatory socioeconomic variables. These proxies describe in a variety of ways generic levels of progress, such as Gross Domestic Product – Purchasing Power per capita, Social Progress Index (SPI) and Corruption Perceptions Index. Specifically, the research tests and quantitatively confirms a long-standing, yet unverified, hypothesis: that variability in cities' performance on MSW can be accounted for by socioeconomic development indices. The results provide a baseline for measuring progress as cities report MSW performance for the sustainable development goal SDG11.6.1 indicator: median rates of controlled recovery and disposal are approximately at 45 % for cities in low-income countries, 75 % in lower-middle, and 100 % for both upper-middle and high-income. Casting light on aspects beyond the SDG metric, on the quality of MSW-related services, show that improvements in service quality often lag improvements in service coverage. Overall, the findings suggest that progress in collection coverage, and controlled recovery and disposal has already taken place in low- and middle-income cities. However, if cities aspire to perform better on MSW management than would have been anticipated by the average socioeconomic development in their country, they should identify ways to overcome systemic underlying failures associated with that socioeconomic level. Most alarmingly, ‘business as usual’ development would substantially increase their waste generation per capita unless new policies are found to promote decoupling.",
        "DOI": "10.1016/j.scitotenv.2023.161913",
        "affiliation_name": "Imperial College London",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Twitter sentiment analysis using support vector machine and deep learning model in e-learning implementation during the Covid-19 outbreak",
        "paper_author": "Kristiyanti D.A.",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2023-05-09",
        "Abstract": "Investigating the effectiveness of using e-learning during the Covid-19 outbreak around the world is very interesting. This can be done by mining public opinion data on the application of e-learning during the Covid-19 outbreak. Twitter Sentiment Analysis is one techniques that can be used by classifying tweet data related to public opinion and classifying it into positive and negative sentiments, with the aim of seeing how public sentiment is related to the application of e-learning to help the government in taking a policy. The stages of research carried out in this study include Data Collection, Data Pre-processing (Tokenizing, Trans-form Case, Stopword Filter, Generate N-Gram and Stemming), Use of Models or Methods such as Support Vector Machine (SVM) algorithm and Deep Learning models, Experiment and Model Assessment using Rapid Miner version 9.9, and Evaluation and Validation Results using Confusion Matrix and ROC Curves. Based on 444 tweet data in English with the keywords #elearningcovid19, #elearning and #covid19, the results of the accuracy and AUC values of the SVM algorithm were superior to the Deep Learning model, namely 90,53% and 87,16%, as well as the AUC value to 0,953 and 0,928. Based on the research results, it turns out that more people in the world agree with the application of e-learning during the Covid-19 outbreak.",
        "DOI": "10.1063/5.0128685",
        "affiliation_name": "Universitas Multimedia Nusantara",
        "affiliation_city": "Tangerang",
        "affiliation_country": "Indonesia"
    },
    {
        "paper_title": "Sentiment analysis of Google Meet and zoom application user reviews on online learning in the Covid-19 period using algorithms support vector machine and Naive Bayes based on particle sarm optimization",
        "paper_author": "Noviriandini A.",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2023-05-09",
        "Abstract": "Indonesia is the world's fourth-largest country that is predicted to experience a significant impact due to the Covid-19 pandemic. Jakarta as the capital of Indonesia with a very densely populated urban area that has felt the immediate consequences of the Covid-19 epidemic. The DKI Jakarta issued by the government a PSBB policy and closed all sectors of educational institutions. Educational institutions are forced to switch from conventional learning methods to online learning methods by using several alternative applications, including Zoom, Google Meet, Google Classroom and other online platforms. From the background previously described, the researcher will conduct a sentiment analysis on user reviews of the Google Meet and Zoom applications on online learning during the Covid-19 pandemic Support Vector Machine and the Nave Bayes method, which is based on Particle Sarm Optimization, were used. The accuracy of the Google Meet application based on the PSO method, employs the SVM algorithm, the value of precision =71.84% as well as the AUC=0.837, whereas the Zoom application uses based on the PSO Naive Bayes algorithm the value of precision=78.75% as well as the AUC=0.664. As a result, the use of a Support Vector Machine (SVM) is a machine that is based on the proposed approach in this study has a better accuracy, allowing it to be utilized to solve issues with sentiment analysis in the users' feedback of online learning applications.",
        "DOI": "10.1063/5.0128579",
        "affiliation_name": "Universitas Bina Sarana Informatika",
        "affiliation_city": "Jakarta",
        "affiliation_country": "Indonesia"
    },
    {
        "paper_title": "Towards Efficient Personalized Driver Behavior Modeling with Machine Unlearning",
        "paper_author": "Song Q.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2023-05-09",
        "Abstract": "Driver Behavior Modeling (DBM) aims to predict and model human driving behaviors, which is typically incorporated into the Advanced Driver Assistance System to enhance transportation safety and improve driving experience. Inverse reinforcement learning (IRL) is a prevailing DBM technique with the goal of modeling the driving policy by recovering an unknown internal reward function from human driver demonstrations. However, the latest IRL-based design is inefficient due to the laborious manual feature engineering processes. Besides, the reward function usually experiences increased prediction errors when deployed for unseen vehicles. In this paper, we propose a novel deep learning-based reward function for IRL-based DBM with efficient model personalization via machine unlearning. We evaluate our approach on a highway simulation constructed using the realistic human driving dataset NGSIM. We deploy our approach on both a server GPU and an embedded GPU. The evaluation results show that our approach achieves a higher prediction accuracy compared with the latest IRL-based DBM approach that uses a weighted sum of trajectory features as the reward function. Our model personalization method obtains the highest accuracy and lowest latency compared with the baselines.",
        "DOI": "10.1145/3576914.3587489",
        "affiliation_name": "City University of Hong Kong",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong"
    },
    {
        "paper_title": "Dělen: Enabling Flexible and Adaptive Model-serving for Multi-tenant Edge AI",
        "paper_author": "Liang Q.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "5",
        "cover_date": "2023-05-09",
        "Abstract": "Model-serving systems expose machine learning (ML) models to applications programmatically via a high-level API. Cloud platforms use these systems to mask the complexities of optimally managing resources and servicing inference requests across multiple applications. Model serving at the edge is now also becoming increasingly important to support inference workloads with tight latency requirements. However, edge model serving differs substantially from cloud model serving in its latency, energy, and accuracy constraints: these systems must support multiple applications with widely different latency and accuracy requirements on embedded edge accelerators with limited computational and energy resources. To address the problem, this paper presents Dělen,1 a flexible and adaptive model-serving system for multi-tenant edge AI. Dělen exposes a high-level API that enables individual edge applications to specify a bound at runtime on the latency, accuracy, or energy of their inference requests. We efficiently implement Dělen using conditional execution in multi-exit deep neural networks (DNNs), which enables granular control over inference requests, and evaluate it on a resource-constrained Jetson Nano edge accelerator. We evaluate Dělen flexibility by implementing state-of-the-art adaptation policies using Dělen's API, and evaluate its adaptability under different workload dynamics and goals when running single and multiple applications.",
        "DOI": "10.1145/3576842.3582375",
        "affiliation_name": "University of Massachusetts Amherst",
        "affiliation_city": "Amherst",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "SiloD: A Co-design of Caching and Scheduling for Deep Learning Clusters",
        "paper_author": "Zhao H.",
        "publication": "Proceedings of the 18th European Conference on Computer Systems, EuroSys 2023",
        "citied_by": "9",
        "cover_date": "2023-05-08",
        "Abstract": "Deep learning training on cloud platforms usually follows the tradition of the separation of storage and computing. The training executes on a compute cluster equipped with GPUs/TPUs while reading data from a separate cluster hosting the storage service. To alleviate the potential bottleneck, a training cluster usually leverages its local storage as a cache to reduce the remote IO from the storage cluster. However, existing deep learning schedulers do not manage storage resources thus fail to consider the diverse caching effects across different training jobs. This could degrade scheduling quality significantly. To address this issue, we present SiloD, a scheduling framework that co-designs the cluster scheduler and the cache subsystems for deep learning training. SiloD treats cache and remote IO as first-class resources and can integrate different state-of-the-art deep learning scheduling policies in a unified scheduling framework. To achieve this, SiloD develops an enhanced job performance estimator to help different schedulers to jointly consider the impact of storage and compute resource allocation while preserving their respective scheduling objectives. The SiloD-enhanced performance estimator leverages the unique data access pattern of deep learning training to develop a closed-form analytic model that captures the diverse cache / remote IO requirements from different training jobs. Evaluations show that SiloD improves the average job completion time, cluster utilization, and fairness by up to 7.4x, 2.57x, and 1.89x, respectively, compared to different combinations of cache systems and cluster schedulers where they operate independently.",
        "DOI": "10.1145/3552326.3567499",
        "affiliation_name": "Microsoft Corporation",
        "affiliation_city": "Redmond",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Conceptual provisions for ensuring balanced development of railway transport enterprises under the conditions of implementation of digital changes in the industry",
        "paper_author": "Dykan V.",
        "publication": "Innovative development of the road and transport complex: Problems and prospects",
        "citied_by": "1",
        "cover_date": "2023-05-07",
        "Abstract": "Today, the world is experiencing an era of radical changes caused by the rapid introduction of digital technologies into all types of economic and social activities. Digital technologies change not only individual sectors of the economy, but also transform the entire system of global economic relations, creating enormous opportunities for sustainable business development and increasing the competitiveness of countries. The technologies of big data, machine learning, virtual and augmented reality, and robotics shape the leadership potential of national economies in the global system of highly effective digital communications and determine their future place on the geo-economic map of the world. Aware of the importance of digital technologies for the development of new horizons of activity in sectors strategic for the economy and ensuring their inclusive development, already today the leading states are implementing a large-scale policy of digital transformation of the country, focused on restructuring business processes, competencies and models of operation of industry, transport, trade, system public administration and education. As mentioned in the previous sections of this monograph, the key driver for the digital transformation of Ukraine's economy is the process of digitalization of infrastructure industries and, above all, domestic railway transport, the activity of which creates a multiplier effect, which is manifested in the ability not only to satisfy the needs of business entities in transportation, but also to stimulate the growth of economic activity in related industries by forming demand for products of metallurgy, instrument and wagon building, chemical and fuel and energy complexes.",
        "DOI": "10.15587/978-617-7319-71-8.CH9",
        "affiliation_name": "Ukrainian State University of Railway Transport",
        "affiliation_city": "Kharkiv",
        "affiliation_country": "Ukraine"
    },
    {
        "paper_title": "Prediction of medical sciences students' performance on high-stakes examinations using machine learning models: A protocol for a systematic review",
        "paper_author": "Mastour H.",
        "publication": "BMJ Open",
        "citied_by": "1",
        "cover_date": "2023-05-04",
        "Abstract": "Introduction Predicting medical science students' performance on high-stakes examinations has received considerable attention. Machine learning (ML) models are well-known approaches to enhance the accuracy of determining the students' performance. Accordingly, we aim to provide a comprehensive framework and systematic review protocol for applying ML in predicting medical science students' performance on high-stakes examinations. Improving the current understanding of the input and output features, preprocessing methods, setting of ML models and required evaluation metrics seems essential. Methods and analysis A systematic review will be conducted by searching the electronic bibliographic databases of MEDLINE/PubMed, EMBASE, SCOPUS and Web of Science. The search will be limited to studies published from January 2013 to June 2023. Studies explicitly predicting student performance in high-stakes examinations and referencing their learning outcomes and use of ML models will be included. Two team members will first screen literature meeting the inclusion criteria at the title, abstract and full-text levels. Second, the Best Evidence Medical Education quality framework rates the included literature. Later, two team members will extract data, including the studies' general data and the ML approach's details. Finally, the information consensus will be reached and submitted for analysis. The synthesised evidence from this review provides helpful information for medical education policy-makers, stakeholders and other researchers in adopting the ML models to evaluate medical science students' performance in high-stakes exams. Ethics and dissemination This systematic review protocol summarises findings of existing publications rather than primary data and does not require an ethics review. The results will be disseminated in publications of peer-reviewed journals.",
        "DOI": "10.1136/bmjopen-2022-064956",
        "affiliation_name": "Pharmaceutical Research Center",
        "affiliation_city": "Mashhad",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "Identification of urban-rural integration types in China – an unsupervised machine learning approach",
        "paper_author": "Zeng Q.",
        "publication": "China Agricultural Economic Review",
        "citied_by": "13",
        "cover_date": "2023-05-02",
        "Abstract": "Purpose: Development of urban-rural integration is essential to fulfill sustainable development goals worldwide, and comprehension about urban-rural integration types has been highlighted as increasingly relevant for an efficient policy design. This paper aims to utilize an unsupervised machine learning approach to identify urban-rural integration typologies based on multidimensional metrics regarding economic, population and social integration in China. Design/methodology/approach: The study introduces partitioning around medoids (PAM) for the identification of urban-rural integration typologies. PAM is a powerful tool for clustering multidimensional data. It identifies clusters by the representative objects called medoids and can be used with arbitrary distance, which help make clustering results more stable and less susceptible to outliers. Findings: The study identifies four clusters: high-level urban-rural integration, urban-rural integration in transition, low-level urban-rural integration and early urban-rural integration in backward stage, showing different characteristics. Based on the clustering results, the study finds continuous improvement in urban-rural integration development in China which is reflected by the changes in the predominate type. However, the development still presents significant regional disparities which is characterized by leading in the east regions and lagging in the western and central regions. Besides, achievement in urban-rural integration varies significantly across provinces. Practical implications: The machine learning techniques could identify urban-rural integration typologies in a multidimensional and objective way, and help formulate and implement targeted strategies and regionally adapted policies to boost urban-rural integration. Originality/value: This is the first paper to use an unsupervised machine learning approach with PAM for the identification of urban-rural integration typologies from a multidimensional perspective. The authors confirm the advantages of this machine learning techniques in identifying urban-rural integration types, compared to a single indicator.",
        "DOI": "10.1108/CAER-03-2022-0045",
        "affiliation_name": "Zhejiang Agriculture and Forestry University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Food price dynamics and regional clusters: machine learning analysis of egg prices in China",
        "paper_author": "Liu C.",
        "publication": "China Agricultural Economic Review",
        "citied_by": "4",
        "cover_date": "2023-05-02",
        "Abstract": "Purpose: The study uses machine learning techniques to cluster regional retail egg prices after 2000 in China. Furthermore, it combines machine learning results with econometric models to study determinants of cluster affiliation. Eggs are an inexpensiv, nutritious and sustainable animal food. Contextually, China is the largest country in the world in terms of both egg production and consumption. Regional clustering can help governments to imporve the precision of price policies and help producers make better investment decisions. The results are purely driven by data. Design/methodology/approach: The study introduces dynamic time warping (DTW) algorithm which takes into account time series properties to analyze provincial egg prices in China. The results are compared with several other algorithms, such as TADPole. DTW is superior, though it is computationally expensive. After the clustering, a multinomial logit model is run to study the determinants of cluster affiliation. Findings: The study identified three clusters. The first cluster including 12 provinces and the second cluster including 2 provinces are the main egg production provinces and their neighboring provinces in China. The third cluster is mainly egg importing regions. Clusters 1 and 2 have higher price volatility. The authors confirm that due to transaction costs, the importing areas may have less price volatility. Practical implications: The machine learning techniques could help governments make more precise policies and help producers make better investment decisions. Originality/value: This is the first paper to use machine learning techniques to cluster food prices. It also combines machine learning and econometric models to better study price dynamics.",
        "DOI": "10.1108/CAER-01-2022-0003",
        "affiliation_name": "Ministry of Agriculture of the People's Republic of China",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Comparison of machine learning predictions of subjective poverty in rural China",
        "paper_author": "Maruejols L.",
        "publication": "China Agricultural Economic Review",
        "citied_by": "10",
        "cover_date": "2023-05-02",
        "Abstract": "Purpose: Despite rising incomes and reduction of extreme poverty, the feeling of being poor remains widespread. Support programs can improve well-being, but they first require identifying who are the households that judge their income is insufficient to meet their basic needs, and what factors are associated with subjective poverty. Design/methodology/approach: Households report the income level they judge is sufficient to make ends meet. Then, they are classified as being subjectively poor if their own monetary income is inferior to the level they indicated. Second, the study compares the performance of three machine learning algorithms, the random forest, support vector machines and least absolute shrinkage and selection operator (LASSO) regression, applied to a set of socioeconomic variables to predict subjective poverty status. Findings: The random forest generates 85.29% of correct predictions using a range of income and non-income predictors, closely followed by the other two techniques. For the middle-income group, the LASSO regression outperforms random forest. Subjective poverty is mostly associated with monetary income for low-income households. However, a combination of low income, low endowment (land, consumption assets) and unusual large expenditure (medical, gifts) constitutes the key predictors of feeling poor for the middle-income households. Practical implications: To reduce the feeling of poverty, policy intervention should continue to focus on increasing incomes. However, improvements in nonincome domains such as health expenditure, education and family demographics can also relieve the feeling of income inadequacy. Methodologically, better performance of either algorithm depends on the data at hand. Originality/value: For the first time, the authors show that prediction techniques are reliable to identify subjective poverty prevalence, with example from rural China. The analysis offers specific attention to the modest-income households, who may feel poor but not be identified as such by objective poverty lines, and is relevant when policy-makers seek to address the “next step” after ending extreme poverty. Prediction performance and mechanisms for three machine learning algorithms are compared.",
        "DOI": "10.1108/CAER-03-2022-0051",
        "affiliation_name": "Southwest University",
        "affiliation_city": "Chongqing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Coalitions in international relations and coordination of agricultural trade policies",
        "paper_author": "Mao R.",
        "publication": "China Agricultural Economic Review",
        "citied_by": "2",
        "cover_date": "2023-05-02",
        "Abstract": "Purpose: The author attempts to examine the existence and pattern of coalitions in international relations across countries, and investigates whether international relations of coalition partners influence a country's enaction of agricultural non-tariff measures (NTMs). Design/methodology/approach: The author adopts a machine learning technique to identify international relation coalition partnerships and use network analysis to characterize the clustering pattern of coalitions with high-frequent records of global event data. The author then constructs a monthly dataset of agricultural NTMs against China and international relations with China of each importer and its coalition partners, and designs a panel structural vector autoregressive (PSVAR) model to estimate impulse response functions of agricultural NTMs with regard to international relation shocks. Findings: The author finds countries to establish coalition partnerships. Two major clusters of coalitions are noted, with one composed of coalitions primarily among “North” countries and the other of coalitions among “South” countries. The United States is found to play a pivotal role by connecting the two clusters. The PSVAR estimation reveals reductions of NTMs against China following improved international relations with China of both the importer and its coalition partners. NTM responses are more substantial for measures that are trade restrictive. These results confirm that coalitions in international relations lead to coordination of agricultural NTMs. Originality/value: The author provides international political insights into agricultural trade policymaking by showing interactions of NTM enaction across countries in the same coalition of international relations. These insights offer useful policy implications to predict and cope with hidden barriers to agricultural trade.",
        "DOI": "10.1108/CAER-01-2022-0011",
        "affiliation_name": "China Academy for Rural Development, Zhejiang University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Quantitative research in entrepreneurship using the R software for data analysis",
        "paper_author": "Pagotto D.D.P.",
        "publication": "REGEPE Entrepreneurship and Small Business Journal",
        "citied_by": "1",
        "cover_date": "2023-05-01",
        "Abstract": "Objective of the study: this editorial aims to present an overview of Brazilian quantitative research in entrepreneurship, as well as describing possibilities for advancing this methodological approach. Methodology and approach: the article consists of an editorial publication, built from bibliographic research of entrepreneurship literature and theoretical reflections. Main Results: Most national entrepreneurship research follows a qualitative approach. Despite its relevance, quantitative research also has multiple potentialities, especially associated with the use of data originating from secondary sources. Main theoretical and methodological contributions: We present public databases that can be used by entrepreneurship researchers to advance theory. Some strategies for using these bases are exemplified through a brief tutorial in R language. We further debate about strategies to strengthen quantitative research in the area. Finally, we bring a research agenda. Relevance/ Originality: contents that are still little explored in the national literature are presented, such as the use of secondary data and machine learning. Social and managerial contributions: some of the databases presented in the study come from government sources and can be used to support the construction of public policies for entrepreneurship. In addition, the precepts on quantitative research presented in this editorial can support managers who work with data analysis to perform more robust studies, regardless of the area, whether practical or academic.",
        "DOI": "10.14211/regepe.esbj.e2257",
        "affiliation_name": "Universidade Federal de Goiás",
        "affiliation_city": "Goiania",
        "affiliation_country": "Brazil"
    },
    {
        "paper_title": "Analyzing Evolution of Basic Research Funding Orientation: Case Study of NSF",
        "paper_author": "Wei H.",
        "publication": "Data Analysis and Knowledge Discovery",
        "citied_by": "0",
        "cover_date": "2023-05-01",
        "Abstract": "[Objective] This paper identifies and analyzes the funding orientation of basic research projects funded in the United States, aiming to provide suggestions for improving the funding layout of science funds in China. [Methods] Based on the literature review, we established a feature system for identifying funding orientation from four dimensions: basic information, collaborative characteristics, project characteristics, and output characteristics. Then, we constructed a recognition model with the help of machine learning. Finally, we conducted the corresponding evolution analysis. [Results] The SVM model with an RBF kernel had a better identification effect. The case analysis of synthetic biology showed that the NSF balanced“free exploration”and“demand-oriented”. The basic research of“free exploration”was consistent throughout. In contrast, the basic research of“demand-oriented”was relatively scarce in the early stages, gradually increasing with the development of the field. Changes in the two funding orientations are closely related to the development stage of the discipline and the national strategic policies. [Limitations] We only chose one field for case analysis, which lacked representativeness. We only included NSF project data and did not include NIH, FDA, and other data, so the comprehensiveness of the data source needs to be strengthened. [Conclusions] This study is a valuable exploration of identifying basic research funding orientation. By identifying and analyzing the funding orientation of NSF projects in synthetic biology, this study can provide suggestions for the funding layout of NSFC in China and promote the coordinated development of basic research in China.",
        "DOI": "10.11925/infotech.2096-3467.2022.0627",
        "affiliation_name": "Beijing Institute of Technology",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Developing policy-making for maximizing the water productivity in agricultural lands",
        "paper_author": "Fang J.",
        "publication": "Water Supply",
        "citied_by": "1",
        "cover_date": "2023-05-01",
        "Abstract": "Soil moisture content improvement is a key process in agricultural production and food security in arid and semi-arid regions. The interaction effect of crop water requirement (CWR) and soil texture on water productivity was evaluated in Harbin, Heilongjiang province, China. A field experiment with two scenarios of compost application (0, 25 ton/ha) and three levels of irrigation policies (deficit irrigation (DI ¼ 0.75CWR), regular irrigation (RI ¼ CWR), and full irrigation (FI ¼ 1.25CWR)) was planned in three replications during 2021–2022. Water productivity was simulated as a criterion to improve the irrigation time for increasing the final biomass. Four strategic crops including potato, corn, wheat, and barley were incorporated into the daily simulation system. Furthermore, a machine-learning random forest algorithm was used to find the best irrigation times. The results showed that the use of adjusted irrigation time and improved soil texture can increase water productivity by reducing evaporation and deep percolation and increasing actual biomass. Estimation of irrigation time in a learning-based method will be optimal when plant growth and soil moisture are monitored on a daily basis.",
        "DOI": "10.2166/ws.2023.091",
        "affiliation_name": "Chongqing University of Science and Technology",
        "affiliation_city": "Chongqing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Impact effect-based grey multivariable time delay model and its application",
        "paper_author": "Ye L.",
        "publication": "Xitong Gongcheng Lilun yu Shijian/System Engineering Theory and Practice",
        "citied_by": "4",
        "cover_date": "2023-05-01",
        "Abstract": "Considering the impact phenomenon in the economic and social system, the impact effect is introduced into the modeling framework. First, the impact effect mechanism and its lagging accumulation mechanism are analyzed. On this basis, we introduce an impact effect item to describe the impact effect, and design the accumulative delay utility term based on Beta function to represent the lagging cumulative impact of impact factors on the system. Then, an impact effect-based grey multivariable time delay model (IEGTDM (1,N)) is constructed. A solution framework based on the whale algorithm is proposed for the parameter solution of the accumulative delay utility term. Finally, we use the energy intensity of Beijing and Shanghai under the impact of carbon trading policy to carry out empirical case analysis. The comparison with other grey prediction models, statistical prediction models and machine learning models shows that IEGTDM(1,N) model possesses better prediction performance, and the proposed model has a strong adaptability to the prediction modeling analysis under the impact effect.",
        "DOI": "10.12011/SETP2022-2405",
        "affiliation_name": "Toronto Metropolitan University",
        "affiliation_city": "Toronto",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Strengthening the scientific base of traditional medicine through international collaboration and partnerships",
        "paper_author": "Chang D.",
        "publication": "Journal of Ayurveda and Integrative Medicine",
        "citied_by": "1",
        "cover_date": "2023-05-01",
        "Abstract": "NA",
        "DOI": "10.1016/j.jaim.2023.100747",
        "affiliation_name": "NICM Health Research Institute",
        "affiliation_city": "Sydney",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Evaluating the impact of water protection policy on urban growth: A case study of Jiaxing",
        "paper_author": "Guan C.H.",
        "publication": "Environment and Planning B: Urban Analytics and City Science",
        "citied_by": "4",
        "cover_date": "2023-05-01",
        "Abstract": "Source water protection can be a greater challenge in cities where the hydric resource is deeply embedded within a rapidly growing urban area. In these types of cities, the delimitation of protection areas along water resources, as one of the main mechanisms for water resource protection, has a direct impact on the way the urban form evolves. On the one hand, narrow protection areas may not be enough to guarantee that the built city does not affect water levels and quality. On the other hand, wide protection areas can result in a fragmented city, with low levels of accessibility and tendencies towards dispersed and disorderly growth. In this article we use the city of Jiaxing, China, as a case study to determine what the optimal size of water protection areas might be. For this, we use two urban growth models and simulate various urban growth scenarios between 2020 and 2040. The results indicate that a protection area of 400 m guarantees a good level of protection of the water resource and a sufficient availability of land that allows an efficient urban growth.",
        "DOI": "10.1177/23998083231163182",
        "affiliation_name": "Indian Institute for Human Settlements",
        "affiliation_city": "Bengaluru",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Application of a deep learning image classifier for identification of Amazonian fishes",
        "paper_author": "Robillard A.J.",
        "publication": "Ecology and Evolution",
        "citied_by": "7",
        "cover_date": "2023-05-01",
        "Abstract": "Given the sharp increase in agricultural and infrastructure development and the paucity of widespread data available to support conservation management decisions, a more rapid and accurate tool for identifying fish fauna in the world's largest freshwater ecosystem, the Amazon, is needed. Current strategies for identification of freshwater fishes require high levels of training and taxonomic expertise for morphological identification or genetic testing for species recognition at a molecular level. To overcome these challenges, we built an image masking model (U-Net) and a convolutional neural net (CNN) to classify Amazonian fish in photographs. Fish used to generate training data were collected and photographed in tributaries in seasonally flooded forests of the upper Morona River valley in Loreto, Peru in 2018 and 2019. Species identifications in the training images (n = 3068) were verified by expert ichthyologists. These images were supplemented with photographs taken of additional Amazonian fish specimens housed in the ichthyological collection of the Smithsonian's National Museum of Natural History. We generated a CNN model that identified 33 genera of fishes with a mean accuracy of 97.9%. Wider availability of accurate freshwater fish image recognition tools, such as the one described here, will enable fishermen, local communities, and citizen scientists to more effectively participate in collecting and sharing data from their territories to inform policy and management decisions that impact them directly.",
        "DOI": "10.1002/ece3.9987",
        "affiliation_name": "Instituto de Investigaciones de la Amazonía Peruana",
        "affiliation_city": "Iquitos",
        "affiliation_country": "Peru"
    },
    {
        "paper_title": "Distinguishing Household Groupings within a Precinct Based on Energy Usage Patterns Using Machine Learning Analysis",
        "paper_author": "Malatesta T.",
        "publication": "Energies",
        "citied_by": "4",
        "cover_date": "2023-05-01",
        "Abstract": "The home can be a complex environment to understand, as well as to model and predict, due to inherent variability between people’s routines and practices. A one-size-fits-all approach does not consider people’s contextual and institutional influences that contribute to their daily routines. These contextual and institutional factors relate to the household structure and relationship between occupants, as well as the working lifestyle of the occupants. One household can consume resources and live quite differently compared to a similar size household with the same number of occupants due to these factors. Predictive analysis of consumption data can identify this difference to create household-specific modelling to predict occupant routines and practices. Using post-occupancy data from the Fairwater Living Laboratory in Sydney that monitored 39 homes built in a green-star community, this research has utilised machine learning approaches and a K-Means clustering method complemented by t-distributed Stochastic Neighbour Embedding (t-SNE) to show how households follow different daily routines and activities resulting in resource consumption. This analysis has identified energy usage patterns and household groupings with each group following similar daily routines and consumption. The comparison between modelling the precinct as a whole and modelling households individually shows how detail can be lost when aggregating household data at a precinct/community level. This detail can explain why policies or technologies are not as effective as their design due to ignoring the delicate aspects of household routines and practices. These household groupings can provide insight for policymakers to help them understand the different profiles that may be present in the community. These findings are useful for net-zero developments and decarbonization of the built environment through modelling occupant behaviour accurately and developing policies and technologies to suit.",
        "DOI": "10.3390/en16104119",
        "affiliation_name": "Curtin University",
        "affiliation_city": "Perth",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Advancing High-Resolution Land Cover Mapping in Colombia: The Importance of a Locally Appropriate Legend",
        "paper_author": "Fagua J.C.",
        "publication": "Remote Sensing",
        "citied_by": "4",
        "cover_date": "2023-05-01",
        "Abstract": "Improving the remote sensing frameworks related to land cover mapping is necessary to make informed policy, development, planning, and natural resource management decisions. These efforts are especially important in tropical countries where technical capacity is limited. Land cover legend specification is a critical first step when mapping land cover, with consequences for its subsequent use and interpretation of results. We integrated the temporal metrics of SAR (Synthetic Aperture Radar) and multispectral data (Sentinel-1 and Sentienel-2) with visual pixel classifications and field surveys using five machine learning algorithms that apply different statistical methods to assess the prediction and mapping of two different land cover legends at a high spatial resolution (10 m) in a tropical region with seasonal flooding. The evaluated legends were CORINE (Coordination of Information on the Environment) and ECOSO, a legend that we defined based on the ecological and socio-economic conditions of the study area. Compared with previous studies, we obtained high accuracies for land cover modeling (kappa = 0.82) and land cover mapping (kappa = 0.76) when using ECOSO. We also found that the CORINE legend generated lower accuracies than the ECOSO legend (kappa = 0.79 for land cover modeling and kappa = 0.61 for the land cover mapping). Although CORINE was developed for European environments, it is the official land cover legend of Colombia, a South American country with tropical ecosystems not found in Europe. Therefore, some of the CORINE classes have ambiguous definitions for the study area, explaining the lower accuracy of its modeling and mapping. We used free and open-access data and software in this research; thus, our methods can be applied in other tropical regions.",
        "DOI": "10.3390/rs15102522",
        "affiliation_name": "Instituto de Investigación de Recursos Biológicos Alexander von Humboldt, Bogota",
        "affiliation_city": "Bogota",
        "affiliation_country": "Colombia"
    },
    {
        "paper_title": "A Deep Reinforcement Learning Design for Virtual Synchronous Generators Accommodating Modular Multilevel Converters",
        "paper_author": "Yang M.",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "5",
        "cover_date": "2023-05-01",
        "Abstract": "The deep reinforcement learning (DRL) technique has gained attention for its potential in designing “virtual network” controllers. This skill utilizes a novel solution that can avoid the specific parameters and system model required in classical dynamic programming algorithms. However, addressing the issue of system uncertainties and performance deterioration remains a challenge. To overcome this challenge, the authors propose a new control prototype using a twin delayed deep deterministic policy gradient (TD3)-based adaptive controller, which replaces the conventional virtual synchronous generator (VSG) module in the modular multilevel converter (MMC) control. In this approach, an adaptive programming module is developed using a critic fuzzy network point of view to determine the optimal control policy. The modification presented in this framework is able to improve the system stability and resist disruptions while retaining the merits of the conventional VSG control model. The proposed approach is implemented and tested using the DRL toolbox in MATLAB/Simulink.",
        "DOI": "10.3390/app13105879",
        "affiliation_name": "China University of Mining and Technology",
        "affiliation_city": "Xuzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Using Genetic Programming to Identify Characteristics of Brazilian Regions in Relation to Rural Credit Allocation",
        "paper_author": "Araújo A.V.",
        "publication": "Agriculture (Switzerland)",
        "citied_by": "1",
        "cover_date": "2023-05-01",
        "Abstract": "Rural credit policies have a strong impact on food production and food security. The attribution of credit policies to agricultural production is one of the main problems preventing the guarantee of agricultural expansion. In this work, we conduct family typology analysis applied to a set of research data to characterize different regions. Through genetic programming, a model was developed using user-defined terms to identify the importance and priority of each criterion used for each region. Access to credit results in economic growth and provides greater income for family farmers, as observed by the results obtained in the model for the Sul region. The Nordeste region indicates that the cost criterion is relevant, and according to previous studies, the Nordeste region has the highest number of family farming households and is also the region with the lowest economic growth. An important aspect discovered by this research is that the allocation of rural credit is not ideal. Another important aspect of the research is the challenge of capturing the degree of diversity across different regions, and the typology is limited in its ability to accurately represent all variations. Therefore, it was possible to characterize how credit is distributed across the country and the main factors that can influence access to credit.",
        "DOI": "10.3390/agriculture13050935",
        "affiliation_name": "Leeds University Business School",
        "affiliation_city": "Leeds",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Predicting the Impact of Change in Air Quality Patterns Due to COVID-19 Lockdown Policies in Multiple Urban Cities of Henan: A Deep Learning Approach",
        "paper_author": "Bhatti M.A.",
        "publication": "Atmosphere",
        "citied_by": "6",
        "cover_date": "2023-05-01",
        "Abstract": "Several countries implemented prevention and control measures in response to the 2019 new coronavirus virus (COVID-19) pandemic. To study the impact of the lockdown due to COVID-19 on multiple cities, this study utilized data from 18 cities of Henan to understand the air quality pattern change during COVID-19 from 2019 to 2021. It examined the temporal and spatial distribution impact. This study firstly utilized a deep learning bi-directional long-term short-term (Bi-LSTM) model to predict air quality patterns during 3 periods, i.e., COVID-A (before COVID-19, i.e., 2019), COVID-B (during COVID-19, i.e., 2020), COVID-C (after COVID-19 cases, i.e., 2021) and obtained the R2 value of more than 72% average in each year and decreased MAE value, which was better than other studies’ deep learning methods. This study secondly focused on the change of pollutants and observed an increase in Air Quality Index by 10%, a decrease in PM2.5 by 14%, PM10 by 18%, NO2 by 14%, and SO2 by 16% during the COVID-B period. This study found an increase in O3 by 31% during the COVID-C period and observed a significant decrease in pollutants during the COVID-C period (PM10 by 42%, PM2.5 by 97%, NO2 by 89%, SO2 by 36%, CO by 58%, O3 by 31%). Lastly, the impact of lockdown policies was studied during the COVID-B period and the results showed that Henan achieved the Grade I standards of air quality standards after lockdown was implemented. Although there were many severe effects of the COVID-19 pandemic on human health and the global economy, lockdowns likely resulted in significant short-term health advantages owing to reduced air pollution and significantly improved ambient air quality. Following COVID-19, the government must take action to address the environmental problems that contributed to the deteriorating air quality.",
        "DOI": "10.3390/atmos14050902",
        "affiliation_name": "Nanjing Normal University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Artificial Intelligence Techniques in Hydrology and Water Resources Management",
        "paper_author": "Chang F.J.",
        "publication": "Water (Switzerland)",
        "citied_by": "20",
        "cover_date": "2023-05-01",
        "Abstract": "The sustainable management of water cycles is crucial in the context of climate change and global warming. It involves managing global, regional, and local water cycles—as well as urban, agricultural, and industrial water cycles—to conserve water resources and their relationships with energy, food, microclimates, biodiversity, ecosystem functioning, and anthropogenic activities. Hydrological modeling is indispensable for achieving this goal, as it is essential for water resources management and mitigation of natural disasters. In recent decades, the application of artificial intelligence (AI) techniques in hydrology and water resources management has made notable advances. In the face of hydro-geo-meteorological uncertainty, AI approaches have proven to be powerful tools for accurately modeling complex, non-linear hydrological processes and effectively utilizing various digital and imaging data sources, such as ground gauges, remote sensing tools, and in situ Internet of Things (IoTs). The thirteen research papers published in this Special Issue make significant contributions to long- and short-term hydrological modeling and water resources management under changing environments using AI techniques coupled with various analytics tools. These contributions, which cover hydrological forecasting, microclimate control, and climate adaptation, can promote hydrology research and direct policy making toward sustainable and integrated water resources management.",
        "DOI": "10.3390/w15101846",
        "affiliation_name": "Tamkang University",
        "affiliation_city": "Taipei",
        "affiliation_country": "Taiwan"
    },
    {
        "paper_title": "PM<inf>2.5</inf> Concentration Prediction in Six Major Chinese Urban Agglomerations: A Comparative Study of Various Machine Learning Methods Based on Meteorological Data",
        "paper_author": "Duan M.",
        "publication": "Atmosphere",
        "citied_by": "3",
        "cover_date": "2023-05-01",
        "Abstract": "The escalating issue of air pollution in China’s rapidly developing urban areas has prompted increased attention to the role of meteorological conditions in PM2.5 pollution. This study examines the spatiotemporal distribution of PM2.5 concentrations and their relationship with meteorological factors in six major Chinese urban agglomerations from 2017 to 2020, using daily average data. Statistical and spatial analysis techniques are employed, alongside the construction of eight machine learning models for prediction purposes. The study also compares the feature importance of various meteorological factors impacting PM2.5 concentrations. Results reveal significant regional differences in both average PM2.5 levels and meteorological influences. The Multilayer Perceptron (MLP) model demonstrates the highest prediction accuracy for PM2.5 concentrations. According to the MLP model’s feature importance identification, temperature is the most significant factor affecting PM2.5 concentrations across all urban agglomerations, while wind speed and precipitation have the least impact. Contributions from air pressure and dew point temperature, however, vary among different urban agglomerations. This research considers the impact of urban agglomerations and meteorological conditions on PM2.5 and also offers valuable artificial intelligence-based insights into the key meteorological factors influencing PM2.5 concentrations in diverse regions, thereby informing the development of effective air pollution control policies.",
        "DOI": "10.3390/atmos14050903",
        "affiliation_name": "Nanjing Agricultural University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Kidney Health and Care: Current Status, Challenges, and Developments",
        "paper_author": "Lin M.Y.",
        "publication": "Journal of Personalized Medicine",
        "citied_by": "5",
        "cover_date": "2023-05-01",
        "Abstract": "The concept of chronic kidney disease (CKD) originated in the 2000s, and an estimated 850 million patients are currently suffering from health threats from different degrees of CKD. However, it is unclear whether the existing CKD care systems are optimal for improving patient prognosis and outcomes, so this review summarizes the burden, existing care models, effectiveness, challenges, and developments of CKD care. Even under the general care principles, there are still significant gaps in our understanding of the causes of CKD, prevention or care resources, and care burdens between countries worldwide. Receiving care from multidisciplinary teams rather than only a nephrologist shows potential profits in comprehensive and preferable outcomes. In addition, we propose a novel CKD care structure that combines modern technologies, biosensors, longitudinal data visualization, machine learning algorithms, and mobile care. The novel care structure could simultaneously change the care process, significantly reduce human contact, and make the vulnerable population less likely to be exposed to infectious diseases such as COVID-19. The information offered should be beneficial, allowing us to rethink future CKD care models and applications to reach the goals of health equality and sustainability.",
        "DOI": "10.3390/jpm13050702",
        "affiliation_name": "Kaohsiung Medical University Chung-Ho Memorial Hospital",
        "affiliation_city": "Kaohsiung",
        "affiliation_country": "Taiwan"
    },
    {
        "paper_title": "Applications of Machine Learning and Determinants of Dividend Decision: Evidence from Indian Firms",
        "paper_author": "Vodwal S.",
        "publication": "Indian Journal of Finance",
        "citied_by": "1",
        "cover_date": "2023-05-01",
        "Abstract": "Purpose: The theories of dividend decision have disentangled the firms’ critical drivers of the dividend announcement, and their performances are empirically evaluated by employing ordinary least squares (OLS). However, after more than half a century of research, the debate over the determinants of dividend policy in firms is inconclusive. Therefore, the current study attempted to contribute to the literature by exploring new insights into the dividend decisions of Indian firms by employing machine learning. Methodology: This study is based on secondary data, and empirical analysis has used a novel dataset of 919 listed Indian nonfinancial firms from 1999-2019. The study utilized the least absolute shrinkage and selection operator and logistic regression methodologies. Findings: The findings revealed that the idiosyncratic variables are critically significant for dividend announcements by Indian firms. The results demonstrated that large, profitable, liquid, and firms with high market share were more likely to announce dividends in India than small, loss-making, illiquid, and low-market share firms. The direct relationship between Tobin’s Q and the likelihood of paying dividends is a new insight into the dividend decision for Indian firms. Practical Implications: The results will guide the dividend seeker investors to hold the shares of a high market share firm to receive the expected dividend. Originality/Value: This current study extended the literature by studying the dividend decisions of Indian firms by employing the machine learning methodology.",
        "DOI": "10.17010/ijf/2023/v17i5/171154",
        "affiliation_name": "Keshav Mahavidyalaya",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Satellite-Based Long-Term Spatiotemporal Trends in Ambient NO<inf>2</inf> Concentrations and Attributable Health Burdens in China From 2005 to 2020",
        "paper_author": "Huang K.",
        "publication": "GeoHealth",
        "citied_by": "11",
        "cover_date": "2023-05-01",
        "Abstract": "Despite the recent development of using satellite remote sensing to predict surface NO2 levels in China, methods for estimating reliable historical NO2 exposure, especially before the establishment of NO2 monitoring network in 2013, are still rare. A gap-filling model was first adopted to impute the missing NO2 column densities from satellite, then an ensemble machine learning model incorporating three base learners was developed to estimate the spatiotemporal pattern of monthly mean NO2 concentrations at 0.05° spatial resolution from 2005 to 2020 in China. Further, we applied the exposure data set with epidemiologically derived exposure response relations to estimate the annual NO2 associated mortality burdens in China. The coverage of satellite NO2 column densities increased from 46.9% to 100% after gap-filling. The ensemble model predictions had good agreement with observations, and the sample-based, temporal and spatial cross-validation (CV) R2 were 0.88, 0.82, and 0.73, respectively. In addition, our model can provide accurate historical NO2 concentrations, with both by-year CV R2 and external separate year validation R2 achieving 0.80. The estimated national NO2 levels showed a increasing trend during 2005–2011, then decreased gradually until 2020, especially in 2012–2015. The estimated annual mortality burden attributable to long-term NO2 exposure ranged from 305 thousand to 416 thousand, and varied considerably across provinces in China. This satellite-based ensemble model could provide reliable long-term NO2 predictions at a high spatial resolution with complete coverage for environmental and epidemiological studies in China. Our results also highlighted the heavy disease burden by NO2 and call for more targeted policies to reduce the emission of nitrogen oxides in China.",
        "DOI": "10.1029/2023GH000798",
        "affiliation_name": "Chinese Academy of Medical Sciences &amp; Peking Union Medical College",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A Survey of Methods and Input Data Types for House Price Prediction",
        "paper_author": "Geerts M.",
        "publication": "ISPRS International Journal of Geo-Information",
        "citied_by": "14",
        "cover_date": "2023-05-01",
        "Abstract": "Predicting house prices is a challenging task that many researchers have attempted to address. As accurate house prices allow better informing parties in the real estate market, improving housing policies and real estate appraisal, a comprehensive overview of house price prediction strategies is valuable for both research and society. In this work, we present a systematic literature review in order to provide insights with regard to the data types and modeling approaches that have been utilized in the current body of research. As such, we identified 93 articles published between 1992 and 2021 presenting a particular technique for house price prediction. Subsequently, we scrutinized these works and scored them according to model and data novelty. A cluster analysis allowed mapping of the property valuation domain and identification of trends. Although conventional methods and traditional input data remain predominant, house price prediction research is slowly adopting more advanced techniques and innovative data sources. In addition, we identify opportunities to include more advanced input data types such as unstructured data and complex spatial data and to introduce deep learning and tailored methods, which could guide further research.",
        "DOI": "10.3390/ijgi12050200",
        "affiliation_name": "Universiteit Gent",
        "affiliation_city": "Ghent",
        "affiliation_country": "Belgium"
    },
    {
        "paper_title": "Value Addition Employing Waste Bio-Materials in Environmental Remedies and Food Sector",
        "paper_author": "Taneja A.",
        "publication": "Metabolites",
        "citied_by": "19",
        "cover_date": "2023-05-01",
        "Abstract": "Overall, combating food waste necessitates a multifaceted approach that includes education, infrastructure, and policy change. By working together to implement these strategies, we can help reduce the negative impacts of food waste and create a more sustainable and equitable food system. The sustained supply of nutrient-rich agrifood commodities is seriously threatened by inefficiencies caused by agricultural losses, which must be addressed. As per the statistical data given by the Food and Agriculture Organisation (FAO) of the United Nations, nearly 33.33% of the food that is produced for utilization is wasted and frittered away on a global level, which can be estimated as a loss of 1.3 billion metric tons per annum, which includes 30% cereals, 20% dairy products 35% seafood and fish, 45% fruits and vegetables, and 20% of meat. This review summarizes the various types of waste originating from various segments of the food industry, such as fruits and vegetables, dairy, marine, and brewery, also focusing on their potential for developing commercially available value-added products such as bioplastics, bio-fertilizers, food additives, antioxidants, antibiotics, biochar, organic acids, and enzymes. The paramount highlights include food waste valorization, which is a sustainable yet profitable alternative to waste management, and harnessing Machine Learning and Artificial Intelligence technology to minimize food waste. Detail of sustainability and feasibility of food waste-derived metabolic chemical compounds, along with the market outlook and recycling of food wastes, have been elucidated in this review.",
        "DOI": "10.3390/metabo13050624",
        "affiliation_name": "Saveetha Dental College And Hospitals",
        "affiliation_city": "Chennai",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Factors Associated with Primary Care Provider’s Job Satisfaction and Organizational Commitment in China: A Machine Learning-Based Random Forest Analysis",
        "paper_author": "Wang Q.",
        "publication": "Healthcare (Switzerland)",
        "citied_by": "1",
        "cover_date": "2023-05-01",
        "Abstract": "The objective of the study is to explore the factors that influence the job satisfaction and organizational commitment of primary care providers in China, with a focus on the impact of the COVID-19 pandemic and the rescission of restriction policies. We utilized the 20-item Minnesota Satisfaction Questionnaire (MSQ) and the 25-item organizational commitment survey to assess job satisfaction and organizational commitment. In total, 435 valid responses were included in our analysis. The average scores for job satisfaction and organizational commitment were 80.6 and 90.8. After a two-step tuning process, we built random forest models by machine learning. The results show income change, working years, working years in the current institute, and age were the four most important features associated with job satisfaction, organizational commitment, and most of their dimensions. The number of professional fields engaged, gender, job status, and types of endowment insurance were least associated. During pandemic time, income-related factors remain a core concern for primary care providers, whereas job security may lose its importance. These findings suggest that financial bonuses may be an effective way to boost morale, and age-specific motivation plans may be necessary.",
        "DOI": "10.3390/healthcare11101432",
        "affiliation_name": "Institute of Medical Information &amp; Library, Chinese Academy of Medical Sciences &amp; Peking Union Medical College",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Persistent mind: The effects of information provision on policy preferences",
        "paper_author": "Kawata K.",
        "publication": "Journal of Policy Modeling",
        "citied_by": "1",
        "cover_date": "2023-05-01",
        "Abstract": "In constitutional states, representatives exchange information, discuss the budget direction and agree on a budget. The agreement is assumed to hold for one fiscal year. To test the validity of this constitutional assumption, we implemented an online panel survey with a randomized conjoint design three times over one year in Japan to track the direction of respondents’ preferences within a multidimensional public policy space. The policy space consisted of spending on education, infrastructure, health insurance, pensions, and poverty relief programs, as well as fiscal retrenchment. Providing information on the poverty rate in the first wave directed respondents’ preferences toward support for poverty relief programs by either increasing or reallocating the budget. The effects persisted in the second wave 5 months later across a diverse range of respondent backgrounds and political positions. By the third wave one year later, the effects had diminished. Once placed in a multidimensional space, information exchange might have a more extended scope than unidimensional approaches have shown. This finding, we believe, can broaden our capability to implement policies for poverty reduction.",
        "DOI": "10.1016/j.jpolmod.2023.05.002",
        "affiliation_name": "The University of Tokyo",
        "affiliation_city": "Tokyo",
        "affiliation_country": "Japan"
    },
    {
        "paper_title": "A Machine Learning Approach Reveals Distinct Predictors of Vaping Dependence for Adolescent Daily and Non-Daily Vapers in the COVID-19 Era",
        "paper_author": "Singh I.",
        "publication": "Healthcare (Switzerland)",
        "citied_by": "2",
        "cover_date": "2023-05-01",
        "Abstract": "Since 2016, there has been a substantial rise in e-cigarette (vaping) dependence among young people. In this prospective cohort study, we aimed to identify the different predictors of vaping dependence over 3 months among adolescents who were baseline daily and non-daily vapers. We recruited ever-vaping Canadian residents aged 16–25 years on social media platforms and asked them to complete a baseline survey in November 2020. A validated vaping dependence score (0–23) summing up their responses to nine questions was calculated at the 3-month follow-up survey. Separate lasso regression models were developed to identify predictors of higher 3-month vaping dependence score among baseline daily and non-daily vapers. Of the 1172 participants, 643 (54.9%) were daily vapers with a mean age of 19.6 ± 2.6 years and 76.4% (n = 895) of them being female. The two models achieved adequate predictive performance. Place of last vape purchase, number of days a pod lasts, and the frequency of nicotine-containing vaping were the most important predictors for dependence among daily vapers, while race, sexual orientation and reporting treatment for heart disease were the most important predictors in non-daily vapers. These findings have implications for vaping control policies that target adolescents at different stages of vape use.",
        "DOI": "10.3390/healthcare11101465",
        "affiliation_name": "Children's Hospital of Eastern Ontario, Ottawa",
        "affiliation_city": "Ottawa",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "COVID-19 Media Chatter and Macroeconomic Reflectors on Black Swan: A Spanish and Indian Stock Markets Comparison",
        "paper_author": "Ghosh I.",
        "publication": "Risks",
        "citied_by": "5",
        "cover_date": "2023-05-01",
        "Abstract": "Predictive analytics of financial markets in developed and emerging economies during the COVID-19 regime is undeniably challenging due to unavoidable uncertainty and the profound proliferation of negative news on different platforms. Tracking the media echo is crucial to explaining and anticipating the abrupt fluctuations in financial markets. The present research attempts to propound a robust framework capable of channeling macroeconomic reflectors and essential media chatter-linked variables to draw precise forecasts of future figures for Spanish and Indian stock markets. The predictive structure combines Isometric Mapping (ISOMAP), which is a non-linear feature transformation tool, and Gradient Boosting Regression (GBR), which is an ensemble machine learning technique to perform predictive modelling. The Explainable Artificial Intelligence (XAI) is used to interpret the black-box type predictive model to infer meaningful insights. The overall results duly justify the incorporation of local and global media chatter indices in explaining the dynamics of respective financial markets. The findings imply marginally better predictability of Indian stock markets than their Spanish counterparts. The current work strives to compare and contrast the reaction of developed and developing financial markets during the COVID-19 pandemic, which has been argued to share a close resemblance to the Black Swan event when applying a robust research framework. The insights linked to the dependence of stock markets on macroeconomic indicators can be leveraged for policy formulations for augmenting household finance.",
        "DOI": "10.3390/risks11050094",
        "affiliation_name": "Institute of Management Technology, Hyderabad",
        "affiliation_city": "Hyderabad",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Informal Sector, ICT Dynamics, and the Sovereign Cost of Debt: A Machine Learning Approach",
        "paper_author": "Kotzinos A.",
        "publication": "Computation",
        "citied_by": "0",
        "cover_date": "2023-05-01",
        "Abstract": "We examine the main effects of ICT penetration and the shadow economy on sovereign credit ratings and the cost of debt, along with possible second-order effects between the two variables, on a dataset of 65 countries from 2001 to 2016. The paper presents a range of machine-learning approaches, including bagging, random forests, gradient-boosting machines, and recurrent neural networks. Furthermore, following recent trends in the emerging field of interpretable ML, based on model-agnostic methods such as feature importance and accumulated local effects, we attempt to explain which factors drive the predictions of the so-called ML black box models. We show that policies facilitating the penetration and use of ICT and aiming to curb the shadow economy may exert an asymmetric impact on sovereign ratings and the cost of debt depending on their present magnitudes, not only independently but also in interaction.",
        "DOI": "10.3390/computation11050090",
        "affiliation_name": "University of Piraeus",
        "affiliation_city": "Piraeus",
        "affiliation_country": "Greece"
    },
    {
        "paper_title": "Technology-Enhanced Learning, Data Sharing, and Machine Learning Challenges in South African Education",
        "paper_author": "Combrink H.M.E.",
        "publication": "Education Sciences",
        "citied_by": "4",
        "cover_date": "2023-05-01",
        "Abstract": "The objective of this paper was to scope the challenges associated with data-sharing governance for machine learning applications in education research (MLER) within the South African context. Machine learning applications have the potential to assist student success and identify areas where students require additional support. However, the implementation of these applications depends on the availability of quality data. This paper highlights the challenges in data-sharing policies across institutions and organisations that make it difficult to standardise data-sharing practices for MLER. This poses a challenge for South African researchers in the MLER space who wish to advance and innovate. The paper proposes viewpoints that policymakers must consider to overcome these challenges of data-sharing practices, ultimately allowing South African researchers to leverage the benefits of machine learning applications in education effectively. By addressing these challenges, South African institutions and organisations can improve educational outcomes and work toward the goal of inclusive and equitable education.",
        "DOI": "10.3390/educsci13050438",
        "affiliation_name": "University of Pretoria",
        "affiliation_city": "Pretoria",
        "affiliation_country": "South Africa"
    },
    {
        "paper_title": "Identifying heterogeneity using recursive partitioning: evidence from SMS nudges encouraging voluntary retirement savings in Mexico",
        "paper_author": "Shah A.M.",
        "publication": "PNAS Nexus",
        "citied_by": "2",
        "cover_date": "2023-05-01",
        "Abstract": "Individuals regularly struggle to save for retirement. Using a large-scale field experiment (N = 97, 149) in Mexico, we test the effectiveness of several behavioral interventions relative to existing policy and each other geared toward improving voluntary retirement savings contributions. We find that an intervention framing savings as a way to secure one's family future significantly improves contribution rates. We leverage recursive partitioning techniques and identify that the overall positive treatment effect masks subpopulations where the treatment is even more effective and other groups where the treatment has a significant negative effect, decreasing contribution rates. Accounting for this variation is significant for theoretical and policy development as well as firm profitability. Our work also provides a methodological framework for how to better design, scale, and deploy behavioral interventions to maximize their effectiveness.",
        "DOI": "10.1093/pnasnexus/pgad058",
        "affiliation_name": "Rotman School of Management",
        "affiliation_city": "Toronto",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Physics guided data-driven model to estimate minimum miscibility pressure (MMP) for hydrocarbon gases",
        "paper_author": "Sinha U.",
        "publication": "Geoenergy Science and Engineering",
        "citied_by": "6",
        "cover_date": "2023-05-01",
        "Abstract": "Local or regional availability of hydrocarbon gases, and being associated with the native fluids makes them a reliable candidate for gas injection processes. As the achievement of miscibility leads to higher recoveries (see the comprehensive review on miscibility by Dindoruk et al., 2021), miscible hydrocarbon injection will be preferred where it is feasible. Using hydrocarbon gases for miscible injection has additional benefits 1) High outlet pressures from the separators (using gas compressors) (Griffith et al., 1970) leading to lesser gas volume to transport, 2) Reduction of flaring: in many places where there is no or not enough infrastructure capacity to transport these gases, such gases are simply flared (Worldbank, 2022; USCUSA, 2022), 3) Being associated with the in-situ fluids, less likely to have issues such as corrosion, or other potential problems. Methane (dominant component in hydrocarbon gas streams) and other live HC blends is classified greenhouse gases (GHG) which are harmful to the environment and such actions do not comply with the tighter carbon/emission reduction policies that are being implemented, especially considering the fact that resulting in the emission of methane, which has 28 to 36 times the global warming potential (GWP) compared to CO2 (EPA2, 2022; Turner et al., 2016; Turner et al., 2019). From the recovery point of view and as well as tuning the equation of state (EOS) for gas injection processes, minimum miscibility pressure (MMP) is a pivotal parameter estimating the recovery factor during gas injection processes. In this study we developed a methodology using Light Gradient Boost (Light GBM) (Ke et al., 2017) based model to estimate: 1) Minimum Miscibility Pressure (MMP) for the injected stream of hydrocarbon (HC) gas, and 2) minimum heavy HC gas (C2H6, C3H8 or their equivalence like CO2 and H2S) that is required to be added for a target MMP into the light HC gas stream. This is also known as minimum miscible enrichment, MME to achieve a prescribed MMP if the pressure requirements for a given lean gas are too high to implement in the field, usually due to compression cost and/or reservoir/well integrity reasons. We validate our MMP model using Leave-One-Out-Cross-Validation (LOOCV) (Shao, 1993) method which is the least unbiased cross -validation method and therefore proves the robustness of the proposed model. We compared the accuracy of the proposed model (AARE = 3.83%) with the leading correlations in the industry, and the proposed model significantly outperforms all the relevant models in the literature as shown in this study (Maklavani et al., 2010; Eakin and Mitch, 1988; Glaso, 1985; Kuo, 1985). The proposed model fully considers the underlying physics of miscibility and takes into consideration all the key and easy to obtain input parameters that influence the miscibility pressure of the injected gas.",
        "DOI": "10.1016/j.geoen.2022.211389",
        "affiliation_name": "University of Houston",
        "affiliation_city": "Houston",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Application of a data-driven DTSF and benchmark models for the prediction of electricity prices in Brazil: A time-series case",
        "paper_author": "Gontijo T.S.",
        "publication": "Journal of Renewable and Sustainable Energy",
        "citied_by": "3",
        "cover_date": "2023-05-01",
        "Abstract": "The global energy market has significantly developed in recent years; proof of this is the creation and promotion of smart grids and technical advances in energy commercialization and transmission. Specifically in the Brazilian context, with the recent modernization of the electricity sector, energy trading prices, previously published on a weekly frequency, are now available on an hourly domain. In this context, the definition and forecasting of prices become increasingly important factors for the economic and financial viability of energy projects. In this scenario of changes in the local regulatory framework, there is a lack of publications based on the new hourly prices in Brazil. This paper presents, in a pioneering way, the Dynamic Time Scan Forecasting (DTSF) method for forecasting hourly energy prices in Brazil. This method searches for similarity patterns in time series and, in previous investigations, showed competitive advantages concerning established forecasting methods. This research aims to test the accuracy of the DTSF method against classical statistical models and machine learning. We used the short-term prices of electricity in Brazil, made available by the Electric Energy Commercialization Chamber. The new DTSF model showed the best predictive performance compared to both the statistical and machine learning models. The DTSF performance was superior considering the evaluation metrics utilized in this paper. We verified that the predictions made by the DTSF showed less variability compared to the other models. Finally, we noticed that there is not an ideal model for all predictive 24 steps ahead forecasts, but there are better models at certain times of the day.",
        "DOI": "10.1063/5.0144873",
        "affiliation_name": "Universidade Federal de Minas Gerais",
        "affiliation_city": "Belo Horizonte",
        "affiliation_country": "Brazil"
    },
    {
        "paper_title": "Predicting nutritional status for women of childbearing age from their economic, health, and demographic features: A supervised machine learning approach",
        "paper_author": "Khudri M.M.",
        "publication": "PLoS ONE",
        "citied_by": "8",
        "cover_date": "2023-05-01",
        "Abstract": "Background Malnutrition imposes enormous costs resulting from lost investments in human capital and increased healthcare expenditures. There is a dearth of research focusing on the prediction of women's body mass index (BMI) and malnutrition outcomes (underweight, overweight, and obesity) in developing countries. This paper attempts to fill out this knowledge gap by predicting the BMI and the risks of malnutrition outcomes for Bangladeshi women of childbearing age from their economic, health, and demographic features. Methods Data from the 2017-18 Bangladesh Demographic and Health Survey and a series of supervised machine learning (SML) techniques are used. Additionally, this study circumvents the imbalanced distribution problem in obesity classification by utilizing an oversampling approach. Results Study findings demonstrate that the support vector machine and k-nearest neighbor are the two best-performing methods in BMI prediction based on the coefficient of determination (R2), root mean square error (RMSE), and mean absolute error (MAE). The combined predictor algorithms consistently yield top specificity, Cohen's kappa, F1-score, and AUC in classifying the malnutrition status, and their performance is robust to alternative standards. The feature importance ranking based on several nonparametric and combined predictors indicates that socioeconomic status, women's age, and breastfeeding status are the most important features in predicting women's nutritional outcomes. Furthermore, the conditional inference trees corroborate that those three features, along with the partner's educational attainment and employment status, significantly predict malnutrition risks. Conclusion To the best of our knowledge, this is the first study that predicts BMI and one of the pioneer studies to classify all three malnutrition outcomes for women of childbearing age in Bangladesh, let alone in any lower-middle income country, using SML techniques. Moreover, in the context of Bangladesh, this paper is the first to identify and rank features that are critical in predicting nutritional outcomes using several feature selection algorithms. The estimators from this study predict the outcomes of interest most accurately and efficiently compared to other existing studies in the relevant literature. Therefore, study findings can aid policymakers in designing policy and programmatic approaches to address the double burden of malnutrition among Bangladeshi women, thereby reducing the country's economic burden.",
        "DOI": "10.1371/journal.pone.0277738",
        "affiliation_name": "Virginia Tech College of Engineering",
        "affiliation_city": "Blacksburg",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "A Demonstration of Modified Treatment Policies to Evaluate Shifts in Mobility and COVID-19 Case Rates in US Counties",
        "paper_author": "Nugent J.R.",
        "publication": "American Journal of Epidemiology",
        "citied_by": "4",
        "cover_date": "2023-05-01",
        "Abstract": "Mixed evidence exists of associations between mobility data and coronavirus disease 2019 (COVID-19) case rates. We aimed to evaluate the county-level impact of reducing mobility on new COVID-19 cases in summer/fall of 2020 in the United States and to demonstrate modified treatment policies to define causal effects with continuous exposures. Specifically, we investigated the impact of shifting the distribution of 10 mobility indexes on the number of newly reported cases per 100,000 residents 2 weeks ahead. Primary analyses used targeted minimum loss-based estimation with Super Learner to avoid parametric modeling assumptions during statistical estimation and flexibly adjust for a wide range of confounders, including recent case rates. We also implemented unadjusted analyses. For most weeks, unadjusted analyses suggested strong associations between mobility indexes and subsequent new case rates. However, after confounder adjustment, none of the indexes showed consistent associations under mobility reduction. Our analysis demonstrates the utility of this novel distribution-shift approach to defining and estimating causal effects with continuous exposures in epidemiology and public health.",
        "DOI": "10.1093/aje/kwad005",
        "affiliation_name": "University of California, Berkeley",
        "affiliation_city": "Berkeley",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Collaborative museum heist with reinforcement learning",
        "paper_author": "Evripidou E.",
        "publication": "Computer Animation and Virtual Worlds",
        "citied_by": "1",
        "cover_date": "2023-05-01",
        "Abstract": "Non-playable characters (NPCs) play a crucial role in enhancing immersion in video games. However, traditional NPC behaviors are often hard-coded using methods such as Finite State Machines, Decision and Behavior trees. This has a few limitations; namely, it is quite difficult to implement complex cooperative behaviors and secondly this makes it easy for human players to identify and exploit patterns in behavior. To overcome these challenges, Reinforcement learning (RL) can be used to generate dynamic and real-time NPC responses to human player actions. In this paper, we report on first results of applying RL techniques to a Non-Zero Sum, adversarial asymmetric game, using a multi-agent team. The game environment simulates a museum heist, where the objective of the successfully trained team of robbers with different skills (Locksmith, Technician) is to steal valuable items from the museum without being detected by the scripted security guards and cameras. Both agents were trained concurrently with separate policies and received both individual and group reward signals. Through this training process, the agents learned to cooperate effectively and use their skills to maximize both individual and team benefits. These results demonstrate the feasibility of realizing the full game where both robbers and security guards are trained at the same time to achieve their adversarial goals.",
        "DOI": "10.1002/cav.2158",
        "affiliation_name": "University of Cyprus",
        "affiliation_city": "Nicosia",
        "affiliation_country": "Cyprus"
    },
    {
        "paper_title": "Exploitation or Exploration? Managerial Myopia, Economic Policy Uncertainty and Ambidextrous Innovation Investment",
        "paper_author": "Yang T.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "6",
        "cover_date": "2023-05-01",
        "Abstract": "In today’s increasingly competitive international environment, original technology innovation has become essential for enhancing enterprises’ sustainability. As key innovation needs constant exploration rather than growing leaps and bounds, it is often ignored by managers who focus on short-term performance. Taking the data of publicly listed Chinese companies from 2010 to 2020 as a sample, this paper put forward the relation between managerial myopia and ambidextrous innovation investment on the basis of a empirical approach combining machine learning technology. Results revealed that managerial myopia has different effects on the ambidextrous innovation investment of enterprises. Specifically, the study finds a significant negative association between managerial myopia and exploratory innovation investment, while there is no significant relationship with regard to exploitative innovation investment. Further study showed that the negative influence is weakened by economic policy uncertainty and stronger in companies with more severe agency problems. By shedding light on the way that managerial myopia affects enterprises’ ambidextrous innovation investment, this research contributes to the literature on the impact of managerial myopia, offering key insights into how to cultivate the core competitiveness of enterprises and ensure their sustainable development.",
        "DOI": "10.3390/su15097173",
        "affiliation_name": "Beijing Jiaotong University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "AI-Enabled Energy Policy for a Sustainable Future",
        "paper_author": "Danish M.S.S.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "32",
        "cover_date": "2023-05-01",
        "Abstract": "The present time is a seminal decade for the transition of the energy sector through the deployment of green energy and the optimization of efficiencies using the power of automation and artificial intelligence (AI), which demands competitive policies to handle multidimensional endeavors via a single platform. The failure of energy policies can have far-reaching socioeconomic consequences when policies do not meet the energy and climate goals throughout the lifecycle of the policy. Such shortcomings are reported to be due to inadequate incentives and poor decision making that needs to promote fairness, equality, equity, and inclusiveness in energy policies and project decision making. The integration of AI in energy sectors poses various challenges that this study aims to analyze through a comprehensive examination of energy policy processes. The study focuses on (1) the decision-making process during the development stage, (2) the implementation management process for the execution stage, (3) the integration of data science, machine learning, and deep learning in energy systems, and (4) the requirements of energy systems in the context of substantiality. Synergistically, an emerging blueprint of policy, data science and AI, engineering practices, management process, business models, and social approaches that provides a multilateral design and implementation reference is propounded. Finally, a novel framework is developed to develop and implement modern energy policies that minimize risks, promote successful implementation, and advance society’s journey towards net zero and carbon neutral objectives.",
        "DOI": "10.3390/su15097643",
        "affiliation_name": "University of the Ryukyus",
        "affiliation_city": "Nishihara",
        "affiliation_country": "Japan"
    },
    {
        "paper_title": "Cropland Extraction in Southern China from Very High-Resolution Images Based on Deep Learning",
        "paper_author": "Xie D.",
        "publication": "Remote Sensing",
        "citied_by": "7",
        "cover_date": "2023-05-01",
        "Abstract": "Accurate cropland information is crucial for the assessment of food security and the formulation of effective agricultural policies. Extracting cropland from remote sensing imagery is challenging due to spectral diversity and mixed pixels. Recent advances in remote sensing technology have facilitated the availability of very high-resolution (VHR) remote sensing images that provide detailed ground information. However, VHR cropland extraction in southern China is difficult because of the high heterogeneity and fragmentation of cropland and the insufficient observations of VHR sensors. To address these challenges, we proposed a deep learning-based method for automated high-resolution cropland extraction. The method used an improved HRRS-U-Net model to accurately identify the extent of cropland and explicitly locate field boundaries. The HRRS-U-Net maintained high-resolution details throughout the network to generate precise cropland boundaries. Additionally, the residual learning (RL) and the channel attention mechanism (CAM) were introduced to extract deeper discriminative representations. The proposed method was evaluated over four city-wide study areas (Qingyuan, Yangjiang, Guangzhou, and Shantou) with a diverse range of agricultural systems, using GaoFen-2 (GF-2) images. The cropland extraction results for the study areas had an overall accuracy (OA) ranging from 97.00% to 98.33%, with F1 scores (F1) of 0.830–0.940 and Kappa coefficients (Kappa) of 0.814–0.929. The OA was 97.85%, F1 was 0.915, and Kappa was 0.901 over all study areas. Moreover, our proposed method demonstrated advantages compared to machine learning methods (e.g., RF) and previous semantic segmentation models, such as U-Net, U-Net++, U-Net3+, and MPSPNet. The results demonstrated the generalization ability and reliability of the proposed method for cropland extraction in southern China using VHR remote images.",
        "DOI": "10.3390/rs15092231",
        "affiliation_name": "South China Agricultural University",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Predicting CO<inf>2</inf> Emissions from Traffic Vehicles for Sustainable and Smart Environment Using a Deep Learning Model",
        "paper_author": "Al-Nefaie A.H.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "19",
        "cover_date": "2023-05-01",
        "Abstract": "Burning fossil fuels results in emissions of carbon dioxide (CO2), which significantly contributes to atmospheric changes and climate disturbances. Consequently, people are becoming concerned about the state of the environment, and governments are required to produce precise projections to develop efficient preventive measures. This study makes a significant contribution to the area by modeling and predicting the CO2 emissions of vehicles using advanced artificial intelligence. The model was constructed using the CO2 emission by vehicles dataset from Kaggle, which includes different parameters, namely, vehicle class, engine size (L), cylinder transmission, fuel type, fuel consumption city (L/100 km), fuel consumption hwy (L/100 km), fuel consumption comb (L/100 km), fuel consumption comb (mpg), and CO2 emissions (g/km). To forecast the CO2 emissions produced by vehicles, a deep learning long short-term memory network (LSTM) model and a bidirectional LSTM (BiLSTM) model were developed. Both models are efficient. Throughout the course of the investigation, the researchers employed four statistical assessment metrics: the mean square error (MSE), the root MSE (RMSE), Pearson’s correlation coefficient (R%), and the determination coefficient (R2). Based on the datasets of experiments carried out by Kaggle, the LSTM and BiLSTM models were created and implemented. The data were arbitrarily split into two phases: training, which included 80% of the total data, and testing, which comprised 20% of the total data. The BiLSTM model performed best in terms of accuracy and achieved high prediction values for MSE and RMSE. The BiLSTM model has the greatest attainable (R2 = 93.78). In addition, R% was used to locate a connection between the dataset’s characteristics to ascertain which characteristics had the highest level of association with CO2 emissions. An original strategy for the accurate forecasting of carbon emissions was developed as a result of this work. Consequently, policymakers may use this work as a potentially beneficial decision-support tool to create and execute successful environmental policies.",
        "DOI": "10.3390/su15097615",
        "affiliation_name": "King Faisal University",
        "affiliation_city": "Al-Ahsa",
        "affiliation_country": "Saudi Arabia"
    },
    {
        "paper_title": "Systematic Literature Review on Fuzzy Hybrid Methods in Photovoltaic Solar Energy: Opportunities, Challenges, and Guidance for Implementation",
        "paper_author": "Kedir N.",
        "publication": "Energies",
        "citied_by": "6",
        "cover_date": "2023-05-01",
        "Abstract": "The application of fuzzy hybrid methods has significantly increased in recent years across various sectors. However, the application of fuzzy hybrid methods for modeling systems or processes, such as fuzzy machine learning, fuzzy simulation, and fuzzy decision-making, has been relatively limited in the energy sector. Moreover, compared to standard methods, the benefits of fuzzy-hybrid methods for capturing complex problems are not adequately explored for the solar energy sector, which is one of the most important renewable energy sources in electric grids. This paper investigates the application of fuzzy hybrid systems in the solar energy sector compared to other sectors through a systematic review of journal articles published from 2012 to 2022. Selection criteria for choosing an appropriate method in each investigated fuzzy hybrid method are also presented and discussed. This study contributes to the existing literature in the solar energy domain by providing a state-of-the-art review of existing fuzzy hybrid techniques to (1) demonstrate their capability for capturing complex problems while overcoming limitations inherent in standard modeling methods, (2) recommend criteria for selecting an appropriate fuzzy hybrid technique for applications in solar energy research, and (3) assess the applicability of fuzzy hybrid techniques for solving practical problems in the solar energy sector.",
        "DOI": "10.3390/en16093795",
        "affiliation_name": "University of Alberta",
        "affiliation_city": "Edmonton",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Simultaneous Classification and Regression for Zakat Under-Reporting Detection",
        "paper_author": "Ben Ismail M.M.",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "0",
        "cover_date": "2023-05-01",
        "Abstract": "Tax revenue represents an essential budget source for most countries around the world. Accordingly, the modernization of relevant technological infrastructure has become a key factor of tax administration strategy for improving tax collection efficiency. In particular, the fiscal consolidation of the Kingdom of Saudi Arabia has been supported by considerable development in tax policy and administration, aimed at raising more taxes from non-oil activities. In fact, non-Saudi investors are liable for income tax in Saudi Arabia. On the other hand, Saudi citizen investors (and citizens of the GCC countries) are liable for Zakat, an Islamic assessment. Typically, taxpayers are in charge of preparing and accurately reporting their Zakat declaration. This allows tax authorities to overview and audit their business activities. However, despite administration efforts to increase taxpayer compliance, considerable revenue remains at under-reporting risk. In this paper, we introduce a novel intelligent approach to support tax authority efforts in detecting under-reporting among Zakat payer declarations. In particular, the proposed solution aims at improving detection accuracy and determining the fraud cases that correspond to a higher revenue at risk. Specifically, we formulate Zakat under-reporting detection as a supervised machine learning task through the design of a deep neural network that performs simultaneous classification and regression tasks. In particular, the proposed network contains an input layer, five hidden layers, and two output layers for classification and regression. Zakat declarations are mapped into the predefined “under-reporting” or “actual declaration” classes. Moreover, the revenue at risk caused by the predicted fraud cases is learned by the designed model. This allows the proposed approach to prioritize the auditing of specific Zakat payers based on the corresponding predicted revenue at risk. A real dataset including 51,919 Zakat declarations was used to validate and assess the designed model. Further, the Synthetic Minority Oversampling Technique (SMOTE) boosted the proposed model performance in terms of classification and prioritization.",
        "DOI": "10.3390/app13095244",
        "affiliation_name": "College of Sciences",
        "affiliation_city": "Riyadh",
        "affiliation_country": "Saudi Arabia"
    },
    {
        "paper_title": "Content and Sentiment Analysis of The New York Times Coronavirus (2019-nCOV) Articles with Natural Language Processing (NLP) and Leximancer",
        "paper_author": "Tunca S.",
        "publication": "Electronics (Switzerland)",
        "citied_by": "5",
        "cover_date": "2023-05-01",
        "Abstract": "The purpose of this study was to prove the use of content and sentiment analysis to understand public discourse on Nytimes.com around the coronavirus (2019-nCOV) pandemic. We examined the pandemic discourses in the article contents, news, expert opinions, and statements of official institutions with natural language processing methods. We analyzed how the mainstream media (Nytimes.com) sets the community agenda. As a method, the textual data for the research were collected with the Orange3 software text-mining tool via the Nytimes.com API, and content analysis was conducted with Leximancer software. The research data were divided into three categories (first, mid, and last) based on the date ranges determined during the pandemic. Using Leximancer concept maps tools, we explained concepts and their relationships by visualizing them to show pandemic discourse. We used VADER sentiment analysis to analyze the pandemic discourse. The results gave us the distance and proximity positions of themes related to Nytimes.com pandemic discourse, revealed according to their conceptual definitions. Additionally, we compared the performance of six machine learning algorithms on the task of text classification. Considering the findings, it is possible to conclude that in Nytimes.com (2019-nCOV) discourse, some concepts have changed on a regular basis while others have remained constant. The pandemic discourse focused on specific concepts that were seen to guide human behavior and presented content that may cause anxiety to readers of Nytimes.com. The results of the sentiment analysis supported these findings. Another result was that the findings showed us that the contents of the coronavirus (2019-nCOV) articles supported official policies. It can be concluded that regarding the coronavirus (2019-nCOV), which has caused profound societal changes and has results such as death, restrictions, and mask use, the discourse did not go beyond a total of 15 main themes and about 100 concepts. The content analysis of Nytimes.com reveals that it has behavioral effects, such as causing fear and anxiety in people. Considering the media dependency of society, this result is important. It can be said that the agenda-setting of society does not go beyond the traditional discourse due to the tendency of individuals to use newspapers and news websites to obtain information.",
        "DOI": "10.3390/electronics12091964",
        "affiliation_name": "Gebze Teknik Üniversitesi",
        "affiliation_city": "Gebze",
        "affiliation_country": "Turkey"
    },
    {
        "paper_title": "Machine-supported decision-making to improve agricultural training participation and gender inclusivity",
        "paper_author": "Reeves N.P.",
        "publication": "PLoS ONE",
        "citied_by": "1",
        "cover_date": "2023-05-01",
        "Abstract": "Women comprise a significant portion of the agricultural workforce in developing countries but are often less likely to attend government sponsored training events. The objective of this study was to assess the feasibility of using machine-supported decision-making to increase overall training turnout while enhancing gender inclusivity. Using data obtained from 1,067 agricultural extension training events in Bangladesh (130,690 farmers), models were created to assess gender-based training patterns (e.g., preferences and availability for training). Using these models, simulations were performed to predict the top (most attended) training events for increasing total attendance (male and female combined) and female attendance, based on gender of the trainer, and when and where training took place. By selecting a mixture of the top training events for total attendance and female attendance, simulations indicate that total and female attendance can be concurrently increased. However, strongly emphasizing female participation can have negative consequences by reducing overall turnout, thus creating an ethical dilemma for policy makers. In addition to balancing the need for increasing overall training turnout with increased female representation, a balance between model performance and machine learning is needed. Model performance can be enhanced by reducing training variety to a few of the top training events. But given that models are early in development, more training variety is recommended to provide a larger solution space to find more optimal solutions that will lead to better future performance. Simulations show that selecting the top 25 training events for total attendance and the top 25 training events for female attendance can increase female participation by over 82% while at the same time increasing total turnout by 14%. In conclusion, this study supports the use of machine-supported decision-making when developing gender inclusivity policies in agriculture extension services and lays the foundation for future applications of machine learning in this area.",
        "DOI": "10.1371/journal.pone.0281428",
        "affiliation_name": "College of Science and Engineering",
        "affiliation_city": "Minneapolis",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Optimizing Battery Charging Using Neural Networks in the Presence of Unknown States and Parameters",
        "paper_author": "Pozzi A.",
        "publication": "Sensors",
        "citied_by": "6",
        "cover_date": "2023-05-01",
        "Abstract": "This work investigates the effectiveness of deep neural networks within the realm of battery charging. This is done by introducing an innovative control methodology that not only ensures safety and optimizes the charging current, but also substantially reduces the computational complexity with respect to traditional model-based approaches. In addition to their high computational costs, model-based approaches are also hindered by their need to accurately know the model parameters and the internal states of the battery, which are typically unmeasurable in a realistic scenario. In this regard, the deep learning-based methodology described in this work was been applied for the first time to the best of the authors’ knowledge, to scenarios where the battery’s internal states cannot be measured and an estimate of the battery’s parameters is unavailable. The reported results from the statistical validation of such a methodology underline the efficacy of this approach in approximating the optimal charging policy.",
        "DOI": "10.3390/s23094404",
        "affiliation_name": "Università Cattolica del Sacro Cuore, Campus di Brescia",
        "affiliation_city": "Brescia",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Examining the non-linear effects of transit accessibility on daily trip duration: A focus on the low-income population",
        "paper_author": "Tao S.",
        "publication": "Journal of Transport Geography",
        "citied_by": "11",
        "cover_date": "2023-05-01",
        "Abstract": "Public transit provides an affordable and reliable transport option especially to the vulnerable groups. However, the relevance of transit accessibility to the daily mobility of different social strata has not been fully understood. It remains unclear to what extent the low-income may benefit from enhanced transit accessibility compared to others. Focusing on an Asian metropolis—Hong Kong, this study investigates the interplay between transit accessibility and daily trip duration with a particular focus on the low-income population via a machine-learning approach (Gradient Boosting Decision Tree). Our findings indicate that network accessibility by Mass Transit Rail (MTR) exerts a weaker effect on the duration of mandatory and discretionary trips of the low-income than for the non-low-income for these trips. This implies the presence of possible barriers of using MTR among the low-income. Moreover, marked threshold effects are identified for both MTR and bus accessibility especially in relation to the mandatory and maintenance trips of the low-income. Based on these findings, policy recommendations are proposed to help strengthen the linkage between improvement of transit accessibility and equitable mobility conditions across society.",
        "DOI": "10.1016/j.jtrangeo.2023.103600",
        "affiliation_name": "Universiteit Gent",
        "affiliation_city": "Ghent",
        "affiliation_country": "Belgium"
    },
    {
        "paper_title": "Delineation of agricultural fields in arid regions from Worldview-2 datasets based on image textural properties",
        "paper_author": "Adhikari A.",
        "publication": "Environmental Monitoring and Assessment",
        "citied_by": "3",
        "cover_date": "2023-05-01",
        "Abstract": "Barren lands are being transformed into agricultural fields with the growing demand for agriculture-based products. Hence, monitoring these regions for better planning and management is crucial. Surveying with high-resolution RS (remote sensing) satellites like Worldview-2 provides a faster and cheaper solution than conventional surveys. In the study, the arid region comprising cropland and barrenlands are efficiently and autonomously delineated using its spectral and textural properties using state-of-the-art random forest (RF) ensemble classifiers. The textural information window size is optimized and at a GLCM (gray-level co-occurrence matrix) window size of 13, a stable trend in classification accuracy was observed. A further rise in window sizes did not improve the classification accuracy; beyond GLCM 19, a decline in accuracy was observed. Comparing GLCM-13 RF with the no-GLCM RF classifier, the GLCM-based classifiers performed better; thus, the textural information assisted in removing isolated crop-classified outputs that are falsely predicted pixel groups. Still, it also obscured information about barren lands present within croplands. Delineation accuracy was 93.8 % for the no-GLCM RF classifier, whereas, for the GLCM-13 RF classifier, an accuracy of 97.3 % was observed. Thus, overall, a 3.5 % improvement in accuracy was observed while using the GLCM RF classifier with window size 13. The textural information with proper calibration over high-spatial resolution datasets improves crop delineation in the present study. Henceforth, a more accurate cropland identification will provide a better estimate of the actual cropland area in such an arid region, which will assist in formulating a better resource management policy.",
        "DOI": "10.1007/s10661-023-11115-x",
        "affiliation_name": "Indian Institute of Technology Roorkee",
        "affiliation_city": "Roorkee",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Learning to Solve 3-D Bin Packing Problem via Deep Reinforcement Learning and Constraint Programming",
        "paper_author": "Jiang Y.",
        "publication": "IEEE Transactions on Cybernetics",
        "citied_by": "19",
        "cover_date": "2023-05-01",
        "Abstract": "Recently, there is a growing attention on applying deep reinforcement learning (DRL) to solve the 3-D bin packing problem (3-D BPP). However, due to the relatively less informative yet computationally heavy encoder, and considerably large action space inherent to the 3-D BPP, existing DRL methods are only able to handle up to 50 boxes. In this article, we propose to alleviate this issue via a DRL agent, which sequentially addresses three subtasks of sequence, orientation, and position, respectively. Specifically, we exploit a multimodal encoder, where a sparse attention subencoder embeds the box state to mitigate the computation while learning the packing policy, and a convolutional neural network subencoder embeds the view state to produce auxiliary spatial representation. We also leverage an action representation learning in the decoder to cope with the large action space of the position subtask. Besides, we integrate the proposed DRL agent into constraint programming (CP) to further improve the solution quality iteratively by exploiting the powerful search framework in CP. The experiments show that both the sole DRL and hybrid methods enable the agent to solve large-scale instances of 120 boxes or more. Moreover, they both could deliver superior performance against the baselines on instances of various scales.",
        "DOI": "10.1109/TCYB.2021.3121542",
        "affiliation_name": "School of Computer Science and Engineering",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore"
    },
    {
        "paper_title": "The impact of commercial health datasets on medical research and health-care algorithms",
        "paper_author": "Alberto I.R.I.",
        "publication": "The Lancet Digital Health",
        "citied_by": "28",
        "cover_date": "2023-05-01",
        "Abstract": "As the health-care industry emerges into a new era of digital health driven by cloud data storage, distributed computing, and machine learning, health-care data have become a premium commodity with value for private and public entities. Current frameworks of health data collection and distribution, whether from industry, academia, or government institutions, are imperfect and do not allow researchers to leverage the full potential of downstream analytical efforts. In this Health Policy paper, we review the current landscape of commercial health data vendors, with special emphasis on the sources of their data, challenges associated with data reproducibility and generalisability, and ethical considerations for data vending. We argue for sustainable approaches to curating open-source health data to enable global populations to be included in the biomedical research community. However, to fully implement these approaches, key stakeholders should come together to make health-care datasets increasingly accessible, inclusive, and representative, while balancing the privacy and rights of individuals whose data are being collected.",
        "DOI": "10.1016/S2589-7500(23)00025-0",
        "affiliation_name": "WPI’s Computer Science Department",
        "affiliation_city": "Worcester",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "How information, social norms, and experience with novel meat substitutes can create positive political feedback and demand-side policy change",
        "paper_author": "Paul Fesenfeld L.",
        "publication": "Food Policy",
        "citied_by": "13",
        "cover_date": "2023-05-01",
        "Abstract": "The food system causes more than a third of the global anthropogenic greenhouse gas emissions, of which half are from livestock. Shifting towards plant-based diets could significantly reduce deforestation, protect biodiversity, and contribute to achieving the Sustainable Development Goals and Paris climate targets. Arguably, large-scale shifts in meat consumption require ambitious policy change. Yet, deep-rooted eating habits, pleasure, cultural status, and personal freedom are just a few of many obstacles to adopt ambitious demand-side policies and reduce meat consumption. Here, we hypothesize that technological innovation in meat substitutes, if effectively combined with social norm and factual informational triggers for behavioral changes, can foster positive political feedback to transform the food system. To test our hypothesis, we conducted survey experiments with citizens (N = 2590) in China and the US – the globally largest meat markets – and analyzed data using different machine learning methods. Our findings show that personal experience with novel plant-based meat substitutes strongly predicts individuals’ intentions to reduce their meat consumption, eat more substitutes, and support public policies to catalyze a transition to more plant-based diets. We also find that in both countries factual and social norm information about the benefits of more plant-based diets can increase citizens’ behavioral change intentions and support for meat reduction policies. Overall, however, social norm information had no significant additional effects on the outcomes compared to the simple factual information treatments. In the US, prior experience with innovative meat substitutes potentially can boost the positive effects of informational campaigns on public support for meat reduction policies. The results offer promising implications for a policy sequencing strategy to create positive political feedback and enable socio-technical tipping dynamics for sustainable food system transformation.",
        "DOI": "10.1016/j.foodpol.2023.102445",
        "affiliation_name": "University of Bath",
        "affiliation_city": "Bath",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "The Future of Heart Allocation Policy: Patient-Specific Variables Over Treatment Strategy",
        "paper_author": "Farr M.",
        "publication": "JACC: Heart Failure",
        "citied_by": "0",
        "cover_date": "2023-05-01",
        "Abstract": "NA",
        "DOI": "10.1016/j.jchf.2023.03.014",
        "affiliation_name": "UT Southwestern Medical School",
        "affiliation_city": "Dallas",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Artificial intelligence and real-world data for drug and food safety – A regulatory science perspective",
        "paper_author": "Thakkar S.",
        "publication": "Regulatory Toxicology and Pharmacology",
        "citied_by": "18",
        "cover_date": "2023-05-01",
        "Abstract": "In 2013, the Global Coalition for Regulatory Science Research (GCRSR) was established with members from over ten countries (www.gcrsr.net). One of the main objectives of GCRSR is to facilitate communication among global regulators on the rise of new technologies with regulatory applications through the annual conference Global Summit on Regulatory Science (GSRS). The 11th annual GSRS conference (GSRS21) focused on “Regulatory Sciences for Food/Drug Safety with Real-World Data (RWD) and Artificial Intelligence (AI).” The conference discussed current advancements in both AI and RWD approaches with a specific emphasis on how they impact regulatory sciences and how regulatory agencies across the globe are pursuing the adaptation and oversight of these technologies. There were presentations from Brazil, Canada, India, Italy, Japan, Germany, Switzerland, Singapore, the United Kingdom, and the United States. These presentations highlighted how various agencies are moving forward with these technologies by either improving the agencies’ operation and/or preparing regulatory mechanisms to approve the products containing these innovations. To increase the content and discussion, the GSRS21 hosted two debate sessions on the question of “Is Regulatory Science Ready for AI?” and a workshop to showcase the analytical data tools that global regulatory agencies have been using and/or plan to apply to regulatory science. Several key topics were highlighted and discussed during the conference, such as the capabilities of AI and RWD to assist regulatory science policies for drug and food safety, the readiness of AI and data science to provide solutions for regulatory science. Discussions highlighted the need for a constant effort to evaluate emerging technologies for fit-for-purpose regulatory applications. The annual GSRS conferences offer a unique platform to facilitate discussion and collaboration across regulatory agencies, modernizing regulatory approaches, and harmonizing efforts.",
        "DOI": "10.1016/j.yrtph.2023.105388",
        "affiliation_name": "ApconiX Ltd.",
        "affiliation_city": "Alderley Edge",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "AwarNS: A framework for developing context-aware reactive mobile applications for health and mental health",
        "paper_author": "González-Pérez A.",
        "publication": "Journal of Biomedical Informatics",
        "citied_by": "6",
        "cover_date": "2023-05-01",
        "Abstract": "In recent years, interest and investment in health and mental health smartphone apps have grown significantly. However, this growth has not been followed by an increase in quality and the incorporation of more advanced features in such applications. This can be explained by an expanding fragmentation of existing mobile platforms along with more restrictive privacy and battery consumption policies, with a consequent higher complexity of developing such smartphone applications. To help overcome these barriers, there is a need for robust, well-designed software development frameworks which are designed to be reliable, power-efficient and ethical with respect to data collection practices, and which support the sense-analyse-act paradigm typically employed in reactive mHealth applications. In this article, we present the AwarNS Framework, a context-aware modular software development framework for Android smartphones, which facilitates transparent, reliable, passive and active data sampling running in the background (sense), on-device and server-side data analysis (analyse), and context-aware just-in-time offline and online intervention capabilities (act). It is based on the principles of versatility, reliability, privacy, reusability, and testability. It offers built-in modules for capturing smartphone and associated wearable sensor data (e.g. IMU sensors, geolocation, Wi-Fi and Bluetooth scans, physical activity, battery level, heart rate), analysis modules for data transformation, selection and filtering, performing geofencing analysis and machine learning regression and classification, and act modules for persistence and various notification deliveries. We describe the framework's design principles and architecture design, explain its capabilities and implementation, and demonstrate its use at the hand of real-life case studies implementing various mobile interventions for different mental disorders used in clinical practice.",
        "DOI": "10.1016/j.jbi.2023.104359",
        "affiliation_name": "Centro de Investigación Biomédica en Red-Fisiopatología de la Obesidad y Nutrición",
        "affiliation_city": "Madrid",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "Forecasting the lithium mineral resources prices in China: Evidence with Facebook Prophet (Fb-P) and Artificial Neural Networks (ANN) methods",
        "paper_author": "Li X.",
        "publication": "Resources Policy",
        "citied_by": "24",
        "cover_date": "2023-05-01",
        "Abstract": "Combining lithium real-time series data with recently developed advanced Artificial Neural Networks (ANN) and Facebook Prophet (Fb-P) algorithms is of particular relevance for identifying and delivering policy-insightful patterns by learning from experimental data without being pre-conditioned and managing investment risk. The prime objective of this study is to forecast lithium mineral resource prices in China. This study uses the Fb-P and ANN techniques to estimate lithium prices utilizing daily historical data between 5 November 2018 and 1 November 2022. In doing so, the empirical estimates help to predict future prices until 20 April 2023. The findings of the Facebook Prophet technique demonstrate that lithium mineral pricing has a very high degree of accuracy and has a long short-term memory at differential frequency days intervals. In contrast to the current price of 572,500 yuan/tonne, it may have been noticed that the market would suddenly surge in the next six months, reaching more than 800000 yuan/tonne. The study attempts to draw novel implications in the context of mineral resource prices in China.",
        "DOI": "10.1016/j.resourpol.2023.103580",
        "affiliation_name": "ECUST School of Business",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A systematic review of machine learning approaches in carbon capture applications",
        "paper_author": "Hussin F.",
        "publication": "Journal of CO2 Utilization",
        "citied_by": "24",
        "cover_date": "2023-05-01",
        "Abstract": "Climate change and global warming are among of the most important environmental issues and require adequate and immediate global action to preserve the planet for future generations. One of the essential technologies used to reduce CO2 emissions and mitigate the worst effects of climate change is carbon capture technology. Many efforts have been made by scientists, industrial sectors, and policy-makers in looking for new technology to reduce greenhouse gas emissions and achieve net-zero emission goals. Research and development in creating new technology involve complex processes and require a digital system to optimize big data prediction as well as to reduce production time. A mathematical and statistical approach such as machine learning plays an important role in solving research problems, whereby this approach provides fast results in predicting big data and cost-efficient tools. In this study, a systematic review and bibliometric analysis were used to analyze the research trend, particularly on the keywords, number of publications, citations, countries, and authorship. This information is important for future research directions for researchers who venture into this area. In this study, the bibliometric analysis focuses on 2 main categories: co-authorship (countries and organizations) and keywords (author keyword). Based on the research trend, the United States (USA), China, Iran, Canada, and the United Kingdom are the leading countries contributing to this field since they have the highest publications and citations. Furthermore, the most common keywords used in the selected articles ranked according to the highest link strength. The top 6 keyword list includes machine learning, artificial neural network, CO2 capture, CO2 solubility, metal-organic frameworks (MOFs) and carbon capture and storage. The findings from this study can be used to open a wider spectrum for the research communities by providing global research trends, current innovations and current technology on machine learning in carbon capture application, identifying the active research areas or hot topics and future research direction to help fight climate change issue using smart advanced technology.",
        "DOI": "10.1016/j.jcou.2023.102474",
        "affiliation_name": "Dawood University of Engineering &amp; Technology (DUET)",
        "affiliation_city": "Karachi",
        "affiliation_country": "Pakistan"
    },
    {
        "paper_title": "An Electric Vehicle Transitioning Framework for Public Fleet Planning",
        "paper_author": "Er Raqabi E.M.",
        "publication": "Transportation Research Part D: Transport and Environment",
        "citied_by": "5",
        "cover_date": "2023-05-01",
        "Abstract": "While electric vehicle (EV) use has proliferated in developed countries, emerging nations have lagged behind because they lack the resources to establish effective EV charging networks. To address this issue, this paper has developed a framework through scenario analysis that aims to minimize the costs involved in establishing such networks and provide policy makers with insights on how to accomplish this. A given set of public institutions possessing EV fleets were first grouped into zones using clustering algorithms. Mathematical models were then developed to identify optimal EV charging station locations serving the EV fleets of these organizations, with an objective to minimize the total cost. Factors such as EV range anxiety and human walking distance were imbedded into the framework, which was examined using real world data from three major cities in Morocco: Rabat, Casablanca, and Fes. With the framework, policy makers can make better planning decisions on EV transitioning.",
        "DOI": "10.1016/j.trd.2023.103732",
        "affiliation_name": "Institut de Recherche en Immunologie et en Cancérologie de l’Université de Montréal",
        "affiliation_city": "Montreal",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Post-prognostics demand management, production, spare parts and maintenance planning for a single-machine system using Reinforcement Learning",
        "paper_author": "Wesendrup K.",
        "publication": "Computers and Industrial Engineering",
        "citied_by": "16",
        "cover_date": "2023-05-01",
        "Abstract": "Production Planning and Control (PPC) is crucial for any manufacturer and comprises steps such as demand management, production, or source planning. Manufacturers achieve competitive advantage by sustaining continuous production, which can be realised through Condition-based Maintenance and Prognostics and Health Management. Hereby, the machine's health can be predicted, and post-prognostics decision-making allows to optimise PPC to meet customer demands and minimise costs. Unfortunately, the complex dynamic, stochastic and intransparent nature of post-prognostics PPC makes it intractable to use ‘traditional’ static or deterministic optimisation techniques or approaches that require an exact mathematical model or objective function. To tackle this, a data-driven post-prognostics Reinforcement Learning model is developed to plan and control the sourcing of spare parts, production, and maintenance of a single-machine production system to maximise production revenue by meeting customer demands and minimising costs. In a case study, Proximal Policy Optimisation, which is well-known from OpenAI's ChatGPT, is applied to a post-prognostics PPC decision-making problem. The Proximal Policy Optimisation is compared to other state-of-the-art learners, and the performance and robustness are evaluated. Analyses show that our model outperforms other learners, as well as reactive and scheduled preventive maintenance strategies and is robust to noise and cost changes.",
        "DOI": "10.1016/j.cie.2023.109216",
        "affiliation_name": "European Research Center for Information Systems",
        "affiliation_city": "Munster",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Revisiting economic diversification in Africa's largest resource-rich nation: Empirical insights from unsupervised machine learning",
        "paper_author": "Awe O.O.",
        "publication": "Resources Policy",
        "citied_by": "2",
        "cover_date": "2023-05-01",
        "Abstract": "Nigeria, as the heartbeat of Africa and the largest resource-rich nation in the continent has been economically bedridden in recent times. This is mainly due to the lack of diversification of its economy and resources. This paper deals with the analysis of the Nigerian economy with a view of determining the constituents of its economic growth using cluster analysis-(an unsupervised machine learning) approach. Results from the two cluster analysis approaches used-(K-Means and Hierarchical Clustering) indicates that crude oil contributes mainly to the Nigerian economy in terms of revenue and it is clearly distinct from other resources/sectors which makes up the components of revenue generation in Nigeria. Spearman's correlation analysis shows that all the economic indicators considered are highly correlated with each other except with Petroleum and Solid Minerals. This is not unconnected with the fact that the Nigerian economy has been largely dependent on oil revenue over the years, besides the fact that sectorial linkages are limited. Policies aimed at the diversification of the Nigerian economy and promoting value chain across sectors is sine qua non to economic progress. The need to ensure that other sectors contribute meaningfully and tangibly to the Nigerian economy in the post-pandemic era should be revisited.",
        "DOI": "10.1016/j.resourpol.2023.103540",
        "affiliation_name": "Anchor University Lagos",
        "affiliation_city": "Lagos",
        "affiliation_country": "Nigeria"
    },
    {
        "paper_title": "Farmers' perception of barriers that difficult the implementation of agriculture 4.0",
        "paper_author": "da Silveira F.",
        "publication": "Agricultural Systems",
        "citied_by": "40",
        "cover_date": "2023-05-01",
        "Abstract": "CONTEXT: Agriculture 4.0 can drive the growth of the agricultural production chain in emerging countries like Brazil, which is known as one of the primary food and meat producers worldwide, by implementing a range of advanced technologies such as the Internet of Things (IoT), Artificial Intelligence (AI), Blockchain, and Machine Learning. However, the development of agriculture 4.0 in Brazil is a complex process, and more needs to be known about the real barriers that impact its adoption among the actors of the agricultural production chain. There need to be more empirical studies about the perception of Brazilian farmers regarding the barriers that may compromise the successful path of agriculture 4.0. OBJECTIVE: This article aims to validate the barriers that difficult the development of agriculture 4.0 in the agricultural production chain of Southern Brazil. METHODS: Twenty-five barriers were chosen for validation based on a Systematic Review of the existing Literature. A confirmatory factor analysis was performed using the statistical tests of Kaiser-Meyer-Olkin and Bartlett. Validation was performed through the perception of farmers (n = 347) distributed among the states of Rio Grande do Sul, Santa Catarina, and Paraná. The data were collected from an online questionnaire that identified the importance of the barriers for the farmers in the sample. RESULTS AND CONCLUSIONS: The most frequent and important barriers mentioned by the farmers were: lack of infrastructure, lack of solutions accessible to farmers, need to foster R&D and innovative business models, age group risk, and lack of efficacy in the data on the rural environment. This information can contribute to constructing a framework that seeks to overcome these barriers, thus facilitating the expansion and dissemination of agriculture 4.0 in Brazil. SIGNIFICANCE: This study found that there is an open discussion about the barriers of agriculture 4.0 in the agricultural production chain. It will be necessary to investigate further the most prominent barriers in this study, as these may have significant impacts on other barriers. New systemic research that seeks to deepen the conditions for implementing agriculture 4.0 must be developed.",
        "DOI": "10.1016/j.agsy.2023.103656",
        "affiliation_name": "Instituto Federal de Educação, Ciȇncia e Tecnologia do Rio Grande do Sul",
        "affiliation_city": "Bento Goncalves",
        "affiliation_country": "Brazil"
    },
    {
        "paper_title": "Cooperative Assist-as-Needed Control for Robotic Rehabilitation: A Two-Player Game Approach",
        "paper_author": "Pezeshki L.",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "13",
        "cover_date": "2023-05-01",
        "Abstract": "This letter presents an adaptive optimal control strategy for developing assist-as-needed robotic rehabilitation. The primary goal is to encourage patient participation and increase the effectiveness of training sessions by minimizing robot intervention while following a predefined path. To achieve this, the problem is modeled as a two-player non-zero-sum game, with cooperative and individual objectives for the human and robot specified as different cost functions. Policy iteration techniques are adopted for learning the optimal solution online. The shared autonomy feature is specifically achieved through seamless adaptation of the robot's autonomy according to its estimation of human intention. The performance of the proposed approach is illustrated in several simulations and experimental studies.",
        "DOI": "10.1109/LRA.2023.3261750",
        "affiliation_name": "Isfahan University of Technology",
        "affiliation_city": "Isfahan",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "Learning spectral-spatial representations from VHR images for fine-scale crop type mapping: A case study of rice-crayfish field extraction in South China",
        "paper_author": "Cai Z.",
        "publication": "ISPRS Journal of Photogrammetry and Remote Sensing",
        "citied_by": "14",
        "cover_date": "2023-05-01",
        "Abstract": "Accurate information on crop type spatial distributions is crucial for yield estimation, agroecological modeling, and agrarian policy development. In comparison with moderate resolution (10–30 m) satellite images, very high-resolution (VHR, ≤ 1 m) images are superior at describing the spatial details and complex texture features of agricultural land. However, VHR images generally suffer from insufficient spatiotemporal coverage and high spectral variation in crop type, making fine-scale crop type mapping at regional or larger scales challenging. In this study, we developed a novel data-driven residual attention U-shape network (RAUNet) to identify crop types at the regional scale based on VHR images. RAUNet was adapted from UNet by introducing a residual module and a gated attention mechanism to strengthen the multilevel representation of crop features from VHR images. We selected rice-crayfish fields (RCFs) in Hubei Province, China, as the case crop type and area to test the performance of RAUNet. Seven models, including RAUNetpan, ResUNet, AttnUNet, UNet, DeepLabV3 + and random forest at the pixel and object levels, were adopted for comparison with RAUNet. Moreover, the model transferability was evaluated over regions with different agricultural landscapes. Our results showed that RAUNet-derived RCF maps obtained an average F1-score and MCC of 0.90 and 0.85 at different cultivation stages, which significantly outperformed other methods. Additionally, RAUNet had good spatiotemporal transferability, with an average F1-score and MCC of 0.93 and 0.88 in four transfer areas. The visualization results of the attention weights indicated RAUNet's prominent capability in combining useful and relevant information from different feature levels to produce refinement predictions for the target crop type. Furthermore, the high-level features derived from RAUNet can well separate RCFs and non-RCFs over different landscapes, which were also consistent across different cultivation periods. These encouraging results suggest that RAUNet works well for fine-scale crop type mapping in smallholder farming systems, and it is an efficient method for land cover mapping based on VHR images at the regional scale.",
        "DOI": "10.1016/j.isprsjprs.2023.03.019",
        "affiliation_name": "Huazhong Agricultural University",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "An interpretable machine-learned model for international oil trade network",
        "paper_author": "Xie W.J.",
        "publication": "Resources Policy",
        "citied_by": "3",
        "cover_date": "2023-05-01",
        "Abstract": "Energy security and energy trade are the cornerstones of global economic and social development. The structural robustness of the international oil trade network (iOTN) plays an important role in the global economy. We integrate the machine learning optimization algorithm, game theory, and utility theory for learning an oil trade decision-making model that contains the benefit endowment and cost endowment of economies in international oil trades. We have reconstructed the network degree, clustering coefficient, and closeness of the iOTN well to verify the effectiveness of the model. In the end, policy simulations based on game theory and agent-based model are carried out in a more realistic environment. We find that export-oriented economies are more vulnerable to being affected than import-oriented economies after receiving external shocks. Moreover, the impact of the increase and decrease of trade friction costs on the international oil trade is asymmetrical, and there are significant differences between international organizations.",
        "DOI": "10.1016/j.resourpol.2023.103513",
        "affiliation_name": "ECUST School of Business",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Mapping Annual Cropping Pattern from Time-Series MODIS EVI Using Parameter-Tuned Random Forest Classifier",
        "paper_author": "Praveen A.",
        "publication": "Journal of the Indian Society of Remote Sensing",
        "citied_by": "5",
        "cover_date": "2023-05-01",
        "Abstract": "Monitoring agriculture growth at different seasons (i.e., Rabi-winter crop, Zaid-summer crop, Kharif-monsoon crop) is an important requirement to understand annual cropping pattern dynamics for food security-related policy and strategy formulation. Cropping pattern is not uniform across a region and hence satellite observation of different time-period over a year is inevitable in such cases, but the associated big-data processing and accuracy requirements makes the problem more pertinent to study. Non-parametric machine learning (ML) algorithms have a higher ability to deal with such information extraction problem related to high-dimensional time-series satellite data, especially to infer multi-growth agriculture patterns. The current study aims to explore one of the popular ML algorithm, i.e., Random Forest (RF), along with two conventional supervised algorithms [i.e., Maximum Likelihood Classifier (MLC) and Spectral Angle Mapper (SAM)] for annual cropping pattern mapping in Bihar State of India using MODIS time-series Enhanced Vegetation Index (EVI). The RF algorithm was tuned to test its sensitivity with diverse combinations of model parameters, training sample data, and input variables. Tuning options for training samples and input variables have provided a possibility to understand the optimality and infer important temporal periods to detect cropping patterns with less data. In addition, the analysis also revealed that though a high degree of collinearity (i.e., r ≥ 0.9) exists in time-series EVI data due to the presence of multiple growing and decaying phases of crop, it does not negatively affect the model performance. The blind random sample-based assessment reveals that RF excelled in accuracy (81.47%) over MLC (67.47%) and SAM (52.53%).",
        "DOI": "10.1007/s12524-023-01676-2",
        "affiliation_name": "Birla Institute of Technology, Mesra",
        "affiliation_city": "Ranchi",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Public discourse and sentiment during Mpox outbreak: an analysis using natural language processing",
        "paper_author": "Anoop V.S.",
        "publication": "Public Health",
        "citied_by": "18",
        "cover_date": "2023-05-01",
        "Abstract": "Objectives: Mpox has been declared a Public Health Emergency of International Concern by the World Health Organization on July 23, 2022. Since early May 2022, Mpox has been continuously reported in several endemic countries with alarming death rates. This led to several discussions and deliberations on the Mpox virus among the general public through social media and platforms such as health forums. This study proposes natural language processing techniques such as topic modeling to unearth the general public's perspectives and sentiments on growing Mpox cases worldwide. Study design: This was a detailed qualitative study using natural language processing on the user-generated comments from social media. Methods: A detailed analysis using topic modeling and sentiment analysis on Reddit comments (n = 289,073) that were posted between June 1 and August 5, 2022, was conducted. While the topic modeling was used to infer major themes related to the health emergency and user concerns, the sentiment analysis was conducted to see how the general public responded to different aspects of the outbreak. Results: The results revealed several interesting and useful themes, such as Mpox symptoms, Mpox transmission, international travel, government interventions, and homophobia from the user-generated contents. The results further confirm that there are many stigmas and fear of the unknown nature of the Mpox virus, which is prevalent in almost all topics and themes unearthed. Conclusions: Analyzing public discourse and sentiments toward health emergencies and disease outbreaks is highly important. The insights that could be leveraged from the user-generated comments from public forums such as social media may be important for community health intervention programs and infodemiology researchers. The findings from this study effectively analyzed the public perceptions that may enable quantifying the effectiveness of measures imposed by governmental administrations. The themes unearthed may also benefit health policy researchers and decision-makers to make informed and data-driven decisions.",
        "DOI": "10.1016/j.puhe.2023.02.018",
        "affiliation_name": "University of Kerala",
        "affiliation_city": "Thiruvananthapuram",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Development and validation of an overdose risk prediction tool using prescription drug monitoring program data",
        "paper_author": "Gellad W.F.",
        "publication": "Drug and Alcohol Dependence",
        "citied_by": "5",
        "cover_date": "2023-05-01",
        "Abstract": "Objectives: To develop and validate a machine-learning algorithm to predict fatal overdose using Pennsylvania Prescription Drug Monitoring Program (PDMP) data. Methods: The training/testing (n = 3020,748) and validation (n = 2237,701) cohorts included Pennsylvania residents with a prescription dispensing from February 2018-September 2021. Potential predictors (n = 222) were measured in the 6 months prior to a random index date. Using a gradient boosting machine, we developed a 20-variable model to predict risk of fatal drug overdose in the 6 months after the index date. Results: Beneficiaries in the training (n = 1,812,448), testing (n = 1,208,300), and validation (n = 2,237,701) samples had similar age, with low rates of fatal overdose during 6-month follow up (0.12%, 0.12%, 0.04%, respectively). The validation c-statistic was 0.86 for predicting fatal overdose using 20 PDMP variables. When ranking individuals based on risk score, the prediction model more accurately identified fatal overdose at 6 months compared to using opioid dosage or opioid/benzodiazepine overlap, although the percentage of individuals in the highest risk percentile who died at 6 months was less than 1%. Conclusions and policy implications: A gradient boosting machine algorithm predicting fatal overdose derived from twenty variables performed well in discriminating risk across testing and validation samples, improving on single factor risk measures like opioid dosage",
        "DOI": "10.1016/j.drugalcdep.2023.109856",
        "affiliation_name": "Pennsylvania Department of Health",
        "affiliation_city": "Harrisburg",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Multi-view support vector machines with sub-view learning",
        "paper_author": "Hao Q.",
        "publication": "Soft Computing",
        "citied_by": "3",
        "cover_date": "2023-05-01",
        "Abstract": "Multi-view learning improves the performance of existing learning tasks by using complementary information between multiple feature sets. In the latest research, multi-view learning model using privileged information is proposed; specific models are PSVM-2V and MCPK. In these models, views complement each other by acting as privileged information policies; however, a single view contains privileged information that can guide the classifier, and the existing framework does not consider it. In order to use this information to correct multi-view support vector machine classifier, we propose a framework for generating a series of small-scale views based on information hidden in a single view, which extends the original multi-view parallel structure to a hierarchical structure with sub-view mechanism. In this paper, two sub-view learning structures SL-PSVM-2V and SL-MCPK are constructed. The two new models fully exploit the data features in the view. Similarly, they follow the principles of consistency and complementarity. We use the standard quadratic programming solver to solve the new model. In 55 groups of classification experiments, the proposed model improves the classification accuracy by about 1.91% on the original basis. SL-MCPK ranks 1.3846 on average in the accuracy experiment, indicating that they have good classification ability. In addition, the computational time statistics and noise data set experiments are carried out to prove the effectiveness of the proposed method from multiple perspectives.",
        "DOI": "10.1007/s00500-023-07884-9",
        "affiliation_name": "Tianjin Agricultural University",
        "affiliation_city": "Tianjin",
        "affiliation_country": "China"
    },
    {
        "paper_title": "COVID-19 pandemic changes the recreational use of Moscow parks in space and time: Outcomes from crowd-sourcing and machine learning",
        "paper_author": "Matasov V.",
        "publication": "Urban Forestry and Urban Greening",
        "citied_by": "16",
        "cover_date": "2023-05-01",
        "Abstract": "The limited access to urban green spaces during the COVID-19 pandemic had a negative impact on the human-nature interaction in cities and human well-being. Number of visitors to green areas, initially declined due to imposed restrictions, was restored after they were lifted as established by several studies across the globe However, little is still known about changes in behavior and preferences of park visitors in the post-COVID time. In this study, we investigated spatial-temporal patterns of recreational activities in the three urban parks in Moscow (Russia) prior, during and after the COVID-19 lockdown (in 2019 and 2020). The selected parks represent two different types: a centrally located park with much infrastructure and open landscapes (Gorky Park) and parks located at the outskirts of the city center with a more forested landscape and little infrastructure (Timiryazevski and Sokolniki parks). Recreational activities were identified based on the analysis of social media photos using machine-learning algorithms. As expected, park closures during lockdown resulted in overall decrease in the number of taken photos. After the parks were re-opened, however, the number of photos did not grow immediately. The number of photos only restored after almost three months, and the visiting peak shifted to autumn. Differences between parks were related to the type of the park and its landscape structure. The lowest decrease in the number of photos was observed for the Timiryazevsky park – a semi-natural green area, while the centrally located Gorky Park was the most affected, likely due to the strictest control measures. In comparison to 2019, photos in 2020 were more evenly distributed across the area in all the three parks. Besides, ‘natural’ areas became the main attractors for the visitors - photos under ‘nature observation’ category became the most popular. Spatial distribution of the recreational activities in post-lockdown period was characterized by larger distances between photos, likely corresponding to the social distancing. COVID-19 pandemic highlighted the value of green areas for citizens, but also changed their recreational preferences and overall behavior in parks. The observed shift from high density of visitors around entertainments and attractions in 2019 to a more homogeneous and less dense distribution along the natural zones in 2020 reveals a new pattern in visitors behavior and preference, which shall be considered in spatial planning of the parks. Increasing availability of natural green areas and their integration in urban green infrastructures can become the most relevant policy to consider the crucial role of urban nature as a source of resilience in turbulent times.",
        "DOI": "10.1016/j.ufug.2023.127911",
        "affiliation_name": "Institute of Physicochemical and Biological Problems of Soil Science of the Russian Academy of Sciences",
        "affiliation_city": "Pushchino",
        "affiliation_country": "Russian Federation"
    },
    {
        "paper_title": "Childhood Family Environment and Osteoporosis in a Population-Based Cohort Study of Middle-to Older-Age Americans",
        "paper_author": "Courtney M.G.",
        "publication": "JBMR Plus",
        "citied_by": "4",
        "cover_date": "2023-05-01",
        "Abstract": "Demographic and early-life socioeconomic and parental investment factors may influence later-life health and development of chronic and progressive diseases, including osteoporosis, a costly condition common among women. The “long arm of childhood” literature links negative early-life exposures to lower socioeconomic attainment and worse adult health. We build on a small literature linking childhood socioeconomic status (SES) and bone health, providing evidence of whether associations exist between lower childhood SES and maternal investment and higher risk of osteoporosis diagnosis. We further examine whether persons identifying with non-White racial/ethnic groups experience underdiagnosis. Data from the nationally representative, population-based cohort Health and Retirement Study (N = 5,490–11,819) were analyzed for participants ages 50–90 to assess these relationships. Using a machine learning algorithm, we estimated seven survey-weighted logit models. Greater maternal investment was linked to lower odds of osteoporosis diagnosis (odds ratio [OR] = 0.80, 95% confidence interval [CI] = 0.69, 0.92), but childhood SES was not (OR = 1.03, 95% CI = 0.94, 1.13). Identifying as Black/African American (OR = 0.56, 95% CI = 0.40, 0.80) was associated with lower odds, and identifying as female (OR = 7.22, 95% CI = 5.54, 9.40) produced higher odds of diagnosis. There were differences in diagnosis across intersectional racial/ethnic and sex identities, after accounting for having a bone density scan, and a model predicting bone density scan receipt demonstrated unequal screening across groups. Greater maternal investment was linked to lower odds of osteoporosis diagnosis, likely reflecting links to life-course accumulation of human capital and childhood nutrition. There is some evidence of underdiagnosis related to bone density scan access. Yet results demonstrated a limited role for the long arm of childhood in later-life osteoporosis diagnosis. Findings suggest that (1) clinicians should consider life-course context when assessing osteoporosis risk and (2) diversity, equity, and inclusivity training for clinicians could improve health equity. © 2023 The Authors. JBMR Plus published by Wiley Periodicals LLC on behalf of American Society for Bone and Mineral Research.",
        "DOI": "10.1002/jbm4.10735",
        "affiliation_name": "University of La Verne",
        "affiliation_city": "La Verne",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Optimal Price Targeting",
        "paper_author": "Smith A.N.",
        "publication": "Marketing Science",
        "citied_by": "10",
        "cover_date": "2023-05-01",
        "Abstract": "We study the profitability of personalized pricing policies in a setting with consumer-level panel data. To compare pricing policies, we propose an inverse probability-weighted estimator of profits, discuss how to handle nonrandom price variation, and show how to apply it in a typical consumer-packaged good market with supermarket scanner data. We generate pricing policies from Bayesian hierarchical choice models, regularized regressions, neural networks, and nonparametric classifiers using different sets of data inputs. We find that the performance of machine learning methods is highly varied, ranging from a 30.7% loss to a 14.9% gain relative to a blanket couponing strategy, whereas hierarchical models generate profit gains in the range of 13−16.7%. Across all models, information on consumers’ purchase histories leads to large improvements in profits, whereas demographic information has only a small impact. We find that out-of-sample fit statistics are uncorrelated with profit estimates and provide poor guidance toward model selection.",
        "DOI": "10.1287/mksc.2022.1387",
        "affiliation_name": "UCL School of Management",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Urban habitats and food insecurity: Lessons learned throughout a pandemic",
        "paper_author": "Vaz E.",
        "publication": "Habitat International",
        "citied_by": "0",
        "cover_date": "2023-05-01",
        "Abstract": "Background: An increasing amount of literature raises the issue of food deserts and urban heterogeneity in larger metropolitan cores throughout North America. Specific to Canadian cities, the disparity between access to health, education, and affordable food is of growing concern. Recently, these drivers seem to be significantly linked to the propagation of COVID-19. This paper explores the spatially-explicit dynamics of food deserts in Toronto, by integrating Geographic Information Systems and machine learning to understand the clusters of food deserts. The integration of spatial analysis with self-organizing maps (SOM) offers insights on the relation between neighborhoods, geodemographic profiles and urban characteristics, and whether one might expect consequences of food insecurity given COVID-19. Methods: The paper starts out with developing a machine learning algorithm based on SOM to define meaningful clusters within the hedonic dataset. Further to this, an exploratory regression was built per cluster as to allow an exploratory spatial analysis to derive an explanatory framework for the key characteristics of socio-economic profiles within the Greater Toronto Area and impacts of SARS-CoV-2. Results: The findings suggest that there are clear spatial profiles within the urban core of Toronto in regards to food deserts, showing a direct relation between socioeconomic characteristics and the results on environmental injustice and livability. These profiles are strongly linked with the areas of COVID-19 occurrence, and share a very similar socio-demographic profile, particularly in regards to young and lower income families. Conclusion: There are several food deserts currently in Toronto, Ontario. The integration of policies that involve public health and spatial decision-support, particularly when linked to machine learning to aggregate characteristics of big data, establish a multi-functional understanding of the complexity of food security. This has a direct relation with diet, environment, and the opportunity to enhance subjective well-being in city cores.",
        "DOI": "10.1016/j.habitatint.2023.102779",
        "affiliation_name": "NOVA Information Management School, Universidade Nova de Lisboa",
        "affiliation_city": "Lisbon",
        "affiliation_country": "Portugal"
    },
    {
        "paper_title": "The role of renewable energy and total factor productivity in reducing CO2 emissions in Azerbaijan. Fresh insights from a new theoretical framework coupled with Autometrics",
        "paper_author": "Hasanov F.J.",
        "publication": "Energy Strategy Reviews",
        "citied_by": "44",
        "cover_date": "2023-05-01",
        "Abstract": "Environmental issues, such as carbon dioxide (CO2) emissions, are humanity's most critical issues. Emissions released by oil-producing countries is not small. This is because these countries have an abundance of fossil fuels with considerably low prices, and the cost of using these natural sources to obtain energy is significantly cheaper than using alternative energy sources. This study examined pollution in Azerbaijan, an oil-producing country. It uses a new theoretically grounded framework in which consumption-based CO2 emissions are a function of renewable energy consumption (REC) and total factor productivity (TFP), as well as income, exports, and imports. REC and TFP have not only emission-reducing properties but also growth-enhancing benefits and are, therefore, very useful to be considered in environmental policies. Econometric analysis was conducted with robustness checks and the state-of-the-art econometric methodology called Autometrics – a machine learning algorithm for model discovery – was employed. CO2 was found to be negatively affected by TFP and REC. Exports also exert a negative impact on CO2, while the effects of income and imports are positive. Our key policy insights are that Azerbaijani policymakers may wish to implement policies that further promote technological improvements, efficiency gains, and transitions to renewable energy.",
        "DOI": "10.1016/j.esr.2023.101079",
        "affiliation_name": "Baku Engineering University",
        "affiliation_city": "Absheron",
        "affiliation_country": "Azerbaijan"
    },
    {
        "paper_title": "T-Ridership: A web tool for reprogramming public transportation fleets to minimize COVID-19 transmission",
        "paper_author": "Imani S.",
        "publication": "SoftwareX",
        "citied_by": "0",
        "cover_date": "2023-05-01",
        "Abstract": "As the outbreak of novel coronavirus disease (COVID-19) continues to spread throughout the world, steps are being taken to limit the impact on public health. In the realm of infectious diseases like COVID-19, social distancing is one of the effective measures to avoid exposure to the virus and reduce its spread. Traveling on public transport can meaningfully facilitate the propagation of the transmission of infectious diseases. Accordingly, responsive actions taken by public transit agencies against risk factors can effectively limit the risk and make transit systems safe. Among the multitude of risk factors that can affect infection spread on public transport, the likelihood of exposure is a major factor that depends on the number of people riding the public transport and can be reduced by socially distanced settings. Considering that many individuals may not act in the socially optimal manner, the necessity of public transit agencies to implement measures and restrictions is vital. In this study, we present a novel web-based application, T-Ridership, based on a hybrid optimized dynamic programming inspired by neural networks algorithm to optimize public transit for safety with respect to COVID-19. Two main steps are taken in the analysis through Metropolitan Transportation Authority (MTA): detecting high-density stations by input data normalization, and then, using these results, the T-Ridership tool automatically determines optimal station order to avoid overcrowded transit vehicles. Effectively our proposed web tool helps public transit to be safe to ride under risk of infections by reducing the density of riders on public transit vehicles as well as trip duration. These results can be used in expanding on and improving policy in public transit, to better plan the scheduled time of trains and buses in a way that prevents high-volume human contact, increases social distance, and reduces the possibility of disease transmission (available at:http://t-ridership.com and GitHub at: https://github.com/Imani-Saba/TRidership).",
        "DOI": "10.1016/j.softx.2023.101350",
        "affiliation_name": "University of Southern California",
        "affiliation_city": "Los Angeles",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "The future of China's urban heat island effects: A machine learning based scenario analysis on climatic-socioeconomic policies",
        "paper_author": "Lan T.",
        "publication": "Urban Climate",
        "citied_by": "18",
        "cover_date": "2023-05-01",
        "Abstract": "Risk assessment and adaptation have become key foci in the examination of urban heat island (UHI) effects. Land use change and population growth are known to impact UHI effects. Taking the Changsha-Zhuzhou-Xiangtan urban agglomeration in China as the study context, a rapid and simple method based on artificial neural networks was employed to spatially estimate UHI effects from 2040 to 2100. Considering carbon neutrality and pro-natalist policies, Representative Concentration Pathways (RCPs) and Shared Socioeconomic Pathways (SSPs) were integrated into combination scenarios: RCP2.6-SSP1, RCP4.5-SSP2, and RCP4.5-SSP3. The risk of UHI effects was assessed through UHI hazard and population exposure. Results showed that compared with 2020, heat islands would decrease by 30% - 34% in area but increase by 29% - 37% in mean intensity by 2100. The population impacted by UHIs would increase by >10 million under the third scenario. Areas at high risk of UHI effects are mainly located in the urban core, with the total value of risk across the study area by 2100 being about 2 - 4 times higher than that in 2020. Our findings provide a forward-looking perspective to identify overall risk and critical areas in relation to UHI effects in the future through which clearer climatic-socioeconomic targets can be empirically grounded to minimize damage to public health.",
        "DOI": "10.1016/j.uclim.2023.101463",
        "affiliation_name": "Laboratory for Earth Surface Processes Ministry of Education",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Gated-Attention Model with Reinforcement Learning for Solving Dynamic Job Shop Scheduling Problem",
        "paper_author": "Gebreyesus G.",
        "publication": "IEEJ Transactions on Electrical and Electronic Engineering",
        "citied_by": "15",
        "cover_date": "2023-05-01",
        "Abstract": "Job shop scheduling problem (JSSP) is one of the well-known NP-hard combinatorial optimization problems (COPs) that aims to optimize the sequential assignment of finite machines to a set of jobs while adhering to specified problem constraints. Conventional solution approaches which include heuristic dispatching rules and evolutionary algorithms has been largely in use to solve JSSPs. Recently, the use of reinforcement learning (RL) has gained popularity for delivering better solution quality for JSSPs. In this research, we propose an end-to-end deep reinforcement learning (DRL) based scheduling model for solving the standard JSSP. Our DRL model uses attention-based encoder of Transformer network to embed the JSSP environment represented as a disjunctive graph. We introduced Gate mechanism to modulate the flow of learnt features by preventing noise features from propagating across the network to enrich the representations of nodes of the disjunctive graph. In addition, we designed a novel Gate-based graph pooling mechanism that preferentially constructs the graph embedding. A simple multi-layer perceptron (MLP) based action selection network is used for sequentially generating optimal schedules. The model is trained using proximal policy optimization (PPO) algorithm which is built on actor critic (AC) framework. Experimental results show that our model outperforms existing heuristics and state of the art DRL based baselines on generated instances and well-known public test benchmarks. © 2023 Institute of Electrical Engineers of Japan. Published by Wiley Periodicals LLC.",
        "DOI": "10.1002/tee.23788",
        "affiliation_name": "Waseda University",
        "affiliation_city": "Tokyo",
        "affiliation_country": "Japan"
    },
    {
        "paper_title": "Does Local Government Competition Reduce Environmental Governance Performance? The Role of Public Value Conflict and Media Sentiment",
        "paper_author": "Guan B.",
        "publication": "Administration and Society",
        "citied_by": "1",
        "cover_date": "2023-05-01",
        "Abstract": "With the increasing attention paid to environmental protection and sustainable development in various countries worldwide, the relationship between local government competition and environmental governance has become more subtle and complex. This paper provides new insight into their relationship based on public value theory and media sentiment perspective. Utilizing panel data from 2012 to 2019 in 216 cities in China, this study integrated Data Envelopment Analysis, Conflicting Attitudes Model, Computer-Aided Text Analysis, and machine learning-based sentiment analysis, as well as nonlinear mediation model to empirically test the relationships among local governments’ competition pressure, public value conflict, media sentiments, and environmental governance performance. The study found that: (1) Competition pressure and environmental governance performance exist in a “U-curved” relationship. (2) The core mechanism of the above relationship lies in the mediating role of public value conflict. Within a specific range, the public value conflict faced by local governments increases as competition pressure increases. This conflict would push local governments into a dilemma and induce them to commit misconduct. However, when competition pressure exceeds this range, the public value conflict faced by local governments will be weakened, leading environmental governance performance to rebound. (3) Negative media sentiments significantly alleviate the negative impact of public value conflict on environmental governance performance. This study helps researchers and policymakers recognize government competition’s influence on environmental governance from a public value perspective, with further exploration and confirmation of the moderating role of media sentiments. It also provides theoretical and policy enlightenment for rethinking the behavior logic of local government and solving the dilemma of local government environmental governance.",
        "DOI": "10.1177/00953997231157744",
        "affiliation_name": "Lanzhou University",
        "affiliation_city": "Lanzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Investigating on the robustness of flow-based intrusion detection system against adversarial samples using Generative Adversarial Networks",
        "paper_author": "Duy P.T.",
        "publication": "Journal of Information Security and Applications",
        "citied_by": "13",
        "cover_date": "2023-05-01",
        "Abstract": "Recently, Software Defined Networking (SDN) has emerged as the key technology in programming and orchestrating security policy in the security operations centers (SOCs) for heterogeneous networks. Typically, machine learning-based intrusion detection systems (ML-IDS) have been deployed and associated with SDN to leverage the features of a programmable network to defend against sophisticated cyberattacks in anomaly detection. Unfortunately, such ML-based IDSs are easily vulnerable to adversarial attacks due to the lack of diverse forms of malicious records in the training dataset. The missing data sample in the training phase can lead to a lower detection rate in real-world scenarios with adversarial settings. In this paper, we explore the ability of Wasserstein Generative Adversarial Networks with Gradient Penalty (WGAN-GP), WGAN-GP with two timescale update rule (WGAN-GP TTUR), and AdvGAN in generating perturbed attack samples to bypass attack detectors. Then, this approach is used to continuously evaluate the robustness of ML-based IDSs and then upgrade them as a service in SDN. The experimental results on CICIDS2018 and InSDN datasets demonstrate that generated adversarial samples can be used to fool targeted IDS. Later, those created samples can supplement the original ones in retraining IDS to improve the resilience of the attack detector.",
        "DOI": "10.1016/j.jisa.2023.103472",
        "affiliation_name": "VNUHCM - University of Information Technology",
        "affiliation_city": "Ho Chi Minh City",
        "affiliation_country": "Viet Nam"
    },
    {
        "paper_title": "A reinforcement learning approach to Automatic Voltage Regulator system",
        "paper_author": "Ayas M.S.",
        "publication": "Engineering Applications of Artificial Intelligence",
        "citied_by": "23",
        "cover_date": "2023-05-01",
        "Abstract": "An Automatic Voltage Regulator (AVR) system utilized to keep the terminal voltage of a synchronous generator at the desired level has received much attention among researchers. Designing an efficient and robust control scheme for the AVR system to maintain a specified voltage level is an important research area. From the control area perspective, reinforcement learning, an adaptive optimal control method, has received increasing attention in reference tracking problems. This article discusses a reinforcement learning approach to an AVR system and its experimental validation. A deep deterministic policy gradient (DDPG) agent working in continuous-time is designed offline to improve dynamic system characteristics of the AVR system besides its robustness against load disturbance, parameter uncertainties, and reference change. In the DDPG agent design process, the limits of the produced control signal are taken into account to perform a feasible simulation similar to a real-time application. The performance of the proposed learning-based controller is analyzed in three categories: transient and steady-state responses, stability analysis, and robustness analysis against parameter uncertainties, reference change, and load disturbance. A comparison with recently published papers employing Fuzzy-PID, PID-F, PIλDND2N2, PIDD2, and PID controllers in which various heuristic optimization algorithms were employed to optimally tune the controller parameters is made. Furthermore, to demonstrate that the behavior of the learning-based approach provides a stable and satisfactory performance, it is analyzed for a real synchronous generator connected to a 230 kV network using Matlab/Simulink environment. The results presented in this paper indicate that the proposed learning-based controller ensures the stability of the AVR system, significantly improves the regulating performance, and most impressively, is robust against parameter uncertainties, reference change, and load disturbance.",
        "DOI": "10.1016/j.engappai.2023.106050",
        "affiliation_name": "Erzurum Technical University",
        "affiliation_city": "Erzurum",
        "affiliation_country": "Türkiye"
    },
    {
        "paper_title": "Spatiotemporal modeling of PM<inf>10</inf> via committee method with in-situ and large scale information: Coupling of machine learning and statistical methods",
        "paper_author": "Mohammadi Y.",
        "publication": "Urban Climate",
        "citied_by": "3",
        "cover_date": "2023-05-01",
        "Abstract": "The aim of this paper is the spatiotemporal characterization of daily PM10 (1 km spatial resolution) using machine learning (ML) models (Random Forest (RF) and Gaussian Process Regression (GPR)) over Tehran as the most polluted city in Iran for policy-makers. The performances of the ML models (which are calibrated via large-scale and different spatially interpolated in situe information) were compared against a benchmark station-based interpolator named Inverse Distance Weighting (IDW) using various statistical metrics. Using hold-out training approach and 70% of the available data, the Kling-Gupta Efficiency (KGE) values of the validation (training) sites were achived 0.60(0.56), 0.61(0.72), and 0.68(0.6) for GPR, RF, and IDW, respectively. Based on the seasonal assessmentof the validation gauges, all models performed similarly well in spring and summer; however, the IDW and ML models had better accuracy in winter and autumn, respectively. Furthermore, the results of Correlated Triple Collocation (CTC) implied that ML-based techniques provided a more accurate spatial distribution over the computational grids. Based on the evaluation of the representated daily mean maps, the IDW produced “Bull's eyes” around the monitoring stations. Although IDW yielded reasonable site-based performance, the IDW method may not deliver a realistic estimation of the pollutant over the study region.",
        "DOI": "10.1016/j.uclim.2023.101494",
        "affiliation_name": "Shahid Beheshti University",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "Analysis of predictions considering mineral prices, residential energy, and environmental risk: Evidence from the USA in COP 26 perspective",
        "paper_author": "Jiang L.",
        "publication": "Resources Policy",
        "citied_by": "12",
        "cover_date": "2023-05-01",
        "Abstract": "The COP 26 member countries reaffirmed their commitment to the energy transition at the most recent 26th Conference of the Parties (COP 26) to reduce traditional energy and critical mineral as essential components to the growth of renewable energy transition. To fill the gap in the current literature, this study examined the effect of mineral price along with other essential factors like environmental risk, economic policy uncertainty, and residential energy use on energy transition. This study employs combined cubic support vector machine learning (CSVML) and Artificial Neural Networks (ANN) to predict energy transition through minerals prices, energy residential, economic policy uncertainty, and environmental risk. In doing so, the authors use the quarterly data from 1991Q1 to 2020Q4. The Cubic SVM has outperformed other models in terms of accuracy in predicting energy transition compared to ANN and other methods. In addition to comparing the findings of CSVM and ANN, it also gave policy implications. It has been predicted that minerals prices, low environmental risk, strong economic policy, and renewable energy use for residential purposes are the factor for a better energy transition in the US economy. As a result, our policy recommendations include maximizing mineral resources for clean energy transitions to achieve the COP 26 goal of a decarbonized or net-zero emissions trajectory for the twenty-first century.",
        "DOI": "10.1016/j.resourpol.2023.103431",
        "affiliation_name": "Guangzhou College of Commerce",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Autonomous anomaly detection on traffic flow time series with reinforcement learning",
        "paper_author": "He D.",
        "publication": "Transportation Research Part C: Emerging Technologies",
        "citied_by": "13",
        "cover_date": "2023-05-01",
        "Abstract": "This study develops an autonomous artificial intelligence (AI) agent to detect anomalies in traffic flow time series data, which can learn anomaly patterns from data without supervision, requiring no ground-truth labels for model training or knowledge of a threshold for anomaly definition. Specifically, our model is based on reinforcement learning, where an agent is built by a Long-Short-Term-Memory (LSTM) model and Q-learning algorithm to incorporate sequential information in time series data into policy optimization. The key contribution of our model is the development of a novel unsupervised reward learning algorithm that automatically learns the reward for an action taken by the agent based on the distribution of data, without requiring a manual specification of a reward function. To test the performance of our model, we conduct a comprehensive set of experimental study on both real-world data from Brisbane city, Australia, and synthetic data simulated according to the distribution of real-world data. We compare the performance of our model against three state-of-the-art models, and the experimental results show that our model outperforms the other models in different parameter settings, with around 90% precision, 80% recall, and 85% F1 score.",
        "DOI": "10.1016/j.trc.2023.104089",
        "affiliation_name": "The University of Queensland",
        "affiliation_city": "Brisbane",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Carpooling: Who is closest to adopting it? An investigation into the potential car-poolers among private vehicle users: A case of a developing country, India",
        "paper_author": "Saxena A.",
        "publication": "Transport Policy",
        "citied_by": "10",
        "cover_date": "2023-05-01",
        "Abstract": "The demand for transit is linked to a country's economic and population growth. As per the Ministry of Road Transport and Highways (MORTH), from 2009 to 2019, India witnessed an average growth rate of 9.9% per annum in registered vehicles. Even though the vehicle ownership is often viewed as a symbol of economic prosperity, the associated negative externalities (congestion, air pollution, road crashes, noise pollution, etc.) are substantial. To counter this, carpooling has been proposed as a solution globally. However, there is a paucity of literature for analysing the difference in travel characteristics of car-poolers among other private vehicle users. The present study aims to bridge this gap by comparing the demographic and travel characteristics of existing car-poolers with two-wheelers and single-occupant car users in Gurugram in the national capital region of India. Using an unsupervised machine learning clustering method (two-step cluster), the findings of the present study indicate that the profile of single-occupant car (SOC) users and car-poolers are highly distinguished from two-wheeler users. However, car-poolers have similar characteristics to single-occupant car users, except for travel cost, income level, and gender, suggesting that SOC users are potential car-poolers. A comparative study of this kind would help city planners and policymakers identify prospective car-poolers, and formulate policies to encourage SOC users to adopt carpooling and promote urban transportation sustainability.",
        "DOI": "10.1016/j.tranpol.2023.03.007",
        "affiliation_name": "Indian Institute of Technology Bombay",
        "affiliation_city": "Mumbai",
        "affiliation_country": "India"
    },
    {
        "paper_title": "ReBADD-SE: Multi-objective molecular optimisation using SELFIES fragment and off-policy self-critical sequence training",
        "paper_author": "Choi J.",
        "publication": "Computers in Biology and Medicine",
        "citied_by": "6",
        "cover_date": "2023-05-01",
        "Abstract": "The discovery of drugs to selectively remove disease-related cells is challenging in computer-aided drug design. Many studies have proposed multi-objective molecular generation methods and demonstrated their superiority using the public benchmark dataset for kinase inhibitor generation tasks. However, the dataset does not contain many molecules that violate Lipinski's rule of five. Thus, it remains unclear whether existing methods are effective in generating molecules violating the rule, such as navitoclax. To address this, we analysed the limitations of existing methods and propose a multi-objective molecular generation method with a novel parsing algorithm for molecular string representation and a modified reinforcement learning method for the efficient training of multi-objective molecular optimisation. The proposed model had success rates of 84% in GSK3b+JNK3 inhibitor generation and 99% in Bcl-2 family inhibitor generation tasks.",
        "DOI": "10.1016/j.compbiomed.2023.106721",
        "affiliation_name": "Yonsei University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "Regret-Optimal Estimation and Control",
        "paper_author": "Goel G.",
        "publication": "IEEE Transactions on Automatic Control",
        "citied_by": "12",
        "cover_date": "2023-05-01",
        "Abstract": "In this article, we consider estimation and control in linear dynamical systems from the perspective of regret minimization. Unlike most prior work in this area, we focus on the problem of designing causal state estimators and causal controllers, which compete against a clairvoyant noncausal policy, instead of the best policy selected in hindsight from some fixed parametric class. We show that regret-optimal filters and regret-optimal controllers can be derived in state space form using operator-theoretic techniques from robust control. Our results can be viewed as extending traditional robust estimation and control, which focuses on minimizing worst-case cost, to minimizing worst-case regret. We propose regret-optimal analogs of model-predictive control and the extended Kalman filter for systems with nonlinear dynamics and present numerical experiments which show that these algorithms can significantly outperform standard approaches to estimation and control.",
        "DOI": "10.1109/TAC.2023.3253304",
        "affiliation_name": "California Institute of Technology Division of Engineering and Applied Science",
        "affiliation_city": "Pasadena",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Flexible Resource Management in High-Throughput Satellite Communication Systems: A Two-Stage Machine Learning Framework",
        "paper_author": "Zhao D.",
        "publication": "IEEE Transactions on Communications",
        "citied_by": "12",
        "cover_date": "2023-05-01",
        "Abstract": "With digitization and globalization in the era of 5G and beyond, research on high-throughput satellites (HTS) to increase communication capacity and improve flexibility is becoming essential. To achieve efficient resource utilization and dynamic traffic demand matching, the multi-dimensional resource management (MDRM) problem of the HTS communication system has been studied in this paper. Since the MDRM problem is a non-convex mixed integer problem, we decompose it into two tractable sub-problems. First, the beam-domain resource configuration problem is formed to enable on-demand coverage. Next, the user-domain resource allocation problem is modeled to enable on-demand communication. Considering the two-domain optimization problem, a two-stage framework is developed based on the combination of self-supervised learning and deep reinforcement learning. Specifically, in the first stage, a maximum co-channel interference based self-supervised learning method is proposed to perform traffic demand matching through demand awareness. In the second stage, a soft frequency reuse based proximal policy optimization approach is presented to further increase the system capacity through interference coordination. The simulation results demonstrate that our proposed two-stage algorithm outperforms the benchmark schemes in terms of spectrum efficiency and demand satisfaction.",
        "DOI": "10.1109/TCOMM.2023.3255239",
        "affiliation_name": "China Academy of Space Technology",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Peer-to-peer energy sharing and trading of renewable energy in smart communities ─ trading pricing models, decision-making and agent-based collaboration",
        "paper_author": "Zhou Y.",
        "publication": "Renewable Energy",
        "citied_by": "103",
        "cover_date": "2023-05-01",
        "Abstract": "Peer-to-peer (P2P) energy sharing can complement other energy management strategies needed in the energy transition to clean energy such as renewables. The recent advances in artificial intelligence, machine learning and internet of things can provide cutting-edge solutions to dynamic information interaction and power exchange, enabling efficient P2P energy sharing and trading schemes. However, the current electricity market fails to comprehensively consider devices degradation costs and transmission losses, fairness distribution and allocation on cost-benefit model, synergistic collaborations and operations for different stakeholders. Here, a comprehensive review on P2P energy sharing and trading is presented covering novel system configurations, energy sharing and marginal/trading price mechanisms, and decision-making in dynamic P2P energy trading combined with synergistic collaboration and operation among key stakeholders. Significances of this review include comprehensive consideration on internal trading pricing, fairness distribution and allocation with active participation in P2P energy sharing, synergistic collaborations and operations with mutual economic benefits. Modelling tools for P2P energy trading platforms for real applications are also reviewed. The review shows that P2P when optimally designed can provide win-win economic benefits to all related stakeholders such as prosumers, consumers, retailers, and aggregators with fair access to distributed energy resources. The P2P energy sharing can improve the system efficiency, reduce energy storage capacity and primary energy consumption, improve renewable penetration, avoid energy quality devaluation and mitigate stressed grid power together with voltage support and congestion management for the local power grid. Employing blockchain technology improves the automation level in P2P thus decreasing human interactions improving the security with transparent, tamper-proof and secure systems, and accelerating real-time settlements with smart contracts. Synergistic collaboration and operation among stakeholders can ensure economic benefits for each participant, fair energy distribution and cost allocation, and decrease reliance on retailers or aggregators. The results presented provide cutting-edge guidelines for increasing the use and acceptance of smart and fair P2P energy sharing and trading frameworks.",
        "DOI": "10.1016/j.renene.2023.02.125",
        "affiliation_name": "The Hong Kong University of Science and Technology (Guangzhou)",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Monitoring spatial patterns of urban vegetation: A comparison of contemporary high-resolution datasets",
        "paper_author": "Duncan J.M.A.",
        "publication": "Landscape and Urban Planning",
        "citied_by": "14",
        "cover_date": "2023-05-01",
        "Abstract": "Fine spatial resolution urban vegetation datasets are crucial for monitoring change in green space and guiding planning and policy initiatives to promote liveable and sustainable cities. Previous studies have demonstrated that finer spatial resolution products are more suited to monitoring urban vegetation than coarse spatial resolution datasets. However, there are differences in the generation of fine spatial resolution datasets that could affect how urban vegetation is represented. To explore how sensitive monitoring and analysis tasks are to the choice of vegetation dataset, a series of comparative analyses were undertaken using three fine spatial resolution datasets in Perth, Western Australia. A technique for quantitative comparison of spatial pattens was used to compare vegetation datasets. There were large areas of Perth where spatial patterns of vegetation were substantially different across datasets. The level of differences in spatial patterns between datasets varied with geographic context such as land use. Elements of a spatial pattern related to grass and tree composition were similar across datasets but there were differences in how shrubs and the configuration of vegetation features were captured. There were differences in the temporal change detection of vegetation patterns across datasets. Spatial patterns of vegetation and land cover generated using the three datasets were used as predictor variables of surface temperatures in machine learning workflows. In some cases, the models learned dataset specific relationships between elements of a vegetation pattern and surface temperature outcomes. In a final modelling analysis, spatial patterns of vegetation were considered as an outcome responding to a disturbance event, a change in dwelling density. The size of the effect of dwelling density change on vegetation patterns varied across vegetation datasets.",
        "DOI": "10.1016/j.landurbplan.2022.104671",
        "affiliation_name": "The University of Western Australia",
        "affiliation_city": "Perth",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "A segmented optimal PID method to consider both regulation performance and damping characteristic of hydroelectric power system",
        "paper_author": "Dong W.",
        "publication": "Renewable Energy",
        "citied_by": "10",
        "cover_date": "2023-05-01",
        "Abstract": "Hydropower has become the main force of grid frequency regulation due to its regulation flexibility and rapid response characteristics. However, when the operating conditions are changed, the continued pursuit of a faster response speed deteriorates the damping characteristics of the system with wider water-head, causing low-frequency oscillations, threatening the power station's safety and stability and the power grid. In this study, an improved quantized damping method is applied. The settling time and damping coefficient variation under different PID parameters and operating conditions are recorded, revealing the contradiction between regulation performance and damping characteristics. Then the segmented optimal PID controller is proposed to balance this contradiction. The twin-delayed deep deterministic policy gradient learning algorithm enables the controller to find the optimal policy online. With the settling time-damping threshold control strategy, the controller optimizes parameters according to operating conditions, and changes parameters when reaching the threshold. The results show that, compared with using a set of PID parameters, the damping of the hydropower system is increased by 0.35 from negative to positive. In contrast, the settling time increases by 11.63s within limits. The proposed controller ensures the safe and coordinated operation of the power grid and the power station of the hydroelectric power system with a wide water-head.",
        "DOI": "10.1016/j.renene.2023.02.091",
        "affiliation_name": "Amrita Vishwa Vidyapeetham Chennai Campus",
        "affiliation_city": "Vengal",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Socio-ecological drivers of demersal fishing activity in the North Sea: The case of three German fleets",
        "paper_author": "Letschert J.",
        "publication": "Ocean and Coastal Management",
        "citied_by": "7",
        "cover_date": "2023-05-01",
        "Abstract": "Worldwide, fisheries face the consequences of climate change and compete with expanding human activities at sea, which may trigger unforeseen reactions of fishers. Hence, knowledge on drivers of fishing behavior is crucial for management and needs to be integrated in resource management policies. In this study, we identify factors influencing fishing activity of North Sea demersal fleets. First, we explore drivers of the North Sea demersal fisheries in scientific literature. Subsequently, we study the effects of identified drivers on the spatio-temporal dynamics of German demersal fisheries using boosted regression trees (BRT), a supervised machine learning technique. An exploratory literature review revealed a lack of studies incorporating biophysical, economic and socio-cultural fishing drivers in a single quantitative analysis. Our BRT analysis contributed to filling this research gap and highlighted the importance of biophysical drivers such as temperature, salinity, and bathymetry for fishing behavior. Contrary to findings of previous studies, our empirical analysis identified quotas and market prices to be irrelevant, except for low brown shrimp prices, which counter-intuitively increased fishing effort. Moreover, economic and socio-cultural variables influencing brown shrimp fishing effort differed from the other fleets, especially determined by increased effort on workdays and reduced effort when fuel prices were high. Our findings provide key information for marine spatial planning and supports the integration of fishing fleet behavior into policies.",
        "DOI": "10.1016/j.ocecoaman.2023.106543",
        "affiliation_name": "Universität Hamburg",
        "affiliation_city": "Hamburg",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Temporal dynamic assessment of household energy consumption and carbon emissions in China: From the perspective of occupants",
        "paper_author": "Su S.",
        "publication": "Sustainable Production and Consumption",
        "citied_by": "19",
        "cover_date": "2023-05-01",
        "Abstract": "Global warming has become a challenge and reducing carbon emissions is an urgent task. Household energy consumption and carbon emissions are substantial and need to be analyzed and assessed. Considering the significant influence of occupants and the possible dynamic changes during the long operation process of buildings, this study proposes a dynamic household energy consumption and carbon emissions assessment model from the occupant's perspective. The model consists of four modules of occupant information collection, energy calculation, machine learning models, and dynamic carbon prediction. Five major energy sources are assessed: space cooling, space heating, hot water, cooking, and domestic appliances. The temporal variations in occupant profiles, behaviors, and the carbon factor of energy are quantified and taken into account. The average carbon emissions of a household during 2020–2060 in three dynamic scenarios are assessed, and general downward trends are revealed. The specific dynamic carbon levels of “double income, no kids” (DINK) households, nuclear households, and three-generation households are also quantified, and obvious temporal changes and significant differences are found. The influence of China's childbearing policy is quantified and discussed. The dynamic assessment results are compared with the static results, showing that the largest accumulated difference during 41 years can reach 25.1 tCO2-eq. This paper proposes an applicable dynamic assessment model from the perspective of occupants, with temporal variations considered. The assessment results provide a strong reference for future energy-saving and emission-reduction plans at the household level.",
        "DOI": "10.1016/j.spc.2023.02.014",
        "affiliation_name": "Bond University",
        "affiliation_city": "Gold Coast",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Energy efficiency and delay determinacy tradeoff in energy harvesting-powered zero-touch deterministic industrial M2M communications",
        "paper_author": "Xu Y.H.",
        "publication": "Engineering Applications of Artificial Intelligence",
        "citied_by": "3",
        "cover_date": "2023-05-01",
        "Abstract": "Data transmission for Industrial Internet of Things (IIoTs) is of the utmost importance, especially in the Industry 5.0 era, where human–machine collaboration is increasingly intensive. On the one hand, as a key characteristic for Industry 5.0, deterministic transmission aims to ensure the data arrive at destination devices accurately, has attracted remarkable attentions. On the other hand, energy efficiency problem is another major concern in Industry 5.0 since the massive Machine-Type Devices (MTDs). Consequently, in this paper, we investigate the tradeoff between energy efficiency and delay determinacy in energy harvesting-powered Zero-Touch Deterministic Industrial Machine-to-Machine (ZT-DI-M2M) communications. In particular, we first derive the probability of the transmission delay falling within a certain time window to characterize the delay determinacy and then figure out the relationship between delay determinacy and the energy efficiency by taking the system control performance into account. After that, in order to model the uncertainty of the stochastic environment, we formulate the problem as a stochastic game by jointly considering transmission power control, frequency spectrum allocation and the selection of base stations. Afterwards, a random graph-based sparse Long Short-Term Memory (LSTM) network is proposed to solve the optimization problem while reducing the computational complexity. Finally, numerical result demonstrates that the proposed sparse LSTM algorithm enables to achieve a specific delay determinacy with a higher energy efficiency as compared to Deep Q-Learning Network (DQN) and Deep Deterministic Policy Gradient (DDPG) algorithms. In addition, we also analysis the influence of sparse coefficient and the size of time window on the relationship between energy efficiency and delay determinacy.",
        "DOI": "10.1016/j.engappai.2023.105997",
        "affiliation_name": "Nanjing Forestry University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Nowcasting Chinese GDP in a data-rich environment: Lessons from machine learning algorithms",
        "paper_author": "Zhang Q.",
        "publication": "Economic Modelling",
        "citied_by": "8",
        "cover_date": "2023-05-01",
        "Abstract": "The ability to estimate current GDP growth before official data are released, known as “nowcasting”, is crucial for the Chinese government to effectively implement economic policy and manage economic uncertainties; however, there is limited research on nowcasting China's GDP in a data-rich environment. We evaluate the performance of various machine learning algorithms, dynamic factor models, static factor models, and MIDAS regressions in nowcasting the Chinese annualised real GDP growth rate in pseudo out-of-sample exercise, using 89 macroeconomic variables from years 1995 to 2020. We find that some machine learning methods outperform the benchmark dynamic factor model. The machine learning method that deserves more attention is ridge regression, which dominates all other models not only in terms of nowcast error but also in effective recognition of the impacts of the Global Financial Crisis and Covid-19 shocks. Policy-wise, our study guides practitioners in selecting appropriate nowcasting models for China's macroeconomy.",
        "DOI": "10.1016/j.econmod.2023.106204",
        "affiliation_name": "Zhejiang Gongshang University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Multiparameter Identification of Bridge Cables Using XGBoost Algorithm",
        "paper_author": "Zhang H.",
        "publication": "Journal of Bridge Engineering",
        "citied_by": "18",
        "cover_date": "2023-05-01",
        "Abstract": "Accurately identifying tension force on cables is of great significance for construction control and the operational status assessment of a bridge during its lifetime. Unlike the conventional vibration methods that encounter problems in the inaccurate identification of short cables and difficulties when identifying multiparameters simultaneously, when solving the vibration differential equation inversely, a novel strategy was proposed that was based on an intelligent algorithm for cable parameter monitoring onsite. The Extreme Gradient Boosting (XGBoost) model was employed to establish the mapping relationship between the natural frequencies of the cable and its tension, bending stiffness, and boundary conditions through data mining. The results revealed that when the measured natural frequencies of a cable were fed into the XGBoost model, the previously mentioned multiparameters could be identified simultaneously with a relative error of <5%. Meanwhile, the proposed intelligent method with the XGBoost algorithm produced a more accurate identification of the cable parameters than the extreme learning machine (ELM) and conventional vibration methods. The current intelligent strategy might provide efficient tools for the simultaneous identification of multiple parameters in cables and, therefore, might facilitate policy decisions for the structural maintenance of cable-supported bridges.",
        "DOI": "10.1061/JBENF2.BEENG-6021",
        "affiliation_name": "College of Civil Engineering and Architecture Zhejiang University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Who are you? Cartel detection using unlabeled data",
        "paper_author": "Silveira D.",
        "publication": "International Journal of Industrial Organization",
        "citied_by": "5",
        "cover_date": "2023-05-01",
        "Abstract": "We propose a data-driven machine learning approach to flag bid-rigging cartels in the Brazilian road maintenance sector. First, we apply a clustering algorithm to group the tenders based on their attributes. Second, we use the labels created by the clustering algorithm as a target variable to predict them using a classifier. We rank the screens according to their relevance to decrease the number of false positive (detecting cartel when it does not exist) and false negative (not detecting cartel when it does exist) predictions. Our results shed light on the need to use a range of screens to recognize the vast profile of strategies practiced by bid-rigging cartels, such as misleading competitive dynamics, bid combination, and cover bidding behavior. Our method can improve cartels’ deterrence in different economic sectors, especially when labeled data are not available. In a controlled environment with a simulated labeled dataset, the overall average accuracy of the algorithm is 99.33%. In a real-world cartel case with a labeled dataset, the overall average accuracy is 80.25%. When applied to the road maintenance unlabeled dataset, our model identified a group containing 273 (31% of the total) suspicious tenders. We conclude by offering a policy prescription discussion for antitrust authorities.",
        "DOI": "10.1016/j.ijindorg.2023.102931",
        "affiliation_name": "University of Alberta",
        "affiliation_city": "Edmonton",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Modelling forest biomass dynamics in relation to climate change in Romania using complex data and machine learning algorithms",
        "paper_author": "Prăvălie R.",
        "publication": "Stochastic Environmental Research and Risk Assessment",
        "citied_by": "8",
        "cover_date": "2023-05-01",
        "Abstract": "Forest biomass controls climate stability, many ecological processes and various ecosystem services. This study analyzes for the first time the recent changes (1987–2018) of forest above-ground live biomass (AGB) in Romania, based on a complex volume of remote sensing and forest inventory data that were modelled yearly using a series of sophisticated statistical algorithms. Subsequently, after modelling interannual AGB data, yearly raster values (~ 2 billion total pixel values) were explored as trends over the 32 years, using the Sen's slope estimator and Mann–Kendall test. A large volume of climate data was also processed in this study, in order to detect possible statistical relationships between climate and forest biomass, after 1987. Results showed a mean multiannual value of forest biomass of ~ 185 t/ha and a total AGB amount (stock) of about 1.25 billion tons (~ 1249 million tons or megatonnes/Mt) across Romania. Regarding forest biomass changes, findings revealed increasing and decreasing AGB trends that account for ~ 70% and 30%, respectively, of the countrywide forest biomass changes. However, it was found that about half (~ 48%) of all positive AGB trends are statistically significant, while negative AGB trends have a statistical confidence on only one-fifth (~ 21%) of their spatial footprint in Romania. Overall, upon averaging and summing up all statistically significant values of positive and negative trends, an average AGB increase of ~ 3 t/ha/yr and a total forest biomass gain of ~ 205 Mt were found in Romania, over the entire 1987–2018 period. The various regional statistics highlight a more complex picture of AGB changes across the country. The analysis of interannual eco-climate data indicated a low to moderate climate signal in AGB changes, revealing that climate change is not a major driving force of AGB dynamics, at least according to the data and methodology applied in this study. The results can be useful to governmental forestry, climate and sustainable development policies in Romania.",
        "DOI": "10.1007/s00477-022-02359-z",
        "affiliation_name": "National Institute for Research and Development in Forestry,\"Marin Drăcea\"",
        "affiliation_city": "Voluntari",
        "affiliation_country": "Romania"
    },
    {
        "paper_title": "Learning to Act Safely With Limited Exposure and Almost Sure Certainty",
        "paper_author": "Castellano A.",
        "publication": "IEEE Transactions on Automatic Control",
        "citied_by": "1",
        "cover_date": "2023-05-01",
        "Abstract": "This article puts forward the concept that learning to take safe actions in unknown environments, even with probability one guarantees, can be achieved without the need for an unbounded number of exploratory trials. This is indeed possible, provided that one is willing to navigate tradeoffs between optimality, level of exposure to unsafe events, and the maximum detection time of unsafe actions. We illustrate this concept in two complementary settings. We first focus on the canonical multiarmed bandit problem and study the intrinsic tradeoffs of learning safety in the presence of uncertainty. Under mild assumptions on sufficient exploration, we provide an algorithm that provably detects all unsafe machines in an (expected) finite number of rounds. The analysis also unveils a tradeoff between the number of rounds needed to secure the environment and the probability of discarding safe machines. We then consider the problem of finding optimal policies for a Markov decision process (MDP) with almost sure constraints. We show that the action-value function satisfies a barrier-based decomposition that allows for the identification of feasible policies independently of the reward process. Using this decomposition, we develop a barrier-learning algorithm, that identifies such unsafe state-action pairs in a finite expected number of steps. Our analysis further highlights a tradeoff between the time lag for the underlying MDP necessary to detect unsafe actions, and the level of exposure to unsafe events. Simulations corroborate our theoretical findings, further illustrating the aforementioned tradeoffs, and suggesting that safety constraints can speed up the learning process.",
        "DOI": "10.1109/TAC.2023.3240925",
        "affiliation_name": "Swanson School of Engineering",
        "affiliation_city": "Pittsburgh",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Unpacking the inter- and intra-urban differences of the association between health and exposure to heat and air quality in Australia using global and local machine learning models",
        "paper_author": "Wang S.",
        "publication": "Science of the Total Environment",
        "citied_by": "11",
        "cover_date": "2023-05-01",
        "Abstract": "Environmental stressors including high temperature and air pollution cause health problems. However, understanding how the combined exposure to heat and air pollution affects both physical and mental health remains insufficient due to the complexity of such effects mingling with human society, urban and natural environments. Our study roots in the Social Ecological Theory and employs a tri-environmental conceptual framework (i.e., across social, built and natural environment) to examine how the combined exposure to heat and air pollution affect self-reported physical and mental health via, for the first time, the fine-grained nationwide investigation in Australia and highlight how such effects vary across inter- and intra-urban areas. We conducted an ecological study to explore the importance of heat and air quality to physical and mental health by considering 48 tri-environmental confounders through the global and local random forest regression models, as advanced machine learning methods with the advantage of revealing the spatial heterogeneity of variables. Our key findings are threefold. First, the social and built environmental factors are important to physical and mental health in both urban and rural areas, and even more important than exposure to heat and air pollution. Second, the relationship between temperature and air quality and health follows a V-shape, reflecting people's different adaptation and tolerance to temperature and air quality. Third, the important roles that heat and air pollution play in physical and mental health are most obvious in the inner-city and near inner-city areas of the major capital cities, as well as in the industrial zones in peri-urban regions and in Darwin city with a low-latitude. We draw several policy implications to minimise the inter- and intra-urban differences in healthcare access and service distribution to populations with different sensitivity to heat and air quality across urban and rural areas. Our conceptual framework can also be applied to examine the relationship between other environmental problems and health outcomes in the era of a warming climate.",
        "DOI": "10.1016/j.scitotenv.2023.162005",
        "affiliation_name": "The University of Queensland",
        "affiliation_city": "Brisbane",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Determining the gender wage gap through causal inference and machine learning models: evidence from Chile",
        "paper_author": "Kristjanpoller W.",
        "publication": "Neural Computing and Applications",
        "citied_by": "4",
        "cover_date": "2023-05-01",
        "Abstract": "In the last decades, there has been increasing awareness of the different types of inequalities that women experience. A very important inequality is the wage gap. Understanding the elements that affect this gap is crucial in order for governments to take the right actions to diminish the gap. It is also important to understand the broader context in which this inequality has evolved over time. In this paper, we develop a causal inference model based on the ideas of Potential Outcome (PO) and Metalearners (ML) to address this important issue. We include a time variable in the causal analysis which helps to determine how the effects have evolved over the last decades. We apply data from 1990 to 2017 from the official government social survey of Chile to fit the models. We then make a deep analysis of each variable using the SHAP framework to see the impact of each variable on the gender wage gap. Sadly, our results indicate that there has been a gap between the earnings of men and women over the last three decades, and the gap actually widened over time. We also find that variable decomposition helps to clarify the different effects as some variables clearly help to diminish this gap. Our results may assist the government of Chile and other organizations to endorse policies that may reduce the gap.",
        "DOI": "10.1007/s00521-023-08221-9",
        "affiliation_name": "Joseph M. Katz Graduate School of Business",
        "affiliation_city": "Pittsburgh",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Exploring the destination image based on the perspective of tourists’ expression using machine learning methods combined with PLTS-PT",
        "paper_author": "Luo Y.",
        "publication": "Soft Computing",
        "citied_by": "3",
        "cover_date": "2023-05-01",
        "Abstract": "With the acceleration of Internet informatization and the vigorous development of social media, tourists are inclined to share their travel feelings online. This paper proposes a new framework for exploring the image of destinations from the perspective of tourists based on the online reviews of scenic spots. This study divides the image of destinations into three dimensions, mining the information of online reviews through statistical analysis and text matching, and analyzing the LDA topic model and emotional tendency. Combining the theory and method of probabilistic linguistic term set to obtain the element endowment information of scenic spots and form the destination image. Finally, the proposed method is successfully applied to the image perception of 10 5A scenic spots, and relevant policy suggestions are provided for managers.",
        "DOI": "10.1007/s00500-023-07815-8",
        "affiliation_name": "Chengdu University of Technology",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Development of an integrated machine-learning and data assimilation framework for NO<inf>x</inf> emission inversion",
        "paper_author": "Chen Y.",
        "publication": "Science of the Total Environment",
        "citied_by": "7",
        "cover_date": "2023-05-01",
        "Abstract": "As major air pollutants, nitrogen oxides (NOx, mainly comprising NO and NO2) not only have adverse effects on human health but also contribute to the formation of secondary pollutants, such as ozone and particulate nitrate. To acquire reasonable NOx simulation results for further analysis, a reasonable emission inventory is needed for three-dimensional chemical transport models (3D-CTMs). In this study, a comprehensive emission adjustment framework for NOx emission, which integrates the simulation results of the 3D-CTM, surface NO2 measurements, the three-dimensional variational data assimilation method, and an ensemble back propagation neural network, was proposed and applied to correct NOx emissions over China for the summers of 2015 and 2020. Compared with the simulation using prior NOx emissions, the root-mean-square error, normalized mean error, and normalized mean bias decreased by approximately 40 %, 40 %, and 60 % in NO2 simulation using posterior NOx emissions corrected by the framework proposed in this work. Compared with the emissions for 2015, the NOx emission generally decreased by an average of 5 % in the simulation domain for 2020, especially in Henan and Anhui provinces, where the percentage reductions reached 24 % and 19 %, respectively. The proposed framework is sufficiently flexible to correct emissions in other periods and regions. The framework can provide reliable and up-to-date emission information and can thus contribute to both scientific research and policy development relating to NOx pollution.",
        "DOI": "10.1016/j.scitotenv.2023.161951",
        "affiliation_name": "Department of Computer Science",
        "affiliation_city": "College Park",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "DroidRL: Feature selection for android malware detection with reinforcement learning",
        "paper_author": "Wu Y.",
        "publication": "Computers and Security",
        "citied_by": "34",
        "cover_date": "2023-05-01",
        "Abstract": "Due to the completely open-source nature of Android, the exploitable vulnerability of malware attacks is increasing. Machine learning, leading to a great evolution in Android malware detection in recent years, is typically applied in the classification phase. Since the correlation between features is ignored in some traditional ranking-based feature selection algorithms, applying wrapper-based feature selection models is a topic worth investigating. Though considering the correlation between features, wrapper-based approaches are time-consuming for exploring all possible valid feature subsets when processing a large number of Android features. To reduce the computational expense of wrapper-based feature selection, a framework named DroidRL is proposed. The framework deploys DDQN algorithm to obtain a subset of features which can be used for effective malware classification. To select a valid subset of features over a larger range, the exploration-exploitation policy is applied in the model training phase. The recurrent neural network (RNN) is used as the decision network of DDQN to give the framework the ability to sequentially select features. Word embedding is applied for feature representation to enhance the framework's ability to find the semantic relevance of features. The framework's feature selection exhibits high performance without any human intervention and can be ported to other feature selection tasks with minor changes. The experiment results show a significant effect when using the Random Forest as DroidRL's classifier, which reaches 95.6% accuracy with only 24 features selected.",
        "DOI": "10.1016/j.cose.2023.103126",
        "affiliation_name": "Business School of Sichuan University",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Neighbourhood-related socioeconomic perinatal health inequalities: An illustration of the mediational g-formula and considerations for the big data context",
        "paper_author": "Ochoa L.B.",
        "publication": "Paediatric and Perinatal Epidemiology",
        "citied_by": "1",
        "cover_date": "2023-05-01",
        "Abstract": "Background: Advances in computing power have enabled the collection, linkage and processing of big data. Big data in conjunction with robust causal inference methods can be used to answer research questions regarding the mechanisms underlying an exposure–outcome relationship. The g-formula is a flexible approach to perform causal mediation analysis that is suited for the big data context. Although this approach has many advantages, it is underused in perinatal epidemiology and didactic explanation for its implementation is still limited. Objective: The aim of this was to provide a didactic application of the mediational g-formula by means of perinatal health inequalities research. Methods: The analytical procedure of the mediational g-formula is illustrated by investigating whether the relationship between neighbourhood socioeconomic status (SES) and small for gestational age (SGA) is mediated by neighbourhood social environment. Data on singleton births that occurred in the Netherlands between 2010 and 2017 (n = 1,217,626) were obtained from the Netherlands Perinatal Registry and linked to sociodemographic national registry data and neighbourhood-level data. The g-formula settings corresponded to a hypothetical improvement in neighbourhood SES from disadvantaged to non-disadvantaged. Results: At the population level, a hypothetical improvement in neighbourhood SES resulted in a 6.3% (95% confidence interval [CI] 5.2, 7.5) relative reduction in the proportion of SGA, that is the total effect. The total effect was decomposed into the natural direct effect (5.6%, 95% CI 5.1, 6.1) and the natural indirect effect (0.7%, 95% CI 0.6, 0.9). In terms of the magnitude of mediation, it was observed the natural indirect effect accounted for 11.4% (95% CI 9.2, 13.6) of the total effect of neighbourhood SES on SGA. Conclusions: The mediational g-formula is a flexible approach to perform causal mediation analysis that is suited for big data contexts in perinatal health research. Its application can contribute to providing valuable insights for the development of policy and public health interventions.",
        "DOI": "10.1111/ppe.12954",
        "affiliation_name": "Erasmus MC",
        "affiliation_city": "Rotterdam",
        "affiliation_country": "Netherlands"
    },
    {
        "paper_title": "An Efficient Distributed Machine Learning Framework in Wireless D2D Networks: Convergence Analysis and System Implementation",
        "paper_author": "Cheng K.",
        "publication": "IEEE Transactions on Vehicular Technology",
        "citied_by": "5",
        "cover_date": "2023-05-01",
        "Abstract": "Facing the heavy traffic burden and data privacy, distributed machine learning (DML) has been envisioned as a promising computing paradigm to enable edge intelligence by extracting and establishing models collaboratively in wireless networks. Unlike centralized methods, DML involves frequent local model exchanging among distributed devices, which confronts low efficiency referring to the convergence rate and delay. To enable edge intelligence, how to train DML efficiently has recently attracted extensive interest in wireless networks. Rather than over a fixed or centralized topology, we study the convergence and system implementation of DML over a wireless device-to-device (D2D) network in this paper. First, we introduce the DML training process and system model in this network, where the total delay in reaching convergence is minimized. Second, we analyze the convergence rate to figure out the special effects of synchronization frequency and network topology. To improve the efficiency of DML training, we propose a system implementation approach to reduce the convergence rate and delay of one iteration, where the network topology and synchronization frequency are set, followed by an optimal resource allocation policy. At last, we perform experiments with an image classification task. Simulation results indicate that our proposed D2D framework can effectively reduce training delay and promote computation efficiency by 39% in a heterogeneous environment, which is concordant with the theoretical analysis.",
        "DOI": "10.1109/TVT.2023.3234550",
        "affiliation_name": "Beijing University of Posts and Telecommunications",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Exploring job running path to predict runtime on multiple production supercomputers",
        "paper_author": "Yang W.",
        "publication": "Journal of Parallel and Distributed Computing",
        "citied_by": "5",
        "cover_date": "2023-05-01",
        "Abstract": "There are massive jobs submitted in the supercomputer, and the job management system is typically deployed to schedule these jobs and allocate compute resources. FCFS (First Come First Serve) is a popular scheduling policy and the job's priority is determined based on the arrival time. However, under the FCFS strategy, if existing idle resources cannot meet the requirement of the head job in the waiting queue, they cannot be allocated to other jobs, which suffers from resource waste. To optimize the resource utilization, the backfilling method is proposed, which allocates the reserved idle compute nodes to a small-size and short-running non-head job, on the premise of not delaying the original head job. Obtaining the job's runtime in advance is necessary for backfilling and the traditional method relies on the user's estimation. Unfortunately, the estimated runtime provided by users is generally overestimated. Many studies extract features from historical job logs and adopt machine learning to predict the runtime. However, traditional features are insufficient to characterize the job. In this paper, we collect job logs from two supercomputers and present a novel runtime prediction framework called PREP. PREP explores the job's running path as a new feature, which implies plentiful information about the job's properties, such as the user, the project, the scale of data sets, and the parameters used. As there is a strong correlation between the job's runtime and its running path, we group jobs with similar paths into a cluster and train a runtime prediction model for each cluster respectively. Extensive evaluations demonstrate that introducing the new feature can achieve higher prediction accuracy (88.5% and 82.3% in two production supercomputers respectively), and our framework has a more desirable prediction performance than other popular strategies like Last-2 and IRPA. In addition, the predicted runtime is inserted into the real job trace of a slurm simulator to verify the advantages of PREP.",
        "DOI": "10.1016/j.jpdc.2023.01.001",
        "affiliation_name": "China Aerodynamics Research and Development Center",
        "affiliation_city": "Mianyang",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Predicting renewable energy production by machine learning methods: The case of Turkey",
        "paper_author": "Yağmur A.",
        "publication": "Environmental Progress and Sustainable Energy",
        "citied_by": "3",
        "cover_date": "2023-05-01",
        "Abstract": "It is considered that the use of renewable energy sources will replace fossil fuels due to global climate change and accompanying decisions taken by states. In this study, unlike the renewable energy production estimation studies in the literature, a model was created by taking the socioeconomic, environmental and energy time series data of the countries. In the study, Turkey, which did not promise a numerical reduction in greenhouse gas emissions unlike other developing countries but has an increasing energy production from renewable energy sources, was chosen. In the study, the data between 1990 and 2020 were used to receive more realistic results by considering the interval before and after the Kyoto protocol. Artificial neural networks and support vector regression among machine learning methods were used to predict the model. As a result of the study, support vector regression had a 92% and artificial neural networks had a successful predictive power of 89.9% according to the coefficient of determination (R2). In the study, the root mean square error value was 0.071 for artificial neural networks and 0.045 for support vector regression; the mean squared error value was 0.005 for artificial neural networks and 0.002 for support vector regression, which was close to the ideal values. Both methods were statistically successful. It is predicted that the model designed because of these successful results obtained in the study would guide the creation of energy policies and contribute to scientific studies.",
        "DOI": "10.1002/ep.14077",
        "affiliation_name": "Akdeniz Üniversitesi",
        "affiliation_city": "Antalya",
        "affiliation_country": "Türkiye"
    },
    {
        "paper_title": "Bank provision reversals and income smoothing: A case study",
        "paper_author": "Aggelopoulos E.",
        "publication": "Journal of Accounting and Public Policy",
        "citied_by": "3",
        "cover_date": "2023-05-01",
        "Abstract": "Incentives for banks to achieve income targets have previously been identified as a strong motivation for income smoothing (IS). Extant literature captures bank IS indirectly via discretionary provision estimations. In turn, our study directly locates IS through loan loss provision reversals. Drawing from bounded rationality perspectives, we investigate a systemic European Bank from January 2006 to September 2017, with 15,931 unique loan portfolio-quarter observations, employing a frequency and machine learning analysis. Our empirical investigation reveals both the main incentives underlying provision reversals recognition and the reported income consequences of such reversals, in times of recession. In particular, we find that provision reversals are principally used to avoid negative reported income (i.e., net losses). There is also some evidence that provision reversals are used to avoid income decline compared to the previous quarter. Finally, we show an asymmetric pattern of provision reversals over time with an emphasis on the early recession years. Our study contributes to the efforts of policy makers (both banking and accounting regulators) to reduce opportunistic, income-increasing actions by bank executives in difficult times.",
        "DOI": "10.1016/j.jaccpubpol.2022.107051",
        "affiliation_name": "University of Patras",
        "affiliation_city": "Rio",
        "affiliation_country": "Greece"
    },
    {
        "paper_title": "Interactive reinforced feature selection with traverse strategy",
        "paper_author": "Liu K.",
        "publication": "Knowledge and Information Systems",
        "citied_by": "6",
        "cover_date": "2023-05-01",
        "Abstract": "In this paper, we propose a single-agent Monte Carlo-based reinforced feature selection method, as well as two efficiency improvement strategies, i.e., early stopping strategy and reward-level interactive strategy. Feature selection is one of the most important technologies in data prepossessing, aiming to find the optimal feature subset for a given downstream machine learning task. Enormous research has been done to improve its effectiveness and efficiency. Recently, the multi-agent reinforced feature selection (MARFS) has achieved great success in improving the performance of feature selection. However, MARFS suffers from the heavy burden of computational cost, which greatly limits its application in real-world scenarios. In this paper, we propose an efficient reinforcement feature selection method, which uses one agent to traverse the whole feature set and decides to select or not select each feature one by one. Specifically, we first develop one behavior policy and use it to traverse the feature set and generate training data. And then, we evaluate the target policy based on the training data and improve the target policy by Bellman equation. Besides, we conduct the importance sampling in an incremental way and propose an early stopping strategy to improve the training efficiency by the removal of skew data. In the early stopping strategy, the behavior policy stops traversing with a probability inversely proportional to the importance sampling weight. In addition, we propose a reward-level and training-level interactive strategy to improve the training efficiency via external advice. What’s more, we propose an incremental descriptive statistics method to represent the state with low computational cost. Finally, we design extensive experiments on real-world data to demonstrate the superiority of the proposed method.",
        "DOI": "10.1007/s10115-022-01812-3",
        "affiliation_name": "Portland State University",
        "affiliation_city": "Portland",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Deep reinforcement learning for class imbalance fault diagnosis of equipment in nuclear power plants",
        "paper_author": "Zhong X.",
        "publication": "Annals of Nuclear Energy",
        "citied_by": "19",
        "cover_date": "2023-05-01",
        "Abstract": "In equipment fault diagnosis in nuclear power plants, there may be far more samples in one class (e.g., a health state) than in another class (e.g., a fault state). The distribution of data in each class is highly skewed. Most machine learning algorithms are suitable for balanced training datasets. When faced with imbalanced samples, these algorithms tend to provide good identification for the majority classes and bias for the minority classes. However, the misclassification of minority classes can lead to high costs. To address the above problem, this paper develops a deep reinforcement learning-based diagnosis method that models fault diagnosis as a sequential decision-making process. At each time step, the agent receives the state of the environment represented by the training samples and then takes a diagnosis action guided by a policy. If the action is correct/incorrect, the agent receives a positive/negative reward. The reward for minority classes is higher than that for majority classes. The agent's goal is to obtain as many cumulative rewards as possible in the process, i.e., to identify the sample as correctly as possible. Six demonstration scenarios are constructed, depending on the selected fault datasets and the designed model structures. Experiments show that the proposed method achieves a higher weighted-averaged F1 score than the classical supervised learning method in most cases of class imbalance. The proposed method has potential applications in the field of class imbalance fault diagnosis of equipment in nuclear power plants.",
        "DOI": "10.1016/j.anucene.2023.109685",
        "affiliation_name": "Swanson School of Engineering",
        "affiliation_city": "Pittsburgh",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Robust Federated Learning Over Noisy Fading Channels",
        "paper_author": "Shah S.M.",
        "publication": "IEEE Internet of Things Journal",
        "citied_by": "4",
        "cover_date": "2023-05-01",
        "Abstract": "The performance capabilities of models trained in a federated learning (FL) setting over wireless networks can be significantly affected by the underlying properties of the transmission channel. Even for shallow models, there can be an acute degradation in performance which necessitates the development of algorithms which are robust to transmission channel effects, such as noise and fading. In this work, we present a two-pronged approach to overcome the limitations of existing wireless machine learning (ML)-based algorithms. First, to tackle the effect of channel noise, we incorporate a novel tracking-based stochastic approximation scheme in the standard federated averaging pipeline which averages out the effect of the channel noise. In contrast to previous works on FL with a noisy channel, we provide exact convergence guarantees for our algorithm without the need to increase the transmission power gain. Second, to combat channel fading and further optimize the power consumption at the client level, we propose an adaptive transmission policy obtained by solving an optimization problem with long-term constraints. The solution is obtained in an online manner via a dual decomposition method. The superior empirical performance of the proposed scheme compared to state-of-the-art works is demonstrated on standard ML tasks.",
        "DOI": "10.1109/JIOT.2022.3230452",
        "affiliation_name": "Hong Kong University of Science and Technology",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong"
    },
    {
        "paper_title": "Black-box attacks on image classification model with advantage actor-critic algorithm in latent space",
        "paper_author": "Kang X.",
        "publication": "Information Sciences",
        "citied_by": "5",
        "cover_date": "2023-05-01",
        "Abstract": "The Internet of Things (IoT) ecosystem that integrates a wide variety of intelligent multimedia applications and services has undergone a tremendous transformation over the years. As an essential approach for securing IoT-based multimedia services, Artificial Intelligence (AI) has been in innovation at a rapid pace. However, many machine learning systems are vulnerable to adversarial examples, including advanced deep neural networks. By making imperceptible modifications to real examples, the prediction will deviate far from the correct value. A deep reinforcement learning-based black-box attacker on the image classification model is introduced in our research. Different from the existing black-box attacks, which require massive queries and trials in the pixel space, the proposed method compresses the images into latent space through variational inference, querying the optimal examples efficiently with actor-critic networks. Rather than patch-to-patch translation with generative adversarial networks in related works, the fake examples are generated by gradually superimposing the perturbations into the latent space at each step through the Markov decision process (MDP) to high stability and good astringency of the attacker. Experiments evaluated on the ImageNet dataset demonstrate that the proposed attacker can generate adversarial images for most samples in limited steps, greatly reducing the accuracy of the model.",
        "DOI": "10.1016/j.ins.2023.01.019",
        "affiliation_name": "Computer Science and Engineering Department, College of Engineering, Qatar University",
        "affiliation_city": "Doha",
        "affiliation_country": "Qatar"
    },
    {
        "paper_title": "Perceived Vulnerability to Disease, Resilience, and Mental Health Outcome of Korean Immigrants amid the COVID-19 Pandemic: A Machine Learning Approach",
        "paper_author": "Choi S.",
        "publication": "Natural Hazards Review",
        "citied_by": "1",
        "cover_date": "2023-05-01",
        "Abstract": "This study examined the predictive ability of perceived vulnerability to disease (PVD), fear of COVID-19, and coping mechanisms on the Korean immigrants' psychological distress level amid the pandemic. Through purposive sampling, both foreign-born and US-born Korean immigrants residing in the US above the age of 18 years were invited to an online survey. Between May and June 2020, data collection took place, which yielded the final sample of 790 participants from 42 states. An artificial neural network (ANN) was used to verify variables that predict the level of psychological distress on the participants. The model with one hidden layer holding six hidden neurons showed the best performance. The error rate was approximately 27%, and the results from the sensitivity analysis, the receiver operating characteristics (ROC) curve, showed that the area under the curve (AUC) was 0.801. The most powerful predicting variables in the neural network were resilience, PVD, and social support. Implications for practice and policy are discussed.",
        "DOI": "10.1061/NHREFO.NHENG-1441",
        "affiliation_name": "Lamar University College of Engineering",
        "affiliation_city": "Beaumont",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Grading and fraud detection of saffron via learning-to-augment incorporated Inception-v4 CNN",
        "paper_author": "Momeny M.",
        "publication": "Food Control",
        "citied_by": "44",
        "cover_date": "2023-05-01",
        "Abstract": "Saffron is a well-known product in the food industry. It is one of the spices that are sometimes adulterated with the sole motive of gaining more economic profit. Today, machine vision systems are widely used in controlling the quality of food and agricultural products as a new, non-destructive, and inexpensive approach. In this study, a machine vision system based on deep learning was used to detect fraud and saffron quality. A dataset of 1869 images was created and categorized in 6 classes including: dried saffron stigma using a dryer; dried saffron stigma using pressing method; pure stem of saffron; sunflower; saffron stem mixed with food coloring; and corn silk mixed with food coloring. A Learning-to-Augment incorporated Inception-v4 Convolutional Neural Network (LAII-v4 CNN) was developed for grading and fraud detection of saffron in images captured by smartphones. The best policies of data augmentation were selected with the proposed LAII-v4 CNN using images corrupted by Gaussian, speckle, and impulse noise to address overfitting the model. The proposed LAII-v4 CNN compared with regular CNN-based methods and traditional classifiers. Ensemble of Bagged Decision Trees, Ensemble of Boosted Decision Trees, k-Nearest Neighbor, Random Under-sampling Boosted Trees, and Support Vector Machine were used for classification of the features extracted by Histograms of Oriented Gradients and Local Binary Patterns, and selected by the Principal Component Analysis. The results showed that the proposed LAII-v4 CNN with an accuracy of 99.5% has achieved the best performance by employing batch normalization, Dropout, and leaky ReLU.",
        "DOI": "10.1016/j.foodcont.2022.109554",
        "affiliation_name": "Centre de Visió per Computador",
        "affiliation_city": "Cerdanyola del Valles",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "Application of machine learning approaches for land cover monitoring in northern Cameroon",
        "paper_author": "Yuh Y.G.",
        "publication": "Ecological Informatics",
        "citied_by": "62",
        "cover_date": "2023-05-01",
        "Abstract": "Machine learning (ML) models are a leading analytical technique used to monitor, map and quantify land use and land cover (LULC) and its change over time. Models such as k-nearest neighbour (kNN), support vector machines (SVM), artificial neural networks (ANN), and random forests (RF) have been used effectively to classify LULC types at a range of geographical scales. However, ML models have not been widely applied in African tropical regions due to methodological challenges that arise from relying on the coarse-resolution satellite images available for these areas. In this study, we compared the performance of four ML algorithms (kNN, SVM, ANN and RF) applied to LULC monitoring within the Mayo Rey department, North Province, Cameroon. We used satellite data from the Landsat 7 Enhanced Thematic Mapper Plus (ETM+) combined with 8 Operational Land Imager (OLI) images of northern Cameroon for November 2000 and November 2020. Our results showed that all four classification algorithms produced relatively high accuracy (overall classification accuracy >80%), with the RF model (> 90% classification accuracy) outperforming the kNN, SVM, and ANN models. We found that approximately 7% of all forested areas (dense forest and woody savanna) were converted to other land cover types between 2000 and 2020; this forest loss is particularly associated with an expansion of both croplands and built-up areas. Our study represents a novel application and comparison of statistical and ML approaches to LULC monitoring using coarse-resolution satellite images in an African tropical forest and savanna setting. The resulting land cover maps serve as an important baseline that will be useful to the Cameroon government for policy development, conservation planning, urban planning, and deforestation and agricultural monitoring.",
        "DOI": "10.1016/j.ecoinf.2022.101955",
        "affiliation_name": "Szkola Glówna Gospodarstwa Wiejskiego w Warszawie",
        "affiliation_city": "Warsaw",
        "affiliation_country": "Poland"
    },
    {
        "paper_title": "Laws and regulations tell how to classify your data: A case study on higher education",
        "paper_author": "Yang M.",
        "publication": "Information Processing and Management",
        "citied_by": "5",
        "cover_date": "2023-05-01",
        "Abstract": "The era of big data has promoted the vigorous development of many industries, boosting the full potential of holistic data-driven analysis, yet, it has also been accompanied accompanied by uninterrupted data breaches. In recent years, especially in China, data security policies and regulations have been promulgated continuously, and many of them have made clear requirements for data classification. As the support of data security initiatives, data classification, nowadays has received the bulk of attention and has been hailed by all walks of life. There is a lot of valuable information contained in the issued regulations, which has already been well exploited in the research of privacy policy compliance verification. Whereas, few scholars have drawn on such information to guide data classification for the purpose of security and compliance. As a step towards this direction, in this paper, we define two information types: one is “regulated data” mentioned in external laws and regulations, another is “non-regulated data”, indicating internal business data produced in a certain organization, and develop a novel generalization-enhanced decision tree classification algorithm called Gen-DT to classify data. In this way, the data covered by the relevant data security regulatory mandates can be quickly identified and handled in full compliance as well. Furthermore, we evaluate the proposed compliance-driven data classification scheme using datasets collected from two famous universities in China and validate that our approach can achieve better performance than existing popular machine learning techniques.",
        "DOI": "10.1016/j.ipm.2022.103240",
        "affiliation_name": "Sichuan University",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Cor-ENTC:correlation with ensembled approach for network traffic classification using SDN technology for future networks",
        "paper_author": "Paramasivam S.",
        "publication": "Journal of Supercomputing",
        "citied_by": "8",
        "cover_date": "2023-05-01",
        "Abstract": "According to a study on advanced technology, resource planning, and design for Fifth Generation (5G) architecture and elsewhere, network traffic classification is a basic and complex part of software-defined networking (SDN). 5G requires end-to-end security that uses its software-defined architecture to automatically monitor the network and classify the traffic flows. To improve the security in network traffic classification, it is necessary to apply a correct set of policy rules to classify future network traffic flows. To create a communication-efficient and intelligent traffic classification framework in the SDN environment, machine learning is used between the data and the control planes. The proposed ensemble learning pre-processing tool to categorize incoming VPN traffic by applying a possible set of policies. The proposed technique is compared with existing classifiers and has a higher accuracy of 98% to 99.9% for ensemble models than single classifiers and other existing options, according to an evaluation of performance.",
        "DOI": "10.1007/s11227-022-04969-4",
        "affiliation_name": "National Institute of Technology Tiruchirappalli",
        "affiliation_city": "Tiruchirappalli",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Data-Driven Wind Farm Control via Multiplayer Deep Reinforcement Learning",
        "paper_author": "Dong H.",
        "publication": "IEEE Transactions on Control Systems Technology",
        "citied_by": "19",
        "cover_date": "2023-05-01",
        "Abstract": "This brief proposes a novel data-driven control scheme to maximize the total power output of wind farms subject to strong aerodynamic interactions among wind turbines. The proposed method is model-free and has strong robustness, adaptability, and applicability. Particularly, distinct from the state-of-the-art data-driven wind farm control methods that commonly use the steady-state or time-averaged data (such as turbines' power outputs under steady wind conditions or from steady-state models) to carry out learning, the proposed method directly mines in-depth the time-series data measured at turbine rotors under time-varying wind conditions to achieve farm-level power maximization. The control scheme is built on a novel multiplayer deep reinforcement learning method (MPDRL), in which a special critic-actor-distractor structure, along with deep neural networks (DNNs), is designed to handle the stochastic feature of wind speeds and learn optimal control policies subject to a user-defined performance metric. The effectiveness, robustness, and scalability of the proposed MPDRL-based wind farm control method are tested by prototypical case studies with a dynamic wind farm simulator (WFSim). Compared with the commonly used greedy strategy, the proposed method leads to clear increases in farm-level power generation in case studies.",
        "DOI": "10.1109/TCST.2022.3223185",
        "affiliation_name": "Faculty of Science, Engineering and Medicine",
        "affiliation_city": "Coventry",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Design of Deep Reinforcement Learning Controller Through Data-assisted Model for Robotic Fish Speed Tracking",
        "paper_author": "Duraisamy P.",
        "publication": "Journal of Bionic Engineering",
        "citied_by": "13",
        "cover_date": "2023-05-01",
        "Abstract": "It is common for robotic fish to generate thrust using reactive force generated by the tail’s physical motion, which interacts with the surrounding fluid. The coupling effect of the body strongly correlates with this thrust. However, hydrodynamics cannot be wholly modeled in analytical form. Therefore, data-assisted modeling is necessary for robotic fish. This work presents the first method of its kind using Genetic Algorithm (GA)-based optimization methods for data-assistive modeling for robotic fish applications. To begin, experimental data are collected in real time with the robotic fish that has been designed and fabricated using 3D printing. Then, the model’s influential parameters are estimated using an optimization problem. Further, a model-based deep reinforcement learning (DRL) controller is proposed to track the desired speed through extensive simulation work. In addition to a deep deterministic policy gradient (DDPG), a twin delayed DDPG (TD3) is employed in the training of the RL agent. Unfortunately, due to its local optimization problem, the RL-DDPG controller failed to perform well during training. In contrast, the RL-TD3 controller effectively learns the control policies and overcomes the local optima problem. As a final step, controller performance is evaluated under different disturbance conditions. In contrast to DDPG and GA-tuned proportional-integral controllers, the proposed model with RL-TD3 controller significantly improves the performance.",
        "DOI": "10.1007/s42235-022-00309-7",
        "affiliation_name": "SASTRA Deemed University",
        "affiliation_city": "Thanjavur",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Graph Convolutional Reinforcement Learning for Advanced Energy-Aware Process Planning",
        "paper_author": "Xiao Q.",
        "publication": "IEEE Transactions on Systems, Man, and Cybernetics: Systems",
        "citied_by": "10",
        "cover_date": "2023-05-01",
        "Abstract": "With the growing demands on green short life-cycle products, advanced energy-Aware process planning (AEPP) becomes critical. A major limitation of the existing methods is the poor resistance to the perturbations encountered in advanced machining systems. Therefore, a graph convolutional reinforcement learning (GCRL) method is proposed to overcome such limitations. In this method, a graph convolutional policy network is trained to rapidly adapt the learned commonalities to specific tasks. Unlike algorithms that fix decision variables before optimization, this method employs graph generation to represent AEPP while taking into consideration the flexibilities of operations, machines, and cutting tools. The problem is reformulated as a novel Markov decision process (MDP) to describe the dynamic generation procedure of process plans. A graph convolutional network (GCN) is concurrently used to perform graph embedding to compress the topology of input graphs. Additionally, reinforcement learning (RL) is used to achieve robust and intuitive learning for process planning. To improve the adaption performance of the proposed GCRL, a two-phase multitask training strategy is adopted. Learning efficiency is improved because agents can incorporate both intertask similarities and task-specific rules. A comprehensive case study, including energy characteristics and algorithm performance analyses, is also performed to validate the developed method.",
        "DOI": "10.1109/TSMC.2022.3219407",
        "affiliation_name": "Hangzhou City University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Robust Losses for Learning Value Functions",
        "paper_author": "Patterson A.",
        "publication": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "citied_by": "10",
        "cover_date": "2023-05-01",
        "Abstract": "Most value function learning algorithms in reinforcement learning are based on the mean squared (projected) Bellman error. However, squared errors are known to be sensitive to outliers, both skewing the solution of the objective and resulting in high-magnitude and high-variance gradients. To control these high-magnitude updates, typical strategies in RL involve clipping gradients, clipping rewards, rescaling rewards, or clipping errors. While these strategies appear to be related to robust losses - like the Huber loss - they are built on semi-gradient update rules which do not minimize a known loss. In this work, we build on recent insights reformulating squared Bellman errors as a saddlepoint optimization problem and propose a saddlepoint reformulation for a Huber Bellman error and Absolute Bellman error. We start from a formalization of robust losses, then derive sound gradient-based approaches to minimize these losses in both the online off-policy prediction and control settings. We characterize the solutions of the robust losses, providing insight into the problem settings where the robust losses define notably better solutions than the mean squared Bellman error. Finally, we show that the resulting gradient-based algorithms are more stable, for both prediction and control, with less sensitivity to meta-parameters.",
        "DOI": "10.1109/TPAMI.2022.3213503",
        "affiliation_name": "University of Alberta",
        "affiliation_city": "Edmonton",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Continuous-Time Fitted Value Iteration for Robust Policies",
        "paper_author": "Lutter M.",
        "publication": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "citied_by": "2",
        "cover_date": "2023-05-01",
        "Abstract": "Solving the Hamilton-Jacobi-Bellman equation is important in many domains including control, robotics and economics. Especially for continuous control, solving this differential equation and its extension the Hamilton-Jacobi-Isaacs equation, is important as it yields the optimal policy that achieves the maximum reward on a give task. In the case of the Hamilton-Jacobi-Isaacs equation, which includes an adversary controlling the environment and minimizing the reward, the obtained policy is also robust to perturbations of the dynamics. In this paper we propose continuous fitted value iteration (cFVI) and robust fitted value iteration (rFVI). These algorithms leverage the non-linear control-affine dynamics and separable state and action reward of many continuous control problems to derive the optimal policy and optimal adversary in closed form. This analytic expression simplifies the differential equations and enables us to solve for the optimal value function using value iteration for continuous actions and states as well as the adversarial case. Notably, the resulting algorithms do not require discretization of states or actions. We apply the resulting algorithms to the Furuta pendulum and cartpole. We show that both algorithms obtain the optimal policy. The robustness Sim2Real experiments on the physical systems show that the policies successfully achieve the task in the real-world. When changing the masses of the pendulum, we observe that robust value iteration is more robust compared to deep reinforcement learning algorithm and the non-robust version of the algorithm. Videos of the experiments are shown at https://sites.google.com/view/rfvi.",
        "DOI": "10.1109/TPAMI.2022.3215769",
        "affiliation_name": "NVIDIA",
        "affiliation_city": "Santa Clara",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Cost-Aware Resource Recommendation for DAG-Based Big Data Workflows: An Apache Spark Case Study",
        "paper_author": "Aseman-Manzar M.M.",
        "publication": "IEEE Transactions on Services Computing",
        "citied_by": "3",
        "cover_date": "2023-05-01",
        "Abstract": "The era of personal resources being sufficient for enterprise big data computations has passed. As computations are executed in the cloud, small policy changes of cloud operators may cause considerable changes in operational costs. Carefully choosing the amount of resources for a given application is thus of great importance. This, however, requires a priori knowledge of the application's performance under different configurations. Creating a performance prediction model needs to account for the heterogeneity of resources and the diversity in application workflows. Previous approaches for heterogeneous environments consider a black-box representation of the application which results in single-purpose models. This paper addresses the problem with two gray-box prediction models using linear programming (LP) and mixed-integer linear programming (MILP). Given a set of available resources, the models consider Apache Spark applications and their Directed Acyclic Graph (DAG) of workflow running on top of a Hadoop-YARN cluster. We then propose a configuration recommendation algorithm to optimize the cost-performance trade-offs when renting machine instances. The accuracy of the proposed models is evaluated with real-world executions of several representative applications on the Wikipedia dataset and the TPC-DS benchmark. The average error of only 3.28% for the proposed prediction models demonstrates the practicality of the proposed approach in handling cost-performance trade-offs.",
        "DOI": "10.1109/TSC.2022.3203010",
        "affiliation_name": "Department of Computer Science and Engineering",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "ADABA: improving the balancing between runtime and accuracy in a new distributed version of the alpha–beta algorithm",
        "paper_author": "Tomaz L.B.P.",
        "publication": "Artificial Intelligence Review",
        "citied_by": "0",
        "cover_date": "2023-05-01",
        "Abstract": "The satisfactory performance of the intelligent agents conceived to solve several real-life problems requires efficient decision-making algorithms. To address issues with high state-spaces within reasonable runtime limits, these algorithms are distributed according to some approaches that can be either synchronous or asynchronous, where the former guarantee the same results as their corresponding serial versions through synchronization points, which causes the undesirable effects of communication overhead and idle processors. To mitigate this, the asynchronous approaches reduce the message exchanges in such a way as to accelerate the runtime without too much compromising the response accuracy. The challenge of enhancing the minimax technique through pruning makes Alpha–Beta a relevant case study in parallelism research. Young Brothers Wait Concept (YBWC) and Asynchronous Parallel Hierarchical Iterative Deepening (APHID) are highlighted among the existing Alpha–Beta distributions. Knowing that APHID proved to be more suitable than YBWC to operate in distributed memory and that shared memory architectures are scarcely available due to their high costs, the primary motivation here is to implement the Asynchronous Distributed Alpha–Beta Algorithm (ADABA), which increases the accuracy and performance of APHID through the enhancement of the slaves’ task ordering policies, the communication process between the processors and the window’s updating strategy. Experiments fulfilled through tournaments involving ADABA-based and APHID-based Checkers agents proved that the player based on the best ADABA version reached, approximately, a victory rate 95% superior and a runtime two times faster than the APHID-based player, keeping the same response accuracy level of its opponent.",
        "DOI": "10.1007/s10462-022-10269-3",
        "affiliation_name": "Instituto Federal de Educação, Ciência e Tecnologia do Triangulo Mineiro (IFTM)",
        "affiliation_city": "Uberaba",
        "affiliation_country": "Brazil"
    },
    {
        "paper_title": "AI-Enabled Experience-Driven Networking: Vision, State-of-the-Art and Future Directions",
        "paper_author": "Tang Y.",
        "publication": "IEEE Network",
        "citied_by": "0",
        "cover_date": "2023-05-01",
        "Abstract": "Modern networks have become immensely complicated, while future networks are expected to be more highly dynamic and sophisticated. In such a complex network environment, it is challenging to design effective control schemes to allocate network resources and manage network systems. Recently, Artificial Intelligence (AI) has made tremendous successes and breakthroughs. Particularly, as a branch of AI technology, the model-free experience-based machine learning (ML) technology has attracted widespread attention in the field of Network Control Problems (NCPs). Motivated by recent breakthroughs in AI technology, we share our vision of deploying ML technology to the field of solving NCPs to achieve experience-driven networking. Compared with the domain knowledge-based heuristic algorithms and fixed policies, which face mounting challenges in achieving desirable performance in highly dynamic and complicated networks, the ML-based methods can accumulate experiences by constantly collecting system feedback, and finally learn desirable/optimal control policies. In this article, we first summarize the differences between the traditional solutions and the ML-based methods and highlight the advantages of experience-driven networking with ML. Then we introduce several state-of-the-art AI-enabled experience-driven works for NCPs. In the end, we shed light on the future directions of adapting ML to more networking problems and the emerging opportunities for experience-driven networking. We hope that our work can help and encourage researchers to propose more innovative experience-driven solutions for the NCPs of modern network systems.",
        "DOI": "10.1109/MNET.106.2100620",
        "affiliation_name": "Midea Group",
        "affiliation_city": "Foshan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Effect of particulate matter (PM<inf>2.5</inf> and PM<inf>10</inf>) on health indicators: climate change scenarios in a Brazilian metropolis",
        "paper_author": "Leão M.L.P.",
        "publication": "Environmental Geochemistry and Health",
        "citied_by": "11",
        "cover_date": "2023-05-01",
        "Abstract": "Recife is recognized as the 16th most vulnerable city to climate change in the world. In addition, the city has levels of air pollutants above the new limits proposed by the World Health Organization (WHO) in 2021. In this sense, the present study had two main objectives: (1) To evaluate the health (and economic) benefits related to the reduction in mean annual concentrations of PM10 and PM2.5 considering the new limits recommended by the WHO: 15 µg/m3 (PM10) and 5 µg/m3 (PM2.5) and (2) To simulate the behavior of these pollutants in scenarios with increased temperature (2 and 4 °C) using machine learning. The averages of PM2.5 and PM10 were above the limits recommended by the WHO. The scenario simulating the reduction in these pollutants below the new WHO limits would avoid more than 130 deaths and 84 hospital admissions for respiratory or cardiovascular problems. This represents a gain of 15.2 months in life expectancy and a cost of almost 160 million dollars. Regarding the simulated temperature increase, the most conservative (+ 2 °C) and most drastic (+ 4 °C) scenarios predict an increase of approximately 6.5 and 15%, respectively, in the concentrations of PM2.5 and PM10, with a progressive increase in deaths attributed to air pollution. The study shows that the increase in temperature will have impacts on air particulate matter and health outcomes. Climate change mitigation and pollution control policies must be implemented for meeting new WHO air quality standards which may have health benefits.",
        "DOI": "10.1007/s10653-022-01331-8",
        "affiliation_name": "Universidade de Pernambuco",
        "affiliation_city": "Recife",
        "affiliation_country": "Brazil"
    },
    {
        "paper_title": "Pandemic lockdown, isolation, and exit policies based on machine learning predictions",
        "paper_author": "Evgeniou T.",
        "publication": "Production and Operations Management",
        "citied_by": "9",
        "cover_date": "2023-05-01",
        "Abstract": "The widespread lockdowns imposed in many countries at the beginning of the COVID-19 pandemic elevated the importance of research on pandemic management when medical solutions such as vaccines are unavailable. We present a framework that combines a standard epidemiological SEIR (susceptible–exposed–infected–removed) model with an equally standard machine learning classification model for clinical severity risk, defined as an individual's risk of needing intensive care unit (ICU) treatment if infected. Using COVID-19–related data and estimates for France as of spring 2020, we then simulate isolation and exit policies. Our simulations show that policies considering clinical risk predictions could relax isolation restrictions for millions of the lowest risk population months earlier while consistently abiding by ICU capacity restrictions. Exit policies without risk predictions, meanwhile, would considerably exceed ICU capacity or require the isolation of a substantial portion of population for over a year in order to not overwhelm the medical system. Sensitivity analyses further decompose the impact of various elements of our models on the observed effects. Our work indicates that predictive modeling based on machine learning and artificial intelligence could bring significant value to managing pandemics. Such a strategy, however, requires governments to develop policies and invest in infrastructure to operationalize personalized isolation and exit policies based on risk predictions at scale. This includes health data policies to train predictive models and apply them to all residents, as well as policies for targeted resource allocation to maintain strict isolation for high-risk individuals.",
        "DOI": "10.1111/poms.13726",
        "affiliation_name": "Mathématiques Appliquées à Paris 5",
        "affiliation_city": "Paris",
        "affiliation_country": "France"
    },
    {
        "paper_title": "Data-driven decision-making model based on artificial intelligence in higher education system of colleges and universities",
        "paper_author": "Teng Y.",
        "publication": "Expert Systems",
        "citied_by": "48",
        "cover_date": "2023-05-01",
        "Abstract": "The quality of management decisions is one of the main issues facing higher education institutions today. Strategic decisions taken by the higher education institutions affect policies, schemes, and actions that the institutions are considering. Machine learning is an emergent artificial intelligence field that utilizes different algorithms, analyses data, and delivers a better understanding of the data contained in a specific context. Hence, in this paper, data-driven decision-making model has been proposed based on artificial intelligence in colleges and universities. Student data, graduation rate and curriculum design have been analysed for administrative decision-making in college or university based on the machine learning method. With the availability of huge quantities and high-quality input training data, machine-learning progressions can attain precise outcomes and enable informed decision-making. The experimental findings show that the suggested model improves the outcome ratio of 90.72%, the performance ratio of 97.62%, prediction ratio of 96.35%, decision-making level of 95.51%, accuracy ratio of 95.61%, an efficiency ratio of 98.14%, graduation rate of 85.86%, data security rate (95.61%) and error rate 33.21% compared to other methods.",
        "DOI": "10.1111/exsy.12820",
        "affiliation_name": "Xi'an University of Technology",
        "affiliation_city": "Xi'an",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Multiparty Dual Learning",
        "paper_author": "Gao Y.",
        "publication": "IEEE Transactions on Cybernetics",
        "citied_by": "8",
        "cover_date": "2023-05-01",
        "Abstract": "The performance of machine learning algorithms heavily relies on the availability of a large amount of training data. However, in reality, data usually reside in distributed parties such as different institutions and may not be directly gathered and integrated due to various data policy constraints. As a result, some parties may suffer from insufficient data available for training machine learning models. In this article, we propose a multiparty dual learning (MPDL) framework to alleviate the problem of limited data with poor quality in an isolated party. Since the knowledge-sharing processes for multiple parties always emerge in dual forms, we show that dual learning is naturally suitable to handle the challenge of missing data, and explicitly exploits the probabilistic correlation and structural relationship between dual tasks to regularize the training process. We introduce a feature-oriented differential privacy with mathematical proof, in order to avoid possible privacy leakage of raw features in the dual inference process. The approach requires minimal modifications to the existing multiparty learning structure, and each party can build flexible and powerful models separately, whose accuracy is no less than nondistributed self-learning approaches. The MPDL framework achieves significant improvement compared with state-of-the-art multiparty learning methods, as we demonstrated through simulations on real-world datasets.",
        "DOI": "10.1109/TCYB.2021.3139076",
        "affiliation_name": "School of Computer Science and Engineering",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore"
    },
    {
        "paper_title": "Inverse Reinforcement Q-Learning Through Expert Imitation for Discrete-Time Systems",
        "paper_author": "Xue W.",
        "publication": "IEEE Transactions on Neural Networks and Learning Systems",
        "citied_by": "31",
        "cover_date": "2023-05-01",
        "Abstract": "In inverse reinforcement learning (RL), there are two agents. An expert target agent has a performance cost function and exhibits control and state behaviors to a learner. The learner agent does not know the expert's performance cost function but seeks to reconstruct it by observing the expert's behaviors and tries to imitate these behaviors optimally by its own response. In this article, we formulate an imitation problem where the optimal performance intent of a discrete-time (DT) expert target agent is unknown to a DT Learner agent. Using only the observed expert's behavior trajectory, the learner seeks to determine a cost function that yields the same optimal feedback gain as the expert's, and thus, imitates the optimal response of the expert. We develop an inverse RL approach with a new scheme to solve the behavior imitation problem. The approach consists of a cost function update based on an extension of RL policy iteration and inverse optimal control, and a control policy update based on optimal control. Then, under this scheme, we develop an inverse reinforcement Q-learning algorithm, which is an extension of RL Q-learning. This algorithm does not require any knowledge of agent dynamics. Proofs of stability, convergence, and optimality are given. A key property about the nonunique solution is also shown. Finally, simulation experiments are presented to show the effectiveness of the new approach.",
        "DOI": "10.1109/TNNLS.2021.3106635",
        "affiliation_name": "The State Key Laboratory of Synthetical Automation for Process Industries",
        "affiliation_city": "Shenyang",
        "affiliation_country": "China"
    },
    {
        "paper_title": "BALANCING MONETARY POLICY IN DEFENCE ECONOMICS IN UKRAINE",
        "paper_author": "Versal N.",
        "publication": "Financial and Credit Activity: Problems of Theory and Practice",
        "citied_by": "2",
        "cover_date": "2023-04-30",
        "Abstract": "The russian federation's launch of a full-scale war against independent Ukraine on Feb-ruary 24, 2022, has presented unprecedented challenges to the country. In addition to resistance on the battlefield, Ukraine must implement adaptive macroeconomic policies to address the situation. This combination of military and economic efforts not only prevents economic collapse but also maintains fragile macroeconomic stability during wartime. Monetary stability becomes especially important, highlighting the absolute ne-cessity for effective implementation of monetary policy. This article aims to identify the key characteristics of Ukraine's defence economy and forecast key policy rates and exchange rates during the war. The prerequisite for forecasting was the analysis of endogenous and exogenous factors determining the current state of the Ukrainian economy: index of business expectations in Ukraine and partner countries, state of international trade and balance of payments, disparities in the labour market, reorientation of the state budget to military needs, devaluation of the national currency, high inflation, increase of financial capital price. Modelling is based on consumer price index (CPI), household inflation expectations, key policy rate of the National Bank of Ukraine, real and nominal effective exchange rates hryvnia to USA dollar, gross and net international reserves, gross and net foreign exchange market interventions, the UK CPI, the USA CPI, EU CPI, and the weighted average yield of domestic government bonds. The methodology involved the use of the VECM model (Vector Error Correlation Model) and the Bagging machine learning method, adapted to time series. Using this methodology enabled an accurate forecast of the key policy rate. In determining the optimal exchange rate, a modified formula was used that takes into account the monetary base, total bank deposits, foreign currency deposits in banks, exchange rate in the black market, and international reserves. This modification enabled the prediction of an exchange rate that closely approximates the official exchange rate.",
        "DOI": "10.55643/fcaptp.2.49.2023.3991",
        "affiliation_name": "Taras Shevchenko National University of Kyiv",
        "affiliation_city": "Kyiv",
        "affiliation_country": "Ukraine"
    },
    {
        "paper_title": "Privacy-Preserving Online Content Moderation: A Federated Learning Use Case",
        "paper_author": "Leonidou P.",
        "publication": "ACM Web Conference 2023 - Companion of the World Wide Web Conference, WWW 2023",
        "citied_by": "0",
        "cover_date": "2023-04-30",
        "Abstract": "Users are exposed to a large volume of harmful content that appears daily on various social network platforms. One solution to users' protection is developing online moderation tools using Machine Learning (ML) techniques for automatic detection or content filtering. On the other hand, the processing of user data requires compliance with privacy policies. In this paper, we propose a framework for developing content moderation tools in a privacy-preserving manner where sensitive information stays on the users' device. For this purpose, we apply Differentially Private Federated Learning (DP-FL), where the training of ML models is performed locally on the users' devices, and only the model updates are shared with a central entity. To demonstrate the utility of our approach, we simulate harmful text classification on Twitter data in a distributed FL fashion- but the overall concept can be generalized to other types of misbehavior, data, and platforms. We show that the performance of the proposed FL framework can be close to the centralized approach - for both the DP-FL and non-DP FL. Moreover, it has a high performance even if a small number of clients (each with a small number of tweets) are available for the FL training. When reducing the number of clients (from fifty to ten) or the tweets per client (from 1K to 100), the classifier can still achieve AUC. Furthermore, we extend the evaluation to four other Twitter datasets that capture different types of user misbehavior and still obtain a promising performance (61% - 80% AUC). Finally, we explore the overhead on the users' devices during the FL training phase and show that the local training does not introduce excessive CPU utilization and memory consumption overhead.",
        "DOI": "10.1145/3543873.3587604",
        "affiliation_name": "Cyprus University of Technology",
        "affiliation_city": "Limassol",
        "affiliation_country": "Cyprus"
    },
    {
        "paper_title": "Temporal Ordinance Mining for Event-Driven Social Media Reaction Analytics",
        "paper_author": "Varde A.S.",
        "publication": "ACM Web Conference 2023 - Companion of the World Wide Web Conference, WWW 2023",
        "citied_by": "1",
        "cover_date": "2023-04-30",
        "Abstract": "As a growing number of policies are adopted to address the substantial rise in urbanization, there is a significant push for smart governance, endowing transparency in decision-making and enabling greater public involvement. The thriving concept of smart governance goes beyond just cities, ultimately aiming at a smart planet. Ordinances (local laws) affect our life with regard to health, business, etc. This is particularly notable during major events such as the recent pandemic, which may lead to rapid changes in ordinances, pertaining for instance to public safety, disaster management, and recovery phases. However, many citizens view ordinances as impervious and complex. This position paper proposes a research agenda enabling novel forms of ordinance content analysis over time and temporal web question answering (QA) for both legislators and the broader public. Along with this, we aim to analyze social media posts so as to track the public opinion before and after the introduction of ordinances. Challenges include addressing concepts changing over time and infusing subtle human reasoning in mining, which we aim to address by harnessing terminology evolution methods and commonsense knowledge sources, respectively. We aim to make the results of the historical ordinance mining and event-driven analysis seamlessly accessible, relying on a robust semantic understanding framework to flexibly support web QA.",
        "DOI": "10.1145/3543873.3587674",
        "affiliation_name": "Hasso-Plattner-Institut für Softwaresystemtechnik GmbH",
        "affiliation_city": "Potsdam",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Privacy-Preserving Online Content Moderation with Federated Learning",
        "paper_author": "Leonidou P.",
        "publication": "ACM Web Conference 2023 - Companion of the World Wide Web Conference, WWW 2023",
        "citied_by": "2",
        "cover_date": "2023-04-30",
        "Abstract": "Users are exposed to a large volume of harmful content that appears daily on various social network platforms. One solution to users' protection is developing online moderation tools using Machine Learning (ML) techniques for automatic detection or content filtering. On the other hand, the processing of user data requires compliance with privacy policies. This paper proposes a privacy-preserving Federated Learning (FL) framework for online content moderation that incorporates Central Differential Privacy (CDP). We simulate the FL training of a classifier for detecting tweets with harmful content, and we show that the performance of the FL framework can be close to the centralized approach. Moreover, it has a high performance even if a small number of clients (each with a small number of tweets) are available for the FL training. When reducing the number of clients (from fifty to ten) or the tweets per client (from 1K to 100), the classifier can still achieve AUC. Furthermore, we extend the evaluation to four other Twitter datasets that capture different types of user misbehavior and still obtain a promising performance (61% - 80% AUC).",
        "DOI": "10.1145/3543873.3587366",
        "affiliation_name": "Cyprus University of Technology",
        "affiliation_city": "Limassol",
        "affiliation_country": "Cyprus"
    },
    {
        "paper_title": "To Store or Not? Online Data Selection for Federated Learning with Limited Storage",
        "paper_author": "Gong C.",
        "publication": "ACM Web Conference 2023 - Proceedings of the World Wide Web Conference, WWW 2023",
        "citied_by": "13",
        "cover_date": "2023-04-30",
        "Abstract": "Machine learning models have been deployed in mobile networks to deal with massive data from different layers to enable automated network management and intelligence on devices. To overcome high communication cost and severe privacy concerns of centralized machine learning, federated learning (FL) has been proposed to achieve distributed machine learning among networked devices. While the computation and communication limitation has been widely studied, the impact of on-device storage on the performance of FL is still not explored. Without an effective data selection policy to filter the massive streaming data on devices, classical FL can suffer from much longer model training time (4 ×) and significant inference accuracy reduction (7%), observed in our experiments. In this work, we take the first step to consider the online data selection for FL with limited on-device storage. We first define a new data valuation metric for data evaluation and selection in FL with theoretical guarantees for speeding up model convergence and enhancing final model accuracy, simultaneously. We further design ODE, a framework of Online Data sElection for FL, to coordinate networked devices to store valuable data samples. Experimental results on one industrial dataset and three public datasets show the remarkable advantages of ODE over the state-of-the-art approaches. Particularly, on the industrial dataset, ODE achieves as high as 2.5 × speedup of training time and 6% increase in inference accuracy, and is robust to various factors in practical environments.",
        "DOI": "10.1145/3543507.3583426",
        "affiliation_name": "Huawei Noah's Ark Lab",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong"
    },
    {
        "paper_title": "Improving anti-jamming decision-making strategies for cognitive radar via multi-agent deep reinforcement learning",
        "paper_author": "Jiang W.",
        "publication": "Digital Signal Processing: A Review Journal",
        "citied_by": "21",
        "cover_date": "2023-04-30",
        "Abstract": "Most of the existing anti-jamming decision-making methods overly rely on the subjective experience of radar operators. However, due to the rapid development of cognitive radar and modern electronic warfare, conventional anti-jamming decision-making methods can no longer adapt to the complex and changing electromagnetic environment. The advent of deep reinforcement learning (DRL) provides a new attractive solution for this issue. In this paper, an adversarial anti-jamming decision-making network for cognitive radar via multi-agent deep reinforcement learning (MDRL) is proposed, which has good self-learning ability and can meet the requirements of intelligent, dynamic and real-time in modern electronic warfare. Since competitive decision-makers are considered and these two confrontational sides are not able to obtain the completely accurate information of each other, the environment model is specifically constructed as a partially observable Markov decision process (POMDP). Then, a decision-making network is designed based on deterministic deep deterministic policy gradient (DDPG) algorithm to explore the competition between cognitive radar and smart jammer. In order to overcome the environment non-stationarity, the decision-making network is trained and tested in a special MDRL framework. The experimental results demonstrate that the proposed method is effective in anti-jamming decision-making system of cognitive radar. Furthermore, the two confrontational sides show high decision-making ability and perform well in the adversarial scenario by comparing with other training policies, which demonstrate that confrontational training with powerful opponents can improve the intelligence level of all agents.",
        "DOI": "10.1016/j.dsp.2023.103952",
        "affiliation_name": "University of Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Time Series and Machine Learning methods for Demand Forecasting: A case study of machine seller in prefabricated concrete industry",
        "paper_author": "Jarukajornjinda T.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-04-27",
        "Abstract": "In this research, Time series and Machine learning methods are conducted to find a suitable monthly demand forecasting models and parameters for a medium-scaled machine seller company in Thailand, who is currently using Simple Moving Average as the current method. Historical demand data are collected for a half decade and used as a training set and cross validation set for different forecasting models (Exponential Smoothing, Autoregressive Integrated Moving Average and Long-Short term memory). The history of demand indicates that the patterns are composed of many types of time series components, including stationary, trend and seasonality. The performance of models is measured by Relative Total Absolute Error (RTAE) because some months contain zero value of demand. The results show that Long-Short term memory is the most suitable method as compared to others.",
        "DOI": "10.1145/3603955.3604012",
        "affiliation_name": "Chulalongkorn University",
        "affiliation_city": "Bangkok",
        "affiliation_country": "Thailand"
    },
    {
        "paper_title": "Estimation of NO2 emission strengths over Riyadh and Madrid from space from a combination of wind-Assigned anomalies and a machine learning technique",
        "paper_author": "Tu Q.",
        "publication": "Atmospheric Measurement Techniques",
        "citied_by": "1",
        "cover_date": "2023-04-26",
        "Abstract": "Nitrogen dioxide (NO2) air pollution provides valuable information for quantifying NOx (NOxgCombining double low linegNOg+gNO2) emissions and exposures. This study presents a comprehensive method to estimate average tropospheric NO2 emission strengths derived from 4-year (May 2018-June 2022) TROPOspheric Monitoring Instrument (TROPOMI) observations by combining a wind-Assigned anomaly approach and a machine learning (ML) method, the so-called gradient descent algorithm. This combined approach is firstly applied to the Saudi Arabian capital city of Riyadh, as a test site, and yields a total emission rate of 1.09×1026gmolec.gs-1. The ML-Trained anomalies fit very well with the wind-Assigned anomalies, with an R2 value of 1.0 and a slope of 0.99. Hotspots of NO2 emissions are apparent at several sites: over a cement plant and power plants as well as over areas along highways. Using the same approach, an emission rate of 1.99×1025gmolec.gs-1 is estimated in the Madrid metropolitan area, Spain. Both the estimate and spatial pattern are comparable with the Copernicus Atmosphere Monitoring Service (CAMS) inventory. Weekly variations in NO2 emission are highly related to anthropogenic activities, such as the transport sector. The NO2 emissions were reduced by 16g% at weekends in Riyadh, and high reductions were found near the city center and in areas along the highway. An average weekend reduction estimate of 28g% was found in Madrid. The regions with dominant sources are located in the east of Madrid, where residential areas and the Madrid-Barajas airport are located. Additionally, due to the COVID-19 lockdowns, the NO2 emissions decreased by 21g% in March-June 2020 in Riyadh compared with the same period in 2019. A much higher reduction (62g%) is estimated for Madrid, where a very strict lockdown policy was implemented. The high emission strengths during lockdown only persist in the residential areas, and they cover smaller areas on weekdays compared with weekends. The spatial patterns of NO2 emission strengths during lockdown are similar to those observed at weekends in both cities. Although our analysis is limited to two cities as test examples, the method has proven to provide reliable and consistent results. It is expected to be suitable for other trace gases and other target regions. However, it might become challenging in some areas with complicated emission sources and topography, and specific NO2 decay times in different regions and seasons should be taken into account. These impacting factors should be considered in the future model to further reduce the uncertainty budget.",
        "DOI": "10.5194/amt-16-2237-2023",
        "affiliation_name": "Karlsruher Institut für Technologie",
        "affiliation_city": "Karlsruhe",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Machine learning building on evaluation of feedback of engineering graduate students: A discriminant analysis approach",
        "paper_author": "Bharath V.",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2023-04-25",
        "Abstract": "The study of evaluation of feedback of graduate students is an important one for better understanding the quality of teaching methodologies adopted by the faculty members in institutes of higher learning. In this study we have used Sentiment Analysis as the basis for evaluating the effectiveness of learning of students based on a sample of 1419 feedback returns. Discriminant Analysis approach is attempted for such an evaluation. Fourteen predetermined variables constitute the overall analysis. According to the study it is founded that Communication and Presentation Skills are the main determinants of differences between two groups of students based on the computation of overall scores for the sampled data. The study has also shown that there exists significant correlation between the predictor variables. A study like this may contribute to framing and formulating policies for effective teaching methdologies to be adopted by the management of higher educational institutions.",
        "DOI": "10.1063/5.0126567",
        "affiliation_name": "SRM Institute of Science and Technology",
        "affiliation_city": "Kattankulathur",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Training a legged robot to walk using machine learning and trajectory control for high positional accuracy",
        "paper_author": "Biswas A.",
        "publication": "AI-Enabled Social Robotics in Human Care Services",
        "citied_by": "1",
        "cover_date": "2023-04-24",
        "Abstract": "Legged robots are a class of biologically inspired robots that use articulated leg mechanisms for locomotion. Legged motion is very complex and requires specialized actuation mechanisms and complicated motion control systems to operate. Traditional legged robots were controlled by purely physics-based, however, recent developments of artificial intelligence (AI), and machine learning (ML) techniques have opened new opportunities to train locomotion skills in a legged robot in a much more efficient way than the traditional physics-based controllers. In this chapter, the authors study how machine learning techniques are used to train quadruped robots in basic locomotion skills, evaluate training accuracy, training speed, and also discussed performance, simulation environment, trajectory control, and how the authors achieved accurate tracking of trajectories. Furthermore, this chapter delves into the details of the actual quadruped robot that the authors built to evaluate locomotion policies and some challenges that were faced in building the real robot.",
        "DOI": "10.4018/978-1-6684-8171-4.ch006",
        "affiliation_name": "Ganpat University",
        "affiliation_city": "Kherva",
        "affiliation_country": "India"
    },
    {
        "paper_title": "A hybrid deep learning approach for abusive text detection",
        "paper_author": "Fale P.N.",
        "publication": "AIP Conference Proceedings",
        "citied_by": "4",
        "cover_date": "2023-04-24",
        "Abstract": "Nowadays, social media is seeing a spike in hostility, which results in a large number of individuals being subjected to online abusive conduct and harassment. The expansion of social platforms has resulted in a significant rise in the amount of user-generated material available online. Users have been given the ability to produce, share, and trade material in order to connect and communicate with one another through these platforms. The internet has, on the other hand, opened up new opportunities for cyber-bullies and haters to communicate their vitriol to a bigger audience, typically in an anonymized manner. Social media has become a frequent place for people to express themselves in slang, rude, and obscene language. The use of slang, abusive, vulgar, and foul language is prohibited by the censorship policies of social media companies. However, due to a lack of resources and research into automatic detection mechanisms for abusive speech methodologies besides just English, this heinous act continues to be practiced despite the existence of such policies. In this paper, we propose a novel hybrid deep learning approach for an abusive text detection system to detect \"not suitable for work\"(NSFW) English text. Here we develop a model for text datasets. The hybrid model for text dataset is based on CNN and LSTM. We also perform the classification using five machine learning algorithms for comparative analysis of the results.",
        "DOI": "10.1063/5.0128071",
        "affiliation_name": "Bhagwant University",
        "affiliation_city": "Ajmer",
        "affiliation_country": "India"
    },
    {
        "paper_title": "FLAP - A Federated Learning Framework for Attribute-based Access Control Policies",
        "paper_author": "Jabal A.A.",
        "publication": "CODASPY 2023 - Proceedings of the 13th ACM Conference on Data and Application Security and Privacy",
        "citied_by": "3",
        "cover_date": "2023-04-24",
        "Abstract": "Technology advances in areas such as sensors, IoT, and robotics, enable new collaborative applications (e.g., autonomous devices). A primary requirement for such collaborations is to have a secure system that enables information sharing and information flow protection. A policy-based management system is a key mechanism for secure selective sharing of protected resources. However, policies in each party of a collaborative environment cannot be static as they have to adapt to different contexts and situations. One advantage of collaborative applications is that each party in the collaboration can take advantage of the knowledge of the other parties for learning or enhancing its own policies. We refer to this learning mechanism as policy transfer. The design of a policy transfer framework has challenges, including policy conflicts and privacy issues. Policy conflicts typically arise because of differences in the obligations of the parties, whereas privacy issues result because of data sharing constraints for sensitive data. Hence, the policy transfer framework should be able to tackle such challenges by considering minimal sharing of data and supporting policy adaptation to address conflict. In the paper, we propose a framework that aims at addressing such challenges. We introduce a formal definition of the policy transfer problem for attribute-based access control policies. We then introduce the transfer methodology which consists of three sequential steps. Finally, we report experimental results.",
        "DOI": "10.1145/3577923.3583641",
        "affiliation_name": "German Jordanian University",
        "affiliation_city": "Amman",
        "affiliation_country": "Jordan"
    },
    {
        "paper_title": "Handbook of Artificial Intelligence in Education",
        "paper_author": "du Boulay B.",
        "publication": "Handbook of Artificial Intelligence in Education",
        "citied_by": "19",
        "cover_date": "2023-04-21",
        "Abstract": "Gathering insightful and stimulating contributions from leading global experts in Artificial Intelligence in Education (AIED), this comprehensive Handbook traces the development of AIED from its early foundations in the 1970s to the present day. The Handbook evaluates the use of AI techniques such as modelling in closed and open domains, machine learning, analytics, language understanding and production to create systems aimed at helping learners, teachers, and educational administrators. Chapters examine theories of affect, metacognition and pedagogy applied in AIED systems; foundational aspects of AIED architecture, design, authoring and evaluation; and collaborative learning, the use of games and psychomotor learning. It concludes with a critical discussion of the wider context of Artificial Intelligence in Education, examining its commercialisation, social and political role, and the ethics of its systems, as well as reviewing the possible challenges and opportunities for AIED in the next 20 years. Providing a broad yet detailed account of the current field of Artificial Intelligence in Education, researchers and advanced students of education technology, innovation policy, and university management will benefit from this thought-provoking Handbook. Chapters will also be useful to support undergraduate courses in AI, computer science, and education.",
        "DOI": "10.4337/9781800375413",
        "affiliation_name": "The University of Sydney",
        "affiliation_city": "Sydney",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "ASSESSMENT OF CROPLAND TRANSITION TO SOLAR FARMS AND OTHER LAND USE/COVER USING RS, GIS AND ANN-CA",
        "paper_author": "Principe J.A.",
        "publication": "International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",
        "citied_by": "4",
        "cover_date": "2023-04-21",
        "Abstract": "The balance between food security and energy security is a national issue of extreme importance. A more stable supply of electricity could be achieved as solar farms expand but at the expense of losing some of the prime agricultural lands which endangers availability of sufficient agricultural produce. This study aims to use ANN-Cellular Automata (CA) via the Geographic Information System (GIS) platform and remote sensing (RS) data to assess the impact of cropland transition to solar farms and other land use/land cover (LULC). Several remotely sensed data were processed including MODIS land cover data (MCD12Q1), VIIRS nighttime lights (VNL v2.1), Advanced Himawari Imager Shortwave Radiation (AHI-SWR) product, and population density (LandScan) as inputs to the Cellular Automata-Artificial Neural Network (CA-ANN) model to simulate LULC changes in Tarlac Province, Philippines via the Modules for Land Use Change Evaluation (MOLUSCE) plugin in QGIS. For years 2019, 2023 and 2027 with 2015 as the base year, results showed an increasing trend for savannas and grassland with ΔLULC values of +11.4% to +15.1% and +0.2% to 3.5%, respectively. Meanwhile, a decreasing trend is observed for built-up/water, forest, and cropland with ΔLULC values of −3.0% to −6.3%, −8.5% to −21.1%, and −3.9% to −4.2%, respectively. Results also showed a conversion of a 100-ha area of croplands to solar farm from year 2019 to 2023 which translates to an estimated monetary loss from agricultural produce due to solar farm conversion amounting to Php 7,584,720.00 (∼USD 138,000) which is equivalent to the total average annual income of about 67 families in Tarlac. Lastly, the simulated 2027 LULC map showed pixels with unrealistic conversions from solar farm (year 2023) to cropland (year 2027). To improve the model, it is recommended to add more spatial data to effectively capture factors that may contribute to the expansion of solar farms in the future. Moreover, high resolution LULC maps (vector maps if available) can be used instead of a course resolution satellite-derived raster data. Nonetheless, this study has demonstrated the use of RS, GIS and machine learning techniques to model cropland conversion to solar farms and other LULC classes. Results from this study can provide scientific data to policy makers, solar industry players and other relevant stakeholders in doing technoeconomic assessment of solar farm development and expansion considering its effect on energy security and food security towards national sustainable development.",
        "DOI": "10.5194/isprs-archives-XLVIII-M-1-2023-255-2023",
        "affiliation_name": "University of the Philippines Diliman, College of Engineering",
        "affiliation_city": "Quezon City",
        "affiliation_country": "Philippines"
    },
    {
        "paper_title": "Translating physiological tools to augment crop breeding",
        "paper_author": "Mamrutha H.M.",
        "publication": "Translating Physiological Tools to Augment Crop Breeding",
        "citied_by": "1",
        "cover_date": "2023-04-19",
        "Abstract": "This book covers different physiological processes, tools, and their application in crop breeding. Each chapter emphasizes on a specific trait/physiological process and its importance in crop, their phenotyping information and how best it can be employed for crop improvement by projecting on success stories in different crops. It covers wide range of physiological topics including advances in field phenotyping, role of endophytic fungi, metabolomics, application of stable isotopes, high throughput phenomics, transpiration efficiency, root phenotyping and root exudates for improved resource use efficiency, cuticular wax and its application, advances in photosynthetic studies, leaf spectral reflectance and physiological breeding in hardy crops like millets. This book also covers the futuristic research areas like artificial intelligence and machine learning. This contributed volume compiles all application parts of physiological tools along with their advanced research in these areas, which is very much need of the hour for both academics and researchers for ready reference. This book will be of interest to teachers, researchers, climate change scientists, capacity builders, and policy makers. Also, the book serves as additional reading material for undergraduate and graduate students of agriculture, physiology, botany, ecology, and environmental sciences. National and international agricultural scientists will also find this a useful resource.",
        "DOI": "10.1007/978-981-19-7498-4",
        "affiliation_name": "ICAR - Indian Institute of Wheat And Barley Research, Karnal",
        "affiliation_city": "Karnal",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Augmenting Human Cognition with an AI-Mediated Intelligent Visual Feedback",
        "paper_author": "Xu S.",
        "publication": "Conference on Human Factors in Computing Systems - Proceedings",
        "citied_by": "3",
        "cover_date": "2023-04-19",
        "Abstract": "In this paper, we introduce an AI-mediated framework that can provide intelligent feedback to augment human cognition. Specifically, we leverage deep reinforcement learning (DRL) to provide adaptive time pressure feedback to improve user performance in a math arithmetic task. Time pressure feedback could either improve or deteriorate user performance by regulating user attention and anxiety. Adaptive time pressure feedback controlled by a DRL policy according to users' real-time performance could potentially solve this trade-off problem. However, the DRL training and hyperparameter tuning may require large amounts of data and iterative user studies. Therefore, we propose a dual-DRL framework that trains a regulation DRL agent to regulate user performance by interacting with another simulation DRL agent that mimics user cognition behaviors from an existing dataset. Our user study demonstrates the feasibility and effectiveness of the dual-DRL framework in augmenting user performance, in comparison to the baseline group.",
        "DOI": "10.1145/3544548.3580905",
        "affiliation_name": "University of California, San Diego",
        "affiliation_city": "La Jolla",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Accountability in Algorithmic Systems: From Principles to Practice",
        "paper_author": "Wilkinson D.",
        "publication": "Conference on Human Factors in Computing Systems - Proceedings",
        "citied_by": "4",
        "cover_date": "2023-04-19",
        "Abstract": "Growing concerns over the societal implications of artificial intelligence has motivated an interdisciplinary push towards mechanisms and tools that hold algorithmic systems accountable. Although there have been considerable strides around defining what it means to hold AI systems accountable, operationalizing those principles have created a barrage of challenges. Researchers, practitioners, and regulators have all raised concerns about the completeness of accountability methods and observed spikes in anxiousness about the potential risk of these tools being manipulated as rubber stamps of approval while harms continue to slip through the cracks. This interactive panel gathers researchers and practitioners with expertise in HCI, Responsible AI, Machine Learning, and Public Policy to critically discuss issues regarding accountability in algorithmic systems to reflect on potential opportunities for re-imagining scalable directions for accountability within these systems.",
        "DOI": "10.1145/3544549.3583747",
        "affiliation_name": "Mozilla Foundation",
        "affiliation_city": "Mountain View",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "JACC Journals’ Pathway Forward With AI Tools: The Future Is Now",
        "paper_author": "Fuster V.",
        "publication": "Journal of the American College of Cardiology",
        "citied_by": "7",
        "cover_date": "2023-04-18",
        "Abstract": "NA",
        "DOI": "10.1016/j.jacc.2023.02.030",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Impact of expenditures and corporate philanthropy disclosure on company value",
        "paper_author": "Fedorova E.",
        "publication": "Corporate Communications",
        "citied_by": "1",
        "cover_date": "2023-04-17",
        "Abstract": "Purpose: The paper aims to estimate how corporate philanthropy expenditures and corporate philanthropy disclosure (in general and in different spheres) affect investment attractiveness of Russian companies. Design/methodology/approach: To assess the degree of corporate philanthropy disclosure the authors compiled lexicons based on a set of techniques: text and frequency analysis, correlations, principal component analysis. To adjust the existing classifications of corporate philanthropic activities to the Russian market the authors employed expert analysis. The empirical research base includes 83 Russian publicly traded companies for the period 2013–2019. To estimate the impact of indicators of corporate philanthropy disclosure on company's investment attractiveness the authors utilized panel data regression and random forest algorithm. Findings: We compiled 2 Russian lexicons: one on general issues of corporate philanthropy and another one on philanthropic activities in various spheres (sports and healthcare; support for certain groups of people; social infrastructure; children protection and youth policy; culture, education and science). 2. The paper observes that the disclosure of non-financial data including that related to general issues of corporate philanthropy as well as to different spheres affects the market capitalization of the largest Russian companies. The results of regression analysis suggest that disclosure of altruism-driven philanthropic activities (such as corporate philanthropy in the sphere of culture, education and science) has a lesser impact on company's investment attractiveness than that of activities driven by business-related motives (sports and healthcare, children protection and youth policy). Research limitations/implications: Our findings are important to management, investors, financial analysts, regulators and various agencies providing guidance on corporate governance and sustainability reporting. However, the authors acknowledge that the research results may lack generalizability due to the sample covering a single national context. Researchers are encouraged to test the proposed approach further on other countries' data by using the authors’ compiled lexicons. Originality/value: The study aims to expand the domains of signaling and agency theories. First, this subject has not been widely examined in terms of emerging markets, the authors’ study is the first to focus on the Russian market. Secondly, the majority of scholars use text analysis to examine not only the impact of charitable donations but also the effect of corporate philanthropy disclosure. Thirdly, the authors provided the authors’ own lexicon of corporate philanthropy disclosure based on machine learning technique and expert analysis. Fourthly, to estimate the impact of corporate philanthropy on company's investment attractiveness the authors used the original approach based on combination of linear (regression), and non-linear methods (permutation importance. The authors’ findings extend the theoretical concept of Peterson et al. (2021): corporate philanthropy is viewed as the company strategy to reinforce its reputation, it helps to establish more efficient relationships with stakeholders which, in its turn, results in the increased business value.",
        "DOI": "10.1108/CCIJ-10-2022-0122",
        "affiliation_name": "Financial University under the Government of the Russian Federation",
        "affiliation_city": "Moscow",
        "affiliation_country": "Russian Federation"
    },
    {
        "paper_title": "Design and Analyze the Machine Learning Based Sarcasm Prediction Model for Social Media Context",
        "paper_author": "Rajani B.",
        "publication": "International Journal of Intelligent Systems and Applications in Engineering",
        "citied_by": "0",
        "cover_date": "2023-04-16",
        "Abstract": "Sentiment Analysis examines the predominance of sarcastic language and its difficulties detecting sentiment in the text. The identification of sarcasm in the text is the focus of automatic sarcasm detection. Sarcasm recognition has been more popular in recent years and has extensive use in sentiment analysis. In this paper, we proposed sarcasm detection using machine learning. In the first phase, data has collected from social media sources such as Twitter and other synthetic datasets. The policy-based data filtration technique is used for data pre-processing and generating the normalized data vectors. The different feature extraction and selection approaches have been used for hybrid feature selection, such as TF-IDF, NLP features, Dependency features and lexicon-based features from the entire context. The various machine learning classification algorithms have been used to predict positive, negative and neutral sarcasm. The WEKA 3.7 machine learning framework has been utilized for classification. As a result, SVM produces higher classification accuracy of 95.60% over the conventional machine learning classifier.",
        "DOI": "NA",
        "affiliation_name": "Sanjay Ghodawat University, Kolhapur",
        "affiliation_city": "Kolhapur",
        "affiliation_country": "India"
    },
    {
        "paper_title": "AI, What Does the Future Hold for Us? Automating Strategic Foresight",
        "paper_author": "Rozanec J.",
        "publication": "ICPE 2023 - Companion of the 2023 ACM/SPEC International Conference on Performance Engineering",
        "citied_by": "2",
        "cover_date": "2023-04-15",
        "Abstract": "There is an increasing awareness that strategic foresight is much needed to guide efficient policy-making. The growing digitalization implies a rising amount of digital evidence of many aspects of society (e.g., science, economy, and politics). Artificial intelligence can process massive amounts of data and extract meaningful information. Furthermore, a knowledge graph can be developed to capture significant aspects of reality, and machine learning models can be used to identify patterns and derive insights. This paper describes how we envision artificial intelligence could be used to create and deliver strategic foresight automatically.",
        "DOI": "10.1145/3578245.3585336",
        "affiliation_name": "Institut \"Jožef Stefan\"",
        "affiliation_city": "Ljubljana",
        "affiliation_country": "Slovenia"
    },
    {
        "paper_title": "Towards an airtightness compliance tool based on machine learning models for naturally ventilated dwellings",
        "paper_author": "Cardoso V.E.M.",
        "publication": "Energy and Buildings",
        "citied_by": "2",
        "cover_date": "2023-04-15",
        "Abstract": "Physical models and probabilistic applications often guide the study and characterization of natural phenomena in engineering. Such is the case of the study of air change rates (ACHs) in buildings for their complex mechanisms and high variability. It is not uncommon for the referred applications to be costly and impractical in both time and computation, resulting in the use of simplified methodologies and setups. The incorporation of airtightness limits to quantify adequate ACHs in national transpositions of the Energy Performance Building Directive (EPBD) exemplifies the issue. This research presents a roadmap for developing an alternative instrument, a compliance tool built with a Machine Learning (ML) framework, that overcomes some simplification issues regarding policy implementation while fulfilling practitioners' needs and general societal use. It relies on dwellings' terrain, geometric and airtightness characteristics, and meteorological data. Results from previous work on a region with a mild heating season in southern Europe apply in training and testing the proposed tool. The tool outputs numerical information on the air change rates performance of the building envelope, and a label, accordingly. On the test set, the best regressor showed mean absolute errors (MAE) below 1.02% for all the response variables, while the best classifier presented an average accuracy of 97.32%. These results are promising for the generalization of this methodology, with potential for application at regional, national, and European Union levels. The developed tool could be a complementary asset to energy certification programmes of either public or private initiatives.",
        "DOI": "10.1016/j.enbuild.2023.112922",
        "affiliation_name": "Institute for Sustainability and Innovation in Structural Engineering",
        "affiliation_city": "Coimbra",
        "affiliation_country": "Portugal"
    },
    {
        "paper_title": "Re-assessing human mortality risks attributed to PM2.5-mediated effects of agricultural ammonia",
        "paper_author": "Cox L.A.",
        "publication": "Environmental Research",
        "citied_by": "8",
        "cover_date": "2023-04-15",
        "Abstract": "How can and should epidemiologists and risk assessors assemble and present evidence for causation of mortality or morbidities by identified agents such as fine particulate matter or other air pollutants? As a motivating example, some scientists have warned recently that ammonia from the production of meat significantly increases human mortality rates in exposed populations by increasing the ambient concentration of fine particulate matter (PM2.5) in air. We reexamine the support for such conclusions, including quantitative calculations that attribute deaths to PM2.5 air pollution by applying associational results such as relative risks, odds ratios, or slope coefficients from regression models to predict the effects on mortality or morbidity of reducing PM2.5 exposures. Taking an outside perspective from the field of causal artificial intelligence (CAI), we conclude that these attribution calculations are methodologically unsound. They produce unreliable conclusions because they ignore an essential distinction between differences in outcomes observed at different levels of exposure and changes in outcomes caused by changing exposure. We find that multiple studies that have examined associations between changes over time in particulate exposure and mortality risk instead of differences in exposures and corresponding mortality risks have found no clear evidence that observed changes in exposure help to predict or explain subsequent changes in mortality risks. We conclude that there is no sound theoretical or empirical reason to believe that reducing ammonia emissions from farms has reduced or would reduce human mortality risks. More generally, applying CAI principles and methods can potentially improve current widespread practices of unsound causal inferences and policy-relevant causal claims that are made without the benefit of formal causal analysis in air pollution health effects research and in other areas of applied epidemiology and public health risk assessment.",
        "DOI": "10.1016/j.envres.2023.115311",
        "affiliation_name": "Cox Associates, Inc.",
        "affiliation_city": "Denver",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Probabilistic modeling of future electricity systems with high renewable energy penetration using machine learning",
        "paper_author": "Mayer M.J.",
        "publication": "Applied Energy",
        "citied_by": "24",
        "cover_date": "2023-04-15",
        "Abstract": "The increasing penetration of weather-dependent renewable energy generation calls for high-resolution modeling of the possible future energy mixes to support the energy strategy and policy decisions. Simulations relying on the data of only a few years, however, are not only unreliable but also unable to quantify the uncertainty resulting from the year-to-year variability of the weather conditions. This paper presents a new method based on artificial neural networks that map the relationship between the weather data from atmospheric reanalysis and the photovoltaic and wind power generation and the electric load. The regression models are trained based on the data of the last 3 to 6 years, and then they are used to generate synthetic hourly renewable power production and load profiles for 42 years as an ensemble representation of possible outcomes in the future. The modeled profiles are post-processed by a novel variance-correction method that ensures the statistical similarity of the modeled and real data and thus the reliability of the simulation based on these profiles. The probabilistic modeling enabled by the proposed approach is demonstrated in two practical applications for the Hungarian electricity system. First, the so-called Dunkelflaute (dark doldrum) events, are analyzed and categorized. The results reveal that Dunkelflaute events most frequently happen on summer nights, and their typical duration is less than 12 h, even though events ranging through multiple days are also possible. Second, the renewable energy supply is modeled for different photovoltaic and wind turbine installed capacities. Based on our calculations, the share of the annual power consumption that weather-dependent renewable generation can directly cover is up to 60% in Hungary, even with very high installed capacities and overproduction, and higher carbon-free electricity share targets can only be achieved with an energy mix containing nuclear power and renewable sources. The proposed method can easily be extended to other countries and used in more detailed electricity market simulations in the future.",
        "DOI": "10.1016/j.apenergy.2023.120801",
        "affiliation_name": "Budapest University of Technology and Economics",
        "affiliation_city": "Budapest",
        "affiliation_country": "Hungary"
    },
    {
        "paper_title": "The application of machine learning-based energy management strategy in a multi-mode plug-in hybrid electric vehicle, part II: Deep deterministic policy gradient algorithm design for electric mode",
        "paper_author": "Ruan J.",
        "publication": "Energy",
        "citied_by": "24",
        "cover_date": "2023-04-15",
        "Abstract": "Machine learning (ML)-based methods have attracted great attention in the multi-objective optimization problems, which is the key challenge in the energy management strategy (EMS) of the multi-power hybrid system. Our recently published research in this journal verified the effectiveness and feasibility of a Deep Deterministic Policy Gradient (DDPG)-based EMS in the charge-sustaining (CS) stage of a multi-mode plug-in hybrid vehicle (PHEV). However, the application of ML-based-EMS in the charge-depletion (CD) stage and the regenerative braking mode of PHEV are still missing. This study proposes a discrete-continuous hybrid actions-based hierarchical EMS to optimally distribute the dual-motor driving force in battery electric driving and regenerative braking. In the upper layer of EMS, DDPG is trained to learn the torque distribution principles of dual-motor operation to achieve better energy efficiency without losing dynamic performance. Meanwhile, the total recoverable braking torque is also determined by the upper layer EMS considering the braking demand, mechanical and electrical braking system conditions, vehicle safety, and the provisions of law. In the lower level of EMS, the driving mode is determined under the guidance of energy consumption optimization. The verified results show that the proposed EMS outperforms other deep reinforcement learning (DRL)-based hierarchical and non-hierarchical EMSs.",
        "DOI": "10.1016/j.energy.2023.126792",
        "affiliation_name": "Chang'an University",
        "affiliation_city": "Xi'an",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Airport terminal passenger forecast under the impact of COVID-19 outbreaks: A case study from China",
        "paper_author": "Tang H.",
        "publication": "Journal of Building Engineering",
        "citied_by": "12",
        "cover_date": "2023-04-15",
        "Abstract": "Passengers significantly affect airport terminal energy consumption and indoor environmental quality. Accurate passenger forecasting provides important insights for airport terminals to optimize their operation and management. However, the COVID-19 pandemic has greatly increased the uncertainty in airport passenger since 2020. There are insufficient studies to investigate which pandemic-related variables should be considered in forecasting airport passenger trends under the impact of COVID-19 outbreaks. In this study, the interrelationship between COVID-19 pandemic trends and passenger traffic at a major airport terminal in China was analyzed on a day-by-day basis. During COVID-19 outbreaks, three stages of passenger change were identified and characterized, i.e., the decline stage, the stabilization stage, and the recovery stage. A typical “sudden drop and slow recovery” pattern of passenger traffic was identified. A LightGBM model including pandemic variables was developed to forecast short-term daily passenger traffic at the airport terminal. The SHapley Additive exPlanations (SHAP) values was used to quantify the contribution of input pandemic variables. Results indicated the inclusion of pandemic variables reduced the model error by 27.7% compared to a baseline model. The cumulative numbers of COVID-19 cases in previous weeks were found to be stronger predictors of future passenger traffic than daily COVID-19 cases in the most recent week. In addition, the impact of pandemic control policies and passengers' travel behavior was discussed. Our empirical findings provide important implications for airport terminal operations in response to the on-going COVID-19 pandemic.",
        "DOI": "10.1016/j.jobe.2022.105740",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Intelligent Deposit Product Recommendation for Flexible Employees of Housing Provident Fund Based on Random Forest",
        "paper_author": "Chen X.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-04-14",
        "Abstract": "With the rapid development of big data and cloud computing, digital technology has been widely used to develop provident funds. We innovatively apply data mining and machine learning methods to analyze the impact factors of provident fund deposits and the recommendation model of the deposit products for flexible employees. After experiments, our model performs well in the performance evaluation index (such as Accuracy, Recall, and F1-score). Through our model and research analysis, the housing provident fund center can comprehensively analyze and mine the real data, promote more popular customer policies, and provide better customer service.",
        "DOI": "10.1145/3616901.3617017",
        "affiliation_name": "Chongqing University",
        "affiliation_city": "Chongqing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Developing Artificial Intelligence (AI) and Machine Learning (ML) Based Soft Sensors for In-Cylinder Predictions with a Real-Time Simulator and a Crank Angle Resolved Engine Model",
        "paper_author": "Jane R.",
        "publication": "SAE Technical Papers",
        "citied_by": "0",
        "cover_date": "2023-04-11",
        "Abstract": "Currently, there are no safe and suitable fuel sources with comparable power density to traditional combustible fuels capable of replacing Internal Combustion Engines (ICEs). For the foreseeable future, civilian and military systems are likely to be reliant on traditional combustible fuels. Hybridization of the vehicle powertrains is the most likely avenue which can reduce emissions, minimize system inefficiencies, and build more sustainable vehicle systems that support the United States Army modernization priorities. Vehicle systems may further be improved by the creation and implementation of artificial intelligence and machine learning (AI/ML) in the form of advanced predictive capabilities and more robust control policies. AI/ML requires numerous characterized and complete datasets, given the sensitive nature of military systems, such data is unlikely to be known or accessible limiting the reach to develop and deploy AI/ML to military systems. With the absence of data, AI/ML may still be developed and deployed to military systems if supported by near-real-time or real-time computationally efficient and effective hardware and software or cloud-based computing. In this research, an OPAL real-time (OPAL-RT) simulator was used to emulate a compression ignition (CI) engine simulation architecture capable of developing and deploying advanced AI/ML predictive algorithms. The simulation architecture could be used for developing online predictive capabilities required to maximize the effectiveness or efficiency of a vehicle. The architecture includes a real-time simulator (RTS), a host PC, and a secondary PC. The RTS simulates a crank angle resolved engine model which utilized pseudo engine dynamometer data in the form of multi-dimensional matrices to emulate quasi-steady state conditions of the engine. The host PC was used to monitor and control the engine while the secondary PC was used to train the AI/ML to predict the per-cylinder generated torque from the crank shaft torque, which was then used to predict the in-cylinder temperature and pressure. The results indicate that using minimal sensor data and pretrained predictive algorithms, in-cylinder characterizations for unobserved engine variables may be achievable, providing an approximate characterization of quasi-steady state in-cylinder conditions.",
        "DOI": "10.4271/2023-01-0102",
        "affiliation_name": "U.S. Army Research Laboratory",
        "affiliation_city": "Adelphi",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Reinforcement learning of control strategies for reducing skin friction drag in a fully developed turbulent channel flow",
        "paper_author": "Sonoda T.",
        "publication": "Journal of Fluid Mechanics",
        "citied_by": "25",
        "cover_date": "2023-04-10",
        "Abstract": "Reinforcement learning is applied to the development of control strategies in order to reduce skin friction drag in a fully developed turbulent channel flow at a low Reynolds number. Motivated by the so-called opposition control (Choi et al., J. Fluid Mech., vol. 253, 1993, pp. 509-543), in which a control input is applied so as to cancel the wall-normal velocity fluctuation on a detection plane at a certain distance from the wall, we consider wall blowing and suction as a control input, and its spatial distribution is determined by the instantaneous streamwise and wall-normal velocity fluctuations at distance 15 wall units above the wall. A deep neural network is used to express the nonlinear relationship between the sensing information and the control input, and it is trained so as to maximize the expected long-term reward, i.e. drag reduction. When only the wall-normal velocity fluctuation is measured and a linear network is used, the present framework reproduces successfully the optimal linear weight for the opposition control reported in a previous study (Chung & Talha, Phys. Fluids, vol. 23, 2011, 025102). In contrast, when a nonlinear network is used, more complex control strategies based on the instantaneous streamwise and wall-normal velocity fluctuations are obtained. Specifically, the obtained control strategies switch abruptly between strong wall blowing and suction for downwelling of a high-speed fluid towards the wall and upwelling of a low-speed fluid away from the wall, respectively. Extracting key features from the obtained policies allows us to develop novel control strategies leading to drag reduction rates as high as 37 %, which is higher than the 23 % achieved by the conventional opposition control at the same Reynolds number. Finding such an effective and nonlinear control policy is quite difficult by relying solely on human insights. The present results indicate that reinforcement learning can be a novel framework for the development of effective control strategies through systematic learning based on a large number of trials.",
        "DOI": "10.1017/jfm.2023.147",
        "affiliation_name": "Institute of Industrial Science",
        "affiliation_city": "Tokyo",
        "affiliation_country": "Japan"
    },
    {
        "paper_title": "JACC Journals’ Pathway Forward With AI Tools: The Future Is Now",
        "paper_author": "Fuster V.",
        "publication": "JACC: Cardiovascular Interventions",
        "citied_by": "0",
        "cover_date": "2023-04-10",
        "Abstract": "NA",
        "DOI": "10.1016/j.jcin.2023.03.001",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Hybrid-biotaxonomy-like machine learning enables an anticipated surface plasmon resonance of Au/Ag nanoparticles assembled on ZnO nanorods",
        "paper_author": "Liao Y.K.",
        "publication": "Journal of Materials Chemistry A",
        "citied_by": "6",
        "cover_date": "2023-04-05",
        "Abstract": "Sustainable energy strategies, particularly alternatives to fossil fuels, e.g., solar-to-hydrogen production, are highly desired due to the energy crisis. Therefore, materials leading to hydrogen production by utilizing water and sunlight are extensively investigated, such as nanomaterials modified by gold nanoparticles (AuNPs) of different structures, which enable photoelectrochemical water splitting through light-to-plasmon resonance. However, light-to-plasmon resonance depends on the gold nanoparticles' properties. Therefore, an accurate projection model, which correlates the fabrication parameters and light-to-plasmon resonance, can facilitate the selection and the subsequent application of AuNPs. In this regard, we established a hybrid-biotaxonomy-like machine learning (ML) model based on genetic algorithm neural networks (GANN) to investigate the light-to-plasmon properties of a six-layer coating of noble metal nanoparticles (NMNPs) on ZnO nanorods. Meanwhile, we understood the plasmonic peak shift of every NMNP coating layer by exploiting the multivariate normal distribution method and the concept of phylogenetic nomenclature from evolutionary developmental biology.",
        "DOI": "10.1039/d3ta00324h",
        "affiliation_name": "ETH Zürich",
        "affiliation_city": "Zurich",
        "affiliation_country": "Switzerland"
    },
    {
        "paper_title": "Historical Redlining, Social Determinants of Health, and Stroke Prevalence in Communities in New York City",
        "paper_author": "Jadow B.M.",
        "publication": "JAMA Network Open",
        "citied_by": "16",
        "cover_date": "2023-04-05",
        "Abstract": "Importance: Historical redlining was a discriminatory housing policy that placed financial services beyond the reach of residents in inner-city communities. The extent of the impact of this discriminatory policy on contemporary health outcomes remains to be elucidated. Objective: To evaluate the associations among historical redlining, social determinants of health (SDOH), and contemporary community-level stroke prevalence in New York City. Design, Setting, and Participants: An ecological, retrospective, cross-sectional study was conducted using New York City data from January 1, 2014, to December 31, 2018. Data from the population-based sample were aggregated on the census tract level. Quantile regression analysis and a quantile regression forests machine learning model were used to determine the significance and overall weight of redlining in relation to other SDOH on stroke prevalence. Data were analyzed from November 5, 2021, to January 31, 2022. Exposures: Social determinants of health included race and ethnicity, median household income, poverty, low educational attainment, language barrier, uninsurance rate, social cohesion, and residence in an area with a shortage of health care professionals. Other covariates included median age and prevalence of diabetes, hypertension, smoking, and hyperlipidemia. Weighted scores for historical redlining (ie, the discriminatory housing policy in effect from 1934 to 1968) were computed using the mean proportion of original redlined territories overlapped on 2010 census tract boundaries in New York City. Main Outcomes and Measures: Stroke prevalence was collected from the Centers for Disease Control and Prevention 500 Cities Project for adults 18 years and older from 2014 to 2018. Results: A total of 2117 census tracts were included in the analysis. After adjusting for SDOH and other relevant covariates, the historical redlining score was independently associated with a higher community-level stroke prevalence (odds ratio [OR], 1.02 [95% CI, 1.02-1.05]; P <.001). Social determinants of health that were positively associated with stroke prevalence included educational attainment (OR, 1.01 [95% CI, 1.01-1.01]; P <.001), poverty (OR, 1.01 [95% CI, 1.01-1.01]; P <.001), language barrier (OR, 1.00 [95% CI, 1.00-1.00]; P <.001), and health care professionals shortage (OR, 1.02 [95% CI, 1.00-1.04]; P =.03). Conclusions and Relevance: This cross-sectional study found that historical redlining was associated with modern-day stroke prevalence in New York City independently of contemporary SDOH and community prevalence of some relevant cardiovascular risk factors.",
        "DOI": "10.1001/jamanetworkopen.2023.5875",
        "affiliation_name": "Rutgers University–New Brunswick",
        "affiliation_city": "New Brunswick",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "A systematic literature review of reinforcement algorithms in machine learning",
        "paper_author": "Kabanda G.",
        "publication": "Handbook of Research on AI and Knowledge Engineering for Real-Time Business Intelligence",
        "citied_by": "3",
        "cover_date": "2023-04-04",
        "Abstract": "Reinforcement learning (RL) is learning from interactions with the environment in order to accomplish certain long-term objectives connected to the environmental condition. Reinforcement learning takes place when action sequences, observations, and rewards are used as inputs, and is hypothesis-based and goal-oriented. The purpose of the research was to conduct a systematic literature review of reinforcement algorithms in machine learning in order to develop a successful multi-agent RL algorithm that can be applied to robotics, network packet routing, energy distribution, and other applications. The robotics-related RL techniques of value-based RL, policy-based RL, model-based RL, deep reinforcement learning, meta-RL, and inverse RL were examined. As a result, the robotics-related RL techniques of value-based RL, policy-based RL, model-based RL, deep RL, meta-RL, and inverse RL were discussed in this research work. The asynchronous advantage actor-critic algorithm (A3C) is one of the best reinforcement algorithms. A3C performs better on deep RLchallenges and is quicker and easier to use.",
        "DOI": "10.4018/978-1-6684-6519-6.ch002",
        "affiliation_name": "Woxsen University",
        "affiliation_city": "Hyderabad",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Boosting poultry farm profits through blockchain technologies, ai, iot, and machine learning",
        "paper_author": "Chouragade G.",
        "publication": "Handbook of Research on AI and Knowledge Engineering for Real-Time Business Intelligence",
        "citied_by": "1",
        "cover_date": "2023-04-04",
        "Abstract": "The foundation of a trustworthy brand has been extensively researched in the FMCG industry. Establishing the proper culture and discipline and conducting a continuous cycle of improvement based on continuous feedback are critical success factors. Increasing agricultural output is one of several projects supported by nearly every administration. The creation of a sustainable methodology for managing demand and supply is a difficult but necessary obligation for these brands. The government's regulations and policies serve to balance its citizens' health risks. Many businesses determine their target customer approach by evaluating customer categories based on their income and spending capacity. These businesses maintain credibility by adhering to international quality standards for chicken products, measured by designated lab test results, customer experience, and brand campaign marketing and publicity. Higher prices for producing high-quality farm products, and uncertainty around quantity, are two of the most significant challenges in the industry of consumable farm products.",
        "DOI": "10.4018/978-1-6684-6519-6.ch009",
        "affiliation_name": "ABSA Bank Ltd.",
        "affiliation_city": null,
        "affiliation_country": "South Africa"
    },
    {
        "paper_title": "Research on air combat decision algorithm based on proximal policy optimization",
        "paper_author": "Zhang B.",
        "publication": "Advances in Aeronautical Science and Engineering",
        "citied_by": "2",
        "cover_date": "2023-04-01",
        "Abstract": "Facing the future combat scenario with manned and unmanned aerial vehicle cooperation，real-time and accurate air combat decision-making is the basis of winning. The complex air environment，transient situation data，and multiple cumbersome combat tasks make coordinated combat with unmanned aerial vehicles a trend in future air combat，replacing single machine combat. However，multi-agent modeling and training processes face difficulties in reward allocation and network convergence. Air combat scenarios for 5v5 manned and unmanned aerial vehicle cooperation，the characteristic model of single agent is abstracted in this paper，and an algorithm based on proximal policy optimization is proposed to obtain the air combat decision sequence by using reward and punishment incentive in the real-time interaction with the environment. The simulation results show that the algorithm proposed in this paper can adapt to the complex battlefield situation and get a stable and reasonable decision-making strategy in con⁃ tinuous action space after training and learning.",
        "DOI": "10.16615/j.cnki.1674-8190.2023.02.17",
        "affiliation_name": "Aviation Industry Corporation of China，Limited",
        "affiliation_city": "Shenyang",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A behavioral approach to personalizing public health",
        "paper_author": "Ruggeri K.",
        "publication": "Behavioural Public Policy",
        "citied_by": "12",
        "cover_date": "2023-04-01",
        "Abstract": "Behavioral policies are increasingly popular in a number of health care contexts. However, evidence of their effectiveness, specifically in low-income and highly disadvantaged populations, is limited. Some positive effects have been found for adaptive interventions, which merge more personalized approaches with advances in data collection and modern analytical methods. These approaches have only recently become feasible, as their implementation requires a confluence of large-scale datasets, contemporary machine learning, and validated behavioral insights. Such methods have considerable potential to improve outcomes without requiring substantial increases in effort on the part of individuals. Using examples from health insurance choice, clinical attendance rates, and prescription of medicines, we present an argument for how adaptive approaches, especially those considering disadvantaged populations explicitly, offer an opportunity to generate equity in public health.",
        "DOI": "10.1017/bpp.2020.31",
        "affiliation_name": "SciencesPo Paris",
        "affiliation_city": "Paris",
        "affiliation_country": "France"
    },
    {
        "paper_title": "Livestock species rather than grazing intensity shape plant guild proportions in interaction with multiple environmental drivers in grassland from the Pyrenees",
        "paper_author": "Rodríguez A.",
        "publication": "Applied Vegetation Science",
        "citied_by": "4",
        "cover_date": "2023-04-01",
        "Abstract": "Questions: Does grazing management shape the proportion of plant guilds (grasses, legumes, forbs and sedges) in mountain grasslands? Which properties of grazing management have the greatest effect on the proportion of plant guilds: grazer diversity/identity or grazing intensity? Are the effects of grazing management on guild proportion modified by other environmental variables that explain plant guild distribution at broad spatial scales?. Location: Mountains in the Pyrenees. Methods: We modelled the proportion of grasses, legumes, forbs and sedges using data from the PASTUS database (n = 96), which contains a wide range of environmental and management conditions due to the high variety of environmental conditions in mountain grasslands in the Pyrenees. We used a machine-learning algorithm to find those variables that best explained the proportions of each plant guild. We focussed on the differences between the levels of grazing intensity and the grazing species included in the model, and on detecting interactions between grazing variables and climate, topography and soil conditions. Results: The proportion of forbs and grasses strongly depended on the grazing livestock species at broad spatial scales. Only soil pH showed a higher overall explanatory power on guild distribution. In general, forbs were favoured in cattle- and grasses in sheep-grazed grasslands, the latter also being favoured on acidic soils, while forbs were favoured in more alkaline soil conditions. However, the effects of those factors (grazing species and soil pH) were modulated through interactions with several other environmental variables, including soil Mg, K and P, and terrain slope. In contrast, grazing intensity was a minor driver of guild distribution. Conclusions: Our results provide information about the relationship between plant functional diversity, indicated by the different plant guild proportions, and grazing management in the Pyrenean grasslands. This information could be useful for developing hypotheses for future experimental studies and for designing policies to improve the management of mountain grasslands.",
        "DOI": "10.1111/avsc.12724",
        "affiliation_name": "Université Clermont Auvergne",
        "affiliation_city": "Clermont-Ferrand",
        "affiliation_country": "France"
    },
    {
        "paper_title": "Proxying economic activity with daytime satellite imagery: Filling data gaps across time and space",
        "paper_author": "Lehnert P.",
        "publication": "PNAS Nexus",
        "citied_by": "4",
        "cover_date": "2023-04-01",
        "Abstract": "This paper develops a novel procedure for proxying economic activity with daytime satellite imagery across time periods and spatial units, for which reliable data on economic activity are otherwise not available. In developing this unique proxy, we apply machine-learning techniques to a historical time series of daytime satellite imagery dating back to 1984. Compared to satellite data on night light intensity, another common economic proxy, our proxy more precisely predicts economic activity at smaller regional levels and over longer time horizons. We demonstrate our measure's usefulness for the example of Germany, where East German data on economic activity are unavailable for detailed regional levels and historical time series. Our procedure is generalizable to any region in the world, and it has great potential for analyzing historical economic developments, evaluating local policy reforms, and controlling for economic activity at highly disaggregated regional levels in econometric applications.",
        "DOI": "10.1093/pnasnexus/pgad099",
        "affiliation_name": "Stanford Graduate School of Education",
        "affiliation_city": "Stanford",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Aboveground biomass stock and change estimation in Amazon rainforest using airborne light detection and ranging, multispectral data, and machine learning algorithms",
        "paper_author": "Marchesan J.",
        "publication": "Journal of Applied Remote Sensing",
        "citied_by": "4",
        "cover_date": "2023-04-01",
        "Abstract": "Developing an efficient method to accurately estimate aboveground biomass in tropical forests is critical to monitoring the carbon stock and implementing policies to reduce emissions caused by deforestation. Thus, the objective of the present study was to estimate aboveground biomass in areas of the Amazonian Forest with selective logging, using the random forest (RF), support vector machine (SVM), and artificial neural network (ANN) machine learning algorithms, using light detection and ranging (LiDAR) data and these combined with OLI/Landsat 8 variables, as well as mapping the biomass for the years 2014 and 2017, allowing one to analyze its dynamics between the years of analysis. The RF and SVM algorithms obtained the lowest error values in all datasets. The association of the variables increased the RF performance. Analyzing the dynamics of biomass, it was observed that the oldest exploration units (2006, 2007, and 2008) have lower biomass stocks. The highest biomass losses in 2017 came from units operated between 2012 and 2013 (the most recent record). Thus, with the method used in this study, it was possible to infer that the machine learning algorithms were efficient in estimating the biomass, emphasizing the RF and the SVM.",
        "DOI": "10.1117/1.JRS.17.024509",
        "affiliation_name": "Universidade Federal de Santa Maria",
        "affiliation_city": "Santa Maria",
        "affiliation_country": "Brazil"
    },
    {
        "paper_title": "Entropy regularized reinforcement learning using large deviation theory",
        "paper_author": "Arriojas A.",
        "publication": "Physical Review Research",
        "citied_by": "2",
        "cover_date": "2023-04-01",
        "Abstract": "Reinforcement learning (RL) is an important field of research in machine learning that is increasingly being applied to complex optimization problems in physics. In parallel, concepts from physics have contributed to important advances in RL with developments such as entropy-regularized RL. While these developments have led to advances in both fields, obtaining analytical solutions for optimization in entropy-regularized RL is currently an open problem. In this paper, we establish a mapping between entropy-regularized RL and research in nonequilibrium statistical mechanics focusing on Markovian processes conditioned on rare events. In the long-time limit, we apply approaches from large deviation theory to derive exact analytical results for the optimal policy and optimal dynamics in Markov decision process (MDP) models of reinforcement learning. The results obtained lead to an analytical and computational framework for entropy-regularized RL which is validated by simulations. The mapping established in this work connects current research in reinforcement learning and nonequilibrium statistical mechanics, thereby opening avenues for the application of analytical and computational approaches from one field to cutting-edge problems in the other.",
        "DOI": "10.1103/PhysRevResearch.5.023085",
        "affiliation_name": "University of Massachusetts Boston",
        "affiliation_city": "Boston",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "A Prefetch-Adaptive Intelligent Cache Replacement Policy Based on Machine Learning",
        "paper_author": "Yang H.J.",
        "publication": "Journal of Computer Science and Technology",
        "citied_by": "8",
        "cover_date": "2023-04-01",
        "Abstract": "Hardware prefetching and replacement policies are two techniques to improve the performance of the memory subsystem. While prefetching hides memory latency and improves performance, interactions take place with the cache replacement policies, thereby introducing performance variability in the application. To improve the accuracy of reuse of cache blocks in the presence of hardware prefetching, we propose Prefetch-Adaptive Intelligent Cache Replacement Policy (PAIC). PAIC is designed with separate predictors for prefetch and demand requests, and uses machine learning to optimize reuse prediction in the presence of prefetching. By distinguishing reuse predictions for prefetch and demand requests, PAIC can better combine the performance benefits from prefetching and replacement policies. We evaluate PAIC on a set of 27 memory-intensive programs from the SPEC 2006 and SPEC 2017. Under single-core configuration, PAIC improves performance over Least Recently Used (LRU) replacement policy by 37.22%, compared with improvements of 32.93% for Signature-based Hit Predictor (SHiP), 34.56% for Hawkeye, and 34.43% for Glider. Under the four-core configuration, PAIC improves performance over LRU by 20.99%, versus 13.23% for SHiP, 17.89% for Hawkeye and 15.50% for Glider.",
        "DOI": "10.1007/s11390-022-1573-3",
        "affiliation_name": "Beijing University of Technology",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A claims-based, machine-learning algorithm to identify patients with pulmonary arterial hypertension",
        "paper_author": "Hyde B.",
        "publication": "Pulmonary Circulation",
        "citied_by": "6",
        "cover_date": "2023-04-01",
        "Abstract": "Many patients with pulmonary arterial hypertension (PAH) experience substantial delays in diagnosis, which is associated with worse outcomes and higher costs. Tools for diagnosing PAH sooner may lead to earlier treatment, which may delay disease progression and adverse outcomes including hospitalization and death. We developed a machine-learning (ML) algorithm to identify patients at risk for PAH earlier in their symptom journey and distinguish them from patients with similar early symptoms not at risk for developing PAH. Our supervised ML model analyzed retrospective, de-identified data from the US-based Optum® Clinformatics® Data Mart claims database (January 2015 to December 2019). Propensity score matched PAH and non-PAH (control) cohorts were established based on observed differences. Random forest models were used to classify patients as PAH or non-PAH at diagnosis and at 6 months prediagnosis. The PAH and non-PAH cohorts included 1339 and 4222 patients, respectively. At 6 months prediagnosis, the model performed well in distinguishing PAH and non-PAH patients, with area under the curve of the receiver operating characteristic of 0.84, recall (sensitivity) of 0.73, and precision of 0.50. Key features distinguishing PAH from non-PAH cohorts were a longer time between first symptom and the prediagnosis model date (i.e., 6 months before diagnosis); more diagnostic and prescription claims, circulatory claims, and imaging procedures, leading to higher overall healthcare resource utilization; and more hospitalizations. Our model distinguishes between patients with and without PAH at 6 months before diagnosis and illustrates the feasibility of using routine claims data to identify patients at a population level who might benefit from PAH-specific screening and/or earlier specialist referral.",
        "DOI": "10.1002/pul2.12237",
        "affiliation_name": "Janssen Pharmaceuticals, Inc.",
        "affiliation_city": "Titusville",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Policy Solutions: Policy questions for ChatGPT and artificial intelligence",
        "paper_author": "Collins J.E.",
        "publication": "Phi Delta Kappan",
        "citied_by": "5",
        "cover_date": "2023-04-01",
        "Abstract": "Columnist Jonathan E. Collins interviews his colleague, Professor Suresh Venkatasubramanian, a professor of data science and computer science at Brown University. He is one of the leading experts on the development of guardrails for engaging with automated programs safely. The two discuss how artificial intelligence and ChatGPT could affect education and how school leaders may react to the new tools.",
        "DOI": "10.1177/00317217231168266",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Technological innovations enhance invasive species management in the anthropocene",
        "paper_author": "Fricke R.M.",
        "publication": "BioScience",
        "citied_by": "9",
        "cover_date": "2023-04-01",
        "Abstract": "Curbing the introduction, spread, and impact of invasive species remains a longstanding management and policy prerogative. In recent decades, globalization and environmental change have further complicated efforts to execute science-based actions that address these challenges. New technologies offer exciting opportunities to advance invasion science knowledge, enhance management actions, and guide policy strategies but are increasingly complex and inaccessible to most practitioners. In the present article, we offer a synthetic perspective of innovative technologies with applications for invasive species management related to pathway intervention, spread prevention, impact mitigation, and public engagement. We also describe tools that augment big data processing required by some methods (e.g., remote sensing, mobile application data), such as automated image and text recognition built on machine learning. Finally, we explore challenges and opportunities for successful integration of emerging technologies into invasive species management, focusing on pipelines that enable practitioners to integrate tools into practice while recognizing logistic and financial constraints.",
        "DOI": "10.1093/biosci/biad018",
        "affiliation_name": "University of Washington",
        "affiliation_city": "Seattle",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "VALUE-GRADIENT BASED FORMULATION OF OPTIMAL CONTROL PROBLEM AND MACHINE LEARNING ALGORITHM",
        "paper_author": "Bensoussan A.",
        "publication": "SIAM Journal on Numerical Analysis",
        "citied_by": "3",
        "cover_date": "2023-04-01",
        "Abstract": "Optimal control problem is typically solved by first finding the value function through the Hamilton-Jacobi equation (HJE) and then taking the minimizer of the Hamiltonian to obtain the control. In this work, instead of focusing on the value function, we propose a new formulation for the gradient of the value function (value-gradient) as a decoupled system of partial differential equations in the context of a continuous-time deterministic discounted optimal control problem. We develop an efficient iterative scheme for this system of equations in parallel by utilizing the fact that they share the same characteristic curves as the HJE for the value function. For the theoretical part, we prove that this iterative scheme converges linearly in L2α sense for some suitable exponent α in a weight function. For the numerical method, we combine a characteristic line method with machine learning techniques. Specifically, we generate multiple characteristic curves at each policy iteration from an ensemble of initial states and compute both the value function and its gradient simultaneously on each curve as the labeled data. Then supervised machine learning is applied to minimize the weighted squared loss for both the value function and its gradients. Experimental results demonstrate that this new method not only significantly increases the accuracy but also improves the efficiency and robustness of the numerical estimates, particularly with less characteristics data or fewer training steps.",
        "DOI": "10.1137/21M1442838",
        "affiliation_name": "The Naveen Jindal School of Management",
        "affiliation_city": "Richardson",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Intelligent proximal-policy-optimization-based decision-making system for humanoid robots",
        "paper_author": "Kuo P.H.",
        "publication": "Advanced Engineering Informatics",
        "citied_by": "5",
        "cover_date": "2023-04-01",
        "Abstract": "With the advancements in technology, robots have gradually replaced humans in different aspects. Allowing robots to handle multiple situations simultaneously and perform different actions depending on the situation has since become a critical topic. Currently, training a robot to perform a designated action is considered an easy task. However, when a robot is required to perform actions in different environments, both resetting and retraining are required, which are time-consuming and inefficient. Therefore, allowing robots to autonomously identify their environment can significantly reduce the time consumed. How to employ machine learning algorithms to achieve autonomous robot learning has formed a research trend in current studies. In this study, to solve the aforementioned problem, a proximal policy optimization algorithm was used to allow a robot to conduct self-training and select an optimal gait pattern to reach its destination successfully. Multiple basic gait patterns were selected, and information-maximizing generative adversarial nets were used to generate gait patterns and allow the robot to choose from numerous gait patterns while walking. The experimental results indicated that, after self-learning, the robot successfully made different choices depending on the situation, verifying this approach's feasibility.",
        "DOI": "10.1016/j.aei.2023.102009",
        "affiliation_name": "National Pingtung University",
        "affiliation_city": "Pingtung",
        "affiliation_country": "Taiwan"
    },
    {
        "paper_title": "Carbon Monoxide Oxidation on the Surface of Palladium Nanoparticles Optimized by Reinforcement Learning",
        "paper_author": "Lifar M.S.",
        "publication": "Journal of Surface Investigation",
        "citied_by": "0",
        "cover_date": "2023-04-01",
        "Abstract": "Abstract: The yield of reaction products depends on the interaction between processes on the catalyst surface: adsorption, activation, reaction, desorption, and others. These processes, in turn, depend on the magnitude of the flows of reaction mixtures, temperature, and pressure. Under stationary conditions, active sites on the surface can be poisoned by reaction by-products or blocked by an excess of adsorbed reactant molecules. Dynamic control of reaction parameters takes into account changes in surface properties and adjusts the temperature, flow rates, and other parameters accordingly. A reinforcement learning algorithm was applied to control the oxidation reaction of carbon monoxide CO on the surface of palladium nanoparticles. The algorithm was trained to maximize the rate of carbon dioxide production based on information about the magnitude of CO, O2, and CO2 flows at each time step. A gradient policy algorithm with a continuous action space was chosen, and observations of the flow rates were extended over several successive time steps, which made it possible to obtain a set of non-stationary solutions. The maximum yield of the product is achieved with a periodic change in gas flows, which ensures a balance between the available adsorption sites and the concentration of activated intermediates. This methodology opens up prospects for optimizing catalytic reactions under nonstationary conditions.",
        "DOI": "10.1134/S1027451023020088",
        "affiliation_name": "Southern Federal University",
        "affiliation_city": "Rostov-on-Don",
        "affiliation_country": "Russian Federation"
    },
    {
        "paper_title": "Land Cover Mapping with Convolutional Neural Networks Using Sentinel-2 Images: Case Study of Rome",
        "paper_author": "Cecili G.",
        "publication": "Land",
        "citied_by": "16",
        "cover_date": "2023-04-01",
        "Abstract": "Land cover monitoring is crucial to understand land transformations at a global, regional and local level, and the development of innovative methodologies is necessary in order to define appropriate policies and land management practices. Deep learning techniques have recently been demonstrated as a useful method for land cover mapping through the classification of remote sensing imagery. This research aims to test and compare the predictive models created using the convolutional neural networks (CNNs) VGG16, DenseNet121 and ResNet50 on multitemporal and single-date Sentinel-2 satellite data. The most promising model was the VGG16 both with single-date and multi-temporal images, which reach an overall accuracy of 71% and which was used to produce an automatically generated EAGLE-compliant land cover map of Rome for 2019. The methodology is part of the land mapping activities of ISPRA and exploits its main products as input and support data. In this sense, it is a first attempt to develop a high-update-frequency land cover classification tool for dynamic areas to be integrated in the framework of the ISPRA monitoring activities for the Italian territory.",
        "DOI": "10.3390/land12040879",
        "affiliation_name": "Italian Institute for Environmental Protection and Research",
        "affiliation_city": "Rome",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Big-Data-Driven Machine Learning for Enhancing Spatiotemporal Air Pollution Pattern Analysis",
        "paper_author": "Zareba M.",
        "publication": "Atmosphere",
        "citied_by": "18",
        "cover_date": "2023-04-01",
        "Abstract": "Air pollution is an important problem for public health. The spatiotemporal analysis is a crucial step for understanding the complex characteristics of air pollution. Using many sensors and high-resolution time-step observations makes this task a big data challenge. In this study, unsupervised machine learning algorithms were applied to analyze spatiotemporal patterns of air pollution. The analysis was conducted using PM10 big data collected from almost 100 sensors located in Krakow, over a period of one year, with data being recorded at 1-h intervals. The analysis results using K-means and SKATER clustering revealed distinct differences between average and maximum values of pollutant concentrations. The study found that the K-means algorithm with Dynamic Time Warping (DTW) was more accurate in identifying yearly patterns and clustering in rapidly and spatially varying data, compared to the SKATER algorithm. Moreover, the clustering analysis of data after kriging greatly facilitated the interpretation of the results. These findings highlight the potential of machine learning techniques and big data analysis for identifying hot-spots, cold-spots, and patterns of air pollution and informing policy decisions related to urban planning, traffic management, and public health interventions.",
        "DOI": "10.3390/atmos14040760",
        "affiliation_name": "AGH University of Krakow",
        "affiliation_city": "Krakow",
        "affiliation_country": "Poland"
    },
    {
        "paper_title": "Efficient Deep Semantic Segmentation for Land Cover Classification Using Sentinel Imagery",
        "paper_author": "Tzepkenlis A.",
        "publication": "Remote Sensing",
        "citied_by": "22",
        "cover_date": "2023-04-01",
        "Abstract": "Nowadays, different machine learning approaches, either conventional or more advanced, use input from different remote sensing imagery for land cover classification and associated decision making. However, most approaches rely heavily on time-consuming tasks to gather accurate annotation data. Furthermore, downloading and pre-processing remote sensing imagery used to be a difficult and time-consuming task that discouraged policy makers to create and use new land cover maps. We argue that by combining recent improvements in deep learning with the use of powerful cloud computing platforms for EO data processing, specifically the Google Earth Engine, we can greatly facilitate the task of land cover classification. For this reason, we modify an efficient semantic segmentation approach (U-TAE) for a satellite image time series to use, as input, a single multiband image composite corresponding to a specific time range. Our motivation is threefold: (a) to improve land cover classification performance and at the same time reduce complexity by using, as input, satellite image composites with reduced noise created using temporal median instead of the original noisy (due to clouds, calibration errors, etc.) images, (b) to assess performance when using as input different combinations of satellite data, including Sentinel-2, Sentinel-1, spectral indices, and ALOS elevation data, and (c) to exploit channel attention instead of the temporal attention used in the original approach. We show that our proposed modification on U-TAE (mIoU: 57.25%) outperforms three other popular approaches, namely random forest (mIoU: 39.69%), U-Net (mIoU: 55.73%), and SegFormer (mIoU: 53.5%), while also using fewer training parameters. In addition, the evaluation reveals that proper selection of the input band combination is necessary for improved performance.",
        "DOI": "10.3390/rs15082027",
        "affiliation_name": "Centre for Research and Technology-Hellas",
        "affiliation_city": "Thessaloniki",
        "affiliation_country": "Greece"
    },
    {
        "paper_title": "Understanding and Predicting Ride-Hailing Fares in Madrid: A Combination of Supervised and Unsupervised Techniques",
        "paper_author": "Silveira-Santos T.",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "2",
        "cover_date": "2023-04-01",
        "Abstract": "App-based ride-hailing mobility services are becoming increasingly popular in cities worldwide. However, key drivers explaining the balance between supply and demand to set final prices remain to a considerable extent unknown. This research intends to understand and predict the behavior of ride-hailing fares by employing statistical and supervised machine learning approaches (such as Linear Regression, Decision Tree, and Random Forest). The data used for model calibration correspond to a ten-month period and were downloaded from the Uber Application Programming Interface for the city of Madrid. The findings reveal that the Random Forest model is the most appropriate for this type of prediction, having the best performance metrics. To further understand the patterns of the prediction errors, the unsupervised technique of cluster analysis (using the k-means clustering method) was applied to explore the variation of the discrepancy between Uber fares predictions and observed values. The analysis identified a small share of observations with high prediction errors (only 1.96%), which are caused by unexpected surges due to imbalances between supply and demand (usually occurring at major events, peak times, weekends, holidays, or when there is a taxi strike). This study helps policymakers understand pricing, demand for services, and pricing schemes in the ride-hailing market.",
        "DOI": "10.3390/app13085147",
        "affiliation_name": "Universidad Politécnica de Madrid",
        "affiliation_city": "Madrid",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "Assessing Forest Biodiversity: A Novel Index to Consider Ecosystem, Species, and Genetic Diversity",
        "paper_author": "Ette J.S.",
        "publication": "Forests",
        "citied_by": "10",
        "cover_date": "2023-04-01",
        "Abstract": "Rates of biodiversity loss remain high, threatening the life support system upon which all human life depends. In a case study, a novel biodiversity composite index (BCI) in line with the Convention on Biological Diversity is established in Tyrol, Austria, based on available national forest inventory and forest typing data. Indicators are referenced by ecological modeling, protected areas, and unmanaged forests using a machine learning approach. Our case study displays an average biodiversity rating of 57% out of 100% for Tyrolean forests. The respective rating for ecosystem diversity is 49%; for genetic diversity, 53%; and for species diversity, 71%. Coniferous forest types are in a more favorable state of preservation than deciduous and mixed forests. The BCI approach is transferable to Central European areas with forest typing. Our objective is to support the conservation of biodiversity and provide guidance to regional forest policy. BCI is useful to set restoration priorities, reach conservation targets, raise effectiveness of financial resources spent on biodiversity conservation, and enhance Sustainable Forest Management.",
        "DOI": "10.3390/f14040709",
        "affiliation_name": "Bundesforschungszentrum für Wald",
        "affiliation_city": "Vienna",
        "affiliation_country": "Austria"
    },
    {
        "paper_title": "Synergistic Use of Earth Observation Driven Techniques to Support the Implementation of Water Framework Directive in Europe: A Review",
        "paper_author": "Samarinas N.",
        "publication": "Remote Sensing",
        "citied_by": "11",
        "cover_date": "2023-04-01",
        "Abstract": "The development of a sustainable water quality monitoring system at national scale remains a big challenge until today, acting as a hindrance for the efficient implementation of the Water Framework Directive (WFD). This work provides valuable insights into the current state-of-the-art Earth Observation (EO) tools and services, proposing a synergistic use of innovative remote sensing technologies, in situ sensors, and databases, with the ultimate goal to support the European Member States in effective WFD implementation. The proposed approach is based on a recent research and scientific analysis for a six-year period (2017–2022) after reviewing 71 peer-reviewed articles in international journals coupled with the scientific results of 11 European-founded research projects related to EO and WFD. Special focus is placed on the EO data sources (spaceborne, in situ, etc.), the sensors in use, the observed water Quality Elements as well as on the computer science techniques (machine/deep learning, artificial intelligence, etc.). The combination of the different technologies can offer, among other things, low-cost monitoring, an increase in the monitored Quality Elements per water body, and a minimization of the percentage of water bodies with unknown ecological status.",
        "DOI": "10.3390/rs15081983",
        "affiliation_name": "School of Agriculture",
        "affiliation_city": "Thessaloniki",
        "affiliation_country": "Greece"
    },
    {
        "paper_title": "AI and Expert Insights for Sustainable Energy Future",
        "paper_author": "Danish M.S.S.",
        "publication": "Energies",
        "citied_by": "8",
        "cover_date": "2023-04-01",
        "Abstract": "This study presents an innovative framework for leveraging the potential of AI in energy systems through a multidimensional approach. Despite the increasing importance of sustainable energy systems in addressing global climate change, comprehensive frameworks for effectively integrating artificial intelligence (AI) and machine learning (ML) techniques into these systems are lacking. The challenge is to develop an innovative, multidimensional approach that evaluates the feasibility of integrating AI and ML into the energy landscape, to identify the most promising AI and ML techniques for energy systems, and to provide actionable insights for performance enhancements while remaining accessible to a varied audience across disciplines. This study also covers the domains where AI can augment contemporary and future energy systems. It also offers a novel framework without echoing established literature by employing a flexible and multicriteria methodology to rank energy systems based on their AI integration prospects. The research also delineates AI integration processes and technique categorizations for energy systems. The findings provide insight into attainable performance enhancements through AI integration and underscore the most promising AI and ML techniques for energy systems via a pioneering framework. This interdisciplinary research connects AI applications in energy and addresses a varied audience through an accessible methodology.",
        "DOI": "10.3390/en16083309",
        "affiliation_name": "Nagoya University",
        "affiliation_city": "Nagoya",
        "affiliation_country": "Japan"
    },
    {
        "paper_title": "Efficient Uncertainty Propagation in Model-Based Reinforcement Learning Unmanned Surface Vehicle Using Unscented Kalman Filter",
        "paper_author": "Wang J.",
        "publication": "Drones",
        "citied_by": "4",
        "cover_date": "2023-04-01",
        "Abstract": "This article tackles the computational burden of propagating uncertainties in the model predictive controller-based policy of the probabilistic model-based reinforcement learning (MBRL) system for an unmanned surface vehicles system (USV). We proposed filtered probabilistic model predictive control using the unscented Kalman filter (FPMPC-UKF) that introduces the unscented Kalman filter (UKF) for a more efficient uncertainty propagation in MBRL. A USV control system based on FPMPC-UKF is developed and evaluated by position-keeping and target-reaching tasks in a real USV data-driven simulation. The experimental results demonstrate a significant superiority of the proposed method in balancing the control performance and computational burdens under different levels of disturbances compared with the related works of USV, and therefore indicate its potential in more challenging USV scenarios with limited computational resources.",
        "DOI": "10.3390/drones7040228",
        "affiliation_name": "Southern University of Science and Technology",
        "affiliation_city": "Shenzhen",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Drone Elevation Control Based on Python-Unity Integrated Framework for Reinforcement Learning Applications",
        "paper_author": "Abbass M.A.B.",
        "publication": "Drones",
        "citied_by": "12",
        "cover_date": "2023-04-01",
        "Abstract": "Reinforcement learning (RL) applications require a huge effort to become established in real-world environments, due to the injury and break down risks during interactions between the RL agent and the environment, in the online training process. In addition, the RL platform tools (e.g., Python OpenAI’s Gym, Unity ML-Agents, PyBullet, DART, MoJoCo, RaiSim, Isaac, and AirSim), that are required to reduce the real-world challenges, suffer from drawbacks (e.g., the limited number of examples and applications, and difficulties in implementation of the RL algorithms, due to difficulties with the programing language). This paper presents an integrated RL framework, based on Python–Unity interaction, to demonstrate the ability to create a new RL platform tool, based on making a stable user datagram protocol (UDP) communication between the RL agent algorithm (developed using the Python programing language as a server), and the simulation environment (created using the Unity simulation software as a client). This Python–Unity integration process, increases the advantage of the overall RL platform (i.e., flexibility, scalability, and robustness), with the ability to create different environment specifications. The challenge of RL algorithms’ implementation and development is also achieved. The proposed framework is validated by applying two popular deep RL algorithms (i.e., Vanilla Policy Gradient (VPG) and Actor-Critic (A2C)), on an elevation control challenge for a quadcopter drone. The validation results for these experimental tests, prove the innovation of the proposed framework, to be used in RL applications, because both implemented algorithms achieve high stability, by achieving convergence to the required performance through the semi-online training process.",
        "DOI": "10.3390/drones7040225",
        "affiliation_name": "Faculty of Engineering Mataria",
        "affiliation_city": "Helwan",
        "affiliation_country": "Egypt"
    },
    {
        "paper_title": "COVID-19 Vaccine Refusal and Delay among Adults in Italy: Evidence from the OBVIOUS Project, a National Survey in Italy",
        "paper_author": "Gori D.",
        "publication": "Vaccines",
        "citied_by": "17",
        "cover_date": "2023-04-01",
        "Abstract": "Background: Vaccine hesitancy was defined by the World Health Organization (WHO) in 2019 as a major threat to global health. In Italy, reluctance to receive vaccines is a widespread phenomenon that was amplified during the COVID-19 pandemic by fear and mistrust in government. This study aims to depict different profiles and characteristics of people reluctant to vaccinate, focusing on the drivers of those who are in favor of and those who are opposed to receiving the COVID-19 vaccine. Methods: A sample of 10,000 Italian residents was collected. A survey on COVID-19 vaccination behavior and possible determinants of vaccine uptake, delay, and refusal was administered to participants through a computer-assisted web interviewing method. Results: In our sample, 83.2% stated that they were vaccinated as soon as possible (“vaccinators”), 8.0% delayed vaccination (“delayers”), and 6.7% refused to be vaccinated (“no-vaccinators”). In general, the results show that being female, aged between 25 and 64, with an education level less than a high school diploma or above a master’s degree, and coming from a rural area were characteristics significantly associated with delaying or refusing COVID-19 vaccination. In addition, it was found that having minimal trust in science and/or government (i.e., 1 or 2 points on a scale from 1 to 10), using alternative medicine as the main source of treatment, and intention to vote for certain parties were characteristics associated with profiles of “delayers” or “no-vaccinators”. Finally, the main reported motivation for delaying or not accepting vaccination was fear of vaccine side effects (55.0% among delayers, 55.6% among no-vaccinators). Conclusion: In this study, three main profiles of those who chose to be vaccinated are described. Since those who are in favor of vaccines and those who are not usually cluster in similar sociodemographic categories, we argue that findings from this study might be useful to policy makers when shaping vaccine strategies and choosing policy instruments.",
        "DOI": "10.3390/vaccines11040839",
        "affiliation_name": "Institute for Health Metrics and Evaluation",
        "affiliation_city": "Seattle",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "A Penetration Method for UAV Based on Distributed Reinforcement Learning and Demonstrations",
        "paper_author": "Li K.",
        "publication": "Drones",
        "citied_by": "3",
        "cover_date": "2023-04-01",
        "Abstract": "The penetration of unmanned aerial vehicles (UAVs) is an essential and important link in modern warfare. Enhancing UAV’s ability of autonomous penetration through machine learning has become a research hotspot. However, the current generation of autonomous penetration strategies for UAVs faces the problem of excessive sample demand. To reduce the sample demand, this paper proposes a combination policy learning (CPL) algorithm that combines distributed reinforcement learning and demonstrations. Innovatively, the action of the CPL algorithm is jointly determined by the initial policy obtained from demonstrations and the target policy in the asynchronous advantage actor-critic network, thus retaining the guiding role of demonstrations in the initial training. In a complex and unknown dynamic environment, 1000 training experiments and 500 test experiments were conducted for the CPL algorithm and related baseline algorithms. The results show that the CPL algorithm has the smallest sample demand, the highest convergence efficiency, and the highest success rate of penetration among all the algorithms, and has strong robustness in dynamic environments.",
        "DOI": "10.3390/drones7040232",
        "affiliation_name": "Beijing Institute of Technology",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Human-Centric Digital Twins in Industry: A Comprehensive Review of Enabling Technologies and Implementation Strategies",
        "paper_author": "Asad U.",
        "publication": "Sensors",
        "citied_by": "57",
        "cover_date": "2023-04-01",
        "Abstract": "The last decade saw the emergence of highly autonomous, flexible, re-configurable Cyber-Physical Systems. Research in this domain has been enhanced by the use of high-fidelity simulations, including Digital Twins, which are virtual representations connected to real assets. Digital Twins have been used for process supervision, prediction, or interaction with physical assets. Interaction with Digital Twins is enhanced by Virtual Reality and Augmented Reality, and Industry 5.0-focused research is evolving with the involvement of the human aspect in Digital Twins. This paper aims to review recent research on Human-Centric Digital Twins (HCDTs) and their enabling technologies. A systematic literature review is performed using the VOSviewer keyword mapping technique. Current technologies such as motion sensors, biological sensors, computational intelligence, simulation, and visualization tools are studied for the development of HCDTs in promising application areas. Domain-specific frameworks and guidelines are formed for different HCDT applications that highlight the workflow and desired outcomes, such as the training of AI models, the optimization of ergonomics, the security policy, task allocation, etc. A guideline and comparative analysis for the effective development of HCDTs are created based on the criteria of Machine Learning requirements, sensors, interfaces, and Human Digital Twin inputs.",
        "DOI": "10.3390/s23083938",
        "affiliation_name": "National University of Sciences and Technology",
        "affiliation_city": "Islamabad",
        "affiliation_country": "Pakistan"
    },
    {
        "paper_title": "Recognition of Hand Gestures Based on EMG Signals with Deep and Double-Deep Q-Networks",
        "paper_author": "Valdivieso Caraguay Á.L.",
        "publication": "Sensors",
        "citied_by": "9",
        "cover_date": "2023-04-01",
        "Abstract": "In recent years, hand gesture recognition (HGR) technologies that use electromyography (EMG) signals have been of considerable interest in developing human–machine interfaces. Most state-of-the-art HGR approaches are based mainly on supervised machine learning (ML). However, the use of reinforcement learning (RL) techniques to classify EMGs is still a new and open research topic. Methods based on RL have some advantages such as promising classification performance and online learning from the user’s experience. In this work, we propose a user-specific HGR system based on an RL-based agent that learns to characterize EMG signals from five different hand gestures using Deep Q-network (DQN) and Double-Deep Q-Network (Double-DQN) algorithms. Both methods use a feed-forward artificial neural network (ANN) for the representation of the agent policy. We also performed additional tests by adding a long–short-term memory (LSTM) layer to the ANN to analyze and compare its performance. We performed experiments using training, validation, and test sets from our public dataset, EMG-EPN-612. The final accuracy results demonstrate that the best model was DQN without LSTM, obtaining classification and recognition accuracies of up to (Formula presented.) and (Formula presented.), respectively. The results obtained in this work demonstrate that RL methods such as DQN and Double-DQN can obtain promising results for classification and recognition problems based on EMG signals.",
        "DOI": "10.3390/s23083905",
        "affiliation_name": "Escuela Politécnica Nacional",
        "affiliation_city": "Quito",
        "affiliation_country": "Ecuador"
    },
    {
        "paper_title": "A Review of Reinforcement Learning-Based Powertrain Controllers: Effects of Agent Selection for Mixed-Continuity Control and Reward Formulation",
        "paper_author": "Egan D.",
        "publication": "Energies",
        "citied_by": "4",
        "cover_date": "2023-04-01",
        "Abstract": "One major cost of improving the automotive fuel economy while simultaneously reducing tailpipe emissions is increased powertrain complexity. This complexity has consequently increased the resources (both time and money) needed to develop such powertrains. Powertrain performance is heavily influenced by the quality of the controller/calibration. Since traditional control development processes are becoming resource-intensive, better alternate methods are worth pursuing. Recently, reinforcement learning (RL), a machine learning technique, has proven capable of creating optimal controllers for complex systems. The model-free nature of RL has the potential to streamline the control development process, possibly reducing the time and money required. This article reviews the impact of choices in two areas on the performance of RL-based powertrain controllers to provide a better awareness of their benefits and consequences. First, we examine how RL algorithm action continuities and control–actuator continuities are matched, via native operation or conversion. Secondly, we discuss the formulation of the reward function. RL is able to optimize control policies defined by a wide spectrum of reward functions, including some functions that are difficult to implement with other techniques. RL action and control–actuator continuity matching affects the ability of the RL-based controller to understand and operate the powertrain while the reward function defines optimal behavior. Finally, opportunities for future RL-based powertrain control development are identified and discussed.",
        "DOI": "10.3390/en16083450",
        "affiliation_name": "Clemson University College of Engineering, Computing and Applied Sciences",
        "affiliation_city": "Clemson",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Do We Need Another CT Scanner?—The Pilot Study of the Adoption of an Evolutionary Algorithm to Investment Decision Making in Healthcare",
        "paper_author": "Kolasa K.",
        "publication": "Tomography",
        "citied_by": "0",
        "cover_date": "2023-04-01",
        "Abstract": "Objectives: The purpose of this study was to assess the feasibility of the adoption of a machine learning (ML) algorithm in support of the investment decisions regarding high cost medical devices based on available clinical and epidemiological evidence. Methods: Following a literature search, the set of epidemiological and clinical need predictors was established. Both the data from The Central Statistical Office and The National Health Fund were used. An evolutionary algorithm (EA) model was developed to obtain the prediction of the need for CT scanners across local counties in Poland (hypothetical scenario). The comparison between the historical allocation and the scenario developed by the EA model based on epidemiological and clinical need predictors was established. Only counties with available CT scanners were included in the study. Results: In total, over 4 million CT scan procedures performed across 130 counties in Poland between 2015 and 2019 were used to develop the EA model. There were 39 cases of agreement between historical data and hypothetical scenarios. In 58 cases, the EA model indicated the need for a lower number of CT scanners than the historical data. A greater number of CT procedures required compared with historical use was predicted for 22 counties. The remaining 11 cases were inconclusive. Conclusions: Machine learning techniques might be successfully applied to support the optimal allocation of limited healthcare resources. Firstly, they enable automatization of health policy making utilising historical, epidemiological, and clinical data. Secondly, they introduce flexibility and transparency thanks to the adoption of ML to investment decisions in the healthcare sector as well.",
        "DOI": "10.3390/tomography9020063",
        "affiliation_name": "Ministerstwo Zdrowia, Poland",
        "affiliation_city": "Warsaw",
        "affiliation_country": "Poland"
    },
    {
        "paper_title": "How to control hydrodynamic force on fluidic pinball via deep reinforcement learning",
        "paper_author": "Feng H.",
        "publication": "Physics of Fluids",
        "citied_by": "3",
        "cover_date": "2023-04-01",
        "Abstract": "Deep reinforcement learning (DRL) for fluidic pinball, three individually rotating cylinders in the uniform flow arranged in an equilaterally triangular configuration, can learn the efficient flow control strategies due to the validity of self-learning and data-driven state estimation for complex fluid dynamic problems. In this work, we present a DRL-based real-time feedback strategy to control the hydrodynamic force on fluidic pinball, i.e., force extremum and tracking, from cylinders' rotation. By adequately designing reward functions and encoding historical observations, and after automatic learning of thousands of iterations, the DRL-based control was shown to make reasonable and valid control decisions in nonparametric control parameter space, which is comparable to and even better than the optimal policy found through lengthy brute-force searching. Subsequently, one of these results was analyzed by a machine learning model that enabled us to shed light on the basis of decision-making and physical mechanisms of the force tracking process. The finding from this work can control hydrodynamic force on the operation of fluidic pinball system and potentially pave the way for exploring efficient active flow control strategies in other complex fluid dynamic problems.",
        "DOI": "10.1063/5.0142949",
        "affiliation_name": "Westlake University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Detecting Remote Access Network Attacks Using Supervised Machine Learning Methods",
        "paper_author": "Ndichu S.",
        "publication": "International Journal of Computer Network and Information Security",
        "citied_by": "2",
        "cover_date": "2023-04-01",
        "Abstract": "Remote access technologies encrypt data to enforce policies and ensure protection. Attackers leverage such techniques to launch carefully crafted evasion attacks introducing malware and other unwanted traffic to the internal network. Traditional security controls such as anti-virus software, firewall, and intrusion detection systems (IDS) decrypt network traffic and employ signature and heuristic-based approaches for malware inspection. In the past, machine learning (ML) approaches have been proposed for specific malware detection and traffic type characterization. However, decryption introduces computational overheads and dilutes the privacy goal of encryption. The ML approaches employ limited features and are not objectively developed for remote access security. This paper presents a novel ML-based approach to encrypted remote access attack detection using a weighted random forest (W-RF) algorithm. Key features are determined using feature importance scores. Class weighing is used to address the imbalanced data distribution problem common in remote access network traffic where attacks comprise only a small proportion of network traffic. Results obtained during the evaluation of the approach on benign virtual private network (VPN) and attack network traffic datasets that comprise verified normal hosts and common attacks in real-world network traffic are presented. With recall and precision of 100%, the approach demonstrates effective performance. The results for k-fold cross-validation and receiver operating characteristic (ROC) mean area under the curve (AUC) demonstrate that the approach effectively detects attacks in encrypted remote access network traffic, successfully averting attackers and network intrusions.",
        "DOI": "10.5815/ijcnis.2023.02.04",
        "affiliation_name": "University of Eldoret",
        "affiliation_city": "Eldoret",
        "affiliation_country": "Kenya"
    },
    {
        "paper_title": "Comparing Global Sentinel-2 Land Cover Maps for Regional Species Distribution Modeling",
        "paper_author": "Venter Z.S.",
        "publication": "Remote Sensing",
        "citied_by": "2",
        "cover_date": "2023-04-01",
        "Abstract": "Mapping the spatial and temporal dynamics of species distributions is necessary for biodiversity conservation land-use planning decisions. Recent advances in remote sensing and machine learning have allowed for high-resolution species distribution modeling that can inform landscape-level decision-making. Here we compare the performance of three popular Sentinel-2 (10-m) land cover maps, including dynamic world (DW), European land cover (ELC10), and world cover (WC), in predicting wild bee species richness over southern Norway. The proportion of grassland habitat within 250 m (derived from the land cover maps), along with temperature and distance to sandy soils, were used as predictors in both Bayesian regularized neural network and random forest models. Models using grassland habitat from DW performed best (RMSE = 2.8 ± 0.03; average ± standard deviation across models), followed by ELC10 (RMSE = 2.85 ± 0.03) and WC (RMSE = 2.87 ± 0.02). All satellite-derived maps outperformed a manually mapped Norwegian land cover dataset called AR5 (RMSE = 3.02 ± 0.02). When validating the model predictions of bee species richness against citizen science data on solitary bee occurrences using generalized linear models, we found that ELC10 performed best (AIC = 2278 ± 4), followed by WC (AIC = 2367 ± 3), and DW (AIC = 2376 ± 3). While the differences in RMSE we observed between models were small, they may be significant when such models are used to prioritize grassland patches within a landscape for conservation subsidies or management policies. Partial dependencies in our models showed that increasing the proportion of grassland habitat is positively associated with wild bee species richness, thereby justifying bee conservation schemes that aim to enhance semi-natural grassland habitat. Our results confirm the utility of satellite-derived land cover maps in supporting high-resolution species distribution modeling and suggest there is scope to monitor changes in species distributions over time given the dense time series provided by products such as DW.",
        "DOI": "10.3390/rs15071749",
        "affiliation_name": "Nord Universitet",
        "affiliation_city": "Bodo",
        "affiliation_country": "Norway"
    },
    {
        "paper_title": "Frailty in Heart Failure: It's Time to Intervene",
        "paper_author": "McDonagh J.",
        "publication": "Heart Lung and Circulation",
        "citied_by": "1",
        "cover_date": "2023-04-01",
        "Abstract": "NA",
        "DOI": "10.1016/j.hlc.2023.03.003",
        "affiliation_name": "Faculty of Science, Medicine and Health",
        "affiliation_city": "Wollongong",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Machine-Learning-Based Optimization for Multiple-IRS-Aided Communication System",
        "paper_author": "Fathy M.",
        "publication": "Electronics (Switzerland)",
        "citied_by": "3",
        "cover_date": "2023-04-01",
        "Abstract": "Due to the benefits of the spectrum and energy efficiency, intelligent reflecting surfaces (IRSs) are regarded as a promising technology for future networks. In this work, we consider a single cellular network where multiple IRSs are deployed to assist the downlink transmissions from the base station (BS) to multiple user equipment (UE). Hence, we aim to jointly optimize the configuration of the BS active beamforming and reflection beamforming of the IRSs that meet the UE’s QoS while allowing the lowest transmit power consumption at the BS. Although the conventional alternating approach is widely used to find converged solutions, its applicability is restricted by high complexity, which is more severe in a dynamic environment. Consequently, an alternative approach, i.e., machine learning (ML), is adopted to find the optimal solution with lower complexity. For the static UE scenario, we propose a low-complexity optimization algorithm based on the new generalized neural network (GRNN). Meanwhile, for the dynamic UE scenario, we propose a deep reinforcement learning (DRL)-based optimization algorithm. Specifically, a deep deterministic policy gradient (DDPG)-based algorithm is designed to address the GRNN algorithm’s restrictions and efficiently handle the dynamic UE scenario. Simulation results confirm that the proposed algorithms can achieve better power-saving performance and convergence with a noteworthy reduction in the computation time compared to the alternating optimization-based approaches. In addition, our results show that the total transmit power at the BS decreases with the increasing number of reflecting units at the IRSs.",
        "DOI": "10.3390/electronics12071703",
        "affiliation_name": "Beijing Institute of Technology",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Tackling Age of Information in Access Policies for Sensing Ecosystems †",
        "paper_author": "Zancanaro A.",
        "publication": "Sensors",
        "citied_by": "3",
        "cover_date": "2023-04-01",
        "Abstract": "Recent technological advancements such as the Internet of Things (IoT) and machine learning (ML) can lead to a massive data generation in smart environments, where multiple sensors can be used to monitor a large number of processes through a wireless sensor network (WSN). This poses new challenges for the extraction and interpretation of meaningful data. In this spirit, age of information (AoI) represents an important metric to quantify the freshness of the data monitored to check for anomalies and operate adaptive control. However, AoI typically assumes a binary representation of the information, which is actually multi-structured. Thus, deep semantic aspects may be lost. In addition, the ambient correlation of multiple sensors may not be taken into account and exploited. To analyze these issues, we study how correlation affects AoI for multiple sensors under two scenarios of (i) concurrent and (ii) time-division multiple access. We show that correlation among sensors improves AoI if concurrent transmissions are allowed, whereas the benefits are much more limited in a time-division scenario. Furthermore, we discuss how ML can be applied to extract relevant information from data and show how it can further optimize the transmission policy with savings of resources. Specifically, we demonstrate, through simulations, that ML techniques can be used to reduce the number of transmissions and that classification errors have no influence on the AoI of the system.",
        "DOI": "10.3390/s23073456",
        "affiliation_name": "Università degli Studi di Milano-Bicocca",
        "affiliation_city": "Milan",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Landslide Susceptibility Mapping and Driving Mechanisms in a Vulnerable Region Based on Multiple Machine Learning Models",
        "paper_author": "Yu H.",
        "publication": "Remote Sensing",
        "citied_by": "22",
        "cover_date": "2023-04-01",
        "Abstract": "Landslides can cause severe damage to both the environment and society, and many statistical, index-based, and inventory-based methods have been developed to assess landslide susceptibility; however, it is still challenging to choose the most effective method and properly identify major driving factors for specific regions. Here, we applied four machine learning algorithms, adaptive boosting (AdaBoost), gradient-boosting decision tree (GBDT), multilayer perceptron (MLP), and random forest (RF), to predict the landslide susceptibility at 30 m spatial scale based on thirteen landslide conditioning factors (LCFs) in a landslide-vulnerable region. Based on inventory landslide points, the classification results were evaluated, and indicated that the performance of the RF (F1-score: 0.85, AUC: 0.92), AdaBoost (F1-score: 0.83, AUC: 0.91), and GBDT (F1-score: 0.83, AUC: 0.88) methods were significantly better than the MLP (F1-score: 0.76, AUC: 0.79) method. The results further indicated that the areas with high and very high landslide risk (susceptibility greater than 0.5) accounted for about 40% of the study region. All four models matched well and predicted similar spatial distribution patterns in landslide susceptibility, with the very high risk areas mostly distributed in the western and southeastern regions. Daoshi, Qingliangfeng, Jinnan, and Linglong towns have the highest landslide risk, with mean susceptibility levels greater than 0.5. The leading contributing factors to landslide susceptibility were slightly different for the four models; however, population density, distance to road, and relief amplitude were generally among the top leading factors for most towns. Our study provided significant information on the highly landslide-prone areas and the major contributing factors for decision-makers and policy planners, and suggested that different areas should take unique precautions to mitigate or avoid severe damage from landslide events.",
        "DOI": "10.3390/rs15071886",
        "affiliation_name": "Zhejiang Agriculture and Forestry University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Mapping the patent landscape of medical machine learning",
        "paper_author": "Aboy M.",
        "publication": "Nature Biotechnology",
        "citied_by": "5",
        "cover_date": "2023-04-01",
        "Abstract": "NA",
        "DOI": "10.1038/s41587-023-01735-6",
        "affiliation_name": "Faculty of Law",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "A Survey of Sentiment Analysis: Approaches, Datasets, and Future Research",
        "paper_author": "Tan K.L.",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "71",
        "cover_date": "2023-04-01",
        "Abstract": "Sentiment analysis is a critical subfield of natural language processing that focuses on categorizing text into three primary sentiments: positive, negative, and neutral. With the proliferation of online platforms where individuals can openly express their opinions and perspectives, it has become increasingly crucial for organizations to comprehend the underlying sentiments behind these opinions to make informed decisions. By comprehending the sentiments behind customers’ opinions and attitudes towards products and services, companies can improve customer satisfaction, increase brand reputation, and ultimately increase revenue. Additionally, sentiment analysis can be applied to political analysis to understand public opinion toward political parties, candidates, and policies. Sentiment analysis can also be used in the financial industry to analyze news articles and social media posts to predict stock prices and identify potential investment opportunities. This paper offers an overview of the latest advancements in sentiment analysis, including preprocessing techniques, feature extraction methods, classification techniques, widely used datasets, and experimental results. Furthermore, this paper delves into the challenges posed by sentiment analysis datasets and discusses some limitations and future research prospects of sentiment analysis. Given the importance of sentiment analysis, this paper provides valuable insights into the current state of the field and serves as a valuable resource for both researchers and practitioners. The information presented in this paper can inform stakeholders about the latest advancements in sentiment analysis and guide future research in the field.",
        "DOI": "10.3390/app13074550",
        "affiliation_name": "Multimedia University",
        "affiliation_city": "Cyberjaya",
        "affiliation_country": "Malaysia"
    },
    {
        "paper_title": "Anomaly Detection Method for Unknown Protocols in a Power Plant ICS Network with Decision Tree",
        "paper_author": "Lee K.M.",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "0",
        "cover_date": "2023-04-01",
        "Abstract": "This study aimed to enhance the stability and security of power plant control network systems by developing detectable models using artificial intelligence machine learning techniques. Due to the closed system operation policy of facility manufacturers, it is challenging to detect and respond to security threats using standard security systems. With the increasing digitization of control systems, the risk of external malware penetration is also on the rise. To address this, machine learning techniques were applied to extract patterns from network traffic data produced at an average of 6.5 TB per month, and fingerprinting was used to detect unregistered terminals accessing the control network. By setting a threshold between transmission amounts and attempts using one month of data, an anomaly judgment model was learned to define patterns of data communication between the origin and destination. The hypothesis was tested using machine learning techniques if a new pattern occurred and no traffic occurred. The study confirmed that this method can be applied to not only plant control systems but also closed-structured control networks, where availability is critical, and other industries that use large amounts of traffic data. Experimental results showed that the proposed model outperformed existing models in terms of detection efficiency and processing time.",
        "DOI": "10.3390/app13074203",
        "affiliation_name": "Dong Seoul College",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "Data-Driven Analysis of Privacy Policies Using LexRank and KL Summarizer for Environmental Sustainability",
        "paper_author": "Md A.Q.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "0",
        "cover_date": "2023-04-01",
        "Abstract": "Natural language processing (NLP) is a field in machine learning that analyses and manipulate huge amounts of data and generates human language. There are a variety of applications of NLP such as sentiment analysis, text summarization, spam filtering, language translation, etc. Since privacy documents are important and legal, they play a vital part in any agreement. These documents are very long, but the important points still have to be read thoroughly. Customers might not have the necessary time or the knowledge to understand all the complexities of a privacy policy document. In this context, this paper proposes an optimal model to summarize the privacy policy in the best possible way. The methodology of text summarization is the process where the summaries from the original huge text are extracted without losing any vital information. Using the proposed idea of a common word reduction process combined with natural language processing algorithms, this paper extracts the sentences in the privacy policy document that hold high weightage and displays them to the customer, and it can save the customer’s time from reading through the entire policy while also providing the customers with only the important lines that they need to know before signing the document. The proposed method uses two different extractive text summarization algorithms, namely LexRank and Kullback Leibler (KL) Summarizer, to summarize the obtained text. According to the results, the summarized sentences obtained via the common word reduction process and text summarization algorithms were more significant than the raw privacy policy text. The introduction of this novel methodology helps to find certain important common words used in a particular sector to a greater depth, thus allowing more in-depth study of a privacy policy. Using the common word reduction process, the sentences were reduced by 14.63%, and by applying extractive NLP algorithms, significant sentences were obtained. The results after applying NLP algorithms showed a 191.52% increase in the repetition of common words in each sentence using the KL summarizer algorithm, while the LexRank algorithm showed a 361.01% increase in the repetition of common words. This implies that common words play a large role in determining a sector’s privacy policies, making our proposed method a real-world solution for environmental sustainability.",
        "DOI": "10.3390/su15075941",
        "affiliation_name": "Vellore Institute of Technology, Chennai",
        "affiliation_city": "Chennai",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Machine Learning Applications for Reliability Engineering: A Review",
        "paper_author": "Payette M.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "12",
        "cover_date": "2023-04-01",
        "Abstract": "The treatment of big data as well as the rapid improvement in the speed of data processing are facilitated by the parallelization of computations, cloud computing as well as the increasing number of artificial intelligence techniques. These developments lead to the multiplication of applications and modeling techniques. Reliability engineering includes several research areas such as reliability, availability, maintainability, and safety (RAMS); prognostics and health management (PHM); and asset management (AM), aiming at the realization of the life cycle value. The expansion of artificial intelligence (AI) modeling techniques combined with the various research topics increases the difficulty of practitioners in identifying the appropriate methodologies and techniques applicable. The objective of this publication is to provide an overview of the different machine learning (ML) techniques from the perspective of traditional modeling techniques. Furthermore, it presents a methodology for data science application and how machine learning can be applied in each step. Then, it will demonstrate how ML techniques can be complementary to traditional approaches, and cases from the literature will be presented.",
        "DOI": "10.3390/su15076270",
        "affiliation_name": "Université du Québec à Trois-Rivières",
        "affiliation_city": "Trois-Rivieres",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Predicting wildfire ignition causes in Southern France using eXplainable Artificial Intelligence (XAI) methods",
        "paper_author": "Bountzouklis C.",
        "publication": "Environmental Research Letters",
        "citied_by": "4",
        "cover_date": "2023-04-01",
        "Abstract": "The percentage of wildfires that are ignited by an undetermined origin is substantial in Europe and Mediterranean France. Forest fire experts have recognized the significance of fires with an unknown ignition source since documentation and research of fire causes are important for creating appropriate fire policies and prevention strategies. The use of machine learning in wildfire science has increased considerably and is driven by the increasing availability of large and high-quality datasets. However, the absence of comprehensive fire-cause data hinders the utility of existing fire databases. This study trains and applies a machine-learning based model to classify the cause of fire ignition based on several environmental and anthropogenic features in Southern France using an eXplainable Artificial Intelligence framework. The results demonstrate that the source of unknown caused wildfires can be predicted at various levels of accuracy/natural fires have the highest accuracy (F1-score 0.87) compared to human-caused fires such as accidental (F1-score 0.74) and arson (F1-score 0.64). Factors related to spatiotemporal properties as well as topographic characteristics are considered the most important features in determining the classification of unknown caused fires for the specific area.",
        "DOI": "10.1088/1748-9326/acc8ee",
        "affiliation_name": "Université Côte d'Azur",
        "affiliation_city": "Nice",
        "affiliation_country": "France"
    },
    {
        "paper_title": "Maximizing user retention with machine learning enabled 6G channel allocation",
        "paper_author": "Singh P.",
        "publication": "International Journal of Information Technology (Singapore)",
        "citied_by": "1",
        "cover_date": "2023-04-01",
        "Abstract": "The paper proposes a machine learning-based user retention technique for the 6G network by identifying and classifying loyal users using supervised machine learning algorithms such as Decision Tree, K-Nearest Neighbor, and Support Vector Machine. The study also suggests a threshold-based channel allocation method to allocate network resources primarily to loyal users. The performance of the proposed algorithm is evaluated using SimPy simulation, and the results show that loyal users experience minimum average waiting time and no call drops compared to normal and recent users. The paper highlights the need for new schemes and policies to retain valuable users and proposes a novel approach that leverages machine learning techniques and 6G aspects to achieve this objective. The proposed algorithm's effectiveness is demonstrated through simulation, which provides useful insights into its performance under different network conditions. The research contributes to the ongoing efforts to enhance user experience in the rapidly evolving field of mobile communication networks.",
        "DOI": "10.1007/s41870-023-01249-z",
        "affiliation_name": "LLoyd Institute of Engineering &amp; Technology",
        "affiliation_city": "Greater Noida",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Regularized Denoising Masked Visual Pretraining for Robust Embodied PointGoal Navigation",
        "paper_author": "Peng J.",
        "publication": "Sensors",
        "citied_by": "1",
        "cover_date": "2023-04-01",
        "Abstract": "Embodied PointGoal navigation is a fundamental task for embodied agents. Recent works have shown that the performance of the embodied navigation agent degrades significantly in the presence of visual corruption, including Spatter, Speckle Noise, and Defocus Blur, showing the weak robustness of the agent. To improve the robustness of embodied navigation agents to various visual corruptions, we propose a navigation framework called Regularized Denoising Masked AutoEncoders Navigation (RDMAE-Nav). In a nutshell, RDMAE-Nav mainly consists of two modules: a visual module and a policy module. In the visual module, a self-supervised pretraining method, dubbed Regularized Denoising Masked AutoEncoders (RDMAE), is designed to enable the Vision Transformers (ViT)-based visual encoder to learn robust representations. The bidirectional Kullback–Leibler divergence is introduced in RDMAE as the regularization term for a denoising masked modeling task. Specifically, RDMAE mitigates the gap between clean and noisy image representations by minimizing the bidirectional Kullback–Leibler divergence. Then, the visual encoder is pretrained by RDMAE. In contrast to existing works, RDMAE-Nav applies denoising masked visual pretraining for PointGoal navigation to improve robustness to various visual corruptions. Finally, the pretrained visual encoder with frozen weights is applied to extract robust visual representations for policy learning in the RDMAE-Nav. Extensive experiments show that RDMAE-Nav performs competitively compared with state of the arts (SOTAs) on various visual corruptions. In detail, RDMAE-Nav performs the absolute improvement: 28.2% in SR and 23.68% in SPL under Spatter; 2.28% in SR and 6.41% in SPL under Speckle Noise; and 9.46% in SR and 9.55% in SPL under Defocus Blur.",
        "DOI": "10.3390/s23073553",
        "affiliation_name": "Institute of Microelectronics Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Predicting the HIV/AIDS Knowledge among the Adolescent and Young Adult Population in Peru: Application of Quasi-Binomial Logistic Regression and Machine Learning Algorithms",
        "paper_author": "Aybar-Flores A.",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "4",
        "cover_date": "2023-04-01",
        "Abstract": "Inadequate knowledge is one of the principal obstacles for preventing HIV/AIDS spread. Worldwide, it is reported that adolescents and young people have a higher vulnerability of being infected. Thus, the need to understand youths’ knowledge towards HIV/AIDS becomes crucial. This study aimed to identify the determinants and develop a predictive model to estimate HIV/AIDS knowledge among this target population in Peru. Data from the 2019 DHS Survey were used. The software RStudio and RapidMiner were used for quasi-binomial logistic regression and computational model building, respectively. Five classification algorithms were considered for model development and their performance was assessed using accuracy, sensitivity, specificity, FPR, FNR, Cohen’s kappa, F1 score and AUC. The results revealed an association between 14 socio-demographic, economic and health factors and HIV/AIDS knowledge. The accuracy levels were estimated between 59.47 and 64.30%, with the random forest model showing the best performance (64.30%). Additionally, the best classifier showed that the gender of the respondent, area of residence, wealth index, region of residence, interviewee’s age, highest educational level, ethnic self-perception, having heard about HIV/AIDS in the past, the performance of an HIV/AIDS screening test and mass media access have a major influence on HIV/AIDS knowledge prediction. The results suggest the usefulness of the associations found and the random forest model as a predictor of knowledge of HIV/AIDS and may aid policy makers to guide and reinforce the planning and implementation of healthcare strategies.",
        "DOI": "10.3390/ijerph20075318",
        "affiliation_name": "Universidad Continental",
        "affiliation_city": "Huancayo",
        "affiliation_country": "Peru"
    },
    {
        "paper_title": "A Survey on Deep Reinforcement Learning Algorithms for Robotic Manipulation",
        "paper_author": "Han D.",
        "publication": "Sensors",
        "citied_by": "80",
        "cover_date": "2023-04-01",
        "Abstract": "Robotic manipulation challenges, such as grasping and object manipulation, have been tackled successfully with the help of deep reinforcement learning systems. We give an overview of the recent advances in deep reinforcement learning algorithms for robotic manipulation tasks in this review. We begin by outlining the fundamental ideas of reinforcement learning and the parts of a reinforcement learning system. The many deep reinforcement learning algorithms, such as value-based methods, policy-based methods, and actor–critic approaches, that have been suggested for robotic manipulation tasks are then covered. We also examine the numerous issues that have arisen when applying these algorithms to robotics tasks, as well as the various solutions that have been put forth to deal with these issues. Finally, we highlight several unsolved research issues and talk about possible future directions for the subject.",
        "DOI": "10.3390/s23073762",
        "affiliation_name": "Gallogly College of Engineering",
        "affiliation_city": "Norman",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Towards an Evolved Immersive Experience: Exploring 5G- and Beyond-Enabled Ultra-Low-Latency Communications for Augmented and Virtual Reality",
        "paper_author": "Hazarika A.",
        "publication": "Sensors",
        "citied_by": "46",
        "cover_date": "2023-04-01",
        "Abstract": "Augmented reality and virtual reality technologies are witnessing an evolutionary change in the 5G and Beyond (5GB) network due to their promising ability to enable an immersive and interactive environment by coupling the virtual world with the real one. However, the requirement of low-latency connectivity, which is defined as the end-to-end delay between the action and the reaction, is very crucial to leverage these technologies for a high-quality immersive experience. This paper provides a comprehensive survey and detailed insight into various advantageous approaches from the hardware and software perspectives, as well as the integration of 5G technology, towards 5GB, in enabling a low-latency environment for AR and VR applications. The contribution of 5GB systems as an outcome of several cutting-edge technologies, such as massive multiple-input, multiple-output (mMIMO) and millimeter wave (mmWave), along with the utilization of artificial intelligence (AI) and machine learning (ML) techniques towards an ultra-low-latency communication system, is also discussed in this paper. The potential of using a visible-light communications (VLC)-guided beam through a learning algorithm for a futuristic, evolved immersive experience of augmented and virtual reality with the ultra-low-latency transmission of multi-sensory tracking information with an optimal scheduling policy is discussed in this paper.",
        "DOI": "10.3390/s23073682",
        "affiliation_name": "Washkewicz College of Engineering",
        "affiliation_city": "Cleveland",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Fault Detection on the Edge and Adaptive Communication for State of Alert in Industrial Internet of Things",
        "paper_author": "Santo Y.",
        "publication": "Sensors",
        "citied_by": "4",
        "cover_date": "2023-04-01",
        "Abstract": "Industrial production and manufacturing systems require automation, reliability, as well as low-latency intelligent control. Industrial Internet of Things (IIoT) is an emerging paradigm that enables precise, low latency, intelligent computing, supported by cutting-edge technology such as edge computing and machine learning. IIoT provides some of the essential building blocks to drive manufacturing systems to the next level of productivity, efficiency, and safety. Hardware failures and faults in IIoT are critical challenges to be faced. These anomalies can cause accidents and financial loss, affect productivity, and mobilize staff by producing false alarms. In this context, this article proposes a framework called Detection and Alert State for Industrial Internet of Things Faults (DASIF). The DASIF framework applies edge computing to execute highly precise and low latency machine learning models to detect industrial IoT faults and autonomously enforce an adaptive communication policy, triggering a state of alert in case of fault detection. The state of alert is a pre-stage countermeasure where the network increases communication reliability by using data replication combined with multiple-path communication. When the system is under alert, it can process a fine-grained inspection of the data for efficient decison-making. DASIF performance was obtained considering a simulation of the IIoT network and a real petrochemical dataset.",
        "DOI": "10.3390/s23073544",
        "affiliation_name": "Universidade Federal do Rio Grande do Norte",
        "affiliation_city": "Natal",
        "affiliation_country": "Brazil"
    },
    {
        "paper_title": "Deep reinforcement learning for turbulent drag reduction in channel flows",
        "paper_author": "Guastoni L.",
        "publication": "European Physical Journal E",
        "citied_by": "40",
        "cover_date": "2023-04-01",
        "Abstract": "We introduce a reinforcement learning (RL) environment to design and benchmark control strategies aimed at reducing drag in turbulent fluid flows enclosed in a channel. The environment provides a framework for computationally efficient, parallelized, high-fidelity fluid simulations, ready to interface with established RL agent programming interfaces. This allows for both testing existing deep reinforcement learning (DRL) algorithms against a challenging task, and advancing our knowledge of a complex, turbulent physical system that has been a major topic of research for over two centuries, and remains, even today, the subject of many unanswered questions. The control is applied in the form of blowing and suction at the wall, while the observable state is configurable, allowing to choose different variables such as velocity and pressure, in different locations of the domain. Given the complex nonlinear nature of turbulent flows, the control strategies proposed so far in the literature are physically grounded, but too simple. DRL, by contrast, enables leveraging the high-dimensional data that can be sampled from flow simulations to design advanced control strategies. In an effort to establish a benchmark for testing data-driven control strategies, we compare opposition control, a state-of-the-art turbulence-control strategy from the literature, and a commonly used DRL algorithm, deep deterministic policy gradient. Our results show that DRL leads to 43% and 30% drag reduction in a minimal and a larger channel (at a friction Reynolds number of 180), respectively, outperforming the classical opposition control by around 20 and 10 percentage points, respectively.",
        "DOI": "10.1140/epje/s10189-023-00285-8",
        "affiliation_name": "Swedish e-Science Research Centre",
        "affiliation_city": "Stockholm",
        "affiliation_country": "Sweden"
    },
    {
        "paper_title": "The new World Mental Health Report: Believing impossible things",
        "paper_author": "Allison S.",
        "publication": "Australasian Psychiatry",
        "citied_by": "4",
        "cover_date": "2023-04-01",
        "Abstract": "Objective: We examine whether the recent World Health Organization (WHO) report on global mental health uses severity of illness as a criterion in priority setting for resource allocation. Conclusions: The WHO does not prioritise severity in the recent landmark World Mental Health Report. It recommends instead the insuperable task of scaling-up interventions for broadly defined mental health conditions, including milder distress, amongst over a billion people, with the majority living in low- and middle-income countries. Schizophrenia, the most severe and disabling of all psychiatric illnesses, is relatively neglected in the WHO report, and the disability associated with bipolar disorder is underestimated. This is inconsistent with the ethical principle of vertical equity, where the most severe illnesses should receive the greatest priority. The global mental health movement must refocus on deinstitutionalisation, and ensure adequate community and general hospital treatment for severe illnesses, especially the 24 million people with schizophrenia.",
        "DOI": "10.1177/10398562231154806",
        "affiliation_name": "School of Medicine and Psychology",
        "affiliation_city": "Canberra",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Disclosure control of machine learning models from trusted research environments (TRE): New challenges and opportunities",
        "paper_author": "Mansouri-Benssassi E.",
        "publication": "Heliyon",
        "citied_by": "2",
        "cover_date": "2023-04-01",
        "Abstract": "Introduction: Artificial intelligence (AI) applications in healthcare and medicine have increased in recent years. To enable access to personal data, Trusted Research Environments (TREs) (otherwise known as Safe Havens) provide safe and secure environments in which researchers can access sensitive personal data and develop AI (in particular machine learning (ML)) models. However, currently few TREs support the training of ML models in part due to a gap in the practical decision-making guidance for TREs in handling model disclosure. Specifically, the training of ML models creates a need to disclose new types of outputs from TREs. Although TREs have clear policies for the disclosure of statistical outputs, the extent to which trained models can leak personal training data once released is not well understood. Background: We review, for a general audience, different types of ML models and their applicability within healthcare. We explain the outputs from training a ML model and how trained ML models can be vulnerable to external attacks to discover personal data encoded within the model. Risks: We present the challenges for disclosure control of trained ML models in the context of training and exporting models from TREs. We provide insights and analyse methods that could be introduced within TREs to mitigate the risk of privacy breaches when disclosing trained models. Discussion: Although specific guidelines and policies exist for statistical disclosure controls in TREs, they do not satisfactorily address these new types of output requests; i.e., trained ML models. There is significant potential for new interdisciplinary research opportunities in developing and adapting policies and tools for safely disclosing ML outputs from TREs.",
        "DOI": "10.1016/j.heliyon.2023.e15143",
        "affiliation_name": "Health Data Research UK",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Challenges and prospects of climate change impact assessment on mangrove environments through mathematical models",
        "paper_author": "Fanous M.",
        "publication": "Environmental Modelling and Software",
        "citied_by": "13",
        "cover_date": "2023-04-01",
        "Abstract": "The impacts of climate change, especially sea-level rise, are an increasing threat to the world's coastal regions. Following recommendations made by the United Nations about the preservation of mangrove environments, particularly given their potential for effective natural defence against wave-driven hazards, a series of experiments have been conducted to quantify the ability of mangroves to counter climate change impacts. To date, these experiments have been limited by computational cost and inability to model multiple scenarios. With improved data quality and availability, machine learning has enormous potential to supplement, or even replace, existing numerical methods. This article presents both an outline of the importance of protecting mangrove environments and a review of methods currently used to quantify the capacity of mangroves to adapt to climate change impacts. In view of the limitations of existing numerical methods, the article also discusses the potential of machine learning as an efficient and effective alternative.",
        "DOI": "10.1016/j.envsoft.2023.105658",
        "affiliation_name": "Coventry University",
        "affiliation_city": "Coventry",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "A Framework for Data-Driven Agent-Based Modelling of Agricultural Land Use",
        "paper_author": "Ravaioli G.",
        "publication": "Land",
        "citied_by": "10",
        "cover_date": "2023-04-01",
        "Abstract": "Agent-based models (ABMs) are particularly suited for simulating the behaviour of agricultural agents in response to land use (LU) policy. However, there is no evidence of their widespread use by policymakers. Here, we carry out a review of LU ABMs to understand how farmers’ decision-making has been modelled. We found that LU ABMs mainly rely on pre-defined behavioural rules at the individual farmers’ level. They prioritise explanatory over predictive purposes, thus limiting the use of ABM for policy assessment. We explore the use of machine learning (ML) as a data-driven alternative for modelling decisions. Integration of ML with ABMs has never been properly applied to LU modelling, despite the increased availability of remote sensing products and agricultural micro-data. Therefore, we also propose a framework to develop data-driven ABMs for agricultural LU. This framework avoids pre-defined theoretical or heuristic rules and instead resorts to ML algorithms to learn agents’ behavioural rules from data. ML models are not directly interpretable, but their analysis can provide novel insights regarding the response of farmers to policy changes. The integration of ML models can also improve the validation of individual behaviours, which increases the ability of ABMs to predict policy outcomes at the micro-level.",
        "DOI": "10.3390/land12040756",
        "affiliation_name": "Instituto Superior Técnico",
        "affiliation_city": "Lisbon",
        "affiliation_country": "Portugal"
    },
    {
        "paper_title": "Comparing machine learning methods for predicting land development intensity",
        "paper_author": "Gu G.",
        "publication": "PLoS ONE",
        "citied_by": "6",
        "cover_date": "2023-04-01",
        "Abstract": "Land development intensity is a comprehensive indicator to measure the degree of saving and intensive land construction and economic production activities. It is also the result of the joint action of natural, social, economic, and ecological elements in land development and utilization. Scientific prediction of land development intensity has particular reference significance for future regional development planning and the formulation of reasonable land use policies. Based on the inter-provincial land development intensity and its influencing factors in China, this study applied four algorithms, XGBoost, random forest model, support vector machine, and decision tree, to simulate and predict the land development intensity, and then compared the prediction accuracy of the four algorithms, and also carried out hyperparameter adjustment and prediction accuracy verification. The results show that the model with the best prediction performance among the four algorithms is XGBoost, and its R2 and MSE between predicted and valid values are 95.66% and 0.16, respectively, which are higher than the other three models. During the training process, the learning curve of the XGBoost model exhibited low fluctuation and fast fitting. Hyperparameter tuning is crucial to exploit the model’s potential. The XGBoost model has the best prediction performance with the best hyperparameter combination of max_depth:19, learning_rate: 0.47, and n_estimatiors:84. This study provides some reference significance for the simulation of land development and utilization dynamics.",
        "DOI": "10.1371/journal.pone.0282476",
        "affiliation_name": "Nanning Normal University",
        "affiliation_city": "Nanning",
        "affiliation_country": "China"
    },
    {
        "paper_title": "An ensemble deep learning approach for predicting cocoa yield",
        "paper_author": "Olofintuyi S.S.",
        "publication": "Heliyon",
        "citied_by": "19",
        "cover_date": "2023-04-01",
        "Abstract": "One important aspect of agriculture is crop yield prediction. This aspect allows decision-makers and farmers to make adequate planning and policies. Before now, various statistical models have been used for crop yield prediction but this approach experienced some hiccups such as time wastage, inaccurate prediction, and difficulties in model usage. Recently, a new trend of deep learning and machine learning are now adopted for crop yield prediction. Deep learning can extract patterns from a large volume of the dataset, thus, they are suitable for prediction. The research work aims to propose an efficient deep-learning technique in the field of cocoa yield prediction. This research presents a deep learning approach for cocoa yield prediction using a Convolutional Neural Network and Recurrent Neural Network (CNN-RNN) with Long Short Term Memory (LSTM). The ensemble approach was adopted because of the nature of the dataset used. Two different sets of the dataset were used, namely; the climatic dataset and the cocoa yield dataset. CNN-RNN with LSTM has some salient features, where CNN was used to handle the climatic dataset, and RNN was employed to handle the cocoa yield prediction in southwest Nigeria. Two major problems generated by the CNN-RNN model are vanishing and exploding gradients and this was handled by LSTM. The proposed model was benchmarked with other machine learning algorithms based on Mean Absolute Error (MAE), Mean Square Error (MSE), Root Mean Square Error (RMSE), and Mean Absolute Percentage Error (MAPE). CNN-RNN with LSTM gave the least mean of absolute error as compared to the other machine learning algorithms which shows the efficiency of the model.",
        "DOI": "10.1016/j.heliyon.2023.e15245",
        "affiliation_name": "Obafemi Awolowo University",
        "affiliation_city": "Ife",
        "affiliation_country": "Nigeria"
    },
    {
        "paper_title": "JACC Journals’ Pathway Forward With AI Tools: The Future Is Now",
        "paper_author": "Fuster V.",
        "publication": "JACC: Asia",
        "citied_by": "1",
        "cover_date": "2023-04-01",
        "Abstract": "NA",
        "DOI": "10.1016/j.jacasi.2023.03.002",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Lengthy early morning instant messages reveal more than you think: Analysing interpersonal relationships using mobile communication metadata",
        "paper_author": "Kqiku L.",
        "publication": "Pervasive and Mobile Computing",
        "citied_by": "0",
        "cover_date": "2023-04-01",
        "Abstract": "Privacy policies are one of the key factors that determine user's privacy exposure. Quite often such policies request users’ consent to ”only” collect their metadata. In this paper, we investigate the information that communication metadata can reveal, specifically, in regards to users’ social circles. To this end, we developed an application and conducted a longitudinal field study with 25 participants, who installed our application on their personal smartphones. Over a period of four weeks, our application collected the metadata of the participants’ communication with their social contacts over four channels, i.e., calls, SMS, e-mails, and Instant Messages (IMs). The content of the communication were not collected and the identities of the participants’ social contacts were encrypted, so that only the users could get access to them. We leverage the collected metadata to examine whether and to what extend it is possible to exploit them in order to classify the participants’ social contacts into four social categories, namely, family members, friends, acquaintances, and colleagues using Machine Learning (ML) techniques. By doing so, we do not only reproduce and replicate an existing study, but also extend it by further considering the metadata about IMs. In our user study, friends and family members call each other and exchange SMS more than acquaintances and colleagues. Moreover, as expected, IMs are exchanged more with friends followed by family, whereas e-mails more between acquaintances and colleagues. In addition, to validate the role of instant messaging channel, we show against our expectations that considering metadata about IMs only slightly improve the prediction of users’ social ties. We also examine the most important features that lead to such predictions. We show that the prediction of social ties can be further enhanced by considering the aforementioned communication channels. Our study results in the f-measure score of 79.4% for a fine-grained classification of four considered social categories, achieving better results than related approaches, and 89.6% when considering the family category against all the others.",
        "DOI": "10.1016/j.pmcj.2023.101781",
        "affiliation_name": "Georg-August-Universität Göttingen",
        "affiliation_city": "Gottingen",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Integrating preventive and predictive maintenance policies with system dynamics: A decision table approach",
        "paper_author": "Bilal Yıldız G.",
        "publication": "Advanced Engineering Informatics",
        "citied_by": "7",
        "cover_date": "2023-04-01",
        "Abstract": "A decision table is a practical tool that helps systems planners to make operational decisions, especially when they are under stress. With the effect of recent trends, such as the use of machine learning, data mining, and reinforcement learning methods, the maintenance decision has been a dynamic issue depending on system conditions. An expert may execute the maintenance or wait for the next periodic maintenance due to lack of maintenance workers, tools or budget, resources, etc., although the intelligent method predicts a failure approaching. Even sometimes, he/she may ignore the current periodic maintenance. Our method allows making some changes in the maintenance plan systematically. It integrates the results of preventive and predictive maintenance policies, and as different from the literature, it allows ignoring some maintenance actions depending on the maintenance resource levels in a decision table. Such a strategy helps to allocate limited resources to maintenance actions reasonably. We conducted an extensive simulation study on a real-life dataset. The preventive maintenance period is determined using classical approaches such as Weibull analysis. A machine learning algorithm is utilized to predict the type of failure. We have analyzed the performance of the proposed decision table approach under a variety of scenarios and with different parameter settings. We also showed the effect of parameter settings and the marginal utility of each maintenance policy. In addition, the approach provides several choices for planners. As a result, the proposed approach improves the system's sustainability compared to traditional policies.",
        "DOI": "10.1016/j.aei.2023.101952",
        "affiliation_name": "Hitit University",
        "affiliation_city": "Corum",
        "affiliation_country": "Turkey"
    },
    {
        "paper_title": "Resource productivity and environmental degradation in EU-27 countries: context of material footprint",
        "paper_author": "Mushafiq M.",
        "publication": "Environmental Science and Pollution Research",
        "citied_by": "9",
        "cover_date": "2023-04-01",
        "Abstract": "This study explores the relationship between the resource productivity and environmental degradation in European Union-27 countries. This study tests this relationship in context of high, moderate, and low material footprint sub-samples; these samples are formed utilizing the expectation–maximization machine learning algorithm. Using the panel data set of EU-27 countries from 2000 to 2020, linear and non-linear autoregressive distributed lag (ARDL) are applied for the symmetric and asymmetric evidence and to test environmental Kuznets curve (EKC), linear ARDL with the quadratic function is included. Results of the symmetric relationship find evidence of resource productivity’s impact on the environmental degradation. In full sample of EU-27, both symmetric and asymmetric methods show that the short run and long run increase of resource productivity lower the environmental degradation. Only long run asymmetric relationship in high material footprint subsamples supports that the resource productivity controls environmental degradation. Results of moderate material footprint sub-sample are mixed. However, low material footprint countries show that resource productivity in long run controls the environmental degradation in symmetry and only positive resource controls productivity in short run in asymmetric relationship. The reason for mixed results is the quadratic nature of sub-samples. EKC hypothesis is validated in moderate and low material footprint sub-samples. This research has many policy implications.",
        "DOI": "10.1007/s11356-023-26631-z",
        "affiliation_name": "Gdańsk University of Technology",
        "affiliation_city": "Gdansk",
        "affiliation_country": "Poland"
    },
    {
        "paper_title": "Study on the impact of intelligent city pilot on green and low-carbon development",
        "paper_author": "Liu X.m.",
        "publication": "Environmental Science and Pollution Research",
        "citied_by": "18",
        "cover_date": "2023-04-01",
        "Abstract": "The intelligent city pilot policy is a major measure in China to promote urban development from factor driven and investment driven to innovation driven. Intelligent city construction can effectively coordinate specialized production factors and information sharing mechanism, promote digital information technology innovation, promote smart industry cluster, and expand ecological scenarios of clean industry application, so as to reduce carbon emissions. This paper reveals the internal mechanism of intelligent city construction to promote carbon emission reduction. Based on the quasi-natural experiments carried out in three batches of pilot construction of intelligent cities since 2012, the difference-in-difference model (DID) is used to identify its impact on urban carbon emissions. The research results show that the pilot construction of intelligent cities is conducive to reducing carbon emissions, which is still robust under multiple scenarios such as placebo test and endogenous test. Heterogeneity analysis shows that the pilot policies have a more significant carbon emission reduction effect on the Beijing-Tianjin-Hebei urban agglomeration, non-resource-based cities, and non-old industrial bases. After further quantitative analysis of 917 pilot policy texts based on Simhash algorithm, Jieba word segmentation, and word frequency statistics, it is found that intelligent industry policies reduce carbon emissions by driving data elements agglomeration and optimizing industrial structure, while intelligent government and intelligent people’s livelihood policies improve energy efficiency and reduce carbon emissions through green technological innovation. Counterfactual tests using machine learning algorithms show that the later the pilot batch, the better the sustainable carbon emission reduction effect of intelligent city pilot policies.",
        "DOI": "10.1007/s11356-023-26579-0",
        "affiliation_name": "Jinling Institute of Technology",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Dataset of cannabis seeds for machine learning applications",
        "paper_author": "Chumchu P.",
        "publication": "Data in Brief",
        "citied_by": "9",
        "cover_date": "2023-04-01",
        "Abstract": "The recent changes in policies in several countries regarding cannabis use has increased cannabis usage and research [1,2]. Cannabis is the second most used psychoactive substance word-wide [3]. Cannabis remains the subject of many research works. The cannabis can be classified into different classes according to their external features like colour, shape, and size using some computer vision and machine learning techniques. Precise classification or recognition is the unmet need of the agriculture business. This attracts many researchers to produce solutions with machine learning and deep learning techniques. Neat and clean dataset is the primary requirements to build accurate and robust machine learning model and minimize misclassification for the real-time environment. To achieve this objective, we have created an image dataset of cannabis seed. Accordingly, we have considered seventeen cannabis seeds to create dataset. The dataset contains 17 subfolders of cannabis seeds and folder is named with the category of seed. We strongly believe the cannabis seeds dataset will be very helpful for training, testing, and validation of cannabis classification or recognition with machine learning models.",
        "DOI": "10.1016/j.dib.2023.108954",
        "affiliation_name": "Vishwakarma University",
        "affiliation_city": "Pune",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Preterm birth and term low birth weight associated with wildfire-specific PM<inf>2.5</inf>: A cohort study in New South Wales, Australia during 2016–2019",
        "paper_author": "Zhang Y.",
        "publication": "Environment International",
        "citied_by": "26",
        "cover_date": "2023-04-01",
        "Abstract": "Background: Exposure to wildfire smoke has been linked with a range of health outcomes. However, to date, evidence is limited for the association between wildfire-specific PM2.5, a primary emission of wildfire smoke, and adverse birth outcomes. Objective: We aimed to estimate the risk and burden of preterm birth/term low birth weight, associated with maternal exposure to wildfire-specific PM2.5. Methods: A total of 330,884 birth records with maternal information were collected from the New South Wales Australia from 2015 to 2019, covering 523 residential communities. Daily wildfire-specific PM2.5 at a 0.25° × 0.25° (≈ 25 km × 25 km) resolution was estimated by a machine learning method combining 3-D chemical transport model (GEOS-Chem) and reanalysis meteorological data. Cox proportional hazards models were implemented to evaluate the association between wildfire-specific PM2.5 and preterm birth/term low birth weight. Number and fraction of preterm birth/term low birth weight attributable to wildfire-specific PM2.5 during pregnancy were calculated. Results: Per one interquartile-range rise in wildfire-specific PM2.5 was found to be associated with 6.9% (HR: 1.069, 95% CI: 1.058–1.081) increased risk of preterm birth and 3.6% (HR: 1.036, 95% CI: 1.014–1.058) higher risk of term low birth weight. The most susceptible gestational window was the 2nd trimester for preterm birth whereas the 1st for term low birth weight. We estimated that 14.30% preterm births and 8.04% term low birth weight cases were attributable to maternal exposure to wildfire-specific PM2.5 during the whole pregnancy. Male infants and mothers aged ≥ 40, experiencing temperature extremes or living in the inner region, and concepted during spring had higher risks of preterm birth/term low birth weight associated with wildfire-specific PM2.5. Comparatively, mothers with advanced age have a higher risk of preterm birth while younger mothers were more likely to deliver term newborns with low birth weight, when being exposed to wildfire-specific PM2.5. Pregnancy-induced hypertension enhanced the risk of preterm birth associated with wildfire-specific PM2.5. Conclusions: This study strengthened robust evidence on the enhanced risk of preterm birth/term low birth weight associated with maternal exposure to wildfire-specific PM2.5. In light of higher frequency and intensity of wildfire occurrences globally, more special attention should be paid to pregnant women by policy makers.",
        "DOI": "10.1016/j.envint.2023.107879",
        "affiliation_name": "Monash University",
        "affiliation_city": "Melbourne",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Neural-network solutions to stochastic reaction networks",
        "paper_author": "Tang Y.",
        "publication": "Nature Machine Intelligence",
        "citied_by": "12",
        "cover_date": "2023-04-01",
        "Abstract": "The stochastic reaction network in which chemical species evolve through a set of reactions is widely used to model stochastic processes in physics, chemistry and biology. To characterize the evolving joint probability distribution in the state space of species counts requires solving a system of ordinary differential equations, the chemical master equation, where the size of the counting state space increases exponentially with the type of species. This makes it challenging to investigate the stochastic reaction network. Here we propose a machine learning approach using a variational autoregressive network to solve the chemical master equation. Training the autoregressive network employs the policy gradient algorithm in the reinforcement learning framework, which does not require any data simulated previously by another method. In contrast with simulating single trajectories, this approach tracks the time evolution of the joint probability distribution, and supports direct sampling of configurations and computing their normalized joint probabilities. We apply the approach to representative examples in physics and biology, and demonstrate that it accurately generates the probability distribution over time. The variational autoregressive network exhibits plasticity in representing the multimodal distribution, cooperates with the conservation law, enables time-dependent reaction rates and is efficient for high-dimensional reaction networks, allowing a flexible upper count limit. The results suggest a general approach to study stochastic reaction networks based on modern machine learning.",
        "DOI": "10.1038/s42256-023-00632-6",
        "affiliation_name": "University of Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A temporal–spatial analysis on the socioeconomic development of rural villages in Thailand and Vietnam based on satellite image data",
        "paper_author": "Wölk F.",
        "publication": "Computer Communications",
        "citied_by": "1",
        "cover_date": "2023-04-01",
        "Abstract": "Obtaining accurate and timely estimates of socio-economic status at fine geographical resolutions is essential for global sustainable development and the fight against poverty. However, data related to local socio-economic dynamics in rural villages is often either unavailable or outdated. To fill this gap, predicting local economic well-being with satellite imagery and machine learning has shown promising results. While most state-of-the-art analyses focus on predicting the levels of socio-economic status, finding temporal changes in rural villages’ economic well-being is essential for tracking the impacts of public policies (targeting e.g., poverty alleviation or access to various public services). In this paper, we propose an approach that utilizes pixel-wise differences in satellite images to classify temporal changes in average and median consumption expenditures (and income) in rural villages in Thailand and Vietnam between 2007 and 2017. This approach is shown to be able to distinguish between “Decline”, “Stagnation” and “Growth” in these outcomes with an F1 score of up to 63.2% using an Extreme Gradient Boosting Classifier model. In addition, we employ regression-based approaches which achieve an R2 of up to 39.5% when predicting actual changes in these outcomes with an Extreme Gradient Boosting Regressor. Our study demonstrates the feasibility of satellite-based estimates for measuring changes in local socio-economic dynamics.",
        "DOI": "10.1016/j.comcom.2023.02.017",
        "affiliation_name": "Georg-August-Universität Göttingen",
        "affiliation_city": "Gottingen",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Hybridizing qualitative coding with natural language processing and deep learning to assess public comments: A case study of the clean power plan",
        "paper_author": "Ha S.",
        "publication": "Energy Research and Social Science",
        "citied_by": "8",
        "cover_date": "2023-04-01",
        "Abstract": "Public comments are a rich source of data on attitudes toward public policy, but the scale poses major challenges for qualitative analyses. Supervised deep learning and natural language processing potentially enable classification of these data but require high quality labeled inputs. This study investigates whether hybridizing rigorous qualitative coding methods with machine learning approaches can classify large amounts of policy-oriented text data (public comments) while keeping manual effort tractable. Using a convolutional neural network, we evaluate spatiotemporal variation in themes and expressed public attitudes toward a specific US climate policy, the Clean Power Plan (CPP), which was proposed in 2014 and repealed in 2019 without ever taking effect. Public comments were solicited for both the proposal and proposed repeal across eight cities, representing a large and highly targeted dataset on dynamic public attitudes toward the CPP. Rigorous manual coding and data augmentation techniques enabled good model performance (F1 scores of 0.71 and 0.81, respectively, for sentiment and topic classification), even with a very small training set. We find that most speakers supported the CPP despite its eventual repeal and uncover notable rhetorical trends like competing narratives of justice for those affected by climate change versus fossil fuel host communities.",
        "DOI": "10.1016/j.erss.2023.103016",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Atlanta",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Forecasting land use and land cover dynamics using combined remote sensing, machine learning algorithm and local perception in the Agoènyivé Plateau, Togo",
        "paper_author": "Yomo M.",
        "publication": "Remote Sensing Applications: Society and Environment",
        "citied_by": "7",
        "cover_date": "2023-04-01",
        "Abstract": "Knowledge of land use and land cover (LULC) dynamics helps policymakers set sustainable land management plans. Population growth, migration, and rural exodus have led to spatial expansion in many cities worldwide, especially in coastal urban settings. This research aims at assessing and predicting LULC over the Agoènyivé plateau (AP) in Togo, using remote sensing and Markov Chain along with local perceptions on LULC change, thus considering four scenarios (i.e., Business-as Usual, afforestation, wetland policy, and building policy). LULC was classified based on the maximum likelihood algorithm using Landsat images. The future scenarios maps were produced using the Multilayer Perceptron (MLP) neural networks-Markov chain modelling approach by considering five driver variables considered to affect future development. The historical change analysis was performed for the periods of 1986–2001, 2001–2011, and 2011–2020, while 1986–2020 was used for model validation and change prediction. Image accuracy assessment was performed using the error or confusion matrices, kappa coefficients (Kscores), Relative Operating Characteristics (ROC), and accuracy scores, with an overall accuracy (Kappa coefficient) of 87% (0.81), 88% (0.81), 90% (0.82), and 92% (0.84) for the years 1986, 2001, 2011, and 2020, respectively. The results showed an increase in built-up areas (38.71% land gain), while a decrease was observed in mixed vegetation/savannah (22.69% land loss), croplands/bare surfaces (22.89%), and wetlands (3.36% land loss) over the period of 1986–2020. An increase in built-up areas (7.79% and 19.79% land gain by 2030 and 2050, respectively) and a decrease in mixed vegetation (1.13% and 2.54% land loss by 2030 and 2050, respectively), croplands and bare surfaces (6.58% and 16.98% loss by 2030 and 2050, respectively), and wetlands (0.13% and 0.31% loss by 2030 and 2050, respectively), are expected under the BAU scenario. Contrary to the wetland policy, which induces change only in the wetlands (0.02% and 0.047% land loss by 2030 and 2050, respectively), the afforestation scenario results in a gain in forest/savannah (0.11% and 0.73% land gain by 2030 and 2050 compared to the BAU scenario), but also a decrease in croplands/bare surfaces (0.64% by 2050 compared to the BAU scenario), whereas the building policy scenario results in a long-term increase in croplands/bare surfaces (2.88% by 2050, compared to the BAU), and wetlands (0.027 and 2.88% by 2050) in addition to the mixed vegetation/savannah. LULC dynamics (including future urbanization trends) for the AP under various scenarios is a step ahead of time, highlighting the importance of the implementation of existing laws and policies, but also providing information to deal with the observed inharmonious spatial development of urban centres, and serving as a dataset source for LULC change impact assessments on related resources.",
        "DOI": "10.1016/j.rsase.2023.100928",
        "affiliation_name": "Trevecca Nazarene University",
        "affiliation_city": "Nashville",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Dynamic Pricing and Placing for Distributed Machine Learning Jobs: An Online Learning Approach",
        "paper_author": "Zhou R.",
        "publication": "IEEE Journal on Selected Areas in Communications",
        "citied_by": "4",
        "cover_date": "2023-04-01",
        "Abstract": "Nowadays distributed machine learning (ML) jobs usually adopt a parameter server (PS) framework to train models over large-scale datasets. Such ML job deploys hundreds of concurrent workers, and model parameter updates are exchanged frequently between workers and PSs. Current practice is that workers and PSs may be placed on different physical servers, bringing uncertainty in jobs' runtime. Existing cloud pricing policy often charges a fixed price according to the job's runtime. Although this pricing strategy is simple to implement, such pricing mechanism is not suitable for distributed ML jobs whose runtime is stochastic and can only be estimated according to its placement after job admission. To supplement existing cloud pricing schemes, we design a dynamic pricing and placement algorithm, DPS, for distributed ML jobs. DPS aims to maximize the cloud service provider's profit, which dynamically calculates unit resource price upon a job's arrival, and determines job's placement to minimize its runtime if offered price is accepted to users. Our design exploits the multi-armed bandit (MAB) technique to learn unknown information based on past sales. DPS balances the exploration and exploitation stage, and selects the best price based on the reward which is related to job runtime. Our learning-based algorithm can increase the provider's profit by 200%, and achieves a sub-linear regret with both the time horizon and the total job number, compared to benchmark pricing schemes. Extensive evaluations using real-world data also validates the efficacy of DPS.",
        "DOI": "10.1109/JSAC.2023.3242707",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Travel Mode Choice Prediction Using Imbalanced Machine Learning",
        "paper_author": "Chen H.",
        "publication": "IEEE Transactions on Intelligent Transportation Systems",
        "citied_by": "8",
        "cover_date": "2023-04-01",
        "Abstract": "Travel mode choice prediction is critical for travel demand prediction, which influences transport resource allocation and transport policies. Travel modes are often characterised by severe class imbalance and inequality, which leads to the inferior predictive performance of minority modes and bias in travel demand prediction. In existing studies, the class imbalance in travel mode prediction has not been addressed with a general approach. Basic resampling methods were adopted without much investigation, and the performance was assessed by commonly used metrics (e.g., accuracy), which is not suitable for predicting highly imbalanced modes. To this end, this paper proposes an evaluation framework to systematically investigate the combination of six over/undersampling techniques and three prediction methods. In a case study using the London Passenger Mode Choice dataset, results show that applying over/undersampling techniques on travel mode substantially improves the F1 score (i.e., the harmonic mean of precision and recall) of minority classes, without considerably downgrading the overall prediction performance or model interpretation. These findings suggest that combining over/undersampling techniques and statistical/machine-learning methods is appropriate for predicting travel mode, which effectively mitigates the influence of class imbalance while achieving high predictive accuracy and model interpretation. In addition, the combination of over/undersampling techniques and prediction methods enriches the model options for predicting mode choice, which would better support transport planning.",
        "DOI": "10.1109/TITS.2023.3237681",
        "affiliation_name": "Key Laboratory of Road and Traffic Engineering of the State Ministry of Education",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Scheduling and Aggregation Design for Asynchronous Federated Learning Over Wireless Networks",
        "paper_author": "Hu C.H.",
        "publication": "IEEE Journal on Selected Areas in Communications",
        "citied_by": "56",
        "cover_date": "2023-04-01",
        "Abstract": "Federated Learning (FL) is a collaborative machine learning (ML) framework that combines on-device training and server-based aggregation to train a common ML model among distributed agents. In this work, we propose an asynchronous FL design with periodic aggregation to tackle the straggler issue in FL systems. Considering limited wireless communication resources, we investigate the effect of different scheduling policies and aggregation designs on the convergence performance. Driven by the importance of reducing the bias and variance of the aggregated model updates, we propose a scheduling policy that jointly considers the channel quality and training data representation of user devices. The effectiveness of our channel-aware data-importance-based scheduling policy, compared with state-of-the-art methods proposed for synchronous FL, is validated through simulations. Moreover, we show that an 'age-aware' aggregation weighting design can significantly improve the learning performance in an asynchronous FL setting.",
        "DOI": "10.1109/JSAC.2023.3242719",
        "affiliation_name": "Linköpings Universitet",
        "affiliation_city": "Linkoping",
        "affiliation_country": "Sweden"
    },
    {
        "paper_title": "Carbon dioxide emission typology and policy implications: Evidence from machine learning",
        "paper_author": "Wang H.",
        "publication": "China Economic Review",
        "citied_by": "15",
        "cover_date": "2023-04-01",
        "Abstract": "The policy design of carbon dioxide (CO2) emission mitigation is a hotly debated topic in the context of “Carbon Peak and Carbon Neutrality” in China. This paper contributes to this debate by employing an unsupervised machine learning algorithm to uncover the CO2 emission typology based on the provincial emission data in China from 2000 to 2018 for a precise design of CO2 emission mitigation policy for heterogenous regional patterns. The results indicate that we can cluster the provinces into four CO2 emission patterns: the under-developed pattern, the coal-dominated pattern, the oil-dominated pattern, and the gas-dominated pattern. Notably, both the under-developed pattern and the coal-dominated pattern have a large amount of CO2 emission from fossil fuels, while the gas-dominated pattern could be regarded as the policy inclination as it relies more on low-carbon fuels. Moreover, we also reveal the transition routes of emission patterns from a dynamic perspective, which could help policymakers better understand the future trend of emission patterns in different regions. On the one hand, the CO2 emission mitigation policies could have specified priorities in different patterns, ensuring the feasibility during the process of policy implementation. On the other hand, establishing a national unified carbon trade market could facilitate efficient energy transition in China, and prevent carbon leakage cross different regions as well.",
        "DOI": "10.1016/j.chieco.2023.101941",
        "affiliation_name": "Southwest University",
        "affiliation_city": "Chongqing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A novel prediction model based on decomposition-integration and error correction for COVID-19 daily confirmed and death cases",
        "paper_author": "Yang H.",
        "publication": "Computers in Biology and Medicine",
        "citied_by": "12",
        "cover_date": "2023-04-01",
        "Abstract": "Coronavirus disease (COVID-19) has infected billion people around the world and affected the economy, but most countries are considering reopening, so the COVID-19 daily confirmed and death cases have increased greatly. It is very necessary to predict the COVID-19 daily confirmed and death cases in order to help every country formulate prevention policies. To enhance the prediction performance, this paper proposes a prediction model based on improved variational mode decomposition by sparrow search algorithm (SVMD), improved kernel extreme learning machine by Aquila optimizer algorithm (AO-KELM) and error correction idea, named SVMD-AO-KELM-error for short-term prediction of COVID-19 cases. Firstly, to solve mode number and penalty factor selection of variational mode decomposition (VMD), an improved VMD based on sparrow search algorithm (SSA), named SVMD, is proposed. SVMD decomposes the COVID-19 case data into some intrinsic mode function (IMF) components and residual is considered. Secondly, to properly selected regularization coefficients and kernel parameters of kernel extreme learning machine (KELM) and improve the prediction performance of KELM, an improved KELM by Aquila optimizer (AO) algorithm, named AO-KELM, is proposed. Each component is predicted by AO-KELM. Then, the prediction error of IMF and residual are predicted by AO-KELM to correct prediction results, which is error correction idea. Finally, prediction results of each component and error prediction results are reconstructed to get final prediction results. Through the simulation experiment of the COVID-19 daily confirmed and death cases in Brazil, Mexico, and Russia and comparison with twelve comparative models, simulation experiment gives that SVMD-AO-KELM-error has best prediction accuracy. It also proves that the proposed model can be used to predict the pandemic COVID-19 cases and offers a novel approach for COVID-19 cases prediction.",
        "DOI": "10.1016/j.compbiomed.2023.106674",
        "affiliation_name": "Xi'an Institute of Posts and Telecommunications",
        "affiliation_city": "Xi'an",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Traffic-aware service relocation in software-defined and intent-based elastic optical networks",
        "paper_author": "Goścień R.",
        "publication": "Computer Networks",
        "citied_by": "14",
        "cover_date": "2023-04-01",
        "Abstract": "The paper focuses on the efficient dynamic routing of unicast and data center (DC)-related requests in elastic optical networks (EONS) implementing software-defined networking (SDN) and intend-based networking (IBN) paradigms. To improve the network performance (measured as a ratio of the accepted traffic), we apply the service relocation (i.e., the adaptive process of changing the assigned DC for an anycast request). To enable a realistic case study, we propose a novel traffic model for DC-oriented and intent-based transport networks. The model reflects patterns observed in real networks and relates them with the economic and demographic parameters of the cities associated with network nodes. Then, we propose a dedicated allocation algorithm and introduce 21 different service relocation policies. These are traffic- and network-aware approaches, which use three data types for decision-making — traffic prediction, bit-rate rejection history, and topological network characteristics. Finally, we perform extensive simulations to: (i) tune the proposed optimization approaches, (ii) compare their efficiency and select the best one, (iii) determine benefits provided by the traffic- and network-aware service relocation in SDN/IBN optical networks. The results prove a high efficiency of the proposed policies, which allowed to serve up to 5.39% more traffic compared to the network with the fixed DC assignment. They also reveal that the most efficient policy makes its decisions based on traffic prediction.",
        "DOI": "10.1016/j.comnet.2023.109660",
        "affiliation_name": "Politechnika Wrocławska",
        "affiliation_city": "Wroclaw",
        "affiliation_country": "Poland"
    },
    {
        "paper_title": "Predicting the issuance of COVID-19 stay-at-home orders in Africa: Using machine learning to develop insight for health policy research",
        "paper_author": "Mansell J.",
        "publication": "International Journal of Disaster Risk Reduction",
        "citied_by": "2",
        "cover_date": "2023-04-01",
        "Abstract": "During the COVID-19 pandemic, many countries have issued stay-at-home orders (SAHOs) to reduce viral transmission. Because of their social and economic consequences, SAHOs are a politically risky decision for governments. Researchers typically attribute public health policymaking to five theoretically significant factors: political, scientific, social, economic, and external. However, a narrow focus on extant theory runs the risk of biasing findings and missing novel insights. This research employs machine learning to shift the focus from theory to data to generate hypotheses and insights “born from the data” and unconstrained by current knowledge. Beneficially, this approach can also confirm the extant theory. We apply machine learning in the form of a random forest classifier to a novel and multiple-domain data set of 88 variables to identify the most significant predictors of the issuance of a COVID-19-related SAHO in African countries (n = 54). Our data set includes a wide range of variables from sources such as the World Health Organization that cover the five principal theoretical factors and previously ignored domains. Generated using 1000 simulations, our model identifies a combination of theoretically significant and novel variables as the most important to the issuance of a SAHO and has a predictive accuracy using 10 variables of 78%, which represents a 56% increase in accuracy compared to simply predicting the modal outcome.",
        "DOI": "10.1016/j.ijdrr.2023.103598",
        "affiliation_name": "Augusta University",
        "affiliation_city": "Augusta",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Predictive trajectory planning for autonomous vehicles at intersections using reinforcement learning",
        "paper_author": "Zhang E.",
        "publication": "Transportation Research Part C: Emerging Technologies",
        "citied_by": "30",
        "cover_date": "2023-04-01",
        "Abstract": "In this work we put forward a predictive trajectory planning framework to help autonomous vehicles plan future trajectories. We develop a partially observable Markov decision process (POMDP) to model this sequential decision making problem, and a deep reinforcement learning solution methodology to learn high-quality policies. The POMDP model utilizes driving scenarios, condensed into graphs, as inputs. More specifically, an input graph contains information on the history trajectory of the subject vehicle, predicted trajectories of other agents in the scene (e.g., other vehicles, pedestrians, and cyclists), as well as predicted risk levels posed by surrounding vehicles to devise safe, comfortable, and energy-efficient trajectories for the subject vehicle to follow. In order to obtain sufficient driving scenarios to use as training data, we propose a simulation framework to generate socially acceptable driving scenarios using a real world autonomous vehicle dataset. The simulation framework utilizes Bayesian Gaussian mixture models to learn trajectory patterns of different agent types, and Gibbs sampling to ensure that the distribution of simulated scenarios matches that of the real-world dataset collected by an autonomous fleet. We evaluate the proposed work in two complex urban driving environments: a non-signalized T-junction and a non-signalized lane merge intersection. Both environments provide vastly more complex driving scenarios compared to a highway driving environment, which has been mostly the focus of previous studies. The framework demonstrates promising performance for planning horizons as long as five seconds. We compare safety, comfort, and energy efficiency of the planned trajectories against human-driven trajectories in both experimental driving environments, and demonstrate that it outperforms human-driven trajectories in a statistically significant fashion in all aspects.",
        "DOI": "10.1016/j.trc.2023.104063",
        "affiliation_name": "University of Michigan, Ann Arbor",
        "affiliation_city": "Ann Arbor",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Concepts of circular economy for sustainable management of electronic wastes: challenges and management options",
        "paper_author": "Srivastav A.L.",
        "publication": "Environmental Science and Pollution Research",
        "citied_by": "73",
        "cover_date": "2023-04-01",
        "Abstract": "Abstract: The electronic and electrical industrial sector is exponentially growing throughout the globe, and sometimes, these wastes are being disposed of and discarded with a faster rate in comparison to the past era due to technology advancements. As the application of electronic devices is increasing due to the digitalization of the world (IT sector, medical, domestic, etc.), a heap of discarded e-waste is also being generated. Per-capita e-waste generation is very high in developed countries as compared to developing countries. Expansion of the global population and advancement of technologies are mainly responsible to increase the e-waste volume in our surroundings. E-waste is responsible for environmental threats as it may contain dangerous and toxic substances like metals which may have harmful effects on the biodiversity and environment. Furthermore, the life span and types of e-waste determine their harmful effects on nature, and unscientific practices of their disposal may elevate the level of threats as observed in most developing countries like India, Nigeria, Pakistan, and China. In the present review paper, many possible approaches have been discussed for effective e-waste management, such as recycling, recovery of precious metals, adopting the concepts of circular economy, formulating relevant policies, and use of advance computational techniques. On the other hand, it may also provide potential secondary resources valuable/critical materials whose primary sources are at significant supply risk. Furthermore, the use of machine learning approaches can also be useful in the monitoring and treatment/processing of e-wastes. Highlights: In 2019, ~ 53.6 million tons of e-wastes generated worldwide. Discarded e-wastes may be hazardous in nature due to presence of heavy metal compositions. Precious metals like gold, silver, and copper can also be procured from e-wastes. Advance tools like artificial intelligence/machine learning can be useful in the management of e-wastes. Graphical abstract: [Figure not available: see fulltext.]",
        "DOI": "10.1007/s11356-023-26052-y",
        "affiliation_name": "Pannalal Girdharlal Dayanand Anglo-Vedic (PGDAV) College",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Improvement of spatial estimation for soil organic carbon stocks in Yuksekova plain using Sentinel 2 imagery and gradient descent–boosted regression tree",
        "paper_author": "Budak M.",
        "publication": "Environmental Science and Pollution Research",
        "citied_by": "6",
        "cover_date": "2023-04-01",
        "Abstract": "Carbon sequestration in earth surface is higher than the atmosphere, and the amount of carbon stored in wetlands is much greater than all other land surfaces. The purpose of this study was to estimate soil organic carbon stocks (SOCS) and investigate spatial distribution pattern of Yuksekova wetlands and surrounding lands in Hakkari province of Turkey using machine learning and remote sensing data. Disturbed and undisturbed soil samples were collected from 10-cm depth in 50 locations differed with land use and land cover. Vegetation, soil, and moisture indices were calculated using Sentinel 2 Multispectral Sensor Instrument (MSI) data. Significant correlations (p≤0.01) were obtained between the indices and SOCS; thus, the remote sensing indices (ARVI 0.43, BI −0.43, GSI −0.39, GNDI 0.44, NDVI 0.44, NDWI 0.38, and SRCI 0.51) were used as covariates in multi-layer perceptron neural network (MLP) and gradient descent–boosted regression tree (GBDT) machine learning models. Mean absolute error, root mean square error, and mean absolute percentage error were 3.94 (Mg C ha −1), 6.64 (Mg C ha−1), and 9.97%, respectively. The simple ratio clay index (SRCI), which represents the soil texture, was the most important factor in the SOCS estimation variance. In addition, the relationship between SRCI and Topsoil Grain Size Index revealed that topsoil clay content is a highly important parameter in spatial variation of SOCS. The spatial SOCS values obtained using the GBDT model and the mean SOCS values of the CORINE land cover classes were significantly different. The land cover has a significant effect on SOC in Yuksekova plain. The mean SOCS for continuously ponded fields was 45.58 Mg C ha−1, which was significantly different from the mean SOCS of arable lands. The mean SOCS in arable lands, with significant areas of natural vegetation, was 50.22 Mg C ha−1 and this amount was significantly higher from the SOCS of other land covers (p<0.01). The wetlands had the highest SOCS (61.46 Mg C ha−1), followed by the lands principally occupied by natural vegetation and used as rangelands around the wetland (50.22 Mg C ha−1). Environmental conditions had significant effect on SOCS in the study area. The use of remote sensing indices instead of using single bands as estimators in the GBDT algorithm minimized radiometric errors, and reliable spatial SOCS information was obtained by using the estimators. Therefore, the spatial estimation of SOCS can be successfully determined with up-to-date machine learning algorithms only using remote sensing predictor variables. Reliable estimation of SOCS in wetlands and surrounding lands can help understand policy and decision makers the importance of wetlands in mitigating the negative impacts of global warming.",
        "DOI": "10.1007/s11356-023-26064-8",
        "affiliation_name": "Siirt Üniversitesi",
        "affiliation_city": "Siirt",
        "affiliation_country": "Turkey"
    },
    {
        "paper_title": "Maintenance in the downstream petroleum industry: A review on methodology and implementation",
        "paper_author": "Wari E.",
        "publication": "Computers and Chemical Engineering",
        "citied_by": "8",
        "cover_date": "2023-04-01",
        "Abstract": "This paper presents a literature review on maintenance operations in the downstream petroleum industry. This process industry comprises facilities ranging from processing units to distribution networks, which makes the maintenance activities diverse. Maintenance optimization approaches from over 120 articles have been organized into two broad categories. The first category implemented maintenance operations according to the criticality of equipment by applying methods like American Petroleum Institute, Analytical Hierarchy Process, and Failure Modes and Effect Analysis. The second category applied various optimal policies by adopting different tools such as mathematical models (probability, statistics, linear or nonlinear optimization methods, fuzzy logic), heuristic (metaheuristics) algorithms (genetic algorithm, firefly algorithm), data analytics (machine learning), and Internet of Things. The review also included maintenance implementation frameworks, planning & scheduling methods, safety, mechanization, and evaluation procedures. It also tracks the recent trends in the maintenance implementation approach, identifies gaps, and recommends future research directions.",
        "DOI": "10.1016/j.compchemeng.2023.108177",
        "affiliation_name": "Cullen College of Engineering",
        "affiliation_city": "Houston",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Energy scheduling for DoS attack over multi-hop networks: Deep reinforcement learning approach",
        "paper_author": "Yang L.",
        "publication": "Neural Networks",
        "citied_by": "6",
        "cover_date": "2023-04-01",
        "Abstract": "This paper studies the energy scheduling for Denial-of-Service (DoS) attack against remote state estimation over multi-hop networks. A smart sensor observes a dynamic system, and transmits its local state estimate to a remote estimator. Due to the limited communication range of the sensor, some relay nodes are employed to deliver data packets from the sensor to the remote estimator, which constitutes a multi-hop network. To maximize the estimation error covariance with energy constraint, a DoS attacker needs to determine the energy level implemented on each channel. This problem is formulated as an associated Markov decision process (MDP), and the existence of an optimal deterministic and stationary policy (DSP) is proved for the attacker. Besides, a simple threshold structure of the optimal policy is obtained, which significantly reduces the computational complexity. Furthermore, an up-to-date deep reinforcement learning (DRL) algorithm, dueling double Q-network (D3QN), is introduced to approximate the optimal policy. Finally, a simulation example illustrates the developed results and verifies the effectiveness of D3QN for optimal DoS attack energy scheduling.",
        "DOI": "10.1016/j.neunet.2023.02.028",
        "affiliation_name": "Concordia University",
        "affiliation_city": "Montreal",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Forecasting Stock Market Crashes via Machine Learning",
        "paper_author": "Dichtl H.",
        "publication": "Journal of Financial Stability",
        "citied_by": "16",
        "cover_date": "2023-04-01",
        "Abstract": "This paper uses a comprehensive set of predictor variables from the five largest Eurozone countries to compare the performance of simple univariate and machine learning-based multivariate models in forecasting stock market crashes. In terms of statistical predictive performance, a support vector machine-based crash prediction model outperforms a random classifier and is superior to the average univariate benchmark as well as a multivariate logistic regression model. Incorporating nonlinear and interactive effects is both imperative and foundation for the outperformance of support vector machines. Their ability to forecast stock market crashes out-of-sample translates into substantial value-added to active investors. From a policy perspective, the use of machine learning-based crash prediction models can help activate macroprudential tools in time.",
        "DOI": "10.1016/j.jfs.2022.101099",
        "affiliation_name": "Universität Hamburg, Fakultät für Betriebswirtschaft",
        "affiliation_city": "Hamburg",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Hybrid forecasting system considering the influence of seasonal factors under energy sustainable development goals",
        "paper_author": "Li G.",
        "publication": "Measurement: Journal of the International Measurement Confederation",
        "citied_by": "11",
        "cover_date": "2023-04-01",
        "Abstract": "As environmental concerns and the energy crisis intensify, advancing the use of new energy sources such as wind and photovoltaic is an important means to achieve sustainable energy development. However, the random fluctuation characteristics and seasonality of wind and light make them difficult to predict, which brings many operational risks to grid security and power dispatch. Therefore, this paper uses the Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN) as the decomposition algorithm, which can effectively avoid the problems of modal confusion and noise residuals and better identify the trend, seasonality and nonlinear characteristics of the series. The Particle Swarm Optimization (PSO) algorithm is also utilized to improve the prediction accuracy and stability of the Extreme Learning Machine (ELM). Combining CEEMDAN and PSO-ELM models, a high-precision hybrid forecasting system that can effectively reduce the effects of stochastic fluctuations is proposed. In this paper, wind speed data from four different seasons at Changma wind farm in China are selected to verify the effectiveness and generalization ability of this hybrid prediction system. The results show that the improvement of ELM by PSO and CEEMDAN significantly improves the prediction accuracy of the model, and the hybrid prediction system can be applied to time series with different data characteristics and variation patterns in each season, and its prediction accuracy and model performance are significantly better than other comparative models.",
        "DOI": "10.1016/j.measurement.2023.112607",
        "affiliation_name": "North China Electric Power University (Baoding)",
        "affiliation_city": "Baoding",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Groundwater quality assessment by multi-model comparison: a comprehensive study during dry and wet periods in semi-arid regions",
        "paper_author": "Wang Z.",
        "publication": "Environmental Science and Pollution Research",
        "citied_by": "2",
        "cover_date": "2023-04-01",
        "Abstract": "With the impact of human engineering activities, groundwater pollution has seriously threatened the health of human life. Accurate water quality assessment is the basis of controlling groundwater pollution and improving groundwater management, especially in specific regions. A typical semi-arid city in Fuxin Province of China is taken as an example. We use remote sensing and GIS to compile four environmental factors, such as rainfall, temperature, LULC, and NDVI, to analyze and screen the correlation of indicators. The differences among the four algorithms were compared by using hyperparameters and model interpretability, including random forest (RF), support vector machine support vector machine (SVM), decision tree (DT), and K-nearest neighbor (KNN). The groundwater quality of the city during the dry and wet periods was comprehensively evaluated. The results show that the RF model has higher integrated precision (MSE = 0.11, 0.035; RMSE = 0.19,0.188; R2 = 0.829,0.811; ROC = 0.98, 0.98). The quality of shallow groundwater is poor in general, 29%, 38%, 33% of the groundwater quality in low-water period is III, IV, V water. Thirty-three percent and 67% of the groundwater quality in the high-water period were IV and V water. The proportion of poor water quality in high-water period was higher than that in low-water period, which was consistent with the actual investigation. This study provides a kind of machine learning method for the semi-arid area, which cannot only promote the sustainable development of groundwater in this area, but also provide reference for the management policy of related departments.",
        "DOI": "10.1007/s11356-023-25937-2",
        "affiliation_name": "Henan University of Urban Construction",
        "affiliation_city": "Pingdingshan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A Resource Allocation Policy for Downlink Communication in Distributed IRS Aided Multiple-Input Single-Output Systems",
        "paper_author": "Ferdouse L.",
        "publication": "IEEE Transactions on Communications",
        "citied_by": "8",
        "cover_date": "2023-04-01",
        "Abstract": "As a technology for 6G wireless communications, Intelligent Reflecting Surfaces (IRSs) are considered as a promising solution to boost the network capacity, spectrum and coverage in multiusers' downlink communication systems. The users in blockage and cell edge areas can utilize this technology for data transfer purpose. In this paper, a machine learning-based policy optimization for downlink communication in distributed IRS aided multiple-input single-output (MISO) systems is proposed. Three categories of users are considered, namely, users who can utilize only the direct links, blockage area users who can utilize only the IRS links, and cell edge or poor link quality of users who can utilize both the direct and IRS links. The sum rate maximization problem is formulated to derive the optimal policy (i.e. communication link, IRS selection, power allocation and reflection coefficients) for those users, considering the IRS selection, link quality, power allocation and IRS reflection constraints. The proposed methods to achieve the optimal policy include reinforcement learning-based model with binary decision tree-based user categories, maximum posterior probability-based IRS selection, fractional programming method-based power and IRS coefficient allocation, and value function-based policy optimization. Through simulations, the sum data rate and energy efficiency performances of different categories of users are obtained and discussed.",
        "DOI": "10.1109/TCOMM.2023.3242352",
        "affiliation_name": "Graduate School of Informatics",
        "affiliation_city": "Kyoto",
        "affiliation_country": "Japan"
    },
    {
        "paper_title": "Prospective predictors of electronic nicotine delivery system initiation in tobacco naive young adults: A machine learning approach",
        "paper_author": "Atuegwu N.C.",
        "publication": "Preventive Medicine Reports",
        "citied_by": "5",
        "cover_date": "2023-04-01",
        "Abstract": "The use of electronic nicotine delivery systems (ENDS) is increasing among young adults. However, there are few studies regarding predictors of ENDS initiation in tobacco-naive young adults. Identifying the risk and protective factors of ENDS initiation that are specific to tobacco-naive young adults will enable the creation of targeted policies and prevention programs. This study used machine learning (ML) to create predictive models, identify risk and protective factors for ENDS initiation for tobacco-naive young adults, and the relationship between these predictors and the prediction of ENDS initiation. We used nationally representative data of tobacco-naive young adults in the U.S drawn from the Population Assessment of Tobacco and Health (PATH) longitudinal cohort survey. Respondents were young adults (18–24 years) who had never used any tobacco products in Wave 4 and who completed Waves 4 and 5 interviews. ML techniques were used to create models and determine predictors at 1-year follow-up from Wave 4 data. Among the 2,746 tobacco-naive young adults at baseline, 309 initiated ENDS use at 1-year follow-up. The top five prospective predictors of ENDS initiation were susceptibility to ENDS, increased days of physical exercise specifically designed to strengthen muscles, frequency of social media use, marijuana use and susceptibility to cigarettes. This study identified previously unreported and emerging predictors of ENDS initiation that warrant further investigation and provided comprehensive information on the predictors of ENDS initiation. Furthermore, this study showed that ML is a promising technique that can aid ENDS monitoring and prevention programs.",
        "DOI": "10.1016/j.pmedr.2023.102148",
        "affiliation_name": "Connecticut Mental Health Center",
        "affiliation_city": "New Haven",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Fuzzy Q-learning approach for autonomic resource provisioning of IoT applications in fog computing environments",
        "paper_author": "Faraji-Mehmandar M.",
        "publication": "Journal of Ambient Intelligence and Humanized Computing",
        "citied_by": "4",
        "cover_date": "2023-04-01",
        "Abstract": "The dramatic growth of smart devices and the Internet of things (IoT) has increased the volume of exchanges and data on the web. The centralized and traditional architecture of cloud computing does not meet the demands of users and proper implementation of latency-sensitive applications due to latency and mass demands of IoT applications that have different needs compared to existing applications. As a result, edge computing has been presented for collecting and processing of data generated by these objects, which facilitates data processing with low latency and close to users at the edge of the network. Its main purpose is to bring computational resources and storage close to the end-user on the network. As far as the storage capacity of fog nodes is limited, the proper use of fog node resources significantly influences their performance. In this paper, a framework based on control MAPE-K loop has been used to obtain the optimal state in workload balance. The users’ workload forecasting model is a combination of linear regression and support vector regression methods and offers better performance compared to the conventional reactive self-assessment methods. In the planning phase, a fuzzy self-learning algorithm is used to determine the automated scale of resource provisioning policy. By comparing three criteria of load delay, cost, and amount of consumed energy in the proposed method and recent works, the proposed method has been able to balance all three criteria optimally.",
        "DOI": "10.1007/s12652-023-04527-7",
        "affiliation_name": "Islamic Azad University, Parand Branch",
        "affiliation_city": "Parand",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "Deep learning reveals rapid vegetation greening in changing climate from 1988 to 2018 on the Qinghai-Tibet Plateau",
        "paper_author": "Lou P.",
        "publication": "Ecological Indicators",
        "citied_by": "25",
        "cover_date": "2023-04-01",
        "Abstract": "Vegetation dynamics in Qinghai-Tibet Plateau (QTP) have been debated in recent decades. Most studies suggest that wetter and warmer climatic conditions would release low temperature constraints and stimulate alpine vegetation growth. Other studies suggest that climate warming might inhibit vegetation growth by increasing soil moisture depletion in the southern QTP. Most of previous studies have relied on vegetation indices derived from satellite observations to retrieve large-scale vegetation changes, and the uncertainty of vegetation indices makes it difficult to accurately characterize the vegetation trends on the QTP. Here, we developed a deep learning algorithm in the Google Earth Engine (GEE) platform to accurately map the land use/cover change (LUCC) on the QTP, and then infer vegetation gain and loss and their drivers during the period 1988–2018. The vegetation on the QTP experienced rapid greening, which was dominated by transitions from bareland to alpine grassland (27.45 × 104 km2) and from alpine grassland to alpine meadow (17.43 × 104 km2) during 1988–2018. Furthermore, although human activities influence vegetation succession at the local scale, the dominant influencing factors affecting vegetation greening on the QTP are precipitation (q-statistic = 23.87 %) and temperature (q-statistic = 11.01 %). A 30-year time series analysis clarified the specific dynamics of vegetation on the QTP, thus contributing to the understanding of the response mechanisms of alpine vegetation under climate change and providing a reference for the formulating of reasonable ecological protection policies and human development strategies.",
        "DOI": "10.1016/j.ecolind.2023.110020",
        "affiliation_name": "Northwest Institute of Eco-Environment and Resources",
        "affiliation_city": "Lanzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A decentralized adaptation of model-free Q-learning for thermal-aware energy-efficient virtual machine placement in cloud data centers",
        "paper_author": "Aghasi A.",
        "publication": "Computer Networks",
        "citied_by": "19",
        "cover_date": "2023-04-01",
        "Abstract": "The traditional method of saving energy in Virtual Machine Placement (VMP) is based on consolidating more virtual machines (VMs) in fewer servers and putting the rest in sleep mode, which may lead to the overheating of servers resulting in performance degradation and cooling cost. The lack of an accurate and computationally efficient model to describe the thermal condition of the data center environment makes it challenging to develop an effective and adaptive VMP mechanism. Although recently, data-driven approaches have acted successfully in model construction, the shortage of clean, adequate, and sufficient amounts of data put limits their generalizability. Moreover, any change in the data center configuration during operation, makes these models prone to error and forces them to repeat the learning process. Thus, researchers turn to applying model-free paradigms such as reinforcement learning. Due to the vast action-state space of real-world applications, scalability is one of the significant challenges in this area. In addition, the delayed feedback of environmental variables such as temperature give rise to exploration costs. In this paper, we present a decentralized implementation of reinforcement learning along with a novel state-action representation to perform the VMP in the data centers to optimize energy consumption and keep the host temperature as low as possible while satisfying Service Level Agreements (SLA). Our experimental results show more than 17% improvement in energy consumption and 12% in CPU temperature reduction compared to baseline algorithms. We also succeeded in accelerating optimal policy convergence after the occurrence of a configuration change.",
        "DOI": "10.1016/j.comnet.2023.109624",
        "affiliation_name": "University of Isfahan",
        "affiliation_city": "Isfahan",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "What are the trend and core knowledge of information security? A citation and co-citation analysis",
        "paper_author": "Shiau W.L.",
        "publication": "Information and Management",
        "citied_by": "23",
        "cover_date": "2023-04-01",
        "Abstract": "Information technology brings business success opportunities, but also causes potential safety hazards to organizations. In response to the increasing academia and industry concerns regarding information security (ISec), this study systematically explored extant ISec research and identified eight core knowledge groups, including (1) intrusion detection, (2) privacy protection, (3) secure machine learning, (4) cryptosystem, (5) data service security, (6) malware analysis, (7) security decision-making, and (8) security management. The detection of research hotspots shows that data service security and risk management garner the most current research attention. Furthermore, we establish a comprehensive ISec framework to help systematically understand and achieve ISec.",
        "DOI": "10.1016/j.im.2023.103774",
        "affiliation_name": "College of Management",
        "affiliation_city": "Taoyuan",
        "affiliation_country": "Taiwan"
    },
    {
        "paper_title": "Deep learning and benchmark machine learning based landslide susceptibility investigation, Garhwal Himalaya (India)",
        "paper_author": "Saha S.",
        "publication": "Quaternary Science Advances",
        "citied_by": "30",
        "cover_date": "2023-04-01",
        "Abstract": "Garhwal Himalaya is the worst affected landslide prone region in Indian subcontinent mainly due to its complex geological settings and active tectonic activities. The data showed that every year, around 400 fatalities occur in Himalayan terrain due to landslide. In the current study, we have mapped the landslide susceptibility zones in the segment of Garhwal Himalaya using robust machine and deep learning algorithms. Individual machine and deep learning models have its own limitations like low generation capacity with nonlinear functions to describe the intricate relationship among predictors. In this study total five models i.e., SVM (Support Vector Machine), RF (Random Forest), bagging, ANN (Artificial Neural Network), DLNN (Deep Learning Neural Network) have been used along with twenty landslide controlling factors. Here, the principal objective of the study is to precisely delineate landslide susceptibility zones of the Garhwal Himalaya. The selecting factors have been considered through multi-collinearity test and information gain ratio statistics and the previous landslide points have been taken as training (70%) and testing (30%) dataset. According to area under curve value (AUC), the DLNN technique has high capability (AUC = 0.925) and accuracy for landslide area demarcation. The approach of integrated physical and social factors creates more precise prediction aptitude that can support large scale landslide management. These high precision models identified most of the parts of Rudraprayag and Tehri Garhwal as a very high landslide susceptibility zone. The generated maps can assist to policy makers for micro scale landslide management and sustainable land use planning particularly in Himalayan terrain.",
        "DOI": "10.1016/j.qsa.2023.100075",
        "affiliation_name": "Sidho-Kanho-Birsha University",
        "affiliation_city": "Purulia",
        "affiliation_country": "India"
    },
    {
        "paper_title": "A hybrid deep leaning model for prediction and parametric sensitivity analysis of noise annoyance",
        "paper_author": "Tiwari S.K.",
        "publication": "Environmental Science and Pollution Research",
        "citied_by": "13",
        "cover_date": "2023-04-01",
        "Abstract": "Noise annoyance is recognized as an expression of physiological and psychological strain in acoustical environment. The studies on prediction of noise annoyance and parametric sensitivity analysis of factors affecting it have been rarely reported in India. A hybrid ConvLSTM technique was developed in the study to predict traffic-induced noise annoyance in 484 people based on ambient noise levels, as well as survey information. Ambient noise levels were obtained at different locations of Dhanbad city using sound level meter at varying intervals, viz. 09AM–12PM, 03PM–06PM, and 08PM–11PM. The proposed method was compared with some well-known neural network techniques such as K-nearest neighbors (KNN), artificial neural network (ANN), recurrent neural network (RNN), and long-short-term memory (LSTM). The experimental results indicate that the proposed method outperforms other techniques and can be a reliable approach for prediction of noise annoyance with an accuracy of 93.8%. It can be concluded from noise maps that the noise levels in all locations of the Dhanbad city were higher than 70 dB(A) and noise sensitivity is the most important input variable of traffic-induced noise annoyance, followed by honking noise, education, exposure hours, LAeq, sleeping disorder, and chronic disease. The study shall facilitate in developing a decision support tool for prediction of noise annoyance and promoting implementation of suitable public policy in urban cities.",
        "DOI": "10.1007/s11356-023-25509-4",
        "affiliation_name": "Saudi Electronic University",
        "affiliation_city": "Riyadh",
        "affiliation_country": "Saudi Arabia"
    },
    {
        "paper_title": "Comprehensive review on ML-based RIS-enhanced IoT systems: basics, research progress and future challenges",
        "paper_author": "Das S.K.",
        "publication": "Computer Networks",
        "citied_by": "44",
        "cover_date": "2023-04-01",
        "Abstract": "Sixth generation (6G) internet of things (IoT) networks will modernize the applications and satisfy user demands through implementing smart and automated systems. Intelligence-based infrastructure, also called reconfigurable intelligent surfaces (RISs), have been introduced as a potential technology striving to improve system performance in terms of data rate, latency, reliability, availability, and connectivity. A huge amount of cost-effective passive components are included in RISs to interact with the impinging electromagnetic waves in a smart way. However, there are still some challenges in RIS system, such as finding the optimal configurations for a large number of RIS components. In this paper, we first provide a complete outline of the advancement of RISs along with machine learning (ML) algorithms and overview the working regulations as well as spectrum allocation in intelligent IoT systems. Also, we discuss the integration of different ML techniques in the context of RIS, including deep reinforcement learning (DRL), federated learning (FL), and FL-deep deterministic policy gradient (FL-DDPG) techniques which are utilized to design the radio propagation atmosphere without using pilot signals or channel state information (CSI). Additionally, in dynamic intelligent IoT networks, the application of existing integrated ML solutions to technical issues like user movement and random variations of wireless channels are surveyed. Finally, we present the main challenges and future directions in integrating RISs and other prominent methods to be applied in upcoming IoT networks.",
        "DOI": "10.1016/j.comnet.2023.109581",
        "affiliation_name": "Military Institute of Science and Technology",
        "affiliation_city": "Dhaka",
        "affiliation_country": "Bangladesh"
    },
    {
        "paper_title": "BFLS: Blockchain and Federated Learning for sharing threat detection models as Cyber Threat Intelligence",
        "paper_author": "Jiang T.",
        "publication": "Computer Networks",
        "citied_by": "27",
        "cover_date": "2023-04-01",
        "Abstract": "Recently, Cyber Threat Intelligence (CTI) sharing has become an important weapon for cyber defenders to mitigate the increasing number of cyber attacks in a proactive and collaborative manner. However, with the dramatic increase in the deployment of shared communications between organizations, data has been a major priority to detect threats in the CTI sharing platform. In the modern environment, a valuable asset is the user's threat data. Privacy policies are necessary to ensure the security of user data in the threat intelligence sharing community. Federated learning acts as a special machine learning technique for privacy preservation and offers to contextualize data in a CTI sharing platform. Therefore, this article proposes a new approach to threat intelligence sharing called BFLS (Blockchain and Federated Learning for sharing threat detection models as Cyber Threat Intelligence), where blockchain-based CTI sharing platforms are used for security and privacy. Federated learning technology is adopted for scalable machine learning applications, such as threat detection. Furthermore, users can obtain a well-trained threat detection model without sending personal data to the central server. Experimental results on the ISCX-IDS-2012 and CIC-DDoS-2019 datasets showed that BFLS can securely share CTI and has high accuracy in threat detection. The accuracies of BFLS are 98.92% and 98.56% on the two datasets, respectively.",
        "DOI": "10.1016/j.comnet.2023.109604",
        "affiliation_name": "South China Normal University",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Design of an IoT platform for data analytics based fault detection and classification in solar PV power plants using CFKC and ODENN",
        "paper_author": "Raj S.",
        "publication": "International Journal of Modeling, Simulation, and Scientific Computing",
        "citied_by": "0",
        "cover_date": "2023-04-01",
        "Abstract": "New policies are commenced all over the globe to diminish the use of fossil fuels, which gives rise to the augmented utilization of solar energy (SE). The photovoltaic (PV) system's performance is extremely environmental variables reliant. Long-range transmission of SE is incompetent as well as complex to carry in the PV system. It can be affected by disparate sorts of faults, which cause severe energy loss all through the system operation. Thus, it is vital to incessantly monitor the solar PV (SPV) system to detect as well classify the faults by preventing energy losses. The IoT applications in SE production engage sensor devices that are fixed to the generation, and transmission, together with distribution equipment. These devices assist in monitoring the operation of the SPV power plant (SPVPP) system remotely in real-time. Presenting a new algorithm that can perform fault detection and classification in a PV system to a higher level of accuracy is the major contribution of this work. Thus, this work designs as well as develops an IoT platform for carrying out analytical tasks that can analyze data generated as of IoT operating systems to detect as well as classify faults in the SPVPP. Initially, the data collected from the dataset is pre-processed in which data duplication is performed using Hadoop distributed file system (HDFS) and then the fault is detected from the pre-processed data using the cosine function based k-means clustering (CFKC) technique in the SPV system. Finally, the obtained fault data is fed into the optimized deep learning centered ENN (ODENN) method which classifies the faults. The proposed techniques detect as well as classify the faults effectively that are experimentally proved by means of comparing them with the prevailing techniques, namely ENN, ANN and SVM, along with KNN in terms of some quality measures. The obtained results for ODENN showed an accuracy of 98.99%, specificity of 97.6%, and a sensitivity of 97.02%.",
        "DOI": "10.1142/S179396232350037X",
        "affiliation_name": "Annamalai University",
        "affiliation_city": "Chidambaram",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Rangeland species potential mapping using machine learning algorithms",
        "paper_author": "Sharifipour B.",
        "publication": "Ecological Engineering",
        "citied_by": "5",
        "cover_date": "2023-04-01",
        "Abstract": "Documenting habitats of rangeland plant species is required to properly manage rangelands and to understand ecosystem processes. A reliable rangeland species potential map can help managers and policy makers design a sustainable grazing system on rangelands. The aim of this study is to map the plant species in the Qurveh City rangelands, Kurdistan Province, Iran, using state-of-the-art machine learning algorithms, including Support Vector Machine (SVM), Artificial Neural Network (ANN), Naïve Bayes (NB), Bayes Net (BN) and Classification and Regression Tree (CART). A total of 185 rangeland species were used in the study, together with 20 conditioning factors, to build and validate models. The One-R feature section technique and multicollinearity test were used, respectively, to determine the most important factors and correlations between them. Model validation was performed using sensitivity, specificity, accuracy, F1-measure, Matthews correlation coefficient (MCC), Kappa, root mean square error (RMSE), and area under the receiver operating characteristic curve (AUC). Results showed that topographic wetness index (TWI), slope angle, elevation, soil phosphorus and soil potassium were the five most important factors to increase the rangeland plants habitat suitability. The Naïve Bayes algorithm (AUC = 0.782) had the highest performance and prediction accuracy and best consistency across the species in the investigated rangeland, followed by the SVM (AUC = 0.763), ANN (AUC = 0.762), CART (AUC = 0.627), and BN (AUC = 0.617) models.",
        "DOI": "10.1016/j.ecoleng.2023.106900",
        "affiliation_name": "University of Tehran",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "What drives the performance of tax administrations? Evidence from selected european countries",
        "paper_author": "Milosavljević M.",
        "publication": "Economic Modelling",
        "citied_by": "7",
        "cover_date": "2023-04-01",
        "Abstract": "An effective, efficient, fair, and trusted tax administration is a top priority for every country in the world; however, tax administration faces many issues, such as corruption, tax avoidance, or lack of flexibility. Some countries perform better in this process, and this paper aims to identify the main drivers of tax administration performance. We analyzed 35 European tax administrations by 12 performance dimensions in 2 consecutive years (2018–2019) and created a comprehensive performance measurement indicator using a data-driven neutral-aggregation approach. The findings indicate that (a) digitalization of tax administrations is the most influential driver of the overall tax administration performance, (b) Nordic countries and Switzerland can serve as role models for tax administration performance, and (c) the country-level results can serve as a proxy for the degree of the shadow economy. These findings guide European policymakers regarding the appropriate policy measures required to improve the performance of tax administration.",
        "DOI": "10.1016/j.econmod.2023.106217",
        "affiliation_name": "University of Belgrade",
        "affiliation_city": "Belgrade",
        "affiliation_country": "Serbia"
    },
    {
        "paper_title": "Off-policy and on-policy reinforcement learning with the Tsetlin machine",
        "paper_author": "Rahimi Gorji S.",
        "publication": "Applied Intelligence",
        "citied_by": "2",
        "cover_date": "2023-04-01",
        "Abstract": "The Tsetlin Machine is a recent supervised learning algorithm that has obtained competitive accuracy- and resource usage results across several benchmarks. It has been used for convolution, classification, and regression, producing interpretable rules in propositional logic. In this paper, we introduce the first framework for reinforcement learning based on the Tsetlin Machine. Our framework integrates the value iteration algorithm with the regression Tsetlin Machine as the value function approximator. To obtain accurate off-policy state-value estimation, we propose a modified Tsetlin Machine feedback mechanism that adapts to the dynamic nature of value iteration. In particular, we show that the Tsetlin Machine is able to unlearn and recover from the misleading experiences that often occur at the beginning of training. A key challenge that we address is mapping the intrinsically continuous nature of state-value learning to the propositional Tsetlin Machine architecture, leveraging probabilistic updates. While accurate off-policy, this mechanism learns significantly slower than neural networks on-policy. However, by introducing multi-step temporal-difference learning in combination with high-frequency propositional logic patterns, we are able to close the performance gap. Several gridworld instances document that our framework can outperform comparable neural network models, despite being based on simple one-level AND-rules in propositional logic. Finally, we propose how the class of models learnt by our Tsetlin Machine for the gridworld problem can be translated into a more understandable graph structure. The graph structure captures the state-value function approximation and the corresponding policy found by the Tsetlin Machine.",
        "DOI": "10.1007/s10489-022-04297-3",
        "affiliation_name": "Universitetet i Agder",
        "affiliation_city": "Kristiansand",
        "affiliation_country": "Norway"
    },
    {
        "paper_title": "CSRC oral communication and corporate disclosure",
        "paper_author": "Hou C.",
        "publication": "Journal of Corporate Finance",
        "citied_by": "4",
        "cover_date": "2023-04-01",
        "Abstract": "Oral communication has increasingly been used as a policy tool by the China Securities Regulatory Commission (CSRC) to regulate the Chinese financial market. However, less is known about whether and how this newly developed policy tool affects corporate decisions. Using machine-learning techniques, this paper develops a measure to evaluate the CSRC's oral emphasis on financial disclosure based on transcripts of its press conferences and official speeches. We find that when the CSRC places more emphasis on disclosure, both the quantity and quality of corporate disclosure are improved. Further evidence suggests that listed firms with external financing plans respond more to CSRC oral communication. Moreover, under political pressure, state-owned enterprises (SOEs) comply more with CSRC oral communication in terms of disclosure quantity but not disclosure quality.",
        "DOI": "10.1016/j.jcorpfin.2022.102351",
        "affiliation_name": "University of Bristol",
        "affiliation_city": "Bristol",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Synthesizing explainable counterfactual policies for algorithmic recourse with program synthesis",
        "paper_author": "De Toni G.",
        "publication": "Machine Learning",
        "citied_by": "3",
        "cover_date": "2023-04-01",
        "Abstract": "Being able to provide counterfactual interventions—sequences of actions we would have had to take for a desirable outcome to happen—is essential to explain how to change an unfavourable decision by a black-box machine learning model (e.g., being denied a loan request). Existing solutions have mainly focused on generating feasible interventions without providing explanations of their rationale. Moreover, they need to solve a separate optimization problem for each user. In this paper, we take a different approach and learn a program that outputs a sequence of explainable counterfactual actions given a user description and a causal graph. We leverage program synthesis techniques, reinforcement learning coupled with Monte Carlo Tree Search for efficient exploration, and rule learning to extract explanations for each recommended action. An experimental evaluation on synthetic and real-world datasets shows how our approach, FARE (eFficient counterfActual REcourse), generates effective interventions by making orders of magnitude fewer queries to the black-box classifier with respect to existing solutions, with the additional benefit of complementing them with interpretable explanations.",
        "DOI": "10.1007/s10994-022-06293-7",
        "affiliation_name": "Bruno Kessler Foundation",
        "affiliation_city": "Trento",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "A Transfer Double Deep Q Network Based DDoS Detection Method for Internet of Vehicles",
        "paper_author": "Li Z.",
        "publication": "IEEE Transactions on Vehicular Technology",
        "citied_by": "27",
        "cover_date": "2023-04-01",
        "Abstract": "Distributed denial of service (DDoS) attacks have become one of the main factors restricting the development of internet of vehicles (IoV). Although some intelligent reinforcement learning based methods have been introduced to mitigate DDoS attacks, there are still many constraints in the training process, such as the long training time and dependence on large labeled data. In this paper, we propose a transfer double deep Q-network (DDQN) based DDoS detection method for IoV. By constructing a Kalman filter based reinforcement learning model, we can constantly improve the performance of DDoS detection in DDQN without depending on large labeled data. Furthermore, by utilizing the knowledge obtained by adjacent similar base stations, we design a transfer DDQN method based on traffic flow similarity to speed up the training of DDQN for a newly added base station. This enables the new base station to obtain DDoS detection policies quickly. We compare our proposed method with other popular machine learning based DDoS detection methods. The experimental results show that the transfer DDQN based DDoS detection method improves F1-measure and detection accuracy by 79.4% and 17.5% on average. Meanwhile, by means of traffic flow similarity based transfer learning, the time consumption and convergence time can be reduced by 41.3% and 31.1%, which makes our method adapt to the dynamic network.",
        "DOI": "10.1109/TVT.2022.3233880",
        "affiliation_name": "Tongji University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Nonlinear relationships and interaction effects of an urban environment on crime incidence: Application of urban big data and an interpretable machine learning method",
        "paper_author": "Kim S.",
        "publication": "Sustainable Cities and Society",
        "citied_by": "39",
        "cover_date": "2023-04-01",
        "Abstract": "While environmental criminology suggests that crime and the urban environment are closely related, some studies suggest a nonlinear relationship. This study analyzed the relationship between crime incidence and the urban environment using urban big data such as points-of-interest (POI), smart civil complaint data, and street image data from Naver Street View in Seoul, Korea. For analysis, the Light Gradient Boosting Machine (LightGBM) model and SHapley Additive exPlanation (SHAP) method have been used. The analysis results confirmed a nonlinear relationship comprising inflection points between crime incidence and the urban environment. Also, this study identified the interaction effects of urban environmental variables on crime incidence. Finally, the hierarchical clustering method was used to identify the contributions of various aspects of the urban environments to crime incidence. Then, this study provides policy implications to prevent potential criminal activities and promote public safety for sustainable cities and societies.",
        "DOI": "10.1016/j.scs.2023.104419",
        "affiliation_name": "Hanyang University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "Validity of Machine Learning in Assessing Large Texts Through Sustainability Indicators",
        "paper_author": "García-Esparza J.A.",
        "publication": "Social Indicators Research",
        "citied_by": "7",
        "cover_date": "2023-04-01",
        "Abstract": "As machine learning becomes more widely used in policy and environmental impact settings, concerns about accuracy and fairness arise. These concerns have piqued the interest of researchers, who have advanced new approaches and theoretical insights to enhance data gathering, treatment and models’ training. Nonetheless, few works have looked at the trade-offs between appropriateness and accuracy in indicator evaluation to comprehend how these constraints and approaches may better redound into policymaking and have a more significant impact across culture and sustainability matters for urban governance. This empirical study fulfils this void by researching indicators’ accuracy and utilizing algorithmic models to test the benefits of large text-based analysis. Here we describe applied work in which we find affinity and occurrence in indicators trade-offs that result be significant in practice to evaluate large texts. In the study, objectivity and fairness are kept substantially without sacrificing accuracy, explicitly focusing on improving the processing of indicators to be truthfully assessed. This observation is robust when cross-referring indicators and unique words. The empirical results advance a novel form of large text analysis through machine intelligence and refute a widely held belief that artificial intelligence text processing necessitates either accepting a significant reduction in accuracy or fairness.",
        "DOI": "10.1007/s11205-023-03075-z",
        "affiliation_name": "Universitat Autònoma de Barcelona",
        "affiliation_city": "Cerdanyola del Valles",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "An improved spatial case-based reasoning considering multiple spatial drivers of geographic events and its application in landslide susceptibility mapping",
        "paper_author": "Zhao Z.",
        "publication": "Catena",
        "citied_by": "14",
        "cover_date": "2023-04-01",
        "Abstract": "Traditional case-based reasoning methods overlook non-stationary spatial drivers of geographical events such as heterogeneity, dependence, and accumulation in case representation, and directly obtain the solution of the most similar cases in case reuse instead of considering the interference of fake similar cases to eliminate the contingency of reasoning, which leads to poor interpretations and low efficiency decisions in complex and heterogeneous geographical environments. This study proposes an improved spatial case-based reasoning (SCBR) considering multiple spatial drivers to overcome above problems and uses landslide susceptibility mapping as an example. Specifically, these spatial drivers were captured, extracted, and integrated into case representation by using geographic self-organizing mapping algorithm, spatial statistic, and spatial adjacent matrix, respectively. Additionally, the K-nearest neighbor method as case retrieval was introduced to retrieve the K similar cases based on the local and global similarity reasoning. Finally, the Gaussian process regression as case reuse method was generated to landslide susceptibility index under the assumption that K similar cases follows Gaussian distribution. Our experimental results show that the precision, F1, recall, and kappa of the proposed SCBR method are 0.974, 0.976, 0.979, and 0.953 which are higher than those of the traditional case-based reasoning (0.931, 0.941, 0.953, and 0.881), long short-term memory (0.951, 0.933, 0.915, and 0.870), and extreme gradient boosting decision tree (0.963, 0.967, 0.972, and 0.945), respectively. In general, the novel approach with better predictive performance can help decision makers to develop policies that reduce the loess of landslides and apply to similar geological events.",
        "DOI": "10.1016/j.catena.2023.106940",
        "affiliation_name": "Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Surface, satellite ozone variations in Northern South America during low anthropogenic emission conditions: a machine learning approach",
        "paper_author": "Casallas A.",
        "publication": "Air Quality, Atmosphere and Health",
        "citied_by": "7",
        "cover_date": "2023-04-01",
        "Abstract": "2020 presented the ideal conditions for studying the air quality response to several emission reductions due to the COVID-19 lockdowns. Numerous studies found that the tropospheric ozone increased even in lockdown conditions, but its reasons are not entirely understood. This research aims to better understand the ozone variations in Northern South America. Satellite and reanalysis data were used to analyze regional ozone variations. An analysis of two of the most polluted Colombian cities was performed by quantifying the changes of ozone and its precursors and by doing a machine learning decomposition to disentangle the contributions that precursors and meteorology made to form O3. The results indicated that regional ozone increased in most areas, especially where wildfires are present. Meteorology is associated with favorable conditions to promote wildfires in Colombia and Venezuela. Regarding the local analysis, the machine learning ensemble shows that the decreased titration process associated with the NO plummeting owing to mobility reduction is the main contributor to the O3 increase (≈50%). These tools lead to conclude that (i) the increase in O3 produced by the reduction of the titration process that would be associated with an improvement in mobile sources technology has to be considered in the new air quality policies, (ii) a boost in international cooperation is essential to control wildfires since an event that occurs in one country can affect others and (iii) a machine learning decomposition approach coupled with sensitivity experiments can help us explain and understand the physicochemical mechanism that drives ozone formation.",
        "DOI": "10.1007/s11869-023-01303-6",
        "affiliation_name": "Universidad Nacional Abierta y a Distancia",
        "affiliation_city": "Bogota",
        "affiliation_country": "Colombia"
    },
    {
        "paper_title": "Curbing Pandemic Through Evolutionary Algorithm-Based Priority Aware Mobility Scheduling",
        "paper_author": "Roy S.",
        "publication": "IEEE Transactions on Intelligent Transportation Systems",
        "citied_by": "1",
        "cover_date": "2023-04-01",
        "Abstract": "COVID-19 is a global pandemic caused by the Severe Acute Respiratory Syndrome Coronavirus 2. While swift vaccine development and distribution have arrested the infection spread rate, it is necessary to design public policies that inform human mobility to curb outbreaks from future strains of the virus. While existing non-pharmaceutical approaches employing network science and machine learning offer promising travel policy solutions, they are guided by epidemiological and economic considerations alone and not human itineraries. We introduce an evolutionary algorithm (EA) based mobility scheduler that incorporates the personalized itineraries of individuals to determine the ideal timing of their mobility. We mathematically analyze the computational efficiency versus the optimality trade-off of the mobility scheduler. Through extensive simulations, we demonstrate that the EA-based mobility scheduler can balance the trade-off between (1) optimality and computational cost and (2) fair and preferential human mobility while reducing contagion under lockdown and no-lockdown as well as even and uneven human mobility traffic scenarios. We show that for two human mobility models, the scheduler exhibits lower infection numbers than a baseline trip-planning approach that directs human traffic along the least congested route to minimize contagion. We discuss that the EA scheduler lends itself to intricate mobility schedules of multiple destination choices with varying priorities and socioeconomic and demographic considerations.",
        "DOI": "10.1109/TITS.2022.3230013",
        "affiliation_name": "VCU College of Engineering",
        "affiliation_city": "Richmond",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "From prediction to decision: Optimizing long-term care placements among older delayed discharge patients",
        "paper_author": "Chuang Y.T.",
        "publication": "Production and Operations Management",
        "citied_by": "7",
        "cover_date": "2023-04-01",
        "Abstract": "This study examines long-term care (LTC) discharge planning among older delayed discharge patients. While awaiting placements in alternate care such as LTC, these patients occupy hospital beds despite not requiring an intensive level of care. This study proposes a novel discharge decision model based on the Markov decision process (MDP) framework, which incorporates predictions regarding the patients' health trajectory and the associated hospital costs. Our machine learning (ML)-based predictive analytics allow for considering heterogeneous health transitions, hence personalized decision making, leading to valuable information for reducing hospital costs. We also develop data-driven cost functions using patient characteristics to estimate the person-level costs associated with the decisions in the optimization model, that is, whether or not to discharge a patient to LTC. The data analyses and cost estimations are based on large historical data collected over 13 years in Ontario, Canada. To solve the resulting high-dimensional MDP models, we develop an index policy, where each patient's index value is calculated using their health complexity (comorbidity), sex, age, and acute length of stay in the hospital. Using extensive numerical experiments, we illustrate the superior performance of the proposed index policy against some benchmarking policies and demonstrate the significance of predictive information in optimizing discharge decisions. Our results also indicate that the value of predictive information increases with LTC bed availability and decreases with hospital capacity. We also demonstrate that with the anticipated exacerbating mismatch between supply and demand, targeted prediction-driven discharge policies, such as the proposed index policy, become even more critical.",
        "DOI": "10.1111/poms.13910",
        "affiliation_name": "Alberta School of Business",
        "affiliation_city": "Edmonton",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Assessing the effects of short-term traffic restriction policies on traffic-related air pollutants",
        "paper_author": "Fang X.R.",
        "publication": "Science of the Total Environment",
        "citied_by": "8",
        "cover_date": "2023-04-01",
        "Abstract": "The implementation of short-term traffic restriction policies (TRPs) during major events positively influences the traffic emission reduction. However, few studies explore the impact of diesel vehicle emissions on air quality during short-term TRP. In particular, the intertwined influences of short-term TRP and Spring Festival remains unclear. Based on Beijing 2022 Olympic Games, this study analyzed the spatiotemporal changes in urban air quality and diesel vehicle emission during short-term TRP. The results showed that the TRPs and Spring Festival contributed equally to the improvement of air quality and reduction of diesel vehicle emissions. The “interruption-recovery” pattern of short-term TRPs is characterized by a longer duration and a slower decline/recovery rate. Additionally, the individual contribution rate of diesel vehicle emissions to urban air pollutants was 15–20 % higher than that of meteorological factors during short-term TRPs.",
        "DOI": "10.1016/j.scitotenv.2023.161451",
        "affiliation_name": "State Key Laboratory of Ocean Engineering",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Leveraging explainable artificial intelligence and big trip data to understand factors influencing willingness to ridesharing",
        "paper_author": "Li Z.",
        "publication": "Travel Behaviour and Society",
        "citied_by": "19",
        "cover_date": "2023-04-01",
        "Abstract": "Carpool-style ridesharing, compared to traditional solo ride-hailing, can reduce traffic congestion, cut per-passenger carbon emissions, reduce parking infrastructure, and provide a more cost-effective way to travel. Despite these benefits, ridesharing only occupies a small percentage of the total ride-hailing trips in cities. This study integrates big trip data with machine learning and eXplainable AI (XAI) to understand the factors that influence willingness to take shared rides. We use the City of Chicago as a case study, and results show that users tend to adopt ridesharing for longer distance trips, and the cost of a trip remains the most important factor. We identify a strong diurnal pattern that people prefer to request shared trips during the morning and afternoon peak hours. We also find socio-economic disparities: users who requested trips from neighbourhoods with a high percentage of non-white, a low median household income, a low percentage of bachelor's degrees, and high vehicle ownership are more likely to share a ride. The findings and the XAI-based analytical framework presented in this study can help transportation network companies and local governments understand ridesharing behaviour and suggest new strategies and policies to promote the proportion of ridesharing for more sustainable and efficient city transportation.",
        "DOI": "10.1016/j.tbs.2022.12.006",
        "affiliation_name": "University of Glasgow",
        "affiliation_city": "Glasgow",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Strategy for mapping soil salt contents during the bare soil period through a satellite image: Optimal calibration set combined with random forest",
        "paper_author": "Xu X.",
        "publication": "Catena",
        "citied_by": "10",
        "cover_date": "2023-04-01",
        "Abstract": "Salt is a basic soil parameter that is especially sensitive to land degradation and global climate warming. Machine learning combined with satellite images offers the potential for effectively surveying soil salt contents (SSC) in the spatial and temporal dimensions. However, outlier spectral samples caused by environmental factors (e.g., particle size distribution, moisture, and vegetation) have interfered with the determination of the mapping relationship between ground-truth SSC and corresponding spectral features, resulting in a random forest (RF) mapping model with low generalization capability. Therefore, a spatial association module was introduced into the RF to remove the outlier spectral samples for building an optimal sample set in order to calibrate an improved random forest (IRF) model for SSC mapping. A total of 233 soil samples and a simultaneous Sentinel-2B multispectral image were acquired for the Weibei Plain of China. The spectral absorption features of SSC were characterized by three indices (band difference, band ratio, and band normalized). Then a spatial association function was developed to identify the outlier spectral samples for optimizing the calibration samples. Lastly, the SSC-responding bands and SSC-related indices were used as input variables to build the improved random forest (IRF) model for regional SSC mapping. Results showed that: (1) bands 3, 8, and 11 in the Sentinel-2B image were the SSC-responding bands, and band 11 had a significant correlation with SSC; (2) the band ratio index synthesized the multi-band spectral information, and effectively enhanced the spectral absorption signal of soil salt; the optimal SSC-related indices included RI34 (reflectance ratio of band 3 and band 4), RI711, NDI611 (normalized reflectance values of band 6 and band 11), and DI45 (reflectance difference value of band 4 and band 5); (3) when the SSC-responding bands and optimal SSC-related indices were input into the IRF remote sensing estimation model, the accuracy parameters R2v and RPIQ were greater (0.89 and 3.52, respectively) than the comparable values with the RF model (R2v = 0.69; RPIQ = 2.90), with accuracy improved by 28.99 % and 21.38 %, respectively, indicating that satisfactory results for regional SSC mapping had been obtained; (4) SSC distribution showed that the values in the central part of the study area were slightly higher than in the southern and northern parts; the highest SSC area was mainly related to the distribution of saltern; additionally, agricultural activities and microtopography contributed significantly to the distribution of SSC at the farm scale. The IRF model based on the SSC-responding bands and optimal SSC-related indices provides a strategy for supporting regional mapping of SSC, and will be useful for environmental policy-making.",
        "DOI": "10.1016/j.catena.2022.106900",
        "affiliation_name": "Beijing Normal University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Themes of resilience in the economics literature: A topic modeling approach",
        "paper_author": "Riepponen T.",
        "publication": "Regional Science Policy and Practice",
        "citied_by": "1",
        "cover_date": "2023-04-01",
        "Abstract": "The concept of resilience has been applied in several fields of academic research and has also grown in popularity among economists. The main contribution of this article is the systematic analysis and interpretation of the existing large body of resilience literature in economics by using topic modeling, a modern machine-learning research method. The advantage of this method is that it offers a more in-depth understanding of the themes in the resilience literature as opposed to the terminological classifications typically used in bibliometric studies. The results show that the identified topics are spread widely across different subareas of economics and deal with diverse themes, such as adaptation to climate change, stability of the financial system, and various types of shocks in regional economies. The findings reveal that the literature can be divided into two domains: one that deal with incremental changes occurring over a long period of time and the other dealing with unexpected, transient, and sudden changes. Furthermore, according to the results, well-known, highly cited research papers combine knowledge from different fields. Policymakers seeking to support cutting-edge research projects may benefit from this finding, as it emphasizes the need for policy measures to enhance cross-fertilized research.",
        "DOI": "10.1111/rsp3.12612",
        "affiliation_name": "Oulu Business School",
        "affiliation_city": "Oulu",
        "affiliation_country": "Finland"
    },
    {
        "paper_title": "School dropout prediction and feature importance exploration in Malawi using household panel data: machine learning approach",
        "paper_author": "Colak Oz H.",
        "publication": "Journal of Computational Social Science",
        "citied_by": "14",
        "cover_date": "2023-04-01",
        "Abstract": "Designing early warning systems through machine learning (ML) models to identify students at risk of dropout can improve targeting mechanisms and lead to efficient social policy interventions in education. School dropout is a culmination of various factors that drive children to leave school, and timely policy responses are most needed to address these underlying factors and improve school retention of children over time. However, applying ML approaches to school dropout prediction is an important challenge, especially in low-income countries, where data collection and management systems are relatively more prone to financial and technical constraints. For this reason, this study suggests using already collected household panel data to predict the probability of school dropout and explore feature importance for primary school children in Malawi through ML models. A rich set of variables is obtained in this study from the household data and used to build Random Forest (RF), least absolute shrinkage and selection operator (LASSO), Ridge and multilayer neural network (MNN) models. The study further explores how performance metrics differ when we embed the training samples' weights representing frequency in sampling design into the cost function of these ML models to discuss the implications of using household data in computational social science. LASSO and MNN models trained with sample weights become more prominent due to their higher recall rates of 80.6% and 78.8%. Compared to the baseline model trained with sample weights, the recall rate gained is roughly 56 percentage points using LASSO and 54 percentage points using MNN. Also, comparing LASSO and MNN trained with and without sample weights reveals that training models with sample weights increase the recall rate roughly by 11 percentage points for LASSO and 12 percentage points for MNN. Lastly, the paper provides a comprehensive and unified approach to better interpret the models using a game-theoretic approach – SHapley Additive exPlanations (SHAP) – to quantify feature importance. As a result, socio-economic characteristics of children, such as working in household farming and father's education level, are among the most important features contributing to the probability of school dropout in ML models. This study argues that the weighted sample structure of household data and its wide range of variables explored through the SHAP method for feature importance can enrich the literature and yield valuable results to harness data science for society.",
        "DOI": "10.1007/s42001-022-00195-3",
        "affiliation_name": "Tilburg University",
        "affiliation_city": "Tilburg",
        "affiliation_country": "Netherlands"
    },
    {
        "paper_title": "A HEURISTICALLY ACCELERATED REINFORCEMENT LEARNING METHOD FOR MAINTENANCE POLICY OF AN ASSEMBLY LINE",
        "paper_author": "Wang X.",
        "publication": "Journal of Industrial and Management Optimization",
        "citied_by": "4",
        "cover_date": "2023-04-01",
        "Abstract": "This paper aims to investigate the maintenance policy for a two-machine one-buffer (2M1B) assembly line system. We assume that the observed quality states of the deteriorating machines in the system are characterized by multiple decreasing yield stages. A semi-Markov decision process (SMDP) model is used for describing the deteriorating process of the system. A heuristically accelerated multi-agent reinforcement learning (HAMRL) method is conducted to solve the problem model. The asynchronous updating rules are introduced in the HAMRL method, and the production time, preventive maintenance (PM) time and corrective repair (CR) time are random, and the deterioration mode of the device is not fixed. Meanwhile, a comparison with a simulated annealing search (SAS) based exploration algorithm and a neighborhood search (NS) based exploration algorithm in reinforcement learning (RL) is presented. The empirical results indicate that the proposed HAMRL algorithm can speed up the learning process, and has a certain advantage for the larger space and the more practical problem. And the maintenance strategy for the 2M1B assembly line system is obtained under the condition of convergent system average cost rate. This paper provides new and practical insights into the application and selection of techniques for maintenance policy of the 2M1B assembly line system.",
        "DOI": "10.3934/jimo.2022047",
        "affiliation_name": "Shenyang Institute of Engineering",
        "affiliation_city": "Shenyang",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Development of reinforced learning based non-linear controller for unmanned aerial vehicle",
        "paper_author": "Din A.F.U.",
        "publication": "Journal of Ambient Intelligence and Humanized Computing",
        "citied_by": "20",
        "cover_date": "2023-04-01",
        "Abstract": "Design complexities of trending UAVs and the operational harsh environments necessitates Control Law formulation utilizing intelligent techniques that are both robust, model-free and adaptable. In this research, an intelligent control architecture for an experimental Unmanned Aerial Vehicle (UAV) having an unconventional inverted V-tail design, is presented. Due to unique design of the vehicle strong roll and yaw coupling exists, making the control of vehicle challenging. To handle UAV’s inherent control complexities, while keeping them computationally acceptable, a variant of distinct Deep Reinforcement learning (DRL) algorithm, namely Reformed Deep Deterministic Policy Gradient (R-DDPG) is proposed. Conventional DDPG algorithm after being modified in its learning architecture becomes capable of intelligently handling the continuous state and control space domains besides controlling the platform in its entire flight regime. The paper illustrates the application of modified DDPG algorithm (namely R-DDPG) towards the design, while the performance of the resulting controller is assessed in simulation using dynamic model of the vehicle. Nonlinear simulations were then performed to analyze UAV performance under different environmental and launch conditions. The effectiveness of the proposed strategy is further demonstrated by comparing the results with the linear controller for the same UAV whose feedback loop gains are optimized by employing technique of optimal control theory achieved through application of Linear quadratic regulator (LQR) based control strategy. The efficacy of the results and performance characteristics, demonstrated the ability of the presented algorithm to dynamically adapt to the changing environment, thereby making it suitable for UAV applications.",
        "DOI": "10.1007/s12652-022-04467-8",
        "affiliation_name": "College of Aeronautical Engineering",
        "affiliation_city": "Risalpur Cantonment",
        "affiliation_country": "Pakistan"
    },
    {
        "paper_title": "Artificial intelligence driven hydrogen and battery technologies – A review",
        "paper_author": "Sai Ramesh A.",
        "publication": "Fuel",
        "citied_by": "54",
        "cover_date": "2023-04-01",
        "Abstract": "The world has recognized the importance of renewable energy and is moving towards a rapid transition to renewable energy and energy efficiency. Advances in electrolysis and cost reductions, as well as the availability of renewable energy sources, have paved the way for the creation of green hydrogen, a completely carbon-free fuel, making it a real contender to revolutionize the energy market. The recent incorporation of artificial intelligence into the energy sector has provided a major breakthrough for the industry. Artificial intelligence algorithms and models such as artificial neural networks, machine learning, support vector regression, and fuzzy logic models can greatly contribute to improving hydrogen energy production, storage, and transportation. They play an important role in predicting various parameters, safety protocols and management of hydrogen production. Furthermore, advances in artificial intelligence are expected to bring huge state-of-the-art technologies and tools for hydrogen and battery technology that could help solve the current energy-oriented crises and problems. This review provides insight into the feasibility of state-of-the-art artificial intelligence for hydrogen and battery technology. The primary focus is to demonstrate the contribution of various AI techniques, its algorithms and models in hydrogen energy industry, as well as smart battery manufacturing, and optimization. Meanwhile, AI models integrated into battery technology play a key role in material discovery, battery design, improved battery manufacturing, diagnostic tools, and optimal battery management systems for smart batteries. With improved performance and longer life, these smart batteries will be integrated into modern robotics, electric vehicles, aerospace and other fields.",
        "DOI": "10.1016/j.fuel.2022.126862",
        "affiliation_name": "Government Arts College, Salem",
        "affiliation_city": "Salem",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Hierarchical Intelligent Operation of Energy Storage Systems in Power Distribution Grids",
        "paper_author": "Hosseini M.M.",
        "publication": "IEEE Transactions on Sustainable Energy",
        "citied_by": "12",
        "cover_date": "2023-04-01",
        "Abstract": "High penetration of distributed energy storage systems (ESS) offers an unparalleled opportunity to reinforce the distribution grid at the local level against upstream disruptions; however, their mass operation under uncertainty of load and renewable generation is computationally expensive. While deep reinforcement learning (DRL) has been suggested to train operator agents capable of handling uncertainty and high dimensionality of the problem, it falls short when safety and feasibility assurances are required in critical operations. This paper proposes a model for hierarchical coupling of DRL and mathematical optimization for operation of ESS in distribution grids, in order to take advantage of DRL fast response while keeping network constraints in check. In the proposed method, strategic scheduling of distributed ESS units are performed locally by fast DRL-trained agents, while critical grid-wide operations such as fault management and voltage control are performed by an optimization-based central controller. The local controller is trained by Twin Delayed Deep Deterministic Policy Gradient (TD3), whose response time is three orders of magnitude faster than stochastic optimization, while the optimality of solutions are similar in both cases.",
        "DOI": "10.1109/TSTE.2022.3222425",
        "affiliation_name": "John and Marcia Price College of Engineering",
        "affiliation_city": "Salt Lake City",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "The personalized recommendation for OTA flight cancellation and change services during the pandemic",
        "paper_author": "Gong Z.",
        "publication": "Journal of Revenue and Pricing Management",
        "citied_by": "2",
        "cover_date": "2023-04-01",
        "Abstract": "The COVID-19 pandemic has had a dramatic impact on people’s travels. Due to the recurrent pandemic and regionally different policies in China, travelers must pay a lot for flight cancellations and changes. To accommodate this, online travel agencies (OTA) can provide a more flexible ancillary as a supplement to the airline company's services. Here, we introduced the upgraded all-in-one (AIO) service package, which offers compensation for flight delays, changes, or refund. We also designed a dynamic recommendation engine (DRE), which can make real-time personalized recommendations. Backed by AB testing, the machine learning-based DRE not only raises the package attach rate without interrupting the flight ordering process, but also helps the customers cut cost when making flight cancellations or changes.",
        "DOI": "10.1057/s41272-022-00403-9",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Robust Optimal Well Control using an Adaptive Multigrid Reinforcement Learning Framework",
        "paper_author": "Dixit A.",
        "publication": "Mathematical Geosciences",
        "citied_by": "4",
        "cover_date": "2023-04-01",
        "Abstract": "Reinforcement learning (RL) is a promising tool for solving robust optimal well control problems where the model parameters are highly uncertain and the system is partially observable in practice. However, the RL of robust control policies often relies on performing a large number of simulations. This could easily become computationally intractable for cases with computationally intensive simulations. To address this bottleneck, an adaptive multigrid RL framework is introduced which is inspired by principles of geometric multigrid methods used in iterative numerical algorithms. RL control policies are initially learned using computationally efficient low-fidelity simulations with coarse grid discretization of the underlying partial differential equations (PDEs). Subsequently, the simulation fidelity is increased in an adaptive manner towards the highest fidelity simulation that corresponds to the finest discretization of the model domain. The proposed framework is demonstrated using a state-of-the-art, model-free policy-based RL algorithm, namely the proximal policy optimization algorithm. Results are shown for two case studies of robust optimal well control problems, which are inspired from SPE-10 model 2 benchmark case studies. Prominent gains in computational efficiency are observed using the proposed framework, saving around 60-70% of the computational cost of its single fine-grid counterpart.",
        "DOI": "10.1007/s11004-022-10033-x",
        "affiliation_name": "Heriot-Watt University",
        "affiliation_city": "Edinburgh",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "A design of experiments Cyber–Physical System for energy modelling and optimisation in end-milling machining",
        "paper_author": "Pantazis D.",
        "publication": "Robotics and Computer-Integrated Manufacturing",
        "citied_by": "9",
        "cover_date": "2023-04-01",
        "Abstract": "Industrial energy consumption accounts for 50% of global use and manufacturers that invest in energy waste reduction strategies can have a significant impact on emission reduction while ensuring they operate within energy usage limits. Exceeding these limits can result in taxation from national and international policy makers and charges from national energy providers. For example, the UK Climate Change Levy, charged to businesses at 0.554 p/kWh can equate to 7.28% of a manufacturing business's energy bill based on an average total usage rate of 7.61 p/kWh. There has been growing interest in optimising the process energy consumption of machining when machine tools are responsible for 13% of industrial energy consumption, generating 16 million tonnes of CO2 emissions in the UK alone but demonstrate less than 30% energy efficiency (Gutowski et al., 2006). This paper presents the design, development and validation of a novel automated Design of Experiments (DoE) toolset that forms part of a larger Cyber–Physical System (CPS). The CPS offers the capability to automate, characterise and predict the power of three-phase industrial machining processes and to select the machining toolpath that optimises energy consumption. Validation of the DoE toolset has been conducted through automation of an industrial three-phase Hurco VM1 computer numerical control (CNC) machine and energy feature extraction with a Hidden Markov Model.",
        "DOI": "10.1016/j.rcim.2022.102469",
        "affiliation_name": "Loughborough University",
        "affiliation_city": "Loughborough",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Federated Multi-Task Learning for Joint Diagnosis of Multiple Mental Disorders on MRI Scans",
        "paper_author": "Huang Z.A.",
        "publication": "IEEE Transactions on Biomedical Engineering",
        "citied_by": "34",
        "cover_date": "2023-04-01",
        "Abstract": "Objective: Deep learning (DL) techniques have been introduced to assist doctors in the interpretation of medical images by detecting image-derived phenotype abnormality. Yet the privacy-preserving policy of medical images disables the effective training of DL model using sufficiently large datasets. As a decentralized computing paradigm to address this issue, federated learning (FL) allows the training process to occur in individual institutions with local datasets, and then aggregates the resultant weights without risk of privacy leakage. Methods: We propose an effective federated multi-task learning (MTL) framework to jointly identify multiple related mental disorders based on functional magnetic resonance imaging data. A federated contrastive learning-based feature extractor is developed to extract high-level features across client models. To ease the optimization conflicts of updating shared parameters in MTL, we present a federated multi-gate mixture of expert classifier for the joint classification. The proposed framework also provides practical modules, including personalized model learning, privacy protection, and federated biomarker interpretation. Results: On real-world datasets, the proposed framework achieves robust diagnosis accuracies of 69.48 ± 1.6%, 71.44 ± 3.2%, and 83.29 ± 3.2% in autism spectrum disorder, attention deficit/hyperactivity disorder, and schizophrenia, respectively. Conclusion: The proposed framework can effectively ease the domain shift between clients via federated MTL. Significance: The current work provides insights into exploiting the advantageous knowledge shared in related mental disorders for improving the generalization capability of computer-aided detection approaches.",
        "DOI": "10.1109/TBME.2022.3210940",
        "affiliation_name": "City University of Hong Kong",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong"
    },
    {
        "paper_title": "Empirical Gittins index strategies with ε-explorations for multi-armed bandit problems",
        "paper_author": "Li X.",
        "publication": "Computational Statistics and Data Analysis",
        "citied_by": "1",
        "cover_date": "2023-04-01",
        "Abstract": "The machine learning/statistics literature has so far considered largely multi-armed bandit (MAB) problems in which the rewards from every arm are assumed independent and identically distributed. For more general MAB models in which every arm evolves according to a rewarded Markov process, it is well known the optimal policy is to pull an arm with the highest Gittins index. When the underlying distributions are unknown, an empirical Gittins index rule with ε-exploration (abbreviated as empirical ε-Gittinx index rule) is proposed to solve such MAB problems. This procedure is constructed by combining the idea of ε-exploration (for exploration) and empirical Gittins indices (for exploitation) computed by applying the Largest-Remaining-Index algorithm to the estimated underlying distribution. The convergence of empirical Gittins indices to the true Gittins indices and expected discounted total rewards of the empirical ε-Gittinx index rule to those of the oracle Gittins index rule is provided. A numerical simulation study is demonstrated to show the behavior of the proposed policies, and its performance over the ε-mean reward is discussed.",
        "DOI": "10.1016/j.csda.2022.107610",
        "affiliation_name": "East China Normal University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Deep Learning-Based Job Placement in Distributed Machine Learning Clusters With Heterogeneous Workloads",
        "paper_author": "Bao Y.",
        "publication": "IEEE/ACM Transactions on Networking",
        "citied_by": "8",
        "cover_date": "2023-04-01",
        "Abstract": "Nowadays, most leading IT companies host a variety of distributed machine learning (ML) workloads in ML clusters to support AI-driven services, such as speech recognition, machine translation, and image processing. While multiple jobs are executed concurrently in a shared cluster to improve resource utilization, interference among co-located ML jobs can lead to significant performance downgrade. Existing cluster schedulers, such as YARN and Mesos, are interference-agnostic in their job placement, leading to suboptimal resource efficiency and usage. Some literature has studied interference-aware job placement policy, but relies on detailed workload profiling and interference modeling, which is not a general solution. In this work, we present Harmony, a deep learning-driven ML cluster scheduler that places heterogeneous training jobs (either with parameter server architecture or all-reduce architecture) in a manner that minimizes interference and maximizes performance (i.e., training completion time minimization). The design of Harmony is based on a carefully designed deep reinforcement learning (DRL) framework enhanced with reward modeling. The DRL integrates a dynamic sequence-to-sequence model with the state-of-the-art techniques to stabilize training and improve convergence, including actor-critic algorithm, job-aware action space exploration, multi-head attention, and experience replay. In view of a common lack of reward samples corresponding to different placement decisions, we build an auxiliary sequence-to-sequence reward prediction model, which is trained with historical samples and used for producing reward for unseen placement. Experiments using real ML workloads in a Kubernetes cluster of 6 GPU servers show that Harmony outperforms representative schedulers by 16%-42% in terms of average job completion time.",
        "DOI": "10.1109/TNET.2022.3202529",
        "affiliation_name": "The University of Hong Kong",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong"
    },
    {
        "paper_title": "Deep learning models and traditional automated techniques for brain tumor segmentation in MRI: a review",
        "paper_author": "Jyothi P.",
        "publication": "Artificial Intelligence Review",
        "citied_by": "58",
        "cover_date": "2023-04-01",
        "Abstract": "Brain is an amazing organ that controls all activities of a human. Any abnormality in the shape of anatomical regions of the brain needs to be detected as early as possible to reduce the mortality rate. It is also beneficial for treatment planning and therapy. The most crucial task is to isolate abnormal areas from normal tissue regions. To identify abnormalities in the earlier stage, various medical imaging modalities were used by medical practitioners as part of the diagnosis. Magnetic Resonance Imaging (MRI) is a non-invasive diagnostic tool used for analyzing the internal structures owing to its capability to provide images with high resolution and better contrast for soft tissues. This survey focuses on studies done in brain MRI. Manual segmentation of abnormal tissues is a time-consuming task, and the performance depends on the expert’s efficiency. Hence automating tumor segmentation plays a vital role in medical imaging applications. This study aims to provide a comprehensive survey on recent works developed in brain tumor segmentation. In this paper, a systematic literature review is presented to the reader to understand three policies, namely classical scheme, machine learning strategy, and deep learning methodology meant for tumor segmentation. Our primary goal is to include classical methods like atlas-based strategy and statistical-based models employed for segmenting tumors from brain MRI. Few studies that utilized machine learning approaches for the segmentation and classification of brain structures are also discussed. After that, the study provides an overview of deep learning-based segmentation models for quantitative analysis of brain MRI. Deep learning plays a vital role in the automatic segmentation of brain tissues. Presently deep learning technique outshines traditional statistical methods and machine learning approaches. An effort is made to enclose the literature on patch-based and semantic-based tissue segmentation presented by researchers working in the discipline of medical imaging. The manuscript discusses the basic convolutional neural network architecture, Data Sets, and the existing deep learning techniques for tissue segmentation coupled with classification. This paper also attempts to summarize the current works in Convolutional Neural networks and Autoencoders that assist researchers in seeking future directions. Finally, this article is concluded with possible developments and open challenges in brain tumor segmentation.",
        "DOI": "10.1007/s10462-022-10245-x",
        "affiliation_name": "Thrissur Educational Trust",
        "affiliation_city": "Thrissur",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Adaptive Cloud Bundle Provisioning and Multi-Workflow Scheduling via Coalition Reinforcement Learning",
        "paper_author": "Wang X.",
        "publication": "IEEE Transactions on Computers",
        "citied_by": "12",
        "cover_date": "2023-04-01",
        "Abstract": "The efficient cloud resource provisioning for the execution of complex workflow applications has always been one of the important research issues. Most of the existing approaches focus on the resource provisioning of single-type virtual machine (VM) instances for the single or multiple workflows, while few consider the situation of provisioning multi-type VM instances simultaneously. As a result, the executing performance of complex workflows degrades. Different from the existing work, this paper proposes an adaptive cloud bundle provisioning and multi-workflow scheduling model to dynamically perform both the horizontal and vertical cloud resource scaling on multi-type VM instances for the execution of complex workflows. Among the model, a depth-first-search coalition reinforcement learning (DFSCRL) provisioning policy is presented to realize the resource scaling, which integrates the physical machine (PM) coalition formation with the Q-learning algorithm, then dynamically generates an optimal multi-type VM instance bundle from the PM coalition, and finally provisions these instances to the concurrent execution of multiple workflows. The theoretical proofs and various experiments with the multifaceted metrics demonstrate that the performance of the proposed algorithms is superior to that of the state-of-the-art relevant policies.",
        "DOI": "10.1109/TC.2022.3191733",
        "affiliation_name": "School of Computing and Information Systems",
        "affiliation_city": "Melbourne",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Predicting Policy: A Psycholinguistic Artificial Intelligence in the United Nations",
        "paper_author": "Gandall K.",
        "publication": "Social Science Computer Review",
        "citied_by": "0",
        "cover_date": "2023-04-01",
        "Abstract": "In organizational theory, institutionalists generally make predictions of corresponding context and policy outcome based on structural processes. Psychoanalytic theory, in contrast, focuses on the rhetorical framing rather than the environment of a policy for predictive outcomes. This study aims to explore the debate over policy prediction by developing a supervised machine learning model to predict for policy success and context in the United Nations (UN). Through data collected with a python web scraper on all UN meetings in the General Assembly (GA) and Security Council (SC) between 1994 and 2020, we parse motions, policies, and conflict indicators, before passing meeting records through the Linguistic Inquiry and Word Count (LIWC) psycholinguistic algorithm. Next, we build 12 different machine learning models to predict for policy passage and context using preprocessed motion and LIWC data; results demonstrate that the psychoanalytic models better predicted for both context and policy outcomes than the institutionalist models, suggesting that the classical political axiom, “actions speak louder than words,” may not be supported by the empirical evidence.",
        "DOI": "10.1177/08944393221095193",
        "affiliation_name": "University of California, Irvine",
        "affiliation_city": "Irvine",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Algorithmic fairness through group parities? The case of COMPAS-SAPMOC",
        "paper_author": "Lagioia F.",
        "publication": "AI and Society",
        "citied_by": "14",
        "cover_date": "2023-04-01",
        "Abstract": "Machine learning classifiers are increasingly used to inform, or even make, decisions significantly affecting human lives. Fairness concerns have spawned a number of contributions aimed at both identifying and addressing unfairness in algorithmic decision-making. This paper critically discusses the adoption of group-parity criteria (e.g., demographic parity, equality of opportunity, treatment equality) as fairness standards. To this end, we evaluate the use of machine learning methods relative to different steps of the decision-making process: assigning a predictive score, linking a classification to the score, and adopting decisions based on the classification. Throughout our inquiry we use the COMPAS system, complemented by a radical simplification of it (our SAPMOC I and SAPMOC II models), as our running examples. Through these examples, we show how a system that is equally accurate for different groups may fail to comply with group-parity standards, owing to different base rates in the population. We discuss the general properties of the statistics determining the satisfaction of group-parity criteria and levels of accuracy. Using the distinction between scoring, classifying, and deciding, we argue that equalisation of classifications/decisions between groups can be achieved thorough group-dependent thresholding. We discuss contexts in which this approach may be meaningful and useful in pursuing policy objectives. We claim that the implementation of group-parity standards should be left to competent human decision-makers, under appropriate scrutiny, since it involves discretionary value-based political choices. Accordingly, predictive systems should be designed in such a way that relevant policy goals can be transparently implemented. Our paper presents three main contributions: (1) it addresses a complex predictive system through the lens of simplified toy models; (2) it argues for selective policy interventions on the different steps of automated decision-making; (3) it points to the limited significance of statistical notions of fairness to achieve social goals.",
        "DOI": "10.1007/s00146-022-01441-y",
        "affiliation_name": "Alma Mater Studiorum Università di Bologna",
        "affiliation_city": "Bologna",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Stock Price Formation: Precepts from a Multi-Agent Reinforcement Learning Model",
        "paper_author": "Lussange J.",
        "publication": "Computational Economics",
        "citied_by": "3",
        "cover_date": "2023-04-01",
        "Abstract": "In the past, the bottom-up study of financial stock markets relied on first-generation multi-agent systems (MAS) , which employed zero-intelligence agents and often required the additional implementation of so-called noise traders to emulate price formation processes. Nowadays, thanks to the tools developed in cognitive science and machine learning, MAS can quantitatively gauge agent learning, a pivotal element for information and stock price estimation in finance. In our previous work, we therefore devised a new generation MAS stock market simulator , which implements two key features: firstly, each agent autonomously learns to perform price forecasting and stock trading via model-free reinforcement learning ; secondly, all agents ’ trading decisions feed a centralised double-auction limit order book, emulating price and volume microstructures. Here, we study which trading strategies (represented as reinforcement learning policies) the agents learn and the time-dependency of their heterogeneity. Our central result is that there are more ways to succeed in trading than to fail. More specifically, we find that : i- better-performing agents learn in time more diverse trading strategies than worse-performing ones, ii- they tend to employ a fundamentalist, rather than chartist, approach to asset price valuation, and iii- their transaction orders are less stringent (i.e. larger bids or lower asks).",
        "DOI": "10.1007/s10614-022-10249-3",
        "affiliation_name": "Laboratoire d'Économie Mathématique et de Microéconomie Appliquée (LEMMA)",
        "affiliation_city": "Paris",
        "affiliation_country": "France"
    },
    {
        "paper_title": "Impacts of the COVID-19 outbreak on older-age cohorts in European Labor Markets: A machine learning exploration of vulnerable groups",
        "paper_author": "Celbiş M.G.",
        "publication": "Regional Science Policy and Practice",
        "citied_by": "8",
        "cover_date": "2023-04-01",
        "Abstract": "We identify vulnerable groups through the examination of their employment status in the face of the initial coronavirus disease 2019 (COVID-19) shock through the application of tree-based ensemble machine learning algorithms on a sample of individuals over 50 years old. The present study elaborates on the findings through various interpretable machine learning techniques, namely Shapley values, individual conditional expectations, partial dependences, and variable importance scores. The structure of the data obtained from the Survey of Health, Aging and Retirement in Europe (SHARE) dataset enables us to specifically observe the before versus the after effects of the pandemic shock on individual job status in spatial labor markets. We identify small but distinct subgroups that may require particular policy interventions. We find that the persons in these groups are prone to pandemic-related job loss owing to different sets of individual-level factors such as employment type and sector, age, education, and prepandemic health status in addition to location-specific factors such as drops in mobility and stringency policies affecting particular regions or countries.",
        "DOI": "10.1111/rsp3.12520",
        "affiliation_name": "Maastricht University School of Business and Economics",
        "affiliation_city": "Maastricht",
        "affiliation_country": "Netherlands"
    },
    {
        "paper_title": "Introducing CARESSER: A framework for in situ learning robot social assistance from expert knowledge and demonstrations",
        "paper_author": "Andriella A.",
        "publication": "User Modeling and User-Adapted Interaction",
        "citied_by": "19",
        "cover_date": "2023-04-01",
        "Abstract": "Socially assistive robots have the potential to augment and enhance therapist’s effectiveness in repetitive tasks such as cognitive therapies. However, their contribution has generally been limited as domain experts have not been fully involved in the entire pipeline of the design process as well as in the automatisation of the robots’ behaviour. In this article, we present aCtive leARning agEnt aSsiStive bEhaviouR (CARESSER), a novel framework that actively learns robotic assistive behaviour by leveraging the therapist’s expertise (knowledge-driven approach) and their demonstrations (data-driven approach). By exploiting that hybrid approach, the presented method enables in situ fast learning, in a fully autonomous fashion, of personalised patient-specific policies. With the purpose of evaluating our framework, we conducted two user studies in a daily care centre in which older adults affected by mild dementia and mild cognitive impairment (N = 22) were requested to solve cognitive exercises with the support of a therapist and later on of a robot endowed with CARESSER. Results showed that: (i) the robot managed to keep the patients’ performance stable during the sessions even more so than the therapist; (ii) the assistance offered by the robot during the sessions eventually matched the therapist’s preferences. We conclude that CARESSER, with its stakeholder-centric design, can pave the way to new AI approaches that learn by leveraging human–human interactions along with human expertise, which has the benefits of speeding up the learning process, eliminating the need for the design of complex reward functions, and finally avoiding undesired states.",
        "DOI": "10.1007/s11257-021-09316-5",
        "affiliation_name": "Universitat Internacional de Catalunya",
        "affiliation_city": "Barcelona",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "Modeling household online shopping demand in the U.S.: a machine learning approach and comparative investigation between 2009 and 2017",
        "paper_author": "Barua L.",
        "publication": "Transportation",
        "citied_by": "5",
        "cover_date": "2023-04-01",
        "Abstract": "Despite the rapid growth of online shopping and research interest in the relationship between online and in-store shopping, national-level modeling and investigation of the demand for online shopping with a prediction focus remain limited in the literature. This paper differs from prior work and leverages two recent releases of the U.S. National Household Travel Survey (NHTS) data for 2009 and 2017 to develop machine learning (ML) models, specifically gradient boosting machine (GBM), for predicting household-level online shopping purchases. The NHTS data allow for not only conducting nationwide investigation but also at the level of households, which is more appropriate than at the individual level given the connected consumption and shopping needs of members in a household. We follow a systematic procedure for model development including employing Recursive Feature Elimination algorithm to select input variables (features) in order to reduce the risk of model overfitting and increase model explainability. Among several ML models, GBM is found to yield the best prediction accuracy. Extensive post-modeling investigation is conducted in a comparative manner between 2009 and 2017, including quantifying the importance of each input variable in predicting online shopping demand, and characterizing value-dependent relationships between demand and the input variables. In doing so, two latest advances in machine learning techniques, namely Shapley value-based feature importance and Accumulated Local Effects plots, are adopted to overcome inherent drawbacks of the popular techniques in current ML modeling. The modeling and investigation are performed at the national level, with a number of findings obtained. The models developed and insights gained can be used for online shopping-related freight demand generation and may also be considered for evaluating the potential impact of relevant policies on online shopping demand.",
        "DOI": "10.1007/s11116-021-10250-z",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Chicago",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Graph-Attention-Based Casual Discovery with Trust Region-Navigated Clipping Policy Optimization",
        "paper_author": "Liu S.",
        "publication": "IEEE Transactions on Cybernetics",
        "citied_by": "6",
        "cover_date": "2023-04-01",
        "Abstract": "In many domains of empirical sciences, discovering the causal structure within variables remains an indispensable task. Recently, to tackle unoriented edges or latent assumptions violation suffered by conventional methods, researchers formulated a reinforcement learning (RL) procedure for causal discovery and equipped a REINFORCE algorithm to search for the best rewarded directed acyclic graph. The two keys to the overall performance of the procedure are the robustness of RL methods and the efficient encoding of variables. However, on the one hand, REINFORCE is prone to local convergence and unstable performance during training. Neither trust region policy optimization, being computationally expensive, nor proximal policy optimization (PPO), suffering from aggregate constraint deviation, is a decent alternative for combinatory optimization problems with considerable individual subactions. We propose a trust region-navigated clipping policy optimization method for causal discovery that guarantees both better search efficiency and steadiness in policy optimization, in comparison with REINFORCE, PPO, and our prioritized sampling-guided REINFORCE implementation. On the other hand, to boost the efficient encoding of variables, we propose a refined graph attention encoder called SDGAT that can grasp more feature information without priori neighborhood information. With these improvements, the proposed method outperforms the former RL method in both synthetic and benchmark datasets in terms of output results and optimization robustness.",
        "DOI": "10.1109/TCYB.2021.3116762",
        "affiliation_name": "National University of Defense Technology China",
        "affiliation_city": "Changsha",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Solving Dynamic Traveling Salesman Problems With Deep Reinforcement Learning",
        "paper_author": "Zhang Z.",
        "publication": "IEEE Transactions on Neural Networks and Learning Systems",
        "citied_by": "89",
        "cover_date": "2023-04-01",
        "Abstract": "A traveling salesman problem (TSP) is a well-known NP-complete problem. Traditional TSP presumes that the locations of customers and the traveling time among customers are fixed and constant. In real-life cases, however, the traffic conditions and customer requests may change over time. To find the most economic route, the decisions can be made constantly upon the time-point when the salesman completes his service of each customer. This brings in a dynamic version of the traveling salesman problem (DTSP), which takes into account the information of real-time traffic and customer requests. DTSP can be extended to a dynamic pickup and delivery problem (DPDP). In this article, we ameliorate the attention model to make it possible to perceive environmental changes. A deep reinforcement learning algorithm is proposed to solve DTSP and DPDP instances with a size of up to 40 customers in 100 locations. Experiments show that our method can capture the dynamic changes and produce a highly satisfactory solution within a very short time. Compared with other baseline approaches, more than 5% improvements can be observed in many cases.",
        "DOI": "10.1109/TNNLS.2021.3105905",
        "affiliation_name": "Newark College of Engineering",
        "affiliation_city": "Newark",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "ANFIS for prediction of epidemic peak and infected cases for COVID-19 in India",
        "paper_author": "Kumar R.",
        "publication": "Neural Computing and Applications",
        "citied_by": "8",
        "cover_date": "2023-04-01",
        "Abstract": "Corona Virus Disease 2019 (COVID-19) is a continuing extensive incident globally affecting several million people's health and sometimes leading to death. The outbreak prediction and making cautious steps is the only way to prevent the spread of COVID-19. This paper presents an Adaptive Neuro-fuzzy Inference System (ANFIS)-based machine learning technique to predict the possible outbreak in India. The proposed ANFIS-based prediction system tracks the growth of epidemic based on the previous data sets fetched from cloud computing. The proposed ANFIS technique predicts the epidemic peak and COVID-19 infected cases through the cloud data sets. The ANFIS is chosen for this study as it has both numerical and linguistic knowledge, and also has ability to classify data and identify patterns. The proposed technique not only predicts the outbreak but also tracks the disease and suggests a measurable policy to manage the COVID-19 epidemic. The obtained prediction shows that the proposed technique very effectively tracks the growth of the COVID-19 epidemic. The result shows the growth of infection rate decreases at end of 2020 and also has delay epidemic peak by 40–60 days. The prediction result using the proposed ANFIS technique shows a low Mean Square Error (MSE) of 1.184 × 10–3 with an accuracy of 86%. The study provides important information for public health providers and the government to control the COVID-19 epidemic.",
        "DOI": "10.1007/s00521-021-06412-w",
        "affiliation_name": "National Institute of Technology Nagaland",
        "affiliation_city": "Dimapur",
        "affiliation_country": "India"
    },
    {
        "paper_title": "CROP YIELD PREDICTION IN BIG DATA USING MARGALEF KERNEL PERCEPTRON BASED WINNOW BROWN BOOST CLASSIFIER",
        "paper_author": "Saritha S.",
        "publication": "Journal of Theoretical and Applied Information Technology",
        "citied_by": "0",
        "cover_date": "2023-03-31",
        "Abstract": "Crop prediction is a very difficult feature obtained by different characteristics like environment, genotype, and their associations. Policy and decision-makers depend on accurate crop yield predictions to ensure timely import and export recommendations to reinforce food security. In agriculture, boosting in machine learning (ML) is utilized to forecast crop yield. Many boosting ML approaches such as classification, prediction, and clustering predict agricultural production. Data mining techniques are a mandatory technique for achieving significant solutions for this issue. To perform the crop prediction based on different weathers in big data analytics is called as Margalef Kernel Perceptron and Winnow Brown Boosting Classification (MKP-WBBC) method. MKP-WBBC method for big data-based crop yield prediction is split into two sections, namely, feature selection and classification. First Margalef Kernel Perceptron-based feature selection is applied to the Crop Yield Prediction dataset to select computationally efficient features even in case of huge voluminous data. Second, with the unique features selected, Winnow Brown Boosting Classification is applied for accurate and precise crop yield prediction. The main contribution of the new crop yield prediction method is the potentiality to produce accurate predictions and reasonable insights in a simultaneous fashion. This was arrived at by the training and learning algorithm to choose the unique features and not only boosts the results other than enhance the cache hit rate to balance prediction accuracy for training data and generalizability to test data. A discussion of the results achieved reveals the productive performance of the MKP-WBBC method to predict crop yield accurately. Furthermore, results indicate the proposed MKP-WBBC can efficiently enhance crop yield prediction performance and analysis the existing methods in different parameters likes, feature selection accuracy, feature selection time, error rate, and air pollution prediction accuracy.",
        "DOI": "NA",
        "affiliation_name": "Sri Krishna Adithya College of Arts and Science",
        "affiliation_city": "Coimbatore",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Real-time recognition and decision making of objects using deep learning ENet based UAV images",
        "paper_author": "Ahmed S.A.",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2023-03-29",
        "Abstract": "Unmanned Aerial Vehicles (UAVs) have been found to have many uses in the maintenance and oversight of civil infrastructure assets. They contribute to scheduled bridge checkups, crisis control, electricity transmission cable oversight and traffic analysis. With more and more uses of UAVs being introduced, a greater focus on individuality and freedom regarding governance of these devices is required to ensure security, competency, and precision. The subject of this study outlines the method and policies to be followed for teaching the principals of the (efficient Neural Network) ENet architecture, machine learning, and using OpenCV to implement semantic segmentation on a collection of images obtained through aerial photography for identification of objects. Possible utilizations of UAVs in the area of transportation are mentioned as well along with the precision and efficiency of training for the application of the ENet architecture, machine learning, and OpenCV to implement semantic segmentation, the optimization selection of operational parameters, and the machine learning and ENet architecture teaching methods and policies drafting process. Through analysis of the object identification results, it was proven that by adhering to a specific set of parameters, the ENet architecture and machine learning procedures can successfully identify objects with an accuracy of 99% when there is no distortion. In addition, using a combination of all three technologies mentioned, it is possible to not only classify objects, but the device is also capable of automated tracking and detection of the objects by real-time processing of streamed videos by the UAVs. The novelty, that the ENET was applied for large class members difference distance among the same objects family.",
        "DOI": "10.1063/5.0119750",
        "affiliation_name": "Al-Kitab University",
        "affiliation_city": "Kirkuk",
        "affiliation_country": "Iraq"
    },
    {
        "paper_title": "Modeling a Conversational Agent using BDI Framework",
        "paper_author": "Ichida A.Y.",
        "publication": "Proceedings of the ACM Symposium on Applied Computing",
        "citied_by": "1",
        "cover_date": "2023-03-27",
        "Abstract": "Building conversational agents to help humans in domain-specific tasks is challenging since the agent needs to understand the natural language and act over it while accessing domain expert knowledge. Modern natural language processing techniques led to an expansion of conversational agents, with recent pretrained language models achieving increasingly accurate language recognition results using ever-larger open datasets. However, the black-box nature of such pretrained language models obscures the agent's reasoning and its motivations when responding, leading to unexplained dialogues. We develop a belief-desire-intention (BDI) agent as a task-oriented dialogue system to introduce mental attitudes similar to humans describing their behavior during a dialogue. We compare the resulting model with a pipeline dialogue model by leveraging existing components from dialogue systems and developing the agent's intention selection as a dialogue policy. We show that combining traditional agent modelling approaches, such as BDI, with more recent learning techniques can result in efficient and scrutable dialogue systems.",
        "DOI": "10.1145/3555776.3577657",
        "affiliation_name": "University of Aberdeen",
        "affiliation_city": "Aberdeen",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "AutoDOViz: Human-Centered Automation for Decision Optimization",
        "paper_author": "Weidele D.K.I.",
        "publication": "International Conference on Intelligent User Interfaces, Proceedings IUI",
        "citied_by": "5",
        "cover_date": "2023-03-27",
        "Abstract": "We present AutoDOViz, an interactive user interface for automated decision optimization (AutoDO) using reinforcement learning (RL). Decision optimization (DO) has classically being practiced by dedicated DO researchers [43] where experts need to spend long periods of time fine tuning a solution through trial-and-error. AutoML pipeline search has sought to make it easier for a data scientist to find the best machine learning pipeline by leveraging automation to search and tune the solution. More recently, these advances have been applied to the domain of AutoDO [36], with a similar goal to find the best reinforcement learning pipeline through algorithm selection and parameter tuning. However, Decision Optimization requires significantly more complex problem specification when compared to an ML problem. AutoDOViz seeks to lower the barrier of entry for data scientists in problem specification for reinforcement learning problems, leverage the benefits of AutoDO algorithms for RL pipeline search and finally, create visualizations and policy insights in order to facilitate the typical interactive nature when communicating problem formulation and solution proposals between DO experts and domain experts. In this paper, we report our findings from semi-structured expert interviews with DO practitioners as well as business consultants, leading to design requirements for human-centered automation for DO with RL. We evaluate a system implementation with data scientists and find that they are significantly more open to engage in DO after using our proposed solution. AutoDOViz further increases trust in RL agent models and makes the automated training and evaluation process more comprehensible. As shown for other automation in ML tasks [33, 59], we also conclude automation of RL for DO can benefit from user and vice-versa when the interface promotes human-in-the-loop.",
        "DOI": "10.1145/3581641.3584094",
        "affiliation_name": "University of Massachusetts Boston",
        "affiliation_city": "Boston",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Patient Cost-Sharing and Utilization of Breast Cancer Diagnostic Imaging by Patients Undergoing Subsequent Testing after a Screening Mammogram",
        "paper_author": "Hughes D.R.",
        "publication": "JAMA Network Open",
        "citied_by": "14",
        "cover_date": "2023-03-27",
        "Abstract": "IMPORTANCE Out-of-pocket costs (OOPCs) have been largely eliminated for screening mammography. However, patients still face OOPCs when undergoing subsequent diagnostic tests after the initial screening, which represents a potential barrier to those who require follow-up testing after initial testing. OBJECTIVE To examine the association between the degree of patient cost-sharing and the use of diagnostic breast cancer imaging after undergoing a screening mammogram. DESIGN, SETTING, AND PARTICIPANTS This retrospective cohort study used medical claims from Optum's deidentified Clinformatics Data Mart Database, a commercial claims database derived from a database of administrative health claims for members of large commercial and Medicare Advantage health plans. The large commercially insured cohort included female patients aged 40 years or older with no prior history of breast cancer undergoing a screening mammogram examination. Data were collected from January 1, 2015, to December 31, 2017, and analysis was conducted from January 2021 to September 2022. EXPOSURES A k-means clustering machine learning algorithm was used to classify patient insurance plans by dominant cost-sharing mechanism. Plan types were then ranked by OOPCs. MAIN OUTCOMES AND MEASURES A multivariable 2-part hurdle regression model was used to examine the association between patient OOPCs and the number and type of diagnostic breast services undergone by patients observed to undergo subsequent testing. RESULTS In our sample, 230 845 women (220 023 [95.3%] aged 40 to 64 years; 16 810 [7.3%] Black, 16 398 [7.1%] Hispanic, and 164 702 [71.3%] White) underwent a screening mammogram in 2016. These patients were covered by 22 828 distinct insurance plans associated with 6 025 741 enrollees and 44 911 473 distinct medical claims. Plans dominated by coinsurancewere found to have the lowest mean (SD) OOPCs ($945 [$1456]), followed by balanced plans ($1017 [$1386]), plans dominated by copays ($1020 [$1408]), and plans dominated by deductibles ($1186 [$1522]).Women underwent significantly fewer subsequent breast imaging procedures in dominantly copay (24 [95% CI, 11-37] procedures per 1000 women) and dominantly deductible (16 [95%CI, 5-28] procedures per 1000 women) plans compared with coinsurance plans. Patients from all plan types underwent fewer breast magnetic resonance imaging (MRI) scans than patients in the lowest OOPC plan (balanced, 5 [95%CI, 2-12] MRIs per 1000 women; copay, 6 [95%CI, 3-6] MRI per 100 women; deductible, 6 [95%CI, 3-9] MRIs per 1000 women. CONCLUSIONS AND RELEVANCE Despite policies designed to remove financial barriers to access for breast cancer screening, significant financial barriers remain for women at risk of breast cancer.",
        "DOI": "10.1001/jamanetworkopen.2023.4893",
        "affiliation_name": "Georgia Institute of Technology",
        "affiliation_city": "Atlanta",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Snape: Reliable and Low-Cost Computing with Mixture of Spot and On-Demand VMs",
        "paper_author": "Yang F.",
        "publication": "International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS",
        "citied_by": "8",
        "cover_date": "2023-03-25",
        "Abstract": "Cloud providers often have resources that are not being fully utilized, and they may offer them at a lower cost to make up for the reduced availability of these resources. However, customers may be hesitant to use such offerings (such as spot VMs) as making trade-offs between cost and resource availability is not always straightforward. In this work, we propose Snape (Spot On-demand Perfect Mixture), an intelligent framework to optimize the cost and resource availability by dynamically mixing on-demand VMs with spot VMs. Through a detailed characterization based on real production traces, we verify that the eviction of spot VMs is predictable to some extent. Snape also leverages constrained reinforcement learning to adjust the mixture policy online. Experiments across different configurations show that Snape achieves 44% savings compared to using only on-demand VMs while maintaining 99.96% availability, which is 2.77% higher than using only spot VMs.",
        "DOI": "10.1145/3582016.3582028",
        "affiliation_name": "Microsoft Corporation",
        "affiliation_city": "Redmond",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Optimizing Substance Use Treatment Selection Using Reinforcement Learning",
        "paper_author": "Baucum M.",
        "publication": "ACM Transactions on Management Information Systems",
        "citied_by": "2",
        "cover_date": "2023-03-25",
        "Abstract": "Substance use disorder (SUD) exacts a substantial economic and social cost in the United States, and it is crucial for SUD treatment providers to match patients with feasible, effective, and affordable treatment plans. The availability of large SUD patient datasets allows for machine learning techniques to predict patient-level SUD outcomes, yet there has been almost no research on whether machine learning can be used to optimize or personalize which treatment plans SUD patients receive. We use contextual bandits (a reinforcement learning technique) to optimally map patients to SUD treatment plans, based on dozens of patient-level and geographic covariates. We also use near-optimal policies to incorporate treatments' time-intensiveness and cost into our recommendations, to aid treatment providers and policymakers in allocating treatment resources. Our personalized treatment recommendation policies are estimated to yield higher remission rates than observed in our original dataset, and they suggest clinical insights to inform future research on data-driven SUD treatment matching.",
        "DOI": "10.1145/3563778",
        "affiliation_name": "University of Tennessee, Knoxville College of Nursing",
        "affiliation_city": "Knoxville",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Modeling for policy and technology assessment: Challenges from computer-based simulations and artificial intelligence",
        "paper_author": "Kaminski A.",
        "publication": "Zeitschrift fur Technikfolgenabschatzung in Theorie und Praxis / Journal for Technology Assessment in Theory and Practice",
        "citied_by": "3",
        "cover_date": "2023-03-24",
        "Abstract": "Modeling for policy has become an integral part of policy making and technology assessment. This became particularly evident to the general public when, during the COVID-19 pandemic, forecasts of infection dynamics based on computer simulations were used to evaluate and justify policy containment measures. Computer models are also playing an increasing role in technology assessment (TA). Computer simulations are used to explore possible futures related to specific technologies, for example, in the area of energy systems analysis. Artificial intelligence (AI) models are also becoming increasingly important. The results is a mix of methods where computer simulations and machine learning converge, posing particular challenges and opening up new research questions. This Special topic brings together case studies from different fields to explore the current state of computational models in general and AI methods in particular for policy and TA.",
        "DOI": "10.14512/tatup.32.1.11",
        "affiliation_name": "Karlsruher Institut für Technologie",
        "affiliation_city": "Karlsruhe",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Robot reporters, machine learning and disinformation: How artificial intelligence is revolutionizing journalism",
        "paper_author": "Paura A.",
        "publication": "Information Disorder: Learning to Recognize Fake News",
        "citied_by": "0",
        "cover_date": "2023-03-20",
        "Abstract": "In the last few years technology has changed the way we consume news as well as the way that media all over the world produces content. Media outlets use algorithms to distribute and target readers; AI is increasingly part of the daily routine of the newsroom to compile finance and sports articles and even to write book or movie reviews; machine learning tools are pivotal in data scraping and in investigative journalism, opening new frontiers for journalists and researchers. At the same time, populist movements, far-right and anti-vax activists, and regimes are exploiting these powerful tools to spread misinformation and propaganda, thanks also to permissive policies put in place by social media companies like Facebook and YouTube and messaging platforms like Telegram. But technology - together with good journalistic practices and human fact-checking - is the only way to tackle the problem of misinformation online. The article describes how AI is helping journalists and fact-checkers dealing with misinformation, and which startups are working on cutting-edge solutions using machine learning to combat fake news.",
        "DOI": "NA",
        "affiliation_name": "Blasting News",
        "affiliation_city": null,
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Assessing the climate and air quality effects of future aerosol mitigation in India using a global climate model combined with statistical downscaling",
        "paper_author": "Miinalainen T.",
        "publication": "Atmospheric Chemistry and Physics",
        "citied_by": "2",
        "cover_date": "2023-03-20",
        "Abstract": "We studied the potential of using machine learning to downscale global-scale climate model output towards ground station data. The aim was to simultaneously analyze both city-level air quality and regional- and global-scale radiative forcing values for anthropogenic aerosols. As the city-level air pollution values are typically underestimated in global-scale models, we used a machine learning approach to downscale fine particulate (PM2.5) concentrations towards measured values. We first simulated the global climate with the aerosol-climate model ECHAM-HAMMOZ and corrected the PM2.5 values for the Indian megacity New Delhi. The downscaling procedure clearly improved the seasonal variation in the model data. The seasonal trends were much better captured in the corrected PM2.5 than in original ECHAM-HAMMOZ PM2.5 when compared to the reference PM2.5 from the ground stations. However, short-term variations showed less extreme values with the downscaling approach. We applied the downscaling model also to simulations where the aerosol emissions were following two different future scenarios: one following the current legislation and one assuming currently maximum feasible emission reductions. The corrected PM2.5 concentrations for the year 2030 showed that mitigating anthropogenic aerosols improves local air quality in New Delhi, with organic carbon reductions contributing most to these improvements. In addition, aerosol emission mitigation also resulted in negative radiative forcing values over most of India. This was mainly due to reductions in absorbing black carbon emissions. For the two future emission scenarios modeled, the radiative forcing due to aerosol-radiation interactions over India was -0.09±0.26 and -0.53±0.31 W m-2, respectively, while the effective radiative forcing values were -2.1±4.6 and 0.06±3.39 W m-2, respectively. Although accompanied by relatively large uncertainties, the obtained results indicate that aerosol mitigation could bring a double benefit in India: better air quality and decreased warming of the local climate. Our results demonstrate that downscaling and bias correction allow more versatile utilization of global-scale climate models. With the help of downscaling, global climate models can be used in applications where one aims to analyze both global and regional effects of policies related to mitigating anthropogenic emissions.",
        "DOI": "10.5194/acp-23-3471-2023",
        "affiliation_name": "Itä-Suomen yliopisto",
        "affiliation_city": "Kuopio",
        "affiliation_country": "Finland"
    },
    {
        "paper_title": "Mapping soil organic carbon distribution across South Africa's major biomes using remote sensing-topo-climatic covariates and Concrete Autoencoder-Deep neural networks",
        "paper_author": "Odebiri O.",
        "publication": "Science of the Total Environment",
        "citied_by": "11",
        "cover_date": "2023-03-20",
        "Abstract": "The management of soil organic carbon (SOC) stocks remains at the forefront of greenhouse gas mitigation. However, unprecedented anthropogenic disturbances emanating from continued land-use change have significantly altered SOC distribution across global biomes leading to considerable carbon losses. Consequently, understanding the spatial distribution of SOC across different biomes, particularly at larger scales, is critical for climate change policy formulation and planning. Advancements in remote sensing, availability of big data, and deep learning architecture offer great potential in large-scale SOC mapping. In this regard, this study mapped SOC distribution across South Africa's major biomes using remotely sensed-topo-climatic data and Concrete Autoencoder-Deep neural networks (CAE-DNN). From the different deep neural frameworks tested, the CAE-DNN model (developed from 26 selected covariates) achieved the best accuracy with an RMSE value of 7.91 t/ha (about 20 % of the mean). Results further showed that SOC stock correlated with general biome coverage, as the Grassland and Savanna biomes contributed the most (32.38 % and 31.28 %) to the overall SOC pool in South Africa. However, despite their smaller footprint, Forests (44.12 t/h) and the Indian Ocean Coastal Belt (43.05 t/h) biomes demonstrated the highest SOC sequestration capacity. The restoration of degraded biomes is advocated for, in order to boost SOC storage; but a balance between carbon sequestration capacity, biodiversity health, and the adequate provision of ecosystem services must be maintained. To this end, these findings provide a guideline to facilitate sustainable SOC stock management within South Africa's major biomes and indeed other regions of the world.",
        "DOI": "10.1016/j.scitotenv.2022.161150",
        "affiliation_name": "University of KwaZulu-Natal",
        "affiliation_city": "Durban",
        "affiliation_country": "South Africa"
    },
    {
        "paper_title": "Development of a Machine Learning Model to Estimate US Firearm Homicides in Near Real Time",
        "paper_author": "Swedo E.A.",
        "publication": "JAMA Network Open",
        "citied_by": "3",
        "cover_date": "2023-03-17",
        "Abstract": "Importance: Firearm homicides are a major public health concern; lack of timely mortality data presents considerable challenges to effective response. Near real-time data sources offer potential for more timely estimation of firearm homicides. Objective: To estimate near real-time burden of weekly and annual firearm homicides in the US. Design, Setting, and Participants: In this prognostic study, anonymous, longitudinal time series data were obtained from multiple data sources, including Google and YouTube search trends related to firearms (2014-2019), emergency department visits for firearm injuries (National Syndromic Surveillance Program, 2014-2019), emergency medical service activations for firearm-related injuries (biospatial, 2014-2019), and National Domestic Violence Hotline contacts flagged with the keyword firearm (2016-2019). Data analysis was performed from September 2021 to September 2022. Main Outcomes and Measures: Weekly estimates of US firearm homicides were calculated using a 2-phase pipeline, first fitting optimal machine learning models for each data stream and then combining the best individual models into a stacked ensemble model. Model accuracy was assessed by comparing predictions of firearm homicides in 2019 to actual firearm homicides identified by National Vital Statistics System death certificates. Results were also compared with a SARIMA (seasonal autoregressive integrated moving average) model, a common method to forecast injury mortality. Results: Both individual and ensemble models yielded highly accurate estimates of firearm homicides. Individual models' mean error for weekly estimates of firearm homicides (root mean square error) varied from 24.95 for emergency department visits to 31.29 for SARIMA forecasting. Ensemble models combining data sources had lower weekly mean error and higher annual accuracy than individual data sources: the all-source ensemble model had a weekly root mean square error of 24.46 deaths and full-year accuracy of 99.74%, predicting the total number of firearm homicides in 2019 within 38 deaths for the entire year (compared with 95.48% accuracy and 652 deaths for the SARIMA model). The model decreased the time lag of reporting weekly firearm homicides from 7 to 8 months to approximately 6 weeks. Conclusions and Relevance: In this prognostic study of diverse secondary data on machine learning, ensemble modeling produced accurate near real-time estimates of weekly and annual firearm homicides and substantially decreased data source time lags. Ensemble model forecasts can accelerate public health practitioners' and policy makers' ability to respond to unanticipated shifts in firearm homicides..",
        "DOI": "10.1001/jamanetworkopen.2023.3413",
        "affiliation_name": "Centers for Disease Control and Prevention",
        "affiliation_city": "Atlanta",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Abnormal Fluctuation Risk Warning of Capital Flows: Based on Machine Learning Perspective",
        "paper_author": "Tan X.",
        "publication": "Modern Economic Science",
        "citied_by": "3",
        "cover_date": "2023-03-15",
        "Abstract": "Extreme capital flows can significantly affect a country's economic and financial conditions and pose challenges to policymaking of authorities. In particular, when the capital flow trend reverses, or an extreme capital flow event occurs, it will have a non-negligible impact on a country. As the world's largest emerging market country, China has long faced large-scale inflows and outflows of international capital. Compared with other emerging economies, China is unique in terms of economic size, financial system, exchange rate regime, and capital account openness. The events that cause abnormal cross-border capital flows in China are different from those in other emerging economies in terms of frequency of occurrence, event characteristics, driving factors, and economic consequences. It can be expected that with the two-way opening of China's financial market, the pressure on China to deal with the large-scale two-way cross-border capital flows will further increase. Based on the new macro research paradigm of capital flow at risk (CFaR), this paper uses the methods of (forecast) quantile regression and stable distribution fitting to identify the risk of China's abnormal cross-border capital flows. Based on a high-dimensional data set containing 371 indicators, the Lasso-PCA double-screening machine learning method is constructed to identify the key early warning factors of different types of risks of abnormal cross-border capital flows. The research results are as follows. (1) The risk indicators of abnormal cross-border capital flows constructed in this paper can better measure the tail risk of capital flow faced by China. (2) The expansion of long-term interest rate spreads is the key early warning factor of the inflow risk of bonds, the tightening of monetary policy in the United States is the key early warning factor of the outflow risk of bonds, the reduction in global risk appetite is the key early warning factor of the inflow risk of equities, and the rise in the nominal effective exchange rate of the US dollar is the key early warning factor of outflow risk of equities. (3) Since 2022, the Federal Reserve's continued interest rate hikes and the strengthening of the US dollar will increase the outflow risk of cross-border capital in China. Compared with previous literature, this paper expands from the following three aspects. First, from the perspective of research, this paper identifies the distribution characteristics and tail risk of cross-border capital flows based on the analysis framework of CFaR, and reveals the mechanism of risks of China's abnormal cross-border capital flows. Second, in terms of research content, this paper uses the machine learning based on high-dimensional data sets method to identify the key early warning factors that have important predictive value for China's abnormal cross-border capital flows, which expands existing research scope to a certain extent. Third, in terms of application prospects, the Lasso-PCA double-screening machine learning method constructed in this paper and indicators of China's abnormal cross-border capital flows can provide a methodological basis and data support for subsequent related research, and provide a reference for policy formulation.",
        "DOI": "10.20069/j.cnki.DJKX.202302002",
        "affiliation_name": "Beihang University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A Deep Reinforcement Learning Framework with Formal Verification",
        "paper_author": "Boudi Z.",
        "publication": "Formal Aspects of Computing",
        "citied_by": "4",
        "cover_date": "2023-03-15",
        "Abstract": "Artificial Intelligence (AI) and data are reshaping organizations and businesses. Human Resources (HR) management and talent development make no exception, as they tend to involve more automation and growing quantities of data. Because this brings implications on workforce, career transparency, and equal opportunities, overseeing what fuels AI and analytical models, their quality standards, integrity, and correctness becomes an imperative for those aspiring to such systems. Based on an ontology transformation to B-machines, this article presents an approach to constructing a valid and error-free career agent with Deep Reinforcement Learning (DRL). In short, the agent's policy is built on a framework we called Multi State-Actor (MuStAc) using a decentralized training approach. Its purpose is to predict both relevant and valid career steps to employees, based on their profiles and company pathways (observations). Observations can comprise various data elements such as the current occupation, past experiences, performance, skills, qualifications, and so on. The policy takes in all these observations and outputs the next recommended career step, in an environment set as the combination of an HR ontology and an Event-B model, which generates action spaces with respect to formal properties. The Event-B model and formal properties are derived using OWL to B transformation.",
        "DOI": "10.1145/3577204",
        "affiliation_name": "Mohammed V University in Rabat",
        "affiliation_city": "Rabat",
        "affiliation_country": "Morocco"
    },
    {
        "paper_title": "A multi-agent reinforcement learning approach for investigating and optimising peer-to-peer prosumer energy markets",
        "paper_author": "May R.",
        "publication": "Applied Energy",
        "citied_by": "23",
        "cover_date": "2023-03-15",
        "Abstract": "Current power grid infrastructure was not designed with climate change in mind, and, therefore, its stability, especially at peak demand periods, has been compromised. Furthermore, in light of the current UN's Intergovernmental Panel on Climate Change reports concerning global warming and the goal of the 2015 Paris climate agreement to constrain global temperature increase to within 1.5–2 °C above pre-industrial levels, urgent sociotechnical measures need to be taken. Together, Smart Microgrid and renewable energy technology have been proposed as a possible solution to help mitigate global warming and grid instability. Within this context, well-managed demand-side flexibility is crucial for efficiently utilising on-site solar energy. To this end, a well-designed dynamic pricing mechanism can organise the actors within such a system to enable the efficient trade of on-site energy, therefore contributing to the decarbonisation and grid security goals alluded to above. However, designing such a mechanism in an economic setting as complex and dynamic as the one above often leads to computationally intractable solutions. To overcome this problem, in this work, we use multi-agent reinforcement learning (MARL) alongside Foundation – an open-source economic simulation framework built by Salesforce Research – to design a dynamic price policy. By incorporating a peer-to-peer (P2P) community of prosumers with heterogeneous demand/supply profiles and battery storage into Foundation, our results from data-driven simulations show that MARL, when compared with a baseline fixed price signal, can learn a dynamic price signal that achieves both a lower community electricity cost, and a higher community self-sufficiency. Furthermore, emergent social–economic behaviours, such as price elasticity, and community coordination leading to high grid feed-in during periods of overall excess photovoltaic (PV) supply and, conversely, high community trading during overall low PV supply, have also been identified. Our proposed approach can be used by practitioners to aid them in designing P2P energy trading markets.",
        "DOI": "10.1016/j.apenergy.2023.120705",
        "affiliation_name": "Högskolan Dalarna",
        "affiliation_city": "Falun",
        "affiliation_country": "Sweden"
    },
    {
        "paper_title": "Carbon efficiency analysis in the provision of drinking water: Estimation of optimal greenhouse gas emissions",
        "paper_author": "Maziotis A.",
        "publication": "Journal of Cleaner Production",
        "citied_by": "10",
        "cover_date": "2023-03-15",
        "Abstract": "Assessing carbon efficiency (CE) in the provision of drinking water services is essential to achieve a net-zero greenhouse gas (GHG) urban water cycle. Previous studies evaluating the CE of water companies are very scarce and employed parametric and non-parametric. Both methodological approaches present limitations such as overfitting issues or require assumptions about the production technology which could lead to less reliable efficiency scores. To overcome these limitations, in this study, and for the first time, we estimated CE of English and Welsh water companies using the Efficiency Analysis Trees (EAT) approach. This technique brings together machine learning and non-linear programming techniques to estimate production frontier and efficiency scores. It also allowed us to quantify the optimal level of GHG emissions in the provision of water services and estimate potential GHG savings. Bootstrap truncated regression methods were employed to quantify the impact of operating characteristics on CE of water companies. The optimal level of GHG emissions was estimated to be between 0.062 and 133.03 tons of CO2 equivalent (CO2eq) per year and per connected property. The average CE was at the level of 0.632. This means that GHG emissions could reduce by 36.8% to maintain the same level of water services. Equivalently, this corresponds to a reduction of 488,321 tons of CO2eq per year. Water only companies exhibited a better performance than water and sewerage companies with an average CE of 0.785 and 0.540, respectively. The performance of the English and Welsh water companies decreased over time. In 2011 the average CE was 0.772 whereas it went down to 0.534 in 2020. It was also estimated that on average water companies could reduce 0.034 tons of CO2eq per cubic meter of drinking water supplied and 16.16 tons of CO2eq/connected property per year. The regression results showed that topography and water treatment complexity had a significant impact on CE. The conclusions of this study are relevant for policy makers to define policies toward a low-carbon urban water cycle.",
        "DOI": "10.1016/j.jclepro.2023.136304",
        "affiliation_name": "Institute of Sustainable Processes",
        "affiliation_city": "Valladolid",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "A hybrid agent-based machine learning method for human-centred energy consumption prediction",
        "paper_author": "Qiao Q.",
        "publication": "Energy and Buildings",
        "citied_by": "27",
        "cover_date": "2023-03-15",
        "Abstract": "Occupant behaviour has significant impacts on the performance of machine learning algorithms when predicting building energy consumption. Due to a variety of reasons (e.g., underperforming building energy management systems or restrictions due to privacy policies), the availability of occupational data has long been an obstacle that hinders the performance of machine learning algorithms in predicting building energy consumption. Therefore, this study proposed an agent-based machine learning model whereby agent-based modelling was employed to generate simulated occupational data as input features for machine learning algorithms for building energy consumption prediction. Boruta feature selection was also introduced in this study to select all relevant features. The results indicated that the performances of machine learning algorithms in predicting building energy consumption were significantly improved when using simulated occupational data, with even greater improvements after conducting Boruta feature selection.",
        "DOI": "10.1016/j.enbuild.2023.112797",
        "affiliation_name": "The University of Manchester",
        "affiliation_city": "Manchester",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Evaluation of projected soil organic carbon stocks under future climate and land cover changes in South Africa using a deep learning approach",
        "paper_author": "Odebiri O.",
        "publication": "Journal of Environmental Management",
        "citied_by": "15",
        "cover_date": "2023-03-15",
        "Abstract": "Environmental degradation and carbon emissions have become a major global concern. This has forced policymakers to consider strategic and long-term contingencies to increase carbon sequestration capacity and mitigate the effects of climate change. Soil organic carbon (SOC) provides a reliable long-lasting mechanism to ameliorate climate change and regulate carbon fluxes. However, unanticipated rates of climate change coupled with the dynamic nature of land-use transformation threatens current mitigation approaches and can jeopardise carbon stock assimilation. To effectively manage and protect SOC stocks, large-scale projections that accurately model both current and future SOC pools are necessary. Hence, this study modelled the effects of simulated climate and land-cover change on SOC inventories across South Africa up to the year 2050. A digital soil mapping strategy in concert with a deep neural network (DNN) was used to model current SOC stocks distribution. Subsequently, WorldClim general circulation models and a space-for-time substitution (SFTS) method were used to derive future SOC stocks under four shared socio-economic emission pathways. Results show a relatively high accuracy with RMSE of 7.44 t/h for current stocks, while future stocks ranged from 11.37 to 13.56 t/h. Depending on emission rates, results showed a reduction in SOC inventories, with overall SOC stocks declining from 5.64 Pg to between 4.97 and 5.38 Pg by 2050. Meanwhile, forests, which account for approximately 1.2 Pg of total SOC in South Africa, were found to have lost more than 1% of their total coverage by 2050. These findings provide a glimpse into the state of South Africa's current and future SOC stock inventories and the influence of climate and land-use change. These findings are valuable to among others policymakers, land use managers and climate change experts in assessing the long-term feasibility of South Africa's existing SOC management protocols and land-use planning agenda. However, to adequately protect future SOC stocks, current land-use planning frameworks need to be re-adjusted to prioritize pressing environmental concerns.",
        "DOI": "10.1016/j.jenvman.2022.117127",
        "affiliation_name": "University of KwaZulu-Natal",
        "affiliation_city": "Durban",
        "affiliation_country": "South Africa"
    },
    {
        "paper_title": "Solving 3D packing problem using Transformer network and reinforcement learning",
        "paper_author": "Que Q.",
        "publication": "Expert Systems with Applications",
        "citied_by": "24",
        "cover_date": "2023-03-15",
        "Abstract": "The three-dimensional packing problem (3D-PP) is a classic NP-hard problem in operations research and computer science. One of the most popular ways to solve the problem is heuristic methods with a search strategy. However, approaches based on machine learning have recently received widespread attention because of their efficiency. In this work, we propose a deep reinforcement learning (DRL) model to solve 3D-PP. Our method employs Transformer architecture as the policy network and uses Proximal Policy Optimization (PPO) to train the network. Compared with previous approaches using DRL, our method presents a novel state representation of packing environment, and introduces plane features for representing the length and width information of container. Our method achieves the new state-of-the-art results for using DRL to solve 3D-PP. The code of our method will be released to facilitate future research.",
        "DOI": "10.1016/j.eswa.2022.119153",
        "affiliation_name": "Xiamen University",
        "affiliation_city": "Xiamen",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Hyperparameter optimized classification pipeline for handling unbalanced urban and rural energy consumption patterns",
        "paper_author": "Kumar Panda D.",
        "publication": "Expert Systems with Applications",
        "citied_by": "5",
        "cover_date": "2023-03-15",
        "Abstract": "Energy consumer locations are required for framing effective energy policies. However, due to privacy concerns, it is becoming increasingly difficult to obtain the locational data of the consumers. Machine learning (ML) based classification strategies can be used to find the locational information of the consumers based on their historical energy consumption patterns. The ML methods in this paper are applied to the Residential Energy Consumption Survey 2009 dataset. In this dataset, the number of consumers in the urban area is higher than the rural area, thus making the classification problem unbalanced. The unbalanced classification problem has been solved in original and transformed or reduced feature space using Monte Carlo based under-sampling of the majority class datapoints. The hyperparameters for each classification algorithm family is represented as an optimized pipeline, obtained using the genetic programming (GP) optimizer. The classification performance metrics are then obtained for different algorithm families on the original and transformed feature spaces. Performance comparisons have been reported using univariate and bivariate distributions of the classification metrics viz. accuracy, geometric mean score (GMS), F1 score, precision, area under the curve (AUC) of receiver operator characteristics (ROC). The energy policy aspects for the urban and rural residential consumers based on the classification results have also been discussed.",
        "DOI": "10.1016/j.eswa.2022.119127",
        "affiliation_name": "University of Exeter",
        "affiliation_city": "Exeter",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Data-Driven Quantitative Intrinsic Hazard Criteria for Nanoproduct Development in a Safe-by-Design Paradigm: A Case Study of Silver Nanoforms",
        "paper_author": "Furxhi I.",
        "publication": "ACS Applied Nano Materials",
        "citied_by": "7",
        "cover_date": "2023-03-10",
        "Abstract": "The current European (EU) policies, that is, the Green Deal, envisage safe and sustainable practices for chemicals, which include nanoforms (NFs), at the earliest stages of innovation. A theoretically safe and sustainable by design (SSbD) framework has been established from EU collaborative efforts toward the definition of quantitative criteria in each SSbD dimension, namely, the human and environmental safety dimension and the environmental, social, and economic sustainability dimensions. In this study, we target the safety dimension, and we demonstrate the journey toward quantitative intrinsic hazard criteria derived from findable, accessible, interoperable, and reusable data. Data were curated and merged for the development of new approach methodologies, that is, quantitative structure-activity relationship models based on regression and classification machine learning algorithms, with the intent to predict a hazard class. The models utilize system (i.e., hydrodynamic size and polydispersity index) and non-system (i.e., elemental composition and core size)-dependent nanoscale features in combination with biological in vitro attributes and experimental conditions for various silver NFs, functional antimicrobial textiles, and cosmetics applications. In a second step, interpretable rules (criteria) followed by a certainty factor were obtained by exploiting a Bayesian network structure crafted by expert reasoning. The probabilistic model shows a predictive capability of ≈78% (average accuracy across all hazard classes). In this work, we show how we shifted from the conceptualization of the SSbD framework toward the realistic implementation with pragmatic instances. This study reveals (i) quantitative intrinsic hazard criteria to be considered in the safety aspects during synthesis stage, (ii) the challenges within, and (iii) the future directions for the generation and distillation of such criteria that can feed SSbD paradigms. Specifically, the criteria can guide material engineers to synthesize NFs that are inherently safer from alternative nanoformulations, at the earliest stages of innovation, while the models enable a fast and cost-efficient in silico toxicological screening of previously synthesized and hypothetical scenarios of yet-to-be synthesized NFs.",
        "DOI": "10.1021/acsanm.3c00173",
        "affiliation_name": "Kemmy Business School",
        "affiliation_city": "Limerick",
        "affiliation_country": "Ireland"
    },
    {
        "paper_title": "Sims-style ‘digital twin’ models can tell us if food systems will weather crises",
        "paper_author": "Mehrabi Z.",
        "publication": "Nature",
        "citied_by": "6",
        "cover_date": "2023-03-09",
        "Abstract": "NA",
        "DOI": "10.1038/d41586-023-00640-x",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Autonomous ships are on the horizon: here’s what we need to know",
        "paper_author": "Negenborn R.R.",
        "publication": "Nature",
        "citied_by": "55",
        "cover_date": "2023-03-02",
        "Abstract": "NA",
        "DOI": "10.1038/d41586-023-00557-5",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Learning to Design Without Prior Data: Discovering Generalizable Design Strategies Using Deep Learning and Tree Search",
        "paper_author": "Raina A.",
        "publication": "Journal of Mechanical Design",
        "citied_by": "7",
        "cover_date": "2023-03-01",
        "Abstract": "Building an Artificial Intelligence (AI) agent that can design on its own has been a goal since the 1980s. Recently, deep learning has shown the ability to learn from large-scale data, enabling significant advances in data-driven design. However, learning over prior data limits us only to solve problems that have been solved before and biases data-driven learning toward existing solutions. The ultimate goal for a design agent is the ability to learn generalizable design behavior in a problem space without having seen it before. We introduce a self-learning agent framework in this work that achieves this goal. This framework integrates a deep policy network with a novel tree search algorithm, where the tree search explores the problem space, and the deep policy network leverages self-generated experience to guide the search further. This framework first demonstrates an ability to discover high-performing generative strategies without any prior data, and second, it illustrates a zero-shot generalization of generative strategies across various unseen boundary conditions. This work evaluates the effectiveness and versatility of the framework by solving multiple versions of two engineering design problems without retraining. Overall, this paper presents a methodology to self-learn high-performing and generalizable problem-solving behavior in an arbitrary problem space, circumventing the need for expert data, existing solutions, and problem-specific learning.",
        "DOI": "10.1115/1.4056221",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Pittsburgh",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Approaches and Methods for Predicting the Trend of Scientific Outputs: A Scoping review",
        "paper_author": "Ghanadinezhad F.",
        "publication": "Scientometrics Research Journal",
        "citied_by": "1",
        "cover_date": "2023-03-01",
        "Abstract": "Purpose: Considering that the volume of publications is growing at an increasing speed, forecasting the research trend and identifying emerging issues is of particular importance. It should be noted that choosing the right method for accurately predicting the research trend is very important, which has been the focus of many researchers in recent years. In this regard, the present research aims to present the findings of research conducted in Iran and the world regarding the most important approaches and proposed methods for predicting the trend of scientific outputs in the future. Methodology: The present study was conducted using the scoping review method. The implementation of the current research includes 5 stages: 1) Identifying research objectives, 2) Identifying related research, 3) Selecting research, 4) Data extraction, and 5) Summarizing, discussing, and reporting the results. To identify relevant research, international databases in English (ScienceDirect, Emerald, Scopus, Web of Science, and ProQuest) and Iranian databases in Persian (Magiran, Noormags, Civilica, SID, and Irandoc) were searched without considering the time limit. In this research, the PRISMA diagram was used for the sampling and data selection process, and the JBI evaluation tool was used to check the quality of selected sources. Finally, 117 effects were analyzed. Findings: An overview of the studies carried out in forecasting research trends shows that these studies have attracted the most attention of researchers in the world and in Iran during the last two decades (especially from 2012 onwards), and the increasing trend in conducting these researches is evident. The most important studies conducted concerning the future of studies were the studies that identified trends and emerging research topics to determine the future direction of studies. In different periods, various approaches have been used to determine emerging issues and predict future research trends, which can be divided into 5 main cat egories: scientometric, quantitative and statistical, qualitative, mixed method, text mining, and machine learning. A review of studies showed that the most common approach used to identify emerging topics and predict future research trends was the scientometric approach. However, in recent years, due to the limitations of quantitative analysis and scientometric methods to determine the direction of future research, and with the increase in the volume of scientific production and the problems resulting from the analysis of large volumes of data, advances in computer technology and word processing tools. Text mining and machine learning approaches have been used to identify emerging areas and predict future research trends due to their high power in big data analysis along with traditional scientometric methods. The most important disciplines that have paid attention to the problem of predicting the trend of scientific outputs are related to sciences and engineering. It seems that paying attention to this issue in the mentioned fields can be because the speed of developments in these fields is higher and as a result of the necessity of conducting studies to predict the future developments of studies to synchronize and deal with them correctly before other fields and more has been The most important sources to be analyzed to achieve the future path of researches are the articles published in journals. The reason for the focus of these studies on journal articles can be that in any scientific field, articles are usually the result of research projects, theses, and other research experiences, which due to the limited access to these sources, can allow researchers to quickly access the results of these studies. and provide more convenience. On the other hand, scientific publications publish the latest scientific achievements and research findings in the shortest time, and this causes researchers and those engaged in scientific activities to be aware of the latest and most reliable scientific and research achievements. Therefore, to study the future trend of scientific outputs, articles have been considered more than other sources. Conclusion: Different models and approaches have been proposed by researchers to follow the evolution of scientific products in the future. But it seems that a combination of quantitative and qualitative approaches is needed to make accurate and reliable predictions and overcome the limitations of each of these methods. Be used simultaneously. In addition, utilizing expert opinions can be considered as a complement to scientometric analysis to predict the future. Having a future research approach in scientific policy-making and research management can play an effective role in charting the prospects of scientific development and provide the possibility of policy-making and planning for the future for researchers and stakeholders in various fields.",
        "DOI": "10.22070/rsci.2022.14777.1515",
        "affiliation_name": "Shahid Chamran University of Ahvaz",
        "affiliation_city": "Ahvaz",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "Prediction of Iranian EFL teachers' burnout level using machine learning algorithms and maslach burnout inventory",
        "paper_author": "Baniadamdizaj S.",
        "publication": "Iran Journal of Computer Science",
        "citied_by": "3",
        "cover_date": "2023-03-01",
        "Abstract": "Burnout results from constantly feeling emotional, physical, and mental stress. Most of the time, it is related to one's job and involves a sense of reduced accomplishment and loss of personal identity. Because accountability pressures, workload, and hours can increase stress, teachers are usually high achievers who like to work hard. They confront significant challenges. They must adapt curricula to a wide range of learning styles, manage to shift education policies, attend to students with special needs, and juggle administrative work. In addition, pay remains low in comparison with other graduate roles. Therefore, after prolonged exposure to poorly managed emotional and interpersonal job stress, many experience teacher burnout, resulting in employee turnover and many socio-economic problems. In this regard, accurate prediction provides essential research and decision-making benefits. To this aim, the Maslach Burnout Inventory was administered to a sample of 1433 Iranian EFL teachers. Moreover, nine different machine learning algorithms were implemented on the data set to predict burnout levels through the Python programming language. The algorithms' performances were also investigated through accuracy. In conclusion, the results of this study demonstrate the prediction of teachers' burnout levels to prevent the destructive consequences of the issue.",
        "DOI": "10.1007/s42044-022-00112-x",
        "affiliation_name": "Kharazmi University",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "Policy Capturing to Support Pilot Decision-Making: A Proof of Concept Study",
        "paper_author": "Marois A.",
        "publication": "Aviation Psychology and Applied Human Factors",
        "citied_by": "2",
        "cover_date": "2023-03-01",
        "Abstract": "Single-pilot operations are cognitively challenging for pilots and could benefit from decision-support tools to mitigate risk-prone situations. The Cognitive Shadow is a prototype tool that employs policy capturing, a data-driven technique used to model decisions, to learn users judgement policies and alert decision discrepancies from one s decision pattern. This proof-of-concept study investigates the potential of policy capturing to model pilots policies facing unstable approaches. Pilots were presented simulated cases and asked whether to continue descent or to go-around while the policy-capturing tool learned their decision pattern and provided feedback. Individual models reached mean predictive accuracy of _ 89% while the group model reached 100%. These results speak to the potential of extracting pilots knowledge using policy capturing to create decision aids.",
        "DOI": "10.1027/2192-0923/a000237",
        "affiliation_name": "Thales Research and Technology Canada",
        "affiliation_city": "Quebec",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "An Investigation of Age-Differentiated Conversations About Electronic Nicotine Delivery Systems on Reddit",
        "paper_author": "Navarro M.A.",
        "publication": "AJPM Focus",
        "citied_by": "1",
        "cover_date": "2023-03-01",
        "Abstract": "Introduction: This study analyzes age-differentiated Reddit conversations about ENDS. Methods: This study combines 2 methods to (1) predict Reddit users’ age into 2 categories (13–20 years [underage] and 21–54 years [of legal age]) using a machine learning algorithm and (2) qualitatively code ENDS-related Reddit posts within the 2 groups. The 25 posts with the highest karma score (number of upvotes minus number of downvotes) for each keyword search (i.e., query) and each predicted age group were qualitatively coded. Results: Of 9, the top 3 topics that emerged were flavor restriction policies, Tobacco 21 policies, and use. Opposition to flavor restriction policies was a prominent subcategory for both groups but was more common in the 21–54 group. The 13–20 group was more likely to discuss opposition to minimum age laws as well as access to flavored ENDS products. The 21–54 group commonly mentioned general vaping use behavior. Conclusions: Users predicted to be in the underage group posted about different ENDS-related topics on Reddit than users predicted to be in the of-legal-age group.",
        "DOI": "10.1016/j.focus.2022.100045",
        "affiliation_name": "Center for Tobacco Products (CTP)",
        "affiliation_city": "Silver Spring",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Joint Device Participation, Dataset Management, and Resource Allocation in Wireless Federated Learning via Deep Reinforcement Learning",
        "paper_author": "Chen J.",
        "publication": "IEEE Transactions on Vehicular Technology",
        "citied_by": "3",
        "cover_date": "2023-03-01",
        "Abstract": "Federated Learning (FL) enables large-scale machine learning without uploading the private data of wireless devices. Due to the heterogeneity and limitation of the devices’ resources, the FL accuracy and latency substantially depend on the device participation and training dataset size. In this letter, to strike a trade-off between the FL accuracy and FL latency, a joint device participation, dataset management and resource allocation (DPDMRA) optimization problem is investigated. To solve the non-convex optimization problem, a Markov decision process is formulated for the resource-limited wireless FL. Moreover, due to the high dimensional continuous action space, a multi-agent softmax deep double deterministic policy gradients (MASD3) method is employed to obtain the optimal DPDMRA strategies. The double actor networks and softmax operator are designed to alleviate the underestimation bias. Simulation results demonstrate that the proposed DRL method can obtain the global optimal policy without complete information in the dynamic environment. Compared with the other baseline schemes, the proposed MASD3 approach can achieve the larger system utility with the better convergence performance.",
        "DOI": "10.1109/TVT.2023.3325843",
        "affiliation_name": "Hubei University of Technology",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Which factors are associated with Open Access publishing? A Springer Nature case study",
        "paper_author": "Momeni F.",
        "publication": "Quantitative Science Studies",
        "citied_by": "4",
        "cover_date": "2023-03-01",
        "Abstract": "Open Access (OA) facilitates access to research articles. However, authors or funders often must pay the publishing costs, preventing authors who do not receive financial support from participating in OA publishing and gaining citation advantage for OA articles. OA may exacerbate existing inequalities in the publication system rather than overcome them. To investigate this, we studied 522,411 articles published by Springer Nature. Employing correlation and regression analyses, we describe the relationship between authors affiliated with countries from different income levels, their choice of publishing model, and the citation impact of their papers. A machine learning classification method helped us to explore the importance of different features in predicting the publishing model. The results show that authors eligible for article processing charge (APC) waivers publish more in gold OA journals than others. In contrast, authors eligible for an APC discount have the lowest ratio of OA publications, leading to the assumption that this discount insufficiently motivates authors to publish in gold OA journals. We found a strong correlation between the journal rank and the publishing model in gold OA journals, whereas the OA option is mostly avoided in hybrid journals. Also, results show that the countries’ income level, seniority, and experience with OA publications are the most predictive factors for OA publishing in hybrid journals.",
        "DOI": "10.1162/qss_a_00253",
        "affiliation_name": "Leibniz-Informationszentrum Wirtschaft",
        "affiliation_city": "Kiel",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Network as a Sensor for Smart Crowd Analysis and Service Improvement",
        "paper_author": "Mu M.",
        "publication": "IEEE Network",
        "citied_by": "1",
        "cover_date": "2023-03-01",
        "Abstract": "With the growing availability of data processing and machine learning infrastructures, crowd analysis is becoming an important tool to tackle economic, social, and environmental challenges in smart communities. The heterogeneous crowd movement data captured by IoT solutions can inform policy-making and quick responses to community events or incidents. However, conventional crowd-monitoring techniques using video cameras and facial recognition are intrusive to everyday life. This article introduces a novel non-intrusive crowd monitoring solution which uses 1,500+ software-defined networks (SDN) assisted WiFi access points as 24/7 sensors to monitor and analyze crowd information. Prototypes and crowd behavior models have been developed using over 900 million WiFi records captured on a university campus. We use a range of data visualization and time-series data analysis tools to uncover complex and dynamic patterns in large-scale crowd data. The results can greatly benefit organizations and individuals in smart communities for data-driven service improvement.",
        "DOI": "10.1109/MNET.001.2200345",
        "affiliation_name": "University of Northampton",
        "affiliation_city": "Northampton",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Machine Learning-Based User Scheduling in Integrated Satellite-HAPS-Ground Networks",
        "paper_author": "Dahrouj H.",
        "publication": "IEEE Network",
        "citied_by": "11",
        "cover_date": "2023-03-01",
        "Abstract": "Integrated space-Air-ground networks promise to offer a valuable solution space for empowering the sixth generation of communication networks (6G), particularly in the context of connecting the unconnected and ultraconnecting the connected. Such digital inclusion thrive makes resource management problems, especially those accounting for load-balancing considerations, of particular interest. The conventional model-based optimization methods, however, often fail to meet the real-Time processing and quality-of-service needs, due to the high heterogeneity of the space-Air-ground networks, and the typical complexity of the classical algorithms. Given the premises of artificial intelligence at automating wireless networks design and the large-scale heterogeneity of non-Terrestrial networks, this article focuses on showcasing the prospects of machine learning in the context of user scheduling in integrated space-Air-ground communications. The article first overviews the most relevant state-of-The art in the context of machine learning applications to the resource allocation problems, with a dedicated attention to space-Airground networks. The article then proposes, and shows the benefit of, one specific use case that uses ensembling deep neural networks for optimizing the user scheduling policies in integrated space-high altitude platform station (HAPS)-ground networks. Finally, the article sheds light on the challenges and open issues that promise to spur the integration of machine learning in space-Air-ground networks, namely, online HAPS power adaptation, learning-based channel sensing, data-driven multi-HAPSs resource management, and intelligent flying taxis-empowered systems.",
        "DOI": "10.1109/MNET.006.2200281",
        "affiliation_name": "King Abdullah University of Science and Technology",
        "affiliation_city": "Thuwal",
        "affiliation_country": "Saudi Arabia"
    },
    {
        "paper_title": "Deploying automated ticket router across the enterprise",
        "paper_author": "Ackerman S.",
        "publication": "AI Magazine",
        "citied_by": "0",
        "cover_date": "2023-03-01",
        "Abstract": "With the recent advances in machine learning, the use of natural language processing (NLP) technology to support various business processes has been increasing. This paper discusses the use of NLP to route more than one million live client tickets annually to the appropriate service personnel in 67 support missions across IBM. Each mission supports a product family with multiple support teams, each requiring different skills for the engineers. We discuss three important aspects of such a large-scale deployment: (i) The use of a centralized team with a common machine learning infrastructure and practices to support the entire enterprise. (ii) The processes and quality of such a deployment from the perspective of one support mission, namely, IBM's z/OS family. (iii) Careful monitoring of the deployed models to detect drifts in the routing behavior. Despite vast differences in the technical contents of the support missions, it is possible to define common processes and metrics across the enterprise, without requiring a dedicated machine learning team for each mission. In addition, we provide examples of the business policies and metrics from the perspective of the z/OS mission to demonstrate the utility of the approach and the outcome.",
        "DOI": "10.1002/aaai.12079",
        "affiliation_name": "IBM Research - Haifa",
        "affiliation_city": "Haifa",
        "affiliation_country": "Israel"
    },
    {
        "paper_title": "Online probabilistic assessment of static voltage stability margin for power systems with a high proportion of renewable energy",
        "paper_author": "Qi J.",
        "publication": "Dianli Xitong Baohu yu Kongzhi/Power System Protection and Control",
        "citied_by": "28",
        "cover_date": "2023-03-01",
        "Abstract": "The randomness, volatility and weak regulation characteristics of renewable energy have brought new challenges to the static voltage safety and stability of a power system. In view of this, an online probability evaluation method of static voltage stability margin of a power system with a high proportion of renewable energy considering the bilateral uncertainties of source and load is proposed. First, the influence of a large number of renewable energies replacing traditional units on static voltage stability margin is analyzed based on the difference of reactive power regulation characteristics between them. Then the influence of renewable energy output uncertainty on the distribution range of the stability margin is analyzed, and source and load uncertainty models are established to generate typical scenarios. Finally, in order to deal with the rapid fluctuation of stability margin brought by renewable energy, an online probability assessment method of stability margin based on optimized ELM-KDE is proposed. The stability margin of typical scenarios is predicted by an optimized extreme learning machine (ELM), and its probability distribution function is accurately obtained by kernel density estimation (KDE). The expected margin of static voltage stability and the risk of static voltage stability are constructed to characterize the results. Simulation tests are carried out on the New England 39 and IEEE 300 node systems, and the results are compared with the traditional Monte Carlo calculation results to verify the effectiveness of the proposed method.",
        "DOI": "10.19783/j.cnki.pspc.220677",
        "affiliation_name": "Wuhan University",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Getting Personal: A Deep Learning Artifact for Text-Based Measurement of Personality",
        "paper_author": "Yang K.",
        "publication": "Information Systems Research",
        "citied_by": "49",
        "cover_date": "2023-03-01",
        "Abstract": "Analysts, managers, and policymakers are interested in predictive analytics capable of offering better foresight. It is generally accepted that in forecasting scenarios involving organizational policies or consumer decision making, personal characteristics, including personality, may be an important predictor of downstream outcomes. The inclusion of personality features in forecasting models has been hindered by the fact that traditional measurement mechanisms are often infeasible. Text-based personality detection has garnered attention because of the public availability of digital textual traces. However, the text machine learning space has bifurcated into two branches: feature-based methods relying on manually crafted human intuition, or deep learning language models that leverage big data and compute, the main commonality being that neither branch generates accurate personality assessments, thereby making personality measures infeasible for downstream forecasting applications. In this study, we propose DeepPerson, a design artifact for text-based personality detection that bridges these two branches by leveraging concepts from relevant psycholinguistic theories in conjunction with advanced deep learning strategies. DeepPerson incorporates novel transfer learning and hierarchical attention networkmethods that use psychological concepts and data augmentation in conjunction with person-level linguistic information. We evaluate the utility of the proposed artifact using an extensive design evaluation on three personality data sets in comparison with state-of-the-art methods proposed in academia and industry. DeepPerson can improve detection of personality dimensions by 10-20 percentage points relative to the best comparison methods. Using case studies in the finance and health domains, we show that more accurate text-based personality detection can translate into significant improvements in downstream applications such as forecasting future firm performance or predicting pandemic infection rates. Our findings have important implications for research at the intersection of design and data science, and practical implications formanagers focused on enabling, producing, or consuming predictive analytics.",
        "DOI": "10.1287/isre.2022.1111",
        "affiliation_name": "Mendoza College of Business",
        "affiliation_city": "Notre Dame",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "A Semi-Automated Workflow for LULC Mapping via Sentinel-2 Data Cubes and Spectral Indices",
        "paper_author": "Chaves M.E.D.",
        "publication": "Automation",
        "citied_by": "8",
        "cover_date": "2023-03-01",
        "Abstract": "Land use and land cover (LULC) mapping initiatives are essential to support decision making related to the implementation of different policies. There is a need for timely and accurate LULC maps. However, building them is challenging. LULC changes affect natural areas and local biodiversity. When they cause landscape fragmentation, the mapping and monitoring of changes are affected. Due to this situation, improving the efforts for LULC mapping and monitoring in fragmented biomes and ecosystems is crucial, and the adequate separability of classes is a key factor in this process. We believe that combining multidimensional Earth observation (EO) data cubes and spectral vegetation indices (VIs) derived from the red edge, near-infrared, and shortwave infrared bands provided by the Sentinel-2/MultiSpectral Instrument (S2/MSI) mission reduces uncertainties in area estimation, leading toward more automated mappings. Here, we present a low-cost semi-automated classification scheme created to identify croplands, pasturelands, natural grasslands, and shrublands from EO data cubes and the Surface Reflectance to Vegetation Indexes (sr2vgi) tool to automate spectral index calculation, with both produced in the scope of the Brazil Data Cube (BDC) project. We used this combination of data and tools to improve LULC mapping in the Brazilian Cerrado biome during the 2018–2019 crop season. The overall accuracy (OA) of our results is (Formula presented.), indicating the potential of the proposed approach to provide timely and accurate LULC mapping from the detection of different vegetation patterns in time series.",
        "DOI": "10.3390/automation4010007",
        "affiliation_name": "Cognizant Technology Solutions",
        "affiliation_city": "Teaneck",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Type-2-Soft-Set Based Uncertainty Aware Task Offloading Framework for Fog Computing Using Apprenticeship Learning",
        "paper_author": "Bhargavi K.",
        "publication": "Cybernetics and Information Technologies",
        "citied_by": "1",
        "cover_date": "2023-03-01",
        "Abstract": "Fog computing is one of the emerging forms of cloud computing which aims to satisfy the ever-increasing computation demands of the mobile applications. Effective offloading of tasks leads to increased efficiency of the fog network, but at the same time it suffers from various uncertainty issues with respect to task demands, fog node capabilities, information asymmetry, missing information, low trust, transaction failures, and so on. Several machine learning techniques have been proposed for the task offloading in fog environments, but they lack efficiency. In this paper, a novel uncertainty proof Type-2-Soft-Set (T2SS) enabled apprenticeship learning based task offloading framework is proposed which formulates the optimal task offloading policies. The performance of the proposed T2SS based apprenticeship learning is compared and found to be better than Q-learning and State-Action-Reward-State-Action (SARSA) learning techniques with respect to performance parameters such as total execution time, throughput, learning rate, and response time.",
        "DOI": "10.2478/cait-2023-0002",
        "affiliation_name": "Siddaganga Institute of Technology",
        "affiliation_city": "Tumkur",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Rethinking Homework in the Age of Artificial Intelligence",
        "paper_author": "Ibrahim H.",
        "publication": "IEEE Intelligent Systems",
        "citied_by": "30",
        "cover_date": "2023-03-01",
        "Abstract": "The evolution of natural language processing techniques has led to the development of advanced conversational tools such as ChatGPT, capable of assisting users with a variety of activities. Media attention has centered on ChatGPT's potential impact, policy implications, and ethical ramifications, particularly in the context of education. As such tools become more accessible, students across the globe may use them to assist with their homework. However, it is still unclear whether ChatGPT's performance is advanced enough to pose a serious risk of plagiarism. We fill this gap by evaluating ChatGPT on two introductory and two advanced university-level courses. We find that ChatGPT receives near-perfect grades on the majority of questions in the introductory courses but has not yet reached the level of sophistication required to pass in advanced courses. Moreover, adding a few full stops or typos may fool a machine learning algorithm designed to detect ChatGPT-generated text. These findings suggest that, at least for some courses, current artificial intelligence tools pose a real threat that can no longer be overlooked by educational institutions.",
        "DOI": "10.1109/MIS.2023.3255599",
        "affiliation_name": "NYU Abu Dhabi",
        "affiliation_city": "Abu Dhabi",
        "affiliation_country": "United Arab Emirates"
    },
    {
        "paper_title": "Disclosing Product Availability in Online Retail",
        "paper_author": "Calvo E.",
        "publication": "Manufacturing and Service Operations Management",
        "citied_by": "25",
        "cover_date": "2023-03-01",
        "Abstract": "Problem definition: Online retailers disclose product availability to influence customer decisions as a form of pressure selling designed to compel customers to rush into a purchase. Can the revelation of this information drive sales and profitability? We study the effect of disclosing product availability on market outcomes—product sales and returns—and identify the contexts where this effect is most powerful. Academic/practical relevance: Increasing sell-out is key for online retailers to remain profitable in the presence of thin margins and complex operations. We provide insights into how their information-disclosure policy—something they can tailor at virtually no cost—can contribute to this important objective. Methodology: We collaborate with an online retailer to procure a year of transaction data on 190,696 products that span 1,290 brands and 472,980 customers. To causally identify our results, we use a generalized difference-in-differences design with matching that exploits one policy of the firm: it discloses product availability only for the last five units. Results: The disclosure of low product availability increases hourly sales—they grow by 13.6%—but these products are more likely to be returned—product return rates increase by 17.0%. Because returns are costly, we also study net sales—product hourly sales minus hourly returns—which increase by 12.5% after the retailer reveals low availability. Managerial implications: The positive effects on sales and profitability amplify over wide assortments and when low-availability signals are abundantly visible and disclosed for deeply discounted products whose sales season is about to end. In addition, we propose a data-driven policy that exploits these results by using machine learning to prescribe the timing of disclosure of scarcity signals in order to boost sales without spiking returns.",
        "DOI": "10.1287/msom.2020.0882",
        "affiliation_name": "Goizueta Business School",
        "affiliation_city": "Atlanta",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Data-Driven Optimization for Commodity Procurement Under Price Uncertainty",
        "paper_author": "Mandl C.",
        "publication": "Manufacturing and Service Operations Management",
        "citied_by": "27",
        "cover_date": "2023-03-01",
        "Abstract": "Problem definition: We study a practice-motivated multiperiod stochastic commodity procurement problem under price uncertainty with forward and spot purchase options. Existing approaches are based on parametric price models, which inevitably involve price model misspecification and generalization error. Academic/practical relevance: We propose a nonparametric, data-driven approach (DDA) that is consistent with the optimal procurement policy structure but without requiring the a priori specification and estimation of stochastic price processes. In addition to historical prices, DDA is able to leverage real-time feature data, such as economic indicators, in solving the problem. Methodology: This paper provides a framework for prescriptive analytics in dynamic commodity procurement, with optimal purchase policies learned directly from data as functions of features, via mixed integer linear programming (MILP) under cost minimization objectives. Hence, DDA focuses on optimal decisions rather than optimal predictions. Furthermore, we combine optimization with regularization from machine learning (ML) to extract decision-relevant data from noise. Results: Based on numerical experiments and empirical data, we show that there is a significant value of feature data for commodity procurement when procurement policy parameters are learned as functions of features. However, overfitting deteriorates the performance of data-driven solutions, which asks for ML extensions to improve out-of-sample generalization. Compared with an internal best-practice benchmark, DDA generates savings of on average 9.1 million euros per annum (4.33%) for 10 years of backtesting. Managerial implications: A practical benefit of DDA is that it yields simple but optimally structured decision rules that are easy to interpret and easy to operationalize. Furthermore, DDA is generalizable and applicable to many other procurement settings.",
        "DOI": "10.1287/msom.2020.0890",
        "affiliation_name": "TUM School of Management, Munich",
        "affiliation_city": "Munich",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Establishing and Applying Disaster Risk Assessment Methodologies for Climate Change Adaptation",
        "paper_author": "Kim K.J.",
        "publication": "Journal of the Architectural Institute of Korea",
        "citied_by": "0",
        "cover_date": "2023-03-01",
        "Abstract": "This study aims to establish a methodology for assessing the risks of various natural disasters and the accrued damages in Korea. With climate change and urban development, the damages caused by natural disasters have intensified and become more diversified; the government needs to establish a useful disaster risk assessment methodology for disaster response. While machine learning, big data analysis, and expert surveys have been used as disaster risk assessment methodologies in various fields, this study proposes an entropy analysis method to evaluate event occurrence risks using information and probability. To validate the effectiveness of this method, a risk assessment was conducted on eleven disaster risk factors related to climate, human life, urban, and the environment to target wind and water disasters coupled with major disasters in Korea. As a result, the semi-basement building index showed the highest risk weighting in metropolitan, provincial, and municipal areas, while the flood-prone area index showed the highest risk weighting in county and district areas. This study is expected to help the government and local governments in establishing disaster response policies, and the proposed analysis method can be used to establish standards for natural disaster risk assessment methodologies.",
        "DOI": "10.5659/JAIK.2023.39.3.161",
        "affiliation_name": "Pusan National University",
        "affiliation_city": "Busan",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "A Comparison of KNN Algorithm and MNL Model for Mode Choice Modelling",
        "paper_author": "Vinayakumar G.",
        "publication": "European Transport - Trasporti Europei",
        "citied_by": "2",
        "cover_date": "2023-03-01",
        "Abstract": "Mode choice modelling helps to identify potential users of traffic and plays an important role in policy and decision-making by the government. With the advancement of artificial intelligence and machine learning techniques, several studies were carried out to analyse the performance of mode choice models in which the backpropagation algorithm was used. However, for faster convergence of parameters, it would be interesting to explore other efficient algorithms of machine learning as the conjugated gradient search in spite of the backpropagation algorithm. The present study adds to the literature about the performance of the K-Nearest Neighbour (KNN) algorithm in mode choice modelling and compared the KNN model with the traditional MNL model. It was unveiled that the variables, which are found significant and important in both models are the same. It is also found that the KNN model is outperforming MNL with a prediction accuracy of 73.84%.",
        "DOI": "10.48295/ET.2023.92.3",
        "affiliation_name": "College of Engineering Trivandrum",
        "affiliation_city": "Thiruvananthapuram",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Erroneous High Occupancy Vehicle Lane Data: Detecting Misconfigured Traffic Sensors With Machine Learning",
        "paper_author": "Fournier N.",
        "publication": "Transportation Research Record",
        "citied_by": "3",
        "cover_date": "2023-03-01",
        "Abstract": "Quality data are vital to the planning and operation of traffic systems. High occupancy vehicle (HOV) lanes, for instance, must comply with federal performance standards. If an agency fails to meet the standards, the facility is considered to be ‘‘degraded’’ and the agency is required to undertake actions that would return the facility to satisfactory operation. This could include removing exempted vehicles (e.g., low-emission vehicles) or increasing toll prices and passenger occupancy limits. Such policy changes may be costly, and may affect related policy goals, such as promoting clean air vehicles. Owing to constant changes in the system (e.g., roadwork, system upgrades), some of the thousands of HOV sensors in California’s transportation system are misconfigured, such as being labeled as general-purpose lanes. In this situation, HOV lane data may be mistakenly aggregated with general-purpose lane data and vice versa, causing a HOV facility to be erroneously reported as degraded and requiring unnecessary policy action. Detecting these misconfigurations is challenging and labor-intensive to accomplish manually. The purpose of this research was to utilize machine learning techniques to detect sensor misconfigurations and to understand the extent to which they affect performance reporting of HOV lanes. The results for Caltrans District 7 (Los Angeles and Ventura counties) showed that about 5% to 8% of Performance Measurement System HOV sensors are misconfigured. Therefore approximately 27 to 44 mi of HOV lanes are erroneously measured, with approximately 10 to 16 mi (38%) of those reporting an erroneously high degradation rating.",
        "DOI": "10.1177/03611981221126515",
        "affiliation_name": "University of California, Berkeley",
        "affiliation_city": "Berkeley",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Machine Learning Approach-based Big Data Imputation Methods for Outdoor Air Quality forecasting",
        "paper_author": "Narasimhan D.",
        "publication": "Journal of Scientific and Industrial Research",
        "citied_by": "4",
        "cover_date": "2023-03-01",
        "Abstract": "Missing data from ambient air databases is a typical issue, but it is much worse in small towns or cities. Missing data is a significant concern for environmental epidemiology. These settings have high pollution exposure levels worldwide, and dataset gaps obstruct health investigations that could later affect local and international policies. When a substantial number of observations contain missing values, the standard errors increase due to the smaller sample size, which may significantly affect the final result. Generally, the performance of various missing value imputation algorithms is proportional to the size of the database and the percentage of missing values within it. This paper proposes and demonstrates an ensemble - imputation - classification framework approach to rebuild air quality information using a dataset from Beijing, China, to forecast air quality. Various single and multiple imputation procedures are utilized to fill the missing records. Then ensemble of diverse classifiers is used on the imputed data to find the air pollution level. The recommended model aims to reduce the error rate and improve accuracy. Extensive testing of datasets with actual missing values has revealed that the suggested methodology significantly enhances the air quality forecasting model's accuracy with multiple imputation and ensemble techniques when compared to other conventional single imputation techniques.",
        "DOI": "10.56042/jsir.v82i03.71764",
        "affiliation_name": "Srinivasa Ramanujan Centre",
        "affiliation_city": "Kumbakonam",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Quantifying the Safe Operating Space for Land-System SDG Achievement via Machine Learning and Scenario Discovery",
        "paper_author": "Khan M.S.",
        "publication": "Earth's Future",
        "citied_by": "2",
        "cover_date": "2023-03-01",
        "Abstract": "We developed a machine learning based surrogate model to identify sustainability pathways through rapid scenario generation and defined the safe operating space for achieving them via scenario discovery. We trained a surrogate model to replicate the Land-Use Trade-Offs integrated model of the Australian land system. Latin hypercube sampling was used to create many scenarios exploring the impact of uncertainties in key drivers including future socio-economic development, climate change mitigation, and agricultural productivity at a granular level. Economic and environmental impacts were evaluated against nationally downscaled SDG targets. Scenario discovery revealed new pathways to achieving five SDG targets for 2050 which required crop yield increases above 1.78 times, a carbon price above 100 AU$ tCO2−1, a >9% biodiversity levy on carbon plantings, and carefully regulated land-use policy. Machine learning based surrogate modeling teamed with scenario discovery revealed the policy and scenario settings required for a more sustainable future for the Australian land sector.",
        "DOI": "10.1029/2022EF003083",
        "affiliation_name": "Deakin University, School of Life and Environmental Sciences",
        "affiliation_city": "Melbourne",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Real-Time Detection System for Data Exfiltration over DNS Tunneling Using Machine Learning",
        "paper_author": "Abualghanam O.",
        "publication": "Electronics (Switzerland)",
        "citied_by": "11",
        "cover_date": "2023-03-01",
        "Abstract": "The domain name system (DNS) plays a vital role in network services for name resolution. By default, this service is seldom blocked by security solutions. Thus, it has been exploited for security breaches using the DNS covert channel (tunnel). One of the greatest current data leakage techniques is DNS tunneling, which uses DNS packets to exfiltrate sensitive and confidential data. Data protection against stealthy exfiltration attacks is critical for human beings and organizations. As a result, many security techniques have been proposed to address exfiltration attacks starting with building security policies and ending with designing security solutions, such as firewalls, intrusion detection or prevention, and others. In this paper, a hybrid DNS tunneling detection system has been proposed based on the packet length and selected features for the network traffic. The proposed system takes advantage of the outcome results conducted using the testbed and Tabu-PIO feature selection algorithm. The evolution of the proposed system has already been completed using three distinct datasets. The experimental outcome results show that the proposed hybrid approach achieved 98.3% accuracy and a 97.6% F-score in the DNS tunneling datasets, which outperforms the other related works’ techniques using the same datasets. Moreover, when the packet length was added into the hybrid approach, the run-time shows better results than when Tabu-PIO was used when the size of the data increases.",
        "DOI": "10.3390/electronics12061467",
        "affiliation_name": "The University of Jordan",
        "affiliation_city": "Amman",
        "affiliation_country": "Jordan"
    },
    {
        "paper_title": "Use of Artificial Intelligence Toward Climate-Neutral Cultural Heritage",
        "paper_author": "Bakirman T.",
        "publication": "Photogrammetric Engineering and Remote Sensing",
        "citied_by": "4",
        "cover_date": "2023-03-01",
        "Abstract": "Cultural heritage (CH) aims to create new strategies and policies for adapting to climate change. Additionally, the goals of sustainable development aim to protect, monitor, and preserve the world’s CH and to take urgent action to combat climate change and its effects. Therefore, developing efficient and accurate techniques toward making CH climate neutral and more resilient is of vital importance. This study aims to provide a holistic solution to monitor and protect CH from climate change, natural hazards, and anthropogenic effects in a sustainable way. In our study, the efficiency of deep learning using low-cost unmanned aerial vehicles and camera images for the documentation and monitoring of CH is investigated. The dense extreme inception network for edge detection and richer convolu-tional feature architectures have been used for the first time in the literature to extract contours and cracks from CH structures. As a result of the study, F1 scores of 61.38% and 61.50% for both ar-chitectures, respectively, were obtained. The results show that the proposed solution can aid in monitoring the protection of CH from climate change, natural disasters, and anthropogenic effects.",
        "DOI": "10.14358/PERS.22-00118R2",
        "affiliation_name": "Yıldız Teknik Üniversitesi",
        "affiliation_city": "Istanbul",
        "affiliation_country": "Turkey"
    },
    {
        "paper_title": "Data Collection Mechanism for UAV-Assisted Cellular Network Based on PPO",
        "paper_author": "Chen T.",
        "publication": "Electronics (Switzerland)",
        "citied_by": "2",
        "cover_date": "2023-03-01",
        "Abstract": "Unmanned aerial vehicles (UAVs) are increasingly gaining in application value in many fields because of their low cost, small size, high mobility and other advantages. In the scenario of traditional cellular networks, UAVs can be used as a kind of aerial mobile base station to collect information of edge users in time. Therefore, UAVs provide a promising communication tool for edge computing. However, due to the limited battery capacity, these may not be able to completely collect all the information. The path planning can ensure that the UAV collects as much data as possible under the limited flight distance, so it is very important to study the path planning of the UAV. In addition, due to the particularity of air-to-ground communication, the flying altitude of the UAV can have a crucial impact on the channel quality between the UAV and the user. As a mature technology, deep reinforcement learning (DRL) is an important algorithm in the field of machine learning which can be deployed in unknown environments. Deep reinforcement learning is applied to the data collection of UAV-assisted cellular networks, so that UAVs can find the best path planning and height joint optimization scheme, which ensures that UAVs can collect more information under the condition of limited energy consumption, save human and material resources as much as possible, and finally achieve higher application value. In this work, we transform the UAV path planning problem into an Markov decision process (MDP) problem. By applying the proximal policy optimization (PPO) algorithm, our proposed algorithm realizes the adaptive path planning of UAV. Simulations are conducted to verify the performance of the proposed scheme compared to the conventional scheme.",
        "DOI": "10.3390/electronics12061376",
        "affiliation_name": "Academy of Military Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "An iterative regulatory process for robot governance",
        "paper_author": "Drukarch H.",
        "publication": "Data and Policy",
        "citied_by": "5",
        "cover_date": "2023-03-01",
        "Abstract": "There is an increasing gap between the policy cycle's speed and that of technological and social change. This gap is becoming broader and more prominent in robotics, that is, movable machines that perform tasks either automatically or with a degree of autonomy. This is because current legislation was unprepared for machine learning and autonomous agents. As a result, the law often lags behind and does not adequately frame robot technologies. This state of affairs inevitably increases legal uncertainty. It is unclear what regulatory frameworks developers have to follow to comply, often resulting in technology that does not perform well in the wild, is unsafe, and can exacerbate biases and lead to discrimination. This paper explores these issues and considers the background, key findings, and lessons learned of the LIAISON project, which stands for Liaising robot development and policymaking, and aims to ideate an alignment model for robots' legal appraisal channeling robot policy development from a hybrid top-down/bottom-up perspective to solve this mismatch. As such, LIAISON seeks to uncover to what extent compliance tools could be used as data generators for robot policy purposes to unravel an optimal regulatory framing for existing and emerging robot technologies.",
        "DOI": "10.1017/dap.2023.3",
        "affiliation_name": "Universiteit Leiden",
        "affiliation_city": "Leiden",
        "affiliation_country": "Netherlands"
    },
    {
        "paper_title": "Flood Monitoring in the Middle and Lower Basin of the Yangtze River Using Google Earth Engine and Machine Learning Methods",
        "paper_author": "Wang J.",
        "publication": "ISPRS International Journal of Geo-Information",
        "citied_by": "6",
        "cover_date": "2023-03-01",
        "Abstract": "Under the background of intensified human activities and global climate warming, the frequency and intensity of flood disasters have increased, causing many casualties and economic losses every year. Given the difficulty of mountain shadow removal from large-scale watershed flood monitoring based on Sentinel-1 SAR images and the Google Earth Engine (GEE) cloud platform, this paper first adopted the Support Vector Machine (SVM) to extract the water body information during flooding. Then, a function model was proposed based on the mountain shadow samples to remove the mountain shadows from the flood maps. Finally, this paper analyzed the flood disasters in the middle and lower basin of the Yangtze River (MLB) in 2020. The main results showed that: (1) compared with the other two methods, the SVM model had the highest accuracy. The accuracy and kappa coefficients of the trained SVM model in the testing dataset were 97.77% and 0.9521, respectively. (2) The function model proposed based on the samples achieved the best effect compared with other shadow removal methods with a shadow recognition rate of 75.46%, and it alleviated the interference of mountain shadows for flood monitoring in a large basin. (3) The flood inundated area was 8526 km2, among which, cropland was severely affected (6160 km2). This study could provide effective suggestions for relevant stakeholders in policy making.",
        "DOI": "10.3390/ijgi12030129",
        "affiliation_name": "Aerospace Information Research Institute",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Reinforcement Learning-Based Lane Change Decision for CAVs in Mixed Traffic Flow under Low Visibility Conditions",
        "paper_author": "Gong B.",
        "publication": "Mathematics",
        "citied_by": "1",
        "cover_date": "2023-03-01",
        "Abstract": "As an important stage in the development of autonomous driving, mixed traffic conditions, consisting of connected autonomous vehicles (CAVs) and human-driven vehicles (HDVs), have attracted more and more attention. In fact, the randomness of human-driven vehicles (HDV) is the largest challenge for connected autonomous vehicles (CAV) to make reasonable decisions, especially in lane change scenarios. In this paper, we propose the problem of lane change decisions for CAV in low visibility and mixed traffic conditions for the first time. First, we consider the randomness of HDV in this environment and construct a finite state machine (FSM) model. Then, this study develops a partially observed Markov decision process (POMDP) for describing the problem of lane change. In addition, we use the modified deep deterministic policy gradient (DDPG) to solve the problem and get the optimal lane change decision in this environment. The reward designing takes the comfort, safety and efficiency of the vehicle into account, and the introduction of transfer learning accelerates the adaptation of CAV to the randomness of HDV. Finally, numerical experiments are conducted. The results show that, compared with the original DDPG, the modified DDPG has a faster convergence velocity. The strategy learned by the modified DDPG can complete the lane change in most of the scenarios. The comparison between the modified DDPG and the rule-based decisions indicates that the modified DDPG has a stronger adaptability to this special environment and can grasp more lane change opportunities.",
        "DOI": "10.3390/math11061556",
        "affiliation_name": "Jilin University",
        "affiliation_city": "Changchun",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Fuzzy Cognitive Maps: Their Role in Explainable Artificial Intelligence",
        "paper_author": "Apostolopoulos I.D.",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "14",
        "cover_date": "2023-03-01",
        "Abstract": "Currently, artificial intelligence is facing several problems with its practical implementation in various application domains. The explainability of advanced artificial intelligence algorithms is a topic of paramount importance, and many discussions have been held recently. Pioneering and classical machine learning and deep learning models behave as black boxes, constraining the logical interpretations that the end users desire. Artificial intelligence applications in industry, medicine, agriculture, and social sciences require the users’ trust in the systems. Users are always entitled to know why and how each method has made a decision and which factors play a critical role. Otherwise, they will always be wary of using new techniques. This paper discusses the nature of fuzzy cognitive maps (FCMs), a soft computational method to model human knowledge and provide decisions handling uncertainty. Though FCMs are not new to the field, they are evolving and incorporate recent advancements in artificial intelligence, such as learning algorithms and convolutional neural networks. The nature of FCMs reveals their supremacy in transparency, interpretability, transferability, and other aspects of explainable artificial intelligence (XAI) methods. The present study aims to reveal and defend the explainability properties of FCMs and to highlight their successful implementation in many domains. Subsequently, the present study discusses how FCMs cope with XAI directions and presents critical examples from the literature that demonstrate their superiority. The study results demonstrate that FCMs are both in accordance with the XAI directives and have many successful applications in domains such as medical decision-support systems, precision agriculture, energy savings, environmental monitoring, and policy-making for the public sector.",
        "DOI": "10.3390/app13063412",
        "affiliation_name": "University of Patras",
        "affiliation_city": "Rio",
        "affiliation_country": "Greece"
    },
    {
        "paper_title": "Leveraging Important Covariate Groups for Corn Yield Prediction",
        "paper_author": "Schumacher B.L.",
        "publication": "Agriculture (Switzerland)",
        "citied_by": "6",
        "cover_date": "2023-03-01",
        "Abstract": "Accurate yield information empowers farmers to adapt, their governments to adopt timely agricultural and food policy interventions, and the markets they supply to prepare for production shifts. Unfortunately, the most representative yield data in the US, provided by the US Department of Agriculture, National Agricultural Statistics Service (USDA-NASS) Surveys, are spatiotemporally patchy and inconsistent. This paper builds a more complete data product by examining the spatiotemporal efficacy of random forests (RF) in predicting county-level yields of corn—the most widely cultivated crop in the US. To meet our objective, we compare RF cross-validated prediction accuracy using several combinations of explanatory variables. We also utilize variable importance measures and partial dependence plots to compare and contextualize how key variables interact with corn yield. Results suggest that RF predicts US corn yields well using a relatively small subset of climate variables along with year and geographical location (RMSE = 17.1 bushels/acre (1.2 tons/hectare)). Of note is the insensitivity of RF prediction accuracy when removing variables traditionally thought to be predictive of yield or variables flagged as important by RF variable importance measures. Understanding what variables are needed to accurately predict corn yields provides a template for applying machine learning approaches to estimate county-level yields for other US crops.",
        "DOI": "10.3390/agriculture13030618",
        "affiliation_name": "Utah State University",
        "affiliation_city": "Logan",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Projection of COVID-19 Positive Cases Considering Hybrid Immunity: Case Study in Tokyo",
        "paper_author": "Kodera S.",
        "publication": "Vaccines",
        "citied_by": "2",
        "cover_date": "2023-03-01",
        "Abstract": "Since the emergence of COVID-19, the forecasting of new daily positive cases and deaths has been one of the essential elements in policy setting and medical resource management worldwide. An essential factor in forecasting is the modeling of susceptible populations and vaccination effectiveness (VE) at the population level. Owing to the widespread viral transmission and wide vaccination campaign coverage, it becomes challenging to model the VE in an efficient and realistic manner, while also including hybrid immunity which is acquired through full vaccination combined with infection. Here, the VE model of hybrid immunity was developed based on an in vitro study and publicly available data. Computational replication of daily positive cases demonstrates a high consistency between the replicated and observed values when considering the effect of hybrid immunity. The estimated positive cases were relatively larger than the observed value without considering hybrid immunity. Replication of the daily positive cases and its comparison would provide useful information of immunity at the population level and thus serve as useful guidance for nationwide policy setting and vaccination strategies.",
        "DOI": "10.3390/vaccines11030633",
        "affiliation_name": "Nagoya Institute of Technology",
        "affiliation_city": "Nagoya",
        "affiliation_country": "Japan"
    },
    {
        "paper_title": "Categorizing Shallow Marine Soundscapes Using Explained Clusters",
        "paper_author": "Parcerisas C.",
        "publication": "Journal of Marine Science and Engineering",
        "citied_by": "2",
        "cover_date": "2023-03-01",
        "Abstract": "Natural marine soundscapes are being threatened by increasing anthropic noise, particularly in shallow coastal waters. To preserve and monitor these soundscapes, understanding them is essential. Here, we propose a new method for semi-supervised categorization of shallow marine soundscapes, with further interpretation of these categories according to concurrent environmental conditions. The proposed methodology uses a nonlinear mapping of short-term spectrograms to a two-dimensional space, followed by a density-based clustering algorithm to identify similar sound environments. A random forest classifier, based on additional environmental data, is used to predict their occurrence. Finally, explainable machine learning tools provide insight into the ecological explanation of the clusters. This methodology was tested in the Belgian part of the North Sea, and resulted in clearly identifiable categories of soundscapes that could be explained by spatial and temporal environmental parameters, such as distance to the shore, bathymetry, tide or season. Classifying soundscapes facilitates their identification, which can be useful for policy making or conservation programs. Soundscape categorization, as proposed in this work, could be used to monitor acoustic trends and patterns in space and time that might provide useful indicators of biodiversity and ecosystem functionality change.",
        "DOI": "10.3390/jmse11030550",
        "affiliation_name": "Universiteit Gent",
        "affiliation_city": "Ghent",
        "affiliation_country": "Belgium"
    },
    {
        "paper_title": "Drivers of Realized Volatility for Emerging Countries with a Focus on South Africa: Fundamentals versus Sentiment",
        "paper_author": "Gupta R.",
        "publication": "Mathematics",
        "citied_by": "2",
        "cover_date": "2023-03-01",
        "Abstract": "We use a quantile machine learning (random forests) approach to analyse the predictive ability of newspapers-based macroeconomic attention indexes (MAIs) on eight major fundamentals of the United States on the realized volatility of a major commodity-exporting emerging stock market, namely South Africa. We compare the performance of the MAIs with the performance of a news sentiment index (NSI) of the US. We find that both fundamentals and sentiment improve predictive performance, but the relative impact of the former is stronger. We document how the impact of fundamentals and sentiment on predictive performance varies across the quantiles of the conditional distribution of realized volatility, and across different prediction horizons. Specifically, fundamentals matter more at the extreme quantiles at short horizons, and at the median in the long-run. In addition, we report several robustness checks (involving sample period and alternative definitions of realized volatility), and indicate that the obtained results for South Africa also tend to carry over to other emerging countries such as, Brazil, China, India, and Russia. Our results have important implications for investors with volatility being an input for portfolio allocation decisions. In addition, with stock market variability also capturing financial uncertainty, its accurate prediction based on US fundamentals and sentiment also has a role in policy design to prevent possible collapse.",
        "DOI": "10.3390/math11061371",
        "affiliation_name": "Helmut Schmidt University - University of the Federal Armed Forces Hamburg",
        "affiliation_city": "Hamburg",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Machine-Learning Approach for Risk Estimation and Risk Prediction of the Effect of Climate on Bovine Respiratory Disease",
        "paper_author": "Gwaka J.K.",
        "publication": "Mathematics",
        "citied_by": "3",
        "cover_date": "2023-03-01",
        "Abstract": "Bovine respiratory disease (BRD) is a major cause of illness and death in cattle; however, its global extent and distribution remain unclear. As climate change continues to impact the environment, it is important to understand the environmental factors contributing to BRD’s emergence and re-emergence. In this study, we used machine-learning models and remotely sensed climate data at 2.5 min (21 km2) resolution environmental layers to estimate the risk of BRD and predict its potential future distribution. We analysed 13,431 BRD cases from 1727 cities worldwide between 2005 and 2021 using two machine-learning models, maximum entropy (MaxEnt) and Boosted Regression Trees (BRT), to predict the risk and geographical distribution of the risk of BRD globally with varying model parameters. Different re-sampling regimes were used to visualise and measure various sources of uncertainty and prediction performance. The best-fitting model was assessed based on the area under the receiver operator curve (AUC-ROC), positive predictive power and Cohen’s Kappa. We found that BRT had better predictive power compared with MaxEnt. Our findings showed that favourable habitats for BRD occurrence were associated with the mean annual temperature, precipitation of the coldest quarter, mean diurnal range and minimum temperature of the coldest month. Similarly, we showed that the risk of BRD is not limited to the currently known suitable regions of Europe and west and central Africa but extends to other areas, such as Russia, China and Australia. This study highlights the need for global surveillance and early detection systems to prevent the spread of disease across borders. The findings also underscore the importance of bio-security surveillance and livestock sector interventions, such as policy-making and farmer education, to address the impact of climate change on animal diseases and prevent emergencies and the spread of BRD to new areas.",
        "DOI": "10.3390/math11061354",
        "affiliation_name": "Department of Mathematics, Statistics and Physics, College of Arts and Sciences, Qatar University",
        "affiliation_city": "Doha",
        "affiliation_country": "Qatar"
    },
    {
        "paper_title": "Predicting Traffic Performance During a Wildfire Using Machine Learning",
        "paper_author": "Hou Z.",
        "publication": "Transportation Research Record",
        "citied_by": "4",
        "cover_date": "2023-03-01",
        "Abstract": "Many places around the world periodically suffer from wildfires that threaten lives and disrupt normal traffic operations. Poor traffic performance during wildfires can inhibit the effectiveness of evacuations. Understanding traffic performance during a wildfire would therefore help transportation operators develop emergency traffic control plans. In this study, we developed a traffic speed and flow prediction model that uses support vector regression (SVR), for use during wildfire incidents. This was constructed using historical data for wildfires in California from 2010 to 2019, which were paired with records of the traffic speed and flow on adjacent highways and the prevailing weather conditions during the wildfire events. The results showed that traffic performance during a wildfire could be predicted using the SVR model. Based on our prediction results, we recommend that policies be implemented to encourage or mandate more detailed data collection of wildfire events, such as the fire’s boundary over time, to facilitate better prediction results in models like the one proposed in this paper. This paper should inspire further work on the topic to improve the model and provide a reliable prediction tool for transportation operators in the future.",
        "DOI": "10.1177/03611981221126509",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Davis",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Proactive Fault Prediction of Fog Devices Using LSTM-CRP Conceptual Framework for IoT Applications",
        "paper_author": "H S.",
        "publication": "Sensors",
        "citied_by": "12",
        "cover_date": "2023-03-01",
        "Abstract": "Technology plays a significant role in our daily lives as real-time applications and services such as video surveillance systems and the Internet of Things (IoT) are rapidly developing. With the introduction of fog computing, a large amount of processing has been done by fog devices for IoT applications. However, a fog device’s reliability may be affected by insufficient resources at fog nodes, which may fail to process the IoT applications. There are obvious maintenance challenges associated with many read-write operations and hazardous edge environments. To increase reliability, scalable fault-predictive proactive methods are needed that predict the failure of inadequate resources of fog devices. In this paper, a Recurrent Neural Network (RNN)-based method to predict proactive faults in the event of insufficient resources in fog devices based on a conceptual Long Short-Term Memory (LSTM) and novel Computation Memory and Power (CRP) rule-based network policy is proposed. To identify the precise cause of failure due to inadequate resources, the proposed CRP is built upon the LSTM network. As part of the conceptual framework proposed, fault detectors and fault monitors prevent the outage of fog nodes while providing services to IoT applications. The results show that the LSTM along with the CRP network policy method achieves a prediction accuracy of 95.16% on the training data and a 98.69% accuracy on the testing data, which significantly outperforms the performance of existing machine learning and deep learning techniques. Furthermore, the presented method predicts proactive faults with a normalized root mean square error of 0.017, providing an accurate prediction of fog node failure. The proposed framework experiments show a significant improvement in the prediction of inaccurate resources of fog nodes by having a minimum delay, low processing time, improved accuracy, and the failure rate of prediction was faster in comparison to traditional LSTM, Support Vector Machines (SVM), and Logistic Regression.",
        "DOI": "10.3390/s23062913",
        "affiliation_name": "Vellore Institute of Technology, Chennai",
        "affiliation_city": "Chennai",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Differentiating Fire Regimes and Their Biophysical Drivers in Central Portugal",
        "paper_author": "Bergonse R.",
        "publication": "Fire",
        "citied_by": "3",
        "cover_date": "2023-03-01",
        "Abstract": "We characterize fire regimes in central Portugal and investigate the degree to which the differences between regimes are influenced by a set of biophysical drivers. Using civil parishes as units of analysis, we employ three complementary parameters to describe the fire regime over a reference period of 44 years (1975–2018), namely cumulative percentage of parish area burned, Gini concentration index of burned area over time, and area-weighted total number of wildfires. Cluster analysis is used to aggregate parishes into groups with similar fire regimes based on these parameters. A classification tree model is then used to assess the capacity of a set of potential biophysical drivers to discriminate between the different parish groups. The results allowed us to distinguish four types of fire regime and show that these can be significantly differentiated using the biophysical drivers, of which land use/land cover (LULC), slope, and spring rainfall are the most important. Among LULC classes, shrubland and herbaceous vegetation play the foremost role, followed by agriculture. Our results highlight the importance of vegetation type, availability, and rate of regeneration, as well as that of topography, in influencing fire regimes in the study area, while suggesting that these regimes should be subject to specific wildfire prevention and mitigation policies.",
        "DOI": "10.3390/fire6030112",
        "affiliation_name": "Centro de Estudos Florestais",
        "affiliation_city": "Lisbon",
        "affiliation_country": "Portugal"
    },
    {
        "paper_title": "A Scientific Perspective on Using Artificial Intelligence in Sustainable Urban Development",
        "paper_author": "Rieder E.",
        "publication": "Big Data and Cognitive Computing",
        "citied_by": "13",
        "cover_date": "2023-03-01",
        "Abstract": "Digital transformation (or digitalization) is the process of continuous further development of digital technologies (such as smart devices, cloud services, and Big Data) that have a lasting impact on our economy and society. In this manner, digitalization is a huge driver for permanent change, even in the field of Sustainable Urban Development. In the wake of digitalization, expectations are changing, placing pressure at the societal level on the design and development of smart environments for everything that means Sustainable Urban Development. In this sense, the solution is the integration of Artificial Intelligence into Sustainable Urban Development, because technology can simplify people’s lives. The aim of this paper is to ascertain which Sustainable Urban Development dimensions are taken into account when integrating Artificial Intelligence and what results can be achieved. These questions formed the basic framework for this research article. In order to make the current state of Artificial Intelligence in Sustainable Urban Development as a snapshot visible, a systematic review of the current literature between 2012 and 2022 was conducted. The data were collected and analyzed using PRISMA. Based on the studies identified, we found a significant growth in studies, starting in 2018, and that Artificial Intelligence applications refer to the Sustainable Urban Development dimensions of environmental protection, economic development, social justice and equity, culture, and governance. The used Artificial Intelligence techniques in Sustainable Urban Development cover a broad field of Artificial Intelligence, such as Artificial Intelligence in general, Machine Learning, Deep Learning, Artificial Neuronal Networks, Operations Research, Predictive Analytics, and Data Mining. However, with the integration of Artificial Intelligence in Sustainable Urban Development, challenges are marked out. These include responsible municipal policies, awareness of data quality, privacy and data security, the formation of partnerships among stakeholders (e.g., local citizens, civil society, industry, and various levels of government), and transparency and traceability in the implementation and rollout of Artificial Intelligence. A first step was taken towards providing an overview of the possible applications of Artificial Intelligence in Sustainable Urban Development. It was clearly shown that Artificial Intelligence is also gaining ground in this sector.",
        "DOI": "10.3390/bdcc7010003",
        "affiliation_name": "Universitatea Alexandru Ioan Cuza",
        "affiliation_city": "Iasi",
        "affiliation_country": "Romania"
    },
    {
        "paper_title": "A Cognitive Model for Technology Adoption",
        "paper_author": "Sobhanmanesh F.",
        "publication": "Algorithms",
        "citied_by": "5",
        "cover_date": "2023-03-01",
        "Abstract": "The widespread adoption of advanced technologies, such as Artificial Intelligence (AI), Machine Learning, and Robotics, is rapidly increasing across the globe. This accelerated pace of change is drastically transforming various aspects of our lives and work, resulting in what is now known as Industry 4.0. As businesses integrate these technologies into their daily operations, it significantly impacts their work tasks and required skill sets. However, the approach to technological transformation varies depending on location, industry, and organization. However, there are no published methods that can adequately forecast the adoption of technology and its impact on society. It is essential to prepare for the future impact of Industry 4.0, and this requires policymakers and business leaders to be equipped with scientifically validated models and metrics. Data-driven scenario planning and decision-making can lead to better outcomes in every area of the business, from learning and development to technology investment. However, the current literature falls short in identifying effective and globally applicable strategies to predict the adoption rate of emerging technologies. Therefore, this paper proposes a novel parametric mathematical model for predicting the adoption rate of emerging technologies through a unique data-driven pipeline. This approach utilizes global indicators for countries to predict the technology adoption curves for each country and industry. The model is thoroughly validated, and the paper outlines highly promising evaluation results. The practical implications of this proposed approach are significant because it provides policymakers and business leaders with valuable insights for decision-making and scenario planning.",
        "DOI": "10.3390/a16030155",
        "affiliation_name": "Macquarie University",
        "affiliation_city": "Sydney",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Exploring Industry-Distress Effects on Loan Recovery: A Double Machine Learning Approach for Quantiles",
        "paper_author": "Chuang H.C.",
        "publication": "Econometrics",
        "citied_by": "1",
        "cover_date": "2023-03-01",
        "Abstract": "In this study, we explore the effect of industry distress on recovery rates by using the unconditional quantile regression (UQR). The UQR provides better interpretative and thus policy-relevant information on the predictive effect of the target variable than the conditional quantile regression. To deal with a broad set of macroeconomic and industry variables, we use the lasso-based double selection to estimate the predictive effects of industry distress and select relevant variables. Our sample consists of 5334 debt and loan instruments in Moody’s Default and Recovery Database from 1990 to 2017. The results show that industry distress decreases recovery rates from 15.80% to 2.94% for the 15th to 55th percentile range and slightly increases the recovery rates in the lower and the upper tails. The UQR provide quantitative measurements to the loss given default during a downturn that the Basel Capital Accord requires.",
        "DOI": "10.3390/econometrics11010006",
        "affiliation_name": "Yuan Ze University",
        "affiliation_city": "Taoyuan",
        "affiliation_country": "Taiwan"
    },
    {
        "paper_title": "Analysis of Archetypes to Determine Time Use and Workload Profiles of Spanish University Professors",
        "paper_author": "Cabero I.",
        "publication": "Education Sciences",
        "citied_by": "4",
        "cover_date": "2023-03-01",
        "Abstract": "Allocation of time use is important to develop appropriate policies, especially in terms of gender equality. Individual well-being depends on many factors, including how time is spent. Therefore, knowing and analysing the time use and workload of academic staff is relevant for academic policy making. We analyse the responses of 703 Spanish academic staff regarding different activities of paid work and household work (unpaid). We use an innovative machine learning technique in this field, archetype analysis, which we introduce step by step while exploring our data. We identify five profiles, and we examine gender inequalities. The findings indicate that there is a higher prevalence of women in the profiles with a greater workload in household activities and teaching-related activities, but the prevalence is the same in the profile with a greater workload in research activities.",
        "DOI": "10.3390/educsci13030295",
        "affiliation_name": "Universidad Jaume I",
        "affiliation_city": "Castellon de la Plana",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "Estimating the Prevalence of Dementia in India Using a Semi-Supervised Machine Learning Approach",
        "paper_author": "Jin H.",
        "publication": "Neuroepidemiology",
        "citied_by": "2",
        "cover_date": "2023-03-01",
        "Abstract": "Introduction: Accurate estimation of dementia prevalence is essential for making effective public and social care policy to support individuals and families suffering from the disease. The purpose of this paper is to estimate the prevalence of dementia in India using a semi-supervised machine learning approach based on a large nationally representative sample. Methods: The sample of this study is adults 60 years or older in the wave 1 (2017-2019) of the Longitudinal Aging Study in India (LASI). A subsample in LASI received extensive cognitive assessment and clinical consensus ratings and therefore has diagnoses of dementia. A semi-supervised machine learning model was developed to predict the status of dementia for LASI participants without diagnoses. After obtaining the predictions, sampling weights and age standardization to the World Health Organization (WHO) standard population were applied to generate the estimate for prevalence of dementia in India. Results: The prevalence of dementia for those aged 60 years and older in India was 8.44% (95% CI: 7.89%-9.01%). The age-standardized prevalence was estimated to be 8.94% (95% CI: 8.36%-9.55%). The prevalence of dementia was greater for those who were older, were females, received no education, and lived in rural areas. Discussion: The prevalence of dementia in India may be higher than prior estimates derived from local studies. These prevalence estimates provide the information necessary for making long-term planning of public and social care policy. The semi-supervised machine learning approach adopted in this paper may also be useful for other large population aging studies that have a similar data structure.",
        "DOI": "10.1159/000528904",
        "affiliation_name": "Faculty of Health and Medical Sciences",
        "affiliation_city": "Guildford",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Identifying Financial Crises Using Machine Learning on Textual Data",
        "paper_author": "Chen M.",
        "publication": "Journal of Risk and Financial Management",
        "citied_by": "6",
        "cover_date": "2023-03-01",
        "Abstract": "We use machine learning techniques on textual data to identify financial crises. The onset of a crisis and its duration have implications for real economic activity, and as such can be valuable inputs into macroprudential, monetary, and fiscal policy. The academic literature and the policy realm rely mostly on expert judgment to determine crises, often with a lag. Consequently, crisis durations and the buildup phases of vulnerabilities are usually determined only with the benefit of hindsight. Although we can identify and forecast a portion of crises worldwide to various degrees with traditional econometric techniques and using readily available market data, we find that textual data helps in reducing false positives and false negatives in out-of-sample testing of such models, especially when the crises are considered more severe. Building a framework that is consistent across countries and in real time can benefit policymakers around the world, especially when international coordination is required across different government policies.",
        "DOI": "10.3390/jrfm16030161",
        "affiliation_name": "Federal Reserve Bank of Boston",
        "affiliation_city": "Boston",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Fine scale 20-year timeseries of plantation forest evapotranspiration for the lower limestone coast",
        "paper_author": "Doody T.M.",
        "publication": "Hydrological Processes",
        "citied_by": "3",
        "cover_date": "2023-03-01",
        "Abstract": "Commercial plantation forestry is a vital industry worldwide, providing natural resources for humans and playing an important role in carbon sequestration. Historically, impacts of plantation development to natural resources such as surface and groundwater was not considered, however demand for water and drying climates has focussed attention on sustainable water use and extraction. The Lower Limestone Coast region of south-east South Australia has an extensive softwood plantation estate. The region is characterised by minimal surface water and substantial, fresh karstic groundwater resources, often <6 m below ground. Rapid expansion over shallow groundwater of the plantation hardwood estate from 2000, prompted concern about impacts of deep-rooted plantations on water resources and notably contracting wetlands. To quantify plantation estate water use, an extensive evapotranspiration (ET) field study (1999–2008) identified plantation areas with <6 m to groundwater as groundwater users, leading to water policy whereby plantations are considered licenced water users. To enable plantation expansion in the region, fine resolution, broadscale ET data is required to ensure optimum use of water licences in the forest industry and account for plantation groundwater extraction. As continual field studies are not feasible, this 20-year, monthly 30 m ET dataset for the region was developed to improve estimates of plantation water use, enabling improved, dynamic estimates of groundwater extraction. The dataset was developed using 21 long-term field sites where ET was derived based on field measured transpiration, soil moisture, canopy interaction and rainfall data. Region specific field ET was combined with machine learning and additional remote sensing layers to estimate monthly ET (R2 = 0.86) across the plantation estate, improving on a national remote sensing ET product. Currently, groundwater extraction is accounted for via modelling which excludes dynamic changes in plantation groundwater extraction, limiting understanding of the hydrological processes influencing available water resources and potentially impeding economic growth.",
        "DOI": "10.1002/hyp.14836",
        "affiliation_name": "School of Ecosystem and Forest Science",
        "affiliation_city": "Melbourne",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Compliance in Time: Lessons from the Inter-American Court of Human Rights",
        "paper_author": "Pérez-Liñán A.",
        "publication": "International Studies Review",
        "citied_by": "11",
        "cover_date": "2023-03-01",
        "Abstract": "This paper integrates the scholarship on compliance with international human rights courts to reflect upon how the literature approaches delays and compliance cycles. Building on this review, we propose a new analytical approach that helps distinguish between reparations prone to immediate or protracted implementation. We introduce two metrics to facilitate the interpretation of delays: the yearly probability of compliance and the expected time to compliance. We also show, using machine-learning tools, how scholars can reconstruct life cycles of compliance. The article illustrates the utility of this approach with an analysis of all cases decided by the Inter-American Court of Human Rights (IACtHR) between 1989 and 2019. This analytical framework provides critical insights for courts and activists seeking to promote interventions at key moments when compliance is most likely. Moreover, the study underscores important lessons for the Inter-American Human Rights System. Current concerns about a compliance \"crisis\"at the IACtHR partly reflect a failure to distinguish between reparation types and the Court's preference for reparations requiring protracted implementation. By modeling compliance life cycles, our study opens a promising research avenue that can facilitate effectual and timely policy intervention.",
        "DOI": "10.1093/isr/viac067",
        "affiliation_name": "University College London",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "A Machine Learning Approach to Predicting Academic Performance in Pennsylvania’s Schools",
        "paper_author": "Chen S.",
        "publication": "Social Sciences",
        "citied_by": "11",
        "cover_date": "2023-03-01",
        "Abstract": "Academic performance prediction is an indispensable task for policymakers. Academic performance is frequently examined using classical statistical software, which can be used to detect logical connections between socioeconomic status and academic performance. These connections, whose accuracy depends on researchers’ experience, determine prediction accuracy. To eliminate the effects of logical relationships on such accuracy, this research used ‘black box’ machine learning models extended with education and socioeconomic data on Pennsylvania to predict academic performance in the state. The decision tree, random forest, logistic regression, support vector machine, and neural network achieved testing accuracies of 48%, 54%, 50%, 51%, and 60%, respectively. The neural network model can be used by policymakers to forecast academic performance, which in turn can aid in the formulation of various policies, such as those regarding funding and teacher selection. Finally, this study demonstrated the feasibility of machine learning as an auxiliary educational decision-making tool for use in the future.",
        "DOI": "10.3390/socsci12030118",
        "affiliation_name": "University of Oxford",
        "affiliation_city": "Oxford",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Machine Learning for Data Center Optimizations: Feature Selection Using Shapley Additive exPlanation (SHAP)",
        "paper_author": "Gebreyesus Y.",
        "publication": "Future Internet",
        "citied_by": "36",
        "cover_date": "2023-03-01",
        "Abstract": "The need for artificial intelligence (AI) and machine learning (ML) models to optimize data center (DC) operations increases as the volume of operations management data upsurges tremendously. These strategies can assist operators in better understanding their DC operations and help them make informed decisions upfront to maintain service reliability and availability. The strategies include developing models that optimize energy efficiency, identifying inefficient resource utilization and scheduling policies, and predicting outages. In addition to model hyperparameter tuning, feature subset selection (FSS) is critical for identifying relevant features for effectively modeling DC operations to provide insight into the data, optimize model performance, and reduce computational expenses. Hence, this paper introduces the Shapley Additive exPlanation (SHAP) values method, a class of additive feature attribution values for identifying relevant features that is rarely discussed in the literature. We compared its effectiveness with several commonly used, importance-based feature selection methods. The methods were tested on real DC operations data streams obtained from the ENEA CRESCO6 cluster with 20,832 cores. To demonstrate the effectiveness of SHAP compared to other methods, we selected the top ten most important features from each method, retrained the predictive models, and evaluated their performance using the MAE, RMSE, and MPAE evaluation criteria. The results presented in this paper demonstrate that the predictive models trained using features selected with the SHAP-assisted method performed well, with a lower error and a reasonable execution time compared to other methods.",
        "DOI": "10.3390/fi15030088",
        "affiliation_name": "Wolaita Sodo University",
        "affiliation_city": "Sodo",
        "affiliation_country": "Ethiopia"
    },
    {
        "paper_title": "Neo-epidemiological machine learning based method for COVID-19 related estimations",
        "paper_author": "Bodaghie M.",
        "publication": "PLoS ONE",
        "citied_by": "1",
        "cover_date": "2023-03-01",
        "Abstract": "The 2019 newfound Coronavirus (COVID-19) still remains as a threatening disease of which new cases are being reported daily from all over the world. The present study aimed at estimating the related rates of morbidity, growth, and mortality for COVID-19 over a three-month period starting from Feb, 19, 2020 to May 18, 2020 in Iran. In addition, it revealed the effect of the mean age, changes in weather temperature and country’s executive policies including social distancing, restrictions on travel, closing public places, shops and educational centers. We have developed a combined neural network to estimate basic reproduction number, growth, and mortality rates of COVID-19. Required data was obtained from daily reports of World Health Organization (WHO), Iran Meteorological Organization (IRIMO) and the Statistics Center of Iran. The technique used in the study encompassed the use of Artificial Neural Network (ANN) combined with Swarm Optimization (PSO) and Bus Transportation Algorithms (BTA). The results of the present study showed that the related mortality rate of COVID-19 is in the range of [0.1], and the point 0.275 as the mortality rate provided the best results in terms of the total training and test squared errors of the network. Furthermore, the value of basic reproduction number for ANN-BTA and ANN-PSO was 1.045 and 1.065, respectively. In the present study, regarding the closest number to the regression line (0.275), the number of patients was equal to 2566200 cases (with and without clinical symptoms) and the growth rate based on arithmetic means was estimated to be 1.0411 and 1.06911, respectively. Reviewing the growth and mortality rates over the course of 90 days, after 45 days of first case detection, the highest increase in mortality rate was reported 158 cases. Also, the highest growth rate was related to the eighth and the eighteenth days after the first case report (2.33). In the present study, the weather variant in relationship to the basic reproduction number and mortality rate was estimated ineffective. In addition, the role of quarantine policies implemented by the Iranian government was estimated to be insignificant concerning the mortality rate. However, the age range was an ifluential factor in mortality rate. Finally, the method proposed in the present study cofirmed the role of the mean age of the country in the mortality rate related to COVID-19 patients at the time of research conduction. The results indicated that if sever quarantine restrictions are not applied and Iranian government does not impose effective interventions, about 60% to 70% of the population (it means around 49 to 58 million people) would be afflicted by COVID-19 during June to September 2021.",
        "DOI": "10.1371/journal.pone.0263991",
        "affiliation_name": "Tehran University of Medical Sciences",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "Does managerial short-termism always matter in a firm's corporate social responsibility performance? Evidence from China",
        "paper_author": "Xu X.",
        "publication": "Heliyon",
        "citied_by": "4",
        "cover_date": "2023-03-01",
        "Abstract": "Using data on Chinese A-share listed firms from 2008 to 2017, we explore how corporate social responsibility (CSR) performance is affected by managerial short-termism and what factors influence the association between the two. First, by employing text analysis in conjunction with machine learning, we construct a new managerial short-termism indicator. Using panel fixed models, we find that managerial short-termism has an adverse impact on CSR performance, and the results are consistent in a series of robustness checks. The heterogeneous test results show that the negative effect is significant only for firms with lower internal corporate governance, for firms in less competitive industries, for firms with less analyst attention, and for state-owned enterprises (SOEs). Additionally, a better institutional environment weakens the negative impact of managerial short-termism on CSR performance. The findings shed light on policy implications for emerging countries.",
        "DOI": "10.1016/j.heliyon.2023.e14240",
        "affiliation_name": "Zhejiang Sci-Tech University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Artificial neural network machine learning prediction of the smoking behavior and health risks perception of Indonesian health professionals",
        "paper_author": "Nuryunarsih D.",
        "publication": "Environmental Analysis Health and Toxicology",
        "citied_by": "2",
        "cover_date": "2023-03-01",
        "Abstract": "Health professionals (HPs) can play an important role in influencing the smoking behavior of their patients and the implementation of smoke-free workplace policies. In some countries physicians and dentists may not have a no-smoking policy in place. Breathing in other people’s tobacco smoke (second-hand smokers) increase the risk of smoking related diseases. Environmental Tobacco smoke ETS causes a similar range of diseases to active smoking, including various cancers, heart disease, stroke, and respiratory diseases. Little is known about the smoking-related attitudes and clinical practices of HPs in Indonesia. Evidence suggests that high smoking rates remain among male HPs; however, the risk perceptions and attitudes to smoking among Indonesian HPs have not been investigated using prediction model artificial neural networks. For this reason, we developed and validated an artificial neural network (ANN) to identify HPs with smoking behavior. The study population consisted of 240 HPs, including 108 (45%) physicians, and 132 (55%) dentists, with more female (n=159) than male participants (n=81) for both professions. Participants were randomly divided into two sets, the training (192) and test (48) sets. The input variables included gender, profession (doctor or dentist), knowledge regarding smoking-related diseases and awareness of smoking provided to their patients, smoke-free policy in place at their workplace, and smoking status. ANN was constructed with data from the training and selection sets and validated in the test set. The performance of ANN was simultaneously evaluated by discrimination and calibration. After the training, we completed the process using the test dataset with a multilayer perceptron network, determined by 36 input variables. Our results suggested that our final ANN concurrently had good precision (89%), accuracy (81%), sensitivity (85%), and area under the curve (AUC; 70%). ANN can be used as a promising tool for the prediction of smoking status based on health risk perceptions of HPs in Indonesia.",
        "DOI": "10.5620/eaht.2023003",
        "affiliation_name": "University of Nottingham",
        "affiliation_city": "Nottingham",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Public Policy Challenges, Regulations, Oversight, Technical, and Ethical Considerations for Autonomous Systems: A Survey",
        "paper_author": "Fard N.E.",
        "publication": "IEEE Technology and Society Magazine",
        "citied_by": "7",
        "cover_date": "2023-03-01",
        "Abstract": "One of the main features of autonomous control systems is solving complicated optimization problems without human intervention in the presence of uncertainty in real time [1]. Autonomous systems (ASs) must have the recognition and discretion potency, evaluation and estimate authority, and decision-making power to independently perform various tasks in a dynamic environment [3]. These systems have a variety of sensors to understand environmental information so that they can distinguish, evaluate, and make decisions based on them [2]. In addition to an autonomous single-agent system, the AS can be designed in the form of multiagents to identify high-risk, hazardous, or inaccessible areas [3], [4]. Robotics and AS fields have led to significant advances in a wide range of areas, including unmanned ground vehicles (UGVs), unmanned aerial vehicles (UAVs), unmanned maritime vehicles (UMVs), artificial intelligence (AI), and self-learning machines [5]. Navigation [6], [7], 3-D path following [8] for autonomous underwater vehicles (AUVs), AUVs for oceanographic research [9], and rescue robots [10] have benefited from the development of autonomy.",
        "DOI": "10.1109/MTS.2023.3241315",
        "affiliation_name": "Concordia University",
        "affiliation_city": "Montreal",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Predicting Low-Level Childhood Lead Exposure in Metro Atlanta Using Ensemble Machine Learning of High-Resolution Raster Cells",
        "paper_author": "Frndak S.",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "2",
        "cover_date": "2023-03-01",
        "Abstract": "Low-level lead exposure in children is a major public health issue. Higher-resolution spatial targeting would significantly improve county and state-wide policies and programs for lead exposure prevention that generally intervene across large geographic areas. We use stack-ensemble machine learning, including an elastic net generalized linear model, gradient-boosted machine, and deep neural network, to predict the number of children with venous blood lead levels (BLLs) ≥2 to <5 µg/dL and ≥5 µg/dL in ~1 km2 raster cells in the metro Atlanta region using a sample of 92,792 children ≤5 years old screened between 2010 and 2018. Permutation-based predictor importance and partial dependence plots were used for interpretation. Maps of predicted vs. observed values were generated to compare model performance. According to the EPA Toxic Release Inventory for air-based toxic release facility density, the percentage of the population below the poverty threshold, crime, and road network density was positively associated with the number of children with low-level lead exposure, whereas the percentage of the white population was inversely associated. While predictions generally matched observed values, cells with high counts of lead exposure were underestimated. High-resolution geographic prediction of lead-exposed children using ensemble machine learning is a promising approach to enhance lead prevention efforts.",
        "DOI": "10.3390/ijerph20054477",
        "affiliation_name": "University at Buffalo, The State University of New York",
        "affiliation_city": "Buffalo",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Chidroid: A Mobile Android Application for Log Collection and Security Analysis in Healthcare and IoMT",
        "paper_author": "Karagiannis S.",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "11",
        "cover_date": "2023-03-01",
        "Abstract": "The Internet of Medical Things (IoMT) is a growing trend that has led to the use of connected devices, known as the Internet of Health. The healthcare domain has been a target of cyberattacks, especially with a large number of IoMT devices connected to hospital networks. This factor could allow attackers to access patients’ personal health information (PHI). This research paper proposes Chidroid, an innovative mobile Android application that can retrieve, collect, and distribute logs from smart healthcare devices. The proposed approach enables the creation of datasets, allowing non-structured data to be parsed into semi-structured or structured data that can be used for machine learning and deep learning, and the proposed approach can serve as a universal policy-based tool to examine and analyse security issues in most recent Android versions by distributing logs for analysis. The validation tests demonstrated that the application could retrieve logs and system metrics from various assets and devices in an efficient manner. The collected logs can provide visibility into the device’s activities and help to detect and mitigate potential security risks. This research introduces a way to perform a security analysis on Android devices that uses minimal system resources and reduces battery consumption by pushing the analysis stage to the edge.",
        "DOI": "10.3390/app13053061",
        "affiliation_name": "Department of Informatics, Ionian University",
        "affiliation_city": "Corfu",
        "affiliation_country": "Greece"
    },
    {
        "paper_title": "Forecast of Advanced Human Capital Gap Based on PSO-BP Neural Network and Coordination Pathway: Example of Beijing–Tianjin–Hebei Region",
        "paper_author": "He M.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "2",
        "cover_date": "2023-03-01",
        "Abstract": "The upgrading of human capital caused by education is significant to regional development. Reasonable predictions of the degree of advanced human capital in different regions are effective for formulating reasonable talent policies and accelerating regional coordinated development. The BP neural network is a widely used prediction technology. PSO-BP neural network has good global search ability, which can accelerate the convergence speed of traditional BP neural network, which is suitable for forecasting larger data. The study takes the provincial data of China from 2005 to 2019 as an example, using PSO-BP neural network algorithm to predict the advanced level of human capital through the influencing factors filtered by OLS regression. The results show that: (1) Innovation ability and urbanization can play a decisive role in advanced human capital filtered by OLS regression; (2) The results of predicting the development trend of advanced human capital in the Beijing–Tianjin–Hebei region in 2020–2025 through the PSO-BP neural network have showed that there is still a large gap between the senior human capital stock in Hebei-Beijing-Tianjin in terms of total and per capita in 2020–2025 compared with other regions in east of China; (3) Giving full attention to elaborate the positive role of economic quality and quantity development are suitable for narrowing the difference of advanced human capital in this region. Through the method of OLS-BP-neural network, this study explores the gap and influencing factors of the Beijing–Tianjin–Hebei region, excavates the reasons for the huge gradient difference in the development of this region, and extends the machine learning prediction method to the analysis of the advanced level of human capital and the research of narrowing the regional development gap.",
        "DOI": "10.3390/su15054671",
        "affiliation_name": "Peking University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Estimating Financial Fraud through Transaction-Level Features and Machine Learning",
        "paper_author": "Alwadain A.",
        "publication": "Mathematics",
        "citied_by": "11",
        "cover_date": "2023-03-01",
        "Abstract": "In today’s world, financial institutions (FIs) play a pivotal role in any country’s economic growth and are vital for intermediation between the providers of investable funds, such as depositors, investors and users. FIs focus on developing effective policies for financial fraud risk mitigation however, timely prediction of financial fraud risk helps overcome it effectively and efficiently. Thus, herein, we propose a novel approach for predicting financial fraud using machine learning. We have used transaction-level features of 6,362,620 transactions from a synthetic dataset and have fed them to various machine-learning classifiers. The correlation of different features is also analysed. Furthermore, around 5000 more data samples were generated using a Conditional Generative Adversarial Network for Tabular Data (CTGAN). The evaluation of the proposed predictor showed higher accuracies which outperformed the previously existing machine-learning-based approaches. Among all 27 classifiers, XGBoost outperformed all other classifiers in terms of accuracy score with 0.999 accuracies, however, when evaluated through exhaustive repeated 10-fold cross-validation, the XGBoost still gave an average accuracy score of 0.998. The findings are particularly relevant to financial institutions and are important for regulators and policymakers who aim to develop new and effective policies for risk mitigation against financial fraud.",
        "DOI": "10.3390/math11051184",
        "affiliation_name": "University of Management and Technology Lahore",
        "affiliation_city": "Lahore",
        "affiliation_country": "Pakistan"
    },
    {
        "paper_title": "Prediction of Irrigation Water Requirements for Green Beans-Based Machine Learning Algorithm Models in Arid Region",
        "paper_author": "Mokhtar A.",
        "publication": "Water Resources Management",
        "citied_by": "14",
        "cover_date": "2023-03-01",
        "Abstract": "Water scarcity is the most obstacle faced by irrigation water requirements, likewise, limited available meteorological data to calculate reference evapotranspiration. Consequently, the focal aims of the investigation are to assess the potential of machine learning models in forecasting irrigation water requirements (IWR) of snap beans by evolving multi-scenarios of inputs parameters to figure out the impact of meteorological, crop, and soil parameters on IWR. Six models were applied, support vector regressor (SVR), random forest (RF), deep neural networks (DNN), convolutional neural networks (CNN), long short-term memory (LSTM), and Hybrid CNN-LSTM. Ten variables including maximum and minimum temperature, Relative humidity, wind speed, precipitation, root depth, basal crop coefficient, soil evaporation, a fraction of surface wetted and, exposed and soil wetted fraction were used as the input data for models with their combination, 8 input scenarios were designed. Overall models, the best scenario was scenario 4 (relative humidity, wind speed, basal crop coefficient, soil evaporation), however, the best scenario for DNN and RF model was scenario 7 (root depth, basal crop coefficient, soil evaporation, fraction of surface wetted, exposed and soil wetted fraction). While the weakest one was the group of climatic factors in scenario 6 (maximum temperature, minimum temperature, relative humidity, wind speed, and precipitation). Among the models, the hybrid LTSM & CNN was the most accurate and the SVR model had the lowest estimation accuracy. The outcomes of this research work could set up a modeling strategy that would set in motion the improvement of efforts to identify the shortages in IWR forecasting, which sequentially may support alleviation strategies such as policies for sustainable water use and water resources management. The current approach was promising and has research value for other similar regions.",
        "DOI": "10.1007/s11269-023-03443-x",
        "affiliation_name": "Agricultural Engineering Research Institute (AENRI)",
        "affiliation_city": "Giza",
        "affiliation_country": "Egypt"
    },
    {
        "paper_title": "Manufacturing Energy Efficiency and Industry 4.0",
        "paper_author": "Salonitis K.",
        "publication": "Energies",
        "citied_by": "6",
        "cover_date": "2023-03-01",
        "Abstract": "This Special Issue of Energies was devoted to the topic of “Manufacturing Energy Efficiency and Industry 4.0”. To a great extent, this issue follows the successful previous Special Issue on “Energy Efficiency of Manufacturing Processes and Systems”, which attracted some significant attention from scholars, practitioners, and policy-makers from all over the world. In total, six papers were published. The main topics included energy efficiency improvement in both the manufacturing process and system levels, as well as how this can be facilitated through the use of Industry 4.0.",
        "DOI": "10.3390/en16052268",
        "affiliation_name": "Cranfield University",
        "affiliation_city": "Cranfield",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Modeling Production-Living-Ecological Space for Chengdu, China: An Analytical Framework Based on Machine Learning with Automatic Parameterization of Environmental Elements",
        "paper_author": "Cao Q.",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "2",
        "cover_date": "2023-03-01",
        "Abstract": "Cities worldwide are facing the dual pressures of growing population and land expansion, leading to the intensification of conflicts in urban productive-living-ecological spaces (PLES). Therefore, the question of “how to dynamically judge the different thresholds of different indicators of PLES” plays an indispensable role in the studies of the multi-scenario simulation of land space changes and needs to be tackled in an appropriate way, given that the process simulation of key elements that affect the evolution of urban systems is yet to achieve complete coupling with PLES utilization configuration schemes. In this paper, we developed a scenario simulation framework combining the dynamic coupling model of Bagging-Cellular Automata (Bagging-CA) to generate various environmental element configuration patterns for urban PLES development. The key merit of our analytical approach is that the weights of different key driving factors under different scenarios are obtained through the automatic parameterized adjustment process, and we enrich the study cases for the vast southwest region in China, which is beneficial for balanced development between eastern and western regions in the country. Finally, we simulate the PLES with the data of finer land use classification, combining a machine learning and multi-objective scenario. Automatic parameterization of environmental elements can help planners and stakeholders understand more comprehensively the complex land space changes caused by the uncertainty of space resources and environment changes, so as to formulate appropriate policies and effectively guide the implementation of land space planning. The multi-scenario simulation method developed in this study has offered new insights and high applicability to other regions for modeling PLES.",
        "DOI": "10.3390/ijerph20053911",
        "affiliation_name": "KU Leuven",
        "affiliation_city": "Leuven",
        "affiliation_country": "Belgium"
    },
    {
        "paper_title": "AI and Energy Justice",
        "paper_author": "Noorman M.",
        "publication": "Energies",
        "citied_by": "10",
        "cover_date": "2023-03-01",
        "Abstract": "Artificial intelligence (AI) techniques are increasingly used to address problems in electricity systems that result from the growing supply of energy from dynamic renewable sources. Researchers have started experimenting with data-driven AI technologies to, amongst other uses, forecast energy usage, optimize cost-efficiency, monitor system health, and manage network congestion. These technologies are said to, on the one hand, empower consumers, increase transparency in pricing, and help maintain the affordability of electricity in the energy transition, while, on the other hand, they may decrease transparency, infringe on privacy, or lead to discrimination, to name a few concerns. One key concern is how AI will affect energy justice. Energy justice is a concept that has emerged predominantly in social science research to highlight that energy related decisions—in particular, as part of the energy transition—should produce just outcomes. The concept has been around for more than a decade, but research that investigates energy (in)justice in the context of digitalized and data-driven electricity systems is still rather scarce. In particular, there is a lack of scholarship focusing on the challenges and questions that arise from the use of AI technologies in the management of electricity systems. The central question of this paper is, therefore: what may be the implications of the use of AI in smart electricity systems from the perspective of energy justice, and what does this mean for the design and regulation of these technologies?",
        "DOI": "10.3390/en16052110",
        "affiliation_name": "Tilburg Institute for Law, Technology, and Society",
        "affiliation_city": "Tilburg",
        "affiliation_country": "Netherlands"
    },
    {
        "paper_title": "Multi-Channel Assessment Policies for Energy-Efficient Data Transmission in Wireless Underground Sensor Networks",
        "paper_author": "Soundararajan R.",
        "publication": "Energies",
        "citied_by": "27",
        "cover_date": "2023-03-01",
        "Abstract": "Wireless Underground Sensor Networks (WUGSNs) transmit data collected from underground objects such as water substances, oil substances, soil contents, and others. In addition, the underground sensor nodes transmit the data to the surface nodes regarding underground irregularities, earthquake, landslides, military border surveillance, and other issues. The channel difficulties of WUGSNs create uncertain communication barriers. Recent research works have proposed different types of channel assessment techniques and security approaches. Moreover, the existing techniques are inadequate to learn the real-time channel attributes in order to build reactive data transmission models. The proposed system implements Deep Learning-based Multi-Channel Learning and Protection Model (DMCAP) using the optimal set of channel attribute classification techniques. The proposed model uses Multi-Channel Ensemble Model, Ensemble Multi-Layer Perceptron (EMLP) Classifiers, Nonlinear Channel Regression models and Nonlinear Entropy Analysis Model, and Ensemble Nonlinear Support Vector Machine (ENLSVM) for evaluating the channel conditions. Additionally, Variable Generative Adversarial Network (VGAN) engine makes the intrusion detection routines under distributed environment. According to the proposed principles, WUGSN channels are classified based on the characteristics such as underground acoustic channels, underground to surface channels and surface to ground station channels. On the classified channel behaviors, EMLP and ENLSVM are operated to extract the Signal to Noise Interference Ratio (SNIR) and channel entropy distortions of multiple channels. Furthermore, the nonlinear regression model was trained for understanding and predicting the link (channel behaviors). The proposed DMCAP has extreme difficulty finding the differences of impacts due to channel issues and malicious attacks. In this regard, the VGAN-Intrusion Detection System (VGAN-IDS) model was configured in the sensor nodes to monitor the channel instabilities against malicious nodes. Thus, the proposed system deeply analyzes multi-channel attribute qualities to improve throughput in uncertain WUGSN. The testbed was created for classified channel parameters (acoustic and air) with uncertain network parameters; the uncertainties of testbed are considered as link failures, noise distortions, interference, node failures, and number of retransmissions. Consequently, the experimental results show that DMCAP attains 10% to 15% of better performance than existing systems through better throughput, minimum retransmission rate, minimum delay, and minimum energy consumption rate. The existing techniques such as Support Vector Machine (SVM) and Random Forest (RF)-based Classification (SMC), Optimal Energy-Efficient Transmission (OETN), and channel-aware multi-path routing principles using Reinforcement Learning model (CRLR) are identified as suitable for the proposed experiments.",
        "DOI": "10.3390/en16052285",
        "affiliation_name": "Amrita Vishwa Vidyapeetham Chennai Campus",
        "affiliation_city": "Vengal",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Contributions of various driving factors to air pollution events: Interpretability analysis from Machine learning perspective",
        "paper_author": "Li T.",
        "publication": "Environment International",
        "citied_by": "34",
        "cover_date": "2023-03-01",
        "Abstract": "The air quality in China has been improved substantially, however fine particulate matter (PM2.5) still remain at a high level in many areas. PM2.5 pollution is a complex process that is attributed to gaseous precursors, chemical, and meteorological factors. Quantifying the contribution of each variable to air pollution can facilitate the formulation of effective policies to precisely eliminate air pollution. In this study, we first used decision plot to map out the decision process of the Random Forest (RF) model for a single hourly data set and constructed a framework for analyzing the causes of air pollution using multiple interpretable methods. Permutation importance was used to qualitatively analyze the effect of each variable on PM2.5 concentrations. The sensitivity of secondary inorganic aerosols (SIA): SO42-, NO3- and NH4+ to PM2.5 was verified by Partial dependence plot (PDP). Shapley Additive Explanation (Shapley) was used to quantify the contribution of drivers behind the ten air pollution events. The RF model can accurately predict PM2.5 concentrations, with determination coefficient (R2) of 0.94, root mean square error (RMSE) and mean absolute error (MAE) of 9.4 μg/m3 and 5.7 μg/m3, respectively. This study revealed that the order of sensitivity of SIA to PM2.5 was NH4+＞NO3-＞SO42-. Fossil fuel and biomass combustion may be contributing factors to air pollution events in Zibo in 2021 autumn–winter. NH4+ contributed 19.9–65.4 μg/m3 among ten air pollution events (APs). K, NO3-, EC and OC were the other main drivers, contributing 8.7 ± 2.7 μg/m3, 6.8 ± 7.5 μg/m3, 3.6 ± 5.8 μg/m3 and 2.5 ± 2.0 μg/m3, respectively. Lower temperature and higher humidity were vital factors that promoted the formation of NO3-. Our study may provide a methodological framework for precise air pollution management.",
        "DOI": "10.1016/j.envint.2023.107861",
        "affiliation_name": "Shandong University",
        "affiliation_city": "Jinan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Detecting Parallel Covert Data Transmission Channels in Video Conferencing Using Machine Learning",
        "paper_author": "Joseph O.",
        "publication": "Electronics (Switzerland)",
        "citied_by": "1",
        "cover_date": "2023-03-01",
        "Abstract": "Covert communication channels are a concept in which a policy-breaking method is used in order to covertly transmit data from inside an organization to an external or accessible point. VoIP and Video systems are exposed to such attacks on different layers, such as the underlying real-time transport protocol (RTP) which uses Transmission Control Protocol (TCP) or User Datagram Protocol (UDP) packet streams to punch a hole through Network address translation (NAT). This paper presents different innovative attack methods utilizing covert communication and RTP channels to spread malware or to create a data leak channel between different organizations. The demonstrated attacks are based on a UDP punch hole created using Skype peer-to-peer video conferencing communication. The different attack methods were successfully able to transmit a small text file in an undetectable manner by observing the communication channel, and without causing interruption to the audio/video channels or creating a noticeable disturbance to the quality. While these attacks are hard to detect by the eye, we show that applying classical Machine Learning algorithms to detect these covert channels on statistical features sampled from the communication channel is effective for one type of attack.",
        "DOI": "10.3390/electronics12051091",
        "affiliation_name": "Ariel University",
        "affiliation_city": "Ariel",
        "affiliation_country": "Israel"
    },
    {
        "paper_title": "Meta-Reinforcement Learning with Dynamic Adaptiveness Distillation",
        "paper_author": "Hu H.",
        "publication": "IEEE Transactions on Neural Networks and Learning Systems",
        "citied_by": "7",
        "cover_date": "2023-03-01",
        "Abstract": "Deep reinforcement learning is confronted with problems of sampling inefficiency and poor task migration capability. Meta-reinforcement learning (meta-RL) enables meta-learners to utilize the task-solving skills trained on similar tasks and quickly adapt to new tasks. However, meta-RL methods lack enough queries toward the relationship between task-agnostic exploitation of data and task-related knowledge introduced by latent context, limiting their effectiveness and generalization ability. In this article, we develop an algorithm for off-policy meta-RL that can provide the meta-learners with self-oriented cognition toward how they adapt to the family of tasks. In our approach, we perform dynamic task-adaptiveness distillation to describe how the meta-learners adjust the exploration strategy in the meta-training process. Our approach also enables the meta-learners to balance the influence of task-agnostic self-oriented adaption and task-related information through latent context reorganization. In our experiments, our method achieves 10%-20% higher asymptotic reward than probabilistic embeddings for actor-critic RL (PEARL).",
        "DOI": "10.1109/TNNLS.2021.3105407",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Evaluation of Land Use Land Cover Changes in Nan Province, Thailand, Using Multi-Sensor Satellite Data and Google Earth Engine",
        "paper_author": "Kruasilp J.",
        "publication": "Environment and Natural Resources Journal",
        "citied_by": "16",
        "cover_date": "2023-03-01",
        "Abstract": "Land use and land cover (LULC) conversion has become a chronic problem in Nan province. The primary factors of changes are lacking arable land, agricultural practices, and agriculture expansion. This study evaluated the usefulness of multi-sensor Landsat-5 (LS5), Landsat-8 (LS8), Sentinel-1 (S1), and Sentinel-2 (S2) satellite data for monitoring changes in LULC in Nan province, Thailand during a 30-year period (1990-2019), using a random forest (RF) model and the cloud-based Google Earth Engine (GEE) platform. Information of established land management policies was also used to describe the LULC changes. The median composite of the input variables selection from multi-sensor data were used to generate datasets. A total of 36 datasets showed the overall accuracy (OA) ranged from 51.70% to 96.95%. Sentinel-2 satellite images combined with the Modified Soil-Adjusted Vegetation Index (MSAVI) and topographic variables provided the highest OA (96.95%). Combination of optical (i.e., S2 and LS8) and S1 Synthetic Aperture Radar (SAR) data expressed better classification accuracy than individual S1 data. Forest cover decreased continuously during five consecutive periods. Coverage of maize and Pará rubber trees rapidly expanded in 2010-2014. These changes indicate an adverse consequence of the established economic development promoted by industrial and export agriculture. The findings strongly support the use of the RF technique, GEE platform and multi-sensor satellite data to enhance LULC classification accuracy in mountainous area. This study recommended that certain informative and science-based evidence will encourage local policymakers to identify priority areas for land management and natural resource conservation.",
        "DOI": "10.32526/ennrj/21/202200200",
        "affiliation_name": "Faculty of Environment and Resource Studies, Mahidol University",
        "affiliation_city": "Nakhon Pathom",
        "affiliation_country": "Thailand"
    },
    {
        "paper_title": "Distributional reinforcement learning for inventory management in multi-echelon supply chains",
        "paper_author": "Wu G.",
        "publication": "Digital Chemical Engineering",
        "citied_by": "16",
        "cover_date": "2023-03-01",
        "Abstract": "Reinforcement Learning (RL) is an effective method to solve stochastic sequential decision-making problems. This is a problem description common to supply chain operations, however, most RL algorithms are tailored for game-based benchmarks. Here, we propose a deep RL method tailored for supply chain problems. The proposed algorithm deploys a derivative free approach to balance exploration and exploitation of the neural policy's parameter space, providing means to avoid low quality local optima. Furthermore, the method allows consideration of risk-sensitive formulations to learn a policy that optimizes, for example, the conditional value-at-risk. The capabilities of our algorithm are tested on a multi-echelon supply chain problem, and several combinatorial optimization problems. The results empirically demonstrate the method's improved sample efficiency compared to the benchmark algorithm proximal policy optimization, and superior performance to shrinking horizon mixed integer formulations. Additionally, its risk-sensitive policy can offer protection from low probability, high severity scenarios. Finally, we provide a sensitivity analysis for technical intuition.",
        "DOI": "10.1016/j.dche.2022.100073",
        "affiliation_name": "Department of Chemical Engineering and Analytical Science",
        "affiliation_city": "Manchester",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Economic growth, renewable and nonrenewable electricity consumption: Fresh evidence from a panel sample of African countries",
        "paper_author": "Espoir D.K.",
        "publication": "Energy Nexus",
        "citied_by": "15",
        "cover_date": "2023-03-01",
        "Abstract": "Energy transition has imposed a policy priority dilemma between economic growth and global warming mitigation. Existing studies in Africa have examined the impact of energy sources on growth but overlooked the differences across countries and regions. This study seeks to achieve two research objectives. First, it examines and compares the impact of renewable electricity consumption (REC) and nonrenewable electricity consumption (NREC) on growth in 51 African countries between 1980 and 2018. The study uses the recent panel estimators of cross-sectional dependence, slope heterogeneity, and cointegration. For the short and long-run marginal effects, the pooled mean group estimator is used. Second, the analysis is extended to account for the heterogeneous effects of energy among African countries in four regional economic communities (EAC, COMESA, SADC, and ECOWAS). Here, we use the random-coefficients linear regression and kernel-based regularized least squares machine learning algorithm. The findings are as follows: (1) there is cointegration amongst the variables, (2) for the entire sample, both REC and NREC have positive and significant effects on growth, but NREC has an enormous impact, (3) the marginal effects of REC and NREC differ across African regions. Given the energy transition dilemma, there is a need for public-private partnership investments to bring a balanced mix between NREC and REC. In addition, the heterogeneity effect suggests that a one-size-fit-all policy designed to increase growth through REC may not yield the same outcome in Africa. Therefore, while policies should speak to the common global agenda, there is a need to internalise and localise the strategies in each country and/or region.",
        "DOI": "10.1016/j.nexus.2022.100165",
        "affiliation_name": "University of Johannesburg",
        "affiliation_city": "Johannesburg",
        "affiliation_country": "South Africa"
    },
    {
        "paper_title": "Water-Food-Energy Nexus in Global Cities: Addressing Complex Urban Interdependencies",
        "paper_author": "Hachaichi M.",
        "publication": "Water Resources Management",
        "citied_by": "13",
        "cover_date": "2023-03-01",
        "Abstract": "Understanding how water, food, and energy interact in the form of the water-food-energy (WFE) nexus is essential for sustainable development which advocates enhancing human well-being and poverty reduction. Moreover, the application of the WFE nexus has seen diverse approaches to its implementation in cities across the globe. There is a need to share knowledge in order to improve urban information exchange which focuses on the WFE nexus’ application and impacts on the United Nations (UN) Sustainable Development Goals. In this study, Natural Language Processing (NLP) and Affinity Propagation Algorithm (APA) are employed to explore and assess the application of the WFE nexus first on a regional basis and then on the city level. The results show that after the exhaustive search of a database containing 32,736 case studies focusing on 2,233 cities, African and Latin American cities have the most potential to encounter resource shortages (i.e., WFE limitation) and are systematically underrepresented in literature. In addition, the study shows that Southern hemisphere cities can benefit from knowledge transfer because of their limited urban intelligence programmes. Hence, with regional and topic bias, there is a potential for more mutual learning links between cities that can increase WFE nexus policy exchange between the Northern and Southern hemispheres through the bottom-up case-study knowledge.",
        "DOI": "10.1007/s11269-023-03455-7",
        "affiliation_name": "Université Grenoble Alpes",
        "affiliation_city": "Saint Martin d'Heres",
        "affiliation_country": "France"
    },
    {
        "paper_title": "A review of the application of artificial intelligence to nuclear reactors: Where we are and what's next",
        "paper_author": "Huang Q.",
        "publication": "Heliyon",
        "citied_by": "37",
        "cover_date": "2023-03-01",
        "Abstract": "As a form of clean energy, nuclear energy has unique advantages compared to other energy sources in the present era, where low-carbon policies are being widely advocated. The exponential growth of artificial intelligence (AI) technology in recent decades has resulted in new opportunities and challenges in terms of improving the safety and economics of nuclear reactors. This study briefly introduces modern AI algorithms such as machine learning, deep learning, and evolutionary computing. Furthermore, several studies on the use of AI techniques for nuclear reactor design optimization as well as operation and maintenance (O&M) are reviewed and discussed. The existing obstacles that prevent the further fusion of AI and nuclear reactor technologies so that they can be scaled to real-world problems are classified into two categories: (1) data issues: insufficient experimental data increases the possibility of data distribution drift and data imbalance; (2) black-box dilemma: methods such as deep learning have poor interpretability. Finally, this study proposes two directions for the future fusion of AI and nuclear reactor technologies: (1) better integration of domain knowledge with data-driven approaches to reduce the high demand for data and improve the model performance and robustness; (2) promoting the use of explainable artificial intelligence (XAI) technologies to enhance the transparency and reliability of the model. In addition, causal learning warrants further attention owing to its inherent ability to solve out-of-distribution generalization (OODG) problems.",
        "DOI": "10.1016/j.heliyon.2023.e13883",
        "affiliation_name": "Nuclear Power Institute of China",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Nighttime light satellite images reveal uneven socioeconomic development along China's land border",
        "paper_author": "Wan N.",
        "publication": "Applied Geography",
        "citied_by": "14",
        "cover_date": "2023-03-01",
        "Abstract": "China shares its board with one developed and thirteen developing countries. A timely, precise, and efficient socioeconomic study of border regions is vital for evaluating political problems and identifying potential economic prospects. Usually, conventional socioeconomic statistical data suffer from significant time lags and unequal statistical scales. This study utilized the random forest model to establish a connection between satellite-derived nighttime light data and the improved human development index (IHDI). The relationship was then applied to predict the IHDI, and differences in its strength, trend, and change pattern by bordering statistical units from 2000 to 2020 were evaluated. Our findings indicate that China's administrative units (AUCs) are more developed and have a greater development trend than their neighbors (AUNs). Except for the Tibet Autonomous Region, all AUCs are spatially more developed than AUNs, with the discrepancy widening between 2000 and 2020. Socioeconomic changes in AUCs predominantly exhibit a forward-leaping development pattern, which may be represented by a logarithmic (53%) or sigmoid (22.6%) function, whereas AUNs' socioeconomic changes exhibit either a late-leaping exponential (34.2%) or static development (18.6%) trend. The IHDI values in AUCs exhibit greater disparity as measured by the Theil index, than the AUNs, primarily due to the natural environment, resource availability, and development policies. In less developed regions, harsh natural surroundings, temperatures, and scarce natural resources hinder socioeconomic growth.",
        "DOI": "10.1016/j.apgeog.2023.102899",
        "affiliation_name": "Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Crude oil price prediction using deep reinforcement learning",
        "paper_author": "Liang X.",
        "publication": "Resources Policy",
        "citied_by": "14",
        "cover_date": "2023-03-01",
        "Abstract": "Crude oil price forecasting has received considerable attention owing to its significance in the commodity market and non-linear complexity in forecasting tasks. This study aims to develop a novel deep reinforcement learning algorithm for multi-step ahead crude oil price forecasting in three major commodity exchanges. The proposed algorithm includes two main improvements: (a) A dynamic action exploration mechanism based on the stochastic processes conforming to commodity price fluctuations is designed for accuracy and generalization. (b) A dynamic update policy of network parameters based on approximate optimization theory is developed to improve the network's learning efficiency. The algorithm's effectiveness is experimentally verified and compared with five state-of-the-art algorithms. The main findings are as follows. (a) DRL's forecasting ability is developed in crude oil price forecasting, which may be extended to the forecasting of other natural resource prices. (b) The proposed algorithm can be applied to the data of the world's three major crude oil price benchmarks with considerable universality. (c) The accuracy of the proposed algorithm declines indistinctively with the expansion of the forecasting step; however, it reflects the actual price and fluctuation. These findings have implications in accelerating the global economic recovery and exploring AI in the energy market.",
        "DOI": "10.1016/j.resourpol.2023.103363",
        "affiliation_name": "Business School of Sichuan University",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Synthesising evidence of the effects of COVID-19 regulatory changes on methadone treatment for opioid use disorder: implications for policy",
        "paper_author": "Krawczyk N.",
        "publication": "The Lancet Public Health",
        "citied_by": "40",
        "cover_date": "2023-03-01",
        "Abstract": "As the USA faces a worsening overdose crisis, improving access to evidence-based treatment for opioid use disorder (OUD) remains a policy priority. Federal regulatory changes in response to the COVID-19 pandemic substantially expanded flexibilities on take-home doses for methadone treatment for OUD. These changes have fuelled questions about the effect of new regulations on OUD outcomes and the potential effect on health of permanently integrating these flexibilities into treatment policy going forward. To aide US policy makers as they consider implementing permanent methadone regulatory changes, we conducted a review synthesising peer-reviewed research on the effect of the flexibilities of methadone take-home policies introduced during COVID-19 on methadone programme operations, OUD patient and provider experiences, and patient health outcomes. We interpret the findings in the context of the federal rule-making process and discuss avenues by which these findings can be incorporated and implemented into US policies on substance use treatment going forward.",
        "DOI": "10.1016/S2468-2667(23)00023-3",
        "affiliation_name": "NYU Grossman School of Medicine",
        "affiliation_city": "New York",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Data-driven approach for instantaneous vehicle emission predicting using integrated deep neural network",
        "paper_author": "Howlader A.M.",
        "publication": "Transportation Research Part D: Transport and Environment",
        "citied_by": "12",
        "cover_date": "2023-03-01",
        "Abstract": "This paper details how instantaneous vehicle emissions, namely, CO2, CO, NOX, and HC from light-duty vehicles, can be predicted using the integrated deep neural network method (NNM). The deep-learning algorithms, i.e., short-term memory (LSTM), gated recurrent unit (GRU), and recurrent neural network (RNN) methods, were applied to predict the emissions. Finally, an integrated method using LSTM, RNN, and GRU was used to determine if the integrated method was better at increasing the prediction performance of vehicle emissions. Each model performance was evaluated by calculating the mean squared error (MSE), root mean squared error (RMSE), and normalize root mean squared error (nRMSE) values. The results indicate that the integrated LSTM NNM provided the best overall emission prediction compared to the other methods. This integrated model can help to develop new policies and regulations for vehicle emissions.",
        "DOI": "10.1016/j.trd.2023.103654",
        "affiliation_name": "California Air Resources Board",
        "affiliation_city": "Sacramento",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Responsible reliance concerning development and use of AI in the military domain",
        "paper_author": "Boulanin V.",
        "publication": "Ethics and Information Technology",
        "citied_by": "6",
        "cover_date": "2023-03-01",
        "Abstract": "In voicing commitments to the principle that the adoption of artificial-intelligence (AI) tools by armed forces should be done responsibly, a growing number of states have referred to a concept of “Responsible AI.” As part of an effort to help develop the substantive contours of that concept in meaningful ways, this position paper introduces a notion of “responsible reliance.” It is submitted that this notion could help the policy conversation expand from its current relatively narrow focus on interactions between an AI system and its end-user to also encompass the wider set of interdependencies involved in fulfilling legal obligations concerning the use of AI in armed conflicts. The authors argue that to respect international humanitarian law and ensure accountability, states ought to devise and maintain a framework that ensures that natural persons involved in the use an AI tool in an armed conflict could responsibly rely at least on: (1) the tool’s technical aspects, (2) the conduct of other people involved in development and use of that AI tool; and (3) the policies and processes implemented at the state level. According to the authors, the “responsible reliance” notion could serve, among other examples, as a basis on which to articulate legal requirements, prohibitions, and permissions across diverse areas, from the design of AI tools to human-machine interactions to configuration of responsible-command frameworks.",
        "DOI": "10.1007/s10676-023-09691-0",
        "affiliation_name": "Harvard Law School",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Context-specific discussion of Airbnb usage knowledge graphs for improving private social systems",
        "paper_author": "Samsudeen S.",
        "publication": "Journal of Combinatorial Optimization",
        "citied_by": "3",
        "cover_date": "2023-03-01",
        "Abstract": "This research concentrates on extracting and context-specific discussion of Airbnb usage knowledge graphs to improve private social systems. The Knowledge-Infused Learning Techniques are applied to the learning and social impact of Airbnb usage user's system. This research Extracting and discusses Airbnb usage using knowledge graphs. This research formulates the two proposed methods for Extracting Airbnb usage knowledge graphs to improve private social systems. This research enables the two potential implications for user expectation Extraction and context-specific discussion about personal social systems. This might be useful to enhance the specific services of personal social systems. This led by using the knowledge graphs concerning the responsibilities and services using response-based Optical Character Recognition. This might be fulfilled with the internal data and explain factor for \"Airbnb private systems\" based on knowledge graphs and machine learning. However, the Graph convolutional networks work based on the Convolutional Neural Networks for automatically Extracting the essential features without any human supervision based on a context-specific discussion of Airbnb systems. The financial portion of the computational social system application is 45.8%, followed by the public health portion at 56.8%, the environment portion at 69.3%, the politics policy portion at 72%, the social behavior portion at 78%, the human behavior portion at 80%, and the social system portion at 85% better performance in the Airbnb usage knowledge process. The efficiency of this analysis is around 67.9%. The input data second level range is 23–39%, the improved accuracy range is 74.38%, and the increased accuracy range is 46.33%. The enhanced accuracy range is 96.5%, and the third-level input data range is 43–59%. This rough comparison result has an efficiency of 62.51%. The outcomes of several social network comparison experiments are compared to the knowledge-infused learning and classification model, and the estimated result is 73.8% efficient.",
        "DOI": "10.1007/s10878-023-00994-y",
        "affiliation_name": "Imam Ja'afar Al-Sadiq University",
        "affiliation_city": "Baghdad",
        "affiliation_country": "Iraq"
    },
    {
        "paper_title": "Factors affecting perceptions in transport – A deep dive into the motorbike ban in Hanoi, Vietnam",
        "paper_author": "Kieu M.",
        "publication": "Case Studies on Transport Policy",
        "citied_by": "4",
        "cover_date": "2023-03-01",
        "Abstract": "The dependence on motorbikes has contributed to traffic problems in Hanoi, Vietnam. Policymakers have considered a controversial ban on nonelectric motorbikes in parts of the city in an effort to reduce congestion and pollution. However, understanding of individual perceptions on critical transport policies, such as this potential ban is lacking, especially in the Global South, with implications for evidence-based policy making. This paper presents the results of some exploratory data analysis and a machine learning application using a travel survey recently conducted in Hanoi. It aims to understand how residents perceive a potential motorbike ban, their perceptions of different mobility modes, as well as their future plans for mobility if motorbikes are banned. This data-driven analysis of policy scenarios shows that awareness of the potential ban, distance to public transport, and individual transport modal choice determine the acceptability of the proposed motorbike ban and its likely success. It also shows that policymakers in Hanoi should also consider citizens’ plans for future vehicle ownership, as the analysis results suggest that cars are likely to replace motorbikes if the ban is implemented.",
        "DOI": "10.1016/j.cstp.2023.100958",
        "affiliation_name": "The Alan Turing Institute",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "The use of artificial intelligence-based innovations in the health sector in Tanzania: A scoping review",
        "paper_author": "Sukums F.",
        "publication": "Health Policy and Technology",
        "citied_by": "14",
        "cover_date": "2023-03-01",
        "Abstract": "Background: Artificial Intelligence (AI) has great potential to transform health systems to improve the quality of healthcare services. However, AI is still new in Tanzania, and there is limited knowledge about the application of AI technology in the Tanzanian health sector. Objectives: This study aims to explore the current status, challenges, and opportunities for AI application in the health system in Tanzania. Methods: A scoping review was conducted using the Preferred Reporting Items for Systematic Review and Meta-Analysis Extensions for Scoping Review (PRISMA-ScR). We searched different electronic databases such as PubMed, Embase, African Journal Online, and Google Scholar. Results: Eighteen (18) studies met the inclusion criteria out of 2,017 studies from different electronic databases and known AI-related project websites. Amongst AI-driven solutions, the studies mostly used machine learning (ML) and deep learning for various purposes, including prediction and diagnosis of diseases and vaccine stock optimisation. The most commonly used algorithms were conventional machine learning, including Random Forest and Neural network, Naive Bayes K-Nearest Neighbour and Logistic regression. Conclusions: This review shows that AI-based innovations may have a role in improving health service delivery, including early outbreak prediction and detection, disease diagnosis and treatment, and efficient management of healthcare resources in Tanzania. Our results indicate the need for developing national AI policies and regulatory frameworks for adopting responsible and ethical AI solutions in the health sector in accordance with the World Health Organisation (WHO) guidance on ethics and governance of AI for health.",
        "DOI": "10.1016/j.hlpt.2023.100728",
        "affiliation_name": "Muhimbili University of Health and Allied Sciences",
        "affiliation_city": "Dar Es Salaam",
        "affiliation_country": "Tanzania"
    },
    {
        "paper_title": "Joint associations of environmental and sociodemographic attributes with active and sedentary travel",
        "paper_author": "Chandrabose M.",
        "publication": "Transportation Research Part D: Transport and Environment",
        "citied_by": "4",
        "cover_date": "2023-03-01",
        "Abstract": "Promoting physically active travel (walking + cycling) is a key strategy to enhance population health. It is important to understand for whom and where active and sedentary travel (car use) are common. However, little research has investigated how attributes of people and places interact in influencing these travel behaviors. Using travel survey data collected from 41,628 Australian adults, we employed decision tree modeling to untangle such complex relationships and to assess the relative importance of sociodemographic and environmental attributes in influencing active and sedentary travel durations. For active travel, attributes of places (suburb-level population density, distance to the nearest city center) were the most influential determinants. For car use, those two environmental attributes interacted with sociodemographic attributes (age, work status, household income) to form subgroups. Decision tree modeling appears to be effective to identify at-risk subgroups that may benefit from policy interventions.",
        "DOI": "10.1016/j.trd.2023.103643",
        "affiliation_name": "Baker Heart and Diabetes Institute",
        "affiliation_city": "Melbourne",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "A hybrid cancer prediction based on multi-omics data and reinforcement learning state action reward state action (SARSA)",
        "paper_author": "Mohammed M.A.",
        "publication": "Computers in Biology and Medicine",
        "citied_by": "25",
        "cover_date": "2023-03-01",
        "Abstract": "These days, the ratio of cancer diseases among patients has been growing day by day. Recently, many cancer cases have been reported in different clinical hospitals. Many machine learning algorithms have been suggested in the literature to predict cancer diseases with the same class types based on trained and test data. However, there are many research rooms available for further research. In this paper, the studies look into the different types of cancer by analyzing, classifying, and processing the multi-omics dataset in a fog cloud network. Based on SARSA on-policy and multi-omics workload learning, made possible by reinforcement learning, the study made new hybrid cancer detection schemes. It consists of different layers, such as clinical data collection via laboratories and tool processes (biopsy, colonoscopy, and mammography) at the distributed omics-based clinics in the network. The study considers the different cancer classes such as carcinomas, sarcomas, leukemias, and lymphomas with their types in work and processes them using the multi-omics distributed clinics in work. In order to solve the problem, the study presents omics cancer workload reinforcement learning state action reward state action “SARSA” (OCWLS) schemes, which are made up of an on-policy learning scheme on different parameters like states, actions, timestamps, reward, accuracy, and processing time constraints. The goal is to process multiple cancer classes and workload feature matching while reducing the time it takes to process in clinical hospitals that are spread out. Simulation results show that OCWLS is better than other machine learning methods regarding+ processing time, extracting features from multiple classes of cancer, and matching in the system.",
        "DOI": "10.1016/j.compbiomed.2023.106617",
        "affiliation_name": "Dawood University of Engineering &amp; Technology (DUET)",
        "affiliation_city": "Karachi",
        "affiliation_country": "Pakistan"
    },
    {
        "paper_title": "Characterising paediatric mortality during and after acute illness in Sub-Saharan Africa and South Asia: a secondary analysis of the CHAIN cohort using a machine learning approach",
        "paper_author": "Diallo A.H.",
        "publication": "eClinicalMedicine",
        "citied_by": "10",
        "cover_date": "2023-03-01",
        "Abstract": "Background: A better understanding of which children are likely to die during acute illness will help clinicians and policy makers target resources at the most vulnerable children. We used machine learning to characterise mortality in the 30-days following admission and the 180-days after discharge from nine hospitals in low and middle-income countries (LMIC). Methods: A cohort of 3101 children aged 2–24 months were recruited at admission to hospital for any acute illness in Bangladesh (Dhaka and Matlab Hospitals), Pakistan (Civil Hospital Karachi), Kenya (Kilifi, Mbagathi, and Migori Hospitals), Uganda (Mulago Hospital), Malawi (Queen Elizabeth Central Hospital), and Burkina Faso (Banfora Hospital) from November 2016 to January 2019. To record mortality, children were observed during their hospitalisation and for 180 days post-discharge. Extreme gradient boosted models of death within 30 days of admission and mortality in the 180 days following discharge were built. Clusters of mortality sharing similar characteristics were identified from the models using Shapley additive values with spectral clustering. Findings: Anthropometric and laboratory parameters were the most influential predictors of both 30-day and post-discharge mortality. No WHO/IMCI syndromes were among the 25 most influential mortality predictors of mortality. For 30-day mortality, two lower-risk clusters (N = 1915, 61%) included children with higher-than-average anthropometry (1% died, 95% CI: 0–2), and children without signs of severe illness (3% died, 95% CI: 2–4%). The two highest risk 30-day mortality clusters (N = 118, 4%) were characterised by high urea and creatinine (70% died, 95% CI: 62–82%); and nutritional oedema with low platelets and reduced consciousness (97% died, 95% CI: 92–100%). For post-discharge mortality risk, two low-risk clusters (N = 1753, 61%) were defined by higher-than-average anthropometry (0% died, 95% CI: 0–1%), and gastroenteritis with lower-than-average anthropometry and without major laboratory abnormalities (0% died, 95% CI: 0–1%). Two highest risk post-discharge clusters (N = 267, 9%) included children leaving against medical advice (30% died, 95% CI: 25–37%), and severely-low anthropometry with signs of illness at discharge (46% died, 95% CI: 34–62%). Interpretation: WHO clinical syndromes are not sufficient at predicting risk. Integrating basic laboratory features such as urea, creatinine, red blood cell, lymphocyte and platelet counts into guidelines may strengthen efforts to identify high-risk children during paediatric hospitalisations. Funding: Bill & Melinda Gates Foundation OPP1131320.",
        "DOI": "10.1016/j.eclinm.2023.101838",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Price elasticity of CO<inf>2</inf> emissions in China: A machine learning approach",
        "paper_author": "Lei H.",
        "publication": "Sustainable Production and Consumption",
        "citied_by": "5",
        "cover_date": "2023-03-01",
        "Abstract": "Quantifying the impacts of carbon prices on urban CO2 emissions is imperative to developing individualized carbon pricing schemes in China's emission trading systems (ETSs). Based on a prefecture-level panel dataset from 2005 to 2017, we use a machine learning approach to predict the individual price elasticities of CO2 emissions for 284 cities in China. The results exhibit significant heterogeneity in price elasticities: The full distribution of price elasticities indicates a right-skewed and leptokurtic characteristic, with an average price elasticity of −4.0 % and −3.6 % in CO2 emissions and CO2 intensity, respectively. We further find that the key city characteristics in predicting price elasticities are population density, industrial structure, and the number of industrial firms. Using the predictions of price elasticities, we conclude with a simulation to illustrate how different carbon pricing policies can help reduce carbon emissions at the national level. The results have important policy implications for China aiming to address low-carbon issues through carbon pricing.",
        "DOI": "10.1016/j.spc.2023.01.005",
        "affiliation_name": "Huazhong University of Science and Technology",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Forecasting container freight rates using the Prophet forecasting method",
        "paper_author": "Saeed N.",
        "publication": "Transport Policy",
        "citied_by": "29",
        "cover_date": "2023-03-01",
        "Abstract": "This study applies three innovative methods in forecasting container freight rates. Firstly, we extracted 471 major disruptive events from the ‘Lloyds List’ database from 2010 until 2020, that may affect freight rates. Secondly, we use Machine Learning (ML) and natural language processing techniques to categorize these events into six distinct categories. These include: “congestion”, “peak demand”, “policy”, “price up”, “overcapacity”, and “coronavirus”. Thirdly, we apply Prophet forecasting on six major container routes by incorporating the six categories of events. The results reveal that ‘overcapacity’ and ‘coronavirus’ led to improved forecasting accuracy of freight rates when compared to the Prophet model without accounting for events. This study provides a more reliable mixed-method approach to improving the accuracy of container freight rate forecasts. The proposed forecasting technique will help policy makers and practitioners to develop and deploy strategies to mitigate the risks associated with the volatility of freight rates and supply chain costings.",
        "DOI": "10.1016/j.tranpol.2023.01.012",
        "affiliation_name": "Handelshögskolan",
        "affiliation_city": "Gothenburg",
        "affiliation_country": "Sweden"
    },
    {
        "paper_title": "A methodology for performance assessment at system level—Identification of operating regimes and anomaly detection in wind turbines",
        "paper_author": "Urmeneta J.",
        "publication": "Renewable Energy",
        "citied_by": "5",
        "cover_date": "2023-03-01",
        "Abstract": "In the growing wind energy sector, as in other high investment sectors, the need to make assets profitable has put the spotlight on maintenance. Efficient solutions which leverage from condition or performance based maintenance policies have been proposed during the last decades, but the proposed methods generally focus on individual components or stand for specific application areas. This paper aims to contribute to the development of performance based maintenance strategies within the wind energy sector by providing a condition monitoring based generic methodology for wind turbine performance assessment at system level. The proposed methodology is based on the detection of critical periods in which low performance is detected repeatedly. Multiple machine learning methods and models are applied to assess the wind turbine performance. This methodology has been applied in a case study with SCADA data of eight wind turbines. An analyst could benefit from the implementation of the methodology and the easy-to-interpret results shown in the proposed control chart, especially in cases in which there is less know-how about which variables have higher impact on systems performance.",
        "DOI": "10.1016/j.renene.2023.01.035",
        "affiliation_name": "Basque Research and Technology Alliance (BRTA)",
        "affiliation_city": "Mendaro",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "Learning-Based Design and Control for Quadrupedal Robots With Parallel-Elastic Actuators",
        "paper_author": "Bjelonic F.",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "18",
        "cover_date": "2023-03-01",
        "Abstract": "Parallel-elastic joints can improve the efficiency and strength of robots by assisting the actuators with additional torques. For these benefits to be realized, a spring needs to be carefully designed. However, designing robots is an iterative and tedious process, often relying on intuition and heuristics. We introduce a design optimization framework that allows us to co-optimize a parallel elastic knee joint and locomotion controller for quadrupedal robots with minimal human intuition. We design a parallel elastic joint and optimize its parameters with respect to the efficiency in a model-free fashion. In the first step, we train a design-conditioned policy using model-free Reinforcement Learning, capable of controlling the quadruped in the predefined range of design parameters. Afterwards, we use Bayesian Optimization to find the best design using the policy. We use this framework to optimize the parallel-elastic spring parameters for the knee of our quadrupedal robot ANYmal together with the optimal controller. We evaluate the optimized design and controller in real-world experiments over various terrains. Our results show that the new system improves the torque-square efficiency of the robot by 33% compared to the baseline and reduces maximum joint torque by 30% without compromising tracking performance. The improved design resulted in 11% longer operation time on flat terrain.",
        "DOI": "10.1109/LRA.2023.3234809",
        "affiliation_name": "ETH Zürich",
        "affiliation_city": "Zurich",
        "affiliation_country": "Switzerland"
    },
    {
        "paper_title": "Forecasting the Monkeypox Outbreak Using ARIMA, Prophet, NeuralProphet, and LSTM Models in the United States",
        "paper_author": "Long B.",
        "publication": "Forecasting",
        "citied_by": "23",
        "cover_date": "2023-03-01",
        "Abstract": "Since May 2022, over 64,000 Monkeypox cases have been confirmed globally up until September 2022. The United States leads the world in cases, with over 25,000 cases nationally. This recent escalation of the Monkeypox outbreak has become a severe and urgent worldwide public health concern. We aimed to develop an efficient forecasting tool that allows health experts to implement effective prevention policies for Monkeypox and shed light on the case development of diseases that share similar characteristics to Monkeypox. This research utilized five machine learning models, namely, ARIMA, LSTM, Prophet, NeuralProphet, and a stacking model, on the Monkeypox datasets from the CDC official website to forecast the next 7-day trend of Monkeypox cases in the United States. The result showed that NeuralProphet achieved the most optimal performance with a RMSE of 49.27 and (Formula presented.) of 0.76. Further, the final trained NeuralProphet was employed to forecast seven days of out-of-sample cases. On the basis of cases, our model demonstrated 95% accuracy.",
        "DOI": "10.3390/forecast5010005",
        "affiliation_name": "Harrisburg University of Science and Technology",
        "affiliation_city": "Harrisburg",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "DDPG based LADRC trajectory tracking control for underactuated unmanned ship under environmental disturbances",
        "paper_author": "Zheng Y.",
        "publication": "Ocean Engineering",
        "citied_by": "27",
        "cover_date": "2023-03-01",
        "Abstract": "Realizing trajectory tracking control of underactuated unmanned ships is critical for safe and stable navigation during offshore operations. The principal difficulties of this task lie in the coupling problem caused by the underactuated system and the uncertainty caused by the modeling error, internal parameter perturbations, and external environment. In this paper, a three-degree-of-freedom ship model is first built based on the manipulative modeling group equation. Then, a trajectory tracking line-of-sight guidance law is proposed to achieve the desired heading angle and forward speed during trajectory tracking, and the Lyapunov function is used to analyze its stability. Furthermore, the heading angle controller and forward speed controller are designed based on the linear active disturbance rejection control framework. In addition, to solve the challenging problem of controller parameter tuning and further improve the controller's robustness, the Deep Deterministic Policy Gradient algorithm is applied to adjust the controller parameters. Finally, the effectiveness of the proposed method is verified by the simulation results of sinusoidal, circular, and square trajectories with wind and current disturbances.",
        "DOI": "10.1016/j.oceaneng.2023.113667",
        "affiliation_name": "Université de Lille",
        "affiliation_city": "Lille",
        "affiliation_country": "France"
    },
    {
        "paper_title": "Minerals resource rent responses to economic performance, greener energy, and environmental policy in China: Combination of ML and ANN outputs",
        "paper_author": "Chen F.",
        "publication": "Resources Policy",
        "citied_by": "51",
        "cover_date": "2023-03-01",
        "Abstract": "The relationship of mineral resources rent with economic performance, greener energy, and environmental policy is crucial because it could impact how welfare and eco-friendly are treated in a nation. This paper employed machine learning (ML) and Artificial Neural Networks (ANN) to predict minerals resourcesthrougheconomic growth, the producer price index, market prices, the environmental policy stringency index (EPSI), and greener energy in China. The ML and ANN outputs showed how responsive quarterly minerals resource rent variations are too dynamic shifts in economic performance and renewable energy use. The more critical prediction accuracy of ML experiments compared to ANN experiments is highlighted by data from mean absolute percentage, mean square, root mean square, and root mean absolute errors, as well as the coefficient of determination. Though China is abundant in natural resources, there is a need to employ these resources efficiently to achieve long-term eco-friendly performance. In addition to comparing the results of ML and ANN, it also provided causality conclusions and policy implications.",
        "DOI": "10.1016/j.resourpol.2023.103307",
        "affiliation_name": "Central University of Kerala",
        "affiliation_city": "Kasaragod",
        "affiliation_country": "India"
    },
    {
        "paper_title": "The role of recall periods when predicting food insecurity: A machine learning application in Nigeria",
        "paper_author": "Villacis A.H.",
        "publication": "Global Food Security",
        "citied_by": "10",
        "cover_date": "2023-03-01",
        "Abstract": "Defining and measuring food insecurity at the household level is critical for policymakers, aid agencies, and international organizations. Food insecurity indicators, such as the Food Consumption Score, are based on a given recall period, usually 7-days. Still, using other indicators or methodologies makes surveys use different recall periods (e.g., 30-days or 12 months). This study uses machine learning methods and four waves of the Nigeria LSMS-ISA datasets to assess the implications of using different recall periods when predicting food insecurity measures. In addition to machine learning methods, the novelty of this study is the use of big data and relevant weather data to predict the food insecurity status of Nigerian farming families. Our results show that experience-based food insecurity indicators, measured using a 7-days recall period, have a high predictability accuracy (78%–90%). More importantly, we find that predictors computed using a 7-day recall period can detect about seven out of ten households considered food-insecure by indicators measured using a recall period of 30-days.",
        "DOI": "10.1016/j.gfs.2023.100671",
        "affiliation_name": "W. P. Carey School of Business",
        "affiliation_city": "Tempe",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Energy efficient behavior modeling for demand side recommender system in solar microgrid applications using multi-agent reinforcement learning model",
        "paper_author": "Onile A.E.",
        "publication": "Sustainable Cities and Society",
        "citied_by": "22",
        "cover_date": "2023-03-01",
        "Abstract": "Electricity consumers are often faced with challenges relating to the choice of an optimal energy saving plan. Increasing integration of transient renewable energy sources promises tantalizing solutions but also poses emerging stability challenges for the electricity grid. Demand side management using battery energy storage systems (BESSs) is crucial towards extending the physical limits of existing electricity grid. However, problems related to consumer behavior towards adoption of energy/battery efficiency measures and consumer comfort feedback exist. In this study, we present BESS technologies that are embedded into the grid and further enhanced with the use of reinforcement learning control and recommendation system technologies for improving the grid reliability, attaining self-consumption and demand response goals. The novelty of the proposed work is highlighted by using a separate class of active controller for BESS technologies, thereby separating it from loads which determine user comfort. Similarly, an adaptive demand side recommender scheme was used to provide recommendations targeting various microgrid entities. The result of the study shows that operating BESS using the multi-agent reinforcement learning control strategy achieved a maximum peak load reduction of about 24.5% alongside 94% comfort improvements in certain loads. The linear reduction in peak load was further enhanced by the BESS efficiency-related recommendations when compared to the baseline scenario.",
        "DOI": "10.1016/j.scs.2023.104392",
        "affiliation_name": "Tallinna Tehnikaülikool",
        "affiliation_city": "Tallinn",
        "affiliation_country": "Estonia"
    },
    {
        "paper_title": "Energy consumption optimization in wastewater treatment plants: Machine learning for monitoring incineration of sewage sludge",
        "paper_author": "Adibimanesh B.",
        "publication": "Sustainable Energy Technologies and Assessments",
        "citied_by": "74",
        "cover_date": "2023-03-01",
        "Abstract": "Biomass management in terms of energy consumption optimization has become a recent challenge for developed countries. Nevertheless, the multiplicity of materials and operating parameters controlling energy consumption in wastewater treatment plants necessitates the need for sophisticated well-organized disciplines in order to minimize energy consumption and dissipation. Sewage sludge (SS) disposal management is the key stage of this process, such that incineration due to the high costs of drying remains a matter of concern. Thus, a combination of experimental investigations and data analysis is required for an efficient plant design. Herein, we propose an intelligent tool based on Machine Learning (ML) algorithms (A: Parallel, B: Artificial Neural Network (ANN), and C: Chained, ML models) by employing SciKit-Learn library in Python, followed by hyper-parameter tuning and the k-fold cross-validation implementation. The optimizer receives simulation data from ASPEN PLUS software, and imitates the behavior of system outputs (namely, Yi: fluidized bed temperature, steam heat transfer rate, and dryer residence time in the SS) to yield optimal changing variables (namely, Xi: feed temperature, air temperature, fume temperature, steam flow rate, moisture content in the feedstock, and steam inlet temperature to dryer). The authenticity and precision of our intelligent optimizer was validated in terms of optimum heat transfer amount (the higher the better) and dryer residence time (the lower the better) by data collected from wastewater treatment plant in Gdynia (Poland), demonstrating excellent predictability of the algorithm. The R2 values for A, B, and C ML models were 0.85, 0.94, and 0.91, respectively. The B model, though slightly revealed better prediction than the C model, estimated the outputs in much lower time than the former. Thus, C model was selected as the computational tool for the optimization purpose. Overall, we claim that the methodology developed herein takes the advantage of ca. 6% saving in the total amount of energy required for incineration unit of SS disposal plant, which is well justified considering the energy crisis raised by the geopolitical issues in the area and also the high cost of energy worldwide.",
        "DOI": "10.1016/j.seta.2023.103040",
        "affiliation_name": "Polskiej Akademii Nauk, Instytut Maszyn Przepływowych im. Roberta Szewalskiego",
        "affiliation_city": "Gdansk",
        "affiliation_country": "Poland"
    },
    {
        "paper_title": "Container terminal daily gate in and gate out forecasting using machine learning methods",
        "paper_author": "Jin J.",
        "publication": "Transport Policy",
        "citied_by": "10",
        "cover_date": "2023-03-01",
        "Abstract": "Container throughput is an essential indicator for measuring the container terminal's efficiency. Gate in and gate out containers are the containers that are transported to and transported out of the terminal respectively. Containers are stacked in the container yard before they leave the terminal. Handling of these containers accounts for a major workload at the terminal. Therefore, accurate short-term forecasting of the daily gate in and gate out containers at a container terminal is critical for operational planning. While most forecasts are made for the strategical level of the overall container throughput, this paper focuses on the daily gate in and gate out container quantities with a case study of Ningbo Zhoushan Port Beilun Second Container Terminal. Traditional approaches are mainly time series prediction algorithms that purely rely on the previously observed values. This paper proposes a novel decomposition-ensemble methodology framework which first divides the daily forecasting into a group of forecasting on a set of vessels which arrive in a certain time range around the predicted day. A novel machine learning method, “Extreme Gradient Boost (XGBoost)”, is used for the vessel level of forecasting, where some temporal-related features of vessels are considered in the model. We then ensemble the predicated value of all vessels in the set as the total gate in/out amount for a day. Experimental results demonstrate that our proposed method achieves notable performance gains compared to the time series-based method ARIMA. In addition, the results of this new technology can be fed into the terminal operating system (ToS) in the Ningbo Beilun container terminal for better management. Specifically, the prediction results facilitate effective real-time decision-making for operational management and policy drafting, such as workload planning, equipment scheduling, yard planning, etc.",
        "DOI": "10.1016/j.tranpol.2022.11.010",
        "affiliation_name": "University of Nottingham Ningbo China",
        "affiliation_city": "Ningbo",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Mapping crop type in Northeast China during 2013–2021 using automatic sampling and tile-based image classification",
        "paper_author": "Xuan F.",
        "publication": "International Journal of Applied Earth Observation and Geoinformation",
        "citied_by": "65",
        "cover_date": "2023-03-01",
        "Abstract": "Northeast China is one of the most major grain banks in China and has an overwhelming influence on food security. To mitigate the challenges caused by increasing food demands and soil protection, crop rotation and fallowing policies have been introduced in Northeast China. These soil protection policies change annual crop planting area and crop distribution. To monitor crop type and its changes on a regional scale in time series, we explore the automatic sampling approach by hexagon strategy and tile-based classification by random forest (RF) algorithm using time-series Landsat-8 Operational Land Imager (OLI) images during 2013–2021. The crop maps have high credibility with the overall accuracies (OA) wall-to-wall ranging from 0.89 to 0.97, and also have close agreement with statistical data city by city. This study provides a highly reliable long-term crop maps dataset, which can be helpful for food security and regional agricultural production management.",
        "DOI": "10.1016/j.jag.2022.103178",
        "affiliation_name": "Ministry of Agriculture of the People's Republic of China",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Building a predictive machine learning model of gentrification in Sydney",
        "paper_author": "Thackway W.",
        "publication": "Cities",
        "citied_by": "17",
        "cover_date": "2023-03-01",
        "Abstract": "In an era of rapid urbanisation and increasing wealth, gentrification is an urban phenomenon impacting many cities around the world. The ability of policymakers and planners to better understand and address gentrification-induced displacement hinges upon proactive intervention strategies. It is in this context that we build a tree-based machine learning (ML) model to predict neighbourhood change in Sydney. Change, in this context, is proxied by the Socioeconomic Index for Advantage and Disadvantage, in addition to census and other ancillary predictors. Our models predict gentrification from 2011 to 2016 with a balanced accuracy of 74.7 %. Additionally, the use of an additive explanation tool enables individual prediction explanations and advanced feature contribution analysis. Using the ML model, we predict future gentrification in Sydney up to 2021. The predictions confirm that gentrification is expanding outwards from the city centre. A spill-over effect is predicted to the south, west and north-west of former gentrifying hotspots. The findings are expected to provide policymakers with a tool to better forecast where likely areas of gentrification will occur. This future insight can then inform suitable policy interventions and responses in planning for more equitable cities outcomes, specifically for vulnerable communities impacted by gentrification and neighbourhood change.",
        "DOI": "10.1016/j.cities.2023.104192",
        "affiliation_name": "UNSW Sydney",
        "affiliation_city": "Sydney",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Reinforcement learning under temporal logic constraints as a sequence modeling problem",
        "paper_author": "Tian D.",
        "publication": "Robotics and Autonomous Systems",
        "citied_by": "2",
        "cover_date": "2023-03-01",
        "Abstract": "Reinforcement learning (RL) under temporal logic typically suffers from slow propagation for credit assignment. Inspired by recent advancements called trajectory transformer in machine learning, the reinforcement learning under Temporal Logic (TL) is modeled as a sequence modeling problem in this paper, where an agent utilizes the transformer to fit the optimal policy satisfying the Finite Linear Temporal Logic (LTLf) tasks. To combat the sparse reward issue, dense reward functions for LTLf are designed. For the sake of reducing the computational complexity, a sparse transformer with local and global attention is constructed to automatically conduct credit assignment, which removes the time-consuming value iteration process. The optimal action is found by the beam search performed in transformers. The proposed method generates a series of policies fitted by sparse transformers, which has sustainably high accuracy in fitting the demonstrations. At last, the effectiveness of the proposed method is demonstrated by simulations in Mini-Grid environments.",
        "DOI": "10.1016/j.robot.2022.104351",
        "affiliation_name": "National University of Singapore",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore"
    },
    {
        "paper_title": "Hierarchical reinforcement learning based energy management strategy of plug-in hybrid electric vehicle for ecological car-following process",
        "paper_author": "Zhang H.",
        "publication": "Applied Energy",
        "citied_by": "26",
        "cover_date": "2023-03-01",
        "Abstract": "The economy-oriented automated hybrid eclectic vehicles (HEV) provide great potential to save energy by optimizing both driving behaviors and power distribution. Recent advances in the ecological car following issue of HEV focus on fusing adaptive cruise control (ACC) and energy management system (EMS) by collaborative optimization. However, series control frameworks ACC+EMS breaks the internal coupling relation between motion control and energy distribution, leading to the natural limitation of its optimization. On the opposite, integrated ACC-EMS promises energy-saving improvement but brings complex optimization problems with multi-scale objectives and large exploration space. The huge computation load restricts the online application of ACC-EMS. To address these problems, a hierarchical reinforcement learning based ACC-EMS strategy is proposed with a hierarchical policy and non-hierarchical execution. The upper layer learns to plan state-of-charge and time-headway trajectories, while the low layer policy learns to achieve the expected goals by outputting control variables executed by the host vehicle. The proposed ACC-EMS strategy were self-learning by interaction in car-following scenario constructed with GPS data on I-880 highway. Comprehensive simulations show the proposed strategy has significantly improved the training speed and stability, compared to the offline global optimum, achieving the energy consumption difference of less than 3% and computational load of less than 600 times.",
        "DOI": "10.1016/j.apenergy.2022.120599",
        "affiliation_name": "Southeast University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Policy Gradient-Based Focal Loss to Reduce False Negative Errors of Convolutional Neural Networks for Pavement Crack Segmentation",
        "paper_author": "Yang E.",
        "publication": "Journal of Infrastructure Systems",
        "citied_by": "6",
        "cover_date": "2023-03-01",
        "Abstract": "Convolutional neural networks (CNNs) have achieved tremendous success in pavement crack segmentation. However, it is difficult for CNN-based crack segmentation methods to minimize false-negative and false-positive errors. Compared with false-positive errors, false-negative errors are more difficult to observe and reduce manually. This paper proposes a fine-tuning method for trained CNNs, called policy gradient-based focal loss (focal-PG loss). The trained CNNs will be further trained by focal-PG loss for only one epoch. The proposed focal-PG loss can be applied to reduce the false-negative errors of the trained CNNs by sacrificing their precision. The experimental results show that focal-PG loss greatly improves the crack recognition rate of the trained encoder-decoder network (EDNet). EDNet (focal-PG loss) achieves an overall precision of 96.05%, recall of 99.68%, and F1-score of 97.83% on 100 validation images. In addition, overall precision of 95.53%, recall of 99.58%, and F1-score of 97.51% are observed for the 150 testing images. U-net, LinkNet, and the feature pyramid network are also tested in the paper to validate the effectiveness of focal-PG loss. The results demonstrate that the focal-PG loss can also improve the performance of the aforementioned networks.",
        "DOI": "10.1061/JITSE4.ISENG-2157",
        "affiliation_name": "College of Engineering, Architecture and Technology",
        "affiliation_city": "Stillwater",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Fiscal responses to COVID-19 outbreak for healthy economies: Modelling with big data analytics",
        "paper_author": "Sariyer G.",
        "publication": "Structural Change and Economic Dynamics",
        "citied_by": "15",
        "cover_date": "2023-03-01",
        "Abstract": "Fiscal responses to the COVID-19 crisis have varied a lot across countries. Using a panel of 127 countries over two separate subperiods between 2020 and 2021, this paper seeks to determine the extent that fiscal responses contributed to the spread and containment of the disease. The study first documents that rich countries, which had the largest total and health-related fiscal responses, achieved the lowest fatality rates, defined as the ratio of COVID-related deaths to cases, despite having the largest recorded numbers of cases and fatalities. The next most successful were less developed economies, whose smaller total fiscal responses included a larger health-related component than emerging market economies. The study used a promising big data analytics technology, the random forest algorithm, to determine which factors explained a country's fatality rate. The findings indicate that a country's fatality ratio over the next period can be almost entirely predicted by its economic development level, fiscal expenditure (both total and health-related), and initial fatality ratio. Finally, the study conducted a counterfactual exercise to show that, had less developed economies implemented the same fiscal responses as the rich (as a share of GDP), then their fatality ratios would have declined by 20.47% over the first period and 2.59% over the second one.",
        "DOI": "10.1016/j.strueco.2022.12.011",
        "affiliation_name": "Izmir Bakircay University",
        "affiliation_city": "Izmir",
        "affiliation_country": "Turkey"
    },
    {
        "paper_title": "Insights in forest structural diversity indicators with machine learning: what is indicated?",
        "paper_author": "Ette J.S.",
        "publication": "Biodiversity and Conservation",
        "citied_by": "3",
        "cover_date": "2023-03-01",
        "Abstract": "Indicator choice is a crucial step in biodiversity assessments. Forest inventories have the potential to overcome data deficits for biodiversity monitoring on large spatial scales which is fundamental to reach biodiversity policy targets. Structural diversity indicators were taken from information theory to describe forest spatial heterogeneity. Their indicative value for forest stand variables is largely unknown. This case study explores these indicator–indicandum relationships in a lowland, European beech (Fagus sylvatica) dominated forest in Austria, Central Europe. We employed five indicators as surrogates for structural diversity which is an important part of forest biodiversity i.e., Clark & Evans-, Shannon, Stand Density, Diameter Differentiation Index, and Crown Competition factor. The indicators are evaluated by machine learning, to detect statistic inter-correlation in an indicator set and the relationship to twenty explanatory stand variables and five variable groups on a landscape scale. Using the R packages randomForest, VSURF, and randomForest Explainer, 1555 sample plots are considered in fifteen models. The model outcome is decisively impacted by the type and number of explanatory variables tested. Relationships to interval-scaled, common stand characteristics can be assessed most effectively. Variables of ‘stand age & density’ are disproportionally indicated by our indicator set while other forest stand characteristics relevant to biodiversity are neglected. Within the indicator set, pronounced inter-correlation is detected. The Shannon Index indicates the overall highest, the Stand Density Index the lowest number of stand characteristics. Machine learning proves to be a useful tool to overcome knowledge gaps and provides additional insights in indicator–indicandum relationships of structural diversity indicators.",
        "DOI": "10.1007/s10531-022-02536-0",
        "affiliation_name": "Bundesforschungszentrum für Wald",
        "affiliation_city": "Vienna",
        "affiliation_country": "Austria"
    },
    {
        "paper_title": "Stochastic dynamic programming solution to transmission scheduling: Multi sensor-multi process with wireless noisy channel",
        "paper_author": "Forootani A.",
        "publication": "Computers and Electrical Engineering",
        "citied_by": "0",
        "cover_date": "2023-03-01",
        "Abstract": "We investigate sensor scheduling for remote estimation when multiple smart sensors monitor multiple stochastic dynamical systems. The sensors transmit their measurements to a remote estimator through a noisy wireless communication channel. Such a remote estimator can receive multiple packets simultaneously sent by local sensors. Sensors transmit their measurements if their Signal Interference and Noise Ratio (SINR) is above a threshold. We compute the optimal policy for sensor scheduling by minimizing expected error covariance subject to total signal transmissions from all sensors. We model this problem as Markov Decision Process (MDP) with discounted cost per stage in the finite time horizon framework, then we employ stochastic Dynamic Programming as the optimization method. A novel algorithm based on sampling and machine learning techniques is proposed as the approximation. At each phase of the DP algorithm, samples are collected using a uniform probability distribution. The data is used to feed Neural Network (NN) and Random Forest (RF) models for cost function and policy approximation. The results of the proposed framework are supported by simulation examples comparing RF and NN as Approximate DP (ADP). Note that this idea builds a bridge among the recent advances in the area of data science, Machine Learning, and the ADP.",
        "DOI": "10.1016/j.compeleceng.2022.108573",
        "affiliation_name": "Bu-Ali Sina University",
        "affiliation_city": "Hamadan",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "Deep reinforcement learning towards real-world dynamic thermal management of data centers",
        "paper_author": "Zhang Q.",
        "publication": "Applied Energy",
        "citied_by": "18",
        "cover_date": "2023-03-01",
        "Abstract": "Deep Reinforcement Learning has been increasingly researched for Dynamic Thermal Management in Data Centers. However, existing works typically evaluate the performance of algorithms on a specific task, utilizing models or data trajectories without discussing in detail their implementation feasibility and their ability to deal with diverse work scenarios. The lack of these works limits the real-world deployment of Deep Reinforcement Learning. To this end, this paper comprehensively evaluates the strengths and limitations of state-of-the-art algorithms by conducting analytical and numerical studies. The analysis is conducted in four dimensions: algorithms, tasks, system dynamics, and knowledge transfer. As an inherent property, the sensitivity to algorithms settings is first evaluated in a simulated data center model. The ability to deal with various tasks and the sensitivity to reward functions are subsequently studied. The trade-off between constraints and power savings is identified by conducting ablation experiments. Next, the performance under different work scenarios is investigated, including various equipment, workload schedules, locations, and power densities. Finally, the transferability of algorithms across tasks and scenarios is also evaluated. The results show that actor-critic, off-policy, and model-based algorithms outperform others in optimality, robustness, and transferability. They can reduce violations and achieve around 8.84% power savings in some scenarios compared to the default controller. However, deploying these algorithms in real-world systems is challenging since they are sensitive to specific hyperparameters, reward functions, and work scenarios. Constraint violations and sample efficiency are some aspects that are still unsatisfactory. This paper presents our well-structured investigations, new findings, and challenges when deploying deep reinforcement learning in Data Centers.",
        "DOI": "10.1016/j.apenergy.2022.120561",
        "affiliation_name": "National University of Singapore",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore"
    },
    {
        "paper_title": "Automated acoustic monitoring captures timing and intensity of bird migration",
        "paper_author": "Van Doren B.M.",
        "publication": "Journal of Applied Ecology",
        "citied_by": "16",
        "cover_date": "2023-03-01",
        "Abstract": "Monitoring small, mobile organisms is crucial for science and conservation, but is technically challenging. Migratory birds are prime examples, often undertaking nocturnal movements of thousands of kilometres over inaccessible and inhospitable geography. Acoustic technology could facilitate widespread monitoring of nocturnal bird migration with minimal human effort. Acoustics complements existing monitoring methods by providing information about individual behaviour and species identities, something generally not possible with tools such as radar. However, the need for expert humans to review audio and identify vocalizations is a challenge to application and development of acoustic technologies. Here, we describe an automated acoustic monitoring pipeline that combines acoustic sensors with machine listening software (BirdVoxDetect). We monitor 4 months of autumn migration in the northeastern United States with five acoustic sensors, extracting nightly estimates of nocturnal calling activity of 14 migratory species with distinctive flight calls. We examine the ability of acoustics to inform two important facets of bird migration: (1) the quantity of migrating birds aloft and (2) the migration timing of individual species. We validate these data with contemporaneous observations from Doppler radars and a large community of citizen scientists, from which we derive independent measures of migration passage and timing. Together, acoustic and weather data produced accurate estimates of the number of actively migrating birds detected with radar. A model combining acoustic data, weather and seasonal timing explained 75% of variation in radar-derived migration intensity. This model outperformed models that lacked acoustic data. Including acoustics in the model decreased prediction error by 33%. A model with only acoustic information outperformed a model comprising weather and date (57% vs. 48% variation explained, respectively). Acoustics also successfully measured migration phenology: species-specific timing estimated by acoustic sensors explained 71% of variation in timing derived from citizen science observations. Synthesis and applications. Our results demonstrate that cost-effective acoustic sensors can monitor bird migration at species resolution at the landscape scale and should be an integral part of management toolkits. Acoustic monitoring presents distinct advantages over radar and human observation, especially in inaccessible and inhospitable locations, and requires significantly less expense. Managers should consider using acoustic tools for monitoring avian movements and identifying and understanding dangerous situations for birds. These recommendations apply to a variety of conservation and policy applications, including mitigating the impacts of light pollution, siting energy infrastructure (e.g. wind turbines) and reducing collisions with structures and aircraft.",
        "DOI": "10.1111/1365-2664.14342",
        "affiliation_name": "NYU Steinhardt",
        "affiliation_city": "New York",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "The future of standardised assessment: Validity and trust in algorithms for assessment and scoring",
        "paper_author": "Aloisi C.",
        "publication": "European Journal of Education",
        "citied_by": "16",
        "cover_date": "2023-03-01",
        "Abstract": "This article considers the challenges of using artificial intelligence (AI) and machine learning (ML) to assist high-stakes standardised assessment. It focuses on the detrimental effect that even state-of-the-art AI and ML systems could have on the validity of national exams of secondary education, and how lower validity would negatively affect trust in the system. To reach this conclusion, three unresolved issues in AI (unreliability, low explainability and bias) are addressed, to show how each of them would compromise the interpretations and uses of exam results (i.e., exam validity). Furthermore, the article relates validity to trust, and specifically to the ABI+ model of trust. Evidence gathered as part of exam validation supports each of the four trust-enabling components of the ABI+ model (ability, benevolence, integrity and predictability). It is argued, therefore, that the three AI barriers to exam validity limit the extent to which an AI-assisted exam system could be trusted. The article suggests that addressing the issues of AI unreliability, low explainability and bias should be sufficient to put AI-assisted exams on par with traditional ones, but might not go as far as fully reassure the public. To achieve this, it is argued that changes to the quality assurance mechanisms of the exam system will be required. This may involve, for example, integrating principled AI frameworks in assessment policy and regulation.",
        "DOI": "10.1111/ejed.12542",
        "affiliation_name": "AQA",
        "affiliation_city": "Manchester",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Listen carefully to experts when you classify data: A generic data classification ontology encoded from regulations",
        "paper_author": "Yang M.",
        "publication": "Information Processing and Management",
        "citied_by": "4",
        "cover_date": "2023-03-01",
        "Abstract": "Along with the proliferation of big data technology, organizations are involved in an overwhelming data ocean, the huge volume of data makes them at a loss in the face of frequent data breaches due to their failure of efficient data security management. Data classification has become a hot topic as a cornerstone of data protection especially in China in recent years, by categorizing information types and distinguishing protective measures at different classification levels. Both the text and tables of the promulgated data classification-related regulations (for simplicity, laws, regulations, policies, and standards are collectively referred to as “regulations”) contain a wealth of valuable information which can guide the work of data classification. To best assist data practitioners, in this paper, we automatically “grasp” expert experience on how to classify data from the analysis of such regulations. We design a framework, GENONTO, that automatically extracts data classification practices (DCPs), such as information types and their corresponding sensitive levels to construct an information type lexicon as well as to encode a generic ontology on top of 38 real-world regulations promulgated in China. GENONTO employs machine learning techniques and natural language processing (NLP) to parse unstructured text and tables. To our knowledge, GENONTO is the first work that explores critical information like the category and the sensitivity of information types from regulations, and organizes them in a structured form of ontology, characterizing the subsumptive relations between different information types. Our research helps provide a well-defined integrated view across regulations and bridges the gap between what experts say and how data practitioners do.",
        "DOI": "10.1016/j.ipm.2022.103186",
        "affiliation_name": "Sichuan University",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Comparison of process-based and statistical approaches for simulation and projections of rainfed crop yields",
        "paper_author": "Eini M.R.",
        "publication": "Agricultural Water Management",
        "citied_by": "16",
        "cover_date": "2023-03-01",
        "Abstract": "Accurate and comprehensive modelling aimed at investigating the impact of climate change on rainfed crop yields is of great importance due to the interconnected issues of water scarcity and food security. Because the process-based and statistical approaches to simulating crop yields are different in nature, a comparison between them is needed. This study investigates the accuracy of crop yield simulations in the historical period as well as future projections using two modelling approaches: 1) a process-based approach employing the Soil and Water Assessment Tool+ (SWAT+) model, and 2) a statistical approach employing a data-driven model, Feed Forward Back Propagation Neural Network (FFBPNN) over a medium-sized catchment in north-western Poland. The application of two potential evapotranspiration methods (Penman-Monteith and Hargreaves) in SWAT+ permitted calibration (2004–2011) and validation (2012–2019) of runoff and yields of winter wheat and spring barley. Different combinations of climatic parameters with a drought index based on Joint Deficit Index were applied to simulate and project rainfed crop yields (winter wheat, barley, potato, rye, rapeseed, sugar beets, cereals, maize for grain, maize for green forage, pulses) with FFBPNN. The results reveal that adding the new drought index helped increase the FFBPNN performance. This approach showed that future yields of the studied crops would slightly increase under RCP8.5 by 2060. Winter wheat and spring barley projections from SWAT+ showed very small changes using both the Penman-Monteith and Hargreaves method. Policy-wise, the results should be of interest to climate change adaptation practitioners and food security experts. Future studies should aim at more thorough investigation of the role of the downscaling technique and extreme events, as well as the effect of elevated CO2 on future crop yields.",
        "DOI": "10.1016/j.agwat.2022.108107",
        "affiliation_name": "Szkola Glówna Gospodarstwa Wiejskiego w Warszawie",
        "affiliation_city": "Warsaw",
        "affiliation_country": "Poland"
    },
    {
        "paper_title": "LEO satellite intelligent-sensing routing algorithm based on a dendrite network",
        "paper_author": "Liu Y.",
        "publication": "Gongcheng Kexue Xuebao/Chinese Journal of Engineering",
        "citied_by": "3",
        "cover_date": "2023-03-01",
        "Abstract": "In a low-Earth orbit (LEO) satellite network, the satellite operation speed is high, the operation cycle is short, and intersatellite links change dynamically. To sense the intersatellite link state in time and select the correct route for an intelligent routing decision, a dendritic network-based intelligent-aware routing algorithm for LEO satellites is proposed in this paper. This algorithm divides the intersatellite link routing of an LEO satellite network into situation-aware, quality-aware, and routing-decision stages and establishes a routing policy framework with real-time correction capability from the source node to the destination. This approach overcomes the problems of the limited selection of routing paths from fixed labels of existing deep learning-based routing algorithms and the long convergence time of reinforcement learning-based routing algorithms.In the intersatellite link situational awareness stage, the intersatellite visibility of the entire LEO satellite network is periodically obtained by analyzing the constraint conditions of the intersatellite link establishment. In the intersatellite link quality perception stage, the final output of the probabilistic forwarding matrix based on the ant colony algorithm is used as the label of the training set, and the corresponding intersatellite link quality is evaluated using the probability value of the current node by selecting the next hop node. By changing the weight coefficients in the path cost function under different load states, more effective training set label data can be collected, which can be consequently used to improve the performance of the trained dendritic network. Moreover, the training set can be optimized in real-time through semi-supervised learning. The trained dendritic network is used to analyze and process the link state parameters, perceive the comprehensive service quality of the link, and output the evaluation value matrix of the next hop routing. It is also used to automatically adjust the weight of the global satellite network link. Meanwhile, the traditional Dijkstra algorithm is optimized to realize the quality perception of the intersatellite link. In the routing decision stage, the reciprocal of the evaluation value matrix is used as the adjacency matrix to pass the shortest-path algorithm. Then, the initial routing path between the source and destination nodes is obtained. Finally, the initial path is corrected via periodic monitoring to cope with the failure of the satellite node. The simulation results show that the routing algorithm based on the dendritic network has low computational complexity and fast convergence. The algorithm can determine the status of the intersatellite link establishment in time, assess the quality of the intersatellite link in real-time, and automatically avoid congested satellite nodes. Accordingly, its end-to-end path delay, delay jitter, and packet loss rate are lower than those of the traditional heuristic routing algorithm and Dijkstra routing algorithm.",
        "DOI": "10.13374/j.issn2095-9389.2021.11.08.007",
        "affiliation_name": "University of Science and Technology Beijing",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Significant increase in natural disturbance impacts on European forests since 1950",
        "paper_author": "Patacca M.",
        "publication": "Global Change Biology",
        "citied_by": "183",
        "cover_date": "2023-03-01",
        "Abstract": "Over the last decades, the natural disturbance is increasingly putting pressure on European forests. Shifts in disturbance regimes may compromise forest functioning and the continuous provisioning of ecosystem services to society, including their climate change mitigation potential. Although forests are central to many European policies, we lack the long-term empirical data needed for thoroughly understanding disturbance dynamics, modeling them, and developing adaptive management strategies. Here, we present a unique database of >170,000 records of ground-based natural disturbance observations in European forests from 1950 to 2019. Reported data confirm a significant increase in forest disturbance in 34 European countries, causing on an average of 43.8 million m3 of disturbed timber volume per year over the 70-year study period. This value is likely a conservative estimate due to under-reporting, especially of small-scale disturbances. We used machine learning techniques for assessing the magnitude of unreported disturbances, which are estimated to be between 8.6 and 18.3 million m3/year. In the last 20 years, disturbances on average accounted for 16% of the mean annual harvest in Europe. Wind was the most important disturbance agent over the study period (46% of total damage), followed by fire (24%) and bark beetles (17%). Bark beetle disturbance doubled its share of the total damage in the last 20 years. Forest disturbances can profoundly impact ecosystem services (e.g., climate change mitigation), affect regional forest resource provisioning and consequently disrupt long-term management planning objectives and timber markets. We conclude that adaptation to changing disturbance regimes must be placed at the core of the European forest management and policy debate. Furthermore, a coherent and homogeneous monitoring system of natural disturbances is urgently needed in Europe, to better observe and respond to the ongoing changes in forest disturbance regimes.",
        "DOI": "10.1111/gcb.16531",
        "affiliation_name": "Université Grenoble Alpes",
        "affiliation_city": "Saint Martin d'Heres",
        "affiliation_country": "France"
    },
    {
        "paper_title": "COVID-19 and human development: An approach for classification of HDI with deep CNN",
        "paper_author": "Kavuran G.",
        "publication": "Biomedical Signal Processing and Control",
        "citied_by": "4",
        "cover_date": "2023-03-01",
        "Abstract": "The measures taken during the pandemic have had lasting effects on people's lives and perceptions of the ability of national and multilateral institutions to drive human development. Policies that changed people's behavior were at the heart of containing the spread of the virus. As a result, it has become a systemic human development crisis affecting health, the economy, education, social life, and accumulated gains. This study shows how the relationship of the Human Development Index (HDI), which has combined effects on health, education, and the economy, should be considered in the context of pandemic factors. First, COVID-19 data of the countries received from a public and credible source were extracted and organized into an acceptable structure. Then, we applied statistical feature selection to determine which variables are closely related to HDI and enabled the Deep Convolutional Neural Network (DCNN) model to give more accurate results. The Continuous Wavelet Transform (CWT) and scalogram methods were used for the time-series data visualization. Three different images of each country are combined into a single image to penetrate each other for ease of processing. These images were made suitable for the input of the ResNet-50 network, which is a pre-trained DCNN model, by going through various preprocessing processes. After the training and validation processes, the feature vectors in the fc1000 layer of the network were drawn and given to the Support Vector Machine Classifier (SVMC) input. We achieved total performance metrics of specificity (88.2%), sensitivity (96.5%), precision (99%), F1 Score (94.9%) and MCC (85.9%).",
        "DOI": "10.1016/j.bspc.2022.104499",
        "affiliation_name": "Malatya Turgut Ozal University",
        "affiliation_city": "Malatya",
        "affiliation_country": "Turkey"
    },
    {
        "paper_title": "Machine learning approaches for constructing the national anti-money laundering index",
        "paper_author": "Zhang G.",
        "publication": "Finance Research Letters",
        "citied_by": "4",
        "cover_date": "2023-03-01",
        "Abstract": "This paper proposes a methodology for constructing the national anti-money laundering (AML) index based on Mutual Evaluation reports and machine learning models. We employ LASSO and random forests to jointly identify the key factors affecting AML, which have policy implications for regulatory authorities to optimize the allocation of AML resources. The random forests five-factor (RF-FF) model proposed in this paper has high prediction accuracy (86.31%) and good out-of-sample predictive ability for the MER-AML index, which is significantly better than competing models such as OLS and relaxed LASSO. The time-series national AML index constructed based on the RF-FF model contributes to overcoming the limitations of existing methods, providing fresh perspectives on the measurement of AML systems, and facilitating empirical studies related to evaluating the controversial AML regime.",
        "DOI": "10.1016/j.frl.2022.103568",
        "affiliation_name": "Chongqing Technology and Business University",
        "affiliation_city": "Chongqing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Forecasting inflation rates be extreme gradient boosting with the genetic algorithm",
        "paper_author": "Li Y.S.",
        "publication": "Journal of Ambient Intelligence and Humanized Computing",
        "citied_by": "4",
        "cover_date": "2023-03-01",
        "Abstract": "One of the most important objectives of monetary institutions is to maintain price stability in countries or regions. Hyperinflation and deflation have adverse influences on economic development and potentially easily lead to a negative economic cycle or a sharp recession. Therefore, accurate inflation rate forecasting is essential in formulating a monetary policy. The Extreme Gradient Boosting (XGBoost) technique has recently become a popular and powerful tool in machine learning, which integrates weak learners and uses the regularization method to decrease overfitting. However, the use of XGBoost in forecasting inflations has not been broadly investigated. Thus, this study aims to utilize the XGBoost approach with the genetic algorithm to forecast inflations for three forecast horizons. The other six forecasting models—random forest (RF), least square support vector regression (LSSVR), backpropagation neural networks (BPNN), general regression neural networks (GRNN), deep belief networks (DBN), and long short-term memory (LSTM)—were employed to predict inflations with the same datasets. The numerical results indicate that the XGBoost models outperformed the other six forecasting models in all forecast horizons. Therefore, the XGBoost technique is a feasible and promising alternative for forecasting inflations.",
        "DOI": "10.1007/s12652-022-04479-4",
        "affiliation_name": "National Chi Nan University",
        "affiliation_city": "Puli",
        "affiliation_country": "Taiwan"
    },
    {
        "paper_title": "A theoretical demonstration for reinforcement learning of PI control dynamics for optimal speed control of DC motors by using Twin Delay Deep Deterministic Policy Gradient Algorithm",
        "paper_author": "Tufenkci S.",
        "publication": "Expert Systems with Applications",
        "citied_by": "24",
        "cover_date": "2023-03-01",
        "Abstract": "To benefit from the advantages of Reinforcement Learning (RL) in industrial control applications, RL methods can be used for optimal tuning of the classical controllers based on the simulation scenarios of operating conditions. In this study, the Twin Delay Deep Deterministic (TD3) policy gradient method, which is an effective actor-critic RL strategy, is implemented to learn optimal Proportional Integral (PI) controller dynamics from a Direct Current (DC) motor speed control simulation environment. For this purpose, the PI controller dynamics are introduced to the actor-network by using the PI-based observer states from the control simulation environment. A suitable Simulink simulation environment is adapted to perform the training process of the TD3 algorithm. The actor-network learns the optimal PI controller dynamics by using the reward mechanism that implements the minimization of the optimal control objective function. A setpoint filter is used to describe the desired setpoint response, and step disturbance signals with random amplitude are incorporated in the simulation environment to improve disturbance rejection control skills with the help of experience based learning in the designed control simulation environment. When the training task is completed, the optimal PI controller coefficients are obtained from the weight coefficients of the actor-network. The performance of the optimal PI dynamics, which were learned by using the TD3 algorithm and Deep Deterministic Policy Gradient algorithm, are compared. Moreover, control performance improvement of this RL based PI controller tuning method (RL-PI) is demonstrated relative to performances of both integer and fractional order PI controllers that were tuned by using several popular metaheuristic optimization algorithms such as Genetic Algorithm, Particle Swarm Optimization, Grey Wolf Optimization and Differential Evolution.",
        "DOI": "10.1016/j.eswa.2022.119192",
        "affiliation_name": "Malatya Turgut Ozal University",
        "affiliation_city": "Malatya",
        "affiliation_country": "Turkey"
    },
    {
        "paper_title": "Post-High School Outcomes of Adolescents with Learning Disabilities: Using Annual State Administrative Data and Predictive Analytics",
        "paper_author": "Yamamoto S.H.",
        "publication": "Journal of Applied Social Science",
        "citied_by": "1",
        "cover_date": "2023-03-01",
        "Abstract": "This study involved the analyses of extant data from two U.S. states of post-school outcomes (PSO) for students with a specific learning disability (SLD) one year after they had exited high school. The purpose of this study was to fill two gaps in the literature. The first gap was to understand what happened to these exiters in the first year after high school related to employment and further education or training at a state level. The second gap was to demonstrate the necessity of local and state education professionals to use PSO data, which is collected annually, by applying predictive analytics (PA) to support their decision making. The data analyses produced two main findings. One, the strongest predictors of PSO were students graduating from high school and their high school classroom placement. Two, PA was reasonably accurate in predicting PSO and demonstrated robust capabilities for reliable use on an annual basis to support policies, programs, and practices. Limitations of this study related to the data and number of predictors. The study concludes with implications of administrative state data use and PA for state and local education professionals and for researchers.",
        "DOI": "10.1177/19367244221133481",
        "affiliation_name": "University of Oregon",
        "affiliation_city": "Eugene",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Sample efficient deep reinforcement learning for Diwata microsatellite reaction wheel attitude control",
        "paper_author": "Sarmiento J.A.R.",
        "publication": "Aerospace Systems",
        "citied_by": "0",
        "cover_date": "2023-03-01",
        "Abstract": "The Philippines has launched Diwata satellites to undertake different scientific missions. Low-orbit microsatellites are prone to external disturbances affecting their pointing accuracy; hence, an autonomous attitude control mechanism is vital to its operations. Deep reinforcement learning (DRL) has been proven effective in learning optimal control. There has been prior work regarding using DRL for the satellite’s reaction wheel attitude control in Mission, Attitude, and Telemetry Analysis (MATA)—a simulation environment using Unity for Diwata satellites. However, results show that the applied methods are sample inefficient and still underperform on specific metrics against Diwata’s current attitude control system. In addition, using Unity’s Machine Learning Agents toolkit (ML-Agents) limits the training to Soft-Actor Critic (SAC) and Proximal Policy Optimization (PPO). This study aims to extend the prior research using Twin-Delayed Deep Deterministic Policy Gradient (TD3) and Prioritized Experience Replay (PER) to improve the performance and sample efficiency of the satellite agent. The training was done using OpenAI Gym connected to the MATA simulation environment. We conclude that TD3-PER outperforms the algorithms of SAC, PPO, and PID of the prior study in both sample efficiency and control performance.",
        "DOI": "10.1007/s42401-022-00169-3",
        "affiliation_name": "University of the Philippines Diliman",
        "affiliation_city": "Quezon City",
        "affiliation_country": "Philippines"
    },
    {
        "paper_title": "Global map of a comprehensive drought/flood index and analysis of controlling environmental factors",
        "paper_author": "Pang J.",
        "publication": "Natural Hazards",
        "citied_by": "2",
        "cover_date": "2023-03-01",
        "Abstract": "Developing an index that can assess the marked changes in water resources is necessary for drought/flood hazard prevention and water resource planning, particularly in the context of global climate change and intensified human activities. In this study, spatiotemporal variations of water resources variables, namely precipitation (P), evapotranspiration (ET), and runoff (R), regarded as water input, output, and storage, respectively, were systematically statistically analyzed. Following this, a comprehensive Copula-based Drought/Flood Index (CDFI) was constructed to identify the drought/flood hazards, and machine learning and game theory were then successively applied to quantify the influence of the Large-scale Climate Indices (LCIs) on drought/flood intensity. The results reveal that temporally, P, ET, and R present similar turning-point years in the 1990s, with values changing from significantly decreasing to insignificantly decreasing, significantly increasing, and insignificantly decreasing, respectively. Spatially, the water resources become drier in the arid area and wetter in the humid area. The CDFI results demonstrate an increased flood probability in the mountains and near-polar rivers, and drought in extremely arid areas. Moreover, the influencing factors of CDFI exhibited notable spatial heterogeneity. CDFI is controlled by the Arctic Oscillation in most regions of the world, and the influence of sea surface temperature on CDFI increases gradually with the decrease of latitude, and extreme events are generally linked to extreme LCIs. Notably, vegetation change (area, type, and growth) affects ET and R, thus enhancing the climate’s impact on drought/flood hazards. The conclusions provide an essential basis for the identification and policy planning of drought/flood hazards caused by the spatiotemporal variations of water resource variables.",
        "DOI": "10.1007/s11069-022-05673-5",
        "affiliation_name": "Beijing Forestry University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Disadvantaged communities have lower access to urban infrastructure",
        "paper_author": "Nicoletti L.",
        "publication": "Environment and Planning B: Urban Analytics and City Science",
        "citied_by": "33",
        "cover_date": "2023-03-01",
        "Abstract": "Disparity in spatial accessibility is strongly associated with growing inequalities among urban communities. Since improving levels of accessibility for certain communities can provide them with upward social mobility and address social exclusion and inequalities in cities, it is important to understand the nature and distribution of spatial accessibility among urban communities. To support decision-makers in achieving inclusion and fairness in policy interventions in cities, we present an open and data-driven framework to understand the spatial nature of accessibility to infrastructure among the different demographics. We find that accessibility to a wide range of infrastructure in any city (54 cities) converges to a Zipf’s law, suggesting that inequalities also appear proportional to growth processes in these cities. Then, assessing spatial inequalities among the socioeconomically clustered urban profiles for 10 of those cities, we find urban communities are distinctly segregated along social and spatial lines. We find low accessibility scores for populations who have a larger share of minorities, earn less and have a relatively lower number of individuals with a university degree. These findings suggest that the reproducible framework we propose may be instrumental in understanding processes leading to spatial inequalities and in supporting cities to devise targeted measures for addressing inequalities for certain underprivileged communities.",
        "DOI": "10.1177/23998083221131044",
        "affiliation_name": "Faculteit Techniek, Bestuur en Management, TU Delft",
        "affiliation_city": "Delft",
        "affiliation_country": "Netherlands"
    },
    {
        "paper_title": "Airtightness evaluation of Canadian dwellings and influencing factors based on measured data and predictive models",
        "paper_author": "Ismaiel M.",
        "publication": "Indoor and Built Environment",
        "citied_by": "9",
        "cover_date": "2023-03-01",
        "Abstract": "The airtightness of buildings has a significant impact on buildings’ energy efficiency, maintenance and occupant comfort. The main goal of this study is to provide an evaluation of the air leakage characteristics of dwellings in different regions in Canada. This study evaluated the key influencing factors on airtightness performance based on a large set of measured data (73,450 dwellings located in Canada with 11 measurement parameters for each). Machine learning models based on multivariate regression (MVR) and Random Forest Ensemble (RFE) were developed to predict the air leakage value. The RFE model, which shows better results than MVR, was used to evaluate the effect of the ageing of buildings. Results showed that the maximum increase in air leakage occurs during the first year after construction – approximately 25%, and then 3.7% in the second year, after which the increase rate becomes insignificant and relatively constant – approximately 0.3% per year. The findings from this study can provide significant information for building designs, building performance simulations and strengthening standards and guidelines policies on indoor environmental quality.",
        "DOI": "10.1177/1420326X221121519",
        "affiliation_name": "University of Alberta",
        "affiliation_city": "Edmonton",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Leveraging machine learning to analyze sentiment from COVID-19 tweets: A global perspective",
        "paper_author": "Rahman M.M.",
        "publication": "Engineering Reports",
        "citied_by": "10",
        "cover_date": "2023-03-01",
        "Abstract": "Since the advent of the worldwide COVID-19 pandemic, analyzing public sentiment has become one of the major concerns for policy and decision-makers. While the priority is to curb the spread of the virus, mass population (user) sentiment analysis is equally important. Though sentiment analysis using different state-of-the-art technologies has been focused on during the COVID-19 pandemic, the reasons behind the variations in public sentiment are yet to be explored. Moreover, how user sentiment varies due to the COVID-19 pandemic from a cross-country perspective has been less focused on. Therefore, the objectives of this study are: to identify the most effective machine learning (ML) technique for classifying public sentiments, to analyze the variations of public sentiment across the globe, and to find the critical contributing factors to sentiment variations. To attain the objectives, 12,000 tweets, 3000 each from the USA, UK, and Bangladesh, were rigorously annotated by three independent reviewers. Based on the labeled tweets, four different boosting ML models, namely, CatBoost, gradient boost, AdaBoost, and XGBoost, are investigated. Next, the top performed ML model predicted sentiment of 300,000 data (100,000 from each country). The public perceptions have been analyzed based on the labeled data. As an outcome, the CatBoost model showed the highest (85.8%) F1-score, followed by gradient boost (84.3%), AdaBoost (78.9%), and XGBoost (83.1%). Second, it was revealed that during the time of the COVID-19 pandemic, the sentiments of the people of the three countries mainly were negative, followed by positive and neutral. Finally, this study identified a few critical concerns that impact primarily varying public sentiment around the globe: lockdown, quarantine, hospital, mask, vaccine, and the like.",
        "DOI": "10.1002/eng2.12572",
        "affiliation_name": "Military Institute of Science and Technology",
        "affiliation_city": "Dhaka",
        "affiliation_country": "Bangladesh"
    },
    {
        "paper_title": "Does increased credibility of elections lead to higher political competition? Evidence from India",
        "paper_author": "Chatterjee S.",
        "publication": "European Journal of Political Economy",
        "citied_by": "0",
        "cover_date": "2023-03-01",
        "Abstract": "A large amount of administrative effort is directed towards making elections credible and reducing electoral fraud in large democracies. However, it is not clear if such policy efforts have a feedback effect on political competition. In this paper, we exploit plausibly exogenous variation in perceptions of electoral credibility following the introduction of a technology-induced voting reform in India and find significant impacts on political competition. Electronic voting machines in India were mandated to include an additional layer of transparency by the introduction of a Voter-Verified Paper Audit Trail (VVPAT). We find that with the introduction of VVPAT, the winning margins and vote share of winners decline whereas the number of candidates in the average race increases. The results are robust to econometric concerns arising out of staggered implementation of the program providing support to our identification design. Our results also point to heterogeneous effects of the VVPAT roll-out in constituencies that received it only once relative to those that got the VVPAT in two successive elections. Interestingly, we note that much of the welfare improvement through increased political competition is reversed with more experience, suggesting the presence of important learning effects.",
        "DOI": "10.1016/j.ejpoleco.2022.102277",
        "affiliation_name": "O.P. Jindal Global University",
        "affiliation_city": "Sonipat",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Towards a wearable education: Understanding the determinants affecting students’ adoption of wearable technologies using machine learning algorithms",
        "paper_author": "Al-Emran M.",
        "publication": "Education and Information Technologies",
        "citied_by": "28",
        "cover_date": "2023-03-01",
        "Abstract": "The emergence of wearable technologies, including smartwatches, has received a considerable attention from scholars across several sectors. However, there is a scarcity of knowledge regarding the determinants affecting the adoption of these wearables in education. Therefore, this research aims to propose a theoretical research model through the integration of the theory of planned behavior (TPB) and protection motivation theory (PMT) to understand the students’ behavioral intention to use smartwatches in learning activities. Through the use of machine learning classification algorithms, the proposed model has been validated using data collected via an online survey from 511 university students. The results indicated that perceived severity, perceived vulnerability, self-efficacy, response efficacy, subjective norm, attitude, and perceived behavioral control have a significant positive impact on students’ behavioral intention to use smartwatches for educational purposes. Besides, response cost was found to have a significant negative effect on students’ behavioral intention. The evidence from these findings provides the policy-makers in higher educational institutions with a clear vision of the most effective policies and best practices to enhance the capacity and potential use of these wearables in educational activities. The theoretical contributions and practical implications were also discussed.",
        "DOI": "10.1007/s10639-022-11294-z",
        "affiliation_name": "Azman Hashim International Business School",
        "affiliation_city": "Kuala Lumpur",
        "affiliation_country": "Malaysia"
    },
    {
        "paper_title": "Efficient Federated DRL-Based Cooperative Caching for Mobile Edge Networks",
        "paper_author": "Tian A.",
        "publication": "IEEE Transactions on Network and Service Management",
        "citied_by": "42",
        "cover_date": "2023-03-01",
        "Abstract": "Edge caching has been regarded as a promising technique for low-latency, high-rate data delivery in future networks, and there is an increasing interest to leverage Machine Learning (ML) for better content placement instead of traditional optimization-based methods due to its self-adaptive ability under complex environments. Despite many efforts on ML-based cooperative caching, there are still several key issues that need to be addressed, especially to reduce computation complexity and communication costs under the optimization of cache efficiency. To this end, in this paper, we propose an efficient cooperative caching (FDDL) framework to address the issues in mobile edge networks. Particularly, we propose a DRL-CA algorithm for cache admission, which extracts a boarder set of attributes from massive requests to improve the cache efficiency. Then, we present an lightweight eviction algorithm for fine-grained replacements of unpopular contents. Moreover, we present a Federated Learning-based parameter sharing mechanism to reduce the signaling overheads in collaborations. We implement an emulation system and evaluate the caching performance of the proposed FDDL. Emulation results show that the proposed FDDL can achieve a higher cache hit ratio and traffic offloading rate than several conventional caching policies and DRL-based caching algorithms, and effectively reduce communication costs and training time.",
        "DOI": "10.1109/TNSM.2022.3198074",
        "affiliation_name": "University of Technology Sydney",
        "affiliation_city": "Sydney",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Exploring benefits and ethical challenges in the rise of mHealth (mobile healthcare) technology for the common good: An analysis of mobile applications for health specialists",
        "paper_author": "Galetsi P.",
        "publication": "Technovation",
        "citied_by": "54",
        "cover_date": "2023-03-01",
        "Abstract": "This study reflects on proliferating ethical initiatives, with an eye on social good in the healthcare industry context while leveraging innovative digital technological infrastructures such as the use of mobile technology for health professionals. A content analysis has been conducted of the descriptions, functions, and user reviews for 168 smartphone apps addressed to health professionals which were classified based on their type regarding diagnosis and features. We used an expanded version of the Communication Privacy Management (CPM) theory to explain the ethical considerations of mHealth apps' selection based on the existing privacy and trustworthiness features, emphasizing apps that utilize smart technologies, such as artificial intelligence (AI). A future agenda is provided for the development of technologically advanced and responsible mHealth apps and their contribution to society. Disease handbook/manual and differential diagnosis are the two most frequently appearing types of mHealth apps. Privacy policy declarations are included in most apps, but a credible source is identified in less than half of the mHealth apps for medical professionals. Apps utilizing AI methods are still few, but users' comments indicate expectations for apps with more smart capabilities. This study is one of the few that offers a multi-layered analysis of the usefulness of health-diagnosis mobile apps for professionals, the ethical challenges that accompany them, and the requirements IT developers must address to increase apps’ use in everyday medical practice for better healthcare and social outcomes.",
        "DOI": "10.1016/j.technovation.2022.102598",
        "affiliation_name": "International Hellenic University",
        "affiliation_city": "Thessaloniki",
        "affiliation_country": "Greece"
    },
    {
        "paper_title": "Optimal time-based strategy for automated negotiation",
        "paper_author": "Mohammad Y.",
        "publication": "Applied Intelligence",
        "citied_by": "5",
        "cover_date": "2023-03-01",
        "Abstract": "Recent years are showing increased adoption of AI technology to automate business and production processes thanks to the recent successes of machine learning techniques. This leads to increased interest in automated negotiation as a method for achieving win-win agreements among self-interested agents. Research in automated negotiation can be traced back to the Nash bargaining game in the mid 20th century. Nevertheless, finding an optimal negotiation strategy against an unknown opponent with an unknown utility function is still an open area of research. The most recent result in this area is the Greedy Concession Algorithm (GCA) which can be shown to be optimal under specific constraints on both the negotiation protocol (non-repeating offers), opponent (static acceptance-model) and search space (deterministic time-based strategies). In this paper, we extend this line of work by providing an algorithmically faster version of GCA called Quick GCA which reduces the time-complexity of the search process from O(K2T) to O(KT) where K is the size of the outcome-space and T is the number of negotiation rounds allowed. Moreover, we show that GCA/QGCA can be applied in a more general setting; Namely with repeating-offers protocols and to search the more general probabilistic time-based strategies. Finally, we heuristically extend QGCA to more general opponents with general time-dependent acceptance-model and negotiation settings (real-time limited negotiations) in three steps called QGCA+, PBS, and PA that iteratively and greedily modify the policy proposed by QGCA+ applied to an approximate static acceptance model. The paper evaluates the proposed approach empirically against state of the art negotiation strategies (winners of all relevant ANAC competition winners) and shows that it outperforms them in a wide variety of negotiation scenarios.",
        "DOI": "10.1007/s10489-022-03662-6",
        "affiliation_name": "NEC Corporation",
        "affiliation_city": "Tokyo",
        "affiliation_country": "Japan"
    },
    {
        "paper_title": "Towards better generalization in quadrotor landing using deep reinforcement learning",
        "paper_author": "Wang J.",
        "publication": "Applied Intelligence",
        "citied_by": "6",
        "cover_date": "2023-03-01",
        "Abstract": "In recent years, the autonomous landing of unmanned aerial vehicles (UAVs) has attracted extensive attention due to the widespread applications of UAVs. With the rapid improvements in machine learning and artificial intelligence, recent research has begun to explore deep reinforcement learning (DRL) to learn the landing policy directly from raw observation data. However, current DRL-based solutions tend to suffer poor generalization to unseen environments. To deal with this issue, we formulate the landing problem as a two-stage DRL problem and bootstrap the DRL procedures by augmenting regular DRL loss with an auxiliary localization task. The auxiliary localization task provides dense supervision signals that aid in landing-relevant representation learning. In particular, two marker localization approaches are delicately designed based on deep classification and regression models, and differences between the two configurations are explored, aiming to answer the fundamental question of how to exploit localization better for representation learning. Furthermore, we propose a novel and flexible sampling strategy called Dynamic Partitioned Experience Replay to stabilize and accelerate the training procedure. Experimental results show that the auxiliary localization tasks combined with the improved sampling strategy aid the trained model to generalize in unseen environments. In addition, the trained model can be seamlessly transferred to the real-world quadrotors and has achieved outstanding landing performances.",
        "DOI": "10.1007/s10489-022-03503-6",
        "affiliation_name": "Peng Cheng Laboratory",
        "affiliation_city": "Shenzhen",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Online Scheduling of Distributed Machine Learning Jobs for Incentivizing Sharing in Multi-Tenant Systems",
        "paper_author": "Wang N.",
        "publication": "IEEE Transactions on Computers",
        "citied_by": "5",
        "cover_date": "2023-03-01",
        "Abstract": "To save cost, companies usually train machine learning (ML) models on a shared multi-tenant system. In this cooperative environment, one of the fundamental challenges is how to distribute resources fairly among tenants such that each tenant is satisfied. A satisfactory allocation policy needs to meet the following properties. First, the performance of each tenant in the shared cluster is at least the same as that in its exclusive cluster partition. Second, no tenant can get more benefits by lying about its demands. Third, tenants cannot use the idle resources of others for free. Moreover, the resource allocation for ML workloads should avoid costly migration overhead. To this end, we propose a three-layer scheduling framework Astraea: i) a batch scheduling framework groups unprocessed ML jobs into multiple batches; ii) a round-by-round algorithm enables tenants to reserve their share of resources and schedule ML jobs in a non-preemptive manner; iii) one-round algorithm based on the primal-dual approach and posted pricing framework, which encourages tenants to report truthful demands and computes a schedule with maximal revenue for each ML job. Besides, our algorithm also allows tenants to trade unused resources on a paid basis. Astraea is proven to achieve performance guarantee, polynomial time complexity and some desirable properties of resource sharing, including batch sharing incentive, strategy-proofness, Pareto efficiency and gain-as-you-contribute fairness. Extensive trace-driven simulations show that our algorithm advances in both fairness and cluster efficiency by at least 20% compared to three state-of-the-art baselines.",
        "DOI": "10.1109/TC.2022.3174566",
        "affiliation_name": "Department of Computer Science",
        "affiliation_city": "Durham",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Approximate MRAM: High-Performance and Power-Efficient Computing With MRAM Chips for Error-Tolerant Applications",
        "paper_author": "Ferdaus F.",
        "publication": "IEEE Transactions on Computers",
        "citied_by": "6",
        "cover_date": "2023-03-01",
        "Abstract": "Approximate computing (AC) leverages the inherent error resilience and is used in many big-data applications from various domains such as multimedia, computer vision, signal processing, and machine learning to improve systems performance and power consumption. Like many other approximate circuits and algorithms, the memory subsystem can also be used to enhance performance and save power significantly. This paper proposes an efficient and effective systematic methodology to construct an approximate non-volatile magneto-resistive RAM (MRAM) framework using consumer-off-the-shelf (COTS) MRAM chips. In the proposed scheme, an extensive experimental characterization of memory errors is performed by manipulating the write latency of MRAM chips which exploits the inherent (intrinsic/extrinsic process variation) stochastic switching behavior of magnetic tunnel junctions (MTJs). The experimental results, involving error-resilient image compression and machine learning applications, reveal that the proposed AC framework provides a significant performance improvement and demonstrates a reduction in MRAM write energy of ∼47.5% on average with negligible or no loss in output quality.",
        "DOI": "10.1109/TC.2022.3174584",
        "affiliation_name": "FIU College of Engineering and Computing",
        "affiliation_city": "Miami",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "2-step Gradient Boosting approach to selectivity bias correction in tax audit: an application to the VAT gap in Italy",
        "paper_author": "Alaimo Di Loro P.",
        "publication": "Statistical Methods and Applications",
        "citied_by": "0",
        "cover_date": "2023-03-01",
        "Abstract": "The revenue loss from tax avoidance can undermine the effectiveness and equity of the government policies. A standard measure of its magnitude is known as the tax gap, that is defined as the difference between the total taxes theoretically collectable and the total taxes actually collected in a given period. Estimation from a micro perspective is usually tackled in the context of bottom-up approaches, where data regularly collected through fiscal audits are analyzed in order to provide inference on the general population. However, the sampling scheme of fiscal audits performed by revenue agencies is not random but characterized by a selection bias toward risky taxpayers. The current standard adopted by the Italian Revenue Agency (IRA) for overcoming this issue in the Tax audit context is the Heckman model, based on linear models for modeling both the selection and the outcome mechanisms. Here we propose the adoption of the CART-based Gradient Boosting in place of standard linear models to account for the complex patterns often arising in the relationships between covariates and outcome. Selection bias is corrected by considering a re-weighting scheme based on propensity scores, attained through the sequential application of a classifier and a regressor. In short we refer to the method as 2-step Gradient Boosting. We argue how this scheme fits the sampling mechanism of the IRA fiscal audits, and it is applied to a sample of VAT declarations from Italian individual firms in the fiscal year 2011. Results show a marked dominance of the proposed method over the currently adopted Heckman model in terms of predictive performances.",
        "DOI": "10.1007/s10260-022-00643-4",
        "affiliation_name": "Libera Università Maria Ss. Assunta",
        "affiliation_city": "Rome",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Robust crisis communication in turbulent times: Conceptualization and empirical evidence from the United States",
        "paper_author": "Zhong W.",
        "publication": "Public Administration",
        "citied_by": "22",
        "cover_date": "2023-03-01",
        "Abstract": "Drawing on recent research on robust governance, we conceptualize robust crisis communication as a dynamic process centered on evolving public communication demands. We propose a three-dimensional measurement for empirically examining the robustness of government crisis communication in the context of the COVID-19 pandemic. We collected 43,642 Twitter messages posted by 50 state governors in the United States from January 1 to June 30, 2020. We applied machine learning algorithms to code the voluminous Twitter data based on messaging topics, sentiments, and interactions. This study found an overall low level of robustness in the governors' crisis communication. Governors most frequently posted reputation management tweets, followed by tweets about the government's handling of the pandemic. This research presents empirical evidence for the heavy influence of politics on governors' crisis communication strategies and highlights the need to understand and build robust crisis communication. 摘要. 本文借鉴稳健性治理的最新相关研究，提出稳健性危机沟通的概念，将其界定为聚焦于满足危机中持续变化的公众沟通需求的动态沟通过程与模式。基于此概念，本文以新型冠状病毒肺炎疫情中的危机沟通为例，构建三维测量方法，用以检验政府危机沟通在内容主题、情感表达和对话互动三方面的稳健程度。本文收集美国50个州长在2020年1月1日至2020年6月30日期间发布的43,642条推特信息，并应用机器学习对所有信息的主题、情感和互动特征进行编码分析。本文发现美国州长危机沟通的稳健性程度总体偏低。美国州长最经常发布与声誉管理相关的推文，其次是政府的疫情防控措施。文章进一步阐释与例证政治因素对州长危机沟通策略的深刻影响，并强调政府部门理解与践行稳健性风险沟通的必要性与重要性。.",
        "DOI": "10.1111/padm.12855",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Does big data serve policy? Not without context. An experiment with in silico social science",
        "paper_author": "Graziul C.",
        "publication": "Computational and Mathematical Organization Theory",
        "citied_by": "1",
        "cover_date": "2023-03-01",
        "Abstract": "The DARPA Ground Truth project sought to evaluate social science by constructing four varied simulated social worlds with hidden causality and unleashed teams of scientists to collect data, discover their causal structure, predict their future, and prescribe policies to create desired outcomes. This large-scale, long-term experiment of in silico social science, about which the ground truth of simulated worlds was known, but not by us, reveals the limits of contemporary quantitative social science methodology. First, problem solving without a shared ontology—in which many world characteristics remain existentially uncertain—poses strong limits to quantitative analysis even when scientists share a common task, and suggests how they could become insurmountable without it. Second, data labels biased the associations our analysts made and assumptions they employed, often away from the simulated causal processes those labels signified, suggesting limits on the degree to which analytic concepts developed in one domain may port to others. Third, the current standard for computational social science publication is a demonstration of novel causes, but this limits the relevance of models to solve problems and propose policies that benefit from the simpler and less surprising answers associated with most important causes, or the combination of all causes. Fourth, most singular quantitative methods applied on their own did not help to solve most analytical challenges, and we explored a range of established and emerging methods, including probabilistic programming, deep neural networks, systems of predictive probabilistic finite state machines, and more to achieve plausible solutions. However, despite these limitations common to the current practice of computational social science, we find on the positive side that even imperfect knowledge can be sufficient to identify robust prediction if a more pluralistic approach is applied. Applying competing approaches by distinct subteams, including at one point the vast TopCoder.com global community of problem solvers, enabled discovery of many aspects of the relevant structure underlying worlds that singular methods could not. Together, these lessons suggest how different a policy-oriented computational social science would be than the computational social science we have inherited. Computational social science that serves policy would need to endure more failure, sustain more diversity, maintain more uncertainty, and allow for more complexity than current institutions support.",
        "DOI": "10.1007/s10588-022-09362-3",
        "affiliation_name": "Rutgers University–New Brunswick",
        "affiliation_city": "New Brunswick",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Neural Photometry-Guided Visual Attribute Transfer",
        "paper_author": "Rodriguez-Pardo C.",
        "publication": "IEEE Transactions on Visualization and Computer Graphics",
        "citied_by": "3",
        "cover_date": "2023-03-01",
        "Abstract": "We present a deep learning-based method for propagating spatially-varying visual material attributes (e.g., texture maps or image stylizations) to larger samples of the same or similar materials. For training, we leverage images of the material taken under multiple illuminations and a dedicated data augmentation policy, making the transfer robust to novel illumination conditions and affine deformations. Our model relies on a supervised image-to-image translation framework and is agnostic to the transferred domain; we showcase a semantic segmentation, a normal map, and a stylization. Following an image analogies approach, the method only requires the training data to contain the same visual structures as the input guidance. Our approach works at interactive rates, making it suitable for material edit applications. We thoroughly evaluate our learning methodology in a controlled setup providing quantitative measures of performance. Last, we demonstrate that training the model on a single material is enough to generalize to materials of the same type without the need for massive datasets.",
        "DOI": "10.1109/TVCG.2021.3133081",
        "affiliation_name": "Universidad Rey Juan Carlos",
        "affiliation_city": "Madrid",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "Algorithmic disclosure rules",
        "paper_author": "Di Porto F.",
        "publication": "Artificial Intelligence and Law",
        "citied_by": "2",
        "cover_date": "2023-03-01",
        "Abstract": "During the past decade, a small but rapidly growing number of Law&Tech scholars have been applying algorithmic methods in their legal research. This Article does it too, for the sake of saving disclosure regulation failure: a normative strategy that has long been considered dead by legal scholars, but conspicuously abused by rule-makers. Existing proposals to revive disclosure duties, however, either focus on the industry policies (e.g. seeking to reduce consumers’ costs of reading) or on rulemaking (e.g. by simplifying linguistic intricacies). But failure may well depend on both. Therefore, this Article develops a `comprehensive approach', suggesting to use computational tools to cope with linguistic and behavioral failures at both the enactment and implementation phases of disclosure duties, thus filling a void in the Law & Tech scholarship. Specifically, it outlines how algorithmic tools can be used in a holistic manner to address the many failures of disclosures from the rulemaking in parliament to consumer screens. It suggests a multi-layered design where lawmakers deploy three tools in order to produce optimal disclosure rules: machine learning, natural language processing, and behavioral experimentation through regulatory sandboxes. To clarify how and why these tasks should be performed, disclosures in the contexts of online contract terms and privacy online are taken as examples. Because algorithmic rulemaking is frequently met with well-justified skepticism, problems of its compatibility with legitimacy, efficacy and proportionality are also discussed.",
        "DOI": "10.1007/s10506-021-09302-7",
        "affiliation_name": "Università del Salento",
        "affiliation_city": "Lecce",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Modeling the progression of COVID-19 deaths using Kalman Filter and AutoML",
        "paper_author": "Han T.",
        "publication": "Soft Computing",
        "citied_by": "16",
        "cover_date": "2023-03-01",
        "Abstract": "The COVID-19 pandemic continues to have a destructive effect on the health and well-being of the global population. A vital step in the battle against it is the successful screening of infected patients, together with one of the effective screening methods being radiology examination using chest radiography. Recognition of epidemic growth patterns across temporal and social factors can improve our capability to create epidemic transmission designs, including the critical job of predicting the estimated intensity of the outbreak morbidity or mortality impact at the end. The study’s primary motivation is to be able to estimate with a certain level of accuracy the number of deaths due to COVID-19, managing to model the progression of the pandemic. Predicting the number of possible deaths from COVID-19 can provide governments and decision-makers with indicators for purchasing respirators and pandemic prevention policies. Thus, this work presents itself as an essential contribution to combating the pandemic. Kalman Filter is a widely used method for tracking and navigation and filtering and time series. Designing and tuning machine learning methods are a labor- and time-intensive task that requires extensive experience. The field of automated machine learning Auto Machine Learning relies on automating this task. Auto Machine Learning tools enable novice users to create useful machine learning units, while experts can use them to free up valuable time for other tasks. This paper presents an objective method of forecasting the COVID-19 outbreak using Kalman Filter and Auto Machine Learning. We use a COVID-19 dataset of Ceará, one of the 27 federative units in Brazil. Ceará has more than 235,222 confirmed cases of COVID-19 and 8850 deaths due to the disease. The TPOT automobile model showed the best result with a 0.99 of R2 score.",
        "DOI": "10.1007/s00500-020-05503-5",
        "affiliation_name": "Dongguan University of Technology",
        "affiliation_city": "Dongguan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Machine Learning-based Simulation Research of On-line Subway Pedestrian Flow Control",
        "paper_author": "Shi J.",
        "publication": "Xitong Fangzhen Xuebao / Journal of System Simulation",
        "citied_by": "0",
        "cover_date": "2023-02-28",
        "Abstract": "For the online optimization of pedestrian flow control in subway station, an algorithm frame for pedestrian flow control in subway station based on machine learning is designed. The pedestrian flow control process of a subway station during morning rush hour is selected, and the agent-based model is built to simulate the control process. The training data is collected through the multiple runs of the model, which is used as the input of deep reinforcement learning network, and the mature net is obtained through adequate training to provide the optimizing scheduling policy. Linking the actual data with the mature net to realize the real-time schedule optimization of subway pedestrian flow control. Simulation experiments show that the framework of the deep reinforcement learning can realize the on-line optimization and the performance is better than traditional algorithm.",
        "DOI": "10.16182/j.issn1004731x.joss.21-0835",
        "affiliation_name": "Tianjin University of Technology",
        "affiliation_city": "Tianjin",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Detecting and Measuring Aggressive Location Harvesting in Mobile Apps via Data-flow Path Embedding",
        "paper_author": "Lu H.",
        "publication": "Proceedings of the ACM on Measurement and Analysis of Computing Systems",
        "citied_by": "4",
        "cover_date": "2023-02-28",
        "Abstract": "Today, location-based services have become prevalent in the mobile platform, where mobile apps provide specific services to a user based on his or her location. Unfortunately, mobile apps can aggressively harvest location data with much higher accuracy and frequency than they need because the coarse-grained access control mechanism currently implemented in mobile operating systems (e.g., Android) cannot regulate such behavior. This unnecessary data collection violates the data minimization policy, yet no previous studies have investigated privacy violations from this perspective, and existing techniques are insufficient to address this violation. To fill this knowledge gap, we take the first step toward detecting and measuring this privacy risk in mobile apps at scale. Particularly, we annotate and release thefirst dataset to characterize those aggressive location harvesting apps and understand the challenges of automatic detection and classification. Next, we present a novel system, LocationScope, to address these challenges by(i) uncovering how an app collects locations and how to use such data through a fine-tuned value set analysis technique,(ii) recognizing the fine-grained location-based services an app provides via embedding data-flow paths, which is a combination of program analysis and machine learning techniques, extracted from its location data usages, and(iii) identifying aggressive apps with an outlier detection technique achieving a precision of 97% in aggressive app detection. Our technique has further been applied to millions of free Android apps from Google Play as of 2019 and 2021. Highlights of our measurements on detected aggressive apps include their growing trend from 2019 to 2021 and the app generators' significant contribution of aggressive location harvesting apps.",
        "DOI": "10.1145/3579447",
        "affiliation_name": "Indiana University Bloomington",
        "affiliation_city": "Bloomington",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Global Convergence of Localized Policy Iteration in Networked Multi-Agent Reinforcement Learning",
        "paper_author": "Zhang Y.",
        "publication": "Proceedings of the ACM on Measurement and Analysis of Computing Systems",
        "citied_by": "15",
        "cover_date": "2023-02-28",
        "Abstract": "We study a multi-agent reinforcement learning (MARL) problem where the agents interact over a given network. The goal of the agents is to cooperatively maximize the average of their entropy-regularized long-term rewards. To overcome the curse of dimensionality and to reduce communication, we propose a Localized Policy Iteration (LPI) algorithm that provably learns a near-globally-optimal policy using only local information. In particular, we show that, despite restricting each agent's attention to only its κ-hop neighborhood, the agents are able to learn a policy with an optimality gap that decays polynomially in κ. In addition, we show the finite-sample convergence of LPI to the global optimal policy, which explicitly captures the trade-off between optimality and computational complexity in choosing κ. Numerical simulations demonstrate the effectiveness of LPI.",
        "DOI": "10.1145/3579443",
        "affiliation_name": "California Institute of Technology",
        "affiliation_city": "Pasadena",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "SLITS: Sparsity-Lightened Intelligent Thread Scheduling",
        "paper_author": "Jin W.",
        "publication": "Proceedings of the ACM on Measurement and Analysis of Computing Systems",
        "citied_by": "2",
        "cover_date": "2023-02-28",
        "Abstract": "A diverse set of scheduling objectives (e.g., resource contention, fairness, priority, etc.) breed a series of objective-specific schedulers for multi-core architectures. Existing designs incorporate thread-to-thread statistics at runtime, and schedule threads based on such an abstraction (we formalize thread-to-thread interaction as the Thread-Interaction Matrix). However, such an abstraction also reveals a consistently-overlooked issue: the Thread-Interaction Matrix (TIM) is highly sparse. Therefore, existing designs can only deliver sub-optimal decisions, since the sparsity issue limits the amount of thread permutations (and its statistics) to be exploited when performing scheduling decisions. We introduce Sparsity-Lightened Intelligent Thread Scheduling (SLITS), a general scheduler design for mitigating the sparsity issue of TIM, with the customizability for different scheduling objectives. SLITS is designed upon the key insight that: the sparsity issue of the TIM can be effectively mitigated via advanced Machine Learning (ML) techniques. SLITS has three components. First, SLITS profiles Thread Interactions for only a small number of thread permutations, and form the TIM using the run-time statistics. Second, SLITS estimates the missing values in the TIM using Factorization Machine (FM), a novel ML technique that can fill in the missing values within a large-scale sparse matrix based on the limited information. Third, SLITS leverages Lazy Reschedule, a general mechanism as the building block for customizing different scheduling policies for different scheduling objectives. We show how SLITS can be (1) customized for different scheduling objectives, including resource contention and fairness; and (2) implemented with only negligible hardware costs. We also discuss how SLITS can be potentially applied to other contexts of thread scheduling. We evaluate two SLITS variants against four state-of-the-art scheduler designs. We highlight that, averaged across 11 benchmarks, SLITS achieves an average speedup of 1.08X over the de facto standard for thread scheduler - the Completely Fair Scheduler, under the 16-core setting for a variety of number of threads (i.e., 32, 64 and 128). Our analysis reveals that the benefits of SLITS are credited to significant improvements of cache utilization. In addition, our experimental results confirm that SLITS is scalable and the benefits are robust when of the number of threads increases. We also perform extensive studies to (1) break down SLITS components to justify the synergy of our design choices, (2) examine the impacts of varying the estimation coverage of FM, (3) justify the necessity of Lazy Reschedule rather than periodic rescheduling, and (4) demonstrate the hardware overheads for SLITS implementations can be marginal (<1% chip area and power).",
        "DOI": "10.1145/3579436",
        "affiliation_name": "Succincter",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Integrating spatial database for predicting soil salinity using machine learning methods in Syrdarya Province, Uzbekistan",
        "paper_author": "Omonov A.",
        "publication": "E3S Web of Conferences",
        "citied_by": "1",
        "cover_date": "2023-02-28",
        "Abstract": "Soil salinization of irrigated lands is a global problem in providing the necessary food and feed to meet the needs of a growing world population. Salinization in arid and semiarid areas can occur when the water table is three and more meters above the soil surface. Nowadays, innovative technologies are widely implemented in agriculture to increase yields and monitor changes in any area timely. Advanced technologies such as remote sensing (R.S.) data have become an economically efficient tool for assessing, detecting, mapping, and monitoring saline areas. This study aims to develop a spatial database for evaluating salinization using R.S. and GIS. This research employs various soil salinity indices based on Landsat 8 OLI images and other related geospatial datasets of the study areas. It aims to predict soil salinity using four machine learning methods (Gaussian Mixture Model (GMM), Random Forest (R.F.), Support Vector Machines (SVM), and K-Nearest Neighbors (KNN)). Results showed that R.F. is the most suitable for predicting the soil salinity in the study area with 93 percent overall accuracy. This research contributes to improving the quality of monitoring and improvement of the state of irrigated lands. Also, it develops a preliminary step toward decision-making tools for agricultural policies, such as managing saline areas related to crop production.",
        "DOI": "10.1051/e3sconf/202337101011",
        "affiliation_name": "Badan Riset dan Inovasi Nasional",
        "affiliation_city": "Central Jakarta",
        "affiliation_country": "Indonesia"
    },
    {
        "paper_title": "Reimagining the machine learning life cycle to improve educational outcomes of students",
        "paper_author": "Liu L.T.",
        "publication": "Proceedings of the National Academy of Sciences of the United States of America",
        "citied_by": "12",
        "cover_date": "2023-02-28",
        "Abstract": "Machine learning (ML) techniques are increasingly prevalent in education, from their use in predicting student dropout to assisting in university admissions and facilitating the rise of massive open online courses (MOOCs). Given the rapid growth of these novel uses, there is a pressing need to investigate how ML techniques support longstanding education principles and goals. In this work, we shed light on this complex landscape drawing on qualitative insights from interviews with education experts. These interviews comprise in-depth evaluations of ML for education (ML4Ed) papers published in preeminent applied ML conferences over the past decade. Our central research goal is to critically examine how the stated or implied education and societal objectives of these papers are aligned with the ML problems they tackle. That is, to what extent does the technical problem formulation, objectives, approach, and interpretation of results align with the education problem at hand? We find that a cross-disciplinary gap exists and is particularly salient in two parts of the ML life cycle: the formulation of an ML problem from education goals and the translation of predictions to interventions. We use these insights to propose an extended ML life cycle, which may also apply to the use of ML in other domains. Our work joins a growing number of meta-analytical studies across education and ML research as well as critical analyses of the societal impact of ML. Specifically, it fills a gap between the prevailing technical understanding of machine learning and the perspective of education researchers working with students and in policy.",
        "DOI": "10.1073/pnas.2204781120",
        "affiliation_name": "Society of Fellows, Harvard University",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Reliable Decision from Multiple Subtasks through Threshold Optimization: Content Moderation in the Wild",
        "paper_author": "Son D.",
        "publication": "WSDM 2023 - Proceedings of the 16th ACM International Conference on Web Search and Data Mining",
        "citied_by": "2",
        "cover_date": "2023-02-27",
        "Abstract": "Social media platforms struggle to protect users from harmful content through content moderation. These platforms have recently leveraged machine learning models to cope with the vast amount of user-generated content daily. Since moderation policies vary depending on countries and types of products, it is common to train and deploy the models per policy. However, this approach is highly inefficient, especially when the policies change, requiring dataset re-labeling and model re-training on the shifted data distribution. To alleviate this cost inefficiency, social media platforms often employ third-party content moderation services that provide prediction scores of multiple subtasks, such as predicting the existence of underage personnel, rude gestures, or weapons, instead of directly providing final moderation decisions. However, making a reliable automated moderation decision from the prediction scores of the multiple subtasks for a specific target policy has not been widely explored yet. In this study, we formulate real-world scenarios of content moderation and introduce a simple yet effective threshold optimization method that searches the optimal thresholds of the multiple subtasks to make a reliable moderation decision in a cost-effective way. Extensive experiments demonstrate that our approach shows better performance in content moderation compared to existing threshold optimization methods and heuristics.",
        "DOI": "10.1145/3539597.3570439",
        "affiliation_name": "Match Group",
        "affiliation_city": "Dallas",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Global Counterfactual Explainer for Graph Neural Networks",
        "paper_author": "Huang Z.",
        "publication": "WSDM 2023 - Proceedings of the 16th ACM International Conference on Web Search and Data Mining",
        "citied_by": "31",
        "cover_date": "2023-02-27",
        "Abstract": "Graph neural networks (GNNs) find applications in various domains such as computational biology, natural language processing, and computer security. Owing to their popularity, there is an increasing need to explain GNN predictions since GNNs are black-box machine learning models. One way to address this is counterfactual reasoning where the objective is to change the GNN prediction by minimal changes in the input graph. Existing methods for counterfactual explanation of GNNs are limited to instance-specific local reasoning. This approach has two major limitations of not being able to offer global recourse policies and overloading human cognitive ability with too much information. In this work, we study the global explainability of GNNs through global counterfactual reasoning. Specifically, we want to find a small set of representative counterfactual graphs that explains all input graphs. Towards this goal, we propose GCFExplainer, a novel algorithm powered by vertex-reinforced random walks on an edit map of graphs with a greedy summary. Extensive experiments on real graph datasets show that the global explanation from GCFExplainer provides important high-level insights of the model behavior and achieves a 46.9% gain in recourse coverage and a 9.5% reduction in recourse cost compared to the state-of-the-art local counterfactual explainers.",
        "DOI": "10.1145/3539597.3570376",
        "affiliation_name": "Indian Institute of Technology Delhi",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India"
    },
    {
        "paper_title": "A case-control study on predicting population risk of suicide using health administrative data: a research protocol",
        "paper_author": "Wang J.L.",
        "publication": "BMJ Open",
        "citied_by": "1",
        "cover_date": "2023-02-27",
        "Abstract": "Introduction Suicide has a complex aetiology and is a result of the interaction among the risk and protective factors at the individual, healthcare system and population levels. Therefore, policy and decision makers and mental health service planners can play an important role in suicide prevention. Although a number of suicide risk predictive tools have been developed, these tools were designed to be used by clinicians for assessing individual risk of suicide. There have been no risk predictive models to be used by policy and decision makers for predicting population risk of suicide at the national, provincial and regional levels. This paper aimed to describe the rationale and methodology for developing risk predictive models for population risk of suicide. Methods and analysis A case-control study design will be used to develop sex-specific risk predictive models for population risk of suicide, using statistical regression and machine learning techniques. Routinely collected health administrative data in Quebec, Canada, and community-level social deprivation and marginalisation data will be used. The developed models will be transformed into the models that can be readily used by policy and decision makers. Two rounds of qualitative interviews with end-users and other stakeholders were proposed to understand their views about the developed models and potential systematic, social and ethical issues for implementation; the first round of qualitative interviews has been completed. We included 9440 suicide cases (7234 males and 2206 females) and 661 780 controls for model development. Three hundred and forty-seven variables at individual, healthcare system and community levels have been identified and will be included in least absolute shrinkage and selection operator regression for feature selection. Ethics and dissemination This study is approved by the Health Research Ethnics Committee of Dalhousie University, Canada. This study takes an integrated knowledge translation approach, involving knowledge users from the beginning of the process.",
        "DOI": "10.1136/bmjopen-2022-066423",
        "affiliation_name": "Institut National de Sante Publique Du Québec",
        "affiliation_city": "Quebec",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "6G native intelligence network architecture enabled by intent abstraction and knowledge",
        "paper_author": "Yang J.",
        "publication": "Tongxin Xuebao/Journal on Communications",
        "citied_by": "1",
        "cover_date": "2023-02-25",
        "Abstract": "6G will evolve to an intelligent network, featured by native intelligence and openness. The study of standardization of intelligent network emphasizes the importance of intent-driven network for network intelligence. However, current intent-based networking takes the intent as “What to do” rather than “What you want”. Moreover, current knowledge defined network (KDN) can partially fulfill “How to configure the network” according to “What to do”. Therefore, a 6G native intelligent network architecture based on the intent abstraction and knowledge was proposed, aiming to achieve “How to configure the network” according to “What you want”. Firstly, an intent abstraction module was designed to obtain “What to do” from “What you want”, composed of intent acquisition, intent translation, intent mapping, and intent modeling. Secondly, the cognitive module was proposed to achieve “How to configure the network” according to “What to do”, which got network knowledge through joint optimization of machine learning and logical reasoning. Finally, enabling key technologies such as intent mapping, network information measurement, network policy generation, and network policy verification were introduced to support the implementation of 6G native intelligence.",
        "DOI": "10.11959/j.issn.1000-436x.2023016",
        "affiliation_name": "Space Engineering University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Evaluating the combined effect of climate and anthropogenic stressors on marine coastal ecosystems: Insights from a systematic review of cumulative impact assessment approaches",
        "paper_author": "Simeoni C.",
        "publication": "Science of the Total Environment",
        "citied_by": "21",
        "cover_date": "2023-02-25",
        "Abstract": "Cumulative impacts increasingly threaten marine and coastal ecosystems. To address this issue, the research community has invested efforts on designing and testing different methodological approaches and tools that apply cumulative impact appraisal schemes for a sound evaluation of the complex interactions and dynamics among multiple pressures affecting marine and coastal ecosystems. Through an iterative scientometric and systematic literature review, this paper provides the state of the art of cumulative impact assessment approaches and applications. It gives a specific attention to cutting-edge approaches that explore and model inter-relations among climatic and anthropogenic pressures, vulnerability and resilience of marine and coastal ecosystems to these pressures, and the resulting changes in ecosystem services flow. Despite recent advances in computer sciences and the rising availability of big data for environmental monitoring and management, this literature review evidenced that the implementation of advanced complex system methods for cumulative risk assessment remains limited. Moreover, experts have only recently started integrating ecosystem services flow into cumulative impact appraisal frameworks, but more as a general assessment endpoint within the overall evaluation process (e.g. changes in the bundle of ecosystem services against cumulative impacts). The review also highlights a lack of integrated approaches and complex tools able to frame, explain, and model spatio-temporal dynamics of marine and coastal ecosystems' response to multiple pressures, as required under relevant EU legislation (e.g., Water Framework and Marine Strategy Framework Directives). Progress in understanding cumulative impacts, exploiting the functionalities of more sophisticated machine learning-based approaches (e.g., big data integration), will support decision-makers in the achievement of environmental and sustainability objectives.",
        "DOI": "10.1016/j.scitotenv.2022.160687",
        "affiliation_name": "Portsmouth Business School",
        "affiliation_city": "Portsmouth",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Machine learning for sugarcane mapping based on segmentation in cloud platform",
        "paper_author": "Sudianto ",
        "publication": "AIP Conference Proceedings",
        "citied_by": "3",
        "cover_date": "2023-02-21",
        "Abstract": "Sugarcane mapping is very important in precision agriculture. Mapping can also help enforce policies in land provision of sugarcane agriculture. Currently, sugarcane mapping is often done conventionally, this is less effective because it is expensive and labor-intensive. Reliable mapping at a low cost and fast can use remote sensing and artificial intelligent. This study proposed an automatic sugarcane classification method based on segmentation on a cloud platform. The proposed method uses machine learning algorithms, namely Classification and Regression Trees (CART), Random Forest (RF), Support Vector Machine (SVM) for classification. Then, the segmentation algorithm used a combination of Simple Non-Iterative Clustering (SNIC) for cluster identification and Gray-Level Co-occurrence Matrix (GLCM) for texture extraction. Segmentation is divided into several intervals to get the best results from the classification method. All stages are performed on the Google Earth Engine (GEE) cloud platform based on the use of Landsat 8 images in 2019-2020. The results obtained the best classification is Random Forest (size segmentation=8, Overall Accuracy=99.0%, Kappa Coefficient=0.99), Classification and Regression Trees (size segmentation=8, Overall Accuracy=97.5%, Kappa Coefficient=0.96) and, Support Vector Machine (size segmentation=15, Overall Accuracy=75.0%, Kappa Coefficient=0.58).",
        "DOI": "10.1063/5.0132180",
        "affiliation_name": "IPB University",
        "affiliation_city": "Bogor",
        "affiliation_country": "Indonesia"
    },
    {
        "paper_title": "Explainable machine learning for public policy: Use cases, gaps, and research directions",
        "paper_author": "Amarasinghe K.",
        "publication": "Data and Policy",
        "citied_by": "17",
        "cover_date": "2023-02-20",
        "Abstract": "Explainability is highly desired in machine learning (ML) systems supporting high-stakes policy decisions in areas such as health, criminal justice, education, and employment. While the field of explainable ML has expanded in recent years, much of this work has not taken real-world needs into account. A majority of proposed methods are designed with generic explainability goals without well-defined use cases or intended end users and evaluated on simplified tasks, benchmark problems/datasets, or with proxy users (e.g., Amazon Mechanical Turk). We argue that these simplified evaluation settings do not capture the nuances and complexities of real-world applications. As a result, the applicability and effectiveness of this large body of theoretical and methodological work in real-world applications are unclear. In this work, we take steps toward addressing this gap for the domain of public policy. First, we identify the primary use cases of explainable ML within public policy problems. For each use case, we define the end users of explanations and the specific goals the explanations have to fulfill. Finally, we map existing work in explainable ML to these use cases, identify gaps in established capabilities, and propose research directions to fill those gaps to have a practical societal impact through ML. The contribution is (a) a methodology for explainable ML researchers to identify use cases and develop methods targeted at them and (b) using that methodology for the domain of public policy and giving an example for the researchers on developing explainable ML methods that result in real-world impact.",
        "DOI": "10.1017/dap.2023.2",
        "affiliation_name": "School of Computer Science",
        "affiliation_city": "Pittsburgh",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Digital Detection of Acacia Mangium Trees via Remote Sensing for Controlling the Invasive Population of Biodiversity Threats: Case Study in Brunei",
        "paper_author": "Idrissi M.",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-02-17",
        "Abstract": "The growth of invasive Acacia Mangium has presented a new biodiversity threat to Brunei, which is situated on the biologically diverse island of Borneo. Hazards to the native flora due to Acacia's fast invasion and threats to forest fires have resulted in increased risks of burnable oil. In line with Brunei's National Climate Change Policy, which is reflected in Brunei Vision 2035, it is crucial to conserve Brunei's extensive forest cover by proactive management of the Acacia population in the country's tropical rainforests. Therefore, In line with Brunei's National Climate Change Policy, which is reflected in the Brunei vision, active management of the Acacia population in Brunei's rainforests is considered crucial as it can determine and scope out the country's extensive forest cover. In order to identify the species of Acacia tree and the coverage, this study uses UAV-based, high-resolution RGB photos that have been analysed by machine learning software. The images captured are tested and analysed using a convolutional neural network (CNN) model which is trained to detect the Acacia tree species highlighting regions that indicated a maximum accuracy of 84% based on the manually annotated datasets.",
        "DOI": "10.1145/3594692.3594697",
        "affiliation_name": "Birmingham City University",
        "affiliation_city": "Birmingham",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Ethical issues, fairness, accountability, and transparency in AI/ML",
        "paper_author": "Sunitha K.",
        "publication": "Handbook of Research on Applications of AI, Digital Twin, and Internet of Things for Sustainable Development",
        "citied_by": "0",
        "cover_date": "2023-02-17",
        "Abstract": "The ethical issues of how the computer evolved to the artificial intelligence and machine learning era are explored in this chapter. To develop these intelligent systems, what are the basic principles, policies, and rules? How are these systems helpful to humankind as well as to society? How are businesses and other relevant organizations adapting AI and ML? AI and ML are booming technology. They have major applications in healthcare, computer vision, traffic networks, manufacturing, business trade markets, and so on.",
        "DOI": "10.4018/978-1-6684-6821-0.ch007",
        "affiliation_name": "CMR College of Engineering &amp; Technology",
        "affiliation_city": "Hyderabad",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Human-robot force cooperation analysis by deep reinforcement learning",
        "paper_author": "Li S.",
        "publication": "Industrial Robot",
        "citied_by": "1",
        "cover_date": "2023-02-17",
        "Abstract": "Purpose: This study aims to realize natural and effort-saving motion behavior and improve effectiveness for different operators in human–robot force cooperation. Design/methodology/approach: The parameter of admittance model is identified by deep deterministic policy gradient (DDPG) to realize human–robot force cooperation for different operators in this paper. The movement coupling problem of hybrid robot is solved by realizing position and pose drags. In DDPG, minimum jerk trajectory is selected as the reward objective function, and the variable prioritized experience replay is applied to balance the exploration and exploitation. Findings: A series of simulations are implemented to validate the superiority and stability of DDPG. Furthermore, three sets of experiments involving mass parameter, damping parameter and DDPG are implemented, the effect of DDPG in real environment is validated and could meet the cooperation demand for different operators. Originality/value: DDPG is applied in admittance model identification to realize human–robot force cooperation for different operators. And minimum jerk trajectory is introduced into reward objective to meet requirement of human arm free movements. The algorithm proposed in this paper could be further extended in the other operation task.",
        "DOI": "10.1108/IR-05-2022-0135",
        "affiliation_name": "Guangxi University",
        "affiliation_city": "Nanning",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Sim-real joint experimental verification for an unmanned surface vehicle formation strategy based on multi-agent deterministic policy gradient and line of sight guidance",
        "paper_author": "Li Y.",
        "publication": "Ocean Engineering",
        "citied_by": "11",
        "cover_date": "2023-02-15",
        "Abstract": "The formation of multiple Unmanned Surface Vehicles (USVs) is an effective way to extend the capabilities of a single USV to satisfy relatively complex tasks in practice. In this study, we proposed a formation-strategy-based deep reinforcement learning method called Multi-agent Deterministic Policy Gradient (MADDPG) to realize multi-USV formation. In this work, Line of Sight (LOS) guidance is integrated into the formation strategy under a leader-follower scheme. With the advantage of ignoring the dynamic model of the USV, the proposed formation strategy has strong migration potential to be transferred to other multi-agent systems. To evaluate the performance of the multi-USV formation, we designed two different scenarios in line with the practical tasks carried out with the multi-USV system covering observation aperture enhancement with the desired formation and dynamic non-cooperative target roundup. The performance of the proposed multi-USV formation strategy was demonstrated in both a simulation environment and a real-world environment. Compared with other deep reinforcement learning-inspired and traditional approaches, our proposed strategy based on MADDPG achieved a higher task success rate. It also outperformed the Deep Deterministic Policy Gradient (DDPG) in other metrics because it can acquire knowledge more effectively from dynamic environments by observing joint information and from the centralized training.",
        "DOI": "10.1016/j.oceaneng.2023.113661",
        "affiliation_name": "Faculty of Robot Science and Engineering, Northeastern University",
        "affiliation_city": "Shenyang",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Obstacle avoidance for environmentally-driven USVs based on deep reinforcement learning in large-scale uncertain environments",
        "paper_author": "Wang P.",
        "publication": "Ocean Engineering",
        "citied_by": "17",
        "cover_date": "2023-02-15",
        "Abstract": "This paper focuses on obstacle avoidance for the environmentally-driven unmanned surface vehicles (USVs) in large-scale and uncertain environments. A novel speed adaptive robust obstacle avoidance (SAROA) approach is proposed with the deep reinforcement learning (DRL). A feature enhanced dynamic training method for the DRL is proposed, which significantly improves the sampling efficiency and accelerates convergence. The sensory cues, the executed action and the reward feedback function are properly designed for reinforcement learning to realize robust obstacle avoidance in uncertain environment. Moreover, the obstacle perception domain and the line-of-sight (LOS) based target tracking method are proposed to enable the USVs to avoid collision as well as to follow the path in large-scale environment. In addition, regarding that the environmentally-driven USV is normally super under-actuated and its speed is uncontrollable, a speed adaptive zone is proposed to adapt the obstacle avoidance policies to various navigation speeds, which significantly improves the adaptability and robustness of the proposed obstacle avoidance strategy. Extensive obstacle avoidance tests for the environmentally-driven USV with different navigation speed are conducted, which demonstrates that the DRL based SAROA approach shows excellent practicability and robustness for the environmentally-driven USVs in large-scale and uncertain environments.",
        "DOI": "10.1016/j.oceaneng.2023.113670",
        "affiliation_name": "Ministry of Natural Resources of the People's Republic of China",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Data-driven prediction and evaluation on future impact of energy transition policies in smart regions",
        "paper_author": "Yang C.",
        "publication": "Applied Energy",
        "citied_by": "11",
        "cover_date": "2023-02-15",
        "Abstract": "To meet widely recognised carbon neutrality targets, over the last decade metropolitan regions around the world have implemented policies to promote the generation and use of sustainable energy. Nevertheless, there is an availability gap in formulating and evaluating these policies in a timely manner, since sustainable energy capacity and generation are dynamically determined by various factors along dimensions based on local economic prosperity and societal green ambitions. We develop a novel data-driven platform to predict and evaluate energy transition policies by applying an artificial neural network and a technology diffusion model. Using Singapore, London, and California as case studies of metropolitan regions at distinctive stages of energy transition, we show that in addition to forecasting renewable energy generation and capacity, the platform is particularly powerful in formulating future policy scenarios. We recommend global application of the proposed methodology to future sustainable energy transition in smart regions.",
        "DOI": "10.1016/j.apenergy.2022.120523",
        "affiliation_name": "Queen's University Belfast",
        "affiliation_city": "Belfast",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Intelligent cleanup scheme for soiled photovoltaic modules",
        "paper_author": "Po-Ching Hwang H.",
        "publication": "Energy",
        "citied_by": "9",
        "cover_date": "2023-02-15",
        "Abstract": "In recent years, solar energy systems have increased significantly worldwide. However, over time, the efficiency of photovoltaic (PV) systems is always affected primarily by soiling deposits on the surfaces of PV modules. The soiling deposits lower the intensity of the irradiation transmittance, and the performance of the PV system is also reduced. Therefore, cleaning PV modules is a very routine and critical task. To reduce the efficiency loss caused by soiling deposits and increase lifetime revenue as much as possible, we propose an intelligent method for monitoring soiling status with a statistical approach, an image processing (IP) scheme, and a machine learning (ML) algorithm. Based on the experimental result, the accuracy of our method is 98.39% which indicates that it classifies the soiling status of solar panels excellently. Therefore, we believe the proposed method can assist maintenance personnel in determining the near-optimal policy of cleaning schedules for PV systems. This also decreases power loss and saves labor and time for long-term maintenance.",
        "DOI": "10.1016/j.energy.2022.126293",
        "affiliation_name": "National Yang Ming Chiao Tung University",
        "affiliation_city": "Hsinchu",
        "affiliation_country": "Taiwan"
    },
    {
        "paper_title": "Support Vector Machine based spectrum handoff scheme for seamless handover in Cognitive Radio Networks",
        "paper_author": "Iyer S.",
        "publication": "Concurrency and Computation: Practice and Experience",
        "citied_by": "2",
        "cover_date": "2023-02-15",
        "Abstract": "The advancements in wireless communication go in leaps and bounds ushering in due attention to spectrum sharing. Spectrum scarcity is one of the major limitations causing hardships in the existing wireless networks. Cognitive Radio Networks (CRNs) emerge as a solution to tide over such humps. It prompts the secondary user (SU) to look out for unused spectrum and utilize them. The CRN helps the SU by permitting it to switch over to unused portions of the spectrum. When a primary user (PU) claims back the spectrum, SU is obliged to perform a spectrum handoff. The SU decides the type of policy to be chosen for the handoff. Such a decision-making step during the handoff of the spectrum is imperative only if a changing policy is required. In this research work, Artificial Neural Networks (ANNs), Logistic Regression and Support Vector Machine (SVM) are proposed and implemented for a seamless handoff in CRN. From the experimental verifications, it is observed that the training accuracy is 97.9% and 97.6% for ANN and SVM, respectively. But during the actual phase, SVM to a certain extent performed better. This is due to the convergence nature of SVM on global minima.",
        "DOI": "10.1002/cpe.7534",
        "affiliation_name": "ST Engineering",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore"
    },
    {
        "paper_title": "New facets of prevention: diet assessment, childhood adversity, influenza vaccination, and guideline implementation",
        "paper_author": "Crea F.",
        "publication": "European Heart Journal",
        "citied_by": "1",
        "cover_date": "2023-02-14",
        "Abstract": "NA",
        "DOI": "10.1093/eurheartj/ehad041",
        "affiliation_name": "Fondazione Policlinico Universitario Agostino Gemelli IRCCS",
        "affiliation_city": "Rome",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Variations of Wintertime Ambient Volatile Organic Compounds in Beijing, China, from 2015 to 2019",
        "paper_author": "Li J.",
        "publication": "Environmental Science and Technology Letters",
        "citied_by": "7",
        "cover_date": "2023-02-14",
        "Abstract": "Air quality in China significantly improved after strict regulations were put in place. To assess the efficacy of volatile organic compound (VOC) control strategies, online measurements of wintertime ambient VOCs at a Beijing urban location were taken in December and January from 2014 to 2019. We assessed the changes in mixing ratios of 81 VOC species and cancer risks of 10 hazard species. The impact of meteorology on VOC mixing ratios was decoupled using a machine-learning approach. The highest and lowest VOC mixing ratios were observed in wintertime 2017 (December 2016 and January 2017) and 2019 (December 2018 and January 2019), respectively, with a decrease in 63.5%. Both emission control policies and meteorological conditions helped reduce VOC mixing ratios in the winter in Beijing. Alkenes and alkynes decreased significantly from 2015 to 2019. In contrast, the contribution of aromatics increased considerably from 2015 to 2017, and many aromatics had higher mixing ratios in 2019 than in 2015, indicating that aromatics may be a key group for future air quality improvement. The cancer risks due to VOC exposure in the wintertime in Beijing in 2019 were lower than in other years but were still much higher than the safety level.",
        "DOI": "10.1021/acs.estlett.2c00919",
        "affiliation_name": "Peking University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Developing a machine learning algorithm to predict probability of retear and functional outcomes in patients undergoing rotator cuff repair surgery: Protocol for a retrospective, multicentre study",
        "paper_author": "Allaart L.J.H.",
        "publication": "BMJ Open",
        "citied_by": "3",
        "cover_date": "2023-02-10",
        "Abstract": "Introduction The effectiveness of rotator cuff tear repair surgery is influenced by multiple patient-related, pathology-centred and technical factors, which is thought to contribute to the reported retear rates between 17% and 94%. Adequate patient selection is thought to be essential in reaching satisfactory results. However, no clear consensus has been reached on which factors are most predictive of successful surgery. A clinical decision tool that encompassed all aspects is still to be made. Artificial intelligence (AI) and machine learning algorithms use complex self-learning models that can be used to make patient-specific decision-making tools. The aim of this study is to develop and train an algorithm that can be used as an online available clinical prediction tool, to predict the risk of retear in patients undergoing rotator cuff repair. Methods and analysis This is a retrospective, multicentre, cohort study using pooled individual patient data from multiple studies of patients who have undergone rotator cuff repair and were evaluated by advanced imaging for healing at a minimum of 6 months after surgery. This study consists of two parts. Part one: collecting all potential factors that might influence retear risks from retrospective multicentre data, aiming to include more than 1000 patients worldwide. Part two: combining all influencing factors into a model that can clinically be used as a prediction tool using machine learning. Ethics and dissemination For safe multicentre data exchange and analysis, our Machine Learning Consortium adheres to the WHO regulation € Policy on Use and Sharing of Data Collected by WHO in Member States Outside the Context of Public Health Emergencies'. The study results will be disseminated through publication in a peer-reviewed journal. Institutional Review Board approval does not apply to the current study protocol.",
        "DOI": "10.1136/bmjopen-2022-063673",
        "affiliation_name": "Université de Genève Faculté de Médecine",
        "affiliation_city": "Geneva",
        "affiliation_country": "Switzerland"
    },
    {
        "paper_title": "The Role of Telehealth in Enabling Sustainable Innovation and Circular Economies in Health",
        "paper_author": "Kalogeropoulos D.",
        "publication": "Telehealth and Medicine Today",
        "citied_by": "1",
        "cover_date": "2023-02-09",
        "Abstract": "Digital health interventions, including the use of telehealth augmented by artificial intelligence (AI), support an increasingly broad range of improvement goals for prevention and treatment. Limitations obstructing the many digital benefits of the targeted healthcare innovations from reaching their full potential include the lack of robust usability and user-centered design, nimble regulatory policy, and lack of adequate high-quality evidence and methodologies to evaluate the performance generalization and clinical robustness. We explore health innovation using different value systems and solutions proposed to overcome the fundamental limitations arising in the data value system. We propose a new telehealth paradigm and incorporate intervention designs, which combine clinical innovation with innovation in data resource development. Machine learning and AI have the potential to enable circular economies for digital and health innovation, in which sustainable solutions can be offered within a data-enabled collaborative and shared digital ecosystem. Alignment of industry standards, adjustments to regulatory policies, and the embrace of new governance models for telehealth-based innovation are essential for this new approach for health innovation scaling, clinical adoption, and social innovation.",
        "DOI": "10.30953/tmt.v8.409",
        "affiliation_name": "University College London",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Visceral fat and attribute-based medicine in chronic kidney disease",
        "paper_author": "Kataoka H.",
        "publication": "Frontiers in Endocrinology",
        "citied_by": "16",
        "cover_date": "2023-02-09",
        "Abstract": "Visceral adipose tissue plays a central role in obesity and metabolic syndrome and is an independent risk factor for both cardiovascular and metabolic disorders. Increased visceral adipose tissue promotes adipokine dysregulation and insulin resistance, leading to several health issues, including systemic inflammation, oxidative stress, and activation of the renin-angiotensin-aldosterone system. Moreover, an increase in adipose tissue directly and indirectly affects the kidneys by increasing renal sodium reabsorption, causing glomerular hyperfiltration and hypertrophy, which leads to increased proteinuria and kidney fibrosis/dysfunction. Although the interest in the adverse effects of obesity on renal diseases has grown exponentially in recent years, the relationship between obesity and renal prognosis remains controversial. This may be attributed to the long clinical course of obesity, numerous obesity-related metabolic complications, and patients’ attributes. Multiple individual attributes influencing the pathophysiology of fat accumulation make it difficult to understand obesity. In such cases, it may be effective to elucidate the pathophysiology by conducting research tailored to individual attributes from the perspective of attribute-based medicine/personalized medicine. We consider the appropriate use of clinical indicators necessary, according to attributes such as chronic kidney disease stage, level of visceral adipose tissue accumulation, age, and sex. Selecting treatments and clinical indicators based on individual attributes will allow for advancements in the clinical management of patients with obesity and chronic kidney disease. In the clinical setting of obesity-related nephropathy, it is first necessary to accumulate attribute-based studies resulting from the accurate evaluation of visceral fat accumulation to establish evidence for promoting personalized medicine.",
        "DOI": "10.3389/fendo.2023.1097596",
        "affiliation_name": "Tokyo Women's Medical University",
        "affiliation_city": "Tokyo",
        "affiliation_country": "Japan"
    },
    {
        "paper_title": "ChatGPT: five priorities for research",
        "paper_author": "van Dis E.A.M.",
        "publication": "Nature",
        "citied_by": "978",
        "cover_date": "2023-02-09",
        "Abstract": "NA",
        "DOI": "10.1038/d41586-023-00288-7",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Persistent coverage of UAVs based on deep reinforcement learning with wonderful life utility",
        "paper_author": "Sun Z.",
        "publication": "Neurocomputing",
        "citied_by": "5",
        "cover_date": "2023-02-07",
        "Abstract": "The optimization problem of persistent coverage for a target region by using unmanned aerial vehicles (UAVs) is addressed in this study. A deep reinforcement learning algorithm (DRL) based on bidirectional recurrent neural networks (BRNN) is proposed to obtain the optimal control output policy of UAVs which manipulate the UAVs to periodically cover the whole target region and to minimize the maximum age of cells. The UAVs coordinate autonomously by using wonderful life utility (WLU) functions and BRNN. Because all control policies share parameters, the algorithm has strong robustness and scalability which enable individual UAV to freely join or leave the task without affecting the operation of the entire system. The algorithm uses consistent outputs to control multiple heterogeneous UAVs. Simulation results are given to illustrate the effectiveness of the proposed method.",
        "DOI": "10.1016/j.neucom.2022.11.091",
        "affiliation_name": "National University of Defense Technology China",
        "affiliation_city": "Changsha",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Research and Discussion on Management of Hydrologic Survey Driven by New Generation Information Technology",
        "paper_author": "Mei J.",
        "publication": "Frontiers in Artificial Intelligence and Applications",
        "citied_by": "0",
        "cover_date": "2023-02-06",
        "Abstract": "Since the 1970s, the Bureau of Hydrology, Changjiang Water Resources Commission has carried out research on automatic flood reporting, innovation of hydrometry methods, and improvement of 'one station, one policy' monitoring and reporting capabilities, laying a technical foundation for the development of hydrologic survey; the construction of Web-based intelligent hydrometry system ('WISH' system) has been carried out, laying an information foundation for hydrologic survey. Through the research and practice in recent years, the management system of 'Mainly patrol survey, combining standing survey with patrol survey' has been gradually established. However, while liberating the productive forces, hydrologic survey also brings some new problems, such as the marginalization of survey stations, hollow management, low survey efficiency and idle assets. Therefore, based on a comprehensive understanding of the management of grass-roots stations, equipment and facilities, and the development of patrol survey, the paper studied and discussed the management mode of hydrologic survey driven by the new generation of information technology in combination with new technologies such as computer vision and machine learning.",
        "DOI": "10.3233/FAIA230057",
        "affiliation_name": "Changjiang Water Resources Commission",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Bahamian seagrass extent and blue carbon accounting using Earth observation",
        "paper_author": "Blume A.",
        "publication": "Frontiers in Marine Science",
        "citied_by": "11",
        "cover_date": "2023-02-06",
        "Abstract": "Seagrasses are among the world’s most productive ecosystems due to their vast ‘blue’ carbon sequestration rates and stocks, yet have a largely untapped potential for climate change mitigation and national climate agendas like the Nationally Determined Contributions of the Paris Agreement. To account for the value of seagrasses for these agendas, spatially explicit high-confidence seagrass ecosystem assessments guided by nationally aggregated data are necessary. Modern Earth Observation advances could provide a scalable technological solution to assess the national extent and blue carbon service of seagrass ecosystems. Here, we developed and applied a scalable Earth Observation framework within the Google Earth Engine cloud computing platform to account the national extent, blue carbon stock and sequestration rate of seagrass ecosystems across the shallow waters of The Bahamas—113,037 km2. Our geospatial ecosystem extent accounting was based on big multi-temporal data analytics of over 18,000 10-m Sentinel-2 images acquired between 2017-2021, and deep feature engineering of multi-temporal spectral, color, object-based and textural metrics with Random Forests machine learning classification. The extent accounting was trained and validated using a nationwide reference data synthesis based on human-guided image annotation, recent space-borne benthic habitat maps, and field data collections. Bahamian seagrass carbon stocks and sequestration rates were quantified using region-specific in-situ seagrass blue carbon data. The mapped Bahamian seagrass extent covers an area up to 46,792 km2, translating into a carbon storage of 723 Mg C, and a sequestration rate of 123 Mt CO2 annually. This equals up to 68 times the amount of CO2 emitted by The Bahamas in 2018, potentially rendering the country carbon-neutral. The developed accounts fill a vast mapping blank in the global seagrass map—29% of the global seagrass extent—highlighting the necessity of including their blue carbon fluxes into national climate agendas and showcasing the need for more cost-effective conservation and restoration efforts for their meadows. We envisage that the synergy between our scalable Earth Observation technology and near-future nation-specific in-situ observations can and will support spatially-explicit seagrass and ocean ecosystem accounting, accelerating effective policy-making, blue carbon crediting, and relevant financial investments in and beyond The Bahamas.",
        "DOI": "10.3389/fmars.2023.1058460",
        "affiliation_name": "Deutsches Zentrum für Luft- und Raumfahrt (DLR)",
        "affiliation_city": "Koln",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Intelligent Identification of Rural Housing in Economically Underdeveloped Areas Based on Deep Learning and Mathematical Morphology",
        "paper_author": "Lao C.",
        "publication": "Tropical Geography",
        "citied_by": "0",
        "cover_date": "2023-02-05",
        "Abstract": "With the rapid growth of China's economy and the supportive government policies and funds, the demand for and the expansion of rural housing in China are on the rise, which tightens requirements for rural housing. However, most of the present rural housing suffers from unplanned growth. The sprawl of rural housing adversely affects the quantity and quality of land resources, in particular, productive agricultural lands; therefore, it is necessary to regulate the growth of rural housing and protect farmlands through spatiotemporally continuous monitoring. Currently, the monitoring of rural housing in China is mainly conducted via in-situ inspection of the national land survey, which is restricted by unfavorable conditions (e.g., weather, outbreaks, and traffic) as well as impairing real-time and reliable control over information collected. To resolve this issue, this study proposed an intelligent model to recognize rural housing in underdeveloped areas based on deep learning and mathematical morphology (MobileNet-MM). The model was based on high-resolution remote sensing data, MobileNetV2 (a convolutional neural network architecture well performing on mobile devices), and mathematical morphology. First, the obtained data were segmented and manually screened and tagged to construct a training dataset. Second, the training dataset was used to train MobileNet-MM, with the expansion operation being used to compensate for identification errors of deep learning. Finally, the accuracy of MobileNet-MM to identify and monitor rural housing was tested, resulting in 84.5% accuracy. The comparison of the accuracies of MobileNet-MM and ResNet34 (a state-of-the-art image classification model) indicated that ResNet34 misclassified a large area of rural housing that was mainly distributed on the edge of the region as well as cropland and vegetation as rural housing, with its weak ability to recognize actual rural housing. The MobileNet-MM model predicted rural housing accurately and land boundary precisely, with the misclassified area being scattered, and its average accuracy, is 10.6% higher than that of ResNet34. The novelties of this study were two-fold: (1) a high-resolution training dataset of rural housing in underdeveloped areas was generated, which provides data support for the development of subsequent models; and (2) an intelligent model to recognize rural housing in underdeveloped areas (MobileNet-MM) based on deep learning and mathematical morphology was proposed.",
        "DOI": "10.13284/j.cnki.rddl.003628",
        "affiliation_name": "Guangdong University of Technology",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Applications of Crowdsourcing in Evidence Synthesis: A Case Study of Cochrane Crowd",
        "paper_author": "Li X.",
        "publication": "Journal of Library and Information Science in Agriculture",
        "citied_by": "0",
        "cover_date": "2023-02-05",
        "Abstract": "[Purpose/Significance] Evidence-informed decision-making is a means to bridge the gap between research and policy and evidence synthesis has become an important tool for evidence-based decision-making in many fields. However, evidence synthesis is resource-intensive, especially when it comes to scientific knowledge on complex issues. The efficiency of evidence synthesis currently cannot meet the needs of decision makers. Crowdsourcing is seen as a potential way to improve the productivity of evidence synthesis. At present, the research and practice on the applications of crowdsourcing in evidence synthesis is still in its infancy. This study takes the application of crowdsourcing in the Cochrane Crowd citizen science project as an example to summarize the practical applications of crowdsourcing in evidence synthesis. The comprehensive analysis of the application mechanism of crowdsourcing in Cochrane Crowd project will provide certain reference and inspiration for the use of crowdsourcing in evidence synthesis, so as to improve the production efficiency of evidence synthesis and provide timely and powerful scientific information for evidence-based decision-making. [Method/Process] The application mechanism of crowdsourcing in the Cochrane Crowd citizen science project was analyzed from five dimensions: crowdsourcer, volunteers, crowdsourcing task, Cochrane Crowd platform and effectiveness evaluation, using literature research, network investigation, case analysis and other methods. Cochrane Crowd provides an easy-to-use interface for contributors to engage volunteers to participate and design, in addition to task-focused learning activities, diverse ways of accessing tasks, interactive online training modules and feedback mechanisms to improve the likelihood of volunteers' performing tasks correctly. At the same time, an agreement algorithm is provided at the platform level to aggregate the crowd classification results, which further improves the possibility of correct classification of records. In addition, the platform has used the records identified by the crowd to build a machine-learning model called as RCT classifier which can predict how likely a new citation is to be described an RCT to reduce the manual burden. [Results/Conclusions] Crowdsourcing is an effective method to improve the efficiency of evidence synthesis and shorten the production cycle. With comprehensive participant training and appropriate quality control mechanisms, it is possible to produce high quality crowdsourcing results that meet the \"gold standard\" of evidence synthesis. In order to motivate volunteers to participate and promote continued engagement, participants are suggested to be provided with clear goals, clear tasks, and timely feedback or rewards. Interest and activity in introducing crowdsourcing into evidence synthesis is growing rapidly, and new tools and platforms to facilitate crowdsourcing also need to be further developed as researchers from different disciplines use crowdsourcing in the evidence synthesis projects. In the future, the application of crowdsourcing in evidence synthesis in different fields and in different stages of evidence synthesis should be further studied.",
        "DOI": "10.13998/j.cnki.issn1002-1248.23-0090",
        "affiliation_name": "Northwest Institute of Eco-Environment and Resources",
        "affiliation_city": "Lanzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Digital transformation to mitigate emergency situations: increasing opioid overdose survival rates through explainable artificial intelligence",
        "paper_author": "Johnson M.",
        "publication": "Industrial Management and Data Systems",
        "citied_by": "26",
        "cover_date": "2023-02-03",
        "Abstract": "Purpose: The global health crisis represents an unprecedented opportunity for the development of artificial intelligence (AI) solutions. This paper aims to integrate explainable AI into the decision-making process in emergency scenarios to help mitigate the high levels of complexity and uncertainty associated with these situations. An AI solution is designed to extract insights into opioid overdose (OD) that can help government agencies to improve their medical emergency response and reduce opioid-related deaths. Design/methodology/approach: This paper employs the design science research paradigm as an overarching framework. Open-access digital data and AI, two essential components within the digital transformation domain, are used to accurately predict OD survival rates. Findings: The proposed AI solution has two primary implications for the advancement of informed emergency management. Results show that it can help not only local agencies plan their resources for timely response to OD incidents, thus improving survival rates, but also governments to identify geographical areas with lower survival rates and their primary contributing factor; hence, they can plan and allocate long-term resources to increase survival rates and help in developing effective emergency-related policies. Originality/value: This paper illustrates that digital transformation, particularly open-access digital data and AI, can improve the emergency management framework (EMF). It also demonstrates that the AI models developed in this study can identify opioid OD trends and determine the significant factors improving survival rates.",
        "DOI": "10.1108/IMDS-04-2021-0248",
        "affiliation_name": "Olin Business School",
        "affiliation_city": "St. Louis",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Validation of depression determinants in caregivers of dementia patients with machine learning algorithms and statistical model",
        "paper_author": "Cho K.",
        "publication": "Frontiers in Medicine",
        "citied_by": "1",
        "cover_date": "2023-02-02",
        "Abstract": "Introduction: Due to its increasing prevalence, dementia is currently one of the most extensively studied health issues. Although it represents a comparatively less-addressed issue, the caregiving burden for dementia patients is likewise receiving attention. Methods: To identify determinants of depression in dementia caregivers, using Community Health Survey (CHS) data collected by the Korea Disease Control and Prevention Agency (KDCA). By setting “dementia caregiver's status of residence with patient” as a standard variable, we selected corresponding CHS data from 2011 to 2019. After refining the data, we split dementia caregiver and general population groups among the dataset (n = 15,708; common variables = 34). We then applied three machine learning algorithms: Extreme Gradient Boosting (XGBoost), Logistic Regression (LR), and Support Vector Classifier (SVC). Subsequently, we selected XGBoost, as it exhibited superior performance to the other algorithms. On the feature importance of XGBoost, we performed a multivariate hierarchical regression analysis to validate the depression causes experienced in each group. We validated the results of the statistical model analysis by performing Welch's t-test on the main determinants exhibited within each group. Results: By verifying the results from machine learning via statistical model analysis, we found “sex” to highly impact depression in dementia caregivers, whereas “status of economic activities” is significantly associated with depression in the general population. Discussion: The evident difference in causes of depression between the two groups may serve as a basis for policy development to improve the mental health of dementia caregivers.",
        "DOI": "10.3389/fmed.2023.1095385",
        "affiliation_name": "Yonsei University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "Long-Term Wetland Monitoring Using the Landsat Archive: A Review",
        "paper_author": "Demarquet Q.",
        "publication": "Remote Sensing",
        "citied_by": "18",
        "cover_date": "2023-02-01",
        "Abstract": "Wetlands, which provide multiple functions and ecosystem services, have decreased and been degraded worldwide for several decades due to human activities and climate change. Managers and scientists need tools to characterize and monitor wetland areas, structure, and functions in the long term and at regional and global scales and assess the effects of planning policies on their conservation status. The Landsat earth observation program has collected satellite images since 1972, which makes it the longest global earth observation record with respect to remote sensing. In this review, we describe how Landsat data have been used for long-term (≥20 years) wetland monitoring. A total of 351 articles were analyzed based on 5 topics and 22 attributes that address long-term wetland monitoring and Landsat data analysis issues. Results showed that (1) the open access Landsat archive successfully highlights changes in wetland areas, structure, and functions worldwide; (2) recent progress in artificial intelligence (AI) and machine learning opens new prospects for analyzing the Landsat archive; (3) most unexplored wetlands can be investigated using the Landsat archive; (4) new cloud-computing tools enable dense Landsat times-series to be processed over large areas. We recommend that future studies focus on changes in wetland functions using AI methods along with cloud computing. This review did not include reports and articles that do not mention the use of Landsat imagery.",
        "DOI": "10.3390/rs15030820",
        "affiliation_name": "Université de Rennes",
        "affiliation_city": "Rennes",
        "affiliation_country": "France"
    },
    {
        "paper_title": "“Nutrition Facts Labels” for Artificial Intelligence/Machine Learning-Based Medical Devices—The Urgent Need for Labeling Standards",
        "paper_author": "Gerke S.",
        "publication": "George Washington Law Review",
        "citied_by": "9",
        "cover_date": "2023-02-01",
        "Abstract": "Artificial Intelligence (“AI”), particularly its subset Machine Learning (“ML”), is quickly entering medical practice. The U.S. Food and Drug Administration (“FDA”) has already cleared or approved more than 520 AI/ ML-based medical devices, and many more devices are in the research and development pipeline. AI/ML-based medical devices are not only used in clinics by health care providers but are also increasingly offered directly to consumers for use, such as apps and wearables. Despite their tremendous potential for improving health care, AI/ML-based medical devices also raise many regulatory issues. This Article focuses on one issue that has not received sustained attention in the legal or policy debate: labeling for AI/ML-based medical devices. Labeling is crucial to prevent harm to patients and consumers (e.g., by reducing the risk of bias) and ensure that users know how to properly use the device and assess its benefits, potential risks, and limitations. It can also support transparency to users and thus promote public trust in new digital health technologies. This Article is the first to identify and thoroughly analyze the unique challenges of labeling for AI/ML-based medical devices and provide solutions to address them. It establishes that there are currently no standards of labeling for AI/ML-based medical devices. This is of particular concern as some of these devices are prone to biases, are opaque (“black boxes”), and have the ability to continuously learn. This Article argues that labeling standards for AI/ML-based medical devices are urgently needed, as the current labeling requirements for medical devices and the FDA’s case-by-case approach for a few AI/ML-based medical devices are insufficient. In particular, it proposes what such standards could look like, including eleven key types of information that should be included on the label, ranging from indications for use and details on the data sets to model limitations, warnings and precautions, and privacy and security. In addition, this Article argues that “nutrition facts labels,” known from food products, are a promising label design for AI/ML-based medical devices. Such labels should also be “dynamic” (rather than static) for adaptive algorithms that can continuously learn. Although this Article focuses on AI/ML-based medical devices, it also has implications for AI/ ML-based products that are not subject to FDA regulation.",
        "DOI": "NA",
        "affiliation_name": "Penn State Dickinson Law",
        "affiliation_city": "Carlisle",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "A MACHINE LEARNING APPROACH TO GDP NOWCASTING: AN EMERGING MARKET EXPERIENCE",
        "paper_author": "Ghosh S.",
        "publication": "Buletin Ekonomi Moneter dan Perbankan",
        "citied_by": "4",
        "cover_date": "2023-02-01",
        "Abstract": "The growth rate of real Gross Domestic Product (GDP), as measured by the National Statistical Office of India, is an important metric for monetary policy making. Because GDP is released with a significant lag, particularly for the emerging market economies, this article presents various methodologies for nowcasting and forecasting GDP, using both traditional time series and machine learning methods. Further, considering the importance of forward-looking information, our nowcasting model incorporates financial market data and an economic uncertainty index, in addition to high-frequency traditional macroeconomic indicators. Our findings suggest an improvement in the performance of nowcasting using a hybrid of machine learning and conventional time series methods.",
        "DOI": "10.59091/1410-8046.2055",
        "affiliation_name": "Bank of India",
        "affiliation_city": "Mumbai",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Sentiment Analysis Model to Predict People's Opinion of the Trimester System in Saudi Arabia",
        "paper_author": "Alsulami M.M.",
        "publication": "International Journal of Engineering Trends and Technology",
        "citied_by": "2",
        "cover_date": "2023-02-01",
        "Abstract": "The trimester system is a new academic system in the education sector in Saudi Arabia. At the beginning of 2021, the Ministry of Education announced the introduction of the trimester system in general education, aiming to overcome the gap between the actual number of study hours in Saudi Arabia and those in international educational systems. This paper, using the sentiment analysis of Twitter data, investigated people's opinions about the trimester system. We extracted and conducted a multi-class classification model using several machine learning classifiers to classify each tweet in terms of its sentiment polarity appropriately. Results showed that both Linear Regression and Random Forest classifiers achieved better performance with multi-class models than other classifiers. The analysis results showed a neutral emotional state of most Saudi users regarding the trimester system. This indicates a need to explain the policies and changes regarding this system to people, so they understand this system better. The results of this research could help decision makers to understand the emotional aspects of the trimester system in the Saudi community.",
        "DOI": "10.14445/22315381/IJETT-V71I2P246",
        "affiliation_name": "Taif University",
        "affiliation_city": "Taif",
        "affiliation_country": "Saudi Arabia"
    },
    {
        "paper_title": "Unraveling the Channels of Food Security of the Households in Northern Kenya: Evidence from an Exclusive Dataset",
        "paper_author": "Rono P.K.",
        "publication": "Current Developments in Nutrition",
        "citied_by": "7",
        "cover_date": "2023-02-01",
        "Abstract": "Background: Most of the 10 million Kenyans lacking food security lived in the arid and semi-arid northern part of the country in a climatic condition of high temperatures and very little rainfall throughout the year. Frequent droughts had devastating effects on the livelihoods and food availability of the population. Objectives: The objective of this study was to assess the food security status of the households in Northern Kenya and examine the factors contributing to their food security. Methods: De-identified secondary data were used from the 2015 Feed the Future household survey conducted in 9 counties of Northern Kenya. The experience-based indicator of food security was derived from the 6-item Household Food Security Survey Module (HFSSM), which categorized sample households into 3 groups: food secure, having low food security, and having very low food security. An ordered probit model and machine learning algorithm, namely ordered random forest, were used to find the most important determinants of food security. Results: Findings suggest that the daily per capita food expenditure, level of education of the household head, and durable asset ownership are the key predictors of food security. Households living in rural areas were likely to have low food security, but their probability of being food secure increased with at least primary education and livestock ownership, thus reflecting the importance of education and livestock production among rural communities in Northern Kenya. Access to improved water and participation in food security programs were found to be more important for food security among rural households than they were for urban households. Conclusions: These results implied that long-term policies on improving access to education, livestock ownership, and improved water may shape the food security status of rural households in Northern Kenya.",
        "DOI": "10.1016/j.cdnut.2022.100005",
        "affiliation_name": "Egerton University",
        "affiliation_city": "Njoro",
        "affiliation_country": "Kenya"
    },
    {
        "paper_title": "Enabling Factors and Durations Data Analytics for Dynamic Freight Parking Limits",
        "paper_author": "Castrellon J.P.",
        "publication": "Transportation Research Record",
        "citied_by": "5",
        "cover_date": "2023-02-01",
        "Abstract": "Freight parking operations occur amid conflicting conditions of public space scarcity, competition with other users, and the inefficient management of loading zones (LZ) at cities’ curbside. The dynamic nature of freight operations, and the static LZ provision and regulation, accentuate these conflicting conditions at specific peak times. This generates supply–demand mismatches of parking infrastructure. These mismatches have motivated the development of Smart LZ that bring together technology, parking infrastructure, and data analytics to allocate space and define dynamic duration limits based on users’ needs. Although the dynamic duration limits unlock the possibility of a responsive LZ management, there is a narrow understanding of factors and analytical tools that support their definition. Therefore, the aim of this paper is twofold. Firstly, to identify factors for enabling dynamic parking durations policies. Secondly, to assess data analytics tools that estimate freight parking durations and LZ occupation levels based on operational and locational features. Semi-structured interviews and focus group analyses showed that public space use assessment, parking demand estimation, enforcement capabilities, and data sharing strategies are the most relevant factors when defining dynamic parking limits. This paper used quantitative models to assess different analytical tools that study LZ occupation and parking durations using tracked freight parking data from the City of Vic (Spain). CatBoost outperformed other machine learning (ML) algorithms and queuing models in estimating LZ occupation and parking durations. This paper contributes to the freight parking field by understanding how data analytics support dynamic parking limits definition, enabling responsive curbside management.",
        "DOI": "10.1177/03611981221115086",
        "affiliation_name": "Universidad Nacional de Colombia",
        "affiliation_city": "Bogota",
        "affiliation_country": "Colombia"
    },
    {
        "paper_title": "Forecasting carbon dioxide emissions: application of a novel two-stage procedure based on machine learning models",
        "paper_author": "Wang C.",
        "publication": "Journal of Water and Climate Change",
        "citied_by": "18",
        "cover_date": "2023-02-01",
        "Abstract": "Accurate forecast of carbon dioxide (CO2) emissions plays a significant role in China’s carbon peaking and carbon neutrality policies. A novel two-stage forecast procedure based on support vector regression (SVR), random forest (RF), ridge regression (Ridge), and artificial neural network (ANN) is proposed and evaluated by comparing it with the single-stage forecast procedure. Nine independent variables’ data (study period: 1985–2020) are used to forecast the CO2 emissions in China. Our results reveal that, when the time gap, h increases from 1 to 8, the average root mean squared error (RMSE) and mean absolute error (MAE) of SVR–SVR, SVR–RF, SVR–Ridge, and SVR–ANN are almost uni-formly lower than errors arising from their single-stage version, respectively. Among these two-stage models, SVR–ANN exhibits the lowest forecast errors, whereas SVR–RF admits the highest. The mean percentage decrease in forecast errors of SVR–SVR vs. SVR, SVR–RF vs. RF, SVR–Ridge vs. Ridge, and SVR–ANN vs. ANN are 36.06, 5.98, 43.05, and 14.81 for RMSE, and 36.06, 6.91, 43.27, and 15.35 for MAE. Our two-stage procedure is also suitable to forecast other variables, such as fossil fuel and renewable energy consumption.",
        "DOI": "10.2166/wcc.2023.331",
        "affiliation_name": "Shanghai Normal University Tianhua College",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Crop Monitoring in Smallholder Farms Using Unmanned Aerial Vehicles to Facilitate Precision Agriculture Practices: A Scoping Review and Bibliometric Analysis",
        "paper_author": "Gokool S.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "39",
        "cover_date": "2023-02-01",
        "Abstract": "In this study, we conducted a scoping review and bibliometric analysis to evaluate the state-of-the-art regarding actual applications of unmanned aerial vehicle (UAV) technologies to guide precision agriculture (PA) practices within smallholder farms. UAVs have emerged as one of the most promising tools to monitor crops and guide PA practices to improve agricultural productivity and promote the sustainable and optimal use of critical resources. However, there is a need to understand how and for what purposes these technologies are being applied within smallholder farms. Using Biblioshiny and VOSviewer, 23 peer-reviewed articles from Scopus and Web of Science were analyzed to acquire a greater perspective on this emerging topical research focus area. The results of these investigations revealed that UAVs have largely been used for monitoring crop growth and development, guiding fertilizer management, and crop mapping but also have the potential to facilitate other PA practices. Several factors may moderate the potential of these technologies. However, due to continuous technological advancements and reductions in ownership and operational costs, there remains much cause for optimism regarding future applications of UAVs and associated technologies to inform policy, planning, and operational decision-making.",
        "DOI": "10.3390/su15043557",
        "affiliation_name": "University of the Western Cape",
        "affiliation_city": "Bellville",
        "affiliation_country": "South Africa"
    },
    {
        "paper_title": "Time Efficiency Improvement in Quadruped Walking with Supervised Training Joint Model",
        "paper_author": "Yeoh C.E.",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "0",
        "cover_date": "2023-02-01",
        "Abstract": "To generate stable walking of a quadruped, the complexity of the configuration of the robot involves a significant amount of optimization that decreases to its time efficiency. To address this issue, a machine learning method was used to build a simplified control policy using joint models for the supervised training of quadruped robots. This study considered 12 joints for a four-legged robot, and each joint value was determined based on the conventional method of walking simulation and prepossessed, equaling 2508 sets of data. For data training, the multilayer perceptron model was used, and the optimized number of epochs used to train the model was 5000. The trained models were implemented in robot walking simulations, and they improved performance with an average distance error of 0.0719 m and a computational time as low as 91.98 s.",
        "DOI": "10.3390/app13042658",
        "affiliation_name": "University of California, Los Angeles",
        "affiliation_city": "Los Angeles",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Identifying Corn Lodging in the Mature Period Using Chinese GF-1 PMS Images",
        "paper_author": "Huang X.",
        "publication": "Remote Sensing",
        "citied_by": "10",
        "cover_date": "2023-02-01",
        "Abstract": "Efficient, fast, and accurate crop lodging monitoring is urgent for farmers, agronomists, insurance loss adjusters, and policymakers. This study aims to explore the potential of Chinese GF-1 PMS high-spatial-resolution images for corn lodging monitoring and to find a robust and efficient way to identify corn lodging accurately and efficiently. Three groups of image features and five machine-learning approaches are used for classifying non-lodged, moderately lodged, and severely lodged areas. Our results reveal that (1) the combination of spectral bands, optimized vegetation indexes, and texture features classify corn lodging with an overall accuracy of 93.81% and a Kappa coefficient of 0.91. (2) The random forest is an efficient, robust, and easy classifier to identify corn lodging with the F1-score of 0.95, 0.92, and 0.95 for non-lodged, moderately lodged, and severely lodged areas, respectively. (3) The GF-1 PMS image has great potential for identifying corn lodging on a regional scale.",
        "DOI": "10.3390/rs15040894",
        "affiliation_name": "Ministry of Agriculture of the People's Republic of China",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Transferability of Covariates to Predict Soil Organic Carbon in Cropland Soils",
        "paper_author": "Broeg T.",
        "publication": "Remote Sensing",
        "citied_by": "20",
        "cover_date": "2023-02-01",
        "Abstract": "Precise knowledge about the soil organic carbon (SOC) content in cropland soils is one requirement to design and execute effective climate and food policies. In digital soil mapping (DSM), machine learning algorithms are used to predict soil properties from covariates derived from traditional soil mapping, digital elevation models, land use, and Earth observation (EO). However, such DSM models are trained for a specific dataset and region and have so far only allowed limited general statements to be made that would enable the models to be transferred to different regions. In this study, we test the transferability of SOC models for cropland soils using five different covariate groups: multispectral soil reflectance composites (satellite), soil legacy data (soil), digital elevation model derivatives (terrain), climate parameters (climate), and combined models (combined). The transferability was analyzed using data from two federal states in southern Germany: Bavaria and Baden-Wuerttemberg. First, baseline models were trained for each state with combined models performing best in both cases (R2 = 0.68/0.48). Next, the models were transferred and tested with soil samples from the other state whose data were not used during model calibration. Only satellite and combined models were transferable, but accuracy declined in both cases. In the final step, models were trained with samples from both states (mixed-data models) and applied to each state separately. This process significantly improved the accuracies of satellite, terrain, and combined models, while it showed no effect on climate models and decreased the models based on soil covariates. The experiment underlines the importance of EO for the transfer and extrapolation of DSM models.",
        "DOI": "10.3390/rs15040876",
        "affiliation_name": "Johann Heinrich von Thünen Institute",
        "affiliation_city": "Braunschweig",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Development of Smart Mobility Infrastructure in Saudi Arabia: A Benchmarking Approach",
        "paper_author": "Alanazi F.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "25",
        "cover_date": "2023-02-01",
        "Abstract": "Smart mobility systems offers solutions for traffic congestion, transport management, emergency, and road safety. However, the success of smart mobility lies in the availability of intelligent transportation infrastructure. This paper studied smart mobility systems in three Asia-Pacific countries (South Korea, Singapore, and Japan) to highlight the major strategies leading their successful journey to become smart cities for aspiring countries, such as the Kingdom of Saudi Arabia (KSA), to emulate. A robust framework for evaluating smart mobility systems in the three countries and Saudi Arabia was developed based on the indicators derived from the smart mobility ecosystem and three major types of transport services (private, public, and emergency). Sixty indicators of smart mobility systems were identified through a rigorous search of the literature and other secondary sources. Robots, drones, IoT, 5G, hyperloop tunnels, and self-driving technologies formed part of the indicators in those countries. The study reveals that the three Asia-Pacific countries are moving head-to-head in terms of smart mobility development. Saudi Arabia can join these smarter countries through inclusive development, standardization, and policy-driven strategies with clear commitments to public, private, and research collaborations in the development of its smart mobility ecosystem. Moreover, cybersecurity must be taken seriously because most of the smart mobility systems use wireless and IoT technologies, which may be vulnerable to hacking, and thus impact system safety. In addition, the smart mobility system should include data analytics, machine learning, and artificial intelligence in developing and monitoring the evaluation in terms of user experience and future adaptability.",
        "DOI": "10.3390/su15043158",
        "affiliation_name": "Jouf University",
        "affiliation_city": "Sakakah",
        "affiliation_country": "Saudi Arabia"
    },
    {
        "paper_title": "Application of Explainable Artificial Intelligence (XAI) in Urban Growth Modeling: A Case Study of Seoul Metropolitan Area, Korea",
        "paper_author": "Kim M.",
        "publication": "Land",
        "citied_by": "14",
        "cover_date": "2023-02-01",
        "Abstract": "Unplanned and rapid urban growth requires the reckless expansion of infrastructure including water, sewage, energy, and transportation facilities, and thus causes environmental problems such as deterioration of old towns, reduction of open spaces, and air pollution. To alleviate and prevent such problems induced by urban growth, the accurate prediction and management of urban expansion is crucial. In this context, this study aims at modeling and predicting urban expansion in Seoul metropolitan area (SMA), Korea, using GIS and XAI techniques. To this end, we examined the effects of land-cover, socio-economic, and environmental features in 2007 and 2019, within the optimal radius from a certain raster cell. Then, this study combined the extreme gradient boosting (XGBoost) model and Shapley additive explanations (SHAP) in analyzing urban expansion. The findings of this study suggest urban growth is dominantly affected by land-cover characteristics, followed by topographic attributes. In addition, the existence of water body and high ECVAM grades tend to significantly reduce the possibility of urban expansion. The findings of this study are expected to provide several policy implications in urban and environmental planning fields, particularly for effective and sustainable management of lands.",
        "DOI": "10.3390/land12020420",
        "affiliation_name": "Korea Environment Institute",
        "affiliation_city": "Sejong",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "GIS-Based Landslide Susceptibility Modeling: A Comparison between Best-First Decision Tree and Its Two Ensembles (BagBFT and RFBFT)",
        "paper_author": "Gui J.",
        "publication": "Remote Sensing",
        "citied_by": "9",
        "cover_date": "2023-02-01",
        "Abstract": "This study aimed to explore and compare the application of current state-of-the-art machine learning techniques, including bagging (Bag) and rotation forest (RF), to assess landslide susceptibility with the base classifier best-first decision tree (BFT). The proposed two novel ensemble frameworks, BagBFT and RFBFT, and the base model BFT, were used to model landslide susceptibility in Zhashui County (China), which suffers from landslides. Firstly, we identified 169 landslides through field surveys and image interpretation. Then, a landslide inventory map was built. These 169 historical landslides were randomly classified into two groups: 70% for training data and 30% for validation data. Then, 15 landslide conditioning factors were considered for mapping landslide susceptibility. The three ensemble outputs were estimated with a receiver operating characteristic (ROC) curve and statistical tests, as well as a new approach, the improved frequency ratio accuracy. The areas under the ROC curve (AUCs) for the training data (success rate) of the three algorithms were 0.722 for BFT, 0.869 for BagBFT, and 0.895 for RFBFT. The AUCs for the validating groups (prediction rates) were 0.718, 0.834, and 0.872, respectively. The frequency ratio accuracy of the three models was 0.76163 for the BFT model, 0.92220 for the BagBFT model, and 0.92224 for the RFBFT model. Both BagBFT and RFBFT ensembles can improve the accuracy of the BFT base model, and RFBFT was relatively better. Therefore, the RFBFT model is the most effective approach for the accurate modeling of landslide susceptibility mapping (LSM). All three models can improve the identification of landslide-prone areas, enhance risk management ability, and afford more detailed information for land-use planning and policy setting.",
        "DOI": "10.3390/rs15041007",
        "affiliation_name": "CINTECX",
        "affiliation_city": "Vigo",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "Savior or Distraction for Survival: Examining the Applicability of Machine Learning for Rural Family Farms in the United Arab Emirates",
        "paper_author": "Gilani S.A.M.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "8",
        "cover_date": "2023-02-01",
        "Abstract": "Machine learning (ML) has seen a substantial increase in its role in improving operations for staff and customers in different industries. However, there appears to be a somewhat limited adoption of ML by farm businesses, highlighted by a review of the literature investigating innovative behaviors by rural businesses. A review of the literature identified a dearth of studies investigating ML adoption by farm businesses in rural regions of the United Arab Emirates (UAE), especially in the context of family-owned farms. Therefore, this paper aims to investigate the drivers and barriers to ML adoption by family/non-family-owned farms in rural UAE. The key research questions are (1) what are the drivers and barriers for rural UAE farms adopting ML? As well as (2) is there a difference in the drivers and barriers between family and non-family-owned farms? Twenty semi-structured interviews were conducted with farm businesses across several rural regions in the UAE. Then, through a Template Analysis (TA), drivers and barriers for rural UAE-based farm owners adopting ML were identified. Interview findings highlighted that farms could benefit from adopting ML in daily operations to save costs and improve efficiency. However, 16 of 20 farms were unaware of the benefits related to ML due to access issues (highlighted by 12 farms) in incorporating ML operations, where they felt that incorporating ML into their operations was costly (identified by 8 farms). It was also identified that non-family-owned farms were more likely to take up ML, which was attributed to local culture influencing family farms (11 farms identified culture as a barrier). This study makes a theoretical contribution by proposing the Machine Learning Adoption Framework (MLAF). In terms of practical implications, this study proposes an ML program specifically targeting the needs of farm owners in rural UAE. Policy-based implications are addressed by the findings aligning with the United Nations’ Sustainability Development Goals 9 (Industry, Innovation, and Infrastructure) and 11 (Sustainable Cities and Communities).",
        "DOI": "10.3390/su15043720",
        "affiliation_name": "Westford University College",
        "affiliation_city": "Sharjah",
        "affiliation_country": "United Arab Emirates"
    },
    {
        "paper_title": "Research on the Policy Analysis of Sustainable Energy Based on Policy Knowledge Graph Technology—A Case Study in China",
        "paper_author": "Sun Y.",
        "publication": "Systems",
        "citied_by": "3",
        "cover_date": "2023-02-01",
        "Abstract": "Nowadays, geopolitical, extreme weather and other emergencies have exacerbated the global energy crisis, and thus, have increased the urgency of the world’s transition to sustainable energy. Sustainable energy policies play an important role in the process of sustainable energy transformation. The research on sustainable energy policy is mainly carried out through conventional qualitative and quantitative methods, in which bibliometrics and meta-analysis methods are paid attention to; however, the mining and analysis of the semantics of the relationships between policies are ignored. This paper uses knowledge graph technology to build a knowledge graph of China’s sustainable energy policy by using 10,815 open official documents of sustainable energy policy issued by China from 1981 to 2022. It forms the relevant policy archive storage and details related organizations. The legal source can be traced through the graph database, where the powerful synergy can be seen, and the policy focus can be monitored. In terms of structural data, this paper uses graph algorithms to identify key policy nodes at different stages, to identify the key government departments for policy issuance and cluster policy issuance departments, and it investigates China’s policy evolution in the issue of sustainable energy policies, the evolution of policy issuance departments, and the power co-evolution process between policy issuance departments. The research found that: (1) China’s sustainable energy policy was initiated in environmental protection, and the relevant policies on collecting pollution charges has continued to play an important policy node. Additionally, the three versions of the Environmental Protection Law of the People’s Republic of China have successively become the main legal source of other sustainable energy transformation policies. (2) The prominent feature of China’s sustainable energy policy transformation has involved transforming the process where the issuance of policies came from a single department to the joint issuance of documents by multiple departments. The joint exercise of government functions and powers by multiple departments jointly promotes sustainable energy policies’ implementation and play. (3) In the future, when formulating sustainable energy policies, the Chinese government should focus on the strategic and systematic aspects of the policies, so that the sustainable energy policies can meet both short-term and long-term development goals. At the same time, the synergy of various policies and measures should be fully played in implementing sustainable energy policies. The establishment of the policy knowledge graph based on publicly-open official documents can facilitate the analysis and visualization of sustainable energy policies, providing new ideas for policy research. This paper introduces the knowledge graph, graph machine learning algorithms and big data technology, which can deepen the depth and breadth of people’s research on sustainable energy policy. This study will help the public policy formulation work in the future and has a positive reference value for the evaluation of the implementation effect of policy objectives.",
        "DOI": "10.3390/systems11020102",
        "affiliation_name": "School of Economics and Management",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Analysis of the Behavior Pattern of Energy Consumption through Online Clustering Techniques",
        "paper_author": "Viera J.",
        "publication": "Energies",
        "citied_by": "3",
        "cover_date": "2023-02-01",
        "Abstract": "Analyzing energy consumption is currently of great interest to define efficient energy management strategies. In particular, studying the evolution of the behavior of the consumption pattern can allow energy policies to be defined according to the time of the year. In this sense, this work proposes to study the evolution of energy behavior patterns using online clustering techniques. In particular, the centroids of the groups constructed by the techniques will represent their consumption patterns. Specifically, two unsupervised online machine learning techniques ideal for the stated objective will be analyzed, X-Means and LAMDA, since they are capable of varying and adapting the number of clusters at runtime. These techniques are applied to energy consumption data in commercial buildings, making groupings on previous groups, in our case, monthly and quarterly. We compared their performance by analyzing the evolution of the patterns over time. The results are very promising since the quality of the consumption patterns obtained is very good according to the performance metrics. Thus, the three main contributions of this article are to propose an approach to determine energy consumption patterns using online non-supervised learning approaches, a methodology to analyze and explain the evolution of energy consumption using centroids of clusters, and a comparison strategy of online learning techniques. The online clustering techniques have qualities of the order of 0.59 and 0.41 for Silhouette and Davies-Boulding, respectively, for X-Means and of the order of 0.71 and 0.24 for Silhouette and Davies-Boulding, respectively, for LAMDA in different datasets of energy. The results are motivating since very good results are obtained in terms of the quality of the clusters, particularly with LAMDA; therefore, analyzing its centroids as the patterns of user behaviors makes a lot of sense.",
        "DOI": "10.3390/en16041649",
        "affiliation_name": "IMDEA Networks Institute",
        "affiliation_city": "Leganes",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "Forecasting Electricity Demand by Neural Networks and Definition of Inputs by Multi-Criteria Analysis",
        "paper_author": "Deina C.",
        "publication": "Energies",
        "citied_by": "6",
        "cover_date": "2023-02-01",
        "Abstract": "The planning of efficient policies based on forecasting electricity demand is essential to guarantee the continuity of energy supply for consumers. Some techniques for forecasting electricity demand have used specific procedures to define input variables, which can be particular to each case study. However, the definition of independent and casual variables is still an issue to be explored. There is a lack of models that could help the selection of independent variables, based on correlate criteria and level of importance integrated with artificial networks, which could directly impact the forecasting quality. This work presents a model that integrates a multi-criteria approach which provides the selection of relevant independent variables and artificial neural networks to forecast the electricity demand in countries. It provides to consider the particularities of each application. To demonstrate the applicability of the model a time series of electricity consumption from a southern region of Brazil was used. The dependent inputs used by the neural networks were selected using a traditional method called Wrapper. As a result of this application, with the multi-criteria ELECTRE I method was possible to recognize temperature and average evaporation as explanatory variables. When the variables selected by the multi-criteria approach were included in the predictive models, were observed more consistent results together with artificial neural networks, better than the traditional linear models. The Radial Basis Function Networks and Extreme Learning Machines stood out as potential techniques to be used integrated with a multi-criteria method to better perform the forecasting.",
        "DOI": "10.3390/en16041712",
        "affiliation_name": "Universidade Tecnológica Federal do Paraná",
        "affiliation_city": "Curitiba",
        "affiliation_country": "Brazil"
    },
    {
        "paper_title": "GIS and SDM-Based Methodology for Resource Optimisation: Feasibility Study for Citrus in Mediterranean Area",
        "paper_author": "Catalano G.A.",
        "publication": "Agronomy",
        "citied_by": "4",
        "cover_date": "2023-02-01",
        "Abstract": "South Italy is characterised by a semi-arid climate with scarce rain and high evaporative demand. Since climate change could worsen this condition, the need to optimise water resources in this area is crucial. In citrus cultivation, which involves one of the most important crops bred in Southern Italy, and more generally in Mediterranean regions, deficit irrigation strategies are implemented in order to cope with limited resource availability. On this basis, knowledge on how the territorial distribution of citrus would change in relation to these strategies represents valuable information for stakeholders. Therefore, the objective of this study was to determine the probability of the presence of citrus in Sicily based on changes in the percentage of water deficit in order to identify and analyse change in the surface area as well as the location of the crop. The methodology was based on the application of species distribution models (SDM) and Geographic Information Systems (GIS) to the case study of the province of Syracuse in Sicily. Different geostatistical and machine learning models were applied based on bioclimatic variables measured over three decades, a Digital Terrain Model and irrigation. Assessment of the outcomes was carried out using classification evaluation metrics. The analysis of the outcomes showed that uncorrelated predictor layers mainly included water input that most affected the probability of the presence of citrus fruits. Moreover, GIS analyses showed that deficit irrigation strategies would generate an overall reduction of cultivation surfaces in the territory (e.g., for the Random Forest model the surface reduction was equal to 41.15%) and a decrease of citrus presence in southern areas of the considered territory. In this area, climate conditions are less favourable in terms of temperature and precipitation; thus, these analyses provide useful information for decision support tools in agriculture and land use policy.",
        "DOI": "10.3390/agronomy13020549",
        "affiliation_name": "Università degli Studi di Catania",
        "affiliation_city": "Catania",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Heterogeneity in US Farms: A New Clustering by Production Potentials",
        "paper_author": "Rasool A.",
        "publication": "Agriculture (Switzerland)",
        "citied_by": "0",
        "cover_date": "2023-02-01",
        "Abstract": "This paper uses agglomerative hierarchical cluster analysis to group 2778 farming-defined counties into six clusters, revealing farm patterns across the contiguous 48 states of the United States. To understand the differences in economic performance and improve farm households’ well-being, economists have endeavored to identify patterns in US farming. The US is a leading global producer and exporter of many agricultural and food products. Our primary objective is to construct a policy-relevant farm clustering to characterize agricultural homogeneity in US farms’ production potential. We identify six clusters that are relatively homogeneous in five dimensions: farm size, farm assets, farm labor, farm output, degree of mechanization, and government programs. Minimizing diversity within a cluster allows for analysis of public policy changes on specific clusters and comparison of differential effects of the change across clusters.",
        "DOI": "10.3390/agriculture13020258",
        "affiliation_name": "Pennsylvania State University",
        "affiliation_city": "University Park",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Multi-Feature-Based Identification of Subtropical Evergreen Tree Species Using Gaofen-2 Imagery and Algorithm Comparison",
        "paper_author": "Yuan J.",
        "publication": "Forests",
        "citied_by": "4",
        "cover_date": "2023-02-01",
        "Abstract": "The species and distribution of trees in a forest are critical to the understanding of forest ecosystem processes and the development of forest management strategies. Subtropical forest landscapes feature a complex canopy structure and high stand density. Studies on the effects of classification algorithms on the remote sensing-based identification of tree species are few. GF-2 is the first satellite in China with sub-meter accuracy which has the high resolution and short replay cycle. Here, we considered three representative tree types (Masson pine, Chinese fir, and broadleaved evergreen trees) in the southern subtropical evergreen broadleaved forest region of China as research objects. We quantitatively compared the effects of five machine learning algorithms, including the backpropagation neural network, k-nearest neighbour, polytomous logistic regression, random forest (RF) and support vector machine (SVM), and four features (vegetation index, band reflectance, textural features, and topographic factors) on tree species identification using Gaofen-2 panchromatic and multispectral remote sensing images and field survey data. All five classification algorithms could effectively identify major tree species in subtropical forest areas (overall accuracy [OA] > 87.40%, kappa coefficient > 81.08%). The SVM model exhibited the best identification ability (OA = 90.27%, kappa coefficient = 85.37%), followed by RF (OA = 88.90%, Kappa coefficient = 83.30%). The combination of band reflectance, vegetation index, and the topographic factor performed exhibited the best, followed by the combination of band reflectance, vegetation index, textural feature, and topographic factor. In addition, we find that the classifier constructed by a single feature is not as effective as the combination of multiple feature factors. The addition of topographic factors can significantly improve the ability of tree species identification. According to the results of the five classifiers, the separability of the three tree species was good. The producer’s accuracy and user’s accuracy of Masson pine were more than 90%, and the evergreen broad-leaved tree and Chinese fir were more than 80%. The commission errors and omission errors of the three tree species were evergreen broadleaved tree > Chinese fir > Masson pine. The variable importance assessment results showed that the normalized difference greenness index, altitude, and the modified soil-adjusted vegetation index were the key variables. The results of this study used GF-2 to accurately identify the main tree species of subtropical evergreen forests in China, which can help forest managers to regularly monitor tree species composition and provide theoretical support for forest managers to formulate policies, monitor sustainable plans for wood mining, and forest conservation and management measures.",
        "DOI": "10.3390/f14020292",
        "affiliation_name": "Jiangxi Normal University",
        "affiliation_city": "Nanchang",
        "affiliation_country": "China"
    },
    {
        "paper_title": "An LEO Constellation Early Warning System Decision-Making Method Based on Hierarchical Reinforcement Learning",
        "paper_author": "Cheng Y.",
        "publication": "Sensors",
        "citied_by": "2",
        "cover_date": "2023-02-01",
        "Abstract": "The cooperative positioning problem of hypersonic vehicles regarding LEO constellations is the focus of this research study on space-based early warning systems. A hypersonic vehicle is highly maneuverable, and its trajectory is uncertain. New challenges are posed for the cooperative positioning capability of the constellation. In recent years, breakthroughs in artificial intelligence technology have provided new avenues for collaborative multi-satellite intelligent autonomous decision-making technology. This paper addresses the problem of multi-satellite cooperative geometric positioning for hypersonic glide vehicles (HGVs) by the LEO-constellation-tracking system. To exploit the inherent advantages of hierarchical reinforcement learning in intelligent decision making while satisfying the constraints of cooperative observations, an autonomous intelligent decision-making algorithm for satellites that incorporates a hierarchical proximal policy optimization with random hill climbing (MAPPO-RHC) is designed. On the one hand, hierarchical decision making is used to reduce the solution space; on the other hand, it is used to maximize the global reward and to uniformly distribute satellite resources. The single-satellite local search method improves the capability of the decision-making algorithm to search the solution space based on the decision-making results of the hierarchical proximal policy-optimization algorithm, combining both random hill climbing and heuristic methods. Finally, the MAPPO-RHC algorithm’s coverage and positioning accuracy performance is simulated and analyzed in two different scenarios and compared with four intelligent satellite decision-making algorithms that have been studied in recent years. From the simulation results, the decision-making results of the MAPPO-RHC algorithm can obtain more balanced resource allocations and higher geometric positioning accuracy. Thus, it is concluded that the MAPPO-RHC algorithm provides a feasible solution for the real-time decision-making problem of the LEO constellation early warning system.",
        "DOI": "10.3390/s23042225",
        "affiliation_name": "Harbin Institute of Technology",
        "affiliation_city": "Harbin",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Impedimetric Sensing: An Emerging Tool for Combating the COVID-19 Pandemic",
        "paper_author": "Ong V.",
        "publication": "Biosensors",
        "citied_by": "15",
        "cover_date": "2023-02-01",
        "Abstract": "The COVID-19 pandemic revealed a pressing need for the development of sensitive and low-cost point-of-care sensors for disease diagnosis. The current standard of care for COVID-19 is quantitative reverse transcriptase polymerase chain reaction (qRT-PCR). This method is sensitive, but takes time, effort, and requires specialized equipment and reagents to be performed correctly. This make it unsuitable for widespread, rapid testing and causes poor individual and policy decision-making. Rapid antigen tests (RATs) are a widely used alternative that provide results quickly but have low sensitivity and are prone to false negatives, particularly in cases with lower viral burden. Electrochemical sensors have shown much promise in filling this technology gap, and impedance spectroscopy specifically has exciting potential in rapid screening of COVID-19. Due to the data-rich nature of impedance measurements performed at different frequencies, this method lends itself to machine-leaning (ML) algorithms for further data processing. This review summarizes the current state of impedance spectroscopy-based point-of-care sensors for the detection of the SARS-CoV-2 virus. This article also suggests future directions to address the technology’s current limitations to move forward in this current pandemic and prepare for future outbreaks.",
        "DOI": "10.3390/bios13020204",
        "affiliation_name": "USC Viterbi School of Engineering",
        "affiliation_city": "Los Angeles",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Examining Exposure to Messaging, Content, and Hate Speech from Partisan News Social Media Posts on Racial and Ethnic Health Disparities",
        "paper_author": "Nguyen T.T.",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "3",
        "cover_date": "2023-02-01",
        "Abstract": "We investigated the content of liberal and conservative news media Facebook posts on race and ethnic health disparities. A total of 3,327,360 liberal and conservative news Facebook posts from the United States (US) from January 2015 to May 2022 were collected from the Crowd Tangle platform and filtered for race and health-related keywords. Qualitative content analysis was conducted on a random sample of 1750 liberal and 1750 conservative posts. Posts were analyzed for a continuum of hate speech using a newly developed method combining faceted Rasch item response theory with deep learning. Across posts referencing Asian, Black, Latinx, Middle Eastern, and immigrants/refugees, liberal news posts had lower hate scores compared to conservative posts. Liberal news posts were more likely to acknowledge and detail the existence of racial/ethnic health disparities, while conservative news posts were more likely to highlight the negative consequences of protests, immigration, and the disenfranchisement of Whites. Facebook posts from liberal and conservative news focus on different themes with fewer discussions of racial inequities in conservative news. Investigating the discourse on race and health in social media news posts may inform our understanding of the public’s exposure to and knowledge of racial health disparities, and policy-level support for ameliorating these disparities.",
        "DOI": "10.3390/ijerph20043230",
        "affiliation_name": "UCSF School of Medicine",
        "affiliation_city": "San Francisco",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Maximum Entropy Exploration in Contextual Bandits with Neural Networks and Energy Based Models",
        "paper_author": "Elwood A.",
        "publication": "Entropy",
        "citied_by": "1",
        "cover_date": "2023-02-01",
        "Abstract": "Contextual bandits can solve a huge range of real-world problems. However, current popular algorithms to solve them either rely on linear models or unreliable uncertainty estimation in non-linear models, which are required to deal with the exploration–exploitation trade-off. Inspired by theories of human cognition, we introduce novel techniques that use maximum entropy exploration, relying on neural networks to find optimal policies in settings with both continuous and discrete action spaces. We present two classes of models, one with neural networks as reward estimators, and the other with energy based models, which model the probability of obtaining an optimal reward given an action. We evaluate the performance of these models in static and dynamic contextual bandit simulation environments. We show that both techniques outperform standard baseline algorithms, such as NN HMC, NN Discrete, Upper Confidence Bound, and Thompson Sampling, where energy based models have the best overall performance. This provides practitioners with new techniques that perform well in static and dynamic settings, and are particularly well suited to non-linear scenarios with continuous action spaces.",
        "DOI": "10.3390/e25020188",
        "affiliation_name": "Universidad Nacional de Educacion a Distancia",
        "affiliation_city": "Madrid",
        "affiliation_country": "Spain"
    },
    {
        "paper_title": "Development of a Reinforcement Learning Algorithm to Optimize Corticosteroid Therapy in Critically Ill Patients with Sepsis",
        "paper_author": "Bologheanu R.",
        "publication": "Journal of Clinical Medicine",
        "citied_by": "8",
        "cover_date": "2023-02-01",
        "Abstract": "Background: The optimal indication, dose, and timing of corticosteroids in sepsis is controversial. Here, we used reinforcement learning to derive the optimal steroid policy in septic patients based on data on 3051 ICU admissions from the AmsterdamUMCdb intensive care database. Methods: We identified septic patients according to the 2016 consensus definition. An actor-critic RL algorithm using ICU mortality as a reward signal was developed to determine the optimal treatment policy from time-series data on 277 clinical parameters. We performed off-policy evaluation and testing in independent subsets to assess the algorithm’s performance. Results: Agreement between the RL agent’s policy and the actual documented treatment reached 59%. Our RL agent’s treatment policy was more restrictive compared to the actual clinician behavior: our algorithm suggested withholding corticosteroids in 62% of the patient states, versus 52% according to the physicians’ policy. The 95% lower bound of the expected reward was higher for the RL agent than clinicians’ historical decisions. ICU mortality after concordant action in the testing dataset was lower both when corticosteroids had been withheld and when corticosteroids had been prescribed by the virtual agent. The most relevant variables were vital parameters and laboratory values, such as blood pressure, heart rate, leucocyte count, and glycemia. Conclusions: Individualized use of corticosteroids in sepsis may result in a mortality benefit, but optimal treatment policy may be more restrictive than the routine clinical practice. Whilst external validation is needed, our study motivates a ‘precision-medicine’ approach to future prospective controlled trials and practice.",
        "DOI": "10.3390/jcm12041513",
        "affiliation_name": "University of Cambridge",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Cognitive Software Defined Networking and Network Function Virtualization and Applications",
        "paper_author": "Sharma S.",
        "publication": "Future Internet",
        "citied_by": "5",
        "cover_date": "2023-02-01",
        "Abstract": "The emergence of Software-Defined Networking (SDN) and Network Function Virtualization (NFV) has revolutionized the Internet. Using SDN, network devices can be controlled from a centralized, programmable control plane that is decoupled from their data plane, whereas with NFV, network functions (such as network address translation, firewall, and intrusion detection) can be virtualized instead of being implemented on proprietary hardware. In addition, Artificial Intelligence (AI) and Machine Learning (ML) techniques will be key to automating network operations and enhancing customer service. Many of the challenges behind SDN and NFV are currently being investigated in several projects all over the world using AI and ML techniques, such as AI- and software-based networking, autonomic networking, and policy-based network management. Contributions to this Special Issue come from the above areas of research. Following a rigorous review process, four excellent articles were accepted that address and go beyond many of the challenges mentioned above.",
        "DOI": "10.3390/fi15020078",
        "affiliation_name": "Technological University Dublin",
        "affiliation_city": "Dublin",
        "affiliation_country": "Ireland"
    },
    {
        "paper_title": "Simulated and Real Robotic Reach, Grasp, and Pick-and-Place Using Combined Reinforcement Learning and Traditional Controls",
        "paper_author": "Lobbezoo A.",
        "publication": "Robotics",
        "citied_by": "15",
        "cover_date": "2023-02-01",
        "Abstract": "The majority of robots in factories today are operated with conventional control strategies that require individual programming on a task-by-task basis, with no margin for error. As an alternative to the rudimentary operation planning and task-programming techniques, machine learning has shown significant promise for higher-level task planning, with the development of reinforcement learning (RL)-based control strategies. This paper reviews the implementation of combined traditional and RL control for simulated and real environments to validate the RL approach for standard industrial tasks such as reach, grasp, and pick-and-place. The goal of this research is to bring intelligence to robotic control so that robotic operations can be completed without precisely defining the environment, constraints, and the action plan. The results from this approach provide optimistic preliminary data on the application of RL to real-world robotics.",
        "DOI": "10.3390/robotics12010012",
        "affiliation_name": "University of Waterloo",
        "affiliation_city": "Waterloo",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "A computational offloading optimization scheme based on deep reinforcement learning in perceptual network",
        "paper_author": "Xing Y.",
        "publication": "PLoS ONE",
        "citied_by": "0",
        "cover_date": "2023-02-01",
        "Abstract": "Currently, the deep integration of the Internet of Things (IoT) and edge computing has improved the computing capability of the IoT perception layer. Existing offloading techniques for edge computing suffer from the single problem of solidifying offloading policies. Based on this, combined with the characteristics of deep reinforcement learning, this paper investigates a computation offloading optimization scheme for the perception layer. The algorithm can adaptively adjust the computational task offloading policy of IoT terminals according to the network changes in the perception layer. Experiments show that the algorithm effectively improves the operational efficiency of the IoT perceptual layer and reduces the average task delay compared with other offloading algorithms.",
        "DOI": "10.1371/journal.pone.0280468",
        "affiliation_name": "Shaheed Benazir Bhutto University",
        "affiliation_city": "Nawabshah",
        "affiliation_country": "Pakistan"
    },
    {
        "paper_title": "Using Machine Learning Methods to Predict Consumer Confidence from Search Engine Data",
        "paper_author": "Han H.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "3",
        "cover_date": "2023-02-01",
        "Abstract": "The consumer confidence index is a leading indicator of regional socioeconomic development. Forecasting research on it helps to grasp the future economic trends and consumption trends of regional development in advance. The data contained on the Internet in the era of big data can truly and timely reflect the current economic trends. This paper constructs a conceptual framework for the relationship between the consumer confidence index and web search keywords. It employed six machine learning and deep learning models: the BP neural network, the convolutional neural network, support vector regression, random forest, the ELMAN neural network, and the extreme learning machine to predict the consumer confidence index. The study shows that the use of machine learning models has a better prediction effect on the consumer confidence index. Compared with other models, the BP neural network and the convolutional neural network have lower error indicators and higher model accuracy, which helps decision-makers forecast the consumer confidence index. Consumers search for various goods and prices, as well as macroeconomics, to understand the economic conditions of the market, which affects the consumer confidence index and consumption decisions. Therefore, web search data can be used to predict consumer confidence. Future research can be extended to other macro indicator-related prediction studies. It is important to promote market consumption and confidence, improve consumption policies, and promote national prosperity.",
        "DOI": "10.3390/su15043100",
        "affiliation_name": "Shandong University of Finance and Economics",
        "affiliation_city": "Jinan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "The Typology of Public Schools in the State of Louisiana and Interventions to Improve Performance: A Machine Learning Approach",
        "paper_author": "Kaliba A.R.",
        "publication": "Education Sciences",
        "citied_by": "1",
        "cover_date": "2023-02-01",
        "Abstract": "Extant literature on education research focuses on evaluating schools’ academic performance rather than the performance of educational institutions. Moreover, the State of Louisiana public school system always performs poorly in education outcomes compared to other school systems in the U.S. One of the limiting factors is the stringent standards applied among heterogeneous schools, steaming from the fit-for-all policies. We use a pairwise controlled manifold approximation technique and gradient boosting machine algorithm to typify Louisiana public schools into homogenous clusters and then characterize each identified group. The analyses uncover critical features of failing and high-performing school systems. Results confirm the heterogeneity of the school system, and each school needs tailored support to buoy its performance. Short-term interventions should focus on customized administrative and academic protocols with malleable interpositions addressing individual school shortcomings such as truancy. Long-term policies must discourse authentic economic development programs to foster community engagement and creativity while allocating strategic resources that cultivate resilience at the school and community levels.",
        "DOI": "10.3390/educsci13020160",
        "affiliation_name": "Southern University and A&amp;M College",
        "affiliation_city": "Baton Rouge",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "The Twin Impacts of Income Inequality and Unemployment on Murder Crime in African Emerging Economies: A Mixed Models Approach",
        "paper_author": "Zungu L.T.",
        "publication": "Economies",
        "citied_by": "5",
        "cover_date": "2023-02-01",
        "Abstract": "This study analyses the dynamic impact of income inequality and unemployment on crime in a panel of 15 African countries during the period 1994–2019 using four models: the panel vector autoregression model, the generalized method of moments model, the fixed-effect model, and machine learning. These models were chosen due to their ability to address the dynamics of several entities. The variables employed for empirical investigation include income inequality, unemployment, and crime. Machine learning was adopted to find which socioeconomic issues contribute to crime between the two issues at hand. The results show that income inequality accounts for 64% of crime, making it the biggest contributor to crime. The findings further show that an unexpected shock in inequality and unemployment has a significant positive impact on crime in these countries. Even when pre-tax income held by the top 10% and male unemployment is adopted, the study yields similar results. Educational entertainment through secondary enrolment was found to increase crime, while it was found to decrease crime through tertiary enrolment at the tertiary level. Finally, economic development was found to decrease crime. From a policy perspective, the current study suggests to the government that some policies are more appropriate for addressing concerns about income inequality and unemployment (income policy or fiscal policy). Therefore, more policies targeting the distribution of income are crucial, as that might decrease income inequality while at the same time decreasing crime. In addition, policymakers should focus on addressing structural challenges through the implementation of sound structural reform policies that aim to attract investment consistent with job creation, human development, and growth in African economies.",
        "DOI": "10.3390/economies11020058",
        "affiliation_name": "University of Zululand",
        "affiliation_city": "Ulundi",
        "affiliation_country": "South Africa"
    },
    {
        "paper_title": "Multi-Source Data and Machine Learning-Based Refined Governance for Responding to Public Health Emergencies in Beijing: A Case Study of COVID-19",
        "paper_author": "Yu D.",
        "publication": "ISPRS International Journal of Geo-Information",
        "citied_by": "0",
        "cover_date": "2023-02-01",
        "Abstract": "The outbreak of COVID-19 in Beijing has been sporadic since the beginning of 2022 and has become increasingly severe since October. In China’s policy of insisting on dynamic clearance, fine-grained management has become the focus of current epidemic prevention and control. In this paper, we conduct a refined COVID-19 risk prediction and identification of its influencing factors in Beijing based on neighborhood-scale spatial statistical units. We obtained geographic coordinate data of COVID-19 cases in Beijing and quantified them into risk indices of each statistical unit. Additionally, spatial autocorrelation was used to analyze the epidemic risk clustering characteristics. With the multi-source data, 20 influencing elements were constructed, and their spatial heterogeneity was explored by screening 8 for Multiscale Geographically weighted regression (MGWR) model analysis. Finally, a neural network classification model was used to predict the risk of COVID-19 within the sixth ring of Beijing. The MGWR model and the neural network classification model showed good performance: the R2 of the MGWR model was 0.770, and the accuracy of the neural network classification model was 0.852. The results of this study show that: (1) COVID-19 risk is uneven, with the highest clustering within the Fifth Ring Road of Beijing; (2) The results of the MGWR model show that population structure, population density, road density, residential area density, and living service facility density have significant spatial heterogeneity on COVID-19 risk; and (3) The prediction results show a high COVID-19 risk, with the most severe risk being in the eastern, southeastern and southern regions. It should be noted that the prediction results are highly consistent with the current epidemic situation in Shijingshan District, Beijing, and can provide a strong reference for fine-grained epidemic prevention and control in Beijing.",
        "DOI": "10.3390/ijgi12020069",
        "affiliation_name": "Swinburne University of Technology",
        "affiliation_city": "Hawthorn",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Estimation of Impacts of Global Factors on World Food Prices: A Comparison of Machine Learning Algorithms and Time Series Econometric Models",
        "paper_author": "Ulussever T.",
        "publication": "Foods",
        "citied_by": "13",
        "cover_date": "2023-02-01",
        "Abstract": "It is a well-felt recent phenomenal fact that global food prices have dramatically increased and attracted attention from practitioners and researchers. In line with this attraction, this study uncovers the impact of global factors on predicting food prices in an empirical comparison by using machine learning algorithms and time series econometric models. Covering eight global explanatory variables and monthly data from January 1991 to May 2021, the results show that machine learning algorithms reveal a better performance than time series econometric models while Multi-layer Perceptron is defined as the best machine learning algorithm among alternatives. Furthermore, the one-month lagged global food prices are found to be the most significant factor on the global food prices followed by raw material prices, fertilizer prices, and oil prices, respectively. Thus, the results highlight the effects of fluctuations in the global variables on global food prices. Additionally, policy implications are discussed.",
        "DOI": "10.3390/foods12040873",
        "affiliation_name": "Gulf University for Science and Technology Kuwait",
        "affiliation_city": "Hawally",
        "affiliation_country": "Kuwait"
    },
    {
        "paper_title": "AI weapons: Russia's war in Ukraine shows why the world must enact a ban",
        "paper_author": "Russell S.",
        "publication": "Nature",
        "citied_by": "28",
        "cover_date": "2023-02-01",
        "Abstract": "NA",
        "DOI": "10.1038/d41586-023-00511-5",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Model-Free λ-Policy Iteration for Discrete-Time Linear Quadratic Regulation",
        "paper_author": "Yang Y.",
        "publication": "IEEE Transactions on Neural Networks and Learning Systems",
        "citied_by": "130",
        "cover_date": "2023-02-01",
        "Abstract": "This article presents a model-free λ-policy iteration ( λ-PI) for the discrete-time linear quadratic regulation (LQR) problem. To solve the algebraic Riccati equation arising from solving the LQR in an iterative manner, we define two novel matrix operators, named the weighted Bellman operator and the composite Bellman operator. Then, the λ-PI algorithm is first designed as a recursion with the weighted Bellman operator, and its equivalent formulation as a fixed-point iteration with the composite Bellman operator is shown. The contraction and monotonic properties of the composite Bellman operator guarantee the convergence of the λ-PI algorithm. In contrast to the PI algorithm, the λ-PI does not require an admissible initial policy, and the convergence rate outperforms the value iteration (VI) algorithm. Model-free extension of the λ-PI algorithm is developed using the off-policy reinforcement learning technique. It is also shown that the off-policy variants of the λ-PI algorithm are robust against the probing noise. Finally, simulation examples are conducted to validate the efficacy of the λ-PI algorithm.",
        "DOI": "10.1109/TNNLS.2021.3098985",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "East Lansing",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Status quo and predictors of Weibo users’ attitudes toward lesbians and gay men in 31 provinces in the Chinese mainland: Analysis based on supervised machine learning and provincial panel data",
        "paper_author": "Zheng Q.",
        "publication": "Frontiers in Psychology",
        "citied_by": "0",
        "cover_date": "2023-02-01",
        "Abstract": "Introduction: Public attitudes toward consensual same-sex relations are crucial to lesbians’ and gay men’s rights and society’s well-being, but research addressing this topic in China is limited. We comprehensively explored the current status and predictors of Weibo users’ attitudes toward individuals who are lesbian or gay (IWLG) at the provincial level in the Chinese mainland. Methods: Natural language processing and machine learning techniques were incorporated to analyze 1,934,008 Weibo posts from January 1, 2010, to December 31, 2020, to evaluate Weibo users’ expressed attitudes toward IWLG in 31 provinces in the Chinese mainland guided by the ABC Model of attitude. Results: Although the general attitudes, feelings, and support for the rights of Weibo users toward IWLG among different provinces were relatively positive, knowledge about IWLG was noticeably inaccurate. Economic development and educational level positively predicted certain aspects of attitudes at the provincial level. Conclusion: Weibo users from different provinces are generally supportive and accepting of people who are gay and the rights of the gay community. However, considerable misconceptions and inaccurate knowledge of IWLG surfaced in Weibo users’ posts. Economic development and educational level were important predictors of specific attitudes toward IWLG at the provincial level. Increased efforts to address the unbalanced and insufficient development between different provinces could help reduce the public’s prejudice, stigma, and discrimination toward IWLG. Policies that facilitate greater implementation of Comprehensive Sexuality Education sequentially and effectively are suggested as well.",
        "DOI": "10.3389/fpsyg.2023.1069589",
        "affiliation_name": "College of Arts &amp; Sciences",
        "affiliation_city": "Memphis",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Time-series analysis of Sentinel-2 satellite images for sunflower yield estimation",
        "paper_author": "Amankulova K.",
        "publication": "Smart Agricultural Technology",
        "citied_by": "22",
        "cover_date": "2023-02-01",
        "Abstract": "Accurate estimates and predictions of sunflower crop yields at the pixel and field level are critically important for farmers, service dealers, and policymakers. Several models based on remote sensing data have been developed in yield assessment, but their robustness—especially in small field scale areas—needs to be examined. Here we aim to develop a robust methodology for estimation/prediction of sunflower yield at pilot field scale using Sentinel-2 remote sensing satellite imagery. We conducted the study in Mezőhegyes, south-eastern Hungary. The Random Forest Regression (RFR), a machine learning technique was used in this research to translate the Sentinel-2 spectral bands to sunflower yield based on crop yield data provided by a combine harvester equipped with a yield-monitoring system. Sentinel-2 images obtained from April to September were used to find the best image for prediction. The satellite image acquired on June 28 was found best and considered further for prediction sunflower yield. A developed training model was tested and validated in 10 different parcels to evaluate the performance of the prediction. We examined the results of the prediction model (predicted) against the actual yield data (observed) collected by a combine harvester. The results demonstrated that using 10 spectral bands from Sentinel-2 imagery the best time to predict sunflower yields was between 85 and 105 d into the growing season during the flowering stage. This model achieved high accuracy with low normalized root means square error (RMSE) ranging from 121.9 to 284.5 kg/ha for different test fields. Our results are promising because they prove the possibility of predicting sunflower grain yield at the pixel or field level, 3–4 months before the harvest, which is crucial for planning food policy.",
        "DOI": "10.1016/j.atech.2022.100098",
        "affiliation_name": "Szegedi Tudományegyetem (SZTE)",
        "affiliation_city": "Szeged",
        "affiliation_country": "Hungary"
    },
    {
        "paper_title": "Sustainable Yield Prediction in Agricultural Areas Based on Fruit Counting Approach",
        "paper_author": "Saddik A.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "7",
        "cover_date": "2023-02-01",
        "Abstract": "A sustainable yield prediction in agricultural fields is a very critical task that aims to help farmers have an idea about agricultural operations. Generally, we can find a variety of applications proposed for this purpose that include fruit counting. These applications are based on Artificial Intelligence, especially Deep Learning (DL) and Machine Learning (ML) approaches. These approaches give reliable counting accuracy, but the problem is the use of a large database to achieve the desired accuracy. That makes these approaches limited. For this reason, in this work, we propose a low-complexity algorithm that aims to count green and red apples based on our real dataset collected in the Moroccan region, Fes-Meknes. This algorithm allowed us to further increase sustainability in agricultural fields based on yield prediction. The proposed approach was based on HSV conversion and the Hough transform for fruit counting. The algorithm was divided into three blocks based on image acquisition and filtering for the first block. The second block is the conversion to HSV and the detection of fruits. Finally, the counting operation for the third block. Subsequently, we proposed an implementation based on the low-cost Raspberry system and a desktop. The results show that we can reach 15 fps in the case of the Raspberry architecture and 40 fps based on the desktop. Our proposed system can inform agricultural policy by providing accurate and timely information on crop production, which can be used to guide decisions on food supply and distribution.",
        "DOI": "10.3390/su15032707",
        "affiliation_name": "Universiapolis - Université Internationale d‘Agadir",
        "affiliation_city": "Agadir",
        "affiliation_country": "Morocco"
    },
    {
        "paper_title": "Comparative Analysis of Statistical and Machine Learning Techniques for Rice Yield Forecasting for Chhattisgarh, India",
        "paper_author": "Satpathi A.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "35",
        "cover_date": "2023-02-01",
        "Abstract": "Crop yield forecasting before harvesting is critical for the creation, implementation, and optimization of policies related to food safety as well as for agro-product storage and marketing. Crop growth and development are influenced by the weather. Therefore, models using weather variables can provide reliable predictions of crop yields. It can be tough to select the best crop production forecasting model. Therefore, in this study, five alternative models, viz., stepwise multiple linear regression (SMLR), an artificial neural network (ANN), the least absolute shrinkage and selection operator (LASSO), an elastic net (ELNET), and ridge regression, were compared in order to discover the best model for rice yield prediction. The outputs from individual models were used to build ensemble models using the generalized linear model (GLM), random forest (RF), cubist and ELNET methods. For the previous 21 years, historical rice yield statistics and meteorological data were collected for three districts under three separate agro-climatic zones of Chhattisgarh, viz., Raipur in the Chhattisgarh plains, Surguja in the northern hills, and Bastar in the southern plateau. The models were calibrated using 80% of these datasets, and the remaining 20% was used for the validation of models. The present study concluded that for rice crop yield forecasting, the performance of the ANN was good for the Raipur ((Formula presented.) = 1, (Formula presented.) = 1 and (Formula presented.) = 0.002, (Formula presented.) = 0.003) and Surguja ((Formula presented.) = 1, (Formula presented.) = 0.99 and (Formula presented.) = 0.004, (Formula presented.) = 0.214) districts as compared to the other models, whereas for Bastar, ELNET ((Formula presented.) = 90, (Formula presented.) = 0.48) and LASSO ((Formula presented.) = 93, (Formula presented.) = 0.568) performed better. The performance of the ensemble model was better compared to the individual models. For Raipur and Surguja, the performance of all the ensemble methods was comparable, whereas for Bastar, random forest (RF) performed better, with R2 = 0.85 and 0.81 for calibration and validation, respectively, as compared to the GLM, cubist, and ELNET approach.",
        "DOI": "10.3390/su15032786",
        "affiliation_name": "ICAR - Central Coastal Agricultural Research Institute, Goa",
        "affiliation_city": "Old Goa",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Predictive Maintenance and Fault Monitoring Enabled by Machine Learning: Experimental Analysis of a TA-48 Multistage Centrifugal Plant Compressor",
        "paper_author": "Achouch M.",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "19",
        "cover_date": "2023-02-01",
        "Abstract": "In an increasingly competitive industrial world, the need to adapt to any change at any time has become a major necessity for every industry to remain competitive and survive in their environments. Industries are undergoing rapid and perpetual changes on several levels. Indeed, the latter requires companies to be more reactive and involved in their policies of continuous improvement in order to satisfy their customers and maximize the quantity and quality of production, while keeping the cost of production as low as possible. Reducing downtime is one of the major objectives of these industries of the future. This paper aimed to apply machine learning algorithms on a TA-48 multistage centrifugal compressor for failure prediction and remaining useful life (RUL), i.e., to reduce system downtime using a predictive maintenance (PdM) approach through the adoption of Industry 4.0 approaches. To achieve our goal, we followed the methodology of the predictive maintenance workflow that allows us to explore and process the data for the model training. Thus, a comparative study of different prediction algorithms was carried out to arrive at the final choice, which is based on the implementation of LSTM neural networks. In addition, its performance was improved as the data sets were fed and incremented. Finally, the model was deployed to allow operators to know the failure times of compressors and subsequently ensure minimum downtime rates by making decisions before failures occur.",
        "DOI": "10.3390/app13031790",
        "affiliation_name": "Université du Québec à Trois-Rivières",
        "affiliation_city": "Trois-Rivieres",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Machine Learning for Road Traffic Accident Improvement and Environmental Resource Management in the Transportation Sector",
        "paper_author": "Megnidio-Tchoukouegno M.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "28",
        "cover_date": "2023-02-01",
        "Abstract": "Despite the measures put in place in different countries, road traffic fatalities are still considered one of the leading causes of death worldwide. Thus, the reduction of traffic fatalities or accidents is one of the contributing factors to attaining sustainability goals. Different factors such as the geometric structure of the road, a non-signalized road network, the mechanical failure of vehicles, inexperienced drivers, a lack of communication skills, distraction and the visual or cognitive impairment of road users have led to this increase in traffic accidents. These factors can be categorized under four headings that are: human, road, vehicle factors and environmental road conditions. The advent of machine learning algorithms is of great importance in analysing the data, extracting hidden patterns, predicting the severity level of accidents and summarizing the information in a useful format. In this study, three machine learning algorithms for classification, such as Decision Tree, LightGBM and XGBoost, were used to model the accuracy of road traffic accidents in the UK for the year 2020 using their default and hyper-tuning parameters. The results show that the high performance of the Decision Tree algorithm with default parameters can predict traffic accident severity and provide reference to the critical variables that need to be monitored to reduce accidents on the roads. This study suggests that preventative strategies such as regular vehicle technical inspection, traffic policy strengthening and the redesign of vehicle protective equipment be implemented to reduce the severity of road accidents caused by vehicle characteristics.",
        "DOI": "10.3390/su15032014",
        "affiliation_name": "Durban University of Technology",
        "affiliation_city": "Durban",
        "affiliation_country": "South Africa"
    },
    {
        "paper_title": "A Systematic Study on Reinforcement Learning Based Applications",
        "paper_author": "Sivamayil K.",
        "publication": "Energies",
        "citied_by": "55",
        "cover_date": "2023-02-01",
        "Abstract": "We have analyzed 127 publications for this review paper, which discuss applications of Reinforcement Learning (RL) in marketing, robotics, gaming, automated cars, natural language processing (NLP), internet of things security, recommendation systems, finance, and energy management. The optimization of energy use is critical in today’s environment. We mainly focus on the RL application for energy management. Traditional rule-based systems have a set of predefined rules. As a result, they may become rigid and unable to adjust to changing situations or unforeseen events. RL can overcome these drawbacks. RL learns by exploring the environment randomly and based on experience, it continues to expand its knowledge. Many researchers are working on RL-based energy management systems (EMS). RL is utilized in energy applications such as optimizing energy use in smart buildings, hybrid automobiles, smart grids, and managing renewable energy resources. RL-based energy management in renewable energy contributes to achieving net zero carbon emissions and a sustainable environment. In the context of energy management technology, RL can be utilized to optimize the regulation of energy systems, such as building heating, ventilation, and air conditioning (HVAC) systems, to reduce energy consumption while maintaining a comfortable atmosphere. EMS can be accomplished by teaching an RL agent to make judgments based on sensor data, such as temperature and occupancy, to modify the HVAC system settings. RL has proven beneficial in lowering energy usage in buildings and is an active research area in smart buildings. RL can be used to optimize energy management in hybrid electric vehicles (HEVs) by learning an optimal control policy to maximize battery life and fuel efficiency. RL has acquired a remarkable position in robotics, automated cars, and gaming applications. The majority of security-related applications operate in a simulated environment. The RL-based recommender systems provide good suggestions accuracy and diversity. This article assists the novice in comprehending the foundations of reinforcement learning and its applications.",
        "DOI": "10.3390/en16031512",
        "affiliation_name": "Birla Institute of Technology and Science, Pilani – Dubai Campus",
        "affiliation_city": "Dubai",
        "affiliation_country": "United Arab Emirates"
    },
    {
        "paper_title": "Sizing of Small Hydropower Plants for Highly Variable Flows in Tropical Run-of-River Installations: A Case Study of the Sebeya River",
        "paper_author": "Gasore G.",
        "publication": "Energies",
        "citied_by": "0",
        "cover_date": "2023-02-01",
        "Abstract": "Rivers in tropical climates are characterized by highly variable flows which are becoming more variable due to climate change. In tropical conditions, most hydropower plants are designed as run-of-river plants with limited water storage. The aim of this study is the selection and sizing of a hydropower plant for highly variable flows, using the Sebeya River as a case study. As is often the case, flow data was incomplete, and the study also demonstrated the use of machine learning to predict the Sebeya flow rate for 2019. Stochastic modeling was used to estimate the energy generation for multiple turbine types and the levelized cost of energy for all configurations, capturing the uncertainty in many of the input parameters. River flow varies between 1.3 m3/s and 5.5 m3/s in a year; the minimum LCOE occurs at the knee in the flow exceedance curve of river flow rate, near 1.8 m3/s. The optimal LCOE for the Sebeya river is around 0.08 $/kwh with an uncertainty of −0.011/+0.009 $/kWh. Additionally, certain turbine types—notably propeller turbines—perform poorly in this type of highly variable flow. The method and findings can be used to guide future investments in small- to mid-sized hydropower plants in similar climatic conditions.",
        "DOI": "10.3390/en16031304",
        "affiliation_name": "University of Rwanda",
        "affiliation_city": "Butare",
        "affiliation_country": "Rwanda"
    },
    {
        "paper_title": "A Retrospective Analysis of National-Scale Agricultural Development in Saudi Arabia from 1990 to 2021",
        "paper_author": "Li T.",
        "publication": "Remote Sensing",
        "citied_by": "4",
        "cover_date": "2023-02-01",
        "Abstract": "Agricultural intensification has resulted in the depletion of groundwater resources in many regions of the world. A prime example is Saudi Arabia, which witnessed dramatic agricultural expansion since the 1970s. To explore the influence of policy interventions aimed to better manage water resources, accurate information on the changes in the number and acreage of center-pivot fields is required. To quantify these metrics, we apply a hybrid machine learning framework, consisting of Density-Based Spatial Clustering of Applications with Noise, Convolutional Neural Networks, and Spectral Clustering, to the annual maximum Normalized Differential Vegetation Index maps obtained from Landsat imagery collected between 1990 to 2021. When evaluated against more than 28,000 manually delineated fields, the approach demonstrated producer’s accuracies ranging from 83.7% to 94.8% and user’s accuracies ranging from 90.2% to 97.9%. The coefficient of determination ((Formula presented.)) between framework-delineated and manually delineated fields was higher than 0.97. Nationally, we found that most fields pre-dated 1990 (covering 8841 km (Formula presented.) in that year) and were primarily located within the central regions covering Hail, Qassim, Riyadh, and Wadi ad-Dawasir. A small decreasing trend in field acreage was observed for the period 1990–2010. However, by 2015, the acreage had increased to approximately 33,000 fields covering 9310 km (Formula presented.). While a maximum extent was achieved in 2016, recent decreases have seen levels return to pre-1990 levels. The gradual decrease between 1990 to 2010 was related to policy initiatives designed to phase-out wheat, while increases between 2010 to 2015 were linked to fodder crop expansion. There is evidence of an agricultural uptick starting in 2021, which is likely in response to global influences such as the COVID-19 pandemic or the conflict in Ukraine. Overall, this work offers the first detailed assessment of long-term agricultural development in Saudi Arabia, and provides important insights related to production metrics such as crop types, crop water consumption, and crop phenology and the overarching impacts of agricultural policy interventions.",
        "DOI": "10.3390/rs15030731",
        "affiliation_name": "King Abdullah University of Science and Technology",
        "affiliation_city": "Thuwal",
        "affiliation_country": "Saudi Arabia"
    },
    {
        "paper_title": "A Review of Biomass-to-Bioenergy Supply Chain Research Using Bibliometric Analysis and Visualization",
        "paper_author": "Helal M.A.",
        "publication": "Energies",
        "citied_by": "20",
        "cover_date": "2023-02-01",
        "Abstract": "Based on current trends and policies aimed at decarbonizing energy systems, the conversion of biomass to bioenergy has the potential to grow rapidly, but such growth depends on the development of efficient, sustainable, and competitive biomass supply chains. As a result, the biomass supply chain has stimulated the interest of a diverse group of researchers across academia, government, and industry, and there is a need to synthesize and categorize the rapidly expanding literature in this field. We conducted a literature review using advanced bibliometric analysis and visualization of 1711 peer-reviewed articles published from January 1992 to August 2022 with the aim of promoting impactful research in both growing and neglected areas of investigation. The results show that there are potential research gaps and opportunities in six critical areas: globalization of supply chain research; incorporation of uncertainty, stochasticity, and risk into supply chain models; investigation of multi-feedstock supply systems; strengthening supply chain resilience; application of inventory control methods; and broader use of machine learning and artificial intelligence in this field. By providing a holistic examination of how biomass-to-bioenergy supply chain research has grown and evolved over this period, our results and subsequent framework and recommendations can aid researchers in developing future studies and can guide stakeholder strategies to identify, diagnose, and address modern challenges that face the bioenergy industry.",
        "DOI": "10.3390/en16031187",
        "affiliation_name": "USDA ARS Rocky Mountain Research Station",
        "affiliation_city": "Missoula",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "A Deep Learning Approach for Exploring the Design Space for the Decarbonization of the Canadian Electricity System",
        "paper_author": "Jahangiri Z.",
        "publication": "Energies",
        "citied_by": "3",
        "cover_date": "2023-02-01",
        "Abstract": "Conventional energy system models have limitations in evaluating complex choices for transitioning to low-carbon energy systems and preventing catastrophic climate change. To address this challenge, we propose a model that allows for the exploration of a broader design space. We develop a supervised machine learning surrogate of a capacity expansion model, based on residual neural networks, that accurately approximates the model’s outputs while reducing the computation cost by five orders of magnitude. This increased efficiency enables the evaluation of the sensitivity of the outputs to the inputs, providing valuable insights into system development factors for the Canadian electricity system between 2030 and 2050. To facilitate the interpretation and communication of a large number of surrogate model results, we propose an easy-to-interpret method using an unsupervised machine learning technique. Our analysis identified key factors and quantified their relationships, showing that the carbon tax and wind energy capital cost are the most impactful factors on emissions in most provinces, and are 2 to 4 times more impactful than other factors on the development of wind and natural gas generations nationally. Our model generates insights that deepen our understanding of the most impactful decarbonization policy interventions.",
        "DOI": "10.3390/en16031352",
        "affiliation_name": "The University of British Columbia",
        "affiliation_city": "Vancouver",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Deep Reinforcement Learning for Charging Scheduling of Electric Vehicles Considering Distribution Network Voltage Stability",
        "paper_author": "Liu D.",
        "publication": "Sensors",
        "citied_by": "20",
        "cover_date": "2023-02-01",
        "Abstract": "The rapid development of electric vehicle (EV) technology and the consequent charging demand have brought challenges to the stable operation of distribution networks (DNs). The problem of the collaborative optimization of the charging scheduling of EVs and voltage control of the DN is intractable because the uncertainties of both EVs and the DN need to be considered. In this paper, we propose a deep reinforcement learning (DRL) approach to coordinate EV charging scheduling and distribution network voltage control. The DRL-based strategy contains two layers, the upper layer aims to reduce the operating costs of power generation of distributed generators and power consumption of EVs, and the lower layer controls the Volt/Var devices to maintain the voltage stability of the distribution network. We model the coordinate EV charging scheduling and voltage control problem in the distribution network as a Markov decision process (MDP). The model considers uncertainties of charging process caused by the charging behavior of EV users, as well as the uncertainty of uncontrollable load, system dynamic electricity price and renewable energy generation. Since the model has a dynamic state space and mixed action outputs, a framework of deep deterministic policy gradient (DDPG) is adopted to train the two-layer agent and the policy network is designed to output discrete and continuous control actions. Simulation and numerical results on the IEEE-33 bus test system demonstrate the effectiveness of the proposed method in collaborative EV charging scheduling and distribution network voltage stabilization.",
        "DOI": "10.3390/s23031618",
        "affiliation_name": "University of Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Safe Decision Controller for Autonomous DrivingBased on Deep Reinforcement Learning inNondeterministic Environment",
        "paper_author": "Chen H.",
        "publication": "Sensors",
        "citied_by": "17",
        "cover_date": "2023-02-01",
        "Abstract": "Autonomous driving systems are crucial complicated cyber–physical systems that combine physical environment awareness with cognitive computing. Deep reinforcement learning is currently commonly used in the decision-making of such systems. However, black-box-based deep reinforcement learning systems do not guarantee system safety and the interpretability of the reward-function settings in the face of complex environments and the influence of uncontrolled uncertainties. Therefore, a formal security reinforcement learning method is proposed. First, we propose an environmental modeling approach based on the influence of nondeterministic environmental factors, which enables the precise quantification of environmental issues. Second, we use the environment model to formalize the reward machine’s structure, which is used to guide the reward-function setting in reinforcement learning. Third, we generate a control barrier function to ensure a safer state behavior policy for reinforcement learning. Finally, we verify the method’s effectiveness in intelligent driving using overtaking and lane-changing scenarios.",
        "DOI": "10.3390/s23031198",
        "affiliation_name": "Hainan University",
        "affiliation_city": "Haikou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "An Improved DDPG and Its Application in Spacecraft Fault Knowledge Graph",
        "paper_author": "Xing X.",
        "publication": "Sensors",
        "citied_by": "2",
        "cover_date": "2023-02-01",
        "Abstract": "We construct a spacecraft performance-fault relationship graph of the control system, which can help space robots locate and repair spacecraft faults quickly. In order to improve the performance-fault relationship graph, we improve the Deep Deterministic Policy Gradient (DDPG) algorithm, and propose a relationship prediction method that combines representation learning reasoning with deep reinforcement learning reasoning. We take the spacecraft performance-fault relationship graph as the agent learning environment and adopt reinforcement learning to realize the optimal interaction between the agent and the environment. Meanwhile, our model uses a deep neural network to construct a complex value function and strategy function, which makes the agent have excellent perceptual decision-making ability and accurate value judgment ability. We evaluate our model on a performance-fault relationship graph of the control system. The experimental results show that our model has high prediction speed and accuracy, which can completely infer the optimal relationship path between entities to complete the spacecraft performance-fault relationship graph.",
        "DOI": "10.3390/s23031223",
        "affiliation_name": "Beijing Institute of Control Engineering",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Prediction of the Tropospheric NO<inf>2</inf> Column Concentration and Distribution Using the Time Sequence-Based versus Influencing Factor-Based Random Forest Regression Model",
        "paper_author": "Geng T.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "1",
        "cover_date": "2023-02-01",
        "Abstract": "The prediction of air pollutants has always been an issue of great concern to the whole of society. In recent years, the prediction and simulation of air pollutants via machine learning have been widely used. In this study, we collected meteorological data and tropospheric NO2 column concentration data in Beijing, China, between 2012 and 2020, and compared the two methods of time sequence-based and influencing factor-based random forest regression in predicting the tropospheric NO2 column concentration. The results showed that prediction of the tropospheric NO2 column concentration using random forest regression was affected by the changes of human activities, especially emergency events and policy variations. The advantage of time sequence analysis lies in its ability to calculate the distribution of air pollutants with a long-time scale of prediction, but it may produce large errors in numerical value. The advantage of influencing factor prediction lies in its high precision and that it can identify the specific impact of each influencing factor on the NO2 column concentration, but it needs more data and work quantities before it can make a prediction about the future.",
        "DOI": "10.3390/su15032748",
        "affiliation_name": "Northwest Normal University China",
        "affiliation_city": "Lanzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Multi-Step Ahead Forecasting of the Energy Consumed by the Residential and Commercial Sectors in the United States Based on a Hybrid CNN-BiLSTM Model",
        "paper_author": "Chen Y.",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "8",
        "cover_date": "2023-02-01",
        "Abstract": "COVID-19 has continuously influenced energy security and caused an enormous impact on human life and social activities due to the stay-at-home orders. After the Omicron wave, the economy and the energy system are gradually recovering, but uncertainty remains due to the virus mutations that could arise. Accurate forecasting of the energy consumed by the residential and commercial sectors is challenging for efficient emergency management and policy-making. Affected by geographical location and long-term evolution, the time series of the energy consumed by the residential and commercial sectors has prominent temporal and spatial characteristics. A hybrid model (CNN-BiLSTM) based on a convolution neural network (CNN) and bidirectional long short-term memory (BiLSTM) is proposed to extract the time series features, where the spatial features of the time series are captured by the CNN layer, and the temporal features are extracted by the BiLSTM layer. Then, the recursive multi-step ahead forecasting strategy is designed for multi-step ahead forecasting, and the grid search is employed to tune the model hyperparameters. Four cases of 24-step ahead forecasting of the energy consumed by the residential and commercial sectors in the United States are given to evaluate the performance of the proposed model, in comparison with 4 deep learning models and 6 popular machine learning models based on 12 evaluation metrics. Results show that CNN-BiLSTM outperforms all other models in four cases, with MAPEs ranging from 4.0034% to 5.4774%, improved from 0.1252% to 49.1410%, compared with other models, which is also about 5 times lower than that of the CNN and 5.9559% lower than the BiLSTM on average. It is evident that the proposed CNN-BiLSTM has improved the prediction accuracy of the CNN and BiLSTM and has great potential in forecasting the energy consumed by the residential and commercial sectors.",
        "DOI": "10.3390/su15031895",
        "affiliation_name": "University of Electronic Science and Technology of China",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Public Perceptions on the Policy of Electronic Cigarettes as Medical Products on Twitter",
        "paper_author": "Lou X.",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "5",
        "cover_date": "2023-02-01",
        "Abstract": "Starting from 1 October 2021, Australia requires a prescription for purchasing nicotine vaping products. On 29 October 2021, the UK provided a guideline to treat e-cigarettes as medical products. This study aims to understand public perceptions of the prescription policy in Australia and the UK on Twitter. Tweets related to e-cigarettes from 20 September 2021 to 31 December 2021 were collected through Twitter streaming API. We adopted both a human and machine learning model to identify a total of 1795 tweets from the UK and Australia related to the prescription policy. We classified them into pro-policy, anti-policy, and neutral-to-policy groups, and further characterized tweets into different topics. Compared to Australia, the proportion of pro-policy tweets in the UK was significantly higher (19.43% vs. 10.92%, p < 0.001), while the proportion of anti-policy tweets was significantly lower (43.4% vs. 50.09%, p = 0.003). The main topics for different attitudes towards the prescription policy between the two countries showed some significant differences, for example, “help quit smoking” in the UK and “health effect of e-cigarettes” in Australia for the positive attitude, “economic effect” in the UK and “preventing smoking cessation” in Australia for the negative attitude, which reflected different public concerns. The findings might provide valuable guidance for other countries to implement a similar policy in the future.",
        "DOI": "10.3390/ijerph20032618",
        "affiliation_name": "Hajim School of Engineering and Applied Sciences",
        "affiliation_city": "Rochester",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "A year of pandemic: Levels, changes and validity of well-being data from Twitter. Evidence from ten countries",
        "paper_author": "Sarracino F.",
        "publication": "PLoS ONE",
        "citied_by": "7",
        "cover_date": "2023-02-01",
        "Abstract": "We use daily happiness scores (Gross National Happiness (GNH)) to illustrate how happiness changed throughout 2020 in ten countries across Europe and the Southern hemisphere. More frequently and regularly available than survey data, the GNH reveals how happiness sharply declined at the onset of the pandemic and lockdown, quickly recovered, and then trended downward throughout much of the year in Europe. GNH is derived by applying sentiment and emotion analysis–based on Natural Language Processing using machine learning algorithms–to Twitter posts (tweets). Using a similar approach, we generate another 11 variables: eight emotions and three new context-specific variables, in particular: trust in national institutions, sadness in relation to loneliness, and fear concerning the economy. Given the novelty of the dataset, we use multiple methods to assess validity. We also assess the correlates of GNH. The results indicate that GNH is negatively correlated with new COVID-19 cases, containment policies, and disgust and positively correlated with staying at home, surprise, and generalised trust. Altogether the analyses indicate tools based on Big Data, such as the GNH, offer relevant data that often fill information gaps and can valuably supplement traditional tools. In this case, the GNH results suggest that both the severity of the pandemic and containment policies negatively correlated with happiness.",
        "DOI": "10.1371/journal.pone.0275028",
        "affiliation_name": "College of Business and Economics",
        "affiliation_city": "Johannesburg",
        "affiliation_country": "South Africa"
    },
    {
        "paper_title": "Imitation Learning-Based Performance-Power Trade-Off Uncore Frequency Scaling Policy for Multicore System",
        "paper_author": "Xiao B.",
        "publication": "Sensors",
        "citied_by": "1",
        "cover_date": "2023-02-01",
        "Abstract": "As the importance of uncore components, such as shared cache slices and memory controllers, increases in processor architecture, the percentage of uncore power consumption in the overall power consumption of multicore processors rises significantly. To maximize the power efficiency of a multicore processor system, we investigate the uncore frequency scaling (UFS) policy and propose a novel imitation learning-based uncore frequency control policy. This policy performs online learning based on the DAgger algorithm and converts the annotation cost of online aggregation data into fine-tuning of the expert model. This design optimizes the online learning efficiency and improves the generality of the UFS policy on unseen loads. On the other hand, we shift our policy optimization target to Performance Per Watt (PPW), i.e., the power efficiency of the processor, to avoid saving a percentage of power while losing a larger percentage of performance. The experimental results show that our proposed policy outperforms the current advanced UFS policy in the benchmark test sequence of SPEC CPU2017. Our policy has a maximum improvement of about 10% relative to the performance-first policies. In the unseen processor load, the tuning decision made by our policy after collecting 50 aggregation data can maintain the processor stably near the optimal power efficiency state.",
        "DOI": "10.3390/s23031449",
        "affiliation_name": "Wuhan University",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A Study on the Impact of Integrating Reinforcement Learning for Channel Prediction and Power Allocation Scheme in MISO-NOMA System",
        "paper_author": "Gaballa M.",
        "publication": "Sensors",
        "citied_by": "4",
        "cover_date": "2023-02-01",
        "Abstract": "In this study, the influence of adopting Reinforcement Learning (RL) to predict the channel parameters for user devices in a Power Domain Multi-Input Single-Output Non-Orthogonal Multiple Access (MISO-NOMA) system is inspected. In the channel prediction-based RL approach, the Q-learning algorithm is developed and incorporated into the NOMA system so that the developed Q-model can be employed to predict the channel coefficients for every user device. The purpose of adopting the developed Q-learning procedure is to maximize the received downlink sum-rate and decrease the estimation loss. To satisfy this aim, the developed Q-algorithm is initialized using different channel statistics and then the algorithm is updated based on the interaction with the environment in order to approximate the channel coefficients for each device. The predicted parameters are utilized at the receiver side to recover the desired data. Furthermore, based on maximizing the sum-rate of the examined user devices, the power factors for each user can be deduced analytically to allocate the optimal power factor for every user device in the system. In addition, this work inspects how the channel prediction based on the developed Q-learning model, and the power allocation policy, can both be incorporated for the purpose of multiuser recognition in the examined MISO-NOMA system. Simulation results, based on several performance metrics, have demonstrated that the developed Q-learning algorithm can be a competitive algorithm for channel estimation when compared to different benchmark schemes such as deep learning-based long short-term memory (LSTM), RL based actor-critic algorithm, RL based state-action-reward-state-action (SARSA) algorithm, and standard channel estimation scheme based on minimum mean square error procedure.",
        "DOI": "10.3390/s23031383",
        "affiliation_name": "Ahlia University",
        "affiliation_city": "Manama",
        "affiliation_country": "Bahrain"
    },
    {
        "paper_title": "Upfront Surgery versus Neoadjuvant Perioperative Chemotherapy for Resectable Colorectal Liver Metastases: A Machine-Learning Decision Tree to Identify the Best Potential Candidates under a Parenchyma-Sparing Policy",
        "paper_author": "Famularo S.",
        "publication": "Cancers",
        "citied_by": "9",
        "cover_date": "2023-02-01",
        "Abstract": "Addressing patients to neoadjuvant systemic chemotherapy followed by surgery rather than surgical resection upfront is controversial in the case of resectable colorectal –liver metastases (CLM). The aim of this study was to develop a machine-learning model to identify the best potential candidates for upfront surgery (UPS) versus neoadjuvant perioperative chemotherapy followed by surgery (NEOS). Patients at first liver resection for CLM were consecutively enrolled and collected into two groups, regardless of whether they had UPS or NEOS. An inverse –probability weighting (IPW) was performed to weight baseline differences; survival analyses; and risk predictions were estimated. A mortality risk model was built by Random-Forest (RF) to assess the best –potential treatment (BPT) for each patient. The characteristics of BPT-upfront and BPT-neoadjuvant candidates were automatically identified after developing a classification –and –regression tree (CART). A total of 448 patients were enrolled between 2008 and 2020: 95 UPS and 353 NEOS. After IPW, two balanced pseudo-populations were obtained: UPS = 432 and NEOS = 440. Neoadjuvant therapy did not significantly affect the risk of mortality (HR 1.44, 95% CI: 0.95–2.17, p = 0.07). A mortality prediction model was fitted by RF. The BPT was NEOS for 364 patients and UPS for 84. At CART, planning R1vasc surgery was the main factor determining the best candidates for NEOS and UPS, followed by primitive tumor localization, number of metastases, sex, and pre-operative CEA. Based on these results, a decision three was developed. The proposed treatment algorithm allows for better allocation according to the patient’s tailored risk of mortality.",
        "DOI": "10.3390/cancers15030613",
        "affiliation_name": "Humanitas University",
        "affiliation_city": "Pieve Emanuele",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Literature Review on Health Emigration in Rare Diseases—A Machine Learning Perspective",
        "paper_author": "Skweres-Kuchta M.",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "1",
        "cover_date": "2023-02-01",
        "Abstract": "The article deals with one of the effects of health inequalities and gaps in access to treatments for rare diseases, namely health-driven emigration. The purpose of the paper is to systematize knowledge about the phenomenon of health emigration observed among families affected by rare diseases, for which reimbursed treatment is available, but only in selected countries. The topic proved to be niche; the issue of “health emigration in rare diseases” is an area for exploration. Therefore, the further analysis used text mining and machine learning methods based on a database selected based on keywords related to this issue. The results made it possible to systematize the guesses made by researchers in management and economic fields, to identify the most common keywords and thematic clusters around the perspective of the patient, drug manufacturer and treatment reimbursement decision-maker, and the perspective integrating all the others. Since the topic of health emigration was not directly addressed in the selected sources, the authors attempted to define the related concepts and discussed the importance of this phenomenon in managing the support system in rare diseases. Thus, they indicated directions for further research in this area.",
        "DOI": "10.3390/ijerph20032483",
        "affiliation_name": "Uniwersytet Szczecinski",
        "affiliation_city": "Szczecin",
        "affiliation_country": "Poland"
    },
    {
        "paper_title": "Analysis of Corn Yield Prediction Potential at Various Growth Phases Using a Process-Based Model and Deep Learning",
        "paper_author": "Ren Y.",
        "publication": "Plants",
        "citied_by": "22",
        "cover_date": "2023-02-01",
        "Abstract": "Early and accurate prediction of grain yield is of great significance for ensuring food security and formulating food policy. The exploration of key growth phases and features is beneficial to improving the efficiency and accuracy of yield prediction. In this study, a hybrid approach using the WOFOST model and deep learning was developed to forecast corn yield, which analysed yield prediction potential at different growth phases and features. The World Food Studies (WOFOST) model was used to build a comprehensive simulated dataset by inputting meteorological, soil, crop and management data. Different feature combinations at various growth phases were designed to forecast yield using machine learning and deep learning methods. The results show that the key features of corn’s vegetative growth stage and reproductive growth stage were growth state features and water-related features, respectively. With the continuous advancement of the crop growth stage, the ability to predict yield continued to improve. Especially after entering the reproductive growth stage, corn kernels begin to form, and the yield prediction performance is significantly improved. The performance of the optimal yield prediction model in flowering (R2 = 0.53, RMSE = 554.84 kg/ha, MRE = 8.27%), in milk maturity (R2 = 0.89, RMSE = 268.76 kg/ha, MRE = 4.01%), and in maturity (R2 = 0.98, RMSE = 102.65 kg/ha, MRE = 1.53%) were given. Thus, our method improves the accuracy of yield prediction, and provides reliable analysis results for predicting yield at various growth phases, which is helpful for farmers and governments in agricultural decision making. This can also be applied to yield prediction for other crops, which is of great value to guide agricultural production.",
        "DOI": "10.3390/plants12030446",
        "affiliation_name": "Aerospace Information Research Institute",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Mitigating Catastrophic Forgetting with Complementary Layered Learning",
        "paper_author": "Mondesire S.",
        "publication": "Electronics (Switzerland)",
        "citied_by": "3",
        "cover_date": "2023-02-01",
        "Abstract": "Catastrophic forgetting is a stability–plasticity imbalance that causes a machine learner to lose previously gained knowledge that is critical for performing a task. The imbalance occurs in transfer learning, negatively affecting the learner’s performance, particularly in neural networks and layered learning. This work proposes a complementary learning technique that introduces long- and short-term memory to layered learning to reduce the negative effects of catastrophic forgetting. In particular, this work proposes the dual memory system in the non-neural network approaches of evolutionary computation and Q-learning instances of layered learning because these techniques are used to develop decision-making capabilities for physical robots. Experiments evaluate the new learning augmentation in a multi-agent system simulation, where autonomous unmanned aerial vehicles learn to collaborate and maneuver to survey an area effectively. Through these direct-policy and value-based learning experiments, the proposed complementary layered learning is demonstrated to significantly improve task performance over standard layered learning, successfully balancing stability and plasticity.",
        "DOI": "10.3390/electronics12030706",
        "affiliation_name": "Winthrop University",
        "affiliation_city": "Rock Hill",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Uncertain-CAM: Uncertainty-Based Ensemble Machine Voting for Improved COVID-19 CXR Classification and Explainability",
        "paper_author": "Aldhahi W.",
        "publication": "Diagnostics",
        "citied_by": "10",
        "cover_date": "2023-02-01",
        "Abstract": "The ongoing coronavirus disease 2019 (COVID-19) pandemic has had a significant impact on patients and healthcare systems across the world. Distinguishing non-COVID-19 patients from COVID-19 patients at the lowest possible cost and in the earliest stages of the disease is a major issue. Additionally, the implementation of explainable deep learning decisions is another issue, especially in critical fields such as medicine. The study presents a method to train deep learning models and apply an uncertainty-based ensemble voting policy to achieve 99% accuracy in classifying COVID-19 chest X-rays from normal and pneumonia-related infections. We further present a training scheme that integrates the cyclic cosine annealing approach with cross-validation and uncertainty quantification that is measured using prediction interval coverage probability (PICP) as final ensemble voting weights. We also propose the Uncertain-CAM technique, which improves deep learning explainability and provides a more reliable COVID-19 classification system. We introduce a new image processing technique to measure the explainability based on ground-truth, and we compared it with the widely adopted Grad-CAM method.",
        "DOI": "10.3390/diagnostics13030441",
        "affiliation_name": "Korea University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "Living with Floods Using State-of-the-Art and Geospatial Techniques: Flood Mitigation Alternatives, Management Measures, and Policy Recommendations",
        "paper_author": "Chakrabortty R.",
        "publication": "Water (Switzerland)",
        "citied_by": "23",
        "cover_date": "2023-02-01",
        "Abstract": "Flood, a distinctive natural calamity, has occurred more frequently in the last few decades all over the world, which is often an unexpected and inevitable natural hazard, but the losses and damages can be managed and controlled by adopting effective measures. In recent times, flood hazard susceptibility mapping has become a prime concern in minimizing the worst impact of this global threat; but the nonlinear relationship between several flood causative factors and the dynamicity of risk levels makes it complicated and confronted with substantial challenges to reliable assessment. Therefore, we have considered SVM, RF, and ANN—three distinctive ML algorithms in the GIS platform—to delineate the flood hazard risk zones of the subtropical Kangsabati river basin, West Bengal, India; which experienced frequent flood events because of intense rainfall throughout the monsoon season. In our study, all adopted ML algorithms are more efficient in solving all the non-linear problems in flood hazard risk assessment; multi-collinearity analysis and Pearson’s correlation coefficient techniques have been used to identify the collinearity issues among all fifteen adopted flood causative factors. In this research, the predicted results are evaluated through six prominent and reliable statistical (“AUC-ROC, specificity, sensitivity, PPV, NPV, F-score”) and one graphical (Taylor diagram) technique and shows that ANN is the most reliable modeling approach followed by RF and SVM models. The values of AUC in the ANN model for the training and validation datasets are 0.901 and 0.891, respectively. The derived result states that about 7.54% and 10.41% of areas accordingly lie under the high and extremely high flood danger risk zones. Thus, this study can help the decision-makers in constructing the proper strategy at the regional and national levels to mitigate the flood hazard in a particular region. This type of information may be helpful to the various authorities to implement this outcome in various spheres of decision making. Apart from this, future researchers are also able to conduct their research byconsidering this methodology in flood susceptibility assessment.",
        "DOI": "10.3390/w15030558",
        "affiliation_name": "The University of Burdwan",
        "affiliation_city": "Bardhaman",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Novel Ensemble Machine Learning Modeling Approach for Groundwater Potential Mapping in Parbhani District of Maharashtra, India",
        "paper_author": "Masroor M.",
        "publication": "Water (Switzerland)",
        "citied_by": "26",
        "cover_date": "2023-02-01",
        "Abstract": "Groundwater is an essential source of water especially in arid and semi-arid regions of the world. The demand for water due to exponential increase in population has created stresses on available groundwater resources. Further, climate change has affected the quantity of water globally. Many parts of Indian cities are experiencing water scarcity. Thus, assessment of groundwater potential is necessary for sustainable utilization and management of water resources. We utilized a novel ensemble approach using artificial neural network multi-layer perceptron (ANN-MLP), random forest (RF), M5 prime (M5P) and support vector machine for regression (SMOReg) models for assessing groundwater potential in the Parbhani district of Maharashtra in India. Ten site-specific influencing factors, elevation, slope, aspect, drainage density, rainfall, water table depth, lineament density, land use land cover, geomorphology, and soil types, were integrated for preparation of groundwater potential zones. The results revealed that the largest area of the district was found under moderate category GWP zone followed by poor, good, very good and very poor. Spatial distribution of GWP zones showed that Poor GWPZs are spread over north, central and southern parts of the district. Very poor GWPZs are mostly found in the north-western and southern parts of the district. The study calls for policy implications to conserve and manage groundwater in these parts. The ensembled model has proved to be effective for assessment of GWP zones. The outcome of the study may help stakeholders efficiently utilize groundwater and devise suitable strategies for its management. Other geographical regions may find the methodology adopted in this study effective for groundwater potential assessment.",
        "DOI": "10.3390/w15030419",
        "affiliation_name": "Institute for Global Environmental Strategies",
        "affiliation_city": "Hayama",
        "affiliation_country": "Japan"
    },
    {
        "paper_title": "Forecasting the United State Dollar(USD)/ Bangladeshi Taka (BDT) exchange rate with deep learning models: Inclusion of macroeconomic factors influencing the currency exchange rates",
        "paper_author": "Biswas A.",
        "publication": "PLoS ONE",
        "citied_by": "7",
        "cover_date": "2023-02-01",
        "Abstract": "Forecasting a currency exchange rate is one of the most challenging tasks nowadays. Due to government monetary policy and some uncertain factors, such as political stability, it becomes difficult to correctly forecast the currency exchange rate. Previously, many investigations have been done to forecast the exchange rate of the United State Dollar(USD)/Bangladeshi Taka(BDT) using statistical time series models, machine learning models, and neural network models. But none of the previous methods considered the underlying macroeconomic factors of the two countries, such as GDP, import/export, government revenue, etc., for forecasting the USD/BDT exchange rate. We have included various time-sensitive macroeconomic features directly impacting the USD/BDT exchange rate to address this issue. These features will create a new dimension for researchers to predict and forecast the USD/BDT exchange rate. We have used various types of models for predicting and forecasting the USD/BDT exchange rate and found that Among all our models, Time Distributed MLP provides the best performance with an RMSE of 0.1984. Finally, we have proposed a pipeline for forecasting the USD/BDT exchange rate, which reduced the RMSE of Time Distributed MLP to 0.1900 and has proven effective in reducing the error of all our models.",
        "DOI": "10.1371/journal.pone.0279602",
        "affiliation_name": "North South University",
        "affiliation_city": "Dhaka",
        "affiliation_country": "Bangladesh"
    },
    {
        "paper_title": "SENTIMENT ANALYSIS OF THE NATIONAL COVID-19 VACCINATION PROGRAM ON TWITTER USING THE BIDIRECTIONAL ENCODER REPRESENTATION FROM TRANSFORMER",
        "paper_author": "Ingkafi D.A.",
        "publication": "ICIC Express Letters",
        "citied_by": "0",
        "cover_date": "2023-02-01",
        "Abstract": "In Indonesia, the implementation of the national COVID-19 (Coronavirus disease of 2019) vaccination programmes has received criticism from various strata of society, especially through social media platforms such as Twitter. Therefore, Twitter can be used as a data source to analyze Indonesian public sentiment regarding the vaccination programme. Various classical machine learning methods exist for sentiment analysis, but these methods require complex feature engineering and do not focus on the importance of word order in a sentence. In this study, a deep learning model, bidirectional encoder representation from transformer (BERT), is used to overcome these problems by conducting experiments to determine the best dataset after pre-processing, the best hyper-parameter, and the best pre-trained model for BERT. The data used in this study were Indonesian Twitter data with a total of 3000 tweets. Our results demonstrate that BERT is suitable for performing sentiment analysis. In our experiments, BERT obtained better results than classical machine learning methods, with a precision of 86.2%, recall of 86%, f1-score of 86%, and accuracy of 86%. The results of the sentiment analysis performed in this study can be considered by the government in formulating policies related to the implementation of vaccination programmes.",
        "DOI": "10.24507/icicel.17.02.201",
        "affiliation_name": "Universitas Diponegoro",
        "affiliation_city": "Semarang",
        "affiliation_country": "Indonesia"
    },
    {
        "paper_title": "Autonomous Tissue Manipulation via Surgical Robot Using Deep Reinforcement Learning and Evolutionary Algorithm",
        "paper_author": "Shahkoo A.A.",
        "publication": "IEEE Transactions on Medical Robotics and Bionics",
        "citied_by": "6",
        "cover_date": "2023-02-01",
        "Abstract": "Robotic surgery makes use of autonomous robots that can perform some surgical tasks on their own. Surgical robots performed well in conjunction with machine learning, particularly reinforcement learning (RL), allowing them to be used in complex environments, such as cutting a pre-determined pattern on soft tissue with surgical scissors and gripper. There is no doubt that soft tissue is deformable, so using a tensioning policy can determine appropriate tension direction from the pinch point at any time to have an accurate cut in the pre-determined trajectory. In this study, we used the deep reinforcement learning (DRL) approach to find an optimal tensioning policy for cutting soft tissues. In addition, we used an evolutionary algorithm with the operators appropriate to the problem and the learned tensioning policy to find the sequence of tensioning actions. The objective of this study is to determine the optimal tensioning policy and the best tensioning action sequence. The experimental results show that using the learned policy results in smaller damage and error and lead to the highest scores compared to the previous studies.",
        "DOI": "10.1109/TMRB.2023.3237772",
        "affiliation_name": "Shahid Beheshti University",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "Sources of Economic Policy Uncertainty in the euro area",
        "paper_author": "Azqueta-Gavaldón A.",
        "publication": "European Economic Review",
        "citied_by": "16",
        "cover_date": "2023-02-01",
        "Abstract": "We create economic policy uncertainty (EPU) indicators for the four largest euro area countries by applying two unsupervised machine learning algorithms to news articles. The procedure allows to uncover components of EPU endogenously for the four European languages. The uncertainty indices computed from January 2000 to May 2019 capture episodes of regulatory change, trade tensions and financial stress. In an evaluation exercise, we use a structural vector autoregression model to study the effects of uncertainty on investment and on private consumption. We document considerable effects for the political and domestic regulation uncertainty components on investment, while the other types show heterogeneous effects across countries. For instance, trade uncertainty influences Germany's investment more than its counterparts. Moreover, we observe strong negative effects of uncertainty on consumption for countries such as Italy (political) and Spain (fiscal, political and domestic regulation).",
        "DOI": "10.1016/j.euroecorev.2023.104373",
        "affiliation_name": "European Commission Joint Research Centre",
        "affiliation_city": "Brussels",
        "affiliation_country": "Belgium"
    },
    {
        "paper_title": "Learning to Break Rocks with Deep Reinforcement Learning",
        "paper_author": "Samtani P.",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "6",
        "cover_date": "2023-02-01",
        "Abstract": "This work proposes a scheme for learning how to break rocks with an impact hammer. The problem is formulated as a Partially Observable Markov's Decision Process, and then solved through deep reinforcement learning. We propose a simple formulation, requiring only a basic sensorization of the hammer's manipulator, and involving just two discrete actions. We use Dueling Double Deep-Q Networks to parameterize the policy, and wield it with an auxiliary output. The proposed auxiliary task is also trained in simulation, and allows deciding when to stop the operation by detecting the absence of a rock from the observed joints' movement. The resulting policy is tested in a real world experimental environment, using a Bobcat E10 mini-excavator, and various rock types. The results show that a good performance can be obtained in a safe, and robust manner. A video showing some of the obtained results is available in https://youtu.be/tMqDJjK6zPo.",
        "DOI": "10.1109/LRA.2023.3236562",
        "affiliation_name": "Centro Avanzado de Tecnología para la Minería",
        "affiliation_city": "Santiago",
        "affiliation_country": "Chile"
    },
    {
        "paper_title": "Higher Education and the Black-White Earnings Gap",
        "paper_author": "Zhou X.",
        "publication": "American Sociological Review",
        "citied_by": "9",
        "cover_date": "2023-02-01",
        "Abstract": "How does higher education shape the Black-White earnings gap? It may help close the gap if Black youth benefit more from attending and completing college than do White youth. On the other hand, Black college-goers are less likely to complete college relative to White students, and this disparity in degree completion helps reproduce racial inequality. In this study, we use a novel causal decomposition and a debiased machine learning method to isolate, quantify, and explain the equalizing and stratifying roles of college. Analyzing data from the NLSY97, we find that a bachelor’s degree has a strong equalizing effect on earnings among men (albeit not among women); yet, at the population level, this equalizing effect is partly offset by unequal likelihoods of bachelor’s completion between Black and White students. Moreover, a bachelor’s degree narrows the male Black-White earnings gap not by reducing the influence of class background and pre-college academic ability, but by lessening the “unexplained” penalty of being Black in the labor market. To illuminate the policy implications of our findings, we estimate counterfactual earnings gaps under a series of stylized educational interventions. We find that interventions that both boost rates of college attendance and bachelor’s completion and close racial disparities in these transitions can substantially reduce the Black-White earnings gap.",
        "DOI": "10.1177/00031224221141887",
        "affiliation_name": "University of Oxford",
        "affiliation_city": "Oxford",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Pl@ntNet Crops: merging citizen science observations and structured survey data to improve crop recognition for agri-food-environment applications",
        "paper_author": "van der Velde M.",
        "publication": "Environmental Research Letters",
        "citied_by": "4",
        "cover_date": "2023-02-01",
        "Abstract": "We present a new application to recognize 218 species of cultivated crops on geo-tagged photos, ‘Pl@ntNet Crops’. The application and underlying algorithms are developed using more than 750k photos voluntarily collected by Pl@ntNet users. The app is then enriched by data and photos coming from the European Union’s (EU) Land Use and Coverage Area frame Survey (LUCAS). During five tri-annual LUCAS campaigns from 2006 to 2018, 242 476 close-up ‘cover’ photos of crops were collected. The survey protocol for these photos specified that ‘the picture should be taken at a close distance, so that the structure of leaves can be clearly seen, as well as flowers or fruits’. This unique labelled data provides an opportunity to further generalize the Pl@ntNet computer vision algorithms to recognize crops and enlarge their geographic representivity across the EU. To include LUCAS cover photos, we semantically match Pl@ntNet species and LUCAS legends, predict the species on LUCAS cover photos with the existing Pl@ntNet algorithm, and consider the accuracy of the classification and the number of species enriched by the photos. By setting a threshold of > 0.5 on the Pl@ntNet prediction probabilities, 70 170 LUCAS photos representing 101 species classified with an accuracy of 0.9 were added to the ‘Crops’ app. The thematic accuracy of the legacy LUCAS data was improved by distinguishing 218 species, opposed to the original 36 LUCAS levels. Official and publicly financed LUCAS datastreams can now be improved because of Pl@ntNet citizen science, photo collection, and deep learning model development. Further use of the app and policy-relevant workflows in the agri-food-environment domain are discussed.",
        "DOI": "10.1088/1748-9326/acadf3",
        "affiliation_name": "European Commission Joint Research Centre",
        "affiliation_city": "Brussels",
        "affiliation_country": "Belgium"
    },
    {
        "paper_title": "Whakairo: A values-led approach to psychiatric epidemiology",
        "paper_author": "Lockett H.",
        "publication": "Australian and New Zealand Journal of Psychiatry",
        "citied_by": "1",
        "cover_date": "2023-02-01",
        "Abstract": "NA",
        "DOI": "10.1177/00048674231151778",
        "affiliation_name": "University of Otago",
        "affiliation_city": "Dunedin",
        "affiliation_country": "New Zealand"
    },
    {
        "paper_title": "From Heroes to Scoundrels: Exploring the effects of online campaigns celebrating frontline workers on COVID-19 outcomes",
        "paper_author": "Polyzos E.",
        "publication": "Technology in Society",
        "citied_by": "5",
        "cover_date": "2023-02-01",
        "Abstract": "This paper examines the effects of online campaigns celebrating frontline workers on COVID-19 outcomes regarding new cases, deaths, and vaccinations, using the United Kingdom as a case study. We implement text and sentiment analysis on Twitter data and feed the result into random regression forests and cointegration analysis. Our combined machine learning and econometric approach shows very weak effects of both the volume and the sentiment of Twitter discussions on new cases, deaths, and vaccinations. On the other hand, established relationships (such as between stringency measures and cases/deaths and between vaccinations and deaths) are confirmed. On the contrary, we find adverse lagged effects from negative sentiment to vaccinations and from new cases to negative sentiment posts. As we assess the knowledge acquired from the COVID-19 crisis, our findings can be used by policy makers, particularly in public health, and prepare for the next pandemic.",
        "DOI": "10.1016/j.techsoc.2023.102198",
        "affiliation_name": "Zayed University",
        "affiliation_city": "Dubai",
        "affiliation_country": "United Arab Emirates"
    },
    {
        "paper_title": "Leveraging Clinical Informatics and Data Science to Improve Care and Facilitate Research in Pediatric Acute Respiratory Distress Syndrome: From the Second Pediatric Acute Lung Injury Consensus Conference",
        "paper_author": "Sanchez-Pinto L.N.",
        "publication": "Pediatric Critical Care Medicine",
        "citied_by": "6",
        "cover_date": "2023-02-01",
        "Abstract": "OBJECTIVES: The use of electronic algorithms, clinical decision support systems, and other clinical informatics interventions is increasing in critical care. Pediatric acute respiratory distress syndrome (PARDS) is a complex, dynamic condition associated with large amounts of clinical data and frequent decisions at the bedside. Novel data-driven technologies that can help screen, prompt, and support clinician decision-making could have a significant impact on patient outcomes. We sought to identify and summarize relevant evidence related to clinical informatics interventions in both PARDS and adult respiratory distress syndrome (ARDS), for the second Pediatric Acute Lung Injury Consensus Conference. DATA SOURCES: MEDLINE (Ovid), Embase (Elsevier), and CINAHL Complete (EBSCOhost). STUDY SELECTION: We included studies of pediatric or adult critically ill patients with or at risk of ARDS that examined automated screening tools, electronic algorithms, or clinical decision support systems. DATA EXTRACTION: Title/abstract review, full text review, and data extraction using a standardized data extraction form. DATA SYNTHESIS: The Grading of Recommendations Assessment, Development and Evaluation approach was used to identify and summarize evidence and develop recommendations. Twenty-six studies were identified for full text extraction to address the Patient/Intervention/Comparator/Outcome questions, and 14 were used for the recommendations/statements. Two clinical recommendations were generated, related to the use of electronic screening tools and automated monitoring of compliance with best practice guidelines. Two research statements were generated, related to the development of multicenter data collaborations and the design of generalizable algorithms and electronic tools. One policy statement was generated, related to the provision of material and human resources by healthcare organizations to empower clinicians to develop clinical informatics interventions to improve the care of patients with PARDS. CONCLUSIONS: We present two clinical recommendations and three statements (two research one policy) for the use of electronic algorithms and clinical informatics tools for patients with PARDS based on a systematic review of the literature and expert consensus.",
        "DOI": "10.1097/PCC.0000000000003155",
        "affiliation_name": "Keck School of Medicine of USC",
        "affiliation_city": "Los Angeles",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Secondary data for global health digitalisation",
        "paper_author": "Näher A.F.",
        "publication": "The Lancet Digital Health",
        "citied_by": "25",
        "cover_date": "2023-02-01",
        "Abstract": "Substantial opportunities for global health intelligence and research arise from the combined and optimised use of secondary data within data ecosystems. Secondary data are information being used for purposes other than those intended when they were collected. These data can be gathered from sources on the verge of widespread use such as the internet, wearables, mobile phone apps, electronic health records, or genome sequencing. To utilise their full potential, we offer guidance by outlining available sources and approaches for the processing of secondary data. Furthermore, in addition to indicators for the regulatory and ethical evaluation of strategies for the best use of secondary data, we also propose criteria for assessing reusability. This overview supports more precise and effective policy decision making leading to earlier detection and better prevention of emerging health threats than is currently the case.",
        "DOI": "10.1016/S2589-7500(22)00195-9",
        "affiliation_name": "Berliner Institut für Gesundheitsforschung",
        "affiliation_city": "Berlin",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Building Trust in Earth Science Findings through Data Traceability and Results Explainability",
        "paper_author": "Olaya P.",
        "publication": "IEEE Transactions on Parallel and Distributed Systems",
        "citied_by": "10",
        "cover_date": "2023-02-01",
        "Abstract": "To trust findings in computational science, scientists need workflows that trace the data provenance and support results explainability. As workflows become more complex, tracing data provenance and explaining results become harder to achieve. In this paper, we propose a computational environment that automatically creates a workflow execution's record trail and invisibly attaches it to the workflow's output, enabling data traceability and results explainability. Our solution transforms existing container technology, includes tools for automatically annotating provenance metadata, and allows effective movement of data and metadata across the workflow execution. We demonstrate the capabilities of our environment with the study of SOMOSPIE, an earth science workflow. Through a suite of machine learning modeling techniques, this workflow predicts soil moisture values from the 27 km resolution satellite data down to higher resolutions necessary for policy making and precision agriculture. By running the workflow in our environment, we can identify the causes of different accuracy measurements for predicted soil moisture values in different resolutions of the input data and link different results to different machine learning methods used during the soil moisture downscaling, all without requiring scientists to know aspects of workflow design and implementation.",
        "DOI": "10.1109/TPDS.2022.3220539",
        "affiliation_name": "University of Delaware",
        "affiliation_city": "Newark",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Toward Human-in-the-Loop AI: Enhancing Deep Reinforcement Learning via Real-Time Human Guidance for Autonomous Driving",
        "paper_author": "Wu J.",
        "publication": "Engineering",
        "citied_by": "96",
        "cover_date": "2023-02-01",
        "Abstract": "Due to its limited intelligence and abilities, machine learning is currently unable to handle various situations thus cannot completely replace humans in real-world applications. Because humans exhibit robustness and adaptability in complex scenarios, it is crucial to introduce humans into the training loop of artificial intelligence (AI), leveraging human intelligence to further advance machine learning algorithms. In this study, a real-time human-guidance-based (Hug)-deep reinforcement learning (DRL) method is developed for policy training in an end-to-end autonomous driving case. With our newly designed mechanism for control transfer between humans and automation, humans are able to intervene and correct the agent's unreasonable actions in real time when necessary during the model training process. Based on this human-in-the-loop guidance mechanism, an improved actor-critic architecture with modified policy and value networks is developed. The fast convergence of the proposed Hug-DRL allows real-time human guidance actions to be fused into the agent's training loop, further improving the efficiency and performance of DRL. The developed method is validated by human-in-the-loop experiments with 40 subjects and compared with other state-of-the-art learning approaches. The results suggest that the proposed method can effectively enhance the training efficiency and performance of the DRL algorithm under human guidance without imposing specific requirements on participants’ expertise or experience.",
        "DOI": "10.1016/j.eng.2022.05.017",
        "affiliation_name": "School of Mechanical and Aerospace Engineering",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore"
    },
    {
        "paper_title": "AutoOLA: Automatic object level augmentation for wheat spikes counting",
        "paper_author": "Zaji A.",
        "publication": "Computers and Electronics in Agriculture",
        "citied_by": "15",
        "cover_date": "2023-02-01",
        "Abstract": "The ability to count wheat spikes is one of the most critical indicators in wheat plant agriculture that correlates positively with yield estimation. Numerous studies have developed machine vision-based high-throughput phenotyping algorithms and demonstrated that the developed models could accurately detect and count wheat spikes. However, the drawback of previously developed models is their expensive and time-consuming data preparation process. While capturing images of wheat is relatively straightforward, annotating thousands of small wheat spikes in the collected dataset requires immense time and effort. This study, therefore, proposes a novel augmentation algorithm, Automatic Object Level Augmentation (AutoOLA), that can considerably reduce the required number of training samples in Deep Learning (DL) models. In this algorithm, different objects in the wheat images, including the spikes, leaves, stems, and backgrounds, are decoupled and augmented independently based on their augmentation policy. The augmented images are then generated by randomly assembling the augmented objects. In addition, an evolutionary optimization algorithm is used to automatically design the types of transformations and their magnitudes for each decoupled object. The results demonstrated that the proposed AutoOLA technique could improve wheat spike counting performance by up to 60 percent. With only one trainable image, the employed hybrid AutoOLA-DL model can predict the number of wheat spikes in the field with Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE) values of 2.695, 2.085, and 13.817, respectively.",
        "DOI": "10.1016/j.compag.2023.107623",
        "affiliation_name": "Swift Current Research and Development Centre",
        "affiliation_city": "Swift Current",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Shrinkage estimation with reinforcement learning of large variance matrices for portfolio selection",
        "paper_author": "Mattera G.",
        "publication": "Intelligent Systems with Applications",
        "citied_by": "14",
        "cover_date": "2023-02-01",
        "Abstract": "A large amount of assets characterizes high-dimensional portfolio selection problems compared to temporal observation. In such a high-dimensional framework, the asset allocation is unfeasible because the covariance matrix obtained with the usual sample estimators cannot be inverted. This paper proposes a new shrinkage estimator based on reinforcement learning for large covariance matrices that is optimal in the context of portfolio selection. The resulting estimator is entirely data-driven since the optimal shrinkage intensity is given by optimizing neural network weights. This paper presents two different architectures: a standard fully connected network for a classical Policy Gradient Agent (PGA) and a Gated Recurrent Unit for a Recurrent Policy Gradient Agent (RPGA). To show the validity of the proposal, an application to asset allocation with Industry portfolios is provided. The results indicate that the RPGA-based approach in shrinkage estimation provides the best performance in out-of-sample comparison.",
        "DOI": "10.1016/j.iswa.2023.200181",
        "affiliation_name": "Sapienza Università di Roma",
        "affiliation_city": "Rome",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "A Novel Closed-Loop Deep Brain Stimulation Technique for Parkinson's Patients Rehabilitation Utilizing Machine Learning",
        "paper_author": "Faraji B.",
        "publication": "IEEE Sensors Journal",
        "citied_by": "7",
        "cover_date": "2023-02-01",
        "Abstract": "Deep brain stimulation (DBS) is an effective treatment for movement problems caused by a variety of neurodegenerative illnesses, including Parkinson's disease (PD). Researchers have created closed-loop DBS (CL-DBS) techniques for reducing hand tremors in many disorders, notably Parkinson's patients, in recent years. As shown in the studies, hand tremor and rigidity symptoms are two sides of the same coin, with hand tremor leading to rigidity fluctuations and vice versa. The following are the primary contributions of this study: 1) depicts the correlation between the two indicators and 2) the suggestion of a proximal policy optimization (PPO)-based non-integer proportional integral derivative (PID) control technique to reduce hand tremor and stiffness at the same time, where the reward function is developed to alter DBS settings when illness levels vary. The PPO technique is combined with the main controller to provide adaptive learning. The controller parameters of the non-integer PID scheme are regarded as changeable controller coefficients in the proposed strategy, which will be adaptively built by the PPO technique via online learning of its neural networks (NNs). In order to demonstrate the advantages and adaptability of the procedure with different models and patients, the recommended strategy is an analysis by computer simulation in a variety of contexts (performance, robustness, and noise) and compared to current methodologies like integer-order PID (IOPID) and fractional-order PID (FOPID). The simulation outcomes showed that the created system performed better and more effectively than previous approaches in coping with parameter changes and outside noise, in addition to simultaneously reducing hand tremor and stiffness.",
        "DOI": "10.1109/JSEN.2022.3228766",
        "affiliation_name": "Aja University of Medical Sciences",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "Cluster analysis of adults unvaccinated for COVID-19 based on behavioral and social factors, National Immunization Survey-Adult COVID Module, United States",
        "paper_author": "Meng L.",
        "publication": "Preventive Medicine",
        "citied_by": "10",
        "cover_date": "2023-02-01",
        "Abstract": "By the end of 2021, approximately 15% of U.S. adults remained unvaccinated against COVID-19, and vaccination initiation rates had stagnated. We used unsupervised machine learning (K-means clustering) to identify clusters of unvaccinated respondents based on Behavioral and Social Drivers (BeSD) of COVID-19 vaccination and compared these clusters to vaccinated participants to better understand social/behavioral factors of non-vaccination. The National Immunization Survey Adult COVID Module collects data on U.S. adults from September 26–December 31,2021 (n = 187,756). Among all participants, 51.6% were male, with a mean age of 61 years, and the majority were non-Hispanic White (62.2%), followed by Hispanic (17.2%), Black (11.9%), and others (8.7%). K-means clustering procedure was used to classify unvaccinated participants into three clusters based on 9 survey BeSD items, including items assessing COVID-19 risk perception, social norms, vaccine confidence, and practical issues. Among unvaccinated adults (N = 23,397), 3 clusters were identified: the “Reachable” (23%), “Less reachable” (27%), and the “Least reachable” (50%). The least reachable cluster reported the lowest concern about COVID-19, mask-wearing behavior, perceived vaccine confidence, and were more likely to be male, non-Hispanic White, with no health conditions, from rural counties, have previously had COVID-19, and have not received a COVID-19 vaccine recommendation from a healthcare provider. This study identified, described, and compared the characteristics of the three unvaccinated subgroups. Public health practitioners, healthcare providers and community leaders can use these characteristics to better tailor messaging for each sub-population. Our findings may also help inform decisionmakers exploring possible policy interventions.",
        "DOI": "10.1016/j.ypmed.2022.107415",
        "affiliation_name": "Leidos Inc.",
        "affiliation_city": "Reston",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Multi-objective deep reinforcement learning for optimal design of wind turbine blade",
        "paper_author": "Wang Z.",
        "publication": "Renewable Energy",
        "citied_by": "15",
        "cover_date": "2023-02-01",
        "Abstract": "The design of a wind turbine blade is a typical complex multi-objective optimization problem, mostly solved by evolutionary algorithms. However, these methods are not effective due to limitations such as inaccurate solutions on Pareto fronts for high-dimensional problems, numerous iterations and low adaptability to problems with similar conditions. To address these issues, two multi-objective deep reinforcement learning models are introduced in this paper from an entirely different perspective. The first model, namely the multi-objective deep deterministic policy gradient (MO-DDPG), extends the existing popular reinforcement learning algorithm DDPG to multi-objective optimization problems by integrating various techniques including modeling of constraints on high-dimensional spaces and generation of Pareto solutions. The second model, namely the multi-objective deep stochastic policy gradient (MO-DSPG), further improves the MO-DDPG by incorporating a random neural network called restricted Boltzmann machine (RBM). An adaptive random agent is trained to transform multiple deterministic policies into an optimal stochastic policy. In addition, neighborhood-based parameter transfer strategy is applied to MO-DSPG in the model training phase to reduce the computation time. Experiments showed that the aerodynamic performance of the blades is improved by both the MO-DDPG and the MO-DSPG models with the hypervolume increasing an average of 6.67% and 9.25% respectively, compared with the state-of-art models. The computational efficiency of MO-DSPG is improved by using the parameter transfer strategy, with its runtime reduced to 72.52% compared with state-of-art models.",
        "DOI": "10.1016/j.renene.2023.01.003",
        "affiliation_name": "Shanghai Jiao Tong University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Alternative risk measurement for the banking system and its nexus with economic growth",
        "paper_author": "Song M.",
        "publication": "Computers and Industrial Engineering",
        "citied_by": "2",
        "cover_date": "2023-02-01",
        "Abstract": "Scientific and reasonable evaluation of bank performance can help them conduct business management and decision-making, while most research ignore using the shadow price of undesirable outputs to evaluate banking performance. This study introduces a dual formulation of network data envelopment analysis to evaluate the shadow price of non-performing loans, which could be regarded as a novel measure of potential financial risk. We further investigate the tradeoff between economic growth and financial risk from micro and macro perspectives. Our results show that the financial risk of city commercial banks and joint-stock banks is higher than that of the big four state-owned and policy banks. Our research proves that economic growth influences the risk levels of banks directly and indirectly. More specifically, our study confirms that economic growth can reduce the bank's risk by reducing the bank's loan capital ratio as an intermediary path, that an increase in the capital adequacy ratio has a positive effect on the bank's risk.",
        "DOI": "10.1016/j.cie.2022.108946",
        "affiliation_name": "Anhui University of Finance and Economics",
        "affiliation_city": "Bengbu",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Crossing the Bridge From Degeneration to Deformity: When Does Sagittal Correction Impact Outcomes in Adult Spinal Deformity Surgery?",
        "paper_author": "Williamson T.K.",
        "publication": "Spine",
        "citied_by": "5",
        "cover_date": "2023-02-01",
        "Abstract": "Background. Patients with less severe adult spinal deformity (ASD) undergo surgical correction and often achieve good clinical outcomes. However, it is not well understood how much clinical improvement is due to sagittal correction rather than treatment of the spondylotic process. Purpose. Determine baseline thresholds in radiographic parameters that, when exceeded, may result in substantive clinical improvement from surgical correction. Study Design. Retrospective. Materials and Methods. ASD patients with BL and two-year data were included. Parameters assessed: sagittal vertical axis, pelvic incidence-lumbar lordosis mismatch, pelvic tilt, T1 pelvic angle, L1 pelvic angle, L4-S1 lordosis, C2-C7 sagittal vertical axis, C2-T3, C2 slope. Outcomes: Good Outcome (GO) at two years: [meeting either: (1) Substantial Clinical Benefit for Oswestry Disability Index (change >18.8), or (2) Oswestry Disability Index <15 and Scoliosis Research Society Total>4.5]. Binary logistic regression assessed each parameter to determine if correction was more likely needed to achieve GO. Conditional inference tree run machine learning analysis generated baseline thresholds for each parameter, above which, correction was necessary to achieve GO. Results. We included 431 ASD patients. There were 223 (50%) that achieved a GO by two years. Binary logistic regression analysis demonstrated, with increasing baseline severity in deformity, sagittal correction was more often seen in those achieving GO for each parameter(all P<0.001). Of patients with baseline T1 pelvic angle above the threshold, 95% required correction to meet GO (95% vs. 54%, P<0.001). A baseline pelvic incidence-lumbar lordosis >10° (74% of patients meeting GO) needed correction to achieve GO (odds ratio: 2.6, 95% confidence interval: 1.4-4.8). A baseline C2 slope >15° also necessitated correction to obtain clinical success (odds ratio: 7.7, 95% confidence interval: 3.7-15.7). Conclusions. Our study highlighted point may be present at which sagittal correction has an outsized influence on clinical improvement, reflecting the line where deformity becomes a significant contributor to disability. These new thresholds give us insight into which patients may be more suitable for sagittal correction, as opposed to intervention for the spondylotic process only, leading to a more efficient utility of surgical intervention for ASD.",
        "DOI": "10.1097/BRS.0000000000004461",
        "affiliation_name": "College of Medicine",
        "affiliation_city": "New York",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Achieving affordable and clean energy through conversion of waste plastic to liquid fuel",
        "paper_author": "Awogbemi O.",
        "publication": "Journal of the Energy Institute",
        "citied_by": "37",
        "cover_date": "2023-02-01",
        "Abstract": "Millions of tons of waste plastic are generated worldwide every year. These waste plastics are nonbiodegradable and are inappropriately disposed thereby constituting sanitary and environmental nuisance to society. The discarded plastics are dumped into drainages and water bodies where they pollute the environment, contaminate aquatic habitats, and harm aquatic animals. Researchers have devised techniques for converting waste plastic to biofuels to replace the dangerous fossil-based fuels and meet the Sustainable Development Goal (SDG) of affordable and clean energy for all targets. This study reviews the strategies for achieving affordable and clean energy through the conversion of waste plastic to biofuels and other value-added products. The classes of plastics and their applications, waste plastic conversion routes and their products, and conversion of waste plastic to bio-oil, biohydrogen, and other renewable fuels are examined. The application of waste plastic in the building and construction industries, production of carbon nanotubes, synthesis of graphene nanosheets as novel biocatalyst, and wastewater treatment are also discussed with a view to exposing other avenues for the utilization of waste plastic. The outcome of this intervention will enrich scholarship by providing updated information on the strategies for converting waste plastic to biofuels and other value-added products. Biofuel researchers, waste managers, environmental enthusiasts, and other stakeholders will be equipped with adequate knowledge on opportunities available in the waste plastic conversion and how to turn the menace of waste plastic into beneficial products. Implementable legislations and policies to encourage investments into waste conversion for cleaner environment, employment generation, and biodiversity. More targeted interdisciplinary studies are required to evolve innovative pathways for sustainable conversion and utilization of waste plastic. The use of appropriate state-of-the-art technologies such as machine learning, artificial intelligence, etc. as well as statistical modeling, and optimization tools are needed to improve the conversion efficiency, optimize process parameters, and guarantee the sustainable production of quality products from waste plastic towards meeting SDG 7.",
        "DOI": "10.1016/j.joei.2022.101154",
        "affiliation_name": "University of Johannesburg",
        "affiliation_city": "Johannesburg",
        "affiliation_country": "South Africa"
    },
    {
        "paper_title": "Real-Time Virtual Machine Scheduling in Industry IoT Network: A Reinforcement Learning Method",
        "paper_author": "Ma X.",
        "publication": "IEEE Transactions on Industrial Informatics",
        "citied_by": "37",
        "cover_date": "2023-02-01",
        "Abstract": "The widespread adoption of Industrial Internet of Things (IIoT)-based applications has driven the emergence and development of cloud-related computing paradigms with the ability to seamlessly leverage cloud resources. Heterogeneous resources, mobility factors in IoT, and dynamic behavior make it challenging for the corresponding virtual machine (VM) scheduling problem to address the processing effectiveness of application requests in these kinds of cloud environments. Based on reinforcement learning theory, this article proposes an online VM scheduling scheme (OSEC) for joint energy consumption and cost optimization that divides the scheduling process into two parts: VM allocation and VM migration. First, all the VMs and the physical machines (PMs) are regarded as a set of states and actions in the cloud environment, and the Q-learning feedback is used to achieve the iterative computation of Q-values to obtain the optimal parallel allocation sequence for multiple VMs. Then, VMs are migrated among the active PMs according to a grouping policy and the best-fit principle to achieve dynamic consolidation of the resources in the data center. Finally, experimental results show that compared with state-of-the-art algorithms under different conditions, the proposed method reduces energy consumption by approximately 18.25%, VM execution costs by approximately 21.34%, and service level agreement (SLA) violations by approximately 90.51%.",
        "DOI": "10.1109/TII.2022.3211622",
        "affiliation_name": "Gachon University",
        "affiliation_city": "Seongnam",
        "affiliation_country": "South Korea"
    },
    {
        "paper_title": "A bi-objective deep reinforcement learning approach for low-carbon-emission high-speed railway alignment design",
        "paper_author": "He Q.",
        "publication": "Transportation Research Part C: Emerging Technologies",
        "citied_by": "12",
        "cover_date": "2023-02-01",
        "Abstract": "Reasonable design and planning of alignments are crucial for both economic investment and the environmental impact of high-speed railway projects. Approaches that can integrate economic investment and environmental factors, thus selecting an economical and eco-friendly railway alignment, are very demanding. To address the above issue, this study focuses on optimizing a railway's comprehensive investment, including the construction and environmental costs, as well as the railway's life-cycle carbon emission caused by the production of building materials and the trains’ energy consumption. A novel railway alignment optimization model is formulated based on the multi-objective reinforcement learning (MORL) framework to reduce the railway total cost, accounting for both the construction cost and environmental factors. In the proposed model, a deep deterministic policy gradient (DDPG) algorithm is enhanced with an envelope algorithm that can optimize the convex envelope of multi-objective Q-values to ensure an efficient consistency between the entire space of preferences in a domain and the corresponding optimal policies. Finally, the proposed model is applied to a real-world high-speed railway project. Results show that the MORL model can automatically explore and optimize railway alignment, and produce less expensive and more eco-friendly solutions than manual work while satisfying various alignment constraints.",
        "DOI": "10.1016/j.trc.2022.104006",
        "affiliation_name": "China Railway Eryuan Engineering Group Co. Ltd.",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Analysing urban growth using machine learning and open data: An artificial neural network modelled case study of five Greek cities",
        "paper_author": "Tsagkis P.",
        "publication": "Sustainable Cities and Society",
        "citied_by": "35",
        "cover_date": "2023-02-01",
        "Abstract": "Urban development if not planned and managed adequately can be unsustainable. Urban growth models have been a powerful toolkit to help tackling this challenge. In this paper, we use a machine learning approach, to apply an urban growth model to five of the largest cities in Greece. Specifically, we first develop a methodology to collect, organise, handle and transform historical open spatial data, concerning various impact factors, into machine learning data. Such factors involve social, economic, biophysical, neighbouring-related and political driving forces, which must be transformed into tabular data. We also provide an artificial neural network (ANN) model and the methodology to train and evaluate it using goodness-of-fit metrics, which in turn provide the best weights of impact factors. Finally, we execute a prediction for 2030, presenting the results and output maps for each of the five case study cities. As our study is based on pan-European datasets, our model can be used for any area within Europe, using the open-source utility developed to support it. In this sense, our work provides local policy-makers and urban planners with an instrument that could help them analyse various future development scenarios and take the right decisions going forward.",
        "DOI": "10.1016/j.scs.2022.104337",
        "affiliation_name": "Huddersfield Business School",
        "affiliation_city": "Huddersfield",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Hierarchically structured task-agnostic continual learning",
        "paper_author": "Hihn H.",
        "publication": "Machine Learning",
        "citied_by": "5",
        "cover_date": "2023-02-01",
        "Abstract": "One notable weakness of current machine learning algorithms is the poor ability of models to solve new problems without forgetting previously acquired knowledge. The Continual Learning paradigm has emerged as a protocol to systematically investigate settings where the model sequentially observes samples generated by a series of tasks. In this work, we take a task-agnostic view of continual learning and develop a hierarchical information-theoretic optimality principle that facilitates a trade-off between learning and forgetting. We derive this principle from a Bayesian perspective and show its connections to previous approaches to continual learning. Based on this principle, we propose a neural network layer, called the Mixture-of-Variational-Experts layer, that alleviates forgetting by creating a set of information processing paths through the network which is governed by a gating policy. Equipped with a diverse and specialized set of parameters, each path can be regarded as a distinct sub-network that learns to solve tasks. To improve expert allocation, we introduce diversity objectives, which we evaluate in additional ablation studies. Importantly, our approach can operate in a task-agnostic way, i.e., it does not require task-specific knowledge, as is the case with many existing continual learning algorithms. Due to the general formulation based on generic utility functions, we can apply this optimality principle to a large variety of learning problems, including supervised learning, reinforcement learning, and generative modeling. We demonstrate the competitive performance of our method on continual reinforcement learning and variants of the MNIST, CIFAR-10, and CIFAR-100 datasets.",
        "DOI": "10.1007/s10994-022-06283-9",
        "affiliation_name": "Universität Ulm",
        "affiliation_city": "Ulm",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Reinforcement learning control for a three-link biped robot with energy-efficient periodic gaits",
        "paper_author": "Pan Z.",
        "publication": "Acta Mechanica Sinica/Lixue Xuebao",
        "citied_by": "1",
        "cover_date": "2023-02-01",
        "Abstract": "Designing a high-performance controller for the walking gaits of biped robots remains an open research area due to their strong nonlinearity and non-smooth responses. To overcome such challenges, a humanoid robot with a torso, i.e., a three-link biped robot involving both impact and friction, is developed firstly. Then, the twin delayed deep deterministic policy gradient algorithm is adopted to design the reinforcement learning controller for the proposed biped robot. For the specified control targets, i.e., energy-efficient periodic gaits for both the downhill and uphill cases, a reward function utilizing the Poincaré map and the power function is constructed to provide guidelines for the controller. Thus, the proposed controller can learn to adaptively output accurate cosine torques to achieve the goal without relying on the pre-designed reference trajectories or embedded unstable periodic gaits. A comparative study between the proposed reinforcement learning and neural network proportion differentiation controllers demonstrates the proposed controller can lead to accurate and energy-efficient periodic gaits and provide strong adaptability and robustness within a wide variety of walking slopes.",
        "DOI": "10.1007/s10409-022-22304-x",
        "affiliation_name": "Hunan University",
        "affiliation_city": "Changsha",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Spatial variations and mechanisms for the stability of terrestrial carbon sink in China",
        "paper_author": "Wang K.",
        "publication": "Science China Earth Sciences",
        "citied_by": "19",
        "cover_date": "2023-02-01",
        "Abstract": "Understanding the stability of terrestrial carbon sinks (S-TCS) contributes to more accurate prediction of the terrestrial carbon sink (TCS) in the context of future global change and helps inform climate change mitigation policies. Here, focusing on China, we analyzed the spatial distribution and driving mechanisms for the S-TCS, quantified by the interannual variability of the TCS, using three independent approaches (atmospheric inversions, ecosystem carbon cycle models, and machine learning models based on flux tower observations). We found that the interannual variability of the TCS in China is relatively small compared with the conterminous United States and geographic Europe, indicating a generally stable TCS in China. Spatially, the S-TCS is lower in the North China Plain, Northeast China Plain, and western Yunnan-Guizhou Plateau than in other regions, with varying underlying mechanisms. Large interannual variations in precipitation and high TCS sensitivities to precipitation fluctuations explain the low S-TCS in the North China Plain and Northeast China Plain, while high TCS sensitivities to temperature variations drive the low S-TCS in the western Yunnan-Guizhou Plateau. Our findings highlight the importance of considering local contexts for stabilizing and enhancing China’s TCS in a changing environment.",
        "DOI": "10.1007/s11430-021-1003-5",
        "affiliation_name": "Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Representation learning for continuous action spaces is beneficial for efficient policy learning",
        "paper_author": "Zhao T.",
        "publication": "Neural Networks",
        "citied_by": "2",
        "cover_date": "2023-02-01",
        "Abstract": "Deep reinforcement learning (DRL) breaks through the bottlenecks of traditional reinforcement learning (RL) with the help of the perception capability of deep learning and has been widely applied in real-world problems. While model-free RL, as a class of efficient DRL methods, performs the learning of state representations simultaneously with policy learning in an end-to-end manner when facing large-scale continuous state and action spaces. However, training such a large policy model requires a large number of trajectory samples and training time. On the other hand, the learned policy often fails to generalize to large-scale action spaces, especially for the continuous action spaces. To address this issue, in this paper we propose an efficient policy learning method in latent state and action spaces. More specifically, we extend the idea of state representations to action representations for better policy generalization capability. Meanwhile, we divide the whole learning task into learning with the large-scale representation models in an unsupervised manner and learning with the small-scale policy model in the RL manner. The small policy model facilitates policy learning, while not sacrificing generalization and expressiveness via the large representation model. Finally, the effectiveness of the proposed method is demonstrated by MountainCar, CarRacing and Cheetah experiments.",
        "DOI": "10.1016/j.neunet.2022.12.009",
        "affiliation_name": "RIKEN Center for Advanced Intelligence Project",
        "affiliation_city": "Tokyo",
        "affiliation_country": "Japan"
    },
    {
        "paper_title": "Online adaptive optimal control algorithm based on synchronous integral reinforcement learning with explorations",
        "paper_author": "Guo L.",
        "publication": "Neurocomputing",
        "citied_by": "11",
        "cover_date": "2023-02-01",
        "Abstract": "In this study, we present a novel algorithm, based on synchronous policy iteration, to solve the continuous-time infinite-horizon optimal control problem of input affine system dynamics. The integral reinforcement is measured as an excitation signal to estimate the solution to the Hamilton–Jacobi–Bellman equation. In addition, the proposed method is completely model-free, that is, no a priori knowledge of the system is required. Using the adaptive tuning law, the actor and critic neural networks can simultaneously approximate the optimal value function and policy. The persistence of excitation condition is required to guarantee the convergence of the two networks. Unlike in traditional policy iteration algorithms, the restriction of the initial admissible policy was eliminated using this method. The effectiveness of the proposed algorithm is verified through numerical simulations.",
        "DOI": "10.1016/j.neucom.2022.11.055",
        "affiliation_name": "Beijing University of Posts and Telecommunications",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Integrated statistical indicators from Scottish linked open government data",
        "paper_author": "Karamanou A.",
        "publication": "Data in Brief",
        "citied_by": "7",
        "cover_date": "2023-02-01",
        "Abstract": "Open Government Data (OGD), including statistical data, such as economic, environmental and social indicators, are data published by the public sector for free reuse. These data have a huge potential when exploited using Machine Learning methods. Linked Data technologies facilitate retrieving integrated statistical indicators by defining and executing SPARQL queries. However, statistical indicators are available in different temporal and spatial granularity levels as well using different units of measurement. This data article describes the integrated statistical indicators that were retrieved from the official Scottish data portal in order to facilitate the exploitation of Machine Learning methods in OGD. Multiple SPARQL queries as well as manual search in the data portal were employed towards this end. The resulted dataset comprises the maximum number of compatible datasets, i.e., datasets with matching temporal and spatial characteristics. In particular, the data include 60 statistical indicators from seven categories such as health and social care, housing, and crime and justice. The indicators refer to the 6,976 “2011 data zones” of Scotland, while the year of reference is 2015. Data are ready to be used by the research community, students, policy makers, and journalists and give rise to plenty of social, business, and research scenarios that can be solved using Machine Learning technologies and methods.",
        "DOI": "10.1016/j.dib.2022.108779",
        "affiliation_name": "Centre for Research and Technology-Hellas",
        "affiliation_city": "Thessaloniki",
        "affiliation_country": "Greece"
    },
    {
        "paper_title": "Understanding the acceptance of emotional artificial intelligence in Japanese healthcare system: A cross-sectional survey of clinic visitors’ attitude",
        "paper_author": "Ho M.T.",
        "publication": "Technology in Society",
        "citied_by": "30",
        "cover_date": "2023-02-01",
        "Abstract": "Despite having one of the most advanced healthcare systems in the world, Japan is expected to experience a shortage of nearly half a million healthcare workers by 2025 due to its rapidly aging population. In response, government authorities plan to implement a wide range of AI-driven healthcare solutions. These include care robots that assist the physically handicapped or elderly, chatbots that provide anonymous online mental health consultation, and diagnostic software utilizing machine learning. Yet one of the most popular smart technologies to augment the nation's already overstrained and undermanned healthcare system is a little known but emerging emotional AI technologies, i.e., deep learning systems trained to read, classify, and respond to human emotions. These technologies are being sold on a commercial level not only to the public but also to rehabilitation centers, local hospitals, and senior citizen residences. Although the augmentation of healthcare services to intelligent machines may seem like a logical step in a country well-known for its long-standing affection toward robots, Japanese society is also known for its adherence to established social relations and traditional institutional practices, especially, in the realm of medical care. In order to gauge Japanese acceptance of emotion-sensing technology, we analyze a dataset of 245 visitors to clinics and hospitals in a typical suburban area in Japan using multiple linear regression. The results show that in general, senior and male patients perceive the emotional AI technology more negatively. For behavioral variables, patients' level of familiarity has positive correlations with attitudes toward emotional AI-based applications in private setting (βFamiliarity_AttitudePri=0.346, p<0.001) and public setting (βFamiliarity_PublicAttitude=0.297, p<0.001); while concern for losing control to AI has negative correlations with the attitudes' variables: private setting (βLosingControl_AttitudePri=−0.262,p=0.002) and public setting (β LosingControl_AttitudePub=-0.188, p=0.044). Interestingly, concerns over violation of privacy and discrimination are non-significant correlates, which contradict the emerging literature on this subject. We further contextualize the findings with insights afforded by an understanding of Japanese culture as well as the relevant literature on care robots in Japan. Finally, policy and education implications to promote emotional AI acceptance to the general and senior members of the society are provided.",
        "DOI": "10.1016/j.techsoc.2022.102166",
        "affiliation_name": "Phenikaa University",
        "affiliation_city": "Hanoi",
        "affiliation_country": "Viet Nam"
    },
    {
        "paper_title": "National trends in retreatment of HCV due to reinfection or treatment failure in Australia",
        "paper_author": "Carson J.M.",
        "publication": "Journal of Hepatology",
        "citied_by": "4",
        "cover_date": "2023-02-01",
        "Abstract": "Background & Aims: Population-level uptake of direct-acting antiviral (DAA) treatment for hepatitis C virus (HCV) infection, including retreatment, can be estimated through administrative pharmaceutical dispensation data. However, the reasons for retreatment are not captured in these data. We developed a machine learning model to classify retreatments as reinfection or treatment failure at a national level. Methods: Retreatment data from the REACH-C cohort (n = 10,843 treated with DAAs; n = 320 retreatments with known reason), were used to train a random forest model. Nested cross validation was undertaken to assess model performance and to optimise hyperparameters. The model was applied to data on DAA retreatment dispensed during 2016-2021 in Australia, to identify the reason for retreatment (treatment failure or reinfection). Results: Average predictive accuracy, precision, sensitivity, specificity and F1-score for the model were 96.3%, 96.5%, 96.3%, 96.3% and 96.3%, respectively. Nationally, 95,272 individuals initiated DAAs, with treatment uptake declining from 32,454 in 2016 to 6,566 in 2021. Of those treated, 6,980 (7%) were retreated. Our model classified 51.8% (95% CI 46.7–53.6%; n = 3,614) of cases as reinfection and 48.2% (95% CI 46.4–53.3%; n = 3,366) as treatment failure. Retreatment for reinfection increased steadily over the study period from 14 in 2016 to 1,092 in 2020, stabilising in 2021. Retreatment for treatment failure increased from 73 in 2016 to 1,077 in 2019, then declined to 515 in 2021. Among individuals retreated for treatment failure, 50% had discontinued initial treatment. Conclusions: We used a novel methodology with high classification accuracy to evaluate DAA retreatment patterns at a national level. Increases in retreatment uptake for treatment failure corresponded to the availability of pangenotypic and salvage regimens. Increasing retreatment uptake for reinfection likely reflects increasing reinfection incidence. Impact and implications: This study used machine learning methodologies to analyse national administrative data and characterise trends in HCV retreatment due to reinfection and treatment failure. Retreatment for reinfection increased over time, reflecting increasing numbers of people at risk for reinfection following HCV cure. Increased retreatment for treatment failure corresponded to the availability of pangenotypic and salvage DAA regimens. The findings of this study can be used by public health agencies and policy makers to guide and assess HCV elimination strategies, while the novel methodology for monitoring trends in HCV retreatment has the potential to be used in other settings, and health conditions.",
        "DOI": "10.1016/j.jhep.2022.09.011",
        "affiliation_name": "The Kirby Institute",
        "affiliation_city": "Kensington",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Generating collective wall-jumping behavior for a robotic swarm with self-teaching automatic curriculum learning",
        "paper_author": "Nie X.",
        "publication": "Artificial Life and Robotics",
        "citied_by": "0",
        "cover_date": "2023-02-01",
        "Abstract": "Swarm robotics (SR) is a research field about how to design a large number of robots so that they can generate meaningful collective behaviors. One of the promising approaches in designing a control policy is reinforcement learning (RL). However, it is well known that the sparse reward problem may arise, especially in cases of solving highly complex problems. Curriculum learning (CL) can be one of the effective approaches to overcoming this difficulty. In this paper, we propose a novel method called Self-Teaching Automatic Curriculum Learning (STACL). The training progress of different lessons is compared by agents to determine which lesson should be trained in the next episode. The collective wall-jumping task, in which the robots have to generate collective wall-jumping behavior to jump over the high wall and reach the goal as soon as possible, is employed to illustrate the effects. Simulation results show that the proposed approach has the fastest convergence speed and the most stable performance. In addition, we also conducted experiments to examine the flexibility of the developed controllers.",
        "DOI": "10.1007/s10015-022-00833-z",
        "affiliation_name": "Hiroshima University",
        "affiliation_city": "Higashihiroshima",
        "affiliation_country": "Japan"
    },
    {
        "paper_title": "A stochastic dynamic programming approach for the machine replacement problem",
        "paper_author": "Forootani A.",
        "publication": "Engineering Applications of Artificial Intelligence",
        "citied_by": "13",
        "cover_date": "2023-02-01",
        "Abstract": "This paper addresses both the modeling and the resolution of the replacement problem for a population of machines. The main objective is the computation of a minimum cost replacement policy, which, based on the status of each machine, determines whether one or more machines have to be replaced over a given finite time horizon. The replacement problem of a set of machines can be regarded as a sequential decision-making problem under uncertainty. Thanks to this, we propose a novel formulation for such problems consisting of a composition of discrete-time multi-state Markov Decision Processes (MDPs), one for each specific machine. The underlying optimization problem is formulated as a stochastic Dynamic Programming (DP), and then solved by using the principles of the backward DP algorithm. Moreover, to deal with the curse of dimensionality due to the high-cardinality state–space of real-world/industrial applications, a new generalized multi-trajectory Least-Squares Temporal Difference (LSTD) based method is introduced. The resulting algorithm computes an approximate optimal cost function by: (i) running Monte Carlo simulations over different trajectories of a given length; (ii) embedding the policy improvement step within the recursive LSTD iterations; (iii) enforcing an off-policy mechanism to improve the LSTD exploration capabilities. A study on the convergence properties of the proposed approach is also provided. Several numerical examples are given to illustrate its effectiveness in terms of parametric sensitivity, computational burden, and performance of the computed policies compared with some heuristics defined in the literature.",
        "DOI": "10.1016/j.engappai.2022.105638",
        "affiliation_name": "Politecnico di Bari",
        "affiliation_city": "Bari",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Gully erosion susceptibility mapping using four machine learning methods in Luzinzi watershed, eastern Democratic Republic of Congo",
        "paper_author": "Chuma G.B.",
        "publication": "Physics and Chemistry of the Earth",
        "citied_by": "18",
        "cover_date": "2023-02-01",
        "Abstract": "Soil erosion by gullying causes severe soil degradation, which in turn leads to severe socio-economic and environmental damages in tropical and subtropical regions. To mitigate these negative effects and guarantee sustainable management of natural resources, gullies must be prevented. Gully management strategies start by devising adequate assessment tools and identification of driving factors and control measures. To achieve this, machine learning methods are essential tools to assist in the identification of driving factors to implement site-specific control measures. This study aimed at assessing the effectiveness of four machine learning methods (Random Forest (RF), Maximum of Entropy (MaxEnt), Artificial Neural Network (ANN), and Boosted Regression Tree (BRT)) to identify gully's driving factors, and predict gully erosion susceptibility in the Luzinzi watershed, in Walungu territory, eastern Democratic Republic of the Congo (DRC). In this study, gullies were first identified through multiple field surveys and then digitized using a very high-resolution image (CNI/airbus) from Google Earth. Overall, 270 gullies were identified, of which 70% (189) were randomly selected to train the four machine learning methods using topographical, hydrological, and environmental factors hypothesized to be gully-related conditioning factors. The remaining 30% (81 gullies) were used for testing studied methods using the threshold-independent area under the receiver operating characteristic (AUROC) and the true skill statistic (TSS) as performance measures. The results showed that RF and MaxEnt algorithms outperformed other methods; performance assessment results showed that the RF model with AUROC = 0.82 (82%) and MaxEnt (0.804: 80.4%) had higher prediction accuracies than BRT: 0.69 (69%) and ANN: 0.55 (55%). TSS results indicated that RF and MaxEnt are best methods in predicting gully susceptibility in Luzinzi watershed. On the other hand, the conditioning factors such as Digital Elevation Model (DEM), Normalized Difference Water Index (NDWI), Normalized Difference Vegetation Index (NDVI), slope, distance to roads, distance to rivers, and Stream Power Index (SPI) played key roles in the gully occurrence. Given the significance of these factors in gullies' occurrence, as shown in this study, policy-makers must adopt strategies that consider these factors to lower the risk of gully occurrence and related consequences at the watershed scale in eastern DRC.",
        "DOI": "10.1016/j.pce.2022.103295",
        "affiliation_name": "Université de Kinshasa",
        "affiliation_city": "Kinshasa",
        "affiliation_country": "Democratic Republic Congo"
    },
    {
        "paper_title": "Planning and Monitoring Equitable Clinical Trial Enrollment Using Goal Programming",
        "paper_author": "Qi M.",
        "publication": "IEEE Journal of Biomedical and Health Informatics",
        "citied_by": "4",
        "cover_date": "2023-02-01",
        "Abstract": "Randomized clinical trial (RCT) studies are the gold standard for scientific evidence on treatment benefits to patients. RCT outcomes may not be generalizable to clinical practice if the trial population is not representative of the patients for which the treatment is intended. Specifically, enrollment plans may not adequately include groups of patients with protected attributes, such as gender, race, or ethnicity. Inequities in RCTs are a major concern for funding agencies such as the National Institutes of Health (NIH) and for policy makers. We address this challenge by proposing a goal-programming approach, explicitly integrating measurable enrollment goals, to design equitable enrollment plans for RCTs. We evaluate our model in both single and multisite settings using the enrollment criteria and study population from the Systolic Blood Pressure Intervention Trial (SPRINT) study. Our model can successfully generate equitable enrollment plans that satisfy multiple goals such as sample representativeness and minimum total financial cost. Our model can detect deviations from a target plan during the enrollment process and update the plan to reduce deviations in the remaining process. Finally, through appropriate site selection in the planning stage, the model can demonstrate the possibility of enrolling a nationally representative study population if geographic constraints exist in multisite recruitment (e.g., clinical centers in a particular region). Our model can be used to prospectively produce and retrospectively evaluate how equitable enrollment plans are based on subjects' protected attributes, and it allows researchers to provide justifications on validity of scientific analysis and evaluation of subgroup disparities.",
        "DOI": "10.1109/JBHI.2022.3219283",
        "affiliation_name": "Department of Computer Science",
        "affiliation_city": "Troy",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "A review of antibiotics and antibiotic resistance genes (ARGs) adsorption by biochar and modified biochar in water",
        "paper_author": "Du L.",
        "publication": "Science of the Total Environment",
        "citied_by": "155",
        "cover_date": "2023-02-01",
        "Abstract": "Antibiotics have been used in massive quantities for human and animal medical treatment, and antibiotic resistance genes (ARGs) are of great concern worldwide. Antibiotics and ARGs are exposed to the natural environment through the discharge of medical wastewater, causing great harm to the environment and human health. Biochar has been widely used as a green and efficient adsorbent to remove pollutants. However, pristine and unmodified biochars are not considered sufficient and efficient to cope with the current serious water pollution. Therefore, researchers have chosen to improve the adsorption capacity of biochar through different modification methods. To have a better understanding of the application of modified biochar, this review summarizes the biochar modification methods and their performance, particularly, molecular imprinting and biochar aging are outlined as new modification methods, influencing factors of biochar and modified biochar in adsorption of antibiotics and ARGs and adsorption mechanisms, wherein adsorption mechanism of ARGs on biochar is found to be different than that of antibiotics. After that, the directions of biochar and modified biochar worthy of research and the issues that need attention are proposed. It can be noted that under the current dual carbon policy, biochar may have wider application prospects in future.",
        "DOI": "10.1016/j.scitotenv.2022.159815",
        "affiliation_name": "Nankai University",
        "affiliation_city": "Tianjin",
        "affiliation_country": "China"
    },
    {
        "paper_title": "An SMDP approach for Reinforcement Learning in HPC cluster schedulers",
        "paper_author": "de Freitas Cunha R.L.",
        "publication": "Future Generation Computer Systems",
        "citied_by": "2",
        "cover_date": "2023-02-01",
        "Abstract": "Deep reinforcement learning applied to computing systems has shown potential for improving system performance, as well as faster discovery of better allocation strategies. In this paper, we map HPC batch job scheduling to the SMDP formalism, and present an online, deep reinforcement learning-based solution that uses a modification of the Proximal Policy Optimization algorithm for minimizing job slowdown with action masking, supporting large action spaces. In our experiments, we assess the effects of noise in run time estimates in our model, evaluating how it behaves in small (64 processors) and large (16384 processors) clusters. We also show our model is robust to changes in workload and in cluster sizes, showing transfer works with changes of cluster size of up to 10×, and changes from synthetic workload generators to supercomputing workload traces. In our experiments, the proposed model outperforms learning models from the literature and classic heuristics, making it a viable modeling approach for robust, transferable, learning scheduling models.",
        "DOI": "10.1016/j.future.2022.09.025",
        "affiliation_name": "Microsoft Corporation",
        "affiliation_city": "Redmond",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Artificial intelligence in spine surgery",
        "paper_author": "Benzakour A.",
        "publication": "International Orthopaedics",
        "citied_by": "34",
        "cover_date": "2023-02-01",
        "Abstract": "The continuous progress of research and clinical trials has offered a wide variety of information concerning the spine and the treatment of the different spinal pathologies that may occur. Planning the best therapy for each patient could be a very difficult and challenging task as it often requires thorough processing of the patient’s history and individual characteristics by the clinician. Clinicians and researchers also face problems when it comes to data availability due to patients’ personal information protection policies. Artificial intelligence refers to the reproduction of human intelligence via special programs and computers that are trained in a way that simulates human cognitive functions. Artificial intelligence implementations to daily clinical practice such as surgical robots that facilitate spine surgery and reduce radiation dosage to medical staff, special algorithms that can predict the possible outcomes of conservative versus surgical treatment in patients with low back pain and disk herniations, and systems that create artificial populations with great resemblance and similar characteristics to real patients are considered to be a novel breakthrough in modern medicine. To enhance the body of the related literature and inform the readers on the clinical applications of artificial intelligence, we performed this review to discuss the contribution of artificial intelligence in spine surgery and pathology.",
        "DOI": "10.1007/s00264-022-05517-8",
        "affiliation_name": "Palestine Polytechnic University",
        "affiliation_city": "Hebron",
        "affiliation_country": "Palestine"
    },
    {
        "paper_title": "Anti-pandemic restrictions, uncertainty and sentiment in seven countries",
        "paper_author": "Charemza W.",
        "publication": "Economic Change and Restructuring",
        "citied_by": "4",
        "cover_date": "2023-02-01",
        "Abstract": "We investigate how the stringency of government anti-pandemic policy measures might affect economic policy uncertainty in countries with different degrees of press freedom, various press reporting styles and writing conventions. We apply a text-based measure of uncertainty using data from over 400,000 press articles from Belarus, Kazakhstan, Poland, Russia, Ukraine, the UK and the USA published before the wide-scale vaccination programmes were introduced. The measure accounts for pandemic-related words and negative sentiment scores weight the selected articles. We then tested the dynamic panel data model where the relative changes in these measures were explained by levels and changes in the stringency measures. We have found that introducing and then maintaining unchanged for a relatively long time a constant level of anti-pandemic stringency measures reduce uncertainty. In contrast, a change in such a level has the opposite effect. This result is robust across the countries, despite their differences in political systems, press control and freedom of speech.",
        "DOI": "10.1007/s10644-022-09447-8",
        "affiliation_name": "Vistula University",
        "affiliation_city": "Warsaw",
        "affiliation_country": "Poland"
    },
    {
        "paper_title": "Ability of machine learning models to identify preferred habitat traits of a small indigenous fish (Chanda nama) in a large river of peninsular India",
        "paper_author": "Raman R.K.",
        "publication": "Environmental Science and Pollution Research",
        "citied_by": "2",
        "cover_date": "2023-02-01",
        "Abstract": "Physical and chemical parameters of river influence the habitat of fish species in aquatic ecosystems. Fish showed a complex relationship with different aquatic factors in river. Machine learning modeling is a useful tool to identify relationships between components of a complex environmental system. We identified the preferred habitat indicators of Chanda nama (a small indigenous fish), in the Krishna River located in peninsular India, using machine learning modeling. Using data on Chanda nama fish distribution (presence/absence) and associated ten physical and chemical parameters of water at 22 sampling sites of the river collected during the year 2001–2002, machine learning models such as random forest, artificial neural network, support vector machine, and k-nearest neighbors were used for classification of Chanda nama distribution in the river. The machine learning model efficiency was evaluated using classification accuracy, Cohen’s kappa coefficient, sensitivity, specificity, and receiver-operating-characteristics. Results showed that random forest is the best model with higher classification accuracy (82%), Cohen’s kappa coefficient (0.55), sensitivity (0.57), specificity (0.76), and receiver-operating-characteristics (0.72) for prediction of the occurrence of Chanda nama in the Krishna River. Random forest model identified three preferred physicochemical habitat traits such as altitude, temperature, and depth for Chanda nama distribution in Krishna River. Our results will be helpful for researcher and policy maker to understand important physical and chemical variables for sustainable management of a small indigenous fish (Chanda nama) in a large tropical river.",
        "DOI": "10.1007/s11356-022-23396-9",
        "affiliation_name": "ICAR - Research Complex for Eastern Region, Patna",
        "affiliation_city": "Patna",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Making Sense of Multimorbidity in Rheumatoid Arthritis",
        "paper_author": "Barber C.E.H.",
        "publication": "Arthritis Care and Research",
        "citied_by": "1",
        "cover_date": "2023-02-01",
        "Abstract": "NA",
        "DOI": "10.1002/acr.24986",
        "affiliation_name": "University of Wisconsin School of Medicine and Public Health",
        "affiliation_city": "Madison",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "FMI real-time co-simulation-based machine deep learning control of HVAC systems in smart buildings: Digital-twins technology",
        "paper_author": "Mohseni S.R.",
        "publication": "Transactions of the Institute of Measurement and Control",
        "citied_by": "9",
        "cover_date": "2023-02-01",
        "Abstract": "As heating, ventilation, and air conditioning (HVAC) systems have become one of the most contributing systems in energy consumption in the world, the control of these large-scale systems remains a challenging duty due to the decoupling effects of control variables. Accordingly, the penetration of these types of systems in all-smart buildings has increased in recent years. Furthermore, the application of digital twin as a fast-growing concept is being developed. In HVAC systems, independent and accurate control of temperature and humidity of the indoor air has been playing an undeniable role in reducing energy consumption. In this paper, to have cost-effective energy management in a single-zone HVAC system, a new reliable digital twin proximal policy optimization (PPO)–based model-independent nonsingular terminal sliding-mode control (MINTSMC) methodology has been proposed. Moreover, due to the nonlinear characteristics of HVAC systems, MINTSMC tends to handle the un-modeled system dynamics and disturbances. For regulating parameters of proposed control, an efficient PPO algorithm has been developed due to its actor-critic-based reinforcement learning. Extensive examinations and comparative analyses with particle swarm optimization designed sliding-mode control and proportional–integral–derivative controller have been made using digital twin of the proposed controller to show the importance, accuracy, and application of this method in the comfort and energy management achievement of HVAC control systems. A digital signal processor computing device has been utilized for implementation by utilizing hardware-in-loop (HIL) in the concept of the digital twin. To determine the interface between established models, software-in-loop, and HIL, the Functional Mock-up Interface has been utilized. The outcomes revealed a superior performance of suggested digital twin-based controller than the compared control methodologies in the compensation of unknown uncertainties, fast-tracking, and smooth response.",
        "DOI": "10.1177/01423312221119635",
        "affiliation_name": "Shahid Bahonar University of Kerman",
        "affiliation_city": "Kerman",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "Online machine learning modeling and predictive control of nonlinear systems with scheduled mode transitions",
        "paper_author": "Hu C.",
        "publication": "AIChE Journal",
        "citied_by": "20",
        "cover_date": "2023-02-01",
        "Abstract": "This work develops a model predictive control (MPC) scheme using online learning of recurrent neural network (RNN) models for nonlinear systems switched between multiple operating regions following a prescribed switching schedule. Specifically, an RNN model is initially developed offline to model process dynamics using the historical operational data collected in a small region around a certain steady-state. After the system is switched to another operating region under a Lyapunov-based MPC with suitable constraints to ensure satisfaction of the prescribed switching schedule policy, RNN models are updated using real-time process data to improve closed-loop performance. A generalization error bound is derived for the updated RNN models using the notion of regret, and closed-loop stability results are established for the switched nonlinear system under RNN-based MPC. Finally, a chemical process example with the operation schedule that requires switching between two steady-states is used to demonstrate the effectiveness of the proposed RNN-MPC scheme.",
        "DOI": "10.1002/aic.17882",
        "affiliation_name": "National University of Singapore",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore"
    },
    {
        "paper_title": "Weibull recurrent neural networks for failure prognosis using histogram data",
        "paper_author": "Dhada M.",
        "publication": "Neural Computing and Applications",
        "citied_by": "2",
        "cover_date": "2023-02-01",
        "Abstract": "Weibull time-to-event recurrent neural networks (WTTE-RNN) is a simple and versatile prognosis algorithm that works by optimising a Weibull survival function using a recurrent neural network. It offers the combined benefits of the sequential nature of the recurrent neural network, and the ability of the Weibull loss function to incorporate censored data. The goal of this paper is to present the first industrial use case of WTTE-RNN for prognosis. Prognosis of turbocharger conditions in a fleet of heavy-duty trucks is presented here, where the condition data used in the case study were recorded as a time series of sparsely sampled histograms. The experiments include comparison of the prediction models trained using data from the entire fleet of trucks vs data from clustered sub-fleets, where it is concluded that clustering is only beneficial as long as the training dataset is large enough for the model to not overfit. Moreover, the censored data from assets that did not fail are also shown to be incorporated while optimising the Weibull loss function and improve prediction performance. Overall, this paper concludes that WTTE-RNN-based failure predictions enable predictive maintenance policies, which are enhanced by identifying the sub-fleets of similar trucks.",
        "DOI": "10.1007/s00521-022-07667-7",
        "affiliation_name": "Department of Engineering",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Sustainable Development Goal for Quality Education (SDG 4): A study on SDG 4 to extract the pattern of association among the indicators of SDG 4 employing a genetic algorithm",
        "paper_author": "Saini M.",
        "publication": "Education and Information Technologies",
        "citied_by": "58",
        "cover_date": "2023-02-01",
        "Abstract": "Sustainable Development Goals (SDG) are at the forefront of government initiatives across the world. The SDGs are primarily concerned with promoting sustainable growth via ensuring wellbeing, economic growth, environmental legislation, and academic advancement. One of the most prominent goals of the SDG is to provide learners with high-quality education (SDG 4). This paper aims to look at the perspectives of the Sustainable Development Goals improvised to provide quality education. We also analyze the existing state of multiple initiatives implemented by the Indian government in the pathway to achieving objectives of quality education (SDG 4). Additionally, a case study is considered for understanding the association among the observed indicators of SDG4. For this purpose, exploratory data analysis, and numerical association rule mining in combination with QuantMiner genetic algorithm approaches have been applied. The outcomes reveal the presence of a significant degree of association among these parameters pointing out the fact that understanding the impact of one (or more) indicator on other related indicators is critical for achieving SDG 4 goals (or factors). These findings will assist governing bodies in taking preventive measures while modifying existing policies and ensuring the effective enactment of SDG 4 goals, which also will subsequently aid in the resolution of issues related to other SDGs.",
        "DOI": "10.1007/s10639-022-11265-4",
        "affiliation_name": "Guru Nanak Dev University",
        "affiliation_city": "Amritsar",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Curricular Robust Reinforcement Learning via GAN-Based Perturbation Through Continuously Scheduled Task Sequence",
        "paper_author": "Li Y.",
        "publication": "Tsinghua Science and Technology",
        "citied_by": "4",
        "cover_date": "2023-02-01",
        "Abstract": "Reinforcement learning (RL), one of three branches of machine learning, aims for autonomous learning and is now greatly driving the artificial intelligence development, especially in autonomous distributed systems, such as cooperative Boston Dynamics robots. However, robust RL has been a challenging problem of reliable aspects due to the gap between laboratory simulation and real world. Existing efforts have been made to approach this problem, such as performing random environmental perturbations in the learning process. However, one cannot guarantee to train with a positive perturbation as bad ones might bring failures to RL. In this work, we treat robust RL as a multi-task RL problem, and propose a curricular robust RL approach. We first present a generative adversarial network (GAN) based task generation model to iteratively output new tasks at the appropriate level of difficulty for the current policy. Furthermore, with these progressive tasks, we can realize curricular learning and finally obtain a robust policy. Extensive experiments in multiple environments demonstrate that our method improves the training stability and is robust to differences in training/test conditions.",
        "DOI": "10.26599/TST.2021.9010076",
        "affiliation_name": "Beijing Jiaotong University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "GRADES: Gradient Descent for Similarity Caching",
        "paper_author": "Sabnis A.",
        "publication": "IEEE/ACM Transactions on Networking",
        "citied_by": "1",
        "cover_date": "2023-02-01",
        "Abstract": "A similarity cache can reply to a query for an object with similar objects stored locally. In some applications of similarity caches, queries and objects are naturally represented as points in a continuous space. This is for example the case of 360° videos where user's head orientation - expressed in spherical coordinates - determines what part of the video needs to be retrieved, or of recommendation systems where a metric learning technique is used to embed the objects in a finite dimensional space with an opportune distance to capture content dissimilarity. Existing similarity caching policies are simple modifications of classic policies like LRU, LFU, and q LRU and ignore the continuous nature of the space where objects are embedded. In this paper, we propose GRADES, a new similarity caching policy that uses gradient descent to navigate the continuous space and find appropriate objects to store in the cache. We provide theoretical convergence guarantees and show GRADES increases the similarity of the objects served by the cache in both applications mentioned above.",
        "DOI": "10.1109/TNET.2022.3187044",
        "affiliation_name": "Manning College of Information &amp; Computer Sciences",
        "affiliation_city": "Amherst",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Integration of Reinforcement Learning in a Virtual Robotic Surgical Simulation",
        "paper_author": "Bourdillon A.T.",
        "publication": "Surgical Innovation",
        "citied_by": "7",
        "cover_date": "2023-02-01",
        "Abstract": "Background. The revolutions in AI hold tremendous capacity to augment human achievements in surgery, but robust integration of deep learning algorithms with high-fidelity surgical simulation remains a challenge. We present a novel application of reinforcement learning (RL) for automating surgical maneuvers in a graphical simulation. Methods. In the Unity3D game engine, the Machine Learning-Agents package was integrated with the NVIDIA FleX particle simulator for developing autonomously behaving RL-trained scissors. Proximal Policy Optimization (PPO) was used to reward movements and desired behavior such as movement along desired trajectory and optimized cutting maneuvers along the deformable tissue-like object. Constant and proportional reward functions were tested, and TensorFlow analytics was used to informed hyperparameter tuning and evaluate performance. Results. RL-trained scissors reliably manipulated the rendered tissue that was simulated with soft-tissue properties. A desirable trajectory of the autonomously behaving scissors was achieved along 1 axis. Proportional rewards performed better compared to constant rewards. Cumulative reward and PPO metrics did not consistently improve across RL-trained scissors in the setting for movement across 2 axes (horizontal and depth). Conclusion. Game engines hold promising potential for the design and implementation of RL-based solutions to simulated surgical subtasks. Task completion was sufficiently achieved in one-dimensional movement in simulations with and without tissue-rendering. Further work is needed to optimize network architecture and parameter tuning for increasing complexity.",
        "DOI": "10.1177/15533506221095298",
        "affiliation_name": "Department of Bioengineering",
        "affiliation_city": "Stanford",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Learning-Based Synthesis of Robust Linear Time-Invariant Controllers",
        "paper_author": "Beaudoin M.A.",
        "publication": "IEEE Transactions on Intelligent Vehicles",
        "citied_by": "0",
        "cover_date": "2023-02-01",
        "Abstract": "Recent advances in learning for control allow to synthesize vehicle controllers from learned system dynamics and maintain robust stability guarantees. However, no approach is well-suited for training robustly-stabilizing linear time-invariant (LTI) controllers using arbitrary learned models of the dynamics. This article introduces a method to do so. It uses a robust control framework to derive robust stability criteria. It also uses simulated policy rollouts to obtain gradients on the controller parameters, which serve to improve the closed-loop performance. By formulating the stability criteria as penalties with computable gradients, they can be used to guide the controller parameters toward robust stability during gradient descent. The approach is flexible as it does not restrict the type of learned model for the simulated rollouts. The robust control framework ensures that the controller is already robustly stabilizing when first implemented on the actual system and no data is yet collected. It also ensures that the system stays stable in the event of a shift in dynamics, given the system behavior remains within assumed uncertainty bounds. We demonstrate the approach by synthesizing a controller for simulated autonomous lane-change maneuvers. This work thus presents a flexible approach to learning robustly stabilizing LTI controllers that takes advantage of modern machine learning techniques.",
        "DOI": "10.1109/TIV.2022.3174029",
        "affiliation_name": "Université McGill",
        "affiliation_city": "Montreal",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Heavy-Head Sampling for Fast Imitation Learning of Machine Learning Based Combinatorial Auction Solver",
        "paper_author": "Peng C.",
        "publication": "Neural Processing Letters",
        "citied_by": "11",
        "cover_date": "2023-02-01",
        "Abstract": "The winner determination problem of a combinatorial auction can be modeled as mixed-integer linear programming, and is a popular benchmark to evaluate modern solvers. Recent advancements in combinatorial optimization improve the branch-and-bound solving process by replacing the time-consuming heuristics with machine learning models. In this paper, by taking advantage of the heavy-head maximum depth distribution of the branch-and-bound solution trees, a heavy-head sampling strategy is proposed for the imitation learning on the combinatorial auction problems. Experimental results show that, under the small-dataset fast-training scheme and using the heavy-head sampling strategy, the final evaluation results of the trained policy on the combinatorial auction problems are improved significantly (in the sense of statistical testing), compared to using the uniform sampling strategy in previous studies.",
        "DOI": "10.1007/s11063-022-10900-y",
        "affiliation_name": "Jishou University",
        "affiliation_city": "Jishou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Supervised Learning Enhanced Quantum Circuit Transformation",
        "paper_author": "Zhou X.",
        "publication": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
        "citied_by": "5",
        "cover_date": "2023-02-01",
        "Abstract": "A quantum circuit transformation (QCT) is required when executing a quantum program in a real quantum processing unit (QPU). By inserting auxiliary SWAP gates, a QCT algorithm transforms a quantum circuit to one that satisfies the connectivity constraint imposed by the QPU. Due to the nonnegligible gate error and the limited qubit coherence time of the QPU, QCT algorithms that minimize gate number or circuit depth or maximize the fidelity of output circuits are in urgent need. Unfortunately, finding optimized transformations often involve exhaustive searches, which are extremely time consuming and not practical for most circuits. In this article, we propose a framework that uses a policy artificial neural network (ANN) trained by supervised learning on shallow circuits to help existing QCT algorithms select the most promising SWAP gate. ANNs can be trained offline in a distributed way and the trained ANN can be easily incorporated into QCT algorithms to enable them to search deeper without bringing too much overhead in time complexity. Exemplary embeddings of the trained ANNs into target QCT algorithms demonstrate that the transformation performance can be consistently improved on QPUs with various connectivity structures and random or realistic quantum circuits.",
        "DOI": "10.1109/TCAD.2022.3179223",
        "affiliation_name": "University of Technology Sydney",
        "affiliation_city": "Sydney",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Coordinated Electric Vehicle Active and Reactive Power Control for Active Distribution Networks",
        "paper_author": "Wang Y.",
        "publication": "IEEE Transactions on Industrial Informatics",
        "citied_by": "47",
        "cover_date": "2023-02-01",
        "Abstract": "The deployment of renewable energy in power systems may raise serious voltage instabilities. Electric vehicles (EVs), owing to their mobility and flexibility characteristics, can provide various ancillary services including active and reactive power. However, the distributed control of EVs under such scenarios is a complex decision-making problem with enormous dynamics and uncertainties. Most existing literature employs model-based approaches to formulate active and reactive power control problems, which require full models and are time-consuming. This article proposes a multiagent reinforcement learning algorithm featuring a deep deterministic policy gradient (DDPG) method and a parameter sharing framework to solve the EVs' coordinated active and reactive power control problem toward both demand-side response and voltage regulations. The proposed algorithm can further enhance the learning stability and scalability with privacy perseverance via the location marginal prices. Simulation results based on a modified IEEE 15-bus network are developed to validate its effectiveness in providing system charging and voltage regulation services. The proposed location marginal price (LMP) PSDDPG algorithm is evaluated to achieve 38%, 16%, and 25% speedup, and 1.58, 0.69, and 0.27 times higher reward over the benchmarks DDPG, TD3, and LMP-DDPG, respectively.",
        "DOI": "10.1109/TII.2022.3169975",
        "affiliation_name": "Imperial College London",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Predicting socioeconomic indicators using transfer learning on imagery data: an application in Brazil",
        "paper_author": "Castro D.A.",
        "publication": "GeoJournal",
        "citied_by": "12",
        "cover_date": "2023-02-01",
        "Abstract": "Censuses and other surveys responsible for gathering socioeconomic data are expensive and time consuming. For this reason, in poor and developing countries there often is a long gap between these surveys, which hinders the appropriate formulation of public policies as well as the development of researches. One possible approach to overcome this challenge for some socioeconomic indicators is to use satellite imagery to estimate these variables, although it is not possible to replace demographic census surveys completely due to its territorial coverage, level of disaggregation of information and large set of information. Even though using orbital images properly requires, at least, a basic remote sensing knowledge level, these images have the advantage of being commonly free and easy to access. In this paper, we use daytime and nighttime satellite imagery and apply a transfer learning technique to estimate average income, GDP per capita and a constructed water index at the city level in two Brazilian states, Bahia and Rio Grande do Sul. The transfer learning approach could explain up to 64% of the variation in city-level variables depending on the state and variable. Although data from different countries may be considerably different, results are consistent with the literature and encouraging as it is a first analysis of its kind for Brazil.",
        "DOI": "10.1007/s10708-022-10618-3",
        "affiliation_name": "The University of Sheffield",
        "affiliation_city": "Sheffield",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Offsetting Unequal Competition Through RL-Assisted Incentive Schemes",
        "paper_author": "Koley P.",
        "publication": "IEEE Transactions on Computational Social Systems",
        "citied_by": "0",
        "cover_date": "2023-02-01",
        "Abstract": "This article investigates the dynamics of competition among organizations with unequal expertise. Multiagent reinforcement learning (MARL) has been used to simulate and understand the impact of various incentive schemes designed to offset such inequality. We design Touch-Mark, a game based on well-known multiagent particle environment, where two teams (weak and strong) with unequal but changing skill levels compete against each other. For training such a game, we propose a novel controller-assisted MARL algorithm C-MADDPG, which empowers each agent with an ensemble of policies along with a supervised controller that by selectively partitioning the sample space and triggers intelligent role division among the teammates. Using C-MADDPG as an underlying framework, we propose an incentive scheme for the weak team such that the final rewards of both teams become the same. We find that despite the incentive, the final reward of the weak team falls short of the strong team. On inspecting, we realize that an overall incentive scheme for the weak team does not incentivize the weaker agents within that team to learn and improve. To offset this, we now specially incentivize the weaker player to learn and, as a result, observe that the weak team beyond an initial phase performs at par with the stronger team. The final goal of this article has been to formulate a dynamic incentive scheme that continuously balances the reward of the two teams. This is achieved by devising an incentive scheme enriched with an RL agent, which takes minimum information from the environment.",
        "DOI": "10.1109/TCSS.2022.3140659",
        "affiliation_name": "ADOBE Systems India Private Limited",
        "affiliation_city": "Noida",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Continuous Reinforcement Learning-Based Energy Management Strategy for Hybrid Electric-Tracked Vehicles",
        "paper_author": "Han R.",
        "publication": "IEEE Journal of Emerging and Selected Topics in Power Electronics",
        "citied_by": "30",
        "cover_date": "2023-02-01",
        "Abstract": "The hybrid electric-tracked vehicles (HETVs) are usually used in both agricultural and industrial applications, while the optimal energy management is critical to fully exploit the potential of HETVs. In this article, the influence of HETVs' steering resistance on the energy distribution is specially considered to model the dynamic demand accurately. Further, a multi-state energy management strategy (EMS) based on deep deterministic policy gradient (DDPG) is proposed for a series HETV in the continuous space. A multidimensional matrix framework is proposed to extract the parameters of the actor network from a trained DDPG-based EMS. Hardware-in-the-loop (HiL) experiment is conducted to validate the real-time tractability of the proposed strategy. Results suggest that the DDPG-based strategy improves the fuel economy remarkably by 13.1% and shows a more robust performance, compared with the double deep $Q$ -learning-based strategy. Though the proposed strategy is trained based on the fixed state of charge (SOC), it still exhibits a strong adaptability to the uncertainty of initial SOCs.",
        "DOI": "10.1109/JESTPE.2021.3135059",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Reinforcement Learning From Hierarchical Critics",
        "paper_author": "Cao Z.",
        "publication": "IEEE Transactions on Neural Networks and Learning Systems",
        "citied_by": "4",
        "cover_date": "2023-02-01",
        "Abstract": "In this study, we investigate the use of global information to speed up the learning process and increase the cumulative rewards of reinforcement learning (RL) in competition tasks. Within the framework of actor-critic RL, we introduce multiple cooperative critics from two levels of a hierarchy and propose an RL from the hierarchical critics (RLHC) algorithm. In our approach, each agent receives value information from local and global critics regarding a competition task and accesses multiple cooperative critics in a top-down hierarchy. Thus, each agent not only receives low-level details, but also considers coordination from higher levels, thereby obtaining global information to improve the training performance. Then, we test the proposed RLHC algorithm against a benchmark algorithm, that is, proximal policy optimization (PPO), under four experimental scenarios consisting of tennis, soccer, banana collection, and crawler competitions within the Unity environment. The results show that RLHC outperforms the benchmark on these four competitive tasks.",
        "DOI": "10.1109/TNNLS.2021.3103642",
        "affiliation_name": "University of South Australia",
        "affiliation_city": "Adelaide",
        "affiliation_country": "Australia"
    },
    {
        "paper_title": "Multi-agent reinforcement learning based textile dyeing workshop dynamic scheduling method",
        "paper_author": "He J.",
        "publication": "Jisuanji Jicheng Zhizao Xitong/Computer Integrated Manufacturing Systems, CIMS",
        "citied_by": "3",
        "cover_date": "2023-01-31",
        "Abstract": "Aiming at the dynamic scheduling problem of textile dyeing workshop in which tasks release dynamically by orders, a Multi-Agent Recurrent Proximal Policy Optimization (MA-RPPO) reinforcement learning(RL) based fully reactive scheduling method was proposed by taking the minimum total tardiness time as the optimization goal. For the two sub-problems of group batching and the vats scheduling in the dyeing workshop, the batching agent and the scheduling agent were designed to group batches and schedule the vats. For the dynamics of dyeing tasks, Long Short Term Memory (LSTM) was introduced to extract workshop dynamic information and improve the adaptive a-bility of the agent; further the interaction mechanism between agents was proposed to achieve global optimization of two sub-problems. The relevant features of constraints and optimization goal were extracted, and the reward function was designed. The agents interacted with the dyeing workshop environment through dynamic scheduling mechanisms to learn the optimal scheduling strategy. The case study from a dyeing enterprise showed that the proposed method was better than some high-performance heuristic rules in different scales of problems, reducing the total tardiness time of products and improving the ability of timely delivery of enterprise effectively.",
        "DOI": "10.13196/j.cims.2023.01.006",
        "affiliation_name": "Shanghai Jiao Tong University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Designing A Model for Fake News Detection in Social Media Using Machine Learning Techniques",
        "paper_author": "Shalini A.K.",
        "publication": "International Journal of Intelligent Systems and Applications in Engineering",
        "citied_by": "7",
        "cover_date": "2023-01-27",
        "Abstract": "Fake identity is a critical problem nowadays of social media. Fake news is rapidly spread by fake identities and bots that generate the trustworthiness issue on social media. Identifying the profiles and accounts using soft computing algorithms is necessary to improve the trustworthiness of social media. In this work, we proposed Recurrent Neural Network to identify fake identities on social media. Initially, we extract data from social media such as Twitter.com using Twitter API. Hybrid feature extraction has been done based on the characteristics of data. It generates training rules which are associated with a fake and legitimate profiles generated by a human. In pre-processing and filtration process, all bot entries are eliminated using a policy-based approach. To generate strict rules that improve the classification accuracy, the training of dataset primarily focuses on attributes such as friends count, the total number of followers, tweet counts, re-tweets count, etc., The Recurrent Neural Network (RNN) categorizes each profile based on training and testing modules. This work focuses on classifying bots or human entries according to their extracted features using machine learning. Once the training phase is completed, features are extracted from the dataset based on the term frequency on which the classification technique is applied. The proposed work is very effective in detecting malicious accounts from an imbalanced dataset in social media. The system provides maximum accuracy for the classification of fake and real identities on the social media dataset. It achieves good accuracy with Recurrent Neural Network (RNN) using the different activation functions. The system improves the classification accuracy with the increase in the number of folds in cross-validation. In experiment analysis, we have done testing on synthetic and real-time social media datasets; We achieve around 96% accuracy on the real-time Twitter dataset while 98% accuracy on synthetic social media datasets.",
        "DOI": "NA",
        "affiliation_name": "Sanjay Ghodawat University, Kolhapur",
        "affiliation_city": "Kolhapur",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Corrosion Detection and Prediction for Underwater pipelines using IoT and Machine Learning Techniques",
        "paper_author": "Parjane V.A.",
        "publication": "International Journal of Intelligent Systems and Applications in Engineering",
        "citied_by": "6",
        "cover_date": "2023-01-27",
        "Abstract": "Pipelines are commonly utilized to transmit chemical fluids over thousands of kilometres all over the globe. The pipes are designed to withstand a variety of environmental loading conditions, providing safe and durable delivery from the manufacturing location to the coast or distribution station. Leaks in piping systems, on the other side, are among the primary causes of numerous damages for pipeline operators and the surroundings. Pipeline failures may cause significant environmental catastrophes, human deaths, and financial losses. Significant research has been devoted to corrosion and localization using alternative strategies to avoid this threat and preserve an efficient and proper transmission infrastructure. This paper proposed a corrosion detection and prediction system using Internet of Things (IoT) and machine learning techniques. The system collaborates with two different methodologies, such as IoT utilized to collect data from underwater pipelines and various learning algorithms to identify corrosion possibilities. We have used analogue sensors such as thickness, GPS, pH, etc., to capture the current event. Based on pH value, impact of pipe thickness for a specific period has been analysed depending on learning algorithm. The standard defines policy rules and has used a semi-supervised learning algorithm for validation. The Q-learning based classification algorithm generates reward and penalty for each event and, based on that, defines the possibility of corrosion. A variety of extraction of features and selection methods were used during this research using the IoT model. An extensive experiment analysis of the proposed algorithm obtains higher classification and detection accuracy over the traditional machine learning classification algorithms.",
        "DOI": "NA",
        "affiliation_name": "B. N College of Engineering &amp; Technology",
        "affiliation_city": "Lucknow",
        "affiliation_country": "India"
    },
    {
        "paper_title": "The built environment’s nonlinear effects on the elderly’s propensity to walk",
        "paper_author": "Zang P.",
        "publication": "Frontiers in Ecology and Evolution",
        "citied_by": "6",
        "cover_date": "2023-01-26",
        "Abstract": "The increased ageing of the population is a vital and upcoming challenge for China. Walking is one of the easiest and most common forms of exercise for older people, and promoting walking among older people is important for reducing medical stress. Streetscape green visibility and the normalised difference vegetation index (NDVI) are perceptible architectural elements, both of which promote walking behaviour. Methodologically we used Baidu Street View images and extracted NDVI from streetscape green visibility and remote sensing to scrutinize the nonlinear effects of streetscape green visibility and NDVI on older people’s walking behaviour. The study adopted a random forest machine learning model. The findings indicate that the impact of streetscape green visibility on elderly walking is superior to NDVI, while both have a favourable influence on senior walking propensity within a particular range but a negative effect on elderly walking inside that range. Overall the built environment had a non-linear effect on the propensity to walk of older people. Therefore, this study allows the calculation of optimal thresholds for the physical environment, which can be used by governments and planners to formulate policies and select appropriate environmental thresholds as indicators to update or build a community walking environment that meets the needs of local older people, depending on their own economic situation.",
        "DOI": "10.3389/fevo.2023.1103140",
        "affiliation_name": "Guangdong University of Technology",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Remote sensing of hedgerows, windbreaks, and winter cover crops in California's Central Coast reveals low adoption but hotspots of use",
        "paper_author": "Thompson J.B.",
        "publication": "Frontiers in Sustainable Food Systems",
        "citied_by": "3",
        "cover_date": "2023-01-25",
        "Abstract": "Non-crop vegetation, such as hedgerows and cover crops, are important on-farm diversification practices that support biodiversity and ecosystem services; however, information about their rates and patterns of adoption are scarce. We used satellite and aerial imagery coupled with machine learning classification to map the use of hedgerows/windbreaks and winter cover crops in California's Central Coast, a globally important agricultural area of intensive fresh produce production. We expected that adoption of both practices would be relatively low and unevenly distributed across the landscape, with higher levels of adoption found in marginal farmland and in less intensively cultivated areas where the pressure to remove non-crop vegetation may be lower. Our remote sensing classification revealed that only ~6% of farmland had winter cover crops in 2021 and 0.26% of farmland had hedgerows or windbreaks in 2018. Thirty-seven percent of ranch parcels had cover crops on at least 5% of the ranch while 22% of ranches had at least one hedgerow/windbreak. Nearly 16% of farmland had other annual winter crops, some of which could provide services similar to cover crops; however, 60% of farmland had bare soil over the winter study period, with the remainder of farmland classified as perennial crops or strawberries. Hotspot analysis showed significant areas of adoption of both practices in the hillier regions of all counties. Finally, qualitative interviews revealed that adoption patterns were likely driven by interrelated effects of topography, land values, and farming models, with organic, diversified farms implementing these practices in less ideal, lower-value farmland. This study demonstrates how remote sensing coupled with qualitative research can be used to map and interpret patterns of important diversification practices, with implications for tracking policy interventions and targeting resources to assist farmers motivated to expand adoption.",
        "DOI": "10.3389/fsufs.2023.1052029",
        "affiliation_name": "Department of Environmental Science, Policy, and Management",
        "affiliation_city": "Berkeley",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Reinforcement-learning-based actuator selection method for active flow control",
        "paper_author": "Paris R.",
        "publication": "Journal of Fluid Mechanics",
        "citied_by": "12",
        "cover_date": "2023-01-25",
        "Abstract": "This paper addresses the issue of actuator selection for active flow control by proposing a novel method built on top of a reinforcement learning agent. Starting from a pre-trained agent using numerous actuators, the algorithm estimates the impact of a potential actuator removal on the value function, indicating the agent's performance. It is applied to two test cases, the one-dimensional Kuramoto-Sivashinsky equation and a laminar bidimensional flow around an airfoil at for different angles of attack ranging from to, to demonstrate its capabilities and limits. The proposed actuator-sparsification method relies on a sequential elimination of the least relevant action components, starting from a fully developed layout. The relevancy of each component is evaluated using metrics based on the value function. Results show that, while still being limited by this intrinsic elimination paradigm (i.e. the sequential elimination), actuator patterns and obtained policies demonstrate relevant performances and allow us to draw an accurate approximation of the Pareto front of performances versus actuator budget.",
        "DOI": "10.1017/jfm.2022.1043",
        "affiliation_name": "Université Paris-Saclay",
        "affiliation_city": "Gif-sur-Yvette",
        "affiliation_country": "France"
    },
    {
        "paper_title": "Editorial: Health technology assessment in cardiovascular diseases",
        "paper_author": "Shah K.",
        "publication": "Frontiers in Cardiovascular Medicine",
        "citied_by": "2",
        "cover_date": "2023-01-24",
        "Abstract": "NA",
        "DOI": "10.3389/fcvm.2023.1108503",
        "affiliation_name": "Indian Institute of Public Health Gandhinagar",
        "affiliation_city": "Gandhinagar",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Clinical Algorithms, Antidiscrimination Laws, and Medical Device Regulation",
        "paper_author": "Goodman K.E.",
        "publication": "JAMA",
        "citied_by": "26",
        "cover_date": "2023-01-24",
        "Abstract": "NA",
        "DOI": "10.1001/jama.2022.23870",
        "affiliation_name": "VA Medical Center",
        "affiliation_city": null,
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Hybrid method to guide sustainable initiatives in higher education: a critical analysis of Brazilian municipalities",
        "paper_author": "Paz T.d.S.R.",
        "publication": "International Journal of Sustainability in Higher Education",
        "citied_by": "9",
        "cover_date": "2023-01-24",
        "Abstract": "Purpose: This paper aims to assist higher education institutions (HEIs) in their decision-making process to define initiatives and foster research projects contributing to sustainable development (SD) and minimizing the deficits found in the municipalities. Design/methodology/approach: A documental analysis was performed to select HEIs and Brazilian regional development indicators. Then, the assessment of the sustainable and institutional performance of Brazilian municipalities that have HEIs consisted of three parts: clustering with an unsupervised machine learning model, ranking with a hybrid multi-criteria decision making method and visualization of sustainability performance with the dashboard. Findings: The critical analysis of institutional and sustainability indicators contributes to a more active role of HEIs in matters of social responsibility, with a more holistic view of the performance and quality of municipal education. Furthermore, this critical analysis creates a scenario where HEIs can develop public policy proposals in partnership with the government to mitigate the main issues identified. Social implications: With this study, HEIs will be able to direct their actions to minimize the deficits found in the municipalities, consolidating their social responsibility. Originality/value: This study proposes a new decision-support tool with a dashboard of indicators so that HEIs can foster research projects with a focus on regional SD.",
        "DOI": "10.1108/IJSHE-07-2021-0281",
        "affiliation_name": "Pontifícia Universidade Católica do Rio de Janeiro",
        "affiliation_city": "Rio de Janeiro",
        "affiliation_country": "Brazil"
    },
    {
        "paper_title": "Statistical modelling of the impact of online courses in higher education on sustainable development",
        "paper_author": "Arango-Uribe M.L.",
        "publication": "International Journal of Sustainability in Higher Education",
        "citied_by": "7",
        "cover_date": "2023-01-24",
        "Abstract": "Purpose: The concept of sustainable development (SD) is a popular response to society’s need to preserve and extend the life span of natural resources. One of the 17 goals of the SD is “education quality” (Fourth Goal of Sustainable Development [SDG-4]). Education quality is an important goal because education is a powerful force that can influence social policies and social change. The SDG-4 must be measured in different contexts, and the tools to quantify its effects require exploration. So, this study aims to propose a statistical model to measure the impact of higher education online courses on SD and a structural equation model (SEM) to find constructs or factors that help us explain a sustainability benefits rate. These proposed models integrate the three areas of sustainability: social, economic and environmental. Design/methodology/approach: A beta regression model suggests features that include the academic and economic opportunities offered by the institution, the involvement in research activities and the quality of the online courses. A structural equation modelling (SEM) analysis allowed selecting the key variables and constructs that are strongly linked to the SD. Findings: One of the key findings showed that the benefit provided by online courses in terms of SD is 62.99% higher than that of offline courses in aspects such as transportation, photocopies, printouts, books, food, clothing, enrolment fees and connectivity. Research limitations/implications: The SEM model needs large sample sizes to have consistent estimations. Thus, despite the obtained estimations in the proposed SEM model being reliable, the authors consider that a limitation of this study was the required time to collect data corresponding to the estimated sample size. Originality/value: This study proposes two novel and different ways to estimate the sustainability benefits rate focused on SDG-4, and machine learning tools are implemented to validate and gain robustness in the estimations of the beta model. Additionally, the SEM model allows us to identify new constructs associated with SDG-4.",
        "DOI": "10.1108/IJSHE-12-2021-0495",
        "affiliation_name": "Instituto Tecnológico Metropolitano",
        "affiliation_city": "Medellin",
        "affiliation_country": "Colombia"
    },
    {
        "paper_title": "An enhanced deep deterministic policy gradient algorithm for intelligent control of robotic arms",
        "paper_author": "Dong R.",
        "publication": "Frontiers in Neuroinformatics",
        "citied_by": "6",
        "cover_date": "2023-01-23",
        "Abstract": "Aiming at the poor robustness and adaptability of traditional control methods for different situations, the deep deterministic policy gradient (DDPG) algorithm is improved by designing a hybrid function that includes different rewards superimposed on each other. In addition, the experience replay mechanism of DDPG is also improved by combining priority sampling and uniform sampling to accelerate the DDPG’s convergence. Finally, it is verified in the simulation environment that the improved DDPG algorithm can achieve accurate control of the robot arm motion. The experimental results show that the improved DDPG algorithm can converge in a shorter time, and the average success rate in the robotic arm end-reaching task is as high as 91.27%. Compared with the original DDPG algorithm, it has more robust environmental adaptability.",
        "DOI": "10.3389/fninf.2023.1096053",
        "affiliation_name": "Jilin Institute of Chemical Technology",
        "affiliation_city": "Jilin",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Safe reinforcement learning for affine nonlinear systems with state constraints and input saturation using control barrier functions",
        "paper_author": "Liu S.",
        "publication": "Neurocomputing",
        "citied_by": "12",
        "cover_date": "2023-01-21",
        "Abstract": "This paper provides a novel safe reinforcement learning (RL) control algorithm to solve safe optimal problems for discrete-time affine nonlinear systems, while the safety and convergence of the control algorithm are proven. The algorithm is proposed based on an adjusted policy iteration (PI) framework using only the measured data along the system trajectories in the environment. The adjusted PI algorithm combines with the system predictive information. Unlike most PI algorithms, an effective method of obtaining an initial safe and stable control policy is given here. In addition, control barrier functions (CBFs) and an input constraint function are introduced to augment reward functions. And the monotonically nonincreasing property of the iterative value function maintains the safe set forward invariant in the PI framework. Moreover, the safety and convergence of the proposed algorithm are proven in theory. Then, the design and implementation of the proposed algorithm are presented based on the identifier-actor-critic structure, where neural networks are employed to approximate the system dynamics, the iterative control policy, and the iterative value function, respectively. Finally, the simulation results illustrate the effectiveness and safety of the proposed algorithm.",
        "DOI": "10.1016/j.neucom.2022.11.006",
        "affiliation_name": "Xiamen University",
        "affiliation_city": "Xiamen",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Toward robust and scalable deep spiking reinforcement learning",
        "paper_author": "Akl M.",
        "publication": "Frontiers in Neurorobotics",
        "citied_by": "8",
        "cover_date": "2023-01-20",
        "Abstract": "Deep reinforcement learning (DRL) combines reinforcement learning algorithms with deep neural networks (DNNs). Spiking neural networks (SNNs) have been shown to be a biologically plausible and energy efficient alternative to DNNs. Since the introduction of surrogate gradient approaches that allowed to overcome the discontinuity in the spike function, SNNs can now be trained with the backpropagation through time (BPTT) algorithm. While largely explored on supervised learning problems, little work has been done on investigating the use of SNNs as function approximators in DRL. Here we show how SNNs can be applied to different DRL algorithms like Deep Q-Network (DQN) and Twin-Delayed Deep Deteministic Policy Gradient (TD3) for discrete and continuous action space environments, respectively. We found that SNNs are sensitive to the additional hyperparameters introduced by spiking neuron models like current and voltage decay factors, firing thresholds, and that extensive hyperparameter tuning is inevitable. However, we show that increasing the simulation time of SNNs, as well as applying a two-neuron encoding to the input observations helps reduce the sensitivity to the membrane parameters. Furthermore, we show that randomizing the membrane parameters, instead of selecting uniform values for all neurons, has stabilizing effects on the training. We conclude that SNNs can be utilized for learning complex continuous control problems with state-of-the-art DRL algorithms. While the training complexity increases, the resulting SNNs can be directly executed on neuromorphic processors and potentially benefit from their high energy efficiency.",
        "DOI": "10.3389/fnbot.2022.1075647",
        "affiliation_name": "Technische Universität München",
        "affiliation_city": "Munich",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Coal-fired power plant CCUS project comprehensive benefit evaluation and forecasting model study",
        "paper_author": "Han J.",
        "publication": "Journal of Cleaner Production",
        "citied_by": "34",
        "cover_date": "2023-01-20",
        "Abstract": "As an important means to reduce CO2 emissions from coal-fired power plants, CCUS project has received more and more attention. In this paper, the comprehensive benefits of CCUS project in coal-fired power plants are defined as economic benefits, environmental benefits and energy benefits, and comprehensive evaluation and prediction analysis of CCUS project in coal-fired power plants are made from these three aspects. The goodness-of-fit of the constructed comprehensive evaluation model can reach 0.99, and the ratio of economic, environmental and energy benefits is about 5: 9: 1. Among them, the best scenario of policy influence, economic development and technological progress can promote CCUS project with benefits of 6.51%, 6.56% and 13.07% respectively. Through case analysis, it can be calculated that when the service life of CCUS project reaches 21.7 years, the investment cost can be recovered from the economic benefits. In the prediction model, the goodness of fit of the economic benefit prediction model is up to 0.95, in which the carbon capture cost of CCUS technology, operation and maintenance cost and carbon price fluctuation play a major role. The goodness of fit of the environmental benefit prediction model can reach 0.90, in which the carbon capture capacity of CCUS project and the world environmental situation are the main influencing factors, and the goodness of fit of the energy benefit prediction model can reach up to 0.95, which is mainly affected by the energy consumption level of CCUS project. The research results show that CCUS project of coal-fired power plants at home and abroad has a good development prospect and remarkable comprehensive benefits. This paper puts forward relevant policy suggestions that are beneficial to the development of CCUS project in coal-fired power plants, with a view to providing empirical evidence for the evaluation and decision-making of CCUS project in coal-fired power plants.",
        "DOI": "10.1016/j.jclepro.2022.135657",
        "affiliation_name": "Northeast Electric Power University",
        "affiliation_city": "Jilin",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Combining Roberta Pre-Trained Language Model and NMF Topic Modeling Technique to Learn from Customer Reviews Analysis",
        "paper_author": "Mensouri D.",
        "publication": "International Journal of Intelligent Systems and Applications in Engineering",
        "citied_by": "7",
        "cover_date": "2023-01-16",
        "Abstract": "In the past few years, more and more researchers have focused on the field of natural language processing and, more specifically, on aspect-level sentiment analysis. Sentiment analysis is often used to analyse people's opinions, or feelings about different entities like products or services. Given the immense amount of data generated daily in various forms on the web, sentiment analysis has become one of the most active areas of research today. In turn, online user reviews are considered a powerful marketing tool and have attracted widespread attention from marketers and academics. This motivates the current study, which focuses on sentiment analysis using four machine learning models, namely recurrent neural networks (LSTM, GRU, Bi-LSTM), and a pre-trained language model (Roberta). The models are trained to categorize customer reviews on online platforms as positive or negative. Then the best model that shows the best results is selected by evaluating each of them based on accuracy, precision, recall, and F1 Score. Finally, a topic modeling technique is used to reveal various topics present in the data and determine what’s pushed the customer to give such a review, as well as provide suppliers with the right decision for each scenario case. Although the approach proposed in this study is applied to analyse the opinions of customers towards the products of a marketplace and reveals different topics present in it, it can be used in any field to know the writer's point of view on, e.g., government policy, individuals, brands, etc.",
        "DOI": "NA",
        "affiliation_name": "Université Abdelmalek Essaadi",
        "affiliation_city": "Tetouan",
        "affiliation_country": "Morocco"
    },
    {
        "paper_title": "Path-following optimal control of autonomous underwater vehicle based on deep reinforcement learning",
        "paper_author": "Wang Z.",
        "publication": "Ocean Engineering",
        "citied_by": "27",
        "cover_date": "2023-01-15",
        "Abstract": "In this paper, optimal control is applied in the path-following control problem for the autonomous underwater vehicle (AUV) when the geometry information of path is known and will not change over time. Aiming at the problem that the optimization time is too long in one control step for complex nonlinear problems, a path-following control method based on Simplified Deep Deterministic Policy Gradient (S-DDPG) algorithm is proposed. In S-DDPG, only the reward in the current state is considered, and the future reward does not need to be predicted, which avoids generating amount of meaningless failed samples and simplifies the training process of neural networks (NNs). The training is performed and completed offline before the beginning of path-following where the NNs are directly used as the controller. The simulation results show that the S-DDPG can make the AUV complete path-following tasks and has obvious advantages compared with other methods.",
        "DOI": "10.1016/j.oceaneng.2022.113407",
        "affiliation_name": "Southern Marine Science and Engineering Guangdong Laboratory",
        "affiliation_city": "Zhuhai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "An ensemble dynamic self-learning model for multiscale carbon price forecasting",
        "paper_author": "Zhang W.",
        "publication": "Energy",
        "citied_by": "36",
        "cover_date": "2023-01-15",
        "Abstract": "Precise carbon price forecasting can provide decision support for policy-makers and investors. However, due to the high non-stationarity and nonlinearity of carbon price series, it is difficult to get accurate forecasting results under volatile situations. To accommodate different scenarios, this paper proposes a dynamic self-learning integrating forecasting model to forecast the carbon price by considering external impact factors. The multi-dimensional time series is initially decomposed into different intrinsic mode functions simultaneously by the noise-assisted multivariate empirical mode decomposition method. After reconstructing the decomposed series into high-frequency, low-frequency, and trend modules, the extreme learning machine optimized by the cosine-based whale optimization algorithm is proposed to predict the carbon price. The dynamic relationships between the carbon price and impact factors are simulated by the sliding window structure, which improves the adaptability of the proposed model. The high prediction accuracy under different situations including extreme scenarios demonstrates the stability of the proposed model. A self-learning algorithm, which can automatically learn the evolving model structure and update model parameters, is designed to alleviate the underfitting/overfitting problem. The comparison results with existing models indicate the superiority of the proposed model.",
        "DOI": "10.1016/j.energy.2022.125820",
        "affiliation_name": "Business School of Sichuan University",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China"
    },
    {
        "paper_title": "On the explainability of convolutional neural networks processing ultrasonic guided waves for damage diagnosis",
        "paper_author": "Lomazzi L.",
        "publication": "Mechanical Systems and Signal Processing",
        "citied_by": "25",
        "cover_date": "2023-01-15",
        "Abstract": "Among the maintenance policies adopted to guarantee the safety of structures throughout their service life, condition-based maintenance policies driven by structural health monitoring approaches have progressively gained importance over the last years. Within this field, among the several methods proposed in the literature to diagnose damage affecting thin-walled structures, satisfactory performances have been achieved by adopting tomographic algorithms to process ultrasonic guided waves, even though with some limitations. Recently, such limitations have been overcome by adopting machine learning-based algorithms, even though their implementation in real life applications is still limited because of the mistrust in neural networks determined by their black box-like nature. To date, however, several explainability algorithms have been proposed to interpret the behaviour of neural networks, in particular in the medical and in the military fields, where trust in the tools adopted must be guaranteed. Thus, exploiting the potentialities of explainability frameworks, in this work the layer-wise relevance propagation algorithm is employed to explain the predictions of convolutional neural networks for classification and for regression that characterise damage by processing ultrasonic guided waves excited and sensed by means of a 2-D network of piezoelectric devices. First, the explainability algorithm is applied to give a score to each sample of the acquired ultrasonic guided waves, then such scores are collected by means of a properly developed aggregation strategy to rank the most informative couples of piezoelectric devices. The capabilities of the explainable damage diagnosis framework are demonstrated by means of a numerical, yet realistic, case study involving a metal plate affected by crack-like damage. In particular, the focus is set on the explanation of the behaviour of the neural networks involved, with the aim of building trust in such algorithms and, possibly, revealing damage-related hidden features of ultrasonic guided waves.",
        "DOI": "10.1016/j.ymssp.2022.109642",
        "affiliation_name": "Politecnico di Milano",
        "affiliation_city": "Milan",
        "affiliation_country": "Italy"
    },
    {
        "paper_title": "Multi-agent actor-critic with time dynamical opponent model",
        "paper_author": "Tian Y.",
        "publication": "Neurocomputing",
        "citied_by": "4",
        "cover_date": "2023-01-14",
        "Abstract": "In multi-agent reinforcement learning, multiple agents learn simultaneously while interacting with a common environment and each other. Since the agents adapt their policies during learning, not only the behavior of a single agent becomes non-stationary, but also the environment as perceived by the agent. This renders it particularly challenging to perform policy improvement. In this paper, we propose to exploit the fact that the agents seek to improve their expected cumulative reward and introduce a novel Time Dynamical Opponent Model (TDOM) to encode the knowledge that the opponent policies tend to improve over time. We motivate TDOM theoretically by deriving a lower bound of the log objective of an individual agent and further propose Multi-Agent Actor-Critic with Time Dynamical Opponent Model (TDOM-AC). We evaluate the proposed TDOM-AC on a differential game and the Multi-agent Particle Environment. We show empirically that TDOM achieves superior opponent behavior prediction during test time. The proposed TDOM-AC methodology outperforms state-of-the-art Actor-Critic methods on the performed tasks in cooperative and especially in mixed cooperative-competitive environments. TDOM-AC results in a more stable training and a faster convergence. Our code is available at https://github.com/Yuantian013/TDOM-AC.",
        "DOI": "10.1016/j.neucom.2022.10.045",
        "affiliation_name": "ETH Zürich",
        "affiliation_city": "Zurich",
        "affiliation_country": "Switzerland"
    },
    {
        "paper_title": "Climate change and population aging may impact the benefits of improved air quality on cardiovascular mortality in Guangzhou: epidemiological evidence and policy implications",
        "paper_author": "Wu C.",
        "publication": "Environmental Science: Advances",
        "citied_by": "0",
        "cover_date": "2023-01-13",
        "Abstract": "Air pollution is the primary environmental risk factor contributing to global cardiovascular mortality. In China, a series of air pollution control policies launched in 2013 have led to substantial improvements in air quality over the past 10 years. However, the health benefits of improved air quality on cardiovascular mortality remain unclear under the combined effects of climate change and population aging. In this study, we investigated dynamic changes in the contribution of air pollution, meteorological conditions and aging to cardiovascular mortality over 9 years (2013-2021) in Guangzhou, China using generalized additive models and machine learning analysis. Although the air quality in Guangzhou has continuously improved since 2013, cardiovascular mortality has increased since 2019 and approached 2013 levels in 2021. Use of the SHapley Additive exPlanation (SHAP) approach to interpret the model outputs revealed that meteorological factors have gradually replaced air pollutants as the main environmental factors affecting cardiovascular mortality since 2016. Concurrently, the impact of population aging on cardiovascular mortality has increased year-on-year. Our results provide important insights into improved air quality related health benefits that could aid development of an early warning service system and national environmental and public health policy related to climate change and population aging.",
        "DOI": "10.1039/d2va00303a",
        "affiliation_name": "Guangdong Center for Disease Control and Prevention",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A COMPARATIVE STUDY OF MACHINE LEARNING CLASSIFIERS FOR CROP TYPE MAPPING USING VEGETATION INDICES",
        "paper_author": "Asgari S.",
        "publication": "ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences",
        "citied_by": "5",
        "cover_date": "2023-01-13",
        "Abstract": "Timely and accurate mapping of crops is crucial for agriculture management, policy-making, and food security. Due to the differences in the product calendars of various crops, it is possible to classify them by investigating the remote sensing Vegetation Indices (VIs) during crop growth season. This study developed a VI-based mapping approach to specifying crop types based on phenological and spectral metrics derived from the sentinel-2 images. We used six spectral VIs (ARVI, CVI, EVI, LAI, GLI, and NDVI) in three supervised machine learning methods, including Random Forest (RF), GBoost (GB), and K-Nearest Neighborhood (KNN) for crop mapping. Field data consisting of wheat, barley, canola, vegetables, and a bare land class, were collected as the testing and training data set. The classification results were evaluated through test samples showing high overall accuracy (OA) and satisfactory class accuracies for the most dominant crop types across different fields despite the variability of planting and harvesting dates. Among the VIs utilized to crop mapping, the Atmospherically Resistant Vegetation Index (ARVI) in all three classification methods achieved better results. The overall accuracy of RF, GB, and KNN models with the ARVI index was 95%, 88%, and 90%, respectively.",
        "DOI": "10.5194/isprs-annals-X-4-W1-2022-79-2023",
        "affiliation_name": "University of Tehran",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran"
    },
    {
        "paper_title": "“Storylistening” in the science policy ecosystem: Expert analysis of narrative can complement and strengthen scientific evidence",
        "paper_author": "Craig C.",
        "publication": "Science",
        "citied_by": "8",
        "cover_date": "2023-01-13",
        "Abstract": "NA",
        "DOI": "10.1126/science.abo1355",
        "affiliation_name": "University of Cambridge",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "Shapley values reveal the drivers of soil organic carbon stock prediction",
        "paper_author": "Wadoux A.M.J.C.",
        "publication": "SOIL",
        "citied_by": "14",
        "cover_date": "2023-01-11",
        "Abstract": "Insights into the controlling factors of soil organic carbon (SOC) stock variation are necessary both for our scientific understanding of the terrestrial carbon balance and to support policies that intend to promote carbon storage in soils to mitigate climate change. In recent years, complex statistical and algorithmic tools from the field of machine learning have become popular for modelling and mapping SOC stocks over large areas. In this paper, we report on the development of a statistical method for interpreting complex models, which we implemented for the study of SOC stock variation. We fitted a random forest machine learning model with 2206 measurements of SOC stocks for the 0-50 cm depth interval from mainland France and used a set of environmental covariates as explanatory variables. We introduce Shapley values, a method from coalitional game theory, and use them to understand how environmental factors influence SOC stock prediction: what is the functional form of the association in the model between SOC stocks and environmental covariates, and how does the covariate importance vary locally from one location to another and between carbon-landscape zones? Results were validated both in light of the existing and well-described soil processes mediating soil carbon storage and with regards to previous studies in the same area. We found that vegetation and topography were overall the most important drivers of SOC stock variation in mainland France but that the set of most important covariates varied greatly among locations and carbon-landscape zones. In two spatial locations with equivalent SOC stocks, there was nearly an opposite pattern in the individual covariate contribution that yielded the prediction - in one case climate variables contributed positively, whereas in the second case climate variables contributed negatively - and this effect was mitigated by land use. We demonstrate that Shapley values are a methodological development that yield useful insights into the importance of factors controlling SOC stock variation in space. This may provide valuable information to understand whether complex empirical models are predicting a property of interest for the right reasons and to formulate hypotheses on the mechanisms driving the carbon sequestration potential of a soil.",
        "DOI": "10.5194/soil-9-21-2023",
        "affiliation_name": "Unité Info&amp;Sols",
        "affiliation_city": "Orleans",
        "affiliation_country": "France"
    },
    {
        "paper_title": "Naturalistic E-Scooter Maneuver Recognition with Federated Contrastive Rider Interaction Learning",
        "paper_author": "Tabatabaie M.",
        "publication": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",
        "citied_by": "7",
        "cover_date": "2023-01-11",
        "Abstract": "Smart micromobility, particularly the electric (e)-scooters, has emerged as an important ubiquitous mobility option that has proliferated within and across many cities in North America and Europe. Due to the fast speed (say, ∼15km/h) and ease of maneuvering, understanding how the micromobility rider interacts with the scooter becomes essential for the e-scooter manufacturers, e-scooter sharing operators, and rider communities in promoting riding safety and relevant policy or regulations. In this paper, we propose FCRIL, a novel Federated maneuver identification and Contrastive e-scooter Rider Interaction Learning system. FCRIL aims at: (i) understanding, learning, and identifying the e-scooter rider interaction behaviors during naturalistic riding (NR) experience (without constraints on the data collection settings); and (ii) providing a novel federated maneuver learning model training and contrastive identification design for our proposed rider interaction learning (RIL). Towards the prototype and case studies of FCRIL, we have harvested an NR behavior dataset based on the inertial measurement units (IMUs), e.g., accelerometer and gyroscope, from the ubiquitous smartphones/embedded IoT devices attached to the e-scooters. Based on the harvested IMU sensor data, we have conducted extensive data analytics to derive the relevant rider maneuver patterns, including time series, spectrogram, and other statistical features, for the RIL model designs. We have designed a contrastive RIL network which takes in these maneuver features with class-to-class differentiation for comprehensive RIL and enhanced identification accuracy. Furthermore, to enhance the dynamic model training efficiency and coping with the emerging micromobility rider data privacy concerns, we have designed a novel asynchronous federated maneuver learning module, which asynchronously takes in multiple sets of model gradients (e.g., based on the IMU data from the riders' smartphones) for dynamic RIL model training and communication overhead reduction. We have conducted extensive experimental studies with different smartphone models and stand-alone IMU sensors on the e-scooters. Our experimental results have demonstrated the accuracy and effectiveness of FCRIL in learning and recognizing the e-scooter rider maneuvers.",
        "DOI": "10.1145/3570345",
        "affiliation_name": "UConn College of Engineering",
        "affiliation_city": "Storrs",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Reinforcement-learning-based control of convectively unstable flows",
        "paper_author": "Xu D.",
        "publication": "Journal of Fluid Mechanics",
        "citied_by": "11",
        "cover_date": "2023-01-10",
        "Abstract": "This work reports the application of a model-free deep reinforcement learning (DRL) based flow control strategy to suppress perturbations evolving in the one-dimensional linearised Kuramoto-Sivashinsky (KS) equation and two-dimensional boundary layer flows. The former is commonly used to model the disturbance developing in flat-plate boundary layer flows. These flow systems are convectively unstable, being able to amplify the upstream disturbance, and are thus difficult to control. The control action is implemented through a volumetric force at a fixed position, and the control performance is evaluated by the reduction of perturbation amplitude downstream. We first demonstrate the effectiveness of the DRL-based control in the KS system subjected to a random upstream noise. The amplitude of perturbation monitored downstream is reduced significantly, and the learnt policy is shown to be robust to both measurement and external noise. One of our focuses is to place sensors optimally in the DRL control using the gradient-free particle swarm optimisation algorithm. After the optimisation process for different numbers of sensors, a specific eight-sensor placement is found to yield the best control performance. The optimised sensor placement in the KS equation is applied directly to control two-dimensional Blasius boundary layer flows, and can efficiently reduce the downstream perturbation energy. Via flow analyses, the control mechanism found by DRL is the opposition control. Besides, it is found that when the flow instability information is embedded in the reward function of DRL to penalise the instability, the control performance can be further improved in this convectively unstable flow.",
        "DOI": "10.1017/jfm.2022.1020",
        "affiliation_name": "National University of Singapore",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore"
    },
    {
        "paper_title": "Realistic Actor-Critic: A framework for balance between value overestimation and underestimation",
        "paper_author": "Li S.",
        "publication": "Frontiers in Neurorobotics",
        "citied_by": "2",
        "cover_date": "2023-01-09",
        "Abstract": "Introduction: The value approximation bias is known to lead to suboptimal policies or catastrophic overestimation bias accumulation that prevent the agent from making the right decisions between exploration and exploitation. Algorithms have been proposed to mitigate the above contradiction. However, we still lack an understanding of how the value bias impact performance and a method for efficient exploration while keeping stable updates. This study aims to clarify the effect of the value bias and improve the reinforcement learning algorithms to enhance sample efficiency. Methods: This study designs a simple episodic tabular MDP to research value underestimation and overestimation in actor-critic methods. This study proposes a unified framework called Realistic Actor-Critic (RAC), which employs Universal Value Function Approximators (UVFA) to simultaneously learn policies with different value confidence-bound with the same neural network, each with a different under overestimation trade-off. Results: This study highlights that agents could over-explore low-value states due to inflexible under-overestimation trade-off in the fixed hyperparameters setting, which is a particular form of the exploration-exploitation dilemma. And RAC performs directed exploration without over-exploration using the upper bounds while still avoiding overestimation using the lower bounds. Through carefully designed experiments, this study empirically verifies that RAC achieves 10x sample efficiency and 25% performance improvement compared to Soft Actor-Critic in the most challenging Humanoid environment. All the source codes are available at https://github.com/ihuhuhu/RAC. Discussion: This research not only provides valuable insights for research on the exploration-exploitation trade-off by studying the frequency of policies access to low-value states under different value confidence-bounds guidance, but also proposes a new unified framework that can be combined with current actor-critic methods to improve sample efficiency in the continuous control domain.",
        "DOI": "10.3389/fnbot.2022.1081242",
        "affiliation_name": "Harbin Engineering University",
        "affiliation_city": "Harbin",
        "affiliation_country": "China"
    },
    {
        "paper_title": "A fractional filter based on reinforcement learning for effective tracking under impulsive noise",
        "paper_author": "Xie X.",
        "publication": "Neurocomputing",
        "citied_by": "32",
        "cover_date": "2023-01-07",
        "Abstract": "It is valuable and meaningful to suppress impulsive noise in system identification. Existing algorithms usually only consider impulsive noise with small frequency and amplitude. Furthermore, few researchers pay attention to the tracking performance of these algorithms. This paper builds a framework based on the deep deterministic policy gradient (DDPG) algorithm with the ability to explore and correct. The enhanced fractional derivative is introduced to further improve the performance of this reinforcement learning-based framework. Thus a fractional least mean square filter algorithm based on reinforcement learning (FrlMS) is proposed. The stability of the FrlMS algorithm is analyzed. Compared with the competing algorithms, the simulation experiments in system identification show that the FrlMS algorithm has satisfactory tracking performance in the face of impulsive noise, especially when the frequency and (or) amplitude are (is) large.",
        "DOI": "10.1016/j.neucom.2022.10.038",
        "affiliation_name": "China University of Petroleum (East China)",
        "affiliation_city": "Qingdao",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Multi-objective fuzzy Q-learning to solve continuous state-action problems",
        "paper_author": "Asgharnia A.",
        "publication": "Neurocomputing",
        "citied_by": "9",
        "cover_date": "2023-01-07",
        "Abstract": "Many real world problems are multi-objective. Thus, the need for multi-objective learning and optimization algorithms is inevitable. Although the multi-objective optimization algorithms are well-studied, the multi-objective learning algorithms have attracted less attention. In this paper, a fuzzy multi-objective reinforcement learning algorithm is proposed, and we refer to it as the multi-objective fuzzy Q-learning (MOFQL) algorithm. The algorithm is implemented to solve a bi-objective reach-avoid game. The majority of the multi-objective reinforcement algorithms proposed address solving problems in the discrete state-action domain. However, the MOFQL algorithm can also handle problems in a continuous state-action domain. A fuzzy inference system (FIS) is implemented to estimate the value function for the bi-objective problem. We used a temporal difference (TD) approach to update the fuzzy rules. The proposed method is a multi-policy multi-objective algorithm and can find the non-convex regions of the Pareto front.",
        "DOI": "10.1016/j.neucom.2022.10.035",
        "affiliation_name": "Carleton University",
        "affiliation_city": "Ottawa",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "Optimal consensus of a class of discrete-time linear multi-agent systems via value iteration with guaranteed admissibility",
        "paper_author": "Li P.",
        "publication": "Neurocomputing",
        "citied_by": "13",
        "cover_date": "2023-01-07",
        "Abstract": "This paper investigates the optimal consensus problem for heterogeneous discrete-time(DT) linear multi-agent systems. The optimal consensus problem is formulated as finding a global Nash equilibrium solution subjected to the defined local performance index. A reinforcement learning(RL) value iteration(VI) algorithm is introduced to obtain the optimal policies in the sense of Nash equilibrium. To ensure the effectiveness of the VI algorithm, the admissibility of the iterative control policies for multi-agent systems is considered. With theoretical analysis, a new termination criterion is established to guarantee the admissibility of the iterative control policies. Furthermore, an online learning framework is designed with an actor-critic neural network(NN) to implement the VI algorithm. Finally, two simulation examples are presented respectively for leader–follower and leaderless multi-agent systems to verify the effectiveness of the proposed method.",
        "DOI": "10.1016/j.neucom.2022.10.032",
        "affiliation_name": "State Key Laboratory of Internet of Things for Smart City",
        "affiliation_city": "Taipa",
        "affiliation_country": "Macao"
    },
    {
        "paper_title": "An analysis of finding the best strategies of water security for water source areas using an integrated IT2FVIKOR with machine learning",
        "paper_author": "Zamri N.",
        "publication": "Frontiers in Environmental Science",
        "citied_by": "2",
        "cover_date": "2023-01-06",
        "Abstract": "Worldwide, water security is adversely affected by factors such as population growth, rural–urban migration, climate, hydrological conditions, over-abstraction of groundwater, and increased per-capita water use. Water security modeling is one of the key strategies to better manage water safety and develop appropriate policies to improve security. In view of the growing global demand for safe water, intelligent methods and algorithms must be developed. Therefore, this paper proposes an integrated interval type-2 Fuzzy VIseKriterijumska Optimizcija I Kompromisno Resenje (IT2FVIKOR) with unsupervised machine learning (ML). This includes IT2FVIKOR for ranking and selecting a set of alternatives. Unsupervised machine learning includes hierarchical clustering, self-organizing map, and autoencoder for clustering, silhouette analysis and elbow method to find the most optimal cluster count, and finally Adjusted Rank Index (ARI) to find the best comparison within two clusters. This proposed integrated method can be divided into a two-phase fuzzy-machine learning-based framework to select the best water security strategies and categorize the polluted area using the water datasets from the Terengganu River, one of Malaysia’s rivers. Phase 1 focuses on the IT2FVIKOR method to select five different strategies with five different criteria using five decision makers for finding the best water security strategies. Phase 2 continues the unsupervised machine learning where three different clustering algorithms, namely, hierarchical clustering, self-organizing map, and autoencoder, are used to cluster the polluted area in the Terengganu River. Silhouette analysis is applied along with the clustering algorithms to estimate the number of optimal clusters in a dataset. Then, the ARI is applied to find the best comparison within the original data with hierarchical clustering, self-organizing map, and autoencoder. Next, the elbow method is applied to double-confirm the best clusters for each clustering algorithm. Last, lists of polluted areas in each cluster are retrieved. Finally, this 2-phase fuzzy-Machine learning–based framework offers an alternative intelligent model to solve the water security problems and find the most polluted area.",
        "DOI": "10.3389/fenvs.2022.971129",
        "affiliation_name": "Universiti Sultan Zainal Abidin",
        "affiliation_city": "Kuala Terengganu",
        "affiliation_country": "Malaysia"
    },
    {
        "paper_title": "ICMLSC 2023 - 2023 7th International Conference on Machine Learning and Soft Computing",
        "paper_author": "NA",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-01-05",
        "Abstract": "The proceedings contain 31 papers. The topics discussed include: research on location problem based on fuzzy multi-criteria decision method; decision model of ship intelligent collision avoidance based on automatic information system data and generic adversary imitation learning-deep deterministic policy gradient; research on energy consumption prediction based on fast attribute reduction of weighted neighborhood rough set with moving horizon; solving multimodal multi-objective problems with local pareto front using a population clustering mechanism; validated computation of Lipschitz constant of recurrent neural networks; neural network optimization objective vector representation based on genetic algorithm and its multi-objective optimization method; improved multilayer perceptron neural networks weights and biases based on the grasshopper optimization algorithm to predict student performance on ambient learning; and spatio-temporal deep fusion graph convolutional networks for crime prediction.",
        "DOI": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Exploring the characteristics and driving forces of orchard expansion in ecological fragile region: A case study of three typical counties in the Loess Plateau",
        "paper_author": "Hu Q.",
        "publication": "Frontiers in Environmental Science",
        "citied_by": "0",
        "cover_date": "2023-01-04",
        "Abstract": "The Loess Plateau exemplifies the type of ecologically fragile region that faces severe poverty challenges in China. Orchards have expanded rapidly over the past few decades and now constitute a considerable part of local economy. Not only do the characteristics of orchard expansion affect local economic development, but also exert additional pressure on the ecological environment. Therefore, it is essential for sustainable development on the Loess Plateau to investigate the characteristics and driving forces of orchard expansion. The Fuxian, Luochuan, Huangling, three typical orchard planting counties were chosen as the study area. Firstly, the orchard was extracted from the land use/cover classification from 1990–2020. It broadens the research approach to the identification of expansion cash crops by using the combination of linear spectral mixture analysis (LSMA) and decision tree. Secondly, the spatiotemporal dynamics of orchard expansion were quantitatively investigated based on spatial geometry center shift, physical geographical features, landscape pattern and orchard planting suitability. Then, we constructed an evaluation indicators system to detect the feature importance and partial dependence of different factors by random forest regression. It is more innovative to employ the machine learning method to investigate driving forces. Finally, the linkages between planting suitability and orchard expansion were further discussed, and subsequent policies were proposed. Findings demonstrated the orchard had continuously expanded over the past 30 years, with the fastest expansion rate during 1990–2005. Increased cohesion was accompanied by a shift in the orchard’s spatial distribution to the north central region and highly suitable planting regions. Slope turned out to be the primary factor affecting the orchard expansion. In the future, regions with aging orchard but high planting suitability should be the preferred choice for orchard expansion. Additionally, the transportation connectivity and governmental assistance are crucial considerations for the future planning of the orchard.",
        "DOI": "10.3389/fenvs.2022.1097236",
        "affiliation_name": "Ministry of Natural Resources of the People's Republic of China",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Deep reinforcement learning empowers automated inverse design and optimization of photonic crystals for nanoscale laser cavities",
        "paper_author": "Li R.",
        "publication": "Nanophotonics",
        "citied_by": "21",
        "cover_date": "2023-01-02",
        "Abstract": "Photonics inverse design relies on human experts to search for a design topology that satisfies certain optical specifications with their experience and intuitions, which is relatively labor-intensive, slow, and sub-optimal. Machine learning has emerged as a powerful tool to automate this inverse design process. However, supervised or semi-supervised deep learning is unsuitable for this task due to: (1) a severe shortage of available training data due to the high computational complexity of physics-based simulations along with a lack of open-source datasets and/or the need for a pre-trained neural network model; (2) the issue of one-to-many mapping or non-unique solutions; and (3) the inability to perform optimization of the photonic structure beyond inverse designing. Reinforcement Learning (RL) has the potential to overcome the above three challenges. Here, we propose Learning to Design Optical-Resonators (L2DO) to leverage RL that learns to autonomously inverse design nanophotonic laser cavities without any prior knowledge while retrieving unique design solutions. L2DO incorporates two different algorithms - Deep Q-learning and Proximal Policy Optimization. We evaluate L2DO on two laser cavities: a long photonic crystal (PC) nanobeam and a PC nanobeam with an L3 cavity, both popular structures for semiconductor lasers. Trained for less than 152 hours on limited hardware resources, L2DO has improved state-of-the-art results in the literature by over 2 orders of magnitude and obtained 10 times better performance than a human expert working the same task for over a month. L2DO first learned to meet the required maxima of Q-factors (>50 million) and then proceeded to optimize some additional good-to-have features (e.g., resonance frequency, modal volume). Compared with iterative human designs and inverse design via supervised learning, L2DO can achieve over two orders of magnitude higher sample-efficiency without suffering from the three issues above. This work confirms the potential of deep RL algorithms to surpass human designs and marks a solid step towards a fully automated AI framework for photonics inverse design.",
        "DOI": "10.1515/nanoph-2022-0692",
        "affiliation_name": "Peng Cheng Laboratory",
        "affiliation_city": "Shenzhen",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Knowledge discovery of zakat administration worldwide from YouTube and Zoom via text mining",
        "paper_author": "Hudaefi F.A.",
        "publication": "Journal of Islamic Accounting and Business Research",
        "citied_by": "9",
        "cover_date": "2023-01-02",
        "Abstract": "Purpose: Zakat (Islamic almsgiving) plays a considerable role in dealing with the socioeconomic issues in times of COVID-19 pandemic, and such roles have been widely discussed in virtual events. This paper aims to discover knowledge of the current global zakat administration from virtual events of zakat (e.g. webinars) on YouTube and Zoom via text mining approach. Design/methodology/approach: The authors purposefully sampled 12 experts from four different virtual zakat events on YouTube and Zoom. The automated text transcription software is used to pull the information from the sampled videos into text documents. A qualitative analysis is operated using text mining approach via machine learning tool (i.e. Orange Data Mining). Four research questions are developed under the Word Cloud visualisation, hierarchal clustering, topic modelling and graph and network theory. Findings: The machine learning identifies the most important words, the relationship between the experts and their top words and discovers hidden themes from the sample. This finding is practically substantial for zakat stakeholders to understand the current issues of global zakat administration and to learn the applicable lessons from the current issues of zakat management worldwide. Research limitations/implications: This study does not establish a positivist generalisation from the findings because of the nature and objective of the study. Practical implications: A policy implication is drawn pertaining to the legislation of zakat as an Islamic financial policy instrument for combating poverty in Muslim society. Social implications: This work supports the notion of “socioeconomic zakat”, implying that zakat as a religious obligation is important in shaping the social and economic processes of a Muslim community. Originality/values: This work marks the novelty in making sense of the unstructured data from virtual events on YouTube and Zoom in the Islamic social finance research.",
        "DOI": "10.1108/JIABR-03-2022-0067",
        "affiliation_name": "Universitas Islam Darussalam",
        "affiliation_city": "Ciamis",
        "affiliation_country": "Indonesia"
    },
    {
        "paper_title": "TERL: Transformer Enhanced Reinforcement Learning for Relation Extraction",
        "paper_author": "Wang Y.",
        "publication": "Proceedings of the 22nd Chinese National Conference on Computational Linguistics, CCL 2023",
        "citied_by": "0",
        "cover_date": "2023-01-01",
        "Abstract": "Relation Extraction (RE) task aims to discover the semantic relation that holds between two entities and contributes to many applications such as knowledge graph construction and completion. Reinforcement Learning (RL) has been widely used for RE task and achieved SOTA results, which are mainly designed with rewards to choose the optimal actions during the training procedure, to improve RE's performance, especially for low-resource conditions. Recent work has shown that offline or online RL can be flexibly formulated as a sequence understanding problem and solved via approaches similar to large-scale pre-training language modeling. To strengthen the ability for understanding the semantic signals interactions among the given text sequence, this paper leverages Transformer architecture for RL-based RE methods, and proposes a generic framework called Transformer Enhanced RL (TERL) towards RE task. Unlike prior RL-based RE approaches that usually fit value functions or compute policy gradients, TERL only outputs the best actions by utilizing a masked Transformer. Experimental results show that the proposed TERL framework can improve many state-of-the-art RL-based RE methods.",
        "DOI": "NA",
        "affiliation_name": "Beijing Police College",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "UniDexGrasp: Universal Robotic Dexterous Grasping via Learning Diverse Proposal Generation and Goal-Conditioned Policy",
        "paper_author": "Xu Y.",
        "publication": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
        "citied_by": "42",
        "cover_date": "2023-01-01",
        "Abstract": "In this work, we tackle the problem of learning universal robotic dexterous grasping from a point cloud observation under a table-top setting. The goal is to grasp and lift up objects in high-quality and diverse ways and generalize across hundreds of categories and even the unseen. Inspired by successful pipelines used in parallel gripper grasping, we split the task into two stages: 1) grasp proposal (pose) generation and 2) goal-conditioned grasp execution. For the first stage, we propose a novel probabilistic model of grasp pose conditioned on the point cloud observation that factorizes rotation from translation and articulation. Trained on our synthesized large-scale dexterous grasp dataset, this model enables us to sample diverse and high-quality dexterous grasp poses for the object point cloud. For the second stage, we propose to replace the motion planning used in parallel gripper grasping with a goal-conditioned grasp policy, due to the complexity involved in dexterous grasping execution. Note that it is very challenging to learn this highly generalizable grasp policy that only takes realistic inputs without oracle states. We thus propose several important innovations, including state canonicalization, object curriculum, and teacher-student distillation. In-tegrating the two stages, our final pipeline becomes the first to achieve universal generalization for dexterous grasping, demonstrating an average success rate of more than 60% on thousands of object instances, which significantly out-performs all baselines, meanwhile showing only a minimal generalization gap.",
        "DOI": "10.1109/CVPR52729.2023.00459",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Multi-Agent Automated Machine Learning",
        "paper_author": "Wang Z.",
        "publication": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
        "citied_by": "2",
        "cover_date": "2023-01-01",
        "Abstract": "In this paper, we propose multi-agent automated machine learning (MA2ML) with the aim to effectively handle joint optimization of modules in automated machine learning (AutoML). MA2ML takes each machine learning module, such as data augmentation (AUG), neural architecture search (NAS), or hyper-parameters (HPO), as an agent and the final performance as the reward, to formulate a multi-agent reinforcement learning problem. MA2ML explicitly assigns credit to each agent according to its marginal contribution to enhance cooperation among modules, and incorporates off-policy learning to improve search efficiency. Theoretically, MA2ML guarantees monotonic improvement of joint optimization. Extensive experiments show that MA2ML yields the state-of-the-art top-1 accuracy on ImageNet under constraints of computational cost, e.g., 79.7%/80.5% with FLOPs fewer than 600M/800M. Exten\\sive ablation studies verify the benefits of credit assignment and off-policy learning of MA2ML.",
        "DOI": "10.1109/CVPR52729.2023.01151",
        "affiliation_name": "Peng Cheng Laboratory",
        "affiliation_city": "Shenzhen",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Predicting newcomer's turnover using Machine Learning Algorithms : A case study of Thai financial firm in Bangkok, Thailand",
        "paper_author": "Kittikunsiri M.",
        "publication": "Proceedings - 2023 International Conference on Data, Information and Computing Science, CDICS 2023",
        "citied_by": "0",
        "cover_date": "2023-01-01",
        "Abstract": "Employee turnover, a critical issue impacting workplace productivity, has prompted organizations to leverage machine learning techniques for predictive analysis. This study specifically targets the prediction of turnover among new employees, utilizing data obtained from a survey conducted at a Thai financial firm in Bangkok, Thailand. Through an evaluation of various machine learning models, the results indicate that the Random Forest model surpasses others. Furthermore, this research highlights crucial factors influencing newcomer turnover, such as comfort with workplace culture, work-fromhome policies, onboarding programs, and satisfaction with the recruitment process. These findings offer actionable insights for HR professionals to focus on these specific areas and improve the experience for new employees, thereby enhancing their retention within the organization.",
        "DOI": "10.1109/CDICS61497.2023.00017",
        "affiliation_name": "Chulalongkorn University",
        "affiliation_city": "Bangkok",
        "affiliation_country": "Thailand"
    },
    {
        "paper_title": "Keeping it Low-Key: Modern-Day Approaches to Privacy-Preserving Machine Learning",
        "paper_author": "Grover J.",
        "publication": "Data Protection in a Post-Pandemic Society: Laws, Regulations, Best Practices and Recent Solutions",
        "citied_by": "1",
        "cover_date": "2023-01-01",
        "Abstract": "The last handful of years has seen immense evolution and adoption of Machine Learning (ML) in multifarious online realms, from influencing our shopping lists to who we connect with or follow on a social network. Additionally, the COVID-19 pandemic has accelerated the growth and consumption of these digital services across the globe. Regardless of our fascination or loathe for it, ML-powered services and products are influencing our decision-making power and dominating our lives heavily. The scope of impact of these intelligent systems implies that the confidentiality of both data and the underlying algorithm is highly critical. A slight slack in the design of these systems can lead to disastrous outcomes propelled by cyber-attacks, reverse engineering, and leakage of sensitive data like personal conversations, financial transactions, medical history, and so on. With an imperative agenda of retaining the confidentiality of data, maintaining the privacy of proprietary design, and staying compliant with the latest regulations and policies, Privacy-Preserving Machine Learning ensures trust among all the stakeholders. This chapter will analyze the contemporary interpretation of Privacy-Preserving Machine Learning and the significance it holds in myriad settings. We will cover prevalent types of privacy attacks on ML systems like inferences of the membership, input, parameter, and property. Next, the exposition will examine some privacy-enhancing techniques such as differential privacy, federated learning, and synthetic data along with modern-day advancements like dataset condensation among others. Furthermore, the synopsis will also discuss some tools to quantify and effectively measure privacy risks in statistical and Machine Learning algorithms. Additionally, we will go over some of the recent policy developments to regulate data protection and privacy worldwide and how that is shaping the industry. In closing, we will leave readers with some thoughts on future directions for the development of better and smarter techniques to maximize data utility and privacy in tandem. The goal is to ignite the public dialogue regarding privacy impacts, ethical consequences, fairness, and real-world harms of non-privacy-compliant ML systems.",
        "DOI": "10.1007/9783031340062_2",
        "affiliation_name": "Twitter, Inc.",
        "affiliation_city": "San Francisco",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Double Duality: Variational Primal-Dual Policy Optimization for Constrained Reinforcement Learning",
        "paper_author": "Li Z.",
        "publication": "Journal of Machine Learning Research",
        "citied_by": "0",
        "cover_date": "2023-01-01",
        "Abstract": "We study the Constrained Convex Markov Decision Process (MDP), where the goal is to minimize a convex functional of the visitation measure, subject to a convex constraint. Designing algorithms for a constrained convex MDP faces several challenges, including (1) handling the large state space, (2) managing the exploration/exploitation tradeoff, and (3) solving the constrained optimization where the objective and the constraint are both nonlinear functions of the visitation measure. In this work, we present a model-based algorithm, Variational Primal-Dual Policy Optimization (VPDPO), in which Lagrangian and Fenchel duality are implemented to reformulate the original constrained problem into an unconstrained primal-dual optimization. The primal variables are updated by model-based value iteration following the principle of Optimism in the Face of Uncertainty (OFU), while the dual variables are updated by gradient ascent. Moreover, by embedding the visitation measure into a finite-dimensional space, we can handle large state spaces by incorporating function approximation. Two notable examples are (1) Kernelized Nonlinear Regulators and (2) Low-rank MDPs. We prove that with an optimistic planning oracle, our algorithm achieves sublinear regret and constraint violation in both cases and can attain the globally optimal policy of the original constrained problem.",
        "DOI": "NA",
        "affiliation_name": "School of Engineering and Applied Science",
        "affiliation_city": "Princeton",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "The Measure and Mismeasure of Fairness",
        "paper_author": "Corbett-Davies S.",
        "publication": "Journal of Machine Learning Research",
        "citied_by": "40",
        "cover_date": "2023-01-01",
        "Abstract": "The field of fair machine learning aims to ensure that decisions guided by algorithms are equitable. Over the last decade, several formal, mathematical definitions of fairness have gained prominence. Here we first assemble and categorize these definitions into two broad families: (1) those that constrain the effects of decisions on disparities; and (2) those that constrain the effects of legally protected characteristics, like race and gender, on decisions. We then show, analytically and empirically, that both families of definitions typically result in strongly Pareto dominated decision policies. For example, in the case of college admissions, adhering to popular formal conceptions of fairness would simultaneously result in lower student-body diversity and a less academically prepared class, relative to what one could achieve by explicitly tailoring admissions policies to achieve desired outcomes. In this sense, requiring that these fairness definitions hold can, perversely, harm the very groups they were designed to protect. In contrast to axiomatic notions of fairness, we argue that the equitable design of algorithms requires grappling with their context-specific consequences, akin to the equitable design of policy. We conclude by listing several open challenges in fair machine learning and offering strategies to ensure algorithms are better aligned with policy goals.",
        "DOI": "NA",
        "affiliation_name": "Stanford Engineering",
        "affiliation_city": "Stanford",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "MARLlib: A Scalable and Efficient Library For Multi-agent Reinforcement Learning",
        "paper_author": "Hu S.",
        "publication": "Journal of Machine Learning Research",
        "citied_by": "7",
        "cover_date": "2023-01-01",
        "Abstract": "A significant challenge facing researchers in the area of multi-agent reinforcement learning (MARL) pertains to the identification of a library that can offer fast and compatible development for multi-agent tasks and algorithm combinations, while obviating the need to consider compatibility issues. In this paper, we present MARLlib, a library designed to address the aforementioned challenge by leveraging three key mechanisms: 1) a standardized multi-agent environment wrapper, 2) an agent-level algorithm implementation, and 3) a flexible policy mapping strategy. By utilizing these mechanisms, MARLlib can effectively disentangle the intertwined nature of the multi-agent task and the learning process of the algorithm, with the ability to automatically alter the training strategy based on the current task’s attributes. The MARLlib library’s source code is publicly accessible on GitHub: https://github.com/Replicable-MARL/MARLlib.",
        "DOI": "NA",
        "affiliation_name": "Mohamed Bin Zayed University of Artificial Intelligence",
        "affiliation_city": "Abu Dhabi",
        "affiliation_country": "United Arab Emirates"
    },
    {
        "paper_title": "A Bibliometric Review of the Carbon Emissions and Machine Learning Research in the Post-COVID-19 Era",
        "paper_author": "Liao P.",
        "publication": "Lecture Notes in Operations Research",
        "citied_by": "0",
        "cover_date": "2023-01-01",
        "Abstract": "The COVID-19 pandemic has dramatically changed people’s lives and had a major impact on the field of research. In this study, bibliometrics was used to analyze 783 articles in the field of the post-COVID-19 era, among which 55 articles were the most popular management directions. Based on this background, carbon emissions and machine learning are selected for further analysis, which are the current hot research directions. There were only 22 and 12 related articles. This shows that management direction is a hot research topic in the post-COVID-19 era, while carbon emissions and machine learning are relatively few. This study sorted out and analyzed the specific research content of these 34 literatures. It is found that there are few literatures that apply machine learning to the study of carbon emission. But it also found that machine learning methods can be applied to carbon emissions even in the post-COVID-19 era. This not only provides scholars with new ideas and directions for their research, but also provides a theoretical basis for enterprises and governments to formulate corresponding carbon emission management policies.",
        "DOI": "10.1007/978-981-99-3626-7_99",
        "affiliation_name": "Shanghai University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Iterated Boxed Pigs Game: A Reinforcement Learning Approach",
        "paper_author": "Milani R.",
        "publication": "Lecture Notes in Operations Research",
        "citied_by": "0",
        "cover_date": "2023-01-01",
        "Abstract": "This paper analyzes the iterated version of the well-known Boxed Pigs game through Reinforcement Learning. In this scenario, there are two differently sized players (pigs) that compete against each other. The core idea is about sacrificing a pay-off in order to generate some rewards. In our iterated version, these pigs play this game repeatedly using different strategies. We carry out two experiments: in the first one, we train two Q-learning agents against each other to see which equilibrium will be generated. In the second one, we pit the Reinforcement Learning agent against a fixed policy pig. The results of this experiment confirm the ability of Reinforcement Learning techniques in finding the best strategy for maximizing the return independently from the other player choices.",
        "DOI": "10.1007/978-3-031-24907-5_74",
        "affiliation_name": "Universität der Bundeswehr München",
        "affiliation_city": "Neubiberg",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Machine Learning Approach to Examine the Influence of the Community Environment on the Quality of Life of the Elderly",
        "paper_author": "Liang Q.",
        "publication": "Lecture Notes in Operations Research",
        "citied_by": "0",
        "cover_date": "2023-01-01",
        "Abstract": "The quality of life (QoL) of the elderly has gradually become the focus of contemporary research. Elderly spent certain time staying at the community in their daily life, while studies have claimed the close relationships between built environment and the QoL of the elderly. With the advancement in the analytical tools, this paper aims to apply the machine learning approach to empirically examine the influence of the community environment on the QoL of the elderly. After extensive literature of relevant knowledge, a questionnaire survey was administered among the elderly. The collected quantitative data were subjected to a series of mathematical and statistical analysis analyses, and regression models for the relationship between community environment and the QoL of the elderly were established through support vector machine method. The results show that: 1) both the factors related the space and environment of the community can influence the QoL of the elderly; and 2) it was interesting to note that none of the facilities factor in the community imposes impact on their QoL. Practical recommendations are put forward according the research results in order to improve the community environment for the elderly, including building enough space, optimizing layout of monitoring equipment, maintaining ventilation to ensure air quality, and so on. This paper mainly contributes to apply the machine learning approach for examining the influence of community environment on the QoL of the elderly, which should enhance current body knowledge about the research related to the built environment for the elderly. The research findings should be helpful for the policy makers, facilities managers and academics to effectively improve existing practices regarding the management of community environment for better QoL of the elderly.",
        "DOI": "10.1007/978-981-99-3626-7_106",
        "affiliation_name": "Southwest Petroleum University China",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China"
    },
    {
        "paper_title": "13th EAI International Conference on Digital Forensics and Cyber Crime, ICDF2C 2022",
        "paper_author": "NA",
        "publication": "Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST",
        "citied_by": "0",
        "cover_date": "2023-01-01",
        "Abstract": "The proceedings contain 28 papers. The special focus in this conference is on Digital Forensics and Cyber Crime. The topics include: Can Image Watermarking Efficiently Protect Deep-Learning-Based Image Classifiers? – A Preliminary Security Analysis of an IP-Protecting Method; digital Forensics Tool Evaluation on Deleted Files; Forensic Analysis of Webex on the iOS Platform; watch Your WeChat Wallet: Digital Forensics Approach on WeChat Payments on Android; crypto Wallet Artifact Detection on Android Devices Using Advanced Machine Learning Techniques; CSCD: A Cyber Security Community Detection Scheme on Online Social Networks; shedding Light on Monopoly: Temporal Analysis of Drug Trades; extracting Spread-Spectrum Hidden Data Based on Better Lattice Decoding; MQTT Traffic Collection and Forensic Analysis Framework; IoT Malicious Traffic Detection Based on FSKDE and Federated DIOT-Pysyft; crime and Incident Watch for Smart Cities: A Sensor-Based Approach; The Lightweight Botnet Detection Model Based on the Improved UNet; on the Application of Active Learning to Handle Data Evolution in Android Malware Detection; Volatility Custom Profiling for Automated Hybrid ELF Malware Detection; the Need for Biometric Anti-spoofing Policies: The Case of Etsy; VPnet: A Vulnerability Prioritization Approach Using Pointer Network and Deep Reinforcement Learning; are External Auditors Capable of Dealing with Cybersecurity Risks?; deep Learning-Based Detection of Cyberattacks in Software-Defined Networks; deep Learning Based Network Intrusion Detection System for Resource-Constrained Environments; poisoning-Attack Detection Using an Auto-encoder for Deep Learning Models; Attribute-Based Proxy Re-encryption with Privacy Protection for Message Dissemination in VANET; PBPAFL: A Federated Learning Framework with Hybrid Privacy Protection for Sensitive Data.",
        "DOI": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Using machine learning techniques to classify factors that influence the occurrence of occupational dermatitis",
        "paper_author": "da Rosa A.C.F.",
        "publication": "Revista Brasileira de Saude Ocupacional",
        "citied_by": "0",
        "cover_date": "2023-01-01",
        "Abstract": "Introduction: to predict work related diseases is a challenge for organizations and the governmental authorities. By means of machine learning (ML) techniques it is possible to identify factors that determine the occurrence of an occupational disease, aiming at taking more effective actions to protect workers. Objective: to predict, by comparing ML techniques, the factors which highly influence the occurrence of occupational dermatitis. Methods: we developed a code in R language and a descriptive analysis of the data and identified the influence factors according to the ML technique that presented the best performance. The database was made available by the Occupational Dermatology Service of Oswaldo Cruz Foundation and assembles information of the workers who experienced cutaneous alterations suggestive of occupational dermatitis between 2000-2014. Results: the techniques which presented the best performance were: neural network, random forest, support vector machine, and naive Bayes. Sex, schooling, and profession were the most adequate variables for the occupational dermatitis prediction models. Conclusion: ML techniques allowed to predict the factors that influence the workers’ safety and health, as well as the parameters that subsidize the procedures implementation, and the most effective policies to prevent occupational dermatitis.",
        "DOI": "10.1590/2317-6369/31620pt2023v48e4",
        "affiliation_name": "Universidade Estadual de Maringá",
        "affiliation_city": "Maringa",
        "affiliation_country": "Brazil"
    },
    {
        "paper_title": "Research on Analytical Modelling of World Population Projections Based on Time Series",
        "paper_author": "Huang C.",
        "publication": "Proceedings - 2023 International Conference on Intelligent Computing, Communication and Convergence, ICI3C 2023",
        "citied_by": "0",
        "cover_date": "2023-01-01",
        "Abstract": "World population projections have always been important social and economic issues with far-reaching implications for policy formulation and resource allocation. This study aims to develop a world population projection model based on time series analysis to estimate future population trends and changes more accurately. The study uses global population data accumulated over the years and employs time series analysis, including seasonal decomposition, trend analysis and cyclical analysis. First, data cleaning and smoothing were performed on the historical population data to eliminate noise and outliers. Then, using time series analysis techniques, we identified potential trends and cyclical patterns. We also combined machine learning methods, such as ARIMA (Autoregressive Integral Moving Average) models and neural networks, to improve the accuracy of the models. After training and validating the model, we successfully developed a model that accurately predicts future world population changes. By fitting the historical data, we are able to predict the trend of national and global population in the coming decades. This time series-based world population projection model has the potential for a wide range of applications and can help governments, international organisations and policy makers to better plan resource allocation, social security and public policies. In addition, it contributes to an in-depth understanding of global population dynamics and provides strong support for addressing future demographic challenges. Future research efforts will further refine the model to take into account more complex factors, such as sociocultural and political changes, in order to improve the accuracy and usability of the projections.",
        "DOI": "10.1109/ICI3C60830.2023.00051",
        "affiliation_name": "Harbin University of Science and Technology",
        "affiliation_city": "Harbin",
        "affiliation_country": "China"
    },
    {
        "paper_title": "Research on the Design Scheme Construction of Industrial Big Data Platform Based on Enterprise Portrait",
        "paper_author": "Yu X.",
        "publication": "Proceedings - 2023 International Conference on Intelligent Computing, Communication and Convergence, ICI3C 2023",
        "citied_by": "0",
        "cover_date": "2023-01-01",
        "Abstract": "The complexity and numerous influencing factors of the industrial chain make precise monitoring, optimized control, and scientific management extremely difficult. However, utilizing big data technology to achieve full factor correlation analysis of industries is an effective way to support the modernization development of industries in the future. On the basis of analyzing the current situation of industrial big data applications, this article elaborates on the functions and objectives of the industrial big data platform, and constructs the overall architecture of the platform from four aspects: technical architecture, data architecture, application architecture, and physical architecture. At the same time, this article also analyzes the two key technologies involved in the platform, and combines machine learning related algorithms to profile enterprises, multi-dimensional mining of enterprise information for data display services. By building this platform, precise industrial management based on big data can be achieved, more scientific and objective industrial guidance policies can be formulated, supporting industrial management decisions, and providing valuable references for industrial informatization and intelligent transformation and upgrading.",
        "DOI": "10.1109/ICI3C60830.2023.00017",
        "affiliation_name": "Guangdong University of Science and Technology",
        "affiliation_city": "Dongguan",
        "affiliation_country": "China"
    },
    {
        "paper_title": "COVID 19 – Monitoring with IoT Devices",
        "paper_author": "Nagaraj A.",
        "publication": "COVID 19 – Monitoring with IoT Devices",
        "citied_by": "0",
        "cover_date": "2023-01-01",
        "Abstract": "In the battle against the COVID-19 pandemic, the integration of Internet of Things (IoT) technologies has played a pivotal role in reshaping public health and healthcare delivery. Interconnected devices have demonstrated their capacity to collect, transmit, and analyze data, significantly impacting various aspects of pandemic management. COVID-19 – Monitoring with IoT Devices is a comprehensive guide to measuring the impact of COVID-19 infection and monitoring outbreak metrics. Beginning with an introduction to SARS-CoV-2 and its symptoms, the book presents chapters on machine learning (supervised and unsupervised algorithms) and techniques to predict COVID-19 outcomes. The book concludes with the role of IoT technology in detecting COVID-19 infections within a community, showcasing different computing models applicable to specific use-cases. Key Features: - Explores the pivotal role of IoT technology in the fight against the COVID-19 pandemic. - Covers a data-driven approach to COVID-19 monitoring by explaining methods for data collection, prediction, and analysis. - Includes specific recommendations for machine learning algorithms designed for COVID-19 monitoring. - Easy-to-read structured chapters suitable for novices in computer science and biomedical engineering. COVID-19 – Monitoring with IoT Devices provides a valuable resource for understanding the role of IoT technology in managing and mitigating the impact of COVID-19, and developing adequate infection control policies. It also showcases the potential of IoT for future research and applications in the healthcare sector. This book is intended for a diverse readership, including academicians, industry professionals, researchers, and healthcare practitioners. Readership: Academicians; Industrialists; researchers, healthcare professionals.",
        "DOI": "10.2174/97898151794531230101",
        "affiliation_name": "St. Francis College",
        "affiliation_city": "Bengaluru",
        "affiliation_country": "India"
    },
    {
        "paper_title": "Data Foundation for Actionable Science",
        "paper_author": "Ziheng S.",
        "publication": "Actionable Science of Global Environment Change: From Big Data to Practical Research",
        "citied_by": "0",
        "cover_date": "2023-01-01",
        "Abstract": "The field of climate and environmental research heavily relies on scientific data to understand the complex interactions between the Earth’s systems and the impacts of human activities. High-quality data is critical to informing evidence-based policies and decision-making that address global environmental challenges. This chapter discusses the importance of a robust data foundation for actionable science within the realm of global environmental change. It overviews the historical transformation of climate research, highlighting the growing significance of data in the field. It then delves into the various types of scientific data, including observational data, remote sensing data, and model output data. The chapter further examines the challenges and limitations associated with these data types, such as data quality, availability, and accessibility. Furthermore, it highlights the importance of data management and sharing practices to promote open and reproducible science. It also discusses emerging data technologies and trends, such as big data and machine learning, and their potential applications in climate and environmental research.",
        "DOI": "10.1007/978-3-031-41758-0_2",
        "affiliation_name": "George Mason University",
        "affiliation_city": "Fairfax",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Actionable Science of Global Environment Change: From Big Data to Practical Research",
        "paper_author": "Ziheng S.",
        "publication": "Actionable Science of Global Environment Change: From Big Data to Practical Research",
        "citied_by": "0",
        "cover_date": "2023-01-01",
        "Abstract": "This volume teaches readers how to sort through the vast mountain of climate and environmental science data to extract actionable insights. With the advancements in sensing technology, we now observe petabytes of data related to climate and the environment. While the volume of data is impressive, collecting big data for the sake of data alone proves to be of limited utility. Instead, our quest is for actionable data that can drive tangible actions and meaningful impact. Yet, unearthing actionable insights from the accumulated big data and delivering them to global stakeholders remains a burgeoning field. Although traditional data mining struggles to keep pace with data accumulation, scientific evolution has spurred the emergence of new technologies like numeric modeling and machine learning. These cutting-edge tools are now tackling grand challenges in climate and the environment, from forecasting extreme climate events and enhancing environmental productivity to monitoringgreenhouse gas emissions, fostering smart environmental solutions, and understanding aerosols. Additionally, they model environmental-human interactions, inform policy, and steer markets towards a healthier and more environment-friendly direction. While there's no universal solution to address all these formidable tasks, this book takes us on a guided journey through three sections, enriched with chapters from domain scientists. Part I defines actionable science and explores what truly renders data actionable. Part II showcases compelling case studies and practical use scenarios, illustrating these principles in action. Finally, Part III provides an insightful glimpse into the future of actionable science, focusing on the pressing climate and environmental issues we must confront. Embark on this illuminating voyage with us, where big data meets practical research, and discover how our collective efforts move us closer to a sustainable and thriving future. This book is an invitation to unlock the mysteries of our environment, transforming data into decisive action for generations to come.",
        "DOI": "10.1007/978-3-031-41758-0",
        "affiliation_name": "George Mason University",
        "affiliation_city": "Fairfax",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Consumer Participation through Digital Review Practices? On the Sociology of the Infrastructures of Consumer Participation",
        "paper_author": "Lamla J.",
        "publication": "Consumer Policy from Below: Paradoxes, Perspectives, Problematizations",
        "citied_by": "0",
        "cover_date": "2023-01-01",
        "Abstract": "Digital evaluation practices are a widespread phenomenon today. Consumers participate in them, sometimes directly, through stars and comments, sometimes indirectly, by training machine-learning recommendation systems through data traces. The article asks what opportunities for articulation consumers receive through this. Do these evaluation practices support a consumer policy from below? Or is it consumer activation that intensifies post-democratic decay, insofar as it merely simulates political participation? To answer this, the article draws on the sociology of infrastructures and argues that evaluation practices take place in pre-curated, steering or nudging decision architectures. These contribute to post-democratic change when they hinder the retrieval of critical competencies through conventionalizing framing and strong simplification of evaluation problems, thus endangering their reproduction.",
        "DOI": "10.1007/978-3-658-42489-3_5",
        "affiliation_name": "Universität Kassel",
        "affiliation_city": "Kassel",
        "affiliation_country": "Germany"
    },
    {
        "paper_title": "Transition Maneuver Optimization of a Hybrid UAV Using Deep Reinforcement Learning",
        "paper_author": "Akhtar M.",
        "publication": "Proceedings of 2023 20th International Bhurban Conference on Applied Sciences and Technology, IBCAST 2023",
        "citied_by": "0",
        "cover_date": "2023-01-01",
        "Abstract": "In the past few years, the increasing need for dynamic mission profiles of UAVs has led to the development of autonomous aerial vehicles with sophisticated path planning, tracking, and control algorithms. Owing to the size and aerodynamic structure of small UAVs, they can achieve maneuvers like VTOL (Vertical Take off and Landing) and transition maneuvers from hover to cruise mode. Trajectory optimization of such maneuvers helps to minimize flight time and path while considering spatial constraints. The emergence of Machine Learning has gained attention for developing control systems based on such algorithms. This study mainly explores the algorithm Deep Deterministic Policy Gradient (DDPG) for path planning and optimization. To enhance the capabilities of small UAVs, a hybrid tiltrotor aircraft model is considered, which aims to hover and transition to an efficient forward flight during the cruise.",
        "DOI": "10.1109/IBCAST59916.2023.10712871",
        "affiliation_name": "National University of Sciences and Technology",
        "affiliation_city": "Islamabad",
        "affiliation_country": "Pakistan"
    },
    {
        "paper_title": "MDL-NAS: A Joint Multi-domain Learning Framework for Vision Transformer",
        "paper_author": "Wang S.",
        "publication": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
        "citied_by": "9",
        "cover_date": "2023-01-01",
        "Abstract": "In this work, we introduce MDL-NAS, a unified framework that integrates multiple vision tasks into a manageable supernet and optimizes these tasks collectively under diverse dataset domains. MDL-NAS is storage-efficient since multiple models with a majority of shared parameters can be deposited into a single one. Technically, MDL-NAS constructs a coarse-to-fine search space, where the coarse search space offers various optimal architectures for different tasks while the fine search space provides fine-grained parameter sharing to tackle the inherent obstacles of multi-domain learning. In the fine search space, we suggest two parameter sharing policies, i.e., sequential sharing policy and mask sharing policy. Compared with previous works, such two sharing policies allow for the partial sharing and non-sharing of parameters at each layer of the network, hence attaining real fine-grained parameter sharing. Finally, we present a joint-subnet search algorithm that finds the optimal architecture and sharing parameters for each task within total resource constraints, challenging the traditional practice that downstream vision tasks are typically equipped with backbone networks designed for image classification. Experimentally, we demonstrate that MDL-NAS families fitted with non-hierarchical or hierarchical transformers deliver competitive performance for all tasks compared with state-of-the-art methods while maintaining efficient storage deployment and computation. We also demonstrate that MDL-NAS allows incremental learning and evades catastrophic forgetting when generalizing to a new task.",
        "DOI": "10.1109/CVPR52729.2023.01924",
        "affiliation_name": "SenseTime Group Limited",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong"
    },
    {
        "paper_title": "Assessment of Various Deep Reinforcement Learning Techniques in Complex Virtual Search-and-Retrieve Environments Compared to Human Performance",
        "paper_author": "Uttrani S.",
        "publication": "Applied Cognitive Science and Technology: Implications of Interactions between Human Cognition and Technology",
        "citied_by": "0",
        "cover_date": "2023-01-01",
        "Abstract": "Recently, the area of deep reinforcement learning (DRL) has seen remark-able advances in fields like medicine, robotics, and automation. Nonetheless, there remains a dearth of understanding about how cutting-edge DRL algorithms, such as Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC), match up against human proficiency in demanding search-and-retrieve operations. Furthermore, there is a scarcity of structured assessments of the efficacy of these algorithms in intricate and dynamic surroundings after hyperparameter adjustment. To tackle this discrep-ancy, the research at hand aims to evaluate and contrast the impact of the proportion of targets to distractions on human and machine agents’ performance in a complex search simulation using a professional gaming engine. Moreover, the influence of the amount of neurons and layers in DRL algorithms will be scrutinized in connection to the search-and-retrieve missions. The task requires an agent (whether human or model) to traverse an environment and gather target objects while sidestepping distractor objects. The results of the study exhibit that humans accomplished better in training scenarios, whereas model agents performed better in test scenarios. More-over, SAC emerged as a superior performer compared to PPO across all test conditions. Furthermore, boosting the amount of units and layers was found to enhance the performance of DRL algorithms. These conclusions imply that similar hyperpa-rameter configurations can be utilized when contrasting models are generated using DRL algorithms. The study also delves into the implications of utilizing AI models to direct human decisions.",
        "DOI": "10.1007/978-981-99-3966-4_9",
        "affiliation_name": "Indian Institute of Technology Mandi",
        "affiliation_city": "Mandi",
        "affiliation_country": "India"
    },
    {
        "paper_title": "The Challenge of Artificial Intelligence to the Art World",
        "paper_author": "Padgett A.D.",
        "publication": "Co-operative Education, Politics, and Art: Creative, Critical and Community Resistance to Corporate Higher Education",
        "citied_by": "1",
        "cover_date": "2023-01-01",
        "Abstract": "This chapter discusses some of the ramifications of the use of artificial intelligence (AI) for the Artworld. Whilst the use of AI is developing at pace within art institutions, ethical dialogues are not keeping pace. This chapter explores some of the reasons behind the resistance to the use of AI in art curating/education and assessment. It also argues that AI may be able to combat some of the negative effects of art curating/education. It shows how machine learning can analyse artworks and identify their historical styles. Models of analysis can be constructed based on phenomenological formal properties of artworks and also on models of artistic genre. Factors such as historical originality and quality within models can also be constructed. These are all useful for the teaching of art. Also, AI offers full transparency in the selections of evaluative parameters of art. AI is radically more democratic than the unaccountable and elitist selection criteria of national galleries and competitions. The author reflects upon his previous experience of bringing the Tate Galleries to an Employment Tribunal in 2005–7, based around the unrepresentative character of their curatorial policy. As an alternative, Padgett argues that AI technology could democratise the elitism of Artworld institutions. As such, he argues that this is a technology worth taking seriously by art educators.",
        "DOI": "10.4324/9781032655352-11",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA"
    },
    {
        "paper_title": "Forecasting Loan Default in Europe with Machine Learning",
        "paper_author": "Barbaglia L.",
        "publication": "Journal of Financial Econometrics",
        "citied_by": "17",
        "cover_date": "2023-01-01",
        "Abstract": "We use a dataset of 12 million residential mortgages to investigate the loan default behavior in several European countries. We model the default occurrence as a function of borrower characteristics, loan-specific variables, and local economic conditions. We compare the performance of a set of machine learning algorithms relative to the logistic regression, finding that they perform significantly better in providing predictions. The most important variables in explaining loan default are the interest rate and the local economic characteristics. The existence of relevant geographical heterogeneity in the variable importance points at the need for regionally tailored risk-assessment policies in Europe.",
        "DOI": "10.1093/jjfinec/nbab010",
        "affiliation_name": "Baruch College",
        "affiliation_city": "New York",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Long-Term Fairness with Unknown Dynamics",
        "paper_author": "Yin T.",
        "publication": "Advances in Neural Information Processing Systems",
        "citied_by": "10",
        "cover_date": "2023-01-01",
        "Abstract": "While machine learning can myopically reinforce social inequalities, it may also be used to dynamically seek equitable outcomes. In this paper, we formalize long-term fairness as an online reinforcement learning problem for a policy affecting human populations. This formulation accommodates dynamical control objectives, such as achieving equitable population states, that cannot be incorporated into static formulations of fairness. We demonstrate that algorithmic solutions to the proposed fairness problem can adapt to unknown dynamics and, by sacrificing short-term incentives, drive the policy-population system towards more desirable equilibria. For the proposed setting, we develop an algorithm that adapts recent work in online learning and prove that this algorithm achieves simultaneous probabilistic bounds on cumulative loss and cumulative violations of fairness. In the classification setting subject to group fairness, we compare our proposed algorithm to several baselines, including the repeated retraining of myopic or distributionally robust classifiers, and to a deep reinforcement learning algorithm that lacks fairness guarantees. Our experiments model human populations according to evolutionary game theory and integrate real-world datasets.",
        "DOI": "NA",
        "affiliation_name": "University of Michigan, Ann Arbor",
        "affiliation_city": "Ann Arbor",
        "affiliation_country": "United States"
    },
    {
        "paper_title": "Maximum State Entropy Exploration using Predecessor and Successor Representations",
        "paper_author": "Jain A.K.",
        "publication": "Advances in Neural Information Processing Systems",
        "citied_by": "3",
        "cover_date": "2023-01-01",
        "Abstract": "Animals have a developed ability to explore that aids them in important tasks such as locating food, exploring for shelter, and finding misplaced items. These exploration skills necessarily track where they have been so that they can plan for finding items with relative efficiency. Contemporary exploration algorithms often learn a less efficient exploration strategy because they either condition only on the current state or simply rely on making random open-loop exploratory moves. In this work, we propose ηψ-Learning, a method to learn efficient exploratory policies by conditioning on past episodic experience to make the next exploratory move. Specifically, ηψ-Learning learns an exploration policy that maximizes the entropy of the state visitation distribution of a single trajectory. Furthermore, we demonstrate how variants of the predecessor representation and successor representations can be combined to predict the state visitation entropy. Our experiments demonstrate the efficacy of ηψ-Learning to strategically explore the environment and maximize the state coverage with limited samples.",
        "DOI": "NA",
        "affiliation_name": "University of Montreal",
        "affiliation_city": "Montreal",
        "affiliation_country": "Canada"
    },
    {
        "paper_title": "A Definition of Continual Reinforcement Learning",
        "paper_author": "Abel D.",
        "publication": "Advances in Neural Information Processing Systems",
        "citied_by": "15",
        "cover_date": "2023-01-01",
        "Abstract": "In a standard view of the reinforcement learning problem, an agent's goal is to efficiently identify a policy that maximizes long-term reward. However, this perspective is based on a restricted view of learning as finding a solution, rather than treating learning as endless adaptation. In contrast, continual reinforcement learning refers to the setting in which the best agents never stop learning. Despite the importance of continual reinforcement learning, the community lacks a simple definition of the problem that highlights its commitments and makes its primary concepts precise and clear. To this end, this paper is dedicated to carefully defining the continual reinforcement learning problem. We formalize the notion of agents that “never stop learning” through a new mathematical language for analyzing and cataloging agents. Using this new language, we define a continual learning agent as one that can be understood as carrying out an implicit search process indefinitely, and continual reinforcement learning as the setting in which the best agents are all continual learning agents. We provide two motivating examples, illustrating that traditional views of multi-task reinforcement learning and continual supervised learning are special cases of our definition. Collectively, these definitions and perspectives formalize many intuitive concepts at the heart of learning, and open new research pathways surrounding continual learning agents.",
        "DOI": "NA",
        "affiliation_name": "DeepMind Technologies Limited",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom"
    },
    {
        "paper_title": "RoboCLIP: One Demonstration is Enough to Learn Robot Policies",
        "paper_author": "Sontakke S.A.",
        "publication": "Advances in Neural Information Processing Systems",
        "citied_by": "10",
        "cover_date": "2023-01-01",
        "Abstract": "Reward specification is a notoriously difficult problem in reinforcement learning, requiring extensive expert supervision to design robust reward functions. Imitation learning (IL) methods attempt to circumvent these problems by utilizing expert demonstrations but typically require a large number of in-domain expert demonstrations. Inspired by advances in the field of Video-and-Language Models (VLMs), we present RoboCLIP, an online imitation learning method that uses a single demonstration (overcoming the large data requirement) in the form of a video demonstration or a textual description of the task to generate rewards without manual reward function design. Additionally, RoboCLIP can also utilize out-of-domain demonstrations, like videos of humans solving the task for reward generation, circumventing the need to have the same demonstration and deployment domains. RoboCLIP utilizes pretrained VLMs without any finetuning for reward generation. Reinforcement learning agents trained with RoboCLIP rewards demonstrate 2-3 times higher zero-shot performance than competing imitation learning methods on downstream robot manipulation tasks, doing so using only one video/text demonstration. Visit our website for experiment videos.",
        "DOI": "NA",
        "affiliation_name": "USC Viterbi School of Engineering",
        "affiliation_city": "Los Angeles",
        "affiliation_country": "United States"
    }
]