[
    {
        "paper_title": "Artificial Intelligence Adoption in the Post COVID-19 New-Normal and Role of Smart Technologies in Transforming Business: a Review",
        "publication": "Journal of Science and Technology Policy Management",
        "citied_by": "74",
        "cover_date": "2024-04-18",
        "Abstract": "Purpose: The purpose of this paper is to give an overview of artificial intelligence (AI) and other AI-enabled technologies and to describe how COVID-19 affects various industries such as health care, manufacturing, retail, food services, education, media and entertainment, banking and insurance, travel and tourism. Furthermore, the authors discuss the tactics in which information technology is used to implement business strategies to transform businesses and to incentivise the implementation of these technologies in current or future emergency situations. Design/methodology/approach: The review provides the rapidly growing literature on the use of smart technology during the current COVID-19 pandemic. Findings: The 127 empirical articles the authors have identified suggest that 39 forms of smart technologies have been used, ranging from artificial intelligence to computer vision technology. Eight different industries have been identified that are using these technologies, primarily food services and manufacturing. Further, the authors list 40 generalised types of activities that are involved including providing health services, data analysis and communication. To prevent the spread of illness, robots with artificial intelligence are being used to examine patients and give drugs to them. The online execution of teaching practices and simulators have replaced the classroom mode of teaching due to the epidemic. The AI-based Blue-dot algorithm aids in the detection of early warning indications. The AI model detects a patient in respiratory distress based on face detection, face recognition, facial action unit detection, expression recognition, posture, extremity movement analysis, visitation frequency detection, sound pressure detection and light level detection. The above and various other applications are listed throughout the paper. Research limitations/implications: Research is largely delimited to the area of COVID-19-related studies. Also, bias of selective assessment may be present. In Indian context, advanced technology is yet to be harnessed to its full extent. Also, educational system is yet to be upgraded to add these technologies potential benefits on wider basis. Practical implications: First, leveraging of insights across various industry sectors to battle the global threat, and smart technology is one of the key takeaways in this field. Second, an integrated framework is recommended for policy making in this area. Lastly, the authors recommend that an internet-based repository should be developed, keeping all the ideas, databases, best practices, dashboard and real-time statistical data. Originality/value: As the COVID-19 is a relatively recent phenomenon, such a comprehensive review does not exist in the extant literature to the best of the authors’ knowledge. The review is rapidly emerging literature on smart technology use during the current COVID-19 pandemic.",
        "DOI": "10.1108/JSTPM-08-2021-0122",
        "paper_author": "Agarwal P.",
        "affiliation_name": "Dayalbagh Educational Institute",
        "affiliation_city": "Agra",
        "affiliation_country": "India",
        "affiliation_id": "60010131",
        "affiliation_state": "UP"
    },
    {
        "paper_title": "Applications of artificial intelligence and machine learning in the financial services industry: A bibliometric review",
        "publication": "Heliyon",
        "citied_by": "31",
        "cover_date": "2024-01-15",
        "Abstract": "This bibliometric review examines the research state of artificial intelligence (AI) and machine learning (ML) applications in the Banking, Financial Services, and Insurance (BFSI) sector. The study focuses on Scopus-indexed articles to identify key research clusters. Following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) protocol, 39,498 articles were screened, resulting in 1045 articles meeting the inclusion criteria. N-gram analysis identified 177 unique terms in the article titles and abstracts. Co-occurrence analysis revealed nine distinct clusters covering fintech, risk management, anti-money laundering, and actuarial science, among others. These clusters offer a comprehensive overview of the multifaceted research landscape. The identified clusters can guide future research and inform study design. Policymakers, researchers, and practitioners in the BFSI sector can benefit from the study's findings, which identify research gaps and opportunities. This study contributes to the growing literature on bibliometrics, providing insights into AI and ML applications in the BFSI sector. The findings have practical implications, advancing our understanding of AI and ML's role in benefiting academia and industry.",
        "DOI": "10.1016/j.heliyon.2023.e23492",
        "paper_author": "Pattnaik D.",
        "affiliation_name": "International Management Institute, Bhubaneswar",
        "affiliation_city": "Bhubaneswar",
        "affiliation_country": "India",
        "affiliation_id": "60113899",
        "affiliation_state": "OR"
    },
    {
        "paper_title": "Examining the research taxonomy of artificial intelligence, deep learning &amp; machine learning in the financial sphere—a bibliometric analysis",
        "publication": "Quality and Quantity",
        "citied_by": "25",
        "cover_date": "2024-02-01",
        "Abstract": "This paper surveys the extant literature on machine learning, artificial intelligence, and deep learning mechanisms within the financial sphere using bibliometric methods. We considered the conceptual and social structure of publications in ML, AI, and DL in finance to better understand the research’s status, development, and growth. The study finds an upsurge in publication trends within this research arena, with a bit of concentration around the financial domain. The institutional contributions from USA and China constitute much of the literature on applying ML and AI in finance. Our analysis identifies emerging research themes, with the most futuristic being ESG scoring using ML and AI. However, we find there is a lack of empirical academic research with a critical appraisal of these algorithmic-based advanced automated financial technologies. There are severe pitfalls in the prediction process using ML and AI due to algorithmic biases, mostly in the areas of insurance, credit scoring and mortgages. Thus, this study indicates the next evolution of ML and DL archetypes in the economic sphere and the need for a strategic turnaround in academics regarding these forces of disruption and innovation that are shaping the future of finance.",
        "DOI": "10.1007/s11135-023-01673-0",
        "paper_author": "Biju A.K.V.N.",
        "affiliation_name": "University of Kerala",
        "affiliation_city": "Thiruvananthapuram",
        "affiliation_country": "India",
        "affiliation_id": "60031566",
        "affiliation_state": "KL"
    },
    {
        "paper_title": "Quantitatively Interpreting Residents Happiness Prediction by Considering Factor-Factor Interactions",
        "publication": "IEEE Transactions on Computational Social Systems",
        "citied_by": "23",
        "cover_date": "2024-02-01",
        "Abstract": "Exploring the high-effect factors of residents' happiness is good for a wide range of policy-making for economics and politics in most countries. Limited to the concerns of sociologists, previous efforts manually predefined the relationship among multifactors to analyze the factor-factor interactions with high interpretability by regression models. Recently, deep learning methods show great promising prediction accuracy by automatically learning additive interaction between factors, while it meets the challenges of interpretability. To this end, an unbiased post-hoc and model-agnostic method is promising to quantitatively interpret the results of the happiness prediction model. Thus, this article proposes a novel solution based on the deep neural network (DNN) and the Shapley value to compute the factor-factor interactions in different coalitions based on coalitional game theory. Aiming at evaluating the pairwise interactions interpretability quality of our solution, experiments are conducted on the Chinese General Social Survey (CGSS) and European Social Survey (ESS) questionnaire datasets. By systematic reviews, the experimental results are highly consistent with academic studies in social science. Analyzed by different factor categories, the new finding is that some factors, e.g., body mass index (BMI) and social media, play a crucial role in happiness prediction but are rarely considered in social science studies. Specifically, there are some heavy interactions across multicategories, such as personal information (health and age) with economics (insurance and income). Therefore, this solution can theoretically support the implications of social decision-making.",
        "DOI": "10.1109/TCSS.2023.3246181",
        "paper_author": "Li L.",
        "affiliation_name": "Wuhan University of Technology",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60022414",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Prostate Cancer Screening with PSA, Kallikrein Panel, and MRI: The ProScreen Randomized Trial",
        "publication": "JAMA",
        "citied_by": "20",
        "cover_date": "2024-05-07",
        "Abstract": "Importance: Prostate-specific antigen (PSA) screening has potential to reduce prostate cancer mortality but frequently detects prostate cancer that is not clinically important. Objective: To describe rates of low-grade (grade group 1) and high-grade (grade groups 2-5) prostate cancer identified among men invited to participate in a prostate cancer screening protocol consisting of a PSA test, a 4-kallikrein panel, and a magnetic resonance imaging (MRI) scan. Design, Setting, and Participants: The ProScreen trial is a clinical trial conducted in Helsinki and Tampere, Finland, that randomized 61193 men aged 50 through 63 years who were free of prostate cancer in a 1:3 ratio to either be invited or not be invited to undergo screening for prostate cancer between February 2018 and July 2020. Interventions: Participating men randomized to the intervention underwent PSA testing. Those with a PSA level of 3.0 ng/mL or higher underwent additional testing for high-grade prostate cancer with a 4-kallikrein panel risk score. Those with a kallikrein panel score of 7.5% or higher underwent an MRI of the prostate gland, followed by targeted biopsies for those with abnormal prostate gland MRI findings. Final data collection occurred through June 31, 2023. Main Outcomes and Measures: In descriptive exploratory analyses, the cumulative incidence of low-grade and high-grade prostate cancer after the first screening round were compared between the group invited to undergo prostate cancer screening and the control group. Results: Of 60745 eligible men (mean [SD] age, 57.2 [4.0] years), 15201 were randomized to be invited and 45544 were randomized not to be invited to undergo prostate cancer screening. Of 15201 eligible males invited to undergo screening, 7744 (51%) participated. Among them, 32 low-grade prostate cancers (cumulative incidence, 0.41%) and 128 high-grade prostate cancers (cumulative incidence, 1.65%) were detected, with 1 cancer grade group result missing. Among the 7457 invited men (49%) who refused participation, 7 low-grade prostate cancers (cumulative incidence, 0.1%) and 44 high-grade prostate cancers (cumulative incidence, 0.6%) were detected, with 7 cancer grade groups missing. For the entire invited screening group, 39 low-grade prostate cancers (cumulative incidence, 0.26%) and 172 high-grade prostate cancers (cumulative incidence, 1.13%) were detected. During a median follow-up of 3.2 years, in the group not invited to undergo screening, 65 low-grade prostate cancers (cumulative incidence, 0.14%) and 282 high-grade prostate cancers (cumulative incidence, 0.62%) were detected. The risk difference for the entire group randomized to the screening invitation vs the control group was 0.11% (95% CI, 0.03%-0.20%) for low-grade and 0.51% (95% CI, 0.33%-0.70%) for high-grade cancer. Conclusions and Relevance: In this preliminary descriptive report from an ongoing randomized clinical trial, 1 additional high-grade cancer per 196 men and 1 low-grade cancer per 909 men were detected among those randomized to be invited to undergo a single prostate cancer screening intervention compared with those not invited to undergo screening. These preliminary findings from a single round of screening should be interpreted cautiously, pending results of the study's primary mortality outcome. Trial Registration: ClinicalTrials.gov Identifier: NCT03423303.",
        "DOI": "10.1001/jama.2024.3841",
        "paper_author": "Auvinen A.",
        "affiliation_name": "Tampere University",
        "affiliation_city": "Tampere",
        "affiliation_country": "Finland",
        "affiliation_id": "60011170",
        "affiliation_state": "Pirkanmaa"
    },
    {
        "paper_title": "A hybrid framework using explainable AI (XAI) in cyber-risk management for defence and recovery against phishing attacks",
        "publication": "Decision Support Systems",
        "citied_by": "20",
        "cover_date": "2024-02-01",
        "Abstract": "Phishing and social engineering contribute to various cyber incidents such as data breaches and ransomware attacks, financial frauds, and denial of service attacks. Often, phishers discuss these attack vectors in dark forums. Further, the probability of phishing attacks and the subsequent loss suffered by the firm are highly correlated. In this context, we propose a hybrid framework using explainable AI techniques to assess cyber-risks generated from correlated phishing attacks. The first phase computes the probability of expert phishers within a community of similar attackers with varying expertise. The second phase calculates the probability of phishing attacks upon a firm even after it has invested in IT security and adopted regulatory steps. The third phase categorises phishing and genuine URLs using various machine-learning-based classifiers. Next, it estimates the joint distribution of phishing attacks using an exponential-beta distribution and quantifies the expected loss using Archimedean Copula. Finally, we offer recommendations for firms through the computation of optimal investments in cyber-insurance versus IT security. First, based on the risk attitude of a firm, it can use this explainable-AI (XAI) framework to optimally invest in building security into its enterprise architecture and plan for cyber-risk mitigation strategies. Second, we identify a long-tail phenomenon demonstrated by the losses suffered during most cyber-attacks, which are not one-off incidents and are correlated. Third, contrary to the belief that cyber-insurance markets are ineffective, it can guide financial firms to design realistic cyber-insurance products.",
        "DOI": "10.1016/j.dss.2023.114102",
        "paper_author": "Biswas B.",
        "affiliation_name": "Trinity Business School",
        "affiliation_city": "Dublin",
        "affiliation_country": "Ireland",
        "affiliation_id": "60116606",
        "affiliation_state": "Leinster"
    },
    {
        "paper_title": "An evolutionary supply chain management service model based on deep learning features for automated glaucoma detection using fundus images",
        "publication": "Engineering Applications of Artificial Intelligence",
        "citied_by": "17",
        "cover_date": "2024-02-01",
        "Abstract": "Glaucoma, a multifaceted eye condition, poses a high risk of vision impairment. Initially, most automated approaches segment the primary system and assess the clinical measurements to classify and screen for glaucoma. The proposed customized convolutional neural network (CNN) model for automated glaucoma detection, built using deep learning techniques, can assist many stakeholders in the supply chain management network. These stakeholders may include eye hospitals, healthcare service providers, doctors, ophthalmologists, patients, insurance companies, etc. The deployed model comprises four learnable layers, i.e., three convolution layers and a flattened layer. The customized CNN model learned the deep features with the least number of tunable parameters. Subsequently, a combined feature reduction strategy called principal component analysis (PCA) and linear discriminant analysis (LDA) to reduce the dimensions of feature sets. Finally, a classification is carried out by utilizing an extreme learning machine (ELM). The hidden node parameters of ELM are optimized with the help of the modified particle swarm optimization (MOD-PSO) technique. The generalized performance of the proposed model has been enhanced by employing 5-fold stratified cross-validation. The proposed model deployed on two standard datasets, G1020 and ORIGA. The experimental results show that the proposed computer-aided diagnosis (CAD) model achieves an accuracy of 97.80% and 98.46% on the G1020 and ORIGA datasets, respectively. The customized CNN model outperforms as compared to other state-of-the-art models with a significantly less number of features and could help the decision-makers of supply chain management networks.",
        "DOI": "10.1016/j.engappai.2023.107449",
        "paper_author": "Sharma S.K.",
        "affiliation_name": "C. V. Raman Global University",
        "affiliation_city": "Bhubaneswar",
        "affiliation_country": "India",
        "affiliation_id": "60078476",
        "affiliation_state": "OR"
    },
    {
        "paper_title": "Analysis of demand forecasting of agriculture using machine learning algorithm",
        "publication": "Environment, Development and Sustainability",
        "citied_by": "15",
        "cover_date": "2024-01-01",
        "Abstract": "The state of India was situated on fertile land and river deltas with appropriate agricultural land. In 2019, agricultural fields, primarily cropland, occupy more than 40% of the country's total surface area. Moreover, agricultural industry revenues less than 3% of the country's Gross Provincial Product (GPP). While manufacturing has become the country's major financial activity, accounting countries represent half of GPP's revenues. The objective of the study is to find a way to improve the financial profitability and efficiency of farming supply chain networks as follows: (1) Fixation of national-level targets for zonal-level groups information after assessment and forecasts that affect production and distribution of agriculture. (2) Producers risk can be reduced by directing people to multiple factory and industrialization options based on market assessment. (3) Insurance costs and reduction of bank borrowing by standardizing the connection between bankers and producers using the structure to centralise land information. (4), Stabilize the agricultural sector by looking at nearby potential destinations for production and regulation of the Public Distribution System (PDS) flow for the security stock. In this paper, the novel ML target prediction algorithm to inform the farmers about the market target product and improve the relationships between the farmer and bankers for centralizing the information about recent government plans. The crop prediction ML algorithm proposed to improve the revenue of agriculture field.",
        "DOI": "10.1007/s10668-022-02783-9",
        "paper_author": "Chelliah B.J.",
        "affiliation_name": "SRM Institute of Science and Technology Ramapuram Campus",
        "affiliation_city": "Chennai",
        "affiliation_country": "India",
        "affiliation_id": "60117285",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Advancing flood damage modeling for coastal Alabama residential properties: A multivariable machine learning approach",
        "publication": "Science of the Total Environment",
        "citied_by": "14",
        "cover_date": "2024-01-10",
        "Abstract": "Flooding is a global threat and predicting flood risk accurately is vital for effective mitigation and increasing society's awareness of the negative impacts of floods. Over the years, researchers have worked on physical and data-driven models to predict flood damage, striving to improve accuracy and understanding. However, the challenge lies in the scarcity and limitedness of comprehensive datasets needed to develop these models. This study aims to enhance the National Flood Insurance Program (NFIP) claims dataset from Hurricane Katrina in coastal Alabama to make it adequate for multi-variable flood damage assessment. The NFIP claims dataset was combined with the Alabama property dataset, simulated flood hazard information, and property location characteristics. Oversampling techniques are employed to address data imbalance in the datasets. Subsequently, several ensemble machine learning approaches, including random forest, extra tree, extreme gradient boosting, and categorical boosting, are utilized to develop multi-variable flood damage models. The validation of these models demonstrates that extreme gradient boosting performs best, achieving satisfactory results in identifying damaged properties with precision (0.89), recall (0.90), and F1-score (0.90), as well as determining relative damage with R-squared (0.59), root mean squared error (0.21), and Spearman correlation (0.70). Utilizing data oversampling techniques improves the model performance of imbalanced flood damage datasets. Despite the dataset's limitations and data augmentation techniques employed, the model's output explanation based on SHapley Additive exPlanations (SHAP) is constructive as it aligns with the study's expectations regarding the interaction of different features to produce the final results.",
        "DOI": "10.1016/j.scitotenv.2023.167872",
        "paper_author": "Museru M.L.",
        "affiliation_name": "Department of Civil, Construction, and Environmental Engineering",
        "affiliation_city": "Birmingham",
        "affiliation_country": "United States",
        "affiliation_id": "60118852",
        "affiliation_state": "AL"
    },
    {
        "paper_title": "Enhancing Medicare Fraud Detection Through Machine Learning: Addressing Class Imbalance With SMOTE-ENN",
        "publication": "IEEE Access",
        "citied_by": "11",
        "cover_date": "2024-01-01",
        "Abstract": "The healthcare fraud detection field is constantly evolving and faces significant challenges, particularly when addressing imbalanced data issues. Previous studies mainly focused on traditional machine learning (ML) techniques, often struggling with imbalanced data. This problem arises in various aspects. It includes the risk of overfitting with Random Oversampling (ROS), noise introduction by the Synthetic Minority Oversampling Technique (SMOTE), and potential crucial information loss with Random Undersampling (RUS). Moreover, improving model performance, exploring hybrid resampling techniques, and enhancing evaluation metrics are crucial for achieving higher accuracy with imbalanced datasets. In this paper, we present a novel approach to tackle the issue of imbalanced datasets in healthcare fraud detection, with a specific focus on the Medicare Part B dataset. First, we carefully extract the categorical feature 'Provider Type' from the dataset. This allows us to generate new, synthetic instances by randomly replicating existing types, thereby increasing the diversity within the minority class. Then, we apply a hybrid resampling method named SMOTE-ENN, which combines the Synthetic Minority Over-sampling Technique (SMOTE) with Edited Nearest Neighbors (ENN). This method aims to balance the dataset by generating synthetic samples and removing noisy data to improve the accuracy of the models. We use six machine learning (ML) models to categorize the instances. When evaluating performance, we rely on common metrics like accuracy, F1 score, recall, precision, and the AUC-ROC curve. We highlight the significance of the Area Under the Precision-Recall Curve (AUPRC) for assessing performance in imbalanced dataset scenarios. The experiments show that Decision Trees (DT) outperformed all the classifiers, achieving a score of 0.99 across all metrics.",
        "DOI": "10.1109/ACCESS.2024.3385781",
        "paper_author": "Bounab R.",
        "affiliation_name": "Université Constantine 2",
        "affiliation_city": "Constantine",
        "affiliation_country": "Algeria",
        "affiliation_id": "60105374",
        "affiliation_state": "Constantine Province"
    },
    {
        "paper_title": "Data reduction techniques for highly imbalanced medicare Big Data",
        "publication": "Journal of Big Data",
        "citied_by": "11",
        "cover_date": "2024-12-01",
        "Abstract": "In the domain of Medicare insurance fraud detection, handling imbalanced Big Data and high dimensionality remains a significant challenge. This study assesses the combined efficacy of two data reduction techniques: Random Undersampling (RUS), and a novel ensemble supervised feature selection method. The techniques are applied to optimize Machine Learning models for fraud identification in the classification of highly imbalanced Big Medicare Data. Utilizing two datasets from The Centers for Medicare & Medicaid Services (CMS) labeled by the List of Excluded Individuals/Entities (LEIE), our principal contribution lies in empirically demonstrating that data reduction techniques applied to these datasets significantly improves classification performance. The study employs a systematic experimental design to investigate various scenarios, ranging from using each technique in isolation to employing them in combination. The results indicate that a synergistic application of both techniques outperforms models that utilize all available features and data. Moreover, reduction in the number of features leads to more explainable models. Given the enormous financial implications of Medicare fraud, our findings not only offer computational advantages but also significantly enhance the effectiveness of fraud detection systems, thereby having the potential to improve healthcare services.",
        "DOI": "10.1186/s40537-023-00869-3",
        "paper_author": "Hancock J.T.",
        "affiliation_name": "FAU College of Engineering and Computer Science",
        "affiliation_city": "Boca Raton",
        "affiliation_country": "United States",
        "affiliation_id": "60144960",
        "affiliation_state": "FL"
    },
    {
        "paper_title": "Teleradiology and technology innovations in radiology: status in India and its role in increasing access to primary health care",
        "publication": "The Lancet Regional Health - Southeast Asia",
        "citied_by": "11",
        "cover_date": "2024-04-01",
        "Abstract": "Background: There is an inequitable distribution of radiology facilities in India. This scoping review aimed at mapping the available technology instruments to improve access to imaging at primary health care; to identify the facilitators and barriers, and the knowledge gaps for widespread adaptation of technology solutions. Methods: A search was conducted using broad inclusive terms non-specific to subtypes of medical imaging devices or informatics. Work published in the English language between 2005 and 2022, conducted primarily in India, and with full manuscripts were included. Two authors independently screened the abstracts against the inclusion criteria for full-text review and a senior author settled discrepancies. Data were extracted using DistillerSR software. Findings: 43 original articles and 52 non-academic materials were finally reviewed. The data was from 10 Indian states with n = 9 from rural settings. The broad trends in original articles were: connectivity using teleradiology (n = 7), mobile digital imaging units (n = 9), artificial intelligence (n = 16); mobile devices and smartphone applications (n = 7); data security (n = 7) and web-based technology (n = 2); public-private partnership (n = 9); cost (n = 2); concordance (n = 19); evaluation (n = 4); implementation (n = 2). Interpretation: Available evidence suggests that teleradiology when combined with AI and mobile digital imaging units can address radiologist shortages; strengthen programs aimed at population screening and emergency care. However, there is insufficient data on the scale of teleradiology networks within India; needs assessment; cost; facilitators, and barriers for implementation of technologies solutions in primary healthcare settings. Regulations governing quality standards, data protection, and confidentiality are unclear. Funding: The authors are The Lancet Citizen's Commission fellows. The Lancet Commission has received financial support from the Lakshmi Mittal and Family South Asia Institute, Harvard University; Christian Medical College, Vellore (CMC), Vellore; Azim Premji Foundation, Infosys; Kirloskar Systems Ltd.; Mahindra & Mahindra Ltd.; Rohini Nilekani Philanthropies; and Serum Institute of India. The views expressed are those of the author(s) and not necessarily those of the Lancet Citizens' Commission or its partners.",
        "DOI": "10.1016/j.lansea.2023.100195",
        "paper_author": "Chandramohan A.",
        "affiliation_name": "Christian Medical College, Vellore",
        "affiliation_city": "Vellore",
        "affiliation_country": "India",
        "affiliation_id": "60000653",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Nature and Mental Health in Urban Texas: A NatureScore-Based Study",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "10",
        "cover_date": "2024-02-01",
        "Abstract": "In this cross-sectional study, we examined the impact of access to nature on mental health utilization in urban neighborhoods using Texas outpatient encounters data merged with NatureScoreTM (0–100; low to high nature levels) and US census data (household income, education, employment, poverty, and insurance coverage) at the zipcode level. Our sample size included 61 million outpatient encounters across 1169 zipcodes, with 63% women and 30% elderly. A total of 369,344 mental health encounters were identified, with anxiety/stress and depression encounters representing 68.3% and 23.6%, respectively. We found that neighborhoods with a NatureScore of 60+ had lower overall mental health utilization than those below 40 (RR 0.51, 95%CI 0.38–0.69). This relationship persisted for depression, bipolar disorder, and anxiety/stress and in neighborhoods with a NatureScore above 80 (p < 0.001). Compared to neighborhoods with a NatureScore below 40, those above 80 had significantly lower depression (aRR 0.68, 95%CI 0.49–0.95) and bipolar (aRR 0.59, 95%CI 0.36–0.99) health encounters after adjusting for demographic and socioeconomic factors. This novel approach, utilizing NatureScore as a proxy for urban greenness, demonstrates the correlation between a higher NatureScore and reduced mental health utilization. Our findings highlight the importance of integrating nature into our healthcare strategies to promote well-being and mental health.",
        "DOI": "10.3390/ijerph21020168",
        "paper_author": "Makram O.M.",
        "affiliation_name": "Houston Methodist",
        "affiliation_city": "Houston",
        "affiliation_country": "United States",
        "affiliation_id": "60008981",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "Recent advances and impending challenges for the radiopharmaceutical sciences in oncology",
        "publication": "The Lancet Oncology",
        "citied_by": "9",
        "cover_date": "2024-06-01",
        "Abstract": "This paper is the first of a Series on theranostics that summarises the current landscape of the radiopharmaceutical sciences as they pertain to oncology. In this Series paper, we describe exciting developments in radiochemistry and the production of radionuclides, the development and translation of theranostics, and the application of artificial intelligence to our field. These developments are catalysing growth in the use of radiopharmaceuticals to the benefit of patients worldwide. We also highlight some of the key issues to be addressed in the coming years to realise the full potential of radiopharmaceuticals to treat cancer.",
        "DOI": "10.1016/S1470-2045(24)00030-5",
        "paper_author": "Lapi S.E.",
        "affiliation_name": "O’Neal Comprehensive Cancer Center",
        "affiliation_city": "Birmingham",
        "affiliation_country": "United States",
        "affiliation_id": "60024925",
        "affiliation_state": "AL"
    },
    {
        "paper_title": "Global, regional, national trends of femur fracture and machine learning prediction: Comprehensive findings and questions from global burden of disease 1990–2019",
        "publication": "Journal of Orthopaedic Translation",
        "citied_by": "9",
        "cover_date": "2024-05-01",
        "Abstract": "Background: Femur fracture is a type of fracture with high disability and mortality. There is no comprehensive analysis and prediction of the global distribution of femur fractures, so we conducted this study. Methods: Age-standardized incidence rate (ASIR), age-standardized prevalence rate (ASPR), and years living with disability (YLDs) of femur fractures (excluding femoral neck) were downloaded from the Global burden of disease database. Trend analysis was performed, and 6 time-series machine learning algorithms were applied to predict the global ASIR, ASPR, and YLDs. Results: ASPR for femur fracture had been increasing in most countries worldwide from 1990 to 2019, with the highest in East Asia (AAPC = 1.25 95%Confidence Interval (1.2, 1.3)) and lowest in Central Latin America (AAPC = −0.74 95%CI (−0.81, −0.67)). However, ASIR showed a significant downward trend worldwide, with East Saharan Africa decreasing the most (AAPC = −4.04 95%CI (−5.56, −2.47)), and East Asia elevating the most (AAPC = 1.11 95%CI (0.87, 1.42)). YLDs were increasing over the world, with East Asia still elevating the most AAPC= (3.9 95%CI (3.85, 3.95)), with the only region of decrease being Eastern Europe (AAPC = −0.28 95%CI (−0.3, −0.26)). Both ASPR and ASIR were higher in women than in men in the >75 year group, whereas YLDs was lower in women than in men in the >60 year group. Globally, the ARIMA model was optimal in the prediction of ASPR, the PROPHET model effected in the prediction of ASIR, and the PROPHET WITH XGBOOST model was the best in the prediction of YLDs. The projections showed increase in both ASPR and YLDs, except for ASIR decreasing by 2030. Conclusions: Our study found a rise in femur fracture ASPR and ASIR from 1990 to 2019 in war conflict areas and East Asia, meanwhile, the YLDs of femur fracture increased in populous countries. In both 1990 and 2019, both ASPR and ASIR were higher in women over 75 years than that in men, but YLDs was higher in men over 60 years than that in women. In 2020–2030, while global femur fracture ASIR might decline, both ASPR and YLDs might rise. The Translational Potential of this article: Femur fracture is a high-energy injury due to direct violence, and in war, conflicting and underdeveloped regions such as East Asia. Accidental injuries may occur due to the rapid development of industry and the frequent traffic accidents. This study suggests that we should focus on elderly women (≥75 years) in the above regions in the future. For older men (>60 years old), more attention should be paid to post-fracture functional rehabilitation and early reintegration into society to reduce the disability rate and lower the socio-economic burden.",
        "DOI": "10.1016/j.jot.2024.03.002",
        "paper_author": "Wu J.",
        "affiliation_name": "Fudan University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60009860",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Voice as an AI Biomarker of Health - Introducing Audiomics",
        "publication": "JAMA Otolaryngology - Head and Neck Surgery",
        "citied_by": "9",
        "cover_date": "2024-04-11",
        "Abstract": "NA",
        "DOI": "10.1001/jamaoto.2023.4807",
        "paper_author": "Bensoussan Y.",
        "affiliation_name": "Morsani College of Medicine",
        "affiliation_city": "Tampa",
        "affiliation_country": "United States",
        "affiliation_id": "60011951",
        "affiliation_state": "FL"
    },
    {
        "paper_title": "Image annotation and curation in radiology: an overview for machine learning practitioners",
        "publication": "European Radiology Experimental",
        "citied_by": "9",
        "cover_date": "2024-12-01",
        "Abstract": "“Garbage in, garbage out” summarises well the importance of high-quality data in machine learning and artificial intelligence. All data used to train and validate models should indeed be consistent, standardised, traceable, correctly annotated, and de-identified, considering local regulations. This narrative review presents a summary of the techniques that are used to ensure that all these requirements are fulfilled, with special emphasis on radiological imaging and freely available software solutions that can be directly employed by the interested researcher. Topics discussed include key imaging concepts, such as image resolution and pixel depth; file formats for medical image data storage; free software solutions for medical image processing; anonymisation and pseudonymisation to protect patient privacy, including compliance with regulations such as the Regulation (EU) 2016/679 “General Data Protection Regulation” (GDPR) and the 1996 United States Act of Congress “Health Insurance Portability and Accountability Act” (HIPAA); methods to eliminate patient-identifying features within images, like facial structures; free and commercial tools for image annotation; and techniques for data harmonisation and normalisation. Relevance statement This review provides an overview of the methods and tools that can be used to ensure high-quality data for machine learning and artificial intelligence applications in radiology. Key points • High-quality datasets are essential for reliable artificial intelligence algorithms in medical imaging. • Software tools like ImageJ and 3D Slicer aid in processing medical images for AI research. • Anonymisation techniques protect patient privacy during dataset preparation. • Machine learning models can accelerate image annotation, enhancing efficiency and accuracy. • Data curation ensures dataset integrity, compliance, and quality for artificial intelligence development.",
        "DOI": "10.1186/s41747-023-00408-y",
        "paper_author": "Galbusera F.",
        "affiliation_name": "Schulthess Klinik",
        "affiliation_city": "Zurich",
        "affiliation_country": "Switzerland",
        "affiliation_id": "60004222",
        "affiliation_state": "ZH"
    },
    {
        "paper_title": "What makes accidents severe! explainable analytics framework with parameter optimization",
        "publication": "European Journal of Operational Research",
        "citied_by": "9",
        "cover_date": "2024-09-01",
        "Abstract": "Most analytics models are built on complex internal learning processes and calculations, which might be unintuitive, opaque, and incomprehensible to humans. Analytics-based decisions must be transparent and intuitive to foster greater human acceptability and confidence in analytics. Explainable analytics models are transparent models in which the primary factors and weights that lead to a prediction can be explained. Typical AI models are non-transparent or opaque models, in which even the designers cannot explain how their models arrive at a specific decision. These transparent models help decision-makers understand their judgments and build trust in analytics. This study introduces an innovative, comprehensive model that fuses descriptive, predictive, and prescriptive analytics, offering a fresh perspective on car accident severity. Our methodological contribution lies in the application of advanced techniques to address data-related challenges, optimize feature selection, develop predictive models, and fine-tune parameters. Importantly, we also incorporate model-agnostic interpretation techniques, further enhancing the transparency and interpretability of our model, and separate explanations from models (i.e., model-agnostic interpretation techniques). Our findings should provide novel insights for a domain expert to understand accident severity. The explainable analytics approach suggested in this study supplements non-transparent machine learning prediction models, particularly optimized ensemble models. Our model's end product is a comprehensible representation of crash severity factors. To obtain a more trustworthy assessment of accident severity, this model may be supplemented with insurance data, medical data such as blood work and pulse rate, and previous medical history.",
        "DOI": "10.1016/j.ejor.2023.11.013",
        "paper_author": "Ahmed A.",
        "affiliation_name": "University of Alabama at Birmingham School of Health Professions",
        "affiliation_city": "Birmingham",
        "affiliation_country": "United States",
        "affiliation_id": "60118868",
        "affiliation_state": "AL"
    },
    {
        "paper_title": "Using a Bayesian Belief Network to detect healthcare fraud",
        "publication": "Expert Systems with Applications",
        "citied_by": "9",
        "cover_date": "2024-03-15",
        "Abstract": "Healthcare fraud detection algorithms are mostly based on applying machine learning methods to payer-claims data transactions to detect fraudulent activities. However, claim transactions are often analyzed in isolation, disregarding the interconnections that naturally exist in the claims generated from a single unit such as the patient, payer, or provider. Fraud detection frameworks have untapped potential to aid downstream processes such as investigations and audits through transparent and interpretable machine learning models. In this study, we show how fraud detection frameworks can benefit from use of a graphical network model called Bayesian Belief Network (BBN) which exploits the relational structure of attributes in a transaction and has better interpretability properties than many competing machine learning methods. BBN model performance is found to be comparable to commonly utilized baseline models such as logistic regression and random forest after validation with a real-life insurance claims dataset. In particular, the F1 score of fraud class for the best performing BBN was 0.15. The proposed BBN model is also considered to be interpretable to auditors and investigators based on common sense qualitative evaluation.",
        "DOI": "10.1016/j.eswa.2023.122241",
        "paper_author": "Kumaraswamy N.",
        "affiliation_name": "The University of Texas at Austin",
        "affiliation_city": "Austin",
        "affiliation_country": "United States",
        "affiliation_id": "60013372",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "Health consequences of structural sexism: Conceptual foundations, empirical evidence and priorities for future research",
        "publication": "Social Science and Medicine",
        "citied_by": "8",
        "cover_date": "2024-06-01",
        "Abstract": "A nascent body of work has begun exploring the health consequences of structural sexism. This article provides an overview of the concept of structural sexism and an elaboration of the potential pathways connecting it to health. Next, it reviews existing measurement approaches and the current state of empirical evidence on the relationship between structural sexism and health in the United States. Finally, it highlights key priorities for future research, which include: expanding and refining measures, increasing public data availability, broadening the scope of inquiry to include a wider range of outcomes, exploring mechanisms, incorporating intersectionality, and applying a life course lens.",
        "DOI": "10.1016/j.socscimed.2023.116379",
        "paper_author": "Homan P.",
        "affiliation_name": "Florida State University",
        "affiliation_city": "Tallahassee",
        "affiliation_country": "United States",
        "affiliation_id": "60002092",
        "affiliation_state": "FL"
    },
    {
        "paper_title": "Development of an automated method for flood inundation monitoring, flood hazard, and soil erosion susceptibility assessment using machine learning and AHP–MCE techniques",
        "publication": "Geoenvironmental Disasters",
        "citied_by": "8",
        "cover_date": "2024-12-01",
        "Abstract": "Background: Operational large-scale flood monitoring using publicly available satellite data is possible with the advent of Sentinel-1 microwave data, which enables near-real-time (at 6-day intervals) flood mapping day and night, even in cloudy monsoon seasons. Automated flood inundation area identification in near-real-time involves advanced geospatial data processing platforms, such as Google Earth Engine and robust methodology (Otsu’s algorithm). Objectives: The current study employs Sentinel-1 microwave data for flood extent mapping using machine learning (ML) algorithms in Assam State, India. We generated a flood hazard and soil erosion susceptibility map by combining multi-source data on weather conditions and soil and terrain characteristics. Random Forest (RF), Classification and Regression Tool (CART), and Support Vector Machine (SVM) ML algorithms were applied to generate the flood hazard map. Furthermore, we employed the multicriteria evaluation (MCE) analytical hierarchical process (AHP) for soil erosion susceptibility mapping. Summary: The highest prediction accuracy was observed for the RF model (overall accuracy [OA] > 82%), followed by the SVM (OA > 82%) and CART (OA > 81%). Over 26% of the study area indicated high flood hazard-prone areas, and approximately 60% showed high and severe potential for soil erosion due to flooding. The automated flood mapping platform is an essential resource for emergency responders and decision-makers, as it helps to guide relief activities by identifying suitable regions and appropriate logistic route planning and improving the accuracy and timeliness ofemergency response efforts. Periodic flood inundation maps will help in long-term planning and policymaking, flood management, soil and biodiversity conservation, land degradation, planning sustainable agriculture interventions, crop insurance, and climate resilience studies.",
        "DOI": "10.1186/s40677-024-00275-8",
        "paper_author": "Prakash A.J.",
        "affiliation_name": "Indian Institute of Technology Kharagpur",
        "affiliation_city": "Kharagpur",
        "affiliation_country": "India",
        "affiliation_id": "60004750",
        "affiliation_state": "WB"
    },
    {
        "paper_title": "An advanced blockchain-based hyperledger fabric solution for tracing fraudulent claims in the healthcare industry",
        "publication": "Decision Analytics Journal",
        "citied_by": "8",
        "cover_date": "2024-03-01",
        "Abstract": "Blockchain and Machine Learning (ML) are state-of-the-art technologies in the digital era. Developing countries have witnessed great digital transformation, and one of the key challenges during this development phase has been to create a platform that integrates the health and vitals of citizens confidentially. The healthcare insurance industry is one sector that has experienced a significant number of instances of fraudulent claims and mismanagement of patient data. These issues often arise due to inadequate technological integration and an over-reliance on manual processes and human intervention. The insurance industry relies on multiple processes between end users to initiate, maintain, and close diverse policies. The proposed model initially recommends a suitable insurance policy for newly admitted patients, but in the case of existing patients, the objectives are to speed up transaction processing and payment settlement securely using a private blockchain. Collectively, these two technologies possess the potential to revolutionize the future. This study aims to incorporate blockchain and ML techniques like Support Vector Machine (SVM) and Random Forest Regression, which can differentiate between fraudulent and legal medical records to recommend personalized policies, streamline claim processing, and ensure the security of sensitive patient information and vital insurance records. The key aim is to create a more patient-centric environment with data transparency. This integrated framework results in a secure, adaptable, and efficient ecosystem that outperforms traditional methods, paving the way for the future of healthcare and insurance services.",
        "DOI": "10.1016/j.dajour.2024.100411",
        "paper_author": "Jena S.K.",
        "affiliation_name": "C. V. Raman Global University",
        "affiliation_city": "Bhubaneswar",
        "affiliation_country": "India",
        "affiliation_id": "60078476",
        "affiliation_state": "OR"
    },
    {
        "paper_title": "Data Mining-based Financial Statement Fraud Detection: Systematic Literature Review and Meta-analysis to Estimate Data Sample Mapping of Fraudulent Companies Against Non-fraudulent Companies",
        "publication": "Global Business Review",
        "citied_by": "8",
        "cover_date": "2024-10-01",
        "Abstract": "Data mining techniques have proven quite effective not only in detecting financial statement frauds but also in discovering other financial crimes, such as credit card frauds, loan and security frauds, corporate frauds, bank and insurance frauds, etc. Classification of data mining techniques, in recent years, has been accepted as one of the most credible methodologies for the detection of symptoms of financial statement frauds through scanning the published financial statements of companies. The retrieved literature that has used data mining classification techniques can be broadly categorized on the basis of the type of technique applied, as statistical techniques and machine learning techniques. The biggest challenge in executing the classification process using data mining techniques lies in collecting the data sample of fraudulent companies and mapping the sample of fraudulent companies against non-fraudulent companies. In this article, a systematic literature review (SLR) of studies from the area of financial statement fraud detection has been conducted. The review has considered research articles published between 1995 and 2020. Further, a meta-analysis has been performed to establish the effect of data sample mapping of fraudulent companies against non-fraudulent companies on the classification methods through comparing the overall classification accuracy reported in the literature. The retrieved literature indicates that a fraudulent sample can either be equally paired with non-fraudulent sample (1:1 data mapping) or be unequally mapped using 1:many ratio to increase the sample size proportionally. Based on the meta-analysis of the research articles, it can be concluded that machine learning approaches, in comparison to statistical approaches, can achieve better classification accuracy, particularly when the availability of sample data is low. High classification accuracy can be obtained with even a 1:1 mapping data set using machine learning classification approaches.",
        "DOI": "10.1177/0972150920984857",
        "paper_author": "Gupta S.",
        "affiliation_name": "Shri Mata Vaishno Devi University",
        "affiliation_city": "Katra",
        "affiliation_country": "India",
        "affiliation_id": "60017187",
        "affiliation_state": "JK"
    },
    {
        "paper_title": "Blockchain integration in healthcare: a comprehensive investigation of use cases, performance issues, and mitigation strategies",
        "publication": "Frontiers in Digital Health",
        "citied_by": "7",
        "cover_date": "2024-01-01",
        "Abstract": "Healthcare is a critical area where blockchain technology (BT) is being heralded as a potential game-changer for facilitating secure and efficient data sharing. The purpose of this review is to examine BT applications, performance challenges, and solutions in healthcare. To begin, This review paper explores popular blockchain networks for data exchange, encompassing both public and permissioned platforms, such as Ethereum and Hyperledger Fabric. This paper analyzes the potential applications of BT’s decentralized, immutable, and smart contract capabilities in healthcare settings, including secure and interoperable health data exchange, patient consent management, drug supply chain oversight, and clinical trial management. The healthcare industry might greatly benefit from the increased privacy, transparency, and accessibility that these technologies provide. Despite BT’s promising medical uses, the technology is not without its drawbacks. High energy consumption, throughput, and scalability are all concerns. We wrapped up by discussing the solutions that have been implemented, including consensus processes, scalability measures like sharding, and off-chain transactions that are designed to mitigate the drawbacks.",
        "DOI": "10.3389/fdgth.2024.1359858",
        "paper_author": "Kasyapa M.S.B.",
        "affiliation_name": "Vellore Institute of Technology",
        "affiliation_city": "Vellore",
        "affiliation_country": "India",
        "affiliation_id": "60010618",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Enhanced thyroid nodule segmentation through U-Net and VGG16 fusion with feature engineering: A comprehensive study",
        "publication": "Computer Methods and Programs in Biomedicine",
        "citied_by": "7",
        "cover_date": "2024-06-01",
        "Abstract": "Background and Objective: The thyroid gland, a key component of the endocrine system, is pivotal in regulating bodily functions. Thermography, a non-invasive imaging technique utilizing infrared cameras, has emerged as a diagnostic tool for thyroid-related conditions, offering advantages such as early detection and risk stratification. Artificial intelligence (AI) has demonstrated success in medical diagnostics, and its integration into thermal imaging analysis holds promise for improving diagnostic capabilities. This study aims to explore the potential of AI, specifically convolutional neural networks (CNNs), in enhancing the analysis of thyroid thermograms for the detection of nodules and abnormalities. Methods: Artificial intelligence (AI) and machine learning techniques are integrated to enhance thyroid thermal image analysis. Specifically, a fusion of U-Net and VGG16, combined with feature engineering (FE), is proposed for accurate thyroid nodule segmentation. The novelty of this research lies in leveraging feature engineering in transfer learning for the segmentation of thyroid nodules, even in the presence of a limited dataset. Results: The study presents results from four conducted studies, demonstrating the efficacy of this approach even with a limited dataset. It's observed that in study 4, using FE has led to a significant improvement in the value of the dice coefficient. Even for the small size of the masked region, incorporating radiomics with FE resulted in significant improvements in the segmentation dice coefficient. It's promising that one can achieve higher dice coefficients by employing different models and refining them. Conclusion: The findings here underscore the potential of AI for precise and efficient segmentation of thyroid nodules, paving the way for improved thyroid health assessment.",
        "DOI": "10.1016/j.cmpb.2024.108209",
        "paper_author": "Etehadtavakol M.",
        "affiliation_name": "SBUMS School of Medicine",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran",
        "affiliation_id": "60112701",
        "affiliation_state": "Tehran"
    }
]