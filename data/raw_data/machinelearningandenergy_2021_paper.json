[
    {
        "paper_title": "Machine Learning Force Fields",
        "publication": "Chemical Reviews",
        "citied_by": "799",
        "cover_date": "2021-08-25",
        "Abstract": "In recent years, the use of machine learning (ML) in computational chemistry has enabled numerous advances previously out of reach due to the computational complexity of traditional electronic-structure methods. One of the most promising applications is the construction of ML-based force fields (FFs), with the aim to narrow the gap between the accuracy of ab initio methods and the efficiency of classical FFs. The key idea is to learn the statistical relation between chemical structure and potential energy without relying on a preconceived notion of fixed chemical bonds or knowledge about the relevant interactions. Such universal ML approximations are in principle only limited by the quality and quantity of the reference data used to train them. This review gives an overview of applications of ML-FFs and the chemical insights that can be obtained from them. The core concepts underlying ML-FFs are described in detail, and a step-by-step guide for constructing and testing them from scratch is given. The text concludes with a discussion of the challenges that remain to be overcome by the next generation of ML-FFs.",
        "DOI": "10.1021/acs.chemrev.0c01111",
        "paper_author": "Unke O.T.",
        "affiliation_name": "Technische Universität Berlin",
        "affiliation_city": "Berlin",
        "affiliation_country": "Germany",
        "affiliation_id": "60011604",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Artificial intelligence in sustainable energy industry: Status Quo, challenges and opportunities",
        "publication": "Journal of Cleaner Production",
        "citied_by": "601",
        "cover_date": "2021-03-20",
        "Abstract": "The energy industry is at a crossroads. Digital technological developments have the potential to change our energy supply, trade, and consumption dramatically. The new digitalization model is powered by the artificial intelligence (AI) technology. The integration of energy supply, demand, and renewable sources into the power grid will be controlled autonomously by smart software that optimizes decision-making and operations. AI will play an integral role in achieving this goal. This study focuses on the use of AI techniques in the energy sector. This study aims to present a realistic baseline that allows researchers and readers to compare their AI efforts, ambitions, new state-of-the-art applications, challenges, and global roles in policymaking. We covered three major aspects, including: i) the use of AI in solar and hydrogen power generation; (ii) the use of AI in supply and demand management control; and (iii) recent advances in AI technology. This study explored how AI techniques outperform traditional models in controllability, big data handling, cyberattack prevention, smart grid, IoT, robotics, energy efficiency optimization, predictive maintenance control, and computational efficiency. Big data, the development of a machine learning model, and AI will play an important role in the future energy market. Our study's findings show that AI is becoming a key enabler of a complex, new and data-related energy industry, providing a key magic tool to increase operational performance and efficiency in an increasingly cut-throat environment. As a result, the energy industry, utilities, power system operators, and independent power producers may need to focus more on AI technologies if they want meaningful results to remain competitive. New competitors, new business strategies, and a more active approach to customers would require informed and flexible regulatory engagement with the associated complexities of customer safety, privacy, and information security. Given the pace of development in information technology, AI and data analysis, regulatory approvals for new services and products in the new Era of digital energy markets can be enforced as quickly and efficiently as possible.",
        "DOI": "10.1016/j.jclepro.2021.125834",
        "paper_author": "Ahmad T.",
        "affiliation_name": "State Key Laboratory of Internet of Things for Smart City",
        "affiliation_city": "Taipa",
        "affiliation_country": "Macao",
        "affiliation_id": "60202520",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "High-entropy ceramics: Review of principles, production and applications",
        "publication": "Materials Science and Engineering R: Reports",
        "citied_by": "545",
        "cover_date": "2021-10-01",
        "Abstract": "High-entropy ceramics with five or more cations have recently attracted significant attention due to their superior properties for various structural and functional applications. Although the multi-component ceramics have been of interest for several decades, the concept of high-entropy ceramics was defined in 2004 by producing the first high-entropy nitride films. Following the introduction of the entropy stabilization concept, significant efforts were started to increase the entropy, minimize the Gibbs free energy and achieve stable single-phase high-entropy ceramics. High-entropy oxides, nitrides, carbides, borides and hydrides are currently the most popular high-entropy ceramics due to their potential for various applications, while the study of other ceramics, such as silicides, sulfides, fluorides, phosphides, phosphates, oxynitrides, carbonitrides and borocarbonitrides, is also growing fast. In this paper, the progress regarding high-entropy ceramics is reviewed from both experimental and theoretical points of view. Different aspects including the history, principles, compositions, crystal structure, theoretical/empirical design (via density functional theory, molecular dynamics simulation, machine learning, CALPHAD and descriptors), production methods and properties are thoroughly reviewed. The paper specifically attempts to answer how these materials with remarkable structures and properties can be used in future applications.",
        "DOI": "10.1016/j.mser.2021.100644",
        "paper_author": "Akrami S.",
        "affiliation_name": "Nagoya Institute of Technology",
        "affiliation_city": "Nagoya",
        "affiliation_country": "Japan",
        "affiliation_id": "60024751",
        "affiliation_state": "Aichi"
    },
    {
        "paper_title": "High-resolution X-ray luminescence extension imaging",
        "publication": "Nature",
        "citied_by": "536",
        "cover_date": "2021-02-18",
        "Abstract": "Current X-ray imaging technologies involving flat-panel detectors have difficulty in imaging three-dimensional objects because fabrication of large-area, flexible, silicon-based photodetectors on highly curved surfaces remains a challenge1–3. Here we demonstrate ultralong-lived X-ray trapping for flat-panel-free, high-resolution, three-dimensional imaging using a series of solution-processable, lanthanide-doped nanoscintillators. Corroborated by quantum mechanical simulations of defect formation and electronic structures, our experimental characterizations reveal that slow hopping of trapped electrons due to radiation-triggered anionic migration in host lattices can induce more than 30 days of persistent radioluminescence. We further demonstrate X-ray luminescence extension imaging with resolution greater than 20 line pairs per millimetre and optical memory longer than 15 days. These findings provide insight into mechanisms underlying X-ray energy conversion through enduring electron trapping and offer a paradigm to motivate future research in wearable X-ray detectors for patient-centred radiography and mammography, imaging-guided therapeutics, high-energy physics and deep learning in radiology.",
        "DOI": "10.1038/s41586-021-03251-6",
        "paper_author": "Ou X.",
        "affiliation_name": "Fuzhou University",
        "affiliation_city": "Fuzhou",
        "affiliation_country": "China",
        "affiliation_id": "60017605",
        "affiliation_state": "Fujian"
    },
    {
        "paper_title": "Pruning and quantization for deep neural network acceleration: A survey",
        "publication": "Neurocomputing",
        "citied_by": "503",
        "cover_date": "2021-10-21",
        "Abstract": "Deep neural networks have been applied in many applications exhibiting extraordinary abilities in the field of computer vision. However, complex network architectures challenge efficient real-time deployment and require significant computation resources and energy costs. These challenges can be overcome through optimizations such as network compression. Network compression can often be realized with little loss of accuracy. In some cases accuracy may even improve. This paper provides a survey on two types of network compression: pruning and quantization. Pruning can be categorized as static if it is performed offline or dynamic if it is performed at run-time. We compare pruning techniques and describe criteria used to remove redundant computations. We discuss trade-offs in element-wise, channel-wise, shape-wise, filter-wise, layer-wise and even network-wise pruning. Quantization reduces computations by reducing the precision of the datatype. Weights, biases, and activations may be quantized typically to 8-bit integers although lower bit width implementations are also discussed including binary neural networks. Both pruning and quantization can be used independently or combined. We compare current techniques, analyze their strengths and weaknesses, present compressed network accuracy results on a number of frameworks, and provide practical guidance for compressing networks.",
        "DOI": "10.1016/j.neucom.2021.07.045",
        "paper_author": "Liang T.",
        "affiliation_name": "University of Science and Technology Beijing",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60018273",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Performance of deep learning vs machine learning in plant leaf disease detection",
        "publication": "Microprocessors and Microsystems",
        "citied_by": "451",
        "cover_date": "2021-02-01",
        "Abstract": "Plants are recognized as essential as they are the primary source of humanity's energy production since they are having nutritious, medicinal, etc. values. At any time between crop farming, plant diseases can affect the leaf, resulting in enormous crop production damages and economic market value. Therefore, in the farming industry, identification of leaf disease plays a crucial role. It needs, however, enormous labor, greater preparation time, and comprehensive plant pathogen knowledge. For the identification of plant disease detection various machine learning (ML) as well as deep learning (DL) methods are developed & examined by various researchers, and many of the times they also got significant results in both cases. Motivated by those existing works, here in this article we are comparing the performance of ML (Support Vector Machine (SVM), Random Forest (RF), Stochastic Gradient Descent (SGD)) & DL (Inception-v3, VGG-16, VGG-19) in terms of citrus plant disease detection. The disease classification accuracy (CA) we received by experimentation is quite impressive as DL methods perform better than that of ML methods in case of disease detection as follows: RF-76.8% > SGD-86.5% > SVM-87% > VGG-19–87.4% > Inception-v3–89% > VGG-16–89.5%. From the result, we can tell that RF is giving the least CA whereas VGG-16 is giving the best in terms of CA.",
        "DOI": "10.1016/j.micpro.2020.103615",
        "paper_author": "Sujatha R.",
        "affiliation_name": "Vellore Institute of Technology",
        "affiliation_city": "Vellore",
        "affiliation_country": "India",
        "affiliation_id": "60010618",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Exceptional piezoelectricity, high thermal conductivity and stiffness and promising photocatalysis in two-dimensional MoSi<inf>2</inf>N<inf>4</inf> family confirmed by first-principles",
        "publication": "Nano Energy",
        "citied_by": "447",
        "cover_date": "2021-04-01",
        "Abstract": "Chemical vapor deposition has been most recently employed to fabricate centimeter-scale high-quality single-layer MoSi2N4 (Science; 2020;369; 670). Motivated by this exciting experimental advance, herein we conduct extensive first-principles based simulations to explore the stability, mechanical properties, lattice thermal conductivity, piezoelectric and flexoelectric response, and photocatalytic and electronic features of MA2Z4 (M = Cr, Mo, W; A = Si, Ge; Z = N, P) monolayers. The considered nanosheets are found to exhibit dynamical stability and remarkably high mechanical properties. Moreover, they show diverse electronic properties from antiferromagnetic metal to half metal and to semiconductors with band gaps ranging from 0.31 to 2.57 eV. Among the studied nanosheets, the MoSi2N4 and WSi2N4 monolayers yield appropriate band edge positions, high electron and hole mobilities, and strong visible light absorption, highly promising for applications in optoelectronics and photocatalytic water splitting. The MoSi2N4 and WSi2N4 monolayers are also predicted to show outstandingly high lattice thermal conductivity of 440 and 500 W/mK, respectively. For the first time we show that machine learning interatomic potentials trained over small supercells can be employed to examine the flexoelectric and piezoelectric properties of complex structures. As the most exciting finding, WSi2N4, CrSi2N4 and MoSi2N4 are found to exhibit the highest piezoelectric coefficients, outperforming all other-known 2D materials. Our results highlight that MA2Z4 nanosheets not only undoubtedly outperform the transition metal dichalcogenides family but also can compete with graphene for applications in nanoelectronics, optoelectronic, energy storage/conversion and thermal management systems.",
        "DOI": "10.1016/j.nanoen.2020.105716",
        "paper_author": "Mortazavi B.",
        "affiliation_name": "Gottfried Wilhelm Leibniz Universität Hannover",
        "affiliation_city": "Hannover",
        "affiliation_country": "Germany",
        "affiliation_id": "60004935",
        "affiliation_state": "Niedersachsen"
    },
    {
        "paper_title": "Four Generations of High-Dimensional Neural Network Potentials",
        "publication": "Chemical Reviews",
        "citied_by": "440",
        "cover_date": "2021-08-25",
        "Abstract": "Since their introduction about 25 years ago, machine learning (ML) potentials have become an important tool in the field of atomistic simulations. After the initial decade, in which neural networks were successfully used to construct potentials for rather small molecular systems, the development of high-dimensional neural network potentials (HDNNPs) in 2007 opened the way for the application of ML potentials in simulations of large systems containing thousands of atoms. To date, many other types of ML potentials have been proposed continuously increasing the range of problems that can be studied. In this review, the methodology of the family of HDNNPs including new recent developments will be discussed using a classification scheme into four generations of potentials, which is also applicable to many other types of ML potentials. The first generation is formed by early neural network potentials designed for low-dimensional systems. High-dimensional neural network potentials established the second generation and are based on three key steps: First, the expression of the total energy as a sum of environment-dependent atomic energy contributions; second, the description of the atomic environments by atom-centered symmetry functions as descriptors fulfilling the requirements of rotational, translational, and permutation invariance; and third, the iterative construction of the reference electronic structure data sets by active learning. In third-generation HDNNPs, in addition, long-range interactions are included employing environment-dependent partial charges expressed by atomic neural networks. In fourth-generation HDNNPs, which are just emerging, in addition, nonlocal phenomena such as long-range charge transfer can be included. The applicability and remaining limitations of HDNNPs are discussed along with an outlook at possible future developments.",
        "DOI": "10.1021/acs.chemrev.0c00868",
        "paper_author": "Behler J.",
        "affiliation_name": "Georg-August-Universität Göttingen",
        "affiliation_city": "Gottingen",
        "affiliation_country": "Germany",
        "affiliation_id": "60031514",
        "affiliation_state": "Niedersachsen"
    },
    {
        "paper_title": "Artificial intelligence based anomaly detection of energy consumption in buildings: A review, current trends and new perspectives",
        "publication": "Applied Energy",
        "citied_by": "409",
        "cover_date": "2021-04-01",
        "Abstract": "Enormous amounts of data are being produced everyday by sub-meters and smart sensors installed in residential buildings. If leveraged properly, that data could assist end-users, energy producers and utility companies in detecting anomalous power consumption and understanding the causes of each anomaly. Therefore, anomaly detection could stop a minor problem becoming overwhelming. Moreover, it will aid in better decision-making to reduce wasted energy and promote sustainable and energy efficient behavior. In this regard, this paper is an in-depth review of existing anomaly detection frameworks for building energy consumption based on artificial intelligence. Specifically, an extensive survey is presented, in which a comprehensive taxonomy is introduced to classify existing algorithms based on different modules and parameters adopted, such as machine learning algorithms, feature extraction approaches, anomaly detection levels, computing platforms and application scenarios. To the best of the authors’ knowledge, this is the first review article that discusses anomaly detection in building energy consumption. Moving forward, important findings along with domain-specific problems, difficulties and challenges that remain unresolved are thoroughly discussed, including the absence of: (i) precise definitions of anomalous power consumption, (ii) annotated datasets, (iii) unified metrics to assess the performance of existing solutions, (iv) platforms for reproducibility and (v) privacy-preservation. Following, insights about current research trends are discussed to widen the applications and effectiveness of the anomaly detection technology before deriving future directions attracting significant attention. This article serves as a comprehensive reference to understand the current technological progress in anomaly detection of energy consumption based on artificial intelligence.",
        "DOI": "10.1016/j.apenergy.2021.116601",
        "paper_author": "Himeur Y.",
        "affiliation_name": "Department of Electrical Engineering, College of Engineering, Qatar University",
        "affiliation_city": "Doha",
        "affiliation_country": "Qatar",
        "affiliation_id": "60197138",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A graph placement methodology for fast chip design",
        "publication": "Nature",
        "citied_by": "403",
        "cover_date": "2021-06-10",
        "Abstract": "Chip floorplanning is the engineering task of designing the physical layout of a computer chip. Despite five decades of research1, chip floorplanning has defied automation, requiring months of intense effort by physical design engineers to produce manufacturable layouts. Here we present a deep reinforcement learning approach to chip floorplanning. In under six hours, our method automatically generates chip floorplans that are superior or comparable to those produced by humans in all key metrics, including power consumption, performance and chip area. To achieve this, we pose chip floorplanning as a reinforcement learning problem, and develop an edge-based graph convolutional neural network architecture capable of learning rich and transferable representations of the chip. As a result, our method utilizes past experience to become better and faster at solving new instances of the problem, allowing chip design to be performed by artificial agents with more experience than any human designer. Our method was used to design the next generation of Google’s artificial intelligence (AI) accelerators, and has the potential to save thousands of hours of human effort for each new generation. Finally, we believe that more powerful AI-designed hardware will fuel advances in AI, creating a symbiotic relationship between the two fields.",
        "DOI": "10.1038/s41586-021-03544-w",
        "paper_author": "Mirhoseini A.",
        "affiliation_name": "Google LLC",
        "affiliation_city": "Mountain View",
        "affiliation_country": "United States",
        "affiliation_id": "60006191",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "A review of machine learning in building load prediction",
        "publication": "Applied Energy",
        "citied_by": "399",
        "cover_date": "2021-03-01",
        "Abstract": "The surge of machine learning and increasing data accessibility in buildings provide great opportunities for applying machine learning to building energy system modeling and analysis. Building load prediction is one of the most critical components for many building control and analytics activities, as well as grid-interactive and energy efficiency building operation. While a large number of research papers exist on the topic of machine-learning-based building load prediction, a comprehensive review from the perspective of machine learning is missing. In this paper, we review the application of machine learning techniques in building load prediction under the organization and logic of the machine learning, which is to perform tasks T using Performance measure P and based on learning from Experience E. Firstly, we review the applications of building load prediction model (task T). Then, we review the modeling algorithms that improve machine learning performance and accuracy (performance P). Throughout the papers, we also review the literature from the data perspective for modeling (experience E), including data engineering from the sensor level to data level, pre-processing, feature extraction and selection. Finally, we conclude with a discussion of well-studied and relatively unexplored fields for future research reference. We also identify the gaps in current machine learning application and predict for future trends and development.",
        "DOI": "10.1016/j.apenergy.2021.116452",
        "paper_author": "Zhang L.",
        "affiliation_name": "National Renewable Energy Laboratory",
        "affiliation_city": "Golden",
        "affiliation_country": "United States",
        "affiliation_id": "60030451",
        "affiliation_state": "CO"
    },
    {
        "paper_title": "Artificial neural networks based optimization techniques: A review",
        "publication": "Electronics (Switzerland)",
        "citied_by": "378",
        "cover_date": "2021-11-01",
        "Abstract": "In the last few years, intensive research has been done to enhance artificial intelligence (AI) using optimization techniques. In this paper, we present an extensive review of artificial neural networks (ANNs) based optimization algorithm techniques with some of the famous optimization techniques, e.g., genetic algorithm (GA), particle swarm optimization (PSO), artificial bee colony (ABC), and backtracking search algorithm (BSA) and some modern developed techniques, e.g., the lightning search algorithm (LSA) and whale optimization algorithm (WOA), and many more. The entire set of such techniques is classified as algorithms based on a population where the initial population is randomly created. Input parameters are initialized within the specified range, and they can provide optimal solutions. This paper emphasizes enhancing the neural network via optimization algorithms by manipulating its tuned parameters or training parameters to obtain the best structure network pattern to dissolve the problems in the best way. This paper includes some results for improving the ANN performance by PSO, GA, ABC, and BSA optimization techniques, respectively, to search for optimal parameters, e.g., the number of neurons in the hidden layers and learning rate. The obtained neural net is used for solving energy management problems in the virtual power plant system.",
        "DOI": "10.3390/electronics10212689",
        "paper_author": "Abdolrasol M.G.M.",
        "affiliation_name": "Universiti Kebangsaan Malaysia",
        "affiliation_city": "Bangi",
        "affiliation_country": "Malaysia",
        "affiliation_id": "60001821",
        "affiliation_state": "Selangor"
    },
    {
        "paper_title": "Open Catalyst 2020 (OC20) Dataset and Community Challenges",
        "publication": "ACS Catalysis",
        "citied_by": "374",
        "cover_date": "2021-05-21",
        "Abstract": "Catalyst discovery and optimization is key to solving many societal and energy challenges including solar fuel synthesis, long-term energy storage, and renewable fertilizer production. Despite considerable effort by the catalysis community to apply machine learning models to the computational catalyst discovery process, it remains an open challenge to build models that can generalize across both elemental compositions of surfaces and adsorbate identity/configurations, perhaps because datasets have been smaller in catalysis than in related fields. To address this, we developed the OC20 dataset, consisting of 1,281,040 density functional theory (DFT) relaxations (∼264,890,000 single-point evaluations) across a wide swath of materials, surfaces, and adsorbates (nitrogen, carbon, and oxygen chemistries). We supplemented this dataset with randomly perturbed structures, short timescale molecular dynamics, and electronic structure analyses. The dataset comprises three central tasks indicative of day-to-day catalyst modeling and comes with predefined train/validation/test splits to facilitate direct comparisons with future model development efforts. We applied three state-of-the-art graph neural network models (CGCNN, SchNet, and DimeNet++) to each of these tasks as baseline demonstrations for the community to build on. In almost every task, no upper limit on model size was identified, suggesting that even larger models are likely to improve on initial results. The dataset and baseline models are both provided as open resources as well as a public leader board to encourage community contributions to solve these important tasks.",
        "DOI": "10.1021/acscatal.0c04525",
        "paper_author": "Chanussot L.",
        "affiliation_name": "Facebook Research",
        "affiliation_city": "Menlo Park",
        "affiliation_country": "United States",
        "affiliation_id": "60111190",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "A survey on deep learning methods for power load and renewable energy forecasting in smart microgrids",
        "publication": "Renewable and Sustainable Energy Reviews",
        "citied_by": "374",
        "cover_date": "2021-07-01",
        "Abstract": "Microgrids have recently emerged as a building block for smart grids combining distributed renewable energy sources (RESs), energy storage devices, and load management methodologies. The intermittent nature of RESs brings several challenges to the smart microgrids, such as reliability, power quality, and balance between supply and demand. Thus, forecasting power generation from RESs, such as wind turbines and solar panels, is becoming essential for the efficient and perpetual operations of the power grid and it also helps in attaining optimal utilization of RESs. Energy demand forecasting is also an integral part of smart microgrids that helps in planning the power generation and energy trading with commercial grid. Machine learning (ML) and deep learning (DL) based models are promising solutions for predicting consumers’ demands and energy generations from RESs. In this context, this manuscript provides a comprehensive survey of the existing DL-based approaches, which are developed for power forecasting of wind turbines and solar panels as well as electric power load forecasting. It also discusses the datasets used to train and test the different DL-based prediction models, enabling future researchers to identify appropriate datasets to use in their work. Even though there are a few related surveys regarding energy management in smart grid applications, they are focused on a specific production application such as either solar or wind. Moreover, none of the surveys review the forecasting schemes for production and load side simultaneously. Finally, previous surveys do not consider the datasets used for forecasting despite their significance in DL-based forecasting approaches. Hence, our survey work is intrinsically different due to its data-centered view, along with presenting DL-based applications for load and energy generation forecasting in both residential and commercial sectors. The comparison of different DL approaches discussed in this manuscript reveals that the efficiency of such forecasting methods is highly dependent on the amount of the historical data and thus a large number of data storage devices and high processing power devices are required to deal with big data. Finally, this study raises several open research problems and opportunities in the area of renewable energy forecasting for smart microgrids.",
        "DOI": "10.1016/j.rser.2021.110992",
        "paper_author": "Aslam S.",
        "affiliation_name": "Cyprus University of Technology",
        "affiliation_city": "Limassol",
        "affiliation_country": "Cyprus",
        "affiliation_id": "60077474",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Advancing Neuromorphic Computing with Loihi: A Survey of Results and Outlook",
        "publication": "Proceedings of the IEEE",
        "citied_by": "354",
        "cover_date": "2021-05-01",
        "Abstract": "Deep artificial neural networks apply principles of the brain's information processing that led to breakthroughs in machine learning spanning many problem domains. Neuromorphic computing aims to take this a step further to chips more directly inspired by the form and function of biological neural circuits, so they can process new knowledge, adapt, behave, and learn in real time at low power levels. Despite several decades of research, until recently, very few published results have shown that today's neuromorphic chips can demonstrate quantitative computational value. This is now changing with the advent of Intel's Loihi, a neuromorphic research processor designed to support a broad range of spiking neural networks with sufficient scale, performance, and features to deliver competitive results compared to state-of-the-art contemporary computing architectures. This survey reviews results that are obtained to date with Loihi across the major algorithmic domains under study, including deep learning approaches and novel approaches that aim to more directly harness the key features of spike-based neuromorphic hardware. While conventional feedforward deep neural networks show modest if any benefit on Loihi, more brain-inspired networks using recurrence, precise spike-timing relationships, synaptic plasticity, stochasticity, and sparsity perform certain computation with orders of magnitude lower latency and energy compared to state-of-the-art conventional approaches. These compelling neuromorphic networks solve a diverse range of problems representative of brain-like computation, such as event-based data processing, adaptive control, constrained optimization, sparse feature regression, and graph search.",
        "DOI": "10.1109/JPROC.2021.3067593",
        "paper_author": "Davies M.",
        "affiliation_name": "Intel Corporation",
        "affiliation_city": "Santa Clara",
        "affiliation_country": "United States",
        "affiliation_id": "60033010",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Prediction of solar energy guided by pearson correlation using machine learning",
        "publication": "Energy",
        "citied_by": "336",
        "cover_date": "2021-06-01",
        "Abstract": "Solar energy forecasting represents a key element in increasing the competitiveness of solar power plants in the energy market and reducing the dependence on fossil fuels in economic and social development. This paper presents an approach for predicting solar energy, based on machine and deep learning techniques. The relevance of the studied models was evaluated for real-time and short-term solar energy forecasting to ensure optimized management and security requirements in this field while using an integral solution based on a single tool and an appropriate predictive model. The datasets we used in this study, represent data from 2016 to 2018 and are related to Errachidia which is a semi-desert climate province in Morocco. Pearson correlation coefficient was deployed to identify the most relevant meteorological inputs from which the models should learn. RF and ANN have provided high accuracies against LR and SVR, which have reported very significant errors. ANN has shown good performance for both real-time and short-term predictions. The key findings were compared with Pirapora in Brazil, which is a tropical climate region, to show the quality and reproducibility of the study.",
        "DOI": "10.1016/j.energy.2021.120109",
        "paper_author": "Jebli I.",
        "affiliation_name": "Ecole Mohammadia d'Ingenieurs, Mohammed V University in Rabat",
        "affiliation_city": "Rabat",
        "affiliation_country": "Morocco",
        "affiliation_id": "60011371",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Advanced Platinum-Based Oxygen Reduction Electrocatalysts for Fuel Cells",
        "publication": "Accounts of Chemical Research",
        "citied_by": "323",
        "cover_date": "2021-01-19",
        "Abstract": "ConspectusFuel cells are among the cutting-edge energy technologies. Their commercial development is still hindered by noble platinum (Pt) catalysts for the oxygen reduction reaction (ORR) at the cathode, which not only determine the energy conversion efficiency and service life but also are closely related to the cost and broad application of fuel cells. Given the bright and enormous future of fuel cells, ORR catalysts should possess highly efficient performance yet meet the acceptable Pt costs for large-scale application. Extensive efforts are concentrated on the optimization of Pt-based nanostructures and upgradation of functional carriers to achieve the low-cost and high-activity Pt-based catalysts. By improving the Pt utilization and accessible surface, reducing Pt consumption and catalyst costs, accelerating mass exchange and electron transfer, alleviating the corrosion and agglomeration of carriers and Pt, accompanying with the assistance of robust yet effective functional supports, the service level and life of Pt-based electrocatalysts would be significantly improved and fuel cells could get into commercial market covering broader applications.In this Account, we focus on the recent development of Pt-based catalysts to figure out the problems associated with ORR catalysts in fuel cells. Recent development of Pt-based catalysts is discussed in different stages: (1) multiscale development of Pt-based nanostructures; (2) multielement regulation over Pt-based alloy composition; (3) upgradation of carbon and noncarbon support architectures; (4) development of integrated Pt-based catalysts for fuel cells. Finally, we propose some future issues (such as reaction mechanism, dynamic evolutions, and structure-activity relationship) for Pt-based catalysts, which mainly involve the preparation strategy of Pt-integrated catalysts (combination of Pt nanostructures with nanocarbons), performance evaluation (standard measurement protocols, laboratory-level rotating disk electrode (RDE) measurements, application-level membrane electrode assembly (MEA) service test), advanced interpretation techniques (spectroscopy, electron microscopy, and in situ monitoring), and cutting-edge simulation/calculations and artificial intelligence (simulation, calculations, machine learning, big data screening). This Account calls for the comprehensive development of multiscale, multicomponent, and high-entropy Pt-based alloy nanostructures, and novel and stable carriers, which provide more available options for rational design of low-cost and high-performance Pt-integrated ORR catalysts. More importantly, it will give an in-depth understanding of the reaction mechanism, dynamic development, and structure-performance relationship for Pt-based catalysts in fuel cells and related energy technologies.",
        "DOI": "10.1021/acs.accounts.0c00488",
        "paper_author": "Huang L.",
        "affiliation_name": "Wuhan National Laboratory for Optoelectronics",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60087294",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Federated Learning over Wireless Networks: Convergence Analysis and Resource Allocation",
        "publication": "IEEE/ACM Transactions on Networking",
        "citied_by": "316",
        "cover_date": "2021-02-01",
        "Abstract": "There is an increasing interest in a fast-growing machine learning technique called Federated Learning (FL), in which the model training is distributed over mobile user equipment (UEs), exploiting UEs' local computation and training data. Despite its advantages such as preserving data privacy, FL still has challenges of heterogeneity across UEs' data and physical resources. To address these challenges, we first propose FEDL, a FL algorithm which can handle heterogeneous UE data without further assumptions except strongly convex and smooth loss functions. We provide a convergence rate characterizing the trade-off between local computation rounds of each UE to update its local model and global communication rounds to update the FL global model. We then employ FEDL in wireless networks as a resource allocation optimization problem that captures the trade-off between FEDL convergence wall clock time and energy consumption of UEs with heterogeneous computing and power resources. Even though the wireless resource allocation problem of FEDL is non-convex, we exploit this problem's structure to decompose it into three sub-problems and analyze their closed-form solutions as well as insights into problem design. Finally, we empirically evaluate the convergence of FEDL with PyTorch experiments, and provide extensive numerical results for the wireless resource allocation sub-problems. Experimental results show that FEDL outperforms the vanilla FedAvg algorithm in terms of convergence rate and test accuracy in various settings.",
        "DOI": "10.1109/TNET.2020.3035770",
        "paper_author": "DInh C.T.",
        "affiliation_name": "School of Computer Science",
        "affiliation_city": "Sydney",
        "affiliation_country": "Australia",
        "affiliation_id": "60099659",
        "affiliation_state": "NSW"
    },
    {
        "paper_title": "6G Wireless Communications Networks: A Comprehensive Survey",
        "publication": "IEEE Access",
        "citied_by": "315",
        "cover_date": "2021-01-01",
        "Abstract": "The commercial fifth-generation (5G) wireless communications networks have already been deployed with the aim of providing high data rates. However, the rapid growth in the number of smart devices and the emergence of the Internet of Everything (IoE) applications, which require an ultra-reliable and low-latency communication, will result in a substantial burden on the 5G wireless networks. As such, the data rate that could be supplied by 5G networks will unlikely sustain the enormous ongoing data traffic explosion. This has motivated research into continuing to advance the existing wireless networks toward the future generation of cellular systems, known as sixth generation (6G). Therefore, it is essential to provide a prospective vision of the 6G and the key enabling technologies for realizing future networks. To this end, this paper presents a comprehensive review/survey of the future evolution of 6G networks. Specifically, the objective of the paper is to provide a comprehensive review/survey about the key enabling technologies for 6G networks, which include a discussion about the main operation principles of each technology, envisioned potential applications, current state-of-the-art research, and the related technical challenges. Overall, this paper provides useful information for industries and academic researchers and discusses the potentials for opening up new research directions.",
        "DOI": "10.1109/ACCESS.2021.3124812",
        "paper_author": "Alsabah M.",
        "affiliation_name": "The University of Sheffield",
        "affiliation_city": "Sheffield",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60001881",
        "affiliation_state": "South Yorkshire"
    },
    {
        "paper_title": "A Vision and Framework for the High Altitude Platform Station (HAPS) Networks of the Future",
        "publication": "IEEE Communications Surveys and Tutorials",
        "citied_by": "307",
        "cover_date": "2021-04-01",
        "Abstract": "A High Altitude Platform Station (HAPS) is a network node that operates in the stratosphere at an of altitude around 20 km and is instrumental for providing communication services. Precipitated by technological innovations in the areas of autonomous avionics, array antennas, solar panel efficiency levels, and battery energy densities, and fueled by flourishing industry ecosystems, the HAPS has emerged as an indispensable component of next-generations of wireless networks. In this article, we provide a vision and framework for the HAPS networks of the future supported by a comprehensive and state-of-the-art literature review. We highlight the unrealized potential of HAPS systems and elaborate on their unique ability to serve metropolitan areas. The latest advancements and promising technologies in the HAPS energy and payload systems are discussed. The integration of the emerging Reconfigurable Smart Surface (RSS) technology in the communications payload of HAPS systems for providing a cost-effective deployment is proposed. A detailed overview of the radio resource management in HAPS systems is presented along with synergistic physical layer techniques, including Faster-Than-Nyquist (FTN) signaling. Numerous aspects of handoff management in HAPS systems are described. The notable contributions of Artificial Intelligence (AI) in HAPS, including machine learning in the design, topology management, handoff, and resource allocation aspects are emphasized. The extensive overview of the literature we provide is crucial for substantiating our vision that depicts the expected deployment opportunities and challenges in the next 10 years (next-generation networks), as well as in the subsequent 10 years (next-next-generation networks).",
        "DOI": "10.1109/COMST.2021.3066905",
        "paper_author": "Kurt G.K.",
        "affiliation_name": "Carleton University",
        "affiliation_city": "Ottawa",
        "affiliation_country": "Canada",
        "affiliation_id": "60017592",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "Short-term offshore wind speed forecast by seasonal ARIMA - A comparison against GRU and LSTM",
        "publication": "Energy",
        "citied_by": "303",
        "cover_date": "2021-07-15",
        "Abstract": "Offshore wind power is one of the fastest-growing energy sources worldwide, which is environmentally friendly and economically competitive. Short-term time series wind speed forecasts are extremely significant for proper and efficient offshore wind energy evaluation and in turn, benefit wind farm owner, grid operators as well as end customers. In this study, a Seasonal Auto-Regression Integrated Moving Average (SARIMA) model is proposed to predict hourly-measured wind speeds in the coastal/offshore area of Scotland. The used datasets consist of three wind speed time series collected at different elevations from a coastal met mast, which was designed to serve for a demonstration offshore wind turbine. To verify SARIMA's performance, the developed predictive model was further compared with the newly developed deep-learning-based algorithms of Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM). Regardless of the recent development of computational power has triggered more advanced machine learning algorithms, the proposed SARIMA model has shown its outperformance in the accuracy of forecasting future lags of offshore wind speeds along with time series. The SARIMA model provided the highest accuracy and robust healthiness among all the three tested predictive models based on corresponding datasets and assessed forecasting horizons.",
        "DOI": "10.1016/j.energy.2021.120492",
        "paper_author": "Liu X.",
        "affiliation_name": "University of Glasgow",
        "affiliation_city": "Glasgow",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60001490",
        "affiliation_state": "Scotland"
    },
    {
        "paper_title": "Deep autoencoder based energy method for the bending, vibration, and buckling analysis of Kirchhoff plates with transfer learning",
        "publication": "European Journal of Mechanics, A/Solids",
        "citied_by": "301",
        "cover_date": "2021-05-01",
        "Abstract": "In this paper, we present a deep autoencoder based energy method (DAEM) for the bending, vibration and buckling analysis of Kirchhoff plates. The DAEM exploits the higher order continuity of the DAEM and integrates a deep autoencoder and the minimum total potential principle in one framework yielding an unsupervised feature learning method. The DAEM is a specific type of feedforward deep neural network (DNN) and can also serve as function approximator. With robust feature extraction capacity, the DAEM can more efficiently identify patterns behind the whole energy system, such as the field variables, natural frequency and critical buckling load factor studied in this paper. The objective function is to minimize the total potential energy. The DAEM performs unsupervised learning based on generated collocation points inside the physical domain so that the total potential energy is minimized at all points. For the vibration and buckling analysis, the loss function is constructed based on Rayleigh's principle and the fundamental frequency and the critical buckling load is extracted. A scaled hyperbolic tangent activation function for the underlying mechanical model is presented which meets the continuity requirement and alleviates the gradient vanishing/explosive problems under bending. The DAEM is implemented using Pytorch and the LBFGS optimizer. To further improve the computational efficiency and enhance the generality of this machine learning method, we employ transfer learning. A comprehensive study of the DAEM configuration is performed for several numerical examples with various geometries, load conditions, and boundary conditions.",
        "DOI": "10.1016/j.euromechsol.2021.104225",
        "paper_author": "Zhuang X.",
        "affiliation_name": "Tongji University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60073652",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "The Future of Memristors: Materials Engineering and Neural Networks",
        "publication": "Advanced Functional Materials",
        "citied_by": "298",
        "cover_date": "2021-02-01",
        "Abstract": "From Deep Blue to AlphaGo, artificial intelligence and machine learning are booming, and neural networks have become the hot research direction. However, due to the size limit of complementary metal–oxide–semiconductor (CMOS) transistors, von Neumann-based computing systems are facing multiple challenges (such as memory walls). As the number of transistors required by the neural network increases, the development of neural networks based on the von Neumann computer is limited by volume and energy consumption. As the fourth basic circuit element, memristor shines in the field of neuromorphic computing. The new computer architecture based on memristor is widely considered as a substitute for the von Neumann architecture and has great potential to deal with the neural network and big data era challenge. This article reviews existing materials and structures of memristors, neurophysiological simulations based on memristors, and applications of memristor-based neural networks. The feasibility and advancement of implementing neural networks using memristors are discussed, the difficulties that need to be overcome at this stage are put forward, and their development prospects and challenges faced are also discussed.",
        "DOI": "10.1002/adfm.202006773",
        "paper_author": "Sun K.",
        "affiliation_name": "Hebei University",
        "affiliation_city": "Baoding",
        "affiliation_country": "China",
        "affiliation_id": "60012293",
        "affiliation_state": "Hebei"
    },
    {
        "paper_title": "A fourth-generation high-dimensional neural network potential with accurate electrostatics including non-local charge transfer",
        "publication": "Nature Communications",
        "citied_by": "295",
        "cover_date": "2021-12-01",
        "Abstract": "Machine learning potentials have become an important tool for atomistic simulations in many fields, from chemistry via molecular biology to materials science. Most of the established methods, however, rely on local properties and are thus unable to take global changes in the electronic structure into account, which result from long-range charge transfer or different charge states. In this work we overcome this limitation by introducing a fourth-generation high-dimensional neural network potential that combines a charge equilibration scheme employing environment-dependent atomic electronegativities with accurate atomic energies. The method, which is able to correctly describe global charge distributions in arbitrary systems, yields much improved energies and substantially extends the applicability of modern machine learning potentials. This is demonstrated for a series of systems representing typical scenarios in chemistry and materials science that are incorrectly described by current methods, while the fourth-generation neural network potential is in excellent agreement with electronic structure calculations.",
        "DOI": "10.1038/s41467-020-20427-2",
        "paper_author": "Ko T.W.",
        "affiliation_name": "Georg-August-Universität Göttingen",
        "affiliation_city": "Gottingen",
        "affiliation_country": "Germany",
        "affiliation_id": "60031514",
        "affiliation_state": "Niedersachsen"
    },
    {
        "paper_title": "A machine learning approach on the relationship among solar and wind energy production, coal consumption, GDP, and CO<inf>2</inf> emissions",
        "publication": "Renewable Energy",
        "citied_by": "290",
        "cover_date": "2021-04-01",
        "Abstract": "China, India, and the USA are the world's biggest energy consumers and CO2 emitters. Being the leading contributors to climate change, these economies are also at the core of environmental solutions. This paper investigates the causal relationship among solar and wind energy production, coal consumption, economic growth, and CO2 emissions for these three countries. To do so, we use an advanced methodology in Machine Learning to verify the predictive causal linkages among variables. The Causal Direction from Dependency (D2C) algorithm set CO2 emissions as the target variable. The obtained results were disaggregated and estimated in a supervised prediction model. The findings, confirmed by three different Machine Learning procedures, showed an interesting output. While a reduction in overall carbon emissions is predicted in China and the US (resulting from the intensive use of renewable sources of energy), India displays critical predictions of a rise in CO2 emissions. This indicates that curbing CO2 emissions cannot be achieved without conducting a comprehensive shift from fossil to renewable resources, although China and the U.S. present a more promising path to sustainability than India. Being an emerging renewable energy leader, India should further enhance the use of low-carbon sources in its power supply and limit its dependence on coal.",
        "DOI": "10.1016/j.renene.2020.11.050",
        "paper_author": "Magazzino C.",
        "affiliation_name": "Università degli Studi Roma Tre",
        "affiliation_city": "Rome",
        "affiliation_country": "Italy",
        "affiliation_id": "60012630",
        "affiliation_state": "RM"
    }
]