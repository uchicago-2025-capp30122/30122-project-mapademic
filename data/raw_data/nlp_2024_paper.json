[
    {
        "paper_title": "Utilising AI Models to Analyse the Relationship between Battlefield Developments in the Russian-Ukrainian War and Fluctuations in Stock Market Values",
        "publication": "Forum Scientiae Oeconomia",
        "citied_by": "0",
        "cover_date": "2024-12-31",
        "Abstract": "This study examines the impact of battlefield developments in the ongoing Russian–Ukrainian war, which to date has lasted over 1000 days, on the stock prices of defence corporations such as BAE Systems, Booz Allen Hamilton, Huntington Ingalls, and Rheinmetall AG. Stock prices were analysed alongside sentiment data extracted from news articles, and processed using machine learning models leveraging natural language processing (NLP). Although the main hypothesis was not confirmed due to methodological and data limitations, the study demonstrated that neural network-based models, specifically long short-term memory (LSTM) networks, effectively captured hidden temporal patterns. The model's performance was evaluated using root mean squared error (RMSE). Alternative models, including XGBoost, ARIMA, and VAR, were also tested but did not yield accurate forecasts. The findings highlight nonlinear patterns in the data and emphasise the importance of hyperparameter optimisation, such as tuning the number of epochs and LSTM layer sizes. Techniques such as Grid Search and Random Search significantly enhanced forecasting performance, resulting in stock price predictions with low RMSE. The full-scale war between Russia and Ukraine, which has been ongoing for more than 1000 days on European soil, triggered market reactions with the first shots fired on the Russian–Ukrainian border on 24 February 2022. Since then, Europe and other countries around the world have supported Ukraine militarily, humanitarianly, and economically while revising their defence doctrines and strategies, as well as increasing and modernising their military capabilities. News of Russian aggression and new orders for military products sparked market reactions, particularly changes in the stock prices of defence companies that ramped up production and continue to expand it. This study selected several of the largest defence corporations, whose stock prices showed significant responses to the full-scale Russian invasion of Ukraine, to examine whether other battlefield events elicit similar stock price reactions as the first reports. To investigate this, artificial intelligence tools, including several machine learning methods, were employed. The purpose of this article is to examine the impact of battlefield developments in the Russian–Ukrainian war on stock volatility among major players in the global defence industry, while also identifying the most effective method for forecasting stock prices by testing various machine learning (ML) models. NLP was used to capture sentiment and analyse news about the Russian–Ukrainian war. Vector AutoRegression, the XGBRegressor, the Long Short-Term Memory (LSTM) neural network, and the ARIMA model were used for stock price forecasting. The main hypothesis that stock price changes are influenced by battlefield developments in the Russian–Ukrainian war was not confirmed due to methodological and data limitations. However, the research results indicate that, in this case, the Long Short-Term Memory (LSTM) model is well-suited for time series forecasting, effectively capturing nonlinear time patterns and long-term dependencies for major defence corporations.",
        "DOI": "10.23762/FSO_VOL12_NO4_5",
        "paper_author": "Melnychenko O.",
        "affiliation_name": "London Academy of Science and Business",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60177669",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "High-Density Polyethylene Film Price Forecast in Southeast Asia Market with Deep Learning",
        "publication": "Engineering Journal",
        "citied_by": "0",
        "cover_date": "2024-12-31",
        "Abstract": "Neural networks (NN) have been used for over a decade to predict time series data, with various algorithms including linear and non-linear models. Forecasting and assessing polymer market prices are crucial for plastic resin producers due to the complexity and uncertainty of resource availability. This encompasses feedstock planning, raw material procurement, technological advancements for product transitions, sales planning, pricing strategies for commercialization, and investments driven by macroeconomic factors. Previous literature primarily utilized numerical data as input for deep learning models. This research contended that structured data by itself was inadequate for models to precisely predict outcomes in the volatile, uncertain, complex, and ambiguous (VUCA) environment. Three deep learning architectures, Long-Short Term Memory (LSTM), Encoder-Decoder, Temporal Convolutional Network and Recurrent Neuron Network (TCNRNN), were reviewed in this research to determine the most effective architecture for analysing structured data. Additionally, Natural Language Processing (NLP) was implemented in this research to gather market sentiment and enhance forecast accuracy. The study utilizes commodity market price announcements, economic indicators, and insight reports from reputable publishers. The study utilizes commodity market prices, economic indicators, and insightful reports. All information was obtained from a reputable publisher. The results were compared with the legacy model, which involved a human analyst and a linear regression model. Model performance was assessed using Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE), Symmetric Mean Absolute Percentage Error (SMAPE), and ANOVA. The linear regression forecast together with the human analyst model has an acceptable accuracy with a MAPE of 45.1%. Neural networks containing sentiment analyzers have been found to surpass the performance of human analysts and a linear regression model, with a MAPE of 17.1%.",
        "DOI": "10.4186/ej.2024.28.12.79",
        "paper_author": "Amapat N.",
        "affiliation_name": "Chulalongkorn University",
        "affiliation_city": "Bangkok",
        "affiliation_country": "Thailand",
        "affiliation_id": "60028190",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "TurkishLex:Development of a Context-Aware Spell Checker for Detecting and Correcting Spelling Errors in Turkish Texts",
        "publication": "Sakarya University Journal of Computer and Information Sciences",
        "citied_by": "0",
        "cover_date": "2024-12-31",
        "Abstract": "In Turkish, correct spelling correction is crucial for effective communication and preserving the integrity of written text. The challenge lies in the complexity of Turkish morphology and spelling, which can lead to frequent and diverse spelling errors. This study presents a spelling checker adapted for Turkish by creating a new Turkish dataset. The proposed spelling checker model effectively captures both minor and major textual changes and can detect the error. Our findings show that the proposed spelling checker system provides high accuracy and reliability with 98.21% accuracy performance with the Symspell module in correcting Turkish texts. This study provides valuable information about the strengths and weaknesses of existing spelling checkers and contributes to the improvement of spelling correction tools for Turkish.",
        "DOI": "10.35377/saucis...1544012",
        "paper_author": "Savcı P.",
        "affiliation_name": "Arçelik A.S.",
        "affiliation_city": "Istanbul",
        "affiliation_country": "Turkey",
        "affiliation_id": "60085877",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Natural Language Processing and Machine Learning Applications For Assessment and Evaluation in Education: Opportunities and New Approaches",
        "publication": "Journal of Measurement and Evaluation in Education and Psychology",
        "citied_by": "0",
        "cover_date": "2024-12-31",
        "Abstract": "This study examines the applications of Artificial Intelligence (AI), Machine Learning (ML) and Natural Language Processing (NLP) technologies in education, particularly in educational assessment and evaluation processes. The study examines the potential of these technologies to contribute to educational assessment and evaluation processes in areas such as automatic item generation, text mining, sentiment analysis, sentence similarity, and providing feedback to students. The study includes both a literature review and sample applications. In the automatic item generation process of the study, language models such as GPT and Gemini are used to generate new educational questions and this process is supported by NLP technologies. The study is enriched with Turkish examples and the results show that these applications can be further developed for Turkish and have potential for other applications.",
        "DOI": "10.21031/epod.1551568",
        "paper_author": "Yilmaz K.",
        "affiliation_name": "Ankara Üniversitesi",
        "affiliation_city": "Ankara",
        "affiliation_country": "Turkey",
        "affiliation_id": "60012603",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Dynamic Task Weighting Mechanism for a Task-Aware Approach to Mitigating Catastrophic Forgetting",
        "publication": "International Journal of Computational and Experimental Science and Engineering",
        "citied_by": "0",
        "cover_date": "2024-12-30",
        "Abstract": "Catastrophic forgetting is still a big issue in sequential learning and in particular for Natural Language Processing (NLP) models that tend to forget knowledge encoded in previous tasks when learning new targets. To do this, we present a Dynamic Task Weighting Mechanism which forms a part of the Adaptive Knowledge Consolidation (AKC) framework. Our method dynamically adjust knowledge retention to task similarity and task specific performance, while contrasted to static regularization approaches such as Elastic Weight Consolidation (EWC) and Synaptic Intelligence (SI). This mechanism is proposed that involves computing task embeddings with pre-trained models BERT and quantifying their similarity from cosine similarity. To complete the above, we compute a similarity score which is merged with normalized task specific performance metrics of accuracy, F1 score to form an importance score. The model trades adaptability in learning in order to retain previously learned knowledge by prioritizing important tasks and minimizing interference from other unrelated tasks. We show that our proposed mechanism substantially mitigates forgetting and results in accuracy improvements on extensive experiments on standard NLP benchmarks such as GLUE, AG News, and SQuAD. Among baseline methods (EWC, SI, and GEM), the model also has the highest average accuracy of 86.7% and the least amount of forgetting of 6.2%.",
        "DOI": "10.22399/ijcesen.985",
        "paper_author": "Ranjith J.",
        "affiliation_name": "Puducherry Technological University",
        "affiliation_city": "Puducherry",
        "affiliation_country": "India",
        "affiliation_id": "60005694",
        "affiliation_state": "PY"
    },
    {
        "paper_title": "Artificial Intelligence Methods for Spanish Documents Classification",
        "publication": "Lengua y Sociedad",
        "citied_by": "0",
        "cover_date": "2024-12-30",
        "Abstract": "The rapid globalization and growing need for cross-language communication necessitate modern, real-time corpora to aid language learners. Traditional methods for creating such corpora, especially in Spanish, are inadequate due to their inability to process the vast and unstructured data available online. This study explores Artificial Intelligence (AI) methodologies for automatic Spanish document acquisition from the web, pre-processing and classifying them in order to build a vast and flexible corpus for Spanish learning. The research applies web crawling using the Scrapy framework to collect data, which is then cleaned and classified using advanced Natural Language Processing (NLP) models. Specifically, the study employs BERT (Bidirectional Encoder Representations from Transformers) and its enhanced variant RoBERTa to achieve document classification. Through a combination of data augmentation techniques and deep learning models, the study achieves high accuracy in classifying Spanish-language texts, demonstrating the potential for using AI to overcome the limitations of traditional corpus-building approaches.",
        "DOI": "10.15381/lengsoc.v23i2.29208",
        "paper_author": "Gonsalves T.",
        "affiliation_name": "Sophia University",
        "affiliation_city": "Tokyo",
        "affiliation_country": "Japan",
        "affiliation_id": "60002784",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "The role of linguistics in artificial intelligence (AI)",
        "publication": "Lengua y Sociedad",
        "citied_by": "0",
        "cover_date": "2024-12-30",
        "Abstract": "In this paper, I analyse the contributions of the Natural Language Processing (NLP) perspective to the development of Artificial Intelligence (AI). It is divided into four main sections. In the first one, I introduce a brief and general conceptualisation of AI, aiming to indicate the place of NLP in the general perspectives of AI. In the second section, I analyse the theoretical linguistic elements that played a relevant role in the evolution of NLP. In the third section, I introduce the relationships between computation and natural language analysis to provide a general concept of their connection. Finally, in the fourth section, I introduce a brief characterisation of Large Language Models (LLMs), Foundational Models (FMs) and their relations with NLP. So, this paper aims to provide a general perspective of this complex field, emphasising the milestone issues.",
        "DOI": "10.15381/lengsoc.v23i2.28972",
        "paper_author": "Vargas-Elizondo C.",
        "affiliation_name": "Instituto Tecnológico de Costa Rica",
        "affiliation_city": "Cartago",
        "affiliation_country": "Costa Rica",
        "affiliation_id": "60099123",
        "affiliation_state": "Cartago"
    },
    {
        "paper_title": "Emerging multifaceted application of artificial intelligence in chest radiography: a narrative review",
        "publication": "Journal of Medical Artificial Intelligence",
        "citied_by": "1",
        "cover_date": "2024-12-30",
        "Abstract": "Background and Objective: Chest radiography, otherwise known as chest X-ray (CXR) is the most in demand and widely performed investigation in radiology department; owing to the multiple combining effect of rise in prevalence of respiratory diseases globally and the growing need of heath assessment for pre-employment, pre-operative and migration purposes. However, this task is already proving overwhelming, placing an immeasurable burden of workload on radiographers, radiologist, and the entire health system; this has resulted in long waiting time, fatigue-based technical error, interpretation error, reporting delays and backlogs. To ameliorate this predicament, medical imaging has witnessed the introduction of artificial intelligence (AI). Thus, with the raid evolutionary trend in technology, this article seeks to review current state of evidence on AI use in CXR and level of progress made to minimize these errors and delays. In addition, point out challenges, as well as unfold areas for future research to better detection rates and improve overall clinical outcomes. Methods: A search for relevant literature that focuses on AI in CXR was conducted with the help of certain keywords [machine learning (ML), chest radiography, deep learning (DL), natural language processing (NLP), expert system (ES) and fuzzy logic (FL)]. Thereafter, a narrative logical approach to technically analysing and synthesizing findings across domains of AI (ML, DL, NLP, ES, FL) and robot technologies as it relates to CXR done. Key Content and Findings: A thorough evaluation of the substance of evidence these studies bring to enhance overall workflow and health outcomes show that ML is very useful in performing administrative and imaging tasks such as exam scheduling, worklist management and image acquisition. On the other hand, DL is better suited for classification tasks on a broad spectrum of chest anomalies in CXR. However, a hybrid approach involving ML-DL, FL-DL and NLP-DL/ML technologies seems to further improve reporting accuracy and offer more insights into CXR interpretation. Further studies on training and refining models for clinical use in this perspective is demanded. Conclusions: AI still in its early stages; this review to serve as road map to implementation and policy making, guide routine practice and improve clinical governance.",
        "DOI": "10.21037/jmai-24-67",
        "paper_author": "Nwaiwu V.C.",
        "affiliation_name": "Lincoln University College, Malaysia",
        "affiliation_city": "Petaling Jaya",
        "affiliation_country": "Malaysia",
        "affiliation_id": "60107209",
        "affiliation_state": "Selangor"
    },
    {
        "paper_title": "Natural language processing: The current state of the art and challenges",
        "publication": "Review of Computer Engineering Research",
        "citied_by": "0",
        "cover_date": "2024-12-27",
        "Abstract": "The use of deep learning techniques in natural language processing (NLP) is examined thoroughly in this study with particular attention to tasks where deep learning has been shown to perform very effectively. The primary strategies explored are phrase embedding, function extraction, and textual content cleaning. These are all essential for sorting textual content statistics and files. It appears in important gear like software programs, hardware and extensively used libraries and cutting-edge programs for deep learning in NLP. In NLP, deep learning is turning into a chief fashion, changing many areas and making large modifications in many fields. This paper stresses how deep analyzing techniques have a good-sized-ranging effect and how vital they may be for shifting the world in advance. This paper also discusses how deep learning may assist in solving modern issues and handling challenging and stressful situations in NLP research. Since those methods are getting more popular, it indicates that they're top at handling many NLP responsibilities. The final part of the evaluation talks about the most current makes use of, developing traits and long-term troubles in NLP. It helps practitioners and lecturers determine and use the capabilities of deep learning in the dynamic field of natural language processing with its applicable facts and examples.",
        "DOI": "10.18488/76.V11I4.4018",
        "paper_author": "Taye M.M.",
        "affiliation_name": "Philadelphia University",
        "affiliation_city": "Amman",
        "affiliation_country": "Jordan",
        "affiliation_id": "60054151",
        "affiliation_state": "Amman"
    },
    {
        "paper_title": "BBGC: BERT-BiLSTM-GCN Fusion with Collaborative Attention Mechanism for Text Classification Model",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2024-12-27",
        "Abstract": "Text categorization serves as a fundamental objective within the domain of Natural Language Processing (NLP).Text data often contains rich structural and contextual information, such as correlations between words, logical relationships between sentences, and diverse contexts, which makes it especially critical to effectively capture and process full-text information. In this paper, a model named BBGC is proposed to improve the capture and processing of full-text information. The model extracts contextual and structural information of text by combining BERT-BiLSTM and Graph Convolutional Network (GCN), respectively, and employs a co-attention mechanism and a gate mechanism to fuse features from different perspectives of the GCN and BERT-BiLSTM, which enhances the attention to key features and integrates features from different sources more effectively. In addition, we conducted experimental validation on four datasets, namely R52, R8, Ohsumed, and MR, with accuracies of 96.65%, 98.72%, 73.83%, and 90.41%, respectively. The experimental results demonstrate the effectiveness of the proposed method in improving the accuracy of text categorization and outperform the existing methods.",
        "DOI": "10.1145/3703187.3703226",
        "paper_author": "Qi X.",
        "affiliation_name": "Shanghai Maritime University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60025785",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Research and Practice on the Intelligence of Customer Service Work Orders in China Telecom",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2024-12-27",
        "Abstract": "With the widespread application of intelligent technology, the digital operational efficiency and customer service experience perception of call centers have become the key to improvement. China Telecom has conducted research and practice on the automation of call center work order processing and intelligent service, aiming to achieve automation of customer service production and intelligent service through technical means such as automatic generation of work order standard templates, intelligent input, and intelligent attribute marking. This study focuses on the production processes of standardized work order creation, intelligent input, attribute marking, and quality inspection. By introducing technologies such as intelligent speech recognition, natural language processing (NLP), and robotic process automation (RPA), the efficiency and quality of work order processing have been improved. The application results show that this study significantly improves the speed and accuracy of work order processing, optimizes the service quality inspection process, and effectively enhances the customer service experience.",
        "DOI": "10.1145/3703187.3703213",
        "paper_author": "Wu Y.",
        "affiliation_name": "Research Institute of China Telecom Corporation Ltd.",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China",
        "affiliation_id": "128679480",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "The Large Language Model GreekLegalRoBERTa",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2024-12-27",
        "Abstract": "We develop four versions of GreekLegalRoBERTa, which are four large language models trained on Greek legal and nonlegal text.We show that our models surpass the performance of GreekLegalBERT, Greek-LegalBERT-v2, and GreekBERT in two tasks involving Greek legal documents: named entity recognition and multi-class legal topic classification.We view our work as a contribution to the study of domain-specific NLP tasks in low-resource languages, like Greek, using modern NLP techniques and methodologies.",
        "DOI": "10.1145/3688671.3688770",
        "paper_author": "Saketos V.",
        "affiliation_name": "The Royal Institute of Technology (KTH)",
        "affiliation_city": "Stockholm",
        "affiliation_country": "Sweden",
        "affiliation_id": "60002014",
        "affiliation_state": "Stockholms"
    },
    {
        "paper_title": "Now I know! Empowering Voters with RAG-enabled LLMs to Eliminate Political Uncertainty",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2024-12-27",
        "Abstract": "Accurate political information is vital for voters to make informed decisions.However, due to the plethora of data and biased sources, accessing concise, factual information still remains a challenge.To tackle this problem, we present an open-access, deployed digital assistant powered by Large Language Models (LLMs), specifically tailored to answer voters' questions and help them vote for the political party they mostly align with.The user can select up to 3 parties, input their question, and get short, summarized answers from the parties' published political agendas, which contain hundreds of pages and, thus, are difficult to navigate for the typical citizen.Our NLP system architecture leverages OpenAI's GPT-4 and incorporates Retrieval-Augmented Generation with Citations (RAG+C) to integrate custom data into LLMs effectively and build user trust.We also describe our database design, underlining the use of an open-source vector database, optimized for high-dimensional semantic search across multiple documents, and a semantic-rich LLM cache, reducing operational expenses and end-user latency time.Our open-access system supports Greek and English and has been deployed live at https://toraksero.gr/for the Greek 2023 Elections, which gathered 30K user sessions and 74% user satisfaction.",
        "DOI": "10.1145/3688671.3688784",
        "paper_author": "Vassos S.",
        "affiliation_name": "helvia.ai",
        "affiliation_city": "Athens",
        "affiliation_country": "Greece",
        "affiliation_id": "130883249",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Greek2MathTex: A Greek Speech-to-Text Framework for LaTeX Equations Generation",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2024-12-27",
        "Abstract": "In the vast majority of the academic and scientific domains, LATEX has established itself as the de facto standard for typesetting complex mathematical equations and formulae.However, LATEXś complex syntax and code-like appearance present accessibility barriers for individuals with disabilities, as well as those unfamiliar with coding conventions.In this paper, we present a novel solution to this challenge through the development of a novel speech-to-LATEX equations system specifically designed for the Greek language.We propose an end-to-end system that harnesses the power of Automatic Speech Recognition (ASR) and Natural Language Processing (NLP) techniques to enable users to verbally dictate mathematical expressions and equations in natural language, which are subsequently converted into LATEX format.We present the architecture and design principles of our system, highlighting key components such as the ASR engine, the LLM-based prompt-driven equations generation mechanism, as well as the application of a custom evaluation metric employed throughout the development process.We have made our system open source and available at https://github.com/magcil/greek-speech-to-math.",
        "DOI": "10.1145/3688671.3688771",
        "paper_author": "Gkritzali E.",
        "affiliation_name": "National Centre for Scientific Research \"DEMOKRITOS\"",
        "affiliation_city": "Athens",
        "affiliation_country": "Greece",
        "affiliation_id": "60001104",
        "affiliation_state": "Attica"
    },
    {
        "paper_title": "Greek Wikipedia: A Study on Abstractive Summarization",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2024-12-27",
        "Abstract": "In the research area of Natural Language Processing (NLP), text summarization (TS) has been defined as the automatic composition of a cohesive and articulate summary, which encapsulates the main ideas and themes from a single or multiple documents.Recent developments from the areas of Deep Learning (DL) and Natural Language Understanding (NLU) have facilitated the development of abstractive TS transformer-based models, which demonstrate better performance than classical extractive ones.In any case, the vast majority of current NLP research concentrates on high-resource languages (e.g., English).Dealing with modern Greek, this paper introduces and elaborates a novel abstractive TS dataset comprising 93, 433 Greek Wikipedia articles and their summaries assigned by human editors.The paper also proposes a series of DL abstractive TS models that were fine-tuned on this dataset for the task of Greek article summarization.A thorough experimentation was conducted for the comparative assessment of the proposed models against well-known extractive summarization ones, using the test subset of the dataset.The results reveal that the proposed abstractive models outperform the extractive ones across various TS evaluation metrics.To enhance the reproducibility of our work, we make publicly available the corresponding dataset, our best performing model and the experimentation code.",
        "DOI": "10.1145/3688671.3688769",
        "paper_author": "Giarelis N.",
        "affiliation_name": "University of Patras",
        "affiliation_city": "Rio",
        "affiliation_country": "Greece",
        "affiliation_id": "60031155",
        "affiliation_state": "Achaia"
    },
    {
        "paper_title": "Crisis communication in the age of AI: Navigating opportunities, challenges, and future horizons",
        "publication": "Media and Crisis Communication",
        "citied_by": "2",
        "cover_date": "2024-12-27",
        "Abstract": "In today's rapidly evolving world, effective communication is essential, particularly in crises like natural disasters or health emergencies. The demand for swift and accurate communication has driven exploration into innovative solutions, with Artificial Intelligence (AI) emerging as a transformative tool in crisis management. Leveraging technologies such as natural language processing, machine learning, and data analytics, AI processes vast information rapidly, aiding in early detection, monitoring, response optimization, and public engagement during crises. However, integrating AI in crisis communication introduces ethical challenges like data privacy and algorithmic biases. This chapter comprehensively explores the impact of AI on contemporary crisis communication, emphasizing real-time information dissemination, sentiment analysis, and public engagement as vital components. It delves into AI technologies like NLP and machine learning, acknowledging ethical concerns, and stressing the importance of balancing automation with human intervention. The chapter concludes by speculating on future trends, urging continued research to mitigate adverse impacts and offering valuable insights into AI's practical applications and relevance in addressing contemporary challenges. This exploration aims to bridge the gap in understanding AI's evolving role in crisis communication.",
        "DOI": "10.4324/9781032619187-9",
        "paper_author": "Cheng Y.",
        "affiliation_name": "NC State University",
        "affiliation_city": "Raleigh",
        "affiliation_country": "United States",
        "affiliation_id": "60004923",
        "affiliation_state": "NC"
    },
    {
        "paper_title": "An Empirical Study of Testing Machine Learning in the Wild",
        "publication": "ACM Transactions on Software Engineering and Methodology",
        "citied_by": "0",
        "cover_date": "2024-12-26",
        "Abstract": "Background: Recently, machine and deep learning (ML/DL) algorithms have been increasingly adopted in many software systems. Due to their inductive nature, ensuring the quality of these systems remains a significant challenge for the research community. Traditionally, software systems were constructed deductively, by writing explicit rules that govern the behavior of the system as program code. However, ML/DL systems infer rules from training data i.e., they are generated inductively. Recent research in ML/DL quality assurance has adapted concepts from traditional software testing, such as mutation testing, to improve reliability. However, it is unclear if these proposed testing techniques are adopted in practice, or if new testing strategies have emerged from real-world ML deployments. There is little empirical evidence about the testing strategies.Aims: To fill this gap, we perform the first fine-grained empirical study on ML testing in the wild to identify the ML properties being tested, the testing strategies, and their implementation throughout the ML workflow.Method: We conducted a mixed-methods study to understand ML software testing practices. We analyzed test files and cases from 11 open-source ML/DL projects on GitHub. Using open coding, we manually examined the testing strategies, tested ML properties, and implemented testing methods to understand their practical application in building and releasing ML/DL software systems.Results: Our findings reveal several key insights: (1) The most common testing strategies, accounting for less than 40%, are Grey-box and White-box methods, such as Negative Testing, Oracle Approximation, and Statistical Testing. (2) A wide range of ML properties are tested, out of which only 20% to 30% are frequently tested, including Consistency, Correctness, and Efficiency. (3) Bias and Fairness is more tested in Recommendation (6%) and Computer Vision (CV) (3.9%) systems, while Security and Privacy is tested in CV (2%), Application Platforms (0.9%), and NLP (0.5%). (4) We identified 13 types of testing methods, such as Unit Testing, Input Testing, and Model Testing.Conclusions: This study sheds light on the current adoption of software testing techniques and highlights gaps and limitations in existing ML testing practices.",
        "DOI": "10.1145/3680463",
        "paper_author": "Openja M.",
        "affiliation_name": "Polytechnique Montréal",
        "affiliation_city": "Montreal",
        "affiliation_country": "Canada",
        "affiliation_id": "60019141",
        "affiliation_state": "QC"
    },
    {
        "paper_title": "EXPLORING THE ROLE OF AI IN CUSTOMER SERVICE COMMUNICATION A CASE STUDY ON CHATBOTS IN DELIVERY APPS IN UAE",
        "publication": "Balkan Social Science Review",
        "citied_by": "0",
        "cover_date": "2024-12-25",
        "Abstract": "This research covers the adoption of AI within the customer service communication domain, focusing on the effectiveness of AI chatbots used in delivery applications within the United Arab Emirates. It probes into the user's interaction with several leading delivery services operating within the region, such as Talabat, Zomato, and Deliveroo. The application of AI chatbots has been considered a popular way to promote customer support through the possibility of speed and efficiency, especially in handling inquiries and resolving basic issues. However, users' perception of the efficacy of chatbots remains mixed, and levels of satisfaction vary greatly. Data were collected from 58 delivery application users to get a better understanding of these experiences. Results show that while many users appreciate the speed of chatbots when resolving order issues, significant challenges persist. These include irrelevant/incorrect responses, overall limited functionality, and difficulty in managing complex queries. These conditions highlight the areas of vital concern where improvement in the chatbots is urgently needed, at least in terms of understanding context, personalization, and even response accuracy. The research then outlines recommendations that can be taken to reduce these shortcomings. Increasing the chatbot's ability to understand the user's intent, making the interaction more human-like and empathetic, and personalization in responses based on particular users' preferences would be considered key ways to increase customer satisfaction. Moreover, enhanced NLP might help chatbots capture incomplete or colloquial inputs of users more accurately, hence leading to smoother, more effective communication. However, although this study has been useful in providing a variety of insights, some of the accompanying limitations include: the limited sample size of 58 respondents prohibits generalizing such findings to the whole population, and the focus on just three specific delivery platforms limits the scope of the study. This research furthers the new understanding of AI chatbot adoption in the delivery industry and lays a foundation for further investigations. Food delivery services can, therefore, develop improvements regarding the identified challenges, enhance user experience, instill trust in chatbot technology, and consequently guarantee customer loyalty. This study has underlined the need to balance automation with user-centered design to ensure AI solutions meet shifting consumer expectations within a more digitized world today.",
        "DOI": "10.46763/bssr242424445m",
        "paper_author": "Mustafa H.",
        "affiliation_name": "Rochester Institute of Technology Dubai",
        "affiliation_city": "Dubai",
        "affiliation_country": "United Arab Emirates",
        "affiliation_id": "60113220",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Words of Wisdom: A Journey through the Realm of Natural Language Processing for Learning Analytics—A Systematic Literature Review",
        "publication": "Journal of Learning Analytics",
        "citied_by": "0",
        "cover_date": "2024-12-25",
        "Abstract": "Learning analytics (LA) involves the measurement, collection, analysis, and reporting of data about learners and their contexts, aiming to understand and optimize both the learning process and the environments in which it occurs. Among many themes that the LA community considers, natural language processing (NLP) algorithms have been widely adopted to extract information from textual data generated in learning environments (e.g., student essays and short answers, online discussion and chat). NLP can shed light on the learning process and student outcomes in different contexts. Based on the importance of NLP for education, this paper conducted a systematic literature review of the application of NLP to understand how the LA community has been applying the methods from this field. Our methodology includes automatic and manual methods to extract information about authors, relevant papers, and specific data related to educational applications and algorithms used in the field. This review selected 156 papers that reveal essential aspects of the topic; e.g., (i) the majority of the works focused on the analysis of online discussions and essay assessment; (ii) in general, the authors did not apply the developed models in real settings; (iii) recent papers selected have begun to evaluate deep learning models (e.g., BERT) more frequently; and (iv) the datasets used in the experimentation are usually small and contain English text. The results of this study and its practical implications are further discussed.",
        "DOI": "10.18608/jla.2024.8403",
        "paper_author": "Ferreira Mello R.",
        "affiliation_name": "CESAR Centro de Estudios e Sistemas Avancados do Recife",
        "affiliation_city": "Recife",
        "affiliation_country": "Brazil",
        "affiliation_id": "60086758",
        "affiliation_state": "PE"
    },
    {
        "paper_title": "A comparison study of tools, frameworks, and libraries in natural language processing",
        "publication": "Cognitive Connections: Unleashing the Potential of Interactive Technologies",
        "citied_by": "0",
        "cover_date": "2024-12-24",
        "Abstract": "Natural Language Processing (NLP) is a concept of Artificial Intelligence (AI) that can be defined as how computers or machines perceive language as humans do [1]. Various Deep Learning (DL) and Machine Learning (ML) models are trained specifically for such tasks, like taking in speech/text input, interpreting it the way humans would, understanding the language, and providing an appropriate output [2, 3]. NLP models rely on various algorithms to perform multiple tasks like speech recognition, sentiment analysis, and in some cases, predicting the words that may be a part of a sentence using their understanding of language and statistics. Over the years many tools, frameworks, and libraries have come into play. In this paper, we have reviewed the developments in NLP and compared the various tools, frameworks, and libraries of NLP concerning multiple parameters, viz., performance, ease of use, use cases, etcetera.",
        "DOI": "NA",
        "paper_author": "Maheshwari A.",
        "affiliation_name": "Vellore Institute of Technology, Chennai",
        "affiliation_city": "Chennai",
        "affiliation_country": "India",
        "affiliation_id": "60106090",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "SoK: Access Control Policy Generation from High-level Natural Language Requirements",
        "publication": "ACM Computing Surveys",
        "citied_by": "0",
        "cover_date": "2024-12-23",
        "Abstract": "Administrator-centered access control failures can cause data breaches, putting organizations at risk of financial loss and reputation damage. Existing graphical policy configuration tools and automated policy generation frameworks attempt to help administrators configure and generate access control policies by avoiding such failures. However, graphical policy configuration tools are prone to human errors, making them unusable. On the other hand, automated policy generation frameworks are prone to erroneous predictions, making them unreliable. Therefore, to find ways to improve their usability and reliability, we conducted a Systematic Literature Review analyzing 49 publications. The thematic analysis of the publications revealed that graphical policy configuration tools are developed to write and visualize policies manually. Moreover, automated policy generation frameworks are developed using machine learning (ML) and natural language processing (NLP) techniques to automatically generate access control policies from high-level requirement specifications. Despite their utility in the access control domain, limitations of these tools, such as the lack of flexibility, and limitations of frameworks, such as the lack of domain adaptation, negatively affect their usability and reliability, respectively. Our study offers recommendations to address these limitations through real-world applications and recent advancements in the NLP domain, paving the way for future research.",
        "DOI": "10.1145/3706057",
        "paper_author": "Jayasundara S.H.",
        "affiliation_name": "The University of Auckland",
        "affiliation_city": "Auckland",
        "affiliation_country": "New Zealand",
        "affiliation_id": "60005686",
        "affiliation_state": "AUK"
    },
    {
        "paper_title": "Machine-Learning-Aided Engineering Hemoglobin as Carbene Transferase for Catalyzing Enantioselective Olefin Cyclopropanation",
        "publication": "JACS Au",
        "citied_by": "0",
        "cover_date": "2024-12-23",
        "Abstract": "In this study, we developed a machine-learning-aided protein design strategy for engineering Vitreoscilla hemoglobin (VHb) as carbene transferase. A Natural Language Processing (NLP) model was used for the first time to construct an algorithm (EESP, enzyme enantioselectivity score predictor) and predict the enantioselectivity of VHb. We identified critical amino acid residue sites by molecular docking and established a simplified mutation library by site-saturated mutagenesis. Based on the simplified mutant library, the trianed EESP scored 160,000 virtual mutants, and 15 predicted high-score mutants were chosen for experimental validation. Among these mutants, VHb-WK (Y29W/P54K) demonstrated the highest diastereoselectivity and enantioselectivity of carbene transferase for the olefin cyclopropanation in aqueous conditions. Subsequently, molecular dynamics simulations were performed to explore the interaction between protein and substrates, finding that the high enantioselectivity of VHb-WK stems from the interactions of R47, Q53, and K84, which narrows the entrance of the enzyme’s pocket, favoring the restriction of the formation of reaction intermediates. Integrating the NLP model and enzyme modification offers significant advantages by reducing economic costs and workloads associated with the protein engineering process.",
        "DOI": "10.1021/jacsau.4c01045",
        "paper_author": "Xie H.",
        "affiliation_name": "Jilin University",
        "affiliation_city": "Changchun",
        "affiliation_country": "China",
        "affiliation_id": "60007711",
        "affiliation_state": "Jilin"
    },
    {
        "paper_title": "Practice and reflection on the construction of health care big data platform in Peking University First Hospital",
        "publication": "Chinese Journal of Medical Science Research Management",
        "citied_by": "0",
        "cover_date": "2024-12-21",
        "Abstract": "Objective This study aims to summarize the experience of building a research data platform at Peking University First Hospital and propose pathways for constructing healthcare big data platforms in medical institutions to provide insights for the further expansion of the application of the health care big data platform and construction of smart healthcare. Methods Hospital multi-domain data was collected through a systematic process. After data classification and cleaning, international medical standards and NLP technology were utilized to achieve data standardization and structuring. At the same time, the eCRF form, data source judgment, and mathematical calculations were optimized to ensure the data quality and application value of the scientific research data platform. Results By carrying out the construction of a scientific research data platform, Peking University First Hospital achieved the transformation of clinical data into research data, completed data collection, pushing, integration, cleaning, and governance, and conducted health care big data research. Conclusions It is recommended to subsequently expand the data scope, promote the transformation of clinical data into research data, enhance clinical experts' participation in prospective design and data collection, use an integrated data platform for automatic data pushing and calling, explore the application of large language models and conduct technical exploration and reserves.",
        "DOI": "10.3760/cma.j.cn113565-20240908-00237",
        "paper_author": "Yang Z.",
        "affiliation_name": "Peking University First Hospital",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60121971",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Practical use of AI for cyber risk management in critical infrastructures: A review",
        "publication": "Intelligent Cybersecurity and Resilience for Critical Industries: Challenges and Applications",
        "citied_by": "0",
        "cover_date": "2024-12-20",
        "Abstract": "The use of AI in cybersecurity has been mainly mobilized through the prism of enhancing existing technologies such as IDS, cryptographic algorithms, or malware analysis. These technologies, initially developed for IT infrastructures, are increasingly being deployed within Industry 4.0 without fundamental changes to cater to OT/IoT systems, which share similarities with pure IT systems. To date, the use of AI technologies within the risk management global process has been sparse. In the realm of cybersecurity, many research that purports enhancing the decision-making through AI, for tasks ranging from risk assessment to incident response, often tends to veer toward refining previous technological components. However, the emergence of generative AI and advances in NLP that could improve and foster reflexional processes, focusing on strategic decision support rather than simple phenomena classification, seems not to be sufficiently mobilized in practice. This review addresses the diverse implementations of AI technologies in enhancing the risk prevention for cybersecurity purposes on CIs, particularly through the risk management prism. An overview of the trends within cybersecurity domains and a focus on where AI applications and research are mobilized will be provided. The discussion then pivots to the critical review of the integration of AI in the cyber risk management process. Exploring how AI can be incorporated into decision-making processes to support comprehensive risk study methodologies that traditionally require expert intervention. Also, evaluating the maturity of AI technologies to address global issues like cyber risk management.",
        "DOI": "NA",
        "paper_author": "Bertholat J.",
        "affiliation_name": "Laboratoire d’Analyse et de Modélisation de Systèmes pour l’Aide à la Décision",
        "affiliation_city": "Paris",
        "affiliation_country": "France",
        "affiliation_id": "60105740",
        "affiliation_state": "Ile-de-France"
    },
    {
        "paper_title": "MORPHOLOGICAL PARSING OF KAZAKH TEXTS WITH DEEP LEARNING APPROACHES",
        "publication": "KazNU Bulletin. Mathematics, Mechanics, Computer Science Series",
        "citied_by": "0",
        "cover_date": "2024-12-20",
        "Abstract": "Morphological analysis is a crucial task in Natural Language Processing (NLP) that greatly contributes to enhancing the performance of large language models (LLMs). Although NLP technologies have seen rapid advancements in recent years, the creation of efficient morphological analysis algorithms for morphologically complex languages, such as Kazakh, continues to be a significant challenge. This research focuses on designing a morphological analysis algorithm for the Kazakh language, specifically optimized for integration with LLMs. The study will address the following tasks: data corpus collection and processing, selection and adaptation of suitable algorithms, and model training and evaluation. This paper delivers a detailed exploration of using deep learning models for the morphological analysis of the Kazakh language, specifically highlighting Recurrent Neural Networks (RNN) and Transformer models. Because of Kazakh is an agglutinative language, where word formation is achieved by attaching multiple suffixes and preffixes, the task of morphological analysis poses 25 unique challenges for computational models. The performance of Recurrent Neural Networks (RNNs) is evaluated, including those with LSTM and GRU enhancements, in comparison with Transformer models, focusing on their capability to analyze the complex morphology of Kazakh. The results outline the benefits and limitations of each approach for processing agglutinative languages, indicating that RNNs are often more effective for Kazakh morphological analysis, whereas Transformer models may require additional fine-tuning to achieve optimal results with such languages.",
        "DOI": "10.26577/JMMCS2024-v124-i4-a4",
        "paper_author": "Mansurova M.E.",
        "affiliation_name": "Al Farabi Kazakh National University",
        "affiliation_city": "Almaty",
        "affiliation_country": "Kazakhstan",
        "affiliation_id": "60071847",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Semantic-Context Embedding Enhanced Attention Fusion BiLSTM: Unraveling Multilingual Sentiments in Product Reviews with Advanced Deep Learning",
        "publication": "Journal of Information and Organizational Sciences",
        "citied_by": "0",
        "cover_date": "2024-12-18",
        "Abstract": "For natural language processing (NLP), sentiment analysis (SA) is crucial since it helps to understand users' feelings and opinions in a variety of contexts. Even though Deep Learning (DL) techniques are becoming more popular in SA, effective model optimization can be difficult because they frequently call for a great deal of hyperparameter tuning. However, because current models can't sufficiently capture the variety of review contexts, it introduces bias and inaccuracies, especially in product reviews. For Multilingual Sentiment Analysis (MSA) in product reviews, this research proposed a Semantic-Context Embedding Enhanced Attention Fusion BiLSTM (SCEEAF-BiLSTM). The proposed model combines Continuous Bag-of-Word (CBoW) and Skipgram techniques to extract semantic context after the preprocessing stages of tokenization, stop word removal, and case normalization. A novel Convolutional BiLSTM with Enhanced Attention (CoBLEA) architecture is introduced for multilingual sentiment prediction to extract comprehensive context representations. The model ultimately shows efficacy in dividing multilingual sentiments into positive, neutral, and negative states, providing a viable method for complex SA in several circumstances. The outcome signifies that the proposed approach obtains a high accuracy attained 0.987, precision attained 0.985, recall attained 0.978 and F1-Score attained 0.986 when compared with prior works. With practical applications in sentiment-driven platforms operating in multiple languages, the research presents a method for complex SA in e-commerce, social media, and customer feedback systems. It also emphasizes the significance of comprehending multilingual opinions for enhancing marketing strategies, driving business decisions, and improving customer satisfaction.",
        "DOI": "10.31341/jios.48.2.9",
        "paper_author": "Purohit A.",
        "affiliation_name": "VIT Bhopal University",
        "affiliation_city": "Sehore",
        "affiliation_country": "India",
        "affiliation_id": "60119615",
        "affiliation_state": "MP"
    },
    {
        "paper_title": "Proactive Detection of Malicious Webpages Using Hybrid Natural Language Processing and Ensemble Learning Techniques",
        "publication": "Journal of Information and Organizational Sciences",
        "citied_by": "0",
        "cover_date": "2024-12-18",
        "Abstract": "The proliferation of malicious webpages presents a growing threat to online security, necessitating advanced detection methods to mitigate risks. This paper proposes a novel approach that integrates Natural Language Processing (NLP) techniques with an ensemble of machine learning models for the proactive detection of malicious web content. By leveraging semantic analysis, lexical patterns, and metadata extraction, the proposed framework enhances the identification of suspicious patterns in web page content. The ensemble model combines decision trees, random forests, and gradient boosting methods, optimizing classification accuracy and reducing false positives. A comprehensive evaluation using a large dataset of web pages, including both benign and malicious examples, demonstrates the superiority of the proposed method over traditional single-model approaches. With accuracy rates exceeding 98%, this framework achieves a robust, scalable solution for real-time web content analysis, providing a critical tool for cybersecurity professionals to detect and block malicious webpages before they can cause harm. Future directions include the integration of deep learning architectures and adaptive filtering techniques to further refine detection capabilities.",
        "DOI": "10.31341/jios.48.2.4",
        "paper_author": "Althaf Ali A.",
        "affiliation_name": "Madanapalle Institute of Technology &amp; Science",
        "affiliation_city": "Madanapalle",
        "affiliation_country": "India",
        "affiliation_id": "60107346",
        "affiliation_state": "AP"
    },
    {
        "paper_title": "AI-Based Hate Speech Detection in Albanian Social Media: New Dataset and Mobile Web Application Integration",
        "publication": "International Journal of Interactive Mobile Technologies",
        "citied_by": "0",
        "cover_date": "2024-12-17",
        "Abstract": "This paper aims to advance AI-based hate speech (HS) detection in the Albanian language, which is resource-limited in natural language processing (NLP). Addressing the challenge of limited data, we developed a human-annotated dataset of over 11,000 comments, carefully curated from various Albanian social media platforms, containing a substantial number of HS instances. The dataset was annotated using a detailed two-layer taxonomy to capture the complex dimensions of HS. To ensure high-quality annotations, three expert annotators applied a majority voting system, achieving a substantial Fleiss’s kappa coefficient of 0.62, underscoring the reliability and consistency of the annotations. We conducted a comparative analysis of several machine learning (ML) algorithms, including support vector machine (SVM), Naïve Bayes (NB), XGBoost, and random forest (RF), paired with various text vectorisation techniques and pre-processing methods. In binary classification, the NB model with term frequencyinverse document frequency (TF-IDF) vectorization achieved the highest performance, with an F1 score of 0.80. For multiclass classification, XGBoost outperformed other models, achieving an F1 score of 0.77. Interestingly, our experiments revealed that pre-processing steps generally reduced model performance, suggesting that raw text inputs work better for the Albanian language. Through error analysis using local interpretable model-agnostic explanations (LIME), we identified key challenges, such as polysemy and irony, which contributed to misclassifications. To demonstrate the practical applicability of our work, we developed a user-friendly mobile web application based on the best-performing model, providing realtime HS detection with the potential for integration into social media platforms.",
        "DOI": "10.3991/ijim.v18i24.50851",
        "paper_author": "Fetahi E.",
        "affiliation_name": "South East European University",
        "affiliation_city": "Tetovo",
        "affiliation_country": "North Macedonia",
        "affiliation_id": "60104022",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "HindiSumm: A Hindi Abstractive Summarization Benchmark Dataset",
        "publication": "ACM Transactions on Asian and Low-Resource Language Information Processing",
        "citied_by": "0",
        "cover_date": "2024-12-17",
        "Abstract": "Abstractive Text Summarization (ATS) is a task to create a novel summary by generating fresh sentences incorporating new words or rephrasing the article. It is a complex task as the model needs to understand the semantic similarity between the sentences of the text. To fulfill this, there is a need for a large annotated benchmark dataset, which is available for resource-rich languages such as English and non-indic languages. In contrast, for the less-resourced languages, such as Indic languages, the available datasets are limited and involve very short summary sentences. Hence, a language-specific abstractive summarization dataset called HindiSumm was introduced for Hindi, consisting of 570,000 text-summary pairs from Navbharat Times across 21 domains. The HindiSumm dataset's efficiency is evaluated extrinsically and intrinsically by using various metrics. Furthermore, two recent multilingual-cased pre-trained models are fine-tuned on the HindiSumm dataset individually. In addition, an ensembled approach using weighted averaging is also incorporated to check the efficacy of the proposed dataset. The model is tested with the in-house created dataset, and results are evaluated on ROUGE scores and show significant improvements of around 13.2% for the proposed HindiSumm compared with other benchmark datasets. In the future, the HindiSumm dataset will promote the progress of ATS for the Indian language.",
        "DOI": "10.1145/3696207",
        "paper_author": "Singh G.",
        "affiliation_name": "Malaviya National Institute of Technology Jaipur",
        "affiliation_city": "Jaipur",
        "affiliation_country": "India",
        "affiliation_id": "60017757",
        "affiliation_state": "RJ"
    },
    {
        "paper_title": "Insights into Natural Language Database Query Errors: from Attention Misalignment to User Handling Strategies",
        "publication": "ACM Transactions on Interactive Intelligent Systems",
        "citied_by": "0",
        "cover_date": "2024-12-17",
        "Abstract": "Querying structured databases with natural language (NL2SQL) has remained a difficult problem for years. Recently, the advancement of machine learning (ML), natural language processing (NLP), and large language models (LLM) have led to significant improvements in performance, with the best model achieving ∼85% percent accuracy on the benchmark Spider dataset. However, there is a lack of a systematic understanding of the types, causes, and effectiveness of error-handling mechanisms of errors for erroneous queries nowadays. To bridge the gap, a taxonomy of errors made by four representative NL2SQL models was built in this work, along with an in-depth analysis of the errors. Second, the causes of model errors were explored by analyzing the model-human attention alignment to the natural language query. Last, a within-subjects user study with 26 participants was conducted to investigate the effectiveness of three interactive error-handling mechanisms in NL2SQL. Findings from this article shed light on the design of model structure and error discovery and repair strategies for natural language data query interfaces in the future.",
        "DOI": "10.1145/3650114",
        "paper_author": "Ning Z.",
        "affiliation_name": "University of Notre Dame",
        "affiliation_city": "Notre Dame",
        "affiliation_country": "United States",
        "affiliation_id": "60021508",
        "affiliation_state": "IN"
    },
    {
        "paper_title": "A Comparison of Representation Learning Methods for Medical Concepts in EHR Databases",
        "publication": "ACM-BCB 2024 - 15th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics",
        "citied_by": "0",
        "cover_date": "2024-12-16",
        "Abstract": "This study evaluates four NLP models-LDA, Word2Vec, GloVe, and BERT-for representing medical concepts in Electronic Health Records (EHR) databases using MIMIC-IV and eICU-CRD datasets. EHR contains detailed and coded information on patient diagnoses, procedures, and medications, with codes holding essential knowledge for tasks like diagnosis prediction and medication recommendations. NLP techniques, which model these codes as words within a sentence-like structure of patient visits, have shown promise in creating vector representations that capture the implicit relationships among codes. However, prior research lacks a comprehensive comparison of these methods for EHR data. Traditional NLP approaches, such as Word2Vec and GloVe, emphasize distributional semantics, while newer models like BERT offer contextual embeddings that capture more nuanced language patterns. In the settings of clinical code embedding pre-train, the results show that GloVe outperforms other models in retaining medical concept semantics and improving prediction tasks, suggesting the need for models that capture both global co-occurrence and nuanced relationships in medical data.",
        "DOI": "10.1145/3698587.3701491",
        "paper_author": "Liu Z.",
        "affiliation_name": "Oxford Suzhou Centre for Advanced Research",
        "affiliation_city": "Suzhou",
        "affiliation_country": "China",
        "affiliation_id": "60278926",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "MetaphorPrompt - An Analogical Reasoning Approach for Extracting Causal Links from Biological Text",
        "publication": "ACM-BCB 2024 - 15th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics",
        "citied_by": "0",
        "cover_date": "2024-12-16",
        "Abstract": "In recent years, Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP), offering significant improvements for extracting complex information from biomedical literature. Our research introduces a novel metaphor-based approach, MetaphorPrompt, to enhance the accuracy of extracting molecular regulatory pathways (MRPs) from biomedical texts. This method employs LLMs such as GPT4 to develop metaphors that map biological processes onto familiar, real-world scenarios, facilitating a better understanding and extracting causal events in MRPs. MetaphorPrompt is tested using the reguloGPT dataset and compared to a baseline method (without metaphors) and reguloGPT’s best prompt. Test results demonstrate improved precision, recall, and F1 scores in node and edge prediction of causal event links through analogical reasoning. The effect of in-context learning (ICL) in MetaphorPrompt is investigated, and it is found that analogical reasoning offers significant improvements over ICL. This supports the claim that LLMs can perform novel problem-solving through analogical reasoning. This work paves the way for more intuitive and user-friendly representations of MRPs in biomedical data, ultimately contributing to advancements in biomedical NLP, knowledge graph construction, and effective applications of LLMs in novel problem-solving through analogical reasoning.",
        "DOI": "10.1145/3698587.3701384",
        "paper_author": "Patel P.",
        "affiliation_name": "Margie and Bill Klesse College of Engineering and Integrated Design",
        "affiliation_city": "San Antonio",
        "affiliation_country": "United States",
        "affiliation_id": "60143633",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "Nonlinear optical properties of lead halide perovskite CsPbBr<inf>3</inf> for ultrafast pulse generation",
        "publication": "Optics Express",
        "citied_by": "0",
        "cover_date": "2024-12-16",
        "Abstract": "Halide perovskites have attracted much attention in the field of nonlinear optics due to their strong quantum constraint, obvious exciton effect, and structural diversity. In this paper, lead halide perovskite CsPbBr3 nanocrystals were synthesized with a size of 50–100 nm. The prepared CsPbBr3 nanocrystals were deposited on the conical microfibers so that the nonlinear optical absorption features can be obtained. Based on a balanced twin-detector system, the modulation depth of the prepared CsPbBr3 nanocrystals was determined as 9.2% and 7.2% at 1 µm and 1.5 µm, respectively. When the CsPbBr3 saturable absorber (SA) was inserted into the Yb-doped fiber laser (YDFL), a stable noise-like pulse (NLP) mode-locking operation was obtained at a central wavelength of 1034 nm with a pulse duration of 415 fs. In the negative dispersive Er-doped fiber laser (EDFL), both the bound-state mode-locking operation with a pulse width of 971 fs and the conventional soliton mode-locking with a pulse width of 799 fs can be achieved at 1531 nm for the first time. These findings not only pave the way for a deeper understanding of the nonlinear optical properties in CsPbBr3 nanocrystals but also provide a solid platform for further exploration of applications in ultrafast photonics.",
        "DOI": "10.1364/OE.545386",
        "paper_author": "Jiang N.",
        "affiliation_name": "Shandong University",
        "affiliation_city": "Jinan",
        "affiliation_country": "China",
        "affiliation_id": "60031031",
        "affiliation_state": "Shandong"
    },
    {
        "paper_title": "PRE-TRAINED BERT ARCHITECTURE ANALYSIS FOR INDONESIAN QUESTION ANSWER MODEL",
        "publication": "Journal of Applied Engineering and Technological Science",
        "citied_by": "0",
        "cover_date": "2024-12-15",
        "Abstract": "Developing a question-and-answer system in Natural Language Processing (NLP) has become a significant concern in the Indonesian language context. One of the main challenges in developing a question-and-answer system is the limited dataset, which can cause instability in system performance. The limitations of the dataset make it difficult for the question-and-answer model to understand and answer questions well. The proposed solution uses Transfer Learning with pre-trained models such as BERT. This research aims to analyze the performance of the BERT model, which has been adapted for question-and-answer tasks in Indonesian. The BERT model uses an Indonesian language dataset adapted specifically for question-and-answer tasks. A customization approach tunes BERT parameters according to the given training data. The results show that the model is improved by minimizing the loss function. Evaluation of the trained model shows that the best validation loss is 0.00057 after 150 epochs. In addition, through in-depth evaluation of the similarity of question texts, the BERT model can answer questions measurably, according to existing knowledge in the dataset.",
        "DOI": "10.37385/jaets.v6i1.4746",
        "paper_author": "Sudianto ",
        "affiliation_name": "Telkom University",
        "affiliation_city": "Bandung",
        "affiliation_country": "Indonesia",
        "affiliation_id": "60103730",
        "affiliation_state": "West Java"
    },
    {
        "paper_title": "Thermal comfort-constrained nonlinear operational optimization of a solar-absorption-radiant cooling system",
        "publication": "Energy Conversion and Management",
        "citied_by": "1",
        "cover_date": "2024-12-15",
        "Abstract": "With the increasing demand for sustainable building solutions, especially under extreme weather conditions, there is a growing need for renewable-powered cooling systems that can minimize energy consumption and carbon emissions. Solar-absorption-radiant cooling systems offer a promising alternative to traditional air conditioning systems, but their effectiveness relies on efficient control strategies. This study investigates the optimal control of a solar-absorption-radiant cooling system for a single-story office building using non-linear programming (NLP) to minimize operating costs while maintaining thermal comfort. This is achieved by directly integrating the building model and thermal comfort calculations within the optimization procedure. By incorporating a solar collector, storage tank, assisting boiler, and absorption chiller, the system achieves a solar fraction of 0.8, minimizing daily operating costs to 2.11 USD and carbon emissions to ∼ 39.1 kgCO2. The system maintains an average PMV of 0.14, an operative temperature of 25.63 °C, and a coefficient of performance of 0.72. The study also explores the impact of varying thermal comfort constraints, ventilation rates, and inlet air temperatures on system performance. Stricter comfort constraints (PMV=-0.2 to 0.2) increase costs and emissions by 30.96 % and 37.5 % respectively, due to increased reliance on the natural gas boiler. Doubling the ventilation rate based on fresh outdoor air increases daily costs and emissions by 19 % and 22.6 % respectively. Conversely, utilizing a supplementary system to supply ventilation air at 25 °C significantly reduces costs and emissions by 26.2 % and 25.4 % respectively, and increases the solar fraction to 0.92. Compared to a conventional system powered solely by a natural gas boiler, the solar-powered system achieves substantial cost savings (45.9 %), reduced carbon emissions (52.5 %), and improved thermal comfort, highlighting the potential of this technology for sustainable building operations.",
        "DOI": "10.1016/j.enconman.2024.119204",
        "paper_author": "Elbakhshwan A.E.",
        "affiliation_name": "Faculty of Engineering",
        "affiliation_city": "Cairo",
        "affiliation_country": "Egypt",
        "affiliation_id": "60002575",
        "affiliation_state": "Cairo"
    },
    {
        "paper_title": "Path Planning with Rear-Wheel Steering for Enhanced Parking Efficiency in Narrow Parking Spaces",
        "publication": "SAE Technical Papers",
        "citied_by": "0",
        "cover_date": "2024-12-13",
        "Abstract": "Path planning in parking scenarios for vehicles with Ackermann steering characteristics is a well studied problem in the literature. However, the recent emergence of four-wheel steering (4WS) chassis has brought new opportunities to the field of motion planning. Compared with front-wheel steering (2WS), 4WS vehicles offer higher flexibility and new maneuver modes such as CrabWalk. To utilize such new potential to further improve parking efficiency, this paper proposes a four-wheel steering oriented planning algorithm for parking scenarios. First, Hybrid A*-4WS is proposed to search for a coarse trajectory from the starting pose to the parking slot, with improved node expansion mechanism to incorporate four-wheel steering characteristics. Then a nonlinear programming (NLP) problem is formulated with four-wheel steering kinematic model to fully utilize the maneuver capability of 4WS vehicles, with OBCA used for collision avoidance constraints. Finally, the two algorithms are sequentially integrated, using the coarse trajectory obtained from Hybrid A*-4WS as the initial guess for optimization. Simulation results demonstrate that our proposed algorithm achieves higher parking efficiency in narrow space, and the efficiency is improved as maximum rear steering angle increases.",
        "DOI": "10.4271/2024-01-7051",
        "paper_author": "Song Y.",
        "affiliation_name": "Tongji University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60073652",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Adaptive Partition Linearization Global Optimization Algorithm and Its Application on the Simultaneous Heat Exchanger Network and Organic Rankine Cycle Optimization",
        "publication": "Industrial and Engineering Chemistry Research",
        "citied_by": "0",
        "cover_date": "2024-12-11",
        "Abstract": "The simultaneous optimization problem of the heat exchanger network and organic Rankine cycle (HEN-ORC) poses significant challenges due to its highly nonconvex and nonlinear equations. We develop an adaptive partition linearization global optimization algorithm which is suitable for a wide range of mixed integer nonlinear programming (MINLP) problems and specially customized for HEN-ORC. The algorithm identifies convex equations of the logarithmic mean temperature function and the power function within the HEN-ORC model, which are relaxed by the first Taylor expansion and piecewise linearization. A multilevel McCormick relaxation is applied for the bilinear/multilinear functions derived from the HEN-ORC energy balance equations. The algorithm achieves global optimality by solving mixed integer linear programming and NLP submodels iteratively, enhancing the lower bound adaptively. Tested on seven heat exchanger networks and waste heat power generation cases, it outperforms two mainstream MINLP global optimization solvers (Baron and Couenne). The current best solutions are obtained for both a HEN and a HEN-ORC case, respectively.",
        "DOI": "10.1021/acs.iecr.4c02620",
        "paper_author": "Hong X.",
        "affiliation_name": "State Key Laboratory of Chemical Engineering, Zhejiang University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60117934",
        "affiliation_state": "Zhejiang"
    },
    {
        "paper_title": "Advancements and Future Directions in Deep Learning-based Natural Language Processing",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2024-12-11",
        "Abstract": "AI is increasingly used in NLP, the principles of which are based on machine learning and deep learning. In this paper, it is found that by using a large-scale corpus for training, the model can learn the syntactic, semantic, and contextual information of the language, and then realize tasks such as natural language generation, text classification, sentiment analysis, and question and answer systems. In addition, machine learning is essential in improving the efficiency of NLP processing. Innovative algorithms such as Transformer, BERT, and GPT have brought breakthroughs in the field of NLP. These algorithms improve the generalization ability and processing efficiency of models by using self-attention mechanisms and pre-training techniques. In the future, research on deep learning-based language models will focus on lightweight, multimodal, cross-language, interpretability, and privacy security. Lightweight models aim to reduce model complexity and computational resource requirements, and multimodal language model research fuses information in multiple language forms into models to improve expressiveness and comprehension. Cross-language modeling research will focus on multilingual processing and knowledge sharing to improve the adaptability and expressiveness of models in different language environments. Meanwhile, interpretability and privacy security issues will also become important directions for future research.",
        "DOI": "10.1063/5.0224436",
        "paper_author": "Xue F.",
        "affiliation_name": "Tongji University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60073652",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Research on Artificial Intelligence Speech Recognition Technology",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2024-12-11",
        "Abstract": "With the development of science and technology, speech recognition technology allows people to have a better human-computer interaction experience, improves productivity, and greatly facilitates people's daily lives. This paper introduces speech recognition technology and basic processes and explains the technical differences between different speech recognition models. It compares previous data based on control variables and analyzes the advantages and disadvantages of different model performances. Through the research of this paper, it is found that the reason why the transformer model has become a popular model in the field of NLP is inseparable from its advantages of long-distance dependency processing and parallel computing, and speech recognition technology is developing in the direction of high recognition accuracy, fast model recognition rate, and less computer storage resources. This paper aims to explain the representative models of each stage of artificial intelligence speech recognition technology, analyze the development direction of this technology, and let people better understand speech recognition technology.",
        "DOI": "10.1063/5.0222853",
        "paper_author": "Yuan T.",
        "affiliation_name": "Nanhang Jincheng College",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "124142207",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Specific Applications of Text Restatement and Sentiment Analysis in Natural Language Processing Projects",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2024-12-11",
        "Abstract": "Natural language processing (NLP) investigates how computers can comprehend, process, and produce natural language that is similar to that of humans. Text restatement has emerged as a prominent area of study in NLP in recent years. This paper aims to analyze text restatement classification and mainstream technical models. This paper concludes that text restatement can be divided into three categories: machine translation, text style transfer, and text simplification. Machine translation is characterized by statistical machine translation based on parallel probabilistic syntax. Supervised learning can be used in text-style transfer tasks. The main method is the Text style transfer method of the seq2seq model. This method illustrates a framework based on reinforcement learning, which relies on the style accuracy scores and content retention scores of converted samples as rewards, thus promoting the generation of more compliant samples. Text simplification uses de-noising autoencoder to learn from simple single sentence corpus and complex sentence corpus respectively, and obtains the autoencoder of simple sentence and complex sentence. However, due to the lack of corpus, translation performance may not be as good as in the general domain. Therefore, research on how to carry out domain adaptation to improve the quality of translation in a specific field is also the development direction of future research. In addition, the new task of text simplification at the document level can also be combined with the text summarization task, which will become the future research direction.",
        "DOI": "10.1063/5.0225494",
        "paper_author": "Li J.",
        "affiliation_name": "Shenyang Ligong University",
        "affiliation_city": "Shenyang",
        "affiliation_country": "China",
        "affiliation_id": "60073663",
        "affiliation_state": "Liaoning"
    },
    {
        "paper_title": "Research on Large Model Text Adversarial Attack",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2024-12-11",
        "Abstract": "With the rapid development of deep learning and large model technologies, the field of natural language processing (NLP) is facing unprecedented opportunities and challenges. However, the widespread application of large models in NLP tasks also exposes them to the risk of text adversarial attacks. This paper focuses on researching and addressing adversarial attacks on large models to enhance their security. Firstly, the paper delves into the fundamental principles of text adversarial attacks, with a particular focus on how attackers construct adversarial samples through methods such as substitution, insertion, and deletion. By assessing the vulnerability of large models and employing causal inference methods to analyze the causal relationships between the model and its inputs, the paper reveals the sources of vulnerability. Additionally, introducing counterfactual analysis provides a more comprehensive understanding of how the model's performance varies under different input conditions. In the design of adversarial attack defense mechanisms, the paper comprehensively considers various ways of text adversarial attacks and introduces the concept of data augmentation. By expanding the training dataset, the paper aims to improve the adaptability of large models to diverse inputs. Simultaneously, optimizing defense mechanisms enhances their specificity and robustness. The successful implementation of this research can deepen the understanding of text adversarial attack issues and potentially offer innovative solutions for constructing more secure and reliable applications of large models.",
        "DOI": "10.1063/5.0225741",
        "paper_author": "Ci X.",
        "affiliation_name": "Tianjin University of Technology",
        "affiliation_city": "Tianjin",
        "affiliation_country": "China",
        "affiliation_id": "60026990",
        "affiliation_state": "Tianjin"
    },
    {
        "paper_title": "Research on Text Classification based on Deep Learning and Word Vector",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2024-12-11",
        "Abstract": "Due to the rapid development of the network, the amount of text data has increased greatly. How to deal with the huge amount of data is a difficult problem people should face. As a consequence, the paper studies text classification based on deep learning and word vector technology, and word vector is an important technology in Natural language processing(NLP). NLP is the ability to convert words into numerical vector representations, thus enabling computers to process and understand textual information. By analyzing different classification methods based on word vectors, it can be better understood the classification and organization of text data. In the current social scenario, the ability to process and analyze data will be crucial. Therefore, a study has important practical significance and application value in the current environment. It aims to apply deep learning and word vector technology to text classification tasks, further improve the accuracy and efficiency of classification, and provide better support for practical applications. Finally, this study also provides some valuable references and enlightenment for research in related fields and promotes the development of the field of natural language processing and text analysis.",
        "DOI": "10.1063/5.0223219",
        "paper_author": "Yu Z.",
        "affiliation_name": "Hebei Geo university",
        "affiliation_city": "Shijiazhuang",
        "affiliation_country": "China",
        "affiliation_id": "60073672",
        "affiliation_state": "Hebei"
    },
    {
        "paper_title": "Research on the Application of Natural Language Processing in the Analysis of Ancient Poems and Texts",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2024-12-11",
        "Abstract": "Chinese classical poetry is a crucial part of Chinese culture, containing the essence of the Chinese nation. The use of natural language processing (NLP) in analyzing ancient poetry and texts has proven to be valuable. This paper explores the application of NLP in this area, discussing its concepts, techniques, and case studies. It compares previous studies, highlights the advantages, limitations, and potential impacts of NLP in analyzing ancient poems and texts. This paper summarizes the importance and potential of natural language processing in the analysis of ancient poetry and text and points out the current challenges and future development directions. It concludes that while NLP has made significant progress, there are still technical challenges to be addressed. In conclusion, the application of natural language processing in the analysis of ancient poems and texts shows its great prospect in the intersection of artificial intelligence and literature research. In the future, with the advancement of technology and the deepening of research, it is believed that NLP will play an increasingly important role in the study of ancient poetry and prose.",
        "DOI": "10.1063/5.0227196",
        "paper_author": "Liu J.",
        "affiliation_name": "Beijing National Day School",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "132091277",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Applications of Transformer in Remote Sensing for Image Scene Classification, Semantic Segmentation, and Change Detection",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2024-12-11",
        "Abstract": "In recent years, with the proposal and widespread application of Transformer, which was originally applied in natural language processing (NLP), Transformer has become the preferred model for NLP tasks. Based on Transformer's great success in the NLP task, this model has also become a hot research direction in the field of computer vision (CV). In the field of CV, models based on convolutional neural network (CNN) architecture have taken a dominant position. CNN adopts hierarchical feature representation, which can effectively extract deep features, but its local nature limits the processing of long-range dependencies. Transformer, based on a self-attention mechanism, is capable of effectively understanding the global context. The successful application of Transformers in visual tasks demonstrates their enormous potential in the CV field. Inspired by this, the remote sensing community has also made extensive exploration of the application of Transformers. This research involves a variety of remote sensing image tasks, and Transformer has shown excellent performance in these tasks. This paper briefly describes the classic research of several remote sensing image tasks, aiming to elaborate on the application status of Transformers in the field of remote sensing. These classic research cases are introduced in detail, and the research results are compared with the traditional methods. Finally, the problems to be solved and the feasible research directions are discussed.",
        "DOI": "10.1063/5.0225051",
        "paper_author": "Zhou P.",
        "affiliation_name": "China University of Geosciences",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60006019",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Metric-based pill recognition with the help of textual and visual cues",
        "publication": "IET Image Processing",
        "citied_by": "0",
        "cover_date": "2024-12-11",
        "Abstract": "Pill image recognition by machine vision can reduce the risk of taking the wrong medications, a severe healthcare problem. Automated dispensing machines or home applications both need reliable image processing techniques to compete with the problem of changing viewing conditions, large number of classes, and the similarity in pill appearance. The problem is attacked with a multi-stream, two-phase metric embedding neural model. To enhance the metric learning procedure, dynamic margin setting is introduced into the loss function. Moreover, it is shown that besides the visual features of drug samples, even free text of drug leaflets (processed with a natural language model) can be used to set the value of the margin in the triplet loss and thus increase the recognition accuracy of testing. Thus, besides using the conventional metric learning approach, the given discriminating features can be explicitly injected into the metric model using the NLP of the free text of pill leaflets or descriptors of images of selected pills. The performance on two datasets is analysed and a 1.6% (two-sided) and 2.89% (one-sided) increase in Top-1 accuracy on the CURE dataset is reported compared to existing best results. The inference time on CPU and GPU makes the proposed model suitable for different kinds of applications in medical pill verification; moreover, the approach applies to other areas of object recognition where few-shot problems arise. The proposed high-level feature injection method (into a low-level metric learning model) can also be exploited in other cases, where class features can be well described with textual or visual cues.",
        "DOI": "10.1049/ipr2.13273",
        "paper_author": "Rádli R.",
        "affiliation_name": "Pannon Egyetem",
        "affiliation_city": "Veszprem",
        "affiliation_country": "Hungary",
        "affiliation_id": "60024419",
        "affiliation_state": "Veszprém"
    },
    {
        "paper_title": "Enhancing Disaster Detection on Social Media with Natural Language Processing",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2024-12-10",
        "Abstract": "In the contemporary globalized society, social media serves as a crucial component of our everyday routines. Among the various platforms, Twitter has emerged as a prominent micro-blogging and social networking site, enabling users to share news, information, and personal reflections. During times of emergencies or disasters, Twitter has proven to be an invaluable communication channel. The widespread usage of smartphones and tablets, individuals can promptly report emergencies in real-time, potentially saving numerous lives by alerting others to take necessary precautions. Recognizing the significance of Twitter in crisis situations, several organizations are actively engaged in programmatically analyzing tweets to identify and respond to disasters and emergencies. Such efforts can benefit millions of internet users by providing timely alerts during times of crisis. However, the main challenge lies in distinguishing between tweets that are directly related to a disaster and unrelated disaster. Unstructured nature of Twitter data is given, the application of Natural Language Processing (NLP) becomes essential for effectively classifying tweets as either \"Related to Disaster\" or \"Not Related to Disaster”. This research paper focuses on the development of a decision tree classifier model that leverages NLP techniques. The model's accuracy is evaluated by making predictions on a test set created from the original dataset. By employing this approach, we aim to address the crucial task of accurately identifying disaster-related tweets amidst the vast sea of Twitter data.",
        "DOI": "10.1063/5.0240645",
        "paper_author": "Bagde V.",
        "affiliation_name": "Yeshvantrao Chavhan College of Engineering",
        "affiliation_city": "Nagpur",
        "affiliation_country": "India",
        "affiliation_id": "124720969",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Rule Based Approach for Social Media Contextual Ambiguity Detection",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2024-12-10",
        "Abstract": "Pragmatic Ambiguity Detection is vital in natural language processing, especially in requirement engineering, as it identifies ambiguities inherent in human language that may mislead automated systems. These ambiguities pose challenges in various NLP tasks like sentiment analysis and machine translation. This study introduces an approach using NLP and semantic web techniques to detect and resolve ambiguities in natural language requirements. The approach pinpoints ambiguous words, offers potential interpretations, and clarifies the intended meaning. This paper presented a hybrid rule-based and machine learning based approach to detect pragmatic ambiguity in textual data. The data is collected from twitter reviews for result evaluation. The proposed hybrid approach encompasses multiple approaches to detect ambiguity. The PoS Ambiguity Algorithm (PAA) identifies multiple verbs and checks for duplicate tokens and ambiguous nouns and verbs. The Context Ambiguity Algorithm (CAA) leverages the BERT model for determining ambiguity. The Sentiment Ambiguity Algorithm (SAA) uses NLTK's sentiment analyser to gauge text sentiment, flagging ambiguity when scores fall below certain thresholds. Then their features are extracted and fused together and fed into voting classifier for final classification of pragmatic ambiguity sense as low, average and high. This research delves into the comparative analysis of various machine learning algorithms on a dataset, assessing their performance based on accuracy. The machine learning models evaluated include Support Vector Machines (SVM), Random Forest (RF), XGBoost, Linear Regression (LR), Decision Tree (DT), Gradient Boosting (GB), k-Nearest Neighbours (k-NN), and a composite Voting Classifier. Results shows the efficacy of the voting classifier over other machine learning approaches.",
        "DOI": "10.1063/5.0240521",
        "paper_author": "Satpute R.S.",
        "affiliation_name": "Datta Meghe Institute of Higher Education &amp; Research (Deemed to be University)",
        "affiliation_city": "Wardha",
        "affiliation_country": "India",
        "affiliation_id": "60094583",
        "affiliation_state": "MH"
    },
    {
        "paper_title": "Empowering Healthcare with NLP: Revolutionizing Medical Record Analysis, Patient Monitoring and Drug Discovery",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2024-12-10",
        "Abstract": "This abstract highlights the game-changing role of Natural Language Processing (NLP) in healthcare. NLP has brought about a paradigm shift by revolutionizing medical record analysis, patient monitoring, and drug discovery. By efficiently extracting insights from unstructured clinical text, NLP optimizes medical record analysis, enhancing diagnoses and patient outcomes. Real-time patient monitoring through NLP-driven systems offers timely insights from electronic health records and wearable device data, improving proactive care. In drug discovery, NLP accelerates research by analyzing vast amounts of literature, aiding in candidate identification and understanding disease mechanisms. Despite challenges like privacy concerns and linguistic complexities, NLP's ethical implementation holds promise in healthcare. As NLP advances, it offers personalized treatment suggestions, early anomaly detection, and streamlined drug development. This abstract showcases NLP's potential to reshape healthcare, fostering efficient practices and innovative solutions while emphasizing responsible integration.",
        "DOI": "10.1063/5.0240706",
        "paper_author": "Vaywhare H.",
        "affiliation_name": "Datta Meghe Institute of Higher Education &amp; Research (Deemed to be University)",
        "affiliation_city": "Wardha",
        "affiliation_country": "India",
        "affiliation_id": "60094583",
        "affiliation_state": "MH"
    },
    {
        "paper_title": "Recommendation System for Animal Healthcare Virtual Conversational AI-Assistant Chabot",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2024-12-10",
        "Abstract": "Healthcare is very important in today’s life. Now Chabot plays very important role in day today life. It is a smart computer software that mimics human interaction by understanding client requests and automating responses to them using AI and NLP. It is efficient to use. The model can be used to treat animals well and boost efficiency in the field of animal healthcare. It analyses information provided by users in natural language and responds with informed and pertinent information. Bots that may be used to triage patientsand route them to the right care are known as health Chabot. These Chabot are more dependable and secure when compared to Google search when consumers are attempting to determine the origin of their symptoms [3]. The objective is to create a recommended medical Chabot using artificial intelligence that can understand the problem and recommended the doctor based on review and location of patient.",
        "DOI": "10.1063/5.0241056",
        "paper_author": "Tembhare L.",
        "affiliation_name": "Yeshwantrao Chavan College of Engineering",
        "affiliation_city": "Nagpur",
        "affiliation_country": "India",
        "affiliation_id": "60099081",
        "affiliation_state": "MH"
    },
    {
        "paper_title": "HEALING THROUGH EMPATHY: MACHINE LEARNING FOR ADAPTIVE THERAPY FOR CHILDREN AND YOUTH IN ARMED CONFLICT",
        "publication": "Sociological Studies of Children and Youth",
        "citied_by": "0",
        "cover_date": "2024-12-10",
        "Abstract": "This chapter introduces EmoGenPath, an innovative machine learning-based model designed to deliver adaptive therapy to children and youth affected by the trauma of armed conflict. The model synthesizes advanced artificial intelligence (AI) techniques, including convolutional neural networks for emotion recognition and an advantage-actor critic-trained reinforcement learning model for therapeutic content tailoring toward goal achievement, to provide a dynamic and personalized therapeutic experience. Recognizing the importance of empathetic and culturally sensitive interventions, EmoGenPath offers a unique approach by prioritizing the emotional states and individual narratives of its users. In regions where conflict has impeded traditional mental health services, this model aims to bridge the gap, facilitating resilience and recovery through a virtual therapeutic environment that can be accessed via low-bandwidth internet connections, ensuring broader reach and impact. This chapter emphasizes the ethical implementation of AI in sensitive settings. It discusses the imperative of privacy, security, and inclusive design, ensuring that the model is responsive to diverse emotional expressions across different ethnicities and backgrounds. Additionally, it outlines the potential of such a model to scale therapeutic resources effectively, delivering tailored interventions with a compassionate approach.",
        "DOI": "10.1108/S1537-466120240000035014",
        "paper_author": "Shah J.M.",
        "affiliation_name": "University of Illinois at Chicago",
        "affiliation_city": "Chicago",
        "affiliation_country": "United States",
        "affiliation_id": "60027561",
        "affiliation_state": "IL"
    },
    {
        "paper_title": "Chatbots: The Future of Education?",
        "publication": "International Journal of Engineering Pedagogy",
        "citied_by": "0",
        "cover_date": "2024-12-10",
        "Abstract": "Chatbots are emerging technologies with the potential to improve teaching and learning processes. This paper conducts a systematic review of research on chatbots in education, focusing on articles published in Online-Journals.org from 2011 to 2024. The aim is to examine the various aspects addressed by the authors, such as design principles, pedagogical roles, interaction styles, and evaluation methods for chatbots in educational contexts. The tools were classified according to the type of user they targeted, revealing that 42% were aimed at students, 11% at teachers, 29% at both types of users, and 18% at external users. The characteristics of the tools along the above dimensions were analyzed, highlighting trends, good practices, and observed limitations. The key findings, challenges, and implications of using chatbots to improve learning outcomes, and experiences were discussed. It was concluded that chatbots are an emerging technology that offers benefits such as teaching personalization, self-learning, and real-time feedback but also poses challenges, such as evaluation and research into their effectiveness for education.",
        "DOI": "10.3991/ijep.v14i8.49715",
        "paper_author": "Plazas Rosas R.A.",
        "affiliation_name": "Universidad del Valle, Cali",
        "affiliation_city": "Cali",
        "affiliation_country": "Colombia",
        "affiliation_id": "60066812",
        "affiliation_state": "Valle del Cauca"
    },
    {
        "paper_title": "VulnScopper: Unveiling Hidden Links Between Unseen Security Entities",
        "publication": "GNNet 2024 - Proceedings of the 3rd GNNet Workshop on Graph Neural Networking Workshop, Co-Located with: CoNEXT 2024",
        "citied_by": "0",
        "cover_date": "2024-12-09",
        "Abstract": "The Common Vulnerabilities and Exposures (CVE) system is crucial for cybersecurity, providing standardized identification of vulnerabilities. In February 2024, the National Vulnerability Database (NVD) announced it could no longer enrich new CVEs due to increasing volumes, significantly impacting global security efforts. This paper introduces VulnScopper, an innovative approach to automate and enhance vulnerability enrichment using Graph Neural Networks (GNNs). VulnScopper combines Knowledge Graphs (KG) with Natural Language Processing (NLP) by leveraging ULTRA, a GNN-based knowledge graph foundation model, alongside a Large Language Model (LLM). VulnScopper’s inductive approach enables it to handle unseen entities, overcoming a crucial limitation of previous CVE enrichment methods. We evaluate VulnScopper on the NVD dataset in inductive and transductive setups for CVE to Common Platform Enumerations (CPE) linking. Our results show that VulnScopper outperforms state-of-the-art techniques, achieving up to 60% Hits@10 accuracy in linking CVEs to CPE on unseen CVE records. We demonstrate VulnScopper’s effectiveness on unseen 2023 CVEs, showcasing its ability to uncover new vulnerable products and potentially reduce vulnerability remediation time.",
        "DOI": "10.1145/3694811.3697819",
        "paper_author": "Alfasi D.",
        "affiliation_name": "Reichman University",
        "affiliation_city": "Herzliya",
        "affiliation_country": "Israel",
        "affiliation_id": "60012513",
        "affiliation_state": "Tel Aviv"
    },
    {
        "paper_title": "OSmart: Whitebox Program Option Fuzzing",
        "publication": "CCS 2024 - Proceedings of the 2024 ACM SIGSAC Conference on Computer and Communications Security",
        "citied_by": "0",
        "cover_date": "2024-12-09",
        "Abstract": "Program options are ubiquitous and serve as a fundamental mechanism for configuring and customizing software behaviors. Given their widespread use, testing program options becomes essential to ensure that the software behaves as expected across various configurations. Existing option-aware fuzzers either mutate options as if they were standard program inputs or employ NLP techniques to deduce relationships among options from the documentation. However, there has not been a whitebox approach that generates option combinations by capturing the inherent execution logic of the program. This paper presents OSmart, a whitebox approach designed to systematically extract program options and effective option combinations that precisely encapsulate the intrinsic execution logic of the program, incorporating both data dependency and control dependency. OSmart successfully inferred 12,560 option combinations from 56 programs. Additionally, OSmart uncovered that more than 67% of evaluated programs have undocumented options. By integrated with AFL++, OSmart discovered 40.3% more paths, which led to the detection of 51 new bugs and the assignment of 18 CVE IDs. Finally, we also compared OSmart with four state-of-the-art option-aware fuzzers on a public benchmark and our tool achieved higher line coverage in 66.7% (20/30) of the evaluated programs.",
        "DOI": "10.1145/3658644.3690228",
        "paper_author": "Wang K.",
        "affiliation_name": "Institute of Software Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60025256",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "TTPXHunter: Actionable Threat Intelligence Extraction as TTPs from Finished Cyber Threat Reports",
        "publication": "Digital Threats: Research and Practice",
        "citied_by": "1",
        "cover_date": "2024-12-09",
        "Abstract": "Understanding the modus operandi of adversaries aids organizations to employ efficient defensive strategies and share intelligence in the community. This knowledge is often present in unstructured natural language text within threat analysis reports. A translation tool is needed to interpret the modus operandi explained in the sentences of the threat report and convert it into a structured format. This research introduces a methodology named TTPXHunter for automated extraction of threat intelligence in terms of Tactics, Techniques, and Procedures (TTPs) from finished cyber threat reports. It leverages cyber domain-specific state-of-the-art natural language model to augment sentences for minority class TTPs and refine pinpointing the TTPs in threat analysis reports significantly. We create two datasets: an augmented sentence-TTP dataset of sentence samples and a real-world cyber threat intelligence report-to-TTP dataset. Further, we evaluate TTPXHunter on the augmented sentence and report datasets. The TTPXHunter achieves the highest performance of f1-score on the augmented dataset, and it also outperforms existing state-of-the-art TTP extraction method by achieving an f1-score of when evaluated over the report dataset. TTPXHunter significantly improves cybersecurity threat intelligence by offering quick, actionable insights into attacker behaviors. This advancement automates threat intelligence analysis and provides a crucial tool for cybersecurity professionals to combat cyber threats.",
        "DOI": "10.1145/3696427",
        "paper_author": "Rani N.",
        "affiliation_name": "Indian Institute of Technology Kanpur",
        "affiliation_city": "Kanpur",
        "affiliation_country": "India",
        "affiliation_id": "60021988",
        "affiliation_state": "UP"
    },
    {
        "paper_title": "MGTBench: Benchmarking Machine-Generated Text Detection",
        "publication": "CCS 2024 - Proceedings of the 2024 ACM SIGSAC Conference on Computer and Communications Security",
        "citied_by": "2",
        "cover_date": "2024-12-09",
        "Abstract": "Nowadays, powerful large language models (LLMs) such as ChatGPT have demonstrated revolutionary power in a variety of natural language processing (NLP) tasks such as text classification, sentiment analysis, language translation, and question-answering. Consequently, the detection of machine-generated texts (MGTs) is becoming increasingly crucial as LLMs become more advanced and prevalent. These models have the ability to generate human-like language, making it challenging to discern whether a text is authored by a human or a machine. This raises concerns regarding authenticity, accountability, and potential bias. However, existing methods for detecting MGTs are evaluated using different model architectures, datasets, and experimental settings, resulting in a lack of a comprehensive evaluation framework that encompasses various methodologies. Furthermore, it remains unclear how existing detection methods would perform against powerful LLMs. In this paper, we fill this gap by proposing the first benchmark framework for MGT detection against powerful LLMs, named MGTBench. Extensive evaluations on public datasets with curated texts generated by various powerful LLMs such as ChatGPT-turbo and Claude demonstrate the effectiveness of different detection methods. Our ablation study shows that a larger number of words in general leads to better performance and most detection methods can achieve similar performance with much fewer training samples. Additionally, our findings reveal that metric-based/model-based detection methods exhibit better transferability across different LLMs/datasets. Furthermore, we delve into a more challenging task: text attribution, where the goal is to identify the originating model of a given text, i.e., whether it is a specific LLM or authored by a human. Our findings indicate that the model-based detection methods still perform well in the text attribution task. To investigate the robustness of different detection methods, we consider three adversarial attacks, namely paraphrasing, random spacing, and adversarial perturbations. We discover that these attacks can significantly diminish detection effectiveness, underscoring the critical need for the development of more robust detection methods. We envision that MGTBench will serve as a benchmark tool to accelerate future investigations involving the evaluation of powerful MGT detection methods on their respective datasets and the development of more advanced MGT detection methods.1",
        "DOI": "10.1145/3658644.3670344",
        "paper_author": "He X.",
        "affiliation_name": "The Hong Kong University of Science and Technology (Guangzhou)",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60276981",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Innovating Edge Detection with Self-Attention Mechanisms and Novel Post-Processing Modules",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2024-12-08",
        "Abstract": "In previous studies on edge extraction, researchers have consistently sought to balance efficiency, quality, and stability. To address this, we propose an edge detection algorithm that leverages the self-attention mechanism and post-processing methods to enhance the quality and stability of edge extraction in image processing. In recent years, self-attention mechanisms have emerged as a core component in many cutting-edge models, particularly in the fields of Natural Language Processing (NLP) and Computer Vision. They are capable of capturing long-range dependencies within the data. Consequently, we introduce the self-attention mechanism to enhance the capability of feature representation. By employing channel compression and scaling factors to stabilize the computation of attention scores, we ensure the reliability of the training process. Next, we combine Gaussian blurring with adaptive thresholding in the post-processing stage to reduce noise and detail interference, thereby improving edge detection accuracy. Experimental results on extensive benchmark datasets demonstrate that the proposed algorithm significantly outperforms existing methods in terms of edge detection accuracy and stability, particularly in the presence of complex backgrounds and noise. Additionally, the method exhibits strong computational efficiency, making it suitable for practical application. These advancements offer new insights and approaches for the development of edge detection technology.",
        "DOI": "10.1145/3700906.3700966",
        "paper_author": "Zheng H.",
        "affiliation_name": "East China University of Science and Technology",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60011069",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Based on the Semantics Analysis of the URL Identification and Malicious Code Detection",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2024-12-08",
        "Abstract": "The traditional malicious URL identification methods usually adopt blacklist technology, heuristic algorithm and machine learning algorithm. This paper considers that natural language processing technology can be introduced when dealing with text with context. This technique is used to help computers understand, interpret, and manipulate human language. It is often used to deal with contextual problems, and when combined with malicious URL directions, it can produce more accurate models than existing methods. In addition to the traditional TF-IDF detection method, this paper introduces N-Gram and Word2vec methods for the first time, a total of three different natural language processing technologies to process and extract URL data. Through a series of experiments, this paper proves that semantic analysis can improve the accuracy of malicious code detection successfully by adjusting parameters of the optimization model. The final experimental results show that the detection rate of malicious URLs by TF-IDF method and N-Gram method combined with various machine learning models is about 85%, while the detection rate of Word2vec method combined with deep learning model reaches 99%, and the detection accuracy is significantly improved.",
        "DOI": "10.1145/3700906.3700941",
        "paper_author": "Mingxin W.",
        "affiliation_name": "Fiberhome Telecommunication Technologies Company Limited",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60073689",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Retrieval-Enhanced Machine Learning: Synthesis and Opportunities",
        "publication": "SIGIR-AP 2024 - Proceedings of the 2024 Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region",
        "citied_by": "0",
        "cover_date": "2024-12-08",
        "Abstract": "Retrieval-enhanced machine learning (REML) refers to the use of information retrieval methods to support reasoning and inference in machine learning tasks. Although relatively recent, these approaches can substantially improve model performance. This includes improved generalization, knowledge grounding, scalability, freshness, attribution, interpretability and on-device learning. To date, despite being influenced by work in the information retrieval community, REML research has predominantly been presented in natural language processing (NLP) conferences. Our tutorial addresses this disconnect by introducing core REML concepts and synthesizing the literature from various domains in machine learning (ML), including, but beyond NLP. What is unique to our approach is that we used consistent notations, to provide researchers with a unified and expandable framework. The tutorial will be presented in lecture format based on an existing manuscript, with supporting materials and a comprehensive reading list available at https://retrieval-enhanced-ml.github.io/SIGIR-AP2024-tutorial.",
        "DOI": "10.1145/3673791.3698439",
        "paper_author": "Diaz F.",
        "affiliation_name": "Carnegie Mellon University",
        "affiliation_city": "Pittsburgh",
        "affiliation_country": "United States",
        "affiliation_id": "60027950",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "Telemedicine-based serious illness conversations, healthcare utilization, and end of life care among patients with advanced lung cancer",
        "publication": "The oncologist",
        "citied_by": "0",
        "cover_date": "2024-12-06",
        "Abstract": "PURPOSE: Little is known about serious illness conversations (SIC) conducted during telemedicine visits and their impact on end-of-life (EOL) outcomes for patients with advanced cancer. PATIENTS AND METHODS: We conducted a retrospective analysis telemedicine visits for patients with metastatic lung cancer conducted during the first surge of the COVID-19 pandemic (October 3, 2020-October 6, 2020). We used natural language processing (NLP) to characterize documentation of SIC domains (ie, goals of care [GOC], limitation of life-sustaining treatment [LLST], prognostic awareness [PA], palliative care [PC], and hospice). We used unadjusted logistic regression to evaluate factors associated with SIC documentation and the relationship between SIC documentation and EOL outcomes. RESULTS: The study included 634 telemedicine visits across 360 patients. Documentation of at least one SIC domain was present in 188 (29.7%) visits with GOC and PA being the most discussed domains. Family presence (odds ratio [OR], 1.66; P = .004), progressive or newly diagnosed disease (OR, 5.42; P < .000), age ≥ 70 (OR, 1.80; P = .009), and male sex (OR, 2.23; P < .000) were associated with a greater likelihood of discussing ≥ 1 SIC domain. Of the 61 patients who died within 12 months of the study period, having ≥ 1 SIC domain discussed was associated with a lower likelihood of hospitalization in the last 30 days of life (OR, 0.27; P = .020). CONCLUSION: In this study of telehealth visits, we identified important factors associated with an increased likelihood of having documentation of an SIC and demonstrated that SIC documentation correlated with lower likelihood of hospitalization at EOL.",
        "DOI": "10.1093/oncolo/oyae216",
        "paper_author": "Dhawale T.M.",
        "affiliation_name": "Massachusetts General Hospital",
        "affiliation_city": "Boston",
        "affiliation_country": "United States",
        "affiliation_id": "60029929",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Protein-Protein Interaction Networks Derived from Classical and Machine Learning-Based Natural Language Processing Tools",
        "publication": "Journal of Proteome Research",
        "citied_by": "0",
        "cover_date": "2024-12-06",
        "Abstract": "The study of protein-protein interactions (PPIs) provides insight into various biological mechanisms, including the binding of antibodies to antigens, enzymes to inhibitors or promoters, and receptors to ligands. Recent studies of PPIs have led to significant biological breakthroughs. For example, the study of PPIs involved in the human:SARS-CoV-2 viral infection mechanism aided in the development of SARS-CoV-2 vaccines. Though several databases exist for the manual curation of PPI networks, text mining methods have been routinely demonstrated as useful alternatives for newly studied or understudied species, where databases are incomplete. Here, the relationship extraction performance of several open-source classical text processing, machine learning (ML)-based natural language processing (NLP), and large language model (LLM)-based NLP tools was compared. Overall, our results indicated that networks derived from classical methods tend to have high true positive rates at the expense of having overconnected networks, ML-based NLP methods have lower true positive rates but networks with the closest structures to the target network, and LLM-based NLP methods tend to exist between the two other approaches, with variable performances. The selection of a specific NLP approach should be tied to the needs of a study and text availability, as models varied in performance due to the amount of text provided.",
        "DOI": "10.1021/acs.jproteome.4c00535",
        "paper_author": "Degnan D.J.",
        "affiliation_name": "Pacific Northwest National Laboratory",
        "affiliation_city": "Richland",
        "affiliation_country": "United States",
        "affiliation_id": "60023471",
        "affiliation_state": "WA"
    },
    {
        "paper_title": "\"We have to learn to work with such systems\": Students' Perceptions of ChatGPT After a Short Educational Intervention on NLP",
        "publication": "SIGCSE Virtual 2024 - Proceedings of the 2024 ACM Virtual Global Computing Education Conference V. 1",
        "citied_by": "0",
        "cover_date": "2024-12-05",
        "Abstract": "Natural Language Processing (NLP) is a critical area of AI that is increasingly integrated into everyday life. The public regularly engages with systems such as Siri, Alexa, and more recently, ChatGPT, yet few understand how these systems work. In this paper, we examine how students perceive NLP technologies after completing a unit on NLP within an AI course designed for non-CS majors. We further present our students' perspectives on the banning of ChatGPT in Italy, where the course was delivered. The NLP unit featured a lecture, an interactive session, and a practical assignment wherein students developed a smart assistant responsive to textual commands. Students, after creating their smart assistants, highlighted challenges such as inadequate training datasets and natural language ambiguity. Opinions on ChatGPT's ban varied, with privacy concerns prevailing. However, a consensus emerged in favor of educational efforts to raise awareness about technology limitations, advocating understanding over outright bans in anticipation of their inevitable integration into daily life.",
        "DOI": "10.1145/3649165.3690113",
        "paper_author": "Kasinidou M.",
        "affiliation_name": "Open University of Cyprus",
        "affiliation_city": "Nicosia",
        "affiliation_country": "Cyprus",
        "affiliation_id": "60108559",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Applying AI to Reduce Software Testing Defects' Rejection",
        "publication": "SAE Technical Papers",
        "citied_by": "0",
        "cover_date": "2024-12-05",
        "Abstract": "The efficiency and accuracy of defect control are critical components in software testing, as they determine the final product's quality and cost. Rejection of defects for various reasons, like non-reproducibility, erroneous classification or inadequate information, is one of the largest issues that testers face. This paper presents an AI-driven approach that reduces the number of defect rejections by using the past defect data to give testers real-time advises and warnings. When a tester reports an issue, the model looks at the problem's description and title, making inferences and recommendations based on historical data to increase the fault's correctness. This feedback strategy reduces rejection rates and increases the overall efficiency of defect management by helping testers resolve potential issues before submitting a defect. The recommended solution involves training an AI model on a large dataset of previous defects, which includes details on DefectTitle, Description, ResolutionCategory, ReasonForRejection (optionally with RejectionCategory) and ResolutionDescription to be more accurate in its recommendation. The model uses natural language processing (NLP) technique to transform textual data into meaningful numerical representations. BERT (Bidirectional Encoder Representation for Transformers) language model is deployed for understanding the context and semantics of defect titles and descriptions. Next, a binary classification technique- Random Forest Algorithm is used to predict if the issue could be accepted or denied. Regular retraining and continuous monitoring ensure that the model remains accurate and flexible over time. This paper covers in depth the deployment process, model training, data preparation, and real-world implications of deploying this system in a real software testing environment. This strategy solves a common software testing problem in a scalable and efficient manner by utilizing artificial intelligence.",
        "DOI": "10.4271/2024-28-0188",
        "paper_author": "Trivedi S.",
        "affiliation_name": "Continental Automotive",
        "affiliation_city": "Bengaluru",
        "affiliation_country": "India",
        "affiliation_id": "122158884",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Possible Threats of IgA Vasculitis in Children: One Center Experience",
        "publication": "Acta Medica Lituanica",
        "citied_by": "0",
        "cover_date": "2024-12-04",
        "Abstract": "Introduction: Immunoglobulin A vasculitis (IgAV) is the most common vasculitis in children. Although typically self-limiting, IgAV may result in serious complications. Our objective was to evaluate the incidence, clinical features, laboratory predictors and outcomes of IgA vasculitis with gastrointestinal (GI) and kidney involvement. Methods: Medical records of patients <18 years of age with newly diagnosed IgAV between 2013 and 2021 in a single center were analyzed. Demographic, clinical, laboratory data, and incidence of GI and kidney involvement data were analyzed. As laboratory predictors, neutrophil, lymphocyte, platelets count, mean platelet volume (MPV) and neutrophil-to-lymphocyte ratio (NLR), platelet-to-lymphocyte ratio (PLR) were calculated. Results: 240 patients with IgAV were included. GI involvement was in 104 patients (43.3%), whereas kidney involvment in 21 patients (8.8%). Age was the only variable associated with increased odds of kidney involvement (OR 3.5, 95% confidence interval 1.39–8.56, p=0.009). None of the laboratory predictors or other tested variables was associated with kidney involvement in univariable logistic regression. The neutrophil and lymphocyte count, NLR and PLR levels were found to be significantly higher in children with GI involvement. There were no bad outcomes: lethal outcome or chronic kidney disease for the patients with GI and kidney involvement in recent study. During two years of surveillance after IgAV diagnosis, 11 cases (4.6%) had indications for kidney biopsy and were diagnosed with IgAV nephritis. Conclusions: Older children were more likely to have kidney disease. Easy obtained laboratory parameters such as NLP, PLR could help to predict GI involvement in early disease stage, but had no value for predicting kidney involvement.",
        "DOI": "10.15388/Amed.2024.31.2.4",
        "paper_author": "Lanzbergaitė-Manuilova E.",
        "affiliation_name": "Vilniaus Universitetas",
        "affiliation_city": "Vilnius",
        "affiliation_country": "Lithuania",
        "affiliation_id": "60059987",
        "affiliation_state": "Vilnius"
    },
    {
        "paper_title": "Exploring Quantization Techniques for Large-Scale Language Models: Methods, Challenges and Future Directions",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2024-12-03",
        "Abstract": "Breakthroughs in natural language processing (NLP) by large-scale language models (LLMs) have led to superior performance in multilingual tasks such as translation, summarization, and Q&A. However, the size and complexity of these models raise challenges in terms of computational requirements, memory usage, and energy consumption. Quantization strategies, as a type of model compression technique, have gained attention for their advantages in reducing model size and accelerating inference speed. In this paper, we review the rapid development of quantization techniques for LLMs, and systematically explore methods such as post-training quantization (PTQ), quantization-aware fine-tuning (QAF), and quantization-aware training (QAT), which provide a comprehensive solution to the resource-intensive problem of LLMs from training to post-deployment. We also analyze state-of-the-art benchmarks and datasets to evaluate the effectiveness of quantitative methods in terms of performance retention and computational efficiency. The purpose of this review is to provide researchers with a snapshot of the latest progress in LLM quantization techniques, helping them to quickly grasp the dynamics of the field and understand the key techniques and challenges, so that they can more efficiently devote themselves to this evolving research area.",
        "DOI": "10.1145/3689236.3695383",
        "paper_author": "Shen A.",
        "affiliation_name": "National University of Defense Technology China",
        "affiliation_city": "Changsha",
        "affiliation_country": "China",
        "affiliation_id": "60024350",
        "affiliation_state": "Hunan"
    },
    {
        "paper_title": "Future Prospects of Large Language Models: Enabling Natural Language Processing in Educational Robotics",
        "publication": "International Journal of Interactive Mobile Technologies",
        "citied_by": "0",
        "cover_date": "2024-12-03",
        "Abstract": "Large language models (LLMs) have recently shown considerable promise in educational robotics by offering generic knowledge necessary in situations when prior programming is not possible. In general, mobile education robots cannot perform tasks like navigation or localization unless they have a working knowledge of maps. In this letter, we tackle the issue of making LLMs more applicable in the field of mobile education robots by helping them to understand Space Graph, a text-based map description. This study, which focuses on LLMs, is divided into several sections. It explores basic natural language processing (NLP) techniques and highlights how they can help create smooth education discussions. Examining the development of LLMs inside NLP systems, the paper explores the benefits and implementation issues of important models utilized in the education sector. Applications useful in educational discussions are described in depth, ranging from patient-focused tools like diagnosis and treatment recommendations to systems that support education providers. We provide thorough instructions and real-world examples for quick engineering, making LLM-based educational robotics solutions more accessible to novices. We demonstrate how LLM-guided upgrades can be easily included in education robotics applications using tutorial-level examples and structured prompt creation. This survey provides a thorough review and helpful advice for leveraging language models in automation development, acting as a road map for researchers navigating the rapidly changing field of LLM-driven educational robotics.",
        "DOI": "10.3991/ijim.v18i23.51419",
        "paper_author": "Vinoth Kumar S.",
        "affiliation_name": "Vel Tech Rangarajan Dr.Sagunthala R&amp;D Institute of Science and Technology",
        "affiliation_city": "Chennai",
        "affiliation_country": "India",
        "affiliation_id": "60104476",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "From comments to conclusion: Machine learning insights into social media",
        "publication": "Multifaceted Uses of Cutting-Edge Technologies and Social Concerns",
        "citied_by": "0",
        "cover_date": "2024-12-02",
        "Abstract": "The way information is disseminated has changed significantly in the current digital world. Social media platforms-most notably YouTube-have become powerful spaces where user-generated content-especially comments-reflects the opinions and sentiments of the general public. Sentiment analysis is an invaluable tool for understanding the general public's sentiment towards social issues. This chapter delves into how digital platforms, especially YouTube, have revolutionized information dissemination and public engagement. It underscores the pivotal role of user-generated comments in mirroring societal sentiments, using sentiment analysis to examine discussions surrounding the Manipur Conflict and General Lok Sabha elections 2024 in India. By analyzing 14,748 comments from 37 videos, this study illuminates the ethnic tensions between the Kuki-Zo and Meitei communities. Employing NLP and ML techniques, the research offers comprehensive insights into public perceptions, categorised across left, central, and right political ideologies, thereby enriching our understanding of the conflict's reception on social media.",
        "DOI": "10.4018/979-8-3693-9591-2.ch015",
        "paper_author": "Rajeev P.A.",
        "affiliation_name": "Vellore Institute of Technology",
        "affiliation_city": "Vellore",
        "affiliation_country": "India",
        "affiliation_id": "60010618",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Reimagining Student Success Prediction: Applying LLMs in Educational AI with XAI",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2024-12-02",
        "Abstract": "Since the conception of Large Language Models (LLMs), their areas of application have increased significantly over time. This is due to their nature of being able to perform natural language processing (NLP) tasks (like question answering, text generation, text summarization, text classification etc.), which gives them flexibility in a multitude of spaces, including in Educational AI (EdAI). Despite their incredible wide range of use, LLMs are typically applied to generative AI, from text to image generation.This paper aims to apply LLMs for a classification task in EdAI, by reproposing the original PreSS (Predicting Student Success) model which makes use of more traditional Machine Learning (ML) algorithms for predicting CS1 students at risk of failing or dropping out. There are two main goals for this work: the first is to identify the best and most accurate method to re-purpose LLMs for a classification task; the second is to explore and access the explainability of the model outputs. For the former we investigate different techniques for using LLMs like Few-Shot Prompting, Fine-Tuning and Transfer Learning using Gemma 2B as base model along with two different kind of prompting techniques. For the latter we focus on attention scores of LLMs transformers, aiming to understanding what are the most important features that the model considers for generating the response. The obtained results are then compared with the previous PreSS model to evaluate whether LLMs can outperform traditional ML algorithms: this paper finds that Naïve Bayes still outperforms all the others, once again confirmed as the best algorithm for predicting student success.",
        "DOI": "10.1145/3701268.3701274",
        "paper_author": "Riello P.",
        "affiliation_name": "Università degli Studi di Napoli Federico II",
        "affiliation_city": "Naples",
        "affiliation_country": "Italy",
        "affiliation_id": "60017293",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Financial Sentiment Analysis Meets LLaMA 3: A Comprehensive Analysis",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2024-12-02",
        "Abstract": "Although large language models have been proven drastically powerful in many NLP tasks, they are not fully investigated in the area of finance. This paper, serving as the complementary to the existing research on large language models, comprehensively analyzes LLaMA 3, one of the most recent and renowned large language models, in the context of financial sentiment analysis. Our experimental results demonstrate that LLaMA 3 has effectively learned domain-specific knowledge in finance during its pre-training, as it greatly outperforms the random and majority baselines. Additionally, to gain insights of the pros and cons of large language models, we find that LLaMA 3 performs better on short sentences compared to long sentences, which indicates the future research directions on improving the performance of large language models.",
        "DOI": "10.1145/3696271.3696299",
        "paper_author": "Mai Z.",
        "affiliation_name": "I-66 Express Mobility Partners",
        "affiliation_city": "Manassas",
        "affiliation_country": "United States",
        "affiliation_id": "131067922",
        "affiliation_state": "VA"
    },
    {
        "paper_title": "AI-assisted programming and AI literacy in computer science education",
        "publication": "Effective Practices in AI Literacy Education: Case Studies and Reflections",
        "citied_by": "0",
        "cover_date": "2024-12-02",
        "Abstract": "The integration of artificial intelligence (AI) in computer science education is transforming teaching methodologies, particularly through AI-assisted programming. This chapter highlights AI's impact on programming education by providing personalised learning, immediate feedback, and using technologies like NLP, ML, and LLMs. It discusses the shift from traditional to AIenhanced approaches, including competitive programming where AI automates tasks such as template generation, unit testing, and edge case analysis. The chapter also explores AI's role in promoting self-regulated learning and enhancing classroom engagement with generative AI and virtual tutors. While noting benefits like increased accessibility and personalised instruction, it addresses ethical considerations and technical limitations. The chapter underscores the need for continuous innovation and collaboration in AI-assisted programming to equip students with modern technological skills.",
        "DOI": "10.1108/978-1-83608-852-320241020",
        "paper_author": "Tan C.W.",
        "affiliation_name": "Nanyang Technological University",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore",
        "affiliation_id": "60005510",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Overview of deep sentence-level entity relation extraction",
        "publication": "Xi'an Dianzi Keji Daxue Xuebao/Journal of Xidian University",
        "citied_by": "0",
        "cover_date": "2024-12-01",
        "Abstract": "Entity relation extraction at statement level (RE) refers to the extraction of semantic relationship between an entity pair from a given statement. It is an important basis for the construction of knowledge graph, natural language processing(NLP), intelligent question answering, Web search and other applications in artificial intelligence (AI), and it is the most cutting-edge basic hot research issue in AI. With the successful application of deep neural networks (DNNs), a variety of RE algorithms based on DNNs have emerged. In recent years, with the requirement of continuous processing and understanding of text information, some deep continuous of entity relation extraction (CRE) algorithms by combining entity relationship extraction and continual learning (CL) have emerged. This kind of algorithms can enable the model to carry out sequential RE of multiple tasks sustainably and efficiently without forgetting the learned knowledge of old tasks. In this paper, various representative deep RE and CRE methods in recent years are surveyed from their deep network model, algorithm framework and performance characteristics, and the research development trends of the RE and CRE are pointed out. We sincerely hope that the extensive survey will inspire more good ideas on the research of the RE and CRE.",
        "DOI": "10.19665/j.issn1001-2400.20240311",
        "paper_author": "Zhao C.",
        "affiliation_name": "Xidian University",
        "affiliation_city": "Xi'an",
        "affiliation_country": "China",
        "affiliation_id": "60025578",
        "affiliation_state": "Shaanxi"
    },
    {
        "paper_title": "Tasks in Named Entity Recognition: Technologies and Tools",
        "publication": "Automatic Control and Computer Sciences",
        "citied_by": "0",
        "cover_date": "2024-12-01",
        "Abstract": "Abstract—: The task of named entity recognition (NER) is to identify and classify words and phrases denoting named entities (NEs), such as people, organizations, geographical names, dates, events, and terms from subject areas. While searching for the best solution, researchers conduct a wide range of experiments with different technologies and input data. A comparison of the results of these experiments shows a significant discrepancy in the quality of NER and poses the problem of determining the conditions and limitations for the application of the used technologies, as well as finding new solutions. An important part in answering these questions is the systematization and analysis of current research and the publication of relevant reviews. In the field of NE recognition, the authors of analytical articles primarily consider mathematical methods of identification and classification and do not pay attention to the specifics of the problem itself. In this survey, the field of NE recognition is considered from the point of view of individual task categories. The authors identify five categories: the classical task of NER, NER subtasks, NER in social media, NER in domain, and NER in natural language processing (NLP) tasks. For each category the authors discuss the quality of the solution, features of the methods, problems, and limitations. Information about current scientific works of each category is given in the form of a table for clarity. This review allows us to draw a number of conclusions. Deep learning methods are the leading methods among state-of-the-art technologies. The main problems are the lack of datasets in open access, strict requirements for computing resources, and the lack of error analysis. A promising area of research in NER is the development of methods based on unsupervised techniques or rule-based learning. Intensively developing language models in existing NLP tools can serve as a possible basis for text preprocessing for NER methods. The article ends with a description and results of experiments with NER tools for Russian-language texts.",
        "DOI": "10.3103/S0146411624700251",
        "paper_author": "Lagutina N.S.",
        "affiliation_name": "Yaroslavl State University",
        "affiliation_city": "Yaroslavl",
        "affiliation_country": "Russian Federation",
        "affiliation_id": "60001420",
        "affiliation_state": "Yaroslavl Oblast"
    },
    {
        "paper_title": "A comprehensive review on detection of hate speech for multi-lingual data",
        "publication": "Social Network Analysis and Mining",
        "citied_by": "0",
        "cover_date": "2024-12-01",
        "Abstract": "The paper addresses the critical challenge of detecting and mitigating hate speech in Hindi across social media platforms, utilizing natural language processing (NLP) techniques. It underscores the complexities associated with the Hindi language, including the prevalence of code-switching, the existence of various dialects, and the inconsistent use of Romanized Hindi. These linguistic factors present significant challenges for developing automated systems capable of reliably identifying hate speech. The paper emphasizes the urgent need to confront hate speech in Hindi due to its potential to incite social unrest, propagate misinformation, and foster a culture of intolerance within India's diverse socio-political landscape. The research aims to address existing gaps in the detection of Hindi hate speech through employing advanced machine learning and deep learning methodologies. The overarching objective is to devise solutions that are not only technologically robust but also deeply attuned to the unique linguistic and cultural nuances of the Hindi language.",
        "DOI": "10.1007/s13278-024-01401-y",
        "paper_author": "Narula R.",
        "affiliation_name": "The NorthCap University",
        "affiliation_city": "Gurugram",
        "affiliation_country": "India",
        "affiliation_id": "60014350",
        "affiliation_state": "HR"
    },
    {
        "paper_title": "A Model for Enhancing Pattern Recognition in Clinical Narrative Datasets through Text-Based Feature Selection and SHAP Technique",
        "publication": "International Journal on Informatics Visualization",
        "citied_by": "0",
        "cover_date": "2024-12-01",
        "Abstract": "Clinical narratives contain crucial patient information for predicting cardiac failure. Accurate and timely cardiac failure recognition (CFR) significantly impacts patient outcomes but faces challenges like limited dataset sizes, feature space sparsity, and underutilization of vital sign data. This study addresses these issues by developing a methodology to improve CFR accuracy and interpretability within clinical narratives. Four datasets—the Framingham Heart Study, Heart Disease from Kaggle, Cleveland Heart Disease, and Heart Failure Clinical Records—undergo preprocessing, including handling missing values, removing duplicates, scaling, encoding categorical variables, and transforming unstructured data using natural language processing (NLP). Various feature selection methods (Chi-Squared, Forward Selection, L1 Regularization) are used to identify influential features for CFR, and the SHapley Additive exPlanations (SHAP) technique is integrated to improve interpretability. Support Vector Machine (SVM), Logistic Regression (LR), and Random Forest (RF) models are trained and evaluated. Performance was evaluated using accuracy, precision, recall, f1-score, and area under the receiver operating characteristic curve (AUC-ROC). Results indicate that L1 Regularization with LR and Chi-Squared with RF perform best for specific datasets. The final model, combining all datasets with Forward Selection and RF, achieves high accuracy (91%), precision (87%), recall (97%), f1-score (91%), and AUC-ROC (94%). This study concludes that advanced text-based feature selection and SHAP interpretability significantly enhance CFR model accuracy and transparency, aiding clinical decision-making. Future research should incorporate more diverse datasets, explore advanced NLP techniques, and validate models in various clinical settings to enhance robustness and applicability.",
        "DOI": "10.62527/joiv.8.4.3664",
        "paper_author": "Dalhatu S.M.",
        "affiliation_name": "Universiti Putra Malaysia",
        "affiliation_city": "Serdang",
        "affiliation_country": "Malaysia",
        "affiliation_id": "60025577",
        "affiliation_state": "Selangor"
    },
    {
        "paper_title": "Punctuation Restoration Method Based on MEGA Network and Hierarchical Prediction",
        "publication": "Jisuanji Gongcheng/Computer Engineering",
        "citied_by": "0",
        "cover_date": "2024-12-01",
        "Abstract": "Punctuation restoration, also known as punctuation prediction, refers to the task of adding appropriate punctuation marks to a text without punctuation to enhance its readability. This is a classic Natural Language Processing (NLP) task. In recent years, with the development of pretraining models and deepening research on punctuation restoration, the performance of punctuation restoration tasks has continuously improved. However, Transformer-based pretraining models have limitations in extracting local information from long-sequence inputs, which hinders the prediction of the final punctuation marks. In addition, previous studies have treated punctuation labels as symbols to be predicted by overlooking the contextual attributes of different punctuation marks and their relationships. To address these issues, this study introduces a Moving average Equipped Gated Attention (MEGA) network as an auxiliary module to enhance the ability of the model to extract local information. Moreover, a hierarchical prediction module is constructed to fully utilize the contextual attributes of different punctuation marks and the relationships between them for the final classification. Experiments are conducted using various transformer-based pretraining models on datasets in different languages. The experimental results on the English punctuation dataset IWSLT demonstrate that applying the MEGA and hierarchical prediction modules to most pretraining models leads to performance gains. Notably, DeBERTaV3 xlarge achieved an F1 score of 85.5% on the REF test set of the IWSLT, which is a 1.2 percentage points improvement compared to the baseline. The proposed model achieved the highest accuracy for the Chinese punctuation dataset.",
        "DOI": "10.19678/j.issn.1000-3428.0068599",
        "paper_author": "Zhang W.",
        "affiliation_name": "Xinjiang University",
        "affiliation_city": "Urumqi",
        "affiliation_country": "China",
        "affiliation_id": "60015780",
        "affiliation_state": "Xinjiang"
    },
    {
        "paper_title": "Detecting privacy compliance of mobile applications from the perspective of the \"minimum necessary\" principle",
        "publication": "Chinese Journal of Network and Information Security",
        "citied_by": "0",
        "cover_date": "2024-12-01",
        "Abstract": "To comply with legal requirements for personal data privacy protection, mobile App developers typically disclose their data collection practices to users through privacy policies. Researchers have proposed various methods using natural language processing (NLP) techniques to analyze privacy policy texts and perform compliance checks. However, most existing studies focus on principles like transparency, openness, and legality, leaving a gap in the evaluation of the‘minimum necessary’principle. For this purpose, a framework called MNPD (minimum necessary principle detection) was proposed for automated compliance checking of applications from the perspective of the‘minimum necessary’principle. Initially, a multi-label text classification model categorized the target App based on its service type to determine the range of‘minimum necessary information’for different App categories. Then, prompt words were constructed to guide the large language model in extracting data collection practices of the App under its basic business functionality mode, transforming them into privacy statement triples and standardizing them. Finally, the compliance checking model conducted consistency checks on the text representation of the target App and evaluated its adherence to the‘minimum necessary’principle. The experimental results show that the proposed method achieves 86.20% F1 score in the automated analysis of 101‘Online Audio-Visual’Apps obtained from Huawei’s application market.",
        "DOI": "10.11959/j.issn.2096-109x.2024084",
        "paper_author": "Yu P.",
        "affiliation_name": "Nanjing University of Post and TeleCommunications",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60009400",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "A survey of datasets in medicine for large language models",
        "publication": "Intelligence and Robotics",
        "citied_by": "0",
        "cover_date": "2024-12-01",
        "Abstract": "With the advent of models such as ChatGPT and other models, large language models (LLMs) have demonstrated unprecedented capabilities in understanding and generating natural language, presenting novel opportunities and challenges within the medicine domain. While there have been many studies focusing on the employment of LLMs in medicine, comprehensive reviews of the datasets utilized in this field remain scarce. This survey seeks to address this gap by providing a comprehensive overview of the datasets in medicine fueling LLMs, highlighting their unique characteristics and the critical roles they play at different stages of LLMs’ development: pre-training, fine-tuning, and evaluation. Ultimately, this survey aims to underline the significance of datasets in realizing the full potential of LLMs to innovate and improve healthcare outcomes.",
        "DOI": "10.20517/ir.2024.27",
        "paper_author": "Zhang D.",
        "affiliation_name": "Southeast University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60005244",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "Implicit offensive language taxonomy",
        "publication": "Lodz Papers in Pragmatics",
        "citied_by": "0",
        "cover_date": "2024-12-01",
        "Abstract": "The aim of this paper is to present a proposal of implicitness typology. The theoretical model we propose is compliant with neo-Gricean pragmatics and is explicitly designed to cover instances of offensive language on social media. The implicitness framework we propound has been empirically verified by means of a corpus-assisted analysis and computational method of word embeddings (Word2Vec and FastText), which, in principle, have supported the schema explicated here. This taxonomy is potentially applicable to the ontology of offensiveness and, thus, to NLP-based research; in particular, it can be useful for automatic detection of implicit offensive language on social media.",
        "DOI": "10.1515/lpp-2024-0049",
        "paper_author": "Bą czkowska A.",
        "affiliation_name": "Uniwersytet Gdański",
        "affiliation_city": "Gdansk",
        "affiliation_country": "Poland",
        "affiliation_id": "60026000",
        "affiliation_state": "Pomeranian"
    },
    {
        "paper_title": "DermAI: An Innovative AI-Driven Chatbot for Enhanced Dermatological Diagnosis and Patient Interaction",
        "publication": "Indonesian Journal of Electrical Engineering and Informatics",
        "citied_by": "0",
        "cover_date": "2024-12-01",
        "Abstract": "Skin disorders constitute a noteworthy public health concern globally, with earnest impacts on both physical and mental well-being. However, effective dermatological care faces challenges in resource-limited regions due to poor in-frastructure, limited access to medical facilities & expertise, and inadequate advanced diagnostic tools. The existing research work majorly focuses on cancer and uncommon skin diseases with models trying to achieve a higher training accuracy with no regards to misclassification rate. The products currently available in the market provide a limited initial diagnosis and suggest consulting a doctor to get an accurate diagnosis or offer a list of other possible skin disorders. To address these challenges, we propose DermAI, an innovative AI-based Chat-bot made entirely of open-source technologies, which integrates the ResNet-50 model and LLM via Chainlit, with Retrieval Augmented Generation(RAG), utilising AstraDB vector database and OpenAI embedding model for person-alised responses. enabling accurate classification of common skin diseases. The proposed DermAI ensures minimal misclassification and comprehensive cover-age of diseases, leveraging Retrieval-Augmented Generation and comparative model analysis. The metrics indicate that the model has a high true positive rate, with a misclassification rate of 2.17%, mean sensitivity, specificity & AUC of 92.6%, 99.8% & 99.9% respectively. This is demonstrated in the situations of melanoma, chickenpox, shingles, impetigo, and nail fungus, where it obtained 100% validation accuracy, a feat not attained by previous studies. Additionally, the model is highly capable of correctly identifying negative cases. The hallucination metric suggests the model may have a minimal tendency to hallucinate as the average hallucination score of 7% which falls far within the manually set threshold value of 50%. By setting the threshold value to 50%, the model gener-ates grounded answers that are pertaining to the knowledge base and also allows it to be flexible with its responses. Overall, DermAI outperforms all solutions proposed in research literature.",
        "DOI": "10.52549/ijeei.v12i4.5806",
        "paper_author": "Rajeshkumar P.",
        "affiliation_name": "SIES Graduate School of Technology",
        "affiliation_city": "Navi Mumbai",
        "affiliation_country": "India",
        "affiliation_id": "60110733",
        "affiliation_state": "MH"
    },
    {
        "paper_title": "A Comparative Evaluation of Transformers in Seq2Seq Code Mutation: Non-Pre-trained Vs. Pre-trained Variants",
        "publication": "Journal of Advanced Research Design",
        "citied_by": "0",
        "cover_date": "2024-12-01",
        "Abstract": "Mutation testing (MT) is a gold standard way to assess the efficacy of software test suites. However, the accuracy of mutation score is affected by the presence of trivial mutants which can be “killed” by even the simplest and most basic test suites. Since the existence of trivial mutants is due to the fixed set of mutation operators that constraints the complexity of code mutations, state-of-the-art recurrent neural network (RNN) model is used for sequence-to-sequence (seq2seq) code mutation without relying on mutation operators. However, the quality of the produced mutants is affected by the limitation of RNN in interpreting the relationships between far-apart tokens of the code to be mutated. Transformers that do not have this limitation, have superseded RNN in seq2seq machine translation domains such as natural language processing (NLP). However, to the best of our knowledge, there is still no research that investigates the performance of transformers in seq2seq code mutation. This paper presents a comparison study that involves different variants of the non-pre-trained transformers, the transformers pre-trained with source code, the transformers pre-trained with natural language, and the state-of-the-art RNN model in seq2seq code mutation. The results show that transformers pre-trained with source code, especially CodeT5, demonstrated the best performance, achieving an average character n-gram F-score (CHRF) of 82.89 and superior code mutation complexity. Since the performance of transformers in seq2seq code mutation has not been previously investigated, the primary contribution of this paper is the best performing transformer for seq2seq code mutation. It establishes the foundation for the future research that proposes an integrated solution which addresses both the high-cost problem and the inaccurate mutation score problem of MT simultaneously, unlike existing solutions which only tackle one of the MT problems and give rise to other MT problems.",
        "DOI": "10.37934/ard.123.1.4565",
        "paper_author": "Yik L.Z.",
        "affiliation_name": "Universiti Teknologi Malaysia",
        "affiliation_city": "Johor Bahru",
        "affiliation_country": "Malaysia",
        "affiliation_id": "60021005",
        "affiliation_state": "Johor"
    },
    {
        "paper_title": "Optimization of coupling coordination state between tunnel engineering and regional environment in mountain railway",
        "publication": "Beijing Jiaotong Daxue Xuebao/Journal of Beijing Jiaotong University",
        "citied_by": "0",
        "cover_date": "2024-12-01",
        "Abstract": "To achieve the optimal coordinated evolution and development of the“tunnel-environment” composite system in mountain railways, a coupling element system is constructed from the perspective of micro-elements. The coordination level is analyzed using the coupling coordination degree model. Next, a double-layer complex network model is used to identify the main control elements for enhancing the greening level of tunnel engineering. Then, a Non-Linear Programming (NLP) model is established to quantify the coupling coordination connection of“tunnel-environment”, and the Simulated Annealing (SA) algorithm is further applied to regulate and optimize its evolution process, using the main control elements as the control variables. Finally, an analysis is conducted using a specific mountain railway tunnel project as a case study. The research results show that the initial coupling coordination degree of the“tunnel-environment”composite system is 0.675 8, indicating a primary coordination state. And the main control elements for coupling regulation are the tunnel section size, tunnel slag utilization rate, vegetation restoration rate at the tunnel entrance, permeability coefficient of the lining, permeability coefficient of the grouting circle, and fan power consumption. When the optimization ratios of these elements are 12.72%, 38.40%, 44.06%, 71.25%, 30.00%, and 23.56%, respectively, the optimal evolution value of coupling coordination is 0.799 0, achieving a well-coordinated state. The proposed model effectively explores the coordinated evolution path of the “tunnel-environment”system and provides new insights for optimizing green tunnel design and promoting the green and coordinated development of“tunnel-environment”in mountain railways.",
        "DOI": "10.11860/j.issn.1673-0291.20230163",
        "paper_author": "Yan L.",
        "affiliation_name": "Lanzhou Jiaotong University",
        "affiliation_city": "Lanzhou",
        "affiliation_country": "China",
        "affiliation_id": "60009806",
        "affiliation_state": "Gansu"
    },
    {
        "paper_title": "A nonlinear mathematical model integrated with the CROPWAT decision support tool to optimize cropping patterns under different climate conditions",
        "publication": "Journal of Water and Climate Change",
        "citied_by": "0",
        "cover_date": "2024-12-01",
        "Abstract": "A nonlinear programming model was developed for optimizing cropping patterns with two objectives: maximizing farming revenue and mini-mizing irrigation water consumption. After determination of dominant crops in the study area, the water requirement of each crop was calculated by CROPWAT. Next, the volume of water used by farmers to irrigate the crops was computed. Based on the costs of agricultural inputs and farming operations and the price of products, the profit earned by farmers was calculated. The calculations were carried out for current and future climate conditions. For the future periods, input data to CROPWAT was obtained by simulating the climate parameters in the general circulation model (Hadley Centre Coupled Model version 3) under three emission scenarios, namely, A2, B1, and A1B. Then, the Long Ashton Research Station Weather Generator statistical model was used for the downscaling of data from the general circulation model. Finally, an NLP optimization model was developed in LINGO-20 to optimize the cropping pattern. The results indicated that by optimizing the cropping pattern, the farming revenue increased up to 65% on average compared with the existing (nonoptimal) cropping pattern, while at the same time, the agricultural water consumption was reduced by 5%.",
        "DOI": "10.2166/wcc.2024.418",
        "paper_author": "Biglari N.",
        "affiliation_name": "Shahid Bahonar University of Kerman",
        "affiliation_city": "Kerman",
        "affiliation_country": "Iran",
        "affiliation_id": "60031268",
        "affiliation_state": "Kerman"
    },
    {
        "paper_title": "INTEGRATING NATURAL LANGUAGE PROCESSING TECHNIQUES OF TEXT MINING INTO FINANCIAL SYSTEM: APPLICATIONS AND LIMITATIONS",
        "publication": "International Journal on Technical and Physical Problems of Engineering",
        "citied_by": "0",
        "cover_date": "2024-12-01",
        "Abstract": "The financial sector, a pivotal force in economic development, increasingly uses the intelligent technologies such as natural language processing to enhance data processing and insight extraction. This research paper through a review process of the time span of 2018-2023 explores the use of text mining as natural language processing techniques in various components of the financial system including asset pricing, corporate finance, derivatives, risk management, and public finance and highlights the need to address the specific problems in the discussion section. We notice that most of the research materials combined probabilistic with vector-space models, and text-data with numerical ones. The most used technique regarding information processing is the information classification technique and the most used algorithms include the long-short term memory and bidirectional encoder models. The research noticed that new specific algorithms are developed and the focus of the financial system is mainly on asset pricing component. The research also proposes a path from engineering perspective for researchers who need to analyze financial text. The challenges regarding text mining perspective such as data quality, context-adaption and model interpretability need to be solved so to integrate advanced natural language processing models and techniques in enhancing financial analysis and prediction.",
        "DOI": "NA",
        "paper_author": "Millo D.",
        "affiliation_name": "University of Tirana",
        "affiliation_city": "Tirana",
        "affiliation_country": "Albania",
        "affiliation_id": "60072128",
        "affiliation_state": "Tirana"
    },
    {
        "paper_title": "CineInsight: NLP-Driven Movie Recommendation Enhancement for Over-The-Top Platforms",
        "publication": "SSRG International Journal of Electronics and Communication Engineering",
        "citied_by": "0",
        "cover_date": "2024-12-01",
        "Abstract": "Computers, through language, understand, interpret, and interact with humans, enabling Natural Language Processing (NLP), a sector of Artificial Intelligence (AI). Selecting a movie to watch may be very difficult because so many options are accessible on different streaming services; people have different likes and preferences, and others are unaware of fantastic films. Objective: In today's digital landscape, creating and executing a movie recommendation system is crucial to addressing the issues of information overload, improving user satisfaction, and maintaining competitiveness in the entertainment sector. Method: This study proposed the CineInsight NaiveFlix Algorithm for a movie recommendation, which leverages a movie review dataset gathered from the websites of YIFY and IMDB. After that, the preprocessed data was pipelined, and the essential stop words in the English language were extracted to improve the Naïve Bayes (NB) model. Subsequently, the audience reviews of the film were categorized as either positive or negative. Results: After comparing the suggested method's performance to the conventional NB model and the linear support vector classification algorithm, it was discovered that the suggested CineInsight NaiveFlix method performs better in categorizing audience movie reviews.",
        "DOI": "10.14445/23488549/IJECE-V11I12P110",
        "paper_author": "Nandhini K.",
        "affiliation_name": "Vels Institute of Science, Technology &amp; Advanced Studies",
        "affiliation_city": "Chennai",
        "affiliation_country": "India",
        "affiliation_id": "60105237",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Modeling of Snow Ablation Optimization Algorithm with Deep Learning Approach for Sentiment Classification on Social Media Corpus",
        "publication": "SSRG International Journal of Electronics and Communication Engineering",
        "citied_by": "0",
        "cover_date": "2024-12-01",
        "Abstract": "Recently, Sentiment Analysis (SA) has been a tedious process in natural language processing (NLP), particularly for Social Media (SM) text, which tends to be brief, noisy, and informal. SA is a way of extracting data about an entity and automatically detecting the subjectivity of that entity. The SA aims to determine whether text created by the user conveys optimistic, adverse, or impartial feelings. The goal is to automatically classify sentiments towards specific features like products, topics, or movies, utilizing Deep Learning (DL) as an advanced technique to meet the growing demand for accurate SA. Therefore, this study proposes a new Snow Ablation Optimization with a Deep Learning for Sentiment Detection and Classification (SAODL-SDC) approach on the SM corpus. The presented SAODL-SDC approach primarily intends to recognize the class of opinions in SM data. In the SAODL-SDC technique, a multi-faceted approach begins with data preprocessing and bag of words (BoWs) feature extraction. The SAODL-SDC technique employs a Convolutional Long Short-Term Memory Autoencoder (CLSTM-AE) technique for sentiment detection. The hyperparameter tuning process using SAO is utilized to improve the effectualness of the CLSTM-AE technique. The SAODL-SDC technique is examined under Sentiment140 and the Airline's datasets. The performance validation of the SAODL-SDC approach portrayed superior accuracy values of 94.28% and 97.00% over existing techniques.",
        "DOI": "10.14445/23488549/IJECE-V11I12P105",
        "paper_author": "Manikandan K.",
        "affiliation_name": "Annamalai University",
        "affiliation_city": "Chidambaram",
        "affiliation_country": "India",
        "affiliation_id": "60027171",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Environmental Policy Implications of Carbon Tax Implementation Using Natural Language Processing",
        "publication": "International Journal of Environmental Impacts",
        "citied_by": "0",
        "cover_date": "2024-12-01",
        "Abstract": "One of the adopted policies to contribute to this effort is the carbon tax policy, which is being implemented in several countries. However, its effectiveness remains heavily affected by public perceptions and reactions. Therefore, this paper explores the environmental policy implications of carbon tax implementation in Indonesia using a Natural Language Processing (NLP) approach. As seen, the data were directly surveyed from 377 respondents and analyzed using the BERT model. After analysis, most respondents feel positive about the carbon tax, stating that with a policy like that, levels of pollution will be reduced in a green economy. Word clouds of text data bring to the fore important keywords on carbon tax — ‘emission’, ‘climate change’, and ‘green economy’-pointing to the actual gist on which the public discourse is centered. The correlation analysis also shows a strong relationship between perceptions of the carbon tax with views on economic and environmental impacts. The implications are useful for policymakers to come up with a communication strategy optimization and an implementation of the carbon tax in Indonesia, considering public concerns and expectations.",
        "DOI": "10.18280/ijei.070411",
        "paper_author": "Azhar R.",
        "affiliation_name": "Universitas Lampung",
        "affiliation_city": "Bandar Lampung",
        "affiliation_country": "Indonesia",
        "affiliation_id": "60069401",
        "affiliation_state": "Lampung"
    },
    {
        "paper_title": "Effect of Anti-nutritional Factors as Feed Additives on Enteric Methane, Blood Serum Indices and Worm Load in Crossbred Calves",
        "publication": "Asian Journal of Dairy and Food Research",
        "citied_by": "0",
        "cover_date": "2024-12-01",
        "Abstract": "Background: Ruminants have the distinct advantage of being able to eat forages and graze on land that is not suitable for growing crops. During breakdown of feed in rumen, 2 to 12 per cent of the gross energy consumed is converted to enteric CH4, which accounts for about 6 per cent of the world’s anthropogenic greenhouse gas emissions. Anti nutritional factor (ANF) plays a vital role in rumen manipulation. It helps in mitigation of methane production and improves the feed efficiency. Methods: Twenty calves of 6-7 months old with an average body weight of 90.2±4.0 kg were distributed randomly into four dietary treatment groups. The calves of the control group (T0) were fed on a basal diet as per ICAR (2013) feeding standard. The basal diet of T1, T2 and T3 were supplemented with NLP, MOP, CO@ 2 per cent on DM basis, respectively. Blood serum and fecal samples were collected and analyzed on 30th, 90th and 120th day of the experiment. The methane emission per animal was measured over fortnightly. Result: It was depicted that supplementation of ANFs as feed additives to the crossbred calves didn’t not affect the blood serum indices but the worm load and methane emission in calves differ significantly (P<0.05).",
        "DOI": "10.18805/ajdfr.DR-2055",
        "paper_author": "Bhosale T.R.",
        "affiliation_name": "College of Agriculture",
        "affiliation_city": "Jalgaon",
        "affiliation_country": "India",
        "affiliation_id": "132102518",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Effective Communication of System-Level Events for Hospital System Health and Nurse Well-Being: A Qualitative Study",
        "publication": "Safety",
        "citied_by": "0",
        "cover_date": "2024-12-01",
        "Abstract": "Many injuries and deaths occur yearly in the United States due to preventable errors; however, documented harm is lower in hospitals with Magnet® designation from the American Nurses Credentialing Center (Silver Spring, MD, USA). This manuscript illustrates insights from Magnet® and non-Magnet® nurses and nursing leaders regarding what impacts hospital system health, how those factors are communicated, and how hospital system health impacts patient safety. A qualitative descriptive approach was used with semi-structured interview questions. Within-methods data triangulation was applied to transcribed interviews using thematic analysis, application of the Goodwin statistic, and natural language processing (NLP). Thematic analysis revealed the four main themes of Healthy Systems, Unhealthy Systems, Patient Safety, and Nurse Well-Being. NLP revealed non-Magnet® leaders and nurses scored highest for anxiety, anger, and sadness, in contrast to Magnet® participants who had the highest levels of positive emotion. Key findings from the Magnet® participants revealed that poaching employees and placing people in the wrong positions harm hospital system health and patient safety. The use of within-methods data triangulation illuminated the contextual depth of the participants’ speech, revealing the alarming emotional state that non-Magnet® nurses and leaders are facing. Healthcare staff’s wellness must be prioritized to cultivate environments that provide safe care.",
        "DOI": "10.3390/safety10040096",
        "paper_author": "Brittain A.C.",
        "affiliation_name": "Washington State University College of Nursing",
        "affiliation_city": "Spokane",
        "affiliation_country": "United States",
        "affiliation_id": "60017579",
        "affiliation_state": "WA"
    },
    {
        "paper_title": "Opportunities and Challenges of Chatbots in Ophthalmology: A Narrative Review",
        "publication": "Journal of Personalized Medicine",
        "citied_by": "1",
        "cover_date": "2024-12-01",
        "Abstract": "Artificial intelligence (AI) is becoming increasingly influential in ophthalmology, particularly through advancements in machine learning, deep learning, robotics, neural networks, and natural language processing (NLP). Among these, NLP-based chatbots are the most readily accessible and are driven by AI-based large language models (LLMs). These chatbots have facilitated new research avenues and have gained traction in both clinical and surgical applications in ophthalmology. They are also increasingly being utilized in studies on ophthalmology-related exams, particularly those containing multiple-choice questions (MCQs). This narrative review evaluates both the opportunities and the challenges of integrating chatbots into ophthalmology research, with separate assessments of studies involving open- and close-ended questions. While chatbots have demonstrated sufficient accuracy in handling MCQ-based studies, supporting their use in education, additional exam security measures are necessary. The research on open-ended question responses suggests that AI-based LLM chatbots could be applied across nearly all areas of ophthalmology. They have shown promise for addressing patient inquiries, offering medical advice, patient education, supporting triage, facilitating diagnosis and differential diagnosis, and aiding in surgical planning. However, the ethical implications, confidentiality concerns, physician liability, and issues surrounding patient privacy remain pressing challenges. Although AI has demonstrated significant promise in clinical patient care, it is currently most effective as a supportive tool rather than as a replacement for human physicians.",
        "DOI": "10.3390/jpm14121165",
        "paper_author": "Sabaner M.C.",
        "affiliation_name": "Kastamonu University",
        "affiliation_city": "Kastamonu",
        "affiliation_country": "Turkey",
        "affiliation_id": "60105220",
        "affiliation_state": "Kastamonu"
    },
    {
        "paper_title": "Advancing Author Gender Identification in Modern Standard Arabic with Innovative Deep Learning and Textual Feature Techniques",
        "publication": "Information (Switzerland)",
        "citied_by": "0",
        "cover_date": "2024-12-01",
        "Abstract": "Author Gender Identification (AGI) is an extensively studied subject owing to its significance in several domains, such as security and marketing. Recognizing an author’s gender may assist marketers in segmenting consumers more effectively and crafting tailored content that aligns with a gender’s preferences. Also, in cybersecurity, identifying an author’s gender might aid in detecting phishing attempts where hackers could imitate individuals of a specific gender. Although studies in Arabic have mostly concentrated on written dialects, such as tweets, there is a paucity of studies addressing Modern Standard Arabic (MSA) in journalistic genres. To address the AGI issue, this work combines the beneficial properties of natural language processing with cutting-edge deep learning methods. Firstly, we propose a large 8k MSA article dataset composed of various columns sourced from news platforms, labeled with each author’s gender. Moreover, we extract and analyze textual features that may be beneficial in identifying gender-related cues through their writings, focusing on semantics and syntax linguistics. Furthermore, we probe several innovative deep learning models, namely, Convolutional Neural Networks (CNNs), LSTM, Bidirectional LSTM (BiLSTM), and Bidirectional Encoder Representations from Transformers (BERT). Beyond that, a novel enhanced BERT model is proposed by incorporating gender-specific textual features. Through various experiments, the results underscore the potential of both BERT and the textual features, resulting in a 91% accuracy for the enhanced BERT model and a range of accuracy from 80% to 90% accuracy for deep learning models. We also employ these features for AGI in informal, dialectal text, with the enhanced BERT model reaching 68.7% accuracy. This demonstrates that these gender-specific textual features are conducive to AGI across MSA and dialectal texts.",
        "DOI": "10.3390/info15120779",
        "paper_author": "Himdi H.",
        "affiliation_name": "University of Jeddah",
        "affiliation_city": "Jeddah",
        "affiliation_country": "Saudi Arabia",
        "affiliation_id": "60108699",
        "affiliation_state": "Makkah Province"
    },
    {
        "paper_title": "Constructing Cybersecurity Stocks Portfolio Using AI",
        "publication": "Forecasting",
        "citied_by": "1",
        "cover_date": "2024-12-01",
        "Abstract": "This study explores the application of artificial intelligence, specifically ChatGPT-4o, in constructing and managing a portfolio of cybersecurity stocks over the period from Q1 2018 to Q1 2024. Leveraging advanced machine learning models, fundamental analysis, sentiment analysis, and optimization techniques, the AI-driven portfolio significantly outperformed leading cybersecurity ETFs, as well as broader market indices such as the Nasdaq 100 (QQQ) and S&P 500 (SPY). The methodology employed included data collection, stock filtering, predictive modeling using Random Forests and Support Vector Machines (SVMs), sentiment analysis through natural language processing (NLP), and portfolio optimization using Mean-Variance Optimization (MVO), with quarterly rebalancing to ensure responsiveness to evolving market conditions. The AI-selected portfolio achieved a total return of 273%, with strong risk-adjusted performance as demonstrated by key metrics such as the Sharpe ratio, highlighting the effectiveness of an AI-based approach in navigating market complexities and generating superior returns. The results of this study indicate that AI-driven portfolio management can uncover investment opportunities that traditional methods may overlook, offering a competitive edge in the cybersecurity sector and promising enhanced predictive accuracy, efficiency, and overall investment success as AI technologies continue to evolve.",
        "DOI": "10.3390/forecast6040053",
        "paper_author": "Aiche A.",
        "affiliation_name": "Western Galilee College",
        "affiliation_city": "Acre",
        "affiliation_country": "Israel",
        "affiliation_id": "60080487",
        "affiliation_state": "HaZafon"
    },
    {
        "paper_title": "Trustworthy AI: Securing Sensitive Data in Large Language Models",
        "publication": "AI (Switzerland)",
        "citied_by": "1",
        "cover_date": "2024-12-01",
        "Abstract": "Large language models (LLMs) have transformed Natural Language Processing (NLP) by enabling robust text generation and understanding. However, their deployment in sensitive domains like healthcare, finance, and legal services raises critical concerns about privacy and data security. This paper proposes a comprehensive framework for embedding trust mechanisms into LLMs to dynamically control the disclosure of sensitive information. The framework integrates three core components: User Trust Profiling, Information Sensitivity Detection, and Adaptive Output Control. By leveraging techniques such as Role-Based Access Control (RBAC), Attribute-Based Access Control (ABAC), Named Entity Recognition (NER), contextual analysis, and privacy-preserving methods like differential privacy, the system ensures that sensitive information is disclosed appropriately based on the user’s trust level. By focusing on balancing data utility and privacy, the proposed solution offers a novel approach to securely deploying LLMs in high-risk environments. Future work will focus on testing this framework across various domains to evaluate its effectiveness in managing sensitive data while maintaining system efficiency.",
        "DOI": "10.3390/ai5040134",
        "paper_author": "Feretzakis G.",
        "affiliation_name": "Hellenic Open University",
        "affiliation_city": "Patra",
        "affiliation_country": "Greece",
        "affiliation_id": "60067711",
        "affiliation_state": "Achaia"
    },
    {
        "paper_title": "Using Artificial Intelligence to Support Peer-to-Peer Discussions in Science Classrooms",
        "publication": "Education Sciences",
        "citied_by": "0",
        "cover_date": "2024-12-01",
        "Abstract": "In successful peer discussions students respond to each other and benefit from supports that focus discussion on one another’s ideas. We explore using artificial intelligence (AI) to form groups and guide peer discussion for grade 7 students. We use natural language processing (NLP) to identify student ideas in science explanations. The identified ideas, along with Knowledge Integration (KI) pedagogy, informed the design of a question bank to support students during the discussion. We compare groups formed by maximizing the variety of ideas among participants to randomly formed groups. We embedded the chat tool in an earth science unit and tested it in two classrooms at the same school. We report on the accuracy of the NLP idea detection, the impact of maximized versus random grouping, and the role of the question bank in focusing the discussion on student ideas. We found that the similarity of student ideas limited the value of maximizing idea variety and that the question bank facilitated students’ use of knowledge integration processes.",
        "DOI": "10.3390/educsci14121411",
        "paper_author": "Billings K.",
        "affiliation_name": "University of California, Berkeley",
        "affiliation_city": "Berkeley",
        "affiliation_country": "United States",
        "affiliation_id": "60025038",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "A Comparison of Responsive and General Guidance to Promote Learning in an Online Science Dialog",
        "publication": "Education Sciences",
        "citied_by": "1",
        "cover_date": "2024-12-01",
        "Abstract": "Students benefit from dialogs about their explanations of complex scientific phenomena, and middle school science teachers cannot realistically provide all the guidance they need. We study ways to extend generative teacher–student dialogs to more students by using AI tools. We compare Responsive web-based dialogs to General web-based dialogs by evaluating the ideas students add and the quality of their revised explanations. We designed the General guidance to motivate and encourage students to revise their explanations, similar to how an experienced classroom teacher might instruct the class. We designed the Responsive guidance to emulate a student–teacher dialog, based on studies of experienced teachers guiding individual students. The analyses comparing the Responsive and the General condition are based on a randomized assignment of a total sample of 507 pre-college students. These students were taught by five different teachers in four schools. A significantly higher proportion of students added new accurate ideas in the Responsive condition compared to the General condition during the dialog. This research shows that by using NLP to identify ideas and assign guidance, students can broaden and refine their ideas. Responsive guidance, inspired by how experienced teachers guide individual students, is more valuable than General guidance.",
        "DOI": "10.3390/educsci14121383",
        "paper_author": "Gerard L.",
        "affiliation_name": "University of California, Berkeley",
        "affiliation_city": "Berkeley",
        "affiliation_country": "United States",
        "affiliation_id": "60025038",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Diversifying Multi-Head Attention in the Transformer Model",
        "publication": "Machine Learning and Knowledge Extraction",
        "citied_by": "0",
        "cover_date": "2024-12-01",
        "Abstract": "Recent studies have shown that, due to redundancy, some heads of the Transformer model can be pruned without diminishing the efficiency of the model. In this paper, we propose a constrained optimization algorithm based on Hebbian learning, which trains specific layers in the Transformer architecture in order to enforce diversification between the different heads in the multi-head attention module. The diversification of the heads is achieved through a single-layer feed-forward neural network that is added to the Transformer architecture and is trained with the proposed algorithm. We utilize the algorithm in three different architectural variations of the baseline Transformer model. In addition to the diversification of the heads, the proposed methodology can be used to prune the heads that capture redundant information. Experiments on diverse NLP tasks, including machine translation, text summarization, question answering and large language modeling, show that our proposed approach consistently improves the performance of baseline Transformer models.",
        "DOI": "10.3390/make6040126",
        "paper_author": "Ampazis N.",
        "affiliation_name": "University of the Aegean",
        "affiliation_city": "Mytilene",
        "affiliation_country": "Greece",
        "affiliation_id": "60017404",
        "affiliation_state": "North Aegean"
    },
    {
        "paper_title": "Identifying technologies in circular economy paradigm through text mining on scientific literature",
        "publication": "PLoS ONE",
        "citied_by": "2",
        "cover_date": "2024-12-01",
        "Abstract": "Technological innovation serves as the catalyst for the shift towards circular practices. Technologies not only address technical challenges, facilitating the transition to a more circular economy, but they also enhance business efficiency and profitability. Furthermore, they promote inclusivity and create job opportunities, ultimately yielding positive societal impacts. The research in this area tends to focus on digital technologies, neglecting other technological areas. Moreover, it heavily relies on literature reviews and expert opinions, potentially introducing biases. In this article we investigate the technological landscape of the circular economy through Natural Language Processing (NLP), examining key technologies used in this sector and the primary challenges in managing these technologies. The methodology is applied to more than 45,000 scientific publications and aims to extract technologies in the text of scientific articles with NLP. The findings of our analysis reveal a strong emphasis on emerging digital, life cycle assessment and biomaterials technologies. Furthermore, we identified seven distinct technological domains within the CE field. Finally, we provide advantages and problems arising in the adoption and implementation of these technologies in an industrial context.",
        "DOI": "10.1371/journal.pone.0312709",
        "paper_author": "Giordano V.",
        "affiliation_name": "Business Engineering for Data Science—B4DS Research Laboratory",
        "affiliation_city": "Pisa",
        "affiliation_country": "Italy",
        "affiliation_id": "130494714",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Leveraging Large Language Models in Tourism: A Comparative Study of the Latest GPT Omni Models and BERT NLP for Customer Review Classification and Sentiment Analysis",
        "publication": "Information (Switzerland)",
        "citied_by": "0",
        "cover_date": "2024-12-01",
        "Abstract": "In today’s rapidly evolving digital landscape, customer reviews play a crucial role in shaping the reputation and success of hotels. Accurately analyzing and classifying the sentiment of these reviews offers valuable insights into customer satisfaction, enabling businesses to gain a competitive edge. This study undertakes a comparative analysis of traditional natural language processing (NLP) models, such as BERT and advanced large language models (LLMs), specifically GPT-4 omni and GPT-4o mini, both pre- and post-fine-tuning with few-shot learning. By leveraging an extensive dataset of hotel reviews, we evaluate the effectiveness of these models in predicting star ratings based on review content. The findings demonstrate that the GPT-4 omni family significantly outperforms the BERT model, achieving an accuracy of 67%, compared to BERT’s 60.6%. GPT-4o, in particular, excelled in accuracy and contextual understanding, showcasing the superiority of advanced LLMs over traditional NLP methods. This research underscores the potential of using sophisticated review evaluation systems in the hospitality industry and positions GPT-4o as a transformative tool for sentiment analysis. It marks a new era in automating and interpreting customer feedback with unprecedented precision.",
        "DOI": "10.3390/info15120792",
        "paper_author": "Roumeliotis K.I.",
        "affiliation_name": "University of the Peloponnese",
        "affiliation_city": "Tripolis",
        "affiliation_country": "Greece",
        "affiliation_id": "60003684",
        "affiliation_state": "Peloponnese"
    },
    {
        "paper_title": "Identification of Scientific Texts Generated by Large Language Models Using Machine Learning",
        "publication": "Computers",
        "citied_by": "0",
        "cover_date": "2024-12-01",
        "Abstract": "Large language models (LLMs) are tools that help us in a variety of activities, from creating well-structured texts to quickly consulting information. But as these new technologies are so easily accessible, many people use them for their own benefit without properly citing the original author, or in other cases the student sector can be heavily compromised because students may opt for a quick answer over understanding and comprehending a specific topic in depth, considerably reducing their basic writing, editing and reading comprehension skills. Therefore, we propose to create a model to identify texts produced by LLM. To do so, we will use natural language processing (NLP) and machine-learning algorithms to recognize texts that mask LLM misuse using different types of adversarial attack, like paraphrasing or translation from one language to another. The main contributions of this work are to identify the texts generated by the large language models, and for this purpose several experiments were developed looking for the best results implementing the f1, accuracy, recall and precision metrics, together with PCA and t-SNE diagrams to see the classification of each one of the texts.",
        "DOI": "10.3390/computers13120346",
        "paper_author": "Soto-Osorio D.",
        "affiliation_name": "Instituto Politécnico Nacional",
        "affiliation_city": "Mexico",
        "affiliation_country": "Mexico",
        "affiliation_id": "60019176",
        "affiliation_state": "DF"
    },
    {
        "paper_title": "Contextual Fine-Tuning of Language Models with Classifier-Driven Content Moderation for Text Generation",
        "publication": "Entropy",
        "citied_by": "0",
        "cover_date": "2024-12-01",
        "Abstract": "In today’s digital age, ensuring the appropriateness of content for children is crucial for their cognitive and emotional development. The rise of automated text generation technologies, such as Large Language Models like LLaMA, Mistral, and Zephyr, has created a pressing need for effective tools to filter and classify suitable content. However, the existing methods often fail to effectively address the intricate details and unique characteristics of children’s literature. This study aims to bridge this gap by developing a robust framework that utilizes fine-tuned language models, classification techniques, and contextual story generation to generate and classify children’s stories based on their suitability. Employing a combination of fine-tuning techniques on models such as LLaMA, Mistral, and Zephyr, alongside a BERT-based classifier, we evaluated the generated stories against established metrics like ROUGE, METEOR, and BERT Scores. The fine-tuned Mistral-7B model achieved a ROUGE-1 score of 0.4785, significantly higher than the base model’s 0.3185, while Zephyr-7B-Beta achieved a METEOR score of 0.4154 compared to its base counterpart’s score of 0.3602. The results indicated that the fine-tuned models outperformed base models, generating content more aligned with human standards. Moreover, the BERT Classifier exhibited high precision (0.95) and recall (0.97) for identifying unsuitable content, further enhancing the reliability of content classification. These findings highlight the potential of advanced language models in generating age-appropriate stories and enhancing content moderation strategies. This research has broader implications for educational technology, content curation, and parental control systems, offering a scalable approach to ensuring children’s exposure to safe and enriching narratives.",
        "DOI": "10.3390/e26121114",
        "paper_author": "Punnaivanam M.",
        "affiliation_name": "College of Engineering, Guindy",
        "affiliation_city": "Chennai",
        "affiliation_country": "India",
        "affiliation_id": "60108956",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Enhancing Personalized Mental Health Support Through Artificial Intelligence: Advances in Speech and Text Analysis Within Online Therapy Platforms",
        "publication": "Information (Switzerland)",
        "citied_by": "0",
        "cover_date": "2024-12-01",
        "Abstract": "Automatic speech recognition (ASR) and natural language processing (NLP) play key roles in advancing human–technology interactions, particularly in healthcare communications. This study aims to enhance French-language online mental health platforms through the adaptation of the QuartzNet 15 × 5 ASR model, selected for its robust performance across a variety of French accents as demonstrated on the Mozilla Common Voice dataset. The adaptation process involved tailoring the ASR model to accommodate various French dialects and idiomatic expressions, and integrating it with an NLP system to refine user interactions. The adapted QuartzNet 15 × 5 model achieved a baseline word error rate (WER) of 14%, and the accompanying NLP system displayed weighted averages of 64.24% in precision, 63.64% in recall, and an F1-score of 62.75%. Notably, critical functionalities such as ‘Prendre Rdv’ (schedule appointment) achieved precision, recall, and F1-scores above 90%. These improvements substantially enhance the functionality and management of user interactions on French-language digital therapy platforms, indicating that continuous adaptation and enhancement of these technologies are beneficial for improving digital mental health interventions, with a focus on linguistic accuracy and user satisfaction.",
        "DOI": "10.3390/info15120813",
        "paper_author": "Jelassi M.",
        "affiliation_name": "Ecole Nationale des Sciences de l'Informatique",
        "affiliation_city": "Manouba",
        "affiliation_country": "Tunisia",
        "affiliation_id": "60070317",
        "affiliation_state": "Manouba"
    },
    {
        "paper_title": "A Comprehensive Review of Deep Learning: Architectures, Recent Advances, and Applications",
        "publication": "Information (Switzerland)",
        "citied_by": "1",
        "cover_date": "2024-12-01",
        "Abstract": "Deep learning (DL) has become a core component of modern artificial intelligence (AI), driving significant advancements across diverse fields by facilitating the analysis of complex systems, from protein folding in biology to molecular discovery in chemistry and particle interactions in physics. However, the field of deep learning is constantly evolving, with recent innovations in both architectures and applications. Therefore, this paper provides a comprehensive review of recent DL advances, covering the evolution and applications of foundational models like convolutional neural networks (CNNs) and Recurrent Neural Networks (RNNs), as well as recent architectures such as transformers, generative adversarial networks (GANs), capsule networks, and graph neural networks (GNNs). Additionally, the paper discusses novel training techniques, including self-supervised learning, federated learning, and deep reinforcement learning, which further enhance the capabilities of deep learning models. By synthesizing recent developments and identifying current challenges, this paper provides insights into the state of the art and future directions of DL research, offering valuable guidance for both researchers and industry experts.",
        "DOI": "10.3390/info15120755",
        "paper_author": "Mienye I.D.",
        "affiliation_name": "University of Johannesburg",
        "affiliation_city": "Johannesburg",
        "affiliation_country": "South Africa",
        "affiliation_id": "60000717",
        "affiliation_state": "Gauteng"
    }
]