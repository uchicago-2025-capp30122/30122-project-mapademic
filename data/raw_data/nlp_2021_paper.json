[
    {
        "paper_title": "KI-HABS: Key Information Guided Hierarchical Abstractive Summarization",
        "publication": "KSII Transactions on Internet and Information Systems",
        "citied_by": "2",
        "cover_date": "2021-12-31",
        "Abstract": "With the unprecedented growth of textual information on the Internet, an efficient automatic summarization system has become an urgent need. Recently, the neural network models based on the encoder-decoder with an attention mechanism have demonstrated powerful capabilities in the sentence summarization task. However, for paragraphs or longer document summarization, these models fail to mine the core information in the input text, which leads to information loss and repetitions. In this paper, we propose an abstractive document summarization method by applying guidance signals of key sentences to the encoder based on the hierarchical encoder-decoder architecture, denoted as KI-HABS. Specifically, we first train an extractor to extract key sentences in the input document by the hierarchical bidirectional GRU. Then, we encode the key sentences to the key information representation in the sentence level. Finally, we adopt key information representation guided selective encoding strategies to filter source information, which establishes a connection between the key sentences and the document. We use the CNN/Daily Mail and Gigaword datasets to evaluate our model. The experimental results demonstrate that our method generates more informative and concise summaries, achieving better performance than the competitive models.",
        "DOI": "10.3837/TIIS.2021.12.001",
        "paper_author": "Zhang M.",
        "affiliation_name": "State Key Laboratory of Mathematical Engineering and Advanced Computing",
        "affiliation_city": "Henan",
        "affiliation_country": "China",
        "affiliation_id": "126471870",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Explanation-based human debugging of nlp models: A survey",
        "publication": "Transactions of the Association for Computational Linguistics",
        "citied_by": "40",
        "cover_date": "2021-12-30",
        "Abstract": "Debugging a machine learning model is hard since the bug usually involves the training data and the learning process. This becomes even harder for an opaque deep learning model if we have no clue about how the model actually works. In this survey, we review papers that exploit explanations to enable humans to give feedback and debug NLP models. We call this problem explanation-based human debugging (EBHD). In particular, we categorize and discuss existing work along three dimensions of EBHD (the bug context, the workflow, and the experimental setting), compile findings on how EBHD components affect the feedback providers, and highlight open problems that could be future research directions.",
        "DOI": "10.1162/tacl_a_00440",
        "paper_author": "Lertvittayakumjorn P.",
        "affiliation_name": "Imperial College London",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60015150",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Idiomatic expression identification using semantic compatibility",
        "publication": "Transactions of the Association for Computational Linguistics",
        "citied_by": "27",
        "cover_date": "2021-12-30",
        "Abstract": "Idiomatic expressions are an integral part of natural language and constantly being added to a language. Owing to their non-compositionality and their ability to take on a figurative or literal meaning depending on the sentential context, they have been a classical challenge for NLP systems. To address this challenge, we study the task of detecting whether a sentence has an idiomatic expression and localizing it when it occurs in a figurative sense. Prior research for this task has studied specific classes of idiomatic expressions offering limited views of their generalizability to new idioms. We propose a multi-stage neural architecture with attention flow as a solution. The network effectively fuses contextual and lexical information at different levels using word and sub-word representations. Empirical evaluations on three of the largest benchmark datasets with idiomatic expressions of varied syntactic patterns and degrees of non-compositionality show that our proposed model achieves new state-of-the-art results. A salient feature of the model is its ability to identify idioms unseen during training with gains from 1.4% to 30.8% over competitive baselines on the largest dataset.",
        "DOI": "10.1162/tacl_a_00442",
        "paper_author": "Zeng Z.",
        "affiliation_name": "The Grainger College of Engineering",
        "affiliation_city": "Urbana",
        "affiliation_country": "United States",
        "affiliation_id": "60158506",
        "affiliation_state": "IL"
    },
    {
        "paper_title": "Development of Focused Crawlers for Building Large Punjabi News Corpus",
        "publication": "Journal of ICT Research and Applications",
        "citied_by": "2",
        "cover_date": "2021-12-28",
        "Abstract": "Web crawlers are as old as the Internet and are most commonly used by search engines to visit websites and index them into repositories. They are not limited to search engines but are also widely utilized to build corpora in different domains and languages. This study developed a focused set of web crawlers for three Punjabi news websites. The web crawlers were developed to extract quality text articles and add them to a local repository to be used in further research. The crawlers were implemented using the Python programming language and were utilized to construct a corpus of more than 134,000 news articles in nine different news genres. The crawler code and extracted corpora were made publicly available to the scientific community for research purposes.",
        "DOI": "10.5614/ITBJ.ICT.RES.APPL.2021.15.3.1",
        "paper_author": "Mahi G.S.",
        "affiliation_name": "Punjabi University",
        "affiliation_city": "Patiala",
        "affiliation_country": "India",
        "affiliation_id": "60000690",
        "affiliation_state": "PB"
    },
    {
        "paper_title": "Deep Text Matching in Medical Question Answering System",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "3",
        "cover_date": "2021-12-28",
        "Abstract": "The retrieval question-answering(Q&A) system based on Q&A library is a system that can retrieve the most similar question from Q&A library to get the correct answer. Classic approaches only use TF-IDF, BM25 and other algorithms to calculate the shallow correlation between the sentences in the input question and the sentences in the Q&A library, without fully considering the semantic information of the sentences. Recently, pre-trained language models have made remarkable achievements in many fields of natural language processing(NLP). In this work we apply the pre-trained language model in the medicine field to the text matching stage of medical question answering system. We also improve and propose a deep text matching model based on BERT, the Potential Topic extraction Medical Bert model(PT-McBERT). We conduct several experiments on the medical text matching dataset CHIP-STS, the results show that our model achieves improvements over the classic methods. Finally, we design a real-world Chinese medical question answering system and apply the optimal model to the question matching stage, which can greatly improve the matching effect of the system.",
        "DOI": "10.1145/3491396.3506536",
        "paper_author": "Zhao X.",
        "affiliation_name": "Qilu University of Technology",
        "affiliation_city": "Jinan",
        "affiliation_country": "China",
        "affiliation_id": "60011592",
        "affiliation_state": "Shandong"
    },
    {
        "paper_title": "Exploiting Artificial Immune System to Optimize Association Rules for Word Sense Disambiguation",
        "publication": "International Journal of Intelligent Systems and Applications in Engineering",
        "citied_by": "1",
        "cover_date": "2021-12-26",
        "Abstract": "Requirement specification is the major activity in software development. Since requirements are gathered from the customers in natural languages they are prone to ambiguities. Ambiguous requirements give many interpretations of the same word or sentence. In order to reduce the problems faced due to requirement ambiguities, many techniques have been proposed in the past. Word Sense Disambiguation is a bottleneck in most of the Natural Language Processing (NLP) applications. The approaches to deal with WSD aims to provide the best possible meaning for the target word which is lexically ambiguous. To reduce the ambiguities and optimize the association mining rules for Word Sense Disambiguation (WSD), this paper proposes a new approach based on Artificial Immune System and Association Rule Mining. The approach shows significant results when tested on a collection of many Software Requirement Specifications (SRS) Documents. The average accuracy provided by the system is 89.2725%. outperforms state of the art methods.",
        "DOI": "10.18201/IJISAE.2021473638",
        "paper_author": "Husain M.S.",
        "affiliation_name": "University of Technology and Applied Sciences Ibri",
        "affiliation_city": "Ibri",
        "affiliation_country": "Oman",
        "affiliation_id": "60281131",
        "affiliation_state": "Ad Dhahirah"
    },
    {
        "paper_title": "A deep learning-based bilingual Hindi and Punjabi named entity recognition system using enhanced word embeddings",
        "publication": "Knowledge-Based Systems",
        "citied_by": "19",
        "cover_date": "2021-12-25",
        "Abstract": "The increasing availability of information on the web makes the task of named entity recognition (NER) more challenging. Named entity recognition is an important pre-processor tool that is concerned with the extraction of entities of our interest such as person, location, organization, gene, protein, number, measurement, etc. The success of earlier named entity recognition systems is highly dependent on rule-based techniques or traditional machine learning algorithms exploiting several linguistic and non-linguistic features. In this article, we propose a novel named entity recognition (NER) system that involves the use of deep learning strategies as well as an enhanced version of word embeddings. We develop a Bidirectional Gated Recurrent Unit (Bi-GRU) and Convolutional Neural Networks (CNN) based bilingual named entity recognition system which is built upon enhanced word embeddings (EWE). Enhanced word embeddings (EWE) are generated by concatenation of FastText word embeddings along with minimal feature embeddings, namely part of speech embeddings, word prefix embeddings, word suffix embeddings, and word length embeddings which improve the computational power of deep learning methods. We perform several experiments using corpora in two different languages. One is IJCNLP-08 NERSSEAL shared task corpora containing annotated dataset in Hindi language and the other is manually annotated dataset in Punjabi language. We also make several experiments on bilingual Hindi and Punjabi dataset. The results of the experiments performed in this work reveal that the Bidirectional GRU and CNN based model along with enhanced word embeddings (EWE) has excelled with Precision, Recall, and F-score value of 92.60%, 90.70%, 91.64% respectively for Hindi, 93.87%, 93.33%, 93.60% respectively for Punjabi and 93.78%, 92.66%, 93.22% respectively for bilingual Hindi and Punjabi named entity recognition. Enhanced word embeddings accelerate the performance of a Bi-GRU and CNN based named entity recognition system without using a large set of features and any sort of gazetteers.",
        "DOI": "10.1016/j.knosys.2021.107601",
        "paper_author": "Goyal A.",
        "affiliation_name": "Goswami Ganesh Dutta S.D. College",
        "affiliation_city": "Chandigarh",
        "affiliation_country": "India",
        "affiliation_id": "60097679",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A light heterogeneous graph collaborative filtering model using textual information",
        "publication": "Knowledge-Based Systems",
        "citied_by": "9",
        "cover_date": "2021-12-25",
        "Abstract": "Due to the development of graph neural networks, graph-based representation learning methods have made great progress in recommender systems. However, data sparsity is still a challenging problem that most graph-based recommendation methods are confronted with. Recent works try to address this problem by utilizing side information. In this paper, we exploit the relevant and easily accessible textual information by advanced natural language processing (NLP) models and propose a light RGCN-based (RGCN, relational graph convolutional network) collaborative filtering method on heterogeneous graphs. Specifically, to incorporate rich textual knowledge, we utilize a pre-trained NLP model to initialize the embeddings of text nodes. Afterward, by performing a simplified RGCN-based node information propagation on the constructed heterogeneous graph, the embeddings of users and items can be adjusted with textual knowledge, which effectively alleviates the negative effects of data sparsity. Moreover, the matching function used by most graph-based representation learning methods is the inner product, which is not appropriate for the obtained embeddings that contain complex semantics. We design a predictive network that combines graph-based representation learning with neural matching function learning, and demonstrate that this architecture can bring a significant performance improvement. Extensive experiments are conducted on three publicly available datasets, and the results verify the superior performance of our method over several baselines.",
        "DOI": "10.1016/j.knosys.2021.107602",
        "paper_author": "Wang C.",
        "affiliation_name": "Huazhong University of Science and Technology",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60025761",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Quantum Probability-inspired Graph Attention Network for Modeling Complex Text Interaction",
        "publication": "Knowledge-Based Systems",
        "citied_by": "7",
        "cover_date": "2021-12-25",
        "Abstract": "Inspired by quantum-like phenomena in human language understanding, recent studies propose quantum probability-inspired neural networks to model natural language by treating words as superposition states and a sentence as a mixed state. However, many complex natural language processing tasks (e.g., emotion–cause pair extraction or joint dialog act recognition and sentiment classification) require modeling the complex and graphical interaction of multiple text pieces (e.g., multiple clauses in a document or multiple utterances in a dialog). The existing quantum probability-inspired neural networks only encode sequential interaction of a sequence of words, but cannot model the complex interaction of text pieces. To generalize the quantum framework from modeling word sequence to modeling complex and graphical text interaction, we propose a Quantum Probability-inspired Graph Attention NeTwork (QPGAT) by combining quantum probability and graph attention mechanism in a unified framework. Specifically, a text interaction graph is firstly constructed to describe the complex interaction of text pieces. Then QPGAT models each text node as a particle in a superposition state and each node's neighborhood in the graph as a mixed system in a mixed state to learn interaction-aware text node representations. We apply QPGAT to the two important and complex NLP tasks, emotion–cause pair extraction and joint dialog act recognition and sentiment classification. Experiment results show that QPGAT is competitive compared with the state-of-the-art methods on the two complex NLP tasks, demonstrating the effectiveness of QPGAT. Moreover, QPGAT can also provide a reasonable post-hoc explanation about the model decision process for emotion–cause pair extraction.",
        "DOI": "10.1016/j.knosys.2021.107557",
        "paper_author": "Yan P.",
        "affiliation_name": "University of Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60027363",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "The Food and Drug Administration Biologics Effectiveness and Safety Initiative Facilitates Detection of Vaccine Administrations From Unstructured Data in Medical Records Through Natural Language Processing",
        "publication": "Frontiers in Digital Health",
        "citied_by": "5",
        "cover_date": "2021-12-22",
        "Abstract": "Introduction: The Food and Drug Administration Center for Biologics Evaluation and Research conducts post-market surveillance of biologic products to ensure their safety and effectiveness. Studies have found that common vaccine exposures may be missing from structured data elements of electronic health records (EHRs), instead being captured in clinical notes. This impacts monitoring of adverse events following immunizations (AEFIs). For example, COVID-19 vaccines have been regularly administered outside of traditional medical settings. We developed a natural language processing (NLP) algorithm to mine unstructured clinical notes for vaccinations not captured in structured EHR data. Methods: A random sample of 1,000 influenza vaccine administrations, representing 995 unique patients, was extracted from a large U.S. EHR database. NLP techniques were used to detect administrations from the clinical notes in the training dataset [80% (N = 797) of patients]. The algorithm was applied to the validation dataset [20% (N = 198) of patients] to assess performance. Full medical charts for 28 randomly selected administration events in the validation dataset were reviewed by clinicians. The NLP algorithm was then applied across the entire dataset (N = 995) to quantify the number of additional events identified. Results: A total of 3,199 administrations were identified in the structured data and clinical notes combined. Of these, 2,740 (85.7%) were identified in the structured data, while the NLP algorithm identified 1,183 (37.0%) administrations in clinical notes; 459 were not also captured in the structured data. This represents a 16.8% increase in the identification of vaccine administrations compared to using structured data alone. The validation of 28 vaccine administrations confirmed 27 (96.4%) as “definite” vaccine administrations; 18 (64.3%) had evidence of a vaccination event in the structured data, while 10 (35.7%) were found solely in the unstructured notes. Discussion: We demonstrated the utility of an NLP algorithm to identify vaccine administrations not captured in structured EHR data. NLP techniques have the potential to improve detection of vaccine administrations not otherwise reported without increasing the analysis burden on physicians or practitioners. Future applications could include refining estimates of vaccine coverage and detecting other exposures, population characteristics, and outcomes not reliably captured in structured EHR data.",
        "DOI": "10.3389/fdgth.2021.777905",
        "paper_author": "Deady M.",
        "affiliation_name": "International Business Machines",
        "affiliation_city": "Armonk",
        "affiliation_country": "United States",
        "affiliation_id": "60021293",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Stop words detection using a long short term memory recurrent neural network",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "2",
        "cover_date": "2021-12-22",
        "Abstract": "Natural language processing is a field of computer science that focuses on understanding and analyzing textual data in any given language. Analyzing textual data is very tedious and leads to erroneous results due to unnecessary and noisy data in the corpus. Stop words are considered noisy data which the English language has already predefined corpus of stop words. Stop words in other languages such as Cebuano and Filipino are not yet supported in many NLP API. In the Philippines, users use different languages to post on Facebook. In this study, a corpus of Facebook posts was utilized in automatically detecting a stop word. A neural network was created based on Bidirectional Long Short term memory (BiLSTM). Word2vec was used to provide word embedding and representation from the corpus. The experimental result shows 72% accuracy in using the model.",
        "DOI": "10.1145/3512576.3512612",
        "paper_author": "Gorro K.D.",
        "affiliation_name": "Cebu Technological University",
        "affiliation_city": "Cebu",
        "affiliation_country": "Philippines",
        "affiliation_id": "60089572",
        "affiliation_state": "Cebu"
    },
    {
        "paper_title": "Auto-generating textual data stories using data science pipelines",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2021-12-22",
        "Abstract": "Understanding a dataset directly is challenging but transforming the results of data analysis into data stories could help people build mental models and understand the dataset easily. In this paper, we present a new framework for data-to-text NLG to generate data stories for specific personas. In order to understand the feasibility of this method and if the human generated story is consistent with the story generated by the data science pipelines, we present two experiments: a data story study with 3 financial experts, 4 Ph.D. students, and 20 Amazon Mechanical Turk workers, which offers several data stories generated by humans; and a validation study involving 39 Amazon Mechanical Turk workers who conducted usability and understandability assessments for 9 high-quality data stories, written by humans and machine. We conduct a qualitative analysis of human-written data stories to determine what people consider when writing data stories and if the human generated story is consistent with the one generated by the data science pipeline. The experimental results show that readers comprehend machine-written data stories as well as they comprehend human-written data stories.",
        "DOI": "10.1145/3508546.3508642",
        "paper_author": "Wang R.",
        "affiliation_name": "University of Aberdeen",
        "affiliation_city": "Aberdeen",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60015875",
        "affiliation_state": "Scotland"
    },
    {
        "paper_title": "An Improved Deep Learning Model: S-TextBLCNN for Traditional Chinese Medicine Formula Classification",
        "publication": "Frontiers in Genetics",
        "citied_by": "14",
        "cover_date": "2021-12-22",
        "Abstract": "Purpose: This study proposes an S-TextBLCNN model for the efficacy of traditional Chinese medicine (TCM) formula classification. This model uses deep learning to analyze the relationship between herb efficacy and formula efficacy, which is helpful in further exploring the internal rules of formula combination. Methods: First, for the TCM herbs extracted from Chinese Pharmacopoeia, natural language processing (NLP) is used to learn and realize the quantitative expression of different TCM herbs. Three features of herb name, herb properties, and herb efficacy are selected to encode herbs and to construct formula-vector and herb-vector. Then, based on 2,664 formulae for stroke collected in TCM literature and 19 formula efficacy categories extracted from Yifang Jijie, an improved deep learning model TextBLCNN consists of a bidirectional long short-term memory (Bi-LSTM) neural network and a convolutional neural network (CNN) is proposed. Based on 19 formula efficacy categories, binary classifiers are established to classify the TCM formulae. Finally, aiming at the imbalance problem of formula data, the over-sampling method SMOTE is used to solve it and the S-TextBLCNN model is proposed. Results: The formula-vector composed of herb efficacy has the best effect on the classification model, so it can be inferred that there is a strong relationship between herb efficacy and formula efficacy. The TextBLCNN model has an accuracy of 0.858 and an F1-score of 0.762, both higher than the logistic regression (acc = 0.561, F1-score = 0.567), SVM (acc = 0.703, F1-score = 0.591), LSTM (acc = 0.723, F1-score = 0.621), and TextCNN (acc = 0.745, F1-score = 0.644) models. In addition, the over-sampling method SMOTE is used in our model to tackle data imbalance, and the F1-score is greatly improved by an average of 47.1% in 19 models. Conclusion: The combination of formula feature representation and the S-TextBLCNN model improve the accuracy in formula efficacy classification. It provides a new research idea for the study of TCM formula compatibility.",
        "DOI": "10.3389/fgene.2021.807825",
        "paper_author": "Cheng N.",
        "affiliation_name": "Hunan University of Chinese Medicine",
        "affiliation_city": "Changsha",
        "affiliation_country": "China",
        "affiliation_id": "60073494",
        "affiliation_state": "Hunan"
    },
    {
        "paper_title": "Prospects for Dutch Emotion Detection: Insights from the New EmotioNL Dataset",
        "publication": "Computational Linguistics in the Netherlands Journal",
        "citied_by": "6",
        "cover_date": "2021-12-21",
        "Abstract": "Although emotion detection has become a crucial research direction in NLP, the main focus is on English resources and data. The main obstacles for more specialized emotion detection are the lack of annotated data in smaller languages and the limited emotion taxonomy. In a first step towards improving emotion detection for Dutch, we present EmotioNL, an emotion dataset consisting of 1,000 Dutch tweets and 1,000 captions from TV-shows, annotated with emotion categories (anger, fear, joy, love, sadness and neutral) and dimensions (valence, arousal and dominance). We evaluate the state-of-the-art Dutch transformer models BERTje and RobBERT on this new dataset, investigate model generalizability across domains and perform a thorough error analysis based on the Component Process Model of emotions.",
        "DOI": "NA",
        "paper_author": "De Bruyne L.",
        "affiliation_name": "Universiteit Gent",
        "affiliation_city": "Ghent",
        "affiliation_country": "Belgium",
        "affiliation_id": "60033316",
        "affiliation_state": "VOV"
    },
    {
        "paper_title": "Tonality analysis in Wolfram Mathematica 12 computer algebra system",
        "publication": "Journal of Physics: Conference Series",
        "citied_by": "1",
        "cover_date": "2021-12-20",
        "Abstract": "Sentiment analysis is a class of methods that are used to automatically determine the tonality of statements in the text. There are many solutions and technologies to define of text tonality at the moment and the choice depends on task. With the release of Wolfram Mathematica 12 versions, it became possible to use included nlp methods for determination of text tonality. This article is about definition of text tonality in the wolfram Mathematica 12 using nlp and compare nlp methods with standard definition function.",
        "DOI": "10.1088/1742-6596/2134/1/012008",
        "paper_author": "Gurin A.A.",
        "affiliation_name": "Plekhanov Russian University of Economics",
        "affiliation_city": "Moscow",
        "affiliation_country": "Russian Federation",
        "affiliation_id": "60030998",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Architecture of Knowledge Extraction System based on NLP",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "2",
        "cover_date": "2021-12-18",
        "Abstract": "Knowledge extraction is to extract useful structured text information from messy free text. Under the current massive information background, it has attracted extensive attention. This paper analyzes the concept of NLP and the application process of NLP algorithm, discusses web information retrieval system, information extraction based on natural language processing and text relationship extraction, and tests the pipeline performance. The results show that the pipeline time is not linearly correlated with the size of the novel, but positively correlated.",
        "DOI": "10.1145/3510858.3510950",
        "paper_author": "Zhuang W.",
        "affiliation_name": "Pactera EDGE",
        "affiliation_city": "Redmond",
        "affiliation_country": "United States",
        "affiliation_id": "121212856",
        "affiliation_state": "WA"
    },
    {
        "paper_title": "Named Entity Recognition using Knowledge Graph Embeddings and DistilBERT",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "2",
        "cover_date": "2021-12-17",
        "Abstract": "Named Entity Recognition (NER) is a Natural Language Processing (NLP) task of identifying entities from a natural language text and classifies them into categories like Person, Location, Organization etc. Pre-trained neural language models (PNLM) based on transformers are state-of-the-art in many NLP task including NER. Analysis of output of DistilBERT, a popular PNLM, reveals that mis-classifications occur when a non-entity word is at a place contextually suitable for an entity. The paper is based on the hypothesis that the performance of a PNLM can be improved by combining it with Knowledge Graph Embeddings (KGE). We show that fine-tuning of DistilBERT along with NumberBatch KGE gives performance improvement over various Open-domain as well as Biomedical-domain datasets.",
        "DOI": "10.1145/3508230.3508252",
        "paper_author": "Mehta S.",
        "affiliation_name": "Visvesvaraya National Institute of Technology, Nagpur",
        "affiliation_city": "Nagpur",
        "affiliation_country": "India",
        "affiliation_id": "60015785",
        "affiliation_state": "MH"
    },
    {
        "paper_title": "Feature Extraction Technique Based on Conv1D and Conv2D Network for Thai Speech Emotion Recognition",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "6",
        "cover_date": "2021-12-17",
        "Abstract": "Speech Emotion Recognition is one of the challenges in Natural Language Processing (NLP) area. There are many factors used to identify emotions in speech, such as pitch, intensity, frequency, duration, and speakers' nationality. This paper implements a speech emotion recognition model specifically for Thai language by classifying it into 5 emotions: Angry, Frustrated, Neutral, Sad, and Happy. This research uses a dataset from VISTEC-depa AI Research Institute of Thailand. There are 21,562 sounds (scripts) divided into 70% of training data and 30% of test data. We use the Mel spectrogram and Mel-frequency Cepstral Coefficients (MFCC) technique for feature extraction and 1D Convolutional Neural Network (Conv1D) all together with 2D Convolutional Neural Network (Conv2D), to classify emotions. With respect to the result, MFCC with Conv2D provides the highest accuracy at 80.59%, and is higher than the baseline study, which is of 71.35%.",
        "DOI": "10.1145/3508230.3508238",
        "paper_author": "Prombut N.",
        "affiliation_name": "Silpakorn University",
        "affiliation_city": "Nakhon Pathom",
        "affiliation_country": "Thailand",
        "affiliation_id": "60018809",
        "affiliation_state": "Nakhon Pathom"
    },
    {
        "paper_title": "Natural Language Processing Applied on Large Scale Data Extraction from Scientific Papers in Fuel Cells",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "3",
        "cover_date": "2021-12-17",
        "Abstract": "Natural language processing (NLP) has a great potential to help scientists automatically extract information from large-scale text datasets. In this paper, we focus on the process of NLP - including text acquisition, text preprocessing, word embedding training, and named entity recognition - applied on 106,181 abstracts of fuel cell papers. Then we evaluate our trained model on its ability of analogy, use the model to analyze the research trend in fuel cell materials and predict new materials. To the best of our knowledge, it is the first time that NLP has been applied in the field of fuel cells. This data-driven technique is demonstrated to have the potential to promote the discoveries of new fuel cell materials.",
        "DOI": "10.1145/3508230.3508256",
        "paper_author": "Yang F.",
        "affiliation_name": "Guangzhou Foreign Language School",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China",
        "affiliation_id": "127564155",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp",
        "publication": "Transactions of the Association for Computational Linguistics",
        "citied_by": "179",
        "cover_date": "2021-12-17",
        "Abstract": "When trained on large, unfiltered crawls from the Internet, language models pick up and reproduce all kinds of undesirable biases that can be found in the data: They often generate racist, sexist, violent, or otherwise toxic language. As large models require millions of training examples to achieve good performance, it is difficult to completely prevent them from being exposed to such content. In this paper, we first demonstrate a surprising finding: Pretrained language models recognize, to a considerable degree, their undesirable biases and the toxicity of the content they produce. We refer to this capability as self-diagnosis. Based on this finding, we then propose a decoding algorithm that, given only a textual description of the undesired behavior, reduces the probability of a language model producing problematic text. We refer to this approach as self-debiasing. Self-debiasing does not rely on manually curated word lists, nor does it require any training data or changes to the model’s parameters. While we by no means eliminate the issue of language models generating biased text, we believe our approach to be an important step in this direction.1.",
        "DOI": "10.1162/tacl_a_00434",
        "paper_author": "Schick T.",
        "affiliation_name": "Ludwig-Maximilians-Universität München",
        "affiliation_city": "Munich",
        "affiliation_country": "Germany",
        "affiliation_id": "60028717",
        "affiliation_state": "Bayern"
    },
    {
        "paper_title": "Hybrid CNN-GRU Framework with Integrated Pre-trained Language Transformer for SMS Phishing Detection",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "9",
        "cover_date": "2021-12-15",
        "Abstract": "Smartphones are prone to SMS phishing due to the rapid growth in the availability of smart mobile technologies driven by Internet connections. Also, detecting phishing SMS is a challenging task due to the unstructured nature of SMS text data with non-linear complex correlations. In this concern, considering the recent advancements in the domain of cybersecurity, we have proposed a hybrid deep learning framework that extracts robust features from SMS texts followed by an automatic detection of Phishing SMS. Due to combining the potential capability of individual models into one hybrid framework, it has outperformed various other individual machine learning and deep learning models. The proposed Phishing Detection framework is an effective hybrid combination of pretrained transformer model, MPNet (Masked and Permuted Language Modeling), with supervised ConvNets (CNN) and Bi-directional Gated Recurrent Units (GRU). It is intended to successfully detect unstructured short phishing text messages that contain complex patterns.",
        "DOI": "10.1145/3508072.3508109",
        "paper_author": "Ulfath R.E.",
        "affiliation_name": "Chittagong University of Engineering and Technology",
        "affiliation_city": "Chittagong",
        "affiliation_country": "Bangladesh",
        "affiliation_id": "60013024",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Big Data framework based on Apache Spark for Industry-specific Lexicon Generation for Stock Market Prediction",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2021-12-15",
        "Abstract": "Research around the process of automatic price prediction of stock markets indicates that published news are an important asset to solve this problem.We further elaborate on an NLP-based approach to generate industry-specific lexicons from news documents exploiting the distributed technology of Apache Spark, with a focus on individuating on a day-to-day scale the correlation between significant stock price variations and the words collected from press releases. Thereafter we apply a binary classification algorithm that builds upon our newly generated lexicons to predict the magnitude of fluctuation of stock market price. Subsequently, by processing news belonging to a large collection of news articles from the most prestigious press agencies, we validate our approach by conducting an experiment on the market history of the US companies belonging to the Standard & Poor 500 index. We also test the performance of the algorithm on a multi-lingual setting, in particular focusing on the Italian stock market and the Italy 40 (FTSE MIB) index. Final data about classification results let us assess the mutual dependence between terms and prices, and help us evaluating the predictive power of our created lexicons.",
        "DOI": "10.1145/3508072.3508196",
        "paper_author": "Angioni S.",
        "affiliation_name": "Università degli Studi di Cagliari",
        "affiliation_city": "Cagliari",
        "affiliation_country": "Italy",
        "affiliation_id": "60032259",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Identifying Heart Failure Symptoms and Poor Self-Management in Home Healthcare: A Natural Language Processing Study",
        "publication": "Studies in Health Technology and Informatics",
        "citied_by": "9",
        "cover_date": "2021-12-15",
        "Abstract": "The goal of this natural language processing (NLP) study was to identify patients in home healthcare with heart failure symptoms and poor self-management (SM). The preliminary lists of symptoms and poor SM status were identified, NLP algorithms were used to refine the lists, and NLP performance was evaluated using 2.3 million home healthcare clinical notes. The overall precision to identify patients with heart failure symptoms and poor SM status was 0.86. The feasibility of methods was demonstrated to identify patients with heart failure symptoms and poor SM documented in home healthcare notes. This study facilitates utilizing key symptom information and patients' SM status from unstructured data in electronic health records. The results of this study can be applied to better individualize symptom management to support heart failure patients' quality-of-life.",
        "DOI": "10.3233/SHTI210653",
        "paper_author": "Chae S.",
        "affiliation_name": "University of Iowa College of Nursing",
        "affiliation_city": "Iowa City",
        "affiliation_country": "United States",
        "affiliation_id": "60032990",
        "affiliation_state": "IA"
    },
    {
        "paper_title": "AI in Healthcare",
        "publication": "Studies in Health Technology and Informatics",
        "citied_by": "30",
        "cover_date": "2021-12-15",
        "Abstract": "The potential value of AI to healthcare, and nursing in particular, ranges from improving quality and efficiency of care to delivering on the promise of personalized and precision medicine. AI systems may become virtually indispensable as ever more data is amassed about every aspect of health. AI can help reduce variability in care, while improving precision, accelerating discovery and reducing disparities. AI can empower patients and potentially allow healthcare professionals to relate to their patients as healers supported by the combined wisdom of the best medical research and analytic technology. There are, however, many challenges to understanding the optimal uses of AI; addressing the technological, systemic, regulatory and attitudinal roadblocks to successful implementation; and integrating AI into the fabric of health care. This paper provides a grounding in the origins and fundamental building blocks of AI, applications in healthcare and for nursing, and the critical challenges facing implementation in healthcare.",
        "DOI": "10.3233/SHTI210726",
        "paper_author": "Koski E.",
        "affiliation_name": "IBM Thomas J. Watson Research Center",
        "affiliation_city": "Yorktown Heights",
        "affiliation_country": "United States",
        "affiliation_id": "60017366",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Beware of proper validation of models for ionic Liquids!",
        "publication": "Journal of Molecular Liquids",
        "citied_by": "28",
        "cover_date": "2021-12-15",
        "Abstract": "The melting point (MP) of an ionic liquid (IL) is one of the key physical properties as it determines the lower limit of the IL working temperature range. In this work, we analysed the recently published studies to predict MP of ILs. While we were able to reproduce the statistical parameters reported by the authors, we found that the performance of the models with new test set data was much lower than the reported statistical values. The discrepancy was due to the validation protocol (random split of the initial set into training/test subsets) that did not allow correct estimation of how contributions of individual ions affect the model performance. Using a more rigorous validation protocol we reached good agreement between the training and test set statistical parameters. We strongly suggest using this protocol for proper validation of models for other properties of ILs to avoid reporting overoptimistic statistical parameters. We also showed that the Transformer Convolutional Neural Network, which was based on the representation of molecules as text (SMILES), proposed a model with significantly higher prediction accuracy as compared to those developed using descriptors that were used in the previous studies. The RMSE of this model is 44 °C and the model is applicable to any type of ILs. The data and developed models are publicly available online at http://ochem.eu/article/135195.",
        "DOI": "10.1016/j.molliq.2021.117722",
        "paper_author": "Makarov D.M.",
        "affiliation_name": "G.A. Krestov Institute of Solution Chemistry of the Russian Academy of Sciences",
        "affiliation_city": "Ivanovo",
        "affiliation_country": "Russian Federation",
        "affiliation_id": "60109736",
        "affiliation_state": "Ivanovo Oblast"
    },
    {
        "paper_title": "Representation learning using Attention Network and CNN for Heterogeneous networks",
        "publication": "Expert Systems with Applications",
        "citied_by": "11",
        "cover_date": "2021-12-15",
        "Abstract": "Network embedding (NE), also known as network representation learning (NRL), is a method to learn a low-dimensional latent representation of nodes in an information network. The real-world data is usually presented in the form of heterogeneous information network (HIN) with multiple types of nodes and edges. Because of the rich information in HINs, it is necessary for a network embedding method to incorporate this information into the low-dimensional potential representation of the nodes as much as possible. In this paper, we propose a semi-supervised representation learning model using a graph attention network and a convolutional neural network (CNN) for HINs, called RANCH. In the part of the graph attention network, we construct a heterogeneous graph attention network using heterogeneous edges to preserve the features of nodes and the structure of network. In the part of the CNN, we leverage a 1D-CNN sentence classification model from natural language processing (NLP) community by adopting edge-constrained truncated random walks to generate node sequences, which can be treated as a corpus of words and sentences. The latter part further integrates the structural information of the network on the basis of the previous part and strengthens the influence of the node's label information on the node representation. We have performed experiments of node classification on three real-world datasets, and the result shows that our model performs better than the state-of-the-arts.",
        "DOI": "10.1016/j.eswa.2021.115628",
        "paper_author": "Tong N.",
        "affiliation_name": "Zhejiang University of Technology",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60026282",
        "affiliation_state": "Zhejiang"
    },
    {
        "paper_title": "Novel Views on Novels:Embedding Multiple Facets of Long Texts",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "3",
        "cover_date": "2021-12-14",
        "Abstract": "Novels are one of the longest document types and thus one of the most complex types of texts. Many NLP tasks utilize document embeddings as machine-understandable semantic representations of documents. However, such document embeddings are optimized for short texts, such as sentences or paragraphs. When faced with longer texts, these models either truncate the long text or split it sequentially into smaller chunks. We show that when applied to a fictional novel, these traditional document embeddings fail to capture all its facets. Complex information, such as time, place, atmosphere, style, and plot is typically not represented adequately. To this end, we propose lib2vec which computes and combines multiple embedding vectors based on various facets. Instead of splitting the text sequentially, lib2vec splits the text semantically based on domain-specific facets. We evaluate the semantic expressiveness using human-assessed book comparisons as well as content-based information retrieval tasks. The results show that our approach outperforms state-of-the-art document embeddings for long texts.",
        "DOI": "10.1145/3486622.3494006",
        "paper_author": "Kohlmeyer L.",
        "affiliation_name": "Hasso-Plattner-Institut für Softwaresystemtechnik GmbH",
        "affiliation_city": "Potsdam",
        "affiliation_country": "Germany",
        "affiliation_id": "60106550",
        "affiliation_state": "Brandenburg"
    },
    {
        "paper_title": "Segmentation-based Phishing URL Detection",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "5",
        "cover_date": "2021-12-14",
        "Abstract": "Uniform resource locators (URLs), used for referencing web pages, play a vital role in cyber fraud because of their complicated structure; phishers, or in other words, attackers, employ tricky by-passing techniques to deceive users. Thus, information extracted from URLs might indicate significant and meaningful patterns essential for phishing detection. To enhance the accuracy of URL-based phishing detection, we need an accurate word segmentation technique to split URLs correctly. However, in contrast to traditional word segmentation techniques used in natural language processing (NLP), URL segmentation requires meticulous attention, as tokenization, the process of turning meaningless data into meaningful data, is not as easy to apply as in NLP. In our work, we concentrate on URL segmentation to propose a novel tokenization method, named URL-Tokenizer, by combining the Bert tokenizer and WordSegment tokenizer, in addition to adopting character-level and word-level segmentations simultaneously. Our experimental evaluations in detecting the phishing URLs show that our proposed method achieves a high accuracy of 95.7% with a balanced dataset, and 97.7% with an imbalanced dataset, whereas baseline models achieved 85.4% with a balanced dataset and 85.1% with an imbalanced dataset.",
        "DOI": "10.1145/3486622.3493983",
        "paper_author": "Aung E.S.",
        "affiliation_name": "Waseda University",
        "affiliation_city": "Tokyo",
        "affiliation_country": "Japan",
        "affiliation_id": "60023462",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Teacher-Student Approach to Cross-Domain Transfer Learning with Multi-level Attention",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2021-12-14",
        "Abstract": "The lack of training data forms a challenging issue for applying NLP models in a new domain. Previous work on cross-domain transfer learning aims to exploit the information from the source domains to do prediction for the target domain. To reduce the noises from the out-of-domain data and improve the model's generalization ability, this work proposes a novel teacher-student approach with multi-task learning that transfers the information from source domains to the target domain with sophisticated weights determined by using attention mechanism at both the instance level and the domain level. The generalization ability is further enhanced by unsupervised data augmentation. We also introduce a subject detection task for co-training the main model. Our approach is evaluated not only on the widely-adopted English dataset, Amazon product reviews, but also on Chinese datasets including product reviews and the discussions about pop musicians. Experimental results show that our approach outperforms state-of-the-art models.",
        "DOI": "10.1145/3486622.3494009",
        "paper_author": "Tang Y.J.",
        "affiliation_name": "Institute of Information Science, Academia Sinica",
        "affiliation_city": "Taipei",
        "affiliation_country": "Taiwan",
        "affiliation_id": "60013651",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Development of a Lexicon for Pain",
        "publication": "Frontiers in Digital Health",
        "citied_by": "7",
        "cover_date": "2021-12-13",
        "Abstract": "Pain has been an area of growing interest in the past decade and is known to be associated with mental health issues. Due to the ambiguous nature of how pain is described in text, it presents a unique natural language processing (NLP) challenge. Understanding how pain is described in text and utilizing this knowledge to improve NLP tasks would be of substantial clinical importance. Not much work has previously been done in this space. For this reason, and in order to develop an English lexicon for use in NLP applications, an exploration of pain concepts within free text was conducted. The exploratory text sources included two hospital databases, a social media platform (Twitter), and an online community (Reddit). This exploration helped select appropriate sources and inform the construction of a pain lexicon. The terms within the final lexicon were derived from three sources—literature, ontologies, and word embedding models. This lexicon was validated by two clinicians as well as compared to an existing 26-term pain sub-ontology and MeSH (Medical Subject Headings) terms. The final validated lexicon consists of 382 terms and will be used in downstream NLP tasks by helping select appropriate pain-related documents from electronic health record (EHR) databases, as well as pre-annotating these words to help in development of an NLP application for classification of mentions of pain within the documents. The lexicon and the code used to generate the embedding models have been made publicly available.",
        "DOI": "10.3389/fdgth.2021.778305",
        "paper_author": "Chaturvedi J.",
        "affiliation_name": "King's College London",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60011520",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Zero-shot reductive paraphrasing for digitally semi-literate",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2021-12-13",
        "Abstract": "People in developing countries with restricted schooling, face hurdles with their digital enablement. Constrained education creates issues with comprehensibility of information on internet and other digital platforms. Most content on digital platforms use enriched vocabulary for more accomplished users and hence does not seem to be very useful for digitally semi-literate users. Artificial Intelligence (AI), Information Retrieval (IR) and Natural Language Processing (NLP) can be applied for text simplification to bridge the digital divide and empower these users. In this paper we propose to achieve reductive paraphrasing using Neural Machine Translation (NMT) framework along with encoder-decoder model and Gated Recurring Unit (GRU). Our approach combines Zero-shot Learning (ZSL) using multi-pivot method to execute our experiment. We have considered English as the base language and three pivot languages (French, German and Spanish) to verify our claims. We have designed the simplified vocabulary from movies for younger audience. It has been observed that using the approach as described in our paper, an improvement of an average of 25% in ease of comprehension can be achieved.",
        "DOI": "10.1145/3503162.3503171",
        "paper_author": "Sharma P.",
        "affiliation_name": "Infosys Technologies Limited India",
        "affiliation_city": "Bengaluru",
        "affiliation_country": "India",
        "affiliation_id": "60069520",
        "affiliation_state": "KA"
    },
    {
        "paper_title": "Quantification of Gender Bias and Sentiment Toward Political Leaders Over 20 Years of Kenyan News Using Natural Language Processing",
        "publication": "Frontiers in Psychology",
        "citied_by": "7",
        "cover_date": "2021-12-10",
        "Abstract": "Background: Despite a 2010 Kenyan constitutional amendment limiting members of elected public bodies to < two-thirds of the same gender, only 22 percent of the 12th Parliament members inaugurated in 2017 were women. Investigating gender bias in the media is a useful tool for understanding socio-cultural barriers to implementing legislation for gender equality. Natural language processing (NLP) methods, such as word embedding and sentiment analysis, can efficiently quantify media biases at a scope previously unavailable in the social sciences. Methods: We trained GloVe and word2vec word embeddings on text from 1998 to 2019 from Kenya’s Daily Nation newspaper. We measured gender bias in these embeddings and used sentiment analysis to predict quantitative sentiment scores for sentences surrounding female leader names compared to male leader names. Results: Bias in leadership words for men and women measured from Daily Nation word embeddings corresponded to temporal trends in men and women’s participation in political leadership (i.e., parliamentary seats) using GloVe (correlation 0.8936, p = 0.0067, r2 = 0.799) and word2vec (correlation 0.844, p = 0.0169, r2 = 0.712) algorithms. Women continue to be associated with domestic terms while men continue to be associated with influence terms, for both regular gender words and female and male political leaders’ names. Male words (e.g., he, him, man) were mentioned 1.84 million more times than female words from 1998 to 2019. Sentiment analysis showed an increase in relative negative sentiment associated with female leaders (p = 0.0152) and an increase in positive sentiment associated with male leaders over time (p = 0.0216). Conclusion: Natural language processing is a powerful method for gaining insights into and quantifying trends in gender biases and sentiment in news media. We found evidence of improvement in gender equality but also a backlash from increased female representation in high-level governmental leadership.",
        "DOI": "10.3389/fpsyg.2021.712646",
        "paper_author": "Pair E.",
        "affiliation_name": "Stanford University School of Medicine",
        "affiliation_city": "Stanford",
        "affiliation_country": "United States",
        "affiliation_id": "60032838",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Listening to Mental Health Crisis Needs at Scale: Using Natural Language Processing to Understand and Evaluate a Mental Health Crisis Text Messaging Service",
        "publication": "Frontiers in Digital Health",
        "citied_by": "7",
        "cover_date": "2021-12-06",
        "Abstract": "The current mental health crisis is a growing public health issue requiring a large-scale response that cannot be met with traditional services alone. Digital support tools are proliferating, yet most are not systematically evaluated, and we know little about their users and their needs. Shout is a free mental health text messaging service run by the charity Mental Health Innovations, which provides support for individuals in the UK experiencing mental or emotional distress and seeking help. Here we study a large data set of anonymised text message conversations and post-conversation surveys compiled through Shout. This data provides an opportunity to hear at scale from those experiencing distress; to better understand mental health needs for people not using traditional mental health services; and to evaluate the impact of a novel form of crisis support. We use natural language processing (NLP) to assess the adherence of volunteers to conversation techniques and formats, and to gain insight into demographic user groups and their behavioural expressions of distress. Our textual analyses achieve accurate classification of conversation stages (weighted accuracy = 88%), behaviours (1-hamming loss = 95%) and texter demographics (weighted accuracy = 96%), exemplifying how the application of NLP to frontline mental health data sets can aid with post-hoc analysis and evaluation of quality of service provision in digital mental health services.",
        "DOI": "10.3389/fdgth.2021.779091",
        "paper_author": "Liu Z.",
        "affiliation_name": "Imperial College London",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60015150",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "BERT-Based Natural Language Processing of Drug Labeling Documents: A Case Study for Classifying Drug-Induced Liver Injury Risk",
        "publication": "Frontiers in Artificial Intelligence",
        "citied_by": "16",
        "cover_date": "2021-12-06",
        "Abstract": "Background & Aims: The United States Food and Drug Administration (FDA) regulates a broad range of consumer products, which account for about 25% of the United States market. The FDA regulatory activities often involve producing and reading of a large number of documents, which is time consuming and labor intensive. To support regulatory science at FDA, we evaluated artificial intelligence (AI)-based natural language processing (NLP) of regulatory documents for text classification and compared deep learning-based models with a conventional keywords-based model. Methods: FDA drug labeling documents were used as a representative regulatory data source to classify drug-induced liver injury (DILI) risk by employing the state-of-the-art language model BERT. The resulting NLP-DILI classification model was statistically validated with both internal and external validation procedures and applied to the labeling data from the European Medicines Agency (EMA) for cross-agency application. Results: The NLP-DILI model developed using FDA labeling documents and evaluated by cross-validations in this study showed remarkable performance in DILI classification with a recall of 1 and a precision of 0.78. When cross-agency data were used to validate the model, the performance remained comparable, demonstrating that the model was portable across agencies. Results also suggested that the model was able to capture the semantic meanings of sentences in drug labeling. Conclusion: Deep learning-based NLP models performed well in DILI classification of drug labeling documents and learned the meanings of complex text in drug labeling. This proof-of-concept work demonstrated that using AI technologies to assist regulatory activities is a promising approach to modernize and advance regulatory science.",
        "DOI": "10.3389/frai.2021.729834",
        "paper_author": "Wu Y.",
        "affiliation_name": "National Center for Toxicological Research",
        "affiliation_city": "Jeffersonville",
        "affiliation_country": "United States",
        "affiliation_id": "60029830",
        "affiliation_state": "AR"
    },
    {
        "paper_title": "Platform-Oblivious Anti-Spam Gateway",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2021-12-06",
        "Abstract": "This paper addresses a novel anti-spam gateway targeting multiple linguistic-based social platforms to expose the outlier property of their spam messages uniformly for effective detection. Instead of labeling ground truth datasets and extracting key features, which are labor-intensive and time-consuming, we start with coarsely mining seed corpora of spams and hams from the target data (aiming for spam classification), before reconstructing them as the reference. To catch each word's rich information in the semantic and syntactic perspectives, we then leverage the natural language processing (NLP) model to embed each word into the high-dimensional vector space and use a neural network to train a spam word model. After that, each message is encoded by using the predicted spam scores from this model for all included stem words. The encoded messages are processed by the prominent outlier techniques to produce their respective scores, allowing us to rank them for making the outlier visible. Our solution is unsupervised, without relying on specifics of any platform or dataset, to be platform-oblivious. Through extensive experiments, our solution is demonstrated to expose spammers' outlier characteristics effectively, outperform all examined unsupervised methods in almost all metrics, and may even better supervised counterparts.",
        "DOI": "10.1145/3485832.3488024",
        "paper_author": "Zhang Y.",
        "affiliation_name": "University of Louisiana at Lafayette",
        "affiliation_city": "Lafayette",
        "affiliation_country": "United States",
        "affiliation_id": "60015161",
        "affiliation_state": "LA"
    },
    {
        "paper_title": "Detecting and Characterizing SMS Spearphishing Attacks",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "15",
        "cover_date": "2021-12-06",
        "Abstract": "Although spearphishing is a well-known security issue and has been widely researched, it is still an evolving threat with emerging forms. In recent years, Short Message Service (SMS) has been revealed as a new distribution channel for spearphishing messages, which already has caused a serious impact in the real world, but has not yet attracted enough attention from the academic community. In this paper, we report the first systemic study to spotlight this emerging threat, SMS spearphishing attack. Through cooperating with a leading security vendor, we obtain 31.96M real-world spam messages that span three months. We design and implement a novel NLP-based detection algorithm, and uncover 90,801 spearphishing messages on the entire dataset. And then, a large-scale measurement was performed on the detected messages to reveal and understand the characteristics of SMS spearphishing attack. Our findings are multi-fold. We discover that SMS spearphishing has a significant negative impact on the real-world, and a large number of victims have been affected. And the distribution of active illicit types between spearphishing message and common spam is quite inconsistent. At the micro-level, to evade detection and increase the probability of success, adversary campaigns have evolved a set of sophisticated strategies. Our research highlights the impact of SMS spearphishing attack is prominent. We call on different communities to work together to mitigate this emerging security threat.",
        "DOI": "10.1145/3485832.3488012",
        "paper_author": "Liu M.",
        "affiliation_name": "Beijing National Research Center for Information Science and Technology",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60104026",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "BadNL: Backdoor Attacks against NLP Models with Semantic-preserving Improvements",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "128",
        "cover_date": "2021-12-06",
        "Abstract": "Deep neural networks (DNNs) have progressed rapidly during the past decade and have been deployed in various real-world applications. Meanwhile, DNN models have been shown to be vulnerable to security and privacy attacks. One such attack that has attracted a great deal of attention recently is the backdoor attack. Specifically, the adversary poisons the target model's training set to mislead any input with an added secret trigger to a target class. Previous backdoor attacks predominantly focus on computer vision (CV) applications, such as image classification. In this paper, we perform a systematic investigation of backdoor attack on NLP models, and propose BadNL, a general NLP backdoor attack framework including novel attack methods. Specifically, we propose three methods to construct triggers, namely BadChar, BadWord, and BadSentence, including basic and semantic-preserving variants. Our attacks achieve an almost perfect attack success rate with a negligible effect on the original model's utility. For instance, using the BadChar, our backdoor attack achieves a 98.9% attack success rate with yielding a utility improvement of 1.5% on the SST-5 dataset when only poisoning 3% of the original set. Moreover, we conduct a user study to prove that our triggers can well preserve the semantics from humans perspective.",
        "DOI": "10.1145/3485832.3485837",
        "paper_author": "Chen X.",
        "affiliation_name": "Peking University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60014966",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "SODA: A System for Cyber Deception Orchestration and Automation",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "16",
        "cover_date": "2021-12-06",
        "Abstract": "Active Cyber Deception (ACD) has emerged as an effective proactive cyber defense technique that can mislead adversaries by presenting falsified data and allow opportunities for engaging with them to learn novel attack techniques. Adversaries often implement their attack techniques within malware and use it as the medium to steal valuable information. Comprehensive malware analysis is required to understand the malware behaviors at technical and tactical levels to create the honey resources and appropriate ploys that can leverage this behavior and mislead malware and APT adversaries. This paper presents SODA, a cyber deception orchestration system that analyzes real-world malware, discovers attack techniques, creates Deception Playbooks, a set of deception actions, and finally orchestrates the environment to deceive malware. SODA extracts Malicious Sub-graphs (MSGs) consisting of WinAPIs from real-world malware and maps them to MITRE ATT&CK techniques. This MSG-to-MITRE mapping describes how ATT&CK techniques are implemented in malware and, as a result, guides the construction of appropriate deception actions. We conducted comprehensive evaluations on SODA with 255 recent malware samples to demonstrate end-to-end deception effectiveness. We observed an average accuracy of 95% in deceiving the malware with negligible overhead for specified deception goals and strategies. Furthermore, our approach successfully extracted MSGs with a 97% recall and our MSG-to-MITRE mapping achieved a top-1 accuracy of 88.75%. More importantly, SODA can serve as a general purpose malware deception factory to automatically produce customized deception playbooks against arbitrary malware.",
        "DOI": "10.1145/3485832.3485918",
        "paper_author": "Sajid M.S.I.",
        "affiliation_name": "The University of North Carolina at Charlotte",
        "affiliation_city": "Charlotte",
        "affiliation_country": "United States",
        "affiliation_id": "60006951",
        "affiliation_state": "NC"
    },
    {
        "paper_title": "DETECTING MALICIOUS ACTIVITIES on TWITTER DATA for SENTIMENT ANALYSIS USING A NOVEL OPTIMIZED MACHINE LEARNING APPROACH",
        "publication": "Journal of Theoretical and Applied Information Technology",
        "citied_by": "1",
        "cover_date": "2021-12-05",
        "Abstract": "In Natural Language Processing (NLP), Twitter data is used for sentiment analysis and it is most prevalent theme in recent era. However, the security attacks on the Twitter data have been increased by hackers which reduced the performance of the sentiment analysis. Thus to detect the malicious activities in the Twitter data, a novel Spider Monkey based Generalized Intelligent (SMbGI) framework is developed in this paper. This model utilizes Twitter-based data about the coronavirus disease 2019 (COVID-19) to detect the malware activities for improving the classification of sentiments. Moreover, this model imposed a malicious attack on the data for recognizing the developed SMbGI model efficiencies. Thus, the proposed SMbGI approach has been effectually detecting malicious functions and enhances sentiment classification. Moreover, Python tool is used for sentiment analysis, and it computed the parameters like accuracy, recall, precision, F-measure, and error rate. Lastly, the attained outcomes are compared with recent existing works to identify the performance of the SMbGI approach.",
        "DOI": "NA",
        "paper_author": "Narasamma V.L.",
        "affiliation_name": "K L Deemed to be University",
        "affiliation_city": "Vaddeswaram",
        "affiliation_country": "India",
        "affiliation_id": "60079446",
        "affiliation_state": "AP"
    },
    {
        "paper_title": "Dancing along Battery: Enabling Transformer with Run-time Reconfigurability on Mobile Devices",
        "publication": "Proceedings - Design Automation Conference",
        "citied_by": "12",
        "cover_date": "2021-12-05",
        "Abstract": "A pruning-based AutoML framework for run-time reconfigurability, namely RT3, is proposed in this work. This enables Transformer-based large Natural Language Processing (NLP) models to be efficiently executed on resource-constrained mobile devices and reconfigured (i.e., switching models for dynamic hardware conditions) at run-time. Such reconfigurability is the key to save energy for battery-powered mobile devices, which widely use dynamic voltage and frequency scaling (DVFS) technique for hardware reconfiguration to prolong battery life. In this work, we creatively explore a hybrid block-structured pruning (BP) and pattern pruning (PP) for Transformer-based models and first attempt to combine hardware and software reconfiguration to maximally save energy for battery-powered mobile devices. Specifically, RT3 integrates two-level optimizations: First, it utilizes an efficient BP as the first-step compression for resource-constrained mobile devices; then, RT3 heuristically generates a shrunken search space based on the first level optimization and searches multiple pattern sets with diverse sparsity for PP via reinforcement learning to support lightweight software reconfiguration, which corresponds to available frequency levels of DVFS (i.e., hardware reconfiguration). At run-time, RT3 can switch the lightweight pattern sets within 45ms to guarantee the required real-time constraint at different frequency levels. Results further show that RT3 can prolong battery life over 4× improvement with less than 1% accuracy loss for Transformer and 1.5% score decrease for DistilBERT.",
        "DOI": "10.1109/DAC18074.2021.9586295",
        "paper_author": "Song Y.",
        "affiliation_name": "East China Normal University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60021200",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Explaining Deep Learning using examples: Optimal feature weighting methods for twin systems using post-hoc, explanation-by-example in XAI",
        "publication": "Knowledge-Based Systems",
        "citied_by": "34",
        "cover_date": "2021-12-05",
        "Abstract": "In this paper, the twin-systems approach is reviewed, implemented, and competitively tested as a post-hoc explanation-by-example solution to the eXplainable Artificial Intelligence (XAI) problem. In twin-systems, an opaque artificial neural network (ANN) is explained by “twinning” it with a more interpretable case-based reasoning (CBR) system, by mapping the feature weights from the former to the latter. Extensive comparative tests are performed, over four experiments, to determine the optimal feature-weighting method for such twin-systems. Twin-systems for traditional multilayer perceptron (MLP) networks (MLP–CBR twins), convolutional neural networks (CNNs; CNN–CBR twins), and transformers for NLP (BERT–CBR twins) are examined. In addition, Feature Activation Maps (FAMs) are explored to enhance explainability by providing an additional layer of explanatory insight. The wider implications of this research on XAI is discussed, and a code library is provided to ease replicability.",
        "DOI": "10.1016/j.knosys.2021.107530",
        "paper_author": "Kenny E.M.",
        "affiliation_name": "University College Dublin",
        "affiliation_city": "Dublin",
        "affiliation_country": "Ireland",
        "affiliation_id": "60005141",
        "affiliation_state": "Leinster"
    },
    {
        "paper_title": "Hybrid deep learning of social media big data for predicting the evolution of COVID-19 transmission",
        "publication": "Knowledge-Based Systems",
        "citied_by": "30",
        "cover_date": "2021-12-05",
        "Abstract": "In this study, a hybrid deep-learning model termed as ODANN, built upon neural networks (NN) coupled with data assimilation and natural language processing (NLP) features extraction methods, has been constructed to concurrently process daily COVID-19 time-series records and large volumes of COVID-19 related Twitter data, as representative of the global community's aggregated emotional responses towards the current pandemic, to model the growth rate in the number of confirmed COVID-19 cases globally via a proposed G parameter. Overall, there were 3 key components to ODANN's development phase, namely: (i) data hydration and pre-processing were performed on COVID-19 related Twitter data ranging between 23 January 2020 and 10 May 2020, which amounted to over 100 million Tweets written in English language; (ii) multiple NLP features extraction methods were subsequently leveraged to encode the hydrated Twitter data into useful semantic word vectors for training ODANN under an optimal set of hyperparameters; and (iii) historical time-series data of defined characteristics were also assimilated into ODANN's selected hidden layer(s) to model the G parameter daily with a lead-time of 1 day. By far, our experimental results demonstrated that by adopting a rolling time-window size of 5 days, with respect to the number of historical time-series records for assimilating different data features, enabled ODANN to outperform other traditional time-series models and recent studies, in terms of the computed RMSE and MAE scores attained from the model's testing step. Overall, the summarized results from ODANN demonstrated its competitive edge in modelling and forecasting the growth rate in the number of COVID-19 cases globally.",
        "DOI": "10.1016/j.knosys.2021.107417",
        "paper_author": "Chew A.W.Z.",
        "affiliation_name": "Bentley Systems, Incorporated",
        "affiliation_city": "Exton",
        "affiliation_country": "United States",
        "affiliation_id": "60078348",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "Mining Latent Semantic Correlation inspired by Qantum Entanglement",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2021-12-04",
        "Abstract": "Text representation learning is the cornerstone of solving downstream problems in Natural Language Processing (NLP). However, mining the potential explanatory factors or semantic associations behind data, rather than simply representing the superficial co-occurrence of words, remains a non-trivial challenge. To this end, we seek inspiration from the Quantum Entanglement (QE) which can effectively provide a complete description for the nature of realities and a globally-determined intrinsic correlation of considered objects, thus proposing a novel representation learning hypothesis called the Latent Semantic Correlation (LSC), namely the implicit internal coherence between the semantic space and its corresponding category space. To construct a multi-granularity representation from sememes to words, phrases, sentences, and higher-level LSC, we implement a QE-inspired Network (QEN) under the constraints of quantum formalism and propose the Local Semantic Measurement (LSM) and Extraction (LSE) for effectively capturing probability distribution information from the entangled state of a bipartite quantum system, which has a clear geometrical motivation but also supports a well-founded probabilistic interpretation. Experimental results conducted on several benchmarking classification tasks prove the validity of the LSC hypothesis and the superiority of QEN.",
        "DOI": "10.1145/3507548.3507598",
        "paper_author": "Li Z.",
        "affiliation_name": "Tianjin University",
        "affiliation_city": "Tianjin",
        "affiliation_country": "China",
        "affiliation_id": "60019533",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Machine learning adversarial attacks: A survey beyond",
        "publication": "Machine Learning Techniques and Analytics for Cloud Security",
        "citied_by": "3",
        "cover_date": "2021-12-03",
        "Abstract": "Machine Learning (ML) has fascinated researchers and developers to an extent that it has been now considered as an astute to most of them. To this continuation, ML integrated with Natural Language Processing (NLP) has yielded extraordinary results in many fields. ML began to splutter in early 2000 and has now been to its zenith. It has radically proved to be sure-fire to the human level performance in many tasks. Many huge companies like Google and Amazon are providing Cloud-based ML as a service (MLaaS) which can be efficiently used by an end user to perform numerous tasks in fields like NLP and Recommender systems. However, these services and their applications like sentiment analysis and text entailment are prone to adversarial attacks which force them to misclassify. This issue has been considered in depth in this chapter with analysis based on comparative study of some of the popular text-based attacks. The chapter also focuses on attacks, namely, in domains like Image, Cloud-based services, and Recommender-based systems.",
        "DOI": "10.1002/9781119764113.ch13",
        "paper_author": "Magoo C.",
        "affiliation_name": "J.C. Bose University of Science and Technology",
        "affiliation_city": "Faridabad",
        "affiliation_country": "India",
        "affiliation_id": "122406539",
        "affiliation_state": "HR"
    },
    {
        "paper_title": "Mapping CKC model through NLP modelling for APT groups reports",
        "publication": "Handbook of Big Data Analytics and Forensics",
        "citied_by": "2",
        "cover_date": "2021-12-02",
        "Abstract": "We are seeing a constant increase in the number of cyberattacks with both monetary and political motives behind them. As such it becomes crucial to identify Advanced Persistent Threats (APT) groups for future risk mitigation by both business and government. Multiple vendors like McAfee and Kaspersky periodically release reports on these APT groups that are absorbed by the security analysts worldwide. These reports identify the Tactics, Techniques, and Procedures (TTPs) used by the threat actors to carry out their operation. One important information to distill from these APT reports is the different stages of operation used by these APT groups. One such framework for classification of various stages in a cyberattack is Cyber Kill Chain (CKC) which was developed by Lockheed Martin to study and differentiate the various cyberattacks. Usually the process of identification of CKC stages in APT reports is a manual and time taking process. In this paper we have proposed the use of semantic search using Latent Semantic Indexing (LSI) and Latent Dirichlet Allocation (LDA) to automatically extract CKC stages from unstructured APT reports. We then compare our results with an existing research that has done such classification manually.",
        "DOI": "10.1007/978-3-030-74753-4_16",
        "paper_author": "Upadhyay A.",
        "affiliation_name": "University of Guelph",
        "affiliation_city": "Guelph",
        "affiliation_country": "Canada",
        "affiliation_id": "60015881",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "Ontological multi-agent system for pandemic zone classification and automated policy formulation",
        "publication": "Intelligent Information Retrieval for Healthcare Systems",
        "citied_by": "0",
        "cover_date": "2021-12-02",
        "Abstract": "This chapter deals with in depth analysis and design of a multi-agent system supported by a thorough knowledge base, Covid19_ONT, developed and enriched in Protégé 5.2.0. NLP is used to extract the information regarding COVID-19 from valuable sources like mygov.in, who.int, nhs.uk and others. India introduced the concept of zones according to the degree and magnitude of the disease. The red, orange and green zones were demarcated through the districts at the lowest level. A district at any time can be in one of the zones and it is also possible that the district might wave between the zones according to the number of active cases. Each of the zones has its own rules to be imposed, called policies. This chapter allows for systematic allotment of zones to the districts and automated policy formulation for the districts by using ontological inference done through the Drool's reasoner.",
        "DOI": "NA",
        "paper_author": "Shankhdhar G.K.",
        "affiliation_name": "Babu Banarasi Das University",
        "affiliation_city": "Lucknow",
        "affiliation_country": "India",
        "affiliation_id": "60097460",
        "affiliation_state": "UP"
    },
    {
        "paper_title": "NLP: Smart texting system based on Android",
        "publication": "Intelligent Information Retrieval for Healthcare Systems",
        "citied_by": "0",
        "cover_date": "2021-12-02",
        "Abstract": "Designing a device like a human which shows human behavior, particularly the capacity of speaking and responding naturally, increase the interest of engineers and scientist. Since the 1930s, the problem of speech and text recognition has been approached progressively because a bell lab scientist HOMER-DUDLEY proposed a system model for speech recognition. For natural language, we need to convert a simple device or machine into a well-sophisticated machine so that a speech can effectively produce. In this, we mainly focus on major points in the development of automatic speech and text recognition. Our text conversion is also a factor used in speech recognition for the last few years. Technique to process improved text-to-speech is disclosed, this improved technology converts the electronic document of our text into audio output which includes both speeches associated with our text as well as from our audio clip. Applying NLP-based data extraction techniques can automatically help the users. Cues to the listener when m/c produced output in a speech about a document. Before a text-to-speech conversion, our audio contextual cue is based on a document. The audio file of a document can be given to the user so that the user can hear all the documents rather than reading text.",
        "DOI": "NA",
        "paper_author": "Chauhan F.",
        "affiliation_name": "Sharda University",
        "affiliation_city": "Greater Noida",
        "affiliation_country": "India",
        "affiliation_id": "60108680",
        "affiliation_state": "UP"
    },
    {
        "paper_title": "Automated Identification of Clinical Procedures in Free-Text Electronic Clinical Records with a Low-Code Named Entity Recognition Workflow",
        "publication": "Methods of Information in Medicine",
        "citied_by": "2",
        "cover_date": "2021-12-02",
        "Abstract": "Background Clinical procedures are often performed in outpatient clinics without prior scheduling at the administrative level, and documentation of the procedure often occurs solely in free-text clinical electronic notes. Natural language processing (NLP), particularly named entity recognition (NER), may provide a solution to extracting procedure data from free-text electronic notes. Methods Free-text notes from outpatient ophthalmology visits were collected from the electronic clinical records at a single institution over 3 months. The Prodigy low-code annotation tool was used to create an annotation dataset and train a custom NER model for clinical procedures. Clinical procedures were extracted from the entire set of clinical notes. Results There were a total of 5,098 clinic notes extracted for the study period; 1,923 clinic notes were used to build the NER model, which included a total of 231 manual annotations. The NER model achieved an F-score of 0.767, a precision of 0.810, and a recall of 0.729. The most common procedures performed included intravitreal injections of therapeutic substances, removal of corneal foreign bodies, and epithelial debridement of corneal ulcers. Conclusion The use of a low-code annotation software tool allows the rapid creation of a custom annotation dataset to train a NER model to identify clinical procedures stored in free-text electronic clinical notes. This enables clinicians to rapidly gather previously unidentified procedural data for quality improvement and auditing purposes. Low-code annotation tools may reduce time and coding barriers to clinician participation in NLP research.",
        "DOI": "10.1055/s-0042-1749358",
        "paper_author": "Macri C.",
        "affiliation_name": "The University of Adelaide",
        "affiliation_city": "Adelaide",
        "affiliation_country": "Australia",
        "affiliation_id": "60009512",
        "affiliation_state": "SA"
    },
    {
        "paper_title": "Signal Phrase Extraction: Gateway to Information Retrieval Improvement in Law Texts",
        "publication": "Frontiers in Artificial Intelligence and Applications",
        "citied_by": "1",
        "cover_date": "2021-12-02",
        "Abstract": "NLP-based techniques can support in improving understanding of legal text documents. In this work we present a semi-automatic framework to extract signal phrases from legislative texts for an arbitrary European language. Through a case study using Dutch legislation, we demonstrate that it is feasible to extract these phrases reliably with a small number of supporting domain experts. Finally, we argue how in future works our framework could be utilized with existing methods to be applied to different languages.",
        "DOI": "10.3233/FAIA210327",
        "paper_author": "Van Der Veen M.",
        "affiliation_name": "Technische Universiteit Eindhoven",
        "affiliation_city": "Eindhoven",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60032882",
        "affiliation_state": "Noord-Brabant"
    },
    {
        "paper_title": "Hybrid AI Framework for Legal Analysis of the EU Legislation Corrigenda",
        "publication": "Frontiers in Artificial Intelligence and Applications",
        "citied_by": "7",
        "cover_date": "2021-12-02",
        "Abstract": "This paper presents an AI use-case developed in the project 'Study on legislation in the era of artificial intelligence and digitization' promoted by the EU Commission Directorate-General for Informatics. We propose a hybrid technical framework where AI techniques, Data Analytics, Semantic Web approaches and LegalXML modelisation produce benefits in legal drafting activity. This paper aims to classify the corrigenda of the EU legislation with the goal to detect some criteria that could prevent errors during the drafting or during the publication process. We use a pipeline of different techniques combining AI, NLP, Data Analytics, Semantic annotation and LegalXML instruments for enriching the non-symbolic AI tools with legal knowledge interpretation to offer to the legal experts.",
        "DOI": "10.3233/FAIA210319",
        "paper_author": "Palmirani M.",
        "affiliation_name": "Alma Mater Studiorum Università di Bologna",
        "affiliation_city": "Bologna",
        "affiliation_country": "Italy",
        "affiliation_id": "60028218",
        "affiliation_state": "BO"
    },
    {
        "paper_title": "Towards Reducing the Pendency of Cases at Court: Automated Case Analysis of Supreme Court Judgments in India",
        "publication": "Frontiers in Artificial Intelligence and Applications",
        "citied_by": "0",
        "cover_date": "2021-12-02",
        "Abstract": "The Indian court system generates huge amounts of data relating to administration, pleadings, litigant behaviour, and court decisions on a regular basis. But the existing Judiciary is incapable of managing these vast troves of data efficiently that causes delays and pendency of a large volume of cases in the courts. Some of these time-consuming tasks involve case briefing, examining the legal issues, facts, legal principles, observations, and other significant aspects submitted by the contending parties in the court. In other words, computational methods to understand the underlying structure of a case document will directly aid the lawyers to perform these tasks efficiently and improve the overall efficiency of the Justice delivery system. Application of Computational techniques (such as Natural Language Processing) can help to gather and sift through these vast troves of information, identify patterns, extract the document structure, draft documents and make the information available online. Traditionally lawyers are trained to examine cases using the Case Law Analysis approach for case briefing. In this article, the authors aim to establish the importance and relevance of the automated case analysis problem in the legal domain. They introduce a novel case analysis structure for the supreme court judgment documents and define twelve different case law labels that are used by legal professionals to identify the structure. Finally the authors propose a method for automated case analysis, which will directly aid the lawyers to prepare speedy and efficient case briefs and drastically reduce the time taken by them in litigation.",
        "DOI": "10.3233/FAIA210321",
        "paper_author": "Pandey S.",
        "affiliation_name": "Indian Institute of Technology Kharagpur",
        "affiliation_city": "Kharagpur",
        "affiliation_country": "India",
        "affiliation_id": "60004750",
        "affiliation_state": "WB"
    },
    {
        "paper_title": "TNNT: The Named Entity Recognition Toolkit",
        "publication": "K-CAP 2021 - Proceedings of the 11th Knowledge Capture Conference",
        "citied_by": "2",
        "cover_date": "2021-12-02",
        "Abstract": "Extraction of categorised named entities from text is a complex task given the availability of a variety of Named Entity Recognition (NER) models and the unstructured information encoded in different source document formats. Processing the documents to extract text, identifying suitable NER models for a task, and obtaining statistical information is important in data analysis to make informed decisions. This paper presents\\footnoteThe manuscript follows guidelines to showcase a demonstration that introduces an overview of how the toolkit works: input document set, initial settings, processing, and output set. The input document set is artificial in order to show various toolkit capabilities. TNNT, a toolkit that automates the extraction of categorised named entities from unstructured information encoded in source documents, using diverse state-of-the-art (SOTA) Natural Language Processing (NLP) tools and NER models.TNNT integrates 21 different NER models as part of a Knowledge Graph Construction Pipeline (KGCP) that takes a document set as input and processes it based on the defined settings, applying the selected blocks of NER models to output the results. The toolkit generates all results with an integrated summary of the extracted entities, enabling enhanced data analysis to support the KGCP, and also, to aid further NLP tasks.",
        "DOI": "10.1145/3460210.3493550",
        "paper_author": "Seneviratne S.",
        "affiliation_name": "The Australian National University",
        "affiliation_city": "Canberra",
        "affiliation_country": "Australia",
        "affiliation_id": "60008950",
        "affiliation_state": "ACT"
    },
    {
        "paper_title": "Identification of discriminative gene-level and protein-level features associated with pathogenic gain-of-function and loss-of-function variants",
        "publication": "American Journal of Human Genetics",
        "citied_by": "25",
        "cover_date": "2021-12-02",
        "Abstract": "Identifying whether a given genetic mutation results in a gene product with increased (gain-of-function; GOF) or diminished (loss-of-function; LOF) activity is an important step toward understanding disease mechanisms because they may result in markedly different clinical phenotypes. Here, we generated an extensive database of documented germline GOF and LOF pathogenic variants by employing natural language processing (NLP) on the available abstracts in the Human Gene Mutation Database. We then investigated various gene- and protein-level features of GOF and LOF variants and applied machine learning and statistical analyses to identify discriminative features. We found that GOF variants were enriched in essential genes, for autosomal-dominant inheritance, and in protein binding and interaction domains, whereas LOF variants were enriched in singleton genes, for protein-truncating variants, and in protein core regions. We developed a user-friendly web-based interface that enables the extraction of selected subsets from the GOF/LOF database by a broad set of annotated features and downloading of up-to-date versions. These results improve our understanding of how variants affect gene/protein function and may ultimately guide future treatment options.",
        "DOI": "10.1016/j.ajhg.2021.10.007",
        "paper_author": "Sevim Bayrak C.",
        "affiliation_name": "Icahn School of Medicine at Mount Sinai",
        "affiliation_city": "New York",
        "affiliation_country": "United States",
        "affiliation_id": "60012981",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Temporal transitions in the post-mitotic nervous system of Caenorhabditis elegans",
        "publication": "Nature",
        "citied_by": "31",
        "cover_date": "2021-12-02",
        "Abstract": "In most animals, the majority of the nervous system is generated and assembled into neuronal circuits during embryonic development1. However, during juvenile stages, nervous systems still undergo extensive anatomical and functional changes to eventually form a fully mature nervous system by the adult stage2,3. The molecular changes in post-mitotic neurons across post-embryonic development and the genetic programs that control these temporal transitions are not well understood4,5. Here, using the model system Caenorhabditis elegans, we comprehensively characterized the distinct functional states (locomotor behaviour) and the corresponding distinct molecular states (transcriptome) of the post-mitotic nervous system across temporal transitions during post-embryonic development. We observed pervasive, neuron-type-specific changes in gene expression, many of which are controlled by the developmental upregulation of the conserved heterochronic microRNA LIN-4 and the subsequent promotion of a mature neuronal transcriptional program through the repression of its target, the transcription factor lin-14. The functional relevance of these molecular transitions are exemplified by a temporally regulated target gene of the LIN-14 transcription factor, nlp-45, a neuropeptide-encoding gene, which we find is required for several distinct temporal transitions in exploratory activity during post-embryonic development. Our study provides insights into regulatory strategies that control neuron-type-specific gene batteries to modulate distinct behavioural states across temporal, sexual and environmental dimensions of post-embryonic development.",
        "DOI": "10.1038/s41586-021-04071-4",
        "paper_author": "Sun H.S.",
        "affiliation_name": "Columbia University",
        "affiliation_city": "New York",
        "affiliation_country": "United States",
        "affiliation_id": "60030162",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Diversification of Time-Varying Tangency Portfolio under Nonlinear Constraints through Semi-Integer Beetle Antennae Search Algorithm",
        "publication": "AppliedMath",
        "citied_by": "8",
        "cover_date": "2021-12-01",
        "Abstract": "In finance, the most efficient portfolio is the tangency portfolio, which is formed by the intersection point of the efficient frontier and the capital market line. This paper defines and explores a time-varying tangency portfolio under nonlinear constraints (TV-TPNC) problem as a nonlinear programming (NLP) problem. Because meta-heuristics are commonly used to solve NLP problems, a semi-integer beetle antennae search (SIBAS) algorithm is proposed for solving cardinality constrained NLP problems and, hence, to solve the TV-TPNC problem. The main results of numerical applications in real-world datasets demonstrate that our method is a splendid substitute for other evolutionary methods.",
        "DOI": "10.3390/appliedmath1010005",
        "paper_author": "Katsikis V.N.",
        "affiliation_name": "National and Kapodistrian University of Athens",
        "affiliation_city": "Athens",
        "affiliation_country": "Greece",
        "affiliation_id": "60028900",
        "affiliation_state": "Attica"
    },
    {
        "paper_title": "Towards structured NLP interpretation via graph explainers",
        "publication": "Applied AI Letters",
        "citied_by": "3",
        "cover_date": "2021-12-01",
        "Abstract": "Natural language processing (NLP) models have been increasingly deployed in real-world applications, and interpretation for textual data has also attracted dramatic attention recently. Most existing methods generate feature importance interpretation, which indicate the contribution of each word towards a specific model prediction. Text data typically possess highly structured characteristics and feature importance explanation cannot fully reveal the rich information contained in text. To bridge this gap, we propose to generate structured interpretations for textual data. Specifically, we pre-process the original text using dependency parsing, which could transform the text from sequences into graphs. Then graph neural networks (GNNs) are utilized to classify the transformed graphs. In particular, we explore two kinds of structured interpretation for pre-trained GNNs: edge-level interpretation and subgraph-level interpretation. Experimental results over three text datasets demonstrate that the structured interpretation can better reveal the structured knowledge encoded in the text. The experimental analysis further indicates that the proposed interpretations can faithfully reflect the decision-making process of the GNN model.",
        "DOI": "10.1002/ail2.58",
        "paper_author": "Yuan H.",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "College Station",
        "affiliation_country": "United States",
        "affiliation_id": "60148980",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "A Natural Language Interface to Relational Databases Using an Online Analytic Processing Hypercube",
        "publication": "AI (Switzerland)",
        "citied_by": "5",
        "cover_date": "2021-12-01",
        "Abstract": "Structured Query Language (SQL) is commonly used in Relational Database Management Systems (RDBMS) and is currently one of the most popular data definition and manipulation languages. Its core functionality is implemented, with only some minor variations, throughout all RDBMS products. It is an effective tool in the process of managing and querying data in relational databases. This paper describes a method to effectively automate the conversion of a data query from a Natural Language Query (NLQ) to Structured Query Language (SQL) with Online Analytical Processing (OLAP) cube data warehouse objects. To obtain or manipulate the data from relational databases, the user must be familiar with SQL and must also write an appropriate and valid SQL statement. However, users who are not familiar with SQL are unable to obtain relevant data through relational databases. To address this, we propose a Natural Language Processing (NLP) model to convert an NLQ into an SQL query. This allows novice users to obtain the required data without having to know any complicated SQL details. The model is also capable of handling complex queries using the OLAP cube technique, which allows data to be pre-calculated and stored in a multi-dimensional and ready-to-use format. A multi-dimensional cube (hypercube) is used to connect with the NLP interface, thereby eliminating long-running data queries and enabling self-service business intelligence. The study demonstrated how the use of hypercube technology helps to increase the system response speed and the ability to process very complex query sentences. The system achieved impressive performance in terms of NLP and the accuracy of generating different query sentences. Using OLAP hypercube technology, the study achieved distinguished results compared to previous studies in terms of the speed of the response of the model to NLQ analysis, the generation of complex SQL statements, and the dynamic display of the results. As a plan for future work, it is recommended to use infinite-dimension (n-D) cubes instead of 4-D cubes to enable ingesting as much data as possible in a single object and to facilitate the execution of query statements that may be too complex in query interfaces running in a data warehouse. The study demonstrated how the use of hypercube technology helps to increase system response speed and process very complex query sentences.",
        "DOI": "10.3390/ai2040043",
        "paper_author": "Hazboun F.H.",
        "affiliation_name": "Arab American University, Palestine",
        "affiliation_city": "Jenin",
        "affiliation_country": "Palestine",
        "affiliation_id": "60108982",
        "affiliation_state": "West Bank"
    },
    {
        "paper_title": "Improving Readability of Online Privacy Policies through DOOP: A Domain Ontology for Online Privacy",
        "publication": "Digital",
        "citied_by": "6",
        "cover_date": "2021-12-01",
        "Abstract": "Privacy policies play an important part in informing users about their privacy concerns by operating as memorandums of understanding (MOUs) between them and online services providers. Research suggests that these policies are infrequently read because they are often lengthy, written in jargon, and incomplete, making them difficult for most users to understand. Users are more likely to read short excerpts of privacy policies if they pertain directly to their concern. In this paper, a novel approach and a proof-of-concept tool are proposed that reduces the amount of privacy policy text a user has to read. It does so using a domain ontology and natural language processing (NLP) to identify key areas of the policies that users should read to address their concerns and take appropriate action. Using the ontology to locate key parts of privacy policies, average reading times were substantially reduced from 29–32 min to 45 s.",
        "DOI": "10.3390/digital1040015",
        "paper_author": "Audich D.A.",
        "affiliation_name": "University of Guelph",
        "affiliation_city": "Guelph",
        "affiliation_country": "Canada",
        "affiliation_id": "60015881",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "AutoGenome: An AutoML tool for genomic research",
        "publication": "Artificial Intelligence in the Life Sciences",
        "citied_by": "5",
        "cover_date": "2021-12-01",
        "Abstract": "Deep learning has achieved great successes in traditional fields like computer vision (CV), natural language processing (NLP), speech processing, and more. These advancements have greatly inspired researchers in genomics and made deep learning in genomics an exciting and popular topic. The convolutional neural network (CNN) and recurrent neural network (RNN) are frequently used to solve genomic sequencing and prediction problems, and multiple layer perception (MLP) and auto-encoders (AE) are frequently used for genomic profiling data like RNA expression data and gene mutation data. Here, we introduce a new neural network architecture-the residual fully-connected neural network (RFCN)-and describe its advantage in modeling genomic profiling data. We also incorporate AutoML algorithms and implement AutoGenome, an end-to-end, automated deep learning framework for genomic studies. By utilizing the proposed RFCN architecture, automatic hyper-parameter search, and neural architecture search algorithms, AutoGenome can automatically train high-performance deep learning models for various kinds of genomic profiling data. To help researchers better understand the trained models, AutoGenome can assess the importance of different features and export the most critical features for supervised learning tasks and the representative latent vectors for unsupervised learning tasks. We expect AutoGenome will become a popular tool in genomic studies.",
        "DOI": "10.1016/j.ailsci.2021.100017",
        "paper_author": "Liu D.",
        "affiliation_name": "Huawei Technologies Co., Ltd.",
        "affiliation_city": "Shenzhen",
        "affiliation_country": "China",
        "affiliation_id": "60092530",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Modeling multi-prototype Chinese word representation learning for word similarity",
        "publication": "Complex and Intelligent Systems",
        "citied_by": "2",
        "cover_date": "2021-12-01",
        "Abstract": "The word similarity task is used to calculate the similarity of any pair of words, and is a basic technology of natural language processing (NLP). The existing method is based on word embedding, which fails to capture polysemy and is greatly influenced by the quality of the corpus. In this paper, we propose a multi-prototype Chinese word representation model (MP-CWR) for word similarity based on synonym knowledge base, including knowledge representation module and word similarity module. For the first module, we propose a dual attention to combine semantic information for jointly learning word knowledge representation. The MP-CWR model utilizes the synonyms as prior knowledge to supplement the relationship between words, which is helpful to solve the challenge of semantic expression due to insufficient data. As for the word similarity module, we propose a multi-prototype representation for each word. Then we calculate and fuse the conceptual similarity of two words to obtain the final result. Finally, we verify the effectiveness of our model on three public data sets with other baseline models. In addition, the experiments also prove the stability and scalability of our MP-CWR model under different corpora.",
        "DOI": "10.1007/s40747-021-00482-y",
        "paper_author": "Yin F.",
        "affiliation_name": "Communication University of China",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60021427",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Dubai Restaurants: A Sentiment Analysis of Tourist Reviews",
        "publication": "Academica Turistica",
        "citied_by": "8",
        "cover_date": "2021-12-01",
        "Abstract": "An enormous amount of information is available on innumerable travel websites, social media and blogs, of which a large part is user-generated content. This web content holds great potential to assess visitor sentiment at a destination; as this identifies a need for building automated systems to extract unknown sentiments from these sources. Sentiment analysis, which includes text mining and natural language processing (nlp) techniques, helps in extracting related sentiments from the data thus stored, in unstructured formats. The extracted sentiment would facilitate better tourist decision making and improve customer service and new product development for tourism enterprises. This study presents a sentiment analysis model to extract the hidden sentiments from tourist reviews about restaurants in Dubai that will guide visitors to the city in taking suitable dining decisions. Sentiment analysis is carried out by extracting tourist reviews about restaurants in Dubai using a web scraping method using text mining techniques with the help of the R statistical software package. The resultant data is further analysed by sentiment analysis tools to extract the hidden sentiments, which are categorized under eight heads. The sentiment analysis helped uncover hidden sentiments along with the frequency of each sentiment category. It also helped to find the difference between tourist sentiment scores with respect to different categories of restaurants. The paper provides a sentiment analysis model which can be used in the future to extract the reviews related to other tourism products besides restaurants, such as accommodation, attractions and accessibility.",
        "DOI": "10.26493/2335-4194.14.165-174",
        "paper_author": "Renganathan V.",
        "affiliation_name": "Banasthali Vidyapith",
        "affiliation_city": "Vanasthali",
        "affiliation_country": "India",
        "affiliation_id": "60028153",
        "affiliation_state": "RJ"
    },
    {
        "paper_title": "Modern Natural Language Processing Technologies for Strategic Analytics",
        "publication": "Scientific and Technical Information Processing",
        "citied_by": "1",
        "cover_date": "2021-12-01",
        "Abstract": "Abstract: This paper provides an overview of the latest natural language processing (NLP) technologies that can be applied in strategic analytics. The main problems in this field and specific tasks that can be solved using NLP tools are investigated. The main areas of application of these tools are considered. Recent advancements in NLP are discussed and their potential is assessed. In conclusion, we outline the directions in which the NLP apparatus should be developed to meet the needs of strategic analytics.",
        "DOI": "10.3103/S0147688221060071",
        "paper_author": "Kuzminov I.F.",
        "affiliation_name": "HSE University",
        "affiliation_city": "Moscow",
        "affiliation_country": "Russian Federation",
        "affiliation_id": "60020513",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "NukeLM: Pre-Trained and Fine-Tuned Language Models for the Nuclear and Energy Domains",
        "publication": "ESARDA Bulletin",
        "citied_by": "0",
        "cover_date": "2021-12-01",
        "Abstract": "Natural language processing (NLP) tasks (text classification, named entity recognition, etc.) have seen revolutionary improvements over the last few years. This is due to language models such as BERT that achieve deep knowledge transfer by using a large pre-trained model, then fine-tuning the model on specific tasks. The BERT architecture has shown even better performance on domain-specific tasks when the model is pre-trained using domain-relevant texts. Inspired by these recent advancements, we have developed NukeLM, a nucleardomain language model pre-trained on 1.5 million abstracts from the U.S. Department of Energy Office of Scientific and Technical Information (OSTI) database. This NukeLM model is then fine-tuned for the classification of research articles into either binary classes (related to the nuclear fuel cycle [NFC] or not) or multiple categories related to the subject of the article. We show that continued pre-training of a BERT-style architecture prior to fine-tuning yields greater performance on both article classification tasks. This information is critical for properly triaging manuscripts, a necessary task for better understanding citation networks that publish in the nuclear space, and for uncovering new areas of research in the nuclear (or nuclear-relevant) domains.",
        "DOI": "10.3011/ESARDA.IJNSNP.2021.9",
        "paper_author": "Burke L.",
        "affiliation_name": "Pacific Northwest National Laboratory",
        "affiliation_city": "Richland",
        "affiliation_country": "United States",
        "affiliation_id": "60023471",
        "affiliation_state": "WA"
    },
    {
        "paper_title": "Artificial Judgement Assistance from teXt (AJAX): Applying Open Domain Question Answering to Nuclear Non-proliferation Analysis",
        "publication": "ESARDA Bulletin",
        "citied_by": "1",
        "cover_date": "2021-12-01",
        "Abstract": "Nuclear non-proliferation analysis is complex and subjective, as the data is sparse, and examples are rare and diverse. While analysing non-proliferation data, it is often desired that the findings be completely auditable such that any claim or assertion can be sourced directly to the reference material from which it was derived. Currently this is accomplished by analysts thoroughly documenting underlying assumptions and clearly referencing details to source documents. This is a labour-intensive and timeconsuming process that can be difficult to scale with geometrically increasing quantities of data. In this work, we describe an approach to leverage bi-directional language models for nuclear non-proliferation analysis. It has been shown recently that these models not only capture language syntax but also some of the relational knowledge present in the training data. We have devised a unique Salt and Pepper strategy for testing the knowledge present in the language models, while also introducing auditability function in our pipeline. We demonstrate that fine-tuning the bi-directional language models on domain specific corpus improves their ability to answer domain-specific factoid questions. Our hope is that the results presented in this paper will further the natural language processing (NLP) field by introducing the ability to audit the answers provided by the language models to bring forward the source of said knowledge.",
        "DOI": "10.3011/ESARDA.IJNSNP.2021.10",
        "paper_author": "Wilson B.",
        "affiliation_name": "Pacific Northwest National Laboratory",
        "affiliation_city": "Richland",
        "affiliation_country": "United States",
        "affiliation_id": "60023471",
        "affiliation_state": "WA"
    },
    {
        "paper_title": "Relational Reasoning Using Neural Networks: A Survey",
        "publication": "International Journal of Uncertainty, Fuzziness and Knowldege-Based Systems",
        "citied_by": "12",
        "cover_date": "2021-12-01",
        "Abstract": "Relational Networks (RN), as one of the most widely used relational reasoning techniques, have achieved great success in many applications such as action and image analysis, speech recognition and text understanding. The use of relational reasoning via RN in neural networks has often been used in recent years. In these instances, RN is composed of various deep learning-based algorithms in simple plug-and-play modules. This is quite advantageous since it circumvents the need for features engineering. This paper surveys the emerging research of deep learning models that make use of RN in tasks such as Natural Language Processing (NLP), Action Recognition, Temporal Relational Reasoning as well as Facial Emotion Recognition (FER). Since, RNs are easy to integrate they have been used in various tasks such as NLP, Recurrent Neural Networks (RNN), Action Recognition, Image Analysis, Object Detection, Temporal Relational Reasoning, as well as for FER. This is due to the fact that RNs use bidirectional LSTM and CNN to solve relational reasoning problems at character and word level. In this paper a comparative review of all relational reasoning-based RN models using deep learning techniques is presented.",
        "DOI": "10.1142/S0218488521400134",
        "paper_author": "Pise A.A.",
        "affiliation_name": "University of the Witwatersrand, Johannesburg",
        "affiliation_city": "Johannesburg",
        "affiliation_country": "South Africa",
        "affiliation_id": "60016218",
        "affiliation_state": "Gauteng"
    },
    {
        "paper_title": "Modern Approaches to Detecting and Classifying Toxic Comments Using Neural Networks",
        "publication": "Automatic Control and Computer Sciences",
        "citied_by": "1",
        "cover_date": "2021-12-01",
        "Abstract": "Abstract—: The rising popularity of online platforms on which users communicate with each other, share opinions about various events, and leave comments has spurred on the development of natural language processing algorithms. Content moderation requires analyzing tens of millions of messages published by users of a given social network daily in real time, in order to prevent the spread of various illegal or offensive information, threats, and other types of toxic comments. Of course, such a large amount of data can be processed quickly enough only automatically. That leads to the problem of teaching computers to “understand” human written speech, which is nontrivial even if understand here means nothing more than classify. The rapid evolution of machine learning technologies has led to ubiquitous implementation of new algorithms. With the use of deep learning technologies, we are now able to quite successfully solve many problems that had for years been considered almost impossible. This article considers algorithms constructed using deep learning technologies and neural networks that solve the problem of detecting and classifying toxic comments. In addition, the article presents the results of testing both the developed algorithms and an ensemble of all considered algorithms on a large training set collected and tagged by Google and Jigsaw.",
        "DOI": "10.3103/S0146411621070117",
        "paper_author": "Morzhov S.V.",
        "affiliation_name": "Yaroslavl State University",
        "affiliation_city": "Yaroslavl",
        "affiliation_country": "Russian Federation",
        "affiliation_id": "60001420",
        "affiliation_state": "Yaroslavl Oblast"
    },
    {
        "paper_title": "Writing to Increase Complexity: Spanish L2 Learners’ Complexity Gains in a College Composition Class",
        "publication": "Hispania",
        "citied_by": "1",
        "cover_date": "2021-12-01",
        "Abstract": "This study examines the linguistic complexity of Spanish as a second language (L2) in learners’ essays across proficiency levels at two timelines of a composition class during a college semester. Data comes from 22 L2 learners of Spanish enrolled in two sections of a third-year composition class at the college level, who were assigned nine compositions (150–250 words each) at different times throughout one semester. Data was prepared using a natural language processing (NLP) pipeline, including UDPipe, an NLP tool that allows annotation, part-of-speech (POS) tagging, lemmatization, and dependency parsing based on Universal Dependencies (UD) treebanks. In addition, the NLTK package and several Python scripts were used on the annotated model to process and extract syntactic and lexical information from the datasets. Results showed that selected predictors of syntactic complexity increased at different stages in the semester and the effect seems more robust in beginner learners. Moreover, the use of lower frequency lexicon appears to be integrated thorough out the semester in both groups of learners. These findings indicate the pedagogical benefits of Spanish composition courses and the specific indices of L2 writing development obtained during a semester of classes across two groups of language proficiency.",
        "DOI": "10.1353/HPN.2021.0120",
        "paper_author": "Restrepo-Ramos F.",
        "affiliation_name": "Minnesota State University",
        "affiliation_city": "Saint Paul",
        "affiliation_country": "United States",
        "affiliation_id": "60007710",
        "affiliation_state": "MN"
    },
    {
        "paper_title": "Evaluation of Logistics Service Quality: Sentiment Analysis of Comment Text Based on Multi-Level Graph Neural Network",
        "publication": "Traitement du Signal",
        "citied_by": "5",
        "cover_date": "2021-12-01",
        "Abstract": "The world is severely impacted by the coronavirus (COVID19). During the epidemic, logistics service, an often-overlooked pillar of the modern society, steps into the spotlight. However, the service capability is inevitably weakened by the epidemic. The fatigued service providers are increasingly unable to meet the high expectations of users, who therefore leave harsh comments on logistics services. It is important for managers to find information that helps to improve management, out of the biased and angry comments. Text sentiment analysis is a fundamental work in natural language processing (NLP). In recent years, graph neural network (GNN) has achieved excellent performance in various NLP tasks. Nevertheless, GNN only considers the adjacent words, as it updates graph nodes. The model thereby emphasizes local features over global features, and misses the intent of the comment text. This paper constructs a triple graph neural network (TGNN) to serve the sentiment analysis of service texts. Firstly, the corresponding node connection windows were applied on different network layers to consider both local and global features. Next, the graph attention network (GAT) was adopted as the message delivery mechanism to fuse the features of all word nodes in the graph. Experimental results show that, the TGNN can evaluate the comment texts on logistics service quality more accurately than the other models.",
        "DOI": "10.18280/ts.380630",
        "paper_author": "Chen W.",
        "affiliation_name": "Wenzhou Business College",
        "affiliation_city": "Wenzhou",
        "affiliation_country": "China",
        "affiliation_id": "117016219",
        "affiliation_state": "Zhejiang"
    },
    {
        "paper_title": "Classification of Image and Text Data Using Deep Learning-Based LSTM Model",
        "publication": "Traitement du Signal",
        "citied_by": "13",
        "cover_date": "2021-12-01",
        "Abstract": "The advent of social networking and the internet has resulted in a huge shift in how consumers express their loyalty and where firms acquire a reputation. Customers and businesses frequently leave comments, and entrepreneurs do the same. These write-ups may be useful to those with the ability to analyse them. However, analysing textual content without the use of computers and the associated tools is time-consuming and difficult. The goal of Sentiment Analysis (SA) is to discover client feedback, points of view, or complaints that describe the product in a more negative or optimistic light. You can expect this to be a result based on this data if you merely read and assess feedback or examine ratings. There was a time when only the use of standard techniques, such as linear regression and Support Vector Machines (SVM), was effective for the task of automatically discovering knowledge from written explanations, but the older approaches have now been mostly replaced by deep neural networks, and deep learning has gotten the job done. Convolution and compressing RNNs are useful for tasks like machine translation, caption creation, and language modelling, however they suffer from gradient disappearance or explosion issues with large words. This research uses a deep learning RNN for movie review sentiment prediction that is quite comparable to Long Short-Term Memory networks. A LSTM model was well suited for modelling long sequential data. Generally, sentence vectorization approaches are used to overcome the inconsistency of sentence form. We made an attempt to look into the effect of hyper parameters like dropout of layers, activation functions and we also tested the model with different neural network settings and showed results that have been presented in the various ways to take the data into account. IMDB is the official movie database which serves as the basis for all of the experimental studies in the proposed model.",
        "DOI": "10.18280/ts.380625",
        "paper_author": "Yechuri P.K.",
        "affiliation_name": "Vel Tech Rangarajan Dr.Sagunthala R&amp;D Institute of Science and Technology",
        "affiliation_city": "Chennai",
        "affiliation_country": "India",
        "affiliation_id": "60104476",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "A Comparative Study of Arabic Part of Speech Taggers Using Literary Text Samples from Saudi Novels",
        "publication": "Information (Switzerland)",
        "citied_by": "7",
        "cover_date": "2021-12-01",
        "Abstract": "Part of Speech (POS) tagging is one of the most common techniques used in natural language processing (NLP) applications and corpus linguistics. Various POS tagging tools have been developed for Arabic. These taggers differ in several aspects, such as in their modeling techniques, tag sets and training and testing data. In this paper we conduct a comparative study of five Arabic POS taggers, namely: Stanford Arabic, CAMeL Tools, Farasa, MADAMIRA and Arabic Linguistic Pipeline (ALP) which examine their performance using text samples from Saudi novels. The testing data has been extracted from different novels that represent different types of narrations. The main result we have obtained indicates that the ALP tagger performs better than others in this particular case, and that Adjective is the most frequent mistagged POS type as compared to Noun and Verb.",
        "DOI": "10.3390/info12120523",
        "paper_author": "Alluhaibi R.",
        "affiliation_name": "Taibah University",
        "affiliation_city": "Medina",
        "affiliation_country": "Saudi Arabia",
        "affiliation_id": "60008920",
        "affiliation_state": "Al Madinah al Munawwarah"
    },
    {
        "paper_title": "English Lessons with the Use of Songs for People with Mild Cognitive Impairment",
        "publication": "Alzheimer's &amp; dementia : the journal of the Alzheimer's Association",
        "citied_by": "0",
        "cover_date": "2021-12-01",
        "Abstract": "BACKGROUND: People suffering from Mild Cognitive Impairment (MCI) need to improve their cognitive abilities in order to avoid the deterioration of their condition. Learning a foreign language is a good way for cognitive enhancement. However, since these people have memory deficits and lack of attention abilities, conventional teaching methods cannot be used. The use of songs in foreign language lessons is proved to be a very good technique for more effective acquisition of the language while also improving well-being and relaxation. METHOD: The Erasmus+ funded \"E.L.So.M.C.I\" program is a 2 year pan-European initiative that aims to develop an educational program on teaching English to people with MCI and using English songs as a main tool for the teaching process. The methodology of this educational program is based on innovative teaching approaches such as \"Communicative Language Teaching\" and \"Natural Approach and the method of \"Neuro-linguistic Programming (NLP)\". These methods place great emphasis on verbal communication, creation of a positive environment in class, reduction of stress and encouragement of learners to learn step by step in a natural and pleasant way. RESULT: In this moment, the project has entered its first operational phase, that has the objective of producing: - A Workshops' Methodological Guide which is a document provides trainers with guidelines for teaching languages that focuses on communication in English (for the learners) and preparation of the lesson plan for each lesson. The Methodological Guide includes two kinds of materials, one for the participants-students and one for the professionals (trainers). - Several Workshops for English learning, addressed to people with MCI (MMSE between 28 and 24). - Co-Created Methodological Guide and Training for trainers via MOOC. CONCLUSION: This project and methodology can improve cognitive functions, prevent from Alzheimer's disease, support the teaching process through the use of songs, reduce stress and increase positive emotions, improve participants' socialization, as well as, develop the sense of belonging to a group.",
        "DOI": "10.1002/alz.058651",
        "paper_author": "Makri M.",
        "affiliation_name": "Aristotle University of Thessaloniki",
        "affiliation_city": "Thessaloniki",
        "affiliation_country": "Greece",
        "affiliation_id": "60015331",
        "affiliation_state": "Central Macedonia"
    },
    {
        "paper_title": "Technology and Dementia Preconference",
        "publication": "Alzheimer's &amp; dementia : the journal of the Alzheimer's Association",
        "citied_by": "2",
        "cover_date": "2021-12-01",
        "Abstract": "BACKGROUND: Various neurodegenerative and psychiatric diseases are related with changes in spoken language [1], although they have seldom been investigated. We evaluate the effectiveness of our language-agnostic Machine Learning (ML) system to detect subtle changes in spoken language that manifest early signs of cognitive decline, thus assisting with its diagnosis. We evaluate our methodology using recordings of speech samples from multiple languages obtained from patient cohorts in early stages of cognitive decline and matched healthy controls. METHOD: Our methodology involves capturing patient audio recordings while they are performing one or more predefined cognitive assessment tasks, involving, e.g., the description of a picture or recounting of an everyday activity. Afterwards, multi-language audio recordings and generated transcripts are analyzed with audio and NLP feature extraction methods [3], ranging from semantic, morpho-syntactic, phonological representations of the input, as well as, more sophisticated linguistic measures. The feature pool is filtered by a Pearson's rho threshold of 0.85. We build a Random Forest classifier out of 100 Decision trees, using the Gini impurity criterion, 5-fold cross-validation for training, elimination and composition-based feature selection, as well as post-selection retraining / fine-tuning. The model's diagnostic performance is evaluated on a test set unseen during training. RESULT: Our results are validated against the diagnosis that is provided by medical experts. Our performance in terms of accuracy (∼82%), f1 (84%) and ROC-AUC score (∼82%) are clear indicators of the effectiveness of speech analysis towards detecting cognitive decline. Moreover, our tree-based classifier produces probability scores that closely follow the proportion of pathological cases in the input data, with a correlation of 94%. CONCLUSION: In the current evaluation we verified our conjectures regarding the strong capacity of speech to predict cognitive decline. Audio analysis and machine learning are proven to be invaluable tools in the prediction of early signs of cognitive decline, which are coupled with a wide spectrum of neurodegenerative and psychiatric diseases. [1] Boschi, Veronica, et al., Frontiers in psychology 8 (2017): 269. [2] Vassiliki Rentoumi et al., Alzheimer's & Dementia, Wiley, volume 16, 2020. [3] Alberdi, Ane et al., Artificial intelligence in medicine 71 (2016): 1-29.",
        "DOI": "10.1002/alz.052520",
        "paper_author": "Rentoumi V.",
        "affiliation_name": "LANGaware Inc",
        "affiliation_city": null,
        "affiliation_country": null,
        "affiliation_id": "127601804",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Active deep learning to detect cognitive concerns in electronic health records",
        "publication": "Alzheimer's &amp; dementia : the journal of the Alzheimer's Association",
        "citied_by": "1",
        "cover_date": "2021-12-01",
        "Abstract": "BACKGROUND: Timely diagnosis of dementia is important to patients and their caregivers for advanced planning, yet dementia is under-diagnosed by healthcare professionals and under-coded in claims data. Sensitive and specific tools to detect cognitive concerns in diverse clinical settings could prompt referral for cognitive evaluation and specialist care. METHOD: We developed a deep learning natural language processing (NLP) method to detect cognitive concerns in unstructured clinician notes from electronic health records (EHR). We leveraged a gold-standard set of ∼1000 patients sampled randomly from three strata: patients with diagnosis codes, patients with specialist visits but no code, or patients with neither. The physician performed a detailed chart review and adjudication of cognitive status, noting \"cognitive concern\" (i.e., any evidence of cognitive difficulties) and rating patients on a 5-point scale: normal, normal vs. MCI, MCI, MCI vs. dementia, and dementia. We used 10% of the labeled data as a test set and the remaining 90% for training and validation of the model to classify patients with any cognitive concerns (normal vs. other). We also built a web-based chart review annotation tool that facilitates labeling and enables an active learning loop to scale up labeling to thousands of charts. RESULT: In a random sample from the gold-standard dataset, 30 out of 80 patients with cognitive concerns had no diagnosis code or medication related to dementia; we hypothesized that our deep learning tool could leverage clinical text to improve detection of cognitive concerns. Indeed, a model with codes and medications had an area under the receiver operating characteristic (AUROC) curve of 0.79, sensitivity of 0.59, and specificity of 1.00 for the binary classification task. The deep learning model improved the AUROC to 0.90, increased sensitivity to 0.79, and maintained specificity of 0.98. Notes from primary care, specialties such as neurology and psychiatry, and social workers had the highest likelihood of containing information. CONCLUSION: The deep learning model was successful in detecting cases without a dementia-related diagnosis code or medication. Automatic processing of electronic medical records with a deep learning tool can be used for early detection of cognitive concern to optimize patient care and predict hospital readmission.",
        "DOI": "10.1002/alz.055362",
        "paper_author": "Magdamo C.G.",
        "affiliation_name": "Massachusetts General Hospital",
        "affiliation_city": "Boston",
        "affiliation_country": "United States",
        "affiliation_id": "60029929",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "A Data-Driven Iterative Approach for Semi-automatically Assessing the Correctness of Medication Value Sets: A Proof of Concept Based on Opioids",
        "publication": "Methods of Information in Medicine",
        "citied_by": "2",
        "cover_date": "2021-12-01",
        "Abstract": "Background Value sets are lists of terms (e.g., opioid medication names) and their corresponding codes from standard clinical vocabularies (e.g., RxNorm) created with the intent of supporting health information exchange and research. Value sets are manually-created and often exhibit errors. Objectives The aim of the study is to develop a semi-automatic, data-centric natural language processing (NLP) method to assess medication-related value set correctness and evaluate it on a set of opioid medication value sets. Methods We developed an NLP algorithm that utilizes value sets containing mostly true positives and true negatives to learn lexical patterns associated with the true positives, and then employs these patterns to identify potential errors in unseen value sets. We evaluated the algorithm on a set of opioid medication value sets, using the recall, precision and F 1-score metrics. We applied the trained model to assess the correctness of unseen opioid value sets based on recall. To replicate the application of the algorithm in real-world settings, a domain expert manually conducted error analysis to identify potential system and value set errors. Results Thirty-eight value sets were retrieved from the Value Set Authority Center, and six (two opioid, four non-opioid) were used to develop and evaluate the system. Average precision, recall, and F 1-score were 0.932, 0.904, and 0.909, respectively on uncorrected value sets; and 0.958, 0.953, and 0.953, respectively after manual correction of the same value sets. On 20 unseen opioid value sets, the algorithm obtained average recall of 0.89. Error analyses revealed that the main sources of system misclassifications were differences in how opioids were coded in the value sets-while the training value sets had generic names mostly, some of the unseen value sets had new trade names and ingredients. Conclusion The proposed approach is data-centric, reusable, customizable, and not resource intensive. It may help domain experts to easily validate value sets.",
        "DOI": "10.1055/s-0041-1740358",
        "paper_author": "Li L.",
        "affiliation_name": "Emory University",
        "affiliation_city": "Atlanta",
        "affiliation_country": "United States",
        "affiliation_id": "60000928",
        "affiliation_state": "GA"
    },
    {
        "paper_title": "Robust Complaint Processing in Portuguese",
        "publication": "Information (Switzerland)",
        "citied_by": "1",
        "cover_date": "2021-12-01",
        "Abstract": "The Natural Language Processing (NLP) community has witnessed huge improvements in the last years. However, most achievements are evaluated on benchmarked curated corpora, with little attention devoted to user-generated content and less-resourced languages. Despite the fact that recent approaches target the development of multi-lingual tools and models, they still underperform in languages such as Portuguese, for which linguistic resources do not abound. This paper exposes a set of challenges encountered when dealing with a real-world complex NLP problem, based on user-generated complaint data in Portuguese. This case study meets the needs of a country-wide governmental institution responsible for food safety and economic surveillance, and its responsibilities in handling a high number of citizen complaints. Beyond looking at the problem from an exclusively academic point of view, we adopt application-level concerns when analyzing the progress obtained through different techniques, including the need to obtain explainable decision support. We discuss modeling choices and provide useful insights for researchers working on similar problems or data.",
        "DOI": "10.3390/info12120525",
        "paper_author": "Lopes-Cardoso H.",
        "affiliation_name": "Universidade do Porto",
        "affiliation_city": "Porto",
        "affiliation_country": "Portugal",
        "affiliation_id": "60007249",
        "affiliation_state": "Porto"
    },
    {
        "paper_title": "Hate Speech in the Arab Electronic Press and Social Networks",
        "publication": "Revue d'Intelligence Artificielle",
        "citied_by": "3",
        "cover_date": "2021-12-01",
        "Abstract": "Nowadays we are witnessing an open world, characterized by globalization which is accompanied by a technology through which information circulates without borders, especially with the widespread use of social networking sites being the most common communication tool, that gives access through various applications to a large space for the presentation of multiple ideas, including extremist ideas, and the spread of hate speech. This paper introduces a system of detection of hate speech in the texts of Arabic read media and social media, which is based on a combined use of NLP, and machine learning methods. The training of the detection model is done on a large Dataset of articles, tweets and comments, collected, balanced and tokenized afterwards using BERT in Arabic. The trained model detects hate speech in Arabic and various Arabic based dialects, by classifying the texts into two classes: Neutral and Abusive. The above-mentioned model is evaluated using precision metrics, recall and f1 score, it has reached an accuracy of 83%.",
        "DOI": "10.18280/ria.350603",
        "paper_author": "Awane W.",
        "affiliation_name": "Hassan II University of Casablanca",
        "affiliation_city": "Casablanca",
        "affiliation_country": "Morocco",
        "affiliation_id": "60025457",
        "affiliation_state": "Casablanca-Settat"
    },
    {
        "paper_title": "A Novel Hybrid Network for Arabic Sentiment Analysis using fine-tuned AraBERT model",
        "publication": "International Journal on Electrical Engineering and Informatics",
        "citied_by": "25",
        "cover_date": "2021-12-01",
        "Abstract": "The pre-trained word embedding models become widely used in Natural Language Processing (NLP), but they disregard the context and sense of the text. We study in this paper, the capacity of pre-trained BERT model (Bidirectional Encoder Representations from Transformers) for the Arabic language to classify Arabic tweets using a hybrid network of two famous models; Bidirectional Long Short Term Memory (BiLSTM) and Gated Recurrent Unit (GRU) inspired by the great achievement of deep learning algorithms. In this context, we fine-tuned the Arabic BERT (AraBERT) parameters and we used it on three merged datasets to impart its knowledge for the Arabic sentiment analysis. For that, we lead the experiments by comparing the AraBERT model in one hand in the word embedding phase, with a statics pre-trained word embeddings method namely AraVec and FastText, and on another hand in the classification phase, we compared the hybrid model with convolutional neural network (CNN), long short-term memory (LSTM), BiLSTM, and GRU, which are prevalently preferred in sentiment analysis. The results demonstrate that the fine-tuned AraBERT model, combined with the hybrid network, achieved peak performance with up to 94% accuracy.",
        "DOI": "10.15676/ijeei.2021.13.4.3",
        "paper_author": "Habbat N.",
        "affiliation_name": "Ecole Supérieure de Technologie de Casablanca",
        "affiliation_city": "Casablanca",
        "affiliation_country": "Morocco",
        "affiliation_id": "60070600",
        "affiliation_state": "Casablanca-Settat"
    },
    {
        "paper_title": "Applying Natural Language Processing for detecting malicious patterns in Android applications",
        "publication": "Forensic Science International: Digital Investigation",
        "citied_by": "7",
        "cover_date": "2021-12-01",
        "Abstract": "With increasing quantity and sophistication, malicious code is becoming difficult to discover and analyze. Modern NLP (Natural Language Processing) techniques have significantly improved, and are being used in practice to accomplish various tasks. Recently, many research works have applied NLP for finding malicious patterns in Android and Windows apps. In this paper, we exploit this fact and apply NLP techniques to an intermediate representation (MAIL – Malware analysis intermediate language) of Android apps to build a similarity index model, named SIMP. We use SIMP to find malicious patterns in Android apps. MAIL provides control flow patterns to enhance the malware analysis and makes the code accessible to NLP techniques for checking semantic similarities. For applying NLP, we consider a MAIL program as one document. The control flow patterns in this program when divided, into specific blocks (words), become sentences. We apply TFIDF and Bag-of-Words over these control flow patterns to build SIMP. Our proposed model, when tested with real malware and benign Android apps using different validation methods, achieved an MCC (Mathews Correlation Coefficient) ≥ 0.94 between the true and predicted values. That indicates, predicting a new sample either as malware or benign with a high success rate.",
        "DOI": "10.1016/j.fsidi.2021.301270",
        "paper_author": "Alam S.",
        "affiliation_name": "Adana Alparslan Türkeş Science and Technology University",
        "affiliation_city": "Adana",
        "affiliation_country": "Turkey",
        "affiliation_id": "60106489",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Developing Core Technologies for Resource-Scarce Nguni Languages",
        "publication": "Information (Switzerland)",
        "citied_by": "4",
        "cover_date": "2021-12-01",
        "Abstract": "The creation of linguistic resources is crucial to the continued growth of research and development efforts in the field of natural language processing, especially for resource-scarce languages. In this paper, we describe the curation and annotation of corpora and the development of multiple linguistic technologies for four official South African languages, namely isiNdebele, Siswati, isiXhosa, and isiZulu. Development efforts included sourcing parallel data for these languages and annotating each on token, orthographic, morphological, and morphosyntactic levels. These sets were in turn used to create and evaluate three core technologies, viz. a lemmatizer, part-of-speech tagger, morphological analyzer for each of the languages. We report on the quality of these technologies which improve on previously developed rule-based technologies as part of a similar initiative in 2013. These resources are made publicly accessible through a local resource agency with the intention of fostering further development of both resources and technologies that may benefit the NLP industry in South Africa.",
        "DOI": "10.3390/info12120520",
        "paper_author": "du Toit J.S.",
        "affiliation_name": "North-West University",
        "affiliation_city": "Potchefstroom",
        "affiliation_country": "South Africa",
        "affiliation_id": "60029714",
        "affiliation_state": "North West"
    },
    {
        "paper_title": "Functional Classification of Urban Parks Based on Urban Functional Zone and Crowd-Sourced Geographical Data",
        "publication": "ISPRS International Journal of Geo-Information",
        "citied_by": "14",
        "cover_date": "2021-12-01",
        "Abstract": "Urban parks have important impacts on urban ecosystems and in disaster prevention. They also have diverse social functions that are important to the living conditions and spatial structures of cities. Identifying and classifying the different types of urban parks are important for analyzing the sustainable development and the greening progress in cities. Existing studies have predominantly focused on the data extraction of urban green spaces as a whole, while there have been relatively few studies that have considered different categories of urban parks and their impact, which makes it difficult to characterize or predict the spatial distribution and structures of urban parks and limits further refinement of urban research. At present, the classification of urban parks relies on the physical features observed in remote sensing images, but these methods are limited when mapping the diverse functions and attributes of urban parks. Crowd-sourced geographic data may more accurately express the social functions of points of interest (POIs) in cities, and, therefore, employing open data sources may assist in data extraction and the classification of different types of urban parks. This paper proposed a multi-source data fusion approach for urban park classification including POI and urban functional zone (UFZ) data. First, the POI data were automatically reclassified using improved natural language processing (NLP) (i.e., text similarity measurements and topic modeling) to establish the links between urban park green-space types and POIs. The reclassified POI data as well as the UFZ data were then subjected to scene-based data fusion, and various types of urban parks were extracted using data attribute analysis and social attribute recognition for urban park mapping. Experimental analysis was conducted across Beijing and Hangzhou to verify the effectiveness of the proposed method, which had an overall classification accuracy of 82.8%. Finally, the urban park types of the two cities were compared and analyzed to obtain the characteristics of urban park types and structures in the two cities, which have different climates and urban structures.",
        "DOI": "10.3390/ijgi10120824",
        "paper_author": "Cao S.",
        "affiliation_name": "Lanzhou Jiaotong University",
        "affiliation_city": "Lanzhou",
        "affiliation_country": "China",
        "affiliation_id": "60009806",
        "affiliation_state": "Gansu"
    },
    {
        "paper_title": "Digital topics on cultural heritage investigated: how can data-driven and data-guided methods support to identify current topics and trends in digital heritage?",
        "publication": "Built Heritage",
        "citied_by": "15",
        "cover_date": "2021-12-01",
        "Abstract": "In research and policies, the identification of trends as well as emerging topics and topics in decline is an important source of information for both academic and innovation management. Since at present policy analysis mostly employs qualitative research methods, the following article presents and assesses different approaches – trend analysis based on questionnaires, quantitative bibliometric surveys, the use of computer-linguistic approaches and machine learning and qualitative investigations. Against this backdrop, this article examines digital applications in cultural heritage and, in particular, built heritage via various investigative frameworks to identify topics of relevance and trendlines, mainly for European Union (EU)-based research and policies. Furthermore, this article exemplifies and assesses the specific opportunities and limitations of the different methodical approaches against the backdrop of data-driven vs. data-guided analytical frameworks. As its major findings, our study shows that both research and policies related to digital applications for cultural heritage are mainly driven by the availability of new technologies. Since policies focus on meta-topics such as digitisation, openness or automation, the research descriptors are more granular. In general, data-driven approaches are promising for identifying topics and trendlines and even predicting the development of near future trends. Conversely, qualitative approaches are able to answer “why” questions with regard to whether topics are emerging due to disruptive innovations or due to new terminologies or whether topics are becoming obsolete because they are common knowledge, as is the case for the term “internet”.",
        "DOI": "10.1186/s43238-021-00045-7",
        "paper_author": "Münster S.",
        "affiliation_name": "Friedrich-Schiller-Universität Jena",
        "affiliation_city": "Jena",
        "affiliation_country": "Germany",
        "affiliation_id": "60029507",
        "affiliation_state": "Thuringen"
    },
    {
        "paper_title": "Development and Validation of a Natural Language Processing Algorithm to Extract Descriptors of Microbial Keratitis from the Electronic Health Record",
        "publication": "Cornea",
        "citied_by": "4",
        "cover_date": "2021-12-01",
        "Abstract": "Purpose:The purpose of this article was to develop and validate a natural language processing (NLP) algorithm to extract qualitative descriptors of microbial keratitis (MK) from electronic health records.Methods:In this retrospective cohort study, patients with MK diagnoses from 2 academic centers were identified using electronic health records. An NLP algorithm was created to extract MK centrality, depth, and thinning. A random sample of patient with MK encounters were used to train the algorithm (400 encounters of 100 patients) and compared with expert chart review. The algorithm was evaluated in internal (n = 100) and external validation data sets (n = 59) in comparison with masked chart review. Outcomes were sensitivity and specificity of the NLP algorithm to extract qualitative MK features as compared with masked chart review performed by an ophthalmologist.Results:Across data sets, gold-standard chart review found centrality was documented in 64.0% to 79.3% of charts, depth in 15.0% to 20.3%, and thinning in 25.4% to 31.3%. Compared with chart review, the NLP algorithm had a sensitivity of 80.3%, 50.0%, and 66.7% for identifying central MK, 85.4%, 66.7%, and 100% for deep MK, and 100.0%, 95.2%, and 100% for thin MK, in the training, internal, and external validation samples, respectively. Specificity was 41.1%, 38.6%, and 46.2% for centrality, 100%, 83.3%, and 71.4% for depth, and 93.3%, 100%, and was not applicable (n = 0) to the external data for thinning, in the samples, respectively.Conclusions:MK features are not documented consistently showing a lack of standardization in recording MK examination elements. NLP shows promise but will be limited if the available clinical data are missing from the chart.",
        "DOI": "10.1097/ICO.0000000000002755",
        "paper_author": "Woodward M.A.",
        "affiliation_name": "University of Michigan Medical School",
        "affiliation_city": "Ann Arbor",
        "affiliation_country": "United States",
        "affiliation_id": "60033182",
        "affiliation_state": "MI"
    },
    {
        "paper_title": "Automated paraphrase quality assessment using language models and transfer learning",
        "publication": "Computers",
        "citied_by": "9",
        "cover_date": "2021-12-01",
        "Abstract": "Learning to paraphrase supports both writing ability and reading comprehension, particularly for less skilled learners. As such, educational tools that integrate automated evaluations of paraphrases can be used to provide timely feedback to enhance learner paraphrasing skills more efficiently and effectively. Paraphrase identification is a popular NLP classification task that involves establishing whether two sentences share a similar meaning. Paraphrase quality assessment is a slightly more complex task, in which pairs of sentences are evaluated in-depth across multiple dimensions. In this study, we focus on four dimensions: lexical, syntactical, semantic, and overall quality. Our study introduces and evaluates various machine learning models using handcrafted features combined with Extra Trees, Siamese neural networks using BiLSTM RNNs, and pretrained BERT-based models, together with transfer learning from a larger general paraphrase corpus, to estimate the quality of paraphrases across the four dimensions. Two datasets are considered for the tasks involving paraphrase quality: ULPC (User Language Paraphrase Corpus) containing 1998 paraphrases and a smaller dataset with 115 paraphrases based on children’s inputs. The paraphrase identification dataset used for the transfer learning task is the MSRP dataset (Microsoft Research Paraphrase Corpus) containing 5801 paraphrases. On the ULPC dataset, our BERT model improves upon the previous baseline by at least 0.1 in F1-score across the four dimensions. When using fine-tuning from ULPC for the children dataset, both the BERT and Siamese neural network models improve upon their original scores by at least 0.11 F1-score. The results of these experiments suggest that transfer learning using generic paraphrase identification datasets can be successful, while at the same time obtaining comparable results in fewer epochs.",
        "DOI": "10.3390/computers10120166",
        "paper_author": "Nicula B.",
        "affiliation_name": "University Politehnica of Bucharest",
        "affiliation_city": "Bucharest",
        "affiliation_country": "Romania",
        "affiliation_id": "60003161",
        "affiliation_state": "Bucharest"
    },
    {
        "paper_title": "MedTAG: a portable and customizable annotation tool for biomedical documents",
        "publication": "BMC Medical Informatics and Decision Making",
        "citied_by": "11",
        "cover_date": "2021-12-01",
        "Abstract": "Background: Semantic annotators and Natural Language Processing (NLP) methods for Named Entity Recognition and Linking (NER+L) require plenty of training and test data, especially in the biomedical domain. Despite the abundance of unstructured biomedical data, the lack of richly annotated biomedical datasets poses hindrances to the further development of NER+L algorithms for any effective secondary use. In addition, manual annotation of biomedical documents performed by physicians and experts is a costly and time-consuming task. To support, organize and speed up the annotation process, we introduce MedTAG, a collaborative biomedical annotation tool that is open-source, platform-independent, and free to use/distribute. Results: We present the main features of MedTAG and how it has been employed in the histopathology domain by physicians and experts to annotate more than seven thousand clinical reports manually. We compare MedTAG with a set of well-established biomedical annotation tools, including BioQRator, ezTag, MyMiner, and tagtog, comparing their pros and cons with those of MedTag. We highlight that MedTAG is one of the very few open-source tools provided with an open license and a straightforward installation procedure supporting cross-platform use. Conclusions: MedTAG has been designed according to five requirements (i.e. available, distributable, installable, workable and schematic) defined in a recent extensive review of manual annotation tools. Moreover, MedTAG satisfies 20 over 22 criteria specified in the same study.",
        "DOI": "10.1186/s12911-021-01706-4",
        "paper_author": "Giachelle F.",
        "affiliation_name": "Università degli Studi di Padova",
        "affiliation_city": "Padua",
        "affiliation_country": "Italy",
        "affiliation_id": "60000481",
        "affiliation_state": "PD"
    },
    {
        "paper_title": "The human right to water and sanitation: Using natural language processing to uncover patterns in academic publishing",
        "publication": "Water (Switzerland)",
        "citied_by": "3",
        "cover_date": "2021-12-01",
        "Abstract": "After years of advocacy and international negotiation, the General Assembly of the United Nations voted to officially recognize a stand-alone human right to water and sanitation on 28 July 2010. Since, academic scholarship has continued to grow in an effort to understand the implications of the codification of this human right. Yet, with this growth, it has become impractical if not impossible for scholars to keep up with the advancement of academic knowledge or to make sense of it in a systematic way. In short, to date, we know very little about the trends in the literature as they have unfolded over the past thirty years and the topics to which scholars have devoted significant attention within the broader field, particularly over time. This is an important area of inquiry, as developing a comprehensive understanding of where prior literature has focused and where it appears to be going offers scholars an opportunity to identify areas in need of refinement and/or increased attention. Given the practicalities of reading thousands of research papers each year, this project utilizes natural language processing (NLP) to identify topics and trends in academic literature on the human right to water and sanitation (HRtWS). NLP provides the opportunity to digest large quantities of text data through machine learning, culminating with descriptive information on trends and topics in the field since 1990. The results of this exercise show that the research related to the human right to water and sanitation has grown exponentially, particularly over the last decade, illustrates the multidisciplinary nature of the literature, and demonstrates the diversity of topics in the field.",
        "DOI": "10.3390/w13243501",
        "paper_author": "Faulkner C.M.",
        "affiliation_name": "US Naval War College",
        "affiliation_city": "Newport",
        "affiliation_country": "United States",
        "affiliation_id": "60024146",
        "affiliation_state": "RI"
    },
    {
        "paper_title": "Combining word embeddings to extract chemical and drug entities in biomedical literature",
        "publication": "BMC Bioinformatics",
        "citied_by": "4",
        "cover_date": "2021-12-01",
        "Abstract": "Background: Natural language processing (NLP) and text mining technologies for the extraction and indexing of chemical and drug entities are key to improving the access and integration of information from unstructured data such as biomedical literature. Methods: In this paper we evaluate two important tasks in NLP: the named entity recognition (NER) and Entity indexing using the SNOMED-CT terminology. For this purpose, we propose a combination of word embeddings in order to improve the results obtained in the PharmaCoNER challenge. Results: For the NER task we present a neural network composed of BiLSTM with a CRF sequential layer where different word embeddings are combined as an input to the architecture. A hybrid method combining supervised and unsupervised models is used for the concept indexing task. In the supervised model, we use the training set to find previously trained concepts, and the unsupervised model is based on a 6-step architecture. This architecture uses a dictionary of synonyms and the Levenshtein distance to assign the correct SNOMED-CT code. Conclusion: On the one hand, the combination of word embeddings helps to improve the recognition of chemicals and drugs in the biomedical literature. We achieved results of 91.41% for precision, 90.14% for recall, and 90.77% for F1-score using micro-averaging. On the other hand, our indexing system achieves a 92.67% F1-score, 92.44% for recall, and 92.91% for precision. With these results in a final ranking, we would be in the first position.",
        "DOI": "10.1186/s12859-021-04188-3",
        "paper_author": "López-Úbeda P.",
        "affiliation_name": "Universidad de Jaén",
        "affiliation_city": "Jaen",
        "affiliation_country": "Spain",
        "affiliation_id": "60014586",
        "affiliation_state": "Jaen"
    },
    {
        "paper_title": "A sequential graph neural network for short text classification",
        "publication": "Algorithms",
        "citied_by": "13",
        "cover_date": "2021-12-01",
        "Abstract": "Short text classification is an important problem of natural language processing (NLP), and graph neural networks (GNNs) have been successfully used to solve different NLP problems. However, few studies employ GNN for short text classification, and most of the existing graph-based models ignore sequential information (e.g., word orders) in each document. In this work, we propose an improved sequence-based feature propagation scheme, which fully uses word representation and document-level word interaction and overcomes the limitations of textual features in short texts. On this basis, we utilize this propagation scheme to construct a lightweight model, sequential GNN (SGNN), and its extended model, ESGNN. Specifically, we build individual graphs for each document in the short text corpus based on word co-occurrence and use a bidirectional long short-term memory network (Bi-LSTM) to extract the sequential features of each document; therefore, word nodes in the document graph retain contextual information. Furthermore, two different simplified graph convolutional networks (GCNs) are used to learn word representations based on their local structures. Finally, word nodes combined with sequential information and local information are incorporated as the document representation. Extensive experiments on seven benchmark datasets demonstrate the effectiveness of our method.",
        "DOI": "10.3390/a14120352",
        "paper_author": "Zhao K.",
        "affiliation_name": "Jilin University",
        "affiliation_city": "Changchun",
        "affiliation_country": "China",
        "affiliation_id": "60007711",
        "affiliation_state": "Jilin"
    },
    {
        "paper_title": "Intelligent transportation systems (ITS): A systematic review using a Natural Language Processing (NLP) approach",
        "publication": "Heliyon",
        "citied_by": "32",
        "cover_date": "2021-12-01",
        "Abstract": "Intelligent Transportation Systems (ITS) is not a new concept. Notably, ITS has been cited in various journal articles and proceedings papers around the world, and it has become increasingly popular. Additionally, ITS involves multidisciplinary science. The growing number of journal articles makes ITS reviews complicated, and research gaps can be difficult to identify. The existing software for systematic reviews still relies on highly laborious tasks, manual reading, and a homogeneous dataset of research articles. This study proposes a framework that can address these issues, return a comprehensive systematic review of ITS, and promote efficient systematic reviews. The proposed framework consists of Natural Language Processing (NLP) methods i.e., Named Entity Recognition (NER), Latent Dirichlet Allocation (LDA), and word embedding (continuous skip-gram). It enables this study to explore the context of research articles and their overall interpretation to determine and define the directions of knowledge growth and ITS development. The framework can systematically separate unrelated documents and simplify the review process for large dataset. To our knowledge, compared to prior research regarding systematic review of ITS, this study offers more thorough review.",
        "DOI": "10.1016/j.heliyon.2021.e08615",
        "paper_author": "Zulkarnain ",
        "affiliation_name": "Universitas Indonesia",
        "affiliation_city": "Depok",
        "affiliation_country": "Indonesia",
        "affiliation_id": "60069377",
        "affiliation_state": "West Java"
    },
    {
        "paper_title": "Transformer-based decoder designs for semantic segmentation on remotely sensed images",
        "publication": "Remote Sensing",
        "citied_by": "52",
        "cover_date": "2021-12-01",
        "Abstract": "Transformers have demonstrated remarkable accomplishments in several natural language processing (NLP) tasks as well as image processing tasks. Herein, we present a deep-learning (DL) model that is capable of improving the semantic segmentation network in two ways. First, utilizing the pre-training Swin Transformer (SwinTF) under Vision Transformer (ViT) as a backbone, the model weights downstream tasks by joining task layers upon the pretrained encoder. Secondly, decoder designs are applied to our DL network with three decoder designs, U-Net, pyramid scene parsing (PSP) network, and feature pyramid network (FPN), to perform pixel-level segmentation. The results are compared with other image labeling state of the art (SOTA) methods, such as global convolutional network (GCN) and ViT. Extensive experiments show that our Swin Transformer (SwinTF) with decoder designs reached a new state of the art on the Thailand Isan Landsat-8 corpus (89.8% F1 score), Thailand North Landsat-8 corpus (63.12% F1 score), and competitive results on ISPRS Vaihingen. Moreover, both our best-proposed methods (SwinTF-PSP and SwinTF-FPN) even outperformed SwinTF with supervised pre-training ViT on the ImageNet-1K in the Thailand, Landsat-8, and ISPRS Vaihingen corpora.",
        "DOI": "10.3390/rs13245100",
        "paper_author": "Panboonyuen T.",
        "affiliation_name": "Chulalongkorn University",
        "affiliation_city": "Bangkok",
        "affiliation_country": "Thailand",
        "affiliation_id": "60028190",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Artificial intelligence-aided clinical annotation of a large multi-cancer genomic dataset",
        "publication": "Nature Communications",
        "citied_by": "31",
        "cover_date": "2021-12-01",
        "Abstract": "To accelerate cancer research that correlates biomarkers with clinical endpoints, methods are needed to ascertain outcomes from electronic health records at scale. Here, we train deep natural language processing (NLP) models to extract outcomes for participants with any of 7 solid tumors in a precision oncology study. Outcomes are extracted from 305,151 imaging reports for 13,130 patients and 233,517 oncologist notes for 13,511 patients, including patients with 6 additional cancer types. NLP models recapitulate outcome annotation from these documents, including the presence of cancer, progression/worsening, response/improvement, and metastases, with excellent discrimination (AUROC > 0.90). Models generalize to cancers excluded from training and yield outcomes correlated with survival. Among patients receiving checkpoint inhibitors, we confirm that high tumor mutation burden is associated with superior progression-free survival ascertained using NLP. Here, we show that deep NLP can accelerate annotation of molecular cancer datasets with clinically meaningful endpoints to facilitate discovery.",
        "DOI": "10.1038/s41467-021-27358-6",
        "paper_author": "Kehl K.L.",
        "affiliation_name": "Dana-Farber Cancer Institute",
        "affiliation_city": "Boston",
        "affiliation_country": "United States",
        "affiliation_id": "60014334",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Can central bank speeches predict financial market turbulence? Evidence from an adaptive NLP sentiment index analysis using XGBoost machine learning technique",
        "publication": "Central Bank Review",
        "citied_by": "20",
        "cover_date": "2021-12-01",
        "Abstract": "Central Bank speeches usually function as aggregators of internal quantitative and qualitative analysis of the institutions regarding the macro economy, the monetary policy and the health of the financial systems. Speeches usually function as a summary of the current status of a countries economic health, the undergoing trends and some future perspectives of the global economy. In this study departing from classical econometrics we employ natural language processing technologies in combination with machine learning techniques in order to filter out the most important signals in the corpus of speeches and translate into a sentiment index for forecasting the future financial markets behaviour. In our analysis, it is evident that central banker's expectations on economy tend to exhibit a predictive ability for financial markets turmoil. Using a combination of dictionaries which are either predefined or build based on historical speeches of the corpus we train an Extreme Gradient Boosting model that generates a sentiment index which signals turmoil with acceptable accuracy when passing a specific threshold.",
        "DOI": "10.1016/j.cbrev.2021.12.002",
        "paper_author": "Petropoulos A.",
        "affiliation_name": "Bank of Greece",
        "affiliation_city": "Athens",
        "affiliation_country": "Greece",
        "affiliation_id": "60003090",
        "affiliation_state": "Attica"
    },
    {
        "paper_title": "Q-switched mode locking noise-like pulse generation from a thulium-doped all-fiber laser based on nonlinear polarization rotation",
        "publication": "Results in Optics",
        "citied_by": "13",
        "cover_date": "2021-12-01",
        "Abstract": "Q-switched mode locking (QML) noise-like pulse (NLP) emission from an all-fiber thulium-doped laser based on the nonlinear polarization rotation effect is reported. The QML emission is obtained in a cavity with net anomalous dispersion in a pump power interval in between the CW laser threshold and the threshold of the NLP regime. Highest-energy QML pulses were observed with a repetition rate of 812 kHz with a pump power of 520 mW at the optical wavelength of 1881.09 nm. A maximum overall energy of 460 nJ at an average output power of 6.4 mW was reached, which corresponds to a burst of mode-locked noise-like sub-pulses with 8.7 ns of pulse duration within a QML envelope of 11 µs. These results demonstrate unconventional pulse operation regime of NLPs and provide insights into the dynamics of mode-locked fiber lasers.",
        "DOI": "10.1016/j.rio.2021.100115",
        "paper_author": "López-Estopier R.",
        "affiliation_name": "Universidad Autónoma de San Luis Potosí",
        "affiliation_city": "San Luis Potosí",
        "affiliation_country": "Mexico",
        "affiliation_id": "60031335",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Extracting social determinants of health from electronic health records using natural language processing: A systematic review",
        "publication": "Journal of the American Medical Informatics Association",
        "citied_by": "121",
        "cover_date": "2021-12-01",
        "Abstract": "Objective: Social determinants of health (SDoH) are nonclinical dispositions that impact patient health risks and clinical outcomes. Leveraging SDoH in clinical decision-making can potentially improve diagnosis, treatment planning, and patient outcomes. Despite increased interest in capturing SDoH in electronic health records (EHRs), such information is typically locked in unstructured clinical notes. Natural language processing (NLP) is the key technology to extract SDoH information from clinical text and expand its utility in patient care and research. This article presents a systematic review of the state-of-the-art NLP approaches and tools that focus on identifying and extracting SDoH data from unstructured clinical text in EHRs. Materials and Methods: A broad literature search was conducted in February 2021 using 3 scholarly databases (ACL Anthology, PubMed, and Scopus) following Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. A total of 6402 publications were initially identified, and after applying the study inclusion criteria, 82 publications were selected for the final review. Results: Smoking status (n = 27), substance use (n = 21), homelessness (n = 20), and alcohol use (n = 15) are the most frequently studied SDoH categories. Homelessness (n = 7) and other less-studied SDoH (eg, education, financial problems, social isolation and support, family problems) are mostly identified using rule-based approaches. In contrast, machine learning approaches are popular for identifying smoking status (n = 13), substance use (n = 9), and alcohol use (n = 9). Conclusion: NLP offers significant potential to extract SDoH data from narrative clinical notes, which in turn can aid in the development of screening tools, risk prediction models, and clinical decision support systems.",
        "DOI": "10.1093/jamia/ocab170",
        "paper_author": "Patra B.G.",
        "affiliation_name": "Weill Cornell Medicine",
        "affiliation_city": "New York",
        "affiliation_country": "United States",
        "affiliation_id": "60007997",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Opinion mining for national security: techniques, domain applications, challenges and research opportunities",
        "publication": "Journal of Big Data",
        "citied_by": "22",
        "cover_date": "2021-12-01",
        "Abstract": "Background: Opinion mining, or sentiment analysis, is a field in Natural Language Processing (NLP). It extracts people’s thoughts, including assessments, attitudes, and emotions toward individuals, topics, and events. The task is technically challenging but incredibly useful. With the explosive growth of the digital platform in cyberspace, such as blogs and social networks, individuals and organisations are increasingly utilising public opinion for their decision-making. In recent years, significant research concerning mining people’s sentiments based on text in cyberspace using opinion mining has been explored. Researchers have applied numerous opinions mining techniques, including machine learning and lexicon-based approach to analyse and classify people’s sentiments based on a text and discuss the existing gap. Thus, it creates a research opportunity for other researchers to investigate and propose improved methods and new domain applications to fill the gap. Methods: In this paper, a structured literature review has been done by considering 122 articles to examine all relevant research accomplished in the field of opinion mining application and the suggested Kansei approach to solve the challenges that occur in mining sentiments based on text in cyberspace. Five different platforms database were systematically searched between 2015 and 2021: ACM (Association for Computing Machinery), IEEE (Advancing Technology for Humanity), SCIENCE DIRECT, SpringerLink, and SCOPUS. Results: This study analyses various techniques of opinion mining as well as the Kansei approach that will help to enhance techniques in mining people’s sentiment and emotion in cyberspace. Most of the study addressed methods including machine learning, lexicon-based approach, hybrid approach, and Kansei approach in mining the sentiment and emotion based on text. The possible societal impacts of the current opinion mining technique, including machine learning and the Kansei approach, along with major trends and challenges, are highlighted. Conclusion: Various applications of opinion mining techniques in mining people’s sentiment and emotion according to the objective of the research, used method, dataset, summarized in this study. This study serves as a theoretical analysis of the opinion mining method complemented by the Kansei approach in classifying people’s sentiments based on text in cyberspace. Kansei approach can measure people’s impressions using artefacts based on senses including sight, feeling and cognition reported precise results for the assessment of human emotion. Therefore, this research suggests that the Kansei approach should be a complementary factor including in the development of a dictionary focusing on emotion in the national security domain. Also, this theoretical analysis will act as a reference to researchers regarding the Kansei approach as one of the techniques to improve hybrid approaches in opinion mining.",
        "DOI": "10.1186/s40537-021-00536-5",
        "paper_author": "Razali N.A.M.",
        "affiliation_name": "National Defense University of Malaysia",
        "affiliation_city": "Kuala Lumpur",
        "affiliation_country": "Malaysia",
        "affiliation_id": "60090676",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Research on Multi-label Text Classification Method Based on tALBERT-CNN",
        "publication": "International Journal of Computational Intelligence Systems",
        "citied_by": "24",
        "cover_date": "2021-12-01",
        "Abstract": "Single-label classification technology has difficulty meeting the needs of text classification, and multi-label text classification has become an important research issue in natural language processing (NLP). Extracting semantic features from different levels and granularities of text is a basic and key task in multi-label text classification research. A topic model is an effective method for the automatic organization and induction of text information. It can reveal the latent semantics of documents and analyze the topics contained in massive information. Therefore, this paper proposes a multi-label text classification method based on tALBERT-CNN: an LDA topic model and ALBERT model are used to obtain the topic vector and semantic context vector of each word (document), a certain fusion mechanism is adopted to obtain in-depth topic and semantic representations of the document, and the multi-label features of the text are extracted through the TextCNN model to train a multi-label classifier. The experimental results obtained on standard datasets show that the proposed method can extract multi-label features from documents, and its performance is better than that of the existing state-of-the-art multi-label text classification algorithms.",
        "DOI": "10.1007/s44196-021-00055-4",
        "paper_author": "Liu W.",
        "affiliation_name": "State Key Laboratory of Mathematical Engineering and Advanced Computing",
        "affiliation_city": "Zhengzhou",
        "affiliation_country": "China",
        "affiliation_id": "113621902",
        "affiliation_state": "Henan"
    },
    {
        "paper_title": "Electronic case report forms generation from pathology reports by ARGO, automatic record generator for onco-hematology",
        "publication": "Scientific Reports",
        "citied_by": "3",
        "cover_date": "2021-12-01",
        "Abstract": "The unstructured nature of Real-World (RW) data from onco-hematological patients and the scarce accessibility to integrated systems restrain the use of RW information for research purposes. Natural Language Processing (NLP) might help in transposing unstructured reports into standardized electronic health records. We exploited NLP to develop an automated tool, named ARGO (Automatic Record Generator for Onco-hematology) to recognize information from pathology reports and populate electronic case report forms (eCRFs) pre-implemented by REDCap. ARGO was applied to hemo-lymphopathology reports of diffuse large B-cell, follicular, and mantle cell lymphomas, and assessed for accuracy (A), precision (P), recall (R) and F1-score (F) on internal (n = 239) and external (n = 93) report series. 326 (98.2%) reports were converted into corresponding eCRFs. Overall, ARGO showed high performance in capturing (1) identification report number (all metrics > 90%), (2) biopsy date (all metrics > 90% in both series), (3) specimen type (86.6% and 91.4% of A, 98.5% and 100.0% of P, 92.5% and 95.5% of F, and 87.2% and 91.4% of R for internal and external series, respectively), (4) diagnosis (100% of P with A, R and F of 90% in both series). We developed and validated a generalizable tool that generates structured eCRFs from real-life pathology reports.",
        "DOI": "10.1038/s41598-021-03204-z",
        "paper_author": "Zaccaria G.M.",
        "affiliation_name": "IRCCS Istituto Tumori Giovanni Paolo II",
        "affiliation_city": "Bari",
        "affiliation_country": "Italy",
        "affiliation_id": "60070487",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "New explainability method for BERT-based model in fake news detection",
        "publication": "Scientific Reports",
        "citied_by": "89",
        "cover_date": "2021-12-01",
        "Abstract": "The ubiquity of social media and their deep integration in the contemporary society has granted new ways to interact, exchange information, form groups, or earn money—all on a scale never seen before. Those possibilities paired with the widespread popularity contribute to the level of impact that social media display. Unfortunately, the benefits brought by them come at a cost. Social Media can be employed by various entities to spread disinformation—so called ‘Fake News’, either to make a profit or influence the behaviour of the society. To reduce the impact and spread of Fake News, a diverse array of countermeasures were devised. These include linguistic-based approaches, which often utilise Natural Language Processing (NLP) and Deep Learning (DL). However, as the latest advancements in the Artificial Intelligence (AI) domain show, the model’s high performance is no longer enough. The explainability of the system’s decision is equally crucial in real-life scenarios. Therefore, the objective of this paper is to present a novel explainability approach in BERT-based fake news detectors. This approach does not require extensive changes to the system and can be attached as an extension for operating detectors. For this purposes, two Explainable Artificial Intelligence (xAI) techniques, Local Interpretable Model-Agnostic Explanations (LIME) and Anchors, will be used and evaluated on fake news data, i.e., short pieces of text forming tweets or headlines. This focus of this paper is on the explainability approach for fake news detectors, as the detectors themselves were part of previous works of the authors.",
        "DOI": "10.1038/s41598-021-03100-6",
        "paper_author": "Szczepański M.",
        "affiliation_name": "ITTI Sp. z o.o.",
        "affiliation_city": "Poznan",
        "affiliation_country": "Poland",
        "affiliation_id": "60159635",
        "affiliation_state": "WP"
    },
    {
        "paper_title": "Nlp Analysis In Social Media Using Big Data Technologies",
        "publication": "Telkomnika (Telecommunication Computing Electronics and Control)",
        "citied_by": "1",
        "cover_date": "2021-12-01",
        "Abstract": "For timely and efficient reactions to disasters, collecting vital and right information is essential. In recent decades, social media platforms such as Twitter, Facebook, Linkedin, Instagram have become valuable sources of information in disaster times. However, the reliability, volume, and velocity of information remain a major concern; this is particular about information issued from disaster locations. This paper proposes an approach for tracking the location of people in danger during times of disaster. The procedure is based on the Twitter application programming interface (API) by using natural language processing (NLP) and big data tools. A number of tweets were analyzed and an accuracy of 86.11% was actualized",
        "DOI": "10.12928/TELKOMNIKA.v19i6.19339",
        "paper_author": "Abu-Alsaad H.A.",
        "affiliation_name": "Mustansiriyah University",
        "affiliation_city": "Baghdad",
        "affiliation_country": "Iraq",
        "affiliation_id": "60071164",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Location analysis for arabic covid-19 twitter data using enhanced dialect identification models",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "9",
        "cover_date": "2021-12-01",
        "Abstract": "The recent surge of social media networks has provided a channel to gather and publish vital medical and health information. The focal role of these networks has become more prominent in periods of crisis, such as the recent pandemic of COVID-19. These social networks have been the leading platform for broadcasting health news updates, precaution instructions, and governmental procedures. They also provide an effective means for gathering public opinion and tracking breaking events and stories. To achieve location-based analysis for social media input, the location information of the users must be captured. Most of the time, this information is either missing or hidden. For some languages, such as Arabic, the users’ location can be predicted from their dialects. The Arabic language has many local dialects for most Arab countries. Natural Language Processing (NLP) techniques have provided several approaches for dialect identification. The recent advanced language models using contextual-based word representations in the continuous domain, such as BERT models, have provided significant improvement for many NLP applications. In this work, we present our efforts to use BERT-based models to improve the dialect identification of Arabic text. We show the results of the developed models to recognize the source of the Arabic country, or the Arabic region, from Twitter data. Our results show 3.4% absolute enhancement in dialect identification accuracy on the regional level over the state-of-the-art result. When we excluded the Modern Standard Arabic (MSA) set, which is formal Arabic language, we achieved 3% absolute gain in accuracy between the three major Arabic dialects over the state-of-the-art level. Finally, we applied the developed models on a recently collected resource for COVID-19 Arabic tweets to recognize the source country from the users’ tweets. We achieved a weighted average accuracy of 97.36%, which proposes a tool to be used by policymakers to support country-level disaster-related activities.",
        "DOI": "10.3390/app112311328",
        "paper_author": "Essam N.",
        "affiliation_name": "The Engineering Company for the Development of Digital Systems",
        "affiliation_city": "Giza",
        "affiliation_country": "Egypt",
        "affiliation_id": "127353628",
        "affiliation_state": "Giza"
    },
    {
        "paper_title": "A multilayer caru framework to obtain probability distribution for paragraph-based sentiment analysis",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "13",
        "cover_date": "2021-12-01",
        "Abstract": "Paragraph-based datasets are hard to analyze by a simple RNN, because a long sequence always contains lengthy problems of long-term dependencies. In this work, we propose a Multilayer Content-Adaptive Recurrent Unit (CARU) network for paragraph information extraction. In addition, we present a type of CNN-based model as an extractor to explore and capture useful features in the hidden state, which represent the content of the entire paragraph. In particular, we introduce the Chebyshev pooling to connect to the end of the CNN-based extractor instead of using the maximum pooling. This can project the features into a probability distribution so as to provide an interpretable evaluation for the final analysis. Experimental results demonstrate the superiority of the proposed approach, being compared to the state-of-the-art models.",
        "DOI": "10.3390/app112311344",
        "paper_author": "Ke W.",
        "affiliation_name": "Macao Polytechnic University",
        "affiliation_city": "Macau",
        "affiliation_country": "Macao",
        "affiliation_id": "60072904",
        "affiliation_state": "NA"
    }
]