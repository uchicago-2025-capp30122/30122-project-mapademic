[
    {
        "paper_title": "HATE SPEECH DETECTION USING LSTM AND NAÏVE BAYES ALGORITHM",
        "publication": "Journal of Theoretical and Applied Information Technology",
        "citied_by": "0",
        "cover_date": "2023-12-31",
        "Abstract": "Hate speech detection requires effective strategies to ensure a safe and inclusive online environment. This research paper presents a comparative study of hate speech detection using Natural Language Processing (NLP) techniques, specifically Naïve Bayes and Long Short-Term Memory (LSTM) approaches. The objective is to develop models capable of automatically identifying and analyzing hate speech in written language. The prevalence and impact of hate speech are emphasized, as it can lead to psychological harm and incite criminal acts. NLP offers a valuable tool for automatically detecting potentially dangerous content and addressing this problem. The study utilizes a dynamically generated dataset containing diverse words and expressions to train and evaluate the Naïve Bayes and LSTM models. The results show that the LSTM and the Naïve Bayes model, achieving an accuracy of 74% and 64%.",
        "DOI": "NA",
        "paper_author": "Nasution A.W.",
        "affiliation_name": "Bina Nusantara University",
        "affiliation_city": "Jakarta",
        "affiliation_country": "Indonesia",
        "affiliation_id": "60103610",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Survey of Threats to Research Literature-dependent Medical AI Solutions",
        "publication": "ACM Computing Surveys",
        "citied_by": "2",
        "cover_date": "2023-12-31",
        "Abstract": "Medical Artificial Intelligence (MedAI) harnesses the power of medical research through AI algorithms and vast data to address healthcare challenges. The security, integrity, and credibility of MedAI tools are paramount, because human lives are at stake. Predatory research, in a culture of \"publish or perish,\"is exploiting the \"pay for publish\"model to infiltrate he research literature repositories. Although, it is challenging to measure the actual predatory research induced data pollution and patient harm, our work shows that the breached integrity of MedAI inputs is a serious threat to trust the MedAI output. We review a wide range of research literature discussing the threats of data pollution in the research literature, feasible attacks impacting MedAI solutions, research literature-based tools, and influence on healthcare. Our contribution lies in presenting a comprehensive literature review, addressing the gap of predatory research vulnerabilities affecting MedAI solutions, and helping to develop robust MedAI solutions in the future.",
        "DOI": "10.1145/3592597",
        "paper_author": "Saini S.",
        "affiliation_name": "Texas A&amp;M University",
        "affiliation_city": "College Station",
        "affiliation_country": "United States",
        "affiliation_id": "60020547",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "A Survey of Adversarial Defenses and Robustness in NLP",
        "publication": "ACM Computing Surveys",
        "citied_by": "58",
        "cover_date": "2023-12-31",
        "Abstract": "In the past few years, it has become increasingly evident that deep neural networks are not resilient enough to withstand adversarial perturbations in input data, leaving them vulnerable to attack. Various authors have proposed strong adversarial attacks for computer vision and Natural Language Processing (NLP) tasks. As a response, many defense mechanisms have also been proposed to prevent these networks from failing. The significance of defending neural networks against adversarial attacks lies in ensuring that the model's predictions remain unchanged even if the input data is perturbed. Several methods for adversarial defense in NLP have been proposed, catering to different NLP tasks such as text classification, named entity recognition, and natural language inference. Some of these methods not only defend neural networks against adversarial attacks but also act as a regularization mechanism during training, saving the model from overfitting. This survey aims to review the various methods proposed for adversarial defenses in NLP over the past few years by introducing a novel taxonomy. The survey also highlights the fragility of advanced deep neural networks in NLP and the challenges involved in defending them.",
        "DOI": "10.1145/3593042",
        "paper_author": "Goyal S.",
        "affiliation_name": "Indian Institute of Technology Madras",
        "affiliation_city": "Chennai",
        "affiliation_country": "India",
        "affiliation_id": "60025757",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension",
        "publication": "ACM Computing Surveys",
        "citied_by": "89",
        "cover_date": "2023-12-31",
        "Abstract": "Alongside huge volumes of research on deep learning models in NLP in the recent years, there has been much work on benchmark datasets needed to track modeling progress. Question answering and reading comprehension have been particularly prolific in this regard, with more than 80 new datasets appearing in the past 2 years. This study is the largest survey of the field to date. We provide an overview of the various formats and domains of the current resources, highlighting the current lacunae for future work. We further discuss the current classifications of \"skills\"that question answering/reading comprehension systems are supposed to acquire and propose a new taxonomy. The supplementary materials survey the current multilingual resources and monolingual resources for languages other than English, and we discuss the implications of overfocusing on English. The study is aimed at both practitioners looking for pointers to the wealth of existing data and at researchers working on new resources.",
        "DOI": "10.1145/3560260",
        "paper_author": "Rogers A.",
        "affiliation_name": "Københavns Universitet",
        "affiliation_city": "Copenhagen",
        "affiliation_country": "Denmark",
        "affiliation_id": "60030840",
        "affiliation_state": "Hovedstaden"
    },
    {
        "paper_title": "Narrative Roads to Rebuild China's Global Image-Sentiment Analysis of Twitter Activities after China's Covid-19 Aid Activities in Italy",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-30",
        "Abstract": "As the digital era unfolds, social media supported by mobile devices and AI technologies are rapidly changing the way people communicate. The impact of public opinion formed on the social network platforms often has an enormous and somehow long-lasting influence on a country's foreign policy and international relations, and international interactions in the form of Twiplomacy are gradually attracting the attention of researchers. However, the attempts to analyze the impact that online public opinion has on actual policy or the inverse are still lacking or failed to explain the problem on the socioeconomic ground. There are very few theoretical explanations of how different pandemic narratives emerges and co-exists, and yet no article that analyzed how the positive and negative narratives about China spread and wrestled in their respective public opinion arenas during such a global event. Under certain analogous sense, Virus like the rumor \"China virus \"and vaccines like the encouraging reports we get from the \"China Aid Activities in Italy\"are fighting each other in the social media battleground and thus in every individual's mind which finally aggregated as the ideology. Hence, to explain the underlying motives for the public to accept and spread different pandemic narratives, we combined the modified SIR model and Glaeser(2005) ' political economic dynamic to cast light on its deep route in the background of the U.S.-China trade conflict and the U.S. presidential election-all three Black swans landed in the same Lake. Our research combined the sentiment semantic analysis methods from the AI-NLP field with the causal mechanism analysis to evaluate the differences in the attitudes of the Italian & international public towards China before and after the Chinese medical aid to Italy to fight the Covid-19 epidemic. Using the public data set from the GitHub project and other open-source scientific data, we applied the Bidirectional Encoder Representations from Transformers model (BERT) on individual tweets to include both emotional and non-emotional terms from a sentence sematic scale rather than word meaning scale which allows us to dig-in rich in topical information and estimate the international image trend of China during the 2020 February-March period. Here inspired by hybrid enrichment framework proposed by Gencoglu & Gruber (2020) but limited with the data, we choose the ARMA model instead to infer the sentimental level and the causality of medical aid event, empirical result shows that the positive change in Italy' public sentiment level is significantly affected by China's aid activities event here. The results of this article also provide insight into how China can improve its international image to promote the progress of many international regional economic and trade cooperation from the narrative perspective. Facts always speak louder than words, with which the adequate speed-up dissemination of fact is also important.",
        "DOI": "10.1145/3644479.3644482",
        "paper_author": "Liu W.",
        "affiliation_name": "Wuhan Business University",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60225899",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Towards kyrgyz stop words",
        "publication": "Kalbotyra",
        "citied_by": "0",
        "cover_date": "2023-12-28",
        "Abstract": "The concept of stop words introduced by H. P. Lun in the mid-20th century plays a huge role in today’s NLP practice. Stop words are used to reduce noisy text data, remove uninformative words, speed up text processing, and minimize the amount of memory required to store data. The Kyrgyz language is an agglutinative Turkic language for which no scientific study of stop words has been previously published in English. In our study, we combined frequency analysis with rule-based linguistic analysis. First, we found the most frequently used words, set a threshold, and removed words below the threshold. This way we got a list of the most frequently used words. Then we reduced the list by excluding from the list all words that do not belong to the category of function words of the Kyrgyz language. Finally, we got a list of 50 words that can be considered stop words in the Kyrgyz language. In our analysis, we used a single corpus of sentences collected and posted as an open source project by one of the local broadcasters.",
        "DOI": "10.15388/Kalbotyra.2023.76.4",
        "paper_author": "Isaev R.",
        "affiliation_name": "Ala-Too International University",
        "affiliation_city": "Bishkek",
        "affiliation_country": "Kyrgyzstan",
        "affiliation_id": "60112718",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "An analysis of optical character recognition-based machine translation for low resource languages",
        "publication": "Building Intelligent Systems Using Machine Learning and Deep Learning: Security, Applications and Its Challenges",
        "citied_by": "0",
        "cover_date": "2023-12-28",
        "Abstract": "Natural Language Processing (NLP) is a sub field under artificial intelligence that deals with the communication languages between computers and humans. A major process in NLP is machine translation which is the conversion of input text from one source language to target language without affecting the meaning. The input and output for NLP is natural language text. To implement the system, a new model for machine translation using Optical Character Recognition (OCR) is employed. The system also uses Bidirectional Recurrent Neural Network (BRNN) and dictionary-based approaches for English to Kannada and Marathi language translation. These two low resource languages are spoken in Indian state of Karnataka and Maharashtra respectively. The efficiency of the system is achieved through the improvement of accuracy and learning by comparing with different datasets and inputs.",
        "DOI": "NA",
        "paper_author": "Mahesha P.",
        "affiliation_name": "JSS Science and Technology University",
        "affiliation_city": "Mysore",
        "affiliation_country": "India",
        "affiliation_id": "60038307",
        "affiliation_state": "KA"
    },
    {
        "paper_title": "Collaboration network of applied linguistics research articles with different methodological orientations",
        "publication": "Studies in Second Language Learning and Teaching",
        "citied_by": "4",
        "cover_date": "2023-12-28",
        "Abstract": "The current study draws on synthetic techniques and bibliometric analysis to explore the patterns of scientific collaboration in light of methodological orientations. We examined 3,992 applied linguistics (AL) articles published in 18 top-tier journals from 2009 to 2018 and analyzed their methodological orientations and scientific collaboration. Considering that the number of co-authored papers outweighs single-authored counterparts, our results revealed that the overall degree of collaboration for AL journals was moderate-to-high (57.7%). In particular, quantitative studies contained the highest degree of collaboration (66.8%). This was followed by systematic reviews (60.9%), and mixed-methods approach (55.7%). Country-wise, our overall findings further indicated that the United States and the United Kingdom were the two main hubs of collaborative activities for quantitative, qualitative, and mixed-methods research. While the USA was the top country in systematic reviews like all other research approaches, the UK was the fifth country in systematic reviews. As for collaborating authors, our findings demonstrated that the most influential quantitative researchers had collaborated on Natural Language Processing (NLP) and data mining. While the mixed-methods researchers had a tendency to collaborate on conceptual issues subscribing to the language testing and assessment strand, the most productive qualitative researchers had collaborated on L2 writing issues. Implications for applied linguistics research are further discussed.",
        "DOI": "10.14746/ssllt.40214",
        "paper_author": "Farsani M.A.",
        "affiliation_name": "Iran University of Science and Technology",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran",
        "affiliation_id": "60012835",
        "affiliation_state": "Tehran"
    },
    {
        "paper_title": "Deep learning in news recommender systems: A comprehensive survey, challenges and future trends",
        "publication": "Neurocomputing",
        "citied_by": "9",
        "cover_date": "2023-12-28",
        "Abstract": "Nowadays, people prefer to read news articles from online sources worldwide due to their easiness and availability. For the last few years, online searching for required information or content has been replaced by item recommendation, and news recommendation is also not an exception. For news recommendations, News Recommender System (NRS) helps the users to find the appropriate and pertinent content, alleviate the problem of information overload, and propose news that would be of interest to news readers. NRS also assists different users all around the world in this regard by recommending the most recent news articles based on their interests and past preferences. Many techniques such as traditional, Deep Learning (DL), and hybrid have been proposed to solve the NRS challenges and issues. DL techniques are considered one of the best techniques and have been successfully applied in various fields such as Natural Language Processing (NLP) and Computer Vision (CV). This survey article provides a detailed analysis of DL models-based techniques to build NRS. In this regard, firstly, a comprehensive comparison is provided between published survey articles on NRS and this research work. Secondly, it discusses the background of recommendation systems and their techniques. Furthermore, NRS is explored along with its current research challenges. Then background knowledge of DL and its methods have been discussed along with the analysis of year-wise published relevant articles having DL as the applied technique. The survey also presents widely used datasets and performance evaluation metrics used in the relevant literature. Finally, a detailed discussion provides several future directions and open research challenges for the researchers to consider DL applications in NRS.",
        "DOI": "10.1016/j.neucom.2023.126881",
        "paper_author": "Talha M.M.",
        "affiliation_name": "COMSATS University Islamabad, Wah Campus",
        "affiliation_city": "Rawalpindi",
        "affiliation_country": "Pakistan",
        "affiliation_id": "60212764",
        "affiliation_state": "Punjab"
    },
    {
        "paper_title": "Prior Knowledge Augmentation Network for Aspect-based Sentiment Analysis",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-27",
        "Abstract": "Aspect-based sentiment analysis is a popular task in Natural Language Processing (NLP). Many methods utilize attention mechanisms and graph neural networks on dependency trees to identify the most relevant opinion words for aspects. However, improvements are limited due to the presence of rare words in small datasets and the inability to guarantee that each token will have a high probability of interacting with aspects. To address these challenges, we propose a Prior Knowledge Augmentation Network (PKAN) model that incorporates rich semantic information from knowledge graphs, syntactic structures, and prior probabilistic position embeddings. Specifically, to better comprehend semantic information, we design a knowledge augmentation module that constructs an adjacency matrix based on knowledge graphs, dependency trees and dataset co-occurrences. Additionally, to capture opinion words more comprehensively, we propose a probabilistic relative position embedding strategy. This strategy assumes that the semantic distribution of tokens at varying distances from the central word obeys a gaussian distribution and constrains the distance between words and aspects within one standard deviation. Experimental results on five public datasets validate the effectiveness of our model.",
        "DOI": "10.1145/3639479.3639489",
        "paper_author": "Feng M.",
        "affiliation_name": "Beijing University of Posts and Telecommunications",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60016930",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Robust Sentiment Classification Based on the Backdoor Adjustment",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-27",
        "Abstract": "Deep NLP models are correlation-based learning, which has a critical limitation of over-fitting over spurious features and shows poor generalization capability in the out-of-distribution (OOD) setting. Existing methods encourage the model to exploit causal features and exclude spurious correlations through Counterfactually Augmented Data (CAD) and feature regularization. However, those methods still face challenges due to the low quality of counterfactual generation and the high cost of annotation. This paper proposes an improved method for OOD generalization motivated by causal inference tools. Specifically, taking the topic of the text as the confounder of the input and the label, the model fits the causal correlation between the representations and the label through the backdoor adjustment to alleviate the exploitation of the spurious correlations. The proposed method is evaluated on the counterfactual adversarial test set (movie review text) and the challenging test set with synonym perturbation (financial news text) provided in the previous work. The experimental results show that this method improves the OOD generalization capability of the sentiment classification models in these two attacks while preserving the predictive ability, especially in the case of long text.",
        "DOI": "10.1145/3639479.3639488",
        "paper_author": "Dai L.",
        "affiliation_name": "Shanghai University of Finance and Economics",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60032744",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Sentiment and Interest Detection in Social Media using GPT-based Large Language Models",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-27",
        "Abstract": "In the ever-expanding realm of social media, deciphering sentiments and identifying relevant topics from various textual content is challenging. This paper comprehensively investigates sentiment analysis and interest/topic detection using cutting-edge language models, ChatGPT3.5 and gpt4All. The methodology encompasses data collection, meticulous text pre-processing, innovative prompt design, and the exploration of zero-shot, one-shot, and few-shot learning techniques. We unveil the nuances of model performance in sentiment analysis and interest/topic detection through a detailed comparative analysis. Our findings highlight the power of ChatGPT3.5 in achieving a substantial accuracy enhancement in sentiment analysis compared to gpt4All. Moreover, we delve into the intricacies of interest detection, demonstrating the complexities of linguistic structures and model biases. We offer a holistic view of social entities' preferences by categorizing topics into distinct domains. A web portal developed using Google's Flutter SDK facilitates the visualization of user-friendly sentiment and interest outcomes. This research contributes to the understanding of sentiment analysis and interest detection and underscores the evolving capabilities of AI and NLP in navigating the dynamic landscape of social media content.",
        "DOI": "10.1145/3639479.3639523",
        "paper_author": "Al Asad M.A.",
        "affiliation_name": "Clark Atlanta University",
        "affiliation_city": "Atlanta",
        "affiliation_country": "United States",
        "affiliation_id": "60012269",
        "affiliation_state": "GA"
    },
    {
        "paper_title": "Sentiment analysis: A review and comparative analysis of existing approaches",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-27",
        "Abstract": "Nowadays the introduction of artificial intelligence technologies into human life is at the peak of its history, including natural language processing (NLP). Consequently, it is becoming more necessary than ever to find new effective solutions for data labeling, on which machine models will be trained and appropriate algorithms will be built. In this paper, we describe the process of sentiment analysis (SA), as well as review approaches at all stages of analysis, publicly available datasets and produced software solutions within the Russian and foreign markets. In addition, we have traced the line of development of approaches for evaluating Russian-language texts in order to take into account the latest and most effective solutions in future work.",
        "DOI": "10.1145/3639479.3639520",
        "paper_author": "Mukhin I.",
        "affiliation_name": "University of Information Technologies Mechanics and Optics",
        "affiliation_city": "Saint Petersburg",
        "affiliation_country": "Russian Federation",
        "affiliation_id": "126614671",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Reinforcement Learning in Natural Language Processing: A Survey",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2023-12-27",
        "Abstract": "Reinforcement learning (RL) is a powerful technique for learning from data and feedback, but its effective application to natural language processing (NLP) tasks remains an open question. Consequently, this paper first introduces the general concepts of RL and the common approaches. Subsequently, we review the task construction settings and the application of RL for various NLP problems, such as machine translation, dialogue system, and text generation. Finally, we discuss some promising research directions and challenges of RL in NLP. We hope that our work can provide a comprehensive overview and inspire more research on this promising yet challenging topic.",
        "DOI": "10.1145/3639479.3639496",
        "paper_author": "Shen Y.",
        "affiliation_name": "Minzu University of China",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60032275",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Artificial intelligence-based data wrangling issues and data analytics process for various domains",
        "publication": "Advancement of Data Processing Methods for Artificial and Computing Intelligence",
        "citied_by": "0",
        "cover_date": "2023-12-26",
        "Abstract": "In day-to-day life, data plays a vital role, and it transforms from information to knowledge, wisdom also provides an opportunity in digital technology. All firms that want to accomplish this ambidexterity should depend on data as a fundamental resource for their operations and create data-driven business models. Applying several functions to the data, such as collecting, analyzing, and displaying it while utilizing all levels of knowledge can help build new products and customers, and reach goals at the appropriate moment. Due to the large volume of data increasing drastically, handling data is crucial for humans. Artificial intelligence allows machines to mimic human brain functions and is also used to identify data types, find relationship between datasets, and recognize knowledge. It also performs explore data, speed up data preparation, automation, and creation of data models. AI in data analytics can uncover ideas, discover new patterns and establish new relationship among the data. NLP plays an important role in data analytics for extracting data, text classification based on the keyword, sentiment analysis, and also AI predictions are accurate, because of access to huge amounts of data, enough power of data, and ML detects patterns and learns. Data wrangling is the process of handling complex data, involving various operations such as discovering, structuring, cleaning, validating, and also enriching the data for betterment. AI is giving the best solution for data wrangling and analytics in various domains, such as marketing, business, healthcare, banking, finance, etc.",
        "DOI": "NA",
        "paper_author": "Pandimurugan V.",
        "affiliation_name": "SRM Institute of Science and Technology",
        "affiliation_city": "Kattankulathur",
        "affiliation_country": "India",
        "affiliation_id": "60014340",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Year 2022 in Medical Natural Language Processing: Availability of Language Models as a Step in the Democratization of NLP in the Biomedical Area",
        "publication": "Yearbook of Medical Informatics",
        "citied_by": "3",
        "cover_date": "2023-12-26",
        "Abstract": "Objectives: To analyse the content of publications within the medical Natural Language Processing (NLP) domain in 2022. Methods: Automatic and manual preselection of publications to be reviewed, and selection of the best NLP papers of the year. Analysis of the important issues. Results: Three best papers have been selected. We also propose an analysis of the content of the NLP publications in 2022, stressing on some of the topics. Conclusion: The main trend in 2022 is certainly related to the availability of large language models, especially those based on Transformers, and to their use by non-NLP researchers. This leads to the democratization of the NLP methods. We also observe the renewal of interest to languages other than English, the continuation of research on information extraction and prediction, the massive use of data from social media, and the consideration of needs and interests of patients.",
        "DOI": "10.1055/s-0043-1768752",
        "paper_author": "Grouin C.",
        "affiliation_name": "Laboratoire Interdisciplinaire des Sciences du Numérique",
        "affiliation_city": "Orsay",
        "affiliation_country": "France",
        "affiliation_id": "60276635",
        "affiliation_state": "Ile-de-France"
    },
    {
        "paper_title": "Exploring the Latest Highlights in Medical Natural Language Processing across Multiple Languages: A Survey",
        "publication": "Yearbook of Medical Informatics",
        "citied_by": "10",
        "cover_date": "2023-12-26",
        "Abstract": "Objectives: This survey aims to provide an overview of the current state of biomedical and clinical Natural Language Processing (NLP) research and practice in Languages other than English (LoE). We pay special attention to data resources, language models, and popular NLP downstream tasks. Methods: We explore the literature on clinical and biomedical NLP from the years 2020-2022, focusing on the challenges of multilinguality and LoE. We query online databases and manually select relevant publications. We also use recent NLP review papers to identify the possible information lacunae. Results: Our work confirms the recent trend towards the use of transformer-based language models for a variety of NLP tasks in medical domains. In addition, there has been an increase in the availability of annotated datasets for clinical NLP in LoE, particularly in European languages such as Spanish, German and French. Common NLP tasks addressed in medical NLP research in LoE include information extraction, named entity recognition, normalization, linking, and negation detection. However, there is still a need for the development of annotated datasets and models specifically tailored to the unique characteristics and challenges of medical text in some of these languages, especially low-resources ones. Lastly, this survey highlights the progress of medical NLP in LoE, and helps at identifying opportunities for future research and development in this field.",
        "DOI": "10.1055/s-0043-1768726",
        "paper_author": "Shaitarova A.",
        "affiliation_name": "Universität Zürich",
        "affiliation_city": "Zurich",
        "affiliation_country": "Switzerland",
        "affiliation_id": "60012614",
        "affiliation_state": "ZH"
    },
    {
        "paper_title": "Paradigm shift of online education system due to COVID-19 pandemic: A sentiment analysis using machine learning",
        "publication": "Research Trends in Artificial Intelligence: Internet of Things",
        "citied_by": "0",
        "cover_date": "2023-12-22",
        "Abstract": "The COVID-19 epidemic has completely altered the environment and every aspect of every individual. The most affected part is the education system and the stakeholders associated with it. Organizations are currently being forced to adapt and alter their strategies in response to the new situation created by the COVID-19 epidemic. The proposed study gathers tweets on online schooling from social media sites like Twitter and Facebook comments in order to conduct a thorough sentiment analysis (SA) during the epidemic. The current study utilizes techniques for natural language processing (NLP) and machine learning (ML) to extract subjective data, establish polarity, and identify how people felt about the educational system prior to and following the COVID-19 crisis. The first step in the proposed study is to retrieve tweets using Twitter APIs before they are ready for rigorous preprocessing. One filtering method is Information Gain (IG). We will identify and examine the latent causes of the unpleasant feelings. We'll look at the machine-learning classification algorithm at the end. The proposed model will analyse the perceptions of people about the online educational system during COVID-19.",
        "DOI": "10.2174/9789815136449123010012",
        "paper_author": "Chapke P.P.",
        "affiliation_name": "C.O. E.T.",
        "affiliation_city": "Amravati",
        "affiliation_country": "India",
        "affiliation_id": "131756241",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Comprehensive study on deep-learning-based online course review analysis",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-22",
        "Abstract": "Under the impact of the pandemic, the acceptance toward online education increased. Therefore, we have witnessed increasing requirements to help the public to determine the quality of online courses. This research is related to sentiment analysis of feedback from the online course. During the process, we utilized the 458,280 reviews from Coursera, across the time from 2019 to 2020. First, to prepare for deep learning, the reviews were transformed by the TF-IDF feature. BiLSTM, Transformer (BERT-based), and LSTM with attention mechanisms were tested on the dataset. The LSTM+attention model produced a result with a precision of 95.41% and F1 score of 95.48%. Under the context of online course sentiment analysis, this study indicates the effectiveness of LSTM with attention.",
        "DOI": "10.1145/3660043.3660210",
        "paper_author": "Yang J.",
        "affiliation_name": "Jinan Foreign Language School",
        "affiliation_city": "Jinan",
        "affiliation_country": "China",
        "affiliation_id": "127727041",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Review: Data and Semantic Augmentation for Relation Classification in Low Resource",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-22",
        "Abstract": "Relation Classification (RC) is a significant study component in Natural Language Processing (NLP) that focuses on matching pairings of entities in natural utterances. Both traditional methods relying on rule matching and statistical features, as well as more contemporary methods utilizing deep learning and Pre-trained Language Model (PLM), excessively depends on vast quantities of data. In reality, numerous domains or subjects sometimes suffer from a scarcity of accessible data. Consequently, numerous academics have shifted their attention towards conducting research in low-resource domains, namely in areas such as semi-supervised learning and weakly supervised learning. However, both of these approaches bring a significant amount of noisy input into the model. Errors may arise in methods utilizing metric learning as a result of inappropriate metric selections. Prompt Learning (PL) has expanded its success in few-shot learning to also include RC tasks. Studies have been carried out to investigate the utilization of PL in enhancing the model's capacity to comprehend and learn textual content. This includes augmenting the sample data with prompt templates to enhance the model's ability to learn from a small amount of labeled data. This study presents a comprehensive overview of the latest research advancements in low-resource reading comprehension (RC). Additionally, it provides a summary of the few-shot RC technique based on pre-training and fine-tuning language models (PL). Lastly, the present challenges in research are examined, and the future trajectory of work on few-shot RC based on pre-training and language models are envisioned.",
        "DOI": "10.1145/3639631.3639665",
        "paper_author": "Li P.",
        "affiliation_name": "National University of Defense Technology China",
        "affiliation_city": "Changsha",
        "affiliation_country": "China",
        "affiliation_id": "60024350",
        "affiliation_state": "Hunan"
    },
    {
        "paper_title": "Aspect-level Information Discrepancies across Heterogeneous Vulnerability Reports: Severity, Types and Detection Methods",
        "publication": "ACM Transactions on Software Engineering and Methodology",
        "citied_by": "3",
        "cover_date": "2023-12-22",
        "Abstract": "Vulnerable third-party libraries pose significant threats to software applications that reuse these libraries. At an industry scale of reuse, manual analysis of third-party library vulnerabilities can be easily overwhelmed by the sheer number of vulnerabilities continually collected from diverse sources for thousands of reused libraries. Our study of four large-scale, actively maintained vulnerability databases (NVD, IBM X-Force, ExploitDB, and Openwall) reveals the wide presence of information discrepancies, in terms of seven vulnerability aspects, i.e., product, version, component, vulnerability type, root cause, attack vector, and impact, between the reports for the same vulnerability from heterogeneous sources. It would be beneficial to integrate and cross-validate multi-source vulnerability information, but it demands automatic aspect extraction and aspect discrepancy detection. In this work, we experimented with a wide range of NLP methods to extract named entities (e.g., product) and free-form phrases (e.g., root cause) from textual vulnerability reports and to detect semantically different aspect mentions between the reports. Our experiments confirm the feasibility of applying NLP methods to automate aspect-level vulnerability analysis and identify the need for domain customization of general NLP methods. Based on our findings, we propose a discrepancy-aware, aspect-level vulnerability knowledge graph and a KG-based web portal that integrates diversified vulnerability key aspect information from heterogeneous vulnerability databases. Our conducted user study proves the usefulness of our web portal. Our study opens the door to new types of vulnerability integration and management, such as vulnerability portraits of a product and explainable prediction of silent vulnerabilities.",
        "DOI": "10.1145/3624734",
        "paper_author": "Sun J.",
        "affiliation_name": "Commonwealth Scientific and Industrial Research Organisation",
        "affiliation_city": "Canberra",
        "affiliation_country": "Australia",
        "affiliation_id": "60029470",
        "affiliation_state": "ACT"
    },
    {
        "paper_title": "Classifying Religious Trends for Iraqi Politicians Tweets on social media Using Machine Learning Techniques",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2023-12-22",
        "Abstract": "Social media is considered one of the most important means of communication between influencers such as (politicians) and the masses, and it is also considered one of the most influential means on public opinion. Analyzing of social media contents are an important task for decision makers in many situations. One of these situations is the opinion of Iraqi politicians toward religion. Knowing these opinions effect the decision making and it can be used for prediction of the behavior of politicians. In this work, an approach was proposed for extracting the religion trends of Iraqi politicians. A dataset of 149,864 tweets for 101 Iraqi politicians were collected. Five classifiers were trained and tested on a collected dataset of 29,022 tweets that classified into religion and non-religion. These classifiers are K-nearest neighbor (KNN), random Forest, gradient boosting, gaussian naive bayes and linear support vector machine. The results show that the best classifier was SVM. The classifier was used to classify Iraqi politicians' dataset of 149,864 tweets of 101 Iraqi politicians for extracting the opinion of them to religion. Also, the religious discourse usage rate was calculated for each politician in the database. The results were 98%, 97.31%, 98.7%, 98.01% and 0.1457 for accuracy, precision, recall, f-measure and mean squared error respectively.",
        "DOI": "10.1063/5.0181906",
        "paper_author": "Jalil A.A.",
        "affiliation_name": "University of Alkafeel",
        "affiliation_city": "Najaf",
        "affiliation_country": "Iraq",
        "affiliation_id": "60118735",
        "affiliation_state": "Najaf"
    },
    {
        "paper_title": "Construction IPT data set: An Iraqi social media Political Dataset",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2023-12-22",
        "Abstract": "Social media platforms are an integral part of the lives of many people and are used as a meansof interaction, participation, and exchange of opinions, as well as used to change opinions and mobilize the masses in case of political aspect. Construction a database that collects texts and tweets written by politicians of various orientations helps in understanding political opinions and trends, as well as helps in understanding the political person himself and knowing the stability or change of his views over a time. In this paper, a dataset of Iraqi politicians' tweets was constructed and presented by collecting their tweets on the Twitter platform. These datasets were tested for extracting the relationship among the Iraqi politicians. Firstly, words embedding were used for representing the words in the entire document (all the tweets of one person) and they were used for construction one embedding into each political person. Lastly, the cosine similarity was used for extracting the relationship among the Iraqi politicians. The results showed that the precision, recall, f-measure and mean squared error were 83%, 85%, 84% and 0.035 respectively. The constructed dataset can be used for many applications of decision making.",
        "DOI": "10.1063/5.0181904",
        "paper_author": "Jalil A.A.",
        "affiliation_name": "University of Alkafeel",
        "affiliation_city": "Najaf",
        "affiliation_country": "Iraq",
        "affiliation_id": "60118735",
        "affiliation_state": "Najaf"
    },
    {
        "paper_title": "Arabic Named Entity Recognition Based on a Sequence-2-Sequence Model with Multi-Head Attention of Transformer Encoder",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2023-12-22",
        "Abstract": "Recurrent neural network variants such as gated recurrent unit (GRU) and long short-term memory (LSTM) are widely used in sequence labeling models and have shown excellent performance in Natural Language Processing (NLP) tasks, including Named Entity Recognition (NER). When it comes to Arabic language processing, most existing NER models employ word embedding to capture similarities between words. In such methods, it becomes challenging to deal with unseen words during inference. On the other hand, few Arabic NER models use attention mechanism to enhance sequence labeling tasks in natural language understanding. To extend the NER state-ofthe-art in the Arabic language, we propose an efficient NER model that leverages the encoder block of the Transformer where both word-level embeddings and character-level embeddings are adopted to solve the problem of out-of-vocabulary. The combined word-level embeddings, and character-level embeddings is fed to an encoder with bidirectional-BiLSTM. The output of the encoder is given to a Multi-head Self-attention layer. Our Multi-head Self-attention implements the encoder block of transformer consisting of a self-attention followed by a feed-forward network. Then, the Conditional Random Fields (CRF) layer performs the classification at the last layer. Our proposed model was trained and evaluated on two public datasets, namely, ANERCorp and AQMAR and compared to recent state-of-the-art papers that used these datasets and F1-measure as performance metric. We obtain an F1-measure of 92.40% on the merged dataset.",
        "DOI": "10.1063/5.0181993",
        "paper_author": "Alsultani H.S.M.",
        "affiliation_name": "University Of Diyala",
        "affiliation_city": "Baqubah",
        "affiliation_country": "Iraq",
        "affiliation_id": "60104392",
        "affiliation_state": "Diyala Governorate"
    },
    {
        "paper_title": "Named entity recognition for natural language understanding using BERT model",
        "publication": "AIP Conference Proceedings",
        "citied_by": "1",
        "cover_date": "2023-12-22",
        "Abstract": "Understanding natural languages are a very complex task for machines. Named entity recognition (NER) helps to understand natural language. NER aims to find mentions of specific identifiers in the text that belong to present semantic kinds such as person, place, organisation, time, money etc. In this paper a BERT (Bidirectional Encoder Representation from Transformation) model is trained with NER that improved the understanding of NLP. BERT is a transformer-based technique. It is trained and tested on CoNLL-2003 English dataset. The Proposed BERT based model gives 98.52% on English CoNLL-2003 NER dataset that is 4% greater than baseline models. The proposed BERT based NER model performed better results than other existing models. So, this features can enhance NLP applications in future like automatic text summarization, information retrieval systems, machine translation, machine reading comprehension, text classification etc.",
        "DOI": "10.1063/5.0181535",
        "paper_author": "Kumar S.",
        "affiliation_name": "Gautam Buddha University",
        "affiliation_city": "Greater Noida",
        "affiliation_country": "India",
        "affiliation_id": "60106285",
        "affiliation_state": "UP"
    },
    {
        "paper_title": "NLP-Based Management of Large Multiple-Choice Test Item Repositories",
        "publication": "Journal of Learning Analytics",
        "citied_by": "3",
        "cover_date": "2023-12-22",
        "Abstract": "Multiple-choice questions (MCQs) are widely used in educational assessments and professional certification exams. Managing large repositories of MCQs, however, poses several challenges due to the high volume of questions and the need to maintain their quality and relevance over time. One of these challenges is the presence of questions that duplicate concepts but are formulated differently. Such questions can indeed elude syntactic controls but provide no added value to the repository. In this paper, we focus on this specific challenge and propose a workflow for the discovery and management of potential duplicate questions in large MCQ repositories. Overall, the workflow comprises three main steps: MCQ preprocessing, similarity computation, and finally a graph-based exploration and analysis of the obtained similarity values. For the preprocessing phase, we consider three main strategies: (i) removing the list of candidate answers from each question, (ii) augmenting each question with the correct answer, or (iii) augmenting each question with all candidate answers. Then, we use deep learning–based natural language processing (NLP) techniques, based on the Transformers architecture, to compute similarities between MCQs based on semantics. Finally, we propose a new approach to graph exploration based on graph communities to analyze the similarities and relationships between MCQs in the graph. We illustrate the approach with a case study of the Competenze Digitali program, a large-scale assessment project by the Italian government. Notes for Practice • Established knowledge: Multiple-choice questions (MCQs) are a commonly used format in educational assessments. However, managing large repositories of MCQs can be challenging, especially when trying to avoid duplicate questions. Existing approaches rely on simple lexical-based methods that may not capture the semantic similarity between questions. • Contribution of the paper: This paper proposes a learning analytics tool that leverages deep learning–based natural language processing (NLP) techniques to compute the semantic similarity between MCQs. The tool allows users to visualize the similarity network of questions and set a threshold to identify possibly duplicate questions. Additionally, the paper proposes three strategies for processing MCQs before computing their similarity: stripping the list of candidate answers, enriching each question with the correct answer, and augmenting each question with all candidate answers. • Implications for practice: The proposed tool provides educators with a powerful means to manage large repositories of MCQs and mitigate the problem of duplicate questions. This can improve the effectiveness of assessments by ensuring that each question meaningfully contributes to the evaluation of student knowledge. Additionally, the tool can be used to match questions with areas of the syllabus, aiding in curriculum mapping and assessment design. Finally, the paper’s use of deep learning–based NLP techniques offers new opportunities to advance the development of educational assessment tools.",
        "DOI": "10.18608/jla.2023.7897",
        "paper_author": "Albano V.",
        "affiliation_name": "Dip. Funzione Pubblica",
        "affiliation_city": "Rome",
        "affiliation_country": "Italy",
        "affiliation_id": "109652767",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Developing a Dynamic Decision-Support Framework for Higher Education Management Systems through Real-time Information Extraction",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2023-12-21",
        "Abstract": "In the changing world of education, it is crucial for institutions to have the ability to make well informed decisions based on data. This paper presents an innovative Decision Support Framework (DDSF), for managing higher education systems. The DDSF utilizes real time information extraction to improve the decision-making process. It combines data analytics and user centric design to provide insights to education administrators. By gathering data from sources like records, financial reports and social media analytics the DDSF uses natural language processing (NLP) and machine learning (ML) algorithms to extract and interpret real time information. This information is then organized in a manner for analysis. The framework is adaptable allowing for the inclusion of emerging data streams ensuring relevance and usefulness. To evaluate the effectiveness of the DDSF we conducted a pilot study at a university. The results indicated improvements in meeting student needs optimizing resource allocation and enhancing operational efficiency. The framework also enables policy development by foreseeing challenges and identifying opportunities, within the education sector. The DDSF brings about a way of managing education by providing a strong platform for institutions to succeed in a competitive and constantly evolving environment. It emphasizes the significance of extracting real time information to guide planning and operational excellence. Future studies will concentrate on expanding the framework, for use, across institutions and incorporating analytics to improve foresight abilities.",
        "DOI": "10.1145/3644713.3644786",
        "paper_author": "Pantin R.",
        "affiliation_name": "Tashkent State University of Economics",
        "affiliation_city": "Tashkent",
        "affiliation_country": "Uzbekistan",
        "affiliation_id": "60071662",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Enhancing Arabic Information Retrieval for Question Answering",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "2",
        "cover_date": "2023-12-21",
        "Abstract": "In the modern landscape of Natural Language Processing (NLP), intelligent chatbots like ChatGPT 3.5 and Google's Bard have shown remarkable competence in generic question-answering (QA) tasks. However, their performance falters when navigating domain-specific QA, particularly in the Arabic language, which is celebrated for its complex morphology and syntax. This paper presents a comprehensive approach to address these issues. The aim of this research is to build a chatbot tailored for a university community. We first create an extensive Arabic Q&A dataset by extracting data from academic documents, employing state-of-the-art Optical Character Recognition (OCR) tools. Then, we evaluate multiple text similarity measures like Pooled FastText Word embedding, BM25 ranking functions, and various semantic sentence embedding models. A thorough performance assessment reveals that the domain-specific model excels at both sentence-level similarity and context-relevance tasks. The developed web application chatbot, leveraging LangChain library and Retrieval Augmented Generation (RAG) methods, outperforms existing chatbots in domain-specific, Arabic language QA scenarios.",
        "DOI": "10.1145/3644713.3644763",
        "paper_author": "Alghamdi M.",
        "affiliation_name": "King Fahd University of Petroleum and Minerals",
        "affiliation_city": "Dhahran",
        "affiliation_country": "Saudi Arabia",
        "affiliation_id": "60009506",
        "affiliation_state": "Ash Sharqiyah"
    },
    {
        "paper_title": "Enhancing user privacy in natural language processing (NLP) systems: Techniques and frameworks for privacy-preserving solutions",
        "publication": "Advanced Applications of Generative AI and Natural Language Processing Models",
        "citied_by": "1",
        "cover_date": "2023-12-21",
        "Abstract": "NLP has witnessed a remarkable improvement in applications, from voice assistants to sentiment analysis and language translations. However, in this process, a huge amount of personal data flows through the NLP system. Over time, a variety of techniques and frameworks have been developed to ensure that NLP systems do not ignore user privacy. This chapter highlights the significance of privacy-enhancing technologies (differential privacy, secure multi-party computation, homomorphic encryption, federated learning, secure data aggregation, tokenization and anonymization) in protecting user privacy within NLP systems. Differential privacy introduces noise to query responses or statistical results to protect individual user privacy. Homomorphic encryption allows computations on encrypted data to maintain privacy. Federated learning facilitates collaborative model training without sharing data. Tokenization and anonymization preserve anonymity by replacing personal information with non-identifiable data. This chapter explores these methodologies and techniques for user privacy in NLP systems.",
        "DOI": "10.4018/9798369305027.ch009",
        "paper_author": "Behera C.K.",
        "affiliation_name": "VIT Bhopal University",
        "affiliation_city": "Sehore",
        "affiliation_country": "India",
        "affiliation_id": "60119615",
        "affiliation_state": "MP"
    },
    {
        "paper_title": "Deep learning approaches for affective computing in text",
        "publication": "Advanced Applications of Generative AI and Natural Language Processing Models",
        "citied_by": "0",
        "cover_date": "2023-12-21",
        "Abstract": "The field of natural language processing (NLP) is one of the first to be addressed since artificial intelligence emerged. NLP has made remarkable advances in recent years thanks to the development of new machine learning techniques, particularly novel deep learning methods such as LSTM networks and transformers. This chapter presents an overview of how deep learning techniques have been applied to NLP in the area of affective computing. The chapter examines traditional and novel deep learning architectures developed for natural language processing (NLP) tasks. These architectures comprise recurrent neural networks (RNNs), long short-term memory (LSTM) networks, and the cutting-edge transformers. Moreover, a methodology for NLP method training and fine-tuning is presented. The chapter also integrates Python code that demonstrates two NLP case studies specializing in the educational domain for text classification and sentiment analysis. In both cases, the transformer-based machine learning model (BERT) produced the best results.",
        "DOI": "10.4018/9798369305027.ch015",
        "paper_author": "Cabada R.Z.",
        "affiliation_name": "Tecnológico Nacional de México",
        "affiliation_city": "Mexico City",
        "affiliation_country": "Mexico",
        "affiliation_id": "60134870",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Revolutionizing conversational AI: Unleashing the power of ChatGPT- based applications in generative AI and natural language processing",
        "publication": "Advanced Applications of Generative AI and Natural Language Processing Models",
        "citied_by": "6",
        "cover_date": "2023-12-21",
        "Abstract": "The emergence of advanced NLP models, like ChatGPT and other conversational AI models, has triggered a revolutionary transformation. This chapter explores the burgeoning field of ChatGPT applications, conducting a comprehensive analysis of their impact across various domains. The chapter assesses their capabilities, challenges, and potential uses, examining the underlying architecture and training methods that enable them to generate contextually relevant and coherent responses. Ethical considerations are also addressed, encompassing concerns about bias, misinformation, and user privacy in real-world conversations. The chapter also acknowledges drawbacks, including occasional inaccuracies or sensitive content generation. In conclusion, ongoing research is vital to enhance model robustness, user experience, and ethical deployment in conversational AI. ChatGPT and similar models are poised to reshape human-machine communication, fostering dynamic, engaging, and valuable conversations.",
        "DOI": "10.4018/9798369305027.ch011",
        "paper_author": "Babu C.V.S.",
        "affiliation_name": "Hindustan Institute of Technolgy and Science",
        "affiliation_city": "Chennai",
        "affiliation_country": "India",
        "affiliation_id": "122928943",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Overview of ChatGPT model architecture",
        "publication": "Advanced Applications of Generative AI and Natural Language Processing Models",
        "citied_by": "2",
        "cover_date": "2023-12-21",
        "Abstract": "This chapter provides a detailed exploration of the ChatGPT model architecture, a cutting-edge natural language processing (NLP) model that has revolutionized conversational AI. Developed by OpenAI, ChatGPT is built upon the GPT-3.5 architecture, a state-of-the-art language model. This chapter presents an extensive study about ChatGPT using a comprehensive analysis of its various recent literatures. This study also focuses on ChatGPT evolution from ELIZA to ChatGPT. In this chapter various reviews of literature, related issues, its architecture, various layers, various ChatGPT versions and its specialization, comparative study of various models, and application is presented. In order to do the comprehensive study various papers from different databases like ACM digital library, Scopus, IEEE, IGI Global, and Willey have been included for the study. Papers selected for the comprehensive study have been reviewed extensively in order to get the details and comprehended information for the readers. Various issues like security, biasness, training, misuse, etc. have been mentioned.",
        "DOI": "10.4018/9798369305027.ch005",
        "paper_author": "Pandey M.K.",
        "affiliation_name": "Pranveer Singh Institute of Technology",
        "affiliation_city": "Kanpur",
        "affiliation_country": "India",
        "affiliation_id": "60115426",
        "affiliation_state": "UP"
    },
    {
        "paper_title": "Simplifying learning experience on a personalized content recommendation system for complex text material in e-learning",
        "publication": "Advanced Applications of Generative AI and Natural Language Processing Models",
        "citied_by": "0",
        "cover_date": "2023-12-21",
        "Abstract": "Complex material is difficult to absorb in e-learning environments, which distracts and lowers learning outcomes. This initiative proposes that consumers watch simple movies with the same topic to improve learning. Text analytics recommends tailored videos to consumers. The algorithm can make more tailored recommendations by evaluating text interaction and learning preferences. The system simplifies learning and makes material more complicated and intelligible. Visual videos aid learning by improving memory and comprehension. Analyze the data before using the advice. NLP can extract key text content and context. Review results are used to develop related topics and themes. Next, find relevant video content using keywords and keywords list. Previous video data can be used to recommend video material. Videos should simplify content to help consumers understand and remember it. Text interactions should be considered when personalising video suggestions. Create user profiles using engagement indicators like time on page, scroll depth, and click behaviour.",
        "DOI": "10.4018/9798369305027.ch006",
        "paper_author": "Angeline R.",
        "affiliation_name": "SRM Institute of Science and Technology Ramapuram Campus",
        "affiliation_city": "Chennai",
        "affiliation_country": "India",
        "affiliation_id": "60117285",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Advanced applications of generative AI and natural language processing models",
        "publication": "Advanced Applications of Generative AI and Natural Language Processing Models",
        "citied_by": "30",
        "cover_date": "2023-12-21",
        "Abstract": "The rapid advancement of technology in Artificial Intelligence (AI), specifically in Natural Language Processing (NLP) and generative AI, presents a challenge for academic scholars to remain informed with the most current information and with the latest techniques and applications. The lack of comprehensive resources also hinders researchers' ability to gain a deep understanding of these subjects and effectively implement these technologies. Advanced Applications of Generative AI and Natural Language Processing Models is a comprehensive resource which offers an effective solution by investigating cutting-edge developments in NLP and generative AI. It provides insights into how these technologies function, their benefits, and the associated challenges. This book serves as a vital reference for deepening knowledge of advanced NLP techniques and staying updated on the latest advancements in generative AI for students, researchers, and professionals in AI, NLP, and computer science. It uses real-world examples and practical applications to empower scholars to apply their knowledge and solve complex problems. Advanced Applications of Generative AI and Natural Language Processing Models is a complete guide that equips researchers with the knowledge needed to utilize NLP and generative AI, including deep learning for NLP, generative models for text generation, language modeling and sentiment analysis, machine translation and multilingual models, ethical considerations in NLP, conversational AI applications, and case studies and real-world applications.",
        "DOI": "10.4018/9798369305027",
        "paper_author": "Obaid A.J.",
        "affiliation_name": "University of Kufa",
        "affiliation_city": "Kufa",
        "affiliation_country": "Iraq",
        "affiliation_id": "60071160",
        "affiliation_state": "Najaf"
    },
    {
        "paper_title": "Modern applications with a focus on training ChatGPT and GPT Models: Exploring generative AI and NLP",
        "publication": "Advanced Applications of Generative AI and Natural Language Processing Models",
        "citied_by": "8",
        "cover_date": "2023-12-21",
        "Abstract": "Generative AI (GAI) and natural language processing (NLP) have emerged as the most exciting and rapidly growing fields in artificial intelligence (AI). This book chapter provides a comprehensive exploration of the advanced applications of GAI and NLP models, with a specific focus on the renowned ChatGPT model. The chapter commences by offering a concise historical overview of the development of GAI and NLP, highlighting crucial milestones and advancements in the field over the period. In order to understand the workings of the current technology sensation, we will take a brief look at the basic building blocks of GPT models, such as transformers. Subsequently, the chapter delves into the introduction of ChatGPT, presenting an extensive overview of the model, elucidating its underlying architecture, and emphasizing its unique capabilities. Furthermore, it will illustrate the training process of the GPT model followed by a fine-tuning process to deal with the current model's shortcomings.",
        "DOI": "10.4018/9798369305027.ch010",
        "paper_author": "Kondurkar I.",
        "affiliation_name": "VIT Bhopal University",
        "affiliation_city": "Sehore",
        "affiliation_country": "India",
        "affiliation_id": "60119615",
        "affiliation_state": "MP"
    },
    {
        "paper_title": "Introduction to ChatGPT",
        "publication": "Advanced Applications of Generative AI and Natural Language Processing Models",
        "citied_by": "22",
        "cover_date": "2023-12-21",
        "Abstract": "This chapter provides a comprehensive overview of ChatGPT, an advanced language model that has gained significant attention in natural language processing (NLP) and artificial intelligence (AI). It outlines the underlying architecture, features, applications, benefits, and limitations of ChatGPT. The chapter highlights ChatGPT's ability to facilitate human-like conversations through its understanding and generation of human-like text. It explores its versatility across domains and languages and its potential in customer support, virtual assistants, chatbots, language translation, content generation, and creative writing. The benefits of ChatGPT, such as improved efficiency and scalability, are discussed, as are the limitations and ethical considerations. The chapter concludes with a future outlook, discussing ongo¬ing research and the potential impact of ChatGPT on communication and human-machine interactions, emphasizing the need for responsible development and deployment of this powerful language model.",
        "DOI": "10.4018/9798369305027.ch001",
        "paper_author": "Shafik W.",
        "affiliation_name": "Dig Connectivity Research Laboratory (DCRLab)",
        "affiliation_city": "Kampala",
        "affiliation_country": "Uganda",
        "affiliation_id": "130475843",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "KAPE: kNN-based Performance Testing for Deep Code Search",
        "publication": "ACM Transactions on Software Engineering and Methodology",
        "citied_by": "0",
        "cover_date": "2023-12-21",
        "Abstract": "Code search is a common yet important activity of software developers. An efficient code search model can largely facilitate the development process and improve the programming quality. Given the superb performance of learning the contextual representations, deep learning models, especially pre-trained languagemodels, have been widely explored for the code search task. However, studies mainly focus on proposing new architectures for ever-better performance on designed test sets but ignore the performance on unseen test data where only natural language queries are available. The same problem in other domains, e.g., CV and NLP, is usually solved by test input selection that uses a subset of the unseen set to reduce the labeling effort. However, approaches from other domains are not directly applicable and still require labeling effort. In this article, we propose the kNN-based performance testing (KAPE) to efficiently solve the problem without manually matching code snippets to test queries. The main idea is to use semantically similar training data to perform the evaluation. Extensive experiments on six programming language datasets, three state-of-theart pre-trained models, and seven baseline methods demonstrate that KAPE can effectively assess the model performance (e.g., CodeBERT achieves MRR 0.5795 on JavaScript) with a slight difference (e.g., 0.0261).",
        "DOI": "10.1145/3624735",
        "paper_author": "Guo Y.",
        "affiliation_name": "Luxembourg Institute of Science and Technology",
        "affiliation_city": "Esch-sur-Alzette",
        "affiliation_country": "Luxembourg",
        "affiliation_id": "60105942",
        "affiliation_state": "Esch-sur-Alzette"
    },
    {
        "paper_title": "A Study Towards Building Content Aware Models in NLP using Genetic Algorithms",
        "publication": "EAI Endorsed Transactions on AI and Robotics",
        "citied_by": "1",
        "cover_date": "2023-12-20",
        "Abstract": "INTRODUCTION: With the advancement in the large language models, often called LLMs, there has been increasing concerns around the usage of these models. As they can generate human-like text and can also perform a number of tasks such as generating code, question answering, essay writing and even generating text for research papers. OBJECTIVES: The generated text is subject to the usage of the original data (using which models are trained) which might be protected or may be personal/private data. The detailed description of such concerns and various potential solutions is discussed in ‘Generative language models and automated influence operations: Emerging threats and potential mitigations’. METHODS: Addressing these concerns becomes the paramount for LLMs usability. There are several directions explored by the researchers and one of the interesting works is around building content aware models. The idea is that the model is aware of the type of content it is learning from and aware what type of content should be used to generate a response to a specific query. RESULTS: In our work we explored direction by applying poisoning techniques to contaminate data and then applying genetic algorithms to extract the non-poisoned content from the poisoned content that can generate a good response when paraphrased. CONCLUSION: While we demonstrated the idea using poisoning techniques and tried to make the model aware of copyrighted content, the same can be extended to detect other types of contents or any other use cases where content awareness is required.",
        "DOI": "10.4108/airo.4078",
        "paper_author": "Tank U.",
        "affiliation_name": "PES University",
        "affiliation_city": "Bengaluru",
        "affiliation_country": "India",
        "affiliation_id": "60097260",
        "affiliation_state": "KA"
    },
    {
        "paper_title": "Special Issue NL4AI 2022: Workshop on natural language for artificial intelligence",
        "publication": "Intelligenza Artificiale",
        "citied_by": "0",
        "cover_date": "2023-12-20",
        "Abstract": "The 2022 edition of NL4AI was co-located with the 21st International Conference of the Italian Association for Artificial Intelligence (AIxIA 2022) and took place on November 30th in Udine, Italy. The call for papers attracted 17 submissions by 52 different authors from Italy (44), the UK (2), Algeria (4), and Germany (2). After the review process, 13 of 17 papers were accepted for publication (acceptance rate 76.47%). In terms of topics, the contributions to the workshop span from pure NLP works to broader proposals bridging NLP with other AI applications. Among the accepted articles, we selected two that we considered the most relevant and inspiring for the Italian Natural Language Processing and Artificial Intelligence communities. The authors of these papers have been invited to extend their contribution to this volume, creating an exclusive venue to provide visibility to their egregious work.",
        "DOI": "10.3233/IA-230057",
        "paper_author": "Nozza D.",
        "affiliation_name": "Università Bocconi",
        "affiliation_city": "Milan",
        "affiliation_country": "Italy",
        "affiliation_id": "60021796",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Factorized Recurrent Neural Network with Attention for Language Identification and Content Detection",
        "publication": "ACM Transactions on Asian and Low-Resource Language Information Processing",
        "citied_by": "0",
        "cover_date": "2023-12-19",
        "Abstract": "Language identification and content detection are essential for ensuring effective digital communication, and content moderation. While extensive research has primarily focused on well-known and widely spoken languages, challenges persist when dealing with indigenous and resource-limited languages, especially between closely similar languages such as Ethiopian languages. This article aims to simultaneously identify the language of a given text and detect its content, and to achieve this, we propose a novel attention-based recurrent neural network framework. The proposed method has an attention-embedded Bidirectional-LSTM architecture with two classifiers that identify the language of a given text and content within the text. The two classifiers share a common feature space before they branched at their task-specific layers where both layers are assisted by attention mechanism. We use five different topics in Six Ethiopian Languages the dataset consists of nearly 22,624 sentences. We compared our result with the classical NLP techniques, the proposed method shortened the data prepossessing steps. We evaluated the model performance using the accuracy metric, achieving results of 98.88% for language identification and 96.5% for text content detection. The dataset, source code, and pretrained model are available at https://github.com/bdu-birhanu/LID_TCD.",
        "DOI": "10.1145/3630607",
        "paper_author": "Belay B.H.",
        "affiliation_name": "Bahir Dar University",
        "affiliation_city": "Bahir Dar",
        "affiliation_country": "Ethiopia",
        "affiliation_id": "60071189",
        "affiliation_state": "Amhara"
    },
    {
        "paper_title": "AlgBERT: Automatic Construction of Annotated Corpus for Sentiment Analysis in Algerian Dialect",
        "publication": "ACM Transactions on Asian and Low-Resource Language Information Processing",
        "citied_by": "0",
        "cover_date": "2023-12-19",
        "Abstract": "Nowadays, sentiment analysis is one of the most crucial research fields of Natural Language Processing (NLP), and it is widely applied in a variety of applications such as marketing and politics. However, the Arabic language still lacks sufficient language resources to enable the tasks of opinion and emotion analysis comparing to other language such as English. Additionally, manual annotation requires a lot of effort and time. In this article, we address this problem and propose a novel automated annotation platform for sentiment analysis called AlgBERT by providing annotated corpus and using deep learning technology that includes many automatic natural language processing algorithms, which is the basis for text classification and opinion analysis. We suggest using BERT model as a method; it is the abbreviation of Bidirectional Encoder Representations from Transformers, as it is one of the most effective technologies in terms of results in different world languages. We used around of 54K comments collected from social networking (Twitter, YouTube) written in Arabic and Algerian dialects. Our AlgBERT system obtained excellent results with an accuracy of 91.04%, and this is considered as one of the best results for opinion analysis in Algerian dialect.",
        "DOI": "10.1145/3632948",
        "paper_author": "Hamadouche K.",
        "affiliation_name": "Université Oran 1",
        "affiliation_city": "Oran",
        "affiliation_country": "Algeria",
        "affiliation_id": "60068757",
        "affiliation_state": "Oran Province"
    },
    {
        "paper_title": "APPLICATION OF NATURAL LANGUAGE PARSING FOR IDENTIFYING NON-SURVEYED BOUNDARIES TOWARDS ENHANCED SYSTEMATIC LAND TITLING: RESULTS FROM PRELIMINARY EXPERIMENT",
        "publication": "Geodesy and Cartography (Vilnius)",
        "citied_by": "0",
        "cover_date": "2023-12-19",
        "Abstract": "The need for the adoption of systematic land titling (SLT) in Nigeria cannot be overemphasised. Nonetheless, the problems of speed and cost of geospatial data acquisition, as well as identification of non-surveyed boundaries, remain unresolved, impeding the effectiveness of SLT for non-surveyed boundaries. The integration of language into Artificial Intelligence (AI) has allowed Natural Language Parsing (NLP) to effectively serve as a tool for communication between humans and computer systems. This study presents preliminary results of testing a prototype application that utilises NLP to convert textual descriptions into graphic sketches as a tool towards the production of a-priori sketches that can aid SLT in non-surveyed boundaries. The study determines that NLP alone cannot be used to achieve the required accuracy in geospatial data for SLT; however, the study concludes that NLP can be integrated alongside other ancillary information to enhance SLT in peri-urban regions.",
        "DOI": "10.3846/gac.2023.18111",
        "paper_author": "Odumosu J.O.",
        "affiliation_name": "Federal University of Technology, Minna",
        "affiliation_city": "Minna",
        "affiliation_country": "Nigeria",
        "affiliation_id": "60012086",
        "affiliation_state": "Niger"
    },
    {
        "paper_title": "Resilience and Precision Assessment of Natural Language Processing Algorithms in Analog In-Memory Computing: A Hardware-Aware Study",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2023-12-18",
        "Abstract": "Natural Language Processing (NLP) serves as a cornerstone technology, facilitating complex human-computer interactions, enabling information retrieval, conducting sentiment analysis, and enhancing language comprehension. With the ever-growing use of NLPs, the conventional 'von Neumann' computing paradigm is rapidly approaching its inherent limitations. In response, Analog In-Memory Computing (AIMC) emerges as a compelling alternative, albeit accompanied by inherent non-idealities when deploying neural networks on such platforms. In this paper, we have evaluated the precision and resilience of various NLP algorithms when executed within the AIMC framework, both with and without the application of hardware-aware training. Our analysis reveals noteworthy insights: Gated Recurrent Unit (GRU) neural networks exhibit enhanced resilience to noise, yielding an average test error of 3.97% following hardware-aware training, as compared to their full precision counterparts. Conversely, Long Short-Term Memory (LSTM) networks demonstrate a slightly higher average test error of 5.67%, indicating a relatively lower tolerance to non-idealities. In contrast, Convolutional Neural Networks (CNNs) manifest a heightened vulnerability, exhibiting an average relative test error of 13.34%. Furthermore, we systematically investigate the sensitivity profiles of the selected neural networks in the presence of specific non-idealities, providing valuable insights into their robustness and susceptibility within the AIMC environment.",
        "DOI": "10.1145/3611315.3633266",
        "paper_author": "Parvaresh A.",
        "affiliation_name": "Technischen Universität Ilmenau",
        "affiliation_city": "Ilmenau",
        "affiliation_country": "Germany",
        "affiliation_id": "60030040",
        "affiliation_state": "Thuringen"
    },
    {
        "paper_title": "Mental stress detection using bidirectional encoder representations from transformers",
        "publication": "Machine Learning Algorithms Using Scikit and TensorFlow Environments",
        "citied_by": "0",
        "cover_date": "2023-12-18",
        "Abstract": "It seems as if people start losing control as they become easily upset, frustrated, and overwhelmed, having problems in resting and quieting their mind, and also feeling bad about themselves, lonely, worthless, and depressed, and avoiding others. If they have experienced the above symptoms, then there is a chance that they are suffering from mental stress. They have to take proper care of their mental health. Stress can be taken care of if it is properly handled and for that detection of stress or the mental state is necessary to provide proper care. The first step in stress detection is sentiment analysis of the users' daily conversations. The authors have proposed an NLP model and have trained it to produce a score for the input ranging between 0 and 1 where 0 is the negative end and 1 is the positive end. The trained model can predict the scores with an accuracy of above 92% on Twitter.",
        "DOI": "10.4018/978-1-6684-8531-6.ch013",
        "paper_author": "Vennila A.",
        "affiliation_name": "Kongu Engineering College",
        "affiliation_city": "Erode",
        "affiliation_country": "India",
        "affiliation_id": "60097144",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Blazing a New Trail in ERP Integration with NLP and Generative AI through APIs: a fraud examination perspective",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-16",
        "Abstract": "This research delves into integrating Natural Language Processing (NLP) and Generative AI within Enterprise Resource Planning (ERP) systems to bolster fraud prevention measures. The study commences by thoroughly mapping the fraud tree taxonomy with existing ERP applications to identify areas susceptible to fraudulent activities, encompassing corruption, asset misappropriation, and financial statement fraud. The research proposes the utilisation of specific NLP and Generative AI APIs tailored to address these areas, effectively building upon these identified vulnerabilities. By establishing standardised criteria for API development, the research provides a comprehensive roadmap for the accounting and finance profession to adopt and implement these advanced technologies to combat fraud more efficiently. The suggested roadmap encompasses crucial stages, including evaluating organisational requirements, assessing API providers, seamlessly integrating APIs into ERP systems, conducting thorough testing, and establishing robust monitoring and governance mechanisms. The findings underscore the tremendous potential of NLP and Generative AI in fortifying fraud prevention endeavours within ERP systems while highlighting the interdisciplinary nature of this research, which amalgamates insights from ERP systems, fraud detection, NLP, and Generative AI. The study also encourages future empirical investigations to validate and refine the proposed solutions within real-world contexts.",
        "DOI": "10.1145/3641032.3641065",
        "paper_author": "Faccia A.",
        "affiliation_name": "University of Birmingham",
        "affiliation_city": "Birmingham",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60019702",
        "affiliation_state": "West Midlands"
    },
    {
        "paper_title": "STACKOVERFLOW DATAWAREHOUSE SYSTEM",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-16",
        "Abstract": "This research addresses the challenge of extracting and analyzing data from Stack Overflow to uncover insights into programming language trends, community contributions, and talent availability. The aim is to provide developers, organizations, and managers with valuable information to make informed decisions. The methodology includes extracting data, cleaning and preprocessing it, loading it into a data warehouse, and creating interactive visualizations. Results show trends in programming languages, leading contributors, and potential talent pools. Conclusions highlight the research's significance in leveraging diverse datasets and attributes for better decision-making, while future work emphasizes the need for a more flexible deployment model, extended dashboard parameters, and integration of advanced techniques like data mining and NLP (Natural Language Processing). The uniqueness of this project is applying user-friendly Power BI on top of standard ETL advanced process and data warehousing techniques. Much more understandable for normal people even without data or tech background.",
        "DOI": "10.1145/3641032.3641057",
        "paper_author": "Khan S.",
        "affiliation_name": "University of Windsor",
        "affiliation_city": "Windsor",
        "affiliation_country": "Canada",
        "affiliation_id": "60012468",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "CoT-STS: A Zero Shot Chain-of-Thought Prompting for Semantic Textual Similarity",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-16",
        "Abstract": "The emergence of Large Language Models (LLMs) have revolutionized the field of Natural Language Processing (NLP) by changing the focus of technical development from features engineering, architecture engineering, and objective engineering to prompt engineering. The main goal of the prompt engineering is to craft clear and concise instructions, known as input prompts, for LLMs to effectively perform the targeted NLP task. Semantic Textual Similarity (STS) is one such significant NLP task, which aims to assess the similarity between the semantic meanings of two input sentences. Numerous approaches have been proposed in the literature, including syntactic similarity evaluations, word-embedding based methods, and dedicated model training. However, these approaches require substantial effort, such as creating extensive annotated datasets and training dedicated STS models. This research introduces CoT-STS, which aims to customize the use of the chain-of-Thought prompting with LLMs for the STS task. We proposed four influential factors as part of the Chain-of-Thought approach, including theme similarity, participating object similarity, similarity of the activity being carried out, and the evaluation of other factors before arriving at the final similarity assessments. The application of the proposed CoT-STS on the BIOSSES dataset achieved a Pearson's correlation of 0.72, surpassing the 0.45 correlation achieved by the standard prompting and the correlation of 0.71 achieved by the existing zero-shot CoT methodology. The result achieved demonstrates the potential of LLMs with an appropriate prompting strategy to significantly improve the performance of the STS task.",
        "DOI": "10.1145/3639592.3639611",
        "paper_author": "Hussain M.",
        "affiliation_name": "Kyung Hee University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea",
        "affiliation_id": "60001873",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Towards the Development of a Recommender System for an OTC Drug Dispenser",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-16",
        "Abstract": "In the age of healthcare digitization, the integration of cutting-edge technologies offers the potential to elevate medical services and enhance patient well-being. A novel solution in this regard is the AI-powered recommendation system for over-The-counter (OTC) drug dispensing, designed to bolster access to affordable and essential medications within neighborhood pharmacies. This drug dispenser employs advanced AI algorithms to provide precise, tailored medication recommendations and dispenses pre-packaged doses. Moreover, its continuous development and enhancement make it adaptable to the changing needs of its intended user base. Thoroughly assessed for its accuracy through intrinsic evaluations conducted by researchers, the system has demonstrated its efficacy in medication decisions. With the potential to enhance healthcare outcomes and reduce disparities in healthcare access, the AI-enabled OTC drug dispenser represents a new era of healthcare efficiency, personalization, and technological empowerment within local community pharmacies.",
        "DOI": "10.1145/3639592.3639608",
        "paper_author": "Dela Calzada W.J.P.",
        "affiliation_name": "University of the Immaculate Conception",
        "affiliation_city": "Davao",
        "affiliation_country": "Philippines",
        "affiliation_id": "60279736",
        "affiliation_state": "Davao del Sur"
    },
    {
        "paper_title": "ChatGPT and causality",
        "publication": "Studies in Science of Science",
        "citied_by": "0",
        "cover_date": "2023-12-15",
        "Abstract": "ChatGPT is a Natural Language Processing (NPL) Model driven by a new generation of artificial intelligence technology. The article discusses in depth the Causal Rules, Causal Representations and Causal Explanations of ChatGPT around causality. By analyzing the underlying logic in ChatGPT's causal cognition and learning, the following three understandings are obtained: First, ChatGPT uses correlation as its underlying causal rule, driven by Deep Neural Networks(DNNs), Large Language Model(LLM) and statistical analysis methods, and realizes the embedding of causality in artificial intelligence in the form of probability. Second, to realize the simulation of human's ability of causal cognition, causal language and causal inference, ChatGPT has a representation similar to human's intelligence in the generation of natural language. Third, relying on the two paths of Computational Causal Explanation and Causal Emergent Explanation generated by intelligence, ChatGPT realizes the input from symbols and information to the output of \" Data Knowledge\", and acquires new capabilities in an emergent way, realizing artificial intelligence The \" Cause and Effect Revolution\" in the field of artificial intelligence constructs a blueprint for the future development of artificial intelligence.",
        "DOI": "NA",
        "paper_author": "You Y.",
        "affiliation_name": "Shanxi University",
        "affiliation_city": "Taiyuan",
        "affiliation_country": "China",
        "affiliation_id": "60002222",
        "affiliation_state": "Shanxi"
    },
    {
        "paper_title": "Natural language processing with machine learning for security requirements analysis: practical approaches",
        "publication": "CyberSecurity in a DevOps Environment: From Requirements to Monitoring",
        "citied_by": "2",
        "cover_date": "2023-12-15",
        "Abstract": "Analyzing security requirements is a tedious task. Quite often they are spread around requirements specifications or specified in a very generic form. The experts have to make sure to extract all the security requirements and properly detail by applying the best practices from appropriate standards such as OWASP ASVS, STIG, or IEC62443. The requirements are specified in various forms, most commonly as statements in natural language. Natural language processing (NLP) has been applied for many years in requirements engineering (RE) for many analysis tasks. However, until recently, the performance on NLP methods on the RE tasks has been questionable. In this chapter, we outline the state of the art in the NLP methods in RE and in particular analysis of security requirements as well as provide practical recipes application of modern transfer learning architectures to several important RE tasks illustrated with an example.",
        "DOI": "10.1007/978-3-031-42212-6_2",
        "paper_author": "Sadovykh A.",
        "affiliation_name": "SOFTEAM",
        "affiliation_city": "Ivry-sur-Seine",
        "affiliation_country": "France",
        "affiliation_id": "130267324",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Data-Driven Analytical Framework for ESG-based Stock Investment Analytics using Machine Learning and Natural Language Processing",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-15",
        "Abstract": "This research explores the intricate relationship between sentiment analysis, stock market dynamics, and Environmental, Social, and Governance (ESG) based investment analytics, harnessing sentiment as a predictive tool for stock price movements. Leveraging Twitter data, Natural Language Processing (NLP), TextBlob, and the scikit-learn RandomForestRegressor, in combination with machine learning algorithms, the study evaluates public sentiment’s impact on stock prices, offering valuable insights to investors and risk managers. Moreover, the findings elucidate the potential to enhance ESG-based investment analytics by incorporating sentiment-derived insights into investment decision-making processes, which is particularly pertinent given the increasing market focus on sustainable investing. Experimental results unveil the potential of sentiment analysis in forecasting stock price changes and augmenting ESG investment strategies, underlining its utility as both a forecasting instrument and a risk management mechanism. However, the research also identifies challenges, including limitations of the Twitter API and the need for data refinement. Strategies to address these challenges are discussed, emphasizing the importance of diversifying data sources and enhancing data quality. This study advances our understanding of sentiment analysis in financial markets and its applicability to ESG-based investment analytics, offering data-driven guidance to navigate the complexities of the stock market landscape. Ultimately, it highlights the promising prospect of integrating social media sentiment analysis with machine learning for more informed stock market predictions, risk management, and sustainable investment strategy formulation.",
        "DOI": "10.1145/3638837.3638873",
        "paper_author": "Cao E.E.C.",
        "affiliation_name": "Issaquah High School",
        "affiliation_city": "Issaquah",
        "affiliation_country": "United States",
        "affiliation_id": "116163617",
        "affiliation_state": "WA"
    },
    {
        "paper_title": "TF-NERD: Tagalog Fine-grained Named Entity Recognition Dataset",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-15",
        "Abstract": "Named entity recognition (NER) is a crucial foundational step in information extraction. This research aims to improve existing natural language processing (NLP) resources for the low-resource language Tagalog by constructing a human-Annotated NER dataset. In this paper, we present TF-NERD, a fine-grained NER dataset that consists of 2,337 paragraphs written in Tagalog and scraped from various internet sources. These entities are classified into 12 categories-art, event, facility, geo-political, language, law, location, nationalities, organization, person, product, and other entities. Benchmark NER transformer models were constructed to assess the quality and usability of the dataset for future NER research. Based on the results, the NER model finetuned with the pretrained roBERTa language model performed best with 82.31% recall and 80.67% precision for entity type classification, and 88.02% recall and 86.27% precision for entity phrase identification.",
        "DOI": "10.1145/3639233.3639341",
        "paper_author": "Ramos R.K.",
        "affiliation_name": "Ateneo de Manila University",
        "affiliation_city": "Quezon City",
        "affiliation_country": "Philippines",
        "affiliation_id": "60071457",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "On Cognitive Level Classification of Assessment-items Using Pre-Trained BERT-based Model",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-15",
        "Abstract": "Outcome based education (OBE) is gaining popularity nowadays due to its effectiveness in preparing learners for their future roles as active participants. Bloom's taxonomy is a well-known educational model as it helps implement the OBE. It offers teachers a tool to encourage progressive thinking and intellectual development in students. Bloom's Taxonomy helps categorize cognitive skills and learning objectives, while the cognitive level classification of assessment items determines the complexity needed to complete the assessment. By using both the cognitive level classification and Bloom's Taxonomy, educators can create assessments that accurately measure students' abilities and target specific learning outcomes. This paper proposes a model that can accurately categorize assessment items according to their cognitive complexity. The paper utilizes the cognitive domains of Bloom's Taxonomy and focuses specifically on the first four levels: remembering, understanding, applying, and analysing. Some previous attempts have been made in this field but, to our best knowledge, this is the first time the Bidirectional architecture of Deep Learning (DL) algorithms and BERT-based Transformer model have been used. By comparing several machine learning (ML) algorithms, DL algorithms with Bidirectional layers and pre-Trained BERT-based Transformer model, it has been found that the BERT model scores the highest among all the other ML and DL algorithms. The performance is evaluated based on accuracy, precision, recall, and F1-score. The BERT-based Transformer model has an accuracy of 89%, where some of the other algorithms performed considerably well (BiLSTM: 84%, LSTM: 83%, BiGRU: 83%, RF: 83%). Also, compared to the state-of-The-Art model, the transformer model scored higher. This suggests that the model can be used to deploy as the classification model. The developed model will help educators with the exact assessment items to implement OBE. Overall, this research contributes to the field of NLP and educational assessment by demonstrating the potential of ML and DL approaches in accurately classifying assessment items.",
        "DOI": "10.1145/3639233.3639331",
        "paper_author": "Dipto A.S.",
        "affiliation_name": "East West University",
        "affiliation_city": "Dhaka",
        "affiliation_country": "Bangladesh",
        "affiliation_id": "60004766",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Sarcasm detection in product reviews using textual entailment approach",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2023-12-15",
        "Abstract": "Sarcasm is a form of sentiment characterized by the use of words that express the opposite of what is meant. Sarcasm detection has applications in multiple domains ranging from sentiment analysis in product reviews to user feedback, and online forums. Sarcasm detection is important to understand user opinions and intentions in areas such as sentiment-based classification and opinion mining. This can result in better product development and customer service. Sarcasm detection can be a challenging task because sarcastic sentences may use positive expressions to convey negative meanings or may use negative sentences to convey positive meanings. Also, sarcastic sentences form a very small component of the entire communication. The increasing use of sarcasm in various social media such as Twitter, Reddit, Amazon product reviews, etc. has highlighted the importance of detecting and understanding sarcasm in various contexts. Sarcasm detection is a challenging problem for NLP systems that often rely on statistical models for performing sentiment analysis. In this research, the focus is on the use of a textual entailment approach for detecting sarcasm. Textual entailment is a natural language inference task that involves determining whether one text (hypothesis) can be derived from another text (premise). The underlying assumption behind this approach is that - if there is a contradiction between the premise and hypothesis, we can say that the hypothesis is sarcastic. To test our approach, an annotated corpus of 3000 product reviews was developed methodically from the Amazon Reviews dataset and tested using the textual entailment approach. The proposed approach achieved an F1 score of 0.76 on this dataset. The result is better than the baseline considered which is the BERT binary classifier which gives an F1 score of 0.48 on the same dataset.",
        "DOI": "10.1145/3639233.3639252",
        "paper_author": "Sinha S.",
        "affiliation_name": "Visvesvaraya National Institute of Technology, Nagpur",
        "affiliation_city": "Nagpur",
        "affiliation_country": "India",
        "affiliation_id": "60015785",
        "affiliation_state": "MH"
    },
    {
        "paper_title": "Named entity recognition for setswana language: A conditional random fields (CRF) approach",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-15",
        "Abstract": "Named Entity Recognition (NER) is a fundamental task in Natural Language Processing (NLP) focused on identifying entities like individuals, organizations, and locations within text. Locating these entities can present initial challenges, and subsequent classification can be equally daunting. This complexity is exemplified in Setswana, where shared naming between locations and personal names adds an extra layer of intricacy. This study introduces a Setswana NER approach, featuring a Setswana Regex Annotator (SERxA) for preliminary entity classification, followed by BRAT tool annotation. Employing the Conditional Random Fields (CRF) algorithm, we establish a supervised statistical machine learning NER model for Setswana. Evaluation using standard metrics on a held-out test set attains impressive F1-scores of 0.94 for person entities and 0.79 for location entities. Our findings underscore the viability of NER in Setswana and emphasize the necessity of nurturing NLP resources for less-resourced languages.",
        "DOI": "10.1145/3639233.3639234",
        "paper_author": "Okgetheng B.",
        "affiliation_name": "University of Botswana",
        "affiliation_city": "Gaborone",
        "affiliation_country": "Botswana",
        "affiliation_id": "60021221",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Fine-Tuning BERT on Twitter and Reddit Data in Luganda and English",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-15",
        "Abstract": "Deep learning techniques, driven by the Transformer architecture and models like BERT, find broad utility. While sentiment analysis in high-resource languages is well-established, it's largely unexplored in low-resource ones. Our focus is on Luganda, a prevalent Ugandan language, spoken by over 21 million people. We utilized three datasets, from social media, to train machine learning models as baseline models and used BERT for deep learning. Our findings enhance sentiment analysis in both Luganda and English. Our approach for data extraction aids domain-specific dataset construction. This research advances NLP and aligns with global deep-learning initiatives.",
        "DOI": "10.1145/3639233.3639344",
        "paper_author": "Kimera R.",
        "affiliation_name": "Handong Global University",
        "affiliation_city": "Pohang",
        "affiliation_country": "South Korea",
        "affiliation_id": "60094234",
        "affiliation_state": "Gyeongsangbuk-do"
    },
    {
        "paper_title": "A Study of Dialog Summarization Across Datasets and Domains",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2023-12-15",
        "Abstract": "This study of dialog summarization covers multi-domain, multimodal and multilingual datasets, and the potential challenges in the different domains. The scope and progress of this rapidly evolving topic rely on the availability of datasets and emerging domains. Such a study can facilitate the cross-application of datasets to different domains to refine models and also aid scenarios where there is a lack of data in privacy-sensitive settings. Further, our work can enable the cross-fertilization of ideas across domains and in different contexts. Our study encompasses current and emerging domains, a comprehensive compilation of datasets, and avenues for further research.",
        "DOI": "10.1145/3639233.3639245",
        "paper_author": "Ranganathan A.",
        "affiliation_name": "Labs",
        "affiliation_city": "Bangalore",
        "affiliation_country": "India",
        "affiliation_id": "106382885",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Exploring Naive Approaches to Tell Apart LLMs Productions from Human-written Text",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-15",
        "Abstract": "Powerful Large Language Models (large LMs or LLMs) such as BERT and GPT are making the task of detecting machine-generated text more and more prominent and crucial to minimize threats posed by text generation models misuse. Nonetheless, only a limited number of efforts exist so far, which can be classified into simple classifiers, zero-shot approaches, and fine-Tuned LMs. These approaches usually rely on LMs whose discrimination accuracy decreases as the size difference in favor of the generator model increases (hence, a detector should always employ a LM with at least the same number of parameters of the source LM). Also, most of these approaches do not explicitly investigate whether the sentence syntactic structure can provide additional information that helps to build better detectors. All these considerations make the generalizing ability of detection methods into question. While generation techniques become more and more capable of producing human-like text, are the detection techniques capable of keeping up if not properly trained? In this paper, we evaluate the most effective (and reproducible) detection method available in the state of the art in order to figure out the limits in its robustness. We complement this analysis by discussing results obtained using a novel naive approach that demonstrably achieves comparable results in terms of robustness with respect to much more advanced and sophisticated state-of-The-Art methods. Code with details on experiments are available at: https://github.com/bancaditalia/gen-Text-detect.",
        "DOI": "10.1145/3639233.3639354",
        "paper_author": "Giudice O.",
        "affiliation_name": "Banca d'Italia",
        "affiliation_city": "Rome",
        "affiliation_country": "Italy",
        "affiliation_id": "60082963",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Extracting structured information from the textual description of geometry word problems",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-15",
        "Abstract": "AI (Artificial Intelligence) is playing its role in every field as automation is the way forward. Solving geometry mathematics word problems (MWPs) automatically can help in smart tutoring and has a plethora of applications in many fields. To solve a geometry MWPs, multi-modal methods are needed to parse the question text using Natural Language Processing (NLP) and diagram using Image Processing (IP) techniques and combine the information from both. The information is then to be structured and appropriate axioms and theorems need to be applied to transform the question into intermediate equations which could be solved by equation solvers. In this entire pipeline, text parsing of the question content is a crucial component which is precisely the main focus of this paper. There have been attempts in the literature and techniques to solve geometry MWPs have been proposed, however they rely on input predicates and do not generate them automatically. In cases where they are generated, regular expressions are used which lack generalisation and scalability. This paper models the text parsing problem as a Relation Extraction and Natural Language Generation problem where from the question text structured information is generated in the form of predicates. We basically generate appropriate geometric tags by using a Named Entity Recognition (NER) annotator and then utilize these tags to generate predicates that represent the question in a machine readable formal language. To test the proposed approach, a custom dataset is created containing about 500 questions from standard elementary school-level Indian text books. It has been demonstrated through experiments on the custom dataset that the suggested method is capable of extracting geometric relations. To evaluate the generated predicates, we create ground truth predicates for each of the questions through skilled domain experts. We also design a unique method to evaluate the generated predicates with the METEOR (Metric for Evaluation of Translation with Explicit ORdering) metric. The experimental findings on this dataset indicate precision (P) of 0.70, recall (R) of 0.60, harmonic mean (Fmean) of 0.60, average penalty (p) of 0.12 and final METEOR Score (M) of 0.54. Furthermore, we experiment the proposed technique on another dataset. When tested on Geometry3K data set, a precision of 0.67, recall of 0.64, harmonic mean (Fmean) of 0.64, penalty (p) of 0.27 and final METEOR Score (M) of 0.42 is obtained.",
        "DOI": "10.1145/3639233.3639255",
        "paper_author": "Boob A.N.",
        "affiliation_name": "Visvesvaraya National Institute of Technology, Nagpur",
        "affiliation_city": "Nagpur",
        "affiliation_country": "India",
        "affiliation_id": "60015785",
        "affiliation_state": "MH"
    },
    {
        "paper_title": "Enhancing Medical Specialty Assignment to Patients using NLP Techniques",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-15",
        "Abstract": "The introduction of Large Language Models (LLMs), and the vast volume of publicly available medical data, amplified the application of NLP to the medical domain. However, LLMs are pretrained on data that are not explicitly relevant to the domain that are applied to and are often biased towards the original data they were pretrained upon. Even when pretrained on domain-specific data, these models typically require time-consuming fine-tuning to achieve good performance for a specific task. To address these limitations, we propose an alternative approach that achieves superior performance while being computationally efficient. Specifically, we utilize keywords to train a deep learning architecture that outperforms a language model pretrained on a large corpus of text. Our proposal does not require pretraining nor fine-tuning and can be applied directly to a specific setting for performing multi-label classification. Our objective is to automatically assign a new patient to the specialty of the medical professional they require, using a dataset that contains medical transcriptions and relevant keywords. To this end, we fine-tune the PubMedBERT model on this dataset, which serves as the baseline for our experiments. We then twice train/fine-tune a DNN and the RoBERTa language model, using both the keywords and the full transcriptions as input. We compare the performance of these approaches using relevant metrics. Our results demonstrate that utilizing keywords for text classification significantly improves classification performance, for both a basic DL architecture and a large language model. Our approach represents a promising and efficient alternative to traditional methods for fine-tuning language models on domain-specific data and has potential applications in various medical domains.",
        "DOI": "10.1145/3639233.3639251",
        "paper_author": "Solomou C.",
        "affiliation_name": "University of York",
        "affiliation_city": "York",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60016418",
        "affiliation_state": "North Yorkshire"
    },
    {
        "paper_title": "Explainable Machine Learning Models for Swahili News Classification",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-15",
        "Abstract": "Although Swahili is considered a well-resourced language, challenges persist in utilizing it for Natural Language Processing (NLP) tasks, primarily due to the limited data availability required for these systems. For instance, obtaining sufficient Swahili news data for classification remains a significant obstacle. This paper addresses the problem of accurate Swahili news classification by leveraging classical machine learning (ML) models and deep neural networks (DNN). Our proposed method involves data acquisition, Exploratory Data Analysis (EDA), and employing modelling techniques using classical ML models, such as Support Vector Machine (SVM), Logistic Regression, Multinomial Naive Bayes, Random Forest, Gradient Boosting, Hard Voting, and Bagging, as well as DNN models including Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), Bidirectional LSTM (Bi-LSTM), and CNN-Bi-LSTM + Attention. The models were evaluated using Area Under the Curve (AUC) metrics and accuracy. Our results demonstrate commendable performance for classical ML classifiers and DNN models, with accuracies above 75%. Notably, the CNN-Bi-LSTM + Attention model achieved an impressive AUC score of 97%. Additionally, explainability using LIME (Local Interpretable Model-agnostic Explanations) provided valuable insights into model decisions. This research contributes to Swahili natural language processing and lays the foundation for further explorations into transformer-based models for improved classification.",
        "DOI": "10.1145/3639233.3639250",
        "paper_author": "Murindanyi S.",
        "affiliation_name": "Makerere University",
        "affiliation_city": "Kampala",
        "affiliation_country": "Uganda",
        "affiliation_id": "60071676",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Empowering Precision in Financial News: A Revolution in Editorial Classification through Cutting-Edge Natural Language Processing",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2023-12-15",
        "Abstract": "Amidst the continuous stream of diverse data on the Bloomberg terminal, distinguishing editorial news articles from regular articles is critical to aid its users in tailoring their news experience and further analyzing the impact of news on global financial markets. In this paper, we propose various Artificial Intelligence and Neural Networks models regarding developing an editorial classifier that generalizes well across various news sources. The training set comprises articles published by news sources from the US. We compare the performance of these models using the Aggregate F1-measure and Binary Classification Performance Metric as evaluation metrics to account for the presence of class imbalance in our data. Further, we gauged our models by comparing their performance on a Zero-Shot dataset which comprised 1805 news articles published by Metro Winnipeg, a Canadian news source.",
        "DOI": "10.1145/3639233.3639343",
        "paper_author": "Khunti S.",
        "affiliation_name": "Chandigarh University",
        "affiliation_city": "Mohali",
        "affiliation_country": "India",
        "affiliation_id": "60111704",
        "affiliation_state": "PB"
    },
    {
        "paper_title": "Towards Semantic Role Labeling of Legal Documents for Improving NLP/IR Tasks in the Law Domain",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-15",
        "Abstract": "Abstract The exponential growth of the number of legal documents online in digital format has posed unforeseen challenges for legal practitioners. Legal documents are often long, and complex, and handling them efficiently needs sophisticated technologies. A pivotal characteristic of a Common Law System is that it accords great importance to prior cases. The lack of familiarity with the law is another serious problem for the common man. In the scope of my doctoral thesis, my primary aim is to develop assistive tools and techniques for use by legal practitioners as well as by the common man. I plan to extract important events or fine-grained information and leverage these to expedite tasks such as finding pertinent prior case documents, summarising legal text, and so on. We strive to create methods that can assist the average person with legal questions, as well as increase public awareness of the law.",
        "DOI": "10.1145/3632754.3632774",
        "paper_author": "Adhikary S.",
        "affiliation_name": "Indian Institute of Science Education and Research Kolkata",
        "affiliation_city": "Mohanpur",
        "affiliation_country": "India",
        "affiliation_id": "60103615",
        "affiliation_state": "WB"
    },
    {
        "paper_title": "L3Cube-MahaSocialNER: A Social Media based Marathi Named Entity Recognition Dataset and BERT models",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-15",
        "Abstract": "This work introduces the L3Cube-MahaSocialNER dataset, the first and largest social media dataset specifically designed for Named Entity Recognition (NER) in the Marathi language. The dataset comprises 18,000 manually labeled sentences covering eight entity classes, addressing challenges posed by social media data, including non-standard language and informal idioms. Deep learning models, including CNN, LSTM, BiLSTM, and Transformer models, are evaluated on the individual dataset with IOB and non-IOB notations. The results demonstrate the effectiveness of these models in accurately recognizing named entities in Marathi informal text. The L3Cube-MahaSocialNER dataset offers user-centric information extraction and supports real-time applications, providing a valuable resource for public opinion analysis, news, and marketing on social media platforms. We also show that the zero-shot results of the regular NER model are poor on the social NER test set thus highlighting the need for more social NER datasets. The datasets and models are publicly available at https://github.com/l3cube-pune/MarathiNLP",
        "DOI": "10.1145/3632754.3632764",
        "paper_author": "Chaudhari H.V.",
        "affiliation_name": "Pune Institute of Computer Technology",
        "affiliation_city": "Pune",
        "affiliation_country": "India",
        "affiliation_id": "60272084",
        "affiliation_state": "MH"
    },
    {
        "paper_title": "Unleashing the Power of Large Language Models: A Hands-On Tutorial",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2023-12-15",
        "Abstract": "LLMs have opened up possibilities for advancing the state-of-the-art in natural language processing (NLP). In this tutorial, we present the audience with an introduction to LLMs and the associated challenges. The tutorial is structured in the following manner. First, we provide a brief preface that outlines the fundamental principles of NLP, following which, we explore the area of distributional representation learning for NLP. Then, we delve into the essential component of transformer-based pretrained language models. We then follow this up with the concept of prompt learning or in-context learning (ICL) and discuss how it is emerging as a popular methodology replacing the conventional supervised learning workflow comprised of pretraining and fine-tuning. We outline the research challenges in ICL, which usually involves finding the correct set of examples and contexts for the purpose of guiding the LLM decoder towards effective predictions. Afterwards, a hands-on coding and demonstration session will be carried out to impart practical knowledge about LLMs and ICL to the tutorial participants.",
        "DOI": "10.1145/3632754.3632943",
        "paper_author": "Santra P.",
        "affiliation_name": "Indian Association for the Cultivation of Science",
        "affiliation_city": "Kolkata",
        "affiliation_country": "India",
        "affiliation_id": "60014761",
        "affiliation_state": "WB"
    },
    {
        "paper_title": "A Keyphrase-Centric Search Engine for Scientific Papers",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-15",
        "Abstract": "The ever increasing interest and participation in scientific research and development has been a major contributor towards the exponential increase of textual and multi-modal scientific data on the Web over the past few years. It is imperative for scientists to keep track of the latest development in their respective area of research, but the ever-expanding landscape of research publications makes it quite challenging. Automating the parsing, retrieval, and understanding of this literature through state-of-the-art NLP and IR tools is the need of the hour. The goal of this doctoral work will be to develop new tools singularly for the low-resource scientific domain. We propose to contribute to this field by not only developing deep learning solutions for these tasks, but also providing the NLP and IR community with unique challenging datasets wherever necessary.",
        "DOI": "10.1145/3632754.3632772",
        "paper_author": "Lahiri A.",
        "affiliation_name": "Indian Association for the Cultivation of Science",
        "affiliation_city": "Kolkata",
        "affiliation_country": "India",
        "affiliation_id": "60014761",
        "affiliation_state": "WB"
    },
    {
        "paper_title": "Comparative Study of Different Machine Learning Models for Detecting Spam Tweet",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2023-12-15",
        "Abstract": "In recent times, the increase in use of electronic gadgets and evolving technologies have paved way to gather and share information across the globe through a well-built framework, social networking platforms. Twitter has grown to become one of the key platforms for communication and news circulation. Millions of users today rely on the content available on social media to make decisions, hence detecting and deleting spam details is critical. This paper focuses on conducting a systematic study on different approaches of detecting spam tweets by making use of the tweet content. This work brings a comparative study between ways of detecting spam tweets by making use of different features of tweet like URLs and mentions and analyzing the tweet subject on a publicly available dataset,” social honeypot dataset”. To detect a spam tweet using different features present in spam, Machine Learning algorithms like – SVM, Naïve Bayes, Random Forest algorithms are used. To detect a spam tweet by analyzing the content of tweet Machine learning algorithms- SVM, Naïve Bayes, Random Forest, Light GBM, BERT model is used. By this study we could conclude that content analysis using NLP techniques gave better accuracy on the dataset.",
        "DOI": "10.1063/5.0178994",
        "paper_author": "Sanjana G.",
        "affiliation_name": "PES University",
        "affiliation_city": "Bengaluru",
        "affiliation_country": "India",
        "affiliation_id": "60097260",
        "affiliation_state": "KA"
    },
    {
        "paper_title": "Assamese Document Classification Using CNN, Multichannel CNN and CNN-SVM",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2023-12-15",
        "Abstract": "The last few years have witnessed an enormous rise of digital documents or contents in the Assamese language on online platforms. This can be attributed primarily to easy access to the Internet. Because of this, a huge collection of unstructured text has been created, which needs to be organized into recognizable categories depending on the content. But the manual organization of such data will consume much effort and time. Therefore, an attempt has been made here to present automated Assamese Text classification systems based on CNN, multichannel CNN and a hybrid model using CNN-SVM, where linear SVM was embedded in the last layer of the CNN. For this research, 634 documents were considered, which were categorized under four classes viz Arts, Children, History and Sports. An accuracy of 96% has been achieved with the CNN. It is observed that the intelligent classification of Assamese documents is quite challenging due to the non-availability of a larger Assamese text corpora and intelligent NLP tools.",
        "DOI": "10.1063/5.0179324",
        "paper_author": "Talukdar C.",
        "affiliation_name": "Gauhati University",
        "affiliation_city": "Guwahati",
        "affiliation_country": "India",
        "affiliation_id": "60016850",
        "affiliation_state": "AS"
    },
    {
        "paper_title": "A natural language processing system for the efficient updating of highly curated pathophysiology mechanism knowledge graphs",
        "publication": "Artificial Intelligence in the Life Sciences",
        "citied_by": "2",
        "cover_date": "2023-12-15",
        "Abstract": "Background: Biomedical knowledge graphs (KG) have become crucial for describing biological findings in a structured manner. To keep up with the constantly changing flow of knowledge, their embedded information must be regularly updated with the latest findings. Natural language processing (NLP) has created new possibilities for automating this upkeep by facilitating information extraction from free text. However, due to annotated and labeled biomedical data limitations, the development of completely autonomous information extraction systems remains a substantial scientific and technological hurdle. This study aims to explore methodologies best suited to support the automatic extraction of causal relationships from biomedical literature with the aim of regular and rapid updating of disease-specific pathophysiology mechanism KGs. Methods: Our proposed approach first searches and retrieves PubMed abstracts using the desired terms and keywords. The extension corpora are then passed through the NLP pipeline for automatic information extraction. We then identify triples representing cause-and-effect relationships and encode this content using the Biological Expression Language (BEL). Finally, domain experts perform an analysis of the completeness, relevance, accuracy, and novelty of the extracted triples. Results: In our test scenario, which is focused on the KG regarding the phosphorylation of the Tau protein, our pipeline successfully contributed novel data, which was then subsequently used to update the KG leading to the identification of six additional upstream regulators of Tau phosphorylation. Conclusion: Here, it is demonstrated that the NLP-based workflow we created is capable of rapidly updating pathophysiology mechanism graphs. As a result, production-scale, semi-automated updating of pre-existing, curated mechanism graphs is enabled.",
        "DOI": "10.1016/j.ailsci.2023.100078",
        "paper_author": "Babaiha N.S.",
        "affiliation_name": "Fraunhofer Institute for Algorithms and Scientific Computing SCAI",
        "affiliation_city": "Sankt Augustin",
        "affiliation_country": "Germany",
        "affiliation_id": "60031902",
        "affiliation_state": "Nordrhein-Westfalen"
    },
    {
        "paper_title": "Optimal Scheduling of Integrated Energy Systems With Multiple CCHPs for High Efficiency and Low Emissions",
        "publication": "IEEE Internet of Things Journal",
        "citied_by": "4",
        "cover_date": "2023-12-15",
        "Abstract": "In order to reach carbon neutrality, there is growing interest in reducing greenhouse gas (GHG) and improving energy efficiency. One way to address this issue is the optimal scheduling of the integrated energy system (IES) with multiple combined cooling heating and power (CCHP) systems as proposed in this article. We model IES as a device with multiple input/output ports by the energy hub (EH) framework and propose a multiobjective optimal model to improve energy efficiency and reduce GHG emissions. The proposed model is constructed as a mixed-integer nonlinear programming (MINLP) due to considering nonlinear couplings of multiple energy flows and the unit commitment of multiple CCHP systems. To improve the computational efficiency, the proposed MINLP model is transformed into a nonlinear programming (NLP) model by a fast unit commitment technique based on the approximation of the aggregated online capacity. Finally, simulation results show the effectiveness of the proposed approach in reducing GHG emissions and improving energy efficiency as well as computational efficiency.",
        "DOI": "10.1109/JIOT.2023.3304644",
        "paper_author": "Xie H.",
        "affiliation_name": "Guangxi University",
        "affiliation_city": "Nanning",
        "affiliation_country": "China",
        "affiliation_id": "60030270",
        "affiliation_state": "Guangxi"
    },
    {
        "paper_title": "PreBit — A multimodal model with Twitter FinBERT embeddings for extreme price movement prediction of Bitcoin",
        "publication": "Expert Systems with Applications",
        "citied_by": "15",
        "cover_date": "2023-12-15",
        "Abstract": "Bitcoin, with its ever-growing popularity, has demonstrated extreme price volatility since its origin. Extreme price fluctuations have been known to occur due to tweets from Elon Musk, Michael Saylor, and others. In this paper, we aim to investigate whether we can leverage Twitter data to predict these extreme price movements. Existing social media models often take a shortcut and include sentiment extracted from tweets. In this work, however, we want to embed the actual tweets in a domain-informed way, and investigate whether they have an impact. Hence, we propose a multimodal deep learning model for predicting extreme price fluctuations that takes as input candlestick data, prices of a variety of correlated assets, technical indicators, as well as Twitter content. To train the model, a new dataset of 5,000 tweets per day containing the keyword ‘Bitcoin’ was collected from 2015 to 2021. This dataset, called PreBit, is made available online1, as is our model.2 Our proposed hybrid multimodal model consists of an SVM model based on price data, which is fused with a text-based Convolutional Neural Network. In the text-based model, we use the sentence-level FinBERT embeddings, pretrained on financial lexicons, so as to capture the full contents of the tweets and feed it to the model in an understandable way. In an ablation study, we explore whether adding social media data from the general public on Bitcoin improves the model's ability to predict extreme price movements. Finally, we propose and backtest a trading strategy based on the predictions of our models with varying prediction threshold and show that it can be used to build a profitable trading strategy with a reduced risk over a ‘hold’ or moving average strategy.",
        "DOI": "10.1016/j.eswa.2023.120838",
        "paper_author": "Zou Y.",
        "affiliation_name": "Singapore University of Technology and Design",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore",
        "affiliation_id": "60104290",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Passively Q-switched mode-locked thulium-doped fiber laser using nonlinear polarization rotation technique",
        "publication": "Ceramics International",
        "citied_by": "17",
        "cover_date": "2023-12-15",
        "Abstract": "A Q-switched mode-locked (QML) thulium-doped fiber laser based on nonlinear polarization rotation (NPR) and negative group velocity dispersion (GVD) is demonstrated. This regime consists in the generation of a mode-locked (ML) pulse train under a Q-switched (QS) envelope. In this regard, by properly adjusting the polarization controllers and increasing the pump power in a range from 1 to 3 W, QML regime is obtained. With a maximum pump power of 3 W, a QS pulse duration of 1.46 μs and a pulse energy of 5.96 μJ are obtained. The maximum frequencies reached are 26 kHz and 2.63 MHz, corresponding to the Q-switched and mode-locked pulses, respectively. This method for generating a QML at 2 μm has the advantages of being eye-safe and robust, providing an ideal dual-frequency comb (DFC) generation method with high potential for various applications.",
        "DOI": "10.1016/j.ceramint.2023.02.056",
        "paper_author": "Durán-Sánchez M.",
        "affiliation_name": "Instituto Nacional de Astrofisica Optica y Electronica",
        "affiliation_city": "Puebla",
        "affiliation_country": "Mexico",
        "affiliation_id": "60030699",
        "affiliation_state": "PUE"
    },
    {
        "paper_title": "Comparing Classical Distance Measures and Word Embeddings for Automatic Short Answer Grading",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-14",
        "Abstract": "In the educational process, students' answers to essay questions are one of the cognitive methods to measure students' understanding of a topic being studied. But checking essay answers is certainly much more difficult than multiple-choice answers. Apart from absorbing much energy and time, it may also be biased depending on the human rater's subjectivity. To overcome this, researchers have already started to develop Automatic Short Answer Grading (ASAG) by exploring the field of natural language processing (NLP). However ASAG research specifically for Indonesian is still limited. This research aims to find the right method to improve the accuracy of the ASAG system in Indonesia focusing on the computer science domain, with a deep learning approach. We started our research by combining the feature extraction method Bag of Words and Term Frequency-Inverse Document Frequency (TF-IDF) with linear regression, Support Vector Regression, and Random Forest Regression. And then employing BERT and FastText. Both experiments produced similar performance, with an F1-score on average of 0, 72, which is categorized as low similarity. This opens opportunities for further research in word embeddings, especially the transformer method that becomes state-of-the-art.",
        "DOI": "10.1145/3638884.3638962",
        "paper_author": "Ripmiatin E.",
        "affiliation_name": "Universitas Indonesia",
        "affiliation_city": "Depok",
        "affiliation_country": "Indonesia",
        "affiliation_id": "60069377",
        "affiliation_state": "West Java"
    },
    {
        "paper_title": "HCAIep 2023 - Proceedings of the 2023 Conference on Human Centered Artificial Intelligence - Education and Practice",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-14",
        "Abstract": "The proceedings contain 21 papers. The topics discussed include: explainability in NLP model: detection of Covid-19 twitter fake news; automated lip reading: enhancing accessibility measures in XR for education; teachers’ motivation for teaching AI in K-12 settings; human-centered AI education in upper-second level: towards a PRIMM-Esque pedagogy for CT 2.0; integrating human factors into trustworthy AI for healthcare; personalized programming education with knowledge tracing; immersive neural network exploration: a VR approach to human-centered AI understanding; promoting human-centered machine translation quality assessment in NLP education; exploring the research gap: generative AI and learning of python programming among post-primary students; and algorithmic biases in causal structure learning for gene regulatory networks.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Promoting Human-centred Machine Translation Quality Assessment in NLP education",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-14",
        "Abstract": "This research advocates for the integration of Human Evaluation (HE) in Machine Translation Quality Assessment (MTQA), countering the over-reliance on Automatic Evaluation Metrics (AEM) and their associated risks. Highlighting the limitations of AEMs and stressing the strength of HE, this project proposes a mixed-methods training approach for Natural Language Processing (NLP) students, using the ADDIE framework. It aims to equip NLP students with robust HE approaches for a more comprehensive MTQA process, so future developers have the skills to ensure the reliability of MT systems and preventing risks and biases to be propagated.",
        "DOI": "10.1145/3633083.3633222",
        "paper_author": "Cavalheiro Camargo J.L.",
        "affiliation_name": "Dublin City University",
        "affiliation_city": "Dublin",
        "affiliation_country": "Ireland",
        "affiliation_id": "60025059",
        "affiliation_state": "Leinster"
    },
    {
        "paper_title": "Explainability in NLP model: Detection of Covid-19 Twitter Fake News",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2023-12-14",
        "Abstract": "Fake news has found fertile ground on social media. A global health crisis such as COVID-19 further helps propagate fake news on social media. Much research has been done to develop AI systems that classify news as real or fake. However, there is a growing concern about trust in these AI systems. To this end, we attempt to improve the trustworthiness of AI text classification systems. We use tools to explore data, explain feature extraction techniques, interpret the ML models implemented, and explain the decision-making progress of AI systems. In this study, we compared five ML classifiers for our experiments: Naive Bayes, Support Vector Machines (SVMs), Logistic Regression, Decision Tree, and Random Forest. The models were trained on 10700 tweets containing 5,600 real and 5,100 fake tweets related to COVID-19. In comparison, the SVMs model performance was the best, with a detection accuracy of 0.93 and F1 scores of 0.94 and 0.93 for real and fake news, respectively. Global and local explanations are included to understand the overall model behavior, ensuring transparency and fostering confidence in AI users. We have chosen the SVMs model for the explanation section as it was the best model in this study.",
        "DOI": "10.1145/3633083.3633212",
        "paper_author": "Yong W.Y.",
        "affiliation_name": "Technological University Dublin",
        "affiliation_city": "Dublin",
        "affiliation_country": "Ireland",
        "affiliation_id": "60012873",
        "affiliation_state": "Leinster"
    },
    {
        "paper_title": "Fastensor: Optimise the Tensor I/O Path from SSD to GPU for Deep Learning Training",
        "publication": "ACM Transactions on Architecture and Code Optimization",
        "citied_by": "2",
        "cover_date": "2023-12-14",
        "Abstract": "In recent years, benefiting from the increase in model size and complexity, deep learning has achieved tremendous success in computer vision (CV) and (NLP). Training deep learning models using accelerators such as GPUs often requires much iterative data to be transferred from NVMe SSD to GPU memory. Much recent work has focused on data transfer during the pre-processing phase and has introduced techniques such as multiprocessing and GPU Direct Storage (GDS) to accelerate it. However, tensor data during training (such as Checkpoints, logs, and intermediate feature maps), which is also time-consuming, is often transferred using traditional serial, long-I/O-path transfer methods.In this article, based on GDS technology, we built Fastensor, an efficient tool for tensor data transfer between the NVMe SSDs and GPUs. To achieve higher tensor data I/O throughput, we optimized the traditional data I/O process. We also proposed a data and runtime context-aware tensor I/O algorithm. Fastensor can select the most suitable data transfer tool for the current tensor from a candidate set of tools during model training. The optimal tool is derived from a dictionary generated by our adaptive exploration algorithm in the first few training iterations. We used Fastensor's unified interface to test the read/write bandwidth and energy consumption of different transfer tools for different sizes of tensor blocks. We found that the execution efficiency of different tensor transfer tools is related to both the tensor block size and the runtime context.We then deployed Fastensor in the widely applicable Pytorch deep learning framework. We showed that Fastensor could perform superior in typical scenarios of model parameter saving and intermediate feature map transfer with the same hardware configuration. Fastensor achieves a 5.37x read performance improvement compared to torch.save() when used for model parameter saving. When used for intermediate feature map transfer, Fastensor can increase the supported training batch size by 20x, while the total read and write speed is increased by 2.96x compared to the torch I/O API.",
        "DOI": "10.1145/3630108",
        "paper_author": "Wei J.",
        "affiliation_name": "Xi'an Jiaotong University",
        "affiliation_city": "Xi'an",
        "affiliation_country": "China",
        "affiliation_id": "60018308",
        "affiliation_state": "Shaanxi"
    },
    {
        "paper_title": "AI-Powered Content Moderation",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2023-12-14",
        "Abstract": "The use of inappropriate language that negatively affects the readers' and viewers' moods should be avoided because content moderation is a crucial component of every blog, media outlet, and social media platform. Additionally, because media is a crucial technology that can be viewed and understood by anyone, regardless of age, and even a young child can now understand how to operate the device, We therefore developed an Artificial Intelligence model that can identify inappropriate contents and words using ML and DL concepts to make content moderation as a part of machines own process. Our model uses NLP and Deep Neural Network strategy to make this possible by removing words using a way similar to sentimental analysis. So, when creating a video, blog, or other media files, we need to be careful that those inappropriate contents and phrases should not offend the readers and children.",
        "DOI": "10.1063/5.0187620",
        "paper_author": "Vidhya K.",
        "affiliation_name": "KPR Institute of Engineering and Technology",
        "affiliation_city": "Coimbatore",
        "affiliation_country": "India",
        "affiliation_id": "60108640",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Social Network Users Mental Stress Identification using Machine Learning",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2023-12-14",
        "Abstract": "World is completely occupied by smart devices communication among peoples become rare instead everyone is communicating with smart devices that too mostly with communication devices such as mobile. Various social network platforms are available now days and worldwide people are connected and communicated without any relation and knowledge of them. Based on chats peoples are connected and communicated which leads to unwanted events happening such as suicide and life loss issues. Currently a greater number of children is using social networks even adults too should be monitored to analyse their mental stress. Machine learning plays major role in predicting human emotion based on their text comments through Natural Language Processing (NLP). In chat, users communicate with their chat list friends will express their thoughts or their opinion regarding events that happening in their life. By analysing these chats through NLP can identify the status of particular person mental pressure. If the chat has normal words then it will be identified to be normal in case if a chat has depressed words then system will starts identifying the pressure. Similarly, this analysis and monitoring will be done for a threshold time and if particular user is identified to be in stress, then it will be indicated to respective user’s responsible person. Hence machine learning based emotion detection is implemented with high accuracy and its application-based deployment achieves high performance compared to other analysis system.",
        "DOI": "10.1063/5.0176771",
        "paper_author": "Subramanian A.",
        "affiliation_name": "Sona College of Technology",
        "affiliation_city": "Salem",
        "affiliation_country": "India",
        "affiliation_id": "60077558",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Exploring the Potential of ChatGPT for Keywords Analysis",
        "publication": "SEG Technical Program Expanded Abstracts",
        "citied_by": "0",
        "cover_date": "2023-12-14",
        "Abstract": "ChatGPT is a natural language processing (NLP) tool based on a transformer-based architecture, which is able to quickly analyze text and generate meaningful results. By applying ChatGPT to analyze extracted keywords, it is possible to identify key phrases and topics in a given aspect that can be used for further study. Based on the 100 most popular keywords extracted from the SEG Annual Meeting from 2014 to 2022, ChatGPT is applied to attempt keyword analysis text generation. The results were generally acceptable, but must be carefully checked. At this stage, text generation cannot completely replace the work of researchers, due to the results of the analysis can then be reviewed and accepted after evaluation. Besides inaccurate results, some risks should be taken into consideration when using it for keyword analysis, which could include plagiarism and copyright issues. Despite these risks, ChatGPT is a powerful tool that can be used to quickly and effectively analyze text and extract meaningful information.",
        "DOI": "10.1190/image2023-3905013.1",
        "paper_author": "Changcheng L.",
        "affiliation_name": "Ltd.",
        "affiliation_city": "Shenzhen",
        "affiliation_country": "China",
        "affiliation_id": "126269406",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "A 'distant' reading of A Muvra: exploring the autonomist press in the Corsican language through topic modelling",
        "publication": "Umanistica Digitale",
        "citied_by": "0",
        "cover_date": "2023-12-14",
        "Abstract": "The aim of this article is to explore the autonomist press in the Corsican language through topic modelling. In order to do so, the effectiveness of two topic modelling methods, LDA and LSA, will be compared on a trilingual corpus-Italian, Corsican and French-not subject to lemmatization, not only in order to assess the results obtained, but also to understand which of these methods comes closest to our previous knowledge on the subject, which is the result of a close reading.",
        "DOI": "10.6092/issn.2532-8816/17642",
        "paper_author": "Paci D.",
        "affiliation_name": "Università degli Studi di Modena e Reggio Emilia",
        "affiliation_city": "Modena",
        "affiliation_country": "Italy",
        "affiliation_id": "60004591",
        "affiliation_state": "MO"
    },
    {
        "paper_title": "Improving NLP techniques by integrating linguistic input to detect hate speech in CMC corpora",
        "publication": "Hate Speech in Social Media: Linguistic Approaches",
        "citied_by": "0",
        "cover_date": "2023-12-12",
        "Abstract": "Hate speech detection research relies heavily on automatic detection models that make use of machine learning (ML), opinion mining, sentiment analysis and polarity detection. The highly informal and speech-like nature of Computer Mediated Communication (CMC) poses many challenges for electronic processing and automatic detection methods. In this study, we describe details of the natural language processing (NLP) techniques applied to obtain a lemmatised and part-of-speech-tagged Portuguese-English CMC corpus. Considering that automatic analysis and annotation tools are optimised for standard written production, we will address the limitations of these tools due to CMC-specific phenomena and how their performance can be improved by integrating linguistic input. We propose a mixed methods approach in which linguistic knowledge, including lexical, syntactic and pragmatic input, is used in conjunction with NLP techniques to trace and analyse fixed expressions in order to detect potential hate speech in user-generated content. Our focus will be on analysing the behaviour of opinion markers that exhibit a certain degree of fixedness as potential pointers to prejudiced hateful content in netlang 's English Subcorpus as a contribution to the optimisation of hate speech detection NLP and ML models.",
        "DOI": "10.1007/978-3-031-38248-2_3",
        "paper_author": "Dias I.",
        "affiliation_name": "Universidade do Minho",
        "affiliation_city": "Braga",
        "affiliation_country": "Portugal",
        "affiliation_id": "60020475",
        "affiliation_state": "Braga"
    },
    {
        "paper_title": "Hate speech in social media: Linguistic approaches",
        "publication": "Hate Speech in Social Media: Linguistic Approaches",
        "citied_by": "3",
        "cover_date": "2023-12-12",
        "Abstract": "This edited book offers insight into the linguistic construction of prejudice and discrimination in social media. Drawing on the outputs of a three-year research project, NETLANG, involving scholars from five European countries (Portugal, Czech Republic, Estonia, Finland and Poland), as well as on external contributions from participants in the project's final conference, the collection brings together a variety of linguistic approaches to the study of online hate speech, ranging from pragmatic to syntactic, morphological, and lexical analyses, with a considerable focus on Natural Language Processing and Corpus Linguistics. Data from English, Portuguese, Danish, Lithuanian, Persian, Polish, and Slovenian are examined, along with various geopolitical contexts for hate speech, especially anti-refugee and anti-immigrant discourse. The authors explore a continuum of overt to covert textual data, namely: (i) structural elements, such as syntactic and morphological patterns which recur throughout the texts; (ii) lexical and stylistic elements, revealing the often implicit ways in which vocabulary choices and rhetorical devices signal the expression of hate; and (iii) interactional elements, concerning the pragmatic relationships established in online communicative exchanges. The chapters cover numerous types of prejudice, such as sexism, racism, nationalism, antisemitism, religious intolerance, ageism, and homo/transphobia. The book will be of interest to an academic readership in Linguistics, Media Studies, Communication Studies, and Social Sciences.",
        "DOI": "10.1007/978-3-031-38248-2",
        "paper_author": "Ermida I.",
        "affiliation_name": "Universidade do Minho",
        "affiliation_city": "Braga",
        "affiliation_country": "Portugal",
        "affiliation_id": "60020475",
        "affiliation_state": "Braga"
    },
    {
        "paper_title": "Distinguishing online hate speech from aggressive speech: A five-factor annotation model",
        "publication": "Hate Speech in Social Media: Linguistic Approaches",
        "citied_by": "0",
        "cover_date": "2023-12-12",
        "Abstract": "This chapter is of an introductory nature, offering a definitional and programmatic reflection about the object under focus in the book. How can hate speech be accurately identified? An overview of the extant scholarship reveals a tendency towards imprecision, dismissiveness and all-inclusiveness. Indeed, hate speech seems to lack a consensual definition-or, rather, a consensual awareness of the need for a definition. This need is particularly central to practical, applied areas of detection and control, respectively, natural language processing (NLP) research and legal scholarship, which have striven to circumscribe the concept. In particular, establishing hate speech dataset properties is particularly dependent on definitional issues, just as is distinguishing hate speech from free speech and other competing and cognate concepts. By revisiting the classic communication framework involving senders, messages, channels and receivers, and enlightening it with current linguistic insight, the chapter puts forth an annotation model of five factors, the positive co-occurrence of which signals hate speech. It then tests the model against a range of examples from three subsets of the NETLANG corpus, classified under the variables of sexism, racism and ageism. Finally, it extends the analysis into other, linguistic, features of online hate speech. Admittedly, the identification model proposed in the chapter is but the first step in understanding such an evasive concept as hate speech. It is therefore acknowledged that, so as to fully assess how hate speech works, it is important to know the way it is negotiated by particular speakers in particular communicative situations, meant for specific audiences at specific points in time.",
        "DOI": "10.1007/978-3-031-38248-2_2",
        "paper_author": "Ermida I.",
        "affiliation_name": "Universidade do Minho",
        "affiliation_city": "Braga",
        "affiliation_country": "Portugal",
        "affiliation_id": "60020475",
        "affiliation_state": "Braga"
    },
    {
        "paper_title": "An End-to-End Solution for Net Promoter Score Estimation and Explanation from Social Media Using Natural Language Processing",
        "publication": "Frontiers in Artificial Intelligence and Applications",
        "citied_by": "0",
        "cover_date": "2023-12-12",
        "Abstract": "The Net Promoter Score (NPS) is often used in customer experience programs for measuring customer loyalty. Increasingly more companies seek to automatically process millions of pieces of customer feedback from social media per month in order to estimate their NPS, leveraging advanced analytics like machine learning (ML) and natural language processing (NLP). Discovering trends and themes in customer interactions helps explain the NPS, empowering companies to improve products and customer experience. In this paper, we describe an end-to-end solution for NPS estimation and explanation from social media. The process includes sentiment analysis on user comments, estimating product information based on text semantics, grouping and tagging user comments for text discovery, and NPS explanation. The solution gives companies the capability to identify overall customer sentiment and common topics in a unified platform, allowing faster analysis and insights on NPS based on customer feedback.",
        "DOI": "10.3233/FAIA231006",
        "paper_author": "Liang X.",
        "affiliation_name": "Amazon.com, Inc.",
        "affiliation_city": "Seattle",
        "affiliation_country": "United States",
        "affiliation_id": "60076757",
        "affiliation_state": "WA"
    },
    {
        "paper_title": "Transformative AI: Responsible, Transparent, and Trustworthy AI Systems",
        "publication": "Transformative AI: Responsible, Transparent, and Trustworthy AI Systems",
        "citied_by": "1",
        "cover_date": "2023-12-12",
        "Abstract": "Transformative AI provides a comprehensive overview of the latest trends, challenges, applications, and opportunities in the field of Artificial Intelligence. The book covers the state of the art in AI research, including machine learning, natural language processing, computer vision, and robotics, and explores how these technologies are transforming various industries and domains, such as healthcare, finance, education, and entertainment. The book also addresses the challenges that come with the widespread adoption of AI, including ethical concerns, bias, and the impact on jobs and society. It provides insights into how to mitigate these challenges and how to design AI systems that are responsible, transparent, and trustworthy. The book offers a forward-looking perspective on the future of AI, exploring the emerging trends and applications that are likely to shape the next decade of AI innovation. It also provides practical guidance for businesses and individuals on how to leverage the power of AI to create new products, services, and opportunities. Overall, the book is an essential read for anyone who wants to stay ahead of the curve in the rapidly evolving field of Artificial Intelligence and understand the impact that this transformative technology will have on our lives in the coming years.",
        "DOI": "NA",
        "paper_author": "Banafa A.",
        "affiliation_name": "San Jose State University",
        "affiliation_city": "San Jose",
        "affiliation_country": "United States",
        "affiliation_id": "60015609",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Improving Scientific Literature Classification: A Parameter-Efficient Transformer-Based Approach",
        "publication": "International Journal of Electrical and Computer Engineering Systems",
        "citied_by": "1",
        "cover_date": "2023-12-12",
        "Abstract": "Transformer-based models have been utilized in natural language processing (NLP) for a wide variety of tasks like summarization, translation, and conversational agents. These models can capture long-term dependencies within the input, so they have significantly more representational capabilities than Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). Nevertheless, these models require significant computational resources in terms of high memory usage, and extensive training time. In this paper, we propose a novel document categorization model, with improved parameter efficiency that encodes text using a single, lightweight, multiheaded attention encoder block. The model also uses a hybrid word and position embedding to represent input tokens. The proposed model is evaluated for the Scientific Literature Classification task (SLC) and is compared with state-of-the-art models that have previously been applied to the task. Ten datasets of varying sizes and class distributions have been employed in the experiments. The proposed model shows significant performance improvements, with a high level of efficiency in terms of parameter and computation resource requirements as compared to other transformer-based models, and outperforms previously used methods.",
        "DOI": "10.32985/ijeces.14.10.4",
        "paper_author": "Ahanger M.M.",
        "affiliation_name": "University of Kashmir",
        "affiliation_city": "Srinagar",
        "affiliation_country": "India",
        "affiliation_id": "60016300",
        "affiliation_state": "JK"
    },
    {
        "paper_title": "A chatbot assistant for farmers towards improving livelihood",
        "publication": "The Future of Smart Agriculture",
        "citied_by": "0",
        "cover_date": "2023-12-11",
        "Abstract": "India is considered a major agricultural powerhouse as this sector counts for about 15% of India's Gross Domestic Product (GDP). However, Indian agriculture still lacks access to modern farming techniques, which results in the wastage of resources. One of the modern technologies used to bridge this technological gap is a user-friendly messaging application known as Chatbot. A chatbot trained with agriculture-related data can assist the farmers in having knowledge about modern farming practices without the hassle of waiting for an expert to help solve their queries. In this context, a natural language-based chatbot model is proposed here, named KisaanBot, as a remote assistant to the farmer, which can respond to their queries. Our assistant is trained by the RASA Open-Source Framework and can be accessed with a Web URL via Dialogflow deployment. The proposed chatbot model identifies the intent and the entity from user utterances and it generates the responses from the trained database, which delivers them to the user. The knowledge base used in training is designed by collecting grouped data from Kisan Call Centre (KCC) and the Indian Council of Agricultural Research (ICAR). This knowledge base comprises the frequently asked questions and the corresponding answers regarding agricultural practices of West Bengal concerned with four domains namely soil, weather, crops, and fertigation. Hence, the dataset comprises three fields, such as intent (the goal of a user query), query (the question related to that intent), and the answer (response from the chatbot). The chatbot is trained with 9 subqueries for each query to develop the knowledge base having 9 queries for a single intent. This can resolve the issue which arises due to different ways of framing a particular question by the users. The proposed system is trained and tested in the 80:20 ratio with the dataset. The efficiency of the proposed model is assessed by the performance measures, such as precision, recall, and f1-score. The overall accuracy thus achieved by the proposed model is 99.82% which shows an improvement over existing works.",
        "DOI": "NA",
        "paper_author": "Hazra R.",
        "affiliation_name": "National Institute of Technology, Durgapur",
        "affiliation_city": "Durgapur",
        "affiliation_country": "India",
        "affiliation_id": "60103561",
        "affiliation_state": "WB"
    },
    {
        "paper_title": "Twitter Sentiment Analysis on Political Tweets",
        "publication": "AIP Conference Proceedings",
        "citied_by": "1",
        "cover_date": "2023-12-11",
        "Abstract": "Twitter is one of the most important social media platforms that is used to keep oneself updated about any political or any news. Twitter generally has a word limit, which also limits the words to be detected in the tweets. Twitter has developed into one of the primary platforms for political discourse and public policy discussions. This paper proposes a Twitter sentiment analyser for political tweets using Machine Learning (ML) and Natural Language Processing (NLP). This paper envisions a model which performs sentiment analysis on tweets discussing a particular political or policy topic from a particular date and performs sentiment analysis on that data. The system is trained with a dataset of 160000 tweets commenting on Indian politics. The system uses NLP for pre-processing and feature extraction. Classification is performed using 3 classifiers namely Random forest, Support Vector Machine (SVM) and Bernoulli Naive Bayes. The highest accuracy obtained was 88.36% using SVM classifier.",
        "DOI": "10.1063/5.0182743",
        "paper_author": "Wyawahare M.",
        "affiliation_name": "Vishwakarma Institute of Technology",
        "affiliation_city": "Pune",
        "affiliation_country": "India",
        "affiliation_id": "60099671",
        "affiliation_state": "MH"
    },
    {
        "paper_title": "Large language model-based information extraction from free-text radiology reports: a scoping review protocol",
        "publication": "BMJ Open",
        "citied_by": "8",
        "cover_date": "2023-12-09",
        "Abstract": "Introduction Radiological imaging is one of the most frequently performed diagnostic tests worldwide. The free-text contained in radiology reports is currently only rarely used for secondary use purposes, including research and predictive analysis. However, this data might be made available by means of information extraction (IE), based on natural language processing (NLP). Recently, a new approach to NLP, large language models (LLMs), has gained momentum and continues to improve performance of IE-related tasks. The objective of this scoping review is to show the state of research regarding IE from free-text radiology reports based on LLMs, to investigate applied methods and to guide future research by showing open challenges and limitations of current approaches. To our knowledge, no systematic or scoping review of IE from radiology reports based on LLMs has been published. Existing publications are outdated and do not comprise LLM-based methods. Methods and analysis This protocol is designed based on the JBI Manual for Evidence Synthesis, chapter 11.2: € Development of a scoping review protocol'. Inclusion criteria and a search strategy comprising four databases (PubMed, IEEE Xplore, Web of Science Core Collection and ACM Digital Library) are defined. Furthermore, we describe the screening process, data charting, analysis and presentation of extracted data. Ethics and dissemination This protocol describes the methodology of a scoping literature review and does not comprise research on or with humans, animals or their data. Therefore, no ethical approval is required. After the publication of this protocol and the conduct of the review, its results are going to be published in an open access journal dedicated to biomedical informatics/digital health.",
        "DOI": "10.1136/bmjopen-2023-076865",
        "paper_author": "Reichenpfader D.",
        "affiliation_name": "Berner Fachhochschule",
        "affiliation_city": "Bern",
        "affiliation_country": "Switzerland",
        "affiliation_id": "60099391",
        "affiliation_state": "BE"
    },
    {
        "paper_title": "Advancing Text Analytics: Instruction Fine-Tuning of QianWen-7B for Sentiment Classification",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-08",
        "Abstract": "The complexity of financial systems and the subtleties of market behavior necessitate sophisticated tools for sentiment analysis. This study presents a fine-tuned QianWen-7B [1] model, a large pre-trained language model, tailored for financial text sentiment classification. Utilizing instruction fine-tuning, we have adapted the model to classify texts into three categories: positive, neutral, and negative. Our dataset comprises 2,879 neutral, 1,362 positive, and 604 negative samples from texts. The model was trained using a fine-tuning framework called QLora [2], with a quantization level of 4 and Lora rank of 8, optimizing for both memory usage and computational efficiency. We employed the Adam optimizer with a learning rate of 5e-5, a batch size of 4, and gradient accumulation to address hardware limitations. The fine-tuned Qwen-7B model achieved an accuracy of 0.8227, outperforming the Deberta-V3-base and Deberta-V3-large models, which underscores the effectiveness of our approach. Our findings illustrate the potential of using large language models with instruction fine-tuning for enhanced text sentiment analysis, paving the way for more informed investment decisions and robust market regulation. The discussion highlights the model's superior performance, challenges in dataset representativeness and class imbalance, and the importance of model interpretability in decision-making. It also points to future enhancements that could further improve the model's applicability and relevance in the rapidly changing financial sector.",
        "DOI": "10.1145/3659211.3659227",
        "paper_author": "Han Y.",
        "affiliation_name": "Yan'an University",
        "affiliation_city": "Xi'an",
        "affiliation_country": "China",
        "affiliation_id": "60073713",
        "affiliation_state": "Shaanxi"
    },
    {
        "paper_title": "Performance Optimization and Real-Time Application of English Machine Translation Driven by Big Data",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-08",
        "Abstract": "The growth of computers has allowed machine translation, which has its roots in NLP, to be applied in a wide variety of settings. The increasing need for translation between languages has highlighted the need of addressing the accuracy of text translation. The development of big data technologies has resulted in a wealth of data suitable for training machine translators. With the help of recent developments in big data and natural language processing, neural network technology is finding widespread use in the field of machine translation. This allows neural machine translation methods to eventually replace their more traditional counterparts. In this paper, we use neural network technology to investigate the state of English-Chinese multilingual translation. To begin, this paper suggests a technique for split-second multitasking. It investigates the best ways to combine different attention calculation methods in order to reap the benefits of all of them. Second, this study suggests a refined LSTM. The network can effectively combine prospective and retrospective data to get more reliable contextual semantic understanding. Third, this study presents a multi-subspace attention mechanism, which utilizes numerous attention computation functions to calculate an attention score and maps the hidden layer states of the ILSTM to many subspaces. Finally, this study constructs a translation model by combining a multi-attention mechanism, an enhanced LSTM, and a multi-subspace attention mechanism. Finally, this study systematically tests the proposed bilingual translation approach, and results from these trials verify the method's viability and its real-time performance.",
        "DOI": "10.1145/3641343.3641425",
        "paper_author": "Chen L.",
        "affiliation_name": "Sichuan University",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China",
        "affiliation_id": "60016521",
        "affiliation_state": "Sichuan"
    },
    {
        "paper_title": "Log Summarisation for Defect Evolution Analysis",
        "publication": "SDD 2023 - Proceedings of the 1st International Workshop on Software Defect Datasets, Co-located with: ESEC/FSE 2023",
        "citied_by": "0",
        "cover_date": "2023-12-08",
        "Abstract": "Log analysis and monitoring are essential aspects in software maintenance and identifying defects. In particular, the temporal nature and vast size of log data leads to an interesting and important research question: How can logs be summarised and monitored over time? While this has been a fundamental topic of research in the software engineering community, work has typically focused on heuristic-, syntax-, or static-based methods. In this work, we suggest an online semantic-based clustering approach to error logs that dynamically updates the log clusters to enable monitoring code error life-cycles. We also introduce a novel metric to evaluate the performance of temporal log clusters. We test our system and evaluation metric with an industrial dataset and find that our solution outperforms similar systems. We hope that our work encourages further temporal exploration in defect datasets.",
        "DOI": "10.1145/3617572.3617881",
        "paper_author": "Dolga R.",
        "affiliation_name": "JPMorgan Chase",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "100541769",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "PROMISE 2023 - Proceedings of the 19th International Conference on Predictive Models and Data Analytics in Software Engineering, Co-located with: ESEC/FSE 2023",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2023-12-08",
        "Abstract": "The proceedings contain 8 papers. The topics discussed include: BuggIn: automatic intrinsic bugs classification model using NLP and ML; do developers fix continuous integration smells?; large scale study of orphan vulnerabilities in the software supply chain; the FormAI dataset: generative AI in software security through the lens of formal verification; comparing word-based and AST-based models for design pattern recognition; on effectiveness of further pre-training on BERT models for story point estimation; automated fairness testing with representative sampling; and model review: a PROMISEing opportunity.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Comparing Word-Based and AST-Based Models for Design Pattern Recognition",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "4",
        "cover_date": "2023-12-08",
        "Abstract": "Design patterns (DPs) provide reusable and general solutions for frequently encountered problems. Patterns are important to maintain the structure and quality of software products, in particular in large and distributed systems like automotive software. Modern language models (like Code2Vec or Word2Vec) indicate a deep understanding of programs, which has been shown to help in such tasks as program repair or program comprehension, and therefore show promise for DPR in industrial contexts. The models are trained in a self-supervised manner, using a large unlabelled code base, which allows them to quantify such abstract concepts as programming styles, coding guidelines, and, to some extent, the semantics of programs. This study demonstrates how two language models-Code2Vec and Word2Vec, trained on two public automotive repositories, can show the separation of programs containing specific DPs. The results show that the Code2Vec and Word2Vec produce average F1-scores of 0.781 and 0.690 on open-source Java programs, showing promise for DPR in practice.",
        "DOI": "10.1145/3617555.3617873",
        "paper_author": "Chand S.",
        "affiliation_name": "Chalmers University of Technology",
        "affiliation_city": "Gothenburg",
        "affiliation_country": "Sweden",
        "affiliation_id": "60000990",
        "affiliation_state": "Vastra Gotaland"
    },
    {
        "paper_title": "BuggIn: Automatic Intrinsic Bugs Classification Model using NLP and ML",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2023-12-08",
        "Abstract": "Recent studies have shown that bugs can be categorized into in-trinsic and extrinsic types. Intrinsic bugs can be backtracked to specific changes in the version control system (VCS), while extrin-sic bugs originate from external changes to the VCS and lack a direct bug-inducing change. Using only intrinsic bugs to train bug prediction models has been reported as beneficial to improve the performance of such models. However, there is currently no auto-mated approach to identify intrinsic bugs. To bridge this gap, our study employs Natural Language Processing (NLP) techniques to automatically identify intrinsic bugs. Specifically, we utilize two embedding techniques, seBERT and TF-IDF, applied to the title and description text of bug reports. The resulting embeddings are fed into well-established machine learning algorithms such as Support Vector Machine, Logistic Regression, Decision Tree, Random Forest, and K-Nearest Neighbors. The primary objective of this paper is to assess the performance of various NLP and machine learning techniques in identifying intrinsic bugs using the textual informa-tion extracted from bug reports. The results demonstrate that both seBERT and TF-IDF can be effectively utilized for intrinsic bug identification. The highest performance scores were achieved by combining TF-IDF with the Decision Tree algorithm and utilizing the bug titles (yielding an F1 score of 78%). This was closely fol-lowed by seBERT, Support Vector Machine, and bug titles (with an F1 score of 77%). In summary, this paper introduces an innovative approach that automates the identification of intrinsic bugs using textual information derived from bug reports.",
        "DOI": "10.1145/3617555.3617875",
        "paper_author": "Bhandari P.",
        "affiliation_name": "University of British Columbia Okanagan",
        "affiliation_city": "Kelowna",
        "affiliation_country": "Canada",
        "affiliation_id": "60012967",
        "affiliation_state": "BC"
    },
    {
        "paper_title": "Legal Text Segmentation Through Breakpoint Detection",
        "publication": "Frontiers in Artificial Intelligence and Applications",
        "citied_by": "1",
        "cover_date": "2023-12-07",
        "Abstract": "Efficient text segmentation plays a crucial role in the legal domain as it helps legal professionals in extracting relevant details from lengthy documents, focusing the attention on the main information within the segments. This paper introduces a novel approach for text segmentation of legal documents by leveraging breakpoint detection and segment classification as a single operation. The proposed solution employs a transformer model pre-trained on pre-processed texts from the Italian National Jurisprudence Archive and fine-tuned on Italian Court of Cassation documents. Experimental results reveal that the suggested methodology outperforms traditional approaches, demonstrating the model's ability to learn the grammatical and structural patterns of legal documents even with perturbed and noisy text.",
        "DOI": "10.3233/FAIA230968",
        "paper_author": "Lombardi A.",
        "affiliation_name": "Eustema S.p.A.",
        "affiliation_city": "Naples",
        "affiliation_country": "Italy",
        "affiliation_id": "123435587",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Multilevel Hate Speech Classification Based on Multilingual Case-Law",
        "publication": "Frontiers in Artificial Intelligence and Applications",
        "citied_by": "0",
        "cover_date": "2023-12-07",
        "Abstract": "This paper presents classification tools to detect hate speech topics using NLP tools in four different languages (Italian, Spanish, Germany, English) using the selected case-law from national and international jurisdiction. The research is conducted inside the FAST-LISA European project with the aim to classify the hate speech in online public debate.",
        "DOI": "10.3233/FAIA230981",
        "paper_author": "Palmirani M.",
        "affiliation_name": "Alma Mater Studiorum Università di Bologna",
        "affiliation_city": "Bologna",
        "affiliation_country": "Italy",
        "affiliation_id": "60028218",
        "affiliation_state": "BO"
    },
    {
        "paper_title": "\"Comparative Prints Suite\" of the United States House of Representatives: NLP for Tracking Changes in Bills and Laws",
        "publication": "Frontiers in Artificial Intelligence and Applications",
        "citied_by": "0",
        "cover_date": "2023-12-07",
        "Abstract": "We demonstrate the 'Comparative Prints Suite', a browser-based family of tools developed for the United States House of Representatives to track changes in United States federal legislation. The tools use a model grammar to parse amendments to bills, execute those amendments into law, and compare versions of bills. The tools are available at to Members and staff in the U.S. House, the Congressional Research Service of the Library of Congress, and other Congressional entities to support the legislative process.",
        "DOI": "10.3233/FAIA230993",
        "paper_author": "Hershowitz A.",
        "affiliation_name": "Palo Alto High School",
        "affiliation_city": "Palo Alto",
        "affiliation_country": "United States",
        "affiliation_id": "117620202",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Automatic Simplification of Legal Texts in Portuguese Using Machine Learning",
        "publication": "Frontiers in Artificial Intelligence and Applications",
        "citied_by": "1",
        "cover_date": "2023-12-07",
        "Abstract": "Texts produced by the Brazilian judiciary have a complex and technical vocabulary, with elaborate use of the Portuguese language and many legal terms difficult to be understood, generating a barrier in communication between the judiciary and the population. In this sense, the Automatic Text Simplification (ATS), activity of the Natural Language Processing (NLP) area, can be applied to improve the readability of these types of text using specialized algorithms, and promote scalability in simplifying them, in view of the great demand in the courts. In this context, this article presents an evaluation of four methods of state of the art in text simplification, evaluated according to readability metrics, to improve the quality of existing texts in the judicial summaries, dataset containing 100 summaries of the Federal Regional Court of the 5th Region (TRF5) and another 100 of the Federal Supreme Court (STF). The methods MUSS(EN), MUSS(PT), Transformers and NMT + Attention were tested, and the results of the simplifications exceeded the FRE readability index of the original texts, making them more readable.",
        "DOI": "10.3233/FAIA230975",
        "paper_author": "Alves A.",
        "affiliation_name": "CESAR Centro de Estudios e Sistemas Avancados do Recife",
        "affiliation_city": "Recife",
        "affiliation_country": "Brazil",
        "affiliation_id": "60086758",
        "affiliation_state": "PE"
    }
]