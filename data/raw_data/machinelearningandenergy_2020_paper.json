[
    {
        "paper_title": "Memory devices and applications for in-memory computing",
        "publication": "Nature Nanotechnology",
        "citied_by": "1362",
        "cover_date": "2020-07-01",
        "Abstract": "Traditional von Neumann computing systems involve separate processing and memory units. However, data movement is costly in terms of time and energy and this problem is aggravated by the recent explosive growth in highly data-centric applications related to artificial intelligence. This calls for a radical departure from the traditional systems and one such non-von Neumann computational approach is in-memory computing. Hereby certain computational tasks are performed in place in the memory itself by exploiting the physical attributes of the memory devices. Both charge-based and resistance-based memory devices are being explored for in-memory computing. In this Review, we provide a broad overview of the key computational primitives enabled by these memory devices as well as their applications spanning scientific computing, signal processing, optimization, machine learning, deep learning and stochastic computing.",
        "DOI": "10.1038/s41565-020-0655-z",
        "paper_author": "Sebastian A.",
        "affiliation_name": "IBM Research - Zurich",
        "affiliation_city": "Ruschlikon",
        "affiliation_country": "Switzerland",
        "affiliation_id": "60029158",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "An energy approach to the solution of partial differential equations in computational mechanics via machine learning: Concepts, implementation and applications",
        "publication": "Computer Methods in Applied Mechanics and Engineering",
        "citied_by": "1249",
        "cover_date": "2020-04-15",
        "Abstract": "Partial Differential Equations (PDEs) are fundamental to model different phenomena in science and engineering mathematically. Solving them is a crucial step towards a precise knowledge of the behavior of natural and engineered systems. In general, in order to solve PDEs that represent real systems to an acceptable degree, analytical methods are usually not enough. One has to resort to discretization methods. For engineering problems, probably the best-known option is the finite element method (FEM). However, powerful alternatives such as mesh-free methods and Isogeometric Analysis (IGA) are also available. The fundamental idea is to approximate the solution of the PDE by means of functions specifically built to have some desirable properties. In this contribution, we explore Deep Neural Networks (DNNs) as an option for approximation. They have shown impressive results in areas such as visual recognition. DNNs are regarded here as function approximation machines. There is great flexibility to define their structure and important advances in the architecture and the efficiency of the algorithms to implement them make DNNs a very interesting alternative to approximate the solution of a PDE. We concentrate on applications that have an interest for Computational Mechanics. Most contributions explore this possibility have adopted a collocation strategy. In this work, we concentrate on mechanical problems and analyze the energetic format of the PDE. The energy of a mechanical system seems to be the natural loss function for a machine learning method to approach a mechanical problem. In order to prove the concepts, we deal with several problems and explore the capabilities of the method for applications in engineering.",
        "DOI": "10.1016/j.cma.2019.112790",
        "paper_author": "Samaniego E.",
        "affiliation_name": "University of Cuenca",
        "affiliation_city": "Cuenca",
        "affiliation_country": "Ecuador",
        "affiliation_id": "60072035",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Accelerated discovery of CO<inf>2</inf> electrocatalysts using active machine learning",
        "publication": "Nature",
        "citied_by": "1012",
        "cover_date": "2020-05-14",
        "Abstract": "The rapid increase in global energy demand and the need to replace carbon dioxide (CO2)-emitting fossil fuels with renewable sources have driven interest in chemical storage of intermittent solar and wind energy1,2. Particularly attractive is the electrochemical reduction of CO2 to chemical feedstocks, which uses both CO2 and renewable energy3–8. Copper has been the predominant electrocatalyst for this reaction when aiming for more valuable multi-carbon products9–16, and process improvements have been particularly notable when targeting ethylene. However, the energy efficiency and productivity (current density) achieved so far still fall below the values required to produce ethylene at cost-competitive prices. Here we describe Cu-Al electrocatalysts, identified using density functional theory calculations in combination with active machine learning, that efficiently reduce CO2 to ethylene with the highest Faradaic efficiency reported so far. This Faradaic efficiency of over 80 per cent (compared to about 66 per cent for pure Cu) is achieved at a current density of 400 milliamperes per square centimetre (at 1.5 volts versus a reversible hydrogen electrode) and a cathodic-side (half-cell) ethylene power conversion efficiency of 55 ± 2 per cent at 150 milliamperes per square centimetre. We perform computational studies that suggest that the Cu-Al alloys provide multiple sites and surface orientations with near-optimal CO binding for both efficient and selective CO2 reduction17. Furthermore, in situ X-ray absorption measurements reveal that Cu and Al enable a favourable Cu coordination environment that enhances C–C dimerization. These findings illustrate the value of computation and machine learning in guiding the experimental exploration of multi-metallic systems that go beyond the limitations of conventional single-metal electrocatalysts.",
        "DOI": "10.1038/s41586-020-2242-8",
        "paper_author": "Zhong M.",
        "affiliation_name": "University of Toronto",
        "affiliation_city": "Toronto",
        "affiliation_country": "Canada",
        "affiliation_id": "60016849",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "Federated learning via over-the-air computation",
        "publication": "IEEE Transactions on Wireless Communications",
        "citied_by": "796",
        "cover_date": "2020-03-01",
        "Abstract": "The stringent requirements for low-latency and privacy of the emerging high-stake applications with intelligent devices such as drones and smart vehicles make the cloud computing inapplicable in these scenarios. Instead, edge machine learning becomes increasingly attractive for performing training and inference directly at network edges without sending data to a centralized data center. This stimulates a nascent field termed as federated learning for training a machine learning model on computation, storage, energy and bandwidth limited mobile devices in a distributed manner. To preserve data privacy and address the issues of unbalanced and non-IID data points across different devices, the federated averaging algorithm has been proposed for global model aggregation by computing the weighted average of locally updated model at each selected device. However, the limited communication bandwidth becomes the main bottleneck for aggregating the locally computed updates. We thus propose a novel over-the-air computation based approach for fast global model aggregation via exploring the superposition property of a wireless multiple-access channel. This is achieved by joint device selection and beamforming design, which is modeled as a sparse and low-rank optimization problem to support efficient algorithms design. To achieve this goal, we provide a difference-of-convex-functions (DC) representation for the sparse and low-rank function to enhance sparsity and accurately detect the fixed-rank constraint in the procedure of device selection. A DC algorithm is further developed to solve the resulting DC program with global convergence guarantees. The algorithmic advantages and admirable performance of the proposed methodologies are demonstrated through extensive numerical results.",
        "DOI": "10.1109/TWC.2019.2961673",
        "paper_author": "Yang K.",
        "affiliation_name": "ShanghaiTech University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60105232",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A review and evaluation of the state-of-the-art in PV solar power forecasting: Techniques and optimization",
        "publication": "Renewable and Sustainable Energy Reviews",
        "citied_by": "790",
        "cover_date": "2020-05-01",
        "Abstract": "Integration of photovoltaics into power grids is difficult as solar energy is highly dependent on climate and geography; often fluctuating erratically. This causes penetrations and voltage surges, system instability, inefficient utilities planning and financial loss. Forecast models can help; however, time stamp, forecast horizon, input correlation analysis, data pre and post-processing, weather classification, network optimization, uncertainty quantification and performance evaluations need consideration. Thus, contemporary forecasting techniques are reviewed and evaluated. Input correlational analyses reveal that solar irradiance is most correlated with Photovoltaic output, and so, weather classification and cloud motion study are crucial. Moreover, the best data cleansing processes: normalization and wavelet transforms, and augmentation using generative adversarial network are recommended for network training and forecasting. Furthermore, optimization of inputs and network parameters, using genetic algorithm and particle swarm optimization, is emphasized. Next, established performance evaluation metrics MAE, RMSE and MAPE are discussed, with suggestions for including economic utility metrics. Subsequently, modelling approaches are critiqued, objectively compared and categorized into physical, statistical, artificial intelligence, ensemble and hybrid approaches. It is determined that ensembles of artificial neural networks are best for forecasting short term photovoltaic power forecast and online sequential extreme learning machine superb for adaptive networks; while Bootstrap technique optimum for estimating uncertainty. Additionally, convolutional neural network is found to excel in eliciting a model's deep underlying non-linear input-output relationships. The conclusions drawn impart fresh insights in photovoltaic power forecast initiatives, especially in the use of hybrid artificial neural networks and evolutionary algorithms.",
        "DOI": "10.1016/j.rser.2020.109792",
        "paper_author": "Ahmed R.",
        "affiliation_name": "The University of Western Australia",
        "affiliation_city": "Perth",
        "affiliation_country": "Australia",
        "affiliation_id": "60031806",
        "affiliation_state": "WA"
    },
    {
        "paper_title": "Energy-based out-of-distribution detection",
        "publication": "Advances in Neural Information Processing Systems",
        "citied_by": "781",
        "cover_date": "2020-01-01",
        "Abstract": "Determining whether inputs are out-of-distribution (OOD) is an essential building block for safely deploying machine learning models in the open world. However, previous methods relying on the softmax confidence score suffer from overconfident posterior distributions for OOD data. We propose a unified framework for OOD detection that uses an energy score. We show that energy scores better distinguish in- and out-of-distribution samples than the traditional approach using the softmax scores. Unlike softmax confidence scores, energy scores are theoretically aligned with the probability density of the inputs and are less susceptible to the overconfidence issue. Within this framework, energy can be flexibly used as a scoring function for any pre-trained neural classifier as well as a trainable cost function to shape the energy surface explicitly for OOD detection. On a CIFAR-10 pre-trained WideResNet, using the energy score reduces the average FPR (at TPR 95%) by 18.03% compared to the softmax confidence score. With energy-based training, our method outperforms the state-of-the-art on common benchmarks.",
        "DOI": "NA",
        "paper_author": "Liu W.",
        "affiliation_name": "Department of Computer Science and Engineering",
        "affiliation_city": "La Jolla",
        "affiliation_country": "United States",
        "affiliation_id": "60121653",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "A review of composite solid-state electrolytes for lithium batteries: Fundamentals, key materials and advanced structures",
        "publication": "Chemical Society Reviews",
        "citied_by": "659",
        "cover_date": "2020-12-07",
        "Abstract": "All-solid-state lithium ion batteries (ASSLBs) are considered next-generation devices for energy storage due to their advantages in safety and potentially high energy density. As the key component in ASSLBs, solid-state electrolytes (SSEs) with non-flammability and good adaptability to lithium metal anodes have attracted extensive attention in recent years. Among the current SSEs, composite solid-state electrolytes (CSSEs) with multiple phases have greater flexibility to customize and combine the advantages of single-phase electrolytes, which have been widely investigated recently and regarded as promising candidates for commercial ASSLBs. Based on existing investigations, herein, we present a comprehensive overview of the recent developments in CSSEs. Initially, we introduce the historical development from solid-state ionic conductors to CSSEs, and then summarize the fundamentals including mechanisms of lithium ion transport, key evaluation parameters, design principles, and key materials. Four main types of advanced structures for CSSEs are classified and highlighted according to the recent progress. Moreover, advanced characterization and computational simulation techniques including machine learning are reviewed for the first time, and the main challenges and perspectives of CSSEs are also provided for their future development. This journal is.",
        "DOI": "10.1039/d0cs00305k",
        "paper_author": "Zheng Y.",
        "affiliation_name": "Waterloo Institute for Nanotechnology",
        "affiliation_city": "Waterloo",
        "affiliation_country": "Canada",
        "affiliation_id": "60115730",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "Client-Edge-Cloud Hierarchical Federated Learning",
        "publication": "IEEE International Conference on Communications",
        "citied_by": "614",
        "cover_date": "2020-06-01",
        "Abstract": "Federated Learning is a collaborative machine learning framework to train a deep learning model without accessing clients' private data. Previous works assume one central parameter server either at the cloud or at the edge. The cloud server can access more data but with excessive communication overhead and long latency, while the edge server enjoys more efficient communications with the clients. To combine their advantages, we propose a client-edge-cloud hierarchical Federated Learning system, supported with a HierFAVG algorithm that allows multiple edge servers to perform partial model aggregation. In this way, the model can be trained faster and better communication-computation trade-offs can be achieved. Convergence analysis is provided for HierFAVG and the effects of key parameters are also investigated, which lead to qualitative design guidelines. Empirical experiments verify the analysis and demonstrate the benefits of this hierarchical architecture in different data distribution scenarios. Particularly, it is shown that by introducing the intermediate edge servers, the model training time and the energy consumption of the end devices can be simultaneously reduced compared to cloud-based Federated Learning.",
        "DOI": "10.1109/ICC40277.2020.9148862",
        "paper_author": "Liu L.",
        "affiliation_name": "Hong Kong University of Science and Technology",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong",
        "affiliation_id": "60008592",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Survey of Multi-Access Edge Computing in 5G and Beyond: Fundamentals, Technology Integration, and State-of-the-Art",
        "publication": "IEEE Access",
        "citied_by": "608",
        "cover_date": "2020-01-01",
        "Abstract": "Driven by the emergence of new compute-intensive applications and the vision of the Internet of Things (IoT), it is foreseen that the emerging 5G network will face an unprecedented increase in traffic volume and computation demands. However, end users mostly have limited storage capacities and finite processing capabilities, thus how to run compute-intensive applications on resource-constrained users has recently become a natural concern. Mobile edge computing (MEC), a key technology in the emerging fifth generation (5G) network, can optimize mobile resources by hosting compute-intensive applications, process large data before sending to the cloud, provide the cloud-computing capabilities within the radio access network (RAN) in close proximity to mobile users, and offer context-aware services with the help of RAN information. Therefore, MEC enables a wide variety of applications, where the real-time response is strictly required, e.g., driverless vehicles, augmented reality, robotics, and immerse media. Indeed, the paradigm shift from 4G to 5G could become a reality with the advent of new technological concepts. The successful realization of MEC in the 5G network is still in its infancy and demands for constant efforts from both academic and industry communities. In this survey, we first provide a holistic overview of MEC technology and its potential use cases and applications. Then, we outline up-to-date researches on the integration of MEC with the new technologies that will be deployed in 5G and beyond. We also summarize testbeds and experimental evaluations, and open source activities, for edge computing. We further summarize lessons learned from state-of-the-art research works as well as discuss challenges and potential future directions for MEC research.",
        "DOI": "10.1109/ACCESS.2020.3001277",
        "paper_author": "Pham Q.V.",
        "affiliation_name": "Pusan National University",
        "affiliation_city": "Busan",
        "affiliation_country": "South Korea",
        "affiliation_id": "60008783",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Machine learning for molecular simulation",
        "publication": "Annual Review of Physical Chemistry",
        "citied_by": "590",
        "cover_date": "2020-04-20",
        "Abstract": "Machine learning (ML) is transforming all areas of science. The complex and time-consuming calculations in molecular simulations are particularly suitable for an ML revolution and have already been profoundly affected by the application of existing ML methods. Here we review recent ML methods for molecular simulation, with particular focus on (deep) neural networks for the prediction of quantum-mechanical energies and forces, on coarse-grained molecular dynamics, on the extraction of free energy surfaces and kinetics, and on generative network approaches to sample molecular equilibrium structures and compute thermodynamics. To explain these methods and illustrate open methodological problems, we review some important principles of molecular physics and describe how they can be incorporated into ML structures. Finally, we identify and describe a list of open challenges for the interface between ML and molecular simulation.",
        "DOI": "10.1146/annurev-physchem-042018-052331",
        "paper_author": "Noé F.",
        "affiliation_name": "Freie Universität Berlin",
        "affiliation_city": "Berlin",
        "affiliation_country": "Germany",
        "affiliation_id": "60030718",
        "affiliation_state": "Berlin"
    },
    {
        "paper_title": "Performance and Cost Assessment of Machine Learning Interatomic Potentials",
        "publication": "Journal of Physical Chemistry A",
        "citied_by": "566",
        "cover_date": "2020-01-30",
        "Abstract": "Machine learning of the quantitative relationship between local environment descriptors and the potential energy surface of a system of atoms has emerged as a new frontier in the development of interatomic potentials (IAPs). Here, we present a comprehensive evaluation of machine learning IAPs (ML-IAPs) based on four local environment descriptors - atom-centered symmetry functions (ACSF), smooth overlap of atomic positions (SOAP), the spectral neighbor analysis potential (SNAP) bispectrum components, and moment tensors - using a diverse data set generated using high-throughput density functional theory (DFT) calculations. The data set comprising bcc (Li, Mo) and fcc (Cu, Ni) metals and diamond group IV semiconductors (Si, Ge) is chosen to span a range of crystal structures and bonding. All descriptors studied show excellent performance in predicting energies and forces far surpassing that of classical IAPs, as well as predicting properties such as elastic constants and phonon dispersion curves. We observe a general trade-off between accuracy and the degrees of freedom of each model and, consequently, computational cost. We will discuss these trade-offs in the context of model selection for molecular dynamics and other applications.",
        "DOI": "10.1021/acs.jpca.9b08723",
        "paper_author": "Zuo Y.",
        "affiliation_name": "Aiiso Yufeng Li Family Department of Chemical and Nano Engineering",
        "affiliation_city": "La Jolla",
        "affiliation_country": "United States",
        "affiliation_id": "60121662",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Artificial intelligence for sustainability: Challenges, opportunities, and a research agenda",
        "publication": "International Journal of Information Management",
        "citied_by": "562",
        "cover_date": "2020-08-01",
        "Abstract": "Artificial intelligence (AI) will transform business practices and industries and has the potential to address major societal problems, including sustainability. Degradation of the natural environment and the climate crisis are exceedingly complex phenomena requiring the most advanced and innovative solutions. Aiming to spur groundbreaking research and practical solutions of AI for environmental sustainability, we argue that AI can support the derivation of culturally appropriate organizational processes and individual practices to reduce the natural resource and energy intensity of human activities. The true value of AI will not be in how it enables society to reduce its energy, water, and land use intensities, but rather, at a higher level, how it facilitates and fosters environmental governance. A comprehensive review of the literature indicates that research regarding AI for sustainability is challenged by (1) overreliance on historical data in machine learning models, (2) uncertain human behavioral responses to AI-based interventions, (3) increased cybersecurity risks, (4) adverse impacts of AI applications, and (5) difficulties in measuring effects of intervention strategies. The review indicates that future studies of AI for sustainability should incorporate (1) multilevel views, (2) systems dynamics approaches, (3) design thinking, (4) psychological and sociological considerations, and (5) economic value considerations to show how AI can deliver immediate solutions without introducing long-term threats to environmental sustainability.",
        "DOI": "10.1016/j.ijinfomgt.2020.102104",
        "paper_author": "Nishant R.",
        "affiliation_name": "Université Laval",
        "affiliation_city": "Quebec",
        "affiliation_country": "Canada",
        "affiliation_id": "60032619",
        "affiliation_state": "QC"
    },
    {
        "paper_title": "Haptic-feedback smart glove as a creative human-machine interface (HMI) for virtual/augmented reality applications",
        "publication": "Science Advances",
        "citied_by": "555",
        "cover_date": "2020-05-01",
        "Abstract": "Human-machine interfaces (HMIs) experience increasing requirements for intuitive and effective manipulation. Current commercialized solutions of glove-based HMI are limited by either detectable motions or the huge cost on fabrication, energy, and computing power. We propose the haptic-feedback smart glove with triboelectric-based finger bending sensors, palm sliding sensor, and piezoelectric mechanical stimulators. The detection of multidirectional bending and sliding events is demonstrated in virtual space using the self-generated triboelectric signals for various degrees of freedom on human hand. We also perform haptic mechanical stimulation via piezoelectric chips to realize the augmented HMI. The smart glove achieves object recognition using machine learning technique, with an accuracy of 96%. Through the integrated demonstration of multidimensional manipulation, haptic feedback, and AI-based object recognition, our glove reveals its potential as a promising solution for low-cost and advanced human-machine interaction, which can benefit diversified areas, including entertainment, home healthcare, sports training, and medical industry.",
        "DOI": "10.1126/sciadv.aaz8693",
        "paper_author": "Zhu M.",
        "affiliation_name": "National University of Singapore",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore",
        "affiliation_id": "60017161",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "DScribe: Library of descriptors for machine learning in materials science",
        "publication": "Computer Physics Communications",
        "citied_by": "523",
        "cover_date": "2020-02-01",
        "Abstract": "DScribe is a software package for machine learning that provides popular feature transformations (“descriptors”) for atomistic materials simulations. DScribe accelerates the application of machine learning for atomistic property prediction by providing user-friendly, off-the-shelf descriptor implementations. The package currently contains implementations for Coulomb matrix, Ewald sum matrix, sine matrix, Many-body Tensor Representation (MBTR), Atom-centered Symmetry Function (ACSF) and Smooth Overlap of Atomic Positions (SOAP). Usage of the package is illustrated for two different applications: formation energy prediction for solids and ionic charge prediction for atoms in organic molecules. The package is freely available under the open-source Apache License 2.0. Program summary: Program Title: DScribe Program Files doi: http://dx.doi.org/10.17632/vzrs8n8pk6.1 Licensing provisions: Apache-2.0 Programming language: Python/C/C++ Supplementary material: Supplementary Information as PDF Nature of problem: The application of machine learning for materials science is hindered by the lack of consistent software implementations for feature transformations. These feature transformations, also called descriptors, are a key step in building machine learning models for property prediction in materials science. Solution method: We have developed a library for creating common descriptors used in machine learning applied to materials science. We provide an implementation the following descriptors: Coulomb matrix, Ewald sum matrix, sine matrix, Many-body Tensor Representation (MBTR), Atom-centered Symmetry Functions (ACSF) and Smooth Overlap of Atomic Positions (SOAP). The library has a python interface with computationally intensive routines written in C or C++. The source code, tutorials and documentation are provided online. A continuous integration mechanism is set up to automatically run a series of regression tests and check code coverage when the codebase is updated.",
        "DOI": "10.1016/j.cpc.2019.106949",
        "paper_author": "Himanen L.",
        "affiliation_name": "Aalto University",
        "affiliation_city": "Espoo",
        "affiliation_country": "Finland",
        "affiliation_id": "60103653",
        "affiliation_state": "Uusimaa"
    },
    {
        "paper_title": "Applications of Artificial Intelligence and Machine learning in smart cities",
        "publication": "Computer Communications",
        "citied_by": "507",
        "cover_date": "2020-03-15",
        "Abstract": "Smart cities are aimed to efficiently manage growing urbanization, energy consumption, maintain a green environment, improve the economic and living standards of their citizens, and raise the people's capabilities to efficiently use and adopt the modern information and communication technology (ICT). In the smart cities concept, ICT is playing a vital role in policy design, decision, implementation, and ultimate productive services. The primary objective of this review is to explore the role of artificial intelligence (AI), machine learning (ML), and deep reinforcement learning (DRL) in the evolution of smart cities. The preceding techniques are efficiently used to design optimal policy regarding various smart city-oriented complex problems. In this survey, we present in-depth details of the applications of the prior techniques in intelligent transportation systems (ITSs), cyber-security, energy-efficient utilization of smart grids (SGs), effective use of unmanned aerial vehicles (UAVs) to assure the best services of 5G and beyond 5G (B5G) communications, and smart health care system in a smart city. Finally, we present various research challenges and future research directions where the aforementioned techniques can play an outstanding role to realize the concept of a smart city.",
        "DOI": "10.1016/j.comcom.2020.02.069",
        "paper_author": "Ullah Z.",
        "affiliation_name": "Università degli Studi di Camerino",
        "affiliation_city": "Camerino",
        "affiliation_country": "Italy",
        "affiliation_id": "60026298",
        "affiliation_state": "MC"
    },
    {
        "paper_title": "Predicting the state of charge and health of batteries using data-driven machine learning",
        "publication": "Nature Machine Intelligence",
        "citied_by": "504",
        "cover_date": "2020-03-01",
        "Abstract": "Machine learning is a specific application of artificial intelligence that allows computers to learn and improve from data and experience via sets of algorithms, without the need for reprogramming. In the field of energy storage, machine learning has recently emerged as a promising modelling approach to determine the state of charge, state of health and remaining useful life of batteries. First, we review the two most studied types of battery models in the literature for battery state prediction: the equivalent circuit and physics-based models. Based on the current limitations of these models, we showcase the promise of various machine learning techniques for fast and accurate battery state prediction. Finally, we highlight the major challenges involved, especially in accurate modelling over length and time, performing in situ calculations and high-throughput data generation. Overall, this work provides insights into real-time, explainable machine learning for battery production, management and optimization in the future.",
        "DOI": "10.1038/s42256-020-0156-7",
        "paper_author": "Ng M.F.",
        "affiliation_name": "A-Star, Institute of High Performance Computing",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore",
        "affiliation_id": "60004678",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "DP-GEN: A concurrent learning platform for the generation of reliable deep learning based potential energy models",
        "publication": "Computer Physics Communications",
        "citied_by": "472",
        "cover_date": "2020-08-01",
        "Abstract": "In recent years, promising deep learning based interatomic potential energy surface (PES) models have been proposed that can potentially allow us to perform molecular dynamics simulations for large scale systems with quantum accuracy. However, making these models truly reliable and practically useful is still a very non-trivial task. A key component in this task is the generation of datasets used in model training. In this paper, we introduce the Deep Potential GENerator (DP-GEN), an open-source software platform that implements the recently proposed ”on-the-fly” learning procedure (Zhang et al. 2019) and is capable of generating uniformly accurate deep learning based PES models in a way that minimizes human intervention and the computational cost for data generation and model training. DP-GEN automatically and iteratively performs three steps: exploration, labeling, and training. It supports various popular packages for these three steps: LAMMPS for exploration, Quantum Espresso, VASP, CP2K, etc. for labeling, and DeePMD-kit for training. It also allows automatic job submission and result collection on different types of machines, such as high performance clusters and cloud machines, and is adaptive to different job management tools, including Slurm, PBS, and LSF. As a concrete example, we illustrate the details of the process for generating a general-purpose PES model for Cu using DP-GEN. Program summary: Program Title: DP-GEN Program Files doi: http://dx.doi.org/10.17632/sxybkgc5xc.1 Licensing provisions: LGPL Programming language: Python Nature of problem: Generating reliable deep learning based potential energy models with minimal human intervention and computational cost. Solution method: The concurrent learning scheme is implemented. Supports for sampling configuration space with LAMMPS, generating ab initio data with Quantum Espresso, VASP, CP2K and training potential models with DeePMD-kit are provided. Supports for different machines including workstations, high performance clusters and cloud machines are provided. Supports for job management tools including Slurm, PBS, LSF are provided.",
        "DOI": "10.1016/j.cpc.2020.107206",
        "paper_author": "Zhang Y.",
        "affiliation_name": "Beijing Institute of Big Data Research",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "101226917",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Reliable Federated Learning for Mobile Networks",
        "publication": "IEEE Wireless Communications",
        "citied_by": "464",
        "cover_date": "2020-04-01",
        "Abstract": "Federated learning, as a promising machine learning approach, has emerged to leverage a distributed personalized dataset from a number of nodes, for example, mobile devices, to improve performance while simultaneously providing privacy preservation for mobile users. In federated learning, training data is widely distributed and maintained on the mobile devices as workers. A central aggregator updates a global model by collecting local updates from mobile devices using their local training data to train the global model in each iteration. However, unreliable data may be uploaded by the mobile devices (i.e., workers), leading to frauds in tasks of federated learning. The workers may perform unreliable updates intentionally, for example, the data poisoning attack, or unintentionally, for example, low-quality data caused by energy constraints or high-speed mobility. Therefore, finding out trusted and reliable workers in federated learning tasks becomes critical. In this article, the concept of reputation is introduced as a metric. Based on this metric, a reliable worker selection scheme is proposed for federated learning tasks. Consortium blockchain is leveraged as a decentralized approach for achieving efficient reputation management of the workers without repudiation and tampering. By numerical analysis, the proposed approach is demonstrated to improve the reliability of federated learning tasks in mobile networks.",
        "DOI": "10.1109/MWC.001.1900119",
        "paper_author": "Kang J.",
        "affiliation_name": "Nanyang Technological University",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore",
        "affiliation_id": "60005510",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Federated Learning over Wireless Fading Channels",
        "publication": "IEEE Transactions on Wireless Communications",
        "citied_by": "461",
        "cover_date": "2020-05-01",
        "Abstract": "We study federated machine learning at the wireless network edge, where limited power wireless devices, each with its own dataset, build a joint model with the help of a remote parameter server (PS). We consider a bandwidth-limited fading multiple access channel (MAC) from the wireless devices to the PS, and propose various techniques to implement distributed stochastic gradient descent (DSGD) over this shared noisy wireless channel. We first propose a digital DSGD (D-DSGD) scheme, in which one device is selected opportunistically for transmission at each iteration based on the channel conditions; the scheduled device quantizes its gradient estimate to a finite number of bits imposed by the channel condition, and transmits these bits to the PS in a reliable manner. Next, motivated by the additive nature of the wireless MAC, we propose a novel analog communication scheme, referred to as the compressed analog DSGD (CA-DSGD), where the devices first sparsify their gradient estimates while accumulating error from previous iterations, and project the resultant sparse vector into a low-dimensional vector for bandwidth reduction. We also design a power allocation scheme to align the received gradient vectors at the PS in an efficient manner. Numerical results show that D-DSGD outperforms other digital approaches in the literature; however, in general the proposed CA-DSGD algorithm converges faster than the D-DSGD scheme, and reaches a higher level of accuracy. We have observed that the gap between the analog and digital schemes increases when the datasets of devices are not independent and identically distributed (i.i.d.). Furthermore, the performance of the CA-DSGD scheme is shown to be robust against imperfect channel state information (CSI) at the devices. Overall these results show clear advantages for the proposed analog over-the-air DSGD scheme, which suggests that learning and communication algorithms should be designed jointly to achieve the best end-to-end performance in machine learning applications at the wireless edge.",
        "DOI": "10.1109/TWC.2020.2974748",
        "paper_author": "Amiri M.M.",
        "affiliation_name": "School of Engineering and Applied Science",
        "affiliation_city": "Princeton",
        "affiliation_country": "United States",
        "affiliation_id": "60141284",
        "affiliation_state": "NJ"
    },
    {
        "paper_title": "Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation",
        "publication": "Machine Learning: Science and Technology",
        "citied_by": "449",
        "cover_date": "2020-12-01",
        "Abstract": "The discovery of novel materials and functional molecules can help to solve some of society's most urgent challenges, ranging from efficient energy harvesting and storage to uncovering novel pharmaceutical drug candidates. Traditionally matter engineering-generally denoted as inverse design-was based massively on human intuition and high-throughput virtual screening. The last few years have seen the emergence of significant interest in computer-inspired designs based on evolutionary or deep learning methods. The major challenge here is that the standard strings molecular representation SMILES shows substantial weaknesses in that task because large fractions of strings do not correspond to valid molecules. Here, we solve this problem at a fundamental level and introduce SELFIES (SELF-referencIng Embedded Strings), a string-based representation of molecules which is 100% robust. Every SELFIES string corresponds to a valid molecule, and SELFIES can represent every molecule. SELFIES can be directly applied in arbitrary machine learning models without the adaptation of the models; each of the generated molecule candidates is valid. In our experiments, the model's internal memory stores two orders of magnitude more diverse molecules than a similar test with SMILES. Furthermore, as all molecules are valid, it allows for explanation and interpretation of the internal working of the generative models.",
        "DOI": "10.1088/2632-2153/aba947",
        "paper_author": "Krenn M.",
        "affiliation_name": "University of Toronto",
        "affiliation_city": "Toronto",
        "affiliation_country": "Canada",
        "affiliation_id": "60016849",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "Using metadynamics to explore complex free-energy landscapes",
        "publication": "Nature Reviews Physics",
        "citied_by": "422",
        "cover_date": "2020-04-01",
        "Abstract": "Metadynamics is an atomistic simulation technique that allows, within the same framework, acceleration of rare events and estimation of the free energy of complex molecular systems. It is based on iteratively ‘filling’ the potential energy of the system by a sum of Gaussians centred along the trajectory followed by a suitably chosen set of collective variables (CVs), thereby forcing the system to migrate from one minimum to the next. The power of metadynamics is demonstrated by the large number of extensions and variants that have been developed. The first scope of this Technical Review is to present a critical comparison of these variants, discussing their advantages and disadvantages. The effectiveness of metadynamics, and that of the numerous alternative methods, is strongly influenced by the choice of the CVs. If an important variable is neglected, the resulting estimate of the free energy is unreliable, and predicted transition mechanisms may be qualitatively wrong. The second scope of this Technical Review is to discuss how the CVs should be selected, how to verify whether the chosen CVs are sufficient or redundant, and how to iteratively improve the CVs using machine learning approaches.",
        "DOI": "10.1038/s42254-020-0153-0",
        "paper_author": "Bussi G.",
        "affiliation_name": "Scuola Internazionale Superiore di Studi Avanzati",
        "affiliation_city": "Trieste",
        "affiliation_country": "Italy",
        "affiliation_id": "60012227",
        "affiliation_state": "TS"
    },
    {
        "paper_title": "Artificial intelligence and machine learning approaches to energy demand-side response: A systematic review",
        "publication": "Renewable and Sustainable Energy Reviews",
        "citied_by": "416",
        "cover_date": "2020-09-01",
        "Abstract": "Recent years have seen an increasing interest in Demand Response (DR) as a means to provide flexibility, and hence improve the reliability of energy systems in a cost-effective way. Yet, the high complexity of the tasks associated with DR, combined with their use of large-scale data and the frequent need for near real-time decisions, means that Artificial Intelligence (AI) and Machine Learning (ML) — a branch of AI — have recently emerged as key technologies for enabling demand-side response. AI methods can be used to tackle various challenges, ranging from selecting the optimal set of consumers to respond, learning their attributes and preferences, dynamic pricing, scheduling and control of devices, learning how to incentivise participants in the DR schemes and how to reward them in a fair and economically efficient way. This work provides an overview of AI methods utilised for DR applications, based on a systematic review of over 160 papers, 40 companies and commercial initiatives, and 21 large-scale projects. The papers are classified with regards to both the AI/ML algorithm(s) used and the application area in energy DR. Next, commercial initiatives are presented (including both start-ups and established companies) and large-scale innovation projects, where AI methods have been used for energy DR. The paper concludes with a discussion of advantages and potential limitations of reviewed AI techniques for different DR tasks, and outlines directions for future research in this fast-growing area.",
        "DOI": "10.1016/j.rser.2020.109899",
        "paper_author": "Antonopoulos I.",
        "affiliation_name": "Heriot-Watt University",
        "affiliation_city": "Edinburgh",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60019656",
        "affiliation_state": "Scotland"
    },
    {
        "paper_title": "A Critical Review of Machine Learning of Energy Materials",
        "publication": "Advanced Energy Materials",
        "citied_by": "412",
        "cover_date": "2020-02-01",
        "Abstract": "Machine learning (ML) is rapidly revolutionizing many fields and is starting to change landscapes for physics and chemistry. With its ability to solve complex tasks autonomously, ML is being exploited as a radically new way to help find material correlations, understand materials chemistry, and accelerate the discovery of materials. Here, an in-depth review of the application of ML to energy materials, including rechargeable alkali-ion batteries, photovoltaics, catalysts, thermoelectrics, piezoelectrics, and superconductors, is presented. A conceptual framework is first provided for ML in materials science, with a broad overview of different ML techniques as well as best practices. This is followed by a critical discussion of how ML is applied in energy materials. This review is concluded with the perspectives on major challenges and opportunities in this exciting field.",
        "DOI": "10.1002/aenm.201903242",
        "paper_author": "Chen C.",
        "affiliation_name": "Aiiso Yufeng Li Family Department of Chemical and Nano Engineering",
        "affiliation_city": "La Jolla",
        "affiliation_country": "United States",
        "affiliation_id": "60121662",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Deep reinforcement learning for power system applications: An overview",
        "publication": "CSEE Journal of Power and Energy Systems",
        "citied_by": "400",
        "cover_date": "2020-03-01",
        "Abstract": "Due to increasing complexity, uncertainty and data dimensions in power systems, conventional methods often meet bottlenecks when attempting to solve decision and control problems. Therefore, data-driven methods toward solving such problems are being extensively studied. Deep reinforcement learning (DRL) is one of these data-driven methods and is regarded as real artificial intelligence (AI). DRL is a combination of deep learning (DL) and reinforcement learning (RL). This field of research has been applied to solve a wide range of complex sequential decision-making problems, including those in power systems. This paper firstly reviews the basic ideas, models, algorithms and techniques of DRL. Applications in power systems such as energy management, demand response, electricity market, operational control, and others are then considered. In addition, recent advances in DRL including the combination of RL with other classical methods, and the prospect and challenges of applications in power systems are also discussed.",
        "DOI": "10.17775/CSEEJPES.2019.00920",
        "paper_author": "Zhang Z.",
        "affiliation_name": "Shanghai Jiao Tong University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60025084",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Novel CNN-GRU-Based Hybrid Approach for Short-Term Residential Load Forecasting",
        "publication": "IEEE Access",
        "citied_by": "400",
        "cover_date": "2020-01-01",
        "Abstract": "Electric energy forecasting domain attracts researchers due to its key role in saving energy resources, where mainstream existing models are based on Gradient Boosting Regression (GBR), Artificial Neural Networks (ANNs), Extreme Learning Machine (ELM) and Support Vector Machine (SVM). These models encounter high-level of non-linearity between input data and output predictions and limited adoptability in real-world scenarios. Meanwhile, energy forecasting domain demands more robustness, higher prediction accuracy and generalization ability for real-world implementation. In this paper, we achieve the mentioned tasks by developing a hybrid sequential learning-based energy forecasting model that employs Convolution Neural Network (CNN) and Gated Recurrent Units (GRU) into a unified framework for accurate energy consumption prediction. The proposed framework has two major phases: (1) data refinement and (2) training, where the data refinement phase applies preprocessing strategies over raw data. In the training phase, CNN features are extracted from input dataset and fed in to GRU, that is selected as optimal and observed to have enhanced sequence learning abilities after extensive experiments. The proposed model is an effective alternative to the previous hybrid models in terms of computational complexity as well prediction accuracy, due to the representative features' extraction potentials of CNNs and effectual gated structure of multi-layered GRU. The experimental evaluation over existing energy forecasting datasets reveal the better performance of our method in terms of preciseness and efficiency. The proposed method achieved the smallest error rate on Appliances Energy Prediction (AEP) and Individual Household Electric Power Consumption (IHEPC) datasets, when compared to other baseline models.",
        "DOI": "10.1109/ACCESS.2020.3009537",
        "paper_author": "Sajjad M.",
        "affiliation_name": "Islamia College, Peshawar",
        "affiliation_city": "Peshawar",
        "affiliation_country": "Pakistan",
        "affiliation_id": "60106555",
        "affiliation_state": "Khyber Pakhtunkhwa"
    }
]