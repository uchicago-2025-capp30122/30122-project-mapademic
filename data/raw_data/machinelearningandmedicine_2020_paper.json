[
    {
        "paper_title": "Introduction to radiomics",
        "publication": "Journal of Nuclear Medicine",
        "citied_by": "987",
        "cover_date": "2020-04-01",
        "Abstract": "Radiomics is a rapidly evolving field of research concerned with the extraction of quantitative metrics-the so-called radiomic features-within medical images. Radiomic features capture tissue and lesion characteristics such as heterogeneity and shape and may, alone or in combination with demographic, histologic, genomic, or proteomic data, be used for clinical problem solving. The goal of this continuing education article is to provide an introduction to the field, covering the basic radiomics workflow: feature calculation and selection, dimensionality reduction, and data processing. Potential clinical applications in nuclear medicine that include PET radiomics-based prediction of treatment response and survival will be discussed. Current limitations of radiomics, such as sensitivity to acquisition parameter variations, and common pitfalls will also be covered.",
        "DOI": "10.2967/JNUMED.118.222893",
        "paper_author": "Mayerhoefer M.E.",
        "affiliation_name": "Memorial Sloan-Kettering Cancer Center",
        "affiliation_city": "New York",
        "affiliation_country": "United States",
        "affiliation_id": "60009343",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "The rise of artificial intelligence in healthcare applications",
        "publication": "Artificial Intelligence in Healthcare",
        "citied_by": "810",
        "cover_date": "2020-01-01",
        "Abstract": "Big data and machine learning are having an impact on most aspects of modern life, from entertainment, commerce, and healthcare. Netflix knows which films and series people prefer to watch, Amazon knows which items people like to buy when and where, and Google knows which symptoms and conditions people are searching for. All this data can be used for very detailed personal profiling, which may be of great value for behavioral understanding and targeting but also has potential for predicting healthcare trends. There is great optimism that the application of artificial intelligence (AI) can provide substantial improvements in all areas of healthcare from diagnostics to treatment. It is generally believed that AI tools will facilitate and enhance human work and not replace the work of physicians and other healthcare staff as such. AI is ready to support healthcare personnel with a variety of tasks from administrative workflow to clinical documentation and patient outreach as well as specialized support such as in image analysis, medical device automation, and patient monitoring. In this chapter, some of the major applications of AI in healthcare will be discussed covering both the applications that are directly associated with healthcare and those in the healthcare value chain such as drug development and ambient assisted living.",
        "DOI": "10.1016/B978-0-12-818438-7.00002-2",
        "paper_author": "Bohr A.",
        "affiliation_name": "CEO and Co-Founder of Sonohaler",
        "affiliation_city": "Copenhagen",
        "affiliation_country": "Denmark",
        "affiliation_id": "127742272",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Explainability for artificial intelligence in healthcare: a multidisciplinary perspective",
        "publication": "BMC Medical Informatics and Decision Making",
        "citied_by": "792",
        "cover_date": "2020-12-01",
        "Abstract": "Background: Explainability is one of the most heavily debated topics when it comes to the application of artificial intelligence (AI) in healthcare. Even though AI-driven systems have been shown to outperform humans in certain analytical tasks, the lack of explainability continues to spark criticism. Yet, explainability is not a purely technological issue, instead it invokes a host of medical, legal, ethical, and societal questions that require thorough exploration. This paper provides a comprehensive assessment of the role of explainability in medical AI and makes an ethical evaluation of what explainability means for the adoption of AI-driven tools into clinical practice. Methods: Taking AI-based clinical decision support systems as a case in point, we adopted a multidisciplinary approach to analyze the relevance of explainability for medical AI from the technological, legal, medical, and patient perspectives. Drawing on the findings of this conceptual analysis, we then conducted an ethical assessment using the “Principles of Biomedical Ethics” by Beauchamp and Childress (autonomy, beneficence, nonmaleficence, and justice) as an analytical framework to determine the need for explainability in medical AI. Results: Each of the domains highlights a different set of core considerations and values that are relevant for understanding the role of explainability in clinical practice. From the technological point of view, explainability has to be considered both in terms how it can be achieved and what is beneficial from a development perspective. When looking at the legal perspective we identified informed consent, certification and approval as medical devices, and liability as core touchpoints for explainability. Both the medical and patient perspectives emphasize the importance of considering the interplay between human actors and medical AI. We conclude that omitting explainability in clinical decision support systems poses a threat to core ethical values in medicine and may have detrimental consequences for individual and public health. Conclusions: To ensure that medical AI lives up to its promises, there is a need to sensitize developers, healthcare professionals, and legislators to the challenges and limitations of opaque algorithms in medical AI and to foster multidisciplinary collaboration moving forward.",
        "DOI": "10.1186/s12911-020-01332-6",
        "paper_author": "Amann J.",
        "affiliation_name": "ETH Zürich",
        "affiliation_city": "Zurich",
        "affiliation_country": "Switzerland",
        "affiliation_id": "60025858",
        "affiliation_state": "ZH"
    },
    {
        "paper_title": "Secure, privacy-preserving and federated machine learning in medical imaging",
        "publication": "Nature Machine Intelligence",
        "citied_by": "730",
        "cover_date": "2020-06-01",
        "Abstract": "The broad application of artificial intelligence techniques in medicine is currently hindered by limited dataset availability for algorithm training and validation, due to the absence of standardized electronic medical records, and strict legal and ethical requirements to protect patient privacy. In medical imaging, harmonized data exchange formats such as Digital Imaging and Communication in Medicine and electronic data storage are the standard, partially addressing the first issue, but the requirements for privacy preservation are equally strict. To prevent patient privacy compromise while promoting scientific research on large datasets that aims to improve patient care, the implementation of technical solutions to simultaneously address the demands for data protection and utilization is mandatory. Here we present an overview of current and next-generation methods for federated, secure and privacy-preserving artificial intelligence with a focus on medical imaging applications, alongside potential attack vectors and future prospects in medical imaging and beyond.",
        "DOI": "10.1038/s42256-020-0186-1",
        "paper_author": "Kaissis G.A.",
        "affiliation_name": "Technische Universität München",
        "affiliation_city": "Munich",
        "affiliation_country": "Germany",
        "affiliation_id": "60019722",
        "affiliation_state": "Bayern"
    },
    {
        "paper_title": "Introduction to machine learning, neural networks, and deep learning",
        "publication": "Translational Vision Science and Technology",
        "citied_by": "661",
        "cover_date": "2020-01-01",
        "Abstract": "Purpose: To present an overview of current machine learning methods and their use in medical research, focusing on select machine learning techniques, best practices, and deep learning. Methods: A systematic literature search in PubMed was performed for articles pertinent to the topic of artificial intelligence methods used in medicine with an emphasis on ophthalmology. Results: A review of machine learning and deep learning methodology for the audience without an extensive technical computer programming background. Conclusions: Artificial intelligence has a promising future in medicine; however, many challenges remain. Translational Relevance: The aim of this review article is to provide the nontechnical readers a layman’s explanation of the machine learning methods being used in medicine today. The goal is to provide the reader a better understanding of the potential and challenges of artificial intelligence within the field of medicine.",
        "DOI": "10.1167/tvst.9.2.14",
        "paper_author": "Choi R.Y.",
        "affiliation_name": "Oregon Health &amp; Science University",
        "affiliation_city": "Portland",
        "affiliation_country": "United States",
        "affiliation_id": "60016733",
        "affiliation_state": "OR"
    },
    {
        "paper_title": "The state of artificial intelligence-based FDA-approved medical devices and algorithms: an online database",
        "publication": "npj Digital Medicine",
        "citied_by": "644",
        "cover_date": "2020-12-01",
        "Abstract": "At the beginning of the artificial intelligence (AI)/machine learning (ML) era, the expectations are high, and experts foresee that AI/ML shows potential for diagnosing, managing and treating a wide variety of medical conditions. However, the obstacles for implementation of AI/ML in daily clinical practice are numerous, especially regarding the regulation of these technologies. Therefore, we provide an insight into the currently available AI/ML-based medical devices and algorithms that have been approved by the US Food & Drugs Administration (FDA). We aimed to raise awareness of the importance of regulatory bodies, clearly stating whether a medical device is AI/ML based or not. Cross-checking and validating all approvals, we identified 64 AI/ML based, FDA approved medical devices and algorithms. Out of those, only 29 (45%) mentioned any AI/ML-related expressions in the official FDA announcement. The majority (85.9%) was approved by the FDA with a 510(k) clearance, while 8 (12.5%) received de novo pathway clearance and one (1.6%) premarket approval (PMA) clearance. Most of these technologies, notably 30 (46.9%), 16 (25.0%), and 10 (15.6%) were developed for the fields of Radiology, Cardiology and Internal Medicine/General Practice respectively. We have launched the first comprehensive and open access database of strictly AI/ML-based medical technologies that have been approved by the FDA. The database will be constantly updated.",
        "DOI": "10.1038/s41746-020-00324-0",
        "paper_author": "Benjamens S.",
        "affiliation_name": "Universitair Medisch Centrum Groningen",
        "affiliation_city": "Groningen",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60006027",
        "affiliation_state": "Groningen"
    },
    {
        "paper_title": "The Future of Healthcare Internet of Things: A Survey of Emerging Technologies",
        "publication": "IEEE Communications Surveys and Tutorials",
        "citied_by": "627",
        "cover_date": "2020-04-01",
        "Abstract": "The impact of the Internet of Things (IoT) on the advancement of the healthcare industry is immense. The ushering of the Medicine 4.0 has resulted in an increased effort to develop platforms, both at the hardware level as well as the underlying software level. This vision has led to the development of Healthcare IoT (H-IoT) systems. The basic enabling technologies include the communication systems between the sensing nodes and the processors; and the processing algorithms for generating an output from the data collected by the sensors. However, at present, these enabling technologies are also supported by several new technologies. The use of Artificial Intelligence (AI) has transformed the H-IoT systems at almost every level. The fog/edge paradigm is bringing the computing power close to the deployed network and hence mitigating many challenges in the process. While the big data allows handling an enormous amount of data. Additionally, the Software Defined Networks (SDNs) bring flexibility to the system while the blockchains are finding the most novel use cases in H-IoT systems. The Internet of Nano Things (IoNT) and Tactile Internet (TI) are driving the innovation in the H-IoT applications. This paper delves into the ways these technologies are transforming the H-IoT systems and also identifies the future course for improving the Quality of Service (QoS) using these new technologies.",
        "DOI": "10.1109/COMST.2020.2973314",
        "paper_author": "Qadri Y.A.",
        "affiliation_name": "Yeungnam University",
        "affiliation_city": "Gyeongsan",
        "affiliation_country": "South Korea",
        "affiliation_id": "60001170",
        "affiliation_state": "Gyeongsangbuk-do"
    },
    {
        "paper_title": "Human postprandial responses to food and potential for precision nutrition",
        "publication": "Nature Medicine",
        "citied_by": "497",
        "cover_date": "2020-06-01",
        "Abstract": "Metabolic responses to food influence risk of cardiometabolic disease, but large-scale high-resolution studies are lacking. We recruited n = 1,002 twins and unrelated healthy adults in the United Kingdom to the PREDICT 1 study and assessed postprandial metabolic responses in a clinical setting and at home. We observed large inter-individual variability (as measured by the population coefficient of variation (s.d./mean, %)) in postprandial responses of blood triglyceride (103%), glucose (68%) and insulin (59%) following identical meals. Person-specific factors, such as gut microbiome, had a greater influence (7.1% of variance) than did meal macronutrients (3.6%) for postprandial lipemia, but not for postprandial glycemia (6.0% and 15.4%, respectively); genetic variants had a modest impact on predictions (9.5% for glucose, 0.8% for triglyceride, 0.2% for C-peptide). Findings were independently validated in a US cohort (n = 100 people). We developed a machine-learning model that predicted both triglyceride (r = 0.47) and glycemic (r = 0.77) responses to food intake. These findings may be informative for developing personalized diet strategies. The ClinicalTrials.gov registration identifier is NCT03479866.",
        "DOI": "10.1038/s41591-020-0934-0",
        "paper_author": "Berry S.E.",
        "affiliation_name": "King's College London",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60011520",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Artificial intelligence with multi-functional machine learning platform development for better healthcare and precision medicine",
        "publication": "Database",
        "citied_by": "473",
        "cover_date": "2020-01-01",
        "Abstract": "Precision medicine is one of the recent and powerful developments in medical care, which has the potential to improve the traditional symptom-driven practice of medicine, allowing earlier interventions using advanced diagnostics and tailoring better and economically personalized treatments. Identifying the best pathway to personalized and population medicine involves the ability to analyze comprehensive patient information together with broader aspects to monitor and distinguish between sick and relatively healthy people, which will lead to a better understanding of biological indicators that can signal shifts in health. While the complexities of disease at the individual level have made it difficult to utilize healthcare information in clinical decision-making, some of the existing constraints have been greatly minimized by technological advancements. To implement effective precision medicine with enhanced ability to positively impact patient outcomes and provide real-time decision support, it is important to harness the power of electronic health records by integrating disparate data sources and discovering patient-specific patterns of disease progression. Useful analytic tools, technologies, databases, and approaches are required to augment networking and interoperability of clinical, laboratory and public health systems, as well as addressing ethical and social issues related to the privacy and protection of healthcare data with effective balance. Developing multifunctional machine learning platforms for clinical data extraction, aggregation, management and analysis can support clinicians by efficiently stratifying subjects to understand specific scenarios and optimize decision-making. Implementation of artificial intelligence in healthcare is a compelling vision that has the potential in leading to the significant improvements for achieving the goals of providing real-time, better personalized and population medicine at lower costs. In this study, we focused on analyzing and discussing various published artificial intelligence and machine learning solutions, approaches and perspectives, aiming to advance academic solutions in paving the way for a new data-centric era of discovery in healthcare.",
        "DOI": "10.1093/database/baaa010",
        "paper_author": "Ahmed Z.",
        "affiliation_name": "Institute for Health, Health Care Policy and Aging Research",
        "affiliation_city": "New Brunswick",
        "affiliation_country": "United States",
        "affiliation_id": "60119172",
        "affiliation_state": "NJ"
    },
    {
        "paper_title": "Artificial intelligence in cancer diagnosis and prognosis: Opportunities and challenges",
        "publication": "Cancer Letters",
        "citied_by": "403",
        "cover_date": "2020-02-28",
        "Abstract": "Cancer is an aggressive disease with a low median survival rate. Ironically, the treatment process is long and very costly due to its high recurrence and mortality rates. Accurate early diagnosis and prognosis prediction of cancer are essential to enhance the patient's survival rate. Developments in statistics and computer engineering over the years have encouraged many scientists to apply computational methods such as multivariate statistical analysis to analyze the prognosis of the disease, and the accuracy of such analyses is significantly higher than that of empirical predictions. Furthermore, as artificial intelligence (AI), especially machine learning and deep learning, has found popular applications in clinical cancer research in recent years, cancer prediction performance has reached new heights. This article reviews the literature on the application of AI to cancer diagnosis and prognosis, and summarizes its advantages. We explore how AI assists cancer diagnosis and prognosis, specifically with regard to its unprecedented accuracy, which is even higher than that of general statistical applications in oncology. We also demonstrate ways in which these methods are advancing the field. Finally, opportunities and challenges in the clinical implementation of AI are discussed. Hence, this article provides a new perspective on how AI technology can help improve cancer diagnosis and prognosis, and continue improving human health in the future.",
        "DOI": "10.1016/j.canlet.2019.12.007",
        "paper_author": "Huang S.",
        "affiliation_name": "Faculty of Health Sciences",
        "affiliation_city": "Taipa",
        "affiliation_country": "Macao",
        "affiliation_id": "60202582",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Artificial intelligence and machine learning to fight covid-19",
        "publication": "Physiological Genomics",
        "citied_by": "390",
        "cover_date": "2020-04-01",
        "Abstract": "NA",
        "DOI": "10.1152/physiolgenomics.00029.2020",
        "paper_author": "Alimadadi A.",
        "affiliation_name": "College of Medicine and Life Sciences",
        "affiliation_city": "Toledo",
        "affiliation_country": "United States",
        "affiliation_id": "60016240",
        "affiliation_state": "OH"
    },
    {
        "paper_title": "Artificial intelligence in COVID-19 drug repurposing",
        "publication": "The Lancet Digital Health",
        "citied_by": "384",
        "cover_date": "2020-12-01",
        "Abstract": "Drug repurposing or repositioning is a technique whereby existing drugs are used to treat emerging and challenging diseases, including COVID-19. Drug repurposing has become a promising approach because of the opportunity for reduced development timelines and overall costs. In the big data era, artificial intelligence (AI) and network medicine offer cutting-edge application of information science to defining disease, medicine, therapeutics, and identifying targets with the least error. In this Review, we introduce guidelines on how to use AI for accelerating drug repurposing or repositioning, for which AI approaches are not just formidable but are also necessary. We discuss how to use AI models in precision medicine, and as an example, how AI models can accelerate COVID-19 drug repurposing. Rapidly developing, powerful, and innovative AI and network medicine technologies can expedite therapeutic development. This Review provides a strong rationale for using AI-based assistive tools for drug repurposing medications for human disease, including during the COVID-19 pandemic.",
        "DOI": "10.1016/S2589-7500(20)30192-8",
        "paper_author": "Zhou Y.",
        "affiliation_name": "Cleveland Clinic Foundation",
        "affiliation_city": "Cleveland",
        "affiliation_country": "United States",
        "affiliation_id": "60021160",
        "affiliation_state": "OH"
    },
    {
        "paper_title": "Machine Learning and Artificial Intelligence: Definitions, Applications, and Future Directions",
        "publication": "Current Reviews in Musculoskeletal Medicine",
        "citied_by": "384",
        "cover_date": "2020-02-01",
        "Abstract": "Purpose of Review: With the unprecedented advancement of data aggregation and deep learning algorithms, artificial intelligence (AI) and machine learning (ML) are poised to transform the practice of medicine. The field of orthopedics, in particular, is uniquely suited to harness the power of big data, and in doing so provide critical insight into elevating the many facets of care provided by orthopedic surgeons. The purpose of this review is to critically evaluate the recent and novel literature regarding ML in the field of orthopedics and to address its potential impact on the future of musculoskeletal care. Recent Findings: Recent literature demonstrates that the incorporation of ML into orthopedics has the potential to elevate patient care through alternative patient-specific payment models, rapidly analyze imaging modalities, and remotely monitor patients. Summary: Just as the business of medicine was once considered outside the domain of the orthopedic surgeon, we report evidence that demonstrates these emerging applications of AI warrant ownership, leverage, and application by the orthopedic surgeon to better serve their patients and deliver optimal, value-based care.",
        "DOI": "10.1007/s12178-020-09600-8",
        "paper_author": "Helm J.M.",
        "affiliation_name": "Cleveland Clinic Foundation",
        "affiliation_city": "Cleveland",
        "affiliation_country": "United States",
        "affiliation_id": "60021160",
        "affiliation_state": "OH"
    },
    {
        "paper_title": "A research agenda for ageing in China in the 21st century (2nd edition): Focusing on basic and translational research, long-term care, policy and social networks",
        "publication": "Ageing Research Reviews",
        "citied_by": "383",
        "cover_date": "2020-12-01",
        "Abstract": "One of the key issues facing public healthcare is the global trend of an increasingly ageing society which continues to present policy makers and caregivers with formidable healthcare and socio-economic challenges. Ageing is the primary contributor to a broad spectrum of chronic disorders all associated with a lower quality of life in the elderly. In 2019, the Chinese population constituted 18 % of the world population, with 164.5 million Chinese citizens aged 65 and above (65+), and 26 million aged 80 or above (80+). China has become an ageing society, and as it continues to age it will continue to exacerbate the burden borne by current family and public healthcare systems. Major healthcare challenges involved with caring for the elderly in China include the management of chronic non-communicable diseases (CNCDs), physical frailty, neurodegenerative diseases, cardiovascular diseases, with emerging challenges such as providing sufficient dental care, combating the rising prevalence of sexually transmitted diseases among nursing home communities, providing support for increased incidences of immune diseases, and the growing necessity to provide palliative care for the elderly. At the governmental level, it is necessary to make long-term strategic plans to respond to the pressures of an ageing society, especially to establish a nationwide, affordable, annual health check system to facilitate early diagnosis and provide access to affordable treatments. China has begun work on several activities to address these issues including the recent completion of the of the Ten-year Health-Care Reform project, the implementation of the Healthy China 2030 Action Plan, and the opening of the National Clinical Research Center for Geriatric Disorders. There are also societal challenges, namely the shift from an extended family system in which the younger provide home care for their elderly family members, to the current trend in which young people are increasingly migrating towards major cities for work, increasing reliance on nursing homes to compensate, especially following the outcomes of the ‘one child policy’ and the ‘empty-nest elderly’ phenomenon. At the individual level, it is important to provide avenues for people to seek and improve their own knowledge of health and disease, to encourage them to seek medical check-ups to prevent/manage illness, and to find ways to promote modifiable health-related behaviors (social activity, exercise, healthy diets, reasonable diet supplements) to enable healthier, happier, longer, and more productive lives in the elderly. Finally, at the technological or treatment level, there is a focus on modern technologies to counteract the negative effects of ageing. Researchers are striving to produce drugs that can mimic the effects of ‘exercising more, eating less’, while other anti-ageing molecules from molecular gerontologists could help to improve ‘healthspan’ in the elderly. Machine learning, ‘Big Data’, and other novel technologies can also be used to monitor disease patterns at the population level and may be used to inform policy design in the future. Collectively, synergies across disciplines on policies, geriatric care, drug development, personal awareness, the use of big data, machine learning and personalized medicine will transform China into a country that enables the most for its elderly, maximizing and celebrating their longevity in the coming decades. This is the 2nd edition of the review paper (Fang EF et al., Ageing Re. Rev. 2015).",
        "DOI": "10.1016/j.arr.2020.101174",
        "paper_author": "Fang E.F.",
        "affiliation_name": "Akershus University Hospital",
        "affiliation_city": "Lorenskog",
        "affiliation_country": "Norway",
        "affiliation_id": "60068728",
        "affiliation_state": "Viken"
    },
    {
        "paper_title": "Genomic and drug target evaluation of 90 cardiovascular proteins in 30,931 individuals",
        "publication": "Nature Metabolism",
        "citied_by": "378",
        "cover_date": "2020-10-01",
        "Abstract": "Circulating proteins are vital in human health and disease and are frequently used as biomarkers for clinical decision-making or as targets for pharmacological intervention. Here, we map and replicate protein quantitative trait loci (pQTL) for 90 cardiovascular proteins in over 30,000 individuals, resulting in 451 pQTLs for 85 proteins. For each protein, we further perform pathway mapping to obtain trans-pQTL gene and regulatory designations. We substantiate these regulatory findings with orthogonal evidence for trans-pQTLs using mouse knockdown experiments (ABCA1 and TRIB1) and clinical trial results (chemokine receptors CCR2 and CCR5), with consistent regulation. Finally, we evaluate known drug targets, and suggest new target candidates or repositioning opportunities using Mendelian randomization. This identifies 11 proteins with causal evidence of involvement in human disease that have not previously been targeted, including EGF, IL-16, PAPPA, SPON1, F3, ADM, CASP-8, CHI3L1, CXCL16, GDF15 and MMP-12. Taken together, these findings demonstrate the utility of large-scale mapping of the genetics of the proteome and provide a resource for future precision studies of circulating proteins in human health.",
        "DOI": "10.1038/s42255-020-00287-2",
        "paper_author": "Folkersen L.",
        "affiliation_name": "Karolinska Institutet",
        "affiliation_city": "Stockholm",
        "affiliation_country": "Sweden",
        "affiliation_id": "60012311",
        "affiliation_state": "Stockholms"
    },
    {
        "paper_title": "Deep learning for tomographic image reconstruction",
        "publication": "Nature Machine Intelligence",
        "citied_by": "358",
        "cover_date": "2020-12-01",
        "Abstract": "Deep-learning-based tomographic imaging is an important application of artificial intelligence and a new frontier of machine learning. Deep learning has been widely used in computer vision and image analysis, which deal with existing images, improve these images, and produce features from them. Since 2016, deep learning techniques have been actively researched for tomographic imaging, especially in the context of biomedicine, with impressive results and great potential. Tomographic reconstruction produces images of multi-dimensional structures from externally measured ‘encoded’ data in the form of various tomographic transforms (integrals, harmonics, echoes and so on). In this Review, we provide a general background, highlight representative results with an emphasis on medical imaging, and discuss key issues that need to be addressed in this emerging field. In particular, tomographic imaging is an integral part of modern medicine, and will play a key role in personalized, preventive and precision medicine and make it intelligent, inexpensive and indiscriminate.",
        "DOI": "10.1038/s42256-020-00273-z",
        "paper_author": "Wang G.",
        "affiliation_name": "Rensselaer Polytechnic Institute",
        "affiliation_city": "Troy",
        "affiliation_country": "United States",
        "affiliation_id": "60025534",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "The importance of interpretability and visualization in machine learning for applications in medicine and health care",
        "publication": "Neural Computing and Applications",
        "citied_by": "358",
        "cover_date": "2020-12-01",
        "Abstract": "In a short period of time, many areas of science have made a sharp transition towards data-dependent methods. In some cases, this process has been enabled by simultaneous advances in data acquisition and the development of networked system technologies. This new situation is particularly clear in the life sciences, where data overabundance has sparked a flurry of new methodologies for data management and analysis. This can be seen as a perfect scenario for the use of machine learning and computational intelligence techniques to address problems in which more traditional data analysis approaches might struggle. But, this scenario also poses some serious challenges. One of them is model interpretability and explainability, especially for complex nonlinear models. In some areas such as medicine and health care, not addressing such challenge might seriously limit the chances of adoption, in real practice, of computer-based systems that rely on machine learning and computational intelligence methods for data analysis. In this paper, we reflect on recent investigations about the interpretability and explainability of machine learning methods and discuss their impact on medicine and health care. We pay specific attention to one of the ways in which interpretability and explainability in this context can be addressed, which is through data and model visualization. We argue that, beyond improving model interpretability as a goal in itself, we need to integrate the medical experts in the design of data analysis interpretation strategies. Otherwise, machine learning is unlikely to become a part of routine clinical and health care practice.",
        "DOI": "10.1007/s00521-019-04051-w",
        "paper_author": "Vellido A.",
        "affiliation_name": "Universitat Politècnica de Catalunya",
        "affiliation_city": "Barcelona",
        "affiliation_country": "Spain",
        "affiliation_id": "60007592",
        "affiliation_state": "Barcelona"
    },
    {
        "paper_title": "How Machine Learning Will Transform Biomedicine",
        "publication": "Cell",
        "citied_by": "347",
        "cover_date": "2020-04-02",
        "Abstract": "This Perspective explores the application of machine learning toward improved diagnosis and treatment. We outline a vision for how machine learning can transform three broad areas of biomedicine: clinical diagnostics, precision treatments, and health monitoring, where the goal is to maintain health through a range of diseases and the normal aging process. For each area, early instances of successful machine learning applications are discussed, as well as opportunities and challenges for machine learning. When these challenges are met, machine learning promises a future of rigorous, outcomes-based medicine with detection, diagnosis, and treatment strategies that are continuously adapted to individual and environmental differences.",
        "DOI": "10.1016/j.cell.2020.03.022",
        "paper_author": "Goecks J.",
        "affiliation_name": "Department of Biomedical Engineering",
        "affiliation_city": "Portland",
        "affiliation_country": "United States",
        "affiliation_id": "60013512",
        "affiliation_state": "OR"
    },
    {
        "paper_title": "Application of Artificial Intelligence to Gastroenterology and Hepatology",
        "publication": "Gastroenterology",
        "citied_by": "347",
        "cover_date": "2020-01-01",
        "Abstract": "Since 2010, substantial progress has been made in artificial intelligence (AI) and its application to medicine. AI is explored in gastroenterology for endoscopic analysis of lesions, in detection of cancer, and to facilitate the analysis of inflammatory lesions or gastrointestinal bleeding during wireless capsule endoscopy. AI is also tested to assess liver fibrosis and to differentiate patients with pancreatic cancer from those with pancreatitis. AI might also be used to establish prognoses of patients or predict their response to treatments, based on multiple factors. We review the ways in which AI may help physicians make a diagnosis or establish a prognosis and discuss its limitations, knowing that further randomized controlled studies will be required before the approval of AI techniques by the health authorities.",
        "DOI": "10.1053/j.gastro.2019.08.058",
        "paper_author": "Le Berre C.",
        "affiliation_name": "CHU de Nantes",
        "affiliation_city": "Nantes",
        "affiliation_country": "France",
        "affiliation_id": "60028048",
        "affiliation_state": "Pays-de-la-Loire"
    },
    {
        "paper_title": "AI in Medical Imaging Informatics: Current Challenges and Future Directions",
        "publication": "IEEE Journal of Biomedical and Health Informatics",
        "citied_by": "342",
        "cover_date": "2020-07-01",
        "Abstract": "This paper reviews state-of-the-art research solutions across the spectrum of medical imaging informatics, discusses clinical translation, and provides future directions for advancing clinical practice. More specifically, it summarizes advances in medical imaging acquisition technologies for different modalities, highlighting the necessity for efficient medical data management strategies in the context of AI in big healthcare data analytics. It then provides a synopsis of contemporary and emerging algorithmic methods for disease classification and organ/ tissue segmentation, focusing on AI and deep learning architectures that have already become the de facto approach. The clinical benefits of in-silico modelling advances linked with evolving 3D reconstruction and visualization applications are further documented. Concluding, integrative analytics approaches driven by associate research branches highlighted in this study promise to revolutionize imaging informatics as known today across the healthcare continuum for both radiology and digital pathology applications. The latter, is projected to enable informed, more accurate diagnosis, timely prognosis, and effective treatment planning, underpinning precision medicine.",
        "DOI": "10.1109/JBHI.2020.2991043",
        "paper_author": "Panayides A.S.",
        "affiliation_name": "University of Cyprus",
        "affiliation_city": "Nicosia",
        "affiliation_country": "Cyprus",
        "affiliation_id": "60071343",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "The Gut Microbiota in Prediabetes and Diabetes: A Population-Based Cross-Sectional Study",
        "publication": "Cell Metabolism",
        "citied_by": "310",
        "cover_date": "2020-09-01",
        "Abstract": "The link between the gut microbiota and type 2 diabetes (T2D) warrants further investigation because of known confounding effects from antidiabetic treatment. Here, we profiled the gut microbiota in a discovery (n = 1,011) and validation (n = 484) cohort comprising Swedish subjects naive for diabetes treatment and grouped by glycemic status. We observed that overall gut microbiota composition was altered in groups with impaired glucose tolerance, combined glucose intolerance and T2D, but not in those with impaired fasting glucose. In addition, the abundance of several butyrate producers and functional potential for butyrate production were decreased both in prediabetes and T2D groups. Multivariate analyses and machine learning microbiome models indicated that insulin resistance was strongly associated with microbial variations. Therefore, our study indicates that the gut microbiota represents an important modifiable factor to consider when developing precision medicine approaches for the prevention and/or delay of T2D.",
        "DOI": "10.1016/j.cmet.2020.06.011",
        "paper_author": "Wu H.",
        "affiliation_name": "Wallenberglaboratoriet",
        "affiliation_city": "Gothenburg",
        "affiliation_country": "Sweden",
        "affiliation_id": "60046006",
        "affiliation_state": "Vastra Gotaland"
    },
    {
        "paper_title": "Deep learning interpretation of echocardiograms",
        "publication": "npj Digital Medicine",
        "citied_by": "309",
        "cover_date": "2020-12-01",
        "Abstract": "Echocardiography uses ultrasound technology to capture high temporal and spatial resolution images of the heart and surrounding structures, and is the most common imaging modality in cardiovascular medicine. Using convolutional neural networks on a large new dataset, we show that deep learning applied to echocardiography can identify local cardiac structures, estimate cardiac function, and predict systemic phenotypes that modify cardiovascular risk but not readily identifiable to human interpretation. Our deep learning model, EchoNet, accurately identified the presence of pacemaker leads (AUC = 0.89), enlarged left atrium (AUC = 0.86), left ventricular hypertrophy (AUC = 0.75), left ventricular end systolic and diastolic volumes (R2 = 0.74 and R2 = 0.70), and ejection fraction (R2 = 0.50), as well as predicted systemic phenotypes of age (R2 = 0.46), sex (AUC = 0.88), weight (R2 = 0.56), and height (R2 = 0.33). Interpretation analysis validates that EchoNet shows appropriate attention to key cardiac structures when performing human-explainable tasks and highlights hypothesis-generating regions of interest when predicting systemic phenotypes difficult for human interpretation. Machine learning on echocardiography images can streamline repetitive tasks in the clinical workflow, provide preliminary interpretation in areas with insufficient qualified cardiologists, and predict phenotypes challenging for human evaluation.",
        "DOI": "10.1038/s41746-019-0216-8",
        "paper_author": "Ghorbani A.",
        "affiliation_name": "Stanford Engineering",
        "affiliation_city": "Stanford",
        "affiliation_country": "United States",
        "affiliation_id": "60141508",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Machine learning for clinical decision support in infectious diseases: a narrative review of current applications",
        "publication": "Clinical Microbiology and Infection",
        "citied_by": "308",
        "cover_date": "2020-05-01",
        "Abstract": "Background: Machine learning (ML) is a growing field in medicine. This narrative review describes the current body of literature on ML for clinical decision support in infectious diseases (ID). Objectives: We aim to inform clinicians about the use of ML for diagnosis, classification, outcome prediction and antimicrobial management in ID. Sources: References for this review were identified through searches of MEDLINE/PubMed, EMBASE, Google Scholar, biorXiv, ACM Digital Library, arXiV and IEEE Xplore Digital Library up to July 2019. Content: We found 60 unique ML-clinical decision support systems (ML-CDSS) aiming to assist ID clinicians. Overall, 37 (62%) focused on bacterial infections, 10 (17%) on viral infections, nine (15%) on tuberculosis and four (7%) on any kind of infection. Among them, 20 (33%) addressed the diagnosis of infection, 18 (30%) the prediction, early detection or stratification of sepsis, 13 (22%) the prediction of treatment response, four (7%) the prediction of antibiotic resistance, three (5%) the choice of antibiotic regimen and two (3%) the choice of a combination antiretroviral therapy. The ML-CDSS were developed for intensive care units (n = 24, 40%), ID consultation (n = 15, 25%), medical or surgical wards (n = 13, 20%), emergency department (n = 4, 7%), primary care (n = 3, 5%) and antimicrobial stewardship (n = 1, 2%). Fifty-three ML-CDSS (88%) were developed using data from high-income countries and seven (12%) with data from low- and middle-income countries (LMIC). The evaluation of ML-CDSS was limited to measures of performance (e.g. sensitivity, specificity) for 57 ML-CDSS (95%) and included data in clinical practice for three (5%). Implications: Considering comprehensive patient data from socioeconomically diverse healthcare settings, including primary care and LMICs, may improve the ability of ML-CDSS to suggest decisions adapted to various clinical contexts. Currents gaps identified in the evaluation of ML-CDSS must also be addressed in order to know the potential impact of such tools for clinicians and patients.",
        "DOI": "10.1016/j.cmi.2019.09.009",
        "paper_author": "Peiffer-Smadja N.",
        "affiliation_name": "National Institute for Health and Care Research",
        "affiliation_city": "Royston",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60278571",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Automated assessment of psychiatric disorders using speech: A systematic review",
        "publication": "Laryngoscope Investigative Otolaryngology",
        "citied_by": "299",
        "cover_date": "2020-02-01",
        "Abstract": "Objective: There are many barriers to accessing mental health assessments including cost and stigma. Even when individuals receive professional care, assessments are intermittent and may be limited partly due to the episodic nature of psychiatric symptoms. Therefore, machine-learning technology using speech samples obtained in the clinic or remotely could one day be a biomarker to improve diagnosis and treatment. To date, reviews have only focused on using acoustic features from speech to detect depression and schizophrenia. Here, we present the first systematic review of studies using speech for automated assessments across a broader range of psychiatric disorders. Methods: We followed the Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) guidelines. We included studies from the last 10 years using speech to identify the presence or severity of disorders within the Diagnostic and Statistical Manual of Mental Disorders (DSM-5). For each study, we describe sample size, clinical evaluation method, speech-eliciting tasks, machine learning methodology, performance, and other relevant findings. Results: 1395 studies were screened of which 127 studies met the inclusion criteria. The majority of studies were on depression, schizophrenia, and bipolar disorder, and the remaining on post-traumatic stress disorder, anxiety disorders, and eating disorders. 63% of studies built machine learning predictive models, and the remaining 37% performed null-hypothesis testing only. We provide an online database with our search results and synthesize how acoustic features appear in each disorder. Conclusion: Speech processing technology could aid mental health assessments, but there are many obstacles to overcome, especially the need for comprehensive transdiagnostic and longitudinal studies. Given the diverse types of data sets, feature extraction, computational methodologies, and evaluation criteria, we provide guidelines for both acquiring data and building machine learning models with a focus on testing hypotheses, open science, reproducibility, and generalizability. Level of Evidence: 3a.",
        "DOI": "10.1002/lio2.354",
        "paper_author": "Low D.M.",
        "affiliation_name": "Harvard Medical School",
        "affiliation_city": "Boston",
        "affiliation_country": "United States",
        "affiliation_id": "60002746",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "BEHRT: Transformer for Electronic Health Records",
        "publication": "Scientific Reports",
        "citied_by": "299",
        "cover_date": "2020-12-01",
        "Abstract": "Today, despite decades of developments in medicine and the growing interest in precision healthcare, vast majority of diagnoses happen once patients begin to show noticeable signs of illness. Early indication and detection of diseases, however, can provide patients and carers with the chance of early intervention, better disease management, and efficient allocation of healthcare resources. The latest developments in machine learning (including deep learning) provides a great opportunity to address this unmet need. In this study, we introduce BEHRT: A deep neural sequence transduction model for electronic health records (EHR), capable of simultaneously predicting the likelihood of 301 conditions in one’s future visits. When trained and evaluated on the data from nearly 1.6 million individuals, BEHRT shows a striking improvement of 8.0–13.2% (in terms of average precision scores for different tasks), over the existing state-of-the-art deep EHR models. In addition to its scalability and superior accuracy, BEHRT enables personalised interpretation of its predictions; its flexible architecture enables it to incorporate multiple heterogeneous concepts (e.g., diagnosis, medication, measurements, and more) to further improve the accuracy of its predictions; its (pre-)training results in disease and patient representations can be useful for future studies (i.e., transfer learning).",
        "DOI": "10.1038/s41598-020-62922-y",
        "paper_author": "Li Y.",
        "affiliation_name": "Oxford Social Sciences Division",
        "affiliation_city": "Oxford",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60175816",
        "affiliation_state": "Oxfordshire"
    },
    {
        "paper_title": "DSTP-RNN: A dual-stage two-phase attention-based recurrent neural network for long-term and multivariate time series prediction",
        "publication": "Expert Systems with Applications",
        "citied_by": "290",
        "cover_date": "2020-04-01",
        "Abstract": "Long-term prediction of multivariate time series is still an important but challenging problem. The key to solve this problem is capturing (1) the spatial correlations at the same time, (2) the spatio-temporal relationships at different times, and (3) long-term dependency of the temporal relationships between different series. Attention-based recurrent neural networks (RNN) can effectively represent and learn the dynamic spatio-temporal relationships between exogenous series and target series, but they only perform well in one-step time prediction and short-term time prediction. In this paper, inspired by human attention mechanism including the dual-stage two-phase (DSTP) model and the influence mechanism of target information and non-target information, we propose DSTP-based RNN (DSTP-RNN) and DSTP-RNN-Ⅱ respectively for long-term time series prediction. Specifically, we first propose the DSTP-based structure to enhance the spatial correlations between exogenous series. The first phase produces violent but decentralized response weight, while the second phase leads to stationary and concentrated response weight. Then, we employ multiple attentions on target series to boost the long-term dependency. Finally, we study the performance of deep spatial attention mechanism and provide interpretation. Experimental results demonstrate that the present work can be successfully used to develop expert or intelligent systems for a wide range of applications, with state-of-the-art performances superior to nine baseline methods on four datasets in the fields of energy, finance, environment and medicine, respectively. Overall, the present work carries a significant value not merely in the domain of machine intelligence and deep learning, but also in the fields of many applications.",
        "DOI": "10.1016/j.eswa.2019.113082",
        "paper_author": "Liu Y.",
        "affiliation_name": "China Agricultural University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60013551",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Comparison of conventional statistical methods with machine learning in medicine: Diagnosis, drug development, and treatment",
        "publication": "Medicina (Lithuania)",
        "citied_by": "289",
        "cover_date": "2020-09-01",
        "Abstract": "Futurists have anticipated that novel autonomous technologies, embedded with machine learning (ML), will substantially influence healthcare. ML is focused on making predictions as accurate as possible, while traditional statistical models are aimed at inferring relationships between variables. The benefits of ML comprise flexibility and scalability compared with conventional statistical approaches, which makes it deployable for several tasks, such as diagnosis and classification, and survival predictions. However, much of ML-based analysis remains scattered, lacking a cohesive structure. There is a need to evaluate and compare the performance of well-developed conventional statistical methods and ML on patient outcomes, such as survival, response to treatment, and patient-reported outcomes (PROs). In this article, we compare the usefulness and limitations of traditional statistical methods and ML, when applied to the medical field. Traditional statistical methods seem to be more useful when the number of cases largely exceeds the number of variables under study and a priori knowledge on the topic under study is substantial such as in public health. ML could be more suited in highly innovative fields with a huge bulk of data, such as omics, radiodiagnostics, drug development, and personalized treatment. Integration of the two approaches should be preferred over a unidirectional choice of either approach.",
        "DOI": "10.3390/medicina56090455",
        "paper_author": "Rajula H.S.R.",
        "affiliation_name": "Università degli Studi di Cagliari",
        "affiliation_city": "Cagliari",
        "affiliation_country": "Italy",
        "affiliation_id": "60032259",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "A 12-lead electrocardiogram database for arrhythmia research covering more than 10,000 patients",
        "publication": "Scientific Data",
        "citied_by": "288",
        "cover_date": "2020-12-01",
        "Abstract": "This newly inaugurated research database for 12-lead electrocardiogram signals was created under the auspices of Chapman University and Shaoxing People’s Hospital (Shaoxing Hospital Zhejiang University School of Medicine) and aims to enable the scientific community in conducting new studies on arrhythmia and other cardiovascular conditions. Certain types of arrhythmias, such as atrial fibrillation, have a pronounced negative impact on public health, quality of life, and medical expenditures. As a non-invasive test, long term ECG monitoring is a major and vital diagnostic tool for detecting these conditions. This practice, however, generates large amounts of data, the analysis of which requires considerable time and effort by human experts. Advancement of modern machine learning and statistical tools can be trained on high quality, large data to achieve exceptional levels of automated diagnostic accuracy. Thus, we collected and disseminated this novel database that contains 12-lead ECGs of 10,646 patients with a 500 Hz sampling rate that features 11 common rhythms and 67 additional cardiovascular conditions, all labeled by professional experts. The dataset consists of 10-second, 12-dimension ECGs and labels for rhythms and other conditions for each subject. The dataset can be used to design, compare, and fine-tune new and classical statistical and machine learning techniques in studies focused on arrhythmia and other cardiovascular conditions.",
        "DOI": "10.1038/s41597-020-0386-x",
        "paper_author": "Zheng J.",
        "affiliation_name": "Chapman University",
        "affiliation_city": "Orange",
        "affiliation_country": "United States",
        "affiliation_id": "60016569",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Artificial intelligence as the next step towards precision pathology",
        "publication": "Journal of Internal Medicine",
        "citied_by": "277",
        "cover_date": "2020-07-01",
        "Abstract": "Pathology is the cornerstone of cancer care. The need for accuracy in histopathologic diagnosis of cancer is increasing as personalized cancer therapy requires accurate biomarker assessment. The appearance of digital image analysis holds promise to improve both the volume and precision of histomorphological evaluation. Recently, machine learning, and particularly deep learning, has enabled rapid advances in computational pathology. The integration of machine learning into routine care will be a milestone for the healthcare sector in the next decade, and histopathology is right at the centre of this revolution. Examples of potential high-value machine learning applications include both model-based assessment of routine diagnostic features in pathology, and the ability to extract and identify novel features that provide insights into a disease. Recent groundbreaking results have demonstrated that applications of machine learning methods in pathology significantly improves metastases detection in lymph nodes, Ki67 scoring in breast cancer, Gleason grading in prostate cancer and tumour-infiltrating lymphocyte (TIL) scoring in melanoma. Furthermore, deep learning models have also been demonstrated to be able to predict status of some molecular markers in lung, prostate, gastric and colorectal cancer based on standard HE slides. Moreover, prognostic (survival outcomes) deep neural network models based on digitized HE slides have been demonstrated in several diseases, including lung cancer, melanoma and glioma. In this review, we aim to present and summarize the latest developments in digital image analysis and in the application of artificial intelligence in diagnostic pathology.",
        "DOI": "10.1111/joim.13030",
        "paper_author": "Acs B.",
        "affiliation_name": "Karolinska Institutet",
        "affiliation_city": "Stockholm",
        "affiliation_country": "Sweden",
        "affiliation_id": "60012311",
        "affiliation_state": "Stockholms"
    },
    {
        "paper_title": "Big data and artificial intelligence modeling for drug discovery",
        "publication": "Annual Review of Pharmacology and Toxicology",
        "citied_by": "277",
        "cover_date": "2020-01-06",
        "Abstract": "Due to the massive data sets available for drug candidates, modern drug discovery has advanced to the big data era. Central to this shift is the development of artificial intelligence approaches to implementing innovative modeling based on the dynamic, heterogeneous, and large nature of drug data sets. As a result, recently developed artificial intelligence approaches such as deep learning and relevant modeling studies provide new solutions to efficacy and safety evaluations of drug candidates based on big data modeling and analysis. The resulting models provided deep insights into the continuum from chemical structure to in vitro, in vivo, and clinical outcomes. The relevant novel data mining, curation, and management techniques provided critical support to recent modeling studies. In summary, the new advancement of artificial intelligence in the big data era has paved the road to future rational drug development and optimization, which will have a significant impact on drug discovery procedures and, eventually, public health.",
        "DOI": "10.1146/annurev-pharmtox-010919-023324",
        "paper_author": "Zhu H.",
        "affiliation_name": "Rutgers University–Camden",
        "affiliation_city": "Camden",
        "affiliation_country": "United States",
        "affiliation_id": "60023184",
        "affiliation_state": "NJ"
    },
    {
        "paper_title": "Gut Microbiome Fermentation Determines the Efficacy of Exercise for Diabetes Prevention",
        "publication": "Cell Metabolism",
        "citied_by": "276",
        "cover_date": "2020-01-07",
        "Abstract": "Exercise is an effective strategy for diabetes management but is limited by the phenomenon of exercise resistance (i.e., the lack of or the adverse response to exercise on metabolic health). Here, in 39 medication-naive men with prediabetes, we found that exercise-induced alterations in the gut microbiota correlated closely with improvements in glucose homeostasis and insulin sensitivity (clinicaltrials.gov entry NCT03240978). The microbiome of responders exhibited an enhanced capacity for biosynthesis of short-chain fatty acids and catabolism of branched-chain amino acids, whereas those of non-responders were characterized by increased production of metabolically detrimental compounds. Fecal microbial transplantation from responders, but not non-responders, mimicked the effects of exercise on alleviation of insulin resistance in obese mice. Furthermore, a machine-learning algorithm integrating baseline microbial signatures accurately predicted personalized glycemic response to exercise in an additional 30 subjects. These findings raise the possibility of maximizing the benefits of exercise by targeting the gut microbiota. Liu et al. identify the gut microbiota as an important determinant in the responsiveness of individuals with prediabetes to exercise for the improvement of glucose metabolism and insulin sensitivity. These findings may help in the implementation of a personalized lifestyle intervention for diabetes prevention.",
        "DOI": "10.1016/j.cmet.2019.11.001",
        "paper_author": "Liu Y.",
        "affiliation_name": "The University of Hong Kong, State Key Laboratory of Pharmaceutical Biotechnology",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong",
        "affiliation_id": "60114497",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "FastMRI: A publicly available raw k-space and DICOM dataset of knee images for accelerated MR image reconstruction using machine learning",
        "publication": "Radiology: Artificial Intelligence",
        "citied_by": "273",
        "cover_date": "2020-01-01",
        "Abstract": "A publicly available dataset containing k-space and image data of knee examinations for accelerated MR image reconstruction using machine learning is presented.",
        "DOI": "10.1148/ryai.2020190007",
        "paper_author": "Knoll F.",
        "affiliation_name": "New York University",
        "affiliation_city": "New York",
        "affiliation_country": "United States",
        "affiliation_id": "60021784",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Electronic Nose and Its Applications: A Survey",
        "publication": "International Journal of Automation and Computing",
        "citied_by": "273",
        "cover_date": "2020-04-01",
        "Abstract": "In the last two decades, improvements in materials, sensors and machine learning technologies have led to a rapid extension of electronic nose (EN) related research topics with diverse applications. The food and beverage industry, agriculture and forestry, medicine and health-care, indoor and outdoor monitoring, military and civilian security systems are the leading fields which take great advantage from the rapidity, stability, portability and compactness of ENs. Although the EN technology provides numerous benefits, further enhancements in both hardware and software components are necessary for utilizing ENs in practice. This paper provides an extensive survey of the EN technology and its wide range of application fields, through a comprehensive analysis of algorithms proposed in the literature, while exploiting related domains with possible future suggestions for this research topic.",
        "DOI": "10.1007/s11633-019-1212-9",
        "paper_author": "Karakaya D.",
        "affiliation_name": "Izmir Ekonomi Üniversitesi",
        "affiliation_city": "Izmir",
        "affiliation_country": "Turkey",
        "affiliation_id": "60020241",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A review of modern technologies for tackling COVID-19 pandemic",
        "publication": "Diabetes and Metabolic Syndrome: Clinical Research and Reviews",
        "citied_by": "266",
        "cover_date": "2020-07-01",
        "Abstract": "Objective: Science and technology sector constituting of data science, machine learning and artificial intelligence are contributing towards COVID-19. The aim of the present study is to discuss the various aspects of modern technology used to fight against COVID-19 crisis at different scales, including medical image processing, disease tracking, prediction outcomes, computational biology and medicines. Methods: A progressive search of the database related to modern technology towards COVID-19 is made. Further, a brief review is done on the extracted information by assessing the various aspects of modern technologies for tackling COVID-19 pandemic. Results: We provide a window of thoughts on review of the technology advances used to decrease and smother the substantial impact of the outburst. Though different studies relating to modern technology towards COVID-19 have come up, yet there are still constrained applications and contributions of technology in this fight. Conclusions: On-going progress in the modern technology has contributed in improving people's lives and hence there is a solid conviction that validated research plans including artificial intelligence will be of significant advantage in helping people to fight this infection.",
        "DOI": "10.1016/j.dsx.2020.05.008",
        "paper_author": "Kumar A.",
        "affiliation_name": "The ICFAI University, Dehradun",
        "affiliation_city": "Dehradun",
        "affiliation_country": "India",
        "affiliation_id": "60194940",
        "affiliation_state": "UK"
    },
    {
        "paper_title": "Predicting Drug Response and Synergy Using a Deep Learning Model of Human Cancer Cells",
        "publication": "Cancer Cell",
        "citied_by": "250",
        "cover_date": "2020-11-09",
        "Abstract": "Most drugs entering clinical trials fail, often related to an incomplete understanding of the mechanisms governing drug response. Machine learning techniques hold immense promise for better drug response predictions, but most have not reached clinical practice due to their lack of interpretability and their focus on monotherapies. We address these challenges by developing DrugCell, an interpretable deep learning model of human cancer cells trained on the responses of 1,235 tumor cell lines to 684 drugs. Tumor genotypes induce states in cellular subsystems that are integrated with drug structure to predict response to therapy and, simultaneously, learn biological mechanisms underlying the drug response. DrugCell predictions are accurate in cell lines and also stratify clinical outcomes. Analysis of DrugCell mechanisms leads directly to the design of synergistic drug combinations, which we validate systematically by combinatorial CRISPR, drug-drug screening in vitro, and patient-derived xenografts. DrugCell provides a blueprint for constructing interpretable models for predictive medicine.",
        "DOI": "10.1016/j.ccell.2020.09.014",
        "paper_author": "Kuenzi B.M.",
        "affiliation_name": "Department of Medicine",
        "affiliation_city": "La Jolla",
        "affiliation_country": "United States",
        "affiliation_id": "60121547",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Artificial intelligence biosensors: Challenges and prospects",
        "publication": "Biosensors and Bioelectronics",
        "citied_by": "235",
        "cover_date": "2020-10-01",
        "Abstract": "Artificial intelligence (AI) and wearable sensors are two essential fields to realize the goal of tailoring the best precision medicine treatment for individual patients. Integration of these two fields enables better acquisition of patient data and improved design of wearable sensors for monitoring the wearers' health, fitness and their surroundings. Currently, as the Internet of Things (IoT), big data and big health move from concept to implementation, AI-biosensors with appropriate technical characteristics are facing new opportunities and challenges. In this paper, the most advanced progress made in the key phases for future wearable and implantable technology from biosensing, wearable biosensing to AI-biosensing is summarized. Without a doubt, material innovation, biorecognition element, signal acquisition and transportation, data processing and intelligence decision system are the most important parts, which are the main focus of the discussion. The challenges and opportunities of AI-biosensors moving forward toward future medicine devices are also discussed.",
        "DOI": "10.1016/j.bios.2020.112412",
        "paper_author": "Jin X.",
        "affiliation_name": "Shenzhen University",
        "affiliation_city": "Shenzhen",
        "affiliation_country": "China",
        "affiliation_id": "60000937",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Enabling Technologies for Personalized and Precision Medicine",
        "publication": "Trends in Biotechnology",
        "citied_by": "233",
        "cover_date": "2020-05-01",
        "Abstract": "Individualizing patient treatment is a core objective of the medical field. Reaching this objective has been elusive owing to the complex set of factors contributing to both disease and health; many factors, from genes to proteins, remain unknown in their role in human physiology. Accurately diagnosing, monitoring, and treating disorders requires advances in biomarker discovery, the subsequent development of accurate signatures that correspond with dynamic disease states, as well as therapeutic interventions that can be continuously optimized and modulated for dose and drug selection. This work highlights key breakthroughs in the development of enabling technologies that further the goal of personalized and precision medicine, and remaining challenges that, when addressed, may forge unprecedented capabilities in realizing truly individualized patient care.",
        "DOI": "10.1016/j.tibtech.2019.12.021",
        "paper_author": "Ho D.",
        "affiliation_name": "National University of Singapore",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore",
        "affiliation_id": "60017161",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Causal inference and counterfactual prediction in machine learning for actionable healthcare",
        "publication": "Nature Machine Intelligence",
        "citied_by": "223",
        "cover_date": "2020-07-01",
        "Abstract": "Big data, high-performance computing, and (deep) machine learning are increasingly becoming key to precision medicine—from identifying disease risks and taking preventive measures, to making diagnoses and personalizing treatment for individuals. Precision medicine, however, is not only about predicting risks and outcomes, but also about weighing interventions. Interventional clinical predictive models require the correct specification of cause and effect, and the calculation of so-called counterfactuals, that is, alternative scenarios. In biomedical research, observational studies are commonly affected by confounding and selection bias. Without robust assumptions, often requiring a priori domain knowledge, causal inference is not feasible. Data-driven prediction models are often mistakenly used to draw causal effects, but neither their parameters nor their predictions necessarily have a causal interpretation. Therefore, the premise that data-driven prediction models lead to trustable decisions/interventions for precision medicine is questionable. When pursuing intervention modelling, the bio-health informatics community needs to employ causal approaches and learn causal structures. Here we discuss how target trials (algorithmic emulation of randomized studies), transportability (the licence to transfer causal effects from one population to another) and prediction invariance (where a true causal model is contained in the set of all prediction models whose accuracy does not vary across different settings) are linchpins to developing and testing intervention models.",
        "DOI": "10.1038/s42256-020-0197-y",
        "paper_author": "Prosperi M.",
        "affiliation_name": "University of Florida College of Medicine",
        "affiliation_city": "Gainesville",
        "affiliation_country": "United States",
        "affiliation_id": "60007567",
        "affiliation_state": "FL"
    },
    {
        "paper_title": "Measuring the Quality of Explanations: The System Causability Scale (SCS): Comparing Human and Machine Explanations",
        "publication": "KI - Kunstliche Intelligenz",
        "citied_by": "222",
        "cover_date": "2020-06-01",
        "Abstract": "Recent success in Artificial Intelligence (AI) and Machine Learning (ML) allow problem solving automatically without any human intervention. Autonomous approaches can be very convenient. However, in certain domains, e.g., in the medical domain, it is necessary to enable a domain expert to understand, why an algorithm came up with a certain result. Consequently, the field of Explainable AI (xAI) rapidly gained interest worldwide in various domains, particularly in medicine. Explainable AI studies transparency and traceability of opaque AI/ML and there are already a huge variety of methods. For example with layer-wise relevance propagation relevant parts of inputs to, and representations in, a neural network which caused a result, can be highlighted. This is a first important step to ensure that end users, e.g., medical professionals, assume responsibility for decision making with AI/ML and of interest to professionals and regulators. Interactive ML adds the component of human expertise to AI/ML processes by enabling them to re-enact and retrace AI/ML results, e.g. let them check it for plausibility. This requires new human–AI interfaces for explainable AI. In order to build effective and efficient interactive human–AI interfaces we have to deal with the question of how to evaluate the quality of explanations given by an explainable AI system. In this paper we introduce our System Causability Scale to measure the quality of explanations. It is based on our notion of Causability (Holzinger et al. in Wiley Interdiscip Rev Data Min Knowl Discov 9(4), 2019) combined with concepts adapted from a widely-accepted usability scale.",
        "DOI": "10.1007/s13218-020-00636-z",
        "paper_author": "Holzinger A.",
        "affiliation_name": "Medizinische Universität Graz",
        "affiliation_city": "Graz",
        "affiliation_country": "Austria",
        "affiliation_id": "60006224",
        "affiliation_state": "Styria"
    },
    {
        "paper_title": "A blockchain and machine learning-based drug supply chain management and recommendation system for smart pharmaceutical industry",
        "publication": "Electronics (Switzerland)",
        "citied_by": "222",
        "cover_date": "2020-05-01",
        "Abstract": "From the last decade, pharmaceutical companies are facing difficulties in tracking their products during the supply chain process, allowing the counterfeiters to add their fake medicines into the market. Counterfeit drugs are analyzed as a very big challenge for the pharmaceutical industry worldwide. As indicated by the statistics, yearly business loss of around $200 billion is reported by US pharmaceutical companies due to these counterfeit drugs. These drugs may not help the patients to recover the disease but have many other dangerous side effects. According to the World Health Organization (WHO) survey report, in under-developed countries every 10th drug use by the consumers is counterfeit and has low quality. Hence, a system that can trace and track drug delivery at every phase is needed to solve the counterfeiting problem. The blockchain has the full potential to handle and track the supply chain process very efficiently. In this paper, we have proposed and implemented a novel blockchain and machine learning-based drug supply chain management and recommendation system (DSCMR). Our proposed system consists of two main modules: blockchain-based drug supply chain management and machine learning-based drug recommendation system for consumers. In the first module, the drug supply chain management system is deployed using Hyperledger fabrics which is capable of continuously monitor and track the drug delivery process in the smart pharmaceutical industry. On the other hand, the N-gram, LightGBM models are used in the machine learning module to recommend the top-rated or best medicines to the customers of the pharmaceutical industry. These models have trained on well known publicly available drug reviews dataset provided by the UCI: an open-source machine learning repository. Moreover, the machine learning module is integrated with this blockchain system with the help of the REST API. Finally, we also perform several tests to check the efficiency and usability of our proposed system.",
        "DOI": "10.3390/electronics9050852",
        "paper_author": "Abbas K.",
        "affiliation_name": "Jeju National University",
        "affiliation_city": "Jeju",
        "affiliation_country": "South Korea",
        "affiliation_id": "60117634",
        "affiliation_state": "Jeju-do"
    },
    {
        "paper_title": "Overview of artificial intelligence-based applications in radiotherapy: Recommendations for implementation and quality assurance",
        "publication": "Radiotherapy and Oncology",
        "citied_by": "219",
        "cover_date": "2020-12-01",
        "Abstract": "Artificial Intelligence (AI) is currently being introduced into different domains, including medicine. Specifically in radiation oncology, machine learning models allow automation and optimization of the workflow. A lack of knowledge and interpretation of these AI models can hold back wide-spread and full deployment into clinical practice. To facilitate the integration of AI models in the radiotherapy workflow, generally applicable recommendations on implementation and quality assurance (QA) of AI models are presented. For commonly used applications in radiotherapy such as auto-segmentation, automated treatment planning and synthetic computed tomography (sCT) the basic concepts are discussed in depth. Emphasis is put on the commissioning, implementation and case-specific and routine QA of AI models needed for a methodical introduction in clinical practice.",
        "DOI": "10.1016/j.radonc.2020.09.008",
        "paper_author": "Vandewinckele L.",
        "affiliation_name": "KU Leuven",
        "affiliation_city": "Leuven",
        "affiliation_country": "Belgium",
        "affiliation_id": "60025063",
        "affiliation_state": "Vlaams-Brabant"
    },
    {
        "paper_title": "Gut microbiome, big data and machine learning to promote precision medicine for cancer",
        "publication": "Nature Reviews Gastroenterology and Hepatology",
        "citied_by": "214",
        "cover_date": "2020-10-01",
        "Abstract": "The gut microbiome has been implicated in cancer in several ways, as specific microbial signatures are known to promote cancer development and influence safety, tolerability and efficacy of therapies. The ‘omics’ technologies used for microbiome analysis continuously evolve and, although much of the research is still at an early stage, large-scale datasets of ever increasing size and complexity are being produced. However, there are varying levels of difficulty in realizing the full potential of these new tools, which limit our ability to critically analyse much of the available data. In this Perspective, we provide a brief overview on the role of gut microbiome in cancer and focus on the need, role and limitations of a machine learning-driven approach to analyse large amounts of complex health-care information in the era of big data. We also discuss the potential application of microbiome-based big data aimed at promoting precision medicine in cancer.",
        "DOI": "10.1038/s41575-020-0327-3",
        "paper_author": "Cammarota G.",
        "affiliation_name": "Fondazione Policlinico Universitario Agostino Gemelli IRCCS",
        "affiliation_city": "Rome",
        "affiliation_country": "Italy",
        "affiliation_id": "60073799",
        "affiliation_state": "Lazio"
    },
    {
        "paper_title": "Generation and evaluation of synthetic patient data",
        "publication": "BMC Medical Research Methodology",
        "citied_by": "214",
        "cover_date": "2020-05-07",
        "Abstract": "Background: Machine learning (ML) has made a significant impact in medicine and cancer research; however, its impact in these areas has been undeniably slower and more limited than in other application domains. A major reason for this has been the lack of availability of patient data to the broader ML research community, in large part due to patient privacy protection concerns. High-quality, realistic, synthetic datasets can be leveraged to accelerate methodological developments in medicine. By and large, medical data is high dimensional and often categorical. These characteristics pose multiple modeling challenges. Methods: In this paper, we evaluate three classes of synthetic data generation approaches; probabilistic models, classification-based imputation models, and generative adversarial neural networks. Metrics for evaluating the quality of the generated synthetic datasets are presented and discussed. Results: While the results and discussions are broadly applicable to medical data, for demonstration purposes we generate synthetic datasets for cancer based on the publicly available cancer registry data from the Surveillance Epidemiology and End Results (SEER) program. Specifically, our cohort consists of breast, respiratory, and non-solid cancer cases diagnosed between 2010 and 2015, which includes over 360,000 individual cases. Conclusions: We discuss the trade-offs of the different methods and metrics, providing guidance on considerations for the generation and usage of medical synthetic data.",
        "DOI": "10.1186/s12874-020-00977-1",
        "paper_author": "Goncalves A.",
        "affiliation_name": "Lawrence Livermore National Laboratory",
        "affiliation_city": "Livermore",
        "affiliation_country": "United States",
        "affiliation_id": "60026175",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Single cell transcriptomics comes of age",
        "publication": "Nature Communications",
        "citied_by": "213",
        "cover_date": "2020-12-01",
        "Abstract": "NA",
        "DOI": "10.1038/s41467-020-18158-5",
        "paper_author": "Aldridge S.",
        "affiliation_name": "Wellcome Sanger Institute",
        "affiliation_city": "Hinxton",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60005517",
        "affiliation_state": "Cambridgeshire"
    },
    {
        "paper_title": "Reinforcement learning for intelligent healthcare applications: A survey",
        "publication": "Artificial Intelligence in Medicine",
        "citied_by": "207",
        "cover_date": "2020-09-01",
        "Abstract": "Discovering new treatments and personalizing existing ones is one of the major goals of modern clinical research. In the last decade, Artificial Intelligence (AI) has enabled the realization of advanced intelligent systems able to learn about clinical treatments and discover new medical knowledge from the huge amount of data collected. Reinforcement Learning (RL), which is a branch of Machine Learning (ML), has received significant attention in the medical community since it has the potentiality to support the development of personalized treatments in accordance with the more general precision medicine vision. This report presents a review of the role of RL in healthcare by investigating past work, and highlighting any limitations and possible future contributions.",
        "DOI": "10.1016/j.artmed.2020.101964",
        "paper_author": "Coronato A.",
        "affiliation_name": "Consiglio Nazionale delle Ricerche",
        "affiliation_city": "Rome",
        "affiliation_country": "Italy",
        "affiliation_id": "60021199",
        "affiliation_state": "RM"
    },
    {
        "paper_title": "Explainable artificial intelligence models using real-world electronic health record data: A systematic scoping review",
        "publication": "Journal of the American Medical Informatics Association",
        "citied_by": "206",
        "cover_date": "2020-07-01",
        "Abstract": "Objective: To conduct a systematic scoping review of explainable artificial intelligence (XAI) models that use real-world electronic health record data, categorize these techniques according to different biomedical applications, identify gaps of current studies, and suggest future research directions. Materials and Methods: We searched MEDLINE, IEEE Xplore, and the Association for Computing Machinery (ACM) Digital Library to identify relevant papers published between January 1, 2009 and May 1, 2019. We summarized these studies based on the year of publication, prediction tasks, machine learning algorithm, dataset(s) used to build the models, the scope, category, and evaluation of the XAI methods. We further assessed the reproducibility of the studies in terms of the availability of data and code and discussed open issues and challenges. Results: Forty-two articles were included in this review. We reported the research trend and most-studied diseases. We grouped XAI methods into 5 categories: knowledge distillation and rule extraction (N = 13), intrinsically interpretable models (N = 9), data dimensionality reduction (N = 8), attention mechanism (N = 7), and feature interaction and importance (N = 5). Discussion: XAI evaluation is an open issue that requires a deeper focus in the case of medical applications. We also discuss the importance of reproducibility of research work in this field, as well as the challenges and opportunities of XAI from 2 medical professionals' point of view. Conclusion: Based on our review, we found that XAI evaluation in medicine has not been adequately and formally practiced. Reproducibility remains a critical concern. Ample opportunities exist to advance XAI research in medicine.",
        "DOI": "10.1093/jamia/ocaa053",
        "paper_author": "Payrovnaziri S.N.",
        "affiliation_name": "Florida State University",
        "affiliation_city": "Tallahassee",
        "affiliation_country": "United States",
        "affiliation_id": "60002092",
        "affiliation_state": "FL"
    },
    {
        "paper_title": "Artificial intelligence in oncology",
        "publication": "Cancer Science",
        "citied_by": "202",
        "cover_date": "2020-05-01",
        "Abstract": "Artificial intelligence (AI) has contributed substantially to the resolution of a variety of biomedical problems, including cancer, over the past decade. Deep learning, a subfield of AI that is highly flexible and supports automatic feature extraction, is increasingly being applied in various areas of both basic and clinical cancer research. In this review, we describe numerous recent examples of the application of AI in oncology, including cases in which deep learning has efficiently solved problems that were previously thought to be unsolvable, and we address obstacles that must be overcome before such application can become more widespread. We also highlight resources and datasets that can help harness the power of AI for cancer research. The development of innovative approaches to and applications of AI will yield important insights in oncology in the coming decade.",
        "DOI": "10.1111/cas.14377",
        "paper_author": "Shimizu H.",
        "affiliation_name": "Kyushu University",
        "affiliation_city": "Fukuoka",
        "affiliation_country": "Japan",
        "affiliation_id": "60011047",
        "affiliation_state": "Fukuoka"
    },
    {
        "paper_title": "The dreem headband compared to polysomnography for electroencephalographic signal acquisition and sleep staging",
        "publication": "Sleep",
        "citied_by": "201",
        "cover_date": "2020-11-01",
        "Abstract": "Study Objectives: The development of ambulatory technologies capable of monitoring brain activity during sleep longitudinally is critical for advancing sleep science. The aim of this study was to assess the signal acquisition and the performance of the automatic sleep staging algorithms of a reduced-montage dry-electroencephalographic (EEG) device (Dreem headband, DH) compared to the gold-standard polysomnography (PSG) scored by five sleep experts. Methods: A total of 25 subjects who completed an overnight sleep study at a sleep center while wearing both a PSG and the DH simultaneously have been included in the analysis. We assessed (1) similarity of measured EEG brain waves between the DH and the PSG; (2) the heart rate, breathing frequency, and respiration rate variability (RRV) agreement between the DH and the PSG; and (3) the performance of the DH's automatic sleep staging according to American Academy of Sleep Medicine guidelines versus PSG sleep experts manual scoring. Results: The mean percentage error between the EEG signals acquired by the DH and those from the PSG for the monitoring of α was 15 ± 3.5%, 16 ± 4.3% for β, 16 ± 6.1% for λ, and 10 ± 1.4% for θ frequencies during sleep. The mean absolute error for heart rate, breathing frequency, and RRV was 1.2 ± 0.5 bpm, 0.3 ± 0.2 cpm, and 3.2 ± 0.6%, respectively. Automatic sleep staging reached an overall accuracy of 83.5 ± 6.4% (F1 score: 83.8 ± 6.3) for the DH to be compared with an average of 86.4 ± 8.0% (F1 score: 86.3 ± 7.4) for the 5 sleep experts. Conclusions: These results demonstrate the capacity of the DH to both monitor sleep-related physiological signals and process them accurately into sleep stages. This device paves the way for, large-scale, longitudinal sleep studies.",
        "DOI": "10.1093/sleep/zsaa097",
        "paper_author": "Arnal P.J.",
        "affiliation_name": "Dreem",
        "affiliation_city": "New York",
        "affiliation_country": "United States",
        "affiliation_id": "125327059",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "DeepVision: Deepfakes Detection Using Human Eye Blinking Pattern",
        "publication": "IEEE Access",
        "citied_by": "201",
        "cover_date": "2020-01-01",
        "Abstract": "In this paper, we propose a new approach to detect Deepfakes generated through the generative adversarial network (GANs) model via an algorithm called DeepVision to analyze a significant change in the pattern of blinking, which is a voluntary and spontaneous action that does not require conscious effort. Human eye blinking pattern has been known to significantly change according to the person's overall physical conditions, cognitive activities, biological factors, and information processing level. For example, an individual's gender or age, the time of day, or the person's emotional state or degree of alertness can all influence the pattern. As a result, Deepfakes can be determined through integrity verification by tracking significant changes in the eye blinking patterns in deepfakes by means of a heuristic method based on the results of medicine, biology, and brain engineering research, as well as machine learning and various algorithms based on engineering and statistical knowledge. This means we can perform integrity verification through tracking significant changes in the eye blinking pattern of a subject in a video. The proposed method called DeepVision is implemented as a measure to verify an anomaly based on the period, repeated number, and elapsed eye blink time when eye blinks were continuously repeated within a very short period of time. DeepVision accurately detected Deepfakes in seven out of eight types of videos (87.5% accuracy rate), suggesting we can overcome the limitations of integrity verification algorithms performed only on the basis of pixels.",
        "DOI": "10.1109/ACCESS.2020.2988660",
        "paper_author": "Jung T.",
        "affiliation_name": "Konkuk University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea",
        "affiliation_id": "60000142",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Digital pathology: Advantages, limitations and emerging perspectives",
        "publication": "Journal of Clinical Medicine",
        "citied_by": "192",
        "cover_date": "2020-11-01",
        "Abstract": "Digital pathology is on the verge of becoming a mainstream option for routine diagnostics. Faster whole slide image scanning has paved the way for this development, but implementation on a large scale is challenging on technical, logistical, and financial levels. Comparative studies have published reassuring data on safety and feasibility, but implementation experiences highlight the need for training and the knowledge of pitfalls. Up to half of the pathologists are reluctant to sign out reports on only digital slides and are concerned about reporting without the tool that has represented their profession since its beginning. Guidelines by international pathology organizations aim to safeguard histology in the digital realm, from image acquisition over the setup of work-stations to long-term image archiving, but must be considered a starting point only. Cost-efficiency analyses and occupational health issues need to be addressed comprehensively. Image analysis is blended into the traditional work-flow, and the approval of artificial intelligence for routine diagnostics starts to challenge human evaluation as the gold standard. Here we discuss experiences from past digital pathology implementations, future possibilities through the addition of artificial intelligence, technical and occupational health challenges, and possible changes to the pathologist’s profession.",
        "DOI": "10.3390/jcm9113697",
        "paper_author": "Jahn S.W.",
        "affiliation_name": "Medizinische Universität Graz",
        "affiliation_city": "Graz",
        "affiliation_country": "Austria",
        "affiliation_id": "60006224",
        "affiliation_state": "Styria"
    },
    {
        "paper_title": "Artificial intelligence within the interplay between natural and artificial computation: Advances in data science, trends and applications",
        "publication": "Neurocomputing",
        "citied_by": "190",
        "cover_date": "2020-10-14",
        "Abstract": "Artificial intelligence and all its supporting tools, e.g. machine and deep learning in computational intelligence-based systems, are rebuilding our society (economy, education, life-style, etc.) and promising a new era for the social welfare state. In this paper we summarize recent advances in data science and artificial intelligence within the interplay between natural and artificial computation. A review of recent works published in the latter field and the state the art are summarized in a comprehensive and self-contained way to provide a baseline framework for the international community in artificial intelligence. Moreover, this paper aims to provide a complete analysis and some relevant discussions of the current trends and insights within several theoretical and application fields covered in the essay, from theoretical models in artificial intelligence and machine learning to the most prospective applications in robotics, neuroscience, brain computer interfaces, medicine and society, in general.",
        "DOI": "10.1016/j.neucom.2020.05.078",
        "paper_author": "Górriz J.M.",
        "affiliation_name": "Universidad de Granada",
        "affiliation_city": "Granada",
        "affiliation_country": "Spain",
        "affiliation_id": "60027844",
        "affiliation_state": "Granada"
    },
    {
        "paper_title": "The future of sleep health: a data-driven revolution in sleep science and medicine",
        "publication": "npj Digital Medicine",
        "citied_by": "187",
        "cover_date": "2020-12-01",
        "Abstract": "In recent years, there has been a significant expansion in the development and use of multi-modal sensors and technologies to monitor physical activity, sleep and circadian rhythms. These developments make accurate sleep monitoring at scale a possibility for the first time. Vast amounts of multi-sensor data are being generated with potential applications ranging from large-scale epidemiological research linking sleep patterns to disease, to wellness applications, including the sleep coaching of individuals with chronic conditions. However, in order to realise the full potential of these technologies for individuals, medicine and research, several significant challenges must be overcome. There are important outstanding questions regarding performance evaluation, as well as data storage, curation, processing, integration, modelling and interpretation. Here, we leverage expertise across neuroscience, clinical medicine, bioengineering, electrical engineering, epidemiology, computer science, mHealth and human–computer interaction to discuss the digitisation of sleep from a inter-disciplinary perspective. We introduce the state-of-the-art in sleep-monitoring technologies, and discuss the opportunities and challenges from data acquisition to the eventual application of insights in clinical and consumer settings. Further, we explore the strengths and limitations of current and emerging sensing methods with a particular focus on novel data-driven technologies, such as Artificial Intelligence.",
        "DOI": "10.1038/s41746-020-0244-4",
        "paper_author": "Perez-Pozuelo I.",
        "affiliation_name": "Department of Medicine",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60120017",
        "affiliation_state": "Cambridgeshire"
    },
    {
        "paper_title": "Machine intelligence in healthcare—perspectives on trustworthiness, explainability, usability, and transparency",
        "publication": "npj Digital Medicine",
        "citied_by": "187",
        "cover_date": "2020-12-01",
        "Abstract": "NA",
        "DOI": "10.1038/s41746-020-0254-2",
        "paper_author": "Cutillo C.M.",
        "affiliation_name": "National Center for Advancing Translational Sciences (NCATS)",
        "affiliation_city": "Bethesda",
        "affiliation_country": "United States",
        "affiliation_id": "60104446",
        "affiliation_state": "MD"
    },
    {
        "paper_title": "Machine learning the ropes: Principles, applications and directions in synthetic chemistry",
        "publication": "Chemical Society Reviews",
        "citied_by": "186",
        "cover_date": "2020-09-07",
        "Abstract": "Machine learning (ML) has emerged as a general, problem-solving paradigm with many applications in computer vision, natural language processing, digital safety, or medicine. By recognizing complex patterns in data, ML bears the potential to modernise the way how many chemical challenges are approached. In this review, an introduction to ML is given from the perspective of synthetic chemistry: starting from the fundamentals regarding algorithms and best-practice workflows, the review covers different applications of machine learning in synthesis planning, property prediction, molecular design, and reactivity prediction. In particular, different approaches of representing and utilizing organic molecules will be discussed-providing synthetic chemists both with the understanding and the tools required to apply machine learning in the context of their research, and pointers for further studying. This journal is",
        "DOI": "10.1039/c9cs00786e",
        "paper_author": "Strieth-Kalthoff F.",
        "affiliation_name": "University of Münster",
        "affiliation_city": "Munster",
        "affiliation_country": "Germany",
        "affiliation_id": "60000401",
        "affiliation_state": "Nordrhein-Westfalen"
    },
    {
        "paper_title": "Artificial intelligence in medical imaging: switching from radiographic pathological data to clinically meaningful endpoints",
        "publication": "The Lancet Digital Health",
        "citied_by": "185",
        "cover_date": "2020-09-01",
        "Abstract": "Artificial intelligence (AI) is a disruptive technology that involves the use of computerised algorithms to dissect complicated data. Among the most promising clinical applications of AI is diagnostic imaging, and mounting attention is being directed at establishing and fine-tuning its performance to facilitate detection and quantification of a wide array of clinical conditions. Investigations leveraging computer-aided diagnostics have shown excellent accuracy, sensitivity, and specificity for the detection of small radiographic abnormalities, with the potential to improve public health. However, outcome assessment in AI imaging studies is commonly defined by lesion detection while ignoring the type and biological aggressiveness of a lesion, which might create a skewed representation of AI's performance. Moreover, the use of non-patient-focused radiographic and pathological endpoints might enhance the estimated sensitivity at the expense of increasing false positives and possible overdiagnosis as a result of identifying minor changes that might reflect subclinical or indolent disease. We argue for refinement of AI imaging studies via consistent selection of clinically meaningful endpoints such as survival, symptoms, and need for treatment.",
        "DOI": "10.1016/S2589-7500(20)30160-6",
        "paper_author": "Oren O.",
        "affiliation_name": "Mayo Clinic",
        "affiliation_city": "Rochester",
        "affiliation_country": "United States",
        "affiliation_id": "60005558",
        "affiliation_state": "MN"
    },
    {
        "paper_title": "A Review of Super-Resolution Single-Molecule Localization Microscopy Cluster Analysis and Quantification Methods",
        "publication": "Patterns",
        "citied_by": "179",
        "cover_date": "2020-06-12",
        "Abstract": "Single-molecule localization microscopy (SMLM) is a relatively new imaging modality, winning the 2014 Nobel Prize in Chemistry, and considered as one of the key super-resolution techniques. SMLM resolution goes beyond the diffraction limit of light microscopy and achieves resolution on the order of 10–20 nm. SMLM thus enables imaging single molecules and study of the low-level molecular interactions at the subcellular level. In contrast to standard microscopy imaging that produces 2D pixel or 3D voxel grid data, SMLM generates big data of 2D or 3D point clouds with millions of localizations and associated uncertainties. This unprecedented breakthrough in imaging helps researchers employ SMLM in many fields within biology and medicine, such as studying cancerous cells and cell-mediated immunity and accelerating drug discovery. However, SMLM data quantification and interpretation methods have yet to keep pace with the rapid advancement of SMLM imaging. Researchers have been actively exploring new computational methods for SMLM data analysis to extract biosignatures of various biological structures and functions. In this survey, we describe the state-of-the-art clustering methods adopted to analyze and quantify SMLM data and examine the capabilities and shortcomings of the surveyed methods. We classify the methods according to (1) the biological application (i.e., the imaged molecules/structures), (2) the data acquisition (such as imaging modality, dimension, resolution, and number of localizations), and (3) the analysis details (2D versus 3D, field of view versus region of interest, use of machine-learning and multi-scale analysis, biosignature extraction, etc.). We observe that the majority of methods that are based on second-order statistics are sensitive to noise and imaging artifacts, have not been applied to 3D data, do not leverage machine-learning formulations, and are not scalable for big-data analysis. Finally, we summarize state-of-the-art methodology, discuss some key open challenges, and identify future opportunities for better modeling and design of an integrated computational pipeline to address the key challenges. Recent developments in super-resolution SMLM imaging techniques enable researchers to study macromolecular structures at the nanometer scale. However, SMLM data quantification and interpretation methods have yet to keep pace with the rapid advancement of SMLM imaging. This article provides a balanced and comprehensive review of state-of-the-art SMLM image analysis methods and ties disparate approaches together in a cohesive manner. Researchers are actively exploring new computational methods to analyze SMLM data, including recent approaches to use data-driven and machine-learning approaches. However, the validation of the SMLM clustering methods remains an open challenge. Potential future directions using multi-modality imaging (e.g., SMLM and electron microscopy) might help validate quantitative SMLM image analysis methods. Super-resolution single-molecule localization microscopy (SMLM) enables localization of components of macromolecular complexes at the nanometer scale. However, determining a complex structure from SMLM data-clustering analysis faces challenges of imaging artifacts, big data, 2D versus 3D data, and so forth. In this Review, we provide a holistic overview of state-of-the-art computational methods leveraged to quantify SMLM data. We classify the methods and list their pros and cons to help the researcher optimally consider the most appropriate quantification method. Finally, we show how the field is growing and draw conclusions about the applicability of data-driven approaches as well as methods validation and benchmarking.",
        "DOI": "10.1016/j.patter.2020.100038",
        "paper_author": "Khater I.M.",
        "affiliation_name": "Simon Fraser University",
        "affiliation_city": "Burnaby",
        "affiliation_country": "Canada",
        "affiliation_id": "60018491",
        "affiliation_state": "BC"
    },
    {
        "paper_title": "A systematic literature review on the use of machine learning in precision livestock farming",
        "publication": "Computers and Electronics in Agriculture",
        "citied_by": "179",
        "cover_date": "2020-12-01",
        "Abstract": "This article presents a systematic literature review of recent works on the use of machine learning (ML) in precision livestock farming (PLF), focusing on two areas of interest: grazing and animal health. This review: (i) highlights opportunities for ML in the livestock sector; (ii) shows the current sensors, software and techniques for data analysis; (iii) details the increasing openness of data sources. It was found that the use of ML in PLF is in a stage of development and has several research challenges. Examples of such challenges are: (i) to develop hybrid models for diagnosis and prescription as a tool for the prevention and control of animal diseases; (ii) to bring together the grazing and animal health issues; (iii) to give autonomy to PLF using autonomous cycles of data analysis tasks and meta-learning; and (iv) to bring together soil and pasture variables because, for both, animal health and animal grazing, the variables used are only behavioral and environmental.",
        "DOI": "10.1016/j.compag.2020.105826",
        "paper_author": "García R.",
        "affiliation_name": "Universidad del Sinu",
        "affiliation_city": "Monteria",
        "affiliation_country": "Colombia",
        "affiliation_id": "60091617",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Thermodynamics and Kinetics of Drug-Target Binding by Molecular Simulation",
        "publication": "Chemical Reviews",
        "citied_by": "176",
        "cover_date": "2020-12-09",
        "Abstract": "Computational studies play an increasingly important role in chemistry and biophysics, mainly thanks to improvements in hardware and algorithms. In drug discovery and development, computational studies can reduce the costs and risks of bringing a new medicine to market. Computational simulations are mainly used to optimize promising new compounds by estimating their binding affinity to proteins. This is challenging due to the complexity of the simulated system. To assess the present and future value of simulation for drug discovery, we review key applications of advanced methods for sampling complex free-energy landscapes at near nonergodicity conditions and for estimating the rate coefficients of very slow processes of pharmacological interest. We outline the statistical mechanics and computational background behind this research, including methods such as steered molecular dynamics and metadynamics. We review recent applications to pharmacology and drug discovery and discuss possible guidelines for the practitioner. Recent trends in machine learning are also briefly discussed. Thanks to the rapid development of methods for characterizing and quantifying rare events, simulation's role in drug discovery is likely to expand, making it a valuable complement to experimental and clinical approaches.",
        "DOI": "10.1021/acs.chemrev.0c00534",
        "paper_author": "Decherchi S.",
        "affiliation_name": "Istituto Italiano di Tecnologia",
        "affiliation_city": "Genoa",
        "affiliation_country": "Italy",
        "affiliation_id": "60102151",
        "affiliation_state": "GE"
    },
    {
        "paper_title": "A review of the important role of cyp2d6 in pharmacogenomics",
        "publication": "Genes",
        "citied_by": "172",
        "cover_date": "2020-11-01",
        "Abstract": "Cytochrome P450 2D6 (CYP2D6) is a critical pharmacogene involved in the metabolism of ~20% of commonly used drugs across a broad spectrum of medical disciplines including psychiatry, pain management, oncology and cardiology. Nevertheless, CYP2D6 is highly polymorphic with single-nucleotide polymorphisms, small insertions/deletions and larger structural variants including multiplications, deletions, tandem arrangements, and hybridisations with non-functional CYP2D7 pseudogenes. The frequency of these variants differs across populations, and they significantly influence the drug-metabolising enzymatic function of CYP2D6. Importantly, altered CYP2D6 function has been associated with both adverse drug reactions and reduced drug efficacy, and there is growing recognition of the clinical and economic burdens associated with suboptimal drug utilisation. To date, pharmacogenomic clinical guidelines for at least 48 CYP2D6-substrate drugs have been developed by prominent pharmacogenomics societies, which contain therapeutic recommendations based on CYP2D6-predicted categories of metaboliser phenotype. Novel algorithms to interpret CYP2D6 function from sequencing data that consider structural variants, and machine learning approaches to characterise the functional impact of novel variants, are being developed. However, CYP2D6 genotyping is yet to be implemented broadly into clinical practice, and so further effort and initiatives are required to overcome the implementation challenges and deliver the potential benefits to the bedside.",
        "DOI": "10.3390/genes11111295",
        "paper_author": "Taylor C.",
        "affiliation_name": "University of Liverpool",
        "affiliation_city": "Liverpool",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60020661",
        "affiliation_state": "Merseyside"
    },
    {
        "paper_title": "Digital pathology and computational image analysis in nephropathology",
        "publication": "Nature Reviews Nephrology",
        "citied_by": "172",
        "cover_date": "2020-11-01",
        "Abstract": "The emergence of digital pathology — an image-based environment for the acquisition, management and interpretation of pathology information supported by computational techniques for data extraction and analysis — is changing the pathology ecosystem. In particular, by virtue of our new-found ability to generate and curate digital libraries, the field of machine vision can now be effectively applied to histopathological subject matter by individuals who do not have deep expertise in machine vision techniques. Although these novel approaches have already advanced the detection, classification, and prognostication of diseases in the fields of radiology and oncology, renal pathology is just entering the digital era, with the establishment of consortia and digital pathology repositories for the collection, analysis and integration of pathology data with other domains. The development of machine-learning approaches for the extraction of information from image data, allows for tissue interrogation in a way that was not previously possible. The application of these novel tools are placing pathology centre stage in the process of defining new, integrated, biologically and clinically homogeneous disease categories, to identify patients at risk of progression, and shifting current paradigms for the treatment and prevention of kidney diseases.",
        "DOI": "10.1038/s41581-020-0321-6",
        "paper_author": "Barisoni L.",
        "affiliation_name": "Duke University School of Medicine",
        "affiliation_city": "Durham",
        "affiliation_country": "United States",
        "affiliation_id": "60005200",
        "affiliation_state": "NC"
    },
    {
        "paper_title": "The ethical, legal and social implications of using artificial intelligence systems in breast cancer care",
        "publication": "Breast",
        "citied_by": "171",
        "cover_date": "2020-02-01",
        "Abstract": "Breast cancer care is a leading area for development of artificial intelligence (AI), with applications including screening and diagnosis, risk calculation, prognostication and clinical decision-support, management planning, and precision medicine. We review the ethical, legal and social implications of these developments. We consider the values encoded in algorithms, the need to evaluate outcomes, and issues of bias and transferability, data ownership, confidentiality and consent, and legal, moral and professional responsibility. We consider potential effects for patients, including on trust in healthcare, and provide some social science explanations for the apparent rush to implement AI solutions. We conclude by anticipating future directions for AI in breast cancer care. Stakeholders in healthcare AI should acknowledge that their enterprise is an ethical, legal and social challenge, not just a technical challenge. Taking these challenges seriously will require broad engagement, imposition of conditions on implementation, and pre-emptive systems of oversight to ensure that development does not run ahead of evaluation and deliberation. Once artificial intelligence becomes institutionalised, it may be difficult to reverse: a proactive role for government, regulators and professional groups will help ensure introduction in robust research contexts, and the development of a sound evidence base regarding real-world effectiveness. Detailed public discussion is required to consider what kind of AI is acceptable rather than simply accepting what is offered, thus optimising outcomes for health systems, professionals, society and those receiving care.",
        "DOI": "10.1016/j.breast.2019.10.001",
        "paper_author": "Carter S.M.",
        "affiliation_name": "University of Wollongong",
        "affiliation_city": "Wollongong",
        "affiliation_country": "Australia",
        "affiliation_id": "60011664",
        "affiliation_state": "NSW"
    },
    {
        "paper_title": "The virtual operative assistant: An explainable artificial intelligence tool for simulation-based training in surgery and medicine",
        "publication": "PLoS ONE",
        "citied_by": "168",
        "cover_date": "2020-01-01",
        "Abstract": "Simulation-based training is increasingly being used for assessment and training of psychomotor skills involved in medicine. The application of artificial intelligence and machine learning technologies has provided new methodologies to utilize large amounts of data for educational purposes. A significant criticism of the use of artificial intelligence in education has been a lack of transparency in the algorithms’ decision-making processes. This study aims to 1) introduce a new framework using explainable artificial intelligence for simulation-based training in surgery, and 2) validate the framework by creating the Virtual Operative Assistant, an automated educational feedback platform. Twenty-eight skilled participants (14 staff neurosurgeons, 4 fellows, 10 PGY 4–6 residents) and 22 novice participants (10 PGY 1–3 residents, 12 medical students) took part in this study. Participants performed a virtual reality subpial brain tumor resection task on the NeuroVR simulator using a simulated ultrasonic aspirator and bipolar. Metrics of performance were developed, and leave-one-out cross validation was employed to train and validate a support vector machine in Matlab. The classifier was combined with a unique educational system to build the Virtual Operative Assistant which provides users with automated feedback on their metric performance with regards to expert proficiency performance benchmarks. The Virtual Operative Assistant successfully classified skilled and novice participants using 4 metrics with an accuracy, specificity and sensitivity of 92, 82 and 100%, respectively. A 2-step feedback system was developed to provide participants with an immediate visual representation of their standing related to expert proficiency performance benchmarks. The educational system outlined establishes a basis for the potential role of integrating artificial intelligence and virtual reality simulation into surgical educational teaching. The potential of linking expertise classification, objective feedback based on proficiency benchmarks, and instructor input creates a novel educational tool by integrating these three components into a formative educational paradigm.",
        "DOI": "10.1371/journal.pone.0229596",
        "paper_author": "Mirchi N.",
        "affiliation_name": "Institut-Hôpital Neurologique de Montréal",
        "affiliation_city": "Montreal",
        "affiliation_country": "Canada",
        "affiliation_id": "60030616",
        "affiliation_state": "QC"
    },
    {
        "paper_title": "Bioinformatics and computational tools for next-generation sequencing analysis in clinical genetics",
        "publication": "Journal of Clinical Medicine",
        "citied_by": "167",
        "cover_date": "2020-01-01",
        "Abstract": "Clinical genetics has an important role in the healthcare system to provide a definitive diagnosis for many rare syndromes. It also can have an influence over genetics prevention, disease prognosis and assisting the selection of the best options of care/treatment for patients. Next-generation sequencing (NGS) has transformed clinical genetics making possible to analyze hundreds of genes at an unprecedented speed and at a lower price when comparing to conventional Sanger sequencing. Despite the growing literature concerning NGS in a clinical setting, this review aims to fill the gap that exists among (bio)informaticians, molecular geneticists and clinicians, by presenting a general overview of the NGS technology and workflow. First, we will review the current NGS platforms, focusing on the two main platforms Illumina and Ion Torrent, and discussing the major strong points and weaknesses intrinsic to each platform. Next, the NGS analytical bioinformatic pipelines are dissected, giving some emphasis to the algorithms commonly used to generate process data and to analyze sequence variants. Finally, the main challenges around NGS bioinformatics are placed in perspective for future developments. Even with the huge achievements made in NGS technology and bioinformatics, further improvements in bioinformatic algorithms are still required to deal with complex and genetically heterogeneous disorders.",
        "DOI": "10.3390/jcm9010132",
        "paper_author": "Pereira R.",
        "affiliation_name": "Universidade do Porto",
        "affiliation_city": "Porto",
        "affiliation_country": "Portugal",
        "affiliation_id": "60007249",
        "affiliation_state": "Porto"
    },
    {
        "paper_title": "Machine learning applications in drug development",
        "publication": "Computational and Structural Biotechnology Journal",
        "citied_by": "165",
        "cover_date": "2020-01-01",
        "Abstract": "Due to the huge amount of biological and medical data available today, along with well-established machine learning algorithms, the design of largely automated drug development pipelines can now be envisioned. These pipelines may guide, or speed up, drug discovery; provide a better understanding of diseases and associated biological phenomena; help planning preclinical wet-lab experiments, and even future clinical trials. This automation of the drug development process might be key to the current issue of low productivity rate that pharmaceutical companies currently face. In this survey, we will particularly focus on two classes of methods: sequential learning and recommender systems, which are active biomedical fields of research.",
        "DOI": "10.1016/j.csbj.2019.12.006",
        "paper_author": "Réda C.",
        "affiliation_name": "Hôpital Robert-Debré AP-HP",
        "affiliation_city": "Paris",
        "affiliation_country": "France",
        "affiliation_id": "60011675",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Machine Learning in Dermatology: Current Applications, Opportunities, and Limitations",
        "publication": "Dermatology and Therapy",
        "citied_by": "162",
        "cover_date": "2020-06-01",
        "Abstract": "Machine learning (ML) has the potential to improve the dermatologist’s practice from diagnosis to personalized treatment. Recent advancements in access to large datasets (e.g., electronic medical records, image databases, omics), faster computing, and cheaper data storage have encouraged the development of ML algorithms with human-like intelligence in dermatology. This article is an overview of the basics of ML, current applications of ML, and potential limitations and considerations for further development of ML. We have identified five current areas of applications for ML in dermatology: (1) disease classification using clinical images; (2) disease classification using dermatopathology images; (3) assessment of skin diseases using mobile applications and personal monitoring devices; (4) facilitating large-scale epidemiology research; and (5) precision medicine. The purpose of this review is to provide a guide for dermatologists to help demystify the fundamentals of ML and its wide range of applications in order to better evaluate its potential opportunities and challenges.",
        "DOI": "10.1007/s13555-020-00372-0",
        "paper_author": "Chan S.",
        "affiliation_name": "University of California, San Francisco",
        "affiliation_city": "San Francisco",
        "affiliation_country": "United States",
        "affiliation_id": "60023691",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Epileptic Seizures Prediction Using Deep Learning Techniques",
        "publication": "IEEE Access",
        "citied_by": "162",
        "cover_date": "2020-01-01",
        "Abstract": "Epilepsy is a very common neurological disease that has affected more than 65 million people worldwide. In more than 30 % of the cases, people affected by this disease cannot be cured with medicines or surgery. However, predicting a seizure before it actually occurs can help in its prevention; through therapeutic intervention. Studies have observed that abnormal activity inside the brain begins a few minutes before the start of a seizure, which is known as preictal state. Many researchers have tried to find a way for predicting this preictal state of a seizure but an effective prediction in terms of high sensitivity and specificity still remains a challenge. The current study, proposes a seizure prediction system that employs deep learning methods. This method includes preprocessing of scalp EEG signals, automated features extraction; using convolution neural network and classification with the support of vector machines. The proposed method has been applied on 24 subjects of scalp EEG dataset of CHBMIT resulting in successfully achieving an average sensitivity and specificity of 92.7% and 90.8% respectively.",
        "DOI": "10.1109/ACCESS.2020.2976866",
        "paper_author": "Muhammad Usman S.",
        "affiliation_name": "Bahria University",
        "affiliation_city": "Islamabad",
        "affiliation_country": "Pakistan",
        "affiliation_id": "60070613",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Multi-Task Joint Learning Model for Segmenting and Classifying Tongue Images Using a Deep Neural Network",
        "publication": "IEEE Journal of Biomedical and Health Informatics",
        "citied_by": "161",
        "cover_date": "2020-09-01",
        "Abstract": "Automatic tongue image segmentation and tongue image classification are two crucial tongue characterization tasks in traditional Chinese medicine (TCM). Due to the complexity of tongue segmentation and finegrained traits of tongue image classification, both tasks are challenging. Fortunately, from the perspective of computer vision, these two tasks are highly interrelated, making them compatible with the idea of Multi-Task Joint learning (MTL). By sharing the underlying parameters and adding two different task loss functions, an MTL method for segmenting and classifying tongue images is proposed in this paper. Moreover, two state-of-The-Art deep neural network variants (UNET and Discriminative Filter Learning (DFL)) are fused into the MTL to perform these two tasks. To the best of our knowledge, our method is the first attempt to manage both tasks simultaneously with MTL. We conducted extensive experiments with the proposed method. The experimental results show that our joint method outperforms the existing tongue characterization methods. Besides, visualizations and ablation studies are provided to aid in understanding our approach, which suggest that our method is highly consistent with human perception.",
        "DOI": "10.1109/JBHI.2020.2986376",
        "paper_author": "Xu Q.",
        "affiliation_name": "Chengdu University of Traditional Chinese Medicine",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China",
        "affiliation_id": "60000196",
        "affiliation_state": "Sichuan"
    },
    {
        "paper_title": "Artificial Intelligence, Speech, and Language Processing Approaches to Monitoring Alzheimer's Disease: A Systematic Review",
        "publication": "Journal of Alzheimer's Disease",
        "citied_by": "160",
        "cover_date": "2020-01-01",
        "Abstract": "Background: Language is a valuable source of clinical information in Alzheimer's disease, as it declines concurrently with neurodegeneration. Consequently, speech and language data have been extensively studied in connection with its diagnosis. Objective: Firstly, to summarize the existing findings on the use of artificial intelligence, speech, and language processing to predict cognitive decline in the context of Alzheimer's disease. Secondly, to detail current research procedures, highlight their limitations, and suggest strategies to address them. Methods: Systematic review of original research between 2000 and 2019, registered in PROSPERO (reference CRD42018116606). An interdisciplinary search covered six databases on engineering (ACM and IEEE), psychology (PsycINFO), medicine (PubMed and Embase), and Web of Science. Bibliographies of relevant papers were screened until December 2019. Results: From 3,654 search results, 51 articles were selected against the eligibility criteria. Four tables summarize their findings: study details (aim, population, interventions, comparisons, methods, and outcomes), data details (size, type, modalities, annotation, balance, availability, and language of study), methodology (pre-processing, feature generation, machine learning, evaluation, and results), and clinical applicability (research implications, clinical potential, risk of bias, and strengths/limitations). Conclusion: Promising results are reported across nearly all 51 studies, but very few have been implemented in clinical research or practice. The main limitations of the field are poor standardization, limited comparability of results, and a degree of disconnect between study aims and clinical applications. Active attempts to close these gaps will support translation of future research into clinical practice.",
        "DOI": "10.3233/JAD-200888",
        "paper_author": "De La Fuente Garcia S.",
        "affiliation_name": "Edinburgh Medical School",
        "affiliation_city": "Edinburgh",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60110825",
        "affiliation_state": "Scotland"
    },
    {
        "paper_title": "The need for a system view to regulate artificial intelligence/machine learning-based software as medical device",
        "publication": "npj Digital Medicine",
        "citied_by": "159",
        "cover_date": "2020-12-01",
        "Abstract": "Artificial intelligence (AI) and Machine learning (ML) systems in medicine are poised to significantly improve health care, for example, by offering earlier diagnoses of diseases or recommending optimally individualized treatment plans. However, the emergence of AI/ML in medicine also creates challenges, which regulators must pay attention to. Which medical AI/ML-based products should be reviewed by regulators? What evidence should be required to permit marketing for AI/ML-based software as a medical device (SaMD)? How can we ensure the safety and effectiveness of AI/ML-based SaMD that may change over time as they are applied to new data? The U.S. Food and Drug Administration (FDA), for example, has recently proposed a discussion paper to address some of these issues. But it misses an important point: we argue that regulators like the FDA need to widen their scope from evaluating medical AI/ML-based products to assessing systems. This shift in perspective—from a product view to a system view—is central to maximizing the safety and efficacy of AI/ML in health care, but it also poses significant challenges for agencies like the FDA who are used to regulating products, not systems. We offer several suggestions for regulators to make this challenging but important transition.",
        "DOI": "10.1038/s41746-020-0262-2",
        "paper_author": "Gerke S.",
        "affiliation_name": "Harvard Law School",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States",
        "affiliation_id": "60017150",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Precision medicine in the era of artificial intelligence: implications in chronic disease management",
        "publication": "Journal of Translational Medicine",
        "citied_by": "157",
        "cover_date": "2020-12-01",
        "Abstract": "Aberrant metabolism is the root cause of several serious health issues, creating a huge burden to health and leading to diminished life expectancy. A dysregulated metabolism induces the secretion of several molecules which in turn trigger the inflammatory pathway. Inflammation is the natural reaction of the immune system to a variety of stimuli, such as pathogens, damaged cells, and harmful substances. Metabolically triggered inflammation, also called metaflammation or low-grade chronic inflammation, is the consequence of a synergic interaction between the host and the exposome—a combination of environmental drivers, including diet, lifestyle, pollutants and other factors throughout the life span of an individual. Various levels of chronic inflammation are associated with several lifestyle-related diseases such as diabetes, obesity, metabolic associated fatty liver disease (MAFLD), cancers, cardiovascular disorders (CVDs), autoimmune diseases, and chronic lung diseases. Chronic diseases are a growing concern worldwide, placing a heavy burden on individuals, families, governments, and health-care systems. New strategies are needed to empower communities worldwide to prevent and treat these diseases. Precision medicine provides a model for the next generation of lifestyle modification. This will capitalize on the dynamic interaction between an individual’s biology, lifestyle, behavior, and environment. The aim of precision medicine is to design and improve diagnosis, therapeutics and prognostication through the use of large complex datasets that incorporate individual gene, function, and environmental variations. The implementation of high-performance computing (HPC) and artificial intelligence (AI) can predict risks with greater accuracy based on available multidimensional clinical and biological datasets. AI-powered precision medicine provides clinicians with an opportunity to specifically tailor early interventions to each individual. In this article, we discuss the strengths and limitations of existing and evolving recent, data-driven technologies, such as AI, in preventing, treating and reversing lifestyle-related diseases.",
        "DOI": "10.1186/s12967-020-02658-5",
        "paper_author": "Subramanian M.",
        "affiliation_name": "Weill Cornell Medicine",
        "affiliation_city": "New York",
        "affiliation_country": "United States",
        "affiliation_id": "60007997",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "M3DISEEN: A novel machine learning approach for predicting the 3D printability of medicines",
        "publication": "International Journal of Pharmaceutics",
        "citied_by": "157",
        "cover_date": "2020-11-30",
        "Abstract": "Artificial intelligence (AI) has the potential to reshape pharmaceutical formulation development through its ability to analyze and continuously monitor large datasets. Fused deposition modeling (FDM) three-dimensional printing (3DP) has made significant advancements in the field of oral drug delivery with personalized drug-loaded formulations being designed, developed and dispensed for the needs of the patient. The FDM 3DP process begins with the production of drug-loaded filaments by hot melt extrusion (HME), followed by the printing of a drug product using a FDM 3D printer. However, the optimization of the fabrication parameters is a time-consuming, empirical trial approach, requiring expert knowledge. Here, M3DISEEN, a web-based pharmaceutical software, was developed to accelerate FDM 3D printing using AI machine learning techniques (MLTs). In total, 614 drug-loaded formulations were designed from a comprehensive list of 145 different pharmaceutical excipients, 3D printed and assessed in-house. To build the predictive tool, a dataset was constructed and models were trained and tested at a ratio of 75:25. Significantly, the AI models predicted key fabrication parameters with accuracies of 76% and 67% for the printability and the filament characteristics, respectively. Furthermore, the AI models predicted the HME and FDM processing temperatures with a mean absolute error of 8.9 °C and 8.3 °C, respectively. Strikingly, the AI models achieved high levels of accuracy by solely inputting the pharmaceutical excipient trade names. Therefore, AI provides an effective holistic modeling technology and software to streamline and advance 3DP as a significant technology within drug development. M3DISEEN is available at (http://m3diseen.com/predictions/).",
        "DOI": "10.1016/j.ijpharm.2020.119837",
        "paper_author": "Elbadawi M.",
        "affiliation_name": "UCL School of Pharmacy",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60016379",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Anonymization through data synthesis using generative adversarial networks (ADS-GAN)",
        "publication": "IEEE Journal of Biomedical and Health Informatics",
        "citied_by": "157",
        "cover_date": "2020-08-01",
        "Abstract": "The medical and machine learning communities are relying on the promise of artificial intelligence (AI) to transform medicine through enabling more accurate decisions and personalized treatment. However, progress is slow. Legal and ethical issues around unconsented patient data and privacy is one of the limiting factors in data sharing, resulting in a significant barrier in accessing routinely collected electronic health records (EHR) by the machine learning community. We propose a novel framework for generating synthetic data that closely approximates the joint distribution of variables in an original EHR dataset, providing a readily accessible, legally and ethically appropriate solution to support more open data sharing, enabling the development of AI solutions. In order to address issues around lack of clarity in defining sufficient anonymization, we created a quantifiable, mathematical definition for 'identifiability'. We used a conditional generative adversarial networks (GAN) framework to generate synthetic data while minimize patient identifiability that is defined based on the probability of re-identification given the combination of all data on any individual patient. We compared models fitted to our synthetically generated data to those fitted to the real data across four independent datasets to evaluate similarity in model performance, while assessing the extent to which original observations can be identified from the synthetic data. Our model, ADS-GAN, consistently outperformed state-of-the-art methods, and demonstrated reliability in the joint distributions. We propose that this method could be used to develop datasets that can be made publicly available while considerably lowering the risk of breaching patient confidentiality.",
        "DOI": "10.1109/JBHI.2020.2980262",
        "paper_author": "Yoon J.",
        "affiliation_name": "UCLA Samueli School of Engineering",
        "affiliation_city": "Los Angeles",
        "affiliation_country": "United States",
        "affiliation_id": "60153950",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Artificial Intelligence in Cardiology: Present and Future",
        "publication": "Mayo Clinic Proceedings",
        "citied_by": "156",
        "cover_date": "2020-05-01",
        "Abstract": "Artificial intelligence (AI) is a nontechnical, popular term that refers to machine learning of various types but most often to deep neural networks. Cardiology is at the forefront of AI in medicine. For this review, we searched PubMed and MEDLINE databases with no date restriction using search terms related to AI and cardiology. Articles were selected for inclusion on the basis of relevance. We highlight the major achievements in recent years in nearly all areas of cardiology and underscore the mounting evidence suggesting how AI will take center stage in the field. Artificial intelligence requires a close collaboration among computer scientists, clinical investigators, clinicians, and other users in order to identify the most relevant problems to be solved. Best practices in the generation and implementation of AI include the selection of ideal data sources, taking into account common challenges during the interpretation, validation, and generalizability of findings, and addressing safety and ethical concerns before final implementation. The future of AI in cardiology and in medicine in general is bright as the collaboration between investigators and clinicians continues to excel.",
        "DOI": "10.1016/j.mayocp.2020.01.038",
        "paper_author": "Lopez-Jimenez F.",
        "affiliation_name": "Mayo Clinic",
        "affiliation_city": "Rochester",
        "affiliation_country": "United States",
        "affiliation_id": "60005558",
        "affiliation_state": "MN"
    },
    {
        "paper_title": "Integrated Multi-Omics Analyses in Oncology: A Review of Machine Learning Methods and Tools",
        "publication": "Frontiers in Oncology",
        "citied_by": "155",
        "cover_date": "2020-06-30",
        "Abstract": "In recent years, high-throughput sequencing technologies provide unprecedented opportunity to depict cancer samples at multiple molecular levels. The integration and analysis of these multi-omics datasets is a crucial and critical step to gain actionable knowledge in a precision medicine framework. This paper explores recent data-driven methodologies that have been developed and applied to respond major challenges of stratified medicine in oncology, including patients' phenotyping, biomarker discovery, and drug repurposing. We systematically retrieved peer-reviewed journals published from 2014 to 2019, select and thoroughly describe the tools presenting the most promising innovations regarding the integration of heterogeneous data, the machine learning methodologies that successfully tackled the complexity of multi-omics data, and the frameworks to deliver actionable results for clinical practice. The review is organized according to the applied methods: Deep learning, Network-based methods, Clustering, Features Extraction, and Transformation, Factorization. We provide an overview of the tools available in each methodological group and underline the relationship among the different categories. Our analysis revealed how multi-omics datasets could be exploited to drive precision oncology, but also current limitations in the development of multi-omics data integration.",
        "DOI": "10.3389/fonc.2020.01030",
        "paper_author": "Nicora G.",
        "affiliation_name": "Università degli Studi di Pavia",
        "affiliation_city": "Pavia",
        "affiliation_country": "Italy",
        "affiliation_id": "60015197",
        "affiliation_state": "PV"
    },
    {
        "paper_title": "Assessing and Mitigating Bias in Medical Artificial Intelligence: The Effects of Race and Ethnicity on a Deep Learning Model for ECG Analysis",
        "publication": "Circulation: Arrhythmia and Electrophysiology",
        "citied_by": "154",
        "cover_date": "2020-03-01",
        "Abstract": "Background: Deep learning algorithms derived in homogeneous populations may be poorly generalizable and have the potential to reflect, perpetuate, and even exacerbate racial/ethnic disparities in health and health care. In this study, we aimed to (1) assess whether the performance of a deep learning algorithm designed to detect low left ventricular ejection fraction using the 12-lead ECG varies by race/ethnicity and to (2) determine whether its performance is determined by the derivation population or by racial variation in the ECG. Methods: We performed a retrospective cohort analysis that included 97 829 patients with paired ECGs and echocardiograms. We tested the model performance by race/ethnicity for convolutional neural network designed to identify patients with a left ventricular ejection fraction ≤35% from the 12-lead ECG. Results: The convolutional neural network that was previously derived in a homogeneous population (derivation cohort, n=44 959; 96.2% non-Hispanic white) demonstrated consistent performance to detect low left ventricular ejection fraction across a range of racial/ethnic subgroups in a separate testing cohort (n=52 870): non-Hispanic white (n=44 524; area under the curve [AUC], 0.931), Asian (n=557; AUC, 0.961), black/African American (n=651; AUC, 0.937), Hispanic/Latino (n=331; AUC, 0.937), and American Indian/Native Alaskan (n=223; AUC, 0.938). In secondary analyses, a separate neural network was able to discern racial subgroup category (black/African American [AUC, 0.84], and white, non-Hispanic [AUC, 0.76] in a 5-class classifier), and a network trained only in non-Hispanic whites from the original derivation cohort performed similarly well across a range of racial/ethnic subgroups in the testing cohort with an AUC of at least 0.930 in all racial/ethnic subgroups. Conclusions: Our study demonstrates that while ECG characteristics vary by race, this did not impact the ability of a convolutional neural network to predict low left ventricular ejection fraction from the ECG. We recommend reporting of performance among diverse ethnic, racial, age, and sex groups for all new artificial intelligence tools to ensure responsible use of artificial intelligence in medicine.",
        "DOI": "10.1161/CIRCEP.119.007988",
        "paper_author": "Noseworthy P.A.",
        "affiliation_name": "Mayo Clinic",
        "affiliation_city": "Rochester",
        "affiliation_country": "United States",
        "affiliation_id": "60005558",
        "affiliation_state": "MN"
    },
    {
        "paper_title": "Big data in IBD: Big progress for clinical practice",
        "publication": "Gut",
        "citied_by": "150",
        "cover_date": "2020-08-01",
        "Abstract": "IBD is a complex multifactorial inflammatory disease of the gut driven by extrinsic and intrinsic factors, including host genetics, the immune system, environmental factors and the gut microbiome. Technological advancements such as next-generation sequencing, high-throughput omics data generation and molecular networks have catalysed IBD research. The advent of artificial intelligence, in particular, machine learning, and systems biology has opened the avenue for the efficient integration and interpretation of big datasets for discovering clinically translatable knowledge. In this narrative review, we discuss how big data integration and machine learning have been applied to translational IBD research. Approaches such as machine learning may enable patient stratification, prediction of disease progression and therapy responses for fine-tuning treatment options with positive impacts on cost, health and safety. We also outline the challenges and opportunities presented by machine learning and big data in clinical IBD research.",
        "DOI": "10.1136/gutjnl-2019-320065",
        "paper_author": "Seyed Tabib N.S.",
        "affiliation_name": "Departement Chronische Ziekten, Metabolisme en Veroudering",
        "affiliation_city": "Leuven",
        "affiliation_country": "Belgium",
        "affiliation_id": "60121260",
        "affiliation_state": "Vlaams-Brabant"
    },
    {
        "paper_title": "Big data analytics in health sector: Theoretical framework, techniques and prospects",
        "publication": "International Journal of Information Management",
        "citied_by": "149",
        "cover_date": "2020-02-01",
        "Abstract": "Clinicians, healthcare providers-suppliers, policy makers and patients are experiencing exciting opportunities in light of new information deriving from the analysis of big data sets, a capability that has emerged in the last decades. Due to the rapid increase of publications in the healthcare industry, we have conducted a structured review regarding healthcare big data analytics. With reference to the resource-based view theory we focus on how big data resources are utilised to create organization values/capabilities, and through content analysis of the selected publications we discuss: the classification of big data types related to healthcare, the associate analysis techniques, the created value for stakeholders, the platforms and tools for handling big health data and future aspects in the field. We present a number of pragmatic examples to show how the advances in healthcare were made possible. We believe that the findings of this review are stimulating and provide valuable information to practitioners, policy makers and researchers while presenting them with certain paths for future research.",
        "DOI": "10.1016/j.ijinfomgt.2019.05.003",
        "paper_author": "Galetsi P.",
        "affiliation_name": "International Hellenic University",
        "affiliation_city": "Thessaloniki",
        "affiliation_country": "Greece",
        "affiliation_id": "60158100",
        "affiliation_state": "Central Macedonia"
    },
    {
        "paper_title": "Trends in 3D Printing Processes for Biomedical Field: Opportunities and Challenges",
        "publication": "Journal of Polymers and the Environment",
        "citied_by": "148",
        "cover_date": "2020-05-01",
        "Abstract": "Additive manufacturing (AM) is considered the latest technology that creates breakthrough innovations and addresses complex medical problems. This is clearly demonstrated by the promising results obtained in regenerative medicine, diagnosis, implants, artificial tissues and organs. This paper provides a basic understanding of the fundamentals of 3D/4D printing along with bioprinting processes. We are briefly discussing about the main printing systems including stereolithography, inkjet 3D printing, extrusion, laser-assisted printing, selective laser melting and Poly-Jet printing. The basic requirements for the selection of successful inks based on polymers, polymer blends, and composites are described. Furthermore, the on-going transition from 3D to 4D printing is highlighted with emphasis on the newest applications in the medical area. Also, a glimpse into the future possibilities and benefits provided by machine learning in the additive manufacturing field is emphasized. Machine learning can improve printing efficiency by using generative design and testing in the pre-fabrication stage. Finally, important limitations and prospects are identified. Within the next few years, AM is set to become an important component in patient-specific medical technologies. Graphic Abstract: [Figure not available: see fulltext.].",
        "DOI": "10.1007/s10924-020-01722-x",
        "paper_author": "Ghilan A.",
        "affiliation_name": "Petru Poni Institute of Macromolecular chemistry",
        "affiliation_city": "Iasi",
        "affiliation_country": "Romania",
        "affiliation_id": "60022305",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Applying Machine Learning in Liver Disease and Transplantation: A Comprehensive Review",
        "publication": "Hepatology",
        "citied_by": "148",
        "cover_date": "2020-03-01",
        "Abstract": "Machine learning (ML) utilizes artificial intelligence to generate predictive models efficiently and more effectively than conventional methods through detection of hidden patterns within large data sets. With this in mind, there are several areas within hepatology where these methods can be applied. In this review, we examine the literature pertaining to machine learning in hepatology and liver transplant medicine. We provide an overview of the strengths and limitations of ML tools and their potential applications to both clinical and molecular data in hepatology. ML has been applied to various types of data in liver disease research, including clinical, demographic, molecular, radiological, and pathological data. We anticipate that use of ML tools to generate predictive algorithms will change the face of clinical practice in hepatology and transplantation. This review will provide readers with the opportunity to learn about the ML tools available and potential applications to questions of interest in hepatology.",
        "DOI": "10.1002/hep.31103",
        "paper_author": "Spann A.",
        "affiliation_name": "Vanderbilt University Medical Center",
        "affiliation_city": "Nashville",
        "affiliation_country": "United States",
        "affiliation_id": "60030769",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "A clinically applicable deep-learning model for detecting intracranial aneurysm in computed tomography angiography images",
        "publication": "Nature Communications",
        "citied_by": "146",
        "cover_date": "2020-12-01",
        "Abstract": "Intracranial aneurysm is a common life-threatening disease. Computed tomography angiography is recommended as the standard diagnosis tool; yet, interpretation can be time-consuming and challenging. We present a specific deep-learning-based model trained on 1,177 digital subtraction angiography verified bone-removal computed tomography angiography cases. The model has good tolerance to image quality and is tested with different manufacturers. Simulated real-world studies are conducted in consecutive internal and external cohorts, in which it achieves an improved patient-level sensitivity and lesion-level sensitivity compared to that of radiologists and expert neurosurgeons. A specific cohort of suspected acute ischemic stroke is employed and it is found that 99.0% predicted-negative cases can be trusted with high confidence, leading to a potential reduction in human workload. A prospective study is warranted to determine whether the algorithm could improve patients’ care in comparison to clinicians’ assessment.",
        "DOI": "10.1038/s41467-020-19527-w",
        "paper_author": "Shi Z.",
        "affiliation_name": "Medical School of Nanjing University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60004667",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "Use of artificial intelligence in infectious diseases",
        "publication": "Artificial Intelligence in Precision Health: From Concept to Applications",
        "citied_by": "145",
        "cover_date": "2020-01-01",
        "Abstract": "Infectious diseases are caused by microorganisms belonging to the class of bacteria, viruses, fungi, or parasites. These pathogens are transmitted, directly or indirectly, and can lead to epidemics or even pandemics. The resulting infection may lead to mild-to-severe symptoms such as life-threatening fever or diarrhea. Infectious diseases may be asymptomatic in some individuals but may lead to disastrous effects in others. Despite the advances in medicine, infectious diseases are a leading cause of death worldwide, especially in low-income countries. With the advent of mathematical tools, scientists are now able to better predict epidemics, understand the specificity of each pathogen, and identify potential targets for drug development. Artificial intelligence and its components have been widely publicized for their ability to better diagnose certain types of cancer from imaging data. This chapter aims at identifying potential applications of machine learning in the field of infectious diseases. We are deliberately focusing on key aspects of infection: diagnosis, transmission, response to treatment, and resistance. We are proposing the use of extreme values as an avenue of interest for future developments in the field of infectious diseases. This chapter covers a series of applications selectively chosen to showcase how artificial intelligence is moving the field of infectious disease further and how it helps institutions to better tackles them, especially in low-income countries.",
        "DOI": "10.1016/B978-0-12-817133-2.00018-5",
        "paper_author": "Agrebi S.",
        "affiliation_name": "Technopark El Gazala",
        "affiliation_city": "Ariana",
        "affiliation_country": "Tunisia",
        "affiliation_id": "129611576",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Hardware and Software Optimizations for Accelerating Deep Neural Networks: Survey of Current Trends, Challenges, and the Road Ahead",
        "publication": "IEEE Access",
        "citied_by": "145",
        "cover_date": "2020-01-01",
        "Abstract": "Currently, Machine Learning (ML) is becoming ubiquitous in everyday life. Deep Learning (DL) is already present in many applications ranging from computer vision for medicine to autonomous driving of modern cars as well as other sectors in security, healthcare, and finance. However, to achieve impressive performance, these algorithms employ very deep networks, requiring a significant computational power, both during the training and inference time. A single inference of a DL model may require billions of multiply-and-accumulated operations, making the DL extremely compute- and energy-hungry. In a scenario where several sophisticated algorithms need to be executed with limited energy and low latency, the need for cost-effective hardware platforms capable of implementing energy-efficient DL execution arises. This paper first introduces the key properties of two brain-inspired models like Deep Neural Network (DNN), and Spiking Neural Network (SNN), and then analyzes techniques to produce efficient and high-performance designs. This work summarizes and compares the works for four leading platforms for the execution of algorithms such as CPU, GPU, FPGA and ASIC describing the main solutions of the state-of-the-art, giving much prominence to the last two solutions since they offer greater design flexibility and bear the potential of high energy-efficiency, especially for the inference process. In addition to hardware solutions, this paper discusses some of the important security issues that these DNN and SNN models may have during their execution, and offers a comprehensive section on benchmarking, explaining how to assess the quality of different networks and hardware systems designed for them.",
        "DOI": "10.1109/ACCESS.2020.3039858",
        "paper_author": "Capra M.",
        "affiliation_name": "Politecnico di Torino",
        "affiliation_city": "Turin",
        "affiliation_country": "Italy",
        "affiliation_id": "60012162",
        "affiliation_state": "TO"
    },
    {
        "paper_title": "Deep learning-based cardiovascular image diagnosis: A promising challenge",
        "publication": "Future Generation Computer Systems",
        "citied_by": "145",
        "cover_date": "2020-09-01",
        "Abstract": "Artificial intelligence (AI) is becoming a vital concept in medicine leading to a rapid emergence of important tools for medical diagnostics. Now, as a crucial machine learning tool in the field of computer vision, deep learning (DL) is being widely used in medical imaging. Furthermore, as reported in the medical literature, DL has been widely used in medical related research. However, the practical application of DL in clinical diagnosis is relatively small, and it is a new field that may have some challenges. How to effectively perform medical image analysis is a major problem in the field of disease diagnosis, and further diagnostic methods need to be developed. At this stage, DL could be viewed as a black box requiring knowledge of its internal workings, and hence presents some crucial technical challenges that need further methodological development. Thereafter with proper diagnostics, pre-operative computerized simulation planning can be carried out for use of appropriate surgical intervention technology. This paper presents important questions on cardiovascular disease (CVD) diagnostics, using this powerful and yet not adequately understood technology. It discusses issues brought by the paradigm shift of AI vis-à-vis DL in CVD diagnostics, provides possible solutions to potential issues, and envisions the future of the related machine intelligence applications. The discussed problems are dissected into the modular aspects of DL in relation to CVD image classification, segmentation, and detection. A proper perspective on management of these issues is the key to a successful technological implementation of DL in modern medical science.",
        "DOI": "10.1016/j.future.2019.09.047",
        "paper_author": "Wong K.K.L.",
        "affiliation_name": "The University of Adelaide",
        "affiliation_city": "Adelaide",
        "affiliation_country": "Australia",
        "affiliation_id": "60009512",
        "affiliation_state": "SA"
    },
    {
        "paper_title": "Artificial intelligence in the interpretation of breast cancer on MRI",
        "publication": "Journal of Magnetic Resonance Imaging",
        "citied_by": "145",
        "cover_date": "2020-05-01",
        "Abstract": "Advances in both imaging and computers have led to the rise in the potential use of artificial intelligence (AI) in various tasks in breast imaging, going beyond the current use in computer-aided detection to include diagnosis, prognosis, response to therapy, and risk assessment. The automated capabilities of AI offer the potential to enhance the diagnostic expertise of clinicians, including accurate demarcation of tumor volume, extraction of characteristic cancer phenotypes, translation of tumoral phenotype features to clinical genotype implications, and risk prediction. The combination of image-specific findings with the underlying genomic, pathologic, and clinical features is becoming of increasing value in breast cancer. The concurrent emergence of newer imaging techniques has provided radiologists with greater diagnostic tools and image datasets to analyze and interpret. Integrating an AI-based workflow within breast imaging enables the integration of multiple data streams into powerful multidisciplinary applications that may lead the path to personalized patient-specific medicine. In this article we describe the goals of AI in breast cancer imaging, in particular MRI, and review the literature as it relates to the current application, potential, and limitations in breast cancer. Level of Evidence: 3. Technical Efficacy: Stage 3. J. Magn. Reson. Imaging 2020;51:1310–1324.",
        "DOI": "10.1002/jmri.26878",
        "paper_author": "Sheth D.",
        "affiliation_name": "The University of Chicago",
        "affiliation_city": "Chicago",
        "affiliation_country": "United States",
        "affiliation_id": "60029278",
        "affiliation_state": "IL"
    },
    {
        "paper_title": "Machine-learning methods for computational science and engineering",
        "publication": "Computation",
        "citied_by": "144",
        "cover_date": "2020-03-01",
        "Abstract": "The re-kindled fascination in machine learning (ML), observed over the last few decades, has also percolated into natural sciences and engineering. ML algorithms are now used in scientific computing, as well as in data-mining and processing. In this paper, we provide a review of the state-of-the-art in ML for computational science and engineering. We discuss ways of using ML to speed up or improve the quality of simulation techniques such as computational fluid dynamics, molecular dynamics, and structural analysis. We explore the ability of ML to produce computationally efficient surrogate models of physical applications that circumvent the need for the more expensive simulation techniques entirely. We also discuss how ML can be used to process large amounts of data, using as examples many different scientific fields, such as engineering, medicine, astronomy and computing. Finally, we review how ML has been used to create more realistic and responsive virtual reality applications.",
        "DOI": "10.3390/computation8010015",
        "paper_author": "Frank M.",
        "affiliation_name": "University of Strathclyde",
        "affiliation_city": "Glasgow",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60024724",
        "affiliation_state": "Scotland"
    },
    {
        "paper_title": "Time to reality check the promises of machine learning-powered precision medicine",
        "publication": "The Lancet Digital Health",
        "citied_by": "141",
        "cover_date": "2020-12-01",
        "Abstract": "Machine learning methods, combined with large electronic health databases, could enable a personalised approach to medicine through improved diagnosis and prediction of individual responses to therapies. If successful, this strategy would represent a revolution in clinical research and practice. However, although the vision of individually tailored medicine is alluring, there is a need to distinguish genuine potential from hype. We argue that the goal of personalised medical care faces serious challenges, many of which cannot be addressed through algorithmic complexity, and call for collaboration between traditional methodologists and experts in medical machine learning to avoid extensive research waste.",
        "DOI": "10.1016/S2589-7500(20)30200-4",
        "paper_author": "Wilkinson J.",
        "affiliation_name": "The University of Manchester",
        "affiliation_city": "Manchester",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60003771",
        "affiliation_state": "Greater Manchester"
    },
    {
        "paper_title": "Combining structured and unstructured data for predictive models: a deep learning approach",
        "publication": "BMC Medical Informatics and Decision Making",
        "citied_by": "141",
        "cover_date": "2020-12-01",
        "Abstract": "Background: The broad adoption of electronic health records (EHRs) provides great opportunities to conduct health care research and solve various clinical problems in medicine. With recent advances and success, methods based on machine learning and deep learning have become increasingly popular in medical informatics. However, while many research studies utilize temporal structured data on predictive modeling, they typically neglect potentially valuable information in unstructured clinical notes. Integrating heterogeneous data types across EHRs through deep learning techniques may help improve the performance of prediction models. Methods: In this research, we proposed 2 general-purpose multi-modal neural network architectures to enhance patient representation learning by combining sequential unstructured notes with structured data. The proposed fusion models leverage document embeddings for the representation of long clinical note documents and either convolutional neural network or long short-term memory networks to model the sequential clinical notes and temporal signals, and one-hot encoding for static information representation. The concatenated representation is the final patient representation which is used to make predictions. Results: We evaluate the performance of proposed models on 3 risk prediction tasks (i.e. in-hospital mortality, 30-day hospital readmission, and long length of stay prediction) using derived data from the publicly available Medical Information Mart for Intensive Care III dataset. Our results show that by combining unstructured clinical notes with structured data, the proposed models outperform other models that utilize either unstructured notes or structured data only. Conclusions: The proposed fusion models learn better patient representation by combining structured and unstructured data. Integrating heterogeneous data types across EHRs helps improve the performance of prediction models and reduce errors.",
        "DOI": "10.1186/s12911-020-01297-6",
        "paper_author": "Zhang D.",
        "affiliation_name": "The Ohio State University",
        "affiliation_city": "Columbus",
        "affiliation_country": "United States",
        "affiliation_id": "60003500",
        "affiliation_state": "OH"
    },
    {
        "paper_title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations",
        "publication": "Computer Graphics Forum",
        "citied_by": "141",
        "cover_date": "2020-06-01",
        "Abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.",
        "DOI": "10.1111/cgf.14034",
        "paper_author": "Chatzimparmpas A.",
        "affiliation_name": "Linnaeus University, Kalmar",
        "affiliation_city": "Kalmar",
        "affiliation_country": "Sweden",
        "affiliation_id": "60104372",
        "affiliation_state": "Kalmar"
    },
    {
        "paper_title": "Classification of stroke disease using machine learning algorithms",
        "publication": "Neural Computing and Applications",
        "citied_by": "141",
        "cover_date": "2020-02-01",
        "Abstract": "This paper presents a prototype to classify stroke that combines text mining tools and machine learning algorithms. Machine learning can be portrayed as a significant tracker in areas like surveillance, medicine, data management with the aid of suitably trained machine learning algorithms. Data mining techniques applied in this work give an overall review about the tracking of information with respect to semantic as well as syntactic perspectives. The proposed idea is to mine patients’ symptoms from the case sheets and train the system with the acquired data. In the data collection phase, the case sheets of 507 patients were collected from Sugam Multispecialty Hospital, Kumbakonam, Tamil Nadu, India. Next, the case sheets were mined using tagging and maximum entropy methodologies, and the proposed stemmer extracts the common and unique set of attributes to classify the strokes. Then, the processed data were fed into various machine learning algorithms such as artificial neural networks, support vector machine, boosting and bagging and random forests. Among these algorithms, artificial neural networks trained with a stochastic gradient descent algorithm outperformed the other algorithms with a higher classification accuracy of 95% and a smaller standard deviation of 14.69.",
        "DOI": "10.1007/s00521-019-04041-y",
        "paper_author": "Govindarajan P.",
        "affiliation_name": "SASTRA Deemed University",
        "affiliation_city": "Thanjavur",
        "affiliation_country": "India",
        "affiliation_id": "60005147",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Open access in silico tools to predict the ADMET profiling of drug candidates",
        "publication": "Expert Opinion on Drug Discovery",
        "citied_by": "139",
        "cover_date": "2020-12-01",
        "Abstract": "Introduction: We are in an era of bioinformatics and cheminformatics where we can predict data in the fields of medicine, the environment, engineering and public health. Approaches with open access in silico tools have revolutionized disease management due to early prediction of the absorption, distribution, metabolism, excretion, and toxicity (ADMET) profiles of the chemically designed and eco-friendly next-generation drugs. Areas covered: This review meticulously encompasses the fundamental functions of open access in silico prediction tools (webservers and standalone software) and advocates their use in drug discovery research for the safety and reliability of any candidate-drug. This review also aims to help support new researchers in the field of drug design. Expert opinion: The choice of in silico tools is critically important for drug discovery and the accuracy of ADMET prediction. The accuracy largely depends on the types of dataset, the algorithm used, the quality of the model, the available endpoints for prediction, and user requirement. The key is to use multiple in silico tools for predictions and comparing the results, followed by the identification of the most probable prediction.",
        "DOI": "10.1080/17460441.2020.1798926",
        "paper_author": "Kar S.",
        "affiliation_name": "College of Science, Engineering and Technology",
        "affiliation_city": "Jackson",
        "affiliation_country": "United States",
        "affiliation_id": "60279426",
        "affiliation_state": "MS"
    },
    {
        "paper_title": "Artificial intelligence education and tools for medical and health informatics students: Systematic review",
        "publication": "JMIR Medical Education",
        "citied_by": "137",
        "cover_date": "2020-01-01",
        "Abstract": "Background: The use of artificial intelligence (AI) in medicine will generate numerous application possibilities to improve patient care, provide real-time data analytics, and enable continuous patient monitoring. Clinicians and health informaticians should become familiar with machine learning and deep learning. Additionally, they should have a strong background in data analytics and data visualization to use, evaluate, and develop AI applications in clinical practice. Objective: The main objective of this study was to evaluate the current state of AI training and the use of AI tools to enhance the learning experience. Methods: A comprehensive systematic review was conducted to analyze the use of AI in medical and health informatics education, and to evaluate existing AI training practices. PRISMA-P (Preferred Reporting Items for Systematic Reviews and Meta-Analysis Protocols) guidelines were followed. The studies that focused on the use of AI tools to enhance medical education and the studies that investigated teaching AI as a new competency were categorized separately to evaluate recent developments. Results: This systematic review revealed that recent publications recommend the integration of AI training into medical and health informatics curricula. Conclusions: To the best of our knowledge, this is the first systematic review exploring the current state of AI education in both medicine and health informatics. Since AI curricula have not been standardized and competencies have not been determined, a framework for specialized AI training in medical and health informatics education is proposed.",
        "DOI": "10.2196/19285",
        "paper_author": "Hasan Sapci A.",
        "affiliation_name": "Adelphi University",
        "affiliation_city": "Garden City",
        "affiliation_country": "United States",
        "affiliation_id": "60022908",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "The impact of machine learning on patient care: A systematic review",
        "publication": "Artificial Intelligence in Medicine",
        "citied_by": "136",
        "cover_date": "2020-03-01",
        "Abstract": "Background: Despite the expanding use of machine learning (ML) in fields such as finance and marketing, its application in the daily practice of clinical medicine is almost non-existent. In this systematic review, we describe the various areas within clinical medicine that have applied the use of ML to improve patient care. Methods: A systematic review was performed in accordance with the PRISMA guidelines using Medline(R), EBM Reviews, Embase, Psych Info, and Cochrane Databases, focusing on human studies that used ML to directly address a clinical problem. Included studies were published from January 1, 2000 to May 1, 2018 and provided metrics on the performance of the utilized ML tool. Results: A total of 1909 unique publications were reviewed, with 378 retrospective articles and 8 prospective articles meeting inclusion criteria. Retrospective publications were found to be increasing in frequency, with 61 % of articles published within the last 4 years. Prospective articles comprised only 2 % of the articles meeting our inclusion criteria. These studies utilized a prospective cohort design with an average sample size of 531. Conclusion: The majority of literature describing the use of ML in clinical medicine is retrospective in nature and often outlines proof-of-concept approaches to impact patient care. We postulate that identifying and overcoming key translational barriers, including real-time access to clinical data, data security, physician approval of “black box” generated results, and performance evaluation will allow for a fundamental shift in medical practice, where specialized tools will aid the healthcare team in providing better patient care.",
        "DOI": "10.1016/j.artmed.2019.101785",
        "paper_author": "Ben-Israel D.",
        "affiliation_name": "Cumming School of Medicine",
        "affiliation_city": "Calgary",
        "affiliation_country": "Canada",
        "affiliation_id": "60000953",
        "affiliation_state": "AB"
    },
    {
        "paper_title": "Application of artificial intelligence technology in oncology: Towards the establishment of precision medicine",
        "publication": "Cancers",
        "citied_by": "134",
        "cover_date": "2020-12-01",
        "Abstract": "In recent years, advances in artificial intelligence (AI) technology have led to the rapid clinical implementation of devices with AI technology in the medical field. More than 60 AI-equipped medical devices have already been approved by the Food and Drug Administration (FDA) in the United States, and the active introduction of AI technology is considered to be an inevitable trend in the future of medicine. In the field of oncology, clinical applications of medical devices using AI technology are already underway, mainly in radiology, and AI technology is expected to be positioned as an important core technology. In particular, “precision medicine,” a medical treatment that selects the most appropriate treatment for each patient based on a vast amount of medical data such as genome information, has become a worldwide trend; AI technology is expected to be utilized in the process of extracting truly useful information from a large amount of medical data and applying it to diagnosis and treatment. In this review, we would like to introduce the history of AI technology and the current state of medical AI, especially in the oncology field, as well as discuss the possibilities and challenges of AI technology in the medical field.",
        "DOI": "10.3390/cancers12123532",
        "paper_author": "Hamamoto R.",
        "affiliation_name": "National Cancer Center Research Institute",
        "affiliation_city": "Chuo-ku",
        "affiliation_country": "Japan",
        "affiliation_id": "60023890",
        "affiliation_state": "Tokyo"
    },
    {
        "paper_title": "Machine learning and clinical epigenetics: A review of challenges for diagnosis and classification",
        "publication": "Clinical Epigenetics",
        "citied_by": "134",
        "cover_date": "2020-04-03",
        "Abstract": "Background: Machine learning is a sub-field of artificial intelligence, which utilises large data sets to make predictions for future events. Although most algorithms used in machine learning were developed as far back as the 1950s, the advent of big data in combination with dramatically increased computing power has spurred renewed interest in this technology over the last two decades. Main body: Within the medical field, machine learning is promising in the development of assistive clinical tools for detection of e.g. cancers and prediction of disease. Recent advances in deep learning technologies, a sub-discipline of machine learning that requires less user input but more data and processing power, has provided even greater promise in assisting physicians to achieve accurate diagnoses. Within the fields of genetics and its sub-field epigenetics, both prime examples of complex data, machine learning methods are on the rise, as the field of personalised medicine is aiming for treatment of the individual based on their genetic and epigenetic profiles. Conclusion: We now have an ever-growing number of reported epigenetic alterations in disease, and this offers a chance to increase sensitivity and specificity of future diagnostics and therapies. Currently, there are limited studies using machine learning applied to epigenetics. They pertain to a wide variety of disease states and have used mostly supervised machine learning methods.",
        "DOI": "10.1186/s13148-020-00842-4",
        "paper_author": "Rauschert S.",
        "affiliation_name": "The Kids Research Institute Australia",
        "affiliation_city": "Perth",
        "affiliation_country": "Australia",
        "affiliation_id": "60000096",
        "affiliation_state": "WA"
    },
    {
        "paper_title": "Medical image registration using deep neural networks: A comprehensive review",
        "publication": "Computers and Electrical Engineering",
        "citied_by": "130",
        "cover_date": "2020-10-01",
        "Abstract": "Image-guided interventions are saving the lives of a large number of patients where the image registration should indeed be considered as the most complex and complicated issue to be tackled. On the other hand, a huge progress in the field of machine learning has recently made by the possibility of implementing deep neural networks on the contemporary many-core GPUs. It has opened up a promising window to challenge with many medical applications in more efficient and effective ways, where the registration is not an exception. In this paper, a comprehensive review on the state-of-the-art literature known as medical image registration using deep neural networks is presented. The review is systematic and encompasses all the related works previously published in the field. Key concepts, statistical analysis from different points of view, confining challenges, novelties and main contributions, key-enabling techniques, future directions, and prospective trends all are discussed and surveyed in details in this comprehensive review. This review allows a deep understanding and insight for the readers active in the field who are investigating the state-of-the-art and seeking to contribute the future literature.",
        "DOI": "10.1016/j.compeleceng.2020.106767",
        "paper_author": "Boveiri H.R.",
        "affiliation_name": "Shiraz University of Technology",
        "affiliation_city": "Shiraz",
        "affiliation_country": "Iran",
        "affiliation_id": "60072431",
        "affiliation_state": "Fars"
    },
    {
        "paper_title": "Radiomics of <sup>18</sup>F-FDG PET/CT images predicts clinical benefit of advanced NSCLC patients to checkpoint blockade immunotherapy",
        "publication": "European Journal of Nuclear Medicine and Molecular Imaging",
        "citied_by": "130",
        "cover_date": "2020-05-01",
        "Abstract": "Introduction: Immunotherapy has improved outcomes for patients with non-small cell lung cancer (NSCLC), yet durable clinical benefit (DCB) is experienced in only a fraction of patients. Here, we test the hypothesis that radiomics features from baseline pretreatment 18F-FDG PET/CT scans can predict clinical outcomes of NSCLC patients treated with checkpoint blockade immunotherapy. Methods: This study included 194 patients with histologically confirmed stage IIIB-IV NSCLC with pretreatment PET/CT images. Radiomics features were extracted from PET, CT, and PET+CT fusion images based on minimum Kullback–Leibler divergence (KLD) criteria. The radiomics features from 99 retrospective patients were used to train a multiparametric radiomics signature (mpRS) to predict DCB using an improved least absolute shrinkage and selection operator (LASSO) method, which was subsequently validated in both retrospective (N = 47) and prospective test cohorts (N = 48). Using these cohorts, the mpRS was also used to predict progression-free survival (PFS) and overall survival (OS) by training nomogram models using multivariable Cox regression analyses with additional clinical characteristics incorporated. Results: The mpRS could predict patients who will receive DCB, with areas under receiver operating characteristic curves (AUCs) of 0.86 (95%CI 0.79–0.94), 0.83 (95%CI 0.71–0.94), and 0.81 (95%CI 0.68–0.92) in the training, retrospective test, and prospective test cohorts, respectively. In the same three cohorts, respectively, nomogram models achieved C-indices of 0.74 (95%CI 0.68–0.80), 0.74 (95%CI 0.66–0.82), and 0.77 (95%CI 0.69–0.84) to predict PFS and C-indices of 0.83 (95%CI 0.77–0.88), 0.83 (95%CI 0.71–0.94), and 0.80 (95%CI 0.69–0.91) to predict OS. Conclusion: PET/CT-based signature can be used prior to initiation of immunotherapy to identify NSCLC patients most likely to benefit from immunotherapy. As such, these data may be leveraged to improve more precise and individualized decision support in the treatment of patients with advanced NSCLC.",
        "DOI": "10.1007/s00259-019-04625-9",
        "paper_author": "Mu W.",
        "affiliation_name": "Moffitt Cancer Center",
        "affiliation_city": "Tampa",
        "affiliation_country": "United States",
        "affiliation_id": "60004803",
        "affiliation_state": "FL"
    },
    {
        "paper_title": "Automated markerless pose estimation in freely moving macaques with OpenMonkeyStudio",
        "publication": "Nature Communications",
        "citied_by": "129",
        "cover_date": "2020-12-01",
        "Abstract": "The rhesus macaque is an important model species in several branches of science, including neuroscience, psychology, ethology, and medicine. The utility of the macaque model would be greatly enhanced by the ability to precisely measure behavior in freely moving conditions. Existing approaches do not provide sufficient tracking. Here, we describe OpenMonkeyStudio, a deep learning-based markerless motion capture system for estimating 3D pose in freely moving macaques in large unconstrained environments. Our system makes use of 62 machine vision cameras that encircle an open 2.45 m × 2.45 m × 2.75 m enclosure. The resulting multiview image streams allow for data augmentation via 3D-reconstruction of annotated images to train a robust view-invariant deep neural network. This view invariance represents an important advance over previous markerless 2D tracking approaches, and allows fully automatic pose inference on unconstrained natural motion. We show that OpenMonkeyStudio can be used to accurately recognize actions and track social interactions.",
        "DOI": "10.1038/s41467-020-18441-5",
        "paper_author": "Bala P.C.",
        "affiliation_name": "College of Science and Engineering",
        "affiliation_city": "Minneapolis",
        "affiliation_country": "United States",
        "affiliation_id": "60152345",
        "affiliation_state": "MN"
    },
    {
        "paper_title": "Explainable Artificial Intelligence: Concepts, Applications, Research Challenges and Visions",
        "publication": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
        "citied_by": "129",
        "cover_date": "2020-01-01",
        "Abstract": "The development of theory, frameworks and tools for Explainable AI (XAI) is a very active area of research these days, and articulating any kind of coherence on a vision and challenges is itself a challenge. At least two sometimes complementary and colliding threads have emerged. The first focuses on the development of pragmatic tools for increasing the transparency of automatically learned prediction models, as for instance by deep or reinforcement learning. The second is aimed at anticipating the negative impact of opaque models with the desire to regulate or control impactful consequences of incorrect predictions, especially in sensitive areas like medicine and law. The formulation of methods to augment the construction of predictive models with domain knowledge can provide support for producing human understandable explanations for predictions. This runs in parallel with AI regulatory concerns, like the European Union General Data Protection Regulation, which sets standards for the production of explanations from automated or semi-automated decision making. Despite the fact that all this research activity is the growing acknowledgement that the topic of explainability is essential, it is important to recall that it is also among the oldest fields of computer science. In fact, early AI was re-traceable, interpretable, thus understandable by and explainable to humans. The goal of this research is to articulate the big picture ideas and their role in advancing the development of XAI systems, to acknowledge their historical roots, and to emphasise the biggest challenges to moving forward.",
        "DOI": "10.1007/978-3-030-57321-8_1",
        "paper_author": "Longo L.",
        "affiliation_name": "Technological University Dublin",
        "affiliation_city": "Dublin",
        "affiliation_country": "Ireland",
        "affiliation_id": "60012873",
        "affiliation_state": "Leinster"
    },
    {
        "paper_title": "Comparing deep learning architectures for sentiment analysis on drug reviews",
        "publication": "Journal of Biomedical Informatics",
        "citied_by": "129",
        "cover_date": "2020-10-01",
        "Abstract": "Since the turn of the century, as millions of user's opinions are available on the web, sentiment analysis has become one of the most fruitful research fields in Natural Language Processing (NLP). Research on sentiment analysis has covered a wide range of domains such as economy, polity, and medicine, among others. In the pharmaceutical field, automatic analysis of online user reviews allows for the analysis of large amounts of user's opinions and to obtain relevant information about the effectiveness and side effects of drugs, which could be used to improve pharmacovigilance systems. Throughout the years, approaches for sentiment analysis have progressed from simple rules to advanced machine learning techniques such as deep learning, which has become an emerging technology in many NLP tasks. Sentiment analysis is not oblivious to this success, and several systems based on deep learning have recently demonstrated their superiority over former methods, achieving state-of-the-art results on standard sentiment analysis datasets. However, prior work shows that very few attempts have been made to apply deep learning to sentiment analysis of drug reviews. We present a benchmark comparison of various deep learning architectures such as Convolutional Neural Networks (CNN) and Long short-term memory (LSTM) recurrent neural networks. We propose several combinations of these models and also study the effect of different pre-trained word embedding models. As transformers have revolutionized the NLP field achieving state-of-art results for many NLP tasks, we also explore Bidirectional Encoder Representations from Transformers (BERT) with a Bi-LSTM for the sentiment analysis of drug reviews. Our experiments show that the usage of BERT obtains the best results, but with a very high training time. On the other hand, CNN achieves acceptable results while requiring less training time.",
        "DOI": "10.1016/j.jbi.2020.103539",
        "paper_author": "Colón-Ruiz C.",
        "affiliation_name": "Universidad Carlos III de Madrid",
        "affiliation_city": "Getafe",
        "affiliation_country": "Spain",
        "affiliation_id": "60001741",
        "affiliation_state": "Madrid"
    },
    {
        "paper_title": "Deep learning workflow in radiology: a primer",
        "publication": "Insights into Imaging",
        "citied_by": "129",
        "cover_date": "2020-12-01",
        "Abstract": "Interest for deep learning in radiology has increased tremendously in the past decade due to the high achievable performance for various computer vision tasks such as detection, segmentation, classification, monitoring, and prediction. This article provides step-by-step practical guidance for conducting a project that involves deep learning in radiology, from defining specifications, to deployment and scaling. Specifically, the objectives of this article are to provide an overview of clinical use cases of deep learning, describe the composition of multi-disciplinary team, and summarize current approaches to patient, data, model, and hardware selection. Key ideas will be illustrated by examples from a prototypical project on imaging of colorectal liver metastasis. This article illustrates the workflow for liver lesion detection, segmentation, classification, monitoring, and prediction of tumor recurrence and patient survival. Challenges are discussed, including ethical considerations, cohorting, data collection, anonymization, and availability of expert annotations. The practical guidance may be adapted to any project that requires automated medical image analysis.",
        "DOI": "10.1186/s13244-019-0832-5",
        "paper_author": "Montagnon E.",
        "affiliation_name": "Centre Hospitalier de L'Université de Montréal",
        "affiliation_city": "Montreal",
        "affiliation_country": "Canada",
        "affiliation_id": "60009657",
        "affiliation_state": "QC"
    }
]