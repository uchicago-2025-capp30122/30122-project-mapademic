[
    {
        "paper_title": "Use of Natural Language Processing to Identify Sexual and Reproductive Health Information in Clinical Text",
        "publication": "Methods of Information in Medicine",
        "citied_by": "0",
        "cover_date": "2022-12-31",
        "Abstract": "Objectives This study aimed to enable clinical researchers without expertise in natural language processing (NLP) to extract and analyze information about sexual and reproductive health (SRH), or other sensitive health topics, from large sets of clinical notes. Methods (1) We retrieved text from the electronic health record as individual notes. (2) We segmented notes into sentences using one of scispaCy's NLP toolkits. (3) We exported sentences to the labeling application Watchful and annotated subsets of these as relevant or irrelevant to various SRH categories by applying a combination of regular expressions and manual annotation. (4) The labeled sentences served as training data to create machine learning models for classifying text; specifically, we used spaCy's default text classification ensemble, comprising a bag-of-words model and a neural network with attention. (5) We applied each model to unlabeled sentences to identify additional references to SRH with novel relevant vocabulary. We used this information and repeated steps 3 to 5 iteratively until the models identified no new relevant sentences for each topic. Finally, we aggregated the labeled data for analysis. Results This methodology was applied to 3,663 Child Neurology notes for 971 female patients. Our search focused on six SRH categories. We validated the approach using two subject matter experts, who independently labeled a sample of 400 sentences. Cohen's kappa values were calculated for each category between the reviewers (menstruation: 1, sexual activity: 0.9499, contraception: 0.9887, folic acid: 1, teratogens: 0.8864, pregnancy: 0.9499). After removing the sentences on which reviewers did not agree, we compared the reviewers' labels to those produced via our methodology, again using Cohen's kappa (menstruation: 1, sexual activity: 1, contraception: 0.9885, folic acid: 1, teratogens: 0.9841, pregnancy: 0.9871). Conclusion Our methodology is reproducible, enables analysis of large amounts of text, and has produced results that are highly comparable to subject matter expert manual review.",
        "DOI": "10.1055/a-2233-2736",
        "paper_author": "Harrison E.I.",
        "affiliation_name": "UPMC Children’s Hospital of Pittsburgh",
        "affiliation_city": "Pittsburgh",
        "affiliation_country": "United States",
        "affiliation_id": "60005708",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "GCNXSS: An Attack Detection Approach for Cross-Site Scripting Based on Graph Convolutional Networks",
        "publication": "KSII Transactions on Internet and Information Systems",
        "citied_by": "3",
        "cover_date": "2022-12-31",
        "Abstract": "Since machine learning was introduced into cross-site scripting (XSS) attack detection, many researchers have conducted related studies and achieved significant results, such as saving time and labor costs by not maintaining a rule database, which is required by traditional XSS attack detection methods. However, this topic came across some problems, such as poor generalization ability, significant false negative rate (FNR) and false positive rate (FPR). Moreover, the automatic clustering property of graph convolutional networks (GCN) has attracted the attention of researchers. In the field of natural language process (NLP), the results of graph embedding based on GCN are automatically clustered in space without any training, which means that text data can be classified just by the embedding process based on GCN. Previously, other methods required training with the help of labeled data after embedding to complete data classification. With the help of the GCN auto-clustering feature and labeled data, this research proposes an approach to detect XSS attacks (called GCNXSS) to mine the dependencies between the units that constitute an XSS payload. First, GCNXSS transforms a URL into a word homogeneous graph based on word co-occurrence relationships. Then, GCNXSS inputs the graph into the GCN model for graph embedding and gets the classification results. Experimental results show that GCNXSS achieved successful results with accuracy, precision, recall, F1-score, FNR, FPR, and predicted time scores of 99.97%, 99.75%, 99.97%, 99.86%, 0.03%, 0.03%, and 0.0461ms. Compared with existing methods, GCNXSS has a lower FNR and FPR with stronger generalization ability.",
        "DOI": "10.3837/tiis.2022.12.013",
        "paper_author": "Pan H.",
        "affiliation_name": "Sichuan University",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China",
        "affiliation_id": "60016521",
        "affiliation_state": "Sichuan"
    },
    {
        "paper_title": "A novel deep belief neural network model for abstractive text summarization",
        "publication": "Deep Learning for Cognitive Computing Systems: Technological Advancements and Applications",
        "citied_by": "0",
        "cover_date": "2022-12-31",
        "Abstract": "Everyday researchers present their research works in various formats such as research articles, reports, patents, and study chapters. All over the world, the number of submissions increases due to technology development that leads to innovations. Search results will provide numerous relevant results for a single word and it is quite difficult for beginners and also for experts to analyze the research work by reading the entire article. Text summarization has evolved to reduce this issue. It automatically examines the articles and extracts the important information from the research works. Due to specific technical aspects, conventional text summarization methods cannot be employed in the text summarization of research works. Researchers pay more attention to bring efficient scientific article text summarization and this research work aims to achieve the goal of text summarization using a deep belief neural network. Abstractive text summarization is proposed in the research work by addressing the importance of the research article summarization. This novel approach attains better performance than conventional text summarization models. The experiental results shows the superior performance of our approach.",
        "DOI": "10.1515/9783110750584-011",
        "paper_author": "Dineshnath G.",
        "affiliation_name": "Puducherry Technological University",
        "affiliation_city": "Puducherry",
        "affiliation_country": "India",
        "affiliation_id": "60005694",
        "affiliation_state": "PY"
    },
    {
        "paper_title": "Artificial intelligence techniques to understand braille: A language for visually impaired individuals",
        "publication": "Handbook of Research on Artificial Intelligence Applications in Literary Works and Social Media",
        "citied_by": "3",
        "cover_date": "2022-12-30",
        "Abstract": "The special-needs individuals have their own literary system called Braille for printing, reading, and writing since 1824 that assist them in easily connecting with each other. Because of the isolated education system, formally called the heterogeneous education system, they faced a lot of problems and challenges from society. Artificial intelligence tools and techniques play a vital role in mitigating communication gap between blind/visually impaired individuals and non-blind people. Thus, this chapter focuses on reviewing artificial intelligence techniques to understand Braille for normal people associated with visual imperative individuals. It is found that natural language processing (NLP) helps in translating native language into Braille. Machine learning and artificial neural network algorithms help in translating by matching the equivalent pattern of Braille. The concept of finite automata is used with natural language processing to recognize and convert the Braille pattern equivalent to their native language pattern and vice versa.",
        "DOI": "10.4018/978-1-6684-6242-3.ch012",
        "paper_author": "Ali S.A.",
        "affiliation_name": "Sindh Madressatul Islam University",
        "affiliation_city": "Karachi",
        "affiliation_country": "Pakistan",
        "affiliation_id": "60194965",
        "affiliation_state": "Sindh"
    },
    {
        "paper_title": "Artificial intelligence techniques in text and sentiment analysis",
        "publication": "Handbook of Research on Artificial Intelligence Applications in Literary Works and Social Media",
        "citied_by": "2",
        "cover_date": "2022-12-30",
        "Abstract": "Of late, text and sentiment analysis have become essential parts of modern marketing. These play a vital role in the division of natural language processing (NLP). It mainly focuses on text classification to examine the intention of the processed text; it can be of positive or negative types. Sentiment analysis dealt with the computational treatment of sentiments, opinions, and subjectivity of text. This chapter tackles a comprehensive approach for the past research solutions that includes various algorithms, enhancements, and applications. This chapter primarily focuses on three aspects. Firstly, the authors present a systematic review of recent works done in the area of text and sentiment analysis; second, they emphasize major concepts, components, functionalities, and classification techniques of text and sentiment analysis. Finally, they provide a comparative study of text and sentiment analysis on the basis of trending research approaches. They conclude the chapter with future directions.",
        "DOI": "10.4018/978-1-6684-6242-3.ch009",
        "paper_author": "Patruni M.R.",
        "affiliation_name": "Vellore Institute of Technology",
        "affiliation_city": "Vellore",
        "affiliation_country": "India",
        "affiliation_id": "60010618",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "An end-to-end framework for information extraction from Italian resumes",
        "publication": "Expert Systems with Applications",
        "citied_by": "23",
        "cover_date": "2022-12-30",
        "Abstract": "Nowadays, recruitment processes are increasingly being automated by intelligent systems which provide best candidates for companies’ open positions, and vice versa. However, extracting information from the unstructured documents involved in these processes (e.g. resumes, jobs’ descriptions) still represents an open challenge because of their high heterogeneity (in the form and style) and the lack of pre-defined standards between different companies and/or countries. In this paper, we address the resume information extraction problem, focusing on documents within the Italian Labor Market. Specifically, we propose an effective and efficient end-to-end framework capable of providing a complete candidate overview including his personal information, skills and work experiences. Specifically, after having extracted the raw data from the resume documents, the system segments them into semantically consistent parts using linguistics patterns. Each segment is further processed with a NER algorithm, based on pre-trained language models, to extract relevant information which an HR specialist could consult in order to assess the suitability of a candidate for a job offer. We collected (and labeled) a new Italian resume dataset and our results prove the effectiveness of the proposed method, especially considering the great advantages our segmentation strategy brings to the NER performance with respect to standard line-based segmentation approaches. In addition, our system achieves promising performance when combined with modern NLP models.",
        "DOI": "10.1016/j.eswa.2022.118487",
        "paper_author": "Barducci A.",
        "affiliation_name": "Via Giovanni Porzio",
        "affiliation_city": "Naples",
        "affiliation_country": "Italy",
        "affiliation_id": "128469197",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Large scale analysis of open MOOC reviews to support learners’ course selection",
        "publication": "Expert Systems with Applications",
        "citied_by": "19",
        "cover_date": "2022-12-30",
        "Abstract": "The recent pandemic has changed the way we see education. During recent years, Massive Open Online Course (MOOC) providers, such as Coursera or edX, are reporting millions of new users signing up on their platforms. Though online review systems are standard among many verticals, no standardized or fully decentralized review systems exist in the MOOC ecosystem. In this vein, we believe that there is an opportunity to leverage available open MOOC reviews in order to build simpler and more transparent reviewing systems, allowing users to really identify the best courses out there. Specifically, in our research we analyze 2.4 million reviews (which is the largest MOOC reviews dataset used until now) from five different platforms in order to determine the following: (1) if the numeric ratings provide discriminant information to learners, (2) if NLP-driven sentiment analysis on textual reviews could provide valuable information to learners, (3) if we can leverage NLP-driven topic finding techniques to infer themes that could be important for learners, and (4) if we can use these models to effectively characterize MOOCs based on the open reviews. Results show that numeric ratings are clearly biased (63% of them are 5-star ratings), and the topic modeling reveals some interesting topics related with course advertisements, the real applicability, or the difficulty of the different courses.",
        "DOI": "10.1016/j.eswa.2022.118400",
        "paper_author": "Gomez M.J.",
        "affiliation_name": "Universidad de Murcia",
        "affiliation_city": "Murcia",
        "affiliation_country": "Spain",
        "affiliation_id": "60000130",
        "affiliation_state": "Murcia"
    },
    {
        "paper_title": "Neuro Linguistic Programming for Academic Cheating in Mathematics Classes",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2022-12-29",
        "Abstract": "This research aims to determine and identify the factors responsible for academic cheating in mathematics learning using the Neuro Linguistic Programming (NLP). This qualitative research with the Miles and Huberman data analysis techniques consists of reduction, presentation, and drawing conclusions used as a case study design. Data were collected from students of one of the schools in Majalengka City using interview techniques and still image data collection. The results showed 14 forms of academic cheating in mathematics learning classified into plagiarism, fabrication, exploitation of other people's weaknesses, wrong cooperation, attempts to cheat before the test, use of prohibited tools during the exam, and manipulation of assignments. A total of 12 factors trigger academic cheating in mathematics learning. These include internal, external, learning, and academic cheating resistance factors. The identification results using NLP showed some students do not aim to commit in academic cheating. However, a total of 9 obstacles were in accordance with their viewpoints using the frame and metamodels. Therefore, students assume academic cheating is an appropriate action to respond to a situation.",
        "DOI": "10.1063/5.0102425",
        "paper_author": "Alwan F.F.",
        "affiliation_name": "Universitas Pendidikan Indonesia",
        "affiliation_city": "Bandung",
        "affiliation_country": "Indonesia",
        "affiliation_id": "60103797",
        "affiliation_state": "West Java"
    },
    {
        "paper_title": "Deception Detection Within and Across Domains: Identifying and Understanding the Performance Gap",
        "publication": "Journal of Data and Information Quality",
        "citied_by": "2",
        "cover_date": "2022-12-28",
        "Abstract": "NLP approaches to automatic deception detection have gained popularity over the past few years, especially with the proliferation of fake reviews and fake news online. However, most previous studies of deception detection have focused on single domains. We currently lack information about how these single-domain models of deception may or may not generalize to new domains. In this work, we conduct empirical studies of cross-domain deception detection in five domains to understand how current models perform when evaluated on new deception domains. Our experimental results reveal a large gap between within and across domain classification performance. Motivated by these findings, we propose methods to understand the differences in performances across domains. We formulate five distance metrics that quantify the distance between pairs of deception domains. We experimentally demonstrate that the distance between a pair of domains negatively correlates with the cross-domain accuracies of the domains. We thoroughly analyze the differences in the domains and the impact of fine-tuning BERT based models by visualization of the sentence embeddings. Finally, we utilize the distance metrics to recommend the optimal source domain for any given target domain. This work highlights the need to develop robust learning algorithms for cross-domain deception detection that generalize and adapt to new domains and contributes toward that goal.",
        "DOI": "10.1145/3561413",
        "paper_author": "Panda S.",
        "affiliation_name": "The City University of New York",
        "affiliation_city": "New York",
        "affiliation_country": "United States",
        "affiliation_id": "60007033",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Transformer Based Implementation for Automatic Book Summarization",
        "publication": "International Journal of Intelligent Systems and Applications in Engineering",
        "citied_by": "2",
        "cover_date": "2022-12-27",
        "Abstract": "Document Summarization is the procedure of generating a meaningful and concise summary of a given document with the inclusion of relevant and topic-important points. There are two approaches-one is picking up the most relevant statements from the document itself and adding it to the Summary known as Extractive and the other is generating sentences for the Summary known as Abstractive Summarization. Training a machine learning model to perform tasks that are time-consuming or very difficult for humans to evaluate is major challenge. Book summarization is one of the complex tasks which is time consuming as well. Traditional machine learning models are getting modified with pre-trained transformers. Transformer based Language models trained in a self-supervised fashion gaining a lot of attention when fine-tuned for Natural Language Processing(NLP) downstream task like text summarization. This work is an attempt to use Transformer based technique for Book Summarization.",
        "DOI": "NA",
        "paper_author": "Porwal S.",
        "affiliation_name": "Vishwakarma Institute of Information Technology",
        "affiliation_city": "Pune",
        "affiliation_country": "India",
        "affiliation_id": "60102227",
        "affiliation_state": "MH"
    },
    {
        "paper_title": "Architectural Design of aChatbotused For Artificial Intelligence with NLP Classification using Deep Learning",
        "publication": "International Journal of Intelligent Systems and Applications in Engineering",
        "citied_by": "1",
        "cover_date": "2022-12-27",
        "Abstract": "A chatbot is a piece of technology that uses natural language to mimic human behaviour. In order to improve customer service and happiness, there are many types of chatbots that can be employed as conversational agents in different business fields. This research proposed novel technique in chatbot data based NLP classification utilizing DL structures. Here the information has been gathered from chatbot and handled for clamor expulsion and standardization. The handled information has been highlight extricated with ordered utilizing Bi-LSTM based Intermittent brain organizations. The trial examination has been completed regarding exactness, accuracy, F-1 score.The trials on two data sets of articles revealed that employing natural language processing and the suggested technique established its viability for creating an automatic categorization system of articles with an accuracy of above 91%.",
        "DOI": "10.18201/ijisae.2022.276",
        "paper_author": "Kohli R.",
        "affiliation_name": "Bhagwant University",
        "affiliation_city": "Ajmer",
        "affiliation_country": "India",
        "affiliation_id": "60282776",
        "affiliation_state": "RJ"
    },
    {
        "paper_title": "BERT based Hierarchical Alternating Co-Attention Visual Question Answering using Bottom-Up Features",
        "publication": "International Journal of Intelligent Systems and Applications in Engineering",
        "citied_by": "1",
        "cover_date": "2022-12-27",
        "Abstract": "Answering a question from a given visual image is a very well-known vision language task where the machine is given a pair of an image and a related question and the task is to generate the natural language answer. Humans can easily relate image content with a given question and reason about how to generate an answer. But automation of this task is challenging as it involves many computer vision and NLP tasks. Most of the literature focus on a novel attention mechanism for joining image and question features ignoring the importance of improving the question feature extraction module. Transformers have changed the way spatial and temporal data is processed. This paper exploits the power of Bidirectional Encoder Representation from Transformer (BERT) as a powerful question feature extractor for the VQA model. A novel method of extracting question features by combining output features from four consecutive encoders of BERT has been proposed. This is from the fact that each encoder layer of the transformer attends to features from the word to a phrase and ultimately to a sentence-level representation. A novel BERT-based hierarchical alternating co-attention VQA using the Bottom-up features model has been proposed. Our model is evaluated on the publicly available benchmark dataset VQA v2.0 and experimental results prove that the model improves upon two baseline models by 9.37% and 0.74% respectively.",
        "DOI": "NA",
        "paper_author": "Koshti D.",
        "affiliation_name": "Sir Padampat Singhania University",
        "affiliation_city": "Udaipur",
        "affiliation_country": "India",
        "affiliation_id": "60094076",
        "affiliation_state": "RJ"
    },
    {
        "paper_title": "Script Event Prediction via Multilingual Event Graph Networks",
        "publication": "ACM Transactions on Asian and Low-Resource Language Information Processing",
        "citied_by": "3",
        "cover_date": "2022-12-27",
        "Abstract": "Predicting what happens next in text plays a critical role in building NLP applications. Many methods including count-based and neural-network-based have been proposed to tackle the task called script event prediction: predicting the most suitable subsequent event from a candidate list given a chain of narrative events (context). However, two problems including event ambiguity and evidence bias hinder the performance of these monolingual approaches. The former means that some events in the event chain are ambiguous. The latter means that both the wrong and correct candidate events can obtain sufficient support from the event context. In this article, we propose a novel multilingual approach to address two issues simultaneously. Specifically, to alleviate the event ambiguity problem, we project the monolingual event chains to parallel cross-lingual event chains, which can provide complementary information for monolingual event disambiguation. To deal with the evidence bias problem, we construct two monolingual event graphs and a cross-lingual event aligned graph to fully explore connections between events. What's more, we design a graph attention mechanism to model the confidence of the complement clues, which controls the information integration from various languages. By modeling the events with graphs instead of pairs or chains, the model can compare the candidate subsequent events simultaneously and choose the more suitable subsequent event as the final answer. Extensive experiments were conducted on the widely used New York Times corpus for script event prediction task and experimental results show that our approach outperforms previous models.",
        "DOI": "10.1145/3557893",
        "paper_author": "Zhou B.",
        "affiliation_name": "University of Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60027363",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Survey on NLP Resources, Tools, and Techniques for Marathi Language Processing",
        "publication": "ACM Transactions on Asian and Low-Resource Language Information Processing",
        "citied_by": "15",
        "cover_date": "2022-12-27",
        "Abstract": "Natural Language Processing (NLP) has been in practice for the past couple of decades, and extensive work has been done for the Western languages, particularly the English language. The Eastern counterpart, especially the languages of the Indian subcontinent, needs attention as not much language processing work has been done on these languages. Western languages are rich in dictionaries, WordNet, and associated tools, while Indian languages are lagging behind in this segment. Marathi is the third most spoken language in India and the 15th most spoken language worldwide. Lack of resources, complex linguistic facts, and the inclusion of prevalent dialects of neighbors have resulted in limited work for Marathi. The aim of this study is to provide an insight into the various linguistic resources, tools, and state-of-The-Art techniques applied to the processing of the Marathi language. Initially, morphological descriptions of the Marathi language are provided, followed by a discussion on the characteristics of the Marathi language. Thereafter, for Marathi language, the availability of corpus, tools, and techniques to be used to develop NLP tasks is reviewed. Finally, gap analysis is discussed in current research and future directions for this new and dynamic area of research are listed that will benefit the Marathi Language Processing research community.",
        "DOI": "10.1145/3548457",
        "paper_author": "Lahoti P.",
        "affiliation_name": "Malaviya National Institute of Technology Jaipur",
        "affiliation_city": "Jaipur",
        "affiliation_country": "India",
        "affiliation_id": "60017757",
        "affiliation_state": "RJ"
    },
    {
        "paper_title": "Amide cluster induced and hydrogen bonding regulated luminescence of linear aliphatic polyamide 1212 with a long alkane chain",
        "publication": "Polymer Chemistry",
        "citied_by": "7",
        "cover_date": "2022-12-27",
        "Abstract": "Luminescence of non-conjugated polymers attracts much attention due to its importance in fundamental and applied research. At present, however, the research on nonconventional luminescent polymers (NLPs) in terms of both the structure regulation and emission mechanism seems to be still in its infancy. Herein, we report for the first time that PA1212, a representative engineering plastic with the longest saturated carbon chain among commercial polyamides, is an efficient NLP, which exhibits dual emission of blue fluorescence and persistent room-temperature green phosphorescence as well as high quantum yield (∼13%). Systematic photophysical characterization demonstrates the important role played by hydrogen-bonded amide groups in the emission, and the emission intensity shows a clear linear positive correlation with the amide clusters constructed from ordered hydrogen bonds in the crystalline region. Moreover, PA1212 in the α form with linearly extended hydrogen-bonded clusters exhibits longer emission wavelengths than in the γ form, owing to the narrower band gap between the valence band maximum and the conduction band minimum. Further variable temperature fluorescence studies reconfirm the amide cluster-triggered emission (CTE) mechanism of PA1212, and show the possibility of application of fluorescence spectra in the study of polyamide aggregation states. This work not only deepens the understanding of CTE, but also provides a new strategy to regulate the luminescence of polyamides, and meanwhile, highlights the application prospects of luminescent polyamides.",
        "DOI": "10.1039/d2py01173e",
        "paper_author": "Yan X.",
        "affiliation_name": "Zhengzhou University",
        "affiliation_city": "Zhengzhou",
        "affiliation_country": "China",
        "affiliation_id": "60018554",
        "affiliation_state": "Henan"
    },
    {
        "paper_title": "Graph learning and network science for natural language processing",
        "publication": "Graph Learning and Network Science for Natural Language Processing",
        "citied_by": "3",
        "cover_date": "2022-12-27",
        "Abstract": "Advances in graph-based natural language processing (NLP) and information retrieval tasks have shown the importance of processing using the Graph of Words method. This book covers recent concrete information, from the basics to advanced level, about graph-based learning, such as neural network-based approaches, computational intelligence for learning parameters and feature reduction, and network science for graph-based NPL. It also contains information about language generation based on graphical theories and language models.",
        "DOI": "10.1201/9781003272649",
        "paper_author": "Garg M.",
        "affiliation_name": "University of Florida",
        "affiliation_city": "Gainesville",
        "affiliation_country": "United States",
        "affiliation_id": "60013959",
        "affiliation_state": "FL"
    },
    {
        "paper_title": "Application of NLP using graph approaches",
        "publication": "Graph Learning and Network Science for Natural Language Processing",
        "citied_by": "1",
        "cover_date": "2022-12-27",
        "Abstract": "NA",
        "DOI": "10.1201/9781003272649-2",
        "paper_author": "Yadav N.S.",
        "affiliation_name": "Manipal University Jaipur",
        "affiliation_city": "Jaipur",
        "affiliation_country": "India",
        "affiliation_id": "60108737",
        "affiliation_state": "RJ"
    },
    {
        "paper_title": "Identifying Actionable Information from Online Reviews",
        "publication": "Data Analysis and Knowledge Discovery",
        "citied_by": "0",
        "cover_date": "2022-12-25",
        "Abstract": "[Objective] This paper explores methods automatically identifying actionable information from online reviews, aiming to help practitioners improve their follow-up work. [Methods] We defined our task as a sentence-level classification procedure, and proposed a span-based model (SAII). First, we encoded the input sentences based on BERT to generate token-level representation. Then, we enumerated all possible spans from the given sentences and generated informative representations with the help of attention mechanism. Third, we proposed a multi-channel filtering strategy to preserve spans close to the key element prototypes. Finally, we merged the refined span-level and context representations to predict actionable information. [Results] We examined the SAII model with two real-world datasets and found it yielded satisfactory results. Compared with the three best existing models, SAII’s F1 value increased by 7.91%/5.42%, 2.10%/2.73%, and 1.94%/1.46%. [Limitations] More research is needed to evaluate the effectiveness of our new model on multimodal datasets of different domains. [Conclusions] The SAII model could effectively identify actionable information from user-generated contents.",
        "DOI": "10.11925/infotech.2096-3467.2022.0109",
        "paper_author": "Shang L.",
        "affiliation_name": "Ltd.",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "126709692",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A novel TCNN–Bi-LSTM deep learning model for predicting sentiments of tweets about COVID-19 vaccines",
        "publication": "Concurrency and Computation: Practice and Experience",
        "citied_by": "5",
        "cover_date": "2022-12-25",
        "Abstract": "Many researchers in various disciplines have focused on extracting meaningful information from social media platforms in recent years. Identification of behaviors and emotions from user posts is examined under the heading of sentiment analysis (SA) studies using the natural language processing (NLP) techniques. In this study, a novel TCNN-Bi-LSTM model using the two-stage convolutional neural network (TCNN) and bidirectional long short-term memory (Bi-LSTM) architectures was proposed. While TCNN layers enable the extraction of strong local features, the output of these layers feeds the Bi-LSTM model that remembers forward-looking information and capture long-term dependencies. In this study, first, preprocessing steps were applied to the raw dataset. Thus, strong features were extracted from the obtained quality dataset using the FastText word embedding technique that pre-trained with location-based and sub-word information features. The experimental results of the proposed method are promising compared to the baseline deep learning and machine learning models. Also, experimental results show that while the FastText data embedding technique achieves the best performance compared to other word embedding techniques in all deep learning classification models, it has not had the same outstanding success in machine learning models. This study aims to investigate the sentiments of tweets about the COVID-19 vaccines and comments on these tweets among Twitter users by using the power of Twitter data. A new dataset collected from Twitter was constructed to be used in experimental results. This study will facilitate detecting inappropriate, incomplete, and erroneous information about vaccination. The results of this study will enable society to broaden its perspective on the administered vaccines. It can also assist the government and healthcare agencies in planning and implementing the vaccination's promotion on time to achieve the herd immunity provided by the vaccination.",
        "DOI": "10.1002/cpe.7387",
        "paper_author": "Aslan S.",
        "affiliation_name": "Malatya Turgut Ozal University",
        "affiliation_city": "Malatya",
        "affiliation_country": "Turkey",
        "affiliation_id": "60117180",
        "affiliation_state": "Malatya"
    },
    {
        "paper_title": "A Survey on Various Approaches Used in Named Entity Recognition for Indian Languages",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2022-12-23",
        "Abstract": "Named Entity Recognition (NER)\"is a application of Artificial Intelligence , Machine Learning and \"Natural Language Processing (NLP\").In NER, various classes of Named entity such as name of a person , an organization name, name of location, name of designation etc., are find out which is required in many NLP activities like question-answering system ,machine translation, artificial intelligence, summarization of documents, academics, robotics, Bioinformatics etc. Mostly NER task was evident for foreign languages but for Indian constitutional languages, due to some challenges present for example scarcity of resources, ambiguity present in languages, morphologically rich behavior of languages etc. ,NER work has been done for few of languages. In our paper, we presented several challenges available in NER for Indian languages and compared them by measuring various standard evaluation metric values like precision, recall and F-measure. In future extension, we would develop a efficient system, which would be more accurate, and which will cater many more Named Entity tags than existing systems ,for Indian languages NER tools.",
        "DOI": "10.1145/3590837.3590892",
        "paper_author": "Vijayvergia R.",
        "affiliation_name": "Govt. Khetan Polytechnic College",
        "affiliation_city": "Jaipur",
        "affiliation_country": "India",
        "affiliation_id": "112293287",
        "affiliation_state": "RJ"
    },
    {
        "paper_title": "A Knowledge Reasoning Model Based on Non-Factoid Information Enhancement",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2022-12-23",
        "Abstract": "Q&A system plays an increasingly important role in the modern society with information explosion, and knowledge reasoning model (KRM) is the main research content of Q&A system. Existing knowledge reasoning models are mainly divided into text-based information retrieval (IR) and knowledge graph embedding (KGE). KGE is superior to IR in terms of storage and reasoning capabilities for massive factoid information, but lacks the ability to reason non-factoid information, merely focus on the mining of structural information without the semantic information. We proposed a knowledge reasoning model based on non-factoid information enhancement (NFE-KRM) in scenic Q&A. It realizes the KGE integrates semantic information (SIKGE) and the unified semantic embedding space (USES), so that NFE-KRM has the ability to answer both factoid and non-factoid questions. We have used a large number of experiments to prove that SIKGE gets a better performance on Mean Rank and Hits@10. NFE-KRM's F1 score and accuracy on the mixed dataset are both competitive.",
        "DOI": "10.1145/3582197.3582250",
        "paper_author": "Xu S.",
        "affiliation_name": "Beijing University of Posts and Telecommunications",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60016930",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Automated Disease Detection Based on Clinical Text Using Topic Modeling",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2022-12-23",
        "Abstract": "The applications of artificial intelligence have become increasingly more common in improving medical diagnosis. This research paper proposes an automated disease detection system using topic modeling with LDA. We select the 10 most common diseases in Samut Prakan city: flu, gastroesophageal reflux, common cold, gastritis, diarrhea, appendicitis, pneumonia, pharyngitis, bronchitis, and irritable bowel syndrome. In the proposed model, the data or symptoms are feeded into website application, the analysis is then performed in real-time using our model which was trained on the chief complaints dataset using NLP, topic modeling, and LDA techniques, and finally, the disease detection results are shown in the website application. The system can help in screening diseases and comparative analysis indicates that our model can help doctors to give timely diagnosis.",
        "DOI": "10.1145/3582197.3582209",
        "paper_author": "Yochum P.",
        "affiliation_name": "Ever Medical Technologies",
        "affiliation_city": null,
        "affiliation_country": null,
        "affiliation_id": "129457257",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Causal Pattern Representation Learning for Extracting Causality from Literature",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2022-12-23",
        "Abstract": "Extracting causality from literature has become an important task due to the essential role of causality. Traditional methods use pattern matching to extract causality, requiring domain knowledge and extensive human effort. Recent researches focus on utilizing pre-trained language models due to their success in Natural Language Processing (NLP). However, long sentences in literature hinders the performance of causality extraction. In this paper, we propose to focus on the representation of causal virtual pattern <head_entity, causal_virtual_trigger, tail_entity> and design a Causal Pattern Representation Learning (CPRL) method to tackle this challenge. For the causal_virtual_trigger representation, CPRL applies the attention mechanism on the shortest dependency path between entities to filter irrelevant information. For the head_entity and tail_entity representation, CPRL applies graph convolution networks to encode word dependency on entities. By crawling health-related literature abstracts, we create a new causality extraction dataset, namely HealthCE, with a size of 3479. Experiments on HealthCE demonstrate the effectiveness of our approach over existing causality extraction and general relation extraction baselines on the task of causality extraction.",
        "DOI": "10.1145/3578741.3578787",
        "paper_author": "Yang J.",
        "affiliation_name": "Hefei University of Technology",
        "affiliation_city": "Hefei",
        "affiliation_country": "China",
        "affiliation_id": "60002836",
        "affiliation_state": "Anhui"
    },
    {
        "paper_title": "The Application of Natural Language Processing and tspDB to Predict Future Bitcoin Price",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2022-12-23",
        "Abstract": "Formerly, predictions of time series were usually based on numbers. In some cases, however, one has to make predictions based on linguistic information. In this paper, we experiment with time series forecasting based on textual information. We combine sentiment analysis with time series forecasting to predict fluctuations in variables that change over time. Due to the time-sensitivity of the target variable, models from the Time Series Prediction Database (tspDB) are used to infer the value of Bitcoin. We implemented an innovative architecture that accepts text as input and produces numerical predictions for certain values. The market value of bitcoin was used to verify the applicability of the architecture. The program inputs Twitter posts and outputs the market value of bitcoin for the next few days. The program first calculates a sentiment score (which reflects social media confidence in the price of bitcoin) and then makes a multivariate time series prediction of the price of bitcoin. The results can provide some insight into how the price of bitcoin fluctuates. The purpose of using Bitcoin as our primary benchmark is that we want to demonstrate a sound application of the combination of NLP and tspDB, and while Bitcoin price is not an objective variable, the method we apply can also be used in other situations that contain linguistic information as input.",
        "DOI": "10.1145/3578741.3578798",
        "paper_author": "Bao K.",
        "affiliation_name": "Hangzhou International School",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China",
        "affiliation_id": "113871776",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Assistance System for Judicial Awards for the Colombian State Legal Defense Agency Through NLP -ANDJE",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2022-12-23",
        "Abstract": "The present work proposes a legal assistance tool based on NLP and a predictive ML model adapted to the particularities of Colombian Justice system for the Legal Defense Agency of the Colombian State (ANDJE), It is developed in 4 parts associated with specific requirements: An Exploratory Data Analysis of the available information, an NLP pre-processing of large volumes of information structured in databases and not structured in scanned PDFs, development of predictive models and user assistance tool based in Transformers. Combining in an original and innovative way predictive, information presentation, similarity searching engine and summary, Question Answering modules for treatment of large legal texts, to increase the probability of success in defending the interests of the Colombian state. Considerations for future works.",
        "DOI": "10.1145/3578741.3578793",
        "paper_author": "Hernandez Gonzalez J.A.",
        "affiliation_name": "Centre Borelli",
        "affiliation_city": "Gif-sur-Yvette",
        "affiliation_country": "France",
        "affiliation_id": "60180341",
        "affiliation_state": "Ile-de-France"
    },
    {
        "paper_title": "Multi-Round Extraction and Dynamic Role Selection Framework for Document-Level Event Extraction",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2022-12-23",
        "Abstract": "Document-level Event Extraction (DEE) aims to extract structured event information from a document, which is an indispensable downstream task for many NLP applications. Argument-scatter and multi-event are its two main challenges. Recent work decomposes this challenging DEE task into multiple steps such as entity recognition, contextual information modeling, and event arguments extraction. Besides, they all extract event arguments in a predefined fixed role order. Though it is effective, its cumbersome steps and fixed extraction order will bring about the problem of error propagation. To address this issue, we propose a Multi-round Extraction and Dynamic Role Selection (MREDRS) Framework for the DEE task. We model the DEE task in an end-to-end manner to avoid these cumbersome steps. Multi-round extraction can deal with multi-event problem. In order to avoid the error propagation problem caused by the fixed role extraction order, we dynamically select the next role according to the current extraction state. We conducted experiments on the commonly used DEE dataset and extensive experimental results demonstrated the effectiveness of our method.",
        "DOI": "10.1145/3578741.3578768",
        "paper_author": "Zhang K.",
        "affiliation_name": "Beijing University of Posts and Telecommunications",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60016930",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "I don't feel so good! Detecting Depressive Tendencies using Transformer-based Multimodal Frameworks",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "2",
        "cover_date": "2022-12-23",
        "Abstract": "One of the most common mental illnesses that affects 5% of adults globally is depression. The advancement of social media has meant that more and more people have gained a platform to voice their thoughts and beliefs. People's social media interactions and posted content can be used to infer critical characteristics such as depressive tendencies which will allow for timely intervention and help. This paper describes a novel supervised approach to detect depressive tendencies in Twitter users using multimodal frameworks which account for user interaction and online behaviour in addition to the tweet content processed using transformers like BERT. The performance of three multimodal frameworks is described with different methods for combining modalities. The best result is obtained a cross-modality based model which improves the baseline by 12% points.",
        "DOI": "10.1145/3578741.3578817",
        "paper_author": "Suri M.",
        "affiliation_name": "Netaji Subhas University of Technology",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India",
        "affiliation_id": "60010633",
        "affiliation_state": "DL"
    },
    {
        "paper_title": "BERT-based Transformer Fine-Tuning for Dutch Wikidata Question-Answering",
        "publication": "Computational Linguistics in the Netherlands Journal",
        "citied_by": "0",
        "cover_date": "2022-12-22",
        "Abstract": "People rely on data to understand the world and inform their decision-making. However, effective access to data has become more challenging over time: data has increased in volume and velocity, as has its variability in truthfulness, utility, and format. Therefore, improving our interfaces to data has become a pressing issue. One type of interface has lately gained renewed attention, driven by advances in artificial intelligence: natural language interfaces. As of yet, though, improvements in natural language processing (nlp) have largely concentrated on English. Thus, we propose a text-based Dutch question-Answering (qa) interface for accessing information on Wikidata (url: https://www.wikidata.org/), driven by a Dutch-To-sparql bert-based transformer model. Said transformer is a type of encoder-decoder model characterised by use of self-Attention. In our application, it is trained to accept sentences in Dutch and to transform these into corresponding sparql queries. By subsequently evaluating the obtained queries at a knowledge base, users can retrieve answers to their questions. Since our model learns end-To-end, we need to train it using a dataset consisting of pairs of Dutch questions and sparql queries. To this end, we closely follow the procedure of Cui et al. (2021). Particularly, we create a Dutch machine-Translated version of LC-QuAD 2.0 (Dubey et al. 2019) and apply entity and relation masking on the nl inputs and sparql outputs for increased generality, producing a dataset with 2,648 examples. We then let the transformer model fine-Tune on the training subset of this dataset, using system-level bleu score as the performance measure. Our final transformer configuration obtains a test bleu score of 51.86, which seems to be in line with results found by Cui et al. (2021). Additionally, we conduct a qualitative analysis of our model s outputs, focusing especially on situations where the predicted sparql queries are incorrect. Here, we observe that queries involving infrequently-used sparql keywords and queries containing literals prove challenging to the transformer, as sometimes do the syntax of sparql and the general length of queries. Finally, we conclude our paper by proposing some potential future directions for our Dutch qa system.1.",
        "DOI": "NA",
        "paper_author": "De Jong N.",
        "affiliation_name": "Rijksuniversiteit Groningen",
        "affiliation_city": "Groningen",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60010023",
        "affiliation_state": "Groningen"
    },
    {
        "paper_title": "Responsibility Framing under the Magnifying Lens of NLP: The Case of Gender-based Violence and Traffic Danger",
        "publication": "Computational Linguistics in the Netherlands Journal",
        "citied_by": "2",
        "cover_date": "2022-12-22",
        "Abstract": "We introduce a framework for the computational analysis of how responsibility is framed in the reporting of two types of socially relevant events: gender-based violence (specifically, femicides in the Italian press), and traffic danger (specifically, traffic crashes in Dutch and Flemish news reports). We advocate for the parallel analysis of these two phenomena under the same theoretical framework, which draws on Frame Semantics, Critical Discourse Analysis and Natural Language Processing. Reusing two existing event-Text datasets we show how computational experiments and the resulting analyses can be run. This work supports the testing and development of tools for NLP practitioners, as well as large-scale linguistic analyses for activists and journalists, in the context of socially impacting events.",
        "DOI": "NA",
        "paper_author": "Minnema G.",
        "affiliation_name": "Rijksuniversiteit Groningen",
        "affiliation_city": "Groningen",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60010023",
        "affiliation_state": "Groningen"
    },
    {
        "paper_title": "Understanding User Requirements for Smart Cockpit of New Energy Vehicles: A Natural Language Process Approach",
        "publication": "SAE Technical Papers",
        "citied_by": "2",
        "cover_date": "2022-12-22",
        "Abstract": "The smart cockpit has become an irreplaceable element for many new automobile brands, particularly New Energy Vehicles (NEV) of \"new forces\". Since the cockpit is a direct interface for the interactions between users and the intelligent and connected functions of the vehicle, any improvements would be easily perceived by users and thus would directly affect user experiences. It would be most important to capture, collect, and understand what users need for a smart cockpit. Users' online comments on existing smart cockpits contain information on users' requirements. However, the current user comment text data is too massive, tanglesome, and sparse to process. How to efficiently mine valuable information from these data is non-trivial. This paper focuses on applying the Natural Language Process (NLP) technology for design, development, improvement, and update of a vehicle company's smart cockpit. By obtaining user comment data from various sources such as eco-system Applications (APP), forums, posts, Questions and Answers(Q&A ), customer services, etc., we aim to mine and quantify user demand for the smart cockpit. A deep learning NLP model named Bidirectional Encoder Representations from Transformers (BERT) is developed. In addition, the incremental pre-trained BERT is proposed to predict the mentioned cockpit feature, user's intention, and emotion from a comment, with one year's data from our cooperated NEV company for model training. Experiment results showed that our model outperforms the conventional BERT in terms of predictive ability and consumed time. Applications in the cooperated company were discussed.",
        "DOI": "10.4271/2022-01-7075",
        "paper_author": "Lin S.",
        "affiliation_name": "Guangdong University of Technology",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60007155",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Learn a prior question-aware feature for machine reading comprehension",
        "publication": "Frontiers in Physics",
        "citied_by": "1",
        "cover_date": "2022-12-22",
        "Abstract": "Machine reading comprehension aims to train machines to comprehend a given context and then answer a series of questions according to their understanding of the context. It is the cornerstone of conversational reading comprehension and question answering tasks. Recently, researches of Machine reading comprehension have experienced considerable development with more and more semantic features being incorporated into end-to-end neural network models, such as pre-trained word embedding features, syntactic features, context and question interaction features, and so on. However, these methods neglect the understanding of the question itself and the information sought by the question. In this paper, we design an auxiliary question-and-answer matching task to learn the features of different types of questions and then integrate these learned features into a classical Machine reading comprehension model architecture to improve its ability to comprehend the questions. Our auxiliary task relies on a simple Question-Answer Pairs dataset generated by ourselves. And we incorporate the learned question-type information into the Machine reading comprehension model by prior attention mechanism. The model we proposed is named PrA-MRC (Prior Attention on Machine reading comprehension). Empirical results show that our approach is effective and interpretable. Our Question-Answer Pairs model achieves an accuracy of 84% and our PrA-MRC model outperforms the baseline model by +0.7 EM and +1.1 F1 on the SQuAD dataset.",
        "DOI": "10.3389/fphy.2022.1085102",
        "paper_author": "Zhang Y.",
        "affiliation_name": "Beijing Jiaotong University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60022381",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Streaming Overlay Architecture for Lightweight LSTM Computation on FPGA SoCs",
        "publication": "ACM Transactions on Reconfigurable Technology and Systems",
        "citied_by": "3",
        "cover_date": "2022-12-22",
        "Abstract": "Long-Short Term Memory (LSTM) networks, and Recurrent Neural Networks (RNNs) in general, have demonstrated their suitability in many time series data applications, especially in Natural Language Processing (NLP). Computationally, LSTMs introduce dependencies on previous outputs in each layer that complicate their computation and the design of custom computing architectures, compared to traditional feed-forward networks. Most neural network acceleration work has focused on optimising the core matrix-vector operations on highly capable FPGAs in server environments. Research that considers the embedded domain has often been unsuitable for streaming inference, relying heavily on batch processing to achieve high throughput. Moreover, many existing accelerator architectures have not focused on fully exploiting the underlying FPGA architecture, resulting in designs that achieve lower operating frequencies than the theoretical maximum. This paper presents a flexible overlay architecture for LSTMs on FPGA SoCs that is built around a streaming dataflow arrangement, uses DSP block capabilities directly, and is tailored to keep parameters within the architecture while moving input data serially to mitigate external memory access overheads. The architecture is designed as an overlay that can be configured to implement alternative models or update model parameters at runtime. It achieves higher operating frequency and demonstrates higher performance than other lightweight LSTM accelerators, as demonstrated in an FPGA SoC implementation.",
        "DOI": "10.1145/3543069",
        "paper_author": "Ioannou L.",
        "affiliation_name": "School of Engineering",
        "affiliation_city": "Coventry",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "100765125",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Joint event causality extraction using dual-channel enhanced neural network",
        "publication": "Knowledge-Based Systems",
        "citied_by": "16",
        "cover_date": "2022-12-22",
        "Abstract": "Event Causality Extraction (ECE) plays an essential role in many Natural Language Processing (NLP), such as event prediction and dialogue generation. Recent research in NLP treats ECE as a sequence labeling problem. However, these methods tend to extract the events and their relevant causality using a single collapsed model, which usually focuses on the textual contents while ignoring the intra-element transitions inside events and inter-event causality transition association across events. In general, ECE should condense the complex relationship of intra-event and the causality transition association among events. Therefore, we propose a novel dual-channel enhanced neural network to address this limitation by taking both global event mentions and causality transition association into account. To extract complete event mentions, a Textual Enhancement Channel(TEC) is constructed to learn important intra-event features from the training data with a wider perception field. Then the Knowledge Enhancement Channel(KEC) incorporates external causality transition knowledge using a Graph Convolutional Network (GCN) to provide complementary information on event causality. Finally, we design a dynamic fusion attention mechanism to measure the importance of the two channels. Thus, our proposed model can incorporate both semantic-level and knowledge-level representations of events to extract the relevant event causality. Experimental results on three public datasets show that our model outperforms the state-of-the-art methods.",
        "DOI": "10.1016/j.knosys.2022.109935",
        "paper_author": "Gao J.",
        "affiliation_name": "Shanghai Jiao Tong University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60025084",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "SCICERO: A deep learning and NLP approach for generating scientific knowledge graphs in the computer science domain",
        "publication": "Knowledge-Based Systems",
        "citied_by": "29",
        "cover_date": "2022-12-22",
        "Abstract": "Science communication has a number of bottlenecks that include the rising number of published research papers and its non-machine-accessible and document-based paradigm, which makes the exploration, reading, and reuse of research outcomes rather inefficient. Recently, Knowledge Graphs (KG), i.e., semantic interlinked networks of entities, have been proposed as a new core technology to describe and curate scholarly information with the goal to make it machine readable and understandable. However, the main drawback of the use of such a technology is that researchers are asked to manually annotate their research papers and add their contributions within the KGs. To address this problem, in this paper we propose SCICERO, a novel KG generation approach that takes in input text from research articles and generates a KG of research entities. SCICERO uses Natural Language Processing techniques to parse the content of scientific papers to discover entities and relationships, exploits state-of-the-art Deep Learning Transformer models to make sense and validate extracted information, and uses Semantic Web best practices to formally represent the extracted entities and relationships, making the written content of research papers machine-actionable. SCICERO has been tested on a dataset of 6.7M papers about Computer Science generating a KG of about 10M entities. It has been evaluated on a manually generated gold standard of 3,600 triples that cover three Computer Science subdomains (Information Retrieval, Natural Language Processing, and Machine Learning) obtaining remarkable results.",
        "DOI": "10.1016/j.knosys.2022.109945",
        "paper_author": "Dessí D.",
        "affiliation_name": "Università degli Studi di Cagliari",
        "affiliation_city": "Cagliari",
        "affiliation_country": "Italy",
        "affiliation_id": "60032259",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Sector-level sentiment analysis with deep learning",
        "publication": "Knowledge-Based Systems",
        "citied_by": "30",
        "cover_date": "2022-12-22",
        "Abstract": "This paper presents new machine learning methods in the context of natural language processing (NLP) to extract useful information from financial news. Traditional NLP approaches, which are based on the use of lexicons or standard machine learning algorithms, ignore the importance of word position and combinations in texts, thereby resulting in low performance. More recently, NLP empowered by deep learning has achieved remarkable results in various tasks such as sentiment analysis. This paper proposes a deep learning solution for sentiment analysis, which is trained exclusively on financial news and combines multiple recurrent neural networks. Subsequently, our sentiment analysis models are enhanced using a semi-supervised learning method that relies on the detection and correction of presumably mislabeled data. The performance of our proposed solution compared favorably against both traditional and state-of-the-art models based on its performance on previously unseen tweet data. Additionally, this study provides a novel research on the prediction of specific economic sectors affected by news articles. Finally, we propose an ensemble of sentiment and sector models to provide a sector-level sentiment analysis with potential applications in the context of sector fund indices.",
        "DOI": "10.1016/j.knosys.2022.109954",
        "paper_author": "Almalis I.",
        "affiliation_name": "Aristotle University of Thessaloniki",
        "affiliation_city": "Thessaloniki",
        "affiliation_country": "Greece",
        "affiliation_id": "60015331",
        "affiliation_state": "Central Macedonia"
    },
    {
        "paper_title": "Application of IoT-enabled CNN for natural language processing",
        "publication": "IoT-enabled Convolutional Neural Networks: Techniques and Applications",
        "citied_by": "6",
        "cover_date": "2022-12-21",
        "Abstract": "Internet of Things (IoT) based systems are used to define communication systems based on machine-to-machine interaction. IoT when integrated with convolution neural network (CNN) can provide a system that can communicate with surroundings using human speech. Natural language processing (NLP) can interact with IoT-based deep learning systems to provide development in the automation field. IoT can connect a network of specific devices and exploit deep learning for feature extraction, namely sensor features, radio frequency features, and speech features. IoT with NLP can develop speech-based recognition systems for home automation systems. Smart home applications can be integrated with voice-command-based IoT devices to communicate specific commands to the devices. In addition, NLP-based IoT devices can help disabled people to perform their daily activities. These devices can monitor their health and provide voice-based security alerts. Also, NLP-enabled IoT devices can be helpful for automating environmental data collections which include geographical activities. However, NLP-based IoT implementation has certain limitations, namely language understanding, change in accent, and change in voice. These challenges restrict the efficient and quick utilization of NLP-based IoT devices. The deep learning technology with a big vocabulary database has provided numerous opportunities to train the voice and command recognition system in IoT. IoT-enabled CNN devices for voice recognition act as a boon to society.",
        "DOI": "NA",
        "paper_author": "Kumar A.",
        "affiliation_name": "Bennett University",
        "affiliation_city": "Greater Noida",
        "affiliation_country": "India",
        "affiliation_id": "60121496",
        "affiliation_state": "UP"
    },
    {
        "paper_title": "An Adaptive Graph Pre-training Framework for Localized Collaborative Filtering",
        "publication": "ACM Transactions on Information Systems",
        "citied_by": "22",
        "cover_date": "2022-12-21",
        "Abstract": "Graph neural networks (GNNs) have been widely applied in the recommendation tasks and have achieved very appealing performance. However, most GNN-based recommendation methods suffer from the problem of data sparsity in practice. Meanwhile, pre-training techniques have achieved great success in mitigating data sparsity in various domains such as natural language processing (NLP) and computer vision (CV). Thus, graph pre-training has the great potential to alleviate data sparsity in GNN-based recommendations. However, pre-training GNNs for recommendations faces unique challenges. For example, user-item interaction graphs in different recommendation tasks have distinct sets of users and items, and they often present different properties. Therefore, the successful mechanisms commonly used in NLP and CV to transfer knowledge from pre-training tasks to downstream tasks such as sharing learned embeddings or feature extractors are not directly applicable to existing GNN-based recommendations models. To tackle these challenges, we delicately design an adaptive graph pre-training framework for localized collaborative filtering (ADAPT). It does not require transferring user/item embeddings, and is able to capture both the common knowledge across different graphs and the uniqueness for each graph simultaneously. Extensive experimental results have demonstrated the effectiveness and superiority of ADAPT.",
        "DOI": "10.1145/3555372",
        "paper_author": "Wang Y.",
        "affiliation_name": "Michigan State University",
        "affiliation_city": "East Lansing",
        "affiliation_country": "United States",
        "affiliation_id": "60031707",
        "affiliation_state": "MI"
    },
    {
        "paper_title": "IDENTIFICATION OF THE EFFECTS OF NEURO-LINGUISTIC PROGRAMMING TECHNIQUES IN MANAGERIAL ENVIRONMENT IN THE CONTEXT OF GENDER DIFFERENCES",
        "publication": "Polish Journal of Management Studies",
        "citied_by": "0",
        "cover_date": "2022-12-21",
        "Abstract": "The primary aim of the research was to find out statistically significant differences in the assessment of selected NLP techniques (representational systems, rapport, pacing, leading) by female and male managers who attended and did not attend an NLP course. The research used an NLPT questionnaire (neuro-linguistic programming techniques). The data were collected from 124 managers-58 female and 66 male managers at all levels of management. The results show that the respondents who attended an NLP course evaluated the examined NLP techniques more positively. Gender differences and the interaction between gender and completion of the NLP course by assessing the NLP techniques were not confirmed. A statistically significant relation was identified only at a level of selected items of NLPT methodology. The study brought new findings in the research of measuring the NLP techniques’ effects in relation to gender and contributed to the knowledge and theory of NLP techniques in the context of gender differences in the managerial environment.",
        "DOI": "10.17512/pjms.2022.26.2.04",
        "paper_author": "Birknerová Z.",
        "affiliation_name": "University of Presov in Presov",
        "affiliation_city": "Presov-Lubotice",
        "affiliation_country": "Slovakia",
        "affiliation_id": "60032255",
        "affiliation_state": "Presov Region"
    },
    {
        "paper_title": "The Arabidopsis NLP7-HB52/54-VAR2 pathway modulates energy utilization in diverse light and nitrogen conditions",
        "publication": "Current Biology",
        "citied_by": "9",
        "cover_date": "2022-12-19",
        "Abstract": "In plants, nitrate is the dominant nitrogen (N) source and a critical nutrient signal regulating various physiological and developmental processes.1,2,3,4 Nitrate-responsive gene regulatory networks are widely believed to control growth, development, and life cycle in addition to N acquisition and utilization,1,2,3,4 and NIN-LIKE PROTEIN (NLP) transcriptional activators have been identified as the master regulators governing the networks.5,6,7 However, it remains to be elucidated how nitrate signaling regulates respective physiological and developmental processes. Here, we have identified a new nitrate-activated transcriptional cascade involved in chloroplast development and the maintenance of chloroplast function in Arabidopsis. This cascade consisting of NLP7 and two homeodomain-leucine zipper (HD-Zip) class I transcription factors, HOMEOBOX PROTEIN52 (HB52) and HB54,8,9 was responsible for nitrate- and light-dependent expression of VAR2 encoding the FtsH2 subunit of the chloroplast FtsH protease involved in the quality control of photodamaged thylakoid membrane proteins.10,11 Consistently, the nitrate-activated NLP7-HB52/54-VAR2 pathway underpinned photosynthetic light energy utilization, especially in high light environments. Furthermore, genetically enhancing the NLP7-HB52/54-VAR2 pathway resulted in improved light energy utilization under high light and low N conditions, a superior agronomic trait. These findings shed light on a new role of nitrate signaling and a novel mechanism for integrating information on N nutrient and light environments, providing a hint for enhancing the light energy utilization of plants in low N environments.",
        "DOI": "10.1016/j.cub.2022.10.024",
        "paper_author": "Ariga T.",
        "affiliation_name": "Graduate School of Agricultural and Life Sciences The University of Tokyo",
        "affiliation_city": "Tokyo",
        "affiliation_country": "Japan",
        "affiliation_id": "60103900",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Developing a Hybrid Neural Network for Part-Of-Speech Tagging and Named Entity Recognition",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2022-12-17",
        "Abstract": "Enabling our computers to understand human languages is very important as we move towards the 4th to 5th industrial revolution. A lot of efforts are already made to fast track this development most especially for highly resourced languages such as English, French, German, among others. The most notable open-source NLP tool built is the Natural Language toolkit, a collection of modules and corpora that provides researchers in Natural Language Processing (NLP) with extremely useful tools and resources. In the Philippines, several researchers have contributed to this advancement mostly for the Filipino language. In this paper, we introduce the design architecture of a hybrid neural network model that combines the best component features of the existing architectures for the POS Tagging and NER tasks. We also present initial experiment results.",
        "DOI": "10.1145/3582099.3582101",
        "paper_author": "Gonzales J.A.H.",
        "affiliation_name": "University of the Immaculate Conception",
        "affiliation_city": "Davao",
        "affiliation_country": "Philippines",
        "affiliation_id": "60279736",
        "affiliation_state": "Davao del Sur"
    },
    {
        "paper_title": "Technology-Enhanced Systemic Quality Assurance with the Aid of Text-Based Emotion Recognition of Facebook Comments for Higher Education Institutions",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2022-12-17",
        "Abstract": "One of the difficult and recently-emerging problems in the realm of natural language processing is the recognition and analysis of emotions (NLP). A current area of research involves identifying a person's emotional state through textual data in addition to recognizing emotions from face and auditory records. Numerous disciplines, including higher education institutions, can use the study of emotions to their advantage. This is especially true given the widespread usage of social media in today's world, when everything is done online. This information could be very helpful in guiding an organization's decisions. The abundant text found in social media, blogs, and other places can be used to explore different text mining findings, such as emotions. This study tackles on making use of the vast amount of information available online especially in social media platforms through the development of iMosyon, an emotion recognition system to help aid higher institutions in their decision-making process. Experimental results were executed and researchers then decided to use the Support Vector Machine (SVM) model due to the fact that it received the highest accuracy score of 78% among the classifiers. The beta testing which made use of Evaluation of Performance Functionality and Software Product Quality shows an overall rating of 4.5 out of 5.0 that indicates that the respondents accepted its functionality and the feedback was good.",
        "DOI": "10.1145/3582099.3582137",
        "paper_author": "Cantil J.B.J.",
        "affiliation_name": "Davao del Sur State College",
        "affiliation_city": "Davao",
        "affiliation_country": "Philippines",
        "affiliation_id": "126179385",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Sentimental Analysis on Social Media Comments with Recurring Models and Pretrained Word Embeddings in Portuguese",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2022-12-16",
        "Abstract": "Natural Language Processing (NLP) techniques are increasingly powerful for interpreting a person's feelings and reaction to a product or service. Sentiment analysis has become a fundamental tool for this interpretation, and it has studies in languages other than English. This type of application is uncommon and unheard of in Portuguese. This article presents a sentiment analysis classification based on Portuguese social media comments. Representation of word embeddings with both pre-trained Glove and Word2Vec models were generated through a corpus entirely in Portuguese. This article presents a set of results with different models of pre-trained layers and deep learning models exclusive to the Portuguese language on social networks. Two classification models were used and compared: (i) Bidirectional Long Short-Term Memory (BI-LSTM) and (ii) Bidirectional Gated Recurrent Unit (BI-GRU), achieving accuracy results of 99.1",
        "DOI": "10.1145/3582768.3582805",
        "paper_author": "Villalobos C.M.",
        "affiliation_name": "Pontifícia Universidade Católica do Rio de Janeiro",
        "affiliation_city": "Rio de Janeiro",
        "affiliation_country": "Brazil",
        "affiliation_id": "60032361",
        "affiliation_state": "RJ"
    },
    {
        "paper_title": "A Semantic Approach to Negation Detection and Word Disambiguation with Natural Language Processing",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "2",
        "cover_date": "2022-12-16",
        "Abstract": "This study aims to demonstrate the methods for detecting negations in a sentence by uniquely evaluating the lexical structure of the text via word-sense disambiguation. The proposed framework examines all the unique features in the various expressions within a text to resolve the contextual usage of all tokens and decipher the effect of negation on sentiment analysis. The application of popular expression detectors skips this important step, thereby neglecting the root words caught in the web of negation and making text classification difficult for machine learning and sentiment analysis. This study adopts the Natural Language Processing (NLP) approach to discover and antonimize words that were negated for better accuracy in text classification using a knowledge base provided by an NLP library called WordHoard. Early results show that our initial analysis improved on traditional sentiment analysis, which sometimes neglects negations or assigns an inverse polarity score. The SentiWordNet analyzer was improved by 35%, the Vader analyzer by 20% and the TextBlob by 6%.",
        "DOI": "10.1145/3582768.3582789",
        "paper_author": "Okpala I.",
        "affiliation_name": "University of Cincinnati",
        "affiliation_city": "Cincinnati",
        "affiliation_country": "United States",
        "affiliation_id": "60025152",
        "affiliation_state": "OH"
    },
    {
        "paper_title": "Natural Language Processing of COVID-19 Reports Involving China in New York Times - a Machine-based Framing Study of Media Language",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2022-12-16",
        "Abstract": "Natural Language Processing (NLP) is a most promising and powerful method for big data analysis. It is gaining increasing attention from language researchers with its potentiality in information extraction, automatic indexing, textual framing, topic modeling, sensitivity analysis and other machine analytics studies. Through employing the LDA topic modeling and NLTK (Natural Language Toolkit) Vader SentimentAnalyser, this research makes a contrastive study of the overall news coverage in New York Times (NYT) against the backdrop of Covid-19 and its China-specific reports, with the aim of addressing what areas of concern were respectively selected and foregrounded to the public in these two types, what sensitivities were revealed and how linguistic devices were used to frame China's response to Covid-19. Analysis of metaphorical expressions in NYT shows that metaphors tended to be employed as a device to realize the dominant negative polarity latent in the reports and thus establish unfavourable images of China. This study deepens the methodological endeavors in media and linguistic studies through combining content analysis and machine-based analysis.",
        "DOI": "10.1145/3582768.3582785",
        "paper_author": "Yang Z.",
        "affiliation_name": "Shanghai Sanda University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60031616",
        "affiliation_state": "Shanghai"
    },
    {
        "paper_title": "iGrade: an automated short answer grading system",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "2",
        "cover_date": "2022-12-16",
        "Abstract": "During the COVID-19 pandemic, most countries rely on E-Learning to apply social distance policy which affects the exams evaluation process. This project aimed to assist instructors in grading the short answer questions for CCSIT courses. By implanting a website application that the instructors could use to upload the students' answers and the ĝiGrade\"software model will grade it. Moreover, the system will reduce the workload on the facilities members by saving time and effort as well as guarantee an objective grading for students. The model used in this project is a state-of-the-art BERT Neural Network model along with layers of BiLSTM that was trained using a dataset that has been collected from previous midterm and final exams of the CIS 211 course. The dataset consists of three categories which are (0, 0.5, 1) with around 1,128 instances. The \"iGrade\"test obtained an accuracy score of 85,4%, demonstrating BERT's superiority and independence from features during short answer grading as a default method in NLP. CCS CONCEPTS •Computing methodologies •Artificial intelligence •Natural language processing",
        "DOI": "10.1145/3582768.3582790",
        "paper_author": "Alhamed D.H.",
        "affiliation_name": "Imam Abdulrahman Bin Faisal University",
        "affiliation_city": "Dammam",
        "affiliation_country": "Saudi Arabia",
        "affiliation_id": "60104334",
        "affiliation_state": "Ash Sharqiyah"
    },
    {
        "paper_title": "Responding to customer queries automatically by customer reviews' based Question Answering",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2022-12-16",
        "Abstract": "The entire world has been undergoing its own digital transformation over the past few decades as technology has advanced in leaps and bounds. Following this, an increase in the number of people using digital platforms for buying products online likewise increases the number of questions or enquiries posted about a product on an online shopping platform like Amazon on a day to day basis. Though we have gone completely digital in posting these questions, the answering of these questions is still manual. The forums are rarely active. By the time the user gets an answer to his question, either he has bought that product already through offline means or has lost interest in buying that product since it is time consuming. Moreover, the questions which are asked are mostly repetitive. At times the answers are already out there since they have already been given to some other user who had asked the same question. Also, lot of answers are embedded in the user reviews. Therefore, the answers can be extracted from the existing product reviews. This may lead to increase in sale and greater customer satisfaction as his query is resolved in much lower response time. We have review-based question answering systems that aim at answering the questions from the reviews given on the product by other customers. However, the existing systems have certain drawbacks due to the use of RNN, like missing attention mechanism etc. In this work, we enhance the performance of the existing review based QA systems by carrying out some prototypical experiments with the basic models of NLP and then moving towards more advanced Language Models while identifying and rectifying the shortcomings of the existing model. Further, in this work a thorough comparative analysis of the models and approaches that have been worked on is presented. We have enhanced the current state of the art existing review QA systems by using BERT, BART and also applied various heuristics for comparison. We achieved the best BLEU score of 0.58 by using BERT, which is an improvement of 0.19 on the current existing system.",
        "DOI": "10.1145/3582768.3582780",
        "paper_author": "Moharkar K.",
        "affiliation_name": "Visvesvaraya National Institute of Technology, Nagpur",
        "affiliation_city": "Nagpur",
        "affiliation_country": "India",
        "affiliation_id": "60015785",
        "affiliation_state": "MH"
    },
    {
        "paper_title": "Measuring Text-to-SQL Semantic Parsing Model on the Question Generalizability",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2022-12-16",
        "Abstract": "One of the challenges in NLP tasks, such as text-to-SQL semantic parsing, is generalization. In the text-to-SQL task, having separate training and testing data can measure one aspect of the generalization: how well the model generalizes to unseen databases. Other aspects, however, remain unaccounted for. We propose a new dataset and a more challenging and thorough evaluation process that focuses on the two challenges of generalizing the text-to-SQL model: database content references and question patterns. We create SPIDER-QG, an augmented dataset that employs three techniques, to assess generalizability. First, we replace the set of values in the existing test set with other values from the same column in the same database. Second, we use the synonym of each value as a replacement instead. Third, we generate new questions for the existing SQL query by back-translating the original question. Our evaluation setup demonstrates the generalization challenges and struggles of the current models.",
        "DOI": "10.1145/3582768.3582782",
        "paper_author": "Julavanich T.",
        "affiliation_name": "The University of Tokyo",
        "affiliation_city": "Tokyo",
        "affiliation_country": "Japan",
        "affiliation_id": "60025272",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "NLP And IR Applications For Financial Reporting And Non-Financial Disclosure. Framework Implementation And Roadmap For Feasible Integration With The Accounting Process",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2022-12-16",
        "Abstract": "Corporations produce financial and non-financial reports containing structured and unstructured data. In general, all organisations report information of some kind. Natural Language Processing (NLP) and Information Retrieval (IR) were fields developed from approximately the 1950s and have presented important applications, especially in the last three decades. Nevertheless, applications in accounting and finance have not developed accordingly, and a comprehensive framework is missing in the existing literature. This paper examines how NLP and IR can facilitate reporting and disclosure, both Financial and Non-Financial. The paper provides a brief literature review on NLP/IR applications in accounting and finance. It better informs and expands on the discussion of NLP/IR applications in academic research, professional organisations (i.e., IFRS), and industry. It explores some innovative applications of NLP/IR in unstructured data and its use in reporting and disclosure and FinTech applications. The main contribution is the definition of a complete framework that consistently analyses the possible NLP/IR applications in the accounting processes. We find that there can be many more applications of NLP/IR in accounting and finance and suggest future directions for research.",
        "DOI": "10.1145/3582768.3582796",
        "paper_author": "Faccia A.",
        "affiliation_name": "University of Birmingham",
        "affiliation_city": "Birmingham",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60019702",
        "affiliation_state": "West Midlands"
    },
    {
        "paper_title": "A corpus of drafts of NLP papers from non-native English speakers",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2022-12-16",
        "Abstract": "We created an English parallel corpus of 3,005 sentence pairs, each containing a well-polished text from ACL Anthology Reference Corpus (ACL-ARC) [1] and corresponding restated drafts collected from 26 non-native writers. The purpose of this paper is to explore the writing features of the drafts from non-native English speakers, so as to benefit research in Academic Writing Aid Systems. We present a feature analysis of the corpus based on handcrafted features. To assess utility, we formulate a draft identification task to automatically recognize drafts from ground truth texts based on hybrid features. We show that the combination of deep semantic features with the optimal handcrafted features improves identification accuracy on the collected data, up to 84.57%.",
        "DOI": "10.1145/3582768.3582797",
        "paper_author": "Wang H.",
        "affiliation_name": "Waseda University",
        "affiliation_city": "Tokyo",
        "affiliation_country": "Japan",
        "affiliation_id": "60023462",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Deep Bidirectional Transformers for Arabic Dialect Identification",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "3",
        "cover_date": "2022-12-15",
        "Abstract": "The rising adoption of social media has led to the widespread dissemination of online textual data. Arabic is among the top five most popular languages worldwide (Arabic is spoken by a total of about 360.2 million people worldwide as a native language). In this regard, Arabic-text data available on social media are presented using different Arabic dialects, such as the Gulf, Iraqi, Egyptian, Levantine, and North Africa dialects. Particularly, identifying the Arabic dialect used in text is of significant value for several natural language processing tasks, such as machine translation, text generation, word correction, and information retrieval. Arabic-dialect identification is a multiclass classification problem in which classes represent different Arabic dialects. In this study, we investigated the performance of two bidirectional deep learning models for Arabic-dialect classification: MARBERT and ARBERT. We analyzed the performance of the models on two publicly available datasets: the Arabic Online Commentary dataset and the Social Media Arabic Dialect Corpus. Extensive experiments were conducted, encompassing binary dialect classification, three-way dialect classification, and multi-way dialect classification. The results indicate that MARBERT consistently achieved higher F1-scores than ARBERT, which can be attributed to the significant differences between the two models, including their architectures, training mechanisms, and data sources.",
        "DOI": "10.1145/3584202.3584243",
        "paper_author": "Alghamdi A.",
        "affiliation_name": "University of Jeddah",
        "affiliation_city": "Jeddah",
        "affiliation_country": "Saudi Arabia",
        "affiliation_id": "60108699",
        "affiliation_state": "Makkah Province"
    },
    {
        "paper_title": "Hate and Aggression Analysis in NLP with Explainable AI",
        "publication": "International Journal of Pattern Recognition and Artificial Intelligence",
        "citied_by": "6",
        "cover_date": "2022-12-15",
        "Abstract": "Social platforms such as Twitter and Facebook have now become only media to express their thoughts, and due to lack of censorship, it often embellishes themselves as an abode for hate towards minorities. People of color, Asian people, Muslims, women, transgenders, and LGBTQ+ communities are often the target of such online hate and aggression. Though several companies have incorporated considerable algorithms on their platforms, nevertheless due to being rather hard to often detect such comments still make it to the platforms, creating a negative space towards targeted people. This research involves the study and comparison of different hate and aggression detection algorithms with intent on two languages, i.e. English and German including machine learning models (linear SVC, logistic regression, multinomial naive Bayes and random forests) with their variations with feature engineering and bag of words and deep learning (CNN-GRU static, TCN static, Seq2Seq) with their variations vis-à-vis Word2Vec embedding. CNN+GRU static + Word2Vec embedding has outperformed all the other techniques with an accuracy of 68.29%.",
        "DOI": "10.1142/S0218001422590364",
        "paper_author": "Raman S.",
        "affiliation_name": "Bharati Vidyapeeth's College of Engineering, New Delhi",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India",
        "affiliation_id": "60076553",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Institutional collaboration recommendation: An expertise-based framework using NLP and network analysis",
        "publication": "Expert Systems with Applications",
        "citied_by": "10",
        "cover_date": "2022-12-15",
        "Abstract": "The shift from ‘trust-based funding’ to ‘performance-based funding’ is one of the factors that has forced institutions to strive for continuous improvement of performance. Several studies have established the importance of collaboration in enhancing the performance of paired institutions. However, identification of suitable institutions for collaboration is sometimes difficult and therefore institutional collaboration recommendation systems can be vital. Currently, there are no well-developed institutional collaboration recommendation systems. In order to bridge this gap, we design a framework that recognizes the thematic strengths and core competencies of institutions, which can in turn be used for collaboration recommendations. The framework, based on NLP and network analysis techniques, is capable of determining the strengths of an institution in different thematic areas within a field and thereby determining the core competency and potential core competency areas of that institution. It makes use of recently proposed expertise indices such as x and x(g) indices for determination of core and potential core competency areas and can toss two kinds of recommendations: (i) for enhancement of strength of strong areas or core competency areas of an institution and (ii) for complementing the potentially strong areas or potential core competency areas of an institution. A major advantage of the system is that it can help to determine and improve the research portfolio of an institution within a field through suitable collaboration, which may lead to the overall improvement of the performance of the institution in that field. The framework is demonstrated by analyzing the performance of 195 Indian institutions in the field of ‘Computer Science’. Upon validation using standard metrics for novelty, coverage and diversity of recommendation systems, the framework is found to be of sufficient coverage and capable of tossing novel and diverse recommendations. The article thus presents an institutional collaboration recommendation system which can be used by institutions to identify potential collaborators.",
        "DOI": "10.1016/j.eswa.2022.118317",
        "paper_author": "Lathabai H.H.",
        "affiliation_name": "Indian Institute of Science",
        "affiliation_city": "Bengaluru",
        "affiliation_country": "India",
        "affiliation_id": "60014097",
        "affiliation_state": "KA"
    },
    {
        "paper_title": "Artificial intelligence for the advancement of health systems. Possible contributions and challenges",
        "publication": "Revista de Derecho de la Seguridad Social, Laborum",
        "citied_by": "1",
        "cover_date": "2022-12-14",
        "Abstract": "The increasing availability of large data sets and new analytical methods, coupled with simultaneous advances in information technology (ICT) infrastructure and mobile population coverage, have given rise to the hope that AI will address future health challenges. AI can use sophisticated algorithms to “learn” characteristics of a large volume of health data and use the knowledge gained to support clinical practice. It may also be equipped with learning and self-correcting capabilities to improve accuracy based on the information received. AI devices used in health can be divided into two main categories: Machine Learning (ML) techniques that analyze structured data such as image, genetic, and electrophysiological data, which attempt to group patient traits or infer the probability of disease outcomes, and natural language processing (NLP) methods that extract information from unstructured data such as clinical notes/medical journals to supplement and enrich structured medical data. Current limitations of iHealth and eHealth in general include financial costs, cultural, language and literacy barriers, power supply problems, data security, and privacy issues. Healthcare jobs most likely to be automated appear to be those where part of their tasks involves the management of digital information, such as radiology and pathology. contrary to those based on human relationships and direct contact with patients.",
        "DOI": "NA",
        "paper_author": "Bermúdez-Tamayo C.",
        "affiliation_name": "Escuela Andaluza Salud Publica",
        "affiliation_city": "Granada",
        "affiliation_country": "Spain",
        "affiliation_id": "60002811",
        "affiliation_state": "Granada"
    },
    {
        "paper_title": "Multifunctional Ternary NLP/ZnO@ l -cysteine- grafted-PANI Bionanocomposites for the Selective Removal of Anionic and Cationic Dyes from Synthetic and Real Water Samples",
        "publication": "ACS Omega",
        "citied_by": "11",
        "cover_date": "2022-12-13",
        "Abstract": "The development of competent adsorbents based on agro-waste materials with multifunctional groups and porosity for the removal of toxic dyes from aqueous solutions is still a challenge. Herein, a bionanocomposite made up of neem leaf powder (NLP), zinc oxide (ZnO), and amino acid (l-cysteine)-functionalized polyaniline (PANI), namely, NLP/ZnO@l-cysteine-grafted-PANI (NZC-g-PANI), has been prepared by an in situ polymerization method. The as-prepared bionanocomposite was tested for the adsorptive removal of three anionic dyes, namely, methyl orange (MO), amido black 10B (AB 10B), and eriochrome black T (EBT), as well as three cationic dyes, namely, brilliant green (BG), crystal violet (CV), and methylene blue (MB), from synthetic aqueous medium. The morphological and structural characteristics of the NZC-g-PANI nanocomposite were examined with the help of HR field emission scanning electron microscopy (FESEM), transmission electron microscopy (TEM), X-ray diffraction (XRD), Fourier transform infrared (FTIR), and Raman spectroscopy. FTIR and Raman studies show that the formulated NZC-g-PANI have an ample number of functional moieties such as carboxyl (-COOH), hydroxyl (-OH), amines (-NH2), and imines (-N═), thus demonstrating outstanding dye removal capacity. C-S linkage helps to attach l-cysteine with polyaniline. Moreover, the predominance of chemisorption via ionic/pi-pi interaction and hydrogen bonding between the NZC-g-PANI nanocomposite and dyes (BG and MO) has been realized by FTIR and fitting of kinetics data to the PSO model. For both BG and MO dyes, the biosorption isotherm was precisely accounted for by the Langmuir isotherm with qmaxvalues of up to 218.27 mg g-1for BG at pH 6 and 558.34 mg g-1for MO at pH 1. Additionally, thermodynamic studies revealed the endothermic and spontaneous nature of adsorption. NZC-g-PANI showed six successive regeneration cycles for cationic (MO: from 96.3 to 90.4%) and anionic (BG: from 94.7 to 88.7%) dyes. Also, batch adsorption operations were validated to demonstrate dye biosorption from real wastewater, such as tap water, river water, and laundry wastewater. Overall, this study indicates that the prepared NZC-g-PANI biosorbent could be used as an effective adsorbent for the removal of various types of anionic as well as cationic dyes from different aqueous solutions.",
        "DOI": "10.1021/acsomega.2c04936",
        "paper_author": "Bir R.",
        "affiliation_name": "Galgotias University",
        "affiliation_city": "Greater Noida",
        "affiliation_country": "India",
        "affiliation_id": "60120915",
        "affiliation_state": "UP"
    },
    {
        "paper_title": "Neural text summarization for Hungarian",
        "publication": "Acta Linguistica Academica",
        "citied_by": "2",
        "cover_date": "2022-12-12",
        "Abstract": "One of the most important NLP tasks for the industry today is to produce an extract from longer text documents. This task is one of the hottest topics for the researchers and they have created some solutions for English. There are two types of the text summarization called extractive and abstractive. The goal of the first task is to find the relevant sentences from the text, while the second one should generate the extraction based on the original text. In this research I have built the first solutions for Hungarian text summarization systems both for extractive and abstractive subtasks. Different kinds of neural transformer-based methods were used and evaluated. I present in this publication the first Hungarian abstractive summarization tool based on mBART and mT5 models, which gained state-of-the-art results.",
        "DOI": "10.1556/2062.2022.00577",
        "paper_author": "Yang Z.G.",
        "affiliation_name": "HUN-REN Hungarian Research Centre for Linguistics",
        "affiliation_city": "Budapest",
        "affiliation_country": "Hungary",
        "affiliation_id": "60020907",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "BiVaSE: A bilingual variational sentence encoder with randomly initialized Transformer layers",
        "publication": "Acta Linguistica Academica",
        "citied_by": "0",
        "cover_date": "2022-12-12",
        "Abstract": "Transformer-based NLP models have achieved state-of-the-art results in many NLP tasks including text classification and text generation. However, the layers of these models do not output any explicit representations for texts units larger than tokens (e.g. sentences), although such representations are required to perform text classification. Sentence encodings are usually obtained by applying a pooling technique during fine-tuning on a specific task. In this paper, a new sentence encoder is introduced. Relying on an autoencoder architecture, it was trained to learn sentence representations from the very beginning of its training. The model was trained on bilingual data with variational Bayesian inference. Sentence representations were evaluated in downstream and linguistic probing tasks. Although the newly introduced encoder generally performs worse than well-known Transformer-based encoders, the experiments show that it was able to learn to incorporate linguistic information in the sentence representations.",
        "DOI": "10.1556/2062.2022.00584",
        "paper_author": "Nyéki B.",
        "affiliation_name": "HUN-REN Hungarian Research Centre for Linguistics",
        "affiliation_city": "Budapest",
        "affiliation_country": "Hungary",
        "affiliation_id": "60020907",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Cross-lingual transfer of knowledge in distributional language models: Experiments in Hungarian",
        "publication": "Acta Linguistica Academica",
        "citied_by": "1",
        "cover_date": "2022-12-12",
        "Abstract": "In this paper, we argue that the very convincing performance of recent deep-neural-model-based NLP applications has demonstrated that the distributionalist approach to language description has proven to be more successful than the earlier subtle rule-based models created by the generative school. The now ubiquitous neural models can naturally handle ambiguity and achieve human-like linguistic performance with most of their training consisting only of noisy raw linguistic data without any multimodal grounding or external supervision refuting Chomsky's argument that some generic neural architecture cannot arrive at the linguistic performance exhibited by humans given the limited input available to children. In addition, we demonstrate in experiments with Hungarian as the target language that the shared internal representations in multilingually trained versions of these models make them able to transfer specific linguistic skills, including structured annotation skills, from one language to another remarkably efficiently.",
        "DOI": "10.1556/2062.2022.00580",
        "paper_author": "Novák A.",
        "affiliation_name": "Pazmany Peter Katolikus Egyetem",
        "affiliation_city": "Budapest",
        "affiliation_country": "Hungary",
        "affiliation_id": "60007562",
        "affiliation_state": "Budapest"
    },
    {
        "paper_title": "Named entities detection by beam search algorithm",
        "publication": "Concurrency and Computation: Practice and Experience",
        "citied_by": "2",
        "cover_date": "2022-12-10",
        "Abstract": "Named entity recognition (NER) is a fundamental process in NLP and a requirement for most processes. This article aims to identify the named entities in the context of social networks. For this purpose, the idea of segmenting text into suitable and unsuitable expressions for the named entities has been used. So the contribution of this article is to process informal text in the Persian language by the Beam search algorithm to detect named entities. Due to the reproductive nature of language, new words and names are always produced, and available NER systems are inefficient in detecting new entities. The other contribution of this article is to make it possible to recognize the emerging named entity by applying dynamic external knowledge. According to a sense of the lack of datasets in low-resource languages, N-Gram and Wikipedia anchor datasets have been prepared for Persian and deployed as external knowledge. Also, a corpus of named entities in Persian from the telegram dataset has been generated. Three native experts have done labeling of this corpus. Evaluation of these three experts and the proposed method shows that the result of the proposed method is acceptable compared to the result of a human-to-human also to other methods.",
        "DOI": "10.1002/cpe.7325",
        "paper_author": "Gholami-Dastgerdi P.",
        "affiliation_name": "University of Tabriz",
        "affiliation_city": "Tabriz",
        "affiliation_country": "Iran",
        "affiliation_id": "60006622",
        "affiliation_state": "East Azerbaijan"
    },
    {
        "paper_title": "Causal Categorization of Mental Health Posts using Transformers",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "7",
        "cover_date": "2022-12-09",
        "Abstract": "With recent developments in digitization of clinical psychology, NLP research community has revolutionized the field of mental health detection on social media. Existing research in mental health analysis revolves around the cross-sectional studies to classify users' intent on social media. For in-depth analysis, we investigate existing classifiers to solve the problem of causal categorization which suggests the inefficiency of learning based methods due to limited training samples. To handle this challenge, we use transformer models and demonstrate the efficacy of a pre-Trained transfer learning on \"CAMS\"dataset [4]. The experimental result improves the accuracy and depicts the importance of identifying cause-And-effect relationships in the underlying text.",
        "DOI": "10.1145/3574318.3574334",
        "paper_author": "Kaur S.",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Overview of the FIRE 2022 track: Information Retrieval from Microblogs during Disasters (IRMiDis)",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2022-12-09",
        "Abstract": "Microblogging sites such as Twitter play an important role in dealing with various mass emergencies including natural disasters and pandemics. Over the last several years, the track on Information Retrieval from Microblogs during Disasters (IRMiDis), organized as part of the FIRE conference series, has provided annotated datasets for developing ML/NLP techniques for utilizing microblogs for various practical tasks that would help authorities better deal with disaster situations. In particular, the FIRE 2022 IRMiDis track focused on two important tasks-(i) to detect the vaccine-related stance of tweets related to COVID-19 vaccines, and (ii) to detect reporting of COVID-19 symptom in tweets.",
        "DOI": "10.1145/3574318.3574319",
        "paper_author": "Poddar S.",
        "affiliation_name": "Indian Institute of Technology Kharagpur",
        "affiliation_city": "Kharagpur",
        "affiliation_country": "India",
        "affiliation_id": "60004750",
        "affiliation_state": "WB"
    },
    {
        "paper_title": "Flow-based network intrusion detection based on BERT masked language model",
        "publication": "CoNEXT-SW 2022 - Proceedings of the International CoNEXT Student Workshop 2022, Part CoNEXT 2022",
        "citied_by": "8",
        "cover_date": "2022-12-09",
        "Abstract": "A Network Intrusion Detection System (NIDS) is an important tool that identifies potential threats to a network. Recently, different flow-based NIDS designs utilizing Machine Learning (ML) algorithms have been proposed as potential solutions to detect intrusions efficiently. However, conventional ML-based classifiers have not seen widespread adoption in the real-world due to their poor domain adaptation capability. In this research, our goal is to explore the possibility of improve the domain adaptation capability of NIDS. Our proposal employs Natural Language Processing (NLP) techniques and Bidirectional Encoder Representations from Transformers (BERT) framework. The proposed method achieved positive results when tested on data from different domains.",
        "DOI": "10.1145/3565477.3569152",
        "paper_author": "Nguyen L.G.",
        "affiliation_name": "Nagaoka University of Technology",
        "affiliation_city": "Nagaoka",
        "affiliation_country": "Japan",
        "affiliation_id": "60018352",
        "affiliation_state": "Niigata"
    },
    {
        "paper_title": "On using pretext tasks to learn representations from network logs",
        "publication": "NativeNI 2022 - Proceedings of the 1st International Workshop on Native Network Intelligence, Part of CoNEXT 2022",
        "citied_by": "1",
        "cover_date": "2022-12-09",
        "Abstract": "Learning meaningful representations from network data is critical to ease the adoption of AI as a cornerstone to process network logs. Since a large portion of such data is textual, Natural Language Processing (NLP) appears as an obvious candidate to learn their representations. Indeed, the literature proposes impressive applications of NLP applied to textual network data. However, in the absence of labels, objectively evaluating the goodness of the learned representations is still an open problem. We call for a systematic adoption of domain-specific pretext tasks to select the best representation from network data. Relying on such tasks enables us to evaluate different representations on side machine learning problems and, ultimately, unveiling the best candidate representations for the more interesting downstream tasks for which labels are scarce or unavailable. We apply pretext tasks in the analysis of logs collected from SSH honeypots. Here, a cumbersome downstream task is to cluster events that exhibit a similar attack pattern. We propose the following pipeline: first, we represent the input data using a classic NLP-based approach. Then, we design pretext tasks to objectively evaluate the representation goodness and to select the best one. Finally, we use the best representation to solve the unsupervised task, which uncovers interesting behaviours and attack patterns. All in all, our proposal can be generalized to other text-based network logs beyond honeypots.",
        "DOI": "10.1145/3565009.3569522",
        "paper_author": "Boffa M.",
        "affiliation_name": "Politecnico di Torino",
        "affiliation_city": "Turin",
        "affiliation_country": "Italy",
        "affiliation_id": "60012162",
        "affiliation_state": "TO"
    },
    {
        "paper_title": "Deep learning research applications for natural language processing",
        "publication": "Deep Learning Research Applications for Natural Language Processing",
        "citied_by": "2",
        "cover_date": "2022-12-09",
        "Abstract": "Humans have the most advanced method of communication, which is known as natural language. While humans can use computers to send voice and text messages to each other, computers do not innately know how to process natural language. In recent years, deep learning has primarily transformed the perspectives of a variety of fields in artificial intelligence (AI), including speech, vision, and natural language processing (NLP). The extensive success of deep learning in a wide variety of applications has served as a benchmark for the many downstream tasks in AI. The field of computer vision has taken great leaps in recent years and surpassed humans in tasks related to detecting and labeling objects thanks to advances in deep learning and neural networks. Deep Learning Research Applications for Natural Language Processing explains the concepts and state-of-the-art research in the fields of NLP, speech, and computer vision. It provides insights into using the tools and libraries in Python for real-world applications. Covering topics such as deep learning algorithms, neural networks, and advanced prediction, this premier reference source is an excellent resource for computational linguists, software engineers, IT managers, computer scientists, students and faculty of higher education, libraries, researchers, and academicians.",
        "DOI": "10.4018/978-1-6684-6001-6",
        "paper_author": "Kumar L.A.",
        "affiliation_name": "PSG College of Technology",
        "affiliation_city": "Coimbatore",
        "affiliation_country": "India",
        "affiliation_id": "60026157",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Misinformation containment using NLP and machine learning: Why the problem is still unsolved",
        "publication": "Deep Learning Research Applications for Natural Language Processing",
        "citied_by": "4",
        "cover_date": "2022-12-09",
        "Abstract": "Despite the increased attention and substantial research into it claiming outstanding successes, the problem of misinformation containment has only been growing in the recent years with not many signs of respite. Misinformation is rapidly changing its latent characteristics and spreading vigorously in a multi-modal fashion, sometimes in a more damaging manner than viruses and other malicious programs on the internet. This chapter examines the existing research in natural language processing and machine learning to stop the spread of misinformation, analyzes why the research has not been practical enough to be incorporated into social media platforms, and provides future research directions. The state-of-theart feature engineering, approaches, and algorithms used for the problem are expounded in the process.",
        "DOI": "10.4018/978-1-6684-6001-6.ch003",
        "paper_author": "Pendyala V.S.",
        "affiliation_name": "San Jose State University",
        "affiliation_city": "San Jose",
        "affiliation_country": "United States",
        "affiliation_id": "60015609",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Enhanced neurologic concept recognition using a named entity recognition model based on transformers",
        "publication": "Frontiers in Digital Health",
        "citied_by": "4",
        "cover_date": "2022-12-08",
        "Abstract": "Although deep learning has been applied to the recognition of diseases and drugs in electronic health records and the biomedical literature, relatively little study has been devoted to the utility of deep learning for the recognition of signs and symptoms. The recognition of signs and symptoms is critical to the success of deep phenotyping and precision medicine. We have developed a named entity recognition model that uses deep learning to identify text spans containing neurological signs and symptoms and then maps these text spans to the clinical concepts of a neuro-ontology. We compared a model based on convolutional neural networks to one based on bidirectional encoder representation from transformers. Models were evaluated for accuracy of text span identification on three text corpora: physician notes from an electronic health record, case histories from neurologic textbooks, and clinical synopses from an online database of genetic diseases. Both models performed best on the professionally-written clinical synopses and worst on the physician-written clinical notes. Both models performed better when signs and symptoms were represented as shorter text spans. Consistent with prior studies that examined the recognition of diseases and drugs, the model based on bidirectional encoder representations from transformers outperformed the model based on convolutional neural networks for recognizing signs and symptoms. Recall for signs and symptoms ranged from 59.5% to 82.0% and precision ranged from 61.7% to 80.4%. With further advances in NLP, fully automated recognition of signs and symptoms in electronic health records and the medical literature should be feasible.",
        "DOI": "10.3389/fdgth.2022.1065581",
        "paper_author": "Azizi S.",
        "affiliation_name": "College of Engineering and Computing",
        "affiliation_city": "Rolla",
        "affiliation_country": "United States",
        "affiliation_id": "60146648",
        "affiliation_state": "MO"
    },
    {
        "paper_title": "Sentiment Analysis of Air Conditioning Users- Comments on e-Commerce Platform Based on NLP Technology",
        "publication": "Advances in Transdisciplinary Engineering",
        "citied_by": "1",
        "cover_date": "2022-12-06",
        "Abstract": "With the rapid development of e-commerce, more and more users shop online through e-commerce platforms and leave a large number of user comments which are valuable for enterprises development. Based on the air conditioning users' comments from JD and other e-commerce platforms, this paper uses Python to conduct data mining, preprocessing of data text and emotional tendency analysis. Finally, the word cloud map method is used to analyze the advantages and disadvantages of Gree and Midea air conditioners in the products and services of e-commerce platforms, and corresponding suggestions for enterprises are promoted.",
        "DOI": "10.3233/ATDE221038",
        "paper_author": "Hu Z.",
        "affiliation_name": "Hubei University of Technology",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60108756",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "DeepCausality: A general AI-powered causal inference framework for free text: A case study of LiverTox",
        "publication": "Frontiers in Artificial Intelligence",
        "citied_by": "8",
        "cover_date": "2022-12-06",
        "Abstract": "Causality plays an essential role in multiple scientific disciplines, including the social, behavioral, and biological sciences and portions of statistics and artificial intelligence. Manual-based causality assessment from a large number of free text-based documents is very time-consuming, labor-intensive, and sometimes even impractical. Herein, we proposed a general causal inference framework named DeepCausality to empirically estimate the causal factors for suspected endpoints embedded in the free text. The proposed DeepCausality seamlessly incorporates AI-powered language models, named entity recognition and Judea Pearl's Do-calculus, into a general framework for causal inference to fulfill different domain-specific applications. We exemplified the utility of the proposed DeepCausality framework by employing the LiverTox database to estimate idiosyncratic drug-induced liver injury (DILI)-related causal terms and generate a knowledge-based causal tree for idiosyncratic DILI patient stratification. Consequently, the DeepCausality yielded a prediction performance with an accuracy of 0.92 and an F-score of 0.84 for the DILI prediction. Notably, 90% of causal terms enriched by the DeepCausality were consistent with the clinical causal terms defined by the American College of Gastroenterology (ACG) clinical guideline for evaluating suspected idiosyncratic DILI (iDILI). Furthermore, we observed a high concordance of 0.91 between the iDILI severity scores generated by DeepCausality and domain experts. Altogether, the proposed DeepCausality framework could be a promising solution for causality assessment from free text and is publicly available through https://github.com/XingqiaoWang/https-github.com-XingqiaoWang-DeepCausality-LiverTox.",
        "DOI": "10.3389/frai.2022.999289",
        "paper_author": "Wang X.",
        "affiliation_name": "Donaghey College of Science, Technology, Engineering, and Mathematics",
        "affiliation_city": "Little Rock",
        "affiliation_country": "United States",
        "affiliation_id": "60158897",
        "affiliation_state": "AR"
    },
    {
        "paper_title": "How consumer opinions are affected by marketers: an empirical examination by deep learning approach",
        "publication": "Journal of Research in Interactive Marketing",
        "citied_by": "11",
        "cover_date": "2022-12-06",
        "Abstract": "Purpose: The natural language processing (NLP) technique enables machines to understand human language. This paper seeks to harness its power to recognise the interaction between marketers and consumers. Hence, this study aims to enhance the conceptual and future development of deep learning in interactive marketing. Design/methodology/approach: This study measures cognitive responses by using actual user postings. Following a typical NLP analysis pipeline with tailored neural network (NN) models, it presents a stylised quantitative method to manifest the underlying relation. Findings: Based on consumer-generated content (CGC) and marketer-generated content (MGC) in the tourism industry, the results reveal that marketers and consumers interact in a subtle way. This study explores beyond simple positive and negative framing, and reveals that they do not resemble each other, not even in abstract form: CGC may complement MGC, but they are incongruent. It validates and supplements preceding findings in the framing effect literature and underpins some marketing wisdom in practice. Research limitations/implications: This research inherits a fundamental limitation of NN model that result interpretability is low. Also, the study may capture the partial phenomenon exhibited by active reviewers; lurker-consumers may behave differently. Originality/value: This research is among the first to explore the interactive aspect of the framing effect with state-of-the-art deep learning language model. It reveals research opportunities by using NLP-extracted latent features to assess textual opinions. It also demonstrates the accessibility of deep learning tools. Practitioners could use the described blueprint to foster their marketing initiatives.",
        "DOI": "10.1108/JRIM-04-2021-0106",
        "paper_author": "Yu B.",
        "affiliation_name": "Macao Polytechnic University",
        "affiliation_city": "Macau",
        "affiliation_country": "Macao",
        "affiliation_id": "60072904",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Investigating Strategies for Clause Recommendation",
        "publication": "Frontiers in Artificial Intelligence and Applications",
        "citied_by": "2",
        "cover_date": "2022-12-05",
        "Abstract": "Clause recommendation is the problem of recommending a clause to a legal contract, given the context of the contract in question and the clause type to which the clause should belong. With not much prior work being done toward the generation of legal contracts, this problem was proposed as a first step toward the bigger problem of contract generation. As an open-ended text generation problem, the distinguishing characteristics of this problem lie in the nature of legal language as a sublanguage and the considerable similarity of textual content within the clauses of a specific type. This similarity aspect in legal clauses drives us to investigate the importance of similar contracts' representation for recommending clauses. In our work, we experiment with generating clauses for 15 commonly occurring clause types in contracts expanding upon the previous work on this problem and analyzing clause recommendations in varying settings using information derived from similar contracts.",
        "DOI": "10.3233/FAIA220450",
        "paper_author": "Joshi S.",
        "affiliation_name": "International Institute of Information Technology, Hyderabad",
        "affiliation_city": "Hyderabad",
        "affiliation_country": "India",
        "affiliation_id": "60000163",
        "affiliation_state": "TS"
    },
    {
        "paper_title": "WhenTheFact: Extracting Events from European Legal Decisions",
        "publication": "Frontiers in Artificial Intelligence and Applications",
        "citied_by": "5",
        "cover_date": "2022-12-05",
        "Abstract": "This paper presents WhenTheFact, a tool that identifies relevant events from European judgments. It is able to extract the structure of the document, as well as when the event happened and who carried it out. WhenTheFact builds then a timeline that allows the user to navigate through the annotations in the document.",
        "DOI": "10.3233/FAIA220470",
        "paper_author": "Navas-Loro M.",
        "affiliation_name": "Universidad Politécnica de Madrid",
        "affiliation_city": "Madrid",
        "affiliation_country": "Spain",
        "affiliation_id": "60028442",
        "affiliation_state": "Madrid"
    },
    {
        "paper_title": "Scribe: A Specialized Collaborative Tool for Legal Judgment Annotation",
        "publication": "Frontiers in Artificial Intelligence and Applications",
        "citied_by": "2",
        "cover_date": "2022-12-05",
        "Abstract": "Scribe is a legal judgment annotation platform. Its objective is to improve dataset quality for machine learning models, to make annotation task faster and easier, and to boost interactions between annotators and developers. The platform manages 3 different classes of annotation: claims, named entities and sections. The platform facilitates the expression of annotation needs by developers. Multiple annotators can quickly respond to these needs by working in parallel. The collaborative process ends when the expected model performance is reached. The platform is organized by modules, maintainable and extensible in addition to its flexibility and unified output result in JSON format. See our demo https://lawbot.unimes.fr/annotateur.",
        "DOI": "10.3233/FAIA220483",
        "paper_author": "Mahmoudi S.A.",
        "affiliation_name": "Université de Nîmes",
        "affiliation_city": "Nimes",
        "affiliation_country": "France",
        "affiliation_id": "60283152",
        "affiliation_state": "Occitanie"
    },
    {
        "paper_title": "Predicting Outcomes of Italian VAT Decisions",
        "publication": "Frontiers in Artificial Intelligence and Applications",
        "citied_by": "9",
        "cover_date": "2022-12-05",
        "Abstract": "This study aims at predicting the outcomes of legal cases based on the textual content of judicial decisions. We present a new corpus of Italian documents, consisting of 226 annotated decisions on Value Added Tax by Regional Tax law commissions. We address the task of predicting whether a request is upheld or rejected in the final decision. We employ traditional classifiers and NLP methods to assess which parts of the decision are more informative for the task.",
        "DOI": "10.3233/FAIA220465",
        "paper_author": "Galli F.",
        "affiliation_name": "Alma Mater Studiorum Università di Bologna",
        "affiliation_city": "Bologna",
        "affiliation_country": "Italy",
        "affiliation_id": "60028218",
        "affiliation_state": "BO"
    },
    {
        "paper_title": "Boosting Neural Networks to Decompile Optimized Binaries",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "11",
        "cover_date": "2022-12-05",
        "Abstract": "Decompilation aims to transform a low-level program language (LPL) (eg., binary file) into its functionally-equivalent high-level program language (HPL) (e.g., C/C++). It is a core technology in software security, especially in vulnerability discovery and malware analysis. In recent years, with the successful application of neural machine translation (NMT) models in natural language processing (NLP), researchers have tried to build neural decompilers by borrowing the idea of NMT. They formulate the decompilation process as a translation problem between LPL and HPL, aiming to reduce the human cost required to develop decompilation tools and improve their generalizability. However, state-of-the-art learning-based decompilers do not cope well with compiler-optimized binaries. Since real-world binaries are mostly compiler-optimized, decompilers that do not consider optimized binaries have limited practical significance. In this paper, we propose a novel learning-based approach named NeurDP, that targets compiler-optimized binaries. NeurDP uses a graph neural network (GNN) model to convert LPL to an intermediate representation (IR), which bridges the gap between source code and optimized binary. We also design an Optimized Translation Unit (OTU) to split functions into smaller code fragments for better translation performance. Evaluation results on datasets containing various types of statements show that NeurDP can decompile optimized binaries with 45.21% higher accuracy than state-of-the-art neural decompilation frameworks.",
        "DOI": "10.1145/3564625.3567998",
        "paper_author": "Cao Y.",
        "affiliation_name": "Institute of Information Engineering",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60273040",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Natural Language Processing: from Bedside to Everywhere",
        "publication": "Yearbook of Medical Informatics",
        "citied_by": "28",
        "cover_date": "2022-12-04",
        "Abstract": "Objectives: Owing to the rapid progress of natural language processing (NLP), the role of NLP in the medical field has radically gained considerable attention from both NLP and medical informatics. Although numerous medical NLP papers are published annually, there is still a gap between basic NLP research and practical product development. This gap raises questions, such as what has medical NLP achieved in each medical field, and what is the burden for the practical use of NLP? This paper aims to clarify the above questions. Methods: We explore the literature on potential NLP products/services applied to various medical/clinical/healthcare areas. Results: This paper introduces clinical applications (bedside applications), in which we introduce the use of NLP for each clinical department, internal medicine, pre-surgery, post-surgery, oncology, radiology, pathology, psychiatry, rehabilitation, obstetrics, and gynecology. Also, we clarify technical problems to be addressed for encouraging bedside applications based on NLP. Conclusions: These results contribute to discussions regarding potentially feasible NLP applications and highlight research gaps for future studies.",
        "DOI": "10.1055/s-0042-1742510",
        "paper_author": "Aramaki E.",
        "affiliation_name": "Nara Institute of Science and Technology",
        "affiliation_city": "Ikoma",
        "affiliation_country": "Japan",
        "affiliation_id": "60025017",
        "affiliation_state": "Nara"
    },
    {
        "paper_title": "Year 2021: COVID-19, Information Extraction and BERTization among the Hottest Topics in Medical Natural Language Processing",
        "publication": "Yearbook of Medical Informatics",
        "citied_by": "1",
        "cover_date": "2022-12-04",
        "Abstract": "Objectives: Analyze the content of publications within the medical natural language processing (NLP) domain in 2021. Methods: Automatic and manual preselection of publications to be reviewed, and selection of the best NLP papers of the year. Analysis of the important issues. Results: Four best papers have been selected in 2021. We also propose an analysis of the content of the NLP publications in 2021, all topics included. Conclusions: The main issues addressed in 2021 are related to the investigation of COVID-related questions and to the further adaptation and use of transformer models. Besides, the trends from the past years continue, such as information extraction and use of information from social networks.",
        "DOI": "10.1055/s-0042-1742547",
        "paper_author": "Grabar N.",
        "affiliation_name": "Université de Lille",
        "affiliation_city": "Lille",
        "affiliation_country": "France",
        "affiliation_id": "60104665",
        "affiliation_state": "Hauts-de-France"
    },
    {
        "paper_title": "Clinical Informatics Approaches to Understand and Address Cancer Disparities",
        "publication": "Yearbook of Medical Informatics",
        "citied_by": "3",
        "cover_date": "2022-12-04",
        "Abstract": "Objectives: Disparities in cancer incidence and outcomes across race, ethnicity, gender, socioeconomic status, and geography are well-documented, but their etiologies are often poorly understood and multifactorial. Clinical informatics can provide tools to better understand and address these disparities by enabling high-throughput analysis of multiple types of data. Here, we review recent efforts in clinical informatics to study and measure disparities in cancer. Methods: We carried out a narrative review of clinical informatics studies related to cancer disparities and bias published from 2018-2021, with a focus on domains such as real-world data (RWD) analysis, natural language processing (NLP), radiomics, genomics, proteomics, metabolomics, and metagenomics. Results: Clinical informatics studies that investigated cancer disparities across race, ethnicity, gender, and age were identified. Most cancer disparities work within clinical informatics used RWD analysis, NLP, radiomics, and genomics. Emerging applications of clinical informatics to understand cancer disparities, including proteomics, metabolomics, and metagenomics, were less well represented in the literature but are promising future research avenues. Algorithmic bias was identified as an important consideration when developing and implementing cancer clinical informatics techniques, and efforts to address this bias were reviewed. Conclusions: In recent years, clinical informatics has been used to probe a range of data sources to understand cancer disparities across different populations. As informatics tools become integrated into clinical decision-making, attention will need to be paid to ensure that algorithmic bias does not amplify existing disparities. In our increasingly interconnected medical systems, clinical informatics is poised to untap the full potential of multi-platform health data to address cancer disparities.",
        "DOI": "10.1055/s-0042-1742511",
        "paper_author": "Chaunzwa T.L.",
        "affiliation_name": "Harvard Medical School",
        "affiliation_city": "Boston",
        "affiliation_country": "United States",
        "affiliation_id": "60002746",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Developing Validated Tools to Identify Pulmonary Embolism in Electronic Databases: Rationale and Design of the PE-EHR+ Study",
        "publication": "Thrombosis and Haemostasis",
        "citied_by": "10",
        "cover_date": "2022-12-03",
        "Abstract": "Background Contemporary pulmonary embolism (PE) research, in many cases, relies on data from electronic health records (EHRs) and administrative databases that use International Classification of Diseases (ICD) codes. Natural language processing (NLP) tools can be used for automated chart review and patient identification. However, there remains uncertainty with the validity of ICD-10 codes or NLP algorithms for patient identification. Methods The PE-EHR+ study has been designed to validate ICD-10 codes as Principal Discharge Diagnosis, or Secondary Discharge Diagnoses, as well as NLP tools set out in prior studies to identify patients with PE within EHRs. Manual chart review by two independent abstractors by predefined criteria will be the reference standard. Sensitivity, specificity, and positive and negative predictive values will be determined. We will assess the discriminatory function of code subgroups for intermediate- and high-risk PE. In addition, accuracy of NLP algorithms to identify PE from radiology reports will be assessed. Results A total of 1,734 patients from the Mass General Brigham health system have been identified. These include 578 with ICD-10 Principal Discharge Diagnosis codes for PE, 578 with codes in the secondary position, and 578 without PE codes during the index hospitalization. Patients within each group were selected randomly from the entire pool of patients at the Mass General Brigham health system. A smaller subset of patients will also be identified from the Yale-New Haven Health System. Data validation and analyses will be forthcoming. Conclusions The PE-EHR+ study will help validate efficient tools for identification of patients with PE in EHRs, improving the reliability of efficient observational studies or randomized trials of patients with PE using electronic databases.",
        "DOI": "10.1055/a-2039-3222",
        "paper_author": "Bikdeli B.",
        "affiliation_name": "Harvard Medical School",
        "affiliation_city": "Boston",
        "affiliation_country": "United States",
        "affiliation_id": "60002746",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Validity, Reliability, and Significance: Empirical Methods for NLP and Data Science",
        "publication": "Synthesis Lectures on Human Language Technologies",
        "citied_by": "4",
        "cover_date": "2022-12-03",
        "Abstract": "Empirical methods are means to answering methodological questions of empirical sciences by statistical techniques. The methodological questions addressed in this book include the problems of validity, reliability, and significance. In the case of machine learning, these correspond to the questions of whether a model predicts what it purports to predict, whether a model's performance is consistent across replications, and whether a performance difference between two models is due to chance, respectively. The goal of this book is to answer these questions by concrete statistical tests that can be applied to assess validity, reliability, and significance of data annotation and machine learning prediction in the fields of NLP and data science. Our focus is on model-based empirical methods where data annotations and model predictions are treated as training data for interpretable probabilistic models from the well-understood families of generalized additive models (GAMs) and linear mixed effects models (LMEMs). Based on the interpretable parameters of the trained GAMs or LMEMs, the book presents model-based statistical tests such as a validity test that allows detecting circular features that circumvent learning. Furthermore, the book discusses a reliability coefficient using variance decomposition based on random effect parameters of LMEMs. Last, a significance test based on the likelihood ratio of nested LMEMs trained on the performance scores of two machine learning models is shown to naturally allow the inclusion of variations in meta-parameter settings into hypothesis testing, and further facilitates a refined system comparison conditional on properties of input data. This book can be used as an introduction to empirical methods for machine learning in general, with a special focus on applications in NLP and data science. The book is self-contained, with an appendix on the mathematical background on GAMs and LMEMs, and with an accompanying webpage including R code to replicate experiments presented in the book.",
        "DOI": "10.2200/S01137ED1V01Y202110HLT055",
        "paper_author": "Riezler S.",
        "affiliation_name": "Universität Heidelberg",
        "affiliation_city": "Heidelberg",
        "affiliation_country": "Germany",
        "affiliation_id": "60016908",
        "affiliation_state": "Baden-Wurttemberg"
    },
    {
        "paper_title": "AmericasNLI: Machine translation and natural language inference systems for Indigenous languages of the Americas",
        "publication": "Frontiers in Artificial Intelligence",
        "citied_by": "8",
        "cover_date": "2022-12-02",
        "Abstract": "Little attention has been paid to the development of human language technology for truly low-resource languages—i.e., languages with limited amounts of digitally available text data, such as Indigenous languages. However, it has been shown that pretrained multilingual models are able to perform crosslingual transfer in a zero-shot setting even for low-resource languages which are unseen during pretraining. Yet, prior work evaluating performance on unseen languages has largely been limited to shallow token-level tasks. It remains unclear if zero-shot learning of deeper semantic tasks is possible for unseen languages. To explore this question, we present AmericasNLI, a natural language inference dataset covering 10 Indigenous languages of the Americas. We conduct experiments with pretrained models, exploring zero-shot learning in combination with model adaptation. Furthermore, as AmericasNLI is a multiway parallel dataset, we use it to benchmark the performance of different machine translation models for those languages. Finally, using a standard transformer model, we explore translation-based approaches for natural language inference. We find that the zero-shot performance of pretrained models without adaptation is poor for all languages in AmericasNLI, but model adaptation via continued pretraining results in improvements. All machine translation models are rather weak, but, surprisingly, translation-based approaches to natural language inference outperform all other models on that task.",
        "DOI": "10.3389/frai.2022.995667",
        "paper_author": "Kann K.",
        "affiliation_name": "College of Engineering and Applied Science",
        "affiliation_city": "Boulder",
        "affiliation_country": "United States",
        "affiliation_id": "60154476",
        "affiliation_state": "CO"
    },
    {
        "paper_title": "Using Primary Care Clinical Text Data and Natural Language Processing to Identify Indicators of COVID-19 in Toronto, Canada",
        "publication": "PLOS Digital Health",
        "citied_by": "0",
        "cover_date": "2022-12-01",
        "Abstract": "The objective of this study was to investigate whether a rule-based natural language processing (NLP) system, applied to primary care clinical text data, could be used to monitor COVID-19 viral activity in Toronto, Canada. We employed a retrospective cohort design. We included primary care patients with a clinical encounter between January 1, 2020 and December 31, 2020 at one of 44 participating clinical sites. During the study timeframe, Toronto first experienced a COVID-19 outbreak between March-2020 and June-2020; followed by a second viral resurgence from October-2020 through December-2020. We used an expert derived dictionary, pattern matching tools and contextual analyzer to classify primary care documents as 1) COVID-19 positive, 2) COVID-19 negative, or 3) unknown COVID-19 status. We applied the COVID-19 biosurveillance system across three primary care electronic medical record text streams: 1) lab text, 2) health condition diagnosis text and 3) clinical notes. We enumerated COVID-19 entities in the clinical text and estimated the proportion of patients with a positive COVID-19 record. We constructed a primary care COVID-19 NLP-derived time series and investigated its correlation with independent/external public health series: 1) lab confirmed COVID-19 cases, 2) COVID-19 hospitalizations, 3) COVID-19 ICU admissions, and 4) COVID-19 intubations. A total of 196,440 unique patients were observed over the study timeframe, of which 4,580 (2.3%) had at least one positive COVID-19 document in their primary care electronic medical record. Our NLP-derived COVID-19 time series describing the temporal dynamics of COVID-19 positivity status over the study timeframe demonstrated a pattern/trend which strongly mirrored that of other external public health series under investigation. We conclude that primary care text data passively collected from electronic medical record systems represent a high quality, low-cost source of information for monitoring/surveilling COVID-19 impacts on community health.",
        "DOI": "10.1371/journal.pdig.0000150",
        "paper_author": "Meaney C.",
        "affiliation_name": "University of Toronto Faculty of Medicine",
        "affiliation_city": "Toronto",
        "affiliation_country": "Canada",
        "affiliation_id": "60021600",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "Artificial Intelligence in the Assessment and Management of Nutrition and Metabolism in Liver Disease",
        "publication": "Current Hepatology Reports",
        "citied_by": "1",
        "cover_date": "2022-12-01",
        "Abstract": "Purpose of Review: Artificial Intelligence (AI) has the potential to transform detection and management of nutrition-related complications through advances in wearable technology, mobile applications, and machine learning. The literature, however, lacks studies specific to the interplay between AI and nutrition in patients with liver disease. The aim of this article is to address the current state of AI in nutrition and metabolic liver disease. We seek to understand how AI can be utilized to address gaps in the care of patients with liver disease, particularly as it relates to their nutrition. Recent Findings: Advances in AI, particularly in deep learning, have led to improved performance of diagnostic and prognostic models across many disease processes. AI-based systems in this realm include predictive modeling, natural language processing (NLP), and image recognition. Summary: Ultimately, large-scale studies are needed to validate the use of AI in assessing and improving nutrition in this population.",
        "DOI": "10.1007/s11901-022-00594-0",
        "paper_author": "Schmidt K.A.",
        "affiliation_name": "Mayo Clinic in Rochester, Minnesota",
        "affiliation_city": "Rochester",
        "affiliation_country": "United States",
        "affiliation_id": "60032610",
        "affiliation_state": "MN"
    },
    {
        "paper_title": "Natural language processing to identify lupus nephritis phenotype in electronic health records",
        "publication": "BMC Medical Informatics and Decision Making",
        "citied_by": "4",
        "cover_date": "2022-12-01",
        "Abstract": "Background: Systemic lupus erythematosus (SLE) is a rare autoimmune disorder characterized by an unpredictable course of flares and remission with diverse manifestations. Lupus nephritis, one of the major disease manifestations of SLE for organ damage and mortality, is a key component of lupus classification criteria. Accurately identifying lupus nephritis in electronic health records (EHRs) would therefore benefit large cohort observational studies and clinical trials where characterization of the patient population is critical for recruitment, study design, and analysis. Lupus nephritis can be recognized through procedure codes and structured data, such as laboratory tests. However, other critical information documenting lupus nephritis, such as histologic reports from kidney biopsies and prior medical history narratives, require sophisticated text processing to mine information from pathology reports and clinical notes. In this study, we developed algorithms to identify lupus nephritis with and without natural language processing (NLP) using EHR data from the Northwestern Medicine Enterprise Data Warehouse (NMEDW). Methods: We developed five algorithms: a rule-based algorithm using only structured data (baseline algorithm) and four algorithms using different NLP models. The first NLP model applied simple regular expression for keywords search combined with structured data. The other three NLP models were based on regularized logistic regression and used different sets of features including positive mention of concept unique identifiers (CUIs), number of appearances of CUIs, and a mixture of three components (i.e. a curated list of CUIs, regular expression concepts, structured data) respectively. The baseline algorithm and the best performing NLP algorithm were externally validated on a dataset from Vanderbilt University Medical Center (VUMC). Results: Our best performing NLP model incorporated features from both structured data, regular expression concepts, and mapped concept unique identifiers (CUIs) and showed improved F measure in both the NMEDW (0.41 vs 0.79) and VUMC (0.52 vs 0.93) datasets compared to the baseline lupus nephritis algorithm. Conclusion: Our NLP MetaMap mixed model improved the F-measure greatly compared to the structured data only algorithm in both internal and external validation datasets. The NLP algorithms can serve as powerful tools to accurately identify lupus nephritis phenotype in EHR for clinical research and better targeted therapies.",
        "DOI": "10.1186/s12911-024-02420-7",
        "paper_author": "Deng Y.",
        "affiliation_name": "Northwestern University Feinberg School of Medicine",
        "affiliation_city": "Chicago",
        "affiliation_country": "United States",
        "affiliation_id": "60013227",
        "affiliation_state": "IL"
    },
    {
        "paper_title": "ERP Staff versus AI recruitment with employment real-time big data",
        "publication": "Discover Artificial Intelligence",
        "citied_by": "6",
        "cover_date": "2022-12-01",
        "Abstract": "The purpose of this study was to evaluate the effectiveness of using natural language processing (NLP) artificial intelligence (AI) in enterprise resources planning (ERP) to identify specialized job candidates in real-time big data—globally across the internet. The central problem was that companies using traditional methods for recruiting remote specialists were missing good candidates because the skilled employees may not be looking for a job yet they may be receptive to an offer. The auxiliary problem was too much data on the internet for human resources management (HRM) staff to make sense of to find the best-fitting candidate. Thus, the research question was: could NLP AI identify good candidates for ERP remote specialist jobs using internet real-time big data? Job criteria were developed using machine learning to identify key skills from existing staff in a case study company. The skills were transformed into ERP remote specialists hiring criteria. The NLP AI software was activated to find the best candidate. The HRM staff at the case study company evaluated the effectiveness of the candidate selected by the NLP AI. The case study company set 70% as the acceptable mean evaluation score. ANOVA was used to determine if HRM staff agreed about their evaluation scores. A Z-test was used to determine if the NLP AI was faster than the mean time needed for HRM to select ERP candidates. The results were that the NLP AI outperformed the humans by a factor of almost 8 h. All HRM staff agreed that the NLP AI was effective in selecting a candidate to match the hiring criteria. The proposed approach might facilitate the research and development of big data, data analytics, NLP AI, and HRM process improvement.",
        "DOI": "10.1007/s44163-022-00037-1",
        "paper_author": "Strang K.D.",
        "affiliation_name": "Data Analytics",
        "affiliation_city": "St. Thomas",
        "affiliation_country": "United States",
        "affiliation_id": "129899385",
        "affiliation_state": "VI"
    },
    {
        "paper_title": "Usable Amharic text corpus for natural language processing applications",
        "publication": "Applied Corpus Linguistics",
        "citied_by": "3",
        "cover_date": "2022-12-01",
        "Abstract": "In this paper, we describe the preparation of a usable Amharic text corpus for different Natural Language Processing (NLP) applications. Natural language applications, such as document classification, topic modeling, machine translation, speech recognition, and others, suffer greatly from a lack of digital resources. This is especially true for Amharic, a resource-constrained, morphologically rich, and complex language. In response to this, a total of 67,739 Amharic news documents consisting of 8 different categories from online sources are collected. The collected corpus passes through a number of pre-processing steps including; data cleaning, text normalization and punctuation correction. To validate the usability of the collected corpora from different domains, a baseline document classification experiment was conducted. Experimental results show that, 84.53% accuracy is registered using deep learning in the absence of linguistic information. Finding indicated that it is possible to use the prepared corpora for different natural language applications in the absence of linguistic resources such as stemmer and dictionary despite the complexity of Amharic language. We are further working towards Amharic news document classification by incorporating a linguistic independent stop-word detection, stemming and unsupervised morphological segmentation of Amharic documents.",
        "DOI": "10.1016/j.acorp.2022.100033",
        "paper_author": "Woldeyohannis M.M.",
        "affiliation_name": "Addis Ababa University",
        "affiliation_city": "Addis Ababa",
        "affiliation_country": "Ethiopia",
        "affiliation_id": "60071181",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Research on Differentiated Guidance of Network Public Opinion Based on Echo Chamber Super-network",
        "publication": "Information studies: Theory and Application",
        "citied_by": "1",
        "cover_date": "2022-12-01",
        "Abstract": "[Purpose/ significance] In view of the echo chamber phenomenon in the dissemination of network public opinion, this study proposes a differentiated guidance model for network public opinion based on echo chamber super-network, which provides practical support for network public opinion controllers to formulate guidance schemes. [Method/ process] Firstly, this paper constructs three sub-networks of users, events and echo chambers, and generates the mapping relationship between the three sub-networks according to the complex interaction behaviors among users, and then constructs the echo chamber supernetwork. Secondly, NLP and other technologies are used to extract feature vector and similarity calculation for the public opinion information in the event network, so as to screen out the target echo chamber and identify the target user according to the mapping relationship. Finally, a differentiated guidance model of network public opinion based on echo chamber super-network is established, which adopts different guidance strategies for target echo chamber users according to the different emotional polarities of public opinion events. [Result/ conclusion] This paper collects data including 40 public opinion events, 26943 forwarded comments and 26113 users for experimental analysis. The results show that the model can effectively guide the target users and thus affect the public opinion situation, and provide theoretical reference for the governance of network public opinion.",
        "DOI": "10.16353/j.cnki.1000-7490.2022.12.018",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A computational approach to analyzing climate strategies of cities pledging net zero",
        "publication": "npj Urban Sustainability",
        "citied_by": "14",
        "cover_date": "2022-12-01",
        "Abstract": "Cities have become primary actors on climate change and are increasingly setting goals aimed at net-zero emissions, which warrants closer examination to understand how they intend to meet these goals. The incomplete and heterogeneous nature of city climate policy documents, however, has made systemic analysis challenging. We analyze 318 climate action documents from cities with net-zero targets using machine learning-based natural language processing (NLP) techniques. We aim to accomplish two goals: (1) determine text patterns that predict ‘ambitious’ net-zero targets; and (2) perform a sectoral analysis to identify patterns and trade-offs in climate action themes. We find that cities with ambitious climate actions tend to emphasize quantitative metrics and specific high-emitting sectors in their plans. Cities predominantly emphasize energy-related actions in their plans, but often at the expense of other sectors, including land-use and climate impacts. The method presented in this paper provides a replicable, scalable approach to analyzing climate action plans and a first step towards facilitating cross-city learning.",
        "DOI": "10.1038/s42949-022-00065-x",
        "paper_author": "Sachdeva S.",
        "affiliation_name": "The University of North Carolina at Chapel Hill",
        "affiliation_city": "Chapel Hill",
        "affiliation_country": "United States",
        "affiliation_id": "60025111",
        "affiliation_state": "NC"
    },
    {
        "paper_title": "Development and evaluation of an interoperable natural language processing system for identifying pneumonia across clinical settings of care and institutions",
        "publication": "JAMIA Open",
        "citied_by": "8",
        "cover_date": "2022-12-01",
        "Abstract": "Objective: To evaluate the feasibility, accuracy, and interoperability of a natural language processing (NLP) system that extracts diagnostic assertions of pneumonia in different clinical notes and institutions. Materials and Methods: A rule-based NLP system was designed to identify assertions of pneumonia in 3 types of clinical notes from electronic health records (EHRs): emergency department notes, radiology reports, and discharge summaries. The lexicon and classification logic were tailored for each note type. The system was first developed and evaluated using annotated notes from the Department of Veterans Affairs (VA). Interoperability was assessed using data from the University of Utah (UU). Results: The NLP system was comprised of 782 rules and achieved moderate-to-high performance in all 3 note types in VA (precision/recall/f1: emergency = 88.1/86.0/87.1; radiology = 71.4/96.2/82.0; discharge = 88.3/93.0/90.1). When applied to UU data, performance was maintained in emergency and radiology but decreased in discharge summaries (emergency = 84.7/94.3/89.3; radiology = 79.7/100.0/87.9; discharge = 65.5/92.7/76.8). Customization with 34 additional rules increased performance for all note types (emergency = 89.3/94.3/91.7; radiology = 87.0/100.0/93.1; discharge = 75.0/95.1/83.4). Conclusion: NLP can be used to accurately identify the diagnosis of pneumonia across different clinical settings and institutions. A limited amount of customization to account for differences in lexicon, clinical definition of pneumonia, and EHR structure can achieve high accuracy without substantial modification.",
        "DOI": "10.1093/jamiaopen/ooac114",
        "paper_author": "Chapman A.B.",
        "affiliation_name": "Veterans Health Administration, U.S. Department of Veteran Affairs",
        "affiliation_city": null,
        "affiliation_country": "United States",
        "affiliation_id": "60002586",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Automated interpretation of stress echocardiography reports using natural language processing",
        "publication": "European Heart Journal - Digital Health",
        "citied_by": "3",
        "cover_date": "2022-12-01",
        "Abstract": "Aims: Stress echocardiography (SE) findings and interpretations are commonly documented in free-text reports. Reusing SE results requires laborious manual reviews. This study aimed to develop and validate an automated method for abstracting SE reports in a large cohort. Methods and results: This study included adult patients who had SE within 30 days of their emergency department visit for suspected acute coronary syndrome in a large integrated healthcare system. An automated natural language processing (NLP) algorithm was developed to abstract SE reports and classify overall SE results into normal, non-diagnostic, infarction, and ischaemia categories. Randomly selected reports (n = 140) were double-blindly reviewed by cardiologists to perform criterion validity of the NLP algorithm. Construct validity was tested on the entire cohort using abstracted SE data and additional clinical variables. The NLP algorithm abstracted 6346 consecutive SE reports. Cardiologists had good agreements on the overall SE results on the 140 reports: Kappa (0.83) and intraclass correlation coefficient (0.89). The NLP algorithm achieved 98.6% specificity and negative predictive value, 95.7% sensitivity, positive predictive value, and F-score on the overall SE results and near-perfect scores on ischaemia findings. The 30-day acute myocardial infarction or death outcomes were highest among patients with ischaemia (5.0%), followed by infarction (1.4%), non-diagnostic (0.8%), and normal (0.3%) results. We found substantial variations in the format and quality of SE reports, even within the same institution. Conclusions: Natural language processing is an accurate and efficient method for abstracting unstructured SE reports. This approach creates new opportunities for research, public health measures, and care improvement.",
        "DOI": "10.1093/ehjdh/ztac047",
        "paper_author": "Zheng C.",
        "affiliation_name": "Kaiser Permanente",
        "affiliation_city": "Oakland",
        "affiliation_country": "United States",
        "affiliation_id": "60017457",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Historical narratives and war representations in the collective memory of Runet communities: Temporal trajectories and semantic networks",
        "publication": "Rusin",
        "citied_by": "2",
        "cover_date": "2022-12-01",
        "Abstract": "The article analyses the collective representation of war narratives in the Russian social network VKontakte in the context of the problems of national self-consciousness and national identity. The material for the study was extracted by automated methods using the VKontakte open API. It amounts to 332,781 unique posts from open historical communities from January 1, 2020 to May 9, 2022. Using the PolyAnalyst platform, the authors found out that the word “war” is the most frequent and most significant (the metric takes into account the average frequency of a word in all texts) in this text collection. A sample of texts with the word “war” (49,736 posts) was subjected to additional analysis with a set of NLP methods to study key narratives and semantic connections. The theoretical and methodological basis of the study is the principles and approaches of modern narratology and the history of memory. The analysis of social media big data has confirmed that the dramatic historical fate of Russia, as a natural consequence of the factors of vast territory, extended borders, and abundance of natural resources, has shaped the mentality of Russians to be always ready for war. Thematic clustering of texts has confirmed the dominance of the practices of commemoration of the Great Patriotic War, reflected the etatism and etacratism of the Russian consciousness, demonstrated public attention to representations of war in art and science, and discovered the “trench truth” block and its place in the collective war representations. Narratives are often rich in details and facts, pragmatism, prosaic and rationalism. Empirical material confirms that the 20th century certainly occupies a special significance in the memory of Russians about wars. The authors comment on the rating of references to wars in the studied text collection and indicate the opposition of the Great Patriotic War to the Second World War, and the Patriotic War of 1812 to the Napoleonic Wars of the early 19th century. Some attention is also given to the issues of information wars in the modern world. The presented war narrative is of interest as the foundation of the “collective imaginary”, on the basis of which the prospects and critical boundaries of the image of the future are constructed.",
        "DOI": "10.17223/18572685/70/16",
        "paper_author": "Trubnikova N.V.",
        "affiliation_name": "Tomsk State University",
        "affiliation_city": "Tomsk",
        "affiliation_country": "Russian Federation",
        "affiliation_id": "60016896",
        "affiliation_state": "Tomsk Oblast"
    },
    {
        "paper_title": "Using deep learning-based natural language processing to identify reasons for statin nonuse in patients with atherosclerotic cardiovascular disease",
        "publication": "Communications Medicine",
        "citied_by": "14",
        "cover_date": "2022-12-01",
        "Abstract": "Background: Statins conclusively decrease mortality in atherosclerotic cardiovascular disease (ASCVD), the leading cause of death worldwide, and are strongly recommended by guidelines. However, real-world statin utilization and persistence are low, resulting in excess mortality. Identifying reasons for statin nonuse at scale across health systems is crucial to developing targeted interventions to improve statin use. Methods: We developed and validated deep learning-based natural language processing (NLP) approaches (Clinical Bidirectional Encoder Representations from Transformers [BERT]) to classify statin nonuse and reasons for statin nonuse using unstructured electronic health records (EHRs) from a diverse healthcare system. Results: We present data from a cohort of 56,530 ASCVD patients, among whom 21,508 (38%) lack guideline-directed statin prescriptions and statins listed as allergies in structured EHR portions. Of these 21,508 patients without prescriptions, only 3,929 (18%) have any discussion of statin use or nonuse in EHR documentation. The NLP classifiers identify statin nonuse with an area under the curve (AUC) of 0.94 (95% CI 0.93–0.96) and reasons for nonuse with a weighted-average AUC of 0.88 (95% CI 0.86–0.91) when evaluated against manual expert chart review in a held-out test set. Clinical BERT identifies key patient-level reasons (side-effects, patient preference) and clinician-level reasons (guideline-discordant practices) for statin nonuse, including differences by type of ASCVD and patient race/ethnicity. Conclusions: Our deep learning NLP classifiers can identify crucial gaps in statin nonuse and reasons for nonuse in high-risk populations to support education, clinical decision support, and potential pathways for health systems to address ASCVD treatment gaps.",
        "DOI": "10.1038/s43856-022-00157-w",
        "paper_author": "Sarraju A.",
        "affiliation_name": "Stanford University School of Medicine",
        "affiliation_city": "Stanford",
        "affiliation_country": "United States",
        "affiliation_id": "60032838",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Gender Asymmetry of Visegrád Group Languages as Reflected by Word Embeddings",
        "publication": "Jazykovedny Casopis",
        "citied_by": "3",
        "cover_date": "2022-12-01",
        "Abstract": "Today, word embeddings have become a standard method in natural language processing, largely due to the availability of large language corpora. The models effectively reflect the semantic relationships between words without any additional linguistic input. Recently, more emphasis has been placed on interpreting the seemingly discriminatory results of some queries, with the goal of de-biasing language models. However, if we consider the vector space to be a reasonably valid model of a linguistic semantic space, does not the asymmetry and subsequent discrimination in word embeddings reflect the (average) discriminatory tendencies inherent in the language? This article explores word embedding models for the Visegrád group languages and we apply basic vector arithmetic to demonstrate the basic language asymmetry present in the models. It is well known that in English models, vector transfers result in eerily accurate predictions when swapping genders (the famous king - man + woman = queen), but these transfers also result in rather uncomplimentary roles for certain occupations (doctor - man + woman = nurse, or computer programmer - man + woman = homemaker). The article explores similar transfers in models of V4 languages - Slovak, Czech, Polish, and Hungarian. With Hungarian gender neutrality, Polish strong generic masculine, and close parallels between Slovak and Czech, we hope to uncover interesting similarities and differences in gender asymmetry in these languages, based on real language data.",
        "DOI": "10.2478/jazcas-2023-0013",
        "paper_author": "Garabík R.",
        "affiliation_name": "Slovak Academy of Sciences",
        "affiliation_city": "Bratislava",
        "affiliation_country": "Slovakia",
        "affiliation_id": "60010330",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Nearest neighbor label propagation for density peak clustering",
        "publication": "Journal of Frontiers of Computer Science and Technology",
        "citied_by": "0",
        "cover_date": "2022-12-01",
        "Abstract": "Dynamic graph-based label propagation for density peaks clustering (DPC-DLP) is an improved algorithm of density peaks clustering (DPC). The related parameters involved in the algorithm are too complex and the algorithm uses labeled data in each iteration, which will lead to the expansion of label errors and the deterioration of clustering effect due to too many iterations. To solve the above problems, this paper proposes a nearest neighbor label propagation for density peak clustering (DPC-NLP), which mainly has three steps. Firstly, the local density and the minimum distance are used to score the sample points and the clustering center is determined according to the score. Then, the clustering backbone is formed by the labels of the clustering center in its nearest neighbor. Finally, the label propagation method based on the nearest neighbor is used to propagate the labels of the clustering backbone to the remaining samples and the final clustering results are formed. The nearest neighbor label propagation algorithm takes full account of the structural association between samples, constantly updates the state of data in the process of propagation, and uses more sufficient information to improve the accuracy of allocation. The algorithm is verified on the synthetic and real-world datasets and compared with the current mainstream clustering algorithms. Experimental results show that DPC-NLP is superior in performance and robustness and can deal with complex data such as manifold and nonlinear data.",
        "DOI": "10.3778/j.issn.1673-9418.2104019",
        "paper_author": "Song P.",
        "affiliation_name": "Jiangnan University",
        "affiliation_city": "Wuxi",
        "affiliation_country": "China",
        "affiliation_id": "60007029",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "Application of Equilibrium Optimizer Algorithm for Solving Linear and non Linear Coordination of Directional Overcurrent Relays",
        "publication": "Advances in Electrical and Electronic Engineering",
        "citied_by": "3",
        "cover_date": "2022-12-01",
        "Abstract": "The safety and reliability of an electrical network depend on the performance of the protections utilized. Therefore, the optimal coordination of the pro-tective devices plays an essential role. In this paper, a new algorithm, Equilibrium Optimizer (EO), which is based on the physical equation of the mass balance, is implemented in the problem of the Optimal Coordination of Directional Overcurrent Relays (DOCRs). Moreover, the proposed method uses Linear Programming (LP), Nonlinear Programming (NLP) and Mixed-Integer Nonlinear Programming (MINLP) in order to optimize the Time Dial Setting (TDS), as well as the Plug Setting (PS), satisfying all possible constraints. Additionally, the performance of EO is evaluated using several benchmarks with different topologies. The results demonstrated the applicability and efficacy of the proposed approach. A comparison with other stud-ies reported in specialized literature is provided to demonstrate the benefits of the proposed approach.",
        "DOI": "10.15598/aeee.v20i4.4647",
        "paper_author": "Guerraiche K.",
        "affiliation_name": "Ecole Supérieure en Génie Electrique et Energétique d’Oran",
        "affiliation_city": "Oran",
        "affiliation_country": "Algeria",
        "affiliation_id": "60105447",
        "affiliation_state": "Oran"
    },
    {
        "paper_title": "When BERT Started Traveling: TourBERT—A Natural Language Processing Model for the Travel Industry",
        "publication": "Digital",
        "citied_by": "6",
        "cover_date": "2022-12-01",
        "Abstract": "In recent years, Natural Language Processing (NLP) has become increasingly important for extracting new insights from unstructured text data, and pre-trained language models now have the ability to perform state-of-the-art tasks like topic modeling, text classification, or sentiment analysis. Currently, BERT is the most widespread and widely used model, but it has been shown that a potential to optimize BERT can be applied to domain-specific contexts. While a number of BERT models that improve downstream tasks’ performance for other domains already exist, an optimized BERT model for tourism has yet to be revealed. This study thus aimed to develop and evaluate TourBERT, a pre-trained BERT model for the tourism industry. It was trained from scratch and outperforms BERT-Base in all tourism-specific evaluations. Therefore, this study makes an essential contribution to the growing importance of NLP in tourism by providing an open-source BERT model adapted to tourism requirements and particularities.",
        "DOI": "10.3390/digital2040030",
        "paper_author": "Arefeva V.",
        "affiliation_name": "Johannes Kepler University Linz",
        "affiliation_city": "Linz",
        "affiliation_country": "Austria",
        "affiliation_id": "60021931",
        "affiliation_state": "Upper Austria"
    },
    {
        "paper_title": "Mental training for young athlete: A case of study of NLP practice",
        "publication": "SSM - Mental Health",
        "citied_by": "5",
        "cover_date": "2022-12-01",
        "Abstract": "Aims: To examine the effect of neurolinguistic programming (NlP) and its processes on psychosomatic skills (cognitive anxiety and somatic anxiety), basic skills (self-confidence and physical self-esteem), and athletic performance of young Tunisian swimmers. Materials: For this study, we use the NLP techniques for mental preparation. Semi-structured interviews were used with the tow psychological scale to measure mental skills. Sportive performance were measured before and after intervention. Methods: This study is quantitative research with the pretest-posttest design. Six adolescent swimmers (3 girls and 3 boys); M age ​= ​15.8 years participated for six months in a mental preparation program based on neurolinguistic programming techniques. Result: The results of this work showed better management of anxiety during the competition, as well as an improvement in self-confidence and physical self-esteem. We also note an important amelioration in the athlete's performance (from 0.77% to 2.5%). Conclusion: In view of the positive results obtained during this study, especially in terms of anxiety management and performance improvement, we suggest more exploration of the effect of different NLP techniques in sport. We also recommend more frequent use of NLP with athletes.",
        "DOI": "10.1016/j.ssmmh.2022.100076",
        "paper_author": "Boughattas W.",
        "affiliation_name": "Taif University",
        "affiliation_city": "Taif",
        "affiliation_country": "Saudi Arabia",
        "affiliation_id": "60091164",
        "affiliation_state": "Makkah al Mukarramah"
    },
    {
        "paper_title": "Influence of Narrative Strategies on Fundraising Outcome: An Exploratory Study of Online Medical Crowdfunding",
        "publication": "Journal of Social Computing",
        "citied_by": "2",
        "cover_date": "2022-12-01",
        "Abstract": "Online medical crowdfunding (OMC) has attracted massive attention and participation in China. Despite its goal to lift the financial burden caused by expensive medical expenditure, little has been done to evaluate its impact on healthcare inequality. We examine the social consequences of OMC based on a large random sample extracted from one of the most widely-used OMC platforms in China. Our analysis shows that a disproportionally high percentage of fundraising campaigns are launched for patients with low socioeconomic status suffering from various illnesses, including many rare diseases. These findings suggest that OMC plays a positive role in providing an alternative channel for disadvantaged patients under the current health insurance system. We further examine whether and the extent to which the narrative style of solicitation text - fundraising campaign description - influences fundraising outcomes using natural language processing (NLP). The results show that expressions conveying optimism tend to result in a higher completion ratio, whereas descriptions engaging in moral mobilization or focusing on financial burden tend to have a negative impact on fundraising outcomes.",
        "DOI": "10.23919/JSC.2022.0015",
        "paper_author": "Zheng L.",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60025278",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Anas platyrhynchos optimizer with deep belief network-based sarcasm detection and classification model",
        "publication": "Journal of Electronic Imaging",
        "citied_by": "0",
        "cover_date": "2022-12-01",
        "Abstract": "Sarcasm is a state of speech where the speaker says something that is externally unfriendly to wound or deride the listener. The design of an automated sarcasm detection model in the domain of natural language processing (NLP) is mainly based on the context of the statement/ statement and sometimes even human beings cannot identify the intrinsic sarcasm in the statement. The recently developed deep learning (DL) models can accomplish better performance than the other traditional models. With this motivation, this study introduces an efficient Anas Platyrhynchos optimizer (APO) with deep belief network (DBN) based sarcasm detection and classification (APODBN-SDC) technique. The proposed APODBN-SDC model intends to properly identify the existence of sarcasm or not. Primarily, the data preprocessing stage encompasses different subprocesses and the preprocessed data can be transformed into the feature vectors using TF-IDFs technique. Finally, the normalized feature vectors are given as input to the DBN model for the detection and classification of sarcasm. Finally, the hyperparameter tuning of the DBN is optimally tuned by the use of APO technique and shows the novelty of the work. The proposed method is tested against the benchmark dataset and the results reported the enhancements of the proposed model with maximum precision of 98.88% and recall of 98.90%.",
        "DOI": "10.1117/1.JEI.32.5.052302",
        "paper_author": "Dakshnamoorthy V.",
        "affiliation_name": "Vellore Institute of Technology",
        "affiliation_city": "Vellore",
        "affiliation_country": "India",
        "affiliation_id": "60010618",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "A Novel Attention-Based BiLSTM-CNN Model in Valence-Arousal Space",
        "publication": "International Journal of Performability Engineering",
        "citied_by": "3",
        "cover_date": "2022-12-01",
        "Abstract": "This paper focuses on analyzing the text sentiment tendency based on the deep learning model and starts with improving the neural network model based on public corpora to provide fine-grained analysis of text sentiment tendency and more accurate predictions. In the existing research, the extraction and utilization of text emotional features are usually based on Valence-Arousal space (VA space), but they do not pay attention to some subjective text details with emotional tendencies, such as the punctuation marks or emotional words, which could in turn to decrease the prediction accuracies made by models. Aiming at this issue, this paper proposes a hybrid Bidirectional Long Short-term Memory (BiLSTM) and Convolution Neural Network (CNN) model with an attention mechanism. Notably, in order to make our models easier to be applied to some light-weight products, we adopt the most basic components of nature language process (NLP) models. Firstly, BiLSTM is used to extract bidirectional context dependency information, and an attention mechanism is exploited to assign different weights to words that play different roles in sentiment judgment. Further, CNN is used to extract the local features of the upper layer's output to ensure the robustness of feature extraction. The experiment shows that the combination of the methods with this order, BiLSTM first and CNN later, can achieve results which are significantly better than the existing baseline models reported in the literature.",
        "DOI": "10.23940/ijpe.22.12.p1.833843",
        "paper_author": "Dai G.",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60025278",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Cytological Diagnostic and Prognostic Methods using Immunocytochemistry (Estrogen Receptor) for Surgical Management of Breast Cancer",
        "publication": "Biomedical and Pharmacology Journal",
        "citied_by": "0",
        "cover_date": "2022-12-01",
        "Abstract": "The purpose of this study was to assess the utility of fine needle aspiration cytology (FNAC), immunocytochemistry(ICC) using estrogen receptor(ER) in diagnosing breast lesions. This was done by comparing it to histopathology with immunohistochemistry(IHC), which serves as the gold standard for diagnosing these lesions. To compare these modalities of investigation,50 samples were collected using FNAC and were compared to the same samples obtained by histopathology. For FNAC the results were as follows, Sensitivity=100%, Specificity=100%,Diagnostic Accuracy=100% Positive Predictive Value(PPV)=100% and Negative Predictive Value(NPV)=100%. For ICC using ER the results were as follows, Sensitivity=100%,Accuracy=100%,Positive Predictive Value=100%,Negative Predictive Value=100%. This indicates that FNAC and ICC using ER can be used as a reliable alternative to gold-standard diagnostic tests when the latter cannot be done due to a lack of resources or in circumstances where there is a need to perform a painless, minimally invasive procedure such as in inoperable breast carcinoma. This study also involved using text data analysis on FNAC reports. On analysis, it was found that the useful words were 11.35% of the data set, implying that the process of normalization, will result in the formation of condensed data, which can then be utilized for assisting clinical chart reviews and clinical decision support systems.",
        "DOI": "10.13005/bpj/2535",
        "paper_author": "Nair R.R.",
        "affiliation_name": "JSS Medical College &amp; Hospital, Mysuru",
        "affiliation_city": "Mysore",
        "affiliation_country": "India",
        "affiliation_id": "60109396",
        "affiliation_state": "KA"
    },
    {
        "paper_title": "Predicting dementia from spontaneous speech using large language models",
        "publication": "PLOS Digital Health",
        "citied_by": "59",
        "cover_date": "2022-12-01",
        "Abstract": "Language impairment is an important biomarker of neurodegenerative disorders such as Alzheimer’s disease (AD). Artificial intelligence (AI), particularly natural language processing (NLP), has recently been increasingly used for early prediction of AD through speech. Yet, relatively few studies exist on using large language models, especially GPT-3, to aid in the early diagnosis of dementia. In this work, we show for the first time that GPT-3 can be utilized to predict dementia from spontaneous speech. Specifically, we leverage the vast semantic knowledge encoded in the GPT-3 model to generate text embedding, a vector representation of the transcribed text from speech, that captures the semantic meaning of the input. We demonstrate that the text embedding can be reliably used to (1) distinguish individuals with AD from healthy controls, and (2) infer the subject’s cognitive testing score, both solely based on speech data. We further show that text embedding considerably outperforms the conventional acoustic feature-based approach and even performs competitively with prevailing fine-tuned models. Together, our results suggest that GPT-3 based text embedding is a viable approach for AD assessment directly from speech and has the potential to improve early diagnosis of dementia.",
        "DOI": "10.1371/journal.pdig.0000168",
        "paper_author": "Agbavor F.",
        "affiliation_name": "Drexel University",
        "affiliation_city": "Philadelphia",
        "affiliation_country": "United States",
        "affiliation_id": "60014662",
        "affiliation_state": "PA"
    }
]