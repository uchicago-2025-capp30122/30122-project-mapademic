[
    {
        "paper_title": "Precision health data: Requirements, challenges and existing techniques for data security and privacy",
        "publication": "Computers in Biology and Medicine",
        "citied_by": "191",
        "cover_date": "2021-02-01",
        "Abstract": "Precision health leverages information from various sources, including omics, lifestyle, environment, social media, medical records, and medical insurance claims to enable personalized care, prevent and predict illness, and precise treatments. It extensively uses sensing technologies (e.g., electronic health monitoring devices), computations (e.g., machine learning), and communication (e.g., interaction between the health data centers). As health data contain sensitive private information, including the identity of patient and carer and medical conditions of the patient, proper care is required at all times. Leakage of these private information affects the personal life, including bullying, high insurance premium, and loss of job due to the medical history. Thus, the security, privacy of and trust on the information are of utmost importance. Moreover, government legislation and ethics committees demand the security and privacy of healthcare data. Besides, the public, who is the data source, always expects the security, privacy, and trust of their data. Otherwise, they can avoid contributing their data to the precision health system. Consequently, as the public is the targeted beneficiary of the system, the effectiveness of precision health diminishes. Herein, in the light of precision health data security, privacy, ethical and regulatory requirements, finding the best methods and techniques for the utilization of the health data, and thus precision health is essential. In this regard, firstly, this paper explores the regulations, ethical guidelines around the world, and domain-specific needs. Then it presents the requirements and investigates the associated challenges. Secondly, this paper investigates secure and privacy-preserving machine learning methods suitable for the computation of precision health data along with their usage in relevant health projects. Finally, it illustrates the best available techniques for precision health data security and privacy with a conceptual system model that enables compliance, ethics clearance, consent management, medical innovations, and developments in the health domain.",
        "DOI": "10.1016/j.compbiomed.2020.104130",
        "paper_author": "Thapa C.",
        "affiliation_name": "Commonwealth Scientific and Industrial Research Organisation",
        "affiliation_city": "Canberra",
        "affiliation_country": "Australia",
        "affiliation_id": "60029470",
        "affiliation_state": "ACT"
    },
    {
        "paper_title": "CheXclusion: Fairness gaps in deep chest X-ray classifiers",
        "publication": "Pacific Symposium on Biocomputing",
        "citied_by": "145",
        "cover_date": "2021-01-01",
        "Abstract": "Machine learning systems have received much attention recently for their ability to achieve expert-level performance on clinical tasks, particularly in medical imaging. Here, we examine the extent to which state-of-the-art deep learning classifiers trained to yield diagnostic labels from X-ray images are biased with respect to protected attributes. We train convolution neural networks to predict 14 diagnostic labels in 3 prominent public chest X-ray datasets: MIMIC-CXR, Chest-Xray8, CheXpert, as well as a multi-site aggregation of all those datasets. We evaluate the TPR disparity – the difference in true positive rates (TPR) – among different protected attributes such as patient sex, age, race, and insurance type as a proxy for socioeconomic status. We demonstrate that TPR disparities exist in the state-of-the-art classifiers in all datasets, for all clinical tasks, and all subgroups. A multi-source dataset corresponds to the smallest disparities, suggesting one way to reduce bias. We find that TPR disparities are not significantly correlated with a subgroup’s proportional disease burden. As clinical models move from papers to products, we encourage clinical decision makers to carefully audit for algorithmic disparities prior to deployment. Our supplementary materials can be found at, http://www.marzyehghassemi.com/chexclusion-supp-3/.",
        "DOI": "NA",
        "paper_author": "Seyyed-Kalantari L.",
        "affiliation_name": "University of Toronto",
        "affiliation_city": "Toronto",
        "affiliation_country": "Canada",
        "affiliation_id": "60016849",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "Predictive modeling of depression and anxiety using electronic health records and a novel machine learning approach with artificial intelligence",
        "publication": "Scientific Reports",
        "citied_by": "128",
        "cover_date": "2021-12-01",
        "Abstract": "Generalized anxiety disorder (GAD) and major depressive disorder (MDD) are highly prevalent and impairing problems, but frequently go undetected, leading to substantial treatment delays. Electronic health records (EHRs) collect a great deal of biometric markers and patient characteristics that could foster the detection of GAD and MDD in primary care settings. We approached the problem of predicting MDD and GAD using a novel machine learning pipeline to re-analyze data from an observational study. The pipeline constitutes an ensemble of algorithmically distinct machine learning methods, including deep learning. A sample of 4,184 undergraduate students completed the study, undergoing a general health screening and completing a psychiatric assessment for MDD and GAD. After explicitly excluding all psychiatric information, 59 biomedical and demographic features from the general health survey in addition to a set of engineered features were used for model training. We assessed the model's performance on a held-out test set and found an AUC of 0.73 (sensitivity: 0.66, specificity: 0.7) and 0.67 (sensitivity: 0.55, specificity: 0.7) for GAD, and MDD, respectively. Additionally, we used advanced techniques (SHAP values) to illuminate which features had the greatest impact on prediction for each disease. The top predictive features for MDD were being satisfied with living conditions and having public health insurance. The top predictive features for GAD were vaccinations being up to date and marijuana use. Our results indicate moderate predictive performance for the application of machine learning methods in detection of GAD and MDD based on EHR data. By identifying important predictors of GAD and MDD, these results may be used in future research to aid in the early detection of MDD and GAD.",
        "DOI": "10.1038/s41598-021-81368-4",
        "paper_author": "Nemesure M.D.",
        "affiliation_name": "Geisel School of Medicine at Dartmouth",
        "affiliation_city": "Hanover",
        "affiliation_country": "United States",
        "affiliation_id": "60007267",
        "affiliation_state": "NH"
    },
    {
        "paper_title": "Sharing ICU Patient Data Responsibly Under the Society of Critical Care Medicine/European Society of Intensive Care Medicine Joint Data Science Collaboration: The Amsterdam University Medical Centers Database (AmsterdamUMCdb) Example∗",
        "publication": "Critical Care Medicine",
        "citied_by": "122",
        "cover_date": "2021-06-01",
        "Abstract": "OBJECTIVES: Critical care medicine is a natural environment for machine learning approaches to improve outcomes for critically ill patients as admissions to ICUs generate vast amounts of data. However, technical, legal, ethical, and privacy concerns have so far limited the critical care medicine community from making these data readily available. The Society of Critical Care Medicine and the European Society of Intensive Care Medicine have identified ICU patient data sharing as one of the priorities under their Joint Data Science Collaboration. To encourage ICUs worldwide to share their patient data responsibly, we now describe the development and release of Amsterdam University Medical Centers Database (AmsterdamUMCdb), the first freely available critical care database in full compliance with privacy laws from both the United States and Europe, as an example of the feasibility of sharing complex critical care data. SETTING: University hospital ICU. SUBJECTS: Data from ICU patients admitted between 2003 and 2016. INTERVENTIONS: We used a risk-based deidentification strategy to maintain data utility while preserving privacy. In addition, we implemented contractual and governance processes, and a communication strategy. Patient organizations, supporting hospitals, and experts on ethics and privacy audited these processes and the database. MEASUREMENTS AND MAIN RESULTS: AmsterdamUMCdb contains approximately 1 billion clinical data points from 23,106 admissions of 20,109 patients. The privacy audit concluded that reidentification is not reasonably likely, and AmsterdamUMCdb can therefore be considered as anonymous information, both in the context of the U.S. Health Insurance Portability and Accountability Act and the European General Data Protection Regulation. The ethics audit concluded that responsible data sharing imposes minimal burden, whereas the potential benefit is tremendous. CONCLUSIONS: Technical, legal, ethical, and privacy challenges related to responsible data sharing can be addressed using a multidisciplinary approach. A risk-based deidentification strategy, that complies with both U.S. and European privacy regulations, should be the preferred approach to releasing ICU patient data. This supports the shared Society of Critical Care Medicine and European Society of Intensive Care Medicine vision to improve critical care outcomes through scientific inquiry of vast and combined ICU datasets.",
        "DOI": "10.1097/CCM.0000000000004916",
        "paper_author": "Thoral P.J.",
        "affiliation_name": "Vrije Universiteit Amsterdam",
        "affiliation_city": "Amsterdam",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60008734",
        "affiliation_state": "Noord-Holland"
    },
    {
        "paper_title": "Application of machine learning and data visualization techniques for decision support in the insurance sector",
        "publication": "International Journal of Information Management Data Insights",
        "citied_by": "98",
        "cover_date": "2021-11-01",
        "Abstract": "The insurance industry has a giant role in the sustainable economic growth of any country. With an increase in the number of insurance buyers, it has become an absolute necessity for an insurance company to have a detailed claim analysis system in place. Claim Analysis is performed by insurance companies to distinguish between fraudulent and genuine claims. Apart from that, Claim Analysis can also be used to understand the client strata in a much better way and implement the results further during the underwriting and acceptance/denial stage of policy enrollment. The main objective of this research work is to identify meaningful and decisive factors for claim filing and acceptance in a learning context through exploratory data analysis (EDA) and feature selection techniques. Also, machine learning algorithms are applied to the datasets and are evaluated using performance metrics.",
        "DOI": "10.1016/j.jjimei.2021.100012",
        "paper_author": "Rawat S.",
        "affiliation_name": "Amity University",
        "affiliation_city": "Noida",
        "affiliation_country": "India",
        "affiliation_id": "60076774",
        "affiliation_state": "UP"
    },
    {
        "paper_title": "Industry quantum computing applications",
        "publication": "EPJ Quantum Technology",
        "citied_by": "97",
        "cover_date": "2021-12-01",
        "Abstract": "Quantum computing promises to overcome computational limitations with better and faster solutions for optimization, simulation, and machine learning problems. Europe and Germany are in the process of successfully establishing research and funding programs with the objective to advance the technology’s ecosystem and industrialization, thereby ensuring digital sovereignty, security, and competitiveness. Such an ecosystem comprises hardware/software solution providers, system integrators, and users from research institutions, start-ups, and industry. The vision of the Quantum Technology and Application Consortium (QUTAC) is to establish and advance the quantum computing ecosystem, supporting the ambitious goals of the German government and various research programs. QUTAC is comprised of ten members representing different industries, in particular automotive manufacturing, chemical and pharmaceutical production, insurance, and technology. In this paper, we survey the current state of quantum computing in these sectors as well as the aerospace industry and identify the contributions of QUTAC to the ecosystem. We propose an application-centric approach for the industrialization of the technology based on proven business impact. This paper identifies 24 different use cases. By formalizing high-value use cases into well-described reference problems and benchmarks, we will guide technological progress and eventually commercialization. Our results will be beneficial to all ecosystem participants, including suppliers, system integrators, software developers, users, policymakers, funding program managers, and investors.",
        "DOI": "10.1140/epjqt/s40507-021-00114-x",
        "paper_author": "Bayerstadler A.",
        "affiliation_name": "Munich Re",
        "affiliation_city": "Munich",
        "affiliation_country": "Germany",
        "affiliation_id": "100384715",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Cotton yield estimation model based on machine learning using time series UAV remote sensing data",
        "publication": "International Journal of Applied Earth Observation and Geoinformation",
        "citied_by": "95",
        "cover_date": "2021-12-15",
        "Abstract": "Crop yield prediction is of great practical significance for farmers to make reasonable decisions, such as decisions on crop insurance, storage demand, cash flow budget, fertilizer, water and other input factors. The traditional yield measurement method is sampling surveys, which require a large area of destructive sampling of cotton fields and consume considerable time and labor costs. This study established a cotton yield estimation model based on time series Unmanned Aerial Vehicle (UAV) remote sensing data. The U-Net semantic segmentation network is used to recognize and extract the boll opening pixels in high-resolution visible images, and the boll opening pixel percentage (BOP) is calculated according to the network extraction results. By combining the multispectral images and the pixel coverage of cotton bolls, a Bayesian regularization BP (back propagation) neural network was used to predict cotton yields. In order to simplify the input parameters of the model, the stepwise sensitivity analysis method is used to eliminate redundant variables and obtain the optimal input feature set. The experimental results show that the R2 of the proposed model is 0.853 at the scale of 0.81 m2 (average results of ten-fold cross validation). This study provides a method that can simultaneously meet the requirements of large-area and small-scale forecasting of cotton yields and provides a new idea for cotton yield measurement and breeding screening.",
        "DOI": "10.1016/j.jag.2021.102511",
        "paper_author": "Xu W.",
        "affiliation_name": "South China Agricultural University",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60032203",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Automated coronary calcium scoring using deep learning with multicenter external validation",
        "publication": "npj Digital Medicine",
        "citied_by": "95",
        "cover_date": "2021-12-01",
        "Abstract": "Coronary artery disease (CAD), the most common manifestation of cardiovascular disease, remains the most common cause of mortality in the United States. Risk assessment is key for primary prevention of coronary events and coronary artery calcium (CAC) scoring using computed tomography (CT) is one such non-invasive tool. Despite the proven clinical value of CAC, the current clinical practice implementation for CAC has limitations such as the lack of insurance coverage for the test, need for capital-intensive CT machines, specialized imaging protocols, and accredited 3D imaging labs for analysis (including personnel and software). Perhaps the greatest gap is the millions of patients who undergo routine chest CT exams and demonstrate coronary artery calcification, but their presence is not often reported or quantitation is not feasible. We present two deep learning models that automate CAC scoring demonstrating advantages in automated scoring for both dedicated gated coronary CT exams and routine non-gated chest CTs performed for other reasons to allow opportunistic screening. First, we trained a gated coronary CT model for CAC scoring that showed near perfect agreement (mean difference in scores = −2.86; Cohen’s Kappa = 0.89, P < 0.0001) with current conventional manual scoring on a retrospective dataset of 79 patients and was found to perform the task faster (average time for automated CAC scoring using a graphics processing unit (GPU) was 3.5 ± 2.1 s vs. 261 s for manual scoring) in a prospective trial of 55 patients with little difference in scores compared to three technologists (mean difference in scores = 3.24, 5.12, and 5.48, respectively). Then using CAC scores from paired gated coronary CT as a reference standard, we trained a deep learning model on our internal data and a cohort from the Multi-Ethnic Study of Atherosclerosis (MESA) study (total training n = 341, Stanford test n = 42, MESA test n = 46) to perform CAC scoring on routine non-gated chest CT exams with validation on external datasets (total n = 303) obtained from four geographically disparate health systems. On identifying patients with any CAC (i.e., CAC ≥ 1), sensitivity and PPV was high across all datasets (ranges: 80–100% and 87–100%, respectively). For CAC ≥ 100 on routine non-gated chest CTs, which is the latest recommended threshold to initiate statin therapy, our model showed sensitivities of 71–94% and positive predictive values in the range of 88–100% across all the sites. Adoption of this model could allow more patients to be screened with CAC scoring, potentially allowing opportunistic early preventive interventions.",
        "DOI": "10.1038/s41746-021-00460-1",
        "paper_author": "Eng D.",
        "affiliation_name": "Stanford University School of Medicine",
        "affiliation_city": "Stanford",
        "affiliation_country": "United States",
        "affiliation_id": "60032838",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Improving Stroke Risk Prediction in the General Population: A Comparative Assessment of Common Clinical Rules, a New Multimorbid Index, and Machine-Learning-Based Algorithms",
        "publication": "Thrombosis and Haemostasis",
        "citied_by": "74",
        "cover_date": "2021-01-01",
        "Abstract": "Background  There are few large studies examining and predicting the diversified cardiovascular/noncardiovascular comorbidity relationships with stroke. We investigated stroke risks in a very large prospective cohort of patients with multimorbidity, using two common clinical rules, a clinical multimorbid index and a machine-learning (ML) approach, accounting for the complex relationships among variables, including the dynamic nature of changing risk factors. Methods  We studied a prospective U.S. cohort of 3,435,224 patients from medical databases in a 2-year investigation. Stroke outcomes were examined in relationship to diverse multimorbid conditions, demographic variables, and other inputs, with ML accounting for the dynamic nature of changing multimorbidity risk factors, two clinical risk scores, and a clinical multimorbid index. Results  Common clinical risk scores had moderate and comparable c indices with stroke outcomes in the training and external validation samples (validation-CHADS 2: c index 0.812, 95% confidence interval [CI] 0.808-0.815; CHA 2 DS 2 -VASc: c index 0.809, 95% CI 0.805-0.812). A clinical multimorbid index had higher discriminant validity values for both the training/external validation samples (validation: c index 0.850, 95% CI 0.847-0.853). The ML-based algorithms yielded the highest discriminant validity values for the gradient boosting/neural network logistic regression formulations with no significant differences among the ML approaches (validation for logistic regression: c index 0.866, 95% CI 0.856-0.876). Calibration of the ML-based formulation was satisfactory across a wide range of predicted probabilities. Decision curve analysis demonstrated that clinical utility for the ML-based formulation was better than that for the two current clinical rules and the newly developed multimorbid tool. Also, ML models and clinical stroke risk scores were more clinically useful than the treat all strategy. Conclusion  Complex relationships of various comorbidities uncovered using a ML approach for diverse (and dynamic) multimorbidity changes have major consequences for stroke risk prediction. This approach may facilitate automated approaches for dynamic risk stratification in the significant presence of multimorbidity, helping in the decision-making process for risk assessment and integrated/holistic management.",
        "DOI": "10.1055/a-1467-2993",
        "paper_author": "Lip G.Y.H.",
        "affiliation_name": "University of Liverpool",
        "affiliation_city": "Liverpool",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60020661",
        "affiliation_state": "Merseyside"
    },
    {
        "paper_title": "Artificial Intelligence and Liability in Medicine: Balancing Safety and Innovation",
        "publication": "Milbank Quarterly",
        "citied_by": "74",
        "cover_date": "2021-09-01",
        "Abstract": "Policy Points With increasing integration of artificial intelligence and machine learning in medicine, there are concerns that algorithm inaccuracy could lead to patient injury and medical liability. While prior work has focused on medical malpractice, the artificial intelligence ecosystem consists of multiple stakeholders beyond clinicians. Current liability frameworks are inadequate to encourage both safe clinical implementation and disruptive innovation of artificial intelligence. Several policy options could ensure a more balanced liability system, including altering the standard of care, insurance, indemnification, special/no-fault adjudication systems, and regulation. Such liability frameworks could facilitate safe and expedient implementation of artificial intelligence and machine learning in clinical care.",
        "DOI": "10.1111/1468-0009.12504",
        "paper_author": "Maliha G.",
        "affiliation_name": "University of Pennsylvania Perelman School of Medicine",
        "affiliation_city": "Philadelphia",
        "affiliation_country": "United States",
        "affiliation_id": "60003711",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "Predicting the risk of chronic kidney disease (Ckd) using machine learning algorithm",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "73",
        "cover_date": "2021-01-01",
        "Abstract": "Background: Creatinine is a type of metabolite of blood that is strongly correlated to glomerular filtration rate (GFR). As measuring GFR is difficult, creatinine value is used for indirectly determining GFR and then the stage of chronic kidney disease (CKD). Adding a creatinine test into routine health examination could detect CKD. As more items for comprehensive examination means higher cost, creatinine testing is not included in the routine health examination in many countries. An algorithm based on common test results, without creatinine test, to evaluate the risk of CKD will increase the chance of its early detection and treatment. Methods: In this study, we used open source data containing 1 million samples. These data contain 23 health-related features, including common diagnostic test results provided by National Health Insurance Sharing Service (NHISS). A low GFR indicates possible chronic kidney disease (CKD). As is commonly accepted in the medical community, a GFR of 60 mL/min is used as the threshold, below which is considered to have CKD. In this study, the first step aims to build a regression model to predict the value of creatinine from 23 features, and then combine the predicted value of creatinine with the original 23 features to evaluate the risk of CKD. We will show by simulation that by the proposed method we can achieve better prediction results compared to direct prediction from 23 features. The data is extremely unbalanced for predicting the target variable creatinine. We used undersampling method and proposed a new cost-sensitive mean-squared error (MSE) loss function to deal with the problem. Regrading model selection, this work used three machine learning models: a bagging tree model named Random Forest, a boosting tree model named XGBoost, and a neural network based model named ResNet. To improve the result of the creatinine predictor, we averaged results from eight predictors, a method known as ensemble learning. Finally, the predicted creatinine and the original 23 features is used to predict the risk of CKD. Results: We optimized results of R-Squared (R2) value to select the appropriate undersampling strategy and the regression model for the regression stage of creatinine prediction. Ensembled model achieved the best performance of R2 of 0.5590. The six factors from 23 are selected from the top of the list of how strongly they affect the creatinine value. They are sex, age, hemoglobin, the level of urine protein, waist circumference, and habit of smoking. Using the predicted value of creatinine, an area under Receiver Operating Characteristic curve (AUC) of 0.76 is achieved while classifying samples for CKD. Conclusions: Using commonly available health parameters, the proposed system can assess the risk of CKD for public health. High-risk subjects can be screened and advised to take a creatinine test for further confirmation. In this way, we can reduce the impact of CKD on public health and facilitate early detection for many, where a blanket test of creatinine is not available for all.",
        "DOI": "10.3390/app11010202",
        "paper_author": "Wang W.",
        "affiliation_name": "Iwate Prefectural University",
        "affiliation_city": "Iwate District",
        "affiliation_country": "Japan",
        "affiliation_id": "60011157",
        "affiliation_state": "Iwate"
    },
    {
        "paper_title": "Machine learning prediction models for chronic kidney disease using national health insurance claim data in Taiwan",
        "publication": "Healthcare (Switzerland)",
        "citied_by": "72",
        "cover_date": "2021-05-01",
        "Abstract": "Chronic kidney disease (CKD) represents a heavy burden on the healthcare system because of the increasing number of patients, high risk of progression to end-stage renal disease, and poor prognosis of morbidity and mortality. The aim of this study is to develop a machine-learning model that uses the comorbidity and medication data obtained from Taiwan’s National Health Insurance Research Database to forecast the occurrence of CKD within the next 6 or 12 months before its onset, and hence its prevalence in the population. A total of 18,000 people with CKD and 72,000 people without CKD diagnosis were selected using propensity score matching. Their demographic, medication and comorbidity data from their respective two-year observation period were used to build a predictive model. Among the approaches investigated, the Convolutional Neural Networks (CNN) model performed best with a test set AUROC of 0.957 and 0.954 for the 6-month and 12-month predictions, respectively. The most prominent predictors in the tree-based models were identified, including diabetes mellitus, age, gout, and medications such as sulfonamides and angiotensins. The model proposed in this study could be a useful tool for policymakers in predicting the trends of CKD in the population. The models can allow close monitoring of people at risk, early detection of CKD, better allocation of resources, and patient-centric management.",
        "DOI": "10.3390/healthcare9050546",
        "paper_author": "Krishnamurthy S.",
        "affiliation_name": "Vellore Institute of Technology",
        "affiliation_city": "Vellore",
        "affiliation_country": "India",
        "affiliation_id": "60010618",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Business process variant analysis: Survey and classification",
        "publication": "Knowledge-Based Systems",
        "citied_by": "70",
        "cover_date": "2021-01-09",
        "Abstract": "It is common for business processes to exhibit a high degree of internal heterogeneity, in the sense that the executions of the process differ widely from each other due to contextual factors, human factors, or deliberate business decisions. For example, a quote-to-cash process in a multinational company is typically executed differently across different countries or even across different regions in the same country. Similarly, an insurance claims handling process might be executed differently across different claims handling centers or across multiple teams within the same claims handling center. A subset of executions of a business process that can be distinguished from others based on a given predicate (e.g. the executions of a process in a given country) is called a process variant. Understanding differences between process variants helps analysts and managers to make informed decisions as to how to standardize or otherwise improve a business process, for example by helping them find out what makes it that a given variant exhibits a higher performance than another one. Process variant analysis is a family of techniques to analyze event logs produced during the execution of a process, in order to identify and explain the differences between two or more process variants. A wide range of methods for process variant analysis have been proposed in the past decade. However, due to the interdisciplinary nature of this field, the proposed methods and the types of differences they can identify vary widely, and there is a lack of a unifying view of the field. To close this gap, this article presents a systematic literature review of methods for process variant analysis. The identified studies are classified according to their inputs, outputs, analysis purpose, underpinning algorithms, and extra-functional characteristics. The paper closes with a broad classification of approaches into three categories based on the paradigm they employ to compare multiple process variants.",
        "DOI": "10.1016/j.knosys.2020.106557",
        "paper_author": "Taymouri F.",
        "affiliation_name": "University of Melbourne",
        "affiliation_city": "Melbourne",
        "affiliation_country": "Australia",
        "affiliation_id": "60026553",
        "affiliation_state": "VIC"
    },
    {
        "paper_title": "The Interplay Between Online Reviews and Physician Demand: An Empirical Investigation",
        "publication": "Management Science",
        "citied_by": "66",
        "cover_date": "2021-12-01",
        "Abstract": "Social media platforms for healthcare services are changing how patients choose physicians. The digitization of healthcare reviews has been providing additional information to patients when choosing their physicians. On the other hand, the growing online information introduces more uncertainty among providers regarding the expected future demand and how different service features can affect patient decisions. In this paper, we derive various service-quality proxies from online reviews and show that leveraging textual information can derive useful operational measures to better understand patient choices. To do so, we study a unique data set from one of the leading appointment-booking websites in the United States. We derive from the text reviews the seven most frequently mentioned topics among patients, namely, bedside manner, diagnosis accuracy, waiting time, service time, insurance process, physician knowledge, and office environment, and then incorporate these service features into a random-coefficient choice model to quantify the economic values of these service-quality proxies. By introducing quality proxies from text reviews, we find the predictive power of patient choice increases significantly, for example, a 6%-12% improvement measured by mean squared error for both in-sample and out-of-sample tests. In addition, our estimation results indicate that contextual description may better characterize users' perceived quality than numerical ratings on the same service feature. Broadly speaking, this paper shows how to incorporate textual information into an econometric model to understand patient choice in healthcare delivery. Our interdisciplinary approach provides a framework that combines machine learning and structural modeling techniques to advance the literature in empirical operations management, information systems, and marketing.",
        "DOI": "10.1287/mnsc.2020.3879",
        "paper_author": "Xu Y.",
        "affiliation_name": "Gies College of Business",
        "affiliation_city": "Champaign",
        "affiliation_country": "United States",
        "affiliation_id": "60134791",
        "affiliation_state": "IL"
    },
    {
        "paper_title": "Machine learning approaches for auto insurance big data",
        "publication": "Risks",
        "citied_by": "65",
        "cover_date": "2021-01-01",
        "Abstract": "The growing trend in the number and severity of auto insurance claims creates a need for new methods to efficiently handle these claims. Machine learning (ML) is one of the methods that solves this problem. As car insurers aim to improve their customer service, these companies have started adopting and applying ML to enhance the interpretation and comprehension of their data for efficiency, thus improving their customer service through a better understanding of their needs. This study considers how automotive insurance providers incorporate machinery learning in their company, and explores how ML models can apply to insurance big data. We utilize various ML methods, such as logistic regression, XGBoost, random forest, decision trees, naïve Bayes, and K-NN, to predict claim occurrence. Furthermore, we evaluate and compare these models’ perfor-mances. The results showed that RF is better than other methods with the accuracy, kappa, and AUC values of 0.8677, 0.7117, and 0.840, respectively.",
        "DOI": "10.3390/risks9020042",
        "paper_author": "Hanafy M.",
        "affiliation_name": "Zhejiang Gongshang University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60012581",
        "affiliation_state": "Zhejiang"
    },
    {
        "paper_title": "Predicting adverse outcomes due to diabetes complications with machine learning using administrative health data",
        "publication": "npj Digital Medicine",
        "citied_by": "63",
        "cover_date": "2021-12-01",
        "Abstract": "Across jurisdictions, government and health insurance providers hold a large amount of data from patient interactions with the healthcare system. We aimed to develop a machine learning-based model for predicting adverse outcomes due to diabetes complications using administrative health data from the single-payer health system in Ontario, Canada. A Gradient Boosting Decision Tree model was trained on data from 1,029,366 patients, validated on 272,864 patients, and tested on 265,406 patients. Discrimination was assessed using the AUC statistic and calibration was assessed visually using calibration plots overall and across population subgroups. Our model predicting three-year risk of adverse outcomes due to diabetes complications (hyper/hypoglycemia, tissue infection, retinopathy, cardiovascular events, amputation) included 700 features from multiple diverse data sources and had strong discrimination (average test AUC = 77.7, range 77.7–77.9). Through the design and validation of a high-performance model to predict diabetes complications adverse outcomes at the population level, we demonstrate the potential of machine learning and administrative health data to inform health planning and healthcare resource allocation for diabetes management.",
        "DOI": "10.1038/s41746-021-00394-8",
        "paper_author": "Ravaut M.",
        "affiliation_name": "Layer 6 AI",
        "affiliation_city": "Toronto",
        "affiliation_country": "Canada",
        "affiliation_id": "121657972",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "FairLens: Auditing black-box clinical decision support systems",
        "publication": "Information Processing and Management",
        "citied_by": "54",
        "cover_date": "2021-09-01",
        "Abstract": "The pervasive application of algorithmic decision-making is raising concerns on the risk of unintended bias in AI systems deployed in critical settings such as healthcare. The detection and mitigation of model bias is a very delicate task that should be tackled with care and involving domain experts in the loop. In this paper we introduce FairLens, a methodology for discovering and explaining biases. We show how this tool can audit a fictional commercial black-box model acting as a clinical decision support system (DSS). In this scenario, the healthcare facility experts can use FairLens on their historical data to discover the biases of the model before incorporating it into the clinical decision flow. FairLens first stratifies the available patient data according to demographic attributes such as age, ethnicity, gender and healthcare insurance; it then assesses the model performance on such groups highlighting the most common misclassifications. Finally, FairLens allows the expert to examine one misclassification of interest by explaining which elements of the affected patients’ clinical history drive the model error in the problematic group. We validate FairLens’ ability to highlight bias in multilabel clinical DSSs introducing a multilabel-appropriate metric of disparity and proving its efficacy against other standard metrics.",
        "DOI": "10.1016/j.ipm.2021.102657",
        "paper_author": "Panigutti C.",
        "affiliation_name": "Scuola Normale Superiore di Pisa",
        "affiliation_city": "Pisa",
        "affiliation_country": "Italy",
        "affiliation_id": "60030674",
        "affiliation_state": "RM"
    },
    {
        "paper_title": "Use of Electronic Health Data for Disease Prediction: A Comprehensive Literature Review",
        "publication": "IEEE/ACM Transactions on Computational Biology and Bioinformatics",
        "citied_by": "53",
        "cover_date": "2021-03-01",
        "Abstract": "Disease prediction has the potential to benefit stakeholders such as the government and health insurance companies. It can identify patients at risk of disease or health conditions. Clinicians can then take appropriate measures to avoid or minimize the risk and in turn, improve quality of care and avoid potential hospital admissions. Due to the recent advancement of tools and techniques for data analytics, disease risk prediction can leverage large amounts of semantic information, such as demographics, clinical diagnosis and measurements, health behaviours, laboratory results, prescriptions and care utilisation. In this regard, electronic health data can be a potential choice for developing disease prediction models. A significant number of such disease prediction models have been proposed in the literature over time utilizing large-scale electronic health databases, different methods, and healthcare variables. The goal of this comprehensive literature review was to discuss different risk prediction models that have been proposed based on electronic health data. Search terms were designed to find relevant research articles that utilized electronic health data to predict disease risks. Online scholarly databases were searched to retrieve results, which were then reviewed and compared in terms of the method used, disease type, and prediction accuracy. This paper provides a comprehensive review of the use of electronic health data for risk prediction models. A comparison of the results from different techniques for three frequently modelled diseases using electronic health data was also discussed in this study. In addition, the advantages and disadvantages of different risk prediction models, as well as their performance, were presented. Electronic health data have been widely used for disease prediction. A few modelling approaches show very high accuracy in predicting different diseases using such data. These modelling approaches have been used to inform the clinical decision process to achieve better outcomes.",
        "DOI": "10.1109/TCBB.2019.2937862",
        "paper_author": "Hossain M.E.",
        "affiliation_name": "Faculty of Engineering",
        "affiliation_city": "Sydney",
        "affiliation_country": "Australia",
        "affiliation_id": "60090755",
        "affiliation_state": "NSW"
    },
    {
        "paper_title": "Gradient Boosted Decision Tree Algorithms for Medicare Fraud Detection",
        "publication": "SN Computer Science",
        "citied_by": "52",
        "cover_date": "2021-07-01",
        "Abstract": "Employing Machine Learning algorithms to identify health insurance fraud is an application of Artificial Intelligence in Healthcare. Insurance fraud spuriously inflates the cost of Healthcare. Therefore, it could limit or even deny patients necessary care and treatment. We use Medicare claims data as input to various algorithms to gauge their performance in fraud detection. The claims data contain categorical features, some of which have thousands of possible values. To the best of our knowledge, this is the first study on using CatBoost and LightGBM to encode categorical data for Medicare fraud detection. We show that CatBoost attains better performance in the task of Medicare fraud detection than other algorithms, attaining a mean AUC value of 0.77452. At a 99% confidence level (with p value 0), our analysis shows that this result is significantly better than the mean AUC value of 0.76132 that LightGBM yields. A second contribution we make is to show that when we include an additional categorical feature (Healthcare provider state), CatBoost yields a mean AUC value of 0.88245, which is also significantly better than the mean AUC value of 0.85137 that LightGBM yields. Our empirical evidence clearly indicates CatBoost is a better alternative to other classifiers for Medicare fraud detection, especially when incorporating categorical features.",
        "DOI": "10.1007/s42979-021-00655-z",
        "paper_author": "Hancock J.T.",
        "affiliation_name": "FAU College of Engineering and Computer Science",
        "affiliation_city": "Boca Raton",
        "affiliation_country": "United States",
        "affiliation_id": "60144960",
        "affiliation_state": "FL"
    },
    {
        "paper_title": "Transformation of insurance technologies in the context of a pandemic",
        "publication": "Insurance Markets and Companies",
        "citied_by": "51",
        "cover_date": "2021-01-01",
        "Abstract": "The COVID-19 pandemic has affected different sectors of the economy, including insurance, and has become a problem and a clear catalyst for innovation. The pandemic has highlighted some inefficiencies of the traditional model of interaction between insurers and their customers and focused on insurance companies' efforts on innovations and investments in the digital future. That is why the article aims to generalize the transformations of the institutional environment in the InsurTech ecosystem in the context of the COVID-19 pandemic and identify prospects for its development in the post-pandemic period. The analysis of the functioning of InsurTech as an ecosystem necessitated the identification of challenges for the insurance market in the context of COVID-19. The peculiarities of the insurance market development have been identified: the blurring of boundaries between insurers, BigTech firms, and technological partners; expanding interaction with policyholders based on the principle of support and the use of social networks; changes in the structure of the implemented insurance services; an increase in insurance fraud cases; the growing demand for parametric insurance products; introduction of a digital approach to the interaction with customers and employees, modernization of technological infrastructure and expansion of data processing capabilities; remote risk identification; acceleration in the use of financial technologies by insurance market participants. There is a transformation of the insurance market under the influence of business processes digitalization because insurers are aware of the importance of InsurTech in the formation of competitive advantages. For many companies, the crisis has strengthened their innovative development strategies and accelerated the implementation of financial technology tools in their business processes against the background of modernization of technological infrastructure. Chatbots, telematics, the Internet of Things, machine learning, artificial intelligence, predictive analytics, etc., are widely used. In the future, InsurTech will also play an important role in introducing digital innovations in the insurance market.",
        "DOI": "10.21511/INS.12(1).2021.01",
        "paper_author": "Volosovych S.",
        "affiliation_name": "State University of Trade and Economics",
        "affiliation_city": "Kyiv",
        "affiliation_country": "Ukraine",
        "affiliation_id": "60103725",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Development of machine learning algorithms for the prediction of financial toxicity in localized breast cancer following surgical treatment",
        "publication": "JCO Clinical Cancer Informatics",
        "citied_by": "46",
        "cover_date": "2021-01-01",
        "Abstract": "PURPOSE Financial burden caused by cancer treatment is associated with material loss, distress, and poorer outcomes. Financial resources exist to support patients but identification of need is difficult. We sought to develop and test a tool to accurately predict an individual's risk of financial toxicity based on clinical, demographic, and patient-reported data prior to initiation of breast cancer treatment. PATIENTS AND METHODS We surveyed 611 patients undergoing breast cancer therapy at MD Anderson Cancer Center. We collected data using the validated COmprehensive Score for financial Toxicity (COST) patientreported outcome measure alongside other financial indicators (credit score, income, and insurance status). We also collected clinical and perioperative data. We trained and tested an ensemble of machine learning (ML) algorithms (neural network, regularized linear model, support vector machines, and a classification tree) to predict financial toxicity. Data were randomly partitioned into training and test samples (2:1 ratio). Predictive performance was assessed using area-under-the-receiver-operating-characteristics-curve (AUROC), accuracy, sensitivity, and specificity.RESULTS In our test sample (N = 203), 48 of 203 women (23.6%) reported significant financial burden. The algorithm ensemble performed well to predict financial burden with an AUROC of 0.85, accuracy of 0.82, sensitivity of 0.85, and specificity of 0.81. Key clinical predictors of financial burden from the linear model were neoadjuvant therapy (βregularized, .11) and autologous, rather than implant-based, reconstruction (βregularized, .06). Notably, radiation and clinical tumor stage had no effect on financial burden. CONCLUSION ML models accurately predicted financial toxicity related to breast cancer treatment. These predictions may inform decision making and care planning to avoid financial distress during cancer treatment or enable targeted financial support. Further research is warranted to validate this tool and assess applicability for other types of cancer.",
        "DOI": "10.1200/CCI.20.00088",
        "paper_author": "Sidey-Gibbons C.",
        "affiliation_name": "The University of Texas MD Anderson Cancer Center",
        "affiliation_city": "Houston",
        "affiliation_country": "United States",
        "affiliation_id": "60015023",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "The value of meteorological data in marine risk assessment",
        "publication": "Reliability Engineering and System Safety",
        "citied_by": "46",
        "cover_date": "2021-05-01",
        "Abstract": "The objective of this paper is to investigate whether the use of meteorological data improves the prediction of marine incidents, as represented by marine insurance claims for a vessel's voyage, both on a stand-alone basis and when combined with vessel-specific features and ship tracking data from the Automated Identification System (AIS). Furthermore, the paper investigates whether predictive performance improves when using machine learning algorithms, such as logistic LASSO regression and eXtreme Gradient Boosted Trees over classical logistic models, and identify dependencies and interaction effects among the risk factors within the SHapley Additive exPlanations framework. The data sample includes weather and AIS data for 42,000 voyages in the North Pacific between January 2013 and August 2019. The results suggest that meteorological information adds value in claims prediction and that short-term complex interactions between the vessel and weather conditions impact marine risk. The research is important for the improved modelling of marine risk on the basis of high-frequency, high-resolution ship tracking and weather data.",
        "DOI": "10.1016/j.ress.2021.107480",
        "paper_author": "Adland R.",
        "affiliation_name": "NHH Norwegian School of Economics",
        "affiliation_city": "Bergen",
        "affiliation_country": "Norway",
        "affiliation_id": "60025233",
        "affiliation_state": "Vestland"
    },
    {
        "paper_title": "Hybrid Approach to Document Anomaly Detection: An Application to Facilitate RPA in Title Insurance",
        "publication": "International Journal of Automation and Computing",
        "citied_by": "46",
        "cover_date": "2021-02-01",
        "Abstract": "Anomaly detection (AD) is an important aspect of various domains and title insurance (TI) is no exception. Robotic process automation (RPA) is taking over manual tasks in TI business processes, but it has its limitations without the support of artificial intelligence (AI) and machine learning (ML). With increasing data dimensionality and in composite population scenarios, the complexity of detecting anomalies increases and AD in automated document management systems (ADMS) is the least explored domain. Deep learning, being the fastest maturing technology can be combined along with traditional anomaly detectors to facilitate and improve the RPAs in TI. We present a hybrid model for AD, using autoencoders (AE) and a one-class support vector machine (OSVM). In the present study, OSVM receives input features representing real-time documents from the TI business, orchestrated and with dimensions reduced by AE. The results obtained from multiple experiments are comparable with traditional methods and within a business acceptable range, regarding accuracy and performance.",
        "DOI": "10.1007/s11633-020-1247-y",
        "paper_author": "Guha A.",
        "affiliation_name": "Christ University",
        "affiliation_city": "Bengaluru",
        "affiliation_country": "India",
        "affiliation_id": "60106812",
        "affiliation_state": "KA"
    },
    {
        "paper_title": "Stop-and-Go: Exploring Backdoor Attacks on Deep Reinforcement Learning-Based Traffic Congestion Control Systems",
        "publication": "IEEE Transactions on Information Forensics and Security",
        "citied_by": "45",
        "cover_date": "2021-01-01",
        "Abstract": "Recent work has shown that the introduction of autonomous vehicles (AVs) in traffic could help reduce traffic jams. Deep reinforcement learning methods demonstrate good performance in complex control problems, including autonomous vehicle control, and have been used in state-of-the-art AV controllers. However, deep neural networks (DNNs) render automated driving vulnerable to machine learning-based attacks. In this work, we explore the backdooring/trojanning of DRL-based AV controllers. We develop a trigger design methodology that is based on well-established principles of traffic physics. The malicious actions include vehicle deceleration and acceleration to cause stop-and-go traffic waves to emerge (congestion attacks) or AV acceleration resulting in the AV crashing into the vehicle in front (insurance attack). We test our attack on single-lane and two-lane circuits. Our experimental results show that the backdoored model does not compromise normal operation performance, with the maximum decrease in cumulative rewards being 1%. Still, it can be maliciously activated to cause a crash or congestion when the corresponding triggers appear.",
        "DOI": "10.1109/TIFS.2021.3114024",
        "paper_author": "Wang Y.",
        "affiliation_name": "NYU Tandon School of Engineering",
        "affiliation_city": "New York",
        "affiliation_country": "United States",
        "affiliation_id": "60108318",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Quantification of continuous flood hazard using random forest classification and flood insurance claims at large spatial scales: A pilot study in southeast Texas",
        "publication": "Natural Hazards and Earth System Sciences",
        "citied_by": "44",
        "cover_date": "2021-03-01",
        "Abstract": "Pre-disaster planning and mitigation necessitate detailed spatial information about flood hazards and their associated risks. In the US, the Federal Emergency Management Agency (FEMA) Special Flood Hazard Area (SFHA) provides important information about areas subject to flooding during the 1% riverine or coastal event. The binary nature of flood hazard maps obscures the distribution of property risk inside of the SFHA and the residual risk outside of the SFHA, which can undermine mitigation efforts. Machine learning techniques provide an alternative approach to estimating flood hazards across large spatial scales at low computational expense. This study presents a pilot study for the Texas Gulf Coast region using random forest classification to predict flood probability across a 30 523 km2 area. Using a record of National Flood Insurance Program (NFIP) claims dating back to 1976 and high-resolution geospatial data, we generate a continuous flood hazard map for 12 US Geological Survey (USGS) eight-digit hydrologic unit code (HUC) watersheds. Results indicate that the random forest model predicts flooding with a high sensitivity (area under the curve, AUC: 0.895), especially compared to the existing FEMA regulatory floodplain. Our model identifies 649 000 structures with at least a 1% annual chance of flooding, roughly 3 times more than are currently identified by FEMA as flood-prone.",
        "DOI": "10.5194/nhess-21-807-2021",
        "paper_author": "Mobley W.",
        "affiliation_name": "Texas A and M University at Galveston",
        "affiliation_city": "Galveston",
        "affiliation_country": "United States",
        "affiliation_id": "60012455",
        "affiliation_state": "TX"
    }
]