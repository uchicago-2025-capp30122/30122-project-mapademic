[
    {
        "paper_title": "Sector-Based Stock Price Prediction with Machine Learning Models",
        "publication": "Sakarya University Journal of Computer and Information Sciences",
        "citied_by": "0",
        "cover_date": "2022-12-31",
        "Abstract": "Stock price prediction is an important topic for investors and companies. The increasing effect of machine learning methods in every field also applies to stock forecasting. In this study, it is aimed to predict the future prices of the stocks of companies in different sectors traded on the Borsa Istanbul (BIST) 30 Index. For the study, the data of two companies selected as examples from each of the holding, white goods, petrochemical, iron and steel, transportation and communication sectors were analyzed. In the study, in addition to the share analysis of the sectors, the price prediction performances of the machine learning algorithm on a sectoral basis were examined. For these tests, XGBoost, Support Vector Machines (SVM), K-nearest neighbors (KNN) and Random Forest (RF) algorithms were used. The obtained results were analyzed with mean absolute error (MAE), mean absolute percent error (MAPE), mean squared error (MSE), and R2 correlation metrics. The best estimations on a sectoral basis were made for companies in the Iron and Steel and Petroleum field. One of the most important innovations in the study is the examination of the effect of current macro changes on the forecasting model. As an example, the effect of the changes in the Central Bank Governors, which took place three times in the 5-year period, on the forecast was investigated. The results showed that the unpredictable effects on the policies after the change of Governors also negatively affected the forecast performance.",
        "DOI": "10.35377/saucis...1200151",
        "paper_author": "Kocaoğlu D.",
        "affiliation_name": "Kocaeli Üniversitesi",
        "affiliation_city": "İzmit",
        "affiliation_country": "Turkey",
        "affiliation_id": "60028583",
        "affiliation_state": "Kocaeli"
    },
    {
        "paper_title": "Intelligent Transportation Systems Architecture: Recommendation for K-AUS",
        "publication": "El-Cezeri Journal of Science and Engineering",
        "citied_by": "2",
        "cover_date": "2022-12-31",
        "Abstract": "The latest advances in technology and the improvement of decision processes with learning methods based on artificial intelligence have put the word \"intelligent\" ahead of all systems that make human life easier. Based on intelligent transportation systems, it is aimed to reduce the damage to the country’s economy and the environment while providing technology-based and faster, safer, more accessible, more sustainable and more efficient transportation. The main goal of creating the intelligent transportation systems architecture is to design and implement human-focused, sustainable transportation systems together with cutting-edge technologies such as industry 4.0 technologies, mobile applications, augmented reality, and the internet of things. Intelligent transportation systems architecture needs to be updated according to C-ITS systems that provide interoperability and data integrity. Therefore, the main factor in creating the architecture of intelligent transport systems is to create system architecture by making complex systems with data integrity and numerous insignificant idle data into systems that communicate with each other and reach the level of interoperability. In this study, intelligent transportation systems policies in the world have been analyzed and systems that have reached the level of interoperability that will provide the basis of C-ITS and intelligent transportation system architecture have been proposed.",
        "DOI": "10.31202/ecjse.1132804",
        "paper_author": "Çapalı B.",
        "affiliation_name": "Süleyman Demirel Üniversitesi",
        "affiliation_city": "Isparta",
        "affiliation_country": "Turkey",
        "affiliation_id": "60000231",
        "affiliation_state": "Isparta"
    },
    {
        "paper_title": "Climatic water balance forecasting with machine learning and deep learning models over Bangladesh",
        "publication": "International Journal of Climatology",
        "citied_by": "6",
        "cover_date": "2022-12-30",
        "Abstract": "Understanding the impact of input variables on black-box machine learning and deep learning models is necessary. Therefore, this study proposed SHapley Additive exPlanations (SHAP) values to address the problem of the interpretability of the output of the support vector machine (SVM), random forest (RF), convolutional neural network (CNN), and long short-term memory (LSTM) models to forecast climatic water balance (CWB) within 1–3 months lead-time. The current study uses two Koppen–Geiger climate zones over Bangladesh: the humid subtropical climate with dry winter and hot summer (Cwa) and the tropical climate (Af-Am). Monthly antecedent CWB, potential evapotranspiration (PET), convective available potential energy (CAPE), relative humidity (RH), air temperature (TEM), El Niño–Southern Oscillation (ENSO), Indian Ocean Dipole (IOD), and North Atlantic oscillation (NAO) are used as the inputs to the proposed model. SHAP values show that CWB (current month) for both climate zones has the maximum impact on the prediction. The effects of atmospheric variables and ocean–atmospheric teleconnections on CWB forecasts over Bangladesh are low. Forecasting results show that the SVM model shows the best performance for CWB forecasts in the Cwa and Af-Am climate zones in terms of antecedent CWB as input. And this model effectively decreases the negative effect of increasing forecast lead-time. Since the proposed model can predict CWB in 3-months lead-time, it would help policy-makers and practitioners to reduce the drought and flood impacts in the future by adopting better preparedness plans.",
        "DOI": "10.1002/joc.7885",
        "paper_author": "Jalal Uddin M.",
        "affiliation_name": "Nanjing University of Information Science &amp; Technology",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60064143",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "A new comprehensive monitoring and diagnostic approach for early detection of mechanical degradation in helicopter transmission systems",
        "publication": "Expert Systems with Applications",
        "citied_by": "10",
        "cover_date": "2022-12-30",
        "Abstract": "Helicopters vulnerabilities specifically lie in single-load-path critical parts that transmit the engine's power to the rotors. A fault in even one single trans- mission's gear component may compromise the whole helicopter, yielding high maintenance costs and safety hazards. In this work, we present an effective di- agnosis and monitoring system for the early detection of the mechanical degra- dation in such components, also capable of providing insights on the damage's causes. The classification task is performed by an ensemble of two learners: a convolutional autoencoder and a distance&density-based unsupervised classifier that use as regressors specific Health Indexes (HIs) and flight parameters. The proposed approach employs the autoencoder reconstruction error information to infer the most probable cause of each detected fault, and enacts post-processing filtering policies that effectively reduce the number of false alarms. Extensive experimental validation witnesses the good performances and the robustness of the proposed approach.",
        "DOI": "10.1016/j.eswa.2022.118412",
        "paper_author": "Leoni J.",
        "affiliation_name": "Politecnico di Milano",
        "affiliation_city": "Milan",
        "affiliation_country": "Italy",
        "affiliation_id": "60023256",
        "affiliation_state": "MI"
    },
    {
        "paper_title": "Importance-Aware Data Pre-Processing and Device Scheduling for Multi-Channel Edge Learning",
        "publication": "Journal of Communications and Information Networks",
        "citied_by": "1",
        "cover_date": "2022-12-25",
        "Abstract": "The large-scale deployment of intelligent Internet of things (IoT) devices have brought increasing needs for computation support in wireless access networks. Applying machine learning (ML) algorithms at the network edge, i.e., edge learning, requires efficient training, in order to adapt themselves to the varying environment. However, the transmission of the training data collected by devices requires huge wireless resources. To address this issue, we exploit the fact that data samples have different importance for training, and use an influence function to represent the importance. Based on the importance metric, we propose a data pre-processing scheme combining data filtering that reduces the size of dataset and data compression that removes redundant information. As a result, the number of data samples as well as the size of every data sample to be transmitted can be substantially reduced while keeping the training accuracy. Furthermore, we propose device scheduling policies, including rate-based and Monte-Carlo-based policies, for multi-device multi-channel systems, maximizing the summation of data importance of scheduled devices. Experiments show that the proposed device scheduling policies bring more than 2% improvement in training accuracy.",
        "DOI": "10.23919/JCIN.2022.10005217",
        "paper_author": "Huang X.",
        "affiliation_name": "Beijing National Research Center for Information Science and Technology",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60104026",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Exploring the multidimensional factors and emergence mechanisms of industrial symbiotic relationships based on machine learning",
        "publication": "Journal of Cleaner Production",
        "citied_by": "5",
        "cover_date": "2022-12-25",
        "Abstract": "Industrial symbiosis is widely sought for industrial sustainability in recent years. While multiple factors are identified important for industrial symbiosis occurrence, the complex interactions among these factors and the emergence mechanisms of industrial symbiotic relationships (SR) remain not well explored in the literature, especially not with empirical data. In this study, we aim to address this knowledge gap based on machine learning and first-hand data collected from a survey of 201 enterprises in Chun'an County, Zhejiang Province, China. The mechanisms behind these symbiotic relationships were explored by simulating the interactions and nonlinear effects of 37 selected factors categorized into sociopolitical, economic, and technological dimensions. We found that the sociopolitical dimension factors (particularly the enterprises' demand for cleaner production and the indirect influence from similar environmental protection behaviors of other enterprises) play the most important role behind industrial SR establishment. Imperfect regulations or lack of legal basis, the scale of economy for SR participation, and the varying quality of reused raw materials or products are identified as key obstacles. Our multidimensional analysis revealed non-linear effects suggests that such a system understanding on influencing factors and the emergence mechanism of industrial SR is necessary and important to formulate relevant policy for boosting industrial symbiosis and thus industrial sustainability.",
        "DOI": "10.1016/j.jclepro.2022.135169",
        "paper_author": "Wang S.",
        "affiliation_name": "Beijing Forestry University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60006782",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Climate change adaptation of smallholders on the Tibetan plateau under government interventions",
        "publication": "Journal of Cleaner Production",
        "citied_by": "13",
        "cover_date": "2022-12-25",
        "Abstract": "The increasing severity of climate change has posed a great challenge to smallholders' livelihoods. In addition to smallholders' autonomous adaptations, policy-makers need to consider how to make sound government interventions to help smallholders effectively adapt to climate change. This study aims to explore how smallholders adapt to climate change under government interventions, and in turn provide recommendations for the government to better promote smallholder adaptation. To achieve this purpose, 1552 household survey data were collected in four agro-pastoral regions of the Tibetan Plateau (TP). A machine learning approach (boosted regression tree, BRT) was used to explore the factors influencing the adaption strategies of smallholders to climate change, especially the role of government interventions in this process. The results show that the smallholders mainly adopted four adaptation strategies (off-home activities, nature reclaiming farmland, raising more livestock, and crop management), while local governments helped them by providing subsidies, training, credit, insurance, and improved varieties; building roads and irrigation facilities; and organizing cooperatives. The results of the BRT model show that the natural capital indicators (elevation, farmland area) were still important factors influencing the smallholders' adoption of adaptation strategies, because natural capital reflects the livelihood basis of smallholders to some extent. The results also suggest that government interventions such as subsidies, cooperatives, and training played an important role in this process. Based on these results, we propose targeted policy recommendations to help local governments improve existing government interventions and to provide lessons for governments in other regions or countries to plan government interventions to promote smallholder adaptation.",
        "DOI": "10.1016/j.jclepro.2022.135171",
        "paper_author": "He X.",
        "affiliation_name": "Institute of Mountain Hazards and Environment, Chinese Academy of Sciences",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China",
        "affiliation_id": "60020259",
        "affiliation_state": "Sichuan"
    },
    {
        "paper_title": "An Analysis of the Relationship between Social Protection Program Status and the Incidence of Food Insecure Households in Aceh Province",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2022-12-22",
        "Abstract": "One of the government's targets in achieving social development is to reduce food insecurity. Limited economic access for households to obtain sufficient food can cause food insecurity. The social protection program is a policy that plays an important role in efforts to fulfil economic access for households to reduce the incidence of food insecurity. A study about the contribution of social protection programs to food insecurity events needs to be carried out. One of the machine learning methods, namely classification tree can be applied to achieve this goal. The data used in this study is from the 2020 Indonesian Social Economic Survey (SUSENAS) in Aceh Province which consists of food insecurity status as an output variable and 7 input variables; PKH, KKS, BPNT, Local Government Assistance, BPJS, Jamkesda, and PIP. The results obtained are that BPJS provides the largest contribution in determining the status of food insecurity with an AUC value of 0.60.",
        "DOI": "10.1063/5.0112345",
        "paper_author": "Ramadhani E.",
        "affiliation_name": "Universitas Syiah Kuala",
        "affiliation_city": "Banda Aceh",
        "affiliation_country": "Indonesia",
        "affiliation_id": "60069406",
        "affiliation_state": "Aceh"
    },
    {
        "paper_title": "Automated circuit sizing with multi-objective optimization based on differential evolution and Bayesian inference",
        "publication": "Knowledge-Based Systems",
        "citied_by": "21",
        "cover_date": "2022-12-22",
        "Abstract": "Manual sizing of analog circuit specifications has become challenging owing to their ever-increasing complexity. Especially for innovative, large-scale circuit designs with numerous design variables, operating conditions, and conflicting objectives to optimize, analog designers must run time-consuming simulations for several weeks to find the optimum configuration. Recently, machine learning and optimization techniques have been applied in the field of analog circuit design, wherein evolutionary algorithms and Bayesian models have shown good results for circuit sizing tasks. In this context, we introduce multi-objective optimization based on differential evolution and Bayesian inference (MODEBI)—a design optimization method based on generalized differential evolution 3 (GDE3) and Gaussian processes (GPs). The proposed method can perform sizing for complex circuits that require optimization of many design variables and conflicting objectives. Although state-of-the-art methods reduce multi-objective problems to single-objective optimization and potentially induce a priori bias, the proposed method searches directly over the multi-objective space using Pareto dominance and ensures that designers are provided with diverse solutions to choose from. To reduce optimization time, we propose using GPs to model the circuit and employing this surrogate model to preselect candidates. However, this results in a more complex offspring selection process, and the diversity in population survival must be specifically addressed. This paper proposes several solutions to these problems, resulting in multiple MODEBI variations. To the best of our knowledge, this is the first method that specifically addresses solution diversity and simultaneously focuses on minimizing the number of simulations required to obtain feasible configurations. The evaluation performed on two voltage regulators with different complexity levels showed that the proposed offspring selection method and survival policy can obtain highly diverse feasible solutions considerably faster than GDE3 or Bayesian optimization-based algorithms.",
        "DOI": "10.1016/j.knosys.2022.109987",
        "paper_author": "Vişan C.",
        "affiliation_name": "University Politehnica of Bucharest",
        "affiliation_city": "Bucharest",
        "affiliation_country": "Romania",
        "affiliation_id": "60003161",
        "affiliation_state": "Bucharest"
    },
    {
        "paper_title": "Exploring Vegetative Indices for Yield Prediction using Sentinel 2 Data – A Study in a Select Region of Karnataka",
        "publication": "International Journal of Intelligent Systems and Applications in Engineering",
        "citied_by": "2",
        "cover_date": "2022-12-20",
        "Abstract": "India is an agrarian economy and largest share of population depend on agriculture. Though there are mechanisms to approximately estimate crop yield by means of controlled experiments or past data, the reliability is limited. As a matter of fact, the crop yield is based on estimates that may suffer from multiple bias. In recent years, remote sensing images augmented with machine learning and deep learning techniques help us get the efficient crop yield statistics based on the crop on the field which help policymakers in devising better policies and governance. Remote sensing images of crops under study when subjected to machine learning techniques, one can classify the images into homogenous crop classes and record crop health / growth indicators such as Normalized Difference Vegetation Index (NDVI), Normalized Difference Water Index (NDWI) and Soil Adjusted Vegetative Index (SAVI) based on the training dataset and image quality, which further leads to crop yield estimates with desired level of accuracy. The present study investigates the relationship between yield influencing parameters such as physical variables, soil, weather characteristics and vegetation indices. Using correlation and multiple regression analysis, most efficient parameters that best estimate the crop yield is determined using the satellite data obtained for a study region in central part of the state of Karnataka. It was found that one of the vegetation indices, Soil Adjusted Vegetative Index values can predict near to accurate crop yield values by the end of 81 days after transplantation of paddy where as NDVI and NDVI can give yield estimated only after 116 days of transplantation. Thus crop yield estimates using SAVI works better in terms of predicting paddy yield at least one month before (after 81 days of transplantation) actual harvesting i.e., after 120 days as practiced by farmers in the study area.",
        "DOI": "NA",
        "paper_author": "Geetha M.",
        "affiliation_name": "Bapuji Institute of Engineering and Technology, Davangere",
        "affiliation_city": "Davangere",
        "affiliation_country": "India",
        "affiliation_id": "60076418",
        "affiliation_state": "KA"
    },
    {
        "paper_title": "Teaching artificial intelligence as a fundamental toolset of medicine",
        "publication": "Cell Reports Medicine",
        "citied_by": "14",
        "cover_date": "2022-12-20",
        "Abstract": "Artificial intelligence (AI) is transforming the practice of medicine. Systems assessing chest radiographs, pathology slides, and early warning systems embedded in electronic health records (EHRs) are becoming ubiquitous in medical practice. Despite this, medical students have minimal exposure to the concepts necessary to utilize and evaluate AI systems, leaving them under prepared for future clinical practice. We must work quickly to bolster undergraduate medical education around AI to remedy this. In this commentary, we propose that medical educators treat AI as a critical component of medical practice that is introduced early and integrated with the other core components of medical school curricula. Equipping graduating medical students with this knowledge will ensure they have the skills to solve challenges arising at the confluence of AI and medicine.",
        "DOI": "10.1016/j.xcrm.2022.100824",
        "paper_author": "Ötleş E.",
        "affiliation_name": "University of Michigan Medical School",
        "affiliation_city": "Ann Arbor",
        "affiliation_country": "United States",
        "affiliation_id": "60033182",
        "affiliation_state": "MI"
    },
    {
        "paper_title": "Global and regional models for identification of cooling technology in thermal power generation for water demand estimations in water-energy nexus studies",
        "publication": "Journal of Cleaner Production",
        "citied_by": "9",
        "cover_date": "2022-12-20",
        "Abstract": "Water-energy nexus studies aim to connect the process of power generation with the corresponding water demand. The lack of information on the currently installed cooling systems at individual power plants is a challenge for the assessment of the water use on the power plant-level, which complicates decision-making in water management, especially in water-stressed regions. In this study, we investigate the spatial and temporal trends in cooling technology installations globally. Based on that, we propose a machine learning model for cooling technology identification on a regional and global level, which uses a combination of feature selection and classification algorithms. The global model demonstrates an average test set accuracy of 85.42%, which corresponds to only a minor underestimation of the actual global water footprint of 1.78% when the cooling technology and water footprint of individual power plant units is unknown. Apart from that, a special emphasis was placed on regions characterized by high and extremely high water stress, where mistakes in water policy planning and water management may lead to an unsustainable water use or even to an overexploitation of water resources. In these regions, the calculated test set accuracy was 80.83%, which is significantly larger than the average accuracy of a majority class model. The results and the method proposed in this study enable cooling system identification in individual power units using information available from other sources, such as the water stress score or seasonal freshwater availability in the region.",
        "DOI": "10.1016/j.jclepro.2022.134842",
        "paper_author": "Lohrmann A.",
        "affiliation_name": "LUT kauppakorkeakoulu",
        "affiliation_city": "Lappeenranta",
        "affiliation_country": "Finland",
        "affiliation_id": "60226620",
        "affiliation_state": "South Karelia"
    },
    {
        "paper_title": "Routing network traffic predict based on firewall logs using Machine Learning",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2022-12-16",
        "Abstract": "This paper employs firewall system log data to address the internet traffic issue caused by misconfigured firewall policies. The UCI machine learning library provided the data set. Data sets are processed using feature engineering to produce classifiable results. Machine learning techniques are used to predict permissible base traffic for specific network traffic based on log data. The machine learning model should be able to predict actions that should be taken quickly in order to avoid unnecessary network latency, which can be detrimental to network performance. The test time must be short, regardless of the complexity of the training. This article tested several machine learning approaches and discovered that LBGM is the most effective.",
        "DOI": "10.1145/3584376.3584525",
        "paper_author": "Li Y.",
        "affiliation_name": "Xinjiang University",
        "affiliation_city": "Urumqi",
        "affiliation_country": "China",
        "affiliation_id": "60015780",
        "affiliation_state": "Xinjiang"
    },
    {
        "paper_title": "Semi-supervised learning based framework for urban level building electricity consumption prediction",
        "publication": "Applied Energy",
        "citied_by": "13",
        "cover_date": "2022-12-15",
        "Abstract": "The spatial feature of building energy consumption in a city is essential for urban level energy planning and policy making. With the increasing availability of urban level building energy benchmarking datasets, machine learning has shown a powerful capability of making data-driven predictions on urban level building energy consumption. However, the building energy benchmarking datasets usually only cover large buildings, which are not sufficient representations of all buildings in a city. Besides building energy benchmarking datasets, many other urban level open datasets are also valuable to building energy prediction, but they do not contain building energy use data, in other words, they are unlabeled. This study proposes a novel framework based on semi-supervised learning to make effective use of the unlabeled datasets to develop more generic urban level data-driven building energy prediction models, and energy mapping with higher space resolution. The framework consists of preliminary labeling, selection of pseudo labeled samples and predictive modelling. Several machine learning algorithms are proposed and compared for generating pseudo labels of building electricity consumption for unlabeled datasets of small and medium-sized buildings. A selection process consisting of convergence testing and screening is designed to select pseudo labeled samples with high credibility to enlarge the labeled dataset. A novel two-level performance evaluation method is proposed to evaluate the performance of the framework at both urban level and district level to enhance the spatial resolution of the predictions. The framework is implemented to model and map the individual electricity consumptions of all buildings in two years in the districts of New York City using multiple open datasets. The results show significant improvement in terms of prediction accuracy at both levels. In addition, the applicability of the model to various buildings in the city is remarkably enhanced.",
        "DOI": "10.1016/j.apenergy.2022.120210",
        "paper_author": "Jin X.",
        "affiliation_name": "The Hong Kong Polytechnic University",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong",
        "affiliation_id": "60008928",
        "affiliation_state": "Hong Kong"
    },
    {
        "paper_title": "Deep reinforcement learning-based strategy for charging station participating in demand response",
        "publication": "Applied Energy",
        "citied_by": "22",
        "cover_date": "2022-12-15",
        "Abstract": "The trend of zero-carbonization has accelerated the prevalence of electric vehicles (EVs) owing to their advantages of low carbon emissions and high energy efficiency. The stochastic and high charging load of EVs results in a non-negligible challenge that may cause grid overload. A promising approach is the participation of charging stations in demand response as load aggregators by coordinating the charging power of electric vehicles. However, improper coordination of charging load may lead to unfulfilled charging demand, which would cause dissatisfaction on the demand side. In this study, the incentive-based and time-varying demand response mechanism is considered when charging stations coordinate charging of multiple EVs. A decentralized decision-making framework is innovatively applied to provide charging power of each EV. The charging process is modeled as a Markov decision process, and a virtual price is designed to help decide the charging power. Deep reinforcement learning algorithms such as deep deterministic policy gradient are applied to determine the charging strategy of multiple and heterogeneous EVs. Numerical experiments are performed to validate the effectiveness of the proposed method. A comparison with an optimal charging strategy and a heuristic rule-based method shows that the proposed method can trade off the revenue from demand response and user satisfaction, as well as reduce the peak load of the charging station. Furthermore, a test with inaccurate departure information indicates the robustness of the proposed method.",
        "DOI": "10.1016/j.apenergy.2022.120140",
        "paper_author": "Jin R.",
        "affiliation_name": "Peking University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60014966",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A machine learning approach to investigate the build-up of surface ozone in Mexico-City",
        "publication": "Journal of Cleaner Production",
        "citied_by": "12",
        "cover_date": "2022-12-15",
        "Abstract": "Ground-level ozone is an important pollutant regarding air quality and climate. Mexico City frequently experiences severe ozone episodes due to a combination of strong ozone precursor emissions and its specific topographical environment which critically impacts meteorological conditions. High ozone levels during these episodes cause harmful effects to the public health and the environment. This necessitates ranking air quality and meteorological variables according to their contributions towards the build-up of ozone. In this study, three machine learning models are used to learn a prediction function with hourly data of eight predictors as input and hourly ground-level ozone mixing ratios as output. One-year hourly data of eight predictors collected in Mexico-City from March 2015 to February 2016 is employed to train and test the models. The best model, capturing ozone peak levels with 92% accuracy during 6–18 March 2016, is used to rank the predictors according to their importance in the build-up of ozone applying a shapley additive explanations approach based on the game theory shapley values. This 6–18 March 2016 period encompassed different meteorological and emission conditions and included a severe ozone smog episode from 12 to 17 March 2016. Such ranking of the air quality and meteorological variables is crucial for policy-making decisions regarding the prevention and mitigation of ozone detrimental effects during severe ozone episodes and provides insight into the functional dependency of ozone on its predictors. The proposed approach showcases Mexico City, but its principles can be applied for ozone episodes at any other location.",
        "DOI": "10.1016/j.jclepro.2022.134638",
        "paper_author": "Ahmad M.",
        "affiliation_name": "College of Natural Sciences and Mathematics",
        "affiliation_city": "Houston",
        "affiliation_country": "United States",
        "affiliation_id": "60151391",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "An interpretable forecasting framework for energy consumption and CO<inf>2</inf> emissions",
        "publication": "Applied Energy",
        "citied_by": "34",
        "cover_date": "2022-12-15",
        "Abstract": "It is a well-established fact that energy consumption and production, as the primary sources of greenhouse gases, contribute to climate change and global warming issues. The analysis and estimation of the factors that contribute to these harmful gases will be of great assistance in the development of policies to reduce carbon dioxide emissions. In addition to identifying the factors related to energy consumption and CO2 emissions, forecasting the variable of interest as accurately as possible has a key role in increasing the efficiency of energy strategies to be implemented. Unlike studies in the literature, this study not only forecasts the future value of energy consumption and CO2 emissions but also determines the relationship between the predictions and the influential variables by revealing the contribution of each variable to the prediction. For this purpose, the study proposes an interpretable forecasting framework based on values of the Shapley additive explanation (SHAP) to provide a simpler explanation of machine learning (ML) models in forecasting energy consumption and CO2 emissions. The results obtained show that the total electricity generation from different energy sources is found to be the most important variable interacting positively with both energy consumption and CO2 emissions. Also, the influence of the predictors on projections made before and after COVID-19 has changed dramatically. The proposed method may assist policymakers in making future energy investments and establishing energy laws more accurately and efficiently as it explains the drivers of the forecasts.",
        "DOI": "10.1016/j.apenergy.2022.120163",
        "paper_author": "Aras S.",
        "affiliation_name": "Dokuz Eylül Üniversitesi",
        "affiliation_city": "Izmir",
        "affiliation_country": "Türkiye",
        "affiliation_id": "60014930",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Assessing the ecological risk induced by PM<inf>2.5</inf> pollution in a fast developing urban agglomeration of southeastern China",
        "publication": "Journal of Environmental Management",
        "citied_by": "14",
        "cover_date": "2022-12-15",
        "Abstract": "High PM2.5 concentration threats ecosystem functions but limited quantitative studies have recognized PM2.5 pollution as an individual stressor in evaluating ecological risk. In this study, we applied a machine-learning-based simulation model incorporating full-coverage satellite-driven PM2.5 dataset to estimate high-resolution ground PM2.5 concentration for the Golden Triangle of Southern Fujian Province, China (GTSF) in 2030 under two Representative Concentration Pathways (RCPs). Based on the simulation output, the ecological risk's spatiotemporal change and the risk for different land cover types, which were caused by PM2.5 pollution, were assessed. We found that the PM2.5 levels and ecological risk in the GTSF under RCP 4.5 would be reduced while those under RCP 8.5 would continue to increase. The regions with the highest ecological risk under RCP 4.5 are the most urbanized and industrialized districts, while those with the highest ecological risk under RCP 8.5 are of the highest rate in urbanization and the greatest decrease in planetary potential layer height. For both base years and 2030 under two RCPs, the ecological risk on developed land is the highest, while that on the forest is the lowest. Our study can provide useful information for environmental policy risk assessment.",
        "DOI": "10.1016/j.jenvman.2022.116284",
        "paper_author": "Wang L.",
        "affiliation_name": "Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60019499",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Performance Assessment of Reinforcement Learning Policies for Battery Lifetime Extension in Mobile Multi-RAT LPWAN Scenarios",
        "publication": "IEEE Internet of Things Journal",
        "citied_by": "2",
        "cover_date": "2022-12-15",
        "Abstract": "Considering the dynamically changing nature of the radio propagation environment, the envisioned battery lifetime of the end device (ED) for massive machine-type communication (mMTC) stands for a critical challenge. As the selected radio technology bounds the battery lifetime, the possibility of choosing among several low-power wide-area network (LPWAN) technologies integrated at a single ED may dramatically improve its lifetime. In this article, we propose a novel approach of battery lifetime extension utilizing reinforcement learning (RL) policies. Notably, the system assesses the radio environment conditions and assigns the appropriate rewards to minimize the overall power consumption and increase reliability. To this aim, we carry out extensive propagation and power measurements campaigns at the city-scale level and then utilize these results for composing real-life use cases for static and mobile deployments. Our numerical results show that RL-based techniques allow for a noticeable increase in EDs' battery lifetime when operating in the multi-RAT mode. Furthermore, out of all considered schemes, the performance of the weighted average policy shows the most consistent results for both considered deployments. Specifically, all RL policies can achieve 90% of their maximum gain during the initialization phase for the stationary EDs while utilizing less than 50 messages. Considering the mobile deployment, the improvements in battery lifetime could reach 200%.",
        "DOI": "10.1109/JIOT.2022.3197834",
        "paper_author": "Stusek M.",
        "affiliation_name": "Brno University of Technology, Faculty of Electrical Engineering and Communication",
        "affiliation_city": "Brno",
        "affiliation_country": "Czech Republic",
        "affiliation_id": "60108593",
        "affiliation_state": "South Moravian Region"
    },
    {
        "paper_title": "Explainable Bayesian networks applied to transport vulnerability",
        "publication": "Expert Systems with Applications",
        "citied_by": "14",
        "cover_date": "2022-12-15",
        "Abstract": "To deal with increasing amounts of data, decision and policymakers frequently turn to advances in machine learning and artificial intelligence to capitalise on the potential reward. But there is also a reluctance to trust black-box models, especially when such models are used to support decisions and policies that affect people directly, like those associated with transport and people's mobility. Recent developments focus on explainable artificial intelligence to bolster models’ trustworthiness. In this paper, we demonstrate the use of an explainable-by-design model, Bayesian Networks, on travel behaviour. The model incorporates various demographic and socioeconomic variables to describe full day activity chains: activity and mode choice, as well as the activity and trip durations. More importantly, this paper shows how the model can be used to provide the most relevant explanation for people's observed travel behaviour. The overall goal is to show that model explanations can be quantified and, therefore, assist policymakers to truly make evidence-based decisions. This goal is achieved through two case studies to explain people's vulnerability as it pertains to their total trip duration.",
        "DOI": "10.1016/j.eswa.2022.118348",
        "paper_author": "de Waal A.",
        "affiliation_name": "University of Pretoria",
        "affiliation_city": "Pretoria",
        "affiliation_country": "South Africa",
        "affiliation_id": "60021902",
        "affiliation_state": "Gauteng"
    },
    {
        "paper_title": "Machine-Learning-Empowered Passive Beamforming and Routing Design for Multi-RIS-Assisted Multihop Networks",
        "publication": "IEEE Internet of Things Journal",
        "citied_by": "14",
        "cover_date": "2022-12-15",
        "Abstract": "This article proposes a novel machine-learning-based routing optimization for the multiple reconfigurable intelligent surfaces (M-RIS)-assisted multihop cooperative networks, in which a practical phase model for reconfigurable intelligent surface (RIS) with the amplitude variation based on the corresponding discrete phase shift is considered. We aim to maximize the end-to-end data rate in the proposed network by jointly optimizing the data transmission path, the passive beamforming design of RIS, and transmit power allocation. To tackle this complicated nonconvex problem, we divide it into two subtasks: 1) the passive beamforming design of the RIS and 2) joint routing and power allocation optimization. First, for the passive beamforming design of RIS, we develop a distributed learning algorithm that employs a cascade forward backpropagation network in each relay node to solve the RIS coefficients optimization problem by directly using the optimization target to train the cascade networks. This solution can avoid the curse of dimensionality of traditional reinforcement learning algorithms in the RIS optimization problem. Then, based on the result of RIS optimization, we introduce the proximal policy optimization (PPO) algorithm with the clipping method to find solutions for joint optimization of routing and power allocation via achieving the long-term benefit in the Markov decision process (MDP). Simulation results show that the proposed learning-based scheme can learn from the environment to improve its policy stability and efficiency in the iterative training process for optimizing routing and RIS and significantly outperform the benchmark schemes.",
        "DOI": "10.1109/JIOT.2022.3195543",
        "paper_author": "Huang C.",
        "affiliation_name": "University of Surrey",
        "affiliation_city": "Guildford",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60021097",
        "affiliation_state": "Surrey"
    },
    {
        "paper_title": "Aspect2Labels: A novelistic decision support system for higher educational institutions by using multi-layer topic modelling approach",
        "publication": "Expert Systems with Applications",
        "citied_by": "22",
        "cover_date": "2022-12-15",
        "Abstract": "Aspect-based sentiment analysis (ABSA) has gained a rising concentration recently. It aims to provide a set of aspect terms and sentiments from a piece of text. Educational Data Mining (EDM) is now an essential tool for analysing pedagogical data. In academic institutions, student feedback is an influential gauge to measure the quality of the teaching–learning process. It helps higher education institutions to reconsider and improve their policies for student recruitment and retention. This paper proposed a situation awareness multi-layer topic modelling and enhanced hybrid machine learning approach for evaluating students’ textual feedback data in academic institutions. The proposed Aspect2Labels (A2L) approach is divided our system into three layers. To preserve semantic information, we extracted general aspects terms in the first layer known as high-level aspects. We pulled low-level aspects terms associated with high-level aspect terms in the second layer and the third layer used for sentiment orientation. We used zero-shot learning, LDA, and different variants of LDA for the aspect extraction process. We performed annotation on unlabelled students’ comments using our proposed A2L approach, and we obtained 91.3% accuracy in this process. We developed and tested novel algorithms for aspect terms mapping to label each aspect term to corresponding feedback. Different machine learning algorithms have been used to classify sentiments according to extracted aspects. We have also proposed and used Variable Global Feature Selection Scheme (VGFSS) and Variable Stopwords Filtering (VSF) to improve the performance of classifiers. We have managed to get 97% and 93% accuracy on the test dataset using Support Vector Machine (SVM) and Artificial Neural Networks (ANN), respectively. We highly suggest that our novel approach of aspect-oriented sentiment analysis could provide adequate understanding to analyse students’ feedback.",
        "DOI": "10.1016/j.eswa.2022.118119",
        "paper_author": "Hussain S.",
        "affiliation_name": "Zhengzhou University",
        "affiliation_city": "Zhengzhou",
        "affiliation_country": "China",
        "affiliation_id": "60018554",
        "affiliation_state": "Henan"
    },
    {
        "paper_title": "Renewable energy strategy analysis in relation to environmental pollution for BRICS, G7, and EU countries by using a machine learning framework and panel data analysis",
        "publication": "Frontiers in Environmental Science",
        "citied_by": "2",
        "cover_date": "2022-12-14",
        "Abstract": "The present research uses machine learning, panel data and time series prediction and forecasting techniques to establish a framework between a series of renewable energy and environmental pollution parameters, considering data for BRICS, G7, and EU countries, which can serve as a tool for optimizing the policy strategy in the sustainable energy production sector. The results indicates that XGBoost model for predicting the renewable energy production capacity reveals the highest feature importance among independent variables is associated with the gas consumption parameter in the case of G7, oil consumption for EU block and GHG emissions for BRICS, respectively. Furthermore, the generalized additive model (GAM) predictions for the EU block reveal the scenario of relatively constant renewable energy capacity if gas consumption increases, while oil consumption increases determine an increase in renewable energy capacity until a kick point, followed by a decrease. The GAM models for G7 revealed the scenario of an upward trend of renewable energy production capacity, as gas consumption increases and renewable energy production capacity decreases while oil consumption increases. In the case of the BRICS geopolitical block, the prediction scenario reveals that, in time, an increase in gas consumption generates an increase in renewable energy production capacity. The PCA emphasizes that renewable energy production capacity and GHG, respectively CO2 emissions, are highly correlated and are integrated into the first component, which explains more than 60% of the variance. The resulting models represent a good prediction capacity and reveal specific peculiarities for each analyzed geopolitical block. The prediction models conclude that the EU economic growth scenario is based on fossil fuel energy sources during the first development stage, followed by a shift to renewable energy sources once it reaches a kick point, during the second development stage. The decrease in renewable energy production capacity when oil consumption increases indicates that fossil fuels are in trend within the G7 economy. In the case of BRICS, it is assumed that gas consumption appears because of increasing the industrial capacity, followed by the increase of economic sustainability, respectively. In addition, the generalized additive models emphasize evolution scenarios with different peculiarities, specific for each analyzed geopolitical block.",
        "DOI": "10.3389/fenvs.2022.1005806",
        "paper_author": "Cristea D.S.",
        "affiliation_name": "Universitatea Dunarea de Jos din Galati",
        "affiliation_city": "Galati",
        "affiliation_country": "Romania",
        "affiliation_id": "60008717",
        "affiliation_state": "Galati"
    },
    {
        "paper_title": "An immediate-return reinforcement learning for the atypical Markov decision processes",
        "publication": "Frontiers in Neurorobotics",
        "citied_by": "2",
        "cover_date": "2022-12-13",
        "Abstract": "The atypical Markov decision processes (MDPs) are decision-making for maximizing the immediate returns in only one state transition. Many complex dynamic problems can be regarded as the atypical MDPs, e.g., football trajectory control, approximations of the compound Poincaré maps, and parameter identification. However, existing deep reinforcement learning (RL) algorithms are designed to maximize long-term returns, causing a waste of computing resources when applied in the atypical MDPs. These existing algorithms are also limited by the estimation error of the value function, leading to a poor policy. To solve such limitations, this paper proposes an immediate-return algorithm for the atypical MDPs with continuous action space by designing an unbiased and low variance target Q-value and a simplified network framework. Then, two examples of atypical MDPs considering the uncertainty are presented to illustrate the performance of the proposed algorithm, i.e., passing the football to a moving player and chipping the football over the human wall. Compared with the existing deep RL algorithms, such as deep deterministic policy gradient and proximal policy optimization, the proposed algorithm shows significant advantages in learning efficiency, the effective rate of control, and computing resource usage.",
        "DOI": "10.3389/fnbot.2022.1012427",
        "paper_author": "Pan Z.",
        "affiliation_name": "Hunan University",
        "affiliation_city": "Changsha",
        "affiliation_country": "China",
        "affiliation_id": "60032356",
        "affiliation_state": "Hunan"
    },
    {
        "paper_title": "Energy Management Strategy for Hybrid Electric Vehicle Based on the Deep Reinforcement Learning Method",
        "publication": "Diangong Jishu Xuebao/Transactions of China Electrotechnical Society",
        "citied_by": "9",
        "cover_date": "2022-12-10",
        "Abstract": "To resolve the problem of poor adaptability to varying driving cycles when energy management strategy for hybrid electric vehicles is running online, a design method of energy management strategy (EMS) with deep reinforcement learning ability is proposed. The presented method determines the optimal change rate of engine power based on the deep deterministic policy gradient algorithm and then establishes the power management strategy of the onboard energy system. The established control strategy includes a two-layer logical framework of offline interactive learning and online update learning. The control parameters are dynamically updated according to the vehicle operation characteristics to improve the vehicle energy-saving effect in online applications. To verify the proposed control strategy, the effectiveness of the algorithm is analyzed with the practical vehicle test data in Shenyang, and compared with the control effect of the particle swarm optimization algorithm. The results show that the proposed deep reinforcement learning EMS can achieve energy-saving effects better than particle swarm optimization-based strategy. Especially when the driving characteristics of vehicles change suddenly, deep reinforcement learning control strategy can achieve better adaptability.",
        "DOI": "10.19595/j.cnki.1000-6753.tces.211342",
        "paper_author": "Chen Z.",
        "affiliation_name": "School of Mechanical Engineering and Automation, Northeastern University",
        "affiliation_city": "Shenyang",
        "affiliation_country": "China",
        "affiliation_id": "60118695",
        "affiliation_state": "Liaoning"
    },
    {
        "paper_title": "Characterizing carbon emissions from China V and China VI gasoline vehicles based on portable emission measurement systems",
        "publication": "Journal of Cleaner Production",
        "citied_by": "46",
        "cover_date": "2022-12-10",
        "Abstract": "On-road vehicle has been a prominent emission source, hence a key target of control for environment, health, and climate concerns. While considerable efforts have been made to investigate criteria pollutants such as adverse gaseous and particulate emissions across the world, there is limited systematic research to characterize on-road carbon emissions, especially as countries strive to set and achieve various climate goals (i.e., carbon neutrality, carbon peaking by a certain year, etc.) through cleaner vehicle technologies such as the latest vehicle emission standard, China VI (2020). This study used the portable emission measurement system (PEMS) to reveal the real-world emissions from two light-duty gasoline vehicles, one meeting the China V standard and the other China VI. The PEMS measurements were performed secondly in urban and suburban areas respectively, and finally yielded 10 h of rich and finely resolved emissions data. In addition of single-factor analysis, this study employed two machine learning models, stepwise regression and light gradient boosting machine (LightGBM), to investigate the coupling effects among vehicle/traffic parameters and identify driving behavior, engine condition, and external environment as key determining factors that affect vehicular carbon emissions. In particular, engine load percent and engine speed were confirmed to be the most dominant factors affecting vehicular carbon emissions under both China V and China VI standards, explaining ∼60% of variability in carbon emissions. As a result, vehicle specific power (VSP) is derived from vehicle travel parameters and used as a surrogate for carbon emission estimation. The external facility and environmental factors turned out significant as well in affecting vehicle carbon emissions: carbon emissions from the same vehicle were significantly higher in an urban setting then in suburban areas, mostly due to the more aggressive driving patterns in congested urban traffic. Travelling under relatively better traffic conditions on expressways gave the lowest per-distance emission factors of CO2, CO, and NOx. Comparing between the China V and VI vehicle, our PEMS data show significant carbon emission reduction benefits (15.9% in CO2, 28.8% in CO) as we move to the new China VI standard, although such benefit was not observed in NOx emissions.",
        "DOI": "10.1016/j.jclepro.2022.134458",
        "paper_author": "Zhu X.h.",
        "affiliation_name": "State Key Laboratory of Ocean Engineering",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60123520",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Understanding robustness in multiscale nutrient abatement: Probabilistic simulation-optimization using Bayesian network emulators",
        "publication": "Journal of Cleaner Production",
        "citied_by": "8",
        "cover_date": "2022-12-10",
        "Abstract": "Ecosystem management in the face of uncertain disturbances has triggered increasing practices of resilience thinking. A multiscale probabilistic simulation-optimization framework is developed based on the nested nature of watersheds to inform decision robustness for Best Management Practices (BMPs). We presented a novel approach using hybrid Bayesian Networks (BNs) as interpretable and probabilistic emulators of process-based models. The hybrid BNs established at the scale of Hydrologic Response Units (HRUs) are embedded into simulation-optimization, whereby we analyze the cost-effectiveness-robustness of candidate BMP strategies at the subbasin scale. The optimal strategy is identified in compliance with water quality standards using watershed-scale integer programming. We apply the approaches in a typical intensively cultivated plateau watershed adjacent to Lake Dianchi, one of the three most eutrophic lakes in China. Our findings suggest that the hybrid BNs, incorporating both quantitative and qualitative information, are reliable emulators of the Soil and Water Assessment Tool (SWAT) in capturing critical pathways of diffuse phosphorus. Tradeoffs among cost, effectiveness, and robustness follow the law of diminishing marginal benefits. The optimum BMP strategies vary with policymakers’ preference toward robustness levels. Our findings indicate that robustness should be accounted for as an additional decision attribute besides costs and pollution mitigation. The benefits of the modeling framework are to (i) reduce over 99% computation complexity and support efficient decision-making under multifaceted uncertainties; (ii) improve interpretability and reliability of machine learning emulators; and (iii) inform policymakers of robustness with the probability of water quality restoration success.",
        "DOI": "10.1016/j.jclepro.2022.134394",
        "paper_author": "Dong F.",
        "affiliation_name": "Jinan University",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60017456",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Impacts of traffic-related particulate matter pollution on semen quality: A retrospective cohort study relying on the random forest model in a megacity of South China",
        "publication": "Science of the Total Environment",
        "citied_by": "15",
        "cover_date": "2022-12-10",
        "Abstract": "Background: Emerging evidence shows the detrimental impacts of particulate matter (PM) on poor semen quality. High-resolution estimates of PM concentrations are conducive to evaluating accurate associations between traffic-related PM exposure and semen quality. Methods: In this study, we firstly developed a random forest model incorporating meteorological factors, land-use information, traffic-related variables, and other spatiotemporal predictors to estimate daily traffic-related PM concentrations, including PM2.5, PM10, and PM1. Then we enrolled 1310 semen donors corresponding to 4912 semen samples during the study period from January 1, 2019, and December 31, 2019 in Guangzhou city, China. Linear mixed models were employed to associate individual exposures to traffic-related PM during the entire (0–90 lag days) and key periods (0–37 and 34–77 lag days) with semen quality parameters, including sperm concentration, sperm count, progressive motility and total motility. Results: The results showed that decreased sperm concentration was associated with PM10 exposures (β: -0.21, 95 % CI: −0.35, −0.07), sperm count was inversely related to both PM2.5 (β: -0.19, 95 % CI: −0.35, −0.02) and PM10 (β: -0.19, 95 % CI: −0.33, −0.05) during the 0–90 days lag exposure window. Besides, PM2.5 and PM10 might diminish sperm concentration by mainly affecting the late phase of sperm development (0–37 lag days). Stratified analyses suggested that PBF and drinking seemed to modify the associations between PM exposure and sperm motility. We did not observe any significant associations of PM1 exposures with semen parameters. Conclusion: Our results indicate that exposure to traffic-related PM2.5 and PM10 pollution throughout spermatogenesis may adversely affect semen quality, especially sperm concentration and count. The findings provided more evidence for the negative associations between traffic-related PM exposure and semen quality, highlighting the necessity to reduce ambient air pollution through environmental policy.",
        "DOI": "10.1016/j.scitotenv.2022.158387",
        "paper_author": "Yu X.",
        "affiliation_name": "Shantou University Medical College",
        "affiliation_city": "Shantou",
        "affiliation_country": "China",
        "affiliation_id": "60010687",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Energy system digitization in the era of AI: A three-layered approach toward carbon neutrality",
        "publication": "Patterns",
        "citied_by": "7",
        "cover_date": "2022-12-09",
        "Abstract": "The transition toward carbon-neutral electricity is one of the biggest game changers in addressing climate change since it addresses the dual challenges of removing carbon emissions from the two largest sectors of emitters: electricity and transportation. The transition to a carbon-neutral electric grid poses significant challenges to conventional paradigms of modern grid planning and operation. Much of the challenge arises from the scale of the decision-making and the uncertainty associated with the energy supply and demand. Artificial intelligence (AI) could potentially have a transformative impact on accelerating the speed and scale of carbon-neutral transition, as many decision-making processes in the power grid can be cast as classic, though challenging, machine-learning tasks. We point out that to amplify AI's impact on carbon-neutral transition of the electric energy systems, the AI algorithms originally developed for other applications should be tailored in three layers of technology, markets, and policy.",
        "DOI": "10.1016/j.patter.2022.100640",
        "paper_author": "Xie L.",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "College Station",
        "affiliation_country": "United States",
        "affiliation_id": "60148980",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "Using apple watch ECG data for heart rate variability monitoring and stress prediction: A pilot study",
        "publication": "Frontiers in Digital Health",
        "citied_by": "11",
        "cover_date": "2022-12-09",
        "Abstract": "Stress is an increasingly prevalent mental health condition that can have serious effects on human health. The development of stress prediction tools would greatly benefit public health by allowing policy initiatives and early stress-reducing interventions. The advent of mobile health technologies including smartphones and smartwatches has made it possible to collect objective, real-time, and continuous health data. We sought to pilot the collection of heart rate variability data from the Apple Watch electrocardiograph (ECG) sensor and apply machine learning techniques to develop a stress prediction tool. Random Forest (RF) and Support Vector Machines (SVM) were used to model stress based on ECG measurements and stress questionnaire data collected from 33 study participants. Data were stratified into socio-demographic classes to further explore our prediction model. Overall, the RF model performed slightly better than SVM, with results having an accuracy within the low end of state-of-the-art. Our models showed specificity in their capacity to assess “no stress” states but were less successful at capturing “stress” states. Overall, the results presented here suggest that, with further development and refinement, Apple Watch ECG sensor data could be used to develop a stress prediction tool. A wearable device capable of continuous, real-time stress monitoring would enable individuals to respond early to changes in their mental health. Furthermore, large-scale data collection from such devices would inform public health initiatives and policies.",
        "DOI": "10.3389/fdgth.2022.1058826",
        "paper_author": "Velmovitsky P.E.",
        "affiliation_name": "University of Waterloo",
        "affiliation_city": "Waterloo",
        "affiliation_country": "Canada",
        "affiliation_id": "60014171",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "A generic learning simulation framework to assess security strategies in cyber-physical production systems",
        "publication": "Computer Networks",
        "citied_by": "2",
        "cover_date": "2022-12-09",
        "Abstract": "Connected systems through computerized networks are at the heart of the Industry of the future. As they merge physical entities with cyber spaces, they fall under the paradigm of cyber-physical production systems. Cybersecurity is a key challenge for such systems, as they are subject to daily attempts of intruders to gain unauthorized access to their internal resources or to compromise their integrity. The fast increase of new attack strategies requires the rapid design and assessment of new defense strategies. It entails a complex, error-prone and time-consuming process, including the clear specification of the attack and defense strategies involved, and the design and implementation of the simulation model allowing to evaluate the performances of the defense strategy. This work intends to make such a process transparent to cybersecurity managers by limiting their workload to the sole specification of the characteristics of the system and the logic of the attack and the defense. It provides a generic hybrid simulation framework for flexible evaluation of cybersecurity policies, which is demonstrated on a SYN flooding application. Therefore, the contribution is twofold: (1) The proposed framework offers a high-level environment allowing various experts to collaborate by graphically modeling a given attack strategy and the envisioned defense strategy, without engaging in heavy implementation efforts. Then the framework's executable infrastructure, which combines simulation with machine learning to understanding the interactions between the attackers & the defender, will allow them assessing the performances of these strategies. The proposed framework differs from state-of-the-art cybersecurity simulation environments in its uniqueness to combining the expressive power of a universal simulation modeling formalism with the user-friendliness of a visual simulation tool. Therefore, it offers at one side, a very high modeling flexibility for easy exploration of various cybersecurity strategies, and at the other side, integrated learning capabilities for allowing self-adaptive user-based cybersecurity strategy design. (2) The application demonstrating the framework focuses on the most encountered and still uncontrolled threats in cybersecurity, i.e. the SYN-Flooding based Denial of Service (DoS) attack. The application targeted is not meant to propose yet another SYN flood detection algorithm or to improve the state-of-the-art in that domain, but to prove the framework operationality. The experimental results obtained showcase the ability of the framework to support learning simulation-based SYN flood defense algorithm design and validation.",
        "DOI": "10.1016/j.comnet.2022.109381",
        "paper_author": "Koïta M.",
        "affiliation_name": "Université d'Abobo-Adjamé",
        "affiliation_city": "Abidjan",
        "affiliation_country": "Cote d'Ivoire",
        "affiliation_id": "60071950",
        "affiliation_state": "Lagunes"
    },
    {
        "paper_title": "A robust control-theory-based exploration strategy in deep reinforcement learning for virtual network embedding",
        "publication": "Computer Networks",
        "citied_by": "6",
        "cover_date": "2022-12-09",
        "Abstract": "Network slice management and, more generally, resource orchestration should be fully automated in 6G networks, as envisioned by the ETSI ENI. In this context, artificial intelligence (AI) and context-aware policies are certainly major options to move in this direction and to adapt service delivery to changing user needs, environmental conditions and business objectives. In this paper, we step towards this objective by addressing the problem of optimal placement of dynamic virtual networks through a self-adaptive learning-based strategy. These constantly evolving networks present, however, several challenges, mainly due to their stochastic nature, and the high dimensionality of the state and the action spaces. This curse of dimensionality requires, indeed, a broader exploration, which is not always compatible with a real-time execution in an operational network. Thus, we propose DQMC, a new strategy for virtual network embedding in mobile networks combining a Deep Reinforcement Learning (DRL) strategy, namely a Deep Q-Network (DQN), and Monte Carlo (MC). As learning is costly in time and computing resources, and sensitive to changes in the network, we suggest a control-theory-based techniques to dynamically leverage exploration in DQMC. This leads to fast, efficient, and sober learning compared to a Monte Carlo-based strategy. This also ensures a reliable solution even in the case of a change in the requests’ sizes or a node's failure, showing promising perspectives for solutions combining control-theory and machine learning.",
        "DOI": "10.1016/j.comnet.2022.109366",
        "paper_author": "Dandachi G.",
        "affiliation_name": "Institut de Recherche en Informatique et Systèmes Aléatoires",
        "affiliation_city": "Rennes",
        "affiliation_country": "France",
        "affiliation_id": "60027031",
        "affiliation_state": "Brittany"
    },
    {
        "paper_title": "Supporting responsible machine learning in heliophysics",
        "publication": "Frontiers in Astronomy and Space Sciences",
        "citied_by": "0",
        "cover_date": "2022-12-07",
        "Abstract": "Over the last decade, Heliophysics researchers have increasingly adopted a variety of machine learning methods such as artificial neural networks, decision trees, and clustering algorithms into their workflow. Adoption of these advanced data science methods had quickly outpaced institutional response, but many professional organizations such as the European Commission, the National Aeronautics and Space Administration (NASA), and the American Geophysical Union have now issued (or will soon issue) standards for artificial intelligence and machine learning that will impact scientific research. These standards add further (necessary) burdens on the individual researcher who must now prepare the public release of data and code in addition to traditional paper writing. Support for these is not reflected in the current state of institutional support, community practices, or governance systems. We examine here some of these principles and how our institutions and community can promote their successful adoption within the Heliophysics discipline.",
        "DOI": "10.3389/fspas.2022.1064233",
        "paper_author": "Narock A.",
        "affiliation_name": "NASA Goddard Space Flight Center",
        "affiliation_city": "Greenbelt",
        "affiliation_country": "United States",
        "affiliation_id": "60006337",
        "affiliation_state": "MD"
    },
    {
        "paper_title": "Deductive Learning in Wireless Mobile Networks Using QoE-Driven Data Analysis",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2022-12-07",
        "Abstract": "Current research studies provide details analysis of mobile networks using QoE based detection. In situations where the overall resource of the system cannot support loading, we propose a QoE-dnven optimization technique. The strict data forwarding policies conventionally prevent this situation. But this would lead to a high probability of blocking and loose revenues for operators. We propose re-adapting the applications, taking into consideration the applications' utility functions. This strategy improves the quality of experience for certain systems and for a fixed number of users and allows more users to be admitted to a predetermined quality. This approach has the potential to be critical in the development of future ultra-dense and environmentally friendly mobile communication networks that are expected to be self-organizing and self-healing. The above diagram represents that accuracy levels of various Classifiers. The Bagging has 72.94% of accuracy level. CV Parameter Selection classifier has 65.93% of accuracy level. The AdaBoostMl classifier has 54.89% of accuracy level. The Classification Via Regression classifier has 56.89% of accuracy level. The Attribute Selected Classifier classifier has 56.89% of accuracy level. The Cost Sensitive Classifier classifier has 65.92% of accuracy level.",
        "DOI": "10.1063/5.0111630",
        "paper_author": "Prakash E.",
        "affiliation_name": "Bharath Institute of Higher Education and Research",
        "affiliation_city": "Chennai",
        "affiliation_country": "India",
        "affiliation_id": "60104605",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Editorial: Rising stars: Clinical diabetes 2021",
        "publication": "Frontiers in Endocrinology",
        "citied_by": "0",
        "cover_date": "2022-12-07",
        "Abstract": "NA",
        "DOI": "10.3389/fendo.2022.1106804",
        "paper_author": "Al Madhoun A.",
        "affiliation_name": "Dasman Diabetes Institute",
        "affiliation_city": "Dasman",
        "affiliation_country": "Kuwait",
        "affiliation_id": "60121864",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Prediction of differential performance between advanced placement exam scores and class grades using machine learning",
        "publication": "Frontiers in Education",
        "citied_by": "5",
        "cover_date": "2022-12-06",
        "Abstract": "Introduction: Past studies have found students to perform differently between class grades and standardized test scores – two essential and complementary measures of student achievement. This study examines predictors of the relative performance between these two measures in the context of the advanced placement (AP) program, namely, we compared students’ AP exam scores to the class grade they received in the corresponding AP course. For example, if a student received a high AP class grade but a low AP exam score, what characteristics about the student or their learning context might explain such discrepancy? Methods: We used machine learning, specifically random forests, and model interpretation methods on data collected from 381 high school students enrolled in an AP Statistics course in the 2017–2018 academic year, and additionally replicated our analyses on a separate cohort of 422 AP Statistics students from the 2018–2019 academic year. Results: Both analyses highlighted students’ school and behavioral engagement as predictors of differential performance between AP class grades and AP exam scores. Discussion: Associations between behavioral engagement and differential performance suggest that the ways in which a student interacts with AP course material to obtain high class grades can differ from study habits that lead to optimal performance on the AP exam. Additionally, school-level differences in relative performance pose equity concerns towards the use of AP exam scores in high-stakes decisions, such as college admissions. Implications are discussed from a pedagogical and policy perspective.",
        "DOI": "10.3389/feduc.2022.1007779",
        "paper_author": "Suzuki H.",
        "affiliation_name": "University of Notre Dame",
        "affiliation_city": "Notre Dame",
        "affiliation_country": "United States",
        "affiliation_id": "60021508",
        "affiliation_state": "IN"
    },
    {
        "paper_title": "DitDetector: Bimodal Learning based on Deceptive Image and Text for Macro Malware Detection",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "10",
        "cover_date": "2022-12-05",
        "Abstract": "Macro malware has always been a severe threat to cyber security although the Microsoft Office suite applies the default macro-disabling policy. Among the defense solutions at different stages of the attack chain, document analysis is more targeted through detecting malicious documents with macro malware. It is effective, especially with machine learning methods, but still faces problems handling malware variants, supporting file formats, and attack countermeasures with advanced attack techniques (e.g., Excel 4.0 macro and remote template injection). In this paper, we find it promising to detect deceptive information embedded in documents which tricks users into enabling macros instead of detecting file metadata or extracted macro codes. Thus, we propose a novel solution for macro malware detection named DitDetector, which leverages bimodal learning based on deceptive images and text. Specifically, we extract preview images of documents based on an image export SDK of Oracle and extract textual information from preview images based on an open-source OCR engine. The bimodal model of DitDetector contains a visual encoder, a textual encoder, and a forward neural network, which learns based on the joint representation of the two encoders' outputs. We evaluate DitDetector on three datasets, including an open-source malicious document dataset (i.e., MalDoc) and two collected real-world adversary datasets (i.e., a database of Excel macros and a database of remote template injection samples). Our experiments show that DitDetector outperforms four existing macro code-based machine learning methods and five reputable Anti-Virus engines. Especially in the real-world test of advanced macro malware, DitDetector gets the F1-score of 99.93% which is at least 3.16% higher than compared solutions.",
        "DOI": "10.1145/3564625.3567982",
        "paper_author": "Yan J.",
        "affiliation_name": "Institute of Software Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60025256",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Risk-aware Fine-grained Access Control in Cyber-physical Contexts",
        "publication": "Digital Threats: Research and Practice",
        "citied_by": "3",
        "cover_date": "2022-12-05",
        "Abstract": "Access to resources by users may need to be granted only upon certain conditions and contexts, perhaps particularly in cyber-physical settings. Unfortunately, creating and modifying context-sensitive access control solutions in dynamic environments creates ongoing challenges to manage the authorization contexts. This article proposes RASA, a context-sensitive access authorization approach and mechanism leveraging unsupervised machine learning to automatically infer risk-based authorization decision boundaries. We explore RASA in a healthcare usage environment, wherein cyber and physical conditions create context-specific risks for protecting private health information. The risk levels are associated with access control decisions recommended by a security policy. A coupling method is introduced to track coexistence of the objects within context using frequency and duration of coexistence, and these are clustered to reveal sets of actions with common risk levels; these are used to create authorization decision boundaries. In addition, we propose a method for assessing the risk level and labelling the clusters with respect to their corresponding risk levels. We evaluate the promise of RASA-generated policies against a heuristic rule-based policy. By employing three different coupling features (frequency-based, duration-based, and combined features), the decisions of the unsupervised method and that of the policy are more than 99% consistent.",
        "DOI": "10.1145/3480468",
        "paper_author": "Liu J.",
        "affiliation_name": "University of Ottawa",
        "affiliation_city": "Ottawa",
        "affiliation_country": "Canada",
        "affiliation_id": "60028897",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "Reinforcement learning for intelligent building energy management system control",
        "publication": "Intelligent Data Mining and Analysis in Power and Energy Systems: Models and Applications for Smarter Efficient Power Systems",
        "citied_by": "0",
        "cover_date": "2022-12-02",
        "Abstract": "A building energy management system (BEMS) is a computer-based system designed to monitor and control a building's energy needs. Modern BEMS rely on the sensing and connectivity capabilities of Internet of Things (IoT) technology to intelligently adjust the energy consumption to reduce cost while respecting the consumers' preferences. Increasingly, control decisions are made based on predictions by models trained using supervised machine learning methods, which still requires control policies to be formulated in a rule-based fashion. When using reinforcement learning (RL) instead, control policies are learned by observing the utility in terms of cost and comfort associated with actions such as a change in the heating system's setpoint. The resulting RL-based controllers can capture not only the dynamics of the building and the associated electrical devices, but also fluctuations in electricity prices and user demand, avoiding the need to combine multiple predictive models with tailored control policies. This chapter will provide an overview of RL-based approaches for BEMS. After sketching the taxonomy of general RL methods, we discuss the implications of relying on the individual methods in a BEMS context. Existing work applying RL is presented along the key devices controlled by BEMS systems. Finally, we summarize the state-of-the-art and sketch limitations and open research directions.",
        "DOI": "10.1002/9781119834052.ch18",
        "paper_author": "Kotevska O.",
        "affiliation_name": "Oak Ridge National Laboratory",
        "affiliation_city": "Oak Ridge",
        "affiliation_country": "United States",
        "affiliation_id": "60024266",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Cocreative interaction: Somax2 and the reach project",
        "publication": "Computer Music Journal",
        "citied_by": "1",
        "cover_date": "2022-12-01",
        "Abstract": "Somax2 is an artificial intelligence (AI)-based multiagent system for human–machine “coimprovisation” that generates stylistically coherent streams while continuously listening and adapting to musicians or other agents. The model on which it is based can be used with little configuration to interact with humans in full autonomy, but it also allows fine real-time control of its generative processes and interaction strategies, closer in this case to a “smart” digital instrument. An offspring of the Omax system, conceived at the Institut de Recherche et Coordination Acoustique/Musique (IRCAM), the Somax2 environment is part of the European Research Council Raising Cocreativity in Cyber–Human Musicianship (REACH) project, which studies distributed creativity as a general template for symbiotic interaction between humans and digital systems. It fosters mixed musical reality involving cocreative AI agents. The REACH project puts forward the idea that cocreativity in cyber–human systems results from the emergence of complex joint behavior, produced by interaction and featuring cross-learning mechanisms. Somax2 is a first step toward this ideal, and already shows life-size achievements. This article describes Somax2 extensively, from its theoretical model to its system architecture, through its listening and learning strategies, representation spaces, and interaction policies.",
        "DOI": "10.1162/comj_a_00662",
        "paper_author": "Assayag G.",
        "affiliation_name": "Sciences et Technologies de la Musique et du Son",
        "affiliation_city": "Paris",
        "affiliation_country": "France",
        "affiliation_id": "60158039",
        "affiliation_state": "Ile-de-France"
    },
    {
        "paper_title": "Using machine learning to predict clean energy stock prices: How important are market volatility and economic policy uncertainty?",
        "publication": "Journal of Climate Finance",
        "citied_by": "22",
        "cover_date": "2022-12-01",
        "Abstract": "The disruptive impacts of climate change have created an urgent need to transition to a low carbon economy and an important part of this transition is an increase in the usage of clean energy. The greater adoption of clean energy is creating new opportunities for clean energy equity investing. The existing literature mostly focuses on the dynamic relationship between clean energy equities, oil prices, technology stock prices, and other important macroeconomic variables like market volatility and economic policy uncertainty. However, there is a shortage of literature on forecasting clean energy stock prices. Forecasting clean energy equity prices is important for making investment decisions. This paper uses machine learning methods to predict the direction of clean energy stock prices. The analysis reveals that random forests, extremely randomized trees, stochastic gradient boosting, and support vector machine have higher prediction accuracy than Lasso or Naïve Bayes. For forecasts in the 10-day to 20-day range, random forests, extremely randomized trees, stochastic gradient boosting, and support vector machine achieve prediction accuracies greater than 85 %. In some cases, prediction accuracy reaches 90%. Lasso prediction accuracy is higher than Naïve Bayes but never greater than 65 %. The MA200, MA50, and WAD technical indicators are, on average, the features most important for predicting clean energy stock price direction. Of the non-technical indicators, VIX and OVX are consistently ranked high in importance. In most cases, EPU is not one of the most important features, Of the forecasting methods considered, extremely randomized trees are very impressive due to high accuracy and short computational time.",
        "DOI": "10.1016/j.jclimf.2022.100002",
        "paper_author": "Sadorsky P.",
        "affiliation_name": "Schulich School of Business",
        "affiliation_city": "Toronto",
        "affiliation_country": "Canada",
        "affiliation_id": "60119111",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "Distance to semi-natural habitats matters for arbuscular mycorrhizal fungi in wheat roots and wheat performance in a temperate agricultural landscape",
        "publication": "Journal of Sustainable Agriculture and Environment",
        "citied_by": "0",
        "cover_date": "2022-12-01",
        "Abstract": "Introduction: The proximity of semi-natural habitats and agricultural fields in an agricultural landscape leads to unavoidable biological, chemical, and physical interactions. Fungi can negatively influence, but also support crop growth in agricultural fields. Therefore, in this field study we investigated the colonisation of arbuscular mycorrhizal (AM) fungi and non-AM fungi in winter-wheat roots as well as winter-wheat performance in distance to semi-natural habitats. Materials and Methods: We sampled in an intensively managed agricultural landscape in North-east Germany along agricultural transition zones, that is, along 50 m-transects from semi-natural habitats like hedgerows and glacially created in-field ponds—so-called kettle holes—into agricultural fields. Results: To our knowledge, we show for the first time that AM fungal colonisation in winter-wheat roots decreased linearly with increasing distance to semi-natural habitats while non-AM fungal root colonisation did not change. Winter-wheat grain yield and biomass slightly increased with increasing distance to hedgerows but not to kettle holes. This clearly shows that there is a difference between different crop performance parameters. Random forest machine learning algorithms confirmed the particular importance of distance to semi-natural habitats for AM fungal root colonisation and for winter-wheat grain yield. Less intensive agricultural management close to semi-natural habitats, for example, no herbicide and pesticide applications as a result of nature protection regulations, may partly explain this pattern. However, spatial response patterns of AM but not of non-AM fungi in wheat roots also point to changed ecological interactions close to semi-natural habitats. Conclusion: Semi-natural and natural habitats in agricultural landscapes are slowly recognised not only to be important for biodiversity conservation, but also for sustainable crop production. Additionally, they may also be a tool for farmers and policy makers to improve sustainable landscape management. And agricultural transition zones are spatially and temporally complex dynamic ecosystems that should be the focus of further investigations.",
        "DOI": "10.1002/sae2.12032",
        "paper_author": "Pirhofer Walzl K.",
        "affiliation_name": "Freie Universität Berlin",
        "affiliation_city": "Berlin",
        "affiliation_country": "Germany",
        "affiliation_id": "60030718",
        "affiliation_state": "Berlin"
    },
    {
        "paper_title": "A computational approach to analyzing climate strategies of cities pledging net zero",
        "publication": "npj Urban Sustainability",
        "citied_by": "14",
        "cover_date": "2022-12-01",
        "Abstract": "Cities have become primary actors on climate change and are increasingly setting goals aimed at net-zero emissions, which warrants closer examination to understand how they intend to meet these goals. The incomplete and heterogeneous nature of city climate policy documents, however, has made systemic analysis challenging. We analyze 318 climate action documents from cities with net-zero targets using machine learning-based natural language processing (NLP) techniques. We aim to accomplish two goals: (1) determine text patterns that predict ‘ambitious’ net-zero targets; and (2) perform a sectoral analysis to identify patterns and trade-offs in climate action themes. We find that cities with ambitious climate actions tend to emphasize quantitative metrics and specific high-emitting sectors in their plans. Cities predominantly emphasize energy-related actions in their plans, but often at the expense of other sectors, including land-use and climate impacts. The method presented in this paper provides a replicable, scalable approach to analyzing climate action plans and a first step towards facilitating cross-city learning.",
        "DOI": "10.1038/s42949-022-00065-x",
        "paper_author": "Sachdeva S.",
        "affiliation_name": "The University of North Carolina at Chapel Hill",
        "affiliation_city": "Chapel Hill",
        "affiliation_country": "United States",
        "affiliation_id": "60025111",
        "affiliation_state": "NC"
    },
    {
        "paper_title": "Construction Direction and Development Path of Digital Educational Resources in the Era of New Educational Infrastructure Construction",
        "publication": "Frontiers of Education in China",
        "citied_by": "1",
        "cover_date": "2022-12-01",
        "Abstract": "Digital educational resources have become essential elements of modern educational public services. China’s national program on new educational infrastructure construction has proposed to take new infrastructure construction of digital educational resources as one of the critical tasks. This paper aims to provide a theoretical basis for advancing the reform and development of digital educational resources in the new era. It classifies the current development and realistic challenges of China’s digital educational resources, and summaries and interprets the directions of the three construction projects of new type resources and tools, the resource supply system, and the resource supervisory system. It depicts the prospect of digital educational resource construction in the intelligent era in the combination of the strategic layout of the policies on new educational infrastructure construction. It also discusses such key technologies supporting new infrastructure construction of digital educational resources as multimodal learning analytics, disciplinary knowledge graphs, machine learning, and blockchain. It finally proposes a specific development path for new infrastructure construction of digital educational resources.",
        "DOI": "10.3868/s110-007-022-0022-1",
        "paper_author": "Ke Q.",
        "affiliation_name": "South China Normal University",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60005816",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Online medical privacy protection strategy under information value-added mechanism",
        "publication": "Chinese Journal of Network and Information Security",
        "citied_by": "0",
        "cover_date": "2022-12-01",
        "Abstract": "China’s economic level and people’s living standards have developed rapidly in recent years, and the medical level and medical technology have made breakthroughs continuously. With the promotion and deepening of “Internet Plus” to business model innovation in various fields, the development of “Internet Plus” medical has been rapidly developed. Due to the continuous development of data processing technologies such as machine learning and data mining, the risk of users’ personal medical data disclosure in the process of online medical treatment has also attracted the attention of researchers. Considering the deductibility of information, the discount mechanism was adopted to describe the change of user’s private information value in different stages of the game. Combined with the current research status in the field of online medical privacy protection motivation, how to mobilize the enthusiasm of both players from the level of privacy protection motivation was explored with game analysis. In view of the game characteristics of users’ strong willingness to continually use the online medical platform and intermittently provide privacy, the repeated game method was adopted to better describe the game process between users and the online medical platform. The tendency change law of the players on both sides of the game was obtained. Moreover, the Nash equilibrium of the game model was analyzed under different model parameters and the change trend of the game strategy of both sides with the progress of the game stage. When the parameters were met 2(cp−cn)≥lp(pn−pp), the user started to choose from “agree to share private data” to “refuse to share private data”. The above conclusion was verified by simulation experiments. Based on the above conclusions, from the perspective of online medical platform and users, policy suggestions on how to realize privacy protection from the level of privacy protection motivation in the process of online medical treatment were given.",
        "DOI": "10.11959/j.issn.2096-109x.2022072",
        "paper_author": "Ming S.",
        "affiliation_name": "Central University of Finance and Economics",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60013131",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "The Importance of Price, Income, and Affordability in the Demand for Cigarettes in Spain",
        "publication": "Addicta: the Turkish Journal on Addictions",
        "citied_by": "1",
        "cover_date": "2022-12-01",
        "Abstract": "In the literature it is commonly accepted that the best mechanism to control smoking is by increasing tobacco prices via taxes. However, there are some studies that indicate that the decrease in tobacco consumption when prices rise is because consumer income is not capable of counteracting said rise. The empirical analysis was developed using a panel of data from the Spanish provinces covering 2002 to 2018. By using Machine Learning assembly models, the importance of price, GDP and affordability as a mechanism for controlling the demand for cigarettes is estimated. The importance of affordability to control tobacco consumption in Spain has grown over time. Furthermore, until 2010, income has generally better explained the demand for cigarettes in the Spanish provinces. However, as of 2010, price is the explanatory variable of the demand function that best explains the behavior of the demand for cigarettes. In these circumstances, the separate estimates of price and income elasticity that have been carried out in Spain so far must be interpreted considering that as of 2010, price is more important than income in explaining the demand for cigarettes. Furthermore, when the relative income price reaches 1%, there seems to be a positive relationship between this variable and the importance of price in explaining the demand for tobacco. Although the demand functions estimated so far are useful to make predictions about the behavior of cigarette demand, the government must consider that price is a good tool to control tobacco consumption from a certain point of affordability. Specifically, it appears that when the relative income price exceeds 1%, the importance of price as a tool to control legal cigarette sales grows each time the relative income price rises.",
        "DOI": "10.5152/ADDICTA.2022.22054",
        "paper_author": "Cadahia P.",
        "affiliation_name": "Universidad de Huelva",
        "affiliation_city": "Huelva",
        "affiliation_country": "Spain",
        "affiliation_id": "60014204",
        "affiliation_state": "Huelva"
    },
    {
        "paper_title": "BEYOND WINDOW DRESSING: PUBLIC PARTICIPATION FOR MARGINALIZED COMMUNITIES IN THE DATAFIED SOCIETY",
        "publication": "Fordham Law Review",
        "citied_by": "5",
        "cover_date": "2022-12-01",
        "Abstract": "We live in a datafied society in which our personal data is being constantly harvested, analyzed, and sold by public and private entities, and yet we have little control over our data and little voice in how it is used. In light of the impacts of algorithmic decision-making systems—including those that run on machine learning and artificial intelligence—there are increasing calls to integrate public participation into the adoption, design, and oversight of these tech tools. Stakeholder input is particularly crucial for members of marginalized groups, who bear the disproportionate harms of data-centric technologies. Yet, recent calls for public participation have been mostly hortatory and without specific strategies or realistic recommendations. As this Article explains, policy makers need not operate from a blank slate. For decades, a variety of American statutory regimes have mandated public participation, such as in the areas of environmental law, land use law, and anti-poverty programs. Such mandates have had outsized effects on communities suffering from economic disadvantage and racial and ethnic discrimination. This Article contends that we should examine these regulatory mandates in thinking about how to include the perspectives of marginalized stakeholders in the datafied society. The core takeaway is that meaningful public participation is extremely challenging and does not happen without intentional and inclusive design. At its best, public input can improve outputs and empower stakeholders. At its worst, it operates as a form of “window dressing,” in which marginalized communities have no real power to effect outcomes, thus generating distrust and alienation. Case studies show that meaningful public participation is most likely to result when there are hard-law requirements for public participation and when decision-makers operate transparently and recognize the value of the public’s expertise. In addition, impacted communities must be provided with capacity-building tools and resources to support their engagement. As legislative proposals to enhance tech accountability—through algorithmic impact assessments, audits, and other tools—gain steam, we must heed these lessons.",
        "DOI": "NA",
        "paper_author": "Gilman M.E.",
        "affiliation_name": "University of Baltimore",
        "affiliation_city": "Baltimore",
        "affiliation_country": "United States",
        "affiliation_id": "60028611",
        "affiliation_state": "MD"
    },
    {
        "paper_title": "The Importance of Forecasting in Industrial Enterprise Management Using Machine Learning",
        "publication": "Scientific and Technical Information Processing",
        "citied_by": "1",
        "cover_date": "2022-12-01",
        "Abstract": "Abstract: In this work, the assessment of the importance of forecasting in making a management decision during an operational control of an enterprise without including individual cases of dominating impact of forecasted values, including the investing policies and strategic enterprise management, was considered. As an alternative to the expert method, a forecast importance assessment method based on gradient-boosted machine learning algorithm analysis of a decision informational field with the subsequent interpretation of acquired results with Shapley values, allowing for a numerical representation of importance. This method is suggested for use as a tool to increase efficiency of intellectual decision-making support systems by the means of including it in the procedure of automated analysis of importance of features. This work is interdisciplinary, and it concerns problems of systems analysis, economics, and psychology.",
        "DOI": "10.3103/S0147688222050173",
        "paper_author": "Vorobev A.V.",
        "affiliation_name": "Kursk State University",
        "affiliation_city": "Kursk",
        "affiliation_country": "Russian Federation",
        "affiliation_id": "60109910",
        "affiliation_state": "Kursk Oblast"
    },
    {
        "paper_title": "Application of a predictive Q-learning algorithm on the multiple-effect evaporator in a sugarcane ethanol biorefinery",
        "publication": "Digital Chemical Engineering",
        "citied_by": "10",
        "cover_date": "2022-12-01",
        "Abstract": "With the recent development of machine learning, reinforcement learning is an interesting alternative to PID controllers. In this context, a discrete predictive Q-learning approach is applied in the control of a sugarcane biorefinery multiple-effect evaporation system. The algorithm is built using Scilab and learns to control the multiple-effect evaporator outlet concentration by manipulating its feed steam flow rate. Based on multiple episodes, the state-actions that consist of discrete changes in steam flow rate are chosen with a greedy algorithm. In order to increase the training efficiency and overcome the large dead time of the system, a neural network is applied to predict the outlet concentration of each control action after reaching the steady-state. The control policy was built and tested through simulations on a phenomenological model. The controller performance was evaluated in set-point tracking and disturbance rejection tests and compared with PID responses. The research showed that the Q-learning controller exhibited better performance than the PID controller.",
        "DOI": "10.1016/j.dche.2022.100049",
        "paper_author": "Emori E.Y.",
        "affiliation_name": "Universidade Estadual de Maringá",
        "affiliation_city": "Maringa",
        "affiliation_country": "Brazil",
        "affiliation_id": "60029498",
        "affiliation_state": "PR"
    },
    {
        "paper_title": "A Machine Learning-Based Energy Management Agent for Fine Dust Concentration Control in Railway Stations",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "4",
        "cover_date": "2022-12-01",
        "Abstract": "This study developed a reinforcement learning-based energy management agent that controls the fine dust concentration by controlling facilities such as blowers and air conditioners to efficiently manage the fine dust concentration in the station. To this end, we formulated an optimization problem based on the Markov decision-making process and developed a model for predicting the concentration of fine dust in the station by training an artificial neural network (ANN) based on supervised learning to develop the transfer function. In addition to the prediction model, the optimal policy for controlling the blower and air conditioner according to the current state was obtained based on the ANN to which the Deep Q-Network (DQN) algorithm was applied. In the case study, it is confirmed that the ANN and DQN of the predictive model were trained based on the actual data of Nam-Gwangju Station to converge to the optimal policy. The comparison between the proposed method and conventional method shows that the proposed method can use less power consumption but achieved better performance on reducing fine dust concentration than the conventional method. In addition, by increasing the value of the ratio that represents the compensation due to the fine dust reduction, the learned agent achieved more reduction on the fine dust concentration by increasing the power consumption of the blower and air conditioner.",
        "DOI": "10.3390/su142315550",
        "paper_author": "Kwon K.B.",
        "affiliation_name": "Cockrell School of Engineering",
        "affiliation_city": "Austin",
        "affiliation_country": "United States",
        "affiliation_id": "60150401",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "Landslide Susceptibility Mapping Using Deep Neural Network and Convolutional Neural Network",
        "publication": "Korean Journal of Remote Sensing",
        "citied_by": "4",
        "cover_date": "2022-12-01",
        "Abstract": "Landslides are one of the most prevalent natural disasters, threating both humans and property. Also landslides can cause damage at the national level, so effective prediction and prevention are essential. Research to produce a landslide susceptibility map with high accuracy is steadily being conducted, and various models have been applied to landslide susceptibility analysis. Pixel-based machine learning models such as frequency ratio models, logistic regression models, ensembles models, and Artificial Neural Networks have been mainly applied. Recent studies have shown that the kernel-based convolutional neural network (CNN) technique is effective and that the spatial characteristics of input data have a significant effect on the accuracy of landslide susceptibility mapping. For this reason, the purpose of this study is to analyze landslide vulnerability using a pixel-based deep neural network model and a patch-based convolutional neural network model. The research area was set up in Gangwon-do, including Inje, Gangneung, and Pyeongchang, where landslides occurred frequently and damaged. Landslide-related factors include slope, curvature, stream power index (SPI), topographic wetness index (TWI), topographic position index (TPI), timber diameter, timber age, lithology, land use, soil depth, soil parent material, lineament density, fault density, normalized difference vegetation index (NDVI) and normalized difference water index (NDWI) were used. Landslide-related factors were built into a spatial database through data preprocessing, and landslide susceptibility map was predicted using deep neural network (DNN) and CNN models. The model and landslide susceptibility map were verified through average precision (AP) and root mean square errors (RMSE), and as a result of the verification, the patch-based CNN model showed 3.4% improved performance compared to the pixel-based DNN model. The results of this study can be used to predict landslides and are expected to serve as a scientific basis for establishing land use policies and landslide management policies.",
        "DOI": "10.7780/kjrs.2022.38.6.2.12",
        "paper_author": "Gong S.H.",
        "affiliation_name": "University of Seoul",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea",
        "affiliation_id": "60027652",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "PV self-consumption prediction methods using supervised machine learning",
        "publication": "E3S Web of Conferences",
        "citied_by": "1",
        "cover_date": "2022-12-01",
        "Abstract": "The increased prevalence of photovoltaic (PV) self-consumption policies across Europe and the world place an increased importance on accurate predictions for life-cycle costing during the planning phase. This study presents several machine learning and regression models for predicting self-consumption, trained on a variety of datasets from Sweden. The results show that advanced ML models have an improved performance over simpler regressions, where the highest performing model, Random Forest, has a mean average error of 1.5 percentage points and an R2 of 0.977. Training models using widely available typical meteorological year (TMY) climate data is also shown to introduce small, acceptable errors when tested against spatially and temporally matched climate and load data. The ability to train the ML models with TMY climate data makes their adoption easier and builds on previous work by demonstrating the robustness of the methodology as a self-consumption prediction tool. The low error and high R2 are a notable improvement over previous estimation models and the minimal input data requirements make them easy to adopt and apply in a wide array of applications.",
        "DOI": "10.1051/e3sconf/202236202003",
        "paper_author": "Tóth M.",
        "affiliation_name": "The Royal Institute of Technology (KTH)",
        "affiliation_city": "Stockholm",
        "affiliation_country": "Sweden",
        "affiliation_id": "60002014",
        "affiliation_state": "Stockholms"
    },
    {
        "paper_title": "The Impact of Digital Technologies on Neoclassical Labour Market",
        "publication": "Danube",
        "citied_by": "1",
        "cover_date": "2022-12-01",
        "Abstract": "Labour market economics works on the principle of a neoclassical approach. Competitive forces, corporate policy, collective bargaining, and state intervention create the reality of the market. Technological progress in advanced technology, robotics, and artificial intelligence is strengthening the economies of several countries. The article deals with the actual influence of digital technologies on the neoclassical paradigm of the labour market and the impacts on the tasks and requirements for the competencies of workers and the level of their qualifications. Research is based on secondary data from relevant sources with emphasis on the quality using the interface Primo database, enabling the search for data in full-fledged databases (Web of Science, ProQuest, etc.). The real minimum wage, employment rate and jobs at risk of automation were analysed to compare the situation in European countries. The main objective is to analyse the magnitude of the impact of digital technologies on the labour market. The results point to technological progress, especially automation, which eliminates job positions by substituting low-skilled workers with machine learning robots. Government can ensure more balanced inclusive growth in the area of education. The change in the concept of teaching leads to higher qualifications and skills for future employees.",
        "DOI": "10.2478/danb-2022-0020",
        "paper_author": "Petrová K.",
        "affiliation_name": "Brno University of Technology",
        "affiliation_city": "Brno",
        "affiliation_country": "Czech Republic",
        "affiliation_id": "60013826",
        "affiliation_state": "South Moravian Region"
    },
    {
        "paper_title": "Manufacturing Resource Scheduling Based on Deep Q-Network",
        "publication": "Wuhan University Journal of Natural Sciences",
        "citied_by": "2",
        "cover_date": "2022-12-01",
        "Abstract": "To optimize machine allocation and task dispatching in smart manufacturing factories, this paper proposes a manufacturing resource scheduling framework based on reinforcement learning (RL). The framework formulates the entire scheduling process as a multi-stage sequential decision problem, and further obtains the scheduling order by the combination of deep convolutional neural network (CNN) and improved deep Q-network (DQN). Specifically, with respect to the representation of the Markov decision process (MDP), the feature matrix is considered as the state space and a set of heuristic dispatching rules are denoted as the action space. In addition, the deep CNN is employed to approximate the state-action values, and the double dueling deep Q-network with prioritized experience replay and noisy network (D3QPN2) is adopted to determine the appropriate action according to the current state. In the experiments, compared with the traditional heuristic method, the proposed method is able to learn high-quality scheduling policy and achieve shorter makespan on the standard public datasets.",
        "DOI": "10.1051/wujns/2022276531",
        "paper_author": "Zhang Y.",
        "affiliation_name": "Tongji University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60073652",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Passive and Privacy-Preserving Human Localization: A Social Distancing Approach Using Commercial Millimeter-Wave Access Points",
        "publication": "IEEE Vehicular Technology Magazine",
        "citied_by": "2",
        "cover_date": "2022-12-01",
        "Abstract": "The pandemic outbreak has profoundly changed our life, especially our social habits and communication behaviors. While this dramatic shock has heavily impacted human interaction rules, novel localization techniques are emerging to help society in complying with new policies, such as social distancing. Wireless sensing and machine learning are well suited to alleviate virus propagation in a privacy-preserving manner. However, their wide deployment requires cost-effective installation and operational solutions.",
        "DOI": "10.1109/MVT.2022.3202025",
        "paper_author": "Devoti F.",
        "affiliation_name": "NEC Laboratories Europe GmbH",
        "affiliation_city": "Heidelberg",
        "affiliation_country": "Germany",
        "affiliation_id": "60078342",
        "affiliation_state": "Baden-Wurttemberg"
    },
    {
        "paper_title": "Energy saving strategy of cloud data computing based on convolutional neural network and policy gradient algorithm",
        "publication": "PLoS ONE",
        "citied_by": "3",
        "cover_date": "2022-12-01",
        "Abstract": "Cloud Data Computing (CDC) is conducive to precise energy-saving management of user data centers based on the real-time energy consumption monitoring of Information Technology equipment. This work aims to obtain the most suitable energy-saving strategies to achieve safe, intelligent, and visualized energy management. First, the theory of Convolutional Neural Network (CNN) is discussed. Besides, an intelligent energy-saving model based on CNN is designed to ameliorate the variable energy consumption, load, and power consumption of the CDC data center. Then, the core idea of the policy gradient (PG) algorithm is introduced. In addition, a CDC task scheduling model is designed based on the PG algorithm, aiming at the uncertainty and volatility of the CDC scheduling tasks. Finally, the performance of different neural network models in the training process is analyzed from the perspective of total energy consumption and load optimization of the CDC center. At the same time, simulation is performed on the CDC task scheduling model based on the PG algorithm to analyze the task scheduling demand. The results demonstrate that the energy consumption of the CNN algorithm in the CDC energy-saving model is better than that of the Elman algorithm and the ecoCloud algorithm. Besides, the CNN algorithm reduces the number of virtual machine migrations in the CDC energy-saving model by 9.30% compared with the Elman algorithm. The Deep Deterministic Policy Gradient (DDPG) algorithm performs the best in task scheduling of the cloud data center, and the average response time of the DDPG algorithm is 141. In contrast, the Deep Q Network algorithm performs poorly. This paper proves that Deep Reinforcement Learning (DRL) and neural networks can reduce the energy consumption of CDC and improve the completion time of CDC tasks, offering a research reference for CDC resource scheduling.",
        "DOI": "10.1371/journal.pone.0279649",
        "paper_author": "Yang D.",
        "affiliation_name": "Xinjiang University",
        "affiliation_city": "Urumqi",
        "affiliation_country": "China",
        "affiliation_id": "60015780",
        "affiliation_state": "Xinjiang"
    },
    {
        "paper_title": "Data Exploration and Classification of News Article Reliability: Deep Learning Study",
        "publication": "JMIR Infodemiology",
        "citied_by": "2",
        "cover_date": "2022-12-01",
        "Abstract": "Background: During the ongoing COVID-19 pandemic, we are being exposed to large amounts of information each day. This \"infodemic\"is defined by the World Health Organization as the mass spread of misleading or false information during a pandemic. This spread of misinformation during the infodemic ultimately leads to misunderstandings of public health orders or direct opposition against public policies. Although there have been efforts to combat misinformation spread, current manual fact-checking methods are insufficient to combat the infodemic. Objective: We propose the use of natural language processing (NLP) and machine learning (ML) techniques to build a model that can be used to identify unreliable news articles online. Methods: First, we preprocessed the ReCOVery data set to obtain 2029 English news articles tagged with COVID-19 keywords from January to May 2020, which are labeled as reliable or unreliable. Data exploration was conducted to determine major differences between reliable and unreliable articles. We built an ensemble deep learning model using the body text, as well as features, such as sentiment, Empath-derived lexical categories, and readability, to classify the reliability. Results: We found that reliable news articles have a higher proportion of neutral sentiment, while unreliable articles have a higher proportion of negative sentiment. Additionally, our analysis demonstrated that reliable articles are easier to read than unreliable articles, in addition to having different lexical categories and keywords. Our new model was evaluated to achieve the following performance metrics: 0.906 area under the curve (AUC), 0.835 specificity, and 0.945 sensitivity. These values are above the baseline performance of the original ReCOVery model. Conclusions: This paper identified novel differences between reliable and unreliable news articles; moreover, the model was trained using state-of-the-art deep learning techniques. We aim to be able to use our findings to help researchers and the public audience more easily identify false information and unreliable media in their everyday lives.",
        "DOI": "10.2196/38839",
        "paper_author": "Zhan K.",
        "affiliation_name": "University of Alberta",
        "affiliation_city": "Edmonton",
        "affiliation_country": "Canada",
        "affiliation_id": "60030835",
        "affiliation_state": "AB"
    },
    {
        "paper_title": "Behavioral Repertoire via Generative Adversarial Policy Networks",
        "publication": "IEEE Transactions on Cognitive and Developmental Systems",
        "citied_by": "3",
        "cover_date": "2022-12-01",
        "Abstract": "Learning algorithms are enabling robots to solve increasingly challenging real-world tasks. These approaches often rely on demonstrations and reproduce the behavior shown. Unexpected changes in the environment or in robot morphology may require using different behaviors to achieve the same effect, for instance, to reach and grasp an object in changing clutter. An emerging paradigm addressing this robustness issue is to learn a diverse set of successful behaviors for a given task, from which a robot can select the most suitable policy when faced with a new environment. In this article, we explore a novel realization of this vision by learning a generative model over policies. Rather than learning a single policy, or a small fixed repertoire, our generative model for policies compactly encodes an unbounded number of policies and allows novel controller variants to be sampled. Leveraging our generative policy network, a robot can sample novel behaviors until it finds one that works for a new scenario. We demonstrate this idea with an application of robust ball throwing in the presence of obstacles, as well as joint-damage-robust throwing. We show that this approach achieves a greater diversity of behaviors than an existing evolutionary approach, while maintaining good efficacy of sampled behaviors, allowing a Baxter robot to hit targets more often when ball throwing in the presence of varying obstacles or joint impediments.",
        "DOI": "10.1109/TCDS.2020.3008574",
        "paper_author": "Jegorova M.",
        "affiliation_name": "The University of Edinburgh",
        "affiliation_city": "Edinburgh",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60027272",
        "affiliation_state": "Scotland"
    },
    {
        "paper_title": "Inferring turbulent environments via machine learning",
        "publication": "European Physical Journal E",
        "citied_by": "6",
        "cover_date": "2022-12-01",
        "Abstract": "Abstract: The problem of classifying turbulent environments from partial observation is key for some theoretical and applied fields, from engineering to earth observation and astrophysics, e.g., to precondition searching of optimal control policies in different turbulent backgrounds, to predict the probability of rare events and/or to infer physical parameters labeling different turbulent setups. To achieve such goal one can use different tools depending on the system’s knowledge and on the quality and quantity of the accessible data. In this context, we assume to work in a model-free setup completely blind to all dynamical laws, but with a large quantity of (good quality) data for training. As a prototype of complex flows with different attractors, and different multi-scale statistical properties we selected 10 turbulent ‘ensembles’ by changing the rotation frequency of the frame of reference of the 3d domain and we suppose to have access to a set of partial observations limited to the instantaneous kinetic energy distribution in a 2d plane, as it is often the case in geophysics and astrophysics. We compare results obtained by a machine learning (ML) approach consisting of a state-of-the-art deep convolutional neural network (DCNN) against Bayesian inference which exploits the information on velocity and entropy moments. First, we discuss the supremacy of the ML approach, presenting also results at changing the number of training data and of the hyper-parameters. Second, we present an ablation study on the input data aimed to perform a ranking on the importance of the flow features used by the DCNN, helping to identify the main physical contents used by the classifier. Finally, we discuss the main limitations of such data-driven methods and potential interesting applications. Graphical Abstract: [Figure not available: see fulltext.].",
        "DOI": "10.1140/epje/s10189-022-00258-3",
        "paper_author": "Buzzicotti M.",
        "affiliation_name": "Università degli Studi di Roma \"Tor Vergata\"",
        "affiliation_city": "Rome",
        "affiliation_country": "Italy",
        "affiliation_id": "60027509",
        "affiliation_state": "RM"
    },
    {
        "paper_title": "Achieving Brazil's Deforestation Target Will Reduce Fire and Deliver Air Quality and Public Health Benefits",
        "publication": "Earth's Future",
        "citied_by": "1",
        "cover_date": "2022-12-01",
        "Abstract": "Climate, deforestation, and forest fires are closely coupled in the Amazon, but models of fire that include these interactions are lacking. We trained machine learning models on temperature, rainfall, deforestation, land-use, and fire data to show that spatial and temporal patterns of fire in the Amazon are strongly modified by deforestation. We find that fire count across the Brazilian Amazon increases by 0.44 percentage points for each percentage point increase in deforestation rate. We used the model to predict that the increased deforestation rate in the Brazilian Amazon from 2013 to 2020 caused a 42% increase in fire counts in 2020. We predict that if Brazil had achieved the deforestation target under the National Policy on Climate Change, there would have been 32% fewer fire counts across the Brazilian Amazon in 2020. Using a regional chemistry-climate model and exposure-response associations, we estimate that the improved air quality due to reduced smoke emission under this scenario would have resulted in 2,300 fewer deaths due to reduced exposure to fine particulate matter. Our analysis demonstrates the air quality and public health benefits that would accrue from reducing deforestation in the Brazilian Amazon.",
        "DOI": "10.1029/2022EF003048",
        "paper_author": "Butt E.W.",
        "affiliation_name": "University of Leeds",
        "affiliation_city": "Leeds",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60012070",
        "affiliation_state": "West Yorkshire"
    },
    {
        "paper_title": "Analysis of Spatial Concentration of Accommodation Establishments Using Machine Learning Techniques and Spatial Analysis Tools",
        "publication": "Journal of Environmental Management and Tourism",
        "citied_by": "3",
        "cover_date": "2022-12-01",
        "Abstract": "Tourism as an economic activity is influenced by social dynamics that can be measured by spatial analysis processes where phenomena such as concentration, trajectory and extension are evidenced, which can guide policy makers effectively for decision-making. In this document, the results of a study of spatial analysis of the concentration of accommodation establishments in the department of Boyacá, Colombia are presented in order to identify the correlation that exists between the territorial concentration, the natural attractions of the department and the type of establishment. It was found that there is evidence of a strong concentration in five clusters associated with three municipalities of the department's offer, however it is observed that statistically there is a high level of dispersion of the establishments explained by the road corridors and a growing development towards the natural attractions which are not fully covered by the current offer.",
        "DOI": "10.14505/jemt.v13.8(64).18",
        "paper_author": "Parra H.",
        "affiliation_name": "Universidad Santo Tomás",
        "affiliation_city": "Bogota",
        "affiliation_country": "Colombia",
        "affiliation_id": "60104910",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Data Mining and Visualisation of Basic Educational Resources for Quality Education",
        "publication": "International Journal of Engineering Trends and Technology",
        "citied_by": "11",
        "cover_date": "2022-12-01",
        "Abstract": "With an increase in educational resources for the growing population, data for Basic Education (BE) is becoming larger, requiring technical data tools to analyze and interpret. This research uses classification and clustering techniques to analyze the data from public schools in Ghana to identify the challenges. Nine (9) data mining algorithms in rapid miner studio 9.10 were used for the analysis to know the most efficient algorithm suitable for the data. These are; Generalized Linear Module (GLM), Naïve Bayes (NB), Logistic Regression (LR), Deep Learning (DL), Decision Tree (DT), Fast Large Margins (FLM), Gradient Boosted Tree (GBT), Random Forest (RF), and Support Vector Machines (SVM). The performance of GBT was seen as more appropriate, and this algorithm's results were presented. Excerpts from the reports are also included in the form of qualitative data. A diagrammatic representation of the interoperability among levels of education for quality education has also been presented. A proposed Neural Network model has been designed for the challenges and solutions. The conclusions draw that addressing the challenges of BE requires educational policy stability and enforcement to maximize resources and minimize the challenges in schools at all levels of education.",
        "DOI": "10.14445/22315381/IJETT-V70I12P228",
        "paper_author": "Inusah F.",
        "affiliation_name": "Kwame Nkrumah University of Science &amp; Technology",
        "affiliation_city": "Kumasi",
        "affiliation_country": "Ghana",
        "affiliation_id": "60071885",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Statistics and Machine Learning in Aviation Environmental Impact Analysis: A Survey of Recent Progress",
        "publication": "Aerospace",
        "citied_by": "15",
        "cover_date": "2022-12-01",
        "Abstract": "The rapid growth of global aviation operations has made its negative environmental impact an international concern. Accurate modeling of aircraft fuel burn, emissions, and noise is the prerequisite for informing new operational procedures, technologies, and policies towards a more sustainable future of aviation. In the past decade, due to the advances in big data technologies and effective algorithms, the transformative data-driven analysis has begun to play a substantial role in aviation environmental impact analysis. The integration of statistical and machine learning methods in the workflow has made such analysis more efficient and accurate. Through summarizing and classifying the representative works in this intersection area, this survey paper aims to extract prevailing research trends and suggest research opportunities for the future. The methodology overview section presents a comprehensive development process and landscape of statistical and machine learning methods for applied researchers. In the main section, relevant works in the literature are organized into seven application themes: data reduction, efficient computation, predictive modeling, uncertainty quantification, pattern discovery, verification and validation, and infrastructure and tools. Each theme contains background information, in-depth discussion, and a summary of representative works. The paper concludes with the proposal of five future opportunities for this research area.",
        "DOI": "10.3390/aerospace9120750",
        "paper_author": "Gao Z.",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Atlanta",
        "affiliation_country": "United States",
        "affiliation_id": "60136858",
        "affiliation_state": "GA"
    },
    {
        "paper_title": "Comparison of Machine Learning-Based Prediction of Qualitative and Quantitative Digital Soil-Mapping Approaches for Eastern Districts of Tamil Nadu, India",
        "publication": "Land",
        "citied_by": "12",
        "cover_date": "2022-12-01",
        "Abstract": "The soil–environmental relationship identified and standardised over the years has expedited the growth of digital soil-mapping techniques; hence, various machine learning algorithms are involved in predicting soil attributes. Therefore, comparing the different machine learning algorithms is essential to provide insights into the performance of the different algorithms in predicting soil information for Indian landscapes. In this study, we compared a suite of six machine learning algorithms to predict quantitative (Cubist, decision tree, k-NN, multiple linear regression, random forest, support vector regression) and qualitative (C5.0, k-NN, multinomial logistic regression, naïve Bayes, random forest, support vector machine) soil information separately at a regional level. The soil information, including the quantitative (pH, OC, and CEC) and qualitative (order, suborder, and great group) attributes, were extracted from the legacy soil maps using stratified random sampling procedures. A total of 4479 soil observations sampled were non-spatially partitioned and intersected with 39 environmental covariate parameters. The predicted maps depicted the complex soil–environmental relationships for the study area at a 30 m spatial resolution. The comparison was facilitated based on the evaluation metrics derived from the test datasets and visual interpretations of the predicted maps. Permutation feature importance analysis was utilised as the model-agnostic interpretation tool to determine the contribution of the covariate parameters to the model’s calibration. The R2 values for the pH, OC, and CEC ranged from 0.19 to 0.38; 0.04 to 0.13; and 0.14 to 0.40, whereas the RMSE values ranged from 0.75 to 0.86; 0.25 to 0.26; and 8.84 to 10.49, respectively. Irrespective of the algorithms, the overall accuracy percentages for the soil order, suborder, and great group class ranged from 31 to 67; 26 to 65; and 27 to 65, respectively. The tree-based ensemble random forest and rule-based tree models’ (Cubist and C5.0) algorithms efficiently predicted the soil properties spatially. However, the efficiency of the other models can be substantially increased by advocating additional parameterisation measures. The range and scale of the quantitative soil attributes, in addition to the sampling frequency and design, greatly influenced the model’s output. The comprehensive comparison of the algorithms can be utilised to support model selection and mapping at a varied scale. The derived digital soil maps will help farmers and policy makers to adopt precision information for making decisions at the farm level leading to productivity enhancements through the optimal use of nutrients and the sustainability of the agricultural ecosystem, ensuring food security.",
        "DOI": "10.3390/land11122279",
        "paper_author": "Kumaraperumal R.",
        "affiliation_name": "Tamil Nadu Agricultural University",
        "affiliation_city": "Coimbatore",
        "affiliation_country": "India",
        "affiliation_id": "60025123",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "A New Framework for Winter Wheat Yield Prediction Integrating Deep Learning and Bayesian Optimization",
        "publication": "Agronomy",
        "citied_by": "24",
        "cover_date": "2022-12-01",
        "Abstract": "Early prediction of winter wheat yield at the regional scale is essential for food policy making and food security, especially in the context of population growth and climate change. Agricultural big data and artificial intelligence (AI) are key technologies for smart agriculture, bringing cost-effective solutions to the agricultural sector. Deep learning-based crop yield forecast has currently emerged as one of the key methods for guiding agricultural production. In this study, we proposed a Bayesian optimization-based long- and short-term memory model (BO-LSTM) to construct a multi-source data fusion-driven crop growth feature extraction algorithm for winter wheat yield prediction. The yield prediction performance of BO-LSTM, support vector machine (SVM), and least absolute shrinkage and selection operator (Lasso) was then compared with multi-source data as input variables. The results showed that effective deep learning hyperparameter optimization is made possible by Bayesian optimization. The BO-LSTM (RMSE = 177.84 kg/ha, R2 = 0.82) model had the highest accuracy of yield prediction with the input combination of “GPP + Climate + LAI + VIs”. BO-LSTM and SVM (RMSE = 185.7 kg/ha, R2 = 0.80) methods outperformed linear regression Lasso (RMSE = 214.5 kg/ha, R2 = 0.76) for winter wheat yield estimation. There were also differences between machine learning and deep learning, BO-LSTM outperformed SVM. indicating that the BO-LSTM model was more effective at capturing data correlations. In order to further verify the robustness of the BO-LSTM method, we explored the performance estimation performance of BO-LSTM in different regions. The results demonstrated that the BO-LSTM model could obtain higher estimation accuracy in regions with concentrated distribution of winter wheat cultivation and less influence of human factors. The approach used in this study can be expected to forecast crop yields, both in regions with a deficit of data and globally; it can also simply and effectively forecast winter wheat yields in a timely way utilizing publicly available multi-source data.",
        "DOI": "10.3390/agronomy12123194",
        "paper_author": "Di Y.",
        "affiliation_name": "Ministry of Agriculture of the People's Republic of China",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60087826",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Information in Streetscapes—Research on Visual Perception Information Quantity of Street Space Based on Information Entropy and Machine Learning",
        "publication": "ISPRS International Journal of Geo-Information",
        "citied_by": "7",
        "cover_date": "2022-12-01",
        "Abstract": "Urban street space is a critical reflection of a city’s vitality and image and a critical component of urban planning. While visual perceptual information about an urban street space can reflect the composition of place elements and spatial relationships, it lacks a unified and comprehensive quantification system. It is frequently presented in the form of element proportions without accounting for realistic factors, such as occlusion, light and shadow, and materials, making it difficult for the data to accurately describe the complex information found in real scenes. The conclusions of related studies are insufficiently focused to serve as a guide for designing solutions, remaining merely theoretical paradigms. As such, this study employed semantic segmentation and information entropy models to generate four visual perceptual information quantity (VPIQ) measures of street space: (1) form; (2) line; (3) texture; and (4) color. Then, at the macro level, the streetscape coefficient of variation (SCV) and K-means cluster entropy (HCK) were proposed to quantify the street’s spatial variation characteristics based on VPIQ. Additionally, we used geographically weighted regression (GWR) to investigate the relationship between VPIQ and street elements at the meso level as well as its practical application. This method can accurately and objectively describe and detect the current state of street spaces, assisting urban planners and decision-makers in making decisions about planning policies, urban regeneration schemes, and how to manage the street environment.",
        "DOI": "10.3390/ijgi11120628",
        "paper_author": "Liu Z.",
        "affiliation_name": "Zhejiang Sci-Tech University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60103821",
        "affiliation_state": "Zhejiang"
    },
    {
        "paper_title": "Phenological normalization can improve in-season classification of maize and soybean: A case study in the central US Corn Belt",
        "publication": "Science of Remote Sensing",
        "citied_by": "11",
        "cover_date": "2022-12-01",
        "Abstract": "Within-season estimates of crop-specific planted area, conditions, and expected yields are critical for decision- and policy-making related to agriculture and food security. However, these estimates require in-season crop type maps which are not currently widely available. Machine learning and remote sensing data can be used to create crop type maps that provide crop-specific land cover classifications to enable timely analysis at field scales throughout the growing season to complement traditional reporting efforts. However, existing methods are often limited by lower performance on test data from seasons with different patterns not seen during training and inability to provide classifications during the growing season when most operationally relevant. We present a new approach to in-season crop type mapping that addresses inter-annual domain shift by normalizing satellite observations by land surface phenology stage. These phenology-normalized observations are input to a neural network that extracts temporal and spatial features using both recurrent and convolutional layers respectively. Using Harmonized Landsat and Sentinel-2 (HLS) and Sentinel-1 SAR observations with test data from the U.S. states of Iowa in 2019 (late planting year) and Illinois in 2020 (standard year), we showed that this method enabled good performance in both scenarios throughout the growing season (71% and 72% in early-season, 80% and 84% in mid-season, 81% and 85% in late-season for Iowa 2019 and Illinois 2020 respectively).",
        "DOI": "10.1016/j.srs.2022.100059",
        "paper_author": "Kerner H.R.",
        "affiliation_name": "University of Maryland, College Park",
        "affiliation_city": "College Park",
        "affiliation_country": "United States",
        "affiliation_id": "60020304",
        "affiliation_state": "MD"
    },
    {
        "paper_title": "Detecting and Quantifying Structural Breaks in Climate",
        "publication": "Econometrics",
        "citied_by": "4",
        "cover_date": "2022-12-01",
        "Abstract": "Structural breaks have attracted considerable attention recently, especially in light of the financial crisis, Great Recession, the COVID-19 pandemic, and war. While structural breaks pose significant econometric challenges, machine learning provides an incisive tool for detecting and quantifying breaks. The current paper presents a unified framework for analyzing breaks; and it implements that framework to test for and quantify changes in precipitation in Mauritania over 1919–1997. These tests detect a decline of one third in mean rainfall, starting around 1970. Because water is a scarce resource in Mauritania, this decline—with adverse consequences on food production—has potential economic and policy consequences.",
        "DOI": "10.3390/econometrics10040033",
        "paper_author": "Ericsson N.R.",
        "affiliation_name": "Federal Reserve Board Division of International Finance",
        "affiliation_city": "Washington, D.C.",
        "affiliation_country": "United States",
        "affiliation_id": "60003623",
        "affiliation_state": "DC"
    },
    {
        "paper_title": "Improving Public Health Policy by Comparing the Public Response during the Start of COVID-19 and Monkeypox on Twitter in Germany: A Mixed Methods Study",
        "publication": "Vaccines",
        "citied_by": "6",
        "cover_date": "2022-12-01",
        "Abstract": "Little is known about monkeypox public concerns since its widespread emergence in many countries. Tweets in Germany were examined in the first three months of COVID-19 and monkeypox to examine concerns and issues raised by the public. Understanding views and positions of the public could help to shape future public health campaigns. Few qualitative studies reviewed large datasets, and the results provide the first instance of the public thinking comparing COVID-19 and monkeypox. We retrieved 15,936 tweets from Germany using query words related to both epidemics in the first three months of each one. A sequential explanatory mixed methods research joined a machine learning approach with thematic analysis using a novel rapid tweet analysis protocol. In COVID-19 tweets, there was the selfing construct or feeling part of the emerging narrative of the spread and response. In contrast, during monkeypox, the public considered othering after the fatigue of the COVID-19 response, or an impersonal feeling toward the disease. During monkeypox, coherence and reconceptualization of new and competing information produced a customer rather than a consumer/producer model. Public healthcare policy should reconsider a one-size-fits-all model during information campaigns and produce a strategic approach embedded within a customer model to educate the public about preventative measures and updates. A multidisciplinary approach could prevent and minimize mis/disinformation.",
        "DOI": "10.3390/vaccines10121985",
        "paper_author": "AL-Ahdal T.",
        "affiliation_name": "Universität Heidelberg",
        "affiliation_city": "Heidelberg",
        "affiliation_country": "Germany",
        "affiliation_id": "60016908",
        "affiliation_state": "Baden-Wurttemberg"
    },
    {
        "paper_title": "Deep Reinforcement Learning for Flow Control Exploits Different Physics for Increasing Reynolds Number Regimes",
        "publication": "Actuators",
        "citied_by": "21",
        "cover_date": "2022-12-01",
        "Abstract": "The increase in emissions associated with aviation requires deeper research into novel sensing and flow-control strategies to obtain improved aerodynamic performances. In this context, data-driven methods are suitable for exploring new approaches to control the flow and develop more efficient strategies. Deep artificial neural networks (ANNs) used together with reinforcement learning, i.e., deep reinforcement learning (DRL), are receiving more attention due to their capabilities of controlling complex problems in multiple areas. In particular, these techniques have been recently used to solve problems related to flow control. In this work, an ANN trained through a DRL agent, coupled with the numerical solver Alya, is used to perform active flow control. The Tensorforce library was used to apply DRL to the simulated flow. Two-dimensional simulations of the flow around a cylinder were conducted and an active control based on two jets located on the walls of the cylinder was considered. By gathering information from the flow surrounding the cylinder, the ANN agent is able to learn through proximal policy optimization (PPO) effective control strategies for the jets, leading to a significant drag reduction. Furthermore, the agent needs to account for the coupled effects of the friction- and pressure-drag components, as well as the interaction between the two boundary layers on both sides of the cylinder and the wake. In the present work, a Reynolds number range beyond those previously considered was studied and compared with results obtained using classical flow-control methods. Significantly different forms of nature in the control strategies were identified by the DRL as the Reynolds number (Formula presented.) increased. On the one hand, for (Formula presented.), the classical control strategy based on an opposition control relative to the wake oscillation was obtained. On the other hand, for (Formula presented.), the new strategy consisted of energization of the boundary layers and the separation area, which modulated the flow separation and reduced the drag in a fashion similar to that of the drag crisis, through a high-frequency actuation. A cross-application of agents was performed for a flow at (Formula presented.), obtaining similar results in terms of the drag reduction with the agents trained at (Formula presented.) and 2000. The fact that two different strategies yielded the same performance made us question whether this Reynolds number regime ((Formula presented.)) belongs to a transition towards a nature-different flow, which would only admits a high-frequency actuation strategy to obtain the drag reduction. At the same time, this finding allows for the application of ANNs trained at lower Reynolds numbers, but are comparable in nature, saving computational resources.",
        "DOI": "10.3390/act11120359",
        "paper_author": "Varela P.",
        "affiliation_name": "Universitat Politècnica de València",
        "affiliation_city": "Valencia",
        "affiliation_country": "Spain",
        "affiliation_id": "60011476",
        "affiliation_state": "Valencia"
    },
    {
        "paper_title": "Research on the Effectiveness of Deep Learning−Based Agency Cost Suppression Strategy: A Case Study of State−Owned Enterprises in Mainland China",
        "publication": "Systems",
        "citied_by": "1",
        "cover_date": "2022-12-01",
        "Abstract": "The mixed ownership reform aims to improve the property rights structure of the state−owned enterprises (SOEs) and reduce agency costs, and the current mixed reform strategies mainly include equity blending by introducing external non−state capital, executive assignments, and employee stock ownership. In this paper, 953 valid data of A−shares listed in Shanghai and Shenzhen from 2008 to 2020 are used as samples to construct the indicators of mixed reform strategy by the literature statistics method. After obtaining multiple impact indicators, the regression impact model of corporate agency cost suppression strategy is constructed by MATLAB software using a machine learning algorithm. On this basis, the performance of multiple machine learning algorithms is compared, and it is found that the integrated optimization−based bag−boosting model is used to study the effect of hybrid reform strategy to reduce the agency costs of SOEs, and the proportional setting of indicators when the effect is optimal is also explored. Finally, the laws of different influencing factors on the agency costs of enterprises are explored separately by the eigenvalue method. The results of the study show that the proportion of shareholding of the first largest non−state shareholder is sin−functional with the agency costs of SOEs when non−state majority shareholders are introduced into SOEs’ equity mix, and the agency costs tend to decrease after SOEs become privately held enterprises. The greater the number and proportion of supervisors appointed by non−state shareholders, the greater the supervisory restraint effect on SOE managers and the better the effect of suppressing agency costs. The participation of non−state−owned shareholders in the company’s business decisions by appointed executives and the special resource advantages of SOEs intensify the occurrence of the self−interest of appointed executives and the increase of agency costs of SOEs. The implementation of an employee stock ownership plan plays the role of employee supervision and restraint on SOE managers, which reduces the agency costs of SOEs. Based on this, it can provide support for the government to improve the hybrid reform policy and promote the process layer by layer, and also provide theoretical reference for SOEs to deepen the equity mix, incentivize employee shareholding, and empower non−state shareholders to govern and thus reduce agency costs.",
        "DOI": "10.3390/systems10060242",
        "paper_author": "Zhai D.",
        "affiliation_name": "Harbin Institute of Technology",
        "affiliation_city": "Harbin",
        "affiliation_country": "China",
        "affiliation_id": "60019616",
        "affiliation_state": "Heilongjiang"
    },
    {
        "paper_title": "Reducing Children’s Obesity in the Age of Telehealth and AI/IoT Technologies in Gulf Countries",
        "publication": "Systems",
        "citied_by": "3",
        "cover_date": "2022-12-01",
        "Abstract": "Childhood obesity has become one of the major health issues in the global population. The increasing prevalence of childhood obesity is associated with serious health issues and comorbidities related to obesity. Several studies mentioned that childhood obesity became even worse recently due to the effect of COVID-19 and the consequent policies and regulations. For that reason, Internet of Things (IoT) technologies should be utilized to overcome the challenges related to obesity management and provide care from a distance to improve the health care services for obesity. However, IoT by itself is a limited resource and it is important to consider other artificial intelligent (AI) components. Thus, this paper contributes into the literature of child obesity management by introducing a comprehensive survey for obesity management covering clinical work measuring the association between sleep disturbances and childhood obesity alongside physical activity and diet and comparatively analyzing the emerging technologies used to prevent childhood obesity. It further contributes to the literature by proposing an interactive smart framework that combines clinical and emerging AI/telehealth technologies to manage child obesity. The proposed framework can be used to reduce children obesity and improve their quality of life using Machine Learning (ML). It utilizes IoT devices to integrate information from different sources and complement it with a mobile application and web-based platform to connect parents and physicians with their child.",
        "DOI": "10.3390/systems10060241",
        "paper_author": "Faisal M.",
        "affiliation_name": "Kuwait College of Science &amp; Technology",
        "affiliation_city": "Doha",
        "affiliation_country": "Kuwait",
        "affiliation_id": "60121848",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "DRLLA: Deep Reinforcement Learning for Link Adaptation",
        "publication": "Telecom",
        "citied_by": "4",
        "cover_date": "2022-12-01",
        "Abstract": "Link adaptation (LA) matches transmission parameters to conditions on the radio link, and therefore plays a major role in telecommunications. Improving LA is within the requirements for next-generation mobile telecommunication systems, and by refining link adaptation, a higher channel efficiency can be achieved (i.e., an increased data rate thanks to lower required bandwidth). Furthermore, by replacing traditional LA algorithms, radio transmission systems can better adapt themselves to a dynamic environment. There are several drawbacks to current state-of-the-art approaches, including predefined and static decision boundaries or relying on a single, low-dimensional metric. Nowadays, a broadly used approach to handle a variety of related input variables is a neural network (NN). NNs are able to make use of multiple inputs, and when combined with reinforcement learning (RL), the so-called deep reinforcement learning (DRL) approach emerges. Using DRL, more complex parameter relationships can be considered in order to recommend the modulation and coding scheme (MCS) used in LA. Hence, this work examines the potential of DRL and includes experiments on different channels. The main contribution of this work lies in using DRL algorithms for LA, optimized for throughput based on a subcarrier observation matrix and a packet success rate feedback system. We apply Natural Actor-Critic (NAC) and Proximal Policy Optimization (PPO) algorithms on simulated channels with a subsequent feasibility study on a prerecorded real-world channel. Empirical results produced by experiments on the examined channels hint that Deep Reinforcement Learning for Link Adaptation (DRLLA) offers good performance indicated by a promising data rate on the additive white Gaussian noise (AWGN) channel, the non-line-of-sight (NLOS) channel, and a prerecorded real-world channel. No matter the channel impairment, the agent is able to respond to changing signal-to-interference-plus-noise-ratio (SINR) levels, as exhibited by expected changes in the effective data rate.",
        "DOI": "10.3390/telecom3040037",
        "paper_author": "Geiser F.",
        "affiliation_name": "Technische Universität München",
        "affiliation_city": "Munich",
        "affiliation_country": "Germany",
        "affiliation_id": "60019722",
        "affiliation_state": "Bayern"
    },
    {
        "paper_title": "A Time Series Model Based on Deep Learning and Integrated Indicator Selection Method for Forecasting Stock Prices and Evaluating Trading Profits",
        "publication": "Systems",
        "citied_by": "10",
        "cover_date": "2022-12-01",
        "Abstract": "A stock forecasting and trading system is a complex information system because a stock trading system needs to be analyzed and modeled using data science, machine learning, and artificial intelligence. Previous time series models have been widely used to forecast stock prices, but due to several shortcomings, these models cannot apply all available information to make a forecast. The relationship between stock prices and related factors is nonlinear and involves nonstationary fluctuations, and accurately forecasting stock prices is not an easy task. Therefore, this study used support vector machines (linear and radial basis functions), gene expression programming, multilayer perceptron regression, and generalized regression neural networks to calculate the importance of indicators. We then integrated the five indicator selection methods to find the key indicators. Next, we used long short-term memory (LSTM) and gated recurrent units (GRU) to build time series models for forecasting stock prices and compare them with the listing models. To evaluate the effectiveness of the proposed model, we collected six different stock market data from 2011 to 2019 to evaluate their forecast performance based on RMSE and MAPE metrics. It is worth mentioning that this study proposes two trading policies to evaluate trading profits and compare them with the listing methods, and their profits are pretty good to investors. After the experiments, the proposed time series model (GRU/LSTM combined with the selected key indicators) exhibits better forecast ability in fluctuating and non-fluctuating environments than the listing models, thus presenting an effective reference for stakeholders.",
        "DOI": "10.3390/systems10060243",
        "paper_author": "Cheng C.H.",
        "affiliation_name": "National Yunlin University of Science and Technology",
        "affiliation_city": "Douliou",
        "affiliation_country": "Taiwan",
        "affiliation_id": "60014261",
        "affiliation_state": "Yunlin"
    },
    {
        "paper_title": "Predicting VO<inf>2</inf>max in Children and Adolescents Aged between 6 and 17 Using Physiological Characteristics and Participation in Sport Activities: A Cross-Sectional Study Comparing Different Regression Models Stratified by Gender",
        "publication": "Children",
        "citied_by": "1",
        "cover_date": "2022-12-01",
        "Abstract": "Background: The aim of this study is to use different regression models to capture the association between cardiorespiratory fitness VO2max (measured in mL/kg/min) and somatometric characteristics and sports activities and making better predictions. Methods: multiple linear regression (MLR), quantile regression (QR), ridge regression (RR), support vector regression (SVR) with three different kernels, artificial neural networks (ANNs), and boosted regression trees (RTs) were compared to explain and predict VO2max and to choose the best performance model. The sample consisted of 4908 children (2314 males and 2594 females) aged between 6 and 17. Cardiorespiratory fitness was assessed by the 20 m maximal multistage shuttle run test and maximal oxygen uptake (VO2max) was calculated. Welch t-tests, Mann–Whitney-U tests, X2 tests, and ANOVA tests were performed. The performance measures were root mean square error (RMSE), mean absolute error (MAE), and coefficient of determination (R2). All analyses were stratified by gender. Results: A comparison of the statistical indices for both the predicted and actual data indicated that in boys, the MLR model outperformed all other models in all indices, followed by the linear SVR model. In girls, the MLR model performed better than the other models in R2 but was outperformed by SVR-RBF in terms of RMSE and MAE. The overweight and obesity categories in both sexes (p < 0.001) and maternal prepregnancy obesity in girls had a significant negative effect on VO2max. Age, weekly football training, track and field, basketball, and swimming had different positive effects based on gender. Conclusion: The MLR model showed remarkable performance against all other models and was competitive with the SVR models. In addition, this study’s data showed that changes in cardiorespiratory fitness were dependent, to a different extent based on gender, on BMI category, weight, height, age, and participation in some organized sports activities. Predictors that are not considered modifiable, such as gender, can be used to guide targeted interventions and policies.",
        "DOI": "10.3390/children9121935",
        "paper_author": "Carayanni V.",
        "affiliation_name": "University of West Attica",
        "affiliation_city": "Athens",
        "affiliation_country": "Greece",
        "affiliation_id": "60110806",
        "affiliation_state": "Attica"
    },
    {
        "paper_title": "Reinforcement Learning Control of Hydraulic Servo System Based on TD3 Algorithm",
        "publication": "Machines",
        "citied_by": "11",
        "cover_date": "2022-12-01",
        "Abstract": "This paper aims at the characteristics of nonlinear, time-varying and parameter coupling in a hydraulic servo system. An intelligent control method is designed that uses self-learning without a model or prior knowledge, in order to achieve certain control effects. The control quantity can be obtained at the current moment through the continuous iteration of a strategy–value network, and the online self-tuning of parameters can be realized. Taking the hydraulic servo system as the experimental object, a twin delayed deep deterministic (TD3) policy gradient was used to reinforce the learning of the system. Additionally, the parameter setting was compared using a deep deterministic policy gradient (DDPG) and a linear–quadratic–Gaussian (LQG) based on linear quadratic Gaussian objective function. To compile the reinforcement learning algorithm and deploy it to the test platform controller for testing, we used the Speedgoat prototype target machine as the controller to build the fast prototype control test platform. MATLAB/Coder and compute unified device architecture (CUDA) were used to generate an S-function. The results show that, compared with other parameter tuning methods, the proposed algorithm can effectively optimize the controller parameters and improve the dynamic response of the system when tracking signals.",
        "DOI": "10.3390/machines10121244",
        "paper_author": "Yuan X.",
        "affiliation_name": "Yanshan University",
        "affiliation_city": "Qinhuangdao",
        "affiliation_country": "China",
        "affiliation_id": "60018465",
        "affiliation_state": "Hebei"
    },
    {
        "paper_title": "Drug Abuse Ontology to Harness Web-Based Data for Substance Use Epidemiology Research: Ontology Development Study",
        "publication": "JMIR Public Health and Surveillance",
        "citied_by": "10",
        "cover_date": "2022-12-01",
        "Abstract": "Background: Web-based resources and social media platforms play an increasingly important role in health-related knowledge and experience sharing. There is a growing interest in the use of these novel data sources for epidemiological surveillance of substance use behaviors and trends. Objective: The key aims were to describe the development and application of the drug abuse ontology (DAO) as a framework for analyzing web-based and social media data to inform public health and substance use research in the following areas: determining user knowledge, attitudes, and behaviors related to nonmedical use of buprenorphine and illicitly manufactured opioids through the analysis of web forum data Prescription Drug Abuse Online Surveillance; analyzing patterns and trends of cannabis product use in the context of evolving cannabis legalization policies in the United States through analysis of Twitter and web forum data (eDrugTrends); assessing trends in the availability of novel synthetic opioids through the analysis of cryptomarket data (eDarkTrends); and analyzing COVID-19 pandemic trends in social media data related to 13 states in the United States as per Mental Health America reports. Methods: The domain and scope of the DAO were defined using competency questions from popular ontology methodology (101 ontology development). The 101 method includes determining the domain and scope of ontology, reusing existing knowledge, enumerating important terms in ontology, defining the classes, their properties and creating instances of the classes. The quality of the ontology was evaluated using a set of tools and best practices recognized by the semantic web community and the artificial intelligence community that engage in natural language processing. Results: The current version of the DAO comprises 315 classes, 31 relationships, and 814 instances among the classes. The ontology is flexible and can easily accommodate new concepts. The integration of the ontology with machine learning algorithms dramatically decreased the false alarm rate by adding external knowledge to the machine learning process. The ontology is recurrently updated to capture evolving concepts in different contexts and applied to analyze data related to social media and dark web marketplaces. Conclusions: The DAO provides a powerful framework and a useful resource that can be expanded and adapted to a wide range of substance use and mental health domains to help advance big data analytics of web-based data for substance use epidemiology research.",
        "DOI": "10.2196/24938",
        "paper_author": "Lokala U.",
        "affiliation_name": "University of South Carolina",
        "affiliation_city": "Columbia",
        "affiliation_country": "United States",
        "affiliation_id": "60018179",
        "affiliation_state": "SC"
    },
    {
        "paper_title": "Predicting the Environmental Change of Carbon Emission Patterns in South Asia: A Deep Learning Approach Using BiLSTM",
        "publication": "Atmosphere",
        "citied_by": "25",
        "cover_date": "2022-12-01",
        "Abstract": "China’s economy has made significant strides in the past three decades. As a direct result of China’s “one belt, one road” (OBOR) initiative, the country’s rate of industrialization and urbanization is currently the fastest in the entire world. This rapid development is largely dependent on the enormous amounts of energy currently being consumed and forms the foundation of the world’s high levels of carbon emissions. It is generally agreed that the production of greenhouse gases, particularly carbon dioxide, is the primary contributor to the current state of climate change. In this paper, a CO2 emission prediction model based on Bi-LSTM is constructed. In order to conduct empirical tests on the model, this study uses data from South Asian countries and China from 2001 to 2020. China’s CO2 emissions from 2022 to 2030 were predicted along with those of other countries in order to study the combined effects of the scientific and technological progress, industrial structures, and energy structure factors affecting CO2 emissions. When compared with the LSTM and GRU methods, the Bi-LSTM model’s results produced lower MAE, MSE, and MAPE values, indicating that it performs better. According to the findings, carbon emissions represent a significant problem that will become much worse in the future due to China and India’s high emissions, particularly in the next 10 years, if the government does not implement policies that help reduce those emissions.",
        "DOI": "10.3390/atmos13122011",
        "paper_author": "Aamir M.",
        "affiliation_name": "Huanggang Normal University",
        "affiliation_city": "Huanggang",
        "affiliation_country": "China",
        "affiliation_id": "60083905",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "A Hybrid Rule-Based and Data-Driven Approach to Illegal Transshipment Identification with Interpretable Behavior Features",
        "publication": "Sensors",
        "citied_by": "2",
        "cover_date": "2022-12-01",
        "Abstract": "Illegal transshipment of maritime ships is usually closely related to illegal activities such as smuggling, human trafficking, piracy plunder, and illegal fishing. Intelligent identification of illegal transshipment has become an important technical means to ensure the safety of maritime transport. However, due to different geographical environments, legal policies and regulatory requirements in each sea area, there are differences in the movement characteristics and geographical distribution of illegal transshipment behavior in different time and space. Moreover, in areas with dense traffic flow, normal navigation behavior can easily be identified as illegal transshipment, resulting in a high rate of misidentification. This paper proposes a hybrid rule-based and data-driven approach to solve the problem of missing identification in fixed threshold methods and introduces a traffic density feature to reduce the misidentification rate in dense traffic areas. The method is both interpretable and adaptable through unsupervised clustering to get suitable threshold distribution combination for regulatory sea areas. The evaluation results in two different sea areas show that the proposed method is applicable. Compared with other widely used identification methods, this method identifies more illegal transshipment events, which are highly suspicious, and gives warning much earlier. The proposed method can even filter out misidentification events from compared methods’ results, which account for more than half of the total number.",
        "DOI": "10.3390/s22249581",
        "paper_author": "Deng L.",
        "affiliation_name": "Beijing Jiaotong University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60022381",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Similarity Analysis in Understanding Online News in Response to Public Health Crisis",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "2",
        "cover_date": "2022-12-01",
        "Abstract": "Background: The “Syphilis No!” campaign the Brazilian Ministry of Health (MoH) launched between November 2018 and March 2019, brought forward the concept \"Test, Treat and Cure\" to remind the population of the importance of syphilis prevention. In this context, this study aims to analyze the similarity of syphilis online news to comprehend how public health communication interventions influence media coverage of the syphilis issue. Methods: This paper presented a computational approach to assess the effectiveness of communication actions on a public health problem. Data were collected between January 2015 and December 2019 and processed using the Hermes ecosystem, which utilizes text mining and machine learning algorithms to cluster similar content. Results: Hermes identified 1049 google-indexed web pages containing the term ’syphilis’ in Brazil. Of these, 619 were categorized as news stories. In total, 157 were grouped into clusters of at least two similar news items and a single cluster with 462 news classified as “single” for not featuring similar news items. From these, 19 clusters were identified in the pre-campaign period, 23 during the campaign, and 115 in the post-campaign. Conclusions: The findings presented in this study show that the volume of syphilis-related news reports has increased in recent years and gained popularity after the SNP started, having been boosted during the campaign and escalating even after its completion.",
        "DOI": "10.3390/ijerph192417049",
        "paper_author": "Cezario S.",
        "affiliation_name": "Universidade Federal do Rio Grande do Norte",
        "affiliation_city": "Natal",
        "affiliation_country": "Brazil",
        "affiliation_id": "60023857",
        "affiliation_state": "RN"
    },
    {
        "paper_title": "Assessing and mapping soil erosion risk zone in Ratlam District, central India",
        "publication": "Regional Sustainability",
        "citied_by": "7",
        "cover_date": "2022-12-01",
        "Abstract": "Evaluation of physical and quantitative data of soil erosion is crucial to the sustainable development of the environment. The extreme form of land degradation through different forms of erosion is one of the major problems in the sub-tropical monsoon-dominated region. In India, tackling soil erosion is one of the major geo-environmental issues for its environment. Thus, identifying soil erosion risk zones and taking preventative actions are vital for crop production management. Soil erosion is induced by climate change, topographic conditions, soil texture, agricultural systems, and land management. In this research, the soil erosion risk zones of Ratlam District was determined by employing the Geographic Information System (GIS), Revised Universal Soil Loss Equation (RUSLE), Analytic Hierarchy Process (AHP), and machine learning algorithms (Random Forest and Reduced Error Pruning (REP) tree). RUSLE measured the rainfall eosivity (R), soil erodibility (K), length of slope and steepness (LS), Land cover and management (C), and support practices (P) factors. Kappa statistic was used to configure model reliability and it was found that Random Forest and AHP have higher reliability than other models. About 14.73% (715.94 km2) of the study area has very low risk to soil erosion, with an average soil erosion rate of 0.00–7.00 × 103 kg/(hm2·a), while about 7.46% (362.52 km2) of the study area has very high risk to soil erosion, with an average soil erosion rate of 30.00 × 103–48.00 × 103 kg/(hm2·a). Slope, elevation, stream density, Stream Power Index (SPI), rainfall, and land use and land cover (LULC) all affect soil erosion. The current study could help the government and non-government agencies to employ developmental projects and policies accordingly. However, the outcomes of the present research also could be used to prevent, monitor, and control soil erosion in the study area by employing restoration measures.",
        "DOI": "10.1016/j.regsus.2022.11.005",
        "paper_author": "Saha S.",
        "affiliation_name": "Raiganj University",
        "affiliation_city": "Raiganj",
        "affiliation_country": "India",
        "affiliation_id": "60209646",
        "affiliation_state": "WB"
    },
    {
        "paper_title": "Hand Gesture Recognition Using EMG-IMU Signals and Deep Q-Networks",
        "publication": "Sensors",
        "citied_by": "14",
        "cover_date": "2022-12-01",
        "Abstract": "Hand gesture recognition systems (HGR) based on electromyography signals (EMGs) and inertial measurement unit signals (IMUs) have been studied for different applications in recent years. Most commonly, cutting-edge HGR methods are based on supervised machine learning methods. However, the potential benefits of reinforcement learning (RL) techniques have shown that these techniques could be a viable option for classifying EMGs. Methods based on RL have several advantages such as promising classification performance and online learning from experience. In this work, we developed an HGR system made up of the following stages: pre-processing, feature extraction, classification, and post-processing. For the classification stage, we built an RL-based agent capable of learning to classify and recognize eleven hand gestures—five static and six dynamic—using a deep Q-network (DQN) algorithm based on EMG and IMU information. The proposed system uses a feed-forward artificial neural network (ANN) for the representation of the agent policy. We carried out the same experiments with two different types of sensors to compare their performance, which are the Myo armband sensor and the G-force sensor. We performed experiments using training, validation, and test set distributions, and the results were evaluated for user-specific HGR models. The final accuracy results demonstrated that the best model was able to reach up to (Formula presented.) and (Formula presented.) for the classification and recognition, respectively, with regard to static gestures, and (Formula presented.) and (Formula presented.) for the classification and recognition, respectively, with regard to dynamic gestures with the Myo armband sensor. The results obtained in this work demonstrated that RL methods such as the DQN are capable of learning a policy from online experience to classify and recognize static and dynamic gestures using EMG and IMU signals.",
        "DOI": "10.3390/s22249613",
        "paper_author": "Vásconez J.P.",
        "affiliation_name": "Escuela Politécnica Nacional",
        "affiliation_city": "Quito",
        "affiliation_country": "Ecuador",
        "affiliation_id": "60072054",
        "affiliation_state": "Pichincha"
    },
    {
        "paper_title": "Assessment of water consumption in households using statistical analysis and regression trees",
        "publication": "Sustainable Cities and Society",
        "citied_by": "29",
        "cover_date": "2022-12-01",
        "Abstract": "Understanding the drivers of water consumption is essential for effective water resources management. This paper evaluates how socioeconomic and demographic variables, construction characteristics and behavioral aspects of residents influence domestic water consumption in the city of Joinville, Southern Brazil, analyzing data from 394 households. Statistical analysis and Univariate Regression Trees were applied to the data set. The results show that the average per capita consumption is 143.67 L/person/day, while the average household monthly water consumption is 14.53 m³/household/month. Amidst the findings in the statistical analysis, socioeconomic aspects such as number of residents, income and presence of children in the household showed an effect on water consumption, as well as building age and typology, number of bathrooms and presence of a bathtub and a swimming pool. The regression trees allowed identifying that the number of residents and area per capita are the variables with greater importance to estimate household and per capita water consumption, respectively. As a key implication of this study, the analysis presented herein provides additional empirical support for the body of knowledge on the factors that influence residential water consumption and can help decision-makers to direct actions and public policies aimed at water conservation in the built environment.",
        "DOI": "10.1016/j.scs.2022.104186",
        "paper_author": "Grespan A.",
        "affiliation_name": "Universidade do Estado de Santa Catarina",
        "affiliation_city": "Florianopolis",
        "affiliation_country": "Brazil",
        "affiliation_id": "60024094",
        "affiliation_state": "SC"
    },
    {
        "paper_title": "Discovering mechanisms for materials microstructure optimization via reinforcement learning of a generative model",
        "publication": "Machine Learning: Science and Technology",
        "citied_by": "6",
        "cover_date": "2022-12-01",
        "Abstract": "The design of materials structure for optimizing functional properties and potentially, the discovery of novel behaviors is a keystone problem in materials science. In many cases microstructural models underpinning materials functionality are available and well understood. However, optimization of average properties via microstructural engineering often leads to combinatorically intractable problems. Here, we explore the use of the reinforcement learning (RL) for microstructure optimization targeting the discovery of the physical mechanisms behind enhanced functionalities. We illustrate that RL can provide insights into the mechanisms driving properties of interest in a 2D discrete Landau ferroelectrics simulator. Intriguingly, we find that non-trivial phenomena emerge if the rewards are assigned to favor physically impossible tasks, which we illustrate through rewarding RL agents to rotate polarization vectors to energetically unfavorable positions. We further find that strategies to induce polarization curl can be non-intuitive, based on analysis of learned agent policies. This study suggests that RL is a promising machine learning method for material design optimization tasks, and for better understanding the dynamics of microstructural simulations.",
        "DOI": "10.1088/2632-2153/aca004",
        "paper_author": "Vasudevan R.K.",
        "affiliation_name": "Oak Ridge National Laboratory",
        "affiliation_city": "Oak Ridge",
        "affiliation_country": "United States",
        "affiliation_id": "60024266",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Unearned premium risk and machine learning techniques",
        "publication": "Frontiers in Applied Mathematics and Statistics",
        "citied_by": "2",
        "cover_date": "2022-12-01",
        "Abstract": "Insurance companies typically divide premiums into earned and unearned premiums. Unearned premium is the portion of premium that is allocated for the remaining period of a policy or premium that still needs to be earned. The unearned premium risk arises when an unearned premium is insufficient to cover future losses. Reserves allocated for the unearned premium risk are called premium deficiency reserves (PDRs). PDR received less attention from the actuarial community compared to other reserves such as reserves for reported but not fully settled (RBNS) claims, and incurred but not reported (IBNR) claims. Existing research on PDR mainly focused on utilizing statistical models. In this article, we apply machine learning models to calculate PDR. We use an extended warranty dataset, which comes under long-duration P & C insurance contracts to demonstrate our models. Using two statistical and two machine learning models, we show that machine learning models predict reserves more accurately than the traditional statistical model. Thus, this article encourages actuaries to consider machine learning models when calculating PDRs for the unearned premium risk.",
        "DOI": "10.3389/fams.2022.1056529",
        "paper_author": "Manathunga V.",
        "affiliation_name": "Middle Tennessee State University",
        "affiliation_city": "Murfreesboro",
        "affiliation_country": "United States",
        "affiliation_id": "60018466",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Revealing underlying factors of absenteeism: A machine learning approach",
        "publication": "Frontiers in Psychology",
        "citied_by": "5",
        "cover_date": "2022-12-01",
        "Abstract": "Introduction: The basis of support is understanding. In machine learning, understanding happens through assimilated knowledge and is centered on six pillars: big data, data volume, value, variety, velocity, and veracity. This study analyzes school attendance problems (SAP), which encompasses its legal statutes, school codes, students’ attendance behaviors, and interventions in a school environment. The support pillars include attention to the physical classroom, school climate, and personal underlying factors impeding engagement, from which socio-emotional factors are often the primary drivers. Methods: This study asked the following research question: What can we learn about specific underlying factors of absenteeism using machine learning approaches? Data were retrieved from one school system available through the proprietary Building Dreams (BD) platform, owned by the Fight for Life Foundation (FFLF), whose mission is to support youth in underserved communities. The BD platform, licensed to K-12 schools, collects student-level data reported by educators on core values associated with in-class participation (a reported—negative or positive—behavior relative to the core values) based on Social–Emotional Learning (SEL) principles. We used a multi-phased approach leveraging several machine learning techniques (clustering, qualitative analysis, classification, and refinement of supervised and unsupervised learning). Unsupervised technique was employed to explore strong boundaries separating students using unlabeled data. Results: From over 20,000 recorded behaviors, we were able to train a classifier with 90.2% accuracy and uncovered a major underlying factor directly affecting absenteeism: the importance of peer relationships. This is an important finding and provides data-driven support for the fundamental idea that peer relationships are a critical factor affecting absenteeism. Discussion: The reported results provide a clear evidence that implementing socio-emotional learning components within a curriculum can improve absenteeism by targeting a root cause. Such knowledge can drive impactful policy and programming changes necessary for supporting the youth in communities overwhelmed with adversities.",
        "DOI": "10.3389/fpsyg.2022.958748",
        "paper_author": "Bowen F.",
        "affiliation_name": "Butler University",
        "affiliation_city": "Indianapolis",
        "affiliation_country": "United States",
        "affiliation_id": "60020268",
        "affiliation_state": "IN"
    },
    {
        "paper_title": "Experiences and perceptions of COVID-19 infection and vaccination among Palestinian refugees in Jerash camp and Jordanian citizens: a comparative cross-sectional study by face-to-face interviews",
        "publication": "Infectious Diseases of Poverty",
        "citied_by": "9",
        "cover_date": "2022-12-01",
        "Abstract": "Background: During the COVID-19 vaccination, the access to vaccines has been unequal among countries and individuals, for example low-income countries displayed significant low levels of vaccination. Furthermore, most refugees are living in developing low-income countries which struggling to access the essential health-care services including vaccination. Thus, the objective of this study was to assess the experiences and perceptions of COVID-19 infection and vaccination among Palestine refugees in Jerash camp compared to resident Jordanian citizens. Methods: A face-to-face interview-based comparative cross-sectional study was carried out among Palestine refugees in Jerash camp located in northern Jordan and Jordanian citizens from different cities in Jordan from October, 2021 to March, 2022. A Chi-square test was used to determine the differences in the experiences and perceptions of COVID-19 infection and vaccination between Palestinian refugees and resident Jordanian citizens. Logistic regression analysis was performed to predict factors associated with the beliefs, barriers and hesitancy towards COVID-19 vaccines. Results: The total number of participants was 992, with 501 (50.5%) Palestinian refugees and 491 (49.5%) Jordanian citizens. Most participants (64.1%) who have never been tested for COVID-19 were from the refugees (P < 0.001), whereas about 80.3% of the participants tested for COVID-19 at private healthcare institutions were citizens (P < 0.001). While 70.0% of the participants who tested positive for COVID-19 (n = 303) were from the refugees (P < 0.001). Compared to the citizens, the refugees had significantly lower levels of beliefs about the safety (P = 0.008) and efficiency (P < 0.001) of COVID-19 vaccines. They also had lower rates of vaccine hesitancy (P = 0.002) and vaccine uptake (P < 0.001), and a higher rate of facing difficulties during registration for COVID-19 vaccination (P < 0.001). Furthermore, refugees have more negative attitudes toward the importance and implementation of COVID-19 precautionary activities, including wearing face masks, practicing social distancing and following proper prevention hygiene compared to citizens (P < 0.001). The regression analysis showed that gender (P < 0.001), age (P < 0.001) and level of education (P = 0.001) were significantly associated with COVID-19 vaccine hesitancy. Also, being a refugee (P < 0.001) and being a male (P = 0.012) were significantly associated with facing more difficulties upon the registration to receive a COVID-19 vaccine. Conclusions: This study showed that, compared to citizens, refugees had lower attitudes and practices toward COVID-19 infection and vaccination. They also had and a lower rate of COVID-19 vaccine hesitancy and uptake with limited access to vaccines. Government sectors and non-government organizations should implement policies and regulations to raise the awareness of refugees towards COVID-19 infection, testing, preventive measures, and the safety and efficacy of vaccines.",
        "DOI": "10.1186/s40249-022-01047-y",
        "paper_author": "Al-Hatamleh M.A.I.",
        "affiliation_name": "School of Medical Sciences, Universiti Sains Malaysia",
        "affiliation_city": "Kubang Kerian",
        "affiliation_country": "Malaysia",
        "affiliation_id": "60002512",
        "affiliation_state": "Kelantan"
    },
    {
        "paper_title": "Emerging paradigms in sepsis",
        "publication": "eBioMedicine",
        "citied_by": "3",
        "cover_date": "2022-12-01",
        "Abstract": "NA",
        "DOI": "10.1016/j.ebiom.2022.104398",
        "paper_author": "Vincent J.L.",
        "affiliation_name": "Hôpital Erasme",
        "affiliation_city": "Brussels",
        "affiliation_country": "Belgium",
        "affiliation_id": "60068575",
        "affiliation_state": "BRU"
    },
    {
        "paper_title": "HomeMonitor: An Enhanced Device Event Detection Method for Smart Home Environment",
        "publication": "Sensors",
        "citied_by": "1",
        "cover_date": "2022-12-01",
        "Abstract": "As more and more smart devices are deployed in homes, the communication between these smart home devices and elastic computing services may face some risks of privacy disclosure. Different device events (such as the camera on, video on, etc.) will generate different data traffic during communication. However, the current smart home system lacks monitoring of these device events, which may cause the disclosure of private data collected by these devices. In this paper, we present our device event monitor system, HomeMonitor. HomeMonitor runs in the OpenWRT system and supports complete event monitoring for smart home devices. HomeMoitor solves the problem that machine learning models for detecting device events do not scale flexibly. It uses the network packet size and the direction of the device event for unique identification during training. When detecting, it only needs to get the packet size and timestamp and then query the policy table for signature matching to control the device events. We evaluated the effectiveness of HomeMonitor, and the experiments show that the match rate of our method is 98.8%, the false positive rate is 1.8%, and the detection time is only 16.67% for PINBALL. The results mean that our method achieves the balance of applicable protocol scope, detection performance, and detection accuracy.",
        "DOI": "10.3390/s22239389",
        "paper_author": "Zhao M.",
        "affiliation_name": "Guangzhou University",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60025345",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Remote Sensing and Machine Learning Tools to Support Wetland Monitoring: A Meta-Analysis of Three Decades of Research",
        "publication": "Remote Sensing",
        "citied_by": "19",
        "cover_date": "2022-12-01",
        "Abstract": "Despite their importance to ecosystem services, wetlands are threatened by pollution and development. Over the last few decades, a growing number of wetland studies employed remote sensing (RS) to scientifically monitor the status of wetlands and support their sustainability. Considering the rapid evolution of wetland studies and significant progress that has been made in the field, this paper constitutes an overview of studies utilizing RS methods in wetland monitoring. It investigates publications from 1990 up to the middle of 2022, providing a systematic survey on RS data type, machine learning (ML) tools, publication details (e.g., authors, affiliations, citations, and publications date), case studies, accuracy metrics, and other parameters of interest for RS-based wetland studies by covering 344 papers. The RS data and ML combination is deemed helpful for wetland monitoring and multi-proxy studies, and it may open up new perspectives for research studies. In a rapidly changing wetlands landscape, integrating multiple RS data types and ML algorithms is an opportunity to advance science support for management decisions. This paper provides insight into the selection of suitable ML and RS data types for the detailed monitoring of wetland-associated systems. The synthesized findings of this paper are essential to determining best practices for environmental management, restoration, and conservation of wetlands. This meta-analysis establishes avenues for future research and outlines a baseline framework to facilitate further scientific research using the latest state-of-art ML tools for processing RS data. Overall, the present work recommends that wetland sustainability requires a special land-use policy and relevant protocols, regulation, and/or legislation.",
        "DOI": "10.3390/rs14236104",
        "paper_author": "Jafarzadeh H.",
        "affiliation_name": "Memorial University of Newfoundland",
        "affiliation_city": "St John's",
        "affiliation_country": "Canada",
        "affiliation_id": "60019000",
        "affiliation_state": "NL"
    },
    {
        "paper_title": "Intelligent air defense task assignment based on hierarchical reinforcement learning",
        "publication": "Frontiers in Neurorobotics",
        "citied_by": "1",
        "cover_date": "2022-12-01",
        "Abstract": "Modern air defense battlefield situations are complex and varied, requiring high-speed computing capabilities and real-time situational processing for task assignment. Current methods struggle to balance the quality and speed of assignment strategies. This paper proposes a hierarchical reinforcement learning architecture for ground-to-air confrontation (HRL-GC) and an algorithm combining model predictive control with proximal policy optimization (MPC-PPO), which effectively combines the advantages of centralized and distributed approaches. To improve training efficiency while ensuring the quality of the final decision. In a large-scale area air defense scenario, this paper validates the effectiveness and superiority of the HRL-GC architecture and MPC-PPO algorithm, proving that the method can meet the needs of large-scale air defense task assignment in terms of quality and speed.",
        "DOI": "10.3389/fnbot.2022.1072887",
        "paper_author": "Liu J.Y.",
        "affiliation_name": "Air Force Engineering University China",
        "affiliation_city": "Xi'an",
        "affiliation_country": "China",
        "affiliation_id": "60069720",
        "affiliation_state": "Shaanxi"
    },
    {
        "paper_title": "Does media sentiment affect stock prices? Evidence from China’s STAR market",
        "publication": "Frontiers in Psychology",
        "citied_by": "4",
        "cover_date": "2022-12-01",
        "Abstract": "Objective: This paper explores the impact of media sentiment on stock prices on the Shanghai Stock Exchange Science and Technology Innovation Board (hereinafter the STAR market) from a behavioral finance perspective. Methods: We collect Baidu News coverage of STAR-listed firms as the text, and measure text sentiment using a machine learning-based text analysis technique. We then empirically examine the impact of media sentiment on STAR market stock prices from two aspects: IPO pricing efficiency and IPO first-day stock performance. Results: (1) Media sentiment has no significant impact on IPO pricing efficiency, thus suggesting that institutional investors participating in such offerings are generally not affected by media sentiment. (2) Optimistic media sentiment has a positive impact on IPO first-day returns, which indicates that individual investors are more easily influenced by media sentiment and therefore likely to abandon their rational judgment. (3) Media sentiment had a greater impact on IPO first-day returns during the COVID-19 pandemic than those before it, which suggests that individual investors are more influenced by media sentiment during pandemics. Discussion: Our findings deepen the understanding of stock price formation on the STAR market, which provide a statistical basis for formulating policy directions and investment strategies.",
        "DOI": "10.3389/fpsyg.2022.1040171",
        "paper_author": "Dong X.",
        "affiliation_name": "Jilin University",
        "affiliation_city": "Changchun",
        "affiliation_country": "China",
        "affiliation_id": "60007711",
        "affiliation_state": "Jilin"
    },
    {
        "paper_title": "Predicting the Forest Canopy Height from LiDAR and Multi-Sensor Data Using Machine Learning over India",
        "publication": "Remote Sensing",
        "citied_by": "19",
        "cover_date": "2022-12-01",
        "Abstract": "Forest canopy height estimates, at a regional scale, help understand the forest carbon storage, ecosystem processes, the development of forest management and the restoration policies to mitigate global climate change, etc. The recent availability of the NASA’s Global Ecosystem Dynamics Investigation (GEDI) LiDAR data has opened up new avenues to assess the plant canopy height at a footprint level. Here, we present a novel approach using the random forest (RF) for the wall-to-wall canopy height estimation over India’s forests (i.e., evergreen forest, deciduous forest, mixed forest, plantation, and shrubland) by employing the high-resolution top-of-the-atmosphere (TOA) reflectance and vegetation indices, the synthetic aperture radar (SAR) backscatters, the topography and tree canopy density, as the proxy variables. The variable importance plot indicated that the SAR backscatters, tree canopy density and the topography are the most influential height predictors. 33.15% of India’s forest cover demonstrated the canopy height <10 m, while 44.51% accounted for 10–20 m and 22.34% of forests demonstrated a higher canopy height (>20 m). This study advocates the importance and use of GEDI data for estimating the canopy height, preferably in data-deficit mountainous regions, where most of India’s natural forest vegetation exists.",
        "DOI": "10.3390/rs14235968",
        "paper_author": "Ghosh S.M.",
        "affiliation_name": "Solid World DAO",
        "affiliation_city": "Tallinn",
        "affiliation_country": "Estonia",
        "affiliation_id": "128965281",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A RoBERTa Approach for Automated Processing of Sustainability Reports",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "10",
        "cover_date": "2022-12-01",
        "Abstract": "There is a strong need and demand from the United Nations, public institutions, and the private sector for classifying government publications, policy briefs, academic literature, and corporate social responsibility reports according to their relevance to the Sustainable Development Goals (SDGs). It is well understood that the SDGs play a major role in the strategic objectives of various entities. However, linking projects and activities to the SDGs has not always been straightforward or possible with existing methodologies. Natural language processing (NLP) techniques offer a new avenue to identify linkages for SDGs from text data. This research examines various machine learning approaches optimized for NLP-based text classification tasks for their success in classifying reports according to their relevance to the SDGs. Extensive experiments have been performed with the recently released Open Source SDG (OSDG) Community Dataset, which contains texts with their related SDG label as validated by community volunteers. Results demonstrate that especially fine-tuned RoBERTa achieves very high performance in the attempted task, which is promising for automated processing of large collections of sustainability reports for detection of relevance to SDGs.",
        "DOI": "10.3390/su142316139",
        "paper_author": "Angin M.",
        "affiliation_name": "Koç Üniversitesi",
        "affiliation_city": "Istanbul",
        "affiliation_country": "Türkiye",
        "affiliation_id": "60006369",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Grid-Based Essential Urban Land Use Classification: A Data and Model Driven Mapping Framework in Xiamen City",
        "publication": "Remote Sensing",
        "citied_by": "5",
        "cover_date": "2022-12-01",
        "Abstract": "Accurate and timely mapping of essential urban land use categories (EULUC) is vital to understanding urban land use distribution, pattern, and composition. Recent advances in leveraging big open data and machine learning algorithms have demonstrated the possibility of large-scale mapping of EULUC in a new cost-effective way. However, they are still limited by the transferability of samples, models, and classification results across space, particularly across different cities. Given the heterogeneities of environmental and socioeconomic conditions among cities, in-depth studies of data and model adaptation towards city-specific EULUC mappings are highly required to support policy making, and urban renewal planning and management practices. In addition, the trending need for timely and detailed small land unit data processing with finer data granularity becomes increasingly important. We proposed a City Meta Unit (CMU) data model and classification framework driven by multisource data and artificial intelligence (AI) algorithms to address these challenges. The CMU Framework was innovatively applied to systematically set up a grid-based data model and classify urban land use with an improved AI algorithm by applying Moore neighborhood correlations. Specifically, we selected Xiamen, Fujian, in China, a coastal city, as the typical testbed to implement this proposed framework and apply an AI transfer learning technique for grid and parcel land-use study. Experimental results with our proposed CMU framework showed that the grid-based land use classification performance achieves overall accuracies of 81.17% and 76.55% for level I (major classes) and level II (minor classes), which is much higher than the parcel-based land use classification (overall accuracies of 72.37% for level I, and 68.99% for level II). We further investigated the relationship between training sample size and classification performance and quantified the contribution of different data sources to urban land use classifications. The CMU framework makes data collections and processing intelligent and efficient, with finer granularity, saving time and cost by using existing open social data. Incorporating the CMU framework with the proposed grid-based model is an effective and new approach for urban land use classification, which can be flexibly extended and applied to various cities.",
        "DOI": "10.3390/rs14236143",
        "paper_author": "Wang X.",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60025278",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Economic aspects of the detection of new strains in a multi-strain epidemiological–mathematical model",
        "publication": "Chaos, Solitons and Fractals",
        "citied_by": "15",
        "cover_date": "2022-12-01",
        "Abstract": "Mankind has struggled with pathogens throughout history. In this context, the contribution of vaccines to the continued economic and social prosperity of humanity is enormous, but it is constantly threatened by the development of vaccine-resistant strains of the pathogen. In this study, we investigate the usage of genomic sequencing tests to detect new strains of a pathogen in a multi-strain pandemic scenario using a mathematical–epidemiological–genomic–economic model. Our model provides a theoretical framework to explore the influence of an extensive number of pharmaceutical interventions in a dynamic multi-strain pandemic. Specifically, we show that while a genomic sequence testing policy can be both economically and epidemiologically efficient, a random sample of the population provides sub-optimal results. Moreover, we demonstrate that the optimal policy is sensitive to the social and economic settings of the population, and provide a machine learning based model that offers a solution to these challenges.",
        "DOI": "10.1016/j.chaos.2022.112823",
        "paper_author": "Shami L.",
        "affiliation_name": "Western Galilee College",
        "affiliation_city": "Acre",
        "affiliation_country": "Israel",
        "affiliation_id": "60080487",
        "affiliation_state": "HaZafon"
    },
    {
        "paper_title": "Effect of dockless bike-sharing scheme on the demand for London Cycle Hire at the disaggregate level using a deep learning approach",
        "publication": "Transportation Research Part A: Policy and Practice",
        "citied_by": "11",
        "cover_date": "2022-12-01",
        "Abstract": "To evaluate the dynamic effects of the dockless bike-sharing scheme on the demand of the London Cycle Hire (LCH) scheme at the station level, a novel bicycle demand prediction model is proposed using the deep learning approach, based on the transaction records at 645 docking stations of LCH in the period between July 2017 and March 2018. First, an intervention response module (IRM) is established to model the time-series trends of bicycle demands at individual LCH docking stations, with and without the dockless bike-sharing scheme. Then, the Graph Neural Networks (GNN) predictors are adopted to predict the demand for LCH, incorporating the learned effects from IRM. Results indicate that the proposed bicycle demand prediction model can achieve promising prediction performances, with higher R-squared (R2), lower Root Mean Squared Errors (RMSE) and lower Mean Absolute Errors (MAE), compared to conventional prediction models. More importantly, the proposed model can recognize the dynamic effects of the dockless bike-sharing scheme on the demand for LCH. For instance, there are possible spillover effects for the influence area of dockless bike-sharing scheme, especially for the neighboring areas that have well-integrated bicycle facilities (e.g., cycle lanes). In addition, the effect of dockless bike sharing on the demand for LCH can magnify over time. Moreover, influences on the demands on weekends are more remarkable than that on weekdays. Findings should improve the understanding on the interdependency between the demands of dockless and docked bike-sharing systems. This should shed light to the optimal management strategy for the docked bike-sharing system that can maximize the operational efficiency and cost-effectiveness.",
        "DOI": "10.1016/j.tra.2022.10.013",
        "paper_author": "Ding H.",
        "affiliation_name": "The Hong Kong Polytechnic University",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong",
        "affiliation_id": "60008928",
        "affiliation_state": "Hong Kong"
    },
    {
        "paper_title": "Energy Demand of the Road Transport Sector of Saudi Arabia—Application of a Causality-Based Machine Learning Model to Ensure Sustainable Environment",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "7",
        "cover_date": "2022-12-01",
        "Abstract": "The road transportation sector in Saudi Arabia has been observing a surging growth of demand trends for the last couple of decades. The main objective of this article is to extract insightful information for the country’s policymakers through a comprehensive investigation of the rising energy trends. In the first phase, it employs econometric analysis to provide the causal relationship between the energy demand of the road transportation sector and different socio-economic elements, including the gross domestic product (GDP), number of registered vehicles, total population, the population in the urban agglomeration, and fuel price. Then, it estimates future energy demand for the sector using two machine-learning models, i.e., artificial neural network (ANN) and support vector regression (SVR). The core features of the future demand model include: (i) removal of the linear trend, (ii) input data projection using a double exponential smoothing technique, and (iii) energy demand prediction using the machine learning models. The findings of the study show that the GDP and urban population have a significant causal relationship with energy demand in the road transportation sector in both the short and long run. The greenhouse gas emissions from the road transportation in Saudi Arabia are directly proportional to energy consumption because the demand is solely met by fossil fuels. Therefore, appropriate policy measures should be taken to reduce energy intensity without compromising the country’s development. In addition, the SVR model outperformed the ANN model in predicting the future energy demand of the sector based on the achieved performance indices. For instance, the correlation coefficients of the SVR and the ANN models were 0.8932 and 0.9925, respectively, for the test datasets. The results show that the SVR is better for predicting energy consumption than the ANN. It is expected that the findings of the study will assist the decision-makers of the country in achieving environmental sustainability goals by initiating appropriate policies.",
        "DOI": "10.3390/su142316064",
        "paper_author": "Rahman M.M.",
        "affiliation_name": "King Faisal University",
        "affiliation_city": "Al-Ahsa",
        "affiliation_country": "Saudi Arabia",
        "affiliation_id": "60030736",
        "affiliation_state": "Ash Sharqiyah"
    },
    {
        "paper_title": "A Hybrid Model for China’s Soybean Spot Price Prediction by Integrating CEEMDAN with Fuzzy Entropy Clustering and CNN-GRU-Attention",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "3",
        "cover_date": "2022-12-01",
        "Abstract": "China’s soybean spot price has historically been highly volatile due to the combined effects of long-term massive import dependence and intricate policies, as well as inherent environmental elements. The accurate prediction of the price is crucial for reducing the amount of soybean-linked risks worldwide and valuable for the long-term sustainability of global agriculture. Therefore, a hybrid prediction model that combines component clustering and a neural network with an attention mechanism has been developed. After fully integrated complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN) processing of the price series, the fuzzy entropy of each component is measured as the complexity characteristic. K-means clustering and reconstruction are applied to the components before being input to the CNN-GRU-Attention network for prediction to improve the model ability and adaptability of the sequences. In the empirical analysis, the proposed model outperforms other decomposition techniques and machine learning algorithms regarding prediction accuracy. After applying the decomposition part, the results have RMSE, MAPE, and MAE values of 49.59%, 22.58%, and 21.99% lower than those of the individual prediction part, respectively. This research presents a novel approach for market participants in the soybean industry for risk response. It gives a new perspective on agricultural product prices in sustainable agricultural marketing, while also providing practical tools for developing public policies and decision-making.",
        "DOI": "10.3390/su142315522",
        "paper_author": "Liu D.",
        "affiliation_name": "Fujian Agriculture and Forestry University",
        "affiliation_city": "Fuzhou",
        "affiliation_country": "China",
        "affiliation_id": "60004630",
        "affiliation_state": "Fujian"
    },
    {
        "paper_title": "The inequalities of different dimensions of visible street urban green space provision: A machine learning approach",
        "publication": "Land Use Policy",
        "citied_by": "21",
        "cover_date": "2022-12-01",
        "Abstract": "Awareness is growing that the uneven provision of street urban green space (UGS) may lead to environmental injustice. Most previous studies have focused on the over-head perspective of street UGS provision. However, only a few studies have evaluated the disparities in visible street UGS provision. While a plethora of studies have focused on a single dimension of visible UGS provision, no previous studies have developed a framework for systematically evaluating visible street UGS provision. This study therefore proposes a novel 4 ‘A′ framework, and aims to assess different dimensions (namely: availability; accessibility; attractiveness; and aesthetics) of visible street UGS provision, using Beijing as a case study. It investigates inequities in different dimensions of visible street UGS provision. In addition, it also explores the extent to which a neighbourhood's economic level is associated with different dimensions of visible street UGS. Our results show that, in Beijing, the four chosen dimensions of visible street UGS provision significantly differ in terms of spatial distribution and the association between them. Furthermore, we found that the value of the Gini index and Moran's I index for attractiveness and aesthetics are higher than those for availability and accessibility, which indicates a more unequal distribution of visible street UGS from a qualitative perspective. We also found that a community's economic level is positively associated with attractiveness and aesthetics, while no evidence was found to support the claim that the economic level of a community associated with availability and accessibility. This study suggests that visible street UGS provision is unequal; therefore, urban planning policy should pay more attention to disparities in visible street UGS provision, particularly in urban areas.",
        "DOI": "10.1016/j.landusepol.2022.106410",
        "paper_author": "Wang R.",
        "affiliation_name": "Queen's University Belfast",
        "affiliation_city": "Belfast",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60029738",
        "affiliation_state": "Northern Ireland"
    },
    {
        "paper_title": "Exploring the Nonlinear and Threshold Effects of Travel Distance on the Travel Mode Choice across Different Groups: An Empirical Study of Guiyang, China",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "8",
        "cover_date": "2022-12-01",
        "Abstract": "Examining how travel distance is associated with travel mode choice is essential for understanding traveler travel patterns and the potential mechanisms of behavioral changes. Although existing studies have explored the effect of travel distance on travel mode choice, most overlook their non-linear relationship and the heterogeneity between groups. In this study, the correlation between travel distance and travel mode choice is explored by applying the random forest model based on resident travel survey data in Guiyang, China. The results show that travel distance is far more important than other determinants for understanding the mechanism of travel mode choice. Travel distance contributes to 42.28% of explanation power for predicting travel mode choice and even 63.24% for walking. Significant nonlinear associations and threshold effects are found between travel distance and travel mode choice, and such nonlinear associations vary significantly across different socioeconomic groups. Policymakers are recommended to understand the group heterogeneity of travel mode choice behavior and to make targeted interventions for different groups with different travel distances. These results can provide beneficial guidance for optimizing the spatial layout of transportation infrastructure and improving the operational efficiency of low-carbon transportation systems.",
        "DOI": "10.3390/ijerph192316045",
        "paper_author": "He M.",
        "affiliation_name": "Kunming University of Science and Technology",
        "affiliation_city": "Kunming",
        "affiliation_country": "China",
        "affiliation_id": "60020675",
        "affiliation_state": "Yunnan"
    },
    {
        "paper_title": "Artificial Intelligence Implementation in Healthcare: A Theory-Based Scoping Review of Barriers and Facilitators",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "43",
        "cover_date": "2022-12-01",
        "Abstract": "There is a large proliferation of complex data-driven artificial intelligence (AI) applications in many aspects of our daily lives, but their implementation in healthcare is still limited. This scoping review takes a theoretical approach to examine the barriers and facilitators based on empirical data from existing implementations. We searched the major databases of relevant scientific publications for articles related to AI in clinical settings, published between 2015 and 2021. Based on the theoretical constructs of the Consolidated Framework for Implementation Research (CFIR), we used a deductive, followed by an inductive, approach to extract facilitators and barriers. After screening 2784 studies, 19 studies were included in this review. Most of the cited facilitators were related to engagement with and management of the implementation process, while the most cited barriers dealt with the intervention’s generalizability and interoperability with existing systems, as well as the inner settings’ data quality and availability. We noted per-study imbalances related to the reporting of the theoretic domains. Our findings suggest a greater need for implementation science expertise in AI implementation projects, to improve both the implementation process and the quality of scientific reporting.",
        "DOI": "10.3390/ijerph192316359",
        "paper_author": "Chomutare T.",
        "affiliation_name": "Norwegian Centre for Telemedicine",
        "affiliation_city": "Tromso",
        "affiliation_country": "Norway",
        "affiliation_id": "113532838",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Staggered-peak production is a mixed blessing in the control of particulate matter pollution",
        "publication": "npj Climate and Atmospheric Science",
        "citied_by": "1",
        "cover_date": "2022-12-01",
        "Abstract": "Staggered-peak production (SP)—a measure to halt industrial production in the heating season—has been implemented in North China Plain to alleviate air pollution. We compared the variations of PM1 composition in Beijing during the SP period in the 2016 heating season (SPhs) with those in the normal production (NP) periods during the 2015 heating season (NPhs) and 2016 non-heating season (NPnhs) to investigate the effectiveness of SP. The PM1 mass concentration decreased from 70.0 ± 54.4 μg m−3 in NPhs to 53.0 ± 56.4 μg m−3 in SPhs, with prominent reductions in primary emissions. However, the fraction of nitrate during SPhs (20.2%) was roughly twice that during NPhs (12.7%) despite a large decrease of NOx, suggesting an efficient transformation of NOx to nitrate during the SP period. This is consistent with the increase of oxygenated organic aerosol (OOA), which almost doubled from NPhs (22.5%) to SPhs (43.0%) in the total organic aerosol (OA) fraction, highlighting efficient secondary formation during SP. The PM1 loading was similar between SPhs (53.0 ± 56.4 μg m−3) and NPnhs (50.7 ± 49.4 μg m−3), indicating a smaller difference in PM pollution between heating and non-heating seasons after the implementation of the SP measure. In addition, a machine learning technique was used to decouple the impact of meteorology on air pollutants. The deweathered results were comparable with the observed results, indicating that meteorological conditions did not have a large impact on the comparison results. Our study indicates that the SP policy is effective in reducing primary emissions but promotes the formation of secondary species.",
        "DOI": "10.1038/s41612-022-00322-x",
        "paper_author": "Wang Y.",
        "affiliation_name": "Institute of Earth Environment",
        "affiliation_city": "Xi'an",
        "affiliation_country": "China",
        "affiliation_id": "60273071",
        "affiliation_state": "Shaanxi"
    },
    {
        "paper_title": "Headwater streams and inland wetlands: Status and advancements of geospatial datasets and maps across the United States",
        "publication": "Earth-Science Reviews",
        "citied_by": "10",
        "cover_date": "2022-12-01",
        "Abstract": "Headwater streams and inland wetlands provide essential functions that support healthy watersheds and downstream waters. However, scientists and aquatic resource managers lack a comprehensive synthesis of national and state stream and wetland geospatial datasets and emerging technologies that can further improve these data. We conducted a review of existing United States (US) federal and state stream and wetland geospatial datasets, focusing on their spatial extent, permanence classifications, and current limitations. We also examined recent peer-reviewed literature for emerging methods that can potentially improve the estimation, representation, and integration of stream and wetland datasets. We found that federal and state datasets rely heavily on the US Geological Survey's National Hydrography Dataset for stream extent and duration information. Only eleven states (22%) had additional stream extent information and seven states (14%) provided additional duration information. Likewise, federal and state wetland datasets primarily use the US Fish and Wildlife Service's National Wetlands Inventory (NWI) Geospatial Dataset, with only two states using non-NWI datasets. Our synthesis revealed that LiDAR-based technologies hold promise for advancing stream and wetland mapping at limited spatial extents. While machine learning techniques may help to scale-up these LiDAR-derived estimates, challenges related to preprocessing and data workflows remain. High-resolution commercial imagery, supported by public imagery and cloud computing, may further aid characterization of the spatial and temporal dynamics of streams and wetlands, especially using multi-platform and multi-temporal machine learning approaches. Models integrating both stream and wetland dynamics are limited, and field-based efforts must remain a key component in developing improved headwater stream and wetland datasets. Continued financial and partnership support of existing databases is also needed to enhance mapping and inform water resources research and policy decisions.",
        "DOI": "10.1016/j.earscirev.2022.104230",
        "paper_author": "Christensen J.R.",
        "affiliation_name": "United States Environmental Protection Agency",
        "affiliation_city": "Washington, D.C.",
        "affiliation_country": "United States",
        "affiliation_id": "60021439",
        "affiliation_state": "DC"
    },
    {
        "paper_title": "Identifying Evacuation Needs and Resources Based on Volunteered Geographic Information: A Case of the Rainstorm in July 2021, Zhengzhou, China",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "5",
        "cover_date": "2022-12-01",
        "Abstract": "Recently, global climate change has led to a high incidence of extreme weather and natural disasters. How to reduce its impact has become an important topic. However, the studies that both consider the disaster’s real-time geographic information and environmental factors in severe rainstorms are still not enough. Volunteered geographic information (VGI) data that was generated during disasters offered possibilities for improving the emergency management abilities of decision-makers and the disaster self-rescue abilities of citizens. Through the case study of the extreme rainstorm disaster in Zhengzhou, China, in July 2021, this paper used machine learning to study VGI issued by residents. The vulnerable people and their demands were identified based on the SOS messages. The importance of various indicators was analyzed by combining open data from socio-economic and built-up environment elements. Potential safe areas with shelter resources in five administrative districts in the disaster-prone central area of Zhengzhou were identified based on these data. This study found that VGI can be a reliable data source for future disaster research. The characteristics of rainstorm hazards were concluded from the perspective of affected people and environmental indicators. The policy recommendations for disaster prevention in the context of public participation were also proposed.",
        "DOI": "10.3390/ijerph192316051",
        "paper_author": "Gao J.",
        "affiliation_name": "Tohoku University",
        "affiliation_city": "Sendai",
        "affiliation_country": "Japan",
        "affiliation_id": "60008435",
        "affiliation_state": "Miyagi"
    },
    {
        "paper_title": "A Convergent Algorithm for Equilibrium Problem to Predict Prospective Mathematics Teachers’ Technology Integrated Competency",
        "publication": "Mathematics",
        "citied_by": "0",
        "cover_date": "2022-12-01",
        "Abstract": "Educational data classification has become an effective tool for exploring the hidden pattern or relationship in educational data and predicting students’ performance or teachers’ competency. This study proposes a new method based on machine learning algorithms to predict the technology-integrated competency of pre-service mathematics teachers. In this paper, we modified the inertial subgradient extragradient algorithm for pseudomonotone equilibrium and proved the weak convergence theorem under some suitable conditions in Hilbert spaces. We then applied to solve data classification by extreme learning machine using the dataset comprised of the technology-integrated competency of 954 pre-service mathematics teachers in a university in northern Thailand, longitudinally collected for five years. The flexibility of our algorithm was shown by comparisons of the choice of different parameters. The performance was calculated and compared with the existing algorithms to be implemented for prediction. The results show that the proposed method achieved a classification accuracy of 81.06%. The predictions were implemented using ten attributes, including demographic information, skills, and knowledge relating to technology developed throughout the teacher education program. Such data driven studies are significant for establishing a prospective teacher competency analysis framework in teacher education and contributing to decision-making for policy design.",
        "DOI": "10.3390/math10234464",
        "paper_author": "Jun-on N.",
        "affiliation_name": "Lampang Rajabhat University",
        "affiliation_city": "Lampang",
        "affiliation_country": "Thailand",
        "affiliation_id": "60194934",
        "affiliation_state": "Lampang"
    },
    {
        "paper_title": "Incorporating Clustering Modification Directions into Reinforcement Learning Based Cost Learning Framework",
        "publication": "Journal of Information Hiding and Multimedia Signal Processing",
        "citied_by": "0",
        "cover_date": "2022-12-01",
        "Abstract": "Content-adaptive image steganography embedding cost learning frameworks based on deep learning can generate a more exquisite embedding probability map within a short time, and such methods have reached remarkable security performance compared to conventional hand-craft based methods and received increasing attention in recent years. However, existing Reinforcement Learning (RL)-based schemes are typically based on single-step state machine, making it difficult for further improvement. This paper ex-tends the existing RL-based framework into two steps to enhance the simulated stego images from policy network to improve the performance, that is, during the training pro-cess, similar to the conventional methods, a module will be added after the policy network, the current embedding direction is adjusted according to the sign of modification directions of the neighborhood. The experimental results show that the proposed module not only improve the performance during the training process, but also enhance the actual security performance compared with single-step based frameworks when countering mul-tiple steganalyzers.",
        "DOI": "NA",
        "paper_author": "Cui J.",
        "affiliation_name": "Harbin Electric Power Vocational Technology College",
        "affiliation_city": "Harbin",
        "affiliation_country": "China",
        "affiliation_id": "112827531",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Early Detection and Control of the Next Epidemic Wave Using Health Communications: Development of an Artificial Intelligence-Based Tool and Its Validation on COVID-19 Data from the US",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "9",
        "cover_date": "2022-12-01",
        "Abstract": "Social media networks highly influence on a broad range of global social life, especially in the context of a pandemic. We developed a mathematical model with a computational tool, called EMIT (Epidemic and Media Impact Tool), to detect and control pandemic waves, using mainly topics of relevance on social media networks and pandemic spread. Using EMIT, we analyzed health-related communications on social media networks for early prediction, detection, and control of an outbreak. EMIT is an artificial intelligence-based tool supporting health communication and policy makers decisions. Thus, EMIT, based on historical data, social media trends and disease spread, offers an predictive estimation of the influence of public health interventions such as social media-based communication campaigns. We have validated the EMIT mathematical model on real world data combining COVID-19 pandemic data in the US and social media data from Twitter. EMIT demonstrated a high level of performance in predicting the next epidemiological wave (AUC = 0.909, F1 = 0.899).",
        "DOI": "10.3390/ijerph192316023",
        "paper_author": "Lazebnik T.",
        "affiliation_name": "University College London",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60022148",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Could Religiosity and Religion Influence the Tax Morale of Individuals? An Empirical Analysis Based on Variable Selection Methods",
        "publication": "Mathematics",
        "citied_by": "2",
        "cover_date": "2022-12-01",
        "Abstract": "When people who adhere to tax morality act in a situation where there is no sense of risk, no acceptance of the government, or no environment conducive to tax compliance, it is easier to see how they are motivated to do so. Tax morality is also known as the ethics of compliance. It is the independent cause that motivates a positive tax behaviour. Employees’ religious beliefs may impact their ideas and actions in organizational life, just as individuals’ attitudes, values, emotions, abilities, and behaviours influence their thoughts and actions at work. Religion can positively influence a worker’s loyalty, morale, and communication. In this context, the research seeks to determine whether religiosity and religion may have an effect on tax morale, examining whether an individual’s religiosity reduces tax evasion and increases the degree of tax morale. Using machine learning variable selection techniques appropriate for categorical variables, we have used the dataset of the Joint EVS/WVS 2017-2020 (European Value Survey/World Value Survey), allowing for comparisons of tax morality in more than 79 nations globally (chi-squared and mutual information). The empirical findings showed that the most important aspects of religiosity, such as religious denomination, belief in God, and the significance of God, along with the degree of trust placed in other religions and churches, have a considerable positive impact on the level of tax morale. Another significant conclusion relates to how much people feel the government is responsible, how much they care about their nation, and how satisfied they are with the political system—findings that have been shown to boost employee morale. The following are a person’s primary traits that indicate their financial morale: an adult above the age of 25, a full-time worker or retired person, married, and living alone. Therefore, employees that are morally upright, trustworthy, diligent, and committed to the workplace values of justice and decency raise morale generally and improve an organisation’s success. A business may enhance its reputation and help to secure its long-term success by establishing behavioural policies.",
        "DOI": "10.3390/math10234497",
        "paper_author": "Davidescu A.A.M.",
        "affiliation_name": "Bucharest University of Economic Studies",
        "affiliation_city": "Bucharest",
        "affiliation_country": "Romania",
        "affiliation_id": "60107810",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Towards Sustainable Fuel Cells and Batteries with an AI Perspective",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "27",
        "cover_date": "2022-12-01",
        "Abstract": "With growing environmental and ecological concerns, innovative energy storage systems are urgently required to develop smart grids and electric vehicles (EVs). Since their invention in the 1970s, rechargeable lithium-ion batteries (LIBs) have risen as a revolutionary innovation due to their superior benefits of high operating potential and energy density. Similarly, fuel cells, especially Proton Exchange Membrane Fuel Cells (PEMFC) and Solid-Oxide Fuel Cells (SOFC), have been developed as an energy storage system for EVs due to their compactness and high-temperature stability, respectively. Various attempts have been made to explore novel materials to enhance existing energy storage technologies. Materials design and development are significantly based on trial-and-error techniques and require substantial human effort and time. Additionally, researchers work on individual materials for specific applications. As a viewpoint, we present the available sustainable routes for electrochemical energy storage, highlighting the use of (i) green materials and processes, (ii) renewables, (iii) the circular economy approach, (iv) regulatory policies, and (v) the data driven approach to find the best materials from several databases with minimal human involvement and time. Finally, we provide an example of a high throughput and machine learning assisted approach for optimizing the properties of several sustainable carbon materials and applying them to energy storage devices. This study can prompt researchers to think, advance, and develop opportunities for future sustainable materials selection, optimization, and application in various electrochemical energy devices utilizing ML.",
        "DOI": "10.3390/su142316001",
        "paper_author": "Ramasubramanian B.",
        "affiliation_name": "National University of Singapore",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore",
        "affiliation_id": "60017161",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Occupations on the map: Using a super learner algorithm to downscale labor statistics",
        "publication": "PLoS ONE",
        "citied_by": "1",
        "cover_date": "2022-12-01",
        "Abstract": "Detailed and accurate labor statistics are fundamental to support social policies that aim to improve the match between labor supply and demand, and support the creation of jobs. Despite overwhelming evidence that labor activities are distributed unevenly across space, detailed statistics on the geographical distribution of labor and work are not readily available. To fill this gap, we demonstrated an approach to create fine-scale gridded occupation maps by means of downscaling district-level labor statistics, informed by remote sensing and other spatial information. We applied a super-learner algorithm that combined the results of different machine learning models to predict the shares of six major occupation categories and the labor force participation rate at a resolution of 30 arc seconds (~1x1 km) in Vietnam. The results were subsequently combined with gridded information on the working-age population to produce maps of the number of workers per occupation. The super learners outperformed (n = 6) or had similar (n = 1) accuracy in comparison to best-performing single machine learning algorithms. A comparison with an independent high-resolution wealth index showed that the shares of the four low-skilled occupation categories (91% of the labor force), were able to explain between 28% and 43% of the spatial variation in wealth in Vietnam, pointing at a strong spatial relationship between work, income and wealth. The proposed approach can also be applied to produce maps of other (labor) statistics, which are only available at aggregated levels.",
        "DOI": "10.1371/journal.pone.0278120",
        "paper_author": "van Dijk M.",
        "affiliation_name": "Wageningen University &amp; Research",
        "affiliation_city": "Wageningen",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60004156",
        "affiliation_state": "Gelderland"
    },
    {
        "paper_title": "Multi-Objective Optimization of Sugarcane Milling System Operations Based on a Deep Data-Driven Model",
        "publication": "Foods",
        "citied_by": "2",
        "cover_date": "2022-12-01",
        "Abstract": "The extraction of sugarcane juice is the first step of sugar production. The optimal values of process indicators and the set values of operating parameters in this process are still determined by workers’ experience, preventing adaptive adjustment of the production process. To address this issue, a multi-objective optimization framework based on a deep data-driven model is proposed to optimize the operation of sugarcane milling systems. First, the sugarcane milling process is abstracted as the interaction of material flow, energy flow, and information flow (MF–EF–IF) by introducing synergetic theory, and each flow’s order parameters and state parameters are obtained. Subsequently, the state parameters of the subsystems are taken as inputs, and the order parameters—including the grinding capacity, electric consumption per ton of sugarcane, and sucrose extraction—are produced as outputs. A collaborative optimization model of the MF–EF–IF of the milling system is established by using a deep kernel extreme learning machine (DK-ELM). The established milling system model is applied for an improved multi-objective chicken swarm optimization (IMOCSO) algorithm to obtain the optimal values of the order parameters. Finally, the milling process is described as a Markov decision process (MDP) with the optimal values of the order parameters as the control objectives, and an improved deep deterministic policy gradient (DDPG) algorithm is employed to achieve the adaptive optimization of the operating parameters under different working conditions of the milling system. Computational experiments indicate that enhanced performance is achieved, with an increase of 3.2 t per hour in grinding capacity, a reduction of 660 W per ton in sugarcane electric consumption, and an increase of 0.03% in the sucrose extraction.",
        "DOI": "10.3390/foods11233845",
        "paper_author": "Li Z.",
        "affiliation_name": "Guangxi University",
        "affiliation_city": "Nanning",
        "affiliation_country": "China",
        "affiliation_id": "60030270",
        "affiliation_state": "Guangxi"
    },
    {
        "paper_title": "Estimation of Vehicle Longitudinal Velocity with Artificial Neural Network",
        "publication": "Sensors",
        "citied_by": "18",
        "cover_date": "2022-12-01",
        "Abstract": "Vehicle dynamics control systems have a fundamental role in smart and autonomous mobility, where one of the most crucial aspects is the vehicle body velocity estimation. In this paper, the problem of a correct evaluation of the vehicle longitudinal velocity for dynamic control applications is approached using a neural networks technique employing a set of measured samples referring to signals usually available on-board, such as longitudinal and lateral acceleration, steering angle, yaw rate and linear wheel speed. Experiments were run on four professional driving circuits with very different characteristics, and the vehicle longitudinal velocity was estimated with different neural network training policies and validated through comparison with the measurements of the one acquired at the vehicle’s center of gravity, provided by an optical Correvit sensor, which serves as the reference (and, therefore, exact) velocity values. The results obtained with the proposed methodology are in good agreement with the reference values in almost all tested conditions, covering both the linear and the nonlinear behavior of the car, proving that artificial neural networks can be efficiently employed onboard, thereby enriching the standard set of control and safety-related electronics.",
        "DOI": "10.3390/s22239516",
        "paper_author": "Napolitano Dell’Annunziata G.",
        "affiliation_name": "Università degli Studi di Napoli Federico II",
        "affiliation_city": "Naples",
        "affiliation_country": "Italy",
        "affiliation_id": "60017293",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "China’s Gridded Manufacturing Dataset",
        "publication": "Scientific Data",
        "citied_by": "4",
        "cover_date": "2022-12-01",
        "Abstract": "The growth of the manufacturing industry is the engine of rapid economic growth in developing regions. Characterizing the geographical distribution of manufacturing firms is critically important for scientists and policymakers. However, data on the manufacturing industry used in previous studies either have a low spatial resolution (or fuzzy classification) or high-resolution information is lacking. Here, we propose a map point-of-interest classification method based on machine learning technology and build a dataset of the distribution of Chinese manufacturing firms called the Gridded Manufacturing Dataset. This dataset includes the number and type of manufacturing firms at a 0.01° latitude by 0.01° longitude scale. It includes all manufacturing firms (classified into seven categories) in China in 2015 (4.56 million) and 2019 (6.19 million). This dataset can be used to characterize temporal and spatial patterns in the distribution of manufacturing firms as well as reveal the mechanisms underlying the development of the manufacturing industry and changes in regional economic policies.",
        "DOI": "10.1038/s41597-022-01848-8",
        "paper_author": "Fan C.",
        "affiliation_name": "Nanjing Forestry University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60025665",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "Addressing people’s current and future states in a reinforcement learning algorithm for persuading to quit smoking and to be physically active",
        "publication": "PLoS ONE",
        "citied_by": "13",
        "cover_date": "2022-12-01",
        "Abstract": "Behavior change applications often assign their users activities such as tracking the number of smoked cigarettes or planning a running route. To help a user complete these activities, an application can persuade them in many ways. For example, it may help the user create a plan or mention the experience of peers. Intuitively, the application should thereby pick the message that is most likely to be motivating. In the simplest case, this could be the message that has been most effective in the past. However, one could consider several other elements in an algorithm to choose a message. Possible elements include the user’s current state (e.g., self-efficacy), the user’s future state after reading a message, and the user’s similarity to the users on which data has been gathered. To test the added value of subsequently incorporating these elements into an algorithm that selects persuasive messages, we conducted an experiment in which more than 500 people in four conditions interacted with a text-based virtual coach. The experiment consisted of five sessions, in each of which participants were suggested a preparatory activity for quitting smoking or increasing physical activity together with a persuasive message. Our findings suggest that adding more elements to the algorithm is effective, especially in later sessions and for people who thought the activities were useful. Moreover, while we found some support for transferring knowledge between the two activity types, there was rather low agreement between the optimal policies computed separately for the two activity types. This suggests limited policy generalizability between activities for quitting smoking and those for increasing physical activity. We see our results as supporting the idea of constructing more complex persuasion algorithms. Our dataset on 2,366 persuasive messages sent to 671 people is published together with this article for researchers to build on our algorithm.",
        "DOI": "10.1371/journal.pone.0277295",
        "paper_author": "Albers N.",
        "affiliation_name": "Delft University of Technology",
        "affiliation_city": "Delft",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60006288",
        "affiliation_state": "Zuid-Holland"
    },
    {
        "paper_title": "Earth Observation and Artificial Intelligence: Understanding emerging ethical issues and opportunities",
        "publication": "IEEE Geoscience and Remote Sensing Magazine",
        "citied_by": "26",
        "cover_date": "2022-12-01",
        "Abstract": "Ethics is a central and growing concern in all applications utilizing artificial intelligence (AI). Earth observation (EO) and remote sensing (RS) research relies heavily on both big data and AI or machine learning (ML). While this reliance is not new, with increasing image resolutions and the growing number of EO/RS use cases that have a direct impact on governance, policy, and the lives of people, ethical issues are taking center stage. In this article, we provide scientists engaged with AI for EO (AI4EO) research, 1) a practically useful overview of the key ethical issues emerging in this field, with concrete examples from within EO/RS to explain these issues, and 2) a first road map (flowchart) that scientists can use to identify ethical issues in their ongoing research. With this, we aim to sensitize scientists to these issues and create a bridge to facilitate constructive and regular communication among scientists engaged in AI4EO research, on the one hand, and ethics research, on the other hand. The article also provides detailed illustrations from four AI4EO research fields to explain how scientists can redesign research questions to more effectively grab ethical opportunities to address real-world problems that are otherwise akin to ethical dilemmas with no win-win solution in sight. The article concludes by providing recommendations to institutions that want to support ethically mindful AI4EO research and provides suggestions for future research in this field.",
        "DOI": "10.1109/MGRS.2022.3208357",
        "paper_author": "Kochupillai M.",
        "affiliation_name": "Technische Universität München",
        "affiliation_city": "Munich",
        "affiliation_country": "Germany",
        "affiliation_id": "60019722",
        "affiliation_state": "Bayern"
    },
    {
        "paper_title": "On the development of an information system for monitoring user opinion and its role for the public",
        "publication": "Journal of Big Data",
        "citied_by": "4",
        "cover_date": "2022-12-01",
        "Abstract": "Social media services and analytics platforms are rapidly growing. A large number of various events happen mostly every day, and the role of social media monitoring tools is also increasing. Social networks are widely used for managing and promoting brands and different services. Thus, most popular social analytics platforms aim for business purposes while monitoring various social, economic, and political problems remains underrepresented and not covered by thorough research. Moreover, most of them focus on resource-rich languages such as the English language, whereas texts and comments in other low-resource languages, such as the Russian and Kazakh languages in social media, are not represented well enough. So, this work is devoted to developing and applying the information system called the OMSystem for analyzing users’ opinions on news portals, blogs, and social networks in Kazakhstan. The system uses sentiment dictionaries of the Russian and Kazakh languages and machine learning algorithms to determine the sentiment of social media texts. The whole structure and functionalities of the system are also presented. The experimental part is devoted to building machine learning models for sentiment analysis on the Russian and Kazakh datasets. Then the performance of the models is evaluated with accuracy, precision, recall, and F1-score metrics. The models with the highest scores are selected for implementation in the OMSystem. Then the OMSystem’s social analytics module is used to thoroughly analyze the healthcare, political and social aspects of the most relevant topics connected with the vaccination against the coronavirus disease. The analysis allowed us to discover the public social mood in the cities of Almaty and Nur-Sultan and other large regional cities of Kazakhstan. The system’s study included two extensive periods: 10-01-2021 to 30-05-2021 and 01-07-2021 to 12-08-2021. In the obtained results, people’s moods and attitudes to the Government’s policies and actions were studied by such social network indicators as the level of topic discussion activity in society, the level of interest in the topic in society, and the mood level of society. These indicators calculated by the OMSystem allowed careful identification of alarming factors of the public (negative attitude to the government regulations, vaccination policies, trust in vaccination, etc.) and assessment of the social mood.",
        "DOI": "10.1186/s40537-022-00660-w",
        "paper_author": "Karyukin V.",
        "affiliation_name": "Al Farabi Kazakh National University",
        "affiliation_city": "Almaty",
        "affiliation_country": "Kazakhstan",
        "affiliation_id": "60071847",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Personalized prediction of optimal water intake in adult population by blended use of machine learning and clinical data",
        "publication": "Scientific Reports",
        "citied_by": "0",
        "cover_date": "2022-12-01",
        "Abstract": "Growing evidence suggests that sustained concentrated urine contributes to chronic metabolic and kidney diseases. Recent results indicate that a daily urinary concentration of 500 mOsm/kg reflects optimal hydration. This study aims at providing personalized advice for daily water intake considering personal intrinsic (age, sex, height, weight) and extrinsic (food and fluid intakes) characteristics to achieve a target urine osmolality (UOsm) of 500 mOsm/kg using machine learning and optimization algorithms. Data from clinical trials on hydration (four randomized and three non-randomized trials) were analyzed. Several machine learning methods were tested to predict UOsm. The predictive performance of the developed algorithm was evaluated against current dietary guidelines. Features linked to urine production and fluid consumption were listed among the most important features with relative importance values ranging from 0.10 to 0.95. XGBoost appeared the most performing approach (Mean Absolute Error (MAE) = 124.99) to predict UOsm. The developed algorithm exhibited the highest overall correct classification rate (85.5%) versus that of dietary guidelines (77.8%). This machine learning application provides personalized advice for daily water intake to achieve optimal hydration and may be considered as a primary prevention tool to counteract the increased incidence of chronic metabolic and kidney diseases.",
        "DOI": "10.1038/s41598-022-21869-y",
        "paper_author": "Dolci A.",
        "affiliation_name": "Health",
        "affiliation_city": "Palaiseau",
        "affiliation_country": "France",
        "affiliation_id": "126243573",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "ETCNN: Extra Tree and Convolutional Neural Network-based Ensemble Model for COVID-19 Tweets Sentiment Classification: ETCNN: COVID-19 Tweets Sentiment Classification",
        "publication": "Pattern Recognition Letters",
        "citied_by": "29",
        "cover_date": "2022-12-01",
        "Abstract": "Pandemics influence people negatively and people experience fear and disappointment. With the global outspread of COVID-19, the sentiments of the general public are substantially influenced, and analyzing their sentiments could help to devise corresponding policies to alleviate negative sentiments. Often the data collected from social media platforms is unstructured leading to low classification accuracy. This study brings forward an ensemble model where the benefits of handcrafted features and automatic feature extraction are combined by machine learning and deep learning models. Unstructured data is obtained, preprocessed, and annotated using TextBlob and VADER before training machine learning models. Similarly, the efficiency of Word2Vec, TF, and TF-IDF features is also analyzed. Results reveal the better performance of the extra tree classifier when trained with TF-IDF features from TextBlob annotated data. Overall, machine learning models perform better with TF-IDF and TextBlob. The proposed model obtains superior performance using both annotation techniques with 0.97 and 0.95 scores of accuracy using TextBlob and VADER respectively with Word2Vec features. Results reveal that use of machine learning and deep learning models together with a voting criterion tends to yield better results than other machine learning models. Analysis of sentiments indicates that predominantly people possess negative sentiments regarding COVID-19.",
        "DOI": "10.1016/j.patrec.2022.11.012",
        "paper_author": "Umer M.",
        "affiliation_name": "The Islamia University of Bahawalpur",
        "affiliation_city": "Bahawalpur",
        "affiliation_country": "Pakistan",
        "affiliation_id": "60037241",
        "affiliation_state": "Punjab"
    },
    {
        "paper_title": "An interpretable RL framework for pre-deployment modeling in ICU hypotension management",
        "publication": "npj Digital Medicine",
        "citied_by": "5",
        "cover_date": "2022-12-01",
        "Abstract": "Computational methods from reinforcement learning have shown promise in inferring treatment strategies for hypotension management and other clinical decision-making challenges. Unfortunately, the resulting models are often difficult for clinicians to interpret, making clinical inspection and validation of these computationally derived strategies challenging in advance of deployment. In this work, we develop a general framework for identifying succinct sets of clinical contexts in which clinicians make very different treatment choices, tracing the effects of those choices, and inferring a set of recommendations for those specific contexts. By focusing on these few key decision points, our framework produces succinct, interpretable treatment strategies that can each be easily visualized and verified by clinical experts. This interrogation process allows clinicians to leverage the model’s use of historical data in tandem with their own expertise to determine which recommendations are worth investigating further e.g. at the bedside. We demonstrate the value of this approach via application to hypotension management in the ICU, an area with critical implications for patient outcomes that lacks data-driven individualized treatment strategies; that said, our framework has broad implications on how to use computational methods to assist with decision-making challenges on a wide range of clinical domains.",
        "DOI": "10.1038/s41746-022-00708-4",
        "paper_author": "Zhang K.",
        "affiliation_name": "Harvard University",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States",
        "affiliation_id": "60009982",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Ischemic stroke of unclear aetiology: a case-by-case analysis and call for a multi-professional predictive, preventive and personalised approach",
        "publication": "EPMA Journal",
        "citied_by": "32",
        "cover_date": "2022-12-01",
        "Abstract": "Due to the reactive medical approach applied to disease management, stroke has reached an epidemic scale worldwide. In 2019, the global stroke prevalence was 101.5 million people, wherefrom 77.2 million (about 76%) suffered from ischemic stroke; 20.7 and 8.4 million suffered from intracerebral and subarachnoid haemorrhage, respectively. Globally in the year 2019 — 3.3, 2.9 and 0.4 million individuals died of ischemic stroke, intracerebral and subarachnoid haemorrhage, respectively. During the last three decades, the absolute number of cases increased substantially. The current prevalence of stroke is 110 million patients worldwide with more than 60% below the age of 70 years. Prognoses by the World Stroke Organisation are pessimistic: globally, it is predicted that 1 in 4 adults over the age of 25 will suffer stroke in their lifetime. Although age is the best known contributing factor, over 16% of all strokes occur in teenagers and young adults aged 15–49 years and the incidence trend in this population is increasing. The corresponding socio-economic burden of stroke, which is the leading cause of disability, is enormous. Global costs of stroke are estimated at 721 billion US dollars, which is 0.66% of the global GDP. Clinically manifested strokes are only the “tip of the iceberg”: it is estimated that the total number of stroke patients is about 14 times greater than the currently applied reactive medical approach is capable to identify and manage. Specifically, lacunar stroke (LS), which is characteristic for silent brain infarction, represents up to 30% of all ischemic strokes. Silent LS, which is diagnosed mainly by routine health check-up and autopsy in individuals without stroke history, has a reported prevalence of silent brain infarction up to 55% in the investigated populations. To this end, silent brain infarction is an independent predictor of ischemic stroke. Further, small vessel disease and silent lacunar brain infarction are considered strong contributors to cognitive impairments, dementia, depression and suicide, amongst others in the general population. In sub-populations such as diabetes mellitus type 2, proliferative diabetic retinopathy is an independent predictor of ischemic stroke. According to various statistical sources, cryptogenic strokes account for 15 to 40% of the entire stroke incidence. The question to consider here is, whether a cryptogenic stroke is fully referable to unidentifiable aetiology or rather to underestimated risks. Considering the latter, translational research might be of great clinical utility to realise innovative predictive and preventive approaches, potentially benefiting high risk individuals and society at large. In this position paper, the consortium has combined multi-professional expertise to provide clear statements towards the paradigm change from reactive to predictive, preventive and personalised medicine in stroke management, the crucial elements of which are:Consolidation of multi-disciplinary expertise including family medicine, predictive and in-depth diagnostics followed by the targeted primary and secondary (e.g. treated cancer) prevention of silent brain infarctionApplication of the health risk assessment focused on sub-optimal health conditions to effectively prevent health-to-disease transitionApplication of AI in medicine, machine learning and treatment algorithms tailored to robust biomarker patternsApplication of innovative screening programmes which adequately consider the needs of young populations.",
        "DOI": "10.1007/s13167-022-00307-z",
        "paper_author": "Golubnitschaja O.",
        "affiliation_name": "Universitätsklinikum Bonn",
        "affiliation_city": "Bonn",
        "affiliation_country": "Germany",
        "affiliation_id": "60030465",
        "affiliation_state": "Nordrhein-Westfalen"
    },
    {
        "paper_title": "Understanding transit ridership in an equity context through a comparison of statistical and machine learning algorithms",
        "publication": "Journal of Transport Geography",
        "citied_by": "12",
        "cover_date": "2022-12-01",
        "Abstract": "Building an accurate model of travel behaviour based on individuals’ characteristics and built environment attributes is of importance for policy-making and transportation planning. Recent experiments with big data and Machine Learning (ML) algorithms toward a better travel behaviour analysis have mainly overlooked socially disadvantaged groups. Accordingly, in this study, we explore the travel behaviour responses of low-income individuals to transit investments in Greater Toronto and Hamilton Area, Canada, using statistical and ML models. We first investigate how the model choice affects the prediction of transit use by the low-income group. This step includes comparing the predictive performance of traditional and ML algorithms and then evaluating a transit investment policy by contrasting the predicted activities and the spatial distribution of transit trips generated by vulnerable households after improving accessibility. We also empirically investigate the proposed transit investment by each algorithm and compare it with the city of Brampton's future transportation plan. While, unsurprisingly, the ML algorithms outperform classical models, there are still doubts about using them due to interpretability concerns. Hence, we adopt recent local and global model-agnostic interpretation tools to interpret how the model arrives at its predictions. Our findings reveal the great potential of ML algorithms for enhanced travel behaviour predictions for low-income strata without considerably sacrificing interpretability.",
        "DOI": "10.1016/j.jtrangeo.2022.103482",
        "paper_author": "Yousefzadeh Barri E.",
        "affiliation_name": "University of Toronto",
        "affiliation_city": "Toronto",
        "affiliation_country": "Canada",
        "affiliation_id": "60016849",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "Intelligent collision risk detection in medium-sized cities of developing countries, using naturalistic driving: A review",
        "publication": "Journal of Traffic and Transportation Engineering (English Edition)",
        "citied_by": "3",
        "cover_date": "2022-12-01",
        "Abstract": "Traffic accidents are one of the most serious problems worldwide, being one of the leading causes of death and economic loss in the world. Low- and middle-income countries, mainly their medium-sized cities, are among the most affected by this problem. 93% of traffic accidents occur in low and middle-income countries, even though these countries have approximately 60% of the world's vehicles. This occurs mainly because in these types of countries, especially in medium-sized cities (target context), there are no ideal conditions for driving, such as adequate road infrastructure, good condition of vehicles, and rigorous safety policies. Advanced data analysis techniques including machine learning (ML) have increasingly been used to solve this problem. Naturalistic driving (ND) can be applied as a data collection method that provides information on traffic accidents. ND commonly uses a vehicle's kinematic data to detect high-risk driving behaviors that could cause an accident. The objectives of this document are to present a review of different alternatives that help in data collection and creation of intelligent solutions related to detection of possible traffic accidents, principally using ND; and to propose an intelligent collision risk detection system (ICRDS) for identification of areas with a high probability of TA in the target context. Through the review, it was possible to analyze and evaluate the devices, variables and algorithms that help characterize a risk event in driving, considering the target context. The development of a prototype of an ICRDS for a medium-sized city in a developing country is considered viable, considering the identified components, with the aim of identifying risk events in driving, and areas of high probability of accidents in the city.",
        "DOI": "10.1016/j.jtte.2022.07.003",
        "paper_author": "Paredes J.J.",
        "affiliation_name": "Universidad del Cauca",
        "affiliation_city": "Popayan",
        "affiliation_country": "Colombia",
        "affiliation_id": "60051434",
        "affiliation_state": "Cauca"
    },
    {
        "paper_title": "Machine learning algorithms for dengue risk assessment: a case study for São Luís do Maranhão",
        "publication": "Computational and Applied Mathematics",
        "citied_by": "3",
        "cover_date": "2022-12-01",
        "Abstract": "This study aims to assess dengue fever risk using Machine Learning techniques, such as logistic regressions, linear discriminant analyses, Naive Bayes, decision tree, and random forest classifiers. This kind of approach to epidemiological problems has been developed to detect risks for diseases occurrence and allows to create public policies based on mathematical models to prevent public health problems. In this study, the models were trained with data from the municipality of São Luís do Maranhão, state of Maranhão, Brazil. The majority of related works analyze states, countries, or continental levels, with greater availability of data. To apply the approach to such a small region, some oversampling techniques were used. The number of cases per neighborhood from 2014 to and 2020 and climatic, territorial, and environmental data was used as input variables to estimate the probability of dengue occurrence in the municipality. Due to the unbalanced database, we used the SMOTE, ADASYN, and DBSMOTE oversampling techniques. The DBSMOTE-trained Random Forest classifier achieved the best results with a 75.1% AUC, 75.43% sensitivity and a 60.53% specificity.",
        "DOI": "10.1007/s40314-022-02101-z",
        "paper_author": "Rocha F.P.",
        "affiliation_name": "Universidade Estadual de Campinas",
        "affiliation_city": "Campinas",
        "affiliation_country": "Brazil",
        "affiliation_id": "60029570",
        "affiliation_state": "SP"
    },
    {
        "paper_title": "Time-series based prediction for energy consumption of smart home data using hybrid convolution-recurrent neural network",
        "publication": "Telematics and Informatics",
        "citied_by": "29",
        "cover_date": "2022-12-01",
        "Abstract": "The rapid increase in technological development has led to the rise in usage of IoT devices for monitoring Electrical Energy Consumption. As countries around the world are committing to United Nations Sustainable Development Goals, reducing carbon footprint has become an eminent priority for policymakers, businesses, and the public. Clean and green energy in the form of electricity has emerged as an alternative to fossil fuel. Since electricity is scarce and in high demand it has become an important problem for identifying robust energy consumption predictive models for powered smart residential homes. In our research we compare SVR, LSTM, GRU, CNN-LSTM, CNN-GRU models for predictive energy consumption data of smart residential homes. Empirical results indicate that with increase in the amount of data the performance of machine learning SVR degraded significantly more as compared to Deep Learning Techniques, which provides conclusive evidence that machine learning techniques are not suitable for the task. Whereas, our proposed CNN-GRU architecture performs 17.4% better in terms of Mean Absolute Error (MAE) with a value of 0.151 compared to the LSTM which has a value of MAE equals to 0.183 for days granularity of data and is only bested by the LSTM by 0.4% in terms of MAE for hour granularity data, where the CNN-GRU has MAE of 0.229 and LSTM achieves the MAE of 0.228. Additionally, CNN-LSTM and LSTM architectures were found effective in identifying outliers.",
        "DOI": "10.1016/j.tele.2022.101907",
        "paper_author": "Bhoj N.",
        "affiliation_name": "Independent Researcher",
        "affiliation_city": "Haldwani",
        "affiliation_country": "India",
        "affiliation_id": "128840708",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Recent trends of digital twin technologies in the energy sector: A comprehensive review",
        "publication": "Sustainable Energy Technologies and Assessments",
        "citied_by": "79",
        "cover_date": "2022-12-01",
        "Abstract": "The purpose of a digital twin (DT) is to gain insight into and predict the performance of a physical product, process, or piece of infrastructure. Numerous advantages accrue from the energy industry's adoption of DT technology, such as improved asset performance, higher profits and efficiencies, and less harmful effects on the environment. This paper's goal is to present a literature evaluation that classifies DT principles, usage patterns, and benefits in the energy sector. A thorough literature review covering the past decade of studies on DT in the energy sector was conducted. The originality of this study is in-depth examination of DT's use across the whole energy value chain from power generation and storage to energy usage in buildings, transportation, and industrial applications. From this analysis, it was clear that there is a growing interest in using DT in the energy industry and minimizing energy use is the primary focus of the literature on digital twins. Growth of DT technologies will be aided by recent developments in machine learning and artificial intelligence, as well as the development of more sophisticated control systems, allowing for the enhancement of energy system efficiency and effectiveness, thereby fostering the clean energy transition, and reshaping the future of energy.",
        "DOI": "10.1016/j.seta.2022.102837",
        "paper_author": "Ghenai C.",
        "affiliation_name": "University of Sharjah",
        "affiliation_city": "Sharjah",
        "affiliation_country": "United Arab Emirates",
        "affiliation_id": "60070813",
        "affiliation_state": "Sharjah"
    },
    {
        "paper_title": "Combined nonlinear effects of urbanization and economic growth on CO2 emissions in Malaysia. An application of QARDL and KRLS",
        "publication": "Urban Climate",
        "citied_by": "103",
        "cover_date": "2022-12-01",
        "Abstract": "Despite adequate literature on the nonlinear nexus between economic growth-environment and urbanization-environment in Malaysia, yet study on the combined nonlinear effect is lacking. Therefore, this work investigates the existence of a combined nonlinear impact of both urbanization and economic growth utilizing the Environmental Kuznets Curve (EKC) hypothesis with the help of yearly data from 1965 to 2018. This study employs the novel Quantile Autoregressive Distributed Lag (QARDL) technique. In addition, the Kernel-based Regularized Least Squares (KRLS) machine learning method is used to test robustness. The available evidence indicates that the observed association is quantile-dependent, which may have led to erroneous conclusions in prior studies relying on traditional models that focus on averages. Overall, the results endorse the legitimacy of EKC and a U-shape linkage between urbanization and the environment in the long and short run. Furthermore, the KRLS estimates also support the findings of QARDL results. Based on the results of the study, a comprehensive policy recommendation for planners and government officials. Urban development and environmental protection issues must be appropriately addressed so that the urbanization development and changes may better adapt to new social development demands.",
        "DOI": "10.1016/j.uclim.2022.101342",
        "paper_author": "Awan A.",
        "affiliation_name": "Nişantaşı Üniversitesi",
        "affiliation_city": "Istanbul",
        "affiliation_country": "Turkey",
        "affiliation_id": "60106414",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "“Flood risk modeling in southern Bagmati corridor, Nepal” (a study from Sarlahi and Rautahat, Nepal)",
        "publication": "Progress in Disaster Science",
        "citied_by": "10",
        "cover_date": "2022-12-01",
        "Abstract": "Flooding is underlying major natural hazard of Nepal and hence socio-economic loss has been paid annually by this hazard. Planning and management of flood based on study is still lack in Nepal. So, the particular study is carried out to assess the flood risk modeling in lower Bagmati river region in Eastern Terai. In this study, total 10 geospatial environment layers and past flood inventory from field were used to run the machine learning model i.e., MaxEnt for risk modeling of flood. The past flood data were separated into 75% for model building and 25% for model validation. The land use land cover change showed the highest contribution (40.8%) to the flood while the lowest contribution was of slope only 0.2%. 9% of total population were in high risk of flood while 39% population were in very low risk. Figure no. 13 shows that 5% of total household were in high risk, 55% were in moderate risk and 20% were in very low risk of flood. Out of total study area about 2.66% of the total area is in very high-risk zone to flood. High risk zone is found to be 4.89%, where as 9.48%, 20.61% and 62.36% are moderate, low, and very low risk zone area. In terms of AUC values, acceptable results were obtained for the test data with 0.931 and the standard deviation 0.019. The values of Area Under Curve (AUC) range from 0.7 to 0.8 and interpreted as fair or good. Finally, this research could directly help in policy, planning, framework, and programming of development intervention to tackle with flood hazard.",
        "DOI": "10.1016/j.pdisas.2022.100260",
        "paper_author": "Shreevastav B.B.",
        "affiliation_name": "Tribhuwan University",
        "affiliation_city": "Kritipur",
        "affiliation_country": "Nepal",
        "affiliation_id": "126885388",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Optimal scheduling of a wind energy dominated distribution network via a deep reinforcement learning approach",
        "publication": "Renewable Energy",
        "citied_by": "14",
        "cover_date": "2022-12-01",
        "Abstract": "With the development of clean energy systems, large-scale renewable energy is being connected to the traditional distribution network, which also brings new challenges to the reliable and economic scheduling of the power grid. To address these challenges, this paper proposes an intelligent scheduling strategy for a wind energy dominated distribution network, which aims to reduce the fluctuation caused by the wind energy. First, the energy scheduling model and objective function of the distribution network system are established and the constraints of various types of components are considered. Then, deep reinforcement learning is introduced to realize real-time decision in distribution network to solve the problem of fluctuation caused by the uncertain wind power output. The energy scheduling method is developed into a Markov decision process based on deep deterministic policy gradient (DDPG) algorithm. Finally, the simulation is verified on the IEEE14 node system. The results verify that the proposed approach can effectively reduce power fluctuations in the distribution network. The superiority of the adopted DDPG algorithm is demonstrated by comparing with the deep Q network algorithm.",
        "DOI": "10.1016/j.renene.2022.10.094",
        "paper_author": "Zhu J.",
        "affiliation_name": "University of Electronic Science and Technology of China",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China",
        "affiliation_id": "60005465",
        "affiliation_state": "Sichuan"
    },
    {
        "paper_title": "Resource-Efficient Distributed Deep Neural Networks Empowered by Intelligent Software-Defined Networking",
        "publication": "IEEE Transactions on Network and Service Management",
        "citied_by": "5",
        "cover_date": "2022-12-01",
        "Abstract": "Contemporary machine learning methods have evolved from conventional algorithms to deep neural networks (DNNs) that are computation- and data- intensive. Thus, they are suitable to be deployed in the cloud that can offer high computational capacity and scalable resources. However, the cloud computing paradigm is not optimal for delay- and energy-sensitive applications. To mitigate these problems, a battery of distributed DNNs have been proposed to allow a fast inference with device-edge-cloud synergy. Furthermore, although distributed deployment of DNNs on real communication networks is an important research topic, the legacy network architecture cannot meet the requirements of these distributed deep neural networks due to the complicated management and manual configuration, etc. To cope with these requirements, we develop a novel and explicit Intelligent Software Defined Networking (ISDN) that aims to manage the bandwidth and computing resources across the network via the SDN paradigm. We first identify the difficulties of deploying distributed intelligent computing in the current network architecture. Then, we explain how to address these problems by introducing the ISDN architecture. Specifically, we develop a dynamic routing method to enable Quality-of-Service (QoS) communication based on the SDN paradigm and propose a Markov Decision Process (MDP) based dynamic task offloading model to achieve the optimal offloading policy of DNN tasks. We develop a simulation platform based on Mininet to measure its performance advantages over traditional architectures. Extensive experimental results show that compared with the traditional network architecture, our architecture based on the SDN paradigm can perform better in terms of both network throughput and resource utilization.",
        "DOI": "10.1109/TNSM.2022.3218173",
        "paper_author": "Lu K.",
        "affiliation_name": "University of Electronic Science and Technology of China",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China",
        "affiliation_id": "60005465",
        "affiliation_state": "Sichuan"
    },
    {
        "paper_title": "On the aggregation of input data for energy system models",
        "publication": "Elektrotechnik und Informationstechnik",
        "citied_by": "1",
        "cover_date": "2022-12-01",
        "Abstract": "The increasing share of variable renewable energy sources in power systems poses new challenges to policy makers and network planners alike because of the sources’ variability and insufficient energy storage capability. This requires the development of new optimization models that consider the inter-temporal connection between different periods aggregating them on a more general level (e.g., days or weeks). However, aggregation models are often empirical and based on common clustering algorithms. In this paper, we carry out a numerical exploration of the relationship between the structure of the system and the hyperparameters required for these aggregation procedures. Our findings indicate that there is valuable information from the power system that can be used to improve the aggregation. This is important because, in most cases, the accuracy of the aggregation cannot be measured exactly.",
        "DOI": "10.1007/s00502-022-01073-6",
        "paper_author": "Cardona-Vasquez D.",
        "affiliation_name": "Technische Universitat Graz",
        "affiliation_city": "Graz",
        "affiliation_country": "Austria",
        "affiliation_id": "60019663",
        "affiliation_state": "Styria"
    },
    {
        "paper_title": "A hierarchical reinforcement learning method for missile evasion and guidance",
        "publication": "Scientific Reports",
        "citied_by": "9",
        "cover_date": "2022-12-01",
        "Abstract": "This paper proposes an algorithm for missile manoeuvring based on a hierarchical proximal policy optimization (PPO) reinforcement learning algorithm, which enables a missile to guide to a target and evade an interceptor at the same time. Based on the idea of task hierarchy, the agent has a two-layer structure, in which low-level agents control basic actions and are controlled by a high-level agent. The low level has two agents called a guidance agent and an evasion agent, which are trained in simple scenarios and embedded in the high-level agent. The high level has a policy selector agent, which chooses one of the low-level agents to activate at each decision moment. The reward functions for each agent are different, considering the guidance accuracy, flight time, and energy consumption metrics, as well as a field-of-view constraint. Simulation shows that the PPO algorithm without a hierarchical structure cannot complete the task, while the hierarchical PPO algorithm has a 100% success rate on a test dataset. The agent shows good adaptability and strong robustness to the second-order lag of autopilot and measurement noises. Compared with a traditional guidance law, the reinforcement learning guidance law has satisfactory guidance accuracy and significant advantages in average time and average energy consumption.",
        "DOI": "10.1038/s41598-022-21756-6",
        "paper_author": "Yan M.",
        "affiliation_name": "Air Force Engineering University China",
        "affiliation_city": "Xi'an",
        "affiliation_country": "China",
        "affiliation_id": "60069720",
        "affiliation_state": "Shaanxi"
    },
    {
        "paper_title": "Multiple driving factors and hierarchical management of PM<inf>2.5</inf>: Evidence from Chinese central urban agglomerations using machine learning model and GTWR",
        "publication": "Urban Climate",
        "citied_by": "16",
        "cover_date": "2022-12-01",
        "Abstract": "In the fast-developing urban agglomerations (UAs), it is of importance to make accurate judgments concerning the multiple driving factors, and establish hierarchical joint management policy. The impact of weather conditions on daily PM2.5 concentrations in the Chinese central UAs was studied using machine learning algorithm, and the analyzed results were integrated into “the proportion of day numbers with negative weather conditions (PDNW)”. Geographically and temporally weighted regression (GTWR) was used to analyze the driving factors of PM2.5 pollution. Results showed that PM2.5 pollution in central China decreased from north to south, and spatial gathering was becoming increasingly prominent. The PM2.5 predicted values decreased smoothly, with barometric pressure and humidity exerting a large effect, and wind speed and direction having a complex effect. Meteorological conditions had a small effect on the annual scale, but the timing of the effect varied in each city. The distribution of PDNW ranged from 23.3% to 55.6%. The proportion of the tertiary industry's GDP (mean − 0.191), education expenditure (mean − 0.057), and the greening rate of urban built-up areas (mean − 0.295) were found to be negatively correlated with PM2.5 pollution. Transportation, urban greening, innovation, and entrepreneurship were driving factors with obvious spatial differences.",
        "DOI": "10.1016/j.uclim.2022.101327",
        "paper_author": "Ou C.",
        "affiliation_name": "Zhongnan University of Economics and Law",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60024979",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Predicting residential electricity consumption patterns based on smart meter and household data: A case study from the Republic of Ireland",
        "publication": "Utilities Policy",
        "citied_by": "19",
        "cover_date": "2022-12-01",
        "Abstract": "We use machine learning algorithms to investigate various aspects of residential electricity consumption for households in the Republic of Ireland. Temperature, day of week, and month of year have an apparent causal effect on consumption. The prevalence of six distinct intra-day load profiles, identified by clustering, changes dramatically between weekdays and weekends as well as seasonally. Key socio-demographic and dwelling characteristics associated with annual load profiles include household makeup and size and occupation of the primary income earner. We further discuss policy and management implications of our findings and propose avenues for future research.",
        "DOI": "10.1016/j.jup.2022.101446",
        "paper_author": "Guo Z.",
        "affiliation_name": "Kent Business School",
        "affiliation_city": "Canterbury",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60162124",
        "affiliation_state": "Kent"
    },
    {
        "paper_title": "Streetscape and business survival: Examining the impact of walkable environments on the survival of restaurant businesses in commercial areas based on street view images",
        "publication": "Journal of Transport Geography",
        "citied_by": "16",
        "cover_date": "2022-12-01",
        "Abstract": "Walkable environments within commercial areas can be an important factor that vitalizes consumers' walking access, thereby increasing stores' survival. Particularly, the survival of restaurant businesses may be more sensitive to surrounding walkability than that of other businesses. Despite many studies on the relationships between walkability and restaurants' survival, prior studies have identified walkable environments at a macro-scale, not at a pedestrian's eye-level view along the street. This study addresses these gaps by investigating the impact of macro- and micro-scale walkable environments on the survival of restaurant businesses in the commercial areas of Seoul, Korea. To identify micro-scale streetscapes in commercial areas, we used deep learning technologies utilizing 82,563 Google Street View panorama images. We employed spatial lag and multiple regression models to clarify the impact of walkable environments on restaurants' survival. This study also accounted for two types of commercial areas to examine how the impact of walkability varies across commercial areas' heterogeneity. We found that streetscapes play a key role in increasing the survival of restaurants in commercial areas. This study may help policymakers develop tailored approaches to vitalize the economies of local commercial areas by considering the environmental characteristics of commercial areas and the impacts of improving the walkable environment.",
        "DOI": "10.1016/j.jtrangeo.2022.103480",
        "paper_author": "Kim S.",
        "affiliation_name": "Hanyang University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea",
        "affiliation_id": "60024872",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Deep reinforcement learning for adaptive path planning and control of an autonomous underwater vehicle",
        "publication": "Applied Ocean Research",
        "citied_by": "61",
        "cover_date": "2022-12-01",
        "Abstract": "Research into intelligent motion planning methods has been driven by the growing autonomy of autonomous underwater vehicles (AUV) in complex unknown environments. Deep reinforcement learning (DRL) algorithms with actor-critic structures are optimal adaptive solutions that render online solutions for completely unknown systems. The present study proposes an adaptive motion planning and obstacle avoidance technique based on deep reinforcement learning for an AUV. The research employs a twin-delayed deep deterministic policy algorithm, which is suitable for Markov processes with continuous actions. Environmental observations are the vehicle's sensor navigation information. Motion planning is carried out without having any knowledge of the environment. A comprehensive reward function has been developed for control purposes. The proposed system is robust to the disturbances caused by ocean currents. The simulation results show that the motion planning system can precisely guide an AUV with six-degrees-of-freedom dynamics towards the target. In addition, the intelligent agent has appropriate generalization power.",
        "DOI": "10.1016/j.apor.2022.103326",
        "paper_author": "Hadi B.",
        "affiliation_name": "Babol Noshirvani University of Technology",
        "affiliation_city": "Babol",
        "affiliation_country": "Iran",
        "affiliation_id": "60089290",
        "affiliation_state": "Mazandaran"
    },
    {
        "paper_title": "Evaluation of machine learning algorithms for predicting direct-acting antiviral treatment failure among patients with chronic hepatitis C infection",
        "publication": "Scientific Reports",
        "citied_by": "6",
        "cover_date": "2022-12-01",
        "Abstract": "Despite the availability of efficacious direct-acting antiviral (DAA) therapy, the number of people infected with hepatitis C virus (HCV) continues to rise, and HCV remains a leading cause of liver-related morbidity, liver transplantation, and mortality. We developed and validated machine learning (ML) algorithms to predict DAA treatment failure. Using the HCV-TARGET registry of adults who initiated all-oral DAA treatment, we developed elastic net (EN), random forest (RF), gradient boosting machine (GBM), and feedforward neural network (FNN) ML algorithms. Model performances were compared with multivariable logistic regression (MLR) by assessing C statistics and other prediction evaluation metrics. Among 6525 HCV-infected adults, 308 patients (4.7%) experienced DAA treatment failure. ML models performed similarly in predicting DAA treatment failure (C statistic [95% CI]: EN, 0.74 [0.69–0.79]; RF, 0.74 [0.69–0.80]; GBM, 0.72 [0.67–0.78]; FNN, 0.75 [0.70–0.80]), and all 4 outperformed MLR (C statistic [95% CI]: 0.51 [0.46–0.57]), and EN used the fewest predictors (n = 27). With Youden index, the EN had 58.4% sensitivity and 77.8% specificity, and nine patients were needed to evaluate to identify 1 DAA treatment failure. Over 60% treatment failure were classified in top three risk decile subgroups. EN-identified predictors included male sex, treatment < 8 weeks, treatment discontinuation due to adverse events, albumin level < 3.5 g/dL, total bilirubin level > 1.2 g/dL, advanced liver disease, and use of tobacco, alcohol, or vitamins. Addressing modifiable factors of DAA treatment failure may reduce the burden of retreatment. Machine learning algorithms have the potential to inform public health policies regarding curative treatment of HCV.",
        "DOI": "10.1038/s41598-022-22819-4",
        "paper_author": "Park H.",
        "affiliation_name": "University of Florida",
        "affiliation_city": "Gainesville",
        "affiliation_country": "United States",
        "affiliation_id": "60013959",
        "affiliation_state": "FL"
    },
    {
        "paper_title": "Large-scale neural network computations and multivariate representations during approach-avoidance conflict decision-making",
        "publication": "NeuroImage",
        "citied_by": "9",
        "cover_date": "2022-12-01",
        "Abstract": "Many real-world situations require navigating decisions for both reward and threat. While there has been significant progress in understanding mechanisms of decision-making and mediating neurocircuitry separately for reward and threat, there is limited understanding of situations where reward and threat contingencies compete to create approach-avoidance conflict (AAC). Here, we leverage computational learning models, independent component analysis (ICA), and multivariate pattern analysis (MVPA) approaches to understand decision-making during a novel task that embeds concurrent reward and threat learning and manipulates congruency between reward and threat probabilities. Computational modeling supported a modified reinforcement learning model where participants integrated reward and threat value into a combined total value according to an individually varying policy parameter, which was highly predictive of decisions to approach reward vs avoid threat during trials where the highest reward option was also the highest threat option (i.e., approach-avoidance conflict). ICA analyses demonstrated unique roles for salience, frontoparietal, medial prefrontal, and inferior frontal networks in differential encoding of reward vs threat prediction error and value signals. The left frontoparietal network uniquely encoded degree of conflict between reward and threat value at the time of choice. MVPA demonstrated that delivery of reward and threat could accurately be decoded within salience and inferior frontal networks, respectively, and that decisions to approach reward vs avoid threat were predicted by the relative degree to which these reward vs threat representations were active at the time of choice. This latter result suggests that navigating AAC decisions involves generating mental representations for possible decision outcomes, and relative activation of these representations may bias subsequent decision-making towards approaching reward or avoiding threat accordingly.",
        "DOI": "10.1016/j.neuroimage.2022.119709",
        "paper_author": "Moughrabi N.",
        "affiliation_name": "Dell Medical School",
        "affiliation_city": "Austin",
        "affiliation_country": "United States",
        "affiliation_id": "60122541",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "A differential evolution with reinforcement learning for multi-objective assembly line feeding problem",
        "publication": "Computers and Industrial Engineering",
        "citied_by": "2",
        "cover_date": "2022-12-01",
        "Abstract": "This paper studies a multi-objective assembly line feeding problem (MALFP), which is a new variant of the assembly line feeding problem in automobile manufacturers. In this problem, part families are delivered through five feeding policies to minimize three objectives simultaneously. To describe the problem, a novel multi-objective mathematical model is formulated. It not only overcomes the difficulty of determining perfect weights for objectives without prior knowledge, but also complements the traditional model by considering extended decisions on receiving warehouses, an extra cost item for policy switching, and a hybrid inventory strategy. To solve the problem, an innovative multi-objective differential evolution with a reinforcement learning (RL) based operator selection mechanism (MODE-RLOSM) is proposed. By solving MALFP with MODE-RLOSM, near-optimal candidate solutions that are suitable for different working conditions are provided to managers for making trade-offs and implementations. Compared with state-of-the-art optimization algorithms as well as a practical decision tree approach, the proposed algorithm shows superiority in cost saving, solution quality, and convergence efficiency. Through ablation study, sensitivity analysis, and RL behavior analysis, we investigate components in MODE-RLOSM and verify their effectiveness and robustness. In addition to bringing significant cost savings, the obtained solution also gives us production enlightenment and thus improves the decision-making efficiency of the enterprise. In our research, we illustrate the influence of part diversity on policy selection, give managers suggestions under different objective preferences, and find it uneconomical to pursue a specific objective excessively.",
        "DOI": "10.1016/j.cie.2022.108714",
        "paper_author": "Tao L.",
        "affiliation_name": "Northeastern University",
        "affiliation_city": "Shenyang",
        "affiliation_country": "China",
        "affiliation_id": "60031863",
        "affiliation_state": "Liaoning"
    },
    {
        "paper_title": "Property valuation using machine learning algorithms on statistical areas in Greater Sydney, Australia",
        "publication": "Land Use Policy",
        "citied_by": "24",
        "cover_date": "2022-12-01",
        "Abstract": "Property valuation plays a significant role in urban economics and is of great importance to various stakeholders who interact and shape the city, including property owners, buyers, banks, land developers, real estate agents, local councils and government planning authorities. In the literature, various predictive models have been proposed to automate the calculation of property value, most of which endeavour to factor in the combination of property characteristics, market factors and location-based attributes associated with individual properties use large citywide databases. At the same time, it has been widely acknowledged that regional sub-areas have impacts on property price prediction. Therefore, this paper aims to investigate the performance of various techniques on sub-areas using the Greater Sydney Region as the study area. The sub-area in this paper is defined as the statistical areas (SAs) as defined by the Australian Bureau of Statistics. In particular, two different SA geographies (SA4, SA3) along with the City Level are adopted to understand the spatial dependence which occurs at different levels. With real-world transaction records and data collected from a diverse range of sources, various methods including the traditional hedonic price model (HPM) and popular machine learning (ML) approaches are implemented and evaluated for property price prediction. Two different property markets for residential property are modelled, being for housing stock and apartment (unit) stock. Experimental results show that Random Forest and Gradient Boosting-based methods outperform other approaches in most scenarios and that the high spatial resolution property sub-area (SA3) improved the performance in terms of overall model accuracy. This research provides insights into how sub-area machine learning models can be employed in real estate to characterize property price, and helps understand the influential factors in different local geographical areas for policy-making.",
        "DOI": "10.1016/j.landusepol.2022.106409",
        "paper_author": "Gao Q.",
        "affiliation_name": "UNSW Sydney",
        "affiliation_city": "Sydney",
        "affiliation_country": "Australia",
        "affiliation_id": "60028333",
        "affiliation_state": "NSW"
    },
    {
        "paper_title": "Sharing is caring: a call for a new era of rare disease research and development",
        "publication": "Orphanet Journal of Rare Diseases",
        "citied_by": "10",
        "cover_date": "2022-12-01",
        "Abstract": "Scientific advances in the understanding of the genetics and mechanisms of many rare diseases with previously unknown etiologies are inspiring optimism in the patient, clinical, and research communities and there is hope that disease-specific treatments are on the way. However, the rare disease community has reached a critical point in which its increasingly fragmented structure and operating models are threatening its ability to harness the full potential of advancing genomic and computational technologies. Changes are therefore needed to overcome these issues plaguing many rare diseases while also supporting economically viable therapy development. In “Data silos are undermining drug development and failing rare disease patients (Orphanet Journal of Rare Disease, Apr 2021),” we outlined many of the broad issues underpinning the increasingly fragmented and siloed nature of the rare disease space, as well as how the issues encountered by this community are representative of biomedical research more generally. Here, we propose several initiatives for key stakeholders - including regulators, private and public foundations, and research institutions - to reorient the rare disease ecosystem and its incentives in a way that we believe would cultivate and accelerate innovation. Specifically, we propose supporting non-proprietary patient registries, greater data standardization, global regulatory harmonization, and new business models that encourage data sharing and research collaboration as the default mode. Leadership needs to be integrated across sectors to drive meaningful change between patients, industry, sponsors, and academic medical centers. To transform the research and development landscape and unlock its vast healthcare, economic, and scientific potential for rare disease patients, a new model is ultimately the goal for all.",
        "DOI": "10.1186/s13023-022-02529-w",
        "paper_author": "Denton N.",
        "affiliation_name": "Penn Medicine",
        "affiliation_city": "Philadelphia",
        "affiliation_country": "United States",
        "affiliation_id": "60023009",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "DROP: Deep relocating option policy for optimal ride-hailing vehicle repositioning",
        "publication": "Transportation Research Part C: Emerging Technologies",
        "citied_by": "15",
        "cover_date": "2022-12-01",
        "Abstract": "In a ride-hailing system, an optimal relocation of vacant vehicles can significantly reduce fleet idling time and balance the supply–demand distribution, enhancing system efficiency and promoting driver satisfaction and retention. Model-free deep reinforcement learning (DRL) has been shown to dynamically learn the relocating policy by actively interacting with the intrinsic dynamics in large-scale ride-hailing systems. However, the issues of sparse reward signals and unbalanced demand and supply distribution place critical barriers in developing effective DRL models. Conventional exploration strategy (e.g., the ϵ-greedy) may barely work under such an environment because of dithering in low-demand regions distant from high-revenue regions. This study proposes the deep relocating option policy (DROP) that supervises vehicle agents to escape from oversupply areas and effectively relocate to potentially underserved areas. We propose to learn the Laplacian embedding of a time-expanded relocation graph, as an approximation representation of the system relocation policy. The embedding generates task-agnostic signals, which in combination with task-dependent signals, constitute the pseudo-reward function for generating DROPs. We present a hierarchical learning framework that trains a high-level relocation policy and a set of low-level DROPs. We note that DROP is a general method and can be incorporated into existing model-free RL advances for further improvements in ride-hailing applications. The effectiveness of our approach is demonstrated using a custom-built high-fidelity simulator with real-world trip record data. We report that DROP significantly improves baseline value and policy iteration algorithms and can effectively resolve the dithering issue in low-demand areas.",
        "DOI": "10.1016/j.trc.2022.103923",
        "paper_author": "Qian X.",
        "affiliation_name": "The University of Alabama",
        "affiliation_city": "Tuscaloosa",
        "affiliation_country": "United States",
        "affiliation_id": "60025371",
        "affiliation_state": "AL"
    },
    {
        "paper_title": "Establishment of line-of-sight optical links between autonomous underwater vehicles: Field experiment and performance validation",
        "publication": "Applied Ocean Research",
        "citied_by": "8",
        "cover_date": "2022-12-01",
        "Abstract": "Establishing a line-of-sight link between autonomous underwater vehicles (AUVs) is an unavoidable challenge for realizing high data rate optical communication in ocean exploration. We propose a method for link establishment by maintaining the relative position and orientation between AUVs. Using a reinforcement learning algorithm, we search for the policy that can suppress external disturbances and optimize the link establishment efficiency. To evaluate the performance of the proposed method, we prepared a hovering AUV to conduct the link establishment experiments. The reinforcement learning policy trained in a simulation environment was deployed on the AUV in real environments. In field experiments, our approach successfully performed the link establishment from the hovering AUV to an autonomous surface vehicle. Based on the experimental results, we evaluate the performance of the AUV in executing the link establishment policy. Comparisons with existing optical search-based link establishment methods are presented.",
        "DOI": "10.1016/j.apor.2022.103385",
        "paper_author": "Weng Y.",
        "affiliation_name": "Institute of Industrial Science",
        "affiliation_city": "Tokyo",
        "affiliation_country": "Japan",
        "affiliation_id": "60180342",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Adaptation of a robotic dialog system for medication reminder in elderly care",
        "publication": "Smart Health",
        "citied_by": "5",
        "cover_date": "2022-12-01",
        "Abstract": "Social robots can assist older adults in their daily life. Verbal conversation is a natural and convenient way for older adults to interact with social robots. However, most of the existing conversation-based robot services, such as medication reminders, are rule-based systems. These systems require many hand-crafted rules and a significant amount of expert knowledge, therefore they cannot adapt to older adults’ characteristics and dialog history. There are many reinforcement learning (RL) based methods for task-oriented dialogues, but they mainly focus on completing the tasks through text-based conversations. Those methods cannot be directly used for elderly care applications involving human–robot interactions (HRI). Considering the above shortcomings, we proposed a dialog system adaptation method (DSAM) for social robots. The DSAM is based on reinforcement learning which considers the characteristics of older adults, the dialog history and user preference to adapt the dialog policy and improve the dialog module. We implemented DSAM in our custom-made ASCCBot social robot. To evaluate DSAM, we firstly tested the dialog agent which was trained by a user simulator with different settings. The results show that the obtained agent achieves a good result with the desired dialog flow compared to the baseline agent. Based on the obtained dialog policy, the adaptation process is evaluated. The results show that with a good success rate, the number of dialog turns is decreased and the NLU module performance is improved by the adaptation process, which proves the effectiveness of DSAM. We also tested DSAM with human subjects. The results show that the average adaptation success rate is 94.7% and the preference distance reaches 0 after 6 rounds of adaptation while creating reminders successfully with a limited amount of user feedback.",
        "DOI": "10.1016/j.smhl.2022.100346",
        "paper_author": "Su Z.",
        "affiliation_name": "Oklahoma State University",
        "affiliation_city": "Stillwater",
        "affiliation_country": "United States",
        "affiliation_id": "60006514",
        "affiliation_state": "OK"
    },
    {
        "paper_title": "Public sentiment on the global outbreak of monkeypox: an unsupervised machine learning analysis of 352,182 twitter posts",
        "publication": "Public Health",
        "citied_by": "39",
        "cover_date": "2022-12-01",
        "Abstract": "Objectives: This study aimed to study the public's sentiments on the current monkeypox outbreaks via an unsupervised machine learning analysis of social media posts. Study design: This was an exploratory analysis of tweets sentiments. Methods: We extracted original tweets containing the terms ‘monkeypox’, ‘monkey pox’ or ‘monkey_pox’ and posted them in the English language from 6 May 2022 (first case detected in the United Kingdom) to 23 July 2022 (when World Health Organization declared Monkeypox to be a global health emergency). Retweets and duplicate tweets were excluded from study. Bidirectional Encoder Representations from Transformers (BERT) Named Entity Recognition. This was followed by topic modelling (specifically BERTopic) and manual thematic analysis by the study team, with independent reviews of the topic labels and themes. Results: Based on topic modelling and thematic analysis of a total of 352,182 Twitter posts, we derived five topics clustered into three major themes related to the public discourse on the ongoing outbreaks. These include concerns of safety, stigmatisation of minority communities, and a general lack of faith in public institutions. The public sentiments underscore growing (and existing) partisanship, personal health worries in relation to the evolving situation, as well as concerns of the media's portrayal of lesbian, gay, bisexual, transgender and queer and minority communities, which might further stigmatise these groups. Conclusions: Monkeypox is an emerging infectious disease of public concern. Our study has highlighted important societal issues, including misinformation, political mistrust and anti-gay stigma that should be sensitively considered when designing public health policies to contain the ongoing outbreaks.",
        "DOI": "10.1016/j.puhe.2022.09.008",
        "paper_author": "Ng Q.X.",
        "affiliation_name": "Singapore General Hospital",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore",
        "affiliation_id": "60017958",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Peer discrimination toward rural migrant students and academic performance in urban China: A machine learning approach",
        "publication": "Cities",
        "citied_by": "0",
        "cover_date": "2022-12-01",
        "Abstract": "Rural students migrating to cities in China encounter discrimination from their local peers, largely due to the institution known as Hukou. Using micro-level data from urban middle schools in China, we confirm that peer discrimination toward migrant students is negatively associated with the academic performance. We perform a simulation study using a novel machine learning technique to generate synthetic data where the bias in the original data is removed. Our finding shows the negative association disappears when students are exposed to a setting that mitigates discrimination. For policy implication, this study implies that practices to increase migrants' awareness of discrimination or reduce locals' unfavorable behavior would be helpful. Such practices should be multidimensional, with students, teachers, and schools.",
        "DOI": "10.1016/j.cities.2022.104027",
        "paper_author": "Lee H.",
        "affiliation_name": "Southwestern University of Finance and Economics",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China",
        "affiliation_id": "60018540",
        "affiliation_state": "Sichuan"
    },
    {
        "paper_title": "An automated closed-loop framework to enforce security policies from anomaly detection",
        "publication": "Computers and Security",
        "citied_by": "4",
        "cover_date": "2022-12-01",
        "Abstract": "Due to the growing complexity and scale of IT systems, there is an increasing need to automate and streamline routine maintenance and security management procedures, to reduce costs and improve productivity. In the case of security incidents, the implementation and application of response actions require significant efforts from operators and developers in translating policies to code. Even if Machine Learning (ML) models are used to find anomalies, they need to be regularly trained/updated to avoid becoming outdated. In an evolving environment, a ML model with outdated training might put at risk the organization it was supposed to defend. To overcome those issues, in this paper we propose an automated closed-loop process with three stages. The first stage focuses on obtaining the Decision Trees (DT) that classify anomalies. In the second stage, DTs are translated into security Policies as Code based on languages recognized by the Policy Engine (PE). In the last stage, the translated security policies feed the Policy Engines that enforce them by converting them into specific instruction sets. We also demonstrate the feasibility of the proposed framework, by presenting an example that encompasses the three stages of the closed-loop process. The proposed framework may integrate a broad spectrum of domains and use cases, being able for instance to support the decide and the act stages of the ETSI Zero-touch Network & Service Management (ZSM) framework.",
        "DOI": "10.1016/j.cose.2022.102949",
        "paper_author": "Henriques J.",
        "affiliation_name": "University of Coimbra, Centre for Informatics and System",
        "affiliation_city": "Coimbra",
        "affiliation_country": "Portugal",
        "affiliation_id": "60106440",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Economic and environmental implications of the nuclear power phase-out in Belgium: Insights from time-series models and a partial differential equations algorithm",
        "publication": "Structural Change and Economic Dynamics",
        "citied_by": "13",
        "cover_date": "2022-12-01",
        "Abstract": "By 2025, Belgium will phase-out nuclear power. Unassessed so far, this policy reform may modify the economic and environmental channels through which energy and society interfere in this country. In this paper, we investigate whether this structural energy change may adversely impact the growth of the Belgian economy (i) and its ability to meet its long-term greenhouse gas emission targets (ii). A multivariate model comprising production factors (labor, capital, and exports), nuclear and renewable energy uses, total primary energy supply, economic growth, and CO2 emissions from the power and heating sector is combined with real time-series data spanning the 1974–2019 period. The analysis consists in sequentially assessing two distinct nexuses (energy-economy and energy-economy-environment) over reduced- and augmented frameworks (excluding and including nuclear energy), and through a two-stage empirical strategy: time-series econometric estimations (Toda-Yamamoto causality test, Impulse Response Functions (IRFs), and the Auto-Regressive Distributed Lags (ARDL) and Machine Learning (ML) experiments with a Partial Differential Equations (PDEs) algorithm. For robustness purposes, we conduct two seminal tests which relate to dynamic predictive processes (T-Mat and Verticality tests). Besides confirming the time-series findings, our ML results highlight the necessity to timely manage the process of nuclear phase-out, along with a progressive deployment of installed renewable energy capacity. This should avoid additional economic costs, energy security threats, and undermining of climate targets. In doing so, this study combines macro-level nexus investigations with the politics and institutional determinants of nuclear energy reliance and seeks to bring inclusive knowledge on this topic.",
        "DOI": "10.1016/j.strueco.2022.10.001",
        "paper_author": "Soytas U.",
        "affiliation_name": "Technical University of Denmark",
        "affiliation_city": "Lyngby",
        "affiliation_country": "Denmark",
        "affiliation_id": "60011373",
        "affiliation_state": "Hovedstaden"
    },
    {
        "paper_title": "The impacts of carbon pricing on the electricity market in Japan",
        "publication": "Humanities and Social Sciences Communications",
        "citied_by": "8",
        "cover_date": "2022-12-01",
        "Abstract": "To achieve carbon neutrality by 2050, Japan should speed up reducing fossil fuel reliance on production, especially for energy-intensive sectors. One way is by implementing a carbon pricing system, converting emissions from fossil fuels to costs of production and consumption. This study focuses on the correlation between the price of wholesale electricity spot market and carbon cost of nine regions in Japan through carbon cost pass-through rate. This paper applies polynomial OLS regression with degree two through machine learning technics to better fit the relationship between electricity price and demand and also applies a generalized additive model to capture the nonlinear relationship between fuel spread and carbon cost and test the robustness of estimated CPTR. The results show that Hokuriku, Kyushu, Shikoku, Tohoku, and Tokyo have a lower value carbon cost pass-through rate while Kansai, Chubu, and Chugoku have a higher rate of carbon cost pass-through. There is a special case in Hokkaido as the negative relationship between electricity price and carbon cost. Those findings are also crucial in supporting future policy adjustments.",
        "DOI": "10.1057/s41599-022-01360-9",
        "paper_author": "Ding D.",
        "affiliation_name": "Anhui Xinhua University",
        "affiliation_city": "Hefei",
        "affiliation_country": "China",
        "affiliation_id": "60268359",
        "affiliation_state": "Anhui"
    },
    {
        "paper_title": "Natural-anthropogenic environment interactively causes the surface urban heat island intensity variations in global climate zones",
        "publication": "Environment International",
        "citied_by": "46",
        "cover_date": "2022-12-01",
        "Abstract": "The inconstant climate change and rapid urbanization substantially disturb the global thermal balance and induce severe urban heat island (UHI) effect, adversely impacting human development and health. Existing literature has revealed the UHI characteristics and driving factors at an urban scale, but interactions between the main factors of a global grid scale assessment on the context of climate zones remain unclear. Therefore, based on the multidimensional climatic and socio-economic statistical datasets, the multi-time scale of surface urban heat island intensity (SUHI) characteristics was investigated in this study to analyze how natural-anthropogenic drivers affect the variance of SUHI and vary in their importance for the changes of other interaction factors. The results show that the mean value of SUHI in summer is higher than in winter, and in daytime is higher than in nighttime on a seasonal and daily scale. SUHIs in different global climate zones have significant differences. When analyzing drivers’ contributions and interactions with LightGBM model and SHAP algorithm, we know that monthly precipitation (PREC), the estimated population (POP) and surface pressure (PRES) are the three major drivers of daytime SUHI. The nighttime SUHI is mainly PREC, POP and anthropogenic heat emission (AHE), the influence rules of the natural drivers are mostly opposite to that of daytime. This study highlights the fundamental role of background climate for designing strategies. Irrigation or artificial rainfall will be effective to mitigate SUHI in low rainfall areas, while it is more effective to reduce AHE in high rainfall areas. In where greening can be difficult in the most developed cities, reducing AHE, increasing per capita GDP and controlling the population scale may also contribute to alleviating the SUHI. This study provides ideas for developing responsive urban heat island mitigation policies in a more realistic setting.",
        "DOI": "10.1016/j.envint.2022.107574",
        "paper_author": "Yuan Y.",
        "affiliation_name": "Fudan University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60009860",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Congested Urban Networks Tend to Be Insensitive to Signal Settings: Implications for Learning-Based Control",
        "publication": "IEEE Transactions on Intelligent Transportation Systems",
        "citied_by": "4",
        "cover_date": "2022-12-01",
        "Abstract": "This paper highlights several properties of large urban networks that can have an impact on machine learning methods applied to traffic signal control. In particular, we note that the average network flow tends to be independent of the signal control policy as density increases past the critical density. We show that this property, which so far has remained under the radar, implies that no control (i.e. a random policy) can be an effective control strategy for a surprisingly large family of networks, especially for networks with short blocks. We also show that this property makes deep reinforcement learning (DRL) methods ineffective when trained under congested conditions, independently of the particular algorithm used. Accordingly, in contrast to the conventional wisdom around learning-based methods promoting the exploration of all states, we find that for urban networks it is advisable to discard any congested data when training, and that doing so will improve performance under all traffic conditions. Our results apply to all possible grid networks thanks to a parametrization introduced here. The impact of the turning probability was found to be very significant, in particular to explain the loss of symmetry observed in the macroscopic fundamental diagram of the networks, which is not captured by existing theories that rely on corridor approximations without turns. Our findings also suggest that supervised learning methods have enormous potential as they require very little examples to produce excellent policies.",
        "DOI": "10.1109/TITS.2022.3208236",
        "paper_author": "Laval J.",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Atlanta",
        "affiliation_country": "United States",
        "affiliation_id": "60136858",
        "affiliation_state": "GA"
    },
    {
        "paper_title": "Fog-GMFA-DRL: Enhanced deep reinforcement learning with hybrid grey wolf and modified moth flame optimization to enhance the load balancing in the fog-IoT environment",
        "publication": "Advances in Engineering Software",
        "citied_by": "7",
        "cover_date": "2022-12-01",
        "Abstract": "Internet of Things (IoT) can facilitate a plethora of data transactions among various servers. In the IoT, fog servers are utilized to achieve effective data transactions from dynamic devices. However, load balancing is still a significant task researcher mainly focus on mitigating the load balancing issue. Some virtual machines may be overloaded when other virtual machines are idle due to a bad scheduling policy. Therefore, the proposed model is based on dynamic load balancing in a fog-IOT environment by utilizing a novel hybrid Grey Wolf Optimization (GWO) with the Modified Moth Flame algorithm (MMFA). In addition, the GMFA mainly helps to enhance Deep reinforcement learning (DRL). The performance of the actor-critic based deep reinforcement learning (DRL) approach is enhanced with the GMFA algorithm, and this combined strategy is named GMFA-DRL. RL offers several advantages regarding resource allocation issues, and simulations demonstrate that it performs better than reactive techniques. The proposed GMFA-DRL approach is implemented through the Python-based platform-Jupyter. The performance is evaluated using performance matrices such as Throughput, Latency, Makespan, Load Balancing Level (LBL), and Energy Consumption. The simulation results illustrate that the proposed model achieves high Throughput, low energy consumption, minimum Latency, minimum Makespan, and load balancing results. Therefore, the proposed approach can be proven more effective than the existing technique.",
        "DOI": "10.1016/j.advengsoft.2022.103295",
        "paper_author": "Gupta S.",
        "affiliation_name": "Netaji Subhas University of Technology,East Campus",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India",
        "affiliation_id": "60282558",
        "affiliation_state": "DL"
    },
    {
        "paper_title": "Artificial intelligence, 21st century competences, and socio-emotional learning in education: More than high-risk?",
        "publication": "European Journal of Education",
        "citied_by": "24",
        "cover_date": "2022-12-01",
        "Abstract": "Over the last two decades, 21st century competences and socio-emotional skills have become a major focus in educational policy. In this article, skills for the 21st century, soft skills, as well as social and emotional skills, are contextualised in the context of technological change, machine learning, and the ethics of artificial intelligence. The use of data-driven AI technologies to model and measure these skills—in this article defined as non-epistemic competence components—can lead to major social challenges that have important implications for educational policies and practices. A moratorium on the use of data on these competence components in machine learning systems is proposed until the society-wide impact is better understood.",
        "DOI": "10.1111/ejed.12531",
        "paper_author": "Tuomi I.",
        "affiliation_name": "Meaning Processing Ltd.",
        "affiliation_city": "Helsinki",
        "affiliation_country": "Finland",
        "affiliation_id": "101379819",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Simulation to Real: Learning Energy-Efficient Slithering Gaits for a Snake-Like Robot",
        "publication": "IEEE Robotics and Automation Magazine",
        "citied_by": "11",
        "cover_date": "2022-12-01",
        "Abstract": "To resemble the body flexibility of biological snakes, snake-like robots are designed as a chain of body modules, which gives them many degrees of freedom (DoF) on the one hand and leads to a challenging task to control them on the other. Compared with conventional model-based control methods, reinforcement learning (RL)-based ones provide promising solutions to design agile and energy-efficient gaits for snake-like robots as RL-based methods can fully exploit the hyperredundant bodies of the robots. However, RL-based methods for snake-like robots have rarely been investigated even in simulations, let alone been deployed on real-world snake-like robots. In this work, we introduce a novel approach for designing energy-efficient gaits for a snake-like robot, which first learns a policy using an RL algorithm in simulation and then transfers it to the real-world testing, thereby leveraging a fast and economical gait-generation process. We evaluate our RL-based approach in both simulations and real-world experiments to demonstrate that it can generate substantially more energy-efficient gaits than those generated by conventional model-based controllers.",
        "DOI": "10.1109/MRA.2022.3204237",
        "paper_author": "Bing Z.",
        "affiliation_name": "Technische Universität München",
        "affiliation_city": "Munich",
        "affiliation_country": "Germany",
        "affiliation_id": "60019722",
        "affiliation_state": "Bayern"
    },
    {
        "paper_title": "Auto-scaling containerized cloud applications: A workload-driven approach",
        "publication": "Simulation Modelling Practice and Theory",
        "citied_by": "11",
        "cover_date": "2022-12-01",
        "Abstract": "Today, cloud computing presents new business opportunities as it offers various technological advantages including elastic computing and efficient pricing strategies. Although, cloud users have access to large amount of resources, it is yet a challenging task to efficiently manage the hardware resources in a cloud environment. In this article, we present PACE (Performance-aware Auto-scaler for Cloud Elasticity), a framework for auto-scaling containerized cloud applications based on workload demand. The framework offers a) reactive auto-scaling using threshold-based rules to avoid application failures during intensive workload tasks and b) proactive auto-scaling using convolutional neural networks (CNN) and K-means to generate elastic scaling policies that incorporate future workload demands. The experimental analysis is based on the Yahoo! Cloud Serving Benchmark (YCSB) executed in Redis containers deployed on the Google Cloud Platform. The proposed framework can automatically adjust cloud resources to satisfy workload demand and ensure Quality of Service (QoS) requirements.",
        "DOI": "10.1016/j.simpat.2022.102654",
        "paper_author": "Chouliaras S.",
        "affiliation_name": "School of Business, Economics and Informatics",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60162130",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Recent advances in applications of artificial intelligence in solid waste management: A review",
        "publication": "Chemosphere",
        "citied_by": "67",
        "cover_date": "2022-12-01",
        "Abstract": "Efficient management of solid waste is essential to lessen its potential health and environmental impacts. However, the current solid waste management practices encounter several challenges. The development of effective waste management systems using advanced technologies is vital to overcome the challenges faced by the current approaches. Artificial Intelligence (AI) has emerged as a powerful tool for applications in various fields. Several studies also reported the applications of AI techniques in the management of solid waste. This article critically reviews the recent advancements in the applications of AI techniques for the management of solid waste. Various AI and hybrid techniques have been successfully employed to predict the performance of various methods used for the generation, segregation, storage, and treatment of solid waste. The key challenges that limit the applications of AI in solid waste are highlighted. These include the availability and selection of applicable data, poor reproducibility, and less evidence of applications in real solid waste. Based on identified gaps and challenges, recommendations for future work are provided. This review is beneficial for all stakeholders in the field of solid waste management, including policy-makers, governments, waste management organizations, municipalities, and researchers.",
        "DOI": "10.1016/j.chemosphere.2022.136631",
        "paper_author": "Ihsanullah I.",
        "affiliation_name": "King Fahd University of Petroleum and Minerals",
        "affiliation_city": "Dhahran",
        "affiliation_country": "Saudi Arabia",
        "affiliation_id": "60009506",
        "affiliation_state": "Ash Sharqiyah"
    },
    {
        "paper_title": "Workflow scheduling in cloud environment – Challenges, tools, limitations &amp; methodologies: A review",
        "publication": "Measurement: Sensors",
        "citied_by": "27",
        "cover_date": "2022-12-01",
        "Abstract": "Workflow scheduling is a framework that simulates major domain applications like gaming, Natural language processing, data science etc., for which the task order automation and demand consistency to be maintained. This increases the complexities for CSPs based on various QoS parameters like cloud deployment models, service types, VM templates, migration process, energy dissipation etc., The recent research survey analysis and marketing perspective parameters include unknown demands, task requirements failure, cost incurs access delay, feasible deadlines, system capability, cache latency, scheduler types and policies, VMs, platform support, fault-tolerant and virtualization types and their limitations. The various survey papers considered were supporting QoS parameters of cloud environment towards industry task automation wherein the researchers will find their way to identify problem definitions with measured approaches and solutions. The analysis would pave way to incorporate machine learning approaches to derive workflows for business, healthcare, Speech recognition, Text recognition; drowsiness detection, road sign detection, metal part identification etc., The analysis done in this paper leads to various challenges and issues found in workflow scheduling approaches. The survey paper is summarized with introduction in chapter 1 followed by comparative conclusions of various research papers and concluded with summary of the findings based on the survey. This survey work can be extended considering each task in workflow and data migration types & issues which incurs budget, legal issues & policies and technical methodologies to find feasible solution for task automation.",
        "DOI": "10.1016/j.measen.2022.100436",
        "paper_author": "Menaka M.",
        "affiliation_name": "Vellore Institute of Technology",
        "affiliation_city": "Vellore",
        "affiliation_country": "India",
        "affiliation_id": "60010618",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Developing environmental hedging point policy with variable demand: A machine learning approach",
        "publication": "International Journal of Production Economics",
        "citied_by": "12",
        "cover_date": "2022-12-01",
        "Abstract": "This study evaluates the effect of carbon emission control policies on organizations' production planning and inventory management. Considering the variation of demands, breakdowns, and environmental uncertainties, we consider environmental Hedging Point Policy to control production level in relation to the costs of inventory, backlog, and emission. The effect of Cap-and-Trade, and Command-and-Control environmental policies on product lines’ strategies are evaluated. We aim to develop a production plan through optimization-based simulation and provide a solution for variable demand. Therefore, a simulation-based optimization on multi-objective particle swarm algorithm has been applied (RMSE = 0.82). To acquire practical and managerial implications, through machine learning, the environmental Hedging Point Policy parameters for variable demands are obtained. The results reveal that the Cap-and-Trade policy is more flexible and effective than the Command-and-Control in terms of reducing costs and using environmentally friendly technologies. Our approach offers an effective solution to help decision makers to dynamically plan operations for variable demands, utilize resources, and manage inventories, and increase productivity.",
        "DOI": "10.1016/j.ijpe.2022.108640",
        "paper_author": "Behnamfar R.",
        "affiliation_name": "University of Tehran",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran",
        "affiliation_id": "60022927",
        "affiliation_state": "Tehran"
    },
    {
        "paper_title": "Identifying driving factors of urban land expansion using Google Earth Engine and machine-learning approaches in Mentougou District, China",
        "publication": "Scientific Reports",
        "citied_by": "8",
        "cover_date": "2022-12-01",
        "Abstract": "The research on driving mechanisms of urban land expansion is hot topic of land science. However, the relative importance of anthropogenic-natural factors and how they affect urban land expansion change are still unclear. Based on the Google Earth Engine platform, this study used the support vector machine classifier to extract land-use datasets of Mentougou district of Beijing, China from 1990 to 2016. Supported by machine-learning approaches, multiple linear regression (MLR) and random forests (RF) were applied and compared to identify the influential factors and their relative importance on urban land expansion. The results show: There was a continuous growth in urban land expansion from 1990 to 2016, the increased area reached 6097.42 ha with an average annual rate of 8.01% and average annual intensity rate of 2.57%, respectively. Factors such as elevation, risk of goaf collapse, accessibility, local fiscal expenditure, industrial restructuring, per capita income in rural area, GDP were important drivers of urban land expansion change. The model comparison indicated that RF had greater ability than MLR to identify the non-linear relationships between urban land expansion and explanatory variables. The influencing factors of urban land expansion should be comprehensively considered to regulate new land policy actions in Mentougou.",
        "DOI": "10.1038/s41598-022-20478-z",
        "paper_author": "Cheng L.L.",
        "affiliation_name": "China University of Mining &amp; Technology, Beijing",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60108755",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Automatic berthing using supervised learning and reinforcement learning",
        "publication": "Ocean Engineering",
        "citied_by": "23",
        "cover_date": "2022-12-01",
        "Abstract": "Although various studies have been conducted on automatic berthing, including offline optimization and online control, real-time berthing control remains a difficult problem. Online control methods without reference trajectories are promising for real-time berthing control. We used reinforcement learning (RL), which is a type of machine learning, to obtain an online control law without reference trajectories. As online control for automatic berthing is difficult, obtaining an appropriate control law with naive reinforcement learning is difficult. Furthermore, almost all existing online control methods do not consider port geometries. This study proposes a method for obtaining online berthing control laws by combining supervised learning (SL) and RL. We first trained the controller using offline-calculated trajectories and then further trained it using RL. Owing to the SL process, the proposed method can start the RL process with a good control policy. We evaluated the control law performance of the proposed method in a simulation environment that considered port geometries and wind disturbances. The experimental results show that the proposed method can achieve a higher success rate and lower safety risk than the naive SL and RL algorithms.",
        "DOI": "10.1016/j.oceaneng.2022.112553",
        "paper_author": "Shimizu S.",
        "affiliation_name": "Yokohama National University",
        "affiliation_city": "Yokohama",
        "affiliation_country": "Japan",
        "affiliation_id": "60018279",
        "affiliation_state": "Kanagawa"
    },
    {
        "paper_title": "Applying an equity lens to liver health and research in Europe",
        "publication": "Journal of Hepatology",
        "citied_by": "8",
        "cover_date": "2022-12-01",
        "Abstract": "Liver disease is a major cause of premature death and disability in Europe. However, morbidity and mortality are not equally distributed in the population. In spite of this, there are few studies addressing the issue of health inequalities in Europe. In this Public Health Corner article, we compare the research conducted on health inequalities in Europe to other settings and highlight the main differences based upon an extensive review of the literature. We report that only 10.2% of studies were led by European institutions or conducted in European populations and that certain topics such as alcohol-related liver disease are largely overlooked. In addition, we discuss the relevance of including a health equity lens when conducting clinical, epidemiological and health systems’ research in liver disease and set out the basic requirements to tackle health inequalities in liver disease in Europe.",
        "DOI": "10.1016/j.jhep.2022.07.021",
        "paper_author": "Ventura-Cots M.",
        "affiliation_name": "Hospital Universitari Vall d'Hebron",
        "affiliation_city": "Barcelona",
        "affiliation_country": "Spain",
        "affiliation_id": "60012486",
        "affiliation_state": "Barcelona"
    },
    {
        "paper_title": "AI, Opacity, and Personal Autonomy",
        "publication": "Philosophy and Technology",
        "citied_by": "23",
        "cover_date": "2022-12-01",
        "Abstract": "Advancements in machine learning have fuelled the popularity of using AI decision algorithms in procedures such as bail hearings, medical diagnoses and recruitment. Academic articles, policy texts, and popularizing books alike warn that such algorithms tend to be opaque: they do not provide explanations for their outcomes. Building on a causal account of transparency and opacity as well as recent work on the value of causal explanation, I formulate a moral concern for opaque algorithms that is yet to receive a systematic treatment in the literature: when such algorithms are used in life-changing decisions, they can obstruct us from effectively shaping our lives according to our goals and preferences, thus undermining our autonomy. I argue that this concern deserves closer attention as it furnishes the call for transparency in algorithmic decision-making with both new tools and new challenges.",
        "DOI": "10.1007/s13347-022-00577-5",
        "paper_author": "Vaassen B.",
        "affiliation_name": "Umeå Universitet",
        "affiliation_city": "Umea",
        "affiliation_country": "Sweden",
        "affiliation_id": "60031040",
        "affiliation_state": "Västerbotten"
    },
    {
        "paper_title": "Heterogeneous catalysis mediated by light, electricity and enzyme via machine learning: Paradigms, applications and prospects",
        "publication": "Chemosphere",
        "citied_by": "17",
        "cover_date": "2022-12-01",
        "Abstract": "Energy crisis and environmental pollution have become the bottleneck of human sustainable development. Therefore, there is an urgent need to develop new catalysts for energy production and environmental remediation. Due to the high cost caused by blind screening and limited valuable computing resources, the traditional experimental methods and theoretical calculations are difficult to meet with the requirements. In the past decades, computer science has made great progress, especially in the field of machine learning (ML). As a new research paradigm, ML greatly accelerates the theoretical calculation methods represented by first principal calculation and molecular dynamics, and establish the physical picture of heterogeneous catalytic processes for energy and environment. This review firstly summarized the general research paradigms of ML in the discovery of catalysts. Then, the latest progresses of ML in light-, electricity- and enzyme-mediated heterogeneous catalysis were reviewed from the perspective of catalytic performance, operating conditions and reaction mechanism. The general guidelines of ML for heterogeneous catalysis were proposed. Finally, the existing problems and future development trend of ML in heterogeneous catalysis mediated by light, electricity and enzyme were summarized. We highly expect that this review will facilitate the interaction between ML and heterogeneous catalysis, and illuminate the development prospect of heterogeneous catalysis.",
        "DOI": "10.1016/j.chemosphere.2022.136447",
        "paper_author": "Zhang W.",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60025278",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Determinants of Electricity Consumption of Energy-Vulnerable Group Using Ensemble Gradient-Boosting Algorithm",
        "publication": "KSCE Journal of Civil Engineering",
        "citied_by": "1",
        "cover_date": "2022-12-01",
        "Abstract": "The increasing energy burden on vulnerable households is critical in modern cities, it is crucial to understand how cities can characterize energy vulnerability and its relationship with the environment. This study modeled relationships between energy consumption and built environmental factors to compare determinants in average and energy-vulnerable households. While the conventional approach of identifying energy vulnerability often relies on household income, this study suggested a new approach by considering the energy-vulnerable group as a low-income class with high energy expenditure. A traditional regression model (semi-log regression) and advanced machine learning algorithm (ensemble gradient boosting, XGboost) were employed to maximize the performance of the modeling processes. The results indicated that the overall modeling performance was superior with regard to the machine learning algorithm, producing the r-squared value of 0.92 for the energy-vulnerable households, compared to the 0.34 of the semi-log regression model. While the direction of the association of the determinants was similar in the average and energy-vulnerable households, the level of association exhibited a clear difference, especially for the effect of income (comparing 0.30 to 0.03) and housing type (comparing -0.45 to -0.63). The study identified several implications regarding urban energy management and policy based on the findings.",
        "DOI": "10.1007/s12205-022-1984-2",
        "paper_author": "Kim H.",
        "affiliation_name": "Pusan National University",
        "affiliation_city": "Busan",
        "affiliation_country": "South Korea",
        "affiliation_id": "60008783",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Detection and moderation of detrimental content on social media platforms: current status and future directions",
        "publication": "Social Network Analysis and Mining",
        "citied_by": "45",
        "cover_date": "2022-12-01",
        "Abstract": "Social Media has become a vital component of every individual's life in society opening a preferred spectrum of virtual communication which provides an individual with a freedom to express their views and thoughts. While virtual communication through social media platforms is highly desirable and has become an inevitable component, the dark side of social media is observed in form of detrimental/objectionable content. The reported detrimental contents are fake news, rumors, hate speech, aggressive, and cyberbullying which raise up as a major concern in the society. Such detrimental content is affecting person’s mental health and also resulted in loss which cannot be always recovered. So, detecting and moderating such content is a prime need of time. All social media platforms including Facebook, Twitter, and YouTube have made huge investments and also framed policies to detect and moderate such detrimental content. It is of paramount importance in the first place to detect such content. After successful detection, it should be moderated. With an overflowing increase in detrimental content on social media platforms, the current manual method to identify such content will never be enough. Manual and semi-automated moderation methods have reported limited success. A fully automated detection and moderation is a need of time to come up with the alarming detrimental content on social media. Artificial Intelligence (AI) has reached across all sectors and provided solutions to almost all problems, social media content detection and moderation is not an exception. So, AI-based methods like Natural Language Processing (NLP) with Machine Learning (ML) algorithms and Deep Neural Networks is rigorously deployed for detection and moderation of detrimental content on social media platforms. While detection of such content has been receiving good attention in the research community, moderation has received less attention. This research study spans into three parts wherein the first part emphasizes on the methods to detect the detrimental components using NLP. The second section describes about methods to moderate such content. The third part summarizes all observations to provide identified research gaps, unreported problems and provide research directions.",
        "DOI": "10.1007/s13278-022-00951-3",
        "paper_author": "Gongane V.U.",
        "affiliation_name": "Pune Institute of Computer Technology",
        "affiliation_city": "Pune",
        "affiliation_country": "India",
        "affiliation_id": "60272084",
        "affiliation_state": "MH"
    },
    {
        "paper_title": "A simulation experiment on ICT and patent intensity in South Africa: An application of the novel dynamic ARDL machine learning model",
        "publication": "Technological Forecasting and Social Change",
        "citied_by": "8",
        "cover_date": "2022-12-01",
        "Abstract": "The aim of this study is to examine the effect of shocks to patent intensity and its empirical and practical policy implications for the South African economy. This stems from the gap in the literature on policy simulation exercises related to the boost in Information and Communications Technology (ICT) and patent intensity in African countries. Hence, this study established the dynamic relationship between patent intensity and economic growth in South Africa for the period of 1980–2020, alongside essential macroeconomic variables such as government expenditure, gross fixed capital formation, labour force, and trade. We use the Autoregressive distributed lag model (ARDL) to capture short-run and long-run relationships, novel dynamic ARDL and Kernel-based Regularized Least Squares (KRLS) to capture the counterfactual shocks in the economic growth. The ARDL result revealed that government expenditure, labour force, and trade openness significantly foster economic growth in the long-run and short-run. Also, while patent intensity and gross fixed capital formation increase the economy in the long-run and short-run, their interaction term significantly diminishes the growth. Further in the analysis is the dynamic ARDL simulation and KRLS, which predicted the counterfactual shocks of economic growth based on a + 26 % change in patent intensity. The result showed that the increasing volume of patent intensity first has a low effect on South Africa economic growth, but later rebound upwardly, thus indicating that change in patent intensity has a long-lasting impact on sustainable economic growth. The direction that is useful for policy is also highlighted and discussed.",
        "DOI": "10.1016/j.techfore.2022.122044",
        "paper_author": "Adedoyin F.F.",
        "affiliation_name": "Bournemouth University",
        "affiliation_city": "Bournemouth",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60029336",
        "affiliation_state": "Dorset"
    },
    {
        "paper_title": "Predicting temporal and spatial variability in flood vulnerability and risk of rural communities at the watershed scale",
        "publication": "Journal of Environmental Management",
        "citied_by": "5",
        "cover_date": "2022-12-01",
        "Abstract": "Due to land-use and hydrology changes, people are constantly exposed to floods. The adverse impact of floods is greater on vulnerable populations that disproportionately inhabit flood-prone areas. This paper reports a comprehensive study on flood vulnerability of flood prone areas in residential areas of the Tajan watershed, Iran in two periods before 2006 and after 2006. Flood prone area were determined by the random forest (RF) and K-nearest neighbor (KNN) machine learning methods. To reduce time and cost, the vulnerability was assessed only in areas with very high flood hazard using 4 main criteria (social, policy, economic, infrastructure), 40 items, and 210 questionnaires across 40 villages. Independent t-test, Kruskal-Wallis, and paired t-test were used for statistical analysis of questionnaire data. The results of machine learning models (MLMs) showed that the RF model with AUC = 0.92% is more accurate in determining flood prone areas. The results of paired t-test showed that the three criteria of social (mean P1 = 2.97 and P2 = 3.35), infrastructure (mean P1 = 2.88 and P2 = 3.25), and policy (mean P1 = 3.02 and P2 = 3.50) had significant changes in both periods. The Kruskal-Wallis test also revealed the mean of all four criteria in both periods and all sub-watersheds, except three sub-watersheds 10 (Khalkhil village), 19 (Tellarem and Kerasp villages), and 23 (Dinehsar and Jafarabad), had a significant difference. The results of the t-test also showed a decrease in vulnerability in the second period (before 2006) compared to the first period (after 2006), so the number of sub-watersheds in the very high vulnerability class was more in the first period than in the second period. A vulnerability map was developed using three factors of risk zone area, area of each sub-watershed, and population of each sub-watershed.",
        "DOI": "10.1016/j.jenvman.2022.116261",
        "paper_author": "Avand M.",
        "affiliation_name": "Tarbiat Modares University",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran",
        "affiliation_id": "60032053",
        "affiliation_state": "Tehran"
    },
    {
        "paper_title": "Learning for MPC with stability &amp; safety guarantees",
        "publication": "Automatica",
        "citied_by": "21",
        "cover_date": "2022-12-01",
        "Abstract": "The combination of learning methods with Model Predictive Control (MPC) has attracted a significant amount of attention in the recent literature. The hope of this combination is to reduce the reliance of MPC schemes on accurate models, and to tap into the fast developing machine learning and reinforcement learning tools to exploit the growing amount of data available for many systems. In particular, the combination of reinforcement learning and MPC has been proposed as a viable and theoretically justified approach to introduce explainable, safe and stable policies in reinforcement learning. However, a formal theory detailing how the safety and stability of an MPC-based policy can be maintained through the parameter updates delivered by the learning tools is still lacking. This paper addresses this gap. The theory is developed for the generic robust MPC case, and applied in simulation in the robust tube-based linear MPC case, where the theory is fairly easy to deploy in practice. The paper focuses on reinforcement learning as a learning tool, but it applies to any learning method that updates the MPC parameters online.",
        "DOI": "10.1016/j.automatica.2022.110598",
        "paper_author": "Gros S.",
        "affiliation_name": "Norges Teknisk-Naturvitenskapelige Universitet",
        "affiliation_city": "Trondheim",
        "affiliation_country": "Norway",
        "affiliation_id": "60013141",
        "affiliation_state": "Trondelag"
    },
    {
        "paper_title": "Factors impacting bike crash severity in urban areas",
        "publication": "Journal of Safety Research",
        "citied_by": "21",
        "cover_date": "2022-12-01",
        "Abstract": "Introduction: Bicycling plays an important role as a major non-motorized travel mode in many urban areas. While increasingly serving as a key part of an integrated transportation demand management system and a sustainable mobility option, interest in biking as an active transportation mode has been unfortunately accompanied by an increase in the number of bike crashes, many with incapacitating injuries or fatal outcomes. Thus, to improve bicycling safety it is crucial to understand the critical factors that influence severe bicyclist crash outcomes, and to identify and prioritize policies and actions to mitigate these risks. Method: The study reported herein was conducted with this objective in mind. Our approach involves the use of classification models (logistic regression, decision tree and random forest), as well as techniques for treating unbalanced data by under sampling, oversampling, and weighted cost sensitivity (CS) learning, applied to bike crash data from the State of Tennessee's two largest urban areas, Nashville and Memphis. Results: The results indicate that random forest with weighted CS offers the potential for greater explanatory accuracy, an important observation given the paucity of efforts to date in applying random forest to bike safety studies. Inadequate lighting conditions, crashes on roadways, speed limits, average annual daily traffic, number of lanes, and weekends are the critical features identified. Conclusion: Based on these results, a series of specific, suggested policy changes are presented for implementation consideration. Practical Applications: There is existing guidance in FHWA Lighting Handbook and TDOT's Roadway Design Guidelines that spell out some engineering design solutions like lighting provisions, bicycle facility design, and traffic calming measures. These measures may alleviate the identified key features impacting fatal and incapacitating bicycle injuries. Further research should be conducted to gauge the efficacy of the solutions suggested.",
        "DOI": "10.1016/j.jsr.2022.08.010",
        "paper_author": "Dash I.",
        "affiliation_name": "Vanderbilt University School of Engineering",
        "affiliation_city": "Nashville",
        "affiliation_country": "United States",
        "affiliation_id": "60010060",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Deep reinforcement learning in an ultrafiltration system: Optimizing operating pressure and chemical cleaning conditions",
        "publication": "Chemosphere",
        "citied_by": "6",
        "cover_date": "2022-12-01",
        "Abstract": "Enhancing engineering efficiency and reducing operating costs are permanent subjects that face all engineers over the world. To effectively improve the performance of filtration systems, it is necessary to determine an optimal operating condition beyond conventional methods of periodic and empirical operation. Herein, this paper proposes an effective approach to finding an optimal operating strategy using deep reinforcement learning (DRL), particularly for an ultrafiltration (UF) system. Deep learning was developed to represent the UF system utilizing a long-short term memory and provided an environment for DRL. DRL was designed to control three actions; operating pressure, cleaning time, and cleaning concentration. Ultimately, DRL proposed the UF system to actively change the operating pressure and cleaning conditions over time toward better water productivity and operating efficiency. DRL denoted ∼20.9% of specific energy consumption can be reduced by increasing average water flux (39.5–43.7 L m−2 h−1) and reducing operating pressure (0.617–0.540 bar). Moreover, the optimal action of DRL was reasonable to achieve better performance beyond the conventional operation. Crucially, this study demonstrated that due to the nature of DRL, the approach is tractable for engineering systems that have structurally complex relationships among operating conditions and resultants.",
        "DOI": "10.1016/j.chemosphere.2022.136364",
        "paper_author": "Park S.",
        "affiliation_name": "Korea Institute of Science and Technology",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea",
        "affiliation_id": "60025960",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "An urban-level prediction of lockdown measures impact on the prevalence of the COVID-19 pandemic",
        "publication": "Genus",
        "citied_by": "4",
        "cover_date": "2022-12-01",
        "Abstract": "The world still suffers from the COVID-19 pandemic, which was identified in late 2019. The number of COVID-19 confirmed cases are increasing every day, and many governments are taking various measures and policies, such as city lockdown. It seriously treats people’s lives and health conditions, and it is highly required to immediately take appropriate actions to minimise the virus spread and manage the COVID-19 outbreak. This paper aims to study the impact of the lockdown schedule on pandemic prevention and control in Ningbo, China. For this, machine learning techniques such as the K-nearest neighbours and Random Forest are used to predict the number of COVID-19 confirmed cases according to five scenarios, including no lockdown and 2 weeks, 1, 3, and 6 months postponed lockdown. According to the results, the random forest machine learning technique outperforms the K-nearest neighbours model in terms of mean squared error and R-square. The results support that taking an early lockdown measure minimises the number of COVID-19 confirmed cases in a city and addresses that late actions lead to a sharp COVID-19 outbreak.",
        "DOI": "10.1186/s41118-022-00174-6",
        "paper_author": "Pourroostaei Ardakani S.",
        "affiliation_name": "University of Nottingham Ningbo China",
        "affiliation_city": "Ningbo",
        "affiliation_country": "China",
        "affiliation_id": "60104720",
        "affiliation_state": "Zhejiang"
    },
    {
        "paper_title": "Risks measurement in banking: A bibliometric and content analysis",
        "publication": "International Social Science Journal",
        "citied_by": "3",
        "cover_date": "2022-12-01",
        "Abstract": "Banking risk has gained interest from researchers after the global financial crisis and implementation of Basel III norms. The present study performs a scientific bibliometric analysis of research on banking risk till data. Research has been conducted on five major types of banking risks – credit, operational, market, liquidity, and interest rate risk – from 967 publications on Scopus database from 1967 to 2021. The results suggest that the research in the area is dominated by the United States, the United Kingdom, and China. The publications in top journals in the domain of finance and economics are less, but the number has increased after 2008–2009. The paper also performs content analysis on top-cited papers of five risk categories to find major themes and theoretical, practical, and methodological contributions. These findings suggest scope for more research in risks other than credit and operating risks, as existing research is centred around modelling risk and predicting default rates. Researchers in the area should focus on programming and machine learning tools. Regarding policy implications, the research identifies key publications in risk modelling, which can be used as the basis of improved supervisory mechanisms and internal controls.",
        "DOI": "10.1111/issj.12371",
        "paper_author": "Vashisht S.",
        "affiliation_name": "Lovely Professional University",
        "affiliation_city": "Phagwara",
        "affiliation_country": "India",
        "affiliation_id": "60094571",
        "affiliation_state": "PB"
    },
    {
        "paper_title": "Insurance fraud detection: Evidence from artificial intelligence and machine learning",
        "publication": "Research in International Business and Finance",
        "citied_by": "64",
        "cover_date": "2022-12-01",
        "Abstract": "This study proposes a framework for fraud detection in the auto insurance industry by using predictive models. The feature selection is performed utilizing a publicly available car insurance dataset and uncovers the most influential feature through Boruta algorithm. Three predictive models (logistic regression, support vector machine, and naïve Bayes) are applied for developing a fraud detection mechanism. Six metrics are computed from the confusion matrix to assess the performance of the predictive model. The results reveal that the support vector machine outperforms in terms of accuracy, and the logistic regression achieves the highest f-measure score. Each influential feature's ranking is performed, and it is revealed that the fault, base policy, and age of the policyholder are the most influential features. The findings of this study are beneficial for fraud detection in the auto insurance industry. Additionally, the underlying framework holds a functionality for real-time problem-solving in the auto insurance industry.",
        "DOI": "10.1016/j.ribaf.2022.101744",
        "paper_author": "Aslam F.",
        "affiliation_name": "COMSATS University Islamabad",
        "affiliation_city": "Islamabad",
        "affiliation_country": "Pakistan",
        "affiliation_id": "60089631",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Antarctic biodiversity predictions through substrate qualities and environmental DNA",
        "publication": "Frontiers in Ecology and the Environment",
        "citied_by": "8",
        "cover_date": "2022-12-01",
        "Abstract": "Antarctic conservation science is crucial for enhancing Antarctic policy and understanding alterations to terrestrial Antarctic biodiversity. Antarctic conservation will have limited long-term impacts in the absence of large-scale biodiversity data, but if such data were available, it is likely to improve environmental protection regimes. To enable the prediction of Antarctic biodiversity across continental spatial scales through proxy variables, in the absence of baseline surveys, we linked Antarctic substrate-derived environmental DNA (eDNA) sequence data from the remote Antarctic Prince Charles Mountains to a selected range of concomitantly collected measurements of substrate properties. We achieved this through application of a statistical method commonly used in machine learning. Our analysis indicated that neutral substrate pH, low conductivity, and certain substrate minerals are important predictors of the presence of basidiomycetes, chlorophytes, ciliophorans, nematodes, and tardigrades. A bootstrapped regression revealed how variations in the identified substrate parameters influence probabilities of detecting eukaryote phyla across vast and remote areas of Antarctica. We believe that our work will improve future taxon distribution modeling and aid in developing more targeted surveys of biodiversity conducted under logistically challenging conditions.",
        "DOI": "10.1002/fee.2560",
        "paper_author": "Czechowski P.",
        "affiliation_name": "University of Otago",
        "affiliation_city": "Dunedin",
        "affiliation_country": "New Zealand",
        "affiliation_id": "60017311",
        "affiliation_state": "OTA"
    },
    {
        "paper_title": "A comprehensive review of digital twin — part 1: modeling and twinning enabling technologies",
        "publication": "Structural and Multidisciplinary Optimization",
        "citied_by": "184",
        "cover_date": "2022-12-01",
        "Abstract": "As an emerging technology in the era of Industry 4.0, digital twin is gaining unprecedented attention because of its promise to further optimize process design, quality control, health monitoring, decision and policy making, and more, by comprehensively modeling the physical world as a group of interconnected digital models. In a two-part series of papers, we examine the fundamental role of different modeling techniques, twinning enabling technologies, and uncertainty quantification and optimization methods commonly used in digital twins. This first paper presents a thorough literature review of digital twin trends across many disciplines currently pursuing this area of research. Then, digital twin modeling and twinning enabling technologies are further analyzed by classifying them into two main categories: physical-to-virtual, and virtual-to-physical, based on the direction in which data flows. Finally, this paper provides perspectives on the trajectory of digital twin technology over the next decade, and introduces a few emerging areas of research which will likely be of great use in future digital twin research. In part two of this review, the role of uncertainty quantification and optimization are discussed, a battery digital twin is demonstrated, and more perspectives on the future of digital twin are shared. Code and preprocessed data for generating all the results and figures presented in the battery digital twin case study in part 2 of this review are available on Github.",
        "DOI": "10.1007/s00158-022-03425-4",
        "paper_author": "Thelen A.",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Ames",
        "affiliation_country": "United States",
        "affiliation_id": "60145770",
        "affiliation_state": "IA"
    },
    {
        "paper_title": "Balancing national economic policy outcomes for sustainable development",
        "publication": "Nature Communications",
        "citied_by": "69",
        "cover_date": "2022-12-01",
        "Abstract": "The 2030 Sustainable Development Goals (SDGs) aim at jointly improving economic, social, and environmental outcomes for human prosperity and planetary health. However, designing national economic policies that support advancement across multiple Sustainable Development Goals is hindered by the complexities of multi-sector economies and often conflicting policies. To address this, we introduce a national-scale design framework that can enable policymakers to sift through complex, non-linear, multi-sector policy spaces to identify efficient policy portfolios that balance economic, social, and environmental goals. The framework combines economy-wide sustainability simulation and artificial intelligence-driven multiobjective, multi-SDG policy search and machine learning. The framework can support multi-sector, multi-actor policy deliberation to screen efficient policy portfolios. We demonstrate the utility of the framework for a case study of Egypt by identifying policy portfolios that achieve efficient mixes of poverty and inequality reduction, economic growth, and climate change mitigation. The results show that integrated policy strategies can help achieve sustainable development while balancing adverse economic, social, and political impacts of reforms.",
        "DOI": "10.1038/s41467-022-32415-9",
        "paper_author": "Basheer M.",
        "affiliation_name": "The University of Manchester",
        "affiliation_city": "Manchester",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60003771",
        "affiliation_state": "Greater Manchester"
    },
    {
        "paper_title": "Ethical Considerations in the Application of Artificial Intelligence to Monitor Social Media for COVID-19 Data",
        "publication": "Minds and Machines",
        "citied_by": "7",
        "cover_date": "2022-12-01",
        "Abstract": "The COVID-19 pandemic and its related policies (e.g., stay at home and social distancing orders) have increased people’s use of digital technology, such as social media. Researchers have, in turn, utilized artificial intelligence to analyze social media data for public health surveillance. For example, through machine learning and natural language processing, they have monitored social media data to examine public knowledge and behavior. This paper explores the ethical considerations of using artificial intelligence to monitor social media to understand the public’s perspectives and behaviors surrounding COVID-19, including potential risks and benefits of an AI-driven approach. Importantly, investigators and ethics committees have a role in ensuring that researchers adhere to ethical principles of respect for persons, beneficence, and justice in a way that moves science forward while ensuring public safety and confidence in the process.",
        "DOI": "10.1007/s11023-022-09610-0",
        "paper_author": "Flores L.",
        "affiliation_name": "University of California, Irvine",
        "affiliation_city": "Irvine",
        "affiliation_country": "United States",
        "affiliation_id": "60007278",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Detecting the city-scale spatial pattern of the urban informal sector by using the street view images: A street vendor massive investigation case",
        "publication": "Cities",
        "citied_by": "12",
        "cover_date": "2022-12-01",
        "Abstract": "Automatically obtaining information on informal practitioners, especially their spatial distribution, has proven challenging when using traditional methods. This study addresses this issue by presenting a street view deep learning method, called the Street Informal Practitioners Spatial Investigation (SIPSI) methodology. This paper's application of this technology focuses on the study case of the street vendor, which is one of the most visible occupations in the informal economy. There were 3907 street vendors that were detected using this method; as well, the kernel density estimation indicated that they agglomerated in a multi-core cluster pattern in the city. Further analysis of the factors that influence agglomeration shows that the street vendors prefer premises that are near the lower level of the road and the higher density population sites, whereas the NIMBY (Not In My Back Yard) syndrome keeps these vendors away from the central City Business Districts and high-rent regions. The presented methodology and the study results contribute to high-efficiency investigations of informal economy employment, and it further assists in advising for the spatial governance policies improvement and implementation in any cities whose street view images are abundant and open-access.",
        "DOI": "10.1016/j.cities.2022.103959",
        "paper_author": "Liu Y.",
        "affiliation_name": "South China Agricultural University",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60032203",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "STSIIML: Study on token shuffling under incomplete information based on machine learning",
        "publication": "International Journal of Intelligent Systems",
        "citied_by": "15",
        "cover_date": "2022-12-01",
        "Abstract": "Transaction data on the public chain is open and transparent to all participants, which poses a potential threat to the privacy of participants. Some privacy-conscious token holders want to employ obfuscation methods to protect the origin and destination of their tokens, and the need for token shuffling services (TSS) arose at a historical moment. The prevailing token shuffling policies rely too heavily on blacklists in the token shuffling process, which is contrary to the idea of decentralization. Therefore, weakening or even eliminating the usage of blacklisting mechanisms in TSSs, which is an urgent issue to be addressed. In this paper, we adopt the idea of machine learning and propose a general framework for TSSs, which replaces the natural role under incomplete information with machine learning, so as to achieve the goal of eliminating the blacklist mechanism in TSSs. Then, the token shuffling process is constructed as an extended game under incomplete information on the basis of different token shuffling policies, and this incomplete information game is analyzed on the basis of poison policy, haircut policy, and suicide policy respectively. Finally, the sequential equilibrium under different games is investigated through simulations. The simulation results show that in the incomplete information game based on the poison policy, the participation of two players in the TSS is a sequential equilibrium, while in the incomplete information game based on the haircut policy, the players do not participate in the TSS is an sequential equilibrium.",
        "DOI": "10.1002/int.23033",
        "paper_author": "Wang Y.",
        "affiliation_name": "Guizhou University",
        "affiliation_city": "Guiyang",
        "affiliation_country": "China",
        "affiliation_id": "60005027",
        "affiliation_state": "Guizhou"
    },
    {
        "paper_title": "Access Control Method for EV Charging Stations Based on State Aggregation and Q-Learning",
        "publication": "Journal of Systems Science and Complexity",
        "citied_by": "1",
        "cover_date": "2022-12-01",
        "Abstract": "This paper presents intelligent access control for a charging station and a framework for dynamically and adaptively managing charging requests from randomly arriving electric vehicles (EVs), to increase the revenue of the station. First, charging service requests from random EV arrivals are described as an event-driven sequential decision process, and the decision-making relies on an event-extended state that is composed of the real-time electricity price, real-time charging station state, and EV arrival event. Second, a state aggregation method is introduced to reduce the state space by first aggregating the charging station state in the form of the remaining charging time and then further aggregating it via sort coding. Besides, mathematical calculations of the code value are provided, and their uniqueness and continuous integer characteristics are proved. Then, a corresponding Q-learning method is proposed to derive an optimal or suboptimal access control policy. The results of a case study demonstrate that the proposed learning optimisation method based on the event-extended state aggregation performs better than flat Q-learning. The space complexity and time complexity are significantly reduced, which substantially improves the learning efficiency and optimisation performance.",
        "DOI": "10.1007/s11424-022-1155-z",
        "paper_author": "Tang Z.",
        "affiliation_name": "Anhui Normal University",
        "affiliation_city": "Wuhu",
        "affiliation_country": "China",
        "affiliation_id": "60009559",
        "affiliation_state": "Anhui"
    },
    {
        "paper_title": "A smart file-level continuous data protection scheme based on security baseline",
        "publication": "International Journal of Intelligent Systems",
        "citied_by": "1",
        "cover_date": "2022-12-01",
        "Abstract": "There is a rapidly growing interest in securing big data due to the rapid development of cloud computing, big data, and other information technologies according to the fourth industrial revolution. Continuous data protection (CDP) is an effective method to deal with huge loss caused by data loss. More optimal design methods are available and studies on the establishment of the knowledge base for an efficient backup data management in the CDP field. In this paper, a knowledge-based smart file-level CDP scheme is suggested. The user's foundation database and context information are applied to machine learning technology, enabling a large amount of files' log data and context information accumulated continuously to be stored in the knowledge base using B + tree structure. This enables high performance and flexibility in the data protection management system. The result of comparative evaluation with different security risk levels for verifying the validity shows that the suggested method presented a higher performance in write/query operations and storage overhead.",
        "DOI": "10.1002/int.23034",
        "paper_author": "Xiao Y.",
        "affiliation_name": "Shandong University of Technology",
        "affiliation_city": "Zibo",
        "affiliation_country": "China",
        "affiliation_id": "60028567",
        "affiliation_state": "Shandong"
    },
    {
        "paper_title": "Economic resilience in times of public health shock: The case of the US states",
        "publication": "Research in Economics",
        "citied_by": "1",
        "cover_date": "2022-12-01",
        "Abstract": "Does adopting social distancing policies amid a health crisis, e.g., COVID-19, hurt economies? Using a machine learning approach at the intermediate stage, we applied a generalized synthetic control method to answer this question. We utilize state policy response differences. Cross-validation, a machine learning approach, is used to produce the “counterfactual” for adopting states—how they “would have behaved” without lockdown orders. We categorize states with social distancing as the treatment group and those without as the control. We employ the state time-period for fixed effects, adjusting for selection bias and endogeneity. We find significant and intuitively explicable impacts on some states, such as West Virginia, but none at the aggregate level, suggesting that social distancing may not affect the entire economy. Our work implies a resilience index utilizing the magnitude and significance of the social distancing measures to rank the states' resilience. These findings help governments and businesses better prepare for shocks.",
        "DOI": "10.1016/j.rie.2022.08.004",
        "paper_author": "Osman S.M.I.",
        "affiliation_name": "Long Island University",
        "affiliation_city": "New York",
        "affiliation_country": "United States",
        "affiliation_id": "60014631",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Simulated trial and error experiments on productivity",
        "publication": "IAES International Journal of Artificial Intelligence",
        "citied_by": "1",
        "cover_date": "2022-12-01",
        "Abstract": "Trial and error experiments in socioeconomics were proved to be beneficial by Nobel prize laureates. However, replication is challenging and costly in term of time and money. The approach required interventions on human society, and moral issues have to be carefully considered in research designs. This work tried to make the approach more feasible by developing virtual economic environment to allow simulated trial and error experiments to take place. This research demonstrated the framework using 19 macroeconomic indicators in 6 interested categories to study the effect on productivity if each indicator value grew by 5 percent for each of 65 countries. Seven predictive models including some machine learning (ML) models were compared. Neural network dominated in accurateness and was selected as the core of the simulator. Experimented results are in full of surprises, and the framework acted as expected to be a data-driven guide toward country-specific policy making.",
        "DOI": "10.11591/ijai.v11.i4.pp1570-1578",
        "paper_author": "Thamprasert K.",
        "affiliation_name": "Chiang Mai University",
        "affiliation_city": "Chiang Mai",
        "affiliation_country": "Thailand",
        "affiliation_id": "60000881",
        "affiliation_state": "Chiang Mai"
    },
    {
        "paper_title": "Diminished Structural Brain Integrity in Long-term Cannabis Users Reflects a History of Polysubstance Use",
        "publication": "Biological Psychiatry",
        "citied_by": "4",
        "cover_date": "2022-12-01",
        "Abstract": "Background: Cannabis legalization and use are outpacing our understanding of its long-term effects on brain and behavior, which is fundamental for effective policy and health practices. Existing studies are limited by small samples, cross-sectional measures, failure to separate long-term from recreational use, and inadequate control for other substance use. Here, we address these limitations by determining the structural brain integrity of long-term cannabis users in the Dunedin Study, a longitudinal investigation of a population-representative birth cohort followed to midlife. Methods: We leveraged prospective measures of cannabis, alcohol, tobacco, and other illicit drug use in addition to structural neuroimaging in 875 study members at age 45 to test for differences in both global and regional gray and white matter integrity between long-term cannabis users and lifelong nonusers. We additionally tested for dose-response associations between continuous measures of cannabis use and brain structure, including careful adjustments for use of other substances. Results: Long-term cannabis users had a thinner cortex, smaller subcortical gray matter volumes, and higher machine learning–predicted brain age than nonusers. However, these differences in structural brain integrity were explained by the propensity of long-term cannabis users to engage in polysubstance use, especially with alcohol and tobacco. Conclusions: These findings suggest that diminished midlife structural brain integrity in long-term cannabis users reflects a broader pattern of polysubstance use, underlining the importance of understanding comorbid substance use in efforts to curb the negative effects of cannabis on brain and behavior as well as establish more effective policy and health practices.",
        "DOI": "10.1016/j.biopsych.2022.06.018",
        "paper_author": "Knodt A.R.",
        "affiliation_name": "Duke University",
        "affiliation_city": "Durham",
        "affiliation_country": "United States",
        "affiliation_id": "60008724",
        "affiliation_state": "NC"
    },
    {
        "paper_title": "Aquarius - Enable Fast, Scalable, Data-Driven Service Management in the Cloud",
        "publication": "IEEE Transactions on Network and Service Management",
        "citied_by": "3",
        "cover_date": "2022-12-01",
        "Abstract": "In order to dynamically manage and update networking policies in cloud data centers, Virtual Network Functions (VNFs) use, and therefore actively collect, networking state information - and in the process, incur additional control signaling and management overhead, especially in larger data centers. In the meantime, VNFs in production prefer distributed and straightforward heuristics over advanced learning algorithms to avoid intractable additional processing latency under high-performance and low-latency networking constraints. This paper identifies the challenges of deploying learning algorithms in the context of cloud data centers, and proposes Aquarius to bridge the application of machine learning (ML) techniques on distributed systems and service management. Aquarius passively yet efficiently gathers reliable observations, and enables the use of ML techniques to collect, infer, and supply accurate networking state information - without incurring additional signaling and management overhead. It offers fine-grained and programmable visibility to distributed VNFs, and enables both open- and close-loop control over networking systems. This paper illustrates the use of Aquarius with a traffic classifier, an auto-scaling system, and a load balancer - and demonstrates the use of three different ML paradigms - unsupervised, supervised, and reinforcement learning, within Aquarius, for network state inference and service management. Testbed evaluations show that Aquarius suitably improves network state visibility and brings notable performance gains for various scenarios with low overhead.",
        "DOI": "10.1109/TNSM.2022.3197130",
        "paper_author": "Yao Z.",
        "affiliation_name": "Cisco Systems",
        "affiliation_city": "San Jose",
        "affiliation_country": "United States",
        "affiliation_id": "60030003",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Large-Scale Automated Sustainability Assessment of Infrastructure Projects Using Machine Learning Algorithms with Multisource Remote Sensing Data",
        "publication": "Journal of Infrastructure Systems",
        "citied_by": "1",
        "cover_date": "2022-12-01",
        "Abstract": "Considering the magnitude, lifespan, and environmental impacts of physical infrastructures, integration of sustainability with development policies has proved to be indispensable; accordingly, several rating systems were nationally developed to enhance implementing sustainability into physical infrastructures. Lack of automation and strategic outlook of the conventional approach to infrastructures' sustainability assessment, exacerbated by the lengthy and costly processes involved, highlights the necessity of adopting comprehensive and innovative measures. This paper principally aims at extending the scope of sustainability rating systems such as Envision by proposing a framework for large-scale and automated assessment of infrastructures. Based on the proposed framework, a single model was developed incorporating remote sensing and GIS techniques alongside the support vector machine (SVM) algorithm into the Envision rating system. The proposed model adds a certain degree of automation in assessment process regarding the criterion N.W.1.2 of Envision rating system (i.e., provide wetland and surface water buffers) as a starting point toward entire automation of the Envision system. Given the quantitative scale of the criterion N.W.1.2, our model automatically extracts (1) wetlands, (2) waterbodies, and (3) roadways through Optical Satellite_Sentinel-2A, Synthetic Aperture Radar (SAR) Satellite_ALOS-1 imagery, and shapefile from Florida Department of Transportation (FDOT). The image-based model then examines whether certain applicable specifications of Envision scoring system are met. The level of achievement is determined, and the final score in the criteria N.W.1.2 is calculated afterward. The results indicate that more than half of the existing road segments in the study area failed to obtain the minimum required score, regulated by Envision. This emphasizes the criticality of considering sustainability indicators in future infrastructure planning. In addition, the validated results confirm the feasibility of automation of other indicators of the Envision system that will help authorities see the bigger picture and make more sustainable decisions for future practices and policies.",
        "DOI": "10.1061/(ASCE)IS.1943-555X.0000703",
        "paper_author": "Shamshirgaran A.",
        "affiliation_name": "University of Tehran",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran",
        "affiliation_id": "60022927",
        "affiliation_state": "Tehran"
    },
    {
        "paper_title": "Federated Learning Over Wireless Channels: Dynamic Resource Allocation and Task Scheduling",
        "publication": "IEEE Transactions on Cognitive Communications and Networking",
        "citied_by": "16",
        "cover_date": "2022-12-01",
        "Abstract": "With the development of federated learning (FL), mobile devices (MDs) are able to train their local models with private data and send them to a central server for aggregation, thereby preventing leakage of sensitive raw data. In this paper, we aim to improve the training performance of FL systems in the context of wireless channels and stochastic energy arrivals of each MD. To this purpose, we dynamically optimize MDs' transmission power and training task scheduling. We first model this dynamic programming problem as a constrained Markov decision process (CMDP). Due to high dimensions of the proposed CMDP problem, we propose online stochastic learning methods to simplify the CMDP and design online algorithms to obtain an efficient policy for all MDs. Since there are long-term constraints in our CMDP, we utilize a Lagrange multipliers approach to tackle this issue. Furthermore, we prove the convergence of the proposed online stochastic learning algorithm. Numerical results indicate that the proposed algorithms can achieve better performance than the benchmark algorithms.",
        "DOI": "10.1109/TCCN.2022.3196009",
        "paper_author": "Chu S.",
        "affiliation_name": "Nanjing University of Science and Technology",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60010080",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "EKF-SIRD model algorithm for predicting the coronavirus (COVID-19) spreading dynamics",
        "publication": "Scientific Reports",
        "citied_by": "17",
        "cover_date": "2022-12-01",
        "Abstract": "In this paper, we study the Covid 19 disease profile in the Algerian territory since February 25, 2020 to February 13, 2021. The idea is to develop a decision support system allowing public health decision and policy-makers to have future statistics (the daily prediction of parameters) of the pandemic; and also encourage citizens for conducting health protocols. Many studies applied traditional epidemic models or machine learning models to forecast the evolution of coronavirus epidemic, but the use of such models alone to make the prediction will be less precise. For this purpose, we assume that the spread of the coronavirus is a moving target described by an epidemic model. On the basis of a SIRD model (Susceptible-Infection-Recovery- Death), we applied the EKF algorithm to predict daily all parameters. These predicted parameters will be much beneficial to hospital managers for updating the available means of hospitalization (beds, oxygen concentrator, etc.) in order to reduce the mortality rate and the infected. Simulations carried out reveal that the EKF seems to be more efficient according to the obtained results.",
        "DOI": "10.1038/s41598-022-16496-6",
        "paper_author": "Sebbagh A.",
        "affiliation_name": "Université 8 Mai 1945 Guelma",
        "affiliation_city": "Guelma",
        "affiliation_country": "Algeria",
        "affiliation_id": "60069303",
        "affiliation_state": "Guelma Province"
    },
    {
        "paper_title": "Reinforcement learning for industrial process control: A case study in flatness control in steel industry",
        "publication": "Computers in Industry",
        "citied_by": "42",
        "cover_date": "2022-12-01",
        "Abstract": "Strip rolling is a typical manufacturing process, in which conventional control approaches are widely applied. Development of the control algorithms requires a mathematical expression of the process by means of the ﬁrst principles or empirical models. However, it is difficult to upgrade the conventional control approaches in response to the ever-changing requirements and environmental conditions because domain knowledge of control engineering, mechanical engineering, and material science is required. Reinforcement learning is a machine learning method that can make the agent learn from interacting with the environment, thus avoiding the need for the above mentioned mathematical expression. This paper proposes a novel approach that combines ensemble learning with reinforcement learning methods for strip rolling control. Based on the proximal policy optimization (PPO), a multi-actor PPO is proposed. Each randomly initialized actor interacts with the environment in parallel, but only the experience from the actor that obtains the highest reward is used for updating the actors. Simulation results show that the proposed method outperforms the conventional control methods and the state-of-the-art reinforcement learning methods in terms of process capability and smoothness.",
        "DOI": "10.1016/j.compind.2022.103748",
        "paper_author": "Deng J.",
        "affiliation_name": "Aalto University",
        "affiliation_city": "Espoo",
        "affiliation_country": "Finland",
        "affiliation_id": "60103653",
        "affiliation_state": "Uusimaa"
    },
    {
        "paper_title": "Aiding the prescriber: developing a machine learning approach to personalized risk modeling for chronic opioid therapy amongst US Army soldiers",
        "publication": "Health Care Management Science",
        "citied_by": "6",
        "cover_date": "2022-12-01",
        "Abstract": "The opioid epidemic is a major policy concern. The widespread availability of opioids, which is fueled by physician prescribing patterns, medication diversion, and the interaction with potential illicit opioid use, has been implicated as proximal cause for subsequent opioid dependence and mortality. Risk indicators related to chronic opioid therapy (COT) at the point of care may influence physicians’ prescribing decisions, potentially reducing rates of dependency and abuse. In this paper, we investigate the performance of machine learning algorithms for predicting the risk of COT. Using data on over 12 million observations of active duty US Army soldiers, we apply machine learning models to predict the risk of COT in the initial months of prescription. We use the area under the curve (AUC) as an overall measure of model performance, and we focus on the positive predictive value (PPV), which reflects the models’ ability to accurately target military members for intervention. Of the many models tested, AUC ranges between 0.83 and 0.87. When we focus on the top 1% of members at highest risk, we observe a PPV value of 8.4% and 20.3% for months 1 and 3, respectively. We further investigate the performance of sparse models that can be implemented in sparse data environments. We find that when the goal is to identify patients at the highest risk of chronic use, these sparse linear models achieve a performance similar to models trained on hundreds of variables. Our predictive models exhibit high accuracy and can alert prescribers to the risk of COT for the highest risk patients. Optimized sparse models identify a parsimonious set of factors to predict COT: initial supply of opioids, the supply of opioids in the month being studied, and the number of prescriptions for psychotropic medications. Future research should investigate the possible effects of these tools on prescriber behavior (e.g., the benefit of clinician nudging at the point of care in outpatient settings).",
        "DOI": "10.1007/s10729-022-09605-4",
        "paper_author": "Bjarnadóttir M.V.",
        "affiliation_name": "Robert H. Smith School of Business",
        "affiliation_city": "College Park",
        "affiliation_country": "United States",
        "affiliation_id": "60097292",
        "affiliation_state": "MD"
    },
    {
        "paper_title": "AADG: Automatic Augmentation for Domain Generalization on Retinal Image Segmentation",
        "publication": "IEEE Transactions on Medical Imaging",
        "citied_by": "50",
        "cover_date": "2022-12-01",
        "Abstract": "Convolutional neural networks have been widely applied to medical image segmentation and have achieved considerable performance. However, the performance may be significantly affected by the domain gap between training data (source domain) and testing data (target domain). To address this issue, we propose a data manipulation based domain generalization method, called Automated Augmentation for Domain Generalization (AADG). Our AADG framework can effectively sample data augmentation policies that generate novel domains and diversify the training set from an appropriate search space. Specifically, we introduce a novel proxy task maximizing the diversity among multiple augmented novel domains as measured by the Sinkhorn distance in a unit sphere space, making automated augmentation tractable. Adversarial training and deep reinforcement learning are employed to efficiently search the objectives. Quantitative and qualitative experiments on 11 publicly-accessible fundus image datasets (four for retinal vessel segmentation, four for optic disc and cup (OD/OC) segmentation and three for retinal lesion segmentation) are comprehensively performed. Two OCTA datasets for retinal vasculature segmentation are further involved to validate cross-modality generalization. Our proposed AADG exhibits state-of-the-art generalization performance and outperforms existing approaches by considerable margins on retinal vessel, OD/OC and lesion segmentation tasks. The learned policies are empirically validated to be model-agnostic and can transfer well to other models. The source code is available at https://github.com/CRazorback/AADG.",
        "DOI": "10.1109/TMI.2022.3193146",
        "paper_author": "Lyu J.",
        "affiliation_name": "Southern University of Science and Technology",
        "affiliation_city": "Shenzhen",
        "affiliation_country": "China",
        "affiliation_id": "60105683",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "DNN-based policies for stochastic AC OPF",
        "publication": "Electric Power Systems Research",
        "citied_by": "9",
        "cover_date": "2022-12-01",
        "Abstract": "A prominent challenge to the safe and optimal operation of the modern power grid arises due to growing uncertainties in loads and renewables. Stochastic optimal power flow (SOPF) formulations provide a mechanism to handle these uncertainties by computing dispatch decisions and control policies that maintain feasibility under uncertainty. Most SOPF formulations consider simple control policies such as affine policies that are mathematically simple and resemble many policies used in current practice. Motivated by the efficacy of machine learning (ML) algorithms and the potential benefits of general control policies for cost and constraint enforcement, we put forth a deep neural network (DNN)-based policy that predicts the generator dispatch decisions in real time in response to uncertainty. The weights of the DNN are learnt using stochastic primal–dual updates that solve the SOPF without the need for prior generation of training labels and can explicitly account for the feasibility constraints in the SOPF. The advantages of the DNN policy over simpler policies and their efficacy in enforcing safety limits and producing near optimal solutions are demonstrated in the context of a chance constrained formulation on a number of test cases.",
        "DOI": "10.1016/j.epsr.2022.108563",
        "paper_author": "Gupta S.",
        "affiliation_name": "Virginia Tech College of Engineering",
        "affiliation_city": "Blacksburg",
        "affiliation_country": "United States",
        "affiliation_id": "60157272",
        "affiliation_state": "VA"
    },
    {
        "paper_title": "Cross-city crash severity analysis with cost-sensitive transfer learning algorithm",
        "publication": "Expert Systems with Applications",
        "citied_by": "10",
        "cover_date": "2022-12-01",
        "Abstract": "Due to the difficulties of data collection in some cities and the relatively small portion of more severe crashes, this paper proposes a cost-sensitive transfer learning framework for more robust crash severity analysis. Specifically, to address the class imbalance issue, cost-sensitive learning method is adopted by assigning unequal cost values to different crash severity levels to obtain unbiased classification results. Moreover, due to the existence of city-irrelevant common crash contributing factors, cross-city crash severity analysis is developed, i.e., transferring common crash severity knowledge from other cities with transfer learning algorithm to assist crash severity modelling in target city that suffers from data scarcity problem. In this study, the crash datasets of Victorian Australia and Seattle are selected as the target and source domains respectively. To address the issue of heterogeneous explanatory features among these two datasets and extract the interpretable common crash severity knowledge, a feature alignment approach is proposed which can represent the cross-city data with unified feature representations on their original explanatory feature spaces. The two proposed models, i.e., cost-sensitive transfer logistic regression (CST-LR) and cost-sensitive transfer support vector machine (CST-SVM), have demonstrated better performance in comparison with twelve commonly used crash severity models, especially when target city crash data is scarce. The most significant crash contributing factors extracted by proposed model also show higher degree of consistency with the true contributing factors obtained from the target dataset, in comparison with the model built without transfer learning. The results could provide policy implications and counter measures for crash severity mitigation.",
        "DOI": "10.1016/j.eswa.2022.118129",
        "paper_author": "Wan J.",
        "affiliation_name": "Hohai University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60010851",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "Forecasting the evolution of fast-changing transportation networks using machine learning",
        "publication": "Nature Communications",
        "citied_by": "19",
        "cover_date": "2022-12-01",
        "Abstract": "Transportation networks play a critical role in human mobility and the exchange of goods, but they are also the primary vehicles for the worldwide spread of infections, and account for a significant fraction of CO2 emissions. We investigate the edge removal dynamics of two mature but fast-changing transportation networks: the Brazilian domestic bus transportation network and the U.S. domestic air transportation network. We use machine learning approaches to predict edge removal on a monthly time scale and find that models trained on data for a given month predict edge removals for the same month with high accuracy. For the air transportation network, we also find that models trained for a given month are still accurate for other months even in the presence of external shocks. We take advantage of this approach to forecast the impact of a hypothetical dramatic reduction in the scale of the U.S. air transportation network as a result of policies to reduce CO2 emissions. Our forecasting approach could be helpful in building scenarios for planning future infrastructure.",
        "DOI": "10.1038/s41467-022-31911-2",
        "paper_author": "Lei W.",
        "affiliation_name": "Northwestern University",
        "affiliation_city": "Evanston",
        "affiliation_country": "United States",
        "affiliation_id": "60007363",
        "affiliation_state": "IL"
    },
    {
        "paper_title": "Survey of Techniques on Data Leakage Protection and Methods to address the Insider threat",
        "publication": "Cluster Computing",
        "citied_by": "16",
        "cover_date": "2022-12-01",
        "Abstract": "Data leakage is a problem that companies and organizations face every day around the world. Mainly the data leak caused by the internal threat posed by authorized personnel to manipulate confidential information. The main objective of this work is to survey the literature to detect the existing techniques to protect against data leakage and to identify the methods used to address the insider threat. For this, a literature review of scientific databases was carried out in the period from 2011 to 2022, which resulted in 42 relevant papers. It was obtained that from 2017 to date, 60% of the studies found are concentrated and that 90% come from conferences and publications in journals. Significant advances were detected in protection systems against data leakage with the incorporation of new techniques and technologies, such as machine learning, blockchain, and digital rights management policies. In 40% of the relevant studies, significant interest was shown in avoiding internal threats. The most used techniques in the analyzed DLP tools were encryption and machine learning.",
        "DOI": "10.1007/s10586-022-03668-2",
        "paper_author": "Herrera Montano I.",
        "affiliation_name": "Universidad de Valladolid",
        "affiliation_city": "Valladolid",
        "affiliation_country": "Spain",
        "affiliation_id": "60024695",
        "affiliation_state": "Valladolid"
    },
    {
        "paper_title": "A daily carbon emission prediction model combining two-stage feature selection and optimized extreme learning machine",
        "publication": "Environmental Science and Pollution Research",
        "citied_by": "23",
        "cover_date": "2022-12-01",
        "Abstract": "Global warming caused by increased carbon emissions is a common challenge for all mankind. Facing the unprecedented pressure of carbon emission reduction, it is particularly important to grasp the dynamics of carbon emission in time and accurately. This paper proposes a novel daily carbon emission forecasting model. Firstly, the daily carbon emission data is decomposed into a series of completely noise-free mode functions by improved complete ensemble empirical mode decomposition method with adaptive noise (ICEEMDAN). Then, a two-stage feature selection method composed of partial autocorrelation function (PACF) and ReliefF is applied to select appropriate input variables for the next prediction process. Finally, the extreme learning machine optimized by improved sparrow search algorithm (ISSA-ELM) is used to predict. The empirical results show that the proposed two-stage feature selection method can further improve the prediction accuracy. After two-stage feature selection, the values of R2, MAPE, and RMSE were improved by 0.55%, 30.23%, and 28.46%, respectively. It can also be found that ISSA has good optimization performance. By combining with ISSA, R2, MAPE, and RMSE improved by 7.60%, 31.97%, and 44.79%, respectively. Therefore, the proposed model can provide a valuable reference for the formulation of carbon emission reduction policies and future carbon emission prediction research.",
        "DOI": "10.1007/s11356-022-21277-9",
        "paper_author": "Kong F.",
        "affiliation_name": "North China Electric Power University (Baoding)",
        "affiliation_city": "Baoding",
        "affiliation_country": "China",
        "affiliation_id": "60108757",
        "affiliation_state": "Hebei"
    },
    {
        "paper_title": "Real-Time Classification of Real-Time Communications",
        "publication": "IEEE Transactions on Network and Service Management",
        "citied_by": "6",
        "cover_date": "2022-12-01",
        "Abstract": "Real-time communication (RTC) applications have become largely popular in the last decade with the spread of broadband and mobile Internet access. Nowadays, these platforms are a fundamental means for connecting people and supporting businesses that increasingly rely on forms of remote work. In this context, it is of paramount importance to operate at the network level to ensure adequate Quality of Experience (QoE) for users, and appropriate traffic management policies are essential to prioritize RTC traffic. This in turn requires the network to be able to identify RTC streams and the type of content they carry. In this paper, we propose a machine learning-based application to classify media streams generated by RTC applications encapsulated in Secure Real-Time Protocol (SRTP) flows in real-time. Using carefully tuned features extracted from packet characteristics, we train models to classify streams into a variety of classes, including media type (audio/video), video quality, and redundant streams. We validate our approach using traffic from over 62 hours of multi-party meetings conducted using two popular RTC applications, namely Cisco Webex Teams and Jitsi Meet. We achieve an overall accuracy of 96% for Webex and 95% for Jitsi, using a lightweight decision tree model that makes decisions based solely on 1 second of real-time traffic. Our results show that models trained for a particular meeting software have difficulty when used with another one, although domain adaptation techniques facilitate the transfer of pre-trained models.",
        "DOI": "10.1109/TNSM.2022.3189628",
        "paper_author": "Perna G.",
        "affiliation_name": "Politecnico di Torino",
        "affiliation_city": "Turin",
        "affiliation_country": "Italy",
        "affiliation_id": "60012162",
        "affiliation_state": "TO"
    },
    {
        "paper_title": "Learning to Transmit Fresh Information in Energy Harvesting Networks",
        "publication": "IEEE Transactions on Green Communications and Networking",
        "citied_by": "9",
        "cover_date": "2022-12-01",
        "Abstract": "We study age of information (AoI) minimization in an ad hoc network consisting of energy harvesting transmitters that are scheduled to send status updates to their intended receivers. The transmission scheduling with power allocation problem over a communication session is first studied assuming apriori knowledge of channel state information, harvested energy, and update packet arrivals, i.e., the offline setting. The global optimal scheduling policy in this case is the solution of a mixed integer linear program which is known to be computationally hard. We propose a supervised-learning-based algorithm to mitigate the high computational complexity. A bidirectional recurrent neural network that interprets user scheduling as a time-series classification problem is trained and tested to achieve near-optimal AoI. Next, we consider online scheduling and power allocation with causal knowledge of the system state, which is an infinite-state Markov decision problem. In this case, the related reinforcement learning problem is solved by a model-free on-policy deep reinforcement learning, where the actor-critic algorithm with deep neural network function approximation is implemented. Comparable AoI to the optimal is demonstrated and faster runtime of learning solvers is observed, verifying the efficacy of learning in terms of both optimality and computational energy efficiency for AoI-focused scheduling and resource allocation problems in wireless networks.",
        "DOI": "10.1109/TGCN.2022.3190007",
        "paper_author": "Leng S.",
        "affiliation_name": "Penn State College of Engineering",
        "affiliation_city": "University Park",
        "affiliation_country": "United States",
        "affiliation_id": "60147936",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "Combining Device Behavioral Models and Building Schema for Cybersecurity of Large-Scale IoT Infrastructure",
        "publication": "IEEE Internet of Things Journal",
        "citied_by": "5",
        "cover_date": "2022-12-01",
        "Abstract": "Modern buildings are increasingly getting connected by adopting a range of IoT devices and applications from video surveillance and lighting to people counting and access control. It has been shown that rich connectivity can make building networks more exposed to cyberattacks and, hence, difficult to manage. Currently, there is no systematic approach for evaluating or enforcing cybersecurity of building systems with a large number of heterogeneous IoT devices. In this article, we aim to enhance cybersecurity of a large-scale IoT infrastructure by formally capturing the expected behavior of the system using the static profile of devices' intended usage, buildings information, and network configurations (predeployment) along with dynamic diagnosis (post-deployment) of network activity using machine-learning models. Our contributions are threefold: 1) we develop a tool that automatically generates a formal ontology of network communications for a connected infrastructure by taking a description of buildings (in the form of Brick schema), device network behavior (in the form of manufacturer usage description (MUD) specifications, MUD profile), and network configurations (address, port, and VLAN) as inputs. We contribute our tool as opensource, and apply it to a subset of our university smart campus testbed, covering 20 IoT devices of three types deployed in seven different buildings. We translate the formal model into network flow rules and enforce them to the network at runtime using programmable networking techniques; 2) we, then, measure the network activity of device-specific flow rules and diagnose their health using a set of trained anomaly detection models (one-class classifiers) each corresponding to a particular type of device and specific building location, and demonstrate how our method detects attacks with reasonable accuracy of 92.5%; and (3) finally, we demonstrate three types of location-defined network policies (deployment, administrative, and organizational) that can be verified by this formal model.",
        "DOI": "10.1109/JIOT.2022.3189350",
        "paper_author": "Hamza A.",
        "affiliation_name": "University of New South Wales, School of Electrical Engineering and Telecommunications",
        "affiliation_city": "Sydney",
        "affiliation_country": "Australia",
        "affiliation_id": "60082337",
        "affiliation_state": "NSW"
    },
    {
        "paper_title": "Multi-agent deep reinforcement learning based Predictive Maintenance on parallel machines",
        "publication": "Robotics and Computer-Integrated Manufacturing",
        "citied_by": "49",
        "cover_date": "2022-12-01",
        "Abstract": "In the context of Industry 4.0, companies understand the advantages of performing Predictive Maintenance (PdM). However, when moving towards PdM, several considerations must be carefully examined. First, they need to have a sufficient number of production machines and relative fault data to generate maintenance predictions. Second, they need to adopt the right maintenance approach, which, ideally, should self-adapt to the machinery, priorities of the organization, technician skills, but also to be able to deal with uncertainty. Reinforcement learning (RL) is envisioned as a key technique in this regard due to its inherent ability to learn by interacting through trials and errors, but very few RL-based maintenance frameworks have been proposed so far in the literature, or are limited in several respects. This paper proposes a new multi-agent approach that learns a maintenance policy performed by technicians, under the uncertainty of multiple machine failures. This approach comprises RL agents that partially observe the state of each machine to coordinate the decision-making in maintenance scheduling, resulting in the dynamic assignment of maintenance tasks to technicians (with different skills) over a set of machines. Experimental evaluation shows that our RL-based maintenance policy outperforms traditional maintenance policies (incl., corrective and preventive ones) in terms of failure prevention and downtime, improving by ≈75% the overall performance.",
        "DOI": "10.1016/j.rcim.2022.102406",
        "paper_author": "Ruiz Rodríguez M.L.",
        "affiliation_name": "University of Luxembourg",
        "affiliation_city": "Esch-sur-Alzette",
        "affiliation_country": "Luxembourg",
        "affiliation_id": "60072562",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Context-Aware Online Client Selection for Hierarchical Federated Learning",
        "publication": "IEEE Transactions on Parallel and Distributed Systems",
        "citied_by": "50",
        "cover_date": "2022-12-01",
        "Abstract": "Federated Learning (FL) has been considered as an appealing framework to tackle data privacy issues of mobile devices compared to conventional Machine Learning (ML). Using Edge Servers (ESs) as intermediaries to perform model aggregation in proximity can reduce the transmission overhead, and it enables great potential in low-latency FL, where the hierarchical architecture of FL (HFL) has been attracted more attention. Designing a proper client selection policy can significantly improve training performance, and it has been widely investigated in conventional FL studies. However, to the best of our knowledge, systematic client selection policies have not yet been fully studied for HFL. In addition, client selection for HFL faces more challenges than conventional FL (e.g., the time-varying connection of client-ES pairs and the limited budget of the Network Operator (NO)). In this article, we investigate a client selection problem for HFL, where the NO learns the number of successful participating clients to improve training performance (i.e., select as many clients in each round) as well as under the limited budget on each ES. An online policy, called Context-aware Online Client Selection (COCS), is developed based on Contextual Combinatorial Multi-Armed Bandit (CC-MAB). COCS observes the side-information (context) of local computing and transmission of client-ES pairs and makes client selection decisions to maximize NO's utility given a limited budget. Theoretically, COCS achieves a sublinear regret compared to an Oracle policy on both strongly convex and non-convex HFL. Simulation results also support the efficiency of the proposed COCS policy on real-world datasets.",
        "DOI": "10.1109/TPDS.2022.3186960",
        "paper_author": "Qu Z.",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Tampa",
        "affiliation_country": "United States",
        "affiliation_id": "60156602",
        "affiliation_state": "FL"
    },
    {
        "paper_title": "From Simulation to Reality: A Learning Framework for Fish-Like Robots to Perform Control Tasks",
        "publication": "IEEE Transactions on Robotics",
        "citied_by": "26",
        "cover_date": "2022-12-01",
        "Abstract": "The fish-like robot is one of the typical underwater robots, which has the advantage of high maneuverability with low noise due to its bioinspired structure and biomimetic locomotion. However, it is challenging to efficiently design motion controllers for such robots to achieve satisfactory performance on specific control tasks in the real underwater environment, since the complex fluid-structure interaction exists during their swimming and exact dynamic models are absent. In this article, we propose a learning framework, incorporating a simulation system and a training methodology, to autonomously and fast train in simulation to create control policies that are capable of directly applying to a type of physical fish-like robots to perform motion control tasks. First, we construct a simulation system combining a data-driven environment and a computational fluid dynamics (CFD)-based environment, thus well balancing the simulation accuracy and the calculation speed. Second, we design a training methodology to train deep reinforcement learning (DRL)-based policies for the robot in our constructed simulation system to perform a specific control task. Then, we use two typical motion control tasks to verify our proposed framework. One is the path-following control task, which is a one-objective problem with dense rewards, while the other is the pose control task which is a two-objective problem with sparse rewards. For each task, the DRL-based control policy trained by our learning framework is directly deployed on the physical fish-like robot to perform the task in the real world. Experimental results show that the policies trained in simulation still work well in the real world, and perform even better in terms of control accuracy and stability compared with the traditional control methods, thus demonstrating the effectiveness of our learning framework.",
        "DOI": "10.1109/TRO.2022.3181014",
        "paper_author": "Zhang T.",
        "affiliation_name": "State Key Laboratory for Turbulence &amp; Complex System",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60124548",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Data fusion and machine learning for ship fuel efficiency modeling: Part I – Voyage report data and meteorological data",
        "publication": "Communications in Transportation Research",
        "citied_by": "46",
        "cover_date": "2022-12-01",
        "Abstract": "The International Maritime Organization has been promoting energy-efficient operational measures to reduce ships' bunker fuel consumption and the accompanying emissions, including speed optimization, trim optimization, weather routing, and the virtual arrival policy. The theoretical foundation of these measures is a model that can accurately forecast a ship's bunker fuel consumption rate according to its sailing speed, displacement/draft, trim, weather conditions, and sea conditions. Voyage report is an important data source for ship fuel efficiency modeling but its information quality on weather and sea conditions is limited by a snapshotting practice with eye inspection. To overcome this issue, this study develops a solution to fuse voyage report data and publicly accessible meteorological data and constructs nine datasets based on this data fusion solution. Eleven widely-adopted machine learning models were tested over these datasets for eight 8100-TEU to 14,000-TEU containerships from a global shipping company. The best datasets found reveal the benefits of fusing voyage report data and meteorological data, as well as the practically acceptable quality of voyage report data. Extremely randomized trees (ET), AdaBoost (AB), Gradient Tree Boosting (GB) and XGBoost (XG) present the best fit and generalization performances. Their R2 values over the best datasets are all above 0.96 and even reach 0.99 to 1.00 for the training set, and 0.74 to 0.90 for the test set. Their fit errors on daily bunker fuel consumption are usually between 0.5 and 4.0 ton/day. These models have good interpretability in explaining the relative importance of different determinants to a ship's fuel consumption rate.",
        "DOI": "10.1016/j.commtr.2022.100074",
        "paper_author": "Li X.",
        "affiliation_name": "Australian Maritime College",
        "affiliation_city": "Launceston",
        "affiliation_country": "Australia",
        "affiliation_id": "60007015",
        "affiliation_state": "TS"
    },
    {
        "paper_title": "Dynamic job shop scheduling based on deep reinforcement learning for multi-agent manufacturing systems",
        "publication": "Robotics and Computer-Integrated Manufacturing",
        "citied_by": "122",
        "cover_date": "2022-12-01",
        "Abstract": "Personalized orders bring challenges to the production paradigm, and there is an urgent need for the dynamic responsiveness and self-adjustment ability of the workshop. Traditional dispatching rules and heuristic algorithms solve the production planning and control problems by making schedules. However, the previous methods cannot work well in a changeable workshop environment when encountering a large number of stochastic disturbances of orders and resources. Recently, the potential of artificial intelligence (AI) algorithms in solving the dynamic scheduling problem has attracted researchers' attention. Therefore, this paper presents a multi-agent manufacturing system based on deep reinforcement learning (DRL), which integrates the self-organization mechanism and self-learning strategy. Firstly, the manufacturing equipment in the workshop is constructed as an equipment agent with the support of edge computing node, and an improved contract network protocol (CNP) is applied to guide the cooperation and competition among multiple agents, so as to complete personalized orders efficiently. Secondly, a multi-layer perceptron is employed to establish the decision-making module called AI scheduler inside the equipment agent. According to the perceived workshop state information, AI scheduler intelligently generates an optimal production strategy to perform task allocation. Then, based on the collected sample trajectories of scheduling process, AI scheduler is periodically trained and updated through the proximal policy optimization (PPO) algorithm to improve its decision-making performance. Finally, in the multi-agent manufacturing system testbed, dynamic events such as stochastic job insertions and unpredictable machine failures are considered in the verification experiments. The experimental results show that the proposed method is capable of obtaining the scheduling solutions that meet various performance metrics, as well as dealing with resource or task disturbances efficiently and autonomously.",
        "DOI": "10.1016/j.rcim.2022.102412",
        "paper_author": "Zhang Y.",
        "affiliation_name": "Nanjing University of Aeronautics and Astronautics",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60021666",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "A novel deep reinforcement learning scheme for task scheduling in cloud computing",
        "publication": "Cluster Computing",
        "citied_by": "29",
        "cover_date": "2022-12-01",
        "Abstract": "Recently, the demand of cloud computing systems has increased drastically due to their significant use in various real-time online and offline applications. Moreover, it is widely being adopted from research, academia and industrial field as a main solution for computation and storage platform. Due to increased workload and big-data, the cloud servers receive huge amount of data storage and computation request which need to be processed through cloud modules by mapping the tasks to available virtual machines. The cloud computing models consume huge amount of energy and resources to complete these tasks. Thus, the energy aware and efficient task scheduling approach need to be developed to mitigate these issues. Several techniques have been introduced for task scheduling, where most of the techniques are based on the heuristic algorithms, where the scheduling problem is considered as NP-hard problem and obtain near optimal solution. But handling the different size of tasks and achieving near optimal solution for varied number of VMs according to the task configuration remains a challenging task. To overcome these issues, we present a machine learning based technique and adopted deep reinforcement learning approach. In the proposed approach, we present a novel policy to maximize the reward for task scheduling actions. An extensive comparative analysis is also presented, which shows that the proposed approach achieves better performance, when compared with existing techniques in terms of makespan, throughput, resource utilization and energy consumption.",
        "DOI": "10.1007/s10586-022-03630-2",
        "paper_author": "Siddesha K.",
        "affiliation_name": "Dr. Ambedkar Institute of Technology",
        "affiliation_city": "Bengaluru",
        "affiliation_country": "India",
        "affiliation_id": "60094585",
        "affiliation_state": "KA"
    },
    {
        "paper_title": "Operationalising ‘toxicity’ in the manosphere: Automation, platform governance and community health",
        "publication": "Convergence",
        "citied_by": "5",
        "cover_date": "2022-12-01",
        "Abstract": "Social media platforms have been struggling to moderate at scale. In an effort to better cope with content moderation discussion has turned to the role that automated machine-learning (ML) tools might play. The development of automated systems by social media platforms is a notoriously opaque process and public values that pertain to the common good are at stake within these often-obscured processes. One site in which social values are being negotiated is in the framing of what is considered ‘toxic’ by platforms in the development of automated moderation processes. This study takes into consideration differing notions of toxicity – community, platform and societal by examining three measures of toxicity and community health (the ML tool Perspective API; Reddit’s 2020 Content Policy; and the Sense of Community Index-2) and how they are operationalised in the context of r/MGTOW – an antifeminist group known for its misogyny. Several stages of content analysis were conducted on the top posts and comments in r/MGTOW to examine how these different measures of toxicity operate. This paper provides insight into the logics and technicalities of automated moderation tools, platform governance structures, and frameworks for understanding community metrics to interrogate existing uses of ‘toxicity’ as applied to cultural or social subcommunities online. We make a distinction between two used terms: civility and toxicity. Our analysis points to a tension between current social framings and operationalised notions of ‘toxicity’. We argue that there is a clear distinction between civility and toxicity – incivility is a measure of internal perceptions of harm within a community, whereas toxicity is a measure of the capacity for social harms outside of the bounds of the community. This nuanced understanding will enable more targeted interventions to be developed to destabilise the internal conditions that make groups like r/MGTOW internally ‘healthy’ yet externally toxic.",
        "DOI": "10.1177/13548565221111075",
        "paper_author": "Trott V.",
        "affiliation_name": "Monash University",
        "affiliation_city": "Melbourne",
        "affiliation_country": "Australia",
        "affiliation_id": "60019578",
        "affiliation_state": "VIC"
    },
    {
        "paper_title": "An assessment of random forest technique using simulation study: illustration with infant mortality in Bangladesh",
        "publication": "Health Information Science and Systems",
        "citied_by": "3",
        "cover_date": "2022-12-01",
        "Abstract": "We aimed to assess different machine learning techniques for predicting infant mortality (<1 year) in Bangladesh. The decision tree (DT), random forest (RF), support vector machine (SVM) and logistic regression (LR) approaches were evaluated through accuracy, sensitivity, specificity, precision, F1-score, receiver operating characteristics curve and k-fold cross-validation via simulations. The Boruta algorithm and chi-square (χ2) test were used for features selection of infant mortality. Overall, the RF technique (Boruta: accuracy = 0.8890, sensitivity = 0.0480, specificity = 0.9789, precision = 0.1960, F1-score = 0.0771, AUC = 0.6590; χ2: accuracy = 0.8856, sensitivity = 0.0536, specificity = 0.9745, precision = 0.1837, F1-score = 0.0828, AUC = 0.6480) showed higher predictive performance for infant mortality compared to other approaches. Age at first marriage and birth, body mass index (BMI), birth interval, place of residence, religion, administrative division, parents education, occupation of mother, media-exposure, wealth index, gender of child, birth order, children ever born, toilet facility and cooking fuel were potential determinants of infant mortality in Bangladesh. Study findings may help women, stakeholders and policy-makers to take necessary steps for reducing infant mortality by creating awareness, expanding educational programs at community levels and public health interventions.",
        "DOI": "10.1007/s13755-022-00180-0",
        "paper_author": "Rahman A.",
        "affiliation_name": "Jahangirnagar University",
        "affiliation_city": "Dhaka",
        "affiliation_country": "Bangladesh",
        "affiliation_id": "60029276",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Legal concerns in health-related artificial intelligence: a scoping review protocol",
        "publication": "Systematic Reviews",
        "citied_by": "16",
        "cover_date": "2022-12-01",
        "Abstract": "Background: Medical innovations offer tremendous hope. Yet, similar innovations in governance (law, policy, ethics) are likely necessary if society is to realize medical innovations’ fruits and avoid their pitfalls. As innovations in artificial intelligence (AI) advance at a rapid pace, scholars across multiple disciplines are articulating concerns in health-related AI that likely require legal responses to ensure the requisite balance. These scholarly perspectives may provide critical insights into the most pressing challenges that will help shape and advance future regulatory reforms. Yet, to the best of our knowledge, there is no comprehensive summary of the literature examining legal concerns in relation to health-related AI. We thus aim to summarize and map the literature examining legal concerns in health-related AI using a scoping review approach. Methods: The scoping review framework developed by (J Soc Res Methodol 8:19-32, 2005) and extended by (Implement Sci 5:69, 2010) and the Preferred Reporting Items for Systematic Reviews and Meta-Analysis extension for scoping reviews (PRISMA-ScR) guided our protocol development. In close consultation with trained librarians, we will develop a highly sensitive search for MEDLINE® (OVID) and adapt it for multiple databases designed to comprehensively capture texts in law, medicine, nursing, pharmacy, other healthcare professions (e.g., dentistry, nutrition), public health, computer science, and engineering. English- and French-language records will be included if they examine health-related AI, describe or prioritize a legal concern in health-related AI or propose a solution thereto, and were published in 2012 or later. Eligibility assessment will be conducted independently and in duplicate at all review stages. Coded data will be analyzed along themes and stratified across discipline-specific literatures. Discussion: This first-of-its-kind scoping review will summarize available literature examining, documenting, or prioritizing legal concerns in health-related AI to advance law and policy reform(s). The review may also reveal discipline-specific concerns, priorities, and proposed solutions to the concerns. It will thereby identify priority areas that should be the focus of future reforms and regulatory options available to stakeholders in reform processes. Trial registration: This protocol was submitted to the Open Science Foundation registration database. See https://osf.io/zav7w.",
        "DOI": "10.1186/s13643-022-01939-y",
        "paper_author": "Da Silva M.",
        "affiliation_name": "University of Ottawa",
        "affiliation_city": "Ottawa",
        "affiliation_country": "Canada",
        "affiliation_id": "60028897",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "Reproducibility of prediction models in health services research",
        "publication": "BMC Research Notes",
        "citied_by": "9",
        "cover_date": "2022-12-01",
        "Abstract": "The field of health services research studies the health care system by examining outcomes relevant to patients and clinicians but also health economists and policy makers. Such outcomes often include health care spending, and utilization of care services. Building accurate prediction models using reproducible research practices for health services research is important for evidence-based decision making. Several systematic reviews have summarized prediction models for outcomes relevant to health services research, but these systematic reviews do not present a thorough assessment of reproducibility and research quality of the prediction modelling studies. In the present commentary, we discuss how recent advances in prediction modelling in other medical fields can be applied to health services research. We also describe the current status of prediction modelling in health services research, and we summarize available methodological guidance for the development, update, external validation and systematic appraisal of prediction models.",
        "DOI": "10.1186/s13104-022-06082-4",
        "paper_author": "Belbasis L.",
        "affiliation_name": "Berliner Institut für Gesundheitsforschung",
        "affiliation_city": "Berlin",
        "affiliation_country": "Germany",
        "affiliation_id": "60126120",
        "affiliation_state": "Berlin"
    },
    {
        "paper_title": "Machine-learning based routing of callers in an Israeli mental health hotline",
        "publication": "Israel Journal of Health Policy Research",
        "citied_by": "3",
        "cover_date": "2022-12-01",
        "Abstract": "Background: Mental health contact centers (also known as Hotlines) offer crisis intervention and counselling by phone calls and online chats. These mental health helplines have shown great success in improving the mental state of the callers, and are increasingly becoming popular in Israel and worldwide. Unfortunately, our knowledge about how to conduct successful routing of callers to counselling agents has been limited due to lack of large-scale data with labeled outcomes of the interactions. To date, many of these contact centers are overwhelmed by chat requests and operate in a simple first-come-first-serve (FCFS) scheduling policy which, combined, may lead to many callers receiving suboptimal counselling or abandoning the service before being treated. In this work our goal is to improve the efficiency of mental health contact centers by using a novel machine-learning based routing policy. Methods: We present a large-scale machine learning-based analysis of real-world data from the online contact center of ERAN, the Israeli Association for Emotional First Aid. The data includes over 35,000 conversations over a 2-years period. Based on this analysis, we present a novel call routing method, that integrates advanced AI-techniques including the Monte Carlo tree search algorithm. We conducted an experiment that included various realistic simulations of incoming calls to contact centers, based on data from ERAN. We divided the simulations into two common settings: standard call flow and heavy call flow. In order to establish a baseline, we compared our proposed solution to two baseline methods: (1) The FCFS method; and (2) a greedy solution based on machine learning predictions. Our comparison focuses on two metrics - the number of calls served and the average feedback of the callers (i.e., quality of the chats). Results: In the preliminary analysis, we identify indicative features that significantly contribute to the effectiveness of a conversation and demonstrate high accuracy in predicting the expected duration and the callers’ feedback. In the routing methods evaluation, we find that in heavy call flow settings, our proposed method significantly outperforms the other methods in both the quantity of served calls and average feedback. Most notably, we find that in the heavy call flow settings, our method improves the average feedback by 24% compared to FCFS and by 4% compared to the greedy solution. Regarding the standard-flow setting, we find that our proposed method significantly outperforms the FCFS method in the callers’ average feedback with a 12% improvement. However, in this setting, we did not find a significant difference between all methods in the quantity of served-calls and no significant difference was found between our proposed method and the greedy solution. Conclusion: The proposed routing policy has the potential to significantly improve the performance of mental health contact centers, especially in peak hours. Leveraging artificial intelligence techniques, such as machine learning algorithms, combined with real-world data can bring about a significant and necessary leap forward in the way mental health hotlines operate and consequently reduce the burden of mental illnesses on health systems. However, implementation and evaluation in an operational contact center is necessary in order to verify that the results replicate in practice.",
        "DOI": "10.1186/s13584-022-00534-9",
        "paper_author": "Kleinerman A.",
        "affiliation_name": "Bar-Ilan University",
        "affiliation_city": "Ramat Gan",
        "affiliation_country": "Israel",
        "affiliation_id": "60002765",
        "affiliation_state": "Tel Aviv District"
    },
    {
        "paper_title": "A machine learning approach to small area estimation: predicting the health, housing and well-being of the population of Netherlands",
        "publication": "International Journal of Health Geographics",
        "citied_by": "23",
        "cover_date": "2022-12-01",
        "Abstract": "Background: Local policymakers require information about public health, housing and well-being at small geographical areas. A municipality can for example use this information to organize targeted activities with the aim of improving the well-being of their residents. Surveys are often used to gather data, but many neighborhoods can have only few or even zero respondents. In that case, estimating the status of the local population directly from survey responses is prone to be unreliable. Methods: Small Area Estimation (SAE) is a technique to provide estimates at small geographical levels with only few or even zero respondents. In classical individual-level SAE, a complex statistical regression model is fitted to the survey responses by using auxiliary administrative data for the population as predictors, the missing responses are then predicted and aggregated to the desired geographical level. In this paper we compare gradient boosted trees (XGBoost), a well-known machine learning technique, to a structured additive regression model (STAR) designed for the specific problem of estimating public health and well-being in the whole population of the Netherlands. Results: We compare the accuracy and performance of these models using out-of-sample predictions with five-fold Cross Validation (5CV). We do this for three data sets of different sample sizes and outcome types. Compared to the STAR model, gradient boosted trees are able to improve both the accuracy of the predictions and the total time taken to get these predictions. Even though the models appear quite similar in overall accuracy, the small area predictions at neighborhood level sometimes differ significantly. It may therefore make sense to pursue slightly more accurate models for better predictions into small areas. However, one of the biggest benefits is that XGBoost does not require prior knowledge or model specification. Data preparation and modelling is much easier, since the method automatically handles missing data, non-linear responses, interactions and accounts for spatial correlation structures. Conclusions: In this paper we provide new nationwide estimates of health, housing and well-being indicators at neighborhood level in the Netherlands, see ’Online materials’. We demonstrate that machine learning provides a good alternative to complex statistical regression modelling for small area estimation in terms of accuracy, robustness, speed and data preparation. These results can be used to make appropriate policy decisions at a local level and make recommendations about which estimation methods are beneficial in terms of accuracy, time and budget constraints.",
        "DOI": "10.1186/s12942-022-00304-5",
        "paper_author": "Viljanen M.",
        "affiliation_name": "Rijksinstituut voor Volksgezondheid en Milieu",
        "affiliation_city": "Bilthoven",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60026125",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Prediction and analysis of COVID-19 daily new cases and cumulative cases: times series forecasting and machine learning models",
        "publication": "BMC Infectious Diseases",
        "citied_by": "36",
        "cover_date": "2022-12-01",
        "Abstract": "Background: COVID-19 poses a severe threat to global human health, especially the USA, Brazil, and India cases continue to increase dynamically, which has a far-reaching impact on people's health, social activities, and the local economic situation. Methods: The study proposed the ARIMA, SARIMA and Prophet models to predict daily new cases and cumulative confirmed cases in the USA, Brazil and India over the next 30 days based on the COVID-19 new confirmed cases and cumulative confirmed cases data set(May 1, 2020, and November 30, 2021) published by the official WHO, Three models were implemented in the R 4.1.1 software with forecast and prophet package. The performance of different models was evaluated by using root mean square error (RMSE), mean absolute error (MAE) and mean absolute percentage error (MAPE). Results: Through the fitting and prediction of daily new case data, we reveal that the Prophet model has more advantages in the prediction of the COVID-19 of the USA, which could compose data components and capture periodic characteristics when the data changes significantly, while SARIMA is more likely to appear over-fitting in the USA. And the SARIMA model captured a seven-day period hidden in daily COVID-19 new cases from 3 countries. While in the prediction of new cumulative cases, the ARIMA model has a better ability to fit and predict the data with a positive growth trend in different countries(Brazil and India). Conclusions: This study can shed light on understanding the outbreak trends and give an insight into the epidemiological control of these regions. Further, the prediction of the Prophet model showed sufficient accuracy in the daily COVID-19 new cases of the USA. The ARIMA model is suitable for predicting Brazil and India, which can help take precautions and policy formulation for this epidemic in other countries.",
        "DOI": "10.1186/s12879-022-07472-6",
        "paper_author": "Wang Y.",
        "affiliation_name": "China Medical University Shenyang",
        "affiliation_city": "Shenyang",
        "affiliation_country": "China",
        "affiliation_id": "60008872",
        "affiliation_state": "Liaoning"
    },
    {
        "paper_title": "Learning models for forecasting hospital resource utilization for COVID-19 patients in Canada",
        "publication": "Scientific Reports",
        "citied_by": "5",
        "cover_date": "2022-12-01",
        "Abstract": "Hospitals in Canada are facing a crisis-level shortage of critical supplies and equipment during the COVID-19 pandemic. This motivates us to create predictive models that can use Canada COVID-19 data and pandemic-related factors to accurately forecast 5 quantities—three related to hospital resource utilization (i.e., the number of hospital beds, ICU beds, and ventilators that will be needed by COVID-19 patients) and two to the pandemic progress (i.e., the number of COVID-19 cases and COVID-19 deaths)—several weeks in advance. We developed a machine learning method that can use information (i.e., resource utilization, pandemic progress, population mobility, weather condition, and public policy) currently known about a region since March 2020, to learn multiple temporal convolutional network (TCN) models every week; each used for forecasting the weekly average of one of these 5 quantities in Canada (respectively, in six specific provinces) for each, in the next 1 (resp., 2,3,4) weeks. To validate the effectiveness of our method, we compared our method, versus other standard models, on the COVID-19 data and hospital resource data, on the tasks of predicting the 116 values (for Canada and its six most populated provinces), every week from Oct 2020 to July 2021, and the 20 values (only for Canada) for four specific times within 9 July to 31 Dec 2021. Experimental results show that our 4640 TCN models (each forecasting a regional target for a specific future time, on a specific date) can produce accurate 1,2,3,4-week forecasts of the utilization of every hospital resource and pandemic progress for each week from 2 Oct 2020 to 2 July 2021, as well as 80 TCN models for each of the four specified times within 9 July and 31 Dec 2021. Compared to other baseline and state-of-the-art predictive models, our TCN models yielded the best forecasts, with the lowest mean absolute percentage error (MAPE). Additional experiments, on the IHME COVID-19 data, demonstrate the effectiveness of our TCN models, in comparison with IHME forecasts. Each of our TCN models used a pre-defined set of features; we experimentally validate the effectiveness of these features by showing that these models perform better than other models that instead used other features. Overall, these experimental results demonstrate that our method can accurately forecast hospital resource utilization and pandemic progress for Canada and for each of the six provinces.",
        "DOI": "10.1038/s41598-022-12491-z",
        "paper_author": "Zhang J.",
        "affiliation_name": "University of Alberta",
        "affiliation_city": "Edmonton",
        "affiliation_country": "Canada",
        "affiliation_id": "60030835",
        "affiliation_state": "AB"
    },
    {
        "paper_title": "The contribution of data-driven poverty alleviation funds in achieving mid-21st-Century multidimensional poverty alleviation planning",
        "publication": "Humanities and Social Sciences Communications",
        "citied_by": "9",
        "cover_date": "2022-12-01",
        "Abstract": "The first Sustainable Development Goal (SDG) is intended to eradicate multi-dimensional poverty globally. The same multidimensional poverty indices for India and the Middle East/Africa in 2020 indicate that 10–14 years are still required to reach the level of China’s poverty eradication. Using machine learning, spatial statistics, and a scenario analysis, we demonstrate how a Monte Carlo simulation of poverty alleviation funds-guided shared socioeconomic pathways (PAFs-SSPs) in China reveals the necessity to adopt an integrated poverty alleviation strategy. This approach employs multi-dimensional development indicators to reduce wide regional differences. We developed the data-driven model framework of a PAFs-SSPs to analyze the multifaceted and long-term planning needs of poverty alleviation policies, which can be applied to the formulation of poverty alleviation policies in different developing countries. Our findings point to the importance of implementing multidimensional development policies in China to achieve the first SDG worldwide.",
        "DOI": "10.1057/s41599-022-01180-x",
        "paper_author": "Yang D.",
        "affiliation_name": "Dalian Maritime University",
        "affiliation_city": "Dalian",
        "affiliation_country": "China",
        "affiliation_id": "60029322",
        "affiliation_state": "Liaoning"
    },
    {
        "paper_title": "Predicting cash holdings using supervised machine learning algorithms",
        "publication": "Financial Innovation",
        "citied_by": "5",
        "cover_date": "2022-12-01",
        "Abstract": "This study predicts the cash holdings policy of Turkish firms, given the 20 selected features with machine learning algorithm methods. 211 listed firms in the Borsa Istanbul are analyzed over the period between 2006 and 2019. Multiple linear regression (MLR), k-nearest neighbors (KNN), support vector regression (SVR), decision trees (DT), extreme gradient boosting algorithm (XGBoost) and multi-layer neural networks (MLNN) are used for prediction. Results reveal that MLR, KNN, and SVR provide high root mean square error (RMSE) and low R2 values. Meanwhile, more complex algorithms, such as DT and especially XGBoost, derive higher accuracy with a 0.73 R2 value. Therefore, using advanced machine learning algorithms, we may predict cash holdings considerably.",
        "DOI": "10.1186/s40854-022-00351-8",
        "paper_author": "Özlem Ş.",
        "affiliation_name": "MEF University",
        "affiliation_city": "Istanbul",
        "affiliation_country": "Turkey",
        "affiliation_id": "60105072",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "What evidence exists on the effects of public policy interventions for achieving environmentally sustainable food consumption? A systematic map protocol",
        "publication": "Environmental Evidence",
        "citied_by": "4",
        "cover_date": "2022-12-01",
        "Abstract": "Background: The global food system is causing considerable environmental harm. A transition towards more sustainable consumption is needed. Targeted public policy interventions are crucial for stimulating such transition. While there is extensive research about the promotion of more environmentally sustainable food consumption, this knowledge is scattered across different sources. This systematic map aims to collate and describe the available evidence on public policy interventions such as laws, directives, taxes and information campaigns, for achieving sustainable food consumption patterns. Methods: We will search bibliographic databases, specialist websites, Google Scholar and bibliographies of relevant reviews. Searches for academic literature will be performed in English, while searches for grey literature will be performed in English, Swedish, Danish and Norwegian. Screening, including consistency checking exercises, will be done at two levels: title and abstract, and full text. We will use machine learning algorithms to support screening at the title and abstract level. Coding and meta-data extraction will include bibliographic information, policy details and context, and measured environmental outcome(s). The evidence base will be summarised narratively using tables and graphs and presented as an online interactive searchable database and a website that will allow for visualisation, filtering and exploring systematic map findings, knowledge gaps and clusters.",
        "DOI": "10.1186/s13750-022-00271-1",
        "paper_author": "Macura B.",
        "affiliation_name": "Stockholm Environment Institute",
        "affiliation_city": "Stockholm",
        "affiliation_country": "Sweden",
        "affiliation_id": "60015157",
        "affiliation_state": "Stockholm"
    },
    {
        "paper_title": "An important component to investigating STEM persistence: the development and validation of the science identity (SciID) scale",
        "publication": "International Journal of STEM Education",
        "citied_by": "18",
        "cover_date": "2022-12-01",
        "Abstract": "Background: Science, technology, engineering, and mathematics (STEM) influence almost every aspect of our daily lives. However, despite the high demand for STEM occupational talent, the STEM pipeline continues leaking, with less than one-sixth of high school students pursuing STEM majors and only 50% of entering STEM college majors matriculating into STEM fields. Science identity has been identified as the most powerful predictor of high school students pursuing an undergraduate STEM major as reported by Chang (Machine learning approach to predicting STEM college major choice, American Educational Research Association (AERA), San Francisco, 2020). Though the construct is gaining lots of attention, it remains largely ill-defined, not operationalized at the high school level, and not based upon traditional identity theory. The purpose of this study was to develop a valid and reliable instrument that measures high school students’ science identity, the Science Identity (SciID) Scale. Results: Subject experts and a small group of high school students provided content validation for the proposed scale. Exploratory factor analysis revealed an optimal two-factor solution, reflecting the traditional two-dimensions of identity theory: Exploration and Commitment. Cronbach’s alpha revealed good internal consistency for both factors. Finally, structural equation modeling confirmed the convergent validity of the instrument with the external variables of science achievement and science career interest. Furthermore, the divergent validity between science identity and science self-concept was also confirmed. Conclusions: Initial results indicate that the SciID Scale is a valid and reliable instrument that accurately measures a high school student’s standing on this construct. The soundness of this instrument will enable policy makers and practitioners to design more effective intervention programs aimed at cultivating high school students’ science identity.",
        "DOI": "10.1186/s40594-022-00351-1",
        "paper_author": "Lockhart M.E.",
        "affiliation_name": "Texas A&amp;M University",
        "affiliation_city": "College Station",
        "affiliation_country": "United States",
        "affiliation_id": "60020547",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "Using ANPR data to create an anonymized linked open dataset on urban bustle",
        "publication": "European Transport Research Review",
        "citied_by": "4",
        "cover_date": "2022-12-01",
        "Abstract": "ANPR cameras allow the automatic detection of vehicle license plates and are increasingly used for law enforcement. However, also statistical data generated by ANPR cameras are a potential source of urban insights. In order for this data to reach its full potential for policy-making, we research how this data can be shared in digital twins, with researchers, for a diverse set of machine learning models, and even Open Data portals. This article’s key objective is to find a way to anonymize and aggregate ANPR data in a way that it still can provide useful visualizations for local decision making. We introduce an approach to aggregate the data with geotemporal binning and publish it by combining nine existing data specifications. We implemented the approach for the city of Kortrijk (Belgium) with 43 ANPR cameras, developed the ANPR Metrics tool to generate the statistical data and dashboards on top of the data, and tested whether mobility experts from the city could deduct valuable insights. We present a couple of insights that were found as a result, as a proof that anonymized ANPR data complements their currently used traffic analysis tools, providing a valuable source for data-driven policy-making.",
        "DOI": "10.1186/s12544-022-00538-1",
        "paper_author": "Van de Vyvere B.",
        "affiliation_name": "Universiteit Gent",
        "affiliation_city": "Ghent",
        "affiliation_country": "Belgium",
        "affiliation_id": "60033316",
        "affiliation_state": "VOV"
    },
    {
        "paper_title": "The role of statisticians in the response to COVID-19 in Israel: a holistic point of view",
        "publication": "Israel Journal of Health Policy Research",
        "citied_by": "7",
        "cover_date": "2022-12-01",
        "Abstract": "The COVID-19 pandemic cast a dramatic spotlight on the use of data as a fundamental component of good decision-making. Evaluating and comparing alternative policies required information on concurrent infection rates and insightful analysis to project them into the future. Statisticians in Israel were involved in these processes early in the pandemic in some silos as an ad-hoc unorganized effort. Informal discussions within the statistical community culminated in a roundtable, organized by three past presidents of the Israel Statistical Association, and hosted by the Samuel Neaman Institute in April 2021. The meeting was designed to provide a forum for exchange of views on the profession’s role during the COVID-19 pandemic, and more generally, on its influence in promoting evidence-based public policy. This paper builds on the insights and discussions that emerged during the roundtable meeting and presents a general framework, with recommendations, for involving statisticians and statistics in decision-making.",
        "DOI": "10.1186/s13584-022-00531-y",
        "paper_author": "Dattner I.",
        "affiliation_name": "University of Haifa",
        "affiliation_city": "Haifa",
        "affiliation_country": "Israel",
        "affiliation_id": "60002999",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Cross-border mobility responses to COVID-19 in Europe: new evidence from facebook data",
        "publication": "Globalization and Health",
        "citied_by": "11",
        "cover_date": "2022-12-01",
        "Abstract": "Background: Assessing the impact of government responses to Covid-19 is crucial to contain the pandemic and improve preparedness for future crises. We investigate here the impact of non-pharmaceutical interventions (NPIs) and infection threats on the daily evolution of cross-border movements of people during the Covid-19 pandemic. We use a unique database on Facebook users’ mobility, and rely on regression and machine learning models to identify the role of infection threats and containment policies. Permutation techniques allow us to compare the impact and predictive power of these two categories of variables. Results: In contrast with studies on within-border mobility, our models point to a stronger importance of containment policies in explaining changes in cross-border traffic as compared with international travel bans and fears of being infected. The latter are proxied by the numbers of Covid-19 cases and deaths at destination. Although the ranking among coercive policies varies across modelling techniques, containment measures in the destination country (such as cancelling of events, restrictions on internal movements and public gatherings), and school closures in the origin country (influencing parental leaves) have the strongest impacts on cross-border movements. Conclusion: While descriptive in nature, our findings have policy-relevant implications. Cross-border movements of people predominantly consist of labor commuting flows and business travels. These economic and essential flows are marginally influenced by the fear of infection and international travel bans. They are mostly governed by the stringency of internal containment policies and the ability to travel.",
        "DOI": "10.1186/s12992-022-00832-6",
        "paper_author": "Docquier F.",
        "affiliation_name": "Luxembourg Institute of Socio-Economic Research (LISER)",
        "affiliation_city": "Esch-sur-Alzette",
        "affiliation_country": "Luxembourg",
        "affiliation_id": "60116751",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Mapping state-sponsored information operations with multi-view modularity clustering",
        "publication": "EPJ Data Science",
        "citied_by": "5",
        "cover_date": "2022-12-01",
        "Abstract": "This paper presents a new computational framework for mapping state-sponsored information operations into distinct strategic units. Utilizing a novel method called multi-view modularity clustering (MVMC), we identify groups of accounts engaged in distinct narrative and network information maneuvers. We then present an analytical pipeline to holistically determine their coordinated and complementary roles within the broader digital campaign. Applying our proposed methodology to disclosed Chinese state-sponsored accounts on Twitter, we discover an overarching operation to protect and manage Chinese international reputation by attacking individual adversaries (Guo Wengui) and collective threats (Hong Kong protestors), while also projecting national strength during global crisis (the COVID-19 pandemic). Psycholinguistic tools quantify variation in narrative maneuvers employing hateful and negative language against critics in contrast to communitarian and positive language to bolster national solidarity. Network analytics further distinguish how groups of accounts used network maneuvers to act as balanced operators, organized masqueraders, and egalitarian echo-chambers. Collectively, this work breaks methodological ground on the interdisciplinary application of unsupervised and multi-view methods for characterizing not just digital campaigns in particular, but also coordinated activity more generally. Moreover, our findings contribute substantive empirical insights around how state-sponsored information operations combine narrative and network maneuvers to achieve interlocking strategic objectives. This bears both theoretical and policy implications for platform regulation and understanding the evolving geopolitical significance of cyberspace.",
        "DOI": "10.1140/epjds/s13688-022-00338-6",
        "paper_author": "Uyheng J.",
        "affiliation_name": "School of Computer Science",
        "affiliation_city": "Pittsburgh",
        "affiliation_country": "United States",
        "affiliation_id": "60136640",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "Prediction of global marginal land resources for Pistacia chinensis Bunge by a machine learning method",
        "publication": "Scientific Reports",
        "citied_by": "5",
        "cover_date": "2022-12-01",
        "Abstract": "Biofuel has attracted worldwide attention due to its potential to combat climate change and meet emission reduction targets. Pistacia chinensis Bunge (P. chinensis) is a prospective plant for producing biodiesel. Estimating the global potential marginal land resources for cultivating this species would be conducive to exploiting bioenergy yielded from it. In this study, we applied a machine learning method, boosted regression tree, to estimate the suitable marginal land for growing P. chinensis worldwide. The result indicated that most of the qualified marginal land is found in Southern Africa, the southern part of North America, the western part of South America, Southeast Asia, Southern Europe, and eastern and southwest coasts of Oceania, for a grand total of 1311.85 million hectares. Besides, we evaluated the relative importance of the environmental variables, revealing the major environmental factors that determine the suitability for growing P. chinensis, which include mean annual water vapor pressure, mean annual temperature, mean solar radiation, and annual cumulative precipitation. The potential global distribution of P. chinensis could provide a valuable basis to guide the formulation of P. chinensis-based biodiesel policies.",
        "DOI": "10.1038/s41598-022-09830-5",
        "paper_author": "Chen S.",
        "affiliation_name": "Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60031150",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "The UK Coronavirus Job Retention Scheme and diet, physical activity, and sleep during the COVID-19 pandemic: evidence from eight longitudinal population surveys",
        "publication": "BMC Medicine",
        "citied_by": "7",
        "cover_date": "2022-12-01",
        "Abstract": "Background: In March 2020, the UK implemented the Coronavirus Job Retention Scheme (furlough) to minimise job losses. Our aim was to investigate associations between furlough and diet, physical activity, and sleep during the early stages of the COVID-19 pandemic. Methods: We analysed data on 25,092 participants aged 16–66 years from eight UK longitudinal studies. Changes in employment, including being furloughed, were based on employment status before and during the first lockdown. Health behaviours included fruit and vegetable consumption, physical activity, and sleep. Study-specific estimates obtained using modified Poisson regression, adjusting for socio-demographic characteristics and pre-pandemic health and health behaviours, were statistically pooled using random effects meta-analysis. Associations were also stratified by sex, age, and education. Results: Across studies, between 8 and 25% of participants were furloughed. Compared to those who remained working, furloughed workers were slightly less likely to be physically inactive (RR = 0.85; [95% CI 0.75–0.97]; I2 = 59%) and did not differ overall with respect to low fruit and vegetable consumption or atypical sleep, although findings for sleep were heterogenous (I2 = 85%). In stratified analyses, furlough was associated with lower fruit and vegetable consumption among males (RR = 1.11; [1.01–1.22]; I2 = 0%) but not females (RR = 0.84; [0.68–1.04]; I2 = 65%). Considering changes in quantity, furloughed workers were more likely than those who remained working to report increases in fruit and vegetable consumption, exercise, and hours of sleep. Conclusions: Those furloughed exhibited similar health behaviours to those who remained in employment during the initial stages of the pandemic. There was little evidence to suggest that adoption of such social protection policies in the post-pandemic recovery period and during future economic crises had adverse effects on population health behaviours.",
        "DOI": "10.1186/s12916-022-02343-y",
        "paper_author": "Wielgoszewska B.",
        "affiliation_name": "UCL Institute of Education",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60006242",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Hybrid data decomposition-based deep learning for Bitcoin prediction and algorithm trading",
        "publication": "Financial Innovation",
        "citied_by": "28",
        "cover_date": "2022-12-01",
        "Abstract": "In recent years, Bitcoin has received substantial attention as potentially high-earning investment. However, its volatile price movement exhibits great financial risks. Therefore, how to accurately predict and capture changing trends in the Bitcoin market is of substantial importance to investors and policy makers. However, empirical works in the Bitcoin forecasting and trading support systems are at an early stage. To fill this void, this study proposes a novel data decomposition-based hybrid bidirectional deep-learning model in forecasting the daily price change in the Bitcoin market and conducting algorithmic trading on the market. Two primary steps are involved in our methodology framework, namely, data decomposition for inner factors extraction and bidirectional deep learning for forecasting the Bitcoin price. Results demonstrate that the proposed model outperforms other benchmark models, including econometric models, machine-learning models, and deep-learning models. Furthermore, the proposed model achieved higher investment returns than all benchmark models and the buy-and-hold strategy in a trading simulation. The robustness of the model is verified through multiple forecasting periods and testing intervals.",
        "DOI": "10.1186/s40854-022-00336-7",
        "paper_author": "Li Y.",
        "affiliation_name": "Academy of Mathematics and System Sciences Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60003707",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "The effectiveness of governmental nonpharmaceutical interventions against COVID-19 at controlling seasonal influenza transmission: an ecological study",
        "publication": "BMC Infectious Diseases",
        "citied_by": "17",
        "cover_date": "2022-12-01",
        "Abstract": "Background: A range of strict nonpharmaceutical interventions (NPIs) were implemented in many countries to combat the coronavirus 2019 (COVID-19) pandemic. These NPIs may also be effective at controlling seasonal influenza virus infections, as influenza viruses have the same transmission path as severe acute respiratory syndrome coronavirus 2. The aim of this study was to evaluate the effects of different NPIs on the control of seasonal influenza. Methods: Data for 14 NPIs implemented in 33 countries and the corresponding influenza virological surveillance data were collected. The influenza suppression index was calculated as the difference between the influenza positivity rate during its period of decline from 2019 to 2020 and during the influenza epidemic seasons in the previous 9 years. A machine learning model was developed using an extreme gradient boosting tree regressor to fit the NPI and influenza suppression index data. The SHapley Additive exPlanations tool was used to characterize the NPIs that suppressed the transmission of influenza. Results: Of all NPIs tested, gathering limitations had the greatest contribution (37.60%) to suppressing influenza transmission during the 2019–2020 influenza season. The three most effective NPIs were gathering limitations, international travel restrictions, and school closures. For these three NPIs, their intensity threshold required to generate an effect were restrictions on the size of gatherings less than 1000 people, ban of travel to all regions or total border closures, and closing only some categories of schools, respectively. There was a strong positive interaction effect between mask-wearing requirements and gathering limitations, whereas merely implementing a mask-wearing requirement, and not other NPIs, diluted the effectiveness of mask-wearing requirements at suppressing influenza transmission. Conclusions: Gathering limitations, ban of travel to all regions or total border closures, and closing some levels of schools were found to be the most effective NPIs at suppressing influenza transmission. It is recommended that the mask-wearing requirement be combined with gathering limitations and other NPIs. Our findings could facilitate the precise control of future influenza epidemics and other potential pandemics.",
        "DOI": "10.1186/s12879-022-07317-2",
        "paper_author": "Qiu Z.",
        "affiliation_name": "Sun Yat-Sen University",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60021182",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "SciSpot: Scientific Computing On Temporally Constrained Cloud Preemptible VMs",
        "publication": "IEEE Transactions on Parallel and Distributed Systems",
        "citied_by": "3",
        "cover_date": "2022-12-01",
        "Abstract": "Scientific computing applications are being increasingly deployed on cloud computing platforms. Transient servers such as EC2 spot instances and Google Preemptible VMs, can be used to lower the costs of running applications on the cloud by up to 10 ×10×. However, the frequent preemptions and resource heterogeneity of these transient servers introduces many challenges in their effective and efficient use. In this paper, we develop techniques for modeling and mitigating preemptions of transient servers, and present SciSpot, a software framework that enables low-cost scientific computing on the cloud. SciSpot deploys applications on Google Cloud Preemptible Virtual Machines that exhibit temporally constrained preemptions: VMs are always preempted in a 24 hour interval. Our empirical analysis shows that the preemption rate is generally bathtub shaped, which raises multiple fundamental challenges in performance modeling and policy design. We develop a new reliability model for temporally constrained preemptions, and use statistical mechanics to show why the bathtub shape is generally exhibited. SciSpot's design is guided by our observation that many emerging scientific computing applications that integrate machine learning with simulations, can be deployed as 'bags' of jobs, which represent multiple instantiations of the same computation with different physical model parameters. For a bag of jobs, SciSpot finds the optimal transient server on-the-fly, by taking into account the price, performance, and preemption rates of different servers. SciSpot reduces costs by 5 ×5× compared to conventional cloud deployments, and reduces makespans by up to 10 ×10× compared to conventional high performance computing clusters.",
        "DOI": "10.1109/TPDS.2022.3157272",
        "paper_author": "Kadupitiya J.C.S.",
        "affiliation_name": "Luddy School of Informatics, Computing, and Engineering",
        "affiliation_city": "Bloomington",
        "affiliation_country": "United States",
        "affiliation_id": "60010875",
        "affiliation_state": "IN"
    },
    {
        "paper_title": "The fine-scale associations between socioeconomic status, density, functionality, and spread of COVID-19 within a high-density city",
        "publication": "BMC Infectious Diseases",
        "citied_by": "16",
        "cover_date": "2022-12-01",
        "Abstract": "Background: Motivated by the need for precise epidemic control and epidemic-resilient urban design, this study aims to reveal the joint and interactive associations between urban socioeconomic, density, connectivity, and functionality characteristics and the COVID-19 spread within a high-density city. Many studies have been made on the associations between urban characteristics and the COVID-19 spread, but there is a scarcity of such studies in the intra-city scale and as regards complex joint and interactive associations by using advanced machine learning approaches. Methods: Differential-evolution-based association rule mining was used to investigate the joint and interactive associations between the urban characteristics and the spatiotemporal distribution of COVID-19 confirmed cases, at the neighborhood scale in Hong Kong. The associations were comparatively studied for the distribution of the cases in four waves of COVID-19 transmission: before Jun 2020 (wave 1 and 2), Jul–Oct 2020 (wave 3), and Nov 2020–Feb 2021 (wave 4), and for local and imported confirmed cases. Results: The first two waves of COVID-19 were found mainly characterized by higher-socioeconomic-status (SES) imported cases. The third-wave outbreak concentrated in densely populated and usually lower-SES neighborhoods, showing a high risk of within-neighborhood virus transmissions jointly contributed by high density and unfavorable SES. Starting with a super-spread which considerably involved high-SES population, the fourth-wave outbreak showed a stronger link to cross-neighborhood transmissions driven by urban functionality. Then the outbreak diffused to lower-SES neighborhoods and interactively aggravated the within-neighborhood pandemic transmissions. Association was also found between a higher SES and a slightly longer waiting period (i.e., the period from symptom onset to diagnosis of symptomatic cases), which further indicated the potential contribution of higher-SES population to the pandemic transmission. Conclusions: The results of this study may provide references to developing precise anti-pandemic measures for specific neighborhoods and virus transmission routes. The study also highlights the essentiality of reliving co-locating overcrowdedness and unfavorable SES for developing epidemic-resilient compact cities, and the higher obligation of higher-SES population to conform anti-pandemic policies.",
        "DOI": "10.1186/s12879-022-07274-w",
        "paper_author": "Zhang A.",
        "affiliation_name": "The Hong Kong Polytechnic University",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong",
        "affiliation_id": "60008928",
        "affiliation_state": "Hong Kong"
    },
    {
        "paper_title": "China’s environmental policy intensity for 1978–2019",
        "publication": "Scientific Data",
        "citied_by": "48",
        "cover_date": "2022-12-01",
        "Abstract": "Improving the measurement of environmental policy intensity would affect not only the selection of variables in environmental policy research but also the research conclusions when evaluating policy effects. Because direct evaluation is lacking, the existing research usually applies data such as pollutant emission data, or the number of policies to construct proxy variables. However, these proxy variables are affected by many assumptions and different selection criteria, and they are inevitably accompanied by endogeneity problems. In this study, China’s environmental policy is comprehensively collected for the first time, and a machine learning algorithm is applied to evaluate the policy intensity. We provide all the policies issued by the Chinese government from 1978 to 2019 and the quantified intensity for each policy. We also distinguish all policies into three types according to their attributes. This dataset can help researchers to further understand China’s environmental policy system. In addition, it provides a valuable dataset for related research on evaluating environmental policy and recommending actions for further improvement.",
        "DOI": "10.1038/s41597-022-01183-y",
        "paper_author": "Zhang G.",
        "affiliation_name": "Lanzhou University",
        "affiliation_city": "Lanzhou",
        "affiliation_country": "China",
        "affiliation_id": "60028265",
        "affiliation_state": "Gansu"
    },
    {
        "paper_title": "Reinforcement learning evaluation of treatment policies for patients with hepatitis C virus",
        "publication": "BMC Medical Informatics and Decision Making",
        "citied_by": "4",
        "cover_date": "2022-12-01",
        "Abstract": "Background: Evaluation of new treatment policies is often costly and challenging in complex conditions, such as hepatitis C virus (HCV) treatment, or in limited-resource settings. We sought to identify hypothetical policies for HCV treatment that could best balance the prevention of cirrhosis while preserving resources (financial or otherwise). Methods: The cohort consisted of 3792 HCV-infected patients without a history of cirrhosis or hepatocellular carcinoma at baseline from the national Veterans Health Administration from 2015 to 2019. To estimate the efficacy of hypothetical treatment policies, we utilized historical data and reinforcement learning to allow for greater flexibility when constructing new HCV treatment strategies. We tested and compared four new treatment policies: a simple stepwise policy based on Aspartate Aminotransferase to Platelet Ratio Index (APRI), a logistic regression based on APRI, a logistic regression on multiple longitudinal and demographic indicators that were prespecified for clinical significance, and a treatment policy based on a risk model developed for HCV infection. Results: The risk-based hypothetical treatment policy achieved the lowest overall risk with a score of 0.016 (90% CI 0.016, 0.019) while treating the most high-risk (346.4 ± 1.4) and the fewest low-risk (361.0 ± 20.1) patients. Compared to hypothetical treatment policies that treated approximately the same number of patients (1843.7 vs. 1914.4 patients), the risk-based policy had more untreated time per patient (7968.4 vs. 7742.9 patient visits), signaling cost reduction for the healthcare system. Conclusions: Off-policy evaluation strategies are useful to evaluate hypothetical treatment policies without implementation. If a quality risk model is available, risk-based treatment strategies can reduce overall risk and prioritize patients while reducing healthcare system costs.",
        "DOI": "10.1186/s12911-022-01789-7",
        "paper_author": "Oselio B.",
        "affiliation_name": "University of Michigan School of Public Health",
        "affiliation_city": "Ann Arbor",
        "affiliation_country": "United States",
        "affiliation_id": "60016660",
        "affiliation_state": "MI"
    },
    {
        "paper_title": "COVID-19 Vaccination Effect on Stock Market and Death Rate in India",
        "publication": "Asia-Pacific Financial Markets",
        "citied_by": "16",
        "cover_date": "2022-12-01",
        "Abstract": "The COVID-19 epidemic has brought attention to the vulnerability of new illnesses, and immunization remains a viable option for resuming normal life. This paper examines the influence of COVID-19 vaccination on the death rate and the performance of stock market in India. For this study, COVID-19 vaccination and death rate data is gathered from the Ministry of Health and Family Welfare (MoHFW) portal, and the data for the stock index is taken from the Bombay Stock Exchange (BSE), India. In order to achieve a precise representation of feature significance and distribution, EDA (Exploratory Data Analysis) is utilized in this study. The impact of COVID-19 immunization on the mortality rate and stock market index is investigated using both statistical analysis and Machine Learning Regression-based models. The models are remarkably accurate in reproducing actual result. The empirical study suggests that vaccination has a strong positive impact on the stock market and reducing the death rate. Furthermore, the policies recommended by government and monetary authorities coupled with COVID-19 vaccine supported the stock market recovery in pandemic.",
        "DOI": "10.1007/s10690-022-09364-w",
        "paper_author": "Behera J.",
        "affiliation_name": "SRM Institute of Science and Technology",
        "affiliation_city": "Kattankulathur",
        "affiliation_country": "India",
        "affiliation_id": "60014340",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Carbon peak and its mitigation implications for China in the post-pandemic era",
        "publication": "Scientific Reports",
        "citied_by": "54",
        "cover_date": "2022-12-01",
        "Abstract": "China’s carbon peak greatly impacts global climate targets. Limited studies have comprehensively analyzed the influence of the COVID-19 pandemic, changing emission network, and recent carbon intensity (CI) reduction on the carbon peak and the corresponding mitigation implications. Using a unique dataset at different levels, we project China’s CO2 emission by 2035 and analyze the time, volume, driver patterns, complex emission network, and policy implications of China’s carbon peak in the post- pandemic era. We develop an ensemble time-series model with machine learning approaches as the projection benchmark, and show that China’s carbon peak will be achieved by 2021–2026 with > 80% probability. Most Chinese cities and counties have not achieved carbon peaks response to the priority-peak policy and the current implementation of CI reduction should thus be strengthened. While there is a \"trade off\" between the application of carbon emission reduction technology and economic recovery in the post-pandemic era, a close cooperation of interprovincial CO2 emission is also warranted.",
        "DOI": "10.1038/s41598-022-07283-4",
        "paper_author": "Chen J.",
        "affiliation_name": "Southwestern University of Finance and Economics",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China",
        "affiliation_id": "60018540",
        "affiliation_state": "Sichuan"
    },
    {
        "paper_title": "Connecting brain and heart: artificial intelligence for sustainable development",
        "publication": "Scientometrics",
        "citied_by": "1",
        "cover_date": "2022-12-01",
        "Abstract": "A key objective of global policies on Artificial Intelligence (AI) is to foster AI research for sustainable development (SD). In this paper, we analyze the inclusion of SD in AI research indexed by the IEEE Xplore database from 2000 to 2019. We address three critical questions: (1) To what extent is AI research addressing the sustainable development goals (SDGs)? (2) Which subject areas of AI show an emerging interest in SD? And (3) What patterns of collaboration between regions of the world are being stimulated by AI? Our scientometric analysis consists of (1) Identifying the number of AI papers that address SDGs in their titles, abstracts, and keywords. (2) Developing a composite indicator based on the number of documents produced, scientific impact, and inventive impact to distinguish areas with an emerging interest in SD; (3) Exploring co-authorship networks at three levels: region, income group, and country. The overall results show that a small share of papers is explicitly focused on SD. Our composite indicator allowed us to identify an emerging interest in SD from Ultrasonics, Ferroelectrics, and Frequency Control, Education, Consumer Electronics, Electrical Engineering, Electromagnetic Compatibility and Interference. Specifically, on AI subjects, we found emerging interests in Prediction Methods, Computation Theory, Machine Learning, Learning (artificial intelligence), and Biological Neural Networks. Inter-regional and inter-income group collaboration are limited, and network power is concentrated in a few countries. The results could be useful to improve the connection between technical knowledge, strategic planning for S&T investment, and SD policies.",
        "DOI": "10.1007/s11192-022-04299-5",
        "paper_author": "Chavarro D.",
        "affiliation_name": "Colombian Society of Engineering Physics (SCIF)",
        "affiliation_city": "Pereira",
        "affiliation_country": "Colombia",
        "affiliation_id": "126633930",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Cooperative Charging Control Strategy for Electric Vehicles Based on Multiagent Deep Reinforcement Learning",
        "publication": "IEEE Transactions on Industrial Informatics",
        "citied_by": "61",
        "cover_date": "2022-12-01",
        "Abstract": "The growth of electric vehicles (EVs) significantly increases the residential electricity demand and potentially leads to the overload of the transformer in the distribution grid. Aiming to coordinate the charging control of EVs, this article formulates the EVs charging problem as a Markov game with an unknown transition function and proposes a cooperative charging control strategy based on the multiagent deep reinforcement learning. The uncertainties from the dynamic electricity price, non-EV residential load consumption and drivers' individual behaviors are considered to construct the dynamic charging environment. Each agent contains a collective-policy model and an independent learner. The collective-policy model is introduced to model other agent's behaviors by approximating their power consumption. The independent learner is used to learn the optimal charging strategy by interacting with the environment. The soft-actor-critic framework is adopted to train the independent learner, enabling the proposed method to address the continuous state and action. Agents are trained with only the local observation and approximation, indicating that the proposed approach is fully decentralized and scalable to the problem with multiple agents. Finally, several numerical studies constructed based on the real-world data demonstrate the effectiveness and scalability of the proposed approach.",
        "DOI": "10.1109/TII.2022.3152218",
        "paper_author": "Yan L.",
        "affiliation_name": "Huazhong University of Science and Technology",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60025761",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "The predictive power of geographic health care utilization for unintentional fatal fall rates",
        "publication": "BMC Public Health",
        "citied_by": "0",
        "cover_date": "2022-12-01",
        "Abstract": "Background: Falls are the leading cause of fatal and nonfatal injuries among adults over 65 years old. The increase in fall mortality rates is likely multifactorial. With a lack of key drivers identified to explain rising rates of death from falls, accurate predictive modelling can be challenging, hindering evidence-based health resource and policy efforts. The objective of this work is to examine the predictive power of geographic utilization and longitudinal trends in mortality from unintentional falls amongst different demographic and geographic strata. Methods: This is a nationwide, retrospective cohort study using the United States Centers for Disease Control (CDC) Web-based Injury Statistics Query and Reporting System (WISQARS) database. The exposure was death from an unintentional fall as determined by the CDC. Outcomes included aggregate and trend crude and age-adjusted death rates. Health care utilization, reimbursement, and cost metrics were also compared. Results: Over 2001 to 2018, 465,486 total deaths due to unintentional falls were recorded with crude and age-adjusted rates of 8.42 and 7.76 per 100,000 population respectively. Comparing age-adjusted rates, males had a significantly higher age-adjusted death rate (9.89 vs. 6.17; p < 0.00001), but both male and female annual age-adjusted mortality rates are expected to rise (Male: + 0.25 rate/year, R2= 0.98; Female: + 0.22 rate/year, R2= 0.99). There were significant increases in death rates commensurate with increasing age, with the adults aged 85 years or older having the highest aggregate (201.1 per 100,000) and trending death rates (+ 8.75 deaths per 100,000/year, R2= 0.99). Machine learning algorithms using health care utilization data were accurate in predicting geographic age-adjusted death rates. Conclusions: Machine learning models have high accuracy in predicting geographic age-adjusted mortality rates from health care utilization data. In the United States from 2001 through 2018, adults aged 85+ years carried the highest death rate from unintentional falls and this rate is forecasted to accelerate.",
        "DOI": "10.1186/s12889-022-12731-x",
        "paper_author": "Crowson M.G.",
        "affiliation_name": "Massachusetts Eye and Ear",
        "affiliation_city": "Boston",
        "affiliation_country": "United States",
        "affiliation_id": "60003828",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "FenceKV: Enabling Efficient Range Query for Key-Value Separation",
        "publication": "IEEE Transactions on Parallel and Distributed Systems",
        "citied_by": "22",
        "cover_date": "2022-12-01",
        "Abstract": "LSM-tree is widely used in key-value stores for big data storage, but it suffers from write amplification brought by frequent compaction operations. An effective solution for this problem is key-value separation, which decouples values from the LSM-tree and stores them in a separate value log. However, existing key-value separation schemes achieve poor range query performance, especially for small key-value pairs, because they focus on mitigating write amplification but neglect access characteristics of the SSD. In this article, we propose FenceKV, which aims to achieve better range query performance while maintaining reasonable update performance for update-intensive workloads. FenceKV employs a new partition method to map values to the storage space based on the key-range to achieve efficient update and range query. Moreover, it adopts a key-range garbage collection policy to mitigate the garbage collection overhead and maintain sequential access for range queries. We compare FenceKV with modern key-value stores with various workloads, and results show that FenceKV can improve the range query performance significantly, while maintaining reasonable update performance compared to the existing designs of key-value separation.",
        "DOI": "10.1109/TPDS.2022.3149003",
        "paper_author": "Tang C.",
        "affiliation_name": "Wuhan National Laboratory for Optoelectronics",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60087294",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Sounds of COVID-19: exploring realistic performance of audio-based digital testing",
        "publication": "npj Digital Medicine",
        "citied_by": "63",
        "cover_date": "2022-12-01",
        "Abstract": "To identify Coronavirus disease (COVID-19) cases efficiently, affordably, and at scale, recent work has shown how audio (including cough, breathing and voice) based approaches can be used for testing. However, there is a lack of exploration of how biases and methodological decisions impact these tools’ performance in practice. In this paper, we explore the realistic performance of audio-based digital testing of COVID-19. To investigate this, we collected a large crowdsourced respiratory audio dataset through a mobile app, alongside symptoms and COVID-19 test results. Within the collected dataset, we selected 5240 samples from 2478 English-speaking participants and split them into participant-independent sets for model development and validation. In addition to controlling the language, we also balanced demographics for model training to avoid potential acoustic bias. We used these audio samples to construct an audio-based COVID-19 prediction model. The unbiased model took features extracted from breathing, coughs and voice signals as predictors and yielded an AUC-ROC of 0.71 (95% CI: 0.65–0.77). We further explored several scenarios with different types of unbalanced data distributions to demonstrate how biases and participant splits affect the performance. With these different, but less appropriate, evaluation strategies, the performance could be overestimated, reaching an AUC up to 0.90 (95% CI: 0.85–0.95) in some circumstances. We found that an unrealistic experimental setting can result in misleading, sometimes over-optimistic, performance. Instead, we reported complete and reliable results on crowd-sourced data, which would allow medical professionals and policy makers to accurately assess the value of this technology and facilitate its deployment.",
        "DOI": "10.1038/s41746-021-00553-x",
        "paper_author": "Han J.",
        "affiliation_name": "Department of Computer Science and Technology",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60119999",
        "affiliation_state": "Cambridgeshire"
    },
    {
        "paper_title": "Shared Mobility Intelligence Using Permissioned Blockchains for Smart Cities",
        "publication": "New Generation Computing",
        "citied_by": "10",
        "cover_date": "2022-12-01",
        "Abstract": "Suggesting tourists/residents about the pollution-free locations and controlling the number of passengers in a shareable vehicle have become crucial tasks to smart city officials as they plummet health issues such as asthma or COVID-19. Recently, city authorities, transport logistic designers, and policymakers have tasked researchers/entrepreneurs to innovate in shared mobility systems. This paper proposes a Blockchain-Enabled Shared Mobility (BESM) architecture that allocates seats to residents/tourists in a shareable vehicle based on air quality and COVID-19 information of traveling locations. BESM involves smart city authorities, vehicle owners, hospital authorities, and residents using permissioned-blockchains to collaboratively decide on allocating travel seats. Experiments were carried out at the IoT Cloud research laboratory to manifest the allocation of seats. For instance, BESM excluded in allocating seats to asthma patients and limited the number of travelers in the cities where COVID-19 cases or pollution levels were higher in numbers using BESM. The pollution levels of cities were monitored using air quality monitoring sensors or predicted using a few prediction algorithms such as Random Forests (RF), Linear Regression (LR), Quantile Regression (QR), Ridge Regression (RR), Lasso Regression (LaR), ElasticNet Regression (ER), Support Vector Machine (SVM), and Recursive Partitioning (RP). In succinct, the article unfolded the primordial importance of the proposed BESM architecture for promoting efficient shared mobility aspects in smart cities.",
        "DOI": "10.1007/s00354-021-00147-x",
        "paper_author": "Benedict S.",
        "affiliation_name": "Indian Institute of Information Technology Kottayam",
        "affiliation_city": "Kottayam",
        "affiliation_country": "India",
        "affiliation_id": "60283049",
        "affiliation_state": "KL"
    },
    {
        "paper_title": "Systematic decomposition of sequence determinants governing CRISPR/Cas9 specificity",
        "publication": "Nature Communications",
        "citied_by": "25",
        "cover_date": "2022-12-01",
        "Abstract": "The specificity of CRISPR/Cas9 genome editing is largely determined by the sequences of guide RNA (gRNA) and the targeted DNA, yet the sequence-dependent rules underlying off-target effects are not fully understood. To systematically explore the sequence determinants governing CRISPR/Cas9 specificity, here we describe a dual-target system to measure the relative cleavage rate between off- and on-target sequences (off-on ratios) of 1902 gRNAs on 13,314 synthetic target sequences, and reveal a set of sequence rules involving 2 factors in off-targeting: 1) a guide-intrinsic mismatch tolerance (GMT) independent of the mismatch context; 2) an “epistasis-like” combinatorial effect of multiple mismatches, which are associated with the free-energy landscape in R-loop formation and are explainable by a multi-state kinetic model. These sequence rules lead to the development of MOFF, a model-based predictor of Cas9-mediated off-target effects. Moreover, the “epistasis-like” combinatorial effect suggests a strategy of allele-specific genome editing using mismatched guides. With the aid of MOFF prediction, this strategy significantly improves the selectivity and expands the application domain of Cas9-based allele-specific editing, as tested in a high-throughput allele-editing screen on 18 cancer hotspot mutations.",
        "DOI": "10.1038/s41467-022-28028-x",
        "paper_author": "Fu R.",
        "affiliation_name": "Virginia Harris Cockrell Cancer Research Center at The University of Texas MD Anderson Cancer Center",
        "affiliation_city": "Smithville",
        "affiliation_country": "United States",
        "affiliation_id": "60020948",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "Optimal Actor-Critic Policy With Optimized Training Datasets",
        "publication": "IEEE Transactions on Emerging Topics in Computational Intelligence",
        "citied_by": "8",
        "cover_date": "2022-12-01",
        "Abstract": "Actor-critic(AC) algorithms are known for their efficacy and high performance in solving reinforcement learning problems, but they also suffer from low sampling efficiency. An AC based policy optimization process is iterative and needs to access the agent-environment to evaluate and update the policy by rolling out the policy, collecting rewards and states (i.e. samples), and learning from them. It ultimately requires a huge number of samples to learn an optimal policy. To improve sampling efficiency, we propose a strategy to optimize the training dataset that contains significantly less samples collected from the AC process. The dataset optimization is made of a best episode only operation, a policy parameter-fitness model, and a genetic algorithm module. The optimal policy network trained by the optimized training dataset exhibits superior performance compared to many contemporary AC algorithms in controlling autonomous dynamical systems. Evaluation on standard benchmarks shows that the method improves sampling efficiency, ensures faster convergence to optima, and is more data-efficient than its counterparts.",
        "DOI": "10.1109/TETCI.2022.3140375",
        "paper_author": "Banerjee C.",
        "affiliation_name": "The University of Newcastle, Australia",
        "affiliation_city": "Callaghan",
        "affiliation_country": "Australia",
        "affiliation_id": "60010571",
        "affiliation_state": "NSW"
    },
    {
        "paper_title": "A machine learning approach for modeling decisions in the out of hospital cardiac arrest care workflow",
        "publication": "BMC Medical Informatics and Decision Making",
        "citied_by": "8",
        "cover_date": "2022-12-01",
        "Abstract": "Background: A growing body of research has shown that machine learning (ML) can be a useful tool to predict how different variable combinations affect out-of-hospital cardiac arrest (OHCA) survival outcomes. However, there remain significant research gaps on the utilization of ML models for decision-making and their impact on survival outcomes. The purpose of this study was to develop ML models that effectively predict hospital’s practice to perform coronary angiography (CA) in adult patients after OHCA and subsequent neurologic outcomes. Methods: We utilized all (N = 2398) patients treated by the Chicago Fire Department Emergency Medical Services included in the Cardiac Arrest Registry to Enhance Survival (CARES) between 2013 and 2018 who survived to hospital admission to develop, test, and analyze ML models for decisions after return of spontaneous circulation (ROSC) and patient survival. ML classification models, including the Embedded Fully Convolutional Network (EFCN) model, were compared based on their ability to predict post-ROSC decisions and survival. Results: The EFCN classification model achieved the best results across tested ML algorithms. The area under the receiver operating characteristic curve (AUROC) for CA and Survival were 0.908 and 0.896 respectively. Through cohort analyses, our model predicts that 18.3% (CI 16.4–20.2) of patients should receive a CA that did not originally, and 30.1% (CI 28.5–31.7) of these would experience improved survival outcomes. Conclusion: ML modeling effectively predicted hospital decisions and neurologic outcomes. ML modeling may serve as a quality improvement tool to inform system level OHCA policies and treatment protocols.",
        "DOI": "10.1186/s12911-021-01730-4",
        "paper_author": "Harford S.",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Chicago",
        "affiliation_country": "United States",
        "affiliation_id": "60137961",
        "affiliation_state": "IL"
    },
    {
        "paper_title": "Learning in continuous action space for developing high dimensional potential energy models",
        "publication": "Nature Communications",
        "citied_by": "33",
        "cover_date": "2022-12-01",
        "Abstract": "Reinforcement learning (RL) approaches that combine a tree search with deep learning have found remarkable success in searching exorbitantly large, albeit discrete action spaces, as in chess, Shogi and Go. Many real-world materials discovery and design applications, however, involve multi-dimensional search problems and learning domains that have continuous action spaces. Exploring high-dimensional potential energy models of materials is an example. Traditionally, these searches are time consuming (often several years for a single bulk system) and driven by human intuition and/or expertise and more recently by global/local optimization searches that have issues with convergence and/or do not scale well with the search dimensionality. Here, in a departure from discrete action and other gradient-based approaches, we introduce a RL strategy based on decision trees that incorporates modified rewards for improved exploration, efficient sampling during playouts and a “window scaling scheme\" for enhanced exploitation, to enable efficient and scalable search for continuous action space problems. Using high-dimensional artificial landscapes and control RL problems, we successfully benchmark our approach against popular global optimization schemes and state of the art policy gradient methods, respectively. We demonstrate its efficacy to parameterize potential models (physics based and high-dimensional neural networks) for 54 different elemental systems across the periodic table as well as alloys. We analyze error trends across different elements in the latent space and trace their origin to elemental structural diversity and the smoothness of the element energy surface. Broadly, our RL strategy will be applicable to many other physical science problems involving search over continuous action spaces.",
        "DOI": "10.1038/s41467-021-27849-6",
        "paper_author": "Manna S.",
        "affiliation_name": "Argonne National Laboratory",
        "affiliation_city": "Lemont",
        "affiliation_country": "United States",
        "affiliation_id": "60028609",
        "affiliation_state": "IL"
    },
    {
        "paper_title": "A multi-step machine learning approach to assess the impact of COVID-19 lockdown on NO<inf>2</inf> attributable deaths in Milan and Rome, Italy",
        "publication": "Environmental Health: A Global Access Science Source",
        "citied_by": "10",
        "cover_date": "2022-12-01",
        "Abstract": "Background: Air pollution is one of the main concerns for the health of European citizens, and cities are currently striving to accomplish EU air pollution regulation. The 2020 COVID-19 lockdown measures can be seen as an unintended but effective experiment to assess the impact of traffic restriction policies on air pollution. Our objective was to estimate the impact of the lockdown measures on NO2 concentrations and health in the two largest Italian cities. Methods: NO2 concentration datasets were built using data deriving from a 1-month citizen science monitoring campaign that took place in Milan and Rome just before the Italian lockdown period. Annual mean NO2 concentrations were estimated for a lockdown scenario (Scenario 1) and a scenario without lockdown (Scenario 2), by applying city-specific annual adjustment factors to the 1-month data. The latter were estimated deriving data from Air Quality Network stations and by applying a machine learning approach. NO2 spatial distribution was estimated at a neighbourhood scale by applying Land Use Random Forest models for the two scenarios. Finally, the impact of lockdown on health was estimated by subtracting attributable deaths for Scenario 1 and those for Scenario 2, both estimated by applying literature-based dose–response function on the counterfactual concentrations of 10 μg/m3. Results: The Land Use Random Forest models were able to capture 41–42% of the total NO2 variability. Passing from Scenario 2 (annual NO2 without lockdown) to Scenario 1 (annual NO2 with lockdown), the population-weighted exposure to NO2 for Milan and Rome decreased by 15.1% and 15.3% on an annual basis. Considering the 10 μg/m3 counterfactual, prevented deaths were respectively 213 and 604. Conclusions: Our results show that the lockdown had a beneficial impact on air quality and human health. However, compliance with the current EU legal limit is not enough to avoid a high number of NO2 attributable deaths. This contribution reaffirms the potentiality of the citizen science approach and calls for more ambitious traffic calming policies and a re-evaluation of the legal annual limit value for NO2 for the protection of human health.",
        "DOI": "10.1186/s12940-021-00825-9",
        "paper_author": "Boniardi L.",
        "affiliation_name": "Università degli Studi di Milano",
        "affiliation_city": "Milan",
        "affiliation_country": "Italy",
        "affiliation_id": "60030318",
        "affiliation_state": "MI"
    },
    {
        "paper_title": "Machine learning for emerging infectious disease field responses",
        "publication": "Scientific Reports",
        "citied_by": "32",
        "cover_date": "2022-12-01",
        "Abstract": "Emerging infectious diseases (EIDs), including the latest COVID-19 pandemic, have emerged and raised global public health crises in recent decades. Without existing protective immunity, an EID may spread rapidly and cause mass casualties in a very short time. Therefore, it is imperative to identify cases with risk of disease progression for the optimized allocation of medical resources in case medical facilities are overwhelmed with a flood of patients. This study has aimed to cope with this challenge from the aspect of preventive medicine by exploiting machine learning technologies. The study has been based on 83,227 hospital admissions with influenza-like illness and we analysed the risk effects of 19 comorbidities along with age and gender for severe illness or mortality risk. The experimental results revealed that the decision rules derived from the machine learning based prediction models can provide valuable guidelines for the healthcare policy makers to develop an effective vaccination strategy. Furthermore, in case the healthcare facilities are overwhelmed by patients with EID, which frequently occurred in the recent COVID-19 pandemic, the frontline physicians can incorporate the proposed prediction models to triage patients suffering minor symptoms without laboratory tests, which may become scarce during an EID disaster. In conclusion, our study has demonstrated an effective approach to exploit machine learning technologies to cope with the challenges faced during the outbreak of an EID.",
        "DOI": "10.1038/s41598-021-03687-w",
        "paper_author": "Chiu H.Y.R.",
        "affiliation_name": "National Taiwan University Hospital",
        "affiliation_city": "Taipei",
        "affiliation_country": "Taiwan",
        "affiliation_id": "60073385",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Large-Scale Machine Learning Cluster Scheduling via Multi-Agent Graph Reinforcement Learning",
        "publication": "IEEE Transactions on Network and Service Management",
        "citied_by": "6",
        "cover_date": "2022-12-01",
        "Abstract": "Efficient scheduling of distributed deep learning (DL) jobs in large GPU clusters is crucial for resource efficiency and job performance. While server sharing among jobs improves resource utilization, interference among co-located DL jobs occurs due to resource contention. Interference-aware job placement has been studied, with white-box approaches based on explicit interference modeling and black-box schedulers with reinforcement learning. In today's clusters containing thousands of GPU servers, running a single scheduler to manage all arrival jobs in a timely and effective manner is challenging, due to the large workload scale. We adopt multiple schedulers in a large-scale cluster/data center, and propose a multi-agent reinforcement learning (MARL) scheduling framework to cooperatively learn fine-grained job placement policies, towards the objective of minimizing job completion time (JCT). To achieve topology-aware placements, our proposed framework uses hierarchical graph neural networks to encode the data center topology and server architecture. In view of a common lack of precise reward samples corresponding to different placements, a job interference model is further devised to predict interference levels in face of various co-locations, for training of the MARL schedulers. Testbed and trace-driven evaluations show that our scheduler framework outperforms representative scheduling schemes by more than 20% in terms of average JCT, and is adaptive to various machine learning cluster topologies.",
        "DOI": "10.1109/TNSM.2021.3139607",
        "paper_author": "Zhao X.",
        "affiliation_name": "The University of Hong Kong",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong",
        "affiliation_id": "60006541",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Multiagent Quantum Deep Reinforcement Learning Method for Distributed Frequency Control of Islanded Microgrids",
        "publication": "IEEE Transactions on Control of Network Systems",
        "citied_by": "47",
        "cover_date": "2022-12-01",
        "Abstract": "This article proposes a data-driven method for distributed frequency control of islanded microgrids based on multiagent quantum deep reinforcement learning (DRL). The proposed method combines the conventional DRL framework with quantum machine learning, and can adaptively obtain the optimal cooperative control strategy. The microgrid secondary frequency control is organized in a distributed manner in which each agent performs the control action only based on the local and neighboring information. To solve the DRL problem, the deep deterministic policy gradient algorithm is derived to tune the agents' parameters. Simulation tests are performed on an islanded microgrid with four distributed generators and a 13-bus microgrid. The results demonstrate that the proposed method can effectively regulate the frequency with better time-delay tolerance, and displays the quantum advantage in parameter reduction.",
        "DOI": "10.1109/TCNS.2022.3140702",
        "paper_author": "Yan R.",
        "affiliation_name": "School of Electrical and Electronic Engineering",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore",
        "affiliation_id": "60118454",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Dynamic Multichannel Access Based on Deep Reinforcement Learning in Distributed Wireless Networks",
        "publication": "IEEE Systems Journal",
        "citied_by": "12",
        "cover_date": "2022-12-01",
        "Abstract": "With the emergence of innovative applications in vertical industries such as smart home and industrial automation, machine communication has shown a spurt of development. Different from the traditional human-oriented cellular communication, machine communication is characterized by strong uncertainty and abruptness, large-scale concurrent device connection as well as uneven and unsaturated data traffic. This article investigates the dynamic multiple-devices multiple-channels access for unsaturated traffic with retransmission mechanism, which is aimed at reducing the long-term data packet loss resulting from buffer overflows and transmission failure. The instant channel selection will lead to a non-negligible impact on the future decision, motivating us to model this problem as a Markov decision process. Limited by the unknown environment knowledge, we proposed a dynamic access policy based on deep reinforcement learning algorithm to optimally select the channel for transmission or keep silent for Internet-of-Things devices. Simulation results confirm that our proposed channel access strategy can reduce the collision and the packet loss of network. Furthermore, it can also work well when coexisting with the devices that adopt time division multiple access (TDMA) or ALOHA protocol.",
        "DOI": "10.1109/JSYST.2021.3134820",
        "paper_author": "Cui Q.",
        "affiliation_name": "Beijing University of Posts and Telecommunications",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60016930",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "A distributed deep reinforcement learning–based integrated dynamic bus control system in a connected environment",
        "publication": "Computer-Aided Civil and Infrastructure Engineering",
        "citied_by": "23",
        "cover_date": "2022-12-01",
        "Abstract": "The bus bunching problem caused by the uncertain interstation travel time and passenger demand rate is a critical issue that impairs transit efficiency. Most current bus control studies focus on single or combined strategies while ignoring the bus system's real-time environmental information. This paper proposed a distributed deep reinforcement learning (DRL)-based generic bus dynamic control method to solve the bus bunching problem by maintaining the schedule adherence, headway regularity, and achieving the consensus in the multiagent system. This study built a bus system that utilizes the bus historical and traffic information by incorporating these characteristics into the environment. After that, a distributed DRL-based bus dynamic control strategy is developed based on the bus system, enabling each bus to adjust its motion by any generic method utilizing the weighted downstream buses' information. Regarding the training process, a distributed proximal policy optimization algorithm is adopted for improving the converging performance. Simulated experiments are conducted to verify the control performance, robustness, feasibility, resilience, and generalization capability, which shows that our strategy can significantly reduce the schedule and headway deviations, prevent the accumulation of deviation downstream, and avoid bus bunching.",
        "DOI": "10.1111/mice.12803",
        "paper_author": "Shi H.",
        "affiliation_name": "UW-Madison College of Engineering",
        "affiliation_city": "Madison",
        "affiliation_country": "United States",
        "affiliation_id": "60153131",
        "affiliation_state": "WI"
    },
    {
        "paper_title": "Model-Based Actor-Critic Learning of Robotic Impedance Control in Complex Interactive Environment",
        "publication": "IEEE Transactions on Industrial Electronics",
        "citied_by": "33",
        "cover_date": "2022-12-01",
        "Abstract": "In complex robot applications, such as human-robot interaction and robot machining, robots should interact with an unknown environment. To learn the interactive skill, a model-based actor-critic learning algorithm and a safety-learning strategy are proposed in this article to find the optimal impedance control, in which the learning process is safe and fully automatic and does not know the system parameter. In the learning algorithm, a critic is defined as a quadratic form of the system states and the external force. A modified deterministic policy gradient algorithm is presented to improve the learning efficiency. The proposed approach utilizes a model-based constraint and a highly efficient learning algorithm. In the safety-learning strategy, the robot is trained under a constant force, and the learned impedance control can transfer to different interaction situations by choosing the suitable impedance index. The effectiveness of the learning algorithm and the performance of the learned impedance control are validated in a UR5 robot. The robot can perform human-robot interaction and robot machining tasks after the training process with 100 s training time.",
        "DOI": "10.1109/TIE.2021.3134082",
        "paper_author": "Zhao X.",
        "affiliation_name": "Huazhong University of Science and Technology",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60025761",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "GIS-based air quality modelling: spatial prediction of PM10 for Selangor State, Malaysia using machine learning algorithms",
        "publication": "Environmental Science and Pollution Research",
        "citied_by": "29",
        "cover_date": "2022-12-01",
        "Abstract": "Rapid urbanization has caused severe deterioration of air quality globally, leading to increased hospitalization and premature deaths. Therefore, accurate prediction of air quality is crucial for mitigation planning to support urban sustainability and resilience. Although some studies have predicted air pollutants such as particulate matter (PM) using machine learning algorithms (MLAs), there is a paucity of studies on spatial hazard assessment with respect to the air quality index (AQI). Incorporating PM in AQI studies is crucial because of its easily inhalable micro-size which has adverse impacts on ecology, environment, and human health. Accurate and timely prediction of the air quality index can ensure adequate intervention to aid air quality management. Therefore, this study undertakes a spatial hazard assessment of the air quality index using particulate matter with a diameter of 10 μm or lesser (PM10) in Selangor, Malaysia, by developing four machine learning models: eXtreme Gradient Boosting (XGBoost), random forest (RF), K-nearest neighbour (KNN), and Naive Bayes (NB). Spatially processed data such as NDVI, SAVI, BU, LST, Ws, slope, elevation, and road density was used for the modelling. The model was trained with 70% of the dataset, while 30% was used for cross-validation. Results showed that XGBoost has the highest overall accuracy and precision of 0.989 and 0.995, followed by random forest (0.989, 0.993), K-nearest neighbour (0.987, 0.984), and Naive Bayes (0.917, 0.922), respectively. The spatial air quality maps were generated by integrating the geographical information system (GIS) with the four MLAs, which correlated with Malaysia’s air pollution index. The maps indicate that air quality in Selangor is satisfactory and posed no threats to health. Nevertheless, the two algorithms with the best performance (XGBoost and RF) indicate that a high percentage of the air quality is moderate. The study concludes that successful air pollution management policies such as green infrastructure practice, improvement of energy efficiency, and restrictions on heavy-duty vehicles can be adopted in Selangor and other Southeast Asian cities to prevent deterioration of air quality in the future. Graphical abstract: [Figure not available: see fulltext.]",
        "DOI": "10.1007/s11356-021-16150-0",
        "paper_author": "Tella A.",
        "affiliation_name": "Universiti Teknologi PETRONAS",
        "affiliation_city": "Seri Iskandar",
        "affiliation_country": "Malaysia",
        "affiliation_id": "60001278",
        "affiliation_state": "Perak"
    },
    {
        "paper_title": "AI under great uncertainty: implications and decision strategies for public policy",
        "publication": "AI and Society",
        "citied_by": "33",
        "cover_date": "2022-12-01",
        "Abstract": "Decisions where there is not enough information for a well-informed decision due to unidentified consequences, options, or undetermined demarcation of the decision problem are called decisions under great uncertainty. This paper argues that public policy decisions on how and if to implement decision-making processes based on machine learning and AI for public use are such decisions. Decisions on public policy on AI are uncertain due to three features specific to the current landscape of AI, namely (i) the vagueness of the definition of AI, (ii) uncertain outcomes of AI implementations and (iii) pacing problems. Given that many potential applications of AI in the public sector concern functions central to the public sphere, decisions on the implementation of such applications are particularly sensitive. Therefore, it is suggested that public policy-makers and decision-makers in the public sector can adopt strategies from the argumentative approach in decision theory to mitigate the established great uncertainty. In particular, the notions of framing and temporal strategies are considered.",
        "DOI": "10.1007/s00146-021-01263-4",
        "paper_author": "Nordström M.",
        "affiliation_name": "The Royal Institute of Technology (KTH)",
        "affiliation_city": "Stockholm",
        "affiliation_country": "Sweden",
        "affiliation_id": "60002014",
        "affiliation_state": "Stockholms"
    },
    {
        "paper_title": "QEMDD: Quantum Inspired Ensemble Model to Detect and Mitigate DDoS Attacks at Various Layers of SDN Architecture",
        "publication": "Wireless Personal Communications",
        "citied_by": "8",
        "cover_date": "2022-12-01",
        "Abstract": "Software-defined networking (SDN) is considered a next-generation networking model. Several networking components are managed through a centralized controller that enables efficiency and flexibility in configuring network devices, implementing policy decisions, and managing the underlying network infrastructure through a programmable unit. Despite its default security protocols, SDN is considered to be contradictory towards DDoS attacks. It is observed from state-of-art studies that intrusion in SDN is possible at various layers of its core architecture. Addressing this problem, this article presents a novel ensemble mechanism inspired by quantum cryptography to secure various layers of SDN. This paper presents a two-fold mechanism to secure communications at the SDN architecture's data plane and control plane. It was firstly addressing the secured communication at the data plane, a novel quantum protocol devised. Further, a machine learning-inspired ensemble classifier is devised to detect DDoS attack-prone traffic at the control plane. Simulation studies presented in this article evidenced that the proposed mechanism outperforms the state of art mechanisms in terms of Accuracy and rate of prediction.",
        "DOI": "10.1007/s11277-021-08805-5",
        "paper_author": "Saritha A.",
        "affiliation_name": "Jawaharlal Nehru Technological University Anantapur",
        "affiliation_city": "Anantapur",
        "affiliation_country": "India",
        "affiliation_id": "60106781",
        "affiliation_state": "AP"
    },
    {
        "paper_title": "Nonparametric conditional density estimation in a deep learning framework for short-term forecasting",
        "publication": "Environmental and Ecological Statistics",
        "citied_by": "2",
        "cover_date": "2022-12-01",
        "Abstract": "Short-term forecasting is an important tool in understanding environmental processes. In this paper, we incorporate machine learning algorithms into a conditional distribution estimator for the purposes of forecasting tropical cyclone intensity. Many machine learning techniques give a single-point prediction of the conditional distribution of the target variable, which does not give a full accounting of the prediction variability. Conditional distribution estimation can provide extra insight on predicted response behavior, which could influence decision-making and policy. We propose a technique that simultaneously estimates the entire conditional distribution and flexibly allows for machine learning techniques to be incorporated. A smooth model is fit over both the target variable and covariates, and a logistic transformation is applied on the model output layer to produce an expression of the conditional density function. We provide two examples of machine learning models that can be used, polynomial regression and deep learning models. To achieve computational efficiency, we propose a case–control sampling approximation to the conditional distribution. A simulation study for four different data distributions highlights the effectiveness of our method compared to other machine learning-based conditional distribution estimation techniques. We then demonstrate the utility of our approach for forecasting purposes using tropical cyclone data from the Atlantic Seaboard. This paper gives a proof of concept for the promise of our method, further computational developments can fully unlock its insights in more complex forecasting and other applications.",
        "DOI": "10.1007/s10651-021-00499-z",
        "paper_author": "Huberman D.B.",
        "affiliation_name": "NC State University",
        "affiliation_city": "Raleigh",
        "affiliation_country": "United States",
        "affiliation_id": "60004923",
        "affiliation_state": "NC"
    },
    {
        "paper_title": "The Relationship Between Economic Growth and Electricity Consumption: Bootstrap ARDL Test with a Fourier Function and Machine Learning Approach",
        "publication": "Computational Economics",
        "citied_by": "9",
        "cover_date": "2022-12-01",
        "Abstract": "In this study, the relationship between electricity and growth of the economy is investigated by applying the newly-developed bootstrap autoregressive-distributed lag test with a Fourier function to examine both the causality and cointegration for China, India, and the United States (US). While it is not possible to detect a long-term cointegration relation among the economy's electricity and growth, the study findings demonstrate the contingency of the causality. The ensemble method in machine learning performs better than conventional methods as electricity is an independent indicator for forecast economics. Concerning the US, previous electricity consumption has a positive impact on the current nature of economic growth. In contrast, the consumption of electricity is negatively affected by the development of the economy. However, for China and India, positive and negative feedback can be observed, respectively. Due to the increased awareness of the environment's adverse effects, China should promote technologies that conserve energy and boost energy efficiency to achieve sustainable development in both environmental and economic terms. In India's context, broadening access to electricity has significance for residents in rural areas and enhances economic growth. It is recommended that policy-makers promote innovative technologies in the US, as the abundant natural and human resources can make valuable contributions to the society and development of the economy.",
        "DOI": "10.1007/s10614-021-10097-7",
        "paper_author": "Wu C.F.",
        "affiliation_name": "Hubei University Of Economics",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60108808",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Q learning algorithm for network resource management in vehicular communication network",
        "publication": "Autonomous Vehicles: Smart Vehicles for Communication",
        "citied_by": "1",
        "cover_date": "2022-11-30",
        "Abstract": "Q learning is a machine learning technique which is used in vehicular communication network. It provides faster communication between vehicles. In this paper, we reviewed various algorithms such as value based, policy based, Model based, Q-learning based algorithm used for reinforcement learning. We highlight working of an agent, importance, applications and terminologies of Q learning. This chapter helps researchers to find out the research gap for further research.",
        "DOI": "10.1002/9781394152636.ch13",
        "paper_author": "Agarwal V.",
        "affiliation_name": "Graphic Era Deemed to be University",
        "affiliation_city": "Dehradun",
        "affiliation_country": "India",
        "affiliation_id": "60103785",
        "affiliation_state": "UK"
    },
    {
        "paper_title": "Atlas: Automate Online Service Configuration in Network Slicing",
        "publication": "CoNEXT 2022 - Proceedings of the 18th International Conference on emerging Networking EXperiments and Technologies",
        "citied_by": "3",
        "cover_date": "2022-11-30",
        "Abstract": "Network slicing achieves cost-efficient slice customization to support heterogeneous applications and services. Configuring cross-domain resources to end-to-end slices based on service-level agreements, however, is challenging, due to the complicated underlying correlations and the simulation-to-reality discrepancy between simulators and real networks. In this paper, we propose Atlas, an online network slicing system, which automates the service configuration of slices via safe and sample-efficient learn-to-configure approaches in three interrelated stages. First, we design a learning-based simulator to reduce the sim-to-real discrepancy, which is accomplished by a new parameter searching method based on Bayesian optimization. Second, we offline train the policy in the augmented simulator via a novel offline algorithm with a Bayesian neural network and parallel Thompson sampling. Third, we online learn the policy in real networks with a novel online algorithm with safe exploration and Gaussian process regression. We implement Atlas on an end-to-end network prototype based on OpenAirInterface RAN, OpenDayLight SDN transport, OpenAir-CN core network, and Docker-based edge server. Experimental results show that, compared to state-of-the-art solutions, Atlas achieves 63.9% and 85.7% regret reduction on resource usage and slice quality of experience during the online learning stage, respectively.",
        "DOI": "10.1145/3555050.3569115",
        "paper_author": "Liu Q.",
        "affiliation_name": "University of Nebraska–Lincoln",
        "affiliation_city": "Lincoln",
        "affiliation_country": "United States",
        "affiliation_id": "60026306",
        "affiliation_state": "NE"
    },
    {
        "paper_title": "Recent Rapid Increase of Cover Crop Adoption Across the U.S. Midwest Detected by Fusing Multi-Source Satellite Data",
        "publication": "Geophysical Research Letters",
        "citied_by": "36",
        "cover_date": "2022-11-28",
        "Abstract": "Cover crops have critical significance for agroecosystem sustainability and have long been promoted in the U.S. Midwest. Knowledge of cover cropping variations and impacts of government policies remains very limited. We developed an accurate and cost-effective approach utilizing satellite fusion data, environmental variables, and machine learning to quantify cover cropping in corn and soybean fields from 2000 to 2021 in the U.S. Midwest. We found that cover crop adoption in most counties was stagnant from 2000 to 2011, but has significantly increased from 2011 to 2021. The adoption of 2021 is four times that of 2011, which was highly correlated to the funding for conservation programs. However, the percentage of cover crop adoption in the U.S. Midwest is still low (7.2%). Our work fills a critical gap in quantifying long-term field-level cover crop adoption at large regions and highlights the potential importance of incentive programs to promote sustainable agricultural practices.",
        "DOI": "10.1029/2022GL100249",
        "paper_author": "Zhou Q.",
        "affiliation_name": "University of Illinois Urbana-Champaign",
        "affiliation_city": "Urbana",
        "affiliation_country": "United States",
        "affiliation_id": "60000745",
        "affiliation_state": "IL"
    },
    {
        "paper_title": "Linked Open Government Data to Predict and Explain House Prices: The Case of Scottish Statistics Portal",
        "publication": "Big Data Research",
        "citied_by": "22",
        "cover_date": "2022-11-28",
        "Abstract": "Accurately estimating the prices of houses is important for various stakeholders including house owners, real estate agencies, government agencies, and policy-makers. Towards this end, traditional statistics and, only recently, advanced machine learning and artificial intelligence models are used. Open Government Data (OGD) have a huge potential especially when combined with AI technologies. OGD are often published as linked data to facilitate data integration and re-usability. EXplainable Artificial Intelligence (XAI) can be used by stakeholders to understand the decisions of a predictive model. This work creates a model that predicts house prices by applying machine learning on linked OGD. We present a case study that uses XGBoost, a powerful machine learning algorithm, and linked OGD from the official Scottish data portal to predict the probability the mean prices of houses in the various data zones of Scotland to be higher than the average price in Scotland. XAI is also used to globally and locally explain the decisions of the model. The created model has Receiver Operating Characteristic (ROC) AUC score 0.923 and Precision Recall Curve (PRC) AUC score 0.891. According to XAI, the variable that mostly affects the decisions of the model is Comparative Illness Factor, an indicator of health conditions. However, local explainability shows that the decisions made in some data zones may be mostly affected by other variables such as the percent of detached dwellings and employment deprived population.",
        "DOI": "10.1016/j.bdr.2022.100355",
        "paper_author": "Karamanou A.",
        "affiliation_name": "University of Macedonia",
        "affiliation_city": "Thessaloniki",
        "affiliation_country": "Greece",
        "affiliation_id": "60001086",
        "affiliation_state": "Central Macedonia"
    },
    {
        "paper_title": "Plastics tsunami: Can a landmark treaty stop waste from choking the oceans?",
        "publication": "Nature",
        "citied_by": "14",
        "cover_date": "2022-11-24",
        "Abstract": "NA",
        "DOI": "10.1038/d41586-022-03793-3",
        "paper_author": "Subramanian M.",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Bibliometric analysis and critical review of the research on big data in the construction industry",
        "publication": "Engineering, Construction and Architectural Management",
        "citied_by": "25",
        "cover_date": "2022-11-24",
        "Abstract": "Purpose: The digital revolution and the use of big data (BD) in particular has important applications in the construction industry. In construction, massive amounts of heterogeneous data need to be analyzed to improve onsite efficiency. This article presents a systematic review and identifies future research directions, presenting valuable conclusions derived from rigorous bibliometric tools. The results of this study may provide guidelines for construction engineering and global policymaking to change the current low-efficiency of construction sites. Design/methodology/approach: This study identifies research trends from 1,253 peer-reviewed papers, using general statistics, keyword co-occurrence analysis, critical review, and qualitative-bibliometric techniques in two rounds of search. Findings: The number of studies in this area rapidly increased from 2012 to 2020. A significant number of publications originated in the UK, China, the US, and Australia, and the smallest number from one of these countries is more than twice the largest number in the remaining countries. Keyword co-occurrence is divided into three clusters: BD application scenarios, emerging technology in BD, and BD management. Currently developing approaches in BD analytics include machine learning, data mining, and heuristic-optimization algorithms such as graph convolutional, recurrent neural networks and natural language processes (NLP). Studies have focused on safety management, energy reduction, and cost prediction. Blockchain integrated with BD is a promising means of managing construction contracts. Research limitations/implications: The study of BD is in a stage of rapid development, and this bibliometric analysis is only a part of the necessary practical analysis. Practical implications: National policies, temporal and spatial distribution, BD flow are interpreted, and the results of this may provide guidelines for policymakers. Overall, this work may develop the body of knowledge, producing a reference point and identifying future development. Originality/value: To our knowledge, this is the first bibliometric review of BD in the construction industry. This study can also benefit construction practitioners by providing them a focused perspective of BD for emerging practices in the construction industry.",
        "DOI": "10.1108/ECAM-01-2021-0005",
        "paper_author": "Lu Y.",
        "affiliation_name": "Tongji University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60073652",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A review of on-road vehicle emission inventory",
        "publication": "E3S Web of Conferences",
        "citied_by": "1",
        "cover_date": "2022-11-23",
        "Abstract": "The large increase in the on-road vehicle population in China has raised sustainability concerns regarding air pollution prevention, energy conservation, and climate change mitigation. Vehicle emission inventory is an irreplaceable tool to characterize the temporal and spatial distribution of the air pollutant and provide guidance to the policy makers with effective vehicle emission controls. This review paper reviewed two kinds of typical vehicle emission inventories. The top-down vehicle emission inventories is calculated based on the static datasets (e.g., vehicle population, vehicle kilometer traveled, and fuel consumption). These inventories could track historical emissions abatement progress and examine potential benefits from future regulations. The technological evolution in intelligent transportation systems have facilitated emission inventories to satisfy the increasing sophisticated management demand. The bottom-up link-level vehicle emission inventories are development based on the availability of the real-world traffic profiles. To simulate the temporal and spatial patterns with high-resolution, traffic demand model and machine learning methods are employed to elucidate traffic emissions.",
        "DOI": "10.1051/e3sconf/202236001027",
        "paper_author": "Yang D.",
        "affiliation_name": "Ministry of Transport of the People's Republic of China",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60087828",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Are general circulation models obsolete?",
        "publication": "Proceedings of the National Academy of Sciences of the United States of America",
        "citied_by": "31",
        "cover_date": "2022-11-22",
        "Abstract": "Traditional general circulation models, or GCMs—that is, three-dimensional dynamical models with unresolved terms represented in equations with tunable parameters—have been a mainstay of climate research for several decades, and some of the pioneering studies have recently been recognized by a Nobel prize in Physics. Yet, there is considerable debate around their continuing role in the future. Frequently mentioned as limitations of GCMs are the structural error and uncertainty across models with different representations of unresolved scales and the fact that the models are tuned to reproduce certain aspects of the observed Earth. We consider these shortcomings in the context of a future generation of models that may address these issues through substantially higher resolution and detail, or through the use of machine learning techniques to match them better to observations, theory, and process models. It is our contention that calibration, far from being a weakness of models, is an essential element in the simulation of complex systems, and contributes to our understanding of their inner workings. Models can be calibrated to reveal both fine-scale detail and the global response to external perturbations. New methods enable us to articulate and improve the connections between the different levels of abstract representation of climate processes, and our understanding resides in an entire hierarchy of models where GCMs will continue to play a central role for the foreseeable future.",
        "DOI": "10.1073/pnas.2202075119",
        "paper_author": "Balaji V.",
        "affiliation_name": "Princeton University",
        "affiliation_city": "Princeton",
        "affiliation_country": "United States",
        "affiliation_id": "60003269",
        "affiliation_state": "NJ"
    },
    {
        "paper_title": "Preparing for the next pandemic: Simulation-based deep reinforcement learning to discover and test multimodal control of systemic inflammation using repurposed immunomodulatory agents",
        "publication": "Frontiers in Immunology",
        "citied_by": "4",
        "cover_date": "2022-11-21",
        "Abstract": "Background: Preparation to address the critical gap in a future pandemic between non-pharmacological measures and the deployment of new drugs/vaccines requires addressing two factors: 1) finding virus/pathogen-agnostic pathophysiological targets to mitigate disease severity and 2) finding a more rational approach to repurposing existing drugs. It is increasingly recognized that acute viral disease severity is heavily driven by the immune response to the infection (“cytokine storm” or “cytokine release syndrome”). There exist numerous clinically available biologics that suppress various pro-inflammatory cytokines/mediators, but it is extremely difficult to identify clinically effective treatment regimens with these agents. We propose that this is a complex control problem that resists standard methods of developing treatment regimens and accomplishing this goal requires the application of simulation-based, model-free deep reinforcement learning (DRL) in a fashion akin to training successful game-playing artificial intelligences (AIs). This proof-of-concept study determines if simulated sepsis (e.g. infection-driven cytokine storm) can be controlled in the absence of effective antimicrobial agents by targeting cytokines for which FDA-approved biologics currently exist. Methods: We use a previously validated agent-based model, the Innate Immune Response Agent-based Model (IIRABM), for control discovery using DRL. DRL training used a Deep Deterministic Policy Gradient (DDPG) approach with a clinically plausible control interval of 6 hours with manipulation of six cytokines for which there are existing drugs: Tumor Necrosis Factor (TNF), Interleukin-1 (IL-1), Interleukin-4 (IL-4), Interleukin-8 (IL-8), Interleukin-12 (IL-12) and Interferon-γ(IFNg). Results: DRL trained an AI policy that could improve outcomes from a baseline Recovered Rate of 61% to one with a Recovered Rate of 90% over ~21 days simulated time. This DRL policy was then tested on four different parameterizations not seen in training representing a range of host and microbe characteristics, demonstrating a range of improvement in Recovered Rate by +33% to +56% Discussion: The current proof-of-concept study demonstrates that significant disease severity mitigation can potentially be accomplished with existing anti-mediator drugs, but only through a multi-modal, adaptive treatment policy requiring implementation with an AI. While the actual clinical implementation of this approach is a projection for the future, the current goal of this work is to inspire the development of a research ecosystem that marries what is needed to improve the simulation models with the development of the sensing/assay technologies to collect the data needed to iteratively refine those models.",
        "DOI": "10.3389/fimmu.2022.995395",
        "paper_author": "Cockrell C.",
        "affiliation_name": "University of Vermont Larner College of Medicine",
        "affiliation_city": "Burlington",
        "affiliation_country": "United States",
        "affiliation_id": "60031576",
        "affiliation_state": "VT"
    },
    {
        "paper_title": "Synthetic data as an enabler for machine learning applications in medicine",
        "publication": "iScience",
        "citied_by": "54",
        "cover_date": "2022-11-18",
        "Abstract": "Synthetic data generation is the process of using machine learning methods to train a model that captures the patterns in a real dataset. Then new or synthetic data can be generated from that trained model. The synthetic data does not have a one-to-one mapping to the original data or to real patients, and therefore has the potential of privacy preserving properties. There is a growing interest in the application of synthetic data across health and life sciences, but to fully realize the benefits, further education, research, and policy innovation is required. This article summarizes the opportunities and challenges of SDG for health data, and provides directions for how this technology can be leveraged to accelerate data access for secondary purposes.",
        "DOI": "10.1016/j.isci.2022.105331",
        "paper_author": "Rajotte J.F.",
        "affiliation_name": "The University of British Columbia",
        "affiliation_city": "Vancouver",
        "affiliation_country": "Canada",
        "affiliation_id": "60010365",
        "affiliation_state": "BC"
    },
    {
        "paper_title": "Impact of enterprise digitalization on green innovation performance under the perspective of production and operation",
        "publication": "Frontiers in Public Health",
        "citied_by": "21",
        "cover_date": "2022-11-17",
        "Abstract": "Introduction: How enterprises should practice digitalization transformation to effectively improve green innovation performance is related to the sustainable development of enterprises and the economy, which is an important issue that needs to be clarified. Methods: This research uses the perspective of production and operation to deconstruct the digitalization of industrial listed enterprises from 2016 to 2020 into six features. A variety of machine learning methods are used, including DBSCAN, CART and other algorithms, to specifically explore the complex impact of enterprise digitalization feature configuration on green innovation performance. Conclusions: (1) The more advanced digitalization transformation the enterprises have, the more possibly the high green innovation performance can be achieved. (2) Digitalization innovation is the digitalization element with the strongest influence ability on green innovation performance. (3) As the advancement of digitalization transformation, enterprises should also focus on digitalization innovation input and digitalization operation output, otherwise they should pay attention to digitalization management and digitalization operation output. Discussion: The conclusions of this research will help enterprises understand their digitalization competitiveness and how to practice digitalization transformation to enhance green innovation performance, and also help the government to formulate policies to promote the development of green innovation in the digital economy era.",
        "DOI": "10.3389/fpubh.2022.971971",
        "paper_author": "Li H.",
        "affiliation_name": "Huaqiao University",
        "affiliation_city": "Quanzhou",
        "affiliation_country": "China",
        "affiliation_id": "60006106",
        "affiliation_state": "Fujian"
    },
    {
        "paper_title": "Using machine learning to assess the livelihood impact of electricity access",
        "publication": "Nature",
        "citied_by": "30",
        "cover_date": "2022-11-17",
        "Abstract": "In many regions of the world, sparse data on key economic outcomes inhibit the development, targeting and evaluation of public policy1,2. We demonstrate how advancements in satellite imagery and machine learning (ML) can help ameliorate these data and inference challenges. In the context of an expansion of the electrical grid across Uganda, we show how a combination of satellite imagery and computer vision can be used to develop local-level livelihood measurements appropriate for inferring the causal impact of electricity access on livelihoods. We then show how ML-based inference techniques deliver more reliable estimates of the causal impact of electrification than traditional alternatives when applied to these data. We estimate that grid access improves village-level asset wealth in rural Uganda by up to 0.15 standard deviations, more than doubling the growth rate during our study period relative to untreated areas. Our results provide country-scale evidence on the impact of grid-based infrastructure investment and our methods provide a low-cost, generalizable approach to future policy evaluation in data-sparse environments.",
        "DOI": "10.1038/s41586-022-05322-8",
        "paper_author": "Ratledge N.",
        "affiliation_name": "Stanford University",
        "affiliation_city": "Stanford",
        "affiliation_country": "United States",
        "affiliation_id": "60012708",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Are plankton nets a thing of the past? An assessment of in situ imaging of zooplankton for large-scale ecosystem assessment and policy decision-making",
        "publication": "Frontiers in Marine Science",
        "citied_by": "14",
        "cover_date": "2022-11-16",
        "Abstract": "Zooplankton are fundamental to aquatic ecosystem services such as carbon and nutrient cycling. Therefore, a robust evidence base of how zooplankton respond to changes in anthropogenic pressures, such as climate change and nutrient loading, is key to implementing effective policy-making and management measures. Currently, the data on which to base this evidence, such as long time-series and large-scale datasets of zooplankton distribution and community composition, are too sparse owing to practical limitations in traditional collection and analysis methods. The advance of in situ imaging technologies that can be deployed at large scales on autonomous platforms, coupled with artificial intelligence and machine learning (AI/ML) for image analysis, promises a solution. However, whether imaging could reasonably replace physical samples, and whether AI/ML can achieve a taxonomic resolution that scientists trust, is currently unclear. We here develop a roadmap for imaging and AI/ML for future zooplankton monitoring and research based on community consensus. To do so, we determined current perceptions of the zooplankton community with a focus on their experience and trust in the new technologies. Our survey revealed a clear consensus that traditional net sampling and taxonomy must be retained, yet imaging will play an important part in the future of zooplankton monitoring and research. A period of overlapping use of imaging and physical sampling systems is needed before imaging can reasonably replace physical sampling for widespread time-series zooplankton monitoring. In addition, comprehensive improvements in AI/ML and close collaboration between zooplankton researchers and AI developers are needed for AI-based taxonomy to be trusted and fully adopted. Encouragingly, the adoption of cutting-edge technologies for zooplankton research may provide a solution to maintaining the critical taxonomic and ecological knowledge needed for future zooplankton monitoring and robust evidence-based policy decision-making.",
        "DOI": "10.3389/fmars.2022.986206",
        "paper_author": "Giering S.L.C.",
        "affiliation_name": "National Oceanography Centre Southampton",
        "affiliation_city": "Southampton",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60010934",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "DeepSolar++: Understanding residential solar adoption trajectories with computer vision and technology diffusion models",
        "publication": "Joule",
        "citied_by": "21",
        "cover_date": "2022-11-16",
        "Abstract": "Although the United States has generally experienced a rapid adoption of residential photovoltaics (PV), many communities are lagging behind. To investigate why, we developed a computer vision model that addresses the challenge of low image resolution to identify the installation year of PVs from historical aerial and satellite images. We used the model to construct a granular spatiotemporal dataset of PV deployment across 46 US states and analyzed these data from a technology adoption life cycle perspective. Our analysis of adoption curves and phases showed that low-income communities are not only delayed in their adoption onset but also saturate more quickly at lower levels. We further demonstrated the value of our data via an illustrative analysis of financial incentives and found that performance-based incentives are positively associated with saturated adoption levels—particularly for lower-income communities. Our study highlights the importance of analyzing PV adoption trajectories from dynamic perspectives to inform policy design.",
        "DOI": "10.1016/j.joule.2022.09.011",
        "paper_author": "Wang Z.",
        "affiliation_name": "Department of Civil &amp; Environmental Engineering",
        "affiliation_city": "Stanford",
        "affiliation_country": "United States",
        "affiliation_id": "60141511",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Tracking Daily Concentrations of PM<inf>2.5</inf>Chemical Composition in China since 2000",
        "publication": "Environmental Science and Technology",
        "citied_by": "141",
        "cover_date": "2022-11-15",
        "Abstract": "PM2.5chemical components play significant roles in the climate, air quality, and public health, and the roles vary due to their different physicochemical properties. Obtaining accurate and timely updated information on China's PM2.5chemical composition is the basis for research and environmental management. Here, we developed a full-coverage near-real-time PM2.5chemical composition data set at 10 km spatial resolution since 2000, combining the Weather Research and Forecasting-Community Multiscale Air Quality modeling system, ground observations, a machine learning algorithm, and multisource-fusion PM2.5data. PM2.5chemical components in our data set are in good agreement with the available observations (correlation coefficients range from 0.64 to 0.75 at a monthly scale from 2000 to 2020 and from 0.67 to 0.80 at a daily scale from 2013 to 2020; most normalized mean biases within ±20%). Our data set reveals the long-term trends in PM2.5chemical composition in China, especially the rapid decreases after 2013 for sulfate, nitrate, ammonium, organic matter, and black carbon, at the rate of -9.0, -7.2, -8.1, -8.4, and -9.2% per year, respectively. The day-to-day variability is also well captured, including evolutions in spatial distribution and shares of PM2.5components. As part of Tracking Air Pollution in China (http://tapdata.org.cn), this daily-updated data set provides large opportunities for health and climate research as well as policy-making in China.",
        "DOI": "10.1021/acs.est.2c06510",
        "paper_author": "Liu S.",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60025278",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Land use optimization in Ningbo City with a coupled GA and PLUS model",
        "publication": "Journal of Cleaner Production",
        "citied_by": "104",
        "cover_date": "2022-11-15",
        "Abstract": "Industrialization and urbanization have resulted in a series of problems such as an excessive intensity of land utilization and an unbalanced structure of land use. Contradictions and conflicts regarding different uses of land are becoming increasingly problematic. The question of how to coordinate the competition among the driving factors of land resources and how to balance and optimize land use types with different functions are urgent problems related to achieving sustainable development. In this study, the traditional model of the land use change process and a machine learning algorithm were integrated to establish an optimal land use allocation model considering macro-quantity control and spatial unit allocation. Taking Ningbo as an example, on the basis of considering the function of land resources and the driving mechanism of land use change, genetic algorithm (GA) and patch-generating land use simulation (PLUS) model were used to optimize the quantitative structure and spatial layout of production−living−ecological land (PLEL), respectively. The GA-PLUS model demonstrated strong robustness. The results show that Ningbo covers a total land area of 9816 km2, and the rates of increase in the area of production land (PL), living land (LL) and ecological land (EL) in Ningbo from 2010 to 2018 were −2.04%, 10.07% and −0.69%, respectively. The main problems were the unbalanced distribution of PLEL, the disordered expansion of industrial production land (IPL) and LL, occupying 3.07% of agricultural production land and 0.67% of EL. Guided by Sustainable Development Goals, the construction goals related to the Beautiful China policy and the development planning goals of Ningbo city, the collaborative optimization of the quantitative structure and spatial layout of PLEL in Ningbo can be realized through the GA-PLUS model. In terms of quantitative structure, Ningbo needs to control the total amount of construction land in the future, and improve the proportion of LL and EL, about 13.04% and 51.75%. In terms of spatial layout, it is necessary to build industrial clusters for IPL in coastal areas, improve the suitability of EL in the north of Ningbo, and address the fact that it is difficult for LL to expand due to the influence of the complex terrain in Yuyao and Xiangshan. The results of this study can guide the optimization and development path of the territorial space of Ningbo, and provide technical support for decision-makers to formulate targeted land space planning and realize regional sustainable development.",
        "DOI": "10.1016/j.jclepro.2022.134004",
        "paper_author": "Li X.",
        "affiliation_name": "China University of Mining &amp; Technology, Beijing",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60108755",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Pseudo RGB-D Face Recognition",
        "publication": "IEEE Sensors Journal",
        "citied_by": "74",
        "cover_date": "2022-11-15",
        "Abstract": "In the last decade, advances and popularity of low-cost RGB-D sensors have enabled us to acquire depth information of objects. Consequently, researchers began to solve face recognition problems by capturing RGB-D face images using these sensors. Until now, it is not easy to acquire the depth of human faces because of limitations imposed by privacy policies, and RGB face images are still more common. Therefore, obtaining the depth map directly from the corresponding RGB image could be helpful to improve the performance of subsequent face processing tasks, such as face recognition. Intelligent creatures can use a large amount of experience to obtain 3D spatial information only from 2D plane scenes. It is machine learning methodology, which is to solve such problems, that can teach computers to generate correct answers by training. To replace the depth sensors by generated pseudo-depth maps, in this article, we propose a pseudo RGB-D face recognition framework and provide data-driven ways to generate the depth maps from 2D face images. Specially we design and implement a generative adversarial network model named 'D+GAN' to perform the multiconditional image-to-image translation with face attributes. By this means, we validate the pseudo RGB-D face recognition with experiments on various datasets. With the cooperation of image fusion technologies, especially non-subsampled shearlet transform (NSST), the accuracy of face recognition has been significantly improved.",
        "DOI": "10.1109/JSEN.2022.3197235",
        "paper_author": "Jin B.",
        "affiliation_name": "University of Coimbra, Institute of Systems and Robotics",
        "affiliation_city": "Coimbra",
        "affiliation_country": "Portugal",
        "affiliation_id": "60106429",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A multi-step predictive deep reinforcement learning algorithm for HVAC control systems in smart buildings",
        "publication": "Energy",
        "citied_by": "41",
        "cover_date": "2022-11-15",
        "Abstract": "The development of the building energy management systems (BEMS) enable users to intelligently control Heating, Ventilation, Air-conditioning and Cooling (HVAC) systems based on digital information. In order to reduce the power consumption cost of the HVAC system while ensuring user satisfaction, a novel HVAC control system for building system based on a multi-step predictive deep reinforcement learning (MSP-DRL) algorithm is proposed in this paper. In the proposed method, the outdoor ambient temperature is predicted first by a featured deep learning method named GC-LSTM, where the Long Short-term Memory (LSTM) is enhanced by the generalized correntropy (GC) loss function to deal with the non-Gaussian characteristics of the collected outdoor temperature. In addition, the proposed temperature prediction model is combined with a reinforcement learning algorithm named Deep Deterministic Policy Gradient (DDPG) aiming to flexibly adjust the output power of the HVAC system under the dynamic changing of electricity prices. Finally, comprehensive simulation based on real world data is delivered. Numerical results show that the GC-LSTM algorithm is more accurate than other counterparts prediction algorithms, and the proposed HVAC control system based on the multi-step prediction deep reinforcement learning algorithm is effective and could save over 12% cost compared to other approaches, where the user comfort is maintained simultaneously.",
        "DOI": "10.1016/j.energy.2022.124857",
        "paper_author": "Liu X.",
        "affiliation_name": "Taiyuan University of Technology",
        "affiliation_city": "Taiyuan",
        "affiliation_country": "China",
        "affiliation_id": "60013268",
        "affiliation_state": "Shanxi"
    },
    {
        "paper_title": "Demurrage pattern analysis using logical analysis of data: A case study of the Ulsan Port Authority",
        "publication": "Expert Systems with Applications",
        "citied_by": "8",
        "cover_date": "2022-11-15",
        "Abstract": "Maritime logistics, which accounts for around 80% of international trade around the world, has been a driving force for economic growth. Increases in maritime traffic, however, lead to increased congestion in berths and terminals. This congestion in turn negatively affects the total ship turnaround time and leads to decreased efficiency in port operations and a higher demurrage rate, which refers to the number of vessels in queue for more than a fixed time period waiting to load/unload out of the total number of vessels entering a port. The demurrage rate directly affects a port's operating profits; thus, this rate needs to stay as low as possible. In this study, we focus on developing a methodology to address the demurrage rate of a port. To this end, we first collect two sets of vessel data (2016 annual data for training and 2019 annual data for validating) for ships entering and leaving the Port of Ulsan in the Republic of Korea and integrate these datasets with berth data and weather data. We tailor the logical analysis of data (LAD) technique to derive the patterns from the training data that mitigate or aggravate the demurrage rate. We use these patterns to predict the demurrage rate for the validating set of data. The overall binary classification results demonstrate the proposed LAD technique's competitive performance, compared with other state-of-the-art machine learning methods. We then analyze the patterns to derive policy suggestions that can lower the demurrage rate at the Port of Ulsan. Our computational experiments find that the availability of tugs or pilots and port arrival times mainly affect the demurrage rate at the Port of Ulsan. Finally, our study showcases new possibilities for using patterns of demurrage and non-demurrage vessels obtained by LAD to help policymakers and port operators address the growing demurrage problem.",
        "DOI": "10.1016/j.eswa.2022.117745",
        "paper_author": "Kweon S.J.",
        "affiliation_name": "Ulsan National Institute of Science and Technology",
        "affiliation_city": "Ulsan",
        "affiliation_country": "South Korea",
        "affiliation_id": "60103153",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "An overview of the 2022 NISO plus conference: Global conversations/Global Connections",
        "publication": "Information Services and Use",
        "citied_by": "2",
        "cover_date": "2022-11-11",
        "Abstract": "This paper offers an overview of some of the highlights of the 2022 NISO Plus Annual Conference that was held virtually from February 15 - February 18, 2022. This was the third such conference and the second to be held in a completely virtual format due to the Pandemic. These conferences have resulted from the merger of NISO and the National Federation of Abstracting and Information Services (NFAIS) in June 2019, replacing the NFAIS Annual Conferences and offering a new, more interactive format. As with last year, there was no general topical theme, but there were topics of interest for everyone working in the information ecosystem - from the practical subjects of standards and metadata quality to preprints, Wikidata, archiving and digital preservation, Open Science and Open Access, and ultimately Globalization of the Information Infrastructure, the Metaverse, and Visions of the Future. With speakers and attendees from around the world and across multiple time zones and continents, it truly was a global conversation!",
        "DOI": "10.3233/ISU-220178",
        "paper_author": "Lawlor B.",
        "affiliation_name": "NFAIS Honorary Fellow",
        "affiliation_city": "Radnor",
        "affiliation_country": "United States",
        "affiliation_id": "117729636",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "Longitudinal neighbourhood determinants with cognitive health and dementia disparities: protocol of the Multi-Ethnic Study of Atherosclerosis Neighborhoods and Aging prospective cohort study",
        "publication": "BMJ Open",
        "citied_by": "7",
        "cover_date": "2022-11-11",
        "Abstract": "Introduction The burden of Alzheimer's disease (AD) and AD-related dementias (ADRD) is increasing nationally and globally, with disproportionate impacts on lower-income, lower education and systematically marginalised older adults. Presence of inequalities in neighbourhood factors (eg, social context, physical and built environments) may affect risk of cognitive decline and be key for intervening on AD/ADRD disparities at the population level. However, existing studies are limited by a dearth of longitudinal, detailed neighbourhood measures linked to rich, prospective cohort data. Our main objective is to identify patterns of neighbourhood change related to prevalence of - and disparities in - cognitive decline and dementia. Methods and analyses We describe the process of collecting, processing and linking extensive neighbourhood data to the Multi-Ethnic Study of Atherosclerosis (MESA), creating a 25+ years dataset. Within the MESA parent study, the MESA Neighborhoods and Aging cohort study will characterise dynamic, longitudinal neighbourhood social and built environment variables relevant to cognition for residential addresses of MESA participants. This includes administering new surveys, expanding residential address histories, calculating new measures derived from spatial data and implementing novel deep learning algorithms on street-level imagery. Applying novel statistical techniques, we will examine associations of neighbourhood environmental characteristics with cognition and clinically relevant AD/ADRD outcomes. We will investigate determinants of disparities in outcomes by socioeconomic position and race/ethnicity and assess the contribution of neighbourhood environments to these disparities. This project will provide new evidence about pathways between neighbourhood environments and cognitive outcomes, with implications for policies to support healthy ageing. Ethics and dissemination This project was approved by the University of Washington and Drexel University Institutional Review Boards (protocols #00009029 and #00014523, and #180900605). Data will be distributed through the MESA Coordinating Center. Findings will be disseminated in peer-reviewed scientific journals, briefs, presentations and on the participant website.",
        "DOI": "10.1136/bmjopen-2022-066971",
        "paper_author": "Hirsch J.A.",
        "affiliation_name": "Drexel University",
        "affiliation_city": "Philadelphia",
        "affiliation_country": "United States",
        "affiliation_id": "60014662",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "Producer-consumer sustainability continuum: Mutual understanding to implement extended producer responsibility",
        "publication": "Journal of Cleaner Production",
        "citied_by": "11",
        "cover_date": "2022-11-10",
        "Abstract": "It is widely accepted today that extended producer responsibility (EPR) is a legal means of ensuring sustainable production. As a result, the producer is responsible for environmental hazards resulting from the consumption of the product after it has been produced. Due to this, legislators and manufacturers pay great attention to this issue since a harmonious and mutually understanding relationship between consumers and producers is crucial to the success of these programs. Through an examination of the sustainability reports published by smartphone manufacturers and tweets regarding the EPR plans and programs within the community, this study analyzes the interaction between consumers and manufacturers for a deeper understanding. To perform topic modeling, machine learning algorithms and natural language processing algorithms have been applied to sustainability reports and tweets from people. The topics extracted from consumers and producers were divided into six general categories that form the roadmap created in the field of EPR. As part of the study, sentiment analysis was conducted in order to understand consumers' and producers' perceptions of this issue. Based on the results of this study, manufacturers have taken steps to address social needs and concerns about the dangers of electronic waste (E-waste). In light of the fact that sustainability covers both production activities and consumption activities at the same time and that a large portion of society views EPR programs in a neutral manner, there is a need for social awareness programs that adequately guide and inform individuals. Hence, culture building among the consumer community is one of the most important aspects of corporate strategy.",
        "DOI": "10.1016/j.jclepro.2022.133880",
        "paper_author": "Jafari S.Q.",
        "affiliation_name": "Shahid Beheshti University",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran",
        "affiliation_id": "60032873",
        "affiliation_state": "Tehran"
    },
    {
        "paper_title": "LPR: Learning-based Page Replacement Scheme for Scientific Applications",
        "publication": "Middleware 2022 Industrial Track - Proceedings of the 23rd International Middleware Conference Industrial Track, Part of Middleware 2022",
        "citied_by": "0",
        "cover_date": "2022-11-07",
        "Abstract": "Recent advances in machine learning techniques open up new opportunities for solving problems in other domains. One of these problems, the page replacement system, attempts to use machine learning techniques since they have a significant impact on application performance. Specifically, scientific applications show certain data access patterns, such as iterative memory access through loops or linked lists, while performing arithmetic operations. For such applications, providing self-tunable page replacement systems can improve application performance. In this paper, we present a Learning-based Page Replacement (LPR) scheme for scientific applications. We propose a model that learns the memory reference patterns of a given application and determines the best-fit page replacement policy online. Using two least/most-recently used (LRU/MRU)-based replacement policies, LPR gives a reward or penalty to each policy according to its previous decisions. LPR evolves its own page replacement policy that can minimize cumulative regrets for each decision. Our scheme provides efficient memory management without explicitly detecting application-specific memory access patterns through self-learning. The experimental results show that our scheme properly detects the changes in memory access patterns and handles page replacement online using the best-fit policy with little overhead.",
        "DOI": "10.1145/3564695.3564777",
        "paper_author": "Kim H.",
        "affiliation_name": "Seoul National University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea",
        "affiliation_id": "60013682",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Poster: A Novel Formal Threat Analyzer for Activity Monitoring-based Smart Home Heating, Ventilation, and Cooling Control System",
        "publication": "Proceedings of the ACM Conference on Computer and Communications Security",
        "citied_by": "1",
        "cover_date": "2022-11-07",
        "Abstract": "Contemporary home control systems determine real-time heating/cooling demands utilizing smart sensor devices, giving rise to demand control heating, ventilation, and cooling (DCHVAC) systems, thus improving the home's energy efficiency. The adoption of activity monitoring in the smart home control system further augments the controller efficiency and improves occupants' comfort and productivity, elderly monitoring, and so forth. Additionally, the learned occupants' activity patterns help embed machine learning (ML)-based abnormality detection capability to track inconsistencies among the zone sensor measurements. Hence, the incorporation of an activity monitoring system assists anomaly detection models (ADMs) in detecting false data injection (FDI) attacks that are being glowingly researched due to their massive damage capability. However, in this work, we propose a novel attack strategy that identified that the knowledge of occupants' activities along with indoor air quality (IAQ) and occupancy sensor measurements allows the attackers to launch even more hazardous attack (i.e., significant increment in energy cost/ worsening health conditions for the occupants). Hence, it is crucial to analyze the security of the activity monitoring-based smart home DCHVAC system. Accordingly, we propose a novel formal threat analyzer that analyzes the threat space of the smart home DCHVAC control system, which is modeled by rule-based control policies and ML-based ADMs. The rules from the ADM are extracted through an efficient algorithm. The constraints associated with the rules are solved through a satisfiability module theorem (SMT)-based solver. %We performed our initial evaluation of the proposed threat analyzer's effectiveness on the Center of Advanced Studies in Adaptive Systems (CASAS) dataset using some metrics. We will further experiment with other metrics along experimenting with our collaborator's dataset (KTH live-in lab) and open-source Örebro datasets for assessing the framework with realistic occupants' activity. Moreover, we also created our prototype testbed for evaluating the feasibility of the proposed attack and threat analyzer.",
        "DOI": "10.1145/3548606.3563547",
        "paper_author": "Haque N.I.",
        "affiliation_name": "Florida International University",
        "affiliation_city": "Miami",
        "affiliation_country": "United States",
        "affiliation_id": "60015206",
        "affiliation_state": "FL"
    },
    {
        "paper_title": "CCSW '22: The 2022 Cloud Computing Security Workshop",
        "publication": "Proceedings of the ACM Conference on Computer and Communications Security",
        "citied_by": "1",
        "cover_date": "2022-11-07",
        "Abstract": "Clouds and massive-scale computing infrastructures are starting to dominate computing and will likely continue to do so for the foreseeable future. Major cloud operators are now comprising millions of cores hosting substantial fractions of corporate and government IT infrastructure. CCSW is the world's premier forum bringing together researchers and practitioners in all security aspects of cloud-centric and outsourced computing, including: ·Side channel attacks ·Cryptographic protocols for cloud security ·Secure cloud resource virtualization mechanisms ·Secure data management outsourcing (e.g., database as a service) ·Privacy and integrity mechanisms for outsourcing ·Foundations of cloud-centric threat models ·Secure computation outsourcing ·Remote attestation mechanisms in clouds ·Sandboxing and VM-based enforcements ·Trust and policy management in clouds ·Secure identity management mechanisms ·Cloud-aware web service security paradigms and mechanisms ·Cloud-centric regulatory compliance issues and mechanisms ·Business and security risk models and clouds ·Cost and usability models and their interaction with security in clouds ·Scalability of security in global-size clouds ·Binary analysis of software for remote attestation and cloud protection ·Network security (DOS, IDS etc.) mechanisms for cloud contexts ·Security for emerging cloud programming models ·Energy/cost/efficiency of security in clouds ·mOpen hardware for cloud ·Machine learning for cloud protection CCSW especially encourages novel paradigms and controversial ideas that are not on the above list. The workshop has historically acted as a fertile ground for creative debate and interaction in security-sensitive areas of computing impacted by clouds. This year marked the 13th anniversary of CCSW. In the past decade, CCSW has had a significant impact in our research community.",
        "DOI": "10.1145/3548606.3563821",
        "paper_author": "Van Dijk M.",
        "affiliation_name": "Centrum Wiskunde &amp; Informatica",
        "affiliation_city": "Amsterdam",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60011575",
        "affiliation_state": "Noord-Holland"
    },
    {
        "paper_title": "StrongBox: A GPU TEE on Arm Endpoints",
        "publication": "Proceedings of the ACM Conference on Computer and Communications Security",
        "citied_by": "21",
        "cover_date": "2022-11-07",
        "Abstract": "A wide range of Arm endpoints leverage integrated and discrete GPUs to accelerate computation such as image processing and numerical processing applications. However, in spite of these important use cases, Arm GPU security has yet to be scrutinized by the community. By exploiting vulnerabilities in the kernel, attackers can directly access sensitive data used during GPU computing, such as personally-identifiable image data in computer vision tasks. Existing work has used Trusted Execution Environments (TEEs) to address GPU security concerns on Intel-based platforms, while there are numerous architectural differences that lead to novel technical challenges in deploying TEEs for Arm GPUs. In addition, extant Arm-based GPU defenses are intended for secure machine learning, and lack generality. There is a need for generalizable and efficient Arm-based GPU security mechanisms. To address these problems, we present StrongBox, the first GPU TEE for secured general computation on Arm endpoints. During confidential computation on Arm GPUs, StrongBox provides an isolated execution environment by ensuring exclusive access to the GPU. Our approach is based in part on a dynamic, fine-grained memory protection policy as Arm-based GPUs typically share a unified memory with the CPU, a stark contrast with Intel-based platforms. Furthermore, by characterizing GPU buffers as secure and non-secure, StrongBox reduces redundant security introspection operations to control access to sensitive data used by the GPU, ultimately reducing runtime overhead. Our design leverages the widely-deployed Arm TrustZone and generic Arm features, without hardware modification or architectural changes. We prototype StrongBox using an off-the-shelf Arm Mali GPU and perform an extensive evaluation. Our results show that StrongBox successfully ensures the GPU computing security with a low (4.70%-15.26%) overhead across several indicative benchmarks.",
        "DOI": "10.1145/3548606.3560627",
        "paper_author": "Deng Y.",
        "affiliation_name": "Southern University of Science and Technology",
        "affiliation_city": "Shenzhen",
        "affiliation_country": "China",
        "affiliation_id": "60105683",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Mapping the Thai Entrepreneurial Ecosystem: Constraints, Trends, Entrepreneurial Intention and Initiative",
        "publication": "AIP Conference Proceedings",
        "citied_by": "1",
        "cover_date": "2022-11-07",
        "Abstract": "Through qualitative interviews, I investigate the main constraints of the Thai entrepreneurial ecosystem and the evolving technology and investment trends that are reshaping Thai industries and the creation of new ventures, with a prominent role played by corporate venture capitalist and corporate innovation. It emerges that there are three main constraints in the Thai ecosystem: a lack of talents, affecting entrepreneurial readiness; a lack of funding; and a lack of policies supporting entrepreneurship. At the same time, novel investment trends in e-commerce, sustainability, borderless economy across Southeast Asia and, inclusion are emerging - mostly fueled by younger demographics- paired with two prominent technology adoptions: blockchain technologies, and artificial intelligence and machine learning. Further, I analyze the main drivers of entrepreneurial behavior and how Thai entrepreneurs solve problems, cognitively adapt, and adopt bricolage and other approaches to execute their entrepreneurial intention and succeed in a nascent and resource-scarce ecosystem. It appears that Thai entrepreneurs conjugate common patterns of entrepreneurial behavior, with some elements unique to the Thai context: higher adaptability due to the scarcity of resources; stronger emphasis on industry and business connections, as a way to establish and scale up the venture; and the combination of Thai traditional management approaches with internationally-aligned practices.",
        "DOI": "10.1063/5.0104465",
        "paper_author": "Borsano P.",
        "affiliation_name": "Chulalongkorn University",
        "affiliation_city": "Bangkok",
        "affiliation_country": "Thailand",
        "affiliation_id": "60028190",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "The 2022 report of the Lancet Countdown on health and climate change: health at the mercy of fossil fuels",
        "publication": "The Lancet",
        "citied_by": "726",
        "cover_date": "2022-11-05",
        "Abstract": "NA",
        "DOI": "10.1016/S0140-6736(22)01540-9",
        "paper_author": "Romanello M.",
        "affiliation_name": "University College London",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60022148",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "CEA-FJSP: Carbon emission-aware flexible job-shop scheduling based on deep reinforcement learning",
        "publication": "Frontiers in Environmental Science",
        "citied_by": "6",
        "cover_date": "2022-11-04",
        "Abstract": "Currently, excessive carbon emission is causing visible damage to the ecosystem and will lead to long-term environmental degradation in the future. The manufacturing industry is one of the main contributors to the carbon emission problem. Therefore, the reduction of carbon emissions should be considered at all levels of production activities. In this paper, the carbon emission as a parvenu indicator is considered parallelly with the nobleman indicator, makespan, in the flexible job-shop scheduling problem. Firstly, the carbon emission is modeled based on the energy consumption of machine operation and the coolant treatment during the production process. Then, a deep reinforcement learning-based scheduling model is proposed to handle the carbon emission-aware flexible job-shop scheduling problem. The proposed model treats scheduling as a Markov decision process, where the scheduling agent and the scheduling environment interact repeatedly via states, actions, and rewards. Next, a deep neural network is employed to parameterize the scheduling policy. Then, the proximal policy optimization algorithm is conducted to drive the deep neural network to learn the objective-oriented optimal mapping from the states to the actions. The experimental results verify that the proposed deep reinforcement learning-based scheduling model has prominent optimization and generalization abilities. Moreover, the proposed model presents a nonlinear optimization effect over the weight combinations.",
        "DOI": "10.3389/fenvs.2022.1059451",
        "paper_author": "Wang S.",
        "affiliation_name": "South China University of Technology",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60024542",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Industry response to strengthened regulations: amount and themes of flavoured electronic cigarette promotion by product vendors and manufacturers on Instagram",
        "publication": "Tobacco Control",
        "citied_by": "7",
        "cover_date": "2022-11-03",
        "Abstract": "Background Social media discussion tends to follow news about proposed or enacted government policies. Thus, digital discourse surveillance may be an effective and unobtrusive way of understanding industry and public response to policies and regulations, including in the domain of tobacco control. Recently, the US Food and Drug Administration restricted sales of flavoured cartridge and disposable vape products. Historically, the tobacco industry used modification of product characteristics, labelling or packaging to work around flavour restrictions. We aimed to characterise strategies used by nicotine product manufacturers and vendors to promote flavoured products on Instagram and to identify policy workaround tactics. Methods Keyword rules were used to collect flavoured electronic cigarette-related Instagram posts from CrowdTangle, from 1 January 2019 to 31 December 2021. Posts were coded for commercial content and promotional strategies using a combination of machine learning methods, keyword algorithms and human coding. Additional exploratory analyses were conducted to identify major discussion themes. Non-English posts were excluded from the analyses. Results Keyword filters captured 113 393 relevant posts from 391 unique accounts, with 46 076 posts referencing flavour promotion (40.6%) and 2124 (2%) posts mentioning alternatives to restricted flavoured products or strategies to evade flavour sales restrictions. Promotional messages featured non-characterising flavour references, 'off-brand' product substitutes, promotion of new flavoured product technologies, innovation, do-it-yourself appeals, global promotion, international delivery and encouraged flavoured product stockpiling. In addition, promotion of refillable devices, e-juice, tank systems and 'box mod' vaporizers was present. Conclusion Social media surveillance can enhance our understanding of public health needs and policy compliance, as well as inform strategies to prevent policy evasion. Examining evolving industry tactics to promote flavoured products in response to regulatory changes can help authorities and practitioners assess policy effectiveness and inform future design and implementation approaches.",
        "DOI": "10.1136/tc-2022-057490",
        "paper_author": "Kostygina G.",
        "affiliation_name": "National Opinion Research Center, Chicago",
        "affiliation_city": "Chicago",
        "affiliation_country": "United States",
        "affiliation_id": "60006805",
        "affiliation_state": "IL"
    },
    {
        "paper_title": "Using machine learning models to predict the duration of the recovery of COVID-19 patients hospitalized in Fangcang shelter hospital during the Omicron BA. 2.2 pandemic",
        "publication": "Frontiers in Medicine",
        "citied_by": "3",
        "cover_date": "2022-11-02",
        "Abstract": "Background: Factors that may influence the recovery of patients with confirmed SARS-CoV-2 infection hospitalized in the Fangcang shelter were explored, and machine learning models were constructed to predict the duration of recovery during the Omicron BA. 2.2 pandemic. Methods: A retrospective study was conducted at Hongqiao National Exhibition and Convention Center Fangcang shelter (Shanghai, China) from April 9, 2022 to April 25, 2022. The demographics, clinical data, inoculation history, and recovery information of the 13,162 enrolled participants were collected. A multivariable logistic regression model was used to identify independent factors associated with 7-day recovery and 14-day recovery. Machine learning algorithms (DT, SVM, RF, DT/AdaBoost, AdaBoost, SMOTEENN/DT, SMOTEENN/SVM, SMOTEENN/RF, SMOTEENN+DT/AdaBoost, and SMOTEENN/AdaBoost) were used to build models for predicting 7-day and 14-day recovery. Results: Of the 13,162 patients in the study, the median duration of recovery was 8 days (interquartile range IQR, 6–10 d), 41.31% recovered within 7 days, and 94.83% recovered within 14 days. Univariate analysis showed that the administrative region, age, cough medicine, comorbidities, diabetes, coronary artery disease (CAD), hypertension, number of comorbidities, CT value of the ORF gene, CT value of the N gene, ratio of ORF/IC, and ratio of N/IC were associated with a duration of recovery within 7 days. Age, gender, vaccination dose, cough medicine, comorbidities, diabetes, CAD, hypertension, number of comorbidities, CT value of the ORF gene, CT value of the N gene, ratio of ORF/IC, and ratio of N/IC were related to a duration of recovery within 14 days. In the multivariable analysis, the receipt of two doses of the vaccination vs. unvaccinated (OR = 1.118, 95% CI = 1.003–1.248; p = 0.045), receipt of three doses of the vaccination vs. unvaccinated (OR = 1.114, 95% CI = 1.004–1.236; p = 0.043), diabetes (OR = 0.383, 95% CI = 0.194–0.749; p = 0.005), CAD (OR = 0.107, 95% CI = 0.016–0.421; p = 0.005), hypertension (OR = 0.371, 95% CI = 0.202–0.674; p = 0.001), and ratio of N/IC (OR = 3.686, 95% CI = 2.939–4.629; p < 0.001) were significantly and independently associated with a duration of recovery within 7 days. Gender (OR = 0.736, 95% CI = 0.63–0.861; p < 0.001), age (30–70) (OR = 0.738, 95% CI = 0.594–0.911; p < 0.001), age (>70) (OR = 0.38, 95% CI = 0292–0.494; p < 0.001), receipt of three doses of the vaccination vs. unvaccinated (OR = 1.391, 95% CI = 1.12–1.719; p = 0.0033), cough medicine (OR = 1.509, 95% CI = 1.075–2.19; p = 0.023), and symptoms (OR = 1.619, 95% CI = 1.306–2.028; p < 0.001) were significantly and independently associated with a duration of recovery within 14 days. The SMOTEEN/RF algorithm performed best, with an accuracy of 90.32%, sensitivity of 92.22%, specificity of 88.31%, F1 score of 90.71%, and AUC of 89.75% for the 7-day recovery prediction; and an accuracy of 93.81%, sensitivity of 93.40%, specificity of 93.81%, F1 score of 93.42%, and AUC of 93.53% for the 14-day recovery prediction. Conclusion: Age and vaccination dose were factors robustly associated with accelerated recovery both on day 7 and day 14 from the onset of disease during the Omicron BA. 2.2 wave. The results suggest that the SMOTEEN/RF-based model could be used to predict the probability of 7-day and 14-day recovery from the Omicron variant of SARS-CoV-2 infection for COVID-19 prevention and control policy in other regions or countries. This may also help to generate external validation for the model.",
        "DOI": "10.3389/fmed.2022.1001801",
        "paper_author": "Xu Y.",
        "affiliation_name": "Chongqing Medical University",
        "affiliation_city": "Chongqing",
        "affiliation_country": "China",
        "affiliation_id": "60020870",
        "affiliation_state": "Chongqing"
    },
    {
        "paper_title": "Uncovering commercial activity in informal cities",
        "publication": "Royal Society Open Science",
        "citied_by": "0",
        "cover_date": "2022-11-02",
        "Abstract": "Knowledge of the spatial organization of economic activity within a city is a key to policy concerns. However, in developing cities with high levels of informality, this information is often unavailable. Recent progress in machine learning together with the availability of street imagery offers an affordable and easily automated solution. Here, we propose an algorithm that can detect what we call visible establishments using street view imagery. By using Medellín, Colombia as a case study, we illustrate how this approach can be used to uncover previously unseen economic activity. By applying spatial analysis to our dataset, we detect a polycentric structure with five distinct clusters located in both the established centre and peripheral areas. Comparing the density of visible establishments with that of registered firms, we infer that informal activity concentrates in poor but densely populated areas. Our findings highlight the large gap between what is captured in official data and the reality on the ground.",
        "DOI": "10.1098/rsos.211841",
        "paper_author": "Straulino D.",
        "affiliation_name": "University College London",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60022148",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Comparison of Machine Learning Methods and Conventional Logistic Regression for the Prediction of In-Hospital Mortality in Acute Biliary Pancreatitis",
        "publication": "Pancreas",
        "citied_by": "2",
        "cover_date": "2022-11-01",
        "Abstract": "Objectives: For population databases, multivariable regressions are established analytical standards. The utilization of machine learning (ML) in population databases is novel. We compared conventional statistical methods and ML for predicting mortality in biliary acute pancreatitis (biliary AP). Methods: Using the Nationwide Readmission Database (2010–2014), we identified patients (age ≥18 years) with admissions for biliary AP. These data were randomly divided into a training (70%) and test set (30%), stratified by the outcome of mortality. The accuracy of ML and logistic regression models in predicting mortality was compared using 3 different assessments. Results: Among 97,027 hospitalizations for biliary AP, mortality rate was 0.97% (n = 944). Predictors of mortality included severe AP, sepsis, increasing age, and nonperformance of cholecystectomy. Assessment metrics for predicting the outcome of mortality, the scaled Brier score (odds ratio [OR], 0.24; 95% confidence interval [CI], 0.16–0.33 vs 0.18; 95% CI, 0.09–0.27), F-measure (OR, 43.4; 95% CI, 38.3–48.6 vs 40.6; 95% CI, 35.7–45.5), and the area under the receiver operating characteristic (OR, 0.96; 95% CI, 0.94–0.97 vs 0.95; 95% CI, 0.94–0.96) were comparable between the ML and logistic regression models, respectively. Conclusions: For population databases, traditional multivariable analysis is noninferior to ML-based algorithms in predictive modeling of hospital outcomes for biliary AP.",
        "DOI": "10.1097/MPA.0000000000002208",
        "paper_author": "Luthra A.K.",
        "affiliation_name": "The Ohio State University Wexner Medical Center",
        "affiliation_city": "Columbus",
        "affiliation_country": "United States",
        "affiliation_id": "60015140",
        "affiliation_state": "OH"
    },
    {
        "paper_title": "Intelligent flow control for vortex-induced vibration of cylinder",
        "publication": "Shuidonglixue Yanjiu yu Jinzhan/Chinese Journal of Hydrodynamics Ser. A",
        "citied_by": "3",
        "cover_date": "2022-11-01",
        "Abstract": "In recent years, the rapidly developing machine learning has provided many novel and effective solutions for active flow control. In this study, the deep reinforcement learning (DRL) is applied to the classical problem, i.e., active control of vortex-induced vibration (VIV) of a circular cylinder, where a closed-loop control is established using a rotary actuator and wake velocity sensors providing feedback signals. The lattice Boltzmann method is applied to simulate the unsteady flow environment under active control, which the immersed boundary method is incorporated for fluid-solid interactions. In this intelligent control, the proximal policy optimization algorithm is adopted to explore the optimal control strategy, which two sets of independent neural networks are used for decision-making and performance-evaluation. Based on this intelligent flow control framework, VIV control is successfully realized, which the transverse displacement is suppressed by 89%. And also the proper orthogonal decomposition analysis is used to research the spatiotemporally varying flow fields. Present study provides detailed references for general solutions of closed-loop active flow control.",
        "DOI": "10.16076/j.cnki.cjhd.2022.06.002",
        "paper_author": "Ren F.",
        "affiliation_name": "Northwestern Polytechnical University",
        "affiliation_city": "Xi'an",
        "affiliation_country": "China",
        "affiliation_id": "60003977",
        "affiliation_state": "Shaanxi"
    },
    {
        "paper_title": "A Combined Neural Network Approach for the Prediction of Admission Rates Related to Respiratory Diseases",
        "publication": "Risks",
        "citied_by": "2",
        "cover_date": "2022-11-01",
        "Abstract": "In this paper, we investigated rates of admission to hospitals (or other health facilities) due to respiratory diseases in a United States working population and their dependence on a number of demographic and health insurance-related factors. We employed neural network (NN) modelling methodology, including a combined actuarial neural network (CANN) approach, and model admission numbers by embedding Poisson and negative binomial count regression models. The aim is to explore the gains in predictive power obtained with the use of NN-based models, when compared to commonly used count regression models, in the context of a large real data set in the area of healthcare insurance. We used nagging predictors, averaging over random calibrations of the NN-based models, to provide more accurate predictions based on a single run, and also employed a k-fold validation process to obtain reliable comparisons between different models. Bias regularisation methods were also developed, aiming at addressing bias issues that are common when fitting NN models. The results demonstrate that NN-based models, with a negative binomial distributional assumption, provide improved predictive performance. This can be important in real data applications, where accurate prediction can drive both personalised and policy-level interventions.",
        "DOI": "10.3390/risks10110217",
        "paper_author": "Jose A.",
        "affiliation_name": "Heriot-Watt University",
        "affiliation_city": "Edinburgh",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60019656",
        "affiliation_state": "Scotland"
    },
    {
        "paper_title": "Twitter-Based Sentiment Analysis and Topic Modeling of Social Media Posts Using Natural Language Processing, to Understand People’s Perspectives Regarding COVID-19 Booster Vaccine Shots in India: Crucial to Expanding Vaccination Coverage",
        "publication": "Vaccines",
        "citied_by": "14",
        "cover_date": "2022-11-01",
        "Abstract": "This study analyzed perceptions of Indians regarding COVID-19 booster dose vaccines using natural language processing techniques, particularly, sentiment analysis and topic modeling. We analyzed tweets generated by Indian citizens for this study. In late July 2022, the Indian government hastened the process of COVID-19 booster dose vaccinations. Understanding the emotions and concerns of the citizens regarding the health policy being implemented will assist the government, health policy officials, and policymakers implement the policy efficiently so that desired results can be achieved. Seventy-six thousand nine hundred seventy-nine tweets were used for this study. The sentiment analysis study revealed that out of those 76,979 tweets, more than half (n = 40,719 tweets (52.8%) had negative sentiments, 24,242 tweets (31.5%) had neutral sentiments, and 12,018 tweets (15.6%) had positive sentiments. Social media posts by Indians on the COVID-19 booster doses have focused on the feelings that younger people do not need vaccines and that vaccinations are unhealthy.",
        "DOI": "10.3390/vaccines10111929",
        "paper_author": "SV P.",
        "affiliation_name": "National Institute of Technology Tiruchirappalli",
        "affiliation_city": "Tiruchirappalli",
        "affiliation_country": "India",
        "affiliation_id": "60005630",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Predicting CO<inf>2</inf> Emission Footprint Using AI through Machine Learning",
        "publication": "Atmosphere",
        "citied_by": "34",
        "cover_date": "2022-11-01",
        "Abstract": "Adequate CO2 is essential for vegetation, but industrial chimneys and land, space and oceanic vehicles exert tons of excessive CO2 and are mostly responsible for the greenhouse effect, global warming and climate change. Due to COVID-19, CO2 emission was in 2020 at its lowest level compared to prior decades. However, it is unknown how long it will take to reduce CO2 emission to a tolerable point. Furthermore, it is also unknown to what extent it can increase or change in the future. Accurate forecasting of CO2 emissions has real significance for choosing the optimum ways of reducing CO2 emissions. Although some existing models have noticeable CO2 emission forecasting accuracy, the models implemented in this work have more efficacy in prediction due to incorporating COVID-19’s effect on CO2 emission. This paper implements four prediction models using SARIMA (SARIMAX) based on ARIMA. The four models are based on the time period of the surge of the COVID-19 pandemic. The main objective of this work is to compare these four models to suggest an effective model to predict the total CO2 emissions for the future. The study forecasts global total CO2 emission from 2022 to 2027 for near future prediction, 2022 to 2054 for future prediction and 2022 to 2072 for far future prediction. Among the various error measures, mean absolute percentage error (MAPE) is chosen for accuracy comparison. The calculation yields different accuracy for the four SARIMAX models. The MAPEs for the four methods are: pre-COV (MAPE: 0.32), start-COV (MAPE: 0.28), trans-COV (MAPE: 0.19), post-COV (MAPE: 0.09). The MAPE value is relatively low for post-COV (MAPE: 0.09). Hence, it can be inferred that post-COV are suitable models to forecast the global total CO2 emission for the future. The post-COV predictions for the global total CO2 emission for the years 2022 to 2027 are: 36,218.59, 36,733.69, 37,238.29, 37,260.88, 37,674.01 and 37,921.47 million tons (MT). This study successfully predicts CO2 emission either for the COVID-19 period or the post-COVID-19 normal periods. The Machine Learning (ML) method used in this study has shown good agreement with the IPCC model in predicting the past emissions, the current emissions due to COVID-19 and the emissions of the upcoming future. These prediction results can be an asset for the decision support system to develop a suitable policy for global CO2 emission reduction. For future research, a number of other external influence variables responsible for CO2 emission can be added for finer forecasts. This research is an original work in predicting COVID-19-affected CO2 emission using AI through the ML methodology.",
        "DOI": "10.3390/atmos13111871",
        "paper_author": "Meng Y.",
        "affiliation_name": "China University of Mining &amp; Technology, Beijing",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60108755",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Ethical Conundrums in the Application of Artificial Intelligence (AI) in Healthcare—A Scoping Review of Reviews",
        "publication": "Journal of Personalized Medicine",
        "citied_by": "52",
        "cover_date": "2022-11-01",
        "Abstract": "Background: With the availability of extensive health data, artificial intelligence has an inordinate capability to expedite medical explorations and revamp healthcare.Artificial intelligence is set to reform the practice of medicine soon. Despite the mammoth advantages of artificial intelligence in the medical field, there exists inconsistency in the ethical and legal framework for the application of AI in healthcare. Although research has been conducted by various medical disciplines investigating the ethical implications of artificial intelligence in the healthcare setting, the literature lacks a holistic approach. Objective: The purpose of this review is to ascertain the ethical concerns of AI applications in healthcare, to identify the knowledge gaps and provide recommendations for an ethical and legal framework. Methodology: Electronic databases Pub Med and Google Scholar were extensively searched based on the search strategy pertaining to the purpose of this review. Further screening of the included articles was done on the grounds of the inclusion and exclusion criteria. Results: The search yielded a total of 1238 articles, out of which 16 articles were identified to be eligible for this review. The selection was strictly based on the inclusion and exclusion criteria mentioned in the manuscript. Conclusion: Artificial intelligence (AI) is an exceedingly puissant technology, with the prospect of advancing medical practice in the years to come. Nevertheless, AI brings with it a colossally abundant number of ethical and legal problems associated with its application in healthcare. There are manifold stakeholders in the legal and ethical issues revolving around AI and medicine. Thus, a multifaceted approach involving policymakers, developers, healthcare providers and patients is crucial to arrive at a feasible solution for mitigating the legal and ethical problems pertaining to AI in healthcare.",
        "DOI": "10.3390/jpm12111914",
        "paper_author": "Prakash S.",
        "affiliation_name": "Panimalar Medical College Hospital &amp; Research Institute",
        "affiliation_city": "Chennai",
        "affiliation_country": "India",
        "affiliation_id": "123435842",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "A Comparative Study of Shallow Machine Learning Models and Deep Learning Models for Landslide Susceptibility Assessment Based on Imbalanced Data",
        "publication": "Forests",
        "citied_by": "17",
        "cover_date": "2022-11-01",
        "Abstract": "A landslide is a type of geological disaster that poses a threat to human lives and property. Landslide susceptibility assessment (LSA) is a crucial tool for landslide prevention. This paper’s primary objective is to compare the performances of conventional shallow machine learning methods and deep learning methods in LSA based on imbalanced data to evaluate the applicability of the two types of LSA models when class-weighted strategies are applied. In this article, logistic regression (LR), random forest (RF), deep fully connected neural network (DFCNN), and long short-term memory (LSTM) neural networks were employed for modeling in the Zigui-Badong area of the Three Gorges Reservoir area, China. Eighteen landslide influence factors were introduced to compare the performance of four models under a class balanced strategy versus a class imbalanced strategy. The Spearman rank correlation coefficient (SRCC) was applied for factor correlation analysis. The results reveal that the elevation and distance to rivers play a dominant role in LSA tasks. It was observed that DFCNN (AUC = 0.87, F1-score = 0.60) and LSTM (AUC = 0.89, F1-score = 0.61) significantly outperformed LR (AUC = 0.89, F1-score = 0.50) and RF (AUC = 0.88, F1-score = 0.50) under the class imbalanced strategy. The RF model achieved comparable outcomes (AUC = 0.90, F1-score = 0.61) to deep learning models under the class balanced strategy and ran at a faster training speed (up to 63 times faster than deep learning models). The LR model performance was inferior to that of the other three models under the balanced strategy. Meanwhile, the deep learning models and the shallow machine learning models showed significant differences in susceptibility spatial patterns. This paper’s findings will aid researchers in selecting appropriate LSA models. It is also valuable for land management policy making and disaster prevention and mitigation.",
        "DOI": "10.3390/f13111908",
        "paper_author": "Xu S.",
        "affiliation_name": "Huzhou University",
        "affiliation_city": "Huzhou",
        "affiliation_country": "China",
        "affiliation_id": "60073558",
        "affiliation_state": "Zhejiang"
    },
    {
        "paper_title": "Text mining tweets on e-cigarette risks and benefits using machine learning following a vaping related lung injury outbreak in the USA",
        "publication": "Healthcare Analytics",
        "citied_by": "3",
        "cover_date": "2022-11-01",
        "Abstract": "Electronic nicotine delivery systems (ENDS) (also known as ‘e-cigarettes’) can support smoking cessation, although the long-term health impacts are not yet known. In 2019, a cluster of lung injury cases in the USA emerged that were ostensibly associated with ENDS use. Subsequent investigations revealed a link with vitamin E acetate, an additive used in some ENDS liquid products containing tetrahydrocannabinol (THC). This became known as the EVALI (E-cigarette or Vaping product use Associated Lung Injury) outbreak. While few cases were reported in the UK, the EVALI outbreak intensified attention on ENDS in general worldwide. We aimed to describe and explore public commentary and discussion on Twitter immediately before, during and following the peak of the EVALI outbreak using text mining techniques. Specifically, topic modelling, operationalised using Latent Dirichlet Allocation (LDA) models, was used to discern discussion topics in 189,658 tweets about ENDS (collected April–December 2019). Individual tweets and Twitter users were assigned to their dominant topics and countries respectively to enable international comparisons. A 10-topic LDA model fit the data best. We organised the ten topics into three broad themes for the purposes of reporting: informal vaping discussion; vaping policy discussion and EVALI news; and vaping commerce. Following EVALI, there were signs that informal vaping discussion topics decreased while discussion topics about vaping policy and the relative health risks and benefits of ENDS increased, not limited to THC products. Though subsequently attributed to THC products, the EVALI outbreak disrupted online public discourses about ENDS generally, amplifying health and policy commentary. There was a relatively stronger presence of commercially oriented tweets among UK Twitter users compared to USA users.",
        "DOI": "10.1016/j.health.2022.100066",
        "paper_author": "Hassan L.",
        "affiliation_name": "Faculty of Biology, Medicine and Health",
        "affiliation_city": "Manchester",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60172345",
        "affiliation_state": "Greater Manchester"
    },
    {
        "paper_title": "Longitudinal analysis of the determinants of life expectancy and healthy life expectancy: A causal approach",
        "publication": "Healthcare Analytics",
        "citied_by": "6",
        "cover_date": "2022-11-01",
        "Abstract": "Understanding the determinants of health is essential for designing effective strategies to advance economic growth, reduce disease and disability, and enhance quality of life. We undertake a comprehensive outlook on public health by incorporating three metrics — life expectancy (LE), healthy life expectancy (HLE), and the discrepancy between the two. We investigate the effects of various health and socio-economic factors on these metrics and employ causal machine learning and statistical methods such as propensity score matching, X-learners, and causal forests to calculate treatment effects. An increase in basic water services and public health expenditure significantly increased average LE whereas high human immunodeficiency virus (HIV) prevalence rates and poverty rates reduced average LE. High gross national income (GNI) per capita and moderate body mass index (BMI) increased HLE whilst high HIV prevalence rates decreased HLE. High public health expenditure and high GNI per capita expand the gap between HLE and LE whereas high HIV prevalence rates and moderate BMI diminish this gap. Results suggest that policymakers should utilize governmental resources to improve public health infrastructure rather than provide fiscal incentives to encourage private healthcare infrastructure. Additionally, more emphasis should be placed on increasing educational levels of the general public by increasing educational expenditure and making educational institutions, public and private, more accountable.",
        "DOI": "10.1016/j.health.2022.100028",
        "paper_author": "Aanegola R.",
        "affiliation_name": "Yale School of Medicine",
        "affiliation_city": "New Haven",
        "affiliation_country": "United States",
        "affiliation_id": "60017994",
        "affiliation_state": "CT"
    },
    {
        "paper_title": "Improving Tuberculosis Treatment Adherence Support: The Case for Targeted Behavioral Interventions",
        "publication": "Manufacturing and Service Operations Management",
        "citied_by": "5",
        "cover_date": "2022-11-01",
        "Abstract": "Problem definition: Lack of patient adherence to treatment protocols is a main barrier to reducing the global disease burden of tuberculosis (TB). We study the operational design of a treatment adherence support (TAS) platform that requires patients to verify their treatment adherence on a daily basis. Academic/practical relevance: Experimental results on the effectiveness of TAS programs have been mixed; and rigorous research is needed on how to structure these motivational programs, particularly in resource-limited settings. Our analysis establishes that patient engagement can be increased by personal sponsor outreach and that patient behavior data can be used to identify at-risk patients for targeted outreach. Methodology: We partner with a TB TAS provider and use data from a completed randomized controlled trial. We use administrative variation in the timing of peer sponsor outreach to evaluate the impact of personal messages on subsequent patient verification behavior. We then develop a rolling-horizon machine learning (ML) framework to generate dynamic risk predictions for patients enrolled on the platform. Results: We find that, on average, sponsor outreach to patients increases the odds ratio of next-day treatment adherence verification by 35%. Furthermore, patients’ prior verification behavior can be used to accurately predict short-term (treatment adherence verification) and long-term (successful treatment completion) outcomes. These results allow the provider to target and implement behavioral interventions to at-risk patients. Managerial implications: Our results indicate that, compared with a benchmark policy, the TAS platform could reach the same number of at-risk patients with 6%–40% less capacity, or reach 2%–20% more at-risk patients with the same capacity, by using various ML-based prioritization policies that leverage patient engagement data. Personal sponsor outreach to all patients is likely to be very costly, so targeted TAS may substantially improve the cost-effectiveness of TAS programs.",
        "DOI": "10.1287/msom.2021.1046",
        "paper_author": "Boutilier J.J.",
        "affiliation_name": "UW-Madison College of Engineering",
        "affiliation_city": "Madison",
        "affiliation_country": "United States",
        "affiliation_id": "60153131",
        "affiliation_state": "WI"
    },
    {
        "paper_title": "Exploration of Urban Emission Mitigation Pathway under the Carbon Neutrality Target: A Case Study of Beijing, China",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "3",
        "cover_date": "2022-11-01",
        "Abstract": "Exploring the urban carbon neutrality pathway is crucial to the overall achievement of the net-zero emissions target in China. Therefore, taking Beijing as a case study, this paper firstly analyzes the CO2 emission drivers by combining the Stochastic Impacts by Regression on Population, Affluence, and Technology (STIRPAT) and partial least squares (PLS) methods. Subsequently, based on the optimized extreme learning machine (ELM) model, this paper projects the CO2 emissions of Beijing during 2021–2060 under different scenarios. The results show that controlling the total energy consumption and increasing the proportion of non-fossil energy consumption and electrification level should be the key measures to implement emission reduction in Beijing. Particularly, the proportion of non-fossil energy consumption and electrification level should be increased to 65% and 73%, respectively, in 2060. In addition, more stringent emission reduction policies need to be implemented to achieve the carbon neutrality target. Under the H−EPS scenario, Beijing’s CO2 emissions peaked in 2010 and will be reduced by a cumulative 109 MtCO2 during 2021–2060. Along with executing emission mitigation policies, Beijing should actively increase carbon sinks and develop carbon capture, utilization, and storage (CCUS) technology. Especially after 2040, the emission reduction produced by carbon sinks and CCUS technology should be no less than 20 MtCO2 per year.",
        "DOI": "10.3390/su142114016",
        "paper_author": "Jiang Z.",
        "affiliation_name": "University of Liverpool Management School",
        "affiliation_city": "Liverpool",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60116446",
        "affiliation_state": "Merseyside"
    },
    {
        "paper_title": "Forecasting Daily COVID-19 Case Counts Using Aggregate Mobility Statistics",
        "publication": "Data",
        "citied_by": "1",
        "cover_date": "2022-11-01",
        "Abstract": "The COVID-19 pandemic has impacted the whole world profoundly. For managing the pandemic, the ability to forecast daily COVID-19 case counts would bring considerable benefit to governments and policymakers. In this paper, we propose to leverage aggregate mobility statistics collected from Google’s Community Mobility Reports (CMRs) toward forecasting future COVID-19 case counts. We utilize features derived from the amount of daily activity in different location categories such as transit stations versus residential areas based on the time series in CMRs, as well as historical COVID-19 daily case and test counts, in forecasting future cases. Our method trains optimized regression models for different countries based on dynamic and data-driven selection of the feature set, regression type, and time period that best fit the country under consideration. The accuracy of our method is evaluated on 13 countries with diverse characteristics. Results show that our method’s forecasts are highly accurate when compared to the real COVID-19 case counts. Furthermore, visual analysis shows that the peaks, plateaus and general trends in case counts are also correctly predicted by our method.",
        "DOI": "10.3390/data7110166",
        "paper_author": "Boru B.",
        "affiliation_name": "Koç Üniversitesi",
        "affiliation_city": "Istanbul",
        "affiliation_country": "Türkiye",
        "affiliation_id": "60006369",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A New Hybrid Model for Mapping Spatial Accessibility to Healthcare Services Using Machine Learning Methods",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "13",
        "cover_date": "2022-11-01",
        "Abstract": "The unequal distribution of healthcare services is the main obstacle to achieving health equity and sustainable development goals. Spatial accessibility to healthcare services is an area of interest for health planners and policymakers. In this study, we focus on the spatial accessibility to four different types of healthcare services, including hospitals, pharmacies, clinics, and medical laboratories at Isfahan’s census blocks level, in a multivariate study. Regarding the nature of spatial accessibility, machine learning unsupervised clustering methods are utilized to analyze the spatial accessibility in the city. Initially, the study area was grouped into five clusters using three unsupervised clustering methods: K-Means, agglomerative, and bisecting K-Means. Then, the intersection of the results of the methods is considered to be conclusive evidence. Finally, using the conclusive evidence, a supervised clustering method, KNN, was applied to generate the map of the spatial accessibility situation in the study area. The findings of this study show that 47%, 22%, and 31% of city blocks in the study area have rich, medium, and poor spatial accessibility, respectively. Additionally, according to the study results, the healthcare services development is structured in a linear pattern along a historical avenue, Chaharbagh. Although the scope of this study was limited in terms of the supply and demand rates, this work gives more information and spatial insights for researchers, planners, and policymakers aiming to improve accessibility to healthcare and sustainable urban development. As a recommendation for further research work, it is suggested that other influencing factors, such as the demand and supply rates, should be integrated into the method.",
        "DOI": "10.3390/su142114106",
        "paper_author": "Khosravi Kazazi A.",
        "affiliation_name": "Shahid Rajaee Teacher Training University",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran",
        "affiliation_id": "60001784",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Review on Blue Economy in Shrimp Sector of Bangladesh",
        "publication": "Egyptian Journal of Aquatic Biology and Fisheries",
        "citied_by": "2",
        "cover_date": "2022-11-01",
        "Abstract": "Rapid global population growth depleting finite terrestrial resources which directed towards sustainable utilization of the oceans. Blue economy is a macro-economy concept where eco-friendly innovations are applied for sustainable production of marine goods and services for social solvency. In this paper, global blue economy definition, blue economy potentials, resources, blue economy in fisheries and aquaculture sector, present shrimp cultivation status and blue economy of shrimp cultivation from Bangladesh perspectives have been reviewed. Potential blue economy sectors in Bangladesh are mariculture, deep sea fishing, biotechnology, marine energy, submarine cable connections etc. Blue economy resources are biological resources, government agencies, human resources, ports-harbors, vessels and submarines, in addition to remote sensing facilities. To embrace blue economy, fisheries and aquaculture should include technologies, such as big data analysis, machine learning, artificial intelligence and precision aquaculture. Shrimp cultivation occurs in Khulna, Chittagong, Cox‟s Bazar, Bhola regions mostly by traditional method. Shrimp pond management, culture practices, harvesting, grading, production, supply and future potential has been addressed. Shrimp production is increasing steadily. Blue economy of shrimp culture involves innovative solutions to settle down unsustainability issues. A conceptual framework addressing major challengaes and sustainable interventions has also been reported. Recommendations are directed to consider government policy declaration, public awareness, coordination among state agencies, establishing national marine data hub, supporting research and development, and employing more manpower, as well as regional and international collaborations.",
        "DOI": "10.21608/EJABF.2022.281432",
        "paper_author": "Sarkar M.S.I.",
        "affiliation_name": "Bangabandhu Sheikh Mujibur Rahman Agricultural University",
        "affiliation_city": "Gazipur",
        "affiliation_country": "Bangladesh",
        "affiliation_id": "60007714",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Classifying Comments on Social Media Related to Living Kidney Donation: Machine Learning Training and Validation Study",
        "publication": "JMIR Medical Informatics",
        "citied_by": "3",
        "cover_date": "2022-11-01",
        "Abstract": "Background: Living kidney donation currently constitutes approximately a quarter of all kidney donations. There exist barriers that preclude prospective donors from donating, such as medical ineligibility and costs associated with donation. A better understanding of perceptions of and barriers to living donation could facilitate the development of effective policies, education opportunities, and outreach strategies and may lead to an increased number of living kidney donations. Prior research focused predominantly on perceptions and barriers among a small subset of individuals who had prior exposure to the donation process. The viewpoints of the general public have rarely been represented in prior research. Objective: The current study designed a web-scraping method and machine learning algorithms for collecting and classifying comments from a variety of online sources. The resultant data set was made available in the public domain to facilitate further investigation of this topic. Methods: We collected comments using Python-based web-scraping tools from the New York Times, YouTube, Twitter, and Reddit. We developed a set of guidelines for the creation of training data and manual classification of comments as either related to living organ donation or not. We then classified the remaining comments using deep learning. Results: A total of 203,219 unique comments were collected from the above sources. The deep neural network model had 84% accuracy in testing data. Further validation of predictions found an actual accuracy of 63%. The final database contained 11,027 comments classified as being related to living kidney donation. Conclusions: The current study lays the groundwork for more comprehensive analyses of perceptions, myths, and feelings about living kidney donation. Web-scraping and machine learning classifiers are effective methods to collect and examine opinions held by the general public on living kidney donation.",
        "DOI": "10.2196/37884",
        "paper_author": "Asghari M.",
        "affiliation_name": "J.B. Speed School of Engineering",
        "affiliation_city": "Louisville",
        "affiliation_country": "United States",
        "affiliation_id": "60146420",
        "affiliation_state": "KY"
    },
    {
        "paper_title": "Ultrafast inverse design of quantum dot optical spectra via a joint TD-DFT learning scheme and deep reinforcement learning",
        "publication": "AIP Advances",
        "citied_by": "3",
        "cover_date": "2022-11-01",
        "Abstract": "Here, we report a case study on inverse design of quantum dot optical spectra using a deep reinforcement learning algorithm for the desired target optical property of semiconductor CdxSeyTex-y quantum dots. Machine learning models were trained to predict the optical absorption and emission spectra by using the training dataset by time dependent density functional theory simulation. We show that the trained deep deterministic policy gradient inverse design agent can infer the molecular structure with an accuracy of less than 1 Å at a fixed computational time of milliseconds and up to 100-1000 times faster than the conventional heuristic particle swam optimization method. Most of the effective inverse design problems based on the surrogate machine learning and reinforcement learning model have been focused on the field of nano-photonics. Few attempts have been made in the field of quantum optical system in a similar manner. For the first time, our results, to our knowledge, provide concrete evidence that for computationally challenging tasks, a well-trained deep reinforcement learning agent can replace the existing quantum simulation and heuristics optimization tool, enabling fast and scalable simulations of the optical property of nanometer sized semiconductor quantum dots.",
        "DOI": "10.1063/5.0127546",
        "paper_author": "Yoshida H.",
        "affiliation_name": "The University of Electro-Communications",
        "affiliation_city": "Chofu",
        "affiliation_country": "Japan",
        "affiliation_id": "60032315",
        "affiliation_state": "Tokyo"
    },
    {
        "paper_title": "Goal-Conditioned Reinforcement Learning within a Human-Robot Disassembly Environment",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "4",
        "cover_date": "2022-11-01",
        "Abstract": "The introduction of collaborative robots in industrial environments reinforces the need to provide these robots with better cognition to accomplish their tasks while fostering worker safety without entering into safety shutdowns that reduce workflow and production times. This paper presents a novel strategy that combines the execution of contact-rich tasks, namely disassembly, with real-time collision avoidance through machine learning for safe human-robot interaction. Specifically, a goal-conditioned reinforcement learning approach is proposed, in which the removal direction of a peg, of varying friction, tolerance, and orientation, is subject to the location of a human collaborator with respect to a 7-degree-of-freedom manipulator at each time step. For this purpose, the suitability of three state-of-the-art actor-critic algorithms is evaluated, and results from simulation and real-world experiments are presented. In reality, the policy’s deployment is achieved through a new scalable multi-control framework that allows a direct transfer of the control policy to the robot and reduces response times. The results show the effectiveness, generalization, and transferability of the proposed approach with two collaborative robots against static and dynamic obstacles, leveraging the set of available solutions in non-monotonic tasks to avoid a potential collision with the human worker.",
        "DOI": "10.3390/app122211610",
        "paper_author": "Elguea-Aguinaco Í.",
        "affiliation_name": "Research &amp; Development Department",
        "affiliation_city": "Vitoria-Gasteiz",
        "affiliation_country": "Spain",
        "affiliation_id": "128964977",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Freshly-opened swidden mapping using Support Vector Machine（SVM）and spatial characteristics in Phongsaly Province，Laos",
        "publication": "National Remote Sensing Bulletin",
        "citied_by": "1",
        "cover_date": "2022-11-01",
        "Abstract": "Swidden agriculture is a widespread but controversial traditional land-use type in the tropics, especially in mountainous Laos with high percentage of forest cover. Driven by population growth, forestry policies, and climate change, swidden agriculture has been experiencing rapid evolution itself and drastic transformations into commercial plantations, such as rubber plantation. However, the remote sensing monitoring of tropical swidden agriculture has always been challenged, primarily because of the spatial and temporal dynamics in agricultural and forest cover, marginal feature compared with modern agriculture, and fragmentation and random distribution of swidden patches, hence with many unsettled issues and very limited information on its involved population, exact distribution and spatio-temporal dynamics. To explore the application potentials of machine learning algorithms in monitoring swidden agriculture, with two Landsat Operational Land Imager (OLI) images acquired in April, or the peak of the 2016 dry season, a support machine algorithm (SVM) was modified by masking out the information of construction land to improve the classification accuracy, or an overall accuracy of 95% and a Kappa coefficient of 0.81, followed by the examination of spatial (e.g., district-level) differences of freshly-opened swidden in Phongsaly Province, Laos, and their characteristics to local settlements and varied-level roads as well as topographical features. The results showed that: (1) Swidden agriculture remains an important land use type in Phongsaly because the newly-opened swidden was about 987.93 km2 (6.10% of the province) in 2016. More swidden patches were detected in the south and west parts of the province, with a fragmented distribution. (2) The area of newly-opened swiddens at district level ranged between 100—210 km2, with Samphanh District ranking the first (1/5) and Boonneua District the last (1/10). (3) Approximately 90% of newly-opened swiddens were concentrated within five km to residential points, particularly within four km. Similarly, these swiddens exhibited a distance decay pattern along the minor roads, tracks and major roads, in particular within a distance of five km of minor roads. (4) The newly-opened swiddens were mainly distributed in low mountain area (500—1000 m) with slope gradients of 15°—25° and aspects of southeast, showing slight variations among the districts of Phongsaly Province. This study provides reference for exploring machine learning algorithms in remote sensing monitoring of swidden agriculture in transition in the tropics.",
        "DOI": "10.11834/jrs.20211113",
        "paper_author": "Li P.",
        "affiliation_name": "Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60031150",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Marked Impacts of Pollution Mitigation on Crop Yields in China",
        "publication": "Earth's Future",
        "citied_by": "22",
        "cover_date": "2022-11-01",
        "Abstract": "Plant growth and crop harvest are impacted by both climate change and air pollution. However, their relative importance in crop yields remains elusive, especially in heavily polluted regions. Here we develop crop yield prediction models, based on a large volume of historical crop data, as well as climate and pollution records in China since 1980. A long-term surface ozone concentration data set is developed from a machine-learning model and various observations. An assessment of four climate and pollution factors reveals the critical role of particulate and ozone pollution in regulating interannual variations of crop yields in China. During 2010–2018, we find that the particulate pollution mitigation outweighs the negative impacts of concurrent climate change, resulting in 0.5%–1.9% net yield increases nationwide, despite of the ozone increases in the North China Plain. Looking to the future, the impacts of climate change, particularly from surface temperature increase, will dominate over pollution factors and profoundly reduce future maize and rice yields by 0.6 to 2.8% 10 yr−1 by 2050. Our findings call for attention on the threat to future global food security from the absence of pollution mitigation and the persistence of global warming.",
        "DOI": "10.1029/2022EF002936",
        "paper_author": "He L.",
        "affiliation_name": "Division of Geological and Planetary Sciences",
        "affiliation_city": "Pasadena",
        "affiliation_country": "United States",
        "affiliation_id": "60086312",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Hourly and Daily PM<inf>2.5</inf> Estimations Using MERRA-2: A Machine Learning Approach",
        "publication": "Earth and Space Science",
        "citied_by": "14",
        "cover_date": "2022-11-01",
        "Abstract": "Health and environmental hazards related to high pollution concentrations have become a serious issue from public policy perspectives and human health. Using Machine Learning (ML) approach, this research aims to improve the estimation of grid-wise PM2.5, a criteria pollutant, by reducing systematic bias from speciation provided by MERRA-from the Modern-Era Retrospective analysis for Research and Applications version 2 (MERRA-2). The ML model was trained using various meteorological parameters and aerosol species simulated by MERRA-2 and ground measurements from Environmental Protection Agency (EPA) air quality system stations. The ML approach significantly improved performance and reduced mean bias in the 0–10 μg m−3 range. We also used the Random Forest ML model for each EPA region using 1 year of collocated data sets. The resulting ML models for each EPA region were validated, and the aggregate data set has a Spearman Rank correlation (SR) of 0.73 (RMSE = 4.8 μg m−3) and 0.69 (RMSE = 5.8 μg m−3) for training and testing, respectively. The SR (and RMSE in μg m−3) increased to 0.81 (3.9), 0.89 (1.6), and 0.90 (1.1) for daily, monthly, and yearly averages, respectively. The results from the initial implementation of the ML model for the global region are encouraging. Still, they require more research and development to overcome challenges associated with data gaps in many parts of the world.",
        "DOI": "10.1029/2022EA002375",
        "paper_author": "Sayeed A.",
        "affiliation_name": "Huntsville Program Office",
        "affiliation_city": "Huntsville",
        "affiliation_country": "United States",
        "affiliation_id": "60021667",
        "affiliation_state": "AL"
    },
    {
        "paper_title": "Remote Sensing Extraction of Agricultural Land in Shandong Province, China, from 2016 to 2020 Based on Google Earth Engine",
        "publication": "Remote Sensing",
        "citied_by": "4",
        "cover_date": "2022-11-01",
        "Abstract": "Timely and effective access to agricultural land-change information is of great significance for the government when formulating agricultural policies. Due to the vast area of Shandong Province, the current research on agricultural land use in Shandong Province is very limited. The classification accuracy of the current classification methods also needs to be improved. In this paper, with the support of the Google Earth Engine (GEE) platform and based on Landsat 8 time series image data, a multiple machine learning algorithm was used to obtain the spatial variation distribution information of agricultural land in Shandong Province from 2016 to 2020. Firstly, a high-quality cloud-free synthetic Landsat 8 image dataset for Shandong Province from 2016 to 2020 was obtained using GEE. Secondly, the thematic index series was calculated to obtain the phenological characteristics of agricultural land, and the time periods with significant differences in terms of water, agricultural land, artificial surface, woodland and bare land were selected for classification. Feature information, such as texture features, spectral features and terrain features, was constructed, and the random forest method was used to select and optimize the features. Thirdly, the random forest, gradient boosting tree, decision tree and ensemble learning algorithms were used for classification, and the accuracy of the four classifiers was compared. The information on agricultural land changes was extracted and the causes were analyzed. The results show the following: (1) the multi-spatial index time series method is more accurate than the single thematic index time series when obtaining phenological characteristics; (2) the ensemble learning method is more accurate than the single classifier. The overall classification accuracy of the five agricultural land-extraction results in Shandong Province obtained by the ensemble learning method was above 0.9; (3) the annual decrease in agricultural land in Shandong Province from 2016 to 2020 was related to the increase in artificial land-surface area and urbanization rate.",
        "DOI": "10.3390/rs14225672",
        "paper_author": "Liu H.",
        "affiliation_name": "Capital Normal University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60020256",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Efficient Asynchronous Federated Learning for AUV Swarm",
        "publication": "Sensors",
        "citied_by": "2",
        "cover_date": "2022-11-01",
        "Abstract": "The development of automatic underwater vehicles (AUVs) has brought about unprecedented profits and opportunities. In order to discover the hidden valuable data detected by an AUV swarm, it is necessary to aggregate the data detected by AUV swarm to generate a powerful machine learning model. Traditional centralized machine learning generates a large number of data exchanges and faces problems of enormous training data, large-scale models, and communication. In underwater environments, radio waves are strongly absorbed, and acoustic communication is the only feasible technology. Unlike electromagnetic wave communication on land, the bandwidth of underwater acoustic communication is extremely limited, with the transmission rate being only (Formula presented.) of the electromagnetic wave. Therefore, traditional centralized machine learning cannot support underwater AUV swarm training. In recent years, federated learning could only interact with model parameters without interacting with data, which greatly reduced communication costs. Therefore, this paper introduces federated learning into the collaboration of an AUV swarm. In order to further reduce the constraints of underwater scarce communication resources on federated learning and alleviate the straggler effect, in this work, we designed an asynchronous federated learning method. Finally, we constructed the optimization problem of minimizing the weighted sum of delay and energy consumption, relying on jointly optimizing the AUV CPU frequency and signal transmission power. In order to solve this complex optimization problem of high-dimensional non-convex time series accumulation, we transformed the problem into a Markov decision process (MDP) and use the proximal policy optimization 2 (PPO2) algorithm to solve this problem. The simulation results demonstrate the effectiveness and superiority of our method.",
        "DOI": "10.3390/s22228727",
        "paper_author": "Meng Z.",
        "affiliation_name": "Xidian University",
        "affiliation_city": "Xi'an",
        "affiliation_country": "China",
        "affiliation_id": "60025578",
        "affiliation_state": "Shaanxi"
    },
    {
        "paper_title": "A Comparative Study of Machine Learning Algorithms for Industry-Specific Freight Generation Model",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "0",
        "cover_date": "2022-11-01",
        "Abstract": "According to Bureau of Transportation Statistics, the U.S. transportation system handled 14,329 million ton-miles of freight per day in 2020. Understanding the generation of these freight shipments is crucial for transportation researchers, planners, and policymakers to design and plan for a more efficient and connected freight transportation system. Traditionally, the freight generation modeling has been based on Ordinary Least Square (OLS) regression, although more advanced Machine Learning (ML) algorithms have been evaluated and proven to have excellent performance in various transportation applications in recent years. Furthermore, one modeling approach applied for one industry might not always be applicable for another as their freight generation logics can be quite different. The objective of this study is to apply and evaluate alternative ML algorithms in the estimation of freight generation for each of 45 industry types. Seven alternative ML algorithms, along with the base OLS regression, were evaluated and compared. In addition, the study considered different combinations of variables in both the original and logarithmic form as well as hyperparameters of those ML algorithms in the model selection for each industry type. The results showed statistically significant improvements in the root mean square error reduction by the alternative ML algorithms over the OLS for over 80% of cases. The study suggests utilizing the alternative ML algorithms can reduce the root mean square error by about 30%, depending on industry types.",
        "DOI": "10.3390/su142215367",
        "paper_author": "Lim H.",
        "affiliation_name": "Oak Ridge National Laboratory",
        "affiliation_city": "Oak Ridge",
        "affiliation_country": "United States",
        "affiliation_id": "60024266",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Two-Phase Stratified Random Forest for Paddy Growth Phase Classification: A Case of Imbalanced Data",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "2",
        "cover_date": "2022-11-01",
        "Abstract": "The United Nations Sustainable Development Goals (SDGs) have had a considerable impact on Indonesia’s national development policies for the period 2015 to 2030. The agricultural industry is one of the world’s most important industries, and it is critical to the achievement of the SDGs. The second major aspect of the SDGs, i.e., zero hunger, addresses food security (SDG 2). To measure the status of food security, accurate statistics on paddy production must be accessible. Paddy phenological classification is a way to determine a food plant’s growth phase. Imbalanced data are a common occurrence in agricultural data, and machine learning is frequently utilized as a technique for classification issues. The current trend in agriculture is to use remote sensing data to classify crops. This paper proposes a new approach—one that uses two phases in the bootstrap stage of the random forest method—called a two-phase stratified random forest (TPSRF). The simulation scenario shows that the proposed TPSRF outperforms CART, SVM, and RF. Furthermore, in its application to paddy growth phase data for 2019 in Lamongan Regency, East Java, Indonesia, the proposed TPSRF showed higher overall accuracy (OA) than the compared methods.",
        "DOI": "10.3390/su142215252",
        "paper_author": "Suryono H.",
        "affiliation_name": "Institut Teknologi Sepuluh Nopember",
        "affiliation_city": "Surabaya",
        "affiliation_country": "Indonesia",
        "affiliation_id": "60070707",
        "affiliation_state": "East Java"
    },
    {
        "paper_title": "PREVIDE: A Qualitative Study to Develop a Decision-Making Framework (PREVention decIDE) for Noncommunicable Disease Prevention in Healthcare Organisations",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "0",
        "cover_date": "2022-11-01",
        "Abstract": "Noncommunicable diseases (NCDs), including obesity, remain a significant global public health challenge. Prevention and public health innovation are needed to effectively address NCDs; however, understanding of how healthcare organisations make prevention decisions is immature. This study aimed to (1) explore how healthcare organisations make decisions for NCD prevention in Queensland, Australia (2) develop a contemporary decision-making framework to guide NCD prevention in healthcare organisations. Cross-sectional and qualitative design, comprising individual semi-structured interviews. Participants (n = 14) were recruited from two organisations: the state public health care system (CareQ) and health promotion/disease prevention agency (PrevQ). Participants held executive, director/manager or project/clinical lead roles. Data were analysed in two phases (1) automated content analysis using machine learning (Leximancer v4.5) (2) researcher-led interpretation of the text analytics. Final themes were consolidated into a proposed decision-making framework (PREVIDE, PREvention decIDE) for NCD prevention in healthcare organisations. Decision-making was driven by four themes: Data, Evidence, Ethics and Health, i.e., data, its quality and the story it tells; traditional and non-traditional sources of evidence; ethical grounding in fairness and equity; and long-term value generated across multiple determinants of health. The strength of evidence was directly proportional to confidence in the ethics of a decision. PREVIDE can be adapted by public health practitioners and policymakers to guide real-world policy, practice and investment decisions for obesity prevention and with further validation, other NCDs and priority settings (e.g., healthcare).",
        "DOI": "10.3390/ijerph192215285",
        "paper_author": "Canfell O.J.",
        "affiliation_name": "The University of Queensland Business School",
        "affiliation_city": "Brisbane",
        "affiliation_country": "Australia",
        "affiliation_id": "60160279",
        "affiliation_state": "QLD"
    },
    {
        "paper_title": "Decentralized Patient-Centric Report and Medical Image Management System Based on Blockchain Technology and the Inter-Planetary File System",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "11",
        "cover_date": "2022-11-01",
        "Abstract": "Several academicians have been actively contributing to establishing a practical solution to storing and distributing medical images and test reports in the research domain of health care in recent years. Current procedures mainly rely on cloud-assisted centralized data centers, which raise maintenance expenditure, necessitate a large amount of storage space, and raise privacy concerns when exchanging data across a network. As a result, it is critically essential to provide a framework that allows for the efficient exchange and storage of large amounts of medical data in a secure setting. In this research, we describe a unique proof-of-concept architecture for a distributed patient-centric test report and image management (PCRIM) system that aims to facilitate patient privacy and control without the need for a centralized infrastructure. We used an Ethereum blockchain and a distributed file system technology called the Inter-Planetary File System in this system (IPFS). Then, to secure a distributed and trustworthy access control policy, we designed an Ethereum smart contract termed the patient-centric access control protocol. The IPFS allows for the decentralized storage of medical metadata, such as images, with worldwide accessibility. We demonstrate how the PCRIM system design enables hospitals, patients, and image requestors to obtain patient-centric data in a distributed and secure manner. Finally, we tested the proposed framework in the Windows environment by deploying a smart contract prototype on an Ethereum TESTNET blockchain. The findings of the study indicate that the proposed strategy is both efficient and practicable.",
        "DOI": "10.3390/ijerph192214641",
        "paper_author": "Mohsan S.A.H.",
        "affiliation_name": "Ocean College Zhejiang University",
        "affiliation_city": "Zhoushan",
        "affiliation_country": "China",
        "affiliation_id": "60117839",
        "affiliation_state": "Zhejiang"
    },
    {
        "paper_title": "Machine Learning and Food Security: Insights for Agricultural Spatial Planning in the Context of Agriculture 4.0",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "15",
        "cover_date": "2022-11-01",
        "Abstract": "Climate change and global warming interconnected with the new contexts created by the COVID-19 pandemic and the Russia-Ukraine conflict have brought serious challenges to national and international organizations, especially in terms of food security and agricultural planning. These circumstances are of particular concern due to the impacts on food chains and the resulting disruptions in supply and price changes. The digital agricultural transition in Era 4.0 can play a decisive role in dealing with these new agendas, where drones and sensors, big data, the internet of things and machine learning all have their inputs. In this context, the main objective of this study is to highlight insights from the literature on the relationships between machine learning and food security and their contributions to agricultural planning in the context of Agriculture 4.0. For this, a systematic review was carried out based on information from text and bibliographic data. The proposed objectives and methodologies represent an innovative approach, namely, the consideration of bibliometric evaluation as a support for a focused literature review related to the topics addressed here. The results of this research show the importance of the digital transition in agriculture to support better policy and planning design and address imbalances in food chains and agricultural markets. New technologies in Era 4.0 and their application through Climate-Smart Agriculture approaches are crucial for sustainable businesses (economically, socially and environmentally) and the food supply. Furthermore, for the interrelationships between machine learning and food security, the literature highlights the relevance of platforms and methods, such as, for example, Google Earth Engine and Random Forest. These and other approaches have been considered to predict crop yield (wheat, barley, rice, maize and soybean), abiotic stress, field biomass and crop mapping with high accuracy (R2 ≈ 0.99 and RMSE ≈ 1%).",
        "DOI": "10.3390/app122211828",
        "paper_author": "Martinho V.J.P.D.",
        "affiliation_name": "Instituto Politécnico de Viseu",
        "affiliation_city": "Viseu",
        "affiliation_country": "Portugal",
        "affiliation_id": "60013042",
        "affiliation_state": "Viseu"
    },
    {
        "paper_title": "Higher Expression of Annexin A2 in Metastatic Bladder Urothelial Carcinoma Promotes Migration and Invasion",
        "publication": "Cancers",
        "citied_by": "7",
        "cover_date": "2022-11-01",
        "Abstract": "In this study, we aim to evaluate the significance of AnxA2 in BLCA and establish its metastatic role in bladder cancer cells. Analysis of TCGA data showed that AnxA2 mRNA expression was significantly higher in BLCA tumors than in normal bladder tissues. High mRNA expression of AnxA2 in BLCA was significantly associated with high pathological grades and stages, non-papillary tumor histology, and poor overall survival (OS), progression-free survival (PFS), and diseases specific survival (DSS). Similarly, we found that AnxA2 expression was higher in bladder cancer cells derived from high-grade metastatic carcinoma than in cells derived from low-grade urothelial carcinoma. AnxA2 expression significantly mobilized to the surface of highly metastatic bladder cancer cells compared to cells derived from low-grade tumors and associated with high plasmin generation and AnxA2 secretion. In addition, the downregulation of AnxA2 cells significantly inhibited the proliferation, migration, and invasion in bladder cancer along with the reduction in proangiogenic factors and cytokines such as PDGF-BB, ANGPT1, ANGPT2, Tie-2, bFGF, GRO, IL-6, IL-8, and MMP-9. These findings suggest that AnxA2 could be a promising biomarker and therapeutic target for high-grade BLCA.",
        "DOI": "10.3390/cancers14225664",
        "paper_author": "Guo C.",
        "affiliation_name": "Texas College of Osteopathic Medicine",
        "affiliation_city": "Fort Worth",
        "affiliation_country": "United States",
        "affiliation_id": "60011470",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "The Privacy-Fairness-Accuracy Frontier: A Computational Law &amp;Economics Toolkit for Making Algorithmic Tradeoffs",
        "publication": "CSLAW 2022 - Proceedings of the 2022 Symposium on Computer Science and Law",
        "citied_by": "2",
        "cover_date": "2022-11-01",
        "Abstract": "Both law and computer science are concerned with developing frameworks for protecting privacy and ensuring fairness. Both fields often consider these two values separately and develop legal doctrines and machine learning metrics in isolation from one another. Yet, privacy and fairness values can conflict, especially when considered alongside the accuracy of an algorithm. The computer science literature often treats this problem as an \"impossibility theorem\"-we can have privacy or fairness but not both. Legal doctrine is similarly constrained by a focus on the inputs to a decision-did the decisionmaker intend to use information about protected attributes. Despite these challenges, there is a way forward. The law has integrated economic frameworks to consider tradeoffs in other domains, and a similar approach can clarify policymakers' thinking around balancing accuracy, privacy, and fairnesss. This piece illustrates this idea by using a law & economics lens to formalize the notion of a Privacy-Fairness-Accuracy frontier, and demonstrating this framework on a consumer lending dataset. An open-source Python software library and GUI will be made available.",
        "DOI": "10.1145/3511265.3550437",
        "paper_author": "Kesari A.",
        "affiliation_name": "New York University",
        "affiliation_city": "New York",
        "affiliation_country": "United States",
        "affiliation_id": "60021784",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Beyond Ads: Sequential Decision-Making Algorithms in Law and Public Policy",
        "publication": "CSLAW 2022 - Proceedings of the 2022 Symposium on Computer Science and Law",
        "citied_by": "4",
        "cover_date": "2022-11-01",
        "Abstract": "We explore the promises and challenges of employing sequential decision-making algorithms-such as bandits, reinforcement learning, and active learning-in law and public policy. While such algorithms have well-characterized performance in the private sector (e.g., online advertising), the tendency to naively apply algorithms motivated by one domain, often online advertisements, can be called the \"advertisement fallacy.\"Our main thesis is that law and public policy pose distinct methodological challenges that the machine learning community has not yet addressed. Machine learning will need to address these methodological problems to move \"beyond ads.\"Public law, for instance, can pose multiple objectives, necessitate batched and delayed feedback, and require systems to learn rational, causal decision-making policies, each of which presents novel questions at the research frontier. We discuss a wide range of potential applications of sequential decision-making algorithms in regulation and governance, including public health, environmental protection, tax administration, occupational safety, and benefits adjudication. We use these examples to highlight research needed to render sequential decision making policy-compliant, adaptable, and effective in the public sector. We also note the potential risks of such deployments and describe how sequential decision systems can also facilitate the discovery of harms. We hope our work inspires more investigation of sequential decision making in law and public policy, which provide unique challenges for machine learning researchers with potential for significant social benefit.",
        "DOI": "10.1145/3511265.3550439",
        "paper_author": "Henderson P.",
        "affiliation_name": "Stanford University",
        "affiliation_city": "Stanford",
        "affiliation_country": "United States",
        "affiliation_id": "60012708",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "CSLAW 2022 - Proceedings of the 2022 Symposium on Computer Science and Law",
        "publication": "CSLAW 2022 - Proceedings of the 2022 Symposium on Computer Science and Law",
        "citied_by": "0",
        "cover_date": "2022-11-01",
        "Abstract": "The proceedings contain 17 papers. The topics discussed include: non-determinism and the lawlessness of machine learning code; using zero-knowledge to reconcile law enforcement secrecy and fair trial rights in criminal cases; can the government compel decryption? don’t trust — verify; formalizing human ingenuity: a quantitative framework for copyright law’s substantial similarity; bridging the computer science - law divide: recommendations from the front lines; multi-regulation computing: examining the legal and policy questions that arise from secure multiparty computation; classification protocols with minimal disclosure; the privacy-fairness-accuracy frontier: a computational law & economics toolkit for making algorithmic tradeoffs; and beyond ads: sequential decision-making algorithms in law and public policy.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Lenvatinib as First-Line Treatment for Unresectable Hepatocellular Carcinoma: A Systematic Review and Meta-Analysis",
        "publication": "Cancers",
        "citied_by": "15",
        "cover_date": "2022-11-01",
        "Abstract": "Lenvatinib was approved in 2018 as a first-line treatment for patients with unresectable hepatocellular carcinoma (HCC). This systematic review and meta-analysis aimed to provide the most updated evidence about the efficacy and safety of lenvatinib as a first-line treatment for unresectable HCC. An electronic search of the PubMed database, Web of Science, Embase, and Cochrane Library was undertaken to identify all relevant studies up to May 2022. The pooled effect sizes were calculated based on the random-effects model. One phase III randomized controlled trial and 23 retrospective studies of 2438 patients were eligible for analysis. For patients treated with lenvatinib as first-line treatment, the pooled median overall survival (OS), median progression-free survival (PFS), 1-year OS rate, 1-year PFS rate, objective response rate (ORR), and disease control rate (DCR) were 11.36 months, 6.68 months, 56.0%, 27.0%, 36.0% and 75.0%, respectively. Lenvatinib showed a significantly superior efficacy compared with sorafenib (HR for OS, 0.85 and HR for PFS, 0.72; OR for ORR, 4.25 and OR for DCR, 2.23). The current study demonstrates that lenvatinib can provide better tumor responses and survival benefits than sorafenib as a first-line treatment for unresectable HCC, with a comparable incidence of adverse events.",
        "DOI": "10.3390/cancers14225525",
        "paper_author": "Wang S.",
        "affiliation_name": "The First Affiliated Hospital of Xiamen University",
        "affiliation_city": "Xiamen",
        "affiliation_country": "China",
        "affiliation_id": "60122372",
        "affiliation_state": "Fujian"
    },
    {
        "paper_title": "A Systematic Review on Modeling Methods and Influential Factors for Mapping Dengue-Related Risk in Urban Settings",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "15",
        "cover_date": "2022-11-01",
        "Abstract": "Dengue fever is an acute mosquito-borne disease that mostly spreads within urban or semi-urban areas in warm climate zones. The dengue-related risk map is one of the most practical tools for executing effective control policies, breaking the transmission chain, and preventing disease outbreaks. Mapping risk at a small scale, such as at an urban level, can demonstrate the spatial heterogeneities in complicated built environments. This review aims to summarize state-of-the-art modeling methods and influential factors in mapping dengue fever risk in urban settings. Data were manually extracted from five major academic search databases following a set of querying and selection criteria, and a total of 28 studies were analyzed. Twenty of the selected papers investigated the spatial pattern of dengue risk by epidemic data, whereas the remaining eight papers developed an entomological risk map as a proxy for potential dengue burden in cities or agglomerated urban regions. The key findings included: (1) Big data sources and emerging data-mining techniques are innovatively employed for detecting hot spots of dengue-related burden in the urban context; (2) Bayesian approaches and machine learning algorithms have become more popular as spatial modeling tools for predicting the distribution of dengue incidence and mosquito presence; (3) Climatic and built environmental variables are the most common factors in making predictions, though the effects of these factors vary with the mosquito species; (4) Socio-economic data may be a better representation of the huge heterogeneity of risk or vulnerability spatial distribution on an urban scale. In conclusion, for spatially assessing dengue-related risk in an urban context, data availability and the purpose for mapping determine the analytical approaches and modeling methods used. To enhance the reliabilities of predictive models, sufficient data about dengue serotyping, socio-economic status, and spatial connectivity may be more important for mapping dengue-related risk in urban settings for future studies.",
        "DOI": "10.3390/ijerph192215265",
        "paper_author": "Yin S.",
        "affiliation_name": "The University of Hong Kong",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong",
        "affiliation_id": "60006541",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Regression discontinuity threshold optimization",
        "publication": "PLoS ONE",
        "citied_by": "0",
        "cover_date": "2022-11-01",
        "Abstract": "Treatments often come with thresholds, e.g. we are given statins if our cholesterol is above a certain threshold. But which statin administration threshold maximizes our quality of life adjusted years? More generally, which threshold would optimize the average expected outcome? Regression discontinuity approaches are used to measure the local average treatment effect (LATE) and more recently also the Marginal Threshold Treatment Effect (MTTE), which shows how marginal changes in the threshold can affect the LATE. We extend this idea to define the problem of optimizing a policy threshold, i.e. selecting a threshold that optimizes the cumulative effect of the treatment on the treated. We present an estimator of the optimal threshold based on a constrained optimization framework. We show how to use machine learning (Gaussian process regression) for non-linear estimation. We also extend the estimation to a conservative threshold that is unlikely to produce harm, and we show how to include policy cost constraints. We apply these results to estimate an optimal tip-maximizing threshold for tip suggestions in taxi cabs Haggag (2014).",
        "DOI": "10.1371/journal.pone.0276755",
        "paper_author": "Marinescu I.",
        "affiliation_name": "University of Pennsylvania",
        "affiliation_city": "Philadelphia",
        "affiliation_country": "United States",
        "affiliation_id": "60006297",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "Archetypes of agri-environmental potential: a multi-scale typology for spatial stratification and upscaling in Europe",
        "publication": "Environmental Research Letters",
        "citied_by": "10",
        "cover_date": "2022-11-01",
        "Abstract": "Developing spatially-targeted policies for farmland in the European Union (EU) requires synthesized, spatially-explicit knowledge of agricultural systems and their environmental conditions. Such synthesis needs to be flexible and scalable in a way that allows the generalization of European landscapes and their agricultural potential into spatial units that are informative at any given resolution and extent. In recent years, typologies of agricultural lands have been substantially improved, however, agriculturally relevant aspects have yet to be included. We here provide a spatial classification approach for identifying archetypal patterns of agri-environmental potential in Europe based on machine-learning clustering of 17 variables on bioclimatic conditions, soil characteristics and topographical parameters. We improve existing typologies by (a) including more recent biophysical data (e.g. agriculturally-important soil parameters), (b) employing a fully data-driven approach that reduces subjectivity in identifying archetypal patterns, and (c) providing a scalable approach suitable both for the entire European continent as well as smaller geographical extents. We demonstrate the utility and scalability of our typology by comparing the archetypes with independent data on cropland cover and field size at the European scale and in three regional case studies in Germany, Czechia and Spain. The resulting archetypes can be used to support spatial stratification, upscaling and designation of more spatially-targeted agricultural policies, such as those in the context of the EU’s Common Agricultural Policy post-2020.",
        "DOI": "10.1088/1748-9326/ac9cf5",
        "paper_author": "Beckmann M.",
        "affiliation_name": "Helmholtz Zentrum für Umweltforschung",
        "affiliation_city": "Leipzig",
        "affiliation_country": "Germany",
        "affiliation_id": "60018229",
        "affiliation_state": "Sachsen"
    },
    {
        "paper_title": "Harmful Drinking Phenotype in a Large Dutch Community Sample",
        "publication": "Alcohol and Alcoholism",
        "citied_by": "0",
        "cover_date": "2022-11-01",
        "Abstract": "Aims: Harmful drinking patterns are shaped by a broad complex interaction of factors, societal and individual, psychological and behavioral. Although previous studies have focused on a few variables at a time, the current study simultaneously examines a large number of variables in order to create a comprehensive view (i.e. phenotype) of harmful drinking, and to rank the main predictors of harmful and non-harmful drinking by order of importance. Methods: We surveyed a large sample of Dutch adults about their habitual drinking characteristics and attitudes, perceptions and motives for drinking. We fed 45 variables into a random forest machine learning model to identify predictors for (1) drinking within and in excess of Dutch guideline recommendations and (2) harmful and non-harmful drinking. Results: In both models, respondents' subjective perceptions of 'responsible drinking', both per occasion and per week, showed the strongest predictive potential for different drinking phenotypes. The next strongest factors were respondents' reason for drinking, motives for drinking and age. Other variables, such as drinking location, knowledge about alcohol-related health risks and consumption of different beverage types, were not strong predictors of drinking phenotypes. Conclusions: Although the direction of the relationship is unclear from the findings, they suggest that interventions and policy measures aimed at individuals and social norms around drinking may offer promise for reducing harmful drinking. Messaging and promotion of drinking guidelines should be tailored with this in mind.",
        "DOI": "10.1093/alcalc/agac041",
        "paper_author": "Hogenelst K.",
        "affiliation_name": "Nederlandse Organisatie voor toegepast natuurwetenschappelijk onderzoek- TNO",
        "affiliation_city": "The Hague",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60019984",
        "affiliation_state": "Zuid-Holland"
    },
    {
        "paper_title": "A double-deck deep reinforcement learning-based energy dispatch strategy for an integrated electricity and district heating system embedded with thermal inertial and operational flexibility",
        "publication": "Energy Reports",
        "citied_by": "9",
        "cover_date": "2022-11-01",
        "Abstract": "With the high penetration of wind power connected to the integrated electricity and district heating systems (IEDHSs), wind power curtailment still inevitably occurs in the traditional IEDHS dispatch. Focusing on the flexibilities of the IEDHS is considered to be a beneficial solution to further promote the integration of wind power. In the district heating network, the thermal inertia is utilized to improve such flexibility. Therefore, an IEDHS dispatch model considering the thermal inertia of district heating network and operational flexibility of generators is proposed in this paper. In addition, to avoid the tendency of traditional reinforcement learning (RL) to fall into local optimality when solving high-dimensional problems, a double-deck deep RL (D3RL) framework is proposed in this study. D3RL combines with a deep deterministic policy gradient (DDPG) agent in the upper level and a conventional optimization solver in the lower level to simplify the action and reward design. In the simulation, the proposed model considering the transmission time delay characteristics of the district heating network and the operational flexibility of generators is verified in four scheduling scenarios. Besides, the superiority of the proposed D3RL method is validated in a larger IEDHS. Numerical results show that the considered scheduling model can use the heat storage characteristics of heating pipelines, reduce operating costs, improve the operational flexibility and encourage wind power utilization. Compared with traditional RL, the proposed optimization method can improve its training speed and convergence performance.",
        "DOI": "10.1016/j.egyr.2022.11.028",
        "paper_author": "Zhang B.",
        "affiliation_name": "Aalborg University",
        "affiliation_city": "Aalborg",
        "affiliation_country": "Denmark",
        "affiliation_id": "60022134",
        "affiliation_state": "Nordjylland"
    },
    {
        "paper_title": "Assessing incentives to increase digital payment acceptance and usage: A machine learning approach",
        "publication": "PLoS ONE",
        "citied_by": "6",
        "cover_date": "2022-11-01",
        "Abstract": "An important step to achieve greater financial inclusion is to increase the acceptance and usage of digital payments. Although consumer adoption of digital payments has improved dramatically globally, the acceptance and usage of digital payments for micro, small, and medium-sized retailers (MSMRs) remain challenging. Using random forest estimation, we identify 14 key predictors out of 190 variables with the largest predictive power for MSMR adoption and usage of digital payments. Using conditional inference trees, we study the importance of sequencing and interactions of various factors such as public policy initiatives, technological advancements, and private sector incentives. We find that in countries with low POS terminal adoption, killer applications such as mobile phone payment apps increase the likelihood of P2B digital transactions. We also find the likelihood of digital P2B payments at MSMRs increases when MSMRs pay their employees and suppliers digitally. The level of ownership of basic financial accounts by consumers and the size of the shadow economy are also important predictors of greater adoption and usage of digital payments. Using causal forest estimation, we find a positive and economically significant marginal effect for merchant and consumer fiscal incentives on POS terminal adoption on average. When countries implement financial inclusion initiatives, POS terminal adoption increases significantly and MSMRs’ share of P2B digital payments also increases. Merchant and consumer fiscal incentives also increase MSMRs’ share of P2B electronic payments.",
        "DOI": "10.1371/journal.pone.0276203",
        "paper_author": "Allen J.",
        "affiliation_name": "The World Bank, USA",
        "affiliation_city": "Washington, D.C.",
        "affiliation_country": "United States",
        "affiliation_id": "60112834",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Modelling Neglected and Underutilised Crops: A Systematic Review of Progress, Challenges, and Opportunities",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "5",
        "cover_date": "2022-11-01",
        "Abstract": "Developing and promoting neglected and underutilised crops (NUS) is essential to building resilience and strengthening food systems. However, a lack of robust, reliable, and scalable evidence impedes the mainstreaming of NUS into policies and strategies to improve food and nutrition security. Well-calibrated and validated crop models can be useful in closing the gap by generating evidence at several spatiotemporal scales needed to inform policy and practice. We, therefore, assessed progress, opportunities, and challenges for modelling NUS using a systematic review. While several models have been calibrated for a range of NUS, few models have been applied to evaluate the growth, yield, and resource use efficiencies of NUS. The low progress in modelling NUS is due, in part, to the vast diversity found within NUS that available models cannot adequately capture. A general lack of research compounds this focus on modelling NUS, which is made even more difficult by a deficiency of robust and accurate ecophysiological data needed to parameterise crop models. Furthermore, opportunities exist for advancing crop model databases and knowledge by tapping into big data and machine learning.",
        "DOI": "10.3390/su142113931",
        "paper_author": "Chimonyo V.G.P.",
        "affiliation_name": "International Maize and Wheat Improvement Center, Harare",
        "affiliation_city": "Harare",
        "affiliation_country": "Zimbabwe",
        "affiliation_id": "60051398",
        "affiliation_state": "Harare"
    },
    {
        "paper_title": "Visual-Predictive Data Analysis Approach for the Academic Performance of Students from a Peruvian University",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "18",
        "cover_date": "2022-11-01",
        "Abstract": "The academic success of university students is a problem that depends in a multi-factorial way on the aspects related to the student and the career itself. A problem with this level of complexity needs to be faced with integral approaches, which involves the complement of numerical quantitative analysis with other types of analysis. This study uses a novel visual-predictive data analysis approach to obtain relevant information regarding the academic performance of students from a Peruvian university. This approach joins together domain understanding and data-visualization analysis, with the construction of machine learning models in order to provide a visual-predictive model of the students’ academic success. Specifically, a trained XGBoost Machine Learning model achieved a performance of up to 91.5% Accuracy. The results obtained alongside a visual data analysis allow us to identify the relevant variables associated with the students’ academic performances. In this study, this novel approach was found to be a valuable tool for developing and targeting policies to support students with lower academic performance or to stimulate advanced students. Moreover, we were able to give some insight into the academic situation of the different careers of the university.",
        "DOI": "10.3390/app122111251",
        "paper_author": "Orrego Granados D.",
        "affiliation_name": "Universidad Peruana Unión",
        "affiliation_city": "Lima",
        "affiliation_country": "Peru",
        "affiliation_id": "60105305",
        "affiliation_state": "Lima"
    },
    {
        "paper_title": "Coupling Process-Based Crop Model and Extreme Climate Indicators with Machine Learning Can Improve the Predictions and Reduce Uncertainties of Global Soybean Yields",
        "publication": "Agriculture (Switzerland)",
        "citied_by": "8",
        "cover_date": "2022-11-01",
        "Abstract": "Soybean is one of the most important agricultural commodities in the world, thus making it important for global food security. However, widely used process-based crop models, such as the GIS-based Environmental Policy Integrated Climate (GEPIC) model, tend to underestimate the impacts of extreme climate events on soybean, which brings large uncertainties. This study proposed an approach of hybrid models to constrain such uncertainties by coupling the GEPIC model and extreme climate indicators using machine learning. Subsequently, the key extreme climate indicators for the globe and main soybean producing countries are explored, and future soybean yield changes and variability are analyzed using the proposed hybrid model. The results show the coupled GEPIC and Random Forest (GEPIC+RF) model (R: 0.812, RMSD: 0.716 t/ha and rRMSD: 36.62%) significantly eliminated uncertainties and underestimation of climate extremes from the GEPIC model (R: 0.138, RMSD: 1.401 t/ha and rRMSD: 71.57%) compared to the other five hybrid models (R: 0.365–0.612, RMSD: 0.928–1.021 and rRMSD: 47.48–52.24%) during the historical period. For global soybean yield and those in Brazil and Argentina, low-temperature-related indices are the main restriction factors, whereas drought is the constraining factor in the USA and China, and combined drought–heat disaster in India. The GEPIC model would overestimate soybean yields by 13.40–27.23%. The GEPIC+RF model reduced uncertainty by 28.45–41.83% for the period of 2040–2099. Our results imply that extreme climate events will possibly cause more losses in soybean in the future than we have expected, which would help policymakers prepare for future agriculture risk and food security under climate change.",
        "DOI": "10.3390/agriculture12111791",
        "paper_author": "Sun Q.",
        "affiliation_name": "Chinese Academy of Meteorological Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60027451",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Joint Communication and Action Learning in Multi-Target Tracking of UAV Swarms with Deep Reinforcement Learning",
        "publication": "Drones",
        "citied_by": "12",
        "cover_date": "2022-11-01",
        "Abstract": "Communication is the cornerstone of UAV swarms to transmit information and achieve cooperation. However, artificially designed communication protocols usually rely on prior expert knowledge and lack flexibility and adaptability, which may limit the communication ability between UAVs and is not conducive to swarm cooperation. This paper adopts a new data-driven approach to study how reinforcement learning can be utilized to jointly learn the cooperative communication and action policies for UAV swarms. Firstly, the communication policy of a UAV is defined, so that the UAV can autonomously decide the content of the message sent out according to its real-time status. Secondly, neural networks are designed to approximate the communication and action policies of the UAV, and their policy gradient optimization procedures are deduced, respectively. Then, a reinforcement learning algorithm is proposed to jointly learn the communication and action policies of UAV swarms. Numerical simulation results verify that the policies learned by the proposed algorithm are superior to the existing benchmark algorithms in terms of multi-target tracking performance, scalability in different scenarios, and robustness under communication failures.",
        "DOI": "10.3390/drones6110339",
        "paper_author": "Zhou W.",
        "affiliation_name": "Aviation University of Air Force",
        "affiliation_city": "Changchun",
        "affiliation_country": "China",
        "affiliation_id": "60073486",
        "affiliation_state": "Jilin"
    },
    {
        "paper_title": "Coupling agricultural system models with machine learning to facilitate regional predictions of management practices and crop production",
        "publication": "Environmental Research Letters",
        "citied_by": "10",
        "cover_date": "2022-11-01",
        "Abstract": "Process-based agricultural system models are a major tool for assessing climate-agriculture-management interactions. However, their application across large scales is limited by computational cost, model uncertainty, and data availability, hindering policy-making for sustainable agricultural production at the scale meaningful for land management by farmers. Using the Agricultural Production System sIMulator (APSIM) as an example model, the APSIM model was run for 101 years from 1980 to 2080 in a typical cropping region (i.e., the Huang-Huai-Hai plain) of China. Then, machine learning (ML)-based models were trained to emulate the performance of the APSIM model and used to map crop production and soil carbon (which is a key indicator of soil health and quality) dynamics under a great number of nitrogen and water management scenarios. We found that ML-based emulators can accurately and quickly reproduce APSIM predictions of crop yield and soil carbon dynamics across the region under different spatial resolutions, and capture main processes driving APSIM predictions with much less input data. In addition, the emulators can be easily and quickly applied to identify optimal nitrogen management to achieve yield potential and sequester soil carbon across the region. The approach can be used for modelling other complex systems and amplifying the usage of agricultural system models for guiding agricultural management strategies and policy-making to address global environmental challenges from agriculture intensification.",
        "DOI": "10.1088/1748-9326/ac9c71",
        "paper_author": "Xiao L.",
        "affiliation_name": "College of Environmental and Resource Sciences",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60117779",
        "affiliation_state": "Zhejiang"
    },
    {
        "paper_title": "Ecological Associations between Obesity Prevalence and Neighborhood Determinants Using Spatial Machine Learning in Chicago, Illinois, USA",
        "publication": "ISPRS International Journal of Geo-Information",
        "citied_by": "9",
        "cover_date": "2022-11-01",
        "Abstract": "Some studies have established relationships between neighborhood conditions and health. However, they neither evaluate the relative importance of neighborhood components in increasing obesity nor, more crucially, how these neighborhood factors vary geographically. We use the geographical random forest to analyze each factor’s spatial variation and contribution to explaining tract-level obesity prevalence in Chicago, Illinois, United States. According to our findings, the geographical random forest outperforms the typically used nonspatial random forest model in terms of the out-of-bag prediction accuracy. In the Chicago tracts, poverty is the most important factor, whereas biking is the least important. Crime is the most critical factor in explaining obesity prevalence in Chicago’s south suburbs while poverty appears to be the most important predictor in the city’s south. For policy planning and evidence-based decision-making, our results suggest that social and ecological patterns of neighborhood characteristics are associated with obesity prevalence. Consequently, interventions should be devised and implemented based on local circumstances rather than generic notions of prevention strategies and healthcare barriers that apply to Chicago.",
        "DOI": "10.3390/ijgi11110550",
        "paper_author": "Lotfata A.",
        "affiliation_name": "Chicago State University",
        "affiliation_city": "Chicago",
        "affiliation_country": "United States",
        "affiliation_id": "60020417",
        "affiliation_state": "IL"
    },
    {
        "paper_title": "Intelligent Control of Groundwater in Slopes with Deep Reinforcement Learning †",
        "publication": "Sensors",
        "citied_by": "5",
        "cover_date": "2022-11-01",
        "Abstract": "The occurrence of landslides has been increasing in recent years due to intense and prolonged rainfall events. Lowering the groundwater in natural and man-made slopes can help to mitigate the hazards. Subsurface drainage systems equipped with pumps have traditionally been regarded as a temporary remedy for lowering the groundwater in geosystems, whereas long-term usage of pumping-based techniques is uncommon due to the associated high operational costs in labor and energy. This study investigates the intelligent control of groundwater in slopes enabled by deep reinforcement learning (DRL), a subfield of machine learning for automated decision-making. The purpose is to develop an autonomous geosystem that can minimize the operating cost and enhance the system’s safety without introducing human errors and interventions. To prove the concept, a seepage analysis model was implemented using a partial differential equation solver, FEniCS, to simulate the geosystem (i.e., a slope equipped with a pump and subjected to rainfall events). A Deep Q-Network (i.e., a DRL learning agent) was trained to learn the optimal control policy for regulating the pump’s flow rate. The objective is to enable intermittent control of the pump’s flow rate (i.e., 0%, 25%, 50%, 75%, and 100% of the pumping capacity) to keep the groundwater close to the target level during rainfall events and consequently help to prevent slope failure. A comparison of the results with traditional proportional-integral-derivative-controlled and uncontrolled water tables showed that the geosystem integrated with DRL can dynamically adapt its response to diverse weather events by adjusting the pump’s flow rate and improve the adopted control policy by gaining more experience over time. In addition, it was observed that the DRL control helped to mitigate slope failure during rainfall events.",
        "DOI": "10.3390/s22218503",
        "paper_author": "Biniyaz A.",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Houghton",
        "affiliation_country": "United States",
        "affiliation_id": "60279453",
        "affiliation_state": "MI"
    },
    {
        "paper_title": "A Machine Learning-Based Anomaly Prediction Service for Software-Defined Networks",
        "publication": "Sensors",
        "citied_by": "4",
        "cover_date": "2022-11-01",
        "Abstract": "Software-defined networking (SDN) has gained tremendous growth and can be exploited in different network scenarios, from data centers to wide-area 5G networks. It shifts control logic from the devices to a centralized entity (programmable controller) for efficient traffic monitoring and flow management. A software-based controller enforces rules and policies on the requests sent by forwarding elements; however, it cannot detect anomalous patterns in the network traffic. Due to this, the controller may install the flow rules against the anomalies, reducing the overall network performance. These anomalies may indicate threats to the network and decrease its performance and security. Machine learning (ML) approaches can identify such traffic flow patterns and predict the systems’ impending threats. We propose an ML-based service to predict traffic anomalies for software-defined networks in this work. We first create a large dataset for network traffic by modeling a programmable data center with a signature-based intrusion-detection system. The feature vectors are pre-processed and are constructed against each flow request by the forwarding element. Then, we input the feature vector of each request to a machine learning classifier for training to predict anomalies. Finally, we use the holdout cross-validation technique to evaluate the proposed approach. The evaluation results specify that the proposed approach is highly accurate. In contrast to baseline approaches (random prediction and zero rule), the performance improvement of the proposed approach in average accuracy, precision, recall, and f-measure is (54.14%, 65.30%, 81.63%, and 73.70%) and (4.61%, 11.13%, 9.45%, and 10.29%), respectively.",
        "DOI": "10.3390/s22218434",
        "paper_author": "Latif Z.",
        "affiliation_name": "Hanyang University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea",
        "affiliation_id": "60024872",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Public Discourse Surrounding Suicide during the COVID-19 Pandemic: An Unsupervised Machine Learning Analysis of Twitter Posts over a One-Year Period",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "6",
        "cover_date": "2022-11-01",
        "Abstract": "Many studies have forewarned the profound emotional and psychosocial impact of the protracted COVID-19 pandemic. This study thus aimed to examine how individuals relate to suicide amid the COVID-19 pandemic from a global perspective via the public Twitter discourse around suicide and COVID-19. Original Twitter tweets from 1 February 2020 to 10 February 2021 were searched, with terms related to “COVID-19”, “suicide”, or “self-harm”. An unsupervised machine learning approach and topic modelling were used to identify topics from unique tweets, with each topic further grouped into themes using manually conducted thematic analysis by the study investigators. A total of 35,904 tweets related to suicide and COVID-19 were processed into 42 topics and six themes. The main themes were: (1) mixed reactions to COVID-19 public health policies and their presumed impact on suicide; (2) biopsychosocial impact of COVID-19 pandemic on suicide and self-harm; (3) comparing mortality rates of COVID-19, suicide, and other leading causes of death; (4) mental health support for individuals at risk of suicide; (5) reported cases and public reactions to news related to COVID-19, suicide, and homicide; and (6) figurative usage of the word suicide. The general public was generally concerned about governments’ responses as well as the perturbing effects on mental health, suicide, the economy, and at-risk populations.",
        "DOI": "10.3390/ijerph192113834",
        "paper_author": "Lim S.R.",
        "affiliation_name": "Singapore General Hospital",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore",
        "affiliation_id": "60017958",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Model predictive control of DC/DC boost converter with reinforcement learning",
        "publication": "Heliyon",
        "citied_by": "26",
        "cover_date": "2022-11-01",
        "Abstract": "Power electronics is seeing an increase in the use of sophisticated self-learning controllers as single board computers and microcontrollers progress faster. Traditional controllers, such as PI controllers, suffer from transient instability difficulties. The duty cycle and output voltage of a DC/DC converter are not linear. Due to this non-linearity, the PI controller generates variable levels of voltage fluctuations depending on the operating region of the converter. In some cases, non-linear controllers outperform PI controllers. The non-linear model of a non-linear controller is determined by data availability. So, a self-calibrating controller that collects data and optimizes itself as the operation goes on is necessary. Iteration and oscillation can be minimized with a well-trained reinforcement learning model utilizing a non-linear policy. A boost converter's output power supply capacity changes with a change in load, due to which the maximum duty cycle limit of a converter also changes. A support vector calibrated by reinforcement learning can dynamically change the duty cycle limit of a converter under variable load. This research highlights how reinforcement learning-based non-linear controllers can improve control and efficiency over standard controllers. The proposed concept is based on a microgrid system. Simulation and experimental analysis have been conducted on how reinforcement learning-based controller works for DC-DC boost converter.",
        "DOI": "10.1016/j.heliyon.2022.e11416",
        "paper_author": "Marahatta A.",
        "affiliation_name": "Kathmandu University",
        "affiliation_city": "Dhulikhel",
        "affiliation_country": "Nepal",
        "affiliation_id": "60071792",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Time to curb the data brokers",
        "publication": "Nature",
        "citied_by": "0",
        "cover_date": "2022-11-01",
        "Abstract": "NA",
        "DOI": "10.1038/d41586-022-03578-8",
        "paper_author": "Hinchliffe L.J.",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Highly accurate oil production forecasting under adjustable policy by a physical approximation network",
        "publication": "Energy Reports",
        "citied_by": "11",
        "cover_date": "2022-11-01",
        "Abstract": "Petroleum is widely used as one of the important energy sources. Engineers need to continuously adjust strategies to maximize oil recovery during development. Then strategy adjustment requires a lot of cost, so it is necessary to select the optimal strategy through production forecasting. In this paper, a self-defined Double-channel Heterogeneous Dynamical Graph network (DHDG) is proposed deal with this problem. It can receive different development strategies predict future production indices base on that. Particularly, the concept of graph network is used for the first time to simulate the well pattern and predict production of all wells in the block at same time. At the same time, in order to fully fit the seepage process between wells, we complicate the edge features of the graph, so that it has enough capacity to express the state information of the formation between wells. Finally, we amplify the features of network output through engineering calculations, which further ensured the integrity of the information transmitted in the recursive process. The example results in two benchmark reservoirs show that it is accurate enough to predict oil production rate in future 600 days and more accurate than LSTM networks.",
        "DOI": "10.1016/j.egyr.2022.10.406",
        "paper_author": "Wang H.",
        "affiliation_name": "China University of Petroleum (East China)",
        "affiliation_city": "Qingdao",
        "affiliation_country": "China",
        "affiliation_id": "60105111",
        "affiliation_state": "Shandong"
    },
    {
        "paper_title": "Regulatory Considerations on the use of Machine Learning based tools in Clinical Trials",
        "publication": "Health and Technology",
        "citied_by": "5",
        "cover_date": "2022-11-01",
        "Abstract": "Background: The widespread increasing use of machine learning (ML) based tools in clinical trials (CTs) impacts the activities of Regulatory Agencies (RAs) that evaluate the development of investigational medicinal products (IMPs) in clinical studies to be carried out through the use of data-driven technologies. The fast progress in this field poses the need to define new approaches and methods to support an agile and structured assessment process. Method: The assessment of key information, characteristics and challenges deriving from the application of ML tools in CTs and their link with the principles for a trustworthy artificial intelligence (AI) that directly affect the decision-making process is investigated. Results: Potential issues are identified during the assessment and areas of greater interaction combining key regulatory points and principles for a trustworthy AI are highlighted. The most impacted areas are those related to technical robustness and safety of the ML tool, in relation to data used and the level of evidence generated. Additional areas of attention emerged, like the ones related to data and algorithm transparency. Conclusion: We evaluate the applicability of a new method to further support the assessment of medicinal products developed using data-driven tools in a CT setting. This is a first step and new paradigms should be adopted to support policy makers and regulatory decisions, capitalizing on technology advancements, considering stakeholders’ feedback and still ensuring a regulatory framework on safety and efficacy. Graphical Abstract: [Figure not available: see fulltext.]",
        "DOI": "10.1007/s12553-022-00708-0",
        "paper_author": "Massella M.",
        "affiliation_name": "Agenzia Italiana del Farmaco",
        "affiliation_city": "Rome",
        "affiliation_country": "Italy",
        "affiliation_id": "60103318",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Hydrology research articles are becoming more topically diverse",
        "publication": "Journal of Hydrology",
        "citied_by": "3",
        "cover_date": "2022-11-01",
        "Abstract": "We used Natural Language Processing (NLP) to assess topic diversity in all research articles (∼75,000) from eighteen water science and hydrology journals published between 1991 and 2019. We found that individual water science and hydrology research articles are becoming increasingly diverse in the sense that, on average, the number of topics represented in individual articles is increasing, which may be a sign of increasing interdisciplinarity. This is true even though the body of water science and hydrology literature as a whole is not becoming more topically diverse. Topics with the largest increases in popularity were Climate Change Impacts, Water Policy & Planning, and Pollutant Removal. Topics with the largest decreases in popularity were Stochastic Models and Numerical Models. At a journal level, Water Resources Research, Journal of Hydrology, and Hydrological Processes are the three most topically diverse journals among the corpus that we studied.",
        "DOI": "10.1016/j.jhydrol.2022.128551",
        "paper_author": "Rahman M.",
        "affiliation_name": "University of California, Davis",
        "affiliation_city": "Davis",
        "affiliation_country": "United States",
        "affiliation_id": "60014439",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Context Aware Mobile Application Pre-Launching Model using KNN Classifier",
        "publication": "International Arab Journal of Information Technology",
        "citied_by": "3",
        "cover_date": "2022-11-01",
        "Abstract": "Mobile applications are the application software which can be executed in mobile devices. The Performance of the mobile application is major factor to be considered while developing the application software. Usually, the user uses a sequence of applications continuously. So, pre-launching of the mobile application is the best methodology used to increase the launch time of the mobile application. In Android Operating System (OS) they use cache policies to increase the launch time. But whenever a new application enters into the cache it removes the existing application from the cache even it is repeatedly used by the user. So the removed application needs to be re-launched again. To rectify it, we suggest K number of applications for pre-launching by calculating the affinity between the applications. Because, the user may uses the set of applications together for more than one time. We discover those applications from the usage pattern based on Launch Delay (LD), Power Consumption (PC), App Affinity, Spatial and Temporal relations and also, a K-Nearest Neighbour (KNN) classifier machine learning algorithm is used to increase the accuracy of prediction.",
        "DOI": "10.34028/iajit/19/6/11",
        "paper_author": "Alagarsamy M.",
        "affiliation_name": "Thiagarajar College of Engineering",
        "affiliation_city": "Madurai",
        "affiliation_country": "India",
        "affiliation_id": "60033058",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Illustrating nonlinear effects of built environment attributes on housing renters’ transit commuting",
        "publication": "Transportation Research Part D: Transport and Environment",
        "citied_by": "40",
        "cover_date": "2022-11-01",
        "Abstract": "The empirical relationships between the built environment and transit use can support policy interventions for transit promotion. Limited studies have emphasized housing renters who are likely to be transit-dependent people and the nonlinear effects of built environment attributes on renters’ behavior. Using household travel data in Beijing, this study uses a decision-tree based gradient boosting machine to explore the nonlinear and threshold relationships between built environment attributes and commuting by transit. Renters are more sensitive to access to transit than owners. The collective contributions of bus stop density and distance to metro station are about 22% for renters and 14% for owners. Furthermore, most variables show non-linear effects on commuting by transit. The effects of bus stop density on renters’ commuting by transit rise sharply twice. One threshold is at a low-density level and the other is at a high-density level. Exploiting the threshold effects can produce cost-effective outcomes.",
        "DOI": "10.1016/j.trd.2022.103503",
        "paper_author": "Ding C.",
        "affiliation_name": "Beihang University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60013789",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "From promise to practice: towards the realisation of AI-informed mental health care",
        "publication": "The Lancet Digital Health",
        "citied_by": "81",
        "cover_date": "2022-11-01",
        "Abstract": "In this Series paper, we explore the promises and challenges of artificial intelligence (AI)-based precision medicine tools in mental health care from clinical, ethical, and regulatory perspectives. The real-world implementation of these tools is increasingly considered the prime solution for key issues in mental health, such as delayed, inaccurate, and inefficient care delivery. Similarly, machine-learning-based empirical strategies are becoming commonplace in psychiatric research because of their potential to adequately deconstruct the biopsychosocial complexity of mental health disorders, and hence to improve nosology of prognostic and preventive paradigms. However, the implementation steps needed to translate these promises into practice are currently hampered by multiple interacting challenges. These obstructions range from the current technology-distant state of clinical practice, over the lack of valid real-world databases required to feed data-intensive AI algorithms, to model development and validation considerations being disconnected from the core principles of clinical utility and ethical acceptability. In this Series paper, we provide recommendations on how these challenges could be addressed from an interdisciplinary perspective to pave the way towards a framework for mental health care, leveraging the combined strengths of human intelligence and AI.",
        "DOI": "10.1016/S2589-7500(22)00153-4",
        "paper_author": "Koutsouleris N.",
        "affiliation_name": "Klinikum der Universität München",
        "affiliation_city": "Munich",
        "affiliation_country": "Germany",
        "affiliation_id": "60000291",
        "affiliation_state": "Bayern"
    },
    {
        "paper_title": "Assessing the impact of hydropower projects in Brazil through data envelopment analysis and machine learning",
        "publication": "Renewable Energy",
        "citied_by": "13",
        "cover_date": "2022-11-01",
        "Abstract": "The aim of this study was to assess the environmental impact of hydroelectric power generation projects and classify them according to their scale of environmental impact. To achieve this objective, the combination of Data Envelopment Analysis (DEA) and Artificial Neural Networks (ANN) techniques was applied to 53 hydroelectric power plant projects in the evaluation phase in Brazil. The main results were: a) the proposed index indicates that 7 of the 10 worst hydroelectric projects are of the Large Hydropower Plant (LHP) type; b) the neural model for predicting the environmental impact of hydroelectric projects has an error of less than 0.001; c) the neural model for classifying hydroelectric projects in terms of their environmental impact reached a performance of 99.0% accuracy. In general, this study contributes to the use of a hybrid decision-making approach based on a combination of DEA-ANN for energy policies, in addition to enabling an improvement in the evaluation of hydroelectric generation projects.",
        "DOI": "10.1016/j.renene.2022.10.066",
        "paper_author": "Bortoluzzi M.",
        "affiliation_name": "Universidade Federal de Mato Grosso do Sul",
        "affiliation_city": "Campo Grande",
        "affiliation_country": "Brazil",
        "affiliation_id": "60009160",
        "affiliation_state": "MS"
    },
    {
        "paper_title": "Bayesian Optimization Allowing for Common Random Numbers",
        "publication": "Operations Research",
        "citied_by": "8",
        "cover_date": "2022-11-01",
        "Abstract": "Bayesian optimization is a powerful tool for expensive stochastic black-box optimization problems, such as simulation-based optimization or machine learning hyperparameter tuning. Many stochastic objective functions implicitly require a random number seed as input. By explicitly reusing a seed, a user can exploit common random numbers, comparing two or more inputs under the same randomly generated scenario, such as a common customer stream in a job shop problem or the same random partition of training data into training and validation sets for a machine learning algorithm. With the aim of finding an input with the best average performance over infinitely many seeds, we propose a novel Gaussian process model that jointly models both the output for each seed and the average over seeds. We then introduce the knowledge gradient for common random numbers that iteratively determines a combination of input and a random seed to evaluate the objective and automatically trades off reusing old seeds and querying new seeds, thus overcoming the need to evaluate inputs in batches or measure differences of pairs as suggested in previous methods. We investigate the knowledge gradient for common random numbers both theoretically and empirically, finding that it achieves significant performance improvements with only moderate added computational cost.",
        "DOI": "10.1287/opre.2021.2208",
        "paper_author": "Pearce M.A.L.",
        "affiliation_name": "University of Warwick",
        "affiliation_city": "Coventry",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60022020",
        "affiliation_state": "West Midlands"
    },
    {
        "paper_title": "Evidence for residential building retrofitting practices using explainable AI and socio-demographic data",
        "publication": "Energy Reports",
        "citied_by": "13",
        "cover_date": "2022-11-01",
        "Abstract": "Extensive retrofits and effective policy measures are needed to meet the ambitious climate goals, particularly in the UK, with the EU's oldest residential building stock. Researchers must investigate the factors influencing retrofits to enable effective and targeted policy measures. To date, however, there is a lack of holistically large-scale quantitative studies accounting for such factors. At the same time, great potential is seen in data-driven solutions and the use of explainable artificial intelligence (XAI). We address this research gap by combining supervised machine learning with XAI employing a three-stage approach: First, we consolidate datasets of Energy Performance Certificates from England and Wales from which we extract conducted retrofits, house prices, and socio-demographic information. Second, we apply an eXtreme Gradient Boosting (XGBoost) model that predicts whether a building has been retrofitted or not. Lastly, we use SHapley Additive exPlanations values (SHAP) as an XAI technique to identify the key factors and relationships that influence the implementation of retrofits. We succeed in substantiating results previously obtained in qualitative or small-scale studies and also find that retrofit-related policies already implemented in regional cases, such as the ”Better Homes for Yorkshire” initiative, can successfully achieve large-scale success through replication in other regions. Further, our results suggest the implementation of income-based CO2 taxes as a reasonable and easy-to-implement policy measure.",
        "DOI": "10.1016/j.egyr.2022.10.060",
        "paper_author": "Wenninger S.",
        "affiliation_name": "Fraunhofer Institute for Applied Information Technology FIT",
        "affiliation_city": "Sankt Augustin",
        "affiliation_country": "Germany",
        "affiliation_id": "60002596",
        "affiliation_state": "Nordrhein-Westfalen"
    },
    {
        "paper_title": "A review of the use of artificial intelligence methods in infrastructure systems",
        "publication": "Engineering Applications of Artificial Intelligence",
        "citied_by": "49",
        "cover_date": "2022-11-01",
        "Abstract": "The artificial intelligence (AI) revolution offers significant opportunities to capitalise on the growth of digitalisation and has the potential to enable the ‘system of systems’ approach required in increasingly complex infrastructure systems. This paper reviews the extent to which research in economic infrastructure sectors has engaged with fields of AI, to investigate the specific AI methods chosen and the purposes to which they have been applied both within and across sectors. Machine learning is found to dominate the research in this field, with methods such as artificial neural networks, support vector machines, and random forests among the most popular. The automated reasoning technique of fuzzy logic has also seen widespread use, due to its ability to incorporate uncertainties in input variables. Across the infrastructure sectors of energy, water and wastewater, transport, and telecommunications, the main purposes to which AI has been applied are network provision, forecasting, routing, maintenance and security, and network quality management. The data-driven nature of AI offers significant flexibility, and work has been conducted across a range of network sizes and at different temporal and geographic scales. However, there remains a lack of integration of planning and policy concerns, such as stakeholder engagement and quantitative feasibility assessment, and the majority of research focuses on a specific type of infrastructure, with an absence of work beyond individual economic sectors. To enable solutions to be implemented into real-world infrastructure systems, research will need to move away from a siloed perspective and adopt a more interdisciplinary perspective that considers the increasing interconnectedness of these systems.",
        "DOI": "10.1016/j.engappai.2022.105472",
        "paper_author": "McMillan L.",
        "affiliation_name": "UCL Engineering",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60176024",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Interpretable machine learning to model biomass and waste gasification",
        "publication": "Bioresource Technology",
        "citied_by": "57",
        "cover_date": "2022-11-01",
        "Abstract": "Machine learning has been regarded as a promising method to better model thermochemical processes such as gasification. However, their black box nature can limit how much one can trust and learn from the developed models. Here seven different machine learning methods have been adopted to model the gasification of biomass and waste across a wide range of operating conditions. Gradient boosting regression has been found to outperform the other model types with a coefficient of determination (R2) of 0.90 when averaged across ten key gasification outputs. Global and local model interpretability methods have been used to illuminate the developed black box models. The studied models were most strongly influenced by the feedstock's particle size and the type of gasifying agent employed. By combining global and local interpretability methods, the understanding of black box models has been improved. This allows policy makers and investors to make more educated decisions about gasification process design.",
        "DOI": "10.1016/j.biortech.2022.128062",
        "paper_author": "Ascher S.",
        "affiliation_name": "University of Glasgow",
        "affiliation_city": "Glasgow",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60001490",
        "affiliation_state": "Scotland"
    },
    {
        "paper_title": "Robustness of evidence reported in preprints during peer review",
        "publication": "The Lancet Global Health",
        "citied_by": "21",
        "cover_date": "2022-11-01",
        "Abstract": "Scientists have expressed concern that the risk of flawed decision making is increased through the use of preprint data that might change after undergoing peer review. This Health Policy paper assesses how COVID-19 evidence presented in preprints changes after review. We quantified attrition dynamics of more than 1000 epidemiological estimates first reported in 100 preprints matched to their subsequent peer-reviewed journal publication. Point estimate values changed an average of 6% during review; the correlation between estimate values before and after review was high (0·99) and there was no systematic trend. Expert peer-review scores of preprint quality were not related to eventual publication in a peer-reviewed journal. Uncertainty was reduced during peer review, with CIs reducing by 7% on average. These results support the use of preprints, a component of biomedical research literature, in decision making. These results can also help inform the use of preprints during the ongoing COVID-19 pandemic and future disease outbreaks.",
        "DOI": "10.1016/S2214-109X(22)00368-0",
        "paper_author": "Nelson L.",
        "affiliation_name": "University of Wisconsin-Madison",
        "affiliation_city": "Madison",
        "affiliation_country": "United States",
        "affiliation_id": "60032179",
        "affiliation_state": "WI"
    },
    {
        "paper_title": "The effect of eye-level street greenness exposure on walking satisfaction: The mediating role of noise and PM<inf>2.5</inf>",
        "publication": "Urban Forestry and Urban Greening",
        "citied_by": "16",
        "cover_date": "2022-11-01",
        "Abstract": "While there are plenty of studies on the effects of neighborhood and park greenness on personal overall satisfaction and walking behavior, the relationship between street greenness exposure and walking satisfaction has received limited attention. Also, the possible pathways by which street greenness exposure affects walking satisfaction need to be further examined. To fill these research gaps, we measured eye-level street greenness using street view images, machine learning techniques and global position systems. A structural equation model was used to examine the mediating effects of objective noise and PM2.5 exposure and related subjective annoyance, on the relationship between street greenness exposure and people's walking satisfaction. The results showed that street greenness exposure not only had a significant direct effect on walking satisfaction, but also has a significant indirect effect on walking satisfaction through subjective environmental annoyances (including noise and PM2.5 annoyances) rather than through objective noise and PM2.5 exposures. Besides physical activity and social interaction, the indirect effect of street greenness exposure on walking satisfaction through subjective environmental pollution annoyance accounted for about 17.39% of the total effect and cannot be ignored. These results suggest that the urban greenness layout policy should not only consider residential greenness but should improve people's environmental perception and walking satisfaction by allocating more greenness on streets with high noise and PM2.5 levels.",
        "DOI": "10.1016/j.ufug.2022.127752",
        "paper_author": "Song J.",
        "affiliation_name": "Sun Yat-Sen University",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60021182",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Employing machine learning to quantify long-term climatological and regulatory impacts on groundwater availability in intensively irrigated regions",
        "publication": "Journal of Hydrology",
        "citied_by": "8",
        "cover_date": "2022-11-01",
        "Abstract": "The steady overexploitation of the Ogallala Aquifer underlying the U.S. High Plains Region has put irrigated crop production at risk, particularly in the Southern and Central High Plains. To manage this issue properly, a data-driven modeling framework is developed and tested that is fast to employ and yet provides reliable long-term groundwater level (GWL) forecasts as a function of climatological and anthropogenic factors. The modeling framework uses the random forests (RF) technique in combination with ordinary kriging, and is tested in Finney County in southwest Kansas. The introduction of groundwater withdrawal potential as a new surrogate for pumping intensity enables the RF model to capture decline in groundwater depletion rate as the system progresses towards aquifer depletion and/or as a result of well retirement policies. The RF model is executed from 2017 to 2099 for 20 different downscaled global climate models (GCMs) for the two representative concentration pathways (RCP) scenarios of 4.5 and 8.5. The results show the aquifer will cease to support irrigated agriculture in most of the county by 2060 under status quo management and average climate conditions. Moreover, climate will likely shift the aquifer's depletion time frame by 15 years or less in most of the study area. The long-term combined impact of well retirement plans and climate conditions on groundwater depletion trends imply well retirement policies do not lead to sustained groundwater savings. This study demonstrates the capacity of machine learning models to serve as a rapid assessment tool, informing policymakers about future groundwater availability in intensively irrigated regions and under different climate and management conditions.",
        "DOI": "10.1016/j.jhydrol.2022.128511",
        "paper_author": "Nozari S.",
        "affiliation_name": "Walter Scott, Jr. College of Engineering",
        "affiliation_city": "Fort Collins",
        "affiliation_country": "United States",
        "affiliation_id": "60136737",
        "affiliation_state": "CO"
    },
    {
        "paper_title": "Machine learning technology in biohydrogen production from agriculture waste: Recent advances and future perspectives",
        "publication": "Bioresource Technology",
        "citied_by": "41",
        "cover_date": "2022-11-01",
        "Abstract": "Agricultural waste biomass has shown great potential to deliver green energy produced by biochemical and thermochemical conversion processes to mitigate future energy crises. Biohydrogen has become more interested in carbon-free and high-energy dense fuels among different biofuels. However, it is challenging to develop models based on experience or theory for precise predictions due to the complexity of biohydrogen production systems and the limitations of human perception. Recent advancements in machine learning (ML) may open up new possibilities. For this reason, this critical study offers a thorough understanding of ML's use in biohydrogen production. The most recent developments in ML-assisted biohydrogen technologies, including biochemical and thermochemical processes, are examined in depth. This review paper also discusses the prediction of biohydrogen production from agricultural waste. Finally, the techno-economic and scientific obstacles to ML application in agriculture waste biomass-based biohydrogen production are summarized.",
        "DOI": "10.1016/j.biortech.2022.128076",
        "paper_author": "Kumar Sharma A.",
        "affiliation_name": "University of Petroleum and Energy Studies",
        "affiliation_city": "Dehradun",
        "affiliation_country": "India",
        "affiliation_id": "60107631",
        "affiliation_state": "UK"
    },
    {
        "paper_title": "Tipping points of marine phytoplankton to multiple environmental stressors",
        "publication": "Nature Climate Change",
        "citied_by": "25",
        "cover_date": "2022-11-01",
        "Abstract": "Globally, anthropogenic climate change is threatening marine species. However, whether and how global marine phytoplankton, which represent the base of marine food webs, will exceed their tipping points under multiple climate factors remain unclear. Here, by establishing machine learning models, we identified the tipping points of global marine phytoplankton production and resistance under eight environmental stressors. Phytoplankton production and resistance are affected by multiple factors and the temperature and partial pressure of carbon dioxide dominate the risks for reaching their tipping points. If the current emission scenario continues, 50% (40–61% at 90% confidence) and 41% (2–80% at 90% confidence) of tropical areas would reach the tipping points of ongoing phytoplankton production and resistance decline, respectively, in 2100. Compared with single- or few-factor studies, machine learning (for example, ensemble machine learning) provides a powerful and realistic solution for policy-makers facing large-scale ecological responses to global climate changes under multiple environmental stressors.",
        "DOI": "10.1038/s41558-022-01489-0",
        "paper_author": "Ban Z.",
        "affiliation_name": "Nankai University",
        "affiliation_city": "Tianjin",
        "affiliation_country": "China",
        "affiliation_id": "60018038",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Estimation of green and blue water evapotranspiration using machine learning algorithms with limited meteorological data: A case study in Amu Darya River Basin, Central Asia",
        "publication": "Computers and Electronics in Agriculture",
        "citied_by": "22",
        "cover_date": "2022-11-01",
        "Abstract": "Green-water evapotranspiration (GWET) and blue-water evapotranspiration (BWET) are much frequently discussed variables in the recent debates of water resources management and water productivity in water-scarce regions. But the deficiency of long-term, on-site records and limited observation stations is a critical challenge in determining the veracity of these variables. The GWET and BWET estimations rely considerably on extensive climate data, water fluxes data, soil parameters, crop distribution, and crop management data. However, obtaining accurate data by on-site observations or by remote sensing products is a difficult task in a data-scarce region and fewer variables are not sufficient to empirically estimate GWET and BWET. Machine learning (ML) is a modern artificial intelligence decision-making tool based on the analysis of fed-data and computer algorithms. This study reported the enormous potential of ML algorithms for estimating BWET and GWET using different sets of available climate variables. Wheat crop BWET and GWET were estimated at 114 meteorological stations in the Amu-Darya River Basin (ADRB) in Central Asia, using four most widely used ML algorithms: artificial neural network (ANN), supported vector machine (SVM), random forest (RF), and k-nearest neighbor (KNN). ML algorithms were trained with 75 % of the data, while tested and validated with 25 % of the data. A set of 24 models of different unique combinations of available variables were attempted to reasonably estimate GWET and BWET, and satisfying results were achieved. RF was found to be the most-promising ML algorithm to estimate BWET and GWET with limited available climate data. The estimated BWET and GWET can be considered in agriculture water resources policies to minimize further risks to the agroecosystem in ADRB.",
        "DOI": "10.1016/j.compag.2022.107403",
        "paper_author": "Azzam A.",
        "affiliation_name": "Aerospace Information Research Institute",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60273019",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "GMIX: Graph-based spatial–temporal multi-agent reinforcement learning for dynamic electric vehicle dispatching system",
        "publication": "Transportation Research Part C: Emerging Technologies",
        "citied_by": "16",
        "cover_date": "2022-11-01",
        "Abstract": "The past decade has witnessed a significant growth of electric vehicles (EVs) deployment in public and private transportation sectors. Dynamic electric vehicle routing aims to plan the routes of EVs to serve dynamically generated customers’ requests while ensuring the battery level by visiting the recharge stations. This paper studies the dynamic electric vehicle routing problem with simultaneous pick-up and delivery and soft time windows (DEVRP-STW). We design a graph-based spatio-temporal multi-agent reinforcement learning (GMIX) framework consisting of a spatio-temporal graph attention network (ST-GAT) and a value decomposition-based multi-agent reinforcement learning algorithm (Graph-QMIX). Graph-QMIX derives a multi-agent soft policy gradient based on QMIX and Soft Actor–Critic and utilises a graph-based mixing network proposed to enhance the interaction of agents. Extensive experiments including a case study using real-world taxi data from New York City and a simulation study are conducted. The proposed GMIX outperforms the baseline algorithms on the request–response ratio (RRO), the average travelling distance per completed request (ATR), the out of energy ratio (OER) and the average waiting time per completed request (AWR).",
        "DOI": "10.1016/j.trc.2022.103886",
        "paper_author": "Zhou T.",
        "affiliation_name": "Faculty of Science, Engineering and Built Environment",
        "affiliation_city": "Geelong",
        "affiliation_country": "Australia",
        "affiliation_id": "60088596",
        "affiliation_state": "VIC"
    },
    {
        "paper_title": "An overview of remote monitoring methods in biodiversity conservation",
        "publication": "Environmental Science and Pollution Research",
        "citied_by": "24",
        "cover_date": "2022-11-01",
        "Abstract": "Conservation of biodiversity is critical for the coexistence of humans and the sustenance of other living organisms within the ecosystem. Identification and prioritization of specific regions to be conserved are impossible without proper information about the sites. Advanced monitoring agencies like the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services (IPBES) had accredited that the sum total of species that are now threatened with extinction is higher than ever before in the past and are progressing toward extinct at an alarming rate. Besides this, the conceptualized global responses to these crises are still inadequate and entail drastic changes. Therefore, more sophisticated monitoring and conservation techniques are required which can simultaneously cover a larger surface area within a stipulated time frame and gather a large pool of data. Hence, this study is an overview of remote monitoring methods in biodiversity conservation via a survey of evidence-based reviews and related studies, wherein the description of the application of some technology for biodiversity conservation and monitoring is highlighted. Finally, the paper also describes various transformative smart technologies like artificial intelligence (AI) and/or machine learning algorithms for enhanced working efficiency of currently available techniques that will aid remote monitoring methods in biodiversity conservation.",
        "DOI": "10.1007/s11356-022-23242-y",
        "paper_author": "Kerry R.G.",
        "affiliation_name": "Utkal University",
        "affiliation_city": "Bhubaneswar",
        "affiliation_country": "India",
        "affiliation_id": "60025619",
        "affiliation_state": "OR"
    },
    {
        "paper_title": "Generative model-based hybrid forecasting model for renewable electricity supply using long short-term memory networks: A case study of South Korea's energy transition policy",
        "publication": "Renewable Energy",
        "citied_by": "18",
        "cover_date": "2022-11-01",
        "Abstract": "Forecasting renewable energy is essential for achieving a sustainable energy future. This study aimed to develop a hybrid deep-learning-based model for forecasting renewable electricity supply in a case study of South Korea in order to assist national-scale energy plan assessments. A generative model based on the variational auto-encoder (VAE) algorithm allows for the harnessing of numerous samples by solving problems such as a lack of sufficient time-series data and their uncertainties. Long short-term memory (LSTM) networks, which are well-suited to facilitate time-series problems, were used in forecasting, and they were compared to other machine learning-based models, such as the gated recurrent unit, deep neural network, and autoregressive integrated moving average models. Performance evaluation metrics, such as coefficient of determination (R2), root-mean-square error (RMSE), mean absolute error (MAE), mean absolute percentage error (MAPE), and weighted absolute percentage error (WAPE), were used to determine the optimal model. Therefore, the developed forecasting model based on the LSTM and VAE is suitable for the case study because it presented the highest R2 score of 0.92 and a decrease of 17%, 24%, 13%, and 14% in the RMSE, MAE, MAPE, and WAPE, respectively. It can be stated that results from this study will have a significant impact on renewable energy planning.",
        "DOI": "10.1016/j.renene.2022.09.058",
        "paper_author": "Lee Y.",
        "affiliation_name": "Gyeongsang National University",
        "affiliation_city": "Jinju",
        "affiliation_country": "South Korea",
        "affiliation_id": "60023075",
        "affiliation_state": "Kyongsangnam-do"
    },
    {
        "paper_title": "Predicting and interpreting financial distress using a weighted boosted tree-based tree",
        "publication": "Engineering Applications of Artificial Intelligence",
        "citied_by": "16",
        "cover_date": "2022-11-01",
        "Abstract": "Financial distress prediction aims at providing an early warning solution of financial distress to help business participants, investors, and regulators to achieve better profit growth and financial risk management. Extreme gradient boosting (XGBoost), has been recognized as a favorable competitor compared with machine learning-based individual classifiers. However, its commercial value for FDP is hindered by two reasons. First, FDP is a classical imbalance issue, traditional XGBoost is considered a cost-insensitive approach that yields skew-sensitive FDP results. Second, XGBoost is a complex ensemble approach that faces the performance-interpretability dilemma, making the decision logic of XGBoost cannot be easily understood. To solve the above limitations, in this study, we first focus on addressing the imbalance issue in FDP by introducing a weighted cost-sensitive XGBoost, reducing the error of misclassifying financial distress firms. Next, we merge the decision rules extracted from the optimized weighted XGBoost to reconstruct a new tree as the approximation of the cost-sensitive ensemble model, making the proposed weighted XGBoost-based tree (XGBoost-W-BT) an accurate and interpretable solution for imbalanced FDP. Experimental results on a Chinese FDP dataset collected from China Security Market Accounting Research Database (CSMARD) showed that XGBoost-W-BT can be an alternative to weighted XGBoost to predict financial distress at an early stage. Besides, the transparent tree-based structure provides an explicit explanation to help industry participants and regulators make scientific policies, guiding investors to make rational investments.",
        "DOI": "10.1016/j.engappai.2022.105466",
        "paper_author": "Liu W.",
        "affiliation_name": "Donghua University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60010953",
        "affiliation_state": "Shanghai"
    },
    {
        "paper_title": "Model enhanced reinforcement learning to enable precision dosing: A theoretical case study with dosing of propofol",
        "publication": "CPT: Pharmacometrics and Systems Pharmacology",
        "citied_by": "8",
        "cover_date": "2022-11-01",
        "Abstract": "Extending the potential of precision dosing requires evaluating methodologies offering more flexibility and higher degree of personalization. Reinforcement learning (RL) holds promise in its ability to integrate multidimensional data in an adaptive process built toward efficient decision making centered on sustainable value creation. For general anesthesia in intensive care units, RL is applied and automatically adjusts dosing through monitoring of patient's consciousness. We further explore the problem of optimal control of anesthesia with propofol by combining RL with state-of-the-art tools used to inform dosing in drug development. In particular, we used pharmacokinetic-pharmacodynamic (PK-PD) modeling as a simulation engine to generate experience from dosing scenarios, which cannot be tested experimentally. Through simulations, we show that, when learning from retrospective trial data, more than 100 patients are needed to reach an accuracy within the range of what is achieved with a standard dosing solution. However, embedding a model of drug effect within the RL algorithm improves accuracy by reducing errors to target by 90% through learning to take dosing actions maximizing long-term benefit. Data residual variability impacts accuracy while the algorithm efficiently coped with up to 50% interindividual variability in the PK and 25% in the PD model's parameters. We illustrate how extending the state definition of the RL agent with meaningful variables is key to achieve high accuracy of optimal dosing policy. These results suggest that RL constitutes an attractive approach for precision dosing when rich data are available or when complemented with synthetic data from model-based tools used in model-informed drug development.",
        "DOI": "10.1002/psp4.12858",
        "paper_author": "Ribba B.",
        "affiliation_name": "F. Hoffmann-La Roche AG",
        "affiliation_city": "Basel",
        "affiliation_country": "Switzerland",
        "affiliation_id": "60008201",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Detection of citrus black spot disease and ripeness level in orange fruit using learning-to-augment incorporated deep networks",
        "publication": "Ecological Informatics",
        "citied_by": "47",
        "cover_date": "2022-11-01",
        "Abstract": "Fruit infected by pests or diseases and fruit harvests with different levels of ripeness cause a lack of marketability, decrease in economic value, and increase in crop waste. In this study, we propose a robust and generalized deep convolutional neural network (CNN) model via fine-tuning the pre-trained models for detecting black spot disease and ripeness levels in orange fruit. A dataset containing 1896 confirmed orange images in the farm in four classes (unripe, half-ripe, ripe, and infected with black spot disease) was used. In order to prevent overfitting and increase the robustness and generalizability of the model, instead of using fundamental data augmentation techniques, a novel learning-to-augment strategy that creates new data using noisy and restored images was employed. Controllers using the Bayesian optimization algorithm were utilized to select the optimal noise parameters of Gaussian, speckle, Poisson, and salt-and-pepper noise to generate new noisy images. A convolutional autoencoder model was developed to produce newly restored images affected by optimized noise density. The dataset augmented by the best policies of the learning-to-augment strategy was used to fine-tune several pre-trained models (GoogleNet, ResNet18, ResNet50, ShuffleNet, MobileNetv2, and DenseNet201). The results showed that the learning-to-augment strategy for the fine-tuned ResNet50 achieved the best performance with 99.5% accuracy, and 100% F-measure by assigning images infected with black spot disease as the positive class. The proposed automatic disease and fruit quality monitoring technique can be also used for the detection of other diseases in agriculture and forestry.",
        "DOI": "10.1016/j.ecoinf.2022.101829",
        "paper_author": "Momeny M.",
        "affiliation_name": "University of Florida Institute of Food and Agricultural Sciences",
        "affiliation_city": "Gainesville",
        "affiliation_country": "United States",
        "affiliation_id": "60010177",
        "affiliation_state": "FL"
    },
    {
        "paper_title": "Multilevel determinants on COVID-19 booster intention among Americans",
        "publication": "Preventive Medicine",
        "citied_by": "12",
        "cover_date": "2022-11-01",
        "Abstract": "The pandemic has disrupted public health and social well-being for more than two years. With the vaccine efficacy waning over time and the spread of new variants, a booster becomes increasingly imperative. This study investigates predictors of the American public's COVID-19 booster intention. A national survey was conducted from September 23rd to October 31st, 2021, on a representative sample. The survey data is merged with state-level indicators of vaccination rate, case rate, political context, and economic recovery. Multilevel regression modeling is adopted for statistical estimation. Results show that a higher proportion of vaccinated people in the network is positively related to one's chance of getting the booster (β = 0.593, p = 0.000). In comparison, a higher proportion of infected people in the network is negatively related to one's intention to become boosted (β = −0.240, p = 0.039). Additionally, the higher educated (β = 0.080, p = 0.001) and older (β = 0.004, p = 0.013) were more likely to say they would get the booster than their counterparts. Meanwhile, the odds of people taking the COVID-19 booster decrease by 3.541 points (p = 0.002) for each unit increase in the case rate at the state level. This study articulates that individual intention to take the booster is a function of their personal characteristics and is also rooted in social networks. These findings contribute to the literature and have policy implications. Knowledge of the profiles among people who intend to take/refuse the booster provides essential information to leverage certain factors and maximize booster uptake to mitigate the pandemic's devastating impact.",
        "DOI": "10.1016/j.ypmed.2022.107269",
        "paper_author": "Hao F.",
        "affiliation_name": "University of South Florida, Tampa",
        "affiliation_city": "Tampa",
        "affiliation_country": "United States",
        "affiliation_id": "60007740",
        "affiliation_state": "FL"
    },
    {
        "paper_title": "Ride-hail to ride rail: Learning to balance supply and demand in ride-hailing services with intermodal mobility options",
        "publication": "Transportation Research Part C: Emerging Technologies",
        "citied_by": "4",
        "cover_date": "2022-11-01",
        "Abstract": "Growing spatiotemporal imbalance in supply and demand in ride-hailing services has been arousing concern. A variety of measures in the perspective of ride-hailing services per se is studied. However, approaching the imbalance problem in the broader perspective of intermodal mobility, which integrates different modes of passenger transportation in a single trip and aims to overcome limitations of any unimodal mobility, remains to be explored. This paper aims to investigate the potential of introducing intermodal mobility options to balance ride-hailing services. We first identify the importance of the availability decision on intermodal mobility options. Then we formulate the availability decision problem as a Markov decision process (MDP). Due to its convoluted system dynamics and large state space, we cast the intractable MDP into a reinforcement learning (RL) problem to approximately learn the availability policy. To stabilize the learning processes, we model the intermodal ride-hailing services as a stochastic queueing network and tailor a family of state-of-the-art RL algorithms to iteratively evaluate and improve the availability policy. Lastly, we test this optimization framework in a large-scale intermodal mobility scenario calibrated with real-world trip data. Results show that the learned availability policy can significantly dissipate riders’ queue and improve the service rates towards more balanced supply and demand.",
        "DOI": "10.1016/j.trc.2022.103887",
        "paper_author": "Qin G.",
        "affiliation_name": "Key Laboratory of Road and Traffic Engineering of the State Ministry of Education",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60129238",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Has COVID-19 changed carer's views of health and care integration in care homes? A sentiment difference-in-difference analysis of on-line service reviews",
        "publication": "Health Policy",
        "citied_by": "5",
        "cover_date": "2022-11-01",
        "Abstract": "Closer integration of health and social care is a policy priority in many countries. The COVID-19 pandemic has reinforced the necessity of joining up health and social care systems, especially in care home settings. However, the meaning and perceived importance of integration for residents’ and carers’ experience is unclear and we do not know whether it has changed during the pandemic. Using unique data from on-line care home service reviews, we combined multiple methods. We used Natural Language Processing with supervised machine learning to construct a measure of sentiment for care home residents' and their relatives’ (measured by AFINN score). Difference-in-difference analysis was used to examine whether experiencing integrated care altered these sentiments by comparing changes in sentiment in reviews related to integration (containing specific terms) to those which were not. Finally, we used network analysis on post-estimation results to assess which specific attributes stakeholders focus on most when detailing their most/least positive experiences of health and care integration in care homes, and whether these attributes changed over the pandemic. Reviews containing integration words were more positive than reviews unrelated to integration in the pre-pandemic period (about 2.3 points on the AFINN score) and remained so during the first year of the pandemic. Overall positive sentiment increased during the COVID-19 period (average by +1.1 points), mainly in reviews mentioning integration terms at the beginning of the first (+2.17, p-value 0.175) and second waves (+3.678, p-value 0.027). The role of care home staff was pivotal in both positive and negative reviews, with a shift from aspects related to care in pre-pandemic to information services during the pandemic, signalling their importance in translating integrated needs-based paradigms into policy and practice.",
        "DOI": "10.1016/j.healthpol.2022.08.010",
        "paper_author": "Almorox E.G.",
        "affiliation_name": "Faculty of Biology, Medicine and Health",
        "affiliation_city": "Manchester",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60172345",
        "affiliation_state": "Greater Manchester"
    },
    {
        "paper_title": "Fairness in vulnerable attribute prediction on social media",
        "publication": "Data Mining and Knowledge Discovery",
        "citied_by": "4",
        "cover_date": "2022-11-01",
        "Abstract": "Historically, policymakers and practitioners relied exclusively on survey and census data to design and plan for assistive interventions; now, social media offer a timely and cost-effective way to reach out to populations otherwise unobserved. This study was designed to address the needs of a non-for-profit organisation to reach out to the young unemployed individuals in Italy with educational and job opportunities via communication channels that are more likely to appeal to younger generations. To this extend, we developed an ad-hoc Facebook application which administers questionnaires while gathering data about the Likes on Facebook Pages. Then, we developed a machine learning framework that successfully predicts the unemployment status of an unseen individual (.74 AUC). However, blindly delegating to the machine learning model the communication intervention may lead to digital discrimination on the basis of socio-demographic characteristics. Here, we propose a framework that aims to optimising both for the prediction performance as well as the most adequate fairness metric. Our framework is based on an adaptive threshold for gender, while we show that it can be expanded for other socio-demographic attributes and generalised for other interventions of assistive character. We present a doubly cross-validated setting that achieves out-of-sample stability and generalisability of results. We compare the behaviour of models that infer on different sets of data and provide an indepth discussion on the most predictive features, demonstrating that the “fairness through unawareness” approach does not suffice to achieve a fair classification since sensitive demographic information can be inferred not only via other sociodemographic attributes but also from behavioural digital patterns. Finally, we thoroughly assess the behaviour of the adaptive threshold approach and provide an in-depth discussion on the advantages but also the implications of such models offering actionable insights. Our results show that careful assessment of fairness metrics should be considered, primarily when AI models are employed for policymaking.",
        "DOI": "10.1007/s10618-022-00855-y",
        "paper_author": "Beiró M.G.",
        "affiliation_name": "Universidad de Buenos Aires",
        "affiliation_city": "Buenos Aires",
        "affiliation_country": "Argentina",
        "affiliation_id": "60001563",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Identifying potential breakthrough research: A machine learning method using scientific papers and Twitter data",
        "publication": "Technological Forecasting and Social Change",
        "citied_by": "18",
        "cover_date": "2022-11-01",
        "Abstract": "Breakthrough research may signal shifts in science, technology, and innovation systems. Early identification of breakthrough research is important not only for scientists, but also for policy makers and R&D experts in developing R&D strategies and allocating R&D resources. Researchers mostly use scientific papers data to identify potential breakthrough research, but they rarely make use of Twitter data related to scientific research and machine learning methods. Analysis of Twitter data is of great significance for us to understand the public's perception of potential breakthrough research and to identify potential breakthrough research. Machine learning methods can assist us in predicting the trend of events by utilizing prior knowledge and experience. Therefore, this paper proposes a framework for identifying potential breakthrough research using machine learning methods with scientific papers and Twitter data. We select solar cells as a case study to verify the valid and flexible of this framework. In this case, we use machine learning method to discover potential breakthrough research from scientific papers, and we use Twitter data mining to analyze Twitter users' sense of and response to the discovered potential breakthrough research, which aims to achieve a more extensive and diverse assessment of the discovered potential breakthrough research. This paper contributes to identifying potential breakthrough research, as well as understanding the emergence and development of breakthrough research. It will be of interest to R&D experts in the field of solar cell technology.",
        "DOI": "10.1016/j.techfore.2022.122042",
        "paper_author": "Li X.",
        "affiliation_name": "Beijing University of Technology",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60022281",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Analysis of the socioeconomic impact due to COVID-19 using a deep clustering approach",
        "publication": "Applied Soft Computing",
        "citied_by": "5",
        "cover_date": "2022-11-01",
        "Abstract": "One of the main problems that countries are currently having is being able to measure the impact of the pandemic in other areas of society (for example, economic or social). In that sense, being able to combine variables about the behavior of COVID-19 with other variables in the environment, to build models about its impact, which help the decision-making of national authorities, is a current challenge. In this sense, this work proposes an approach that allows monitoring the socioeconomic behavior of the regions/departments of a country (in this case, Colombia) due to the effect of COVID-19. To do this, an approach is proposed in which the behavior of the infected is initially predicted, and together with other context variables (climate, economics and socials) determines the current socioeconomic situation of a region. This classification of a region, with the pattern that characterizes it, is a fundamental input for those who make decisions. Thus, this work presents an approach based on machine learning techniques to identify regions with similar socioeconomic behaviors due to COVID-19, so they should eventually have similar public policies. The proposed hybrid model initially consists of a time series prediction model of infected, to which are added several context variables (climate, socioeconomic, incidence of COVID-19 at the level of deaths, suspects, etc.) in an unsupervised learning model, to determine the socioeconomic impact in the regions. Particularly, the unsupervised model groups similar regions together, and the pattern of each group describes the socioeconomic similarities between them, to help decision-makers in the process of defining policies to be implemented in the regions. The experiments showed the ability of the hybrid model to follow the evolution of the regions after 4 weeks. The quality metrics for the predictive model were around the values of 0.35 for MAPE and 0.68 for R2, and in the case of the clustering model were around the values of 0.3 for the Silhouette index and 0.6 for the Davies–Boulding index. The hybrid model allowed determining things like some regions that initially belonged to a group with a very low incidence of positive cases and very unfavorable socioeconomic conditions, became part of groups with moderately high incidences. Our preliminary results are very satisfactory since they allow studying the evolution of the socioeconomic impact in each region/department.",
        "DOI": "10.1016/j.asoc.2022.109606",
        "paper_author": "Quintero Y.",
        "affiliation_name": "Universidad EAFIT",
        "affiliation_city": "Medellin",
        "affiliation_country": "Colombia",
        "affiliation_id": "60061982",
        "affiliation_state": "Antioquia"
    },
    {
        "paper_title": "Trends in energy policy coordination research on supporting low-carbon energy development",
        "publication": "Environmental Impact Assessment Review",
        "citied_by": "25",
        "cover_date": "2022-11-01",
        "Abstract": "The transition of the world's energy system to a low-carbon system is instrumental in achieving global carbon neutrality. This transition of the energy system requires coordination and cooperation among different energy-focused agents as well as integrated development between the agents and the system. However, the low-carbon transition of the overall system cannot be achieved by the spontaneous evolution of the system itself, but requires policy guidance and the coordination of different directions and measures. Policy coordination can effectively integrate the goals and measures of energy policy, and can drive the development of agents within the energy system. In-depth survey and analysis on energy policy coordination show that more attention should be paid to these two research topics: dual carbon target and supply security of the energy system. Specifically, demand-side response, consumption-side reforms, and energy management innovation would become new research priorities. And future research methods of energy policy coordination should focus on quantitative analysis and machine learning. Further, econometric methods should be combined to explore the stack effect of multiple agents within the energy systems. Based on different logical approaches associated with energy policy choices, a multi-agent and multi-dimensional policy coordination framework should be constructed to provide implementation strategies for a coordinated evolutionary transition to low-carbon energy.",
        "DOI": "10.1016/j.eiar.2022.106903",
        "paper_author": "Nie Y.",
        "affiliation_name": "Lanzhou University",
        "affiliation_city": "Lanzhou",
        "affiliation_country": "China",
        "affiliation_id": "60028265",
        "affiliation_state": "Gansu"
    },
    {
        "paper_title": "Gotham city. Predicting ‘corrupted’ municipalities with machine learning",
        "publication": "Technological Forecasting and Social Change",
        "citied_by": "11",
        "cover_date": "2022-11-01",
        "Abstract": "The economic costs of white-collar crimes, such as corruption, bribery, embezzlement, abuse of authority, and fraud, are substantial. How to eradicate them is a mounting task in many countries. Using police archives, we apply machine learning algorithms to predict corruption crimes in Italian municipalities. Drawing on input data from 2011, our classification trees correctly forecast over 70 % (about 80 %) of the municipalities that will experience corruption episodes (an increase in corruption crimes) over the period 2012–2014. We show that algorithmic predictions could strengthen the ability of the 2012 Italy's anti-corruption law to fight white-collar delinquencies and prevent the occurrence of such crimes while preserving transparency and accountability of the policymaker.",
        "DOI": "10.1016/j.techfore.2022.122016",
        "paper_author": "de Blasio G.",
        "affiliation_name": "Banca d'Italia",
        "affiliation_city": "Rome",
        "affiliation_country": "Italy",
        "affiliation_id": "60082963",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Lifetime health costs of intimate partner violence: A prospective longitudinal cohort study with linked data for out-of-hospital and pharmaceutical costs",
        "publication": "Economic Modelling",
        "citied_by": "12",
        "cover_date": "2022-11-01",
        "Abstract": "The health effects of intimate partner violence (IPV) can be long term, developing years after the IPV began and persisting after it has ceased. We aim to quantify the excess lifetime out-of-hospital and pharmaceutical health costs of women who experience IPV in Australia by applying a novel combination of econometric and actuarial techniques to a large and unique dataset. We find that women with a history of IPV have AUD48,413 (2020) higher lifetime health costs per person than women who do not experience IPV. This suggests that the adverse health impact of IPV leads to increased health costs over one's lifetime regardless of whether the initial IPV experience is early or later in life, and policies reducing the incidence of IPV will have long-term impacts on the healthcare system.",
        "DOI": "10.1016/j.econmod.2022.106013",
        "paper_author": "William J.",
        "affiliation_name": "ANU College of Business &amp; Economics",
        "affiliation_city": "Canberra",
        "affiliation_country": "Australia",
        "affiliation_id": "60159917",
        "affiliation_state": "ACT"
    },
    {
        "paper_title": "Optimization of autonomous vehicle speed control mechanisms using hybrid DDPG-SHAP-DRL-stochastic algorithm",
        "publication": "Advances in Engineering Software",
        "citied_by": "6",
        "cover_date": "2022-11-01",
        "Abstract": "Autonomous Vehicles (AV) are the future milestones of the automobile industry, which functions without the intervention of human being. Numerous researches have been stimulated by leading automobile sectors of the world, to address the anticipated challenges in implementing the autonomous vehicles in a practical scenario. The speed control mechanism is the predominant challenge which acts in the basis of Machine Learning mechanism is the major thrust area associated with autonomous vehicles. Reinforcement Learning (RL) is the effective algorithm to solve the challenges associated with the autonomous driving of vehicles and its decision on complex scenarios. A simulative environment is advantageous for training and validation of an RL algorithm because it reduces risk and saves resources. This research work introduces a novel hybrid algorithm composed of Deep Deterministic Policy Gradient (DDPG) - SHapley Additive exPlanations (SHAP) – Deep Reinforcement Learning (DRL)-stochastic algorithm. The primary objective of this research work is to introduce an RL environment for optimizing longitudinal control.",
        "DOI": "10.1016/j.advengsoft.2022.103245",
        "paper_author": "Syavasya C.V.S.R.",
        "affiliation_name": "GITAM University",
        "affiliation_city": "Visakhapatnam",
        "affiliation_country": "India",
        "affiliation_id": "60026146",
        "affiliation_state": "AP"
    },
    {
        "paper_title": "A Novel Constraint-Based Knee- Guided Neuroevolutionary Algorithm for Context-Specific ECG Early Classification",
        "publication": "IEEE Journal of Biomedical and Health Informatics",
        "citied_by": "6",
        "cover_date": "2022-11-01",
        "Abstract": "Cardiovascular diseases (CVDs) are considered the greatest threat to human life according to World Health Organization. Early classification of CVDs and the appropriate follow-up treatment are crucial for preventing sudden deaths. Electrocardiogram (ECG) is one of the most common non-invasive tools used to evaluate the state of the heart, which can be exploited to automatically diagnose as well. However, the importance of diagnosing CVDs is varying in different context-specific scenarios. For example, ST-segment elevation (STE) is an acute myocardial infarction indicator for patients associated with chest pain and cardiac biomarker. In in-hospital healthcare, STE should be diagnosed with a higher priority than the other phenotypes of ECG. Hence, the context-specific requirements should be considered in ECG early classification problems. We formalize the ECG early classification problem as the context-specific time series classification problem. We propose a novel Constraint-based Knee-guided Neuroevolutionary Algorithm (CKNA) based on the Snippet Policy Networks V2 to solve this problem. To validate the proposed method, we perform a series of experiments on two public ECG datasets under various context-specific simulated scenarios after consulting with physicians specializing in the area. Experimental results show that CKNA significantly improves the average recall of disease classification by 5.5% compared to the competing baseline under user-specified requirements. Moreover, experimental results prove that CKNA presents a feasible solution for the early classifying of cardiac arrhythmias under different user-specified scenarios.",
        "DOI": "10.1109/JBHI.2022.3199377",
        "paper_author": "Huang Y.",
        "affiliation_name": "National Yang Ming Chiao Tung University",
        "affiliation_city": "Hsinchu",
        "affiliation_country": "Taiwan",
        "affiliation_id": "60199493",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Decoding river pollution trends and their landscape determinants in an ecologically fragile karst basin using a machine learning model",
        "publication": "Environmental Research",
        "citied_by": "21",
        "cover_date": "2022-11-01",
        "Abstract": "Karst watersheds accommodate high landscape complexity and are influenced by both human-induced and natural activity, which affects the formation and process of runoff, sediment connectivity and contaminant transport and alters natural hydrological and nutrient cycling. However, physical monitoring stations are costly and labor-intensive, which has confined the assessment of water quality impairments on spatial scale. The geographical characteristics of catchments are potential influencing factors of water quality, often overlooked in previous studies of highly heterogeneous karst landscape. To solve this problem, we developed a machining learning method and applied Extreme Gradient Boosting (XGBoost) to predict the spatial distribution of water quality in the world's most ecologically fragile karst watershed. We used the Shapley Addition interpretation (SHAP) to explain the potential determinants. Before this process, we first used the water quality damage index (WQI-DET) to evaluate the water quality impairment status and determined that CODMn, TN and TP were causing river water quality impairments in the WRB. Second, we selected 46 watershed features based on the three key processes (sources-mobilization-transport) which affect the temporal and spatial variation of river pollutants to predict water quality in unmonitored reaches and decipher the potential determinants of river impairments. The predicting range of CODMn spanned from 1.39 mg/L to 17.40 mg/L. The predictions of TP and TN ranged from 0.02 to 1.31 mg/L and 0.25–5.72 mg/L, respectively. In general, the XGBoost model performs well in predicting the concentration of water quality in the WRB. SHAP explained that pollutant levels may be driven by three factors: anthropogenic sources (agricultural pollution inputs), fragile soils (low organic carbon content and high soil permeability to water flow), and pollutant transport mechanisms (TWI, carbonate rocks). Our study provides key data to support decision-making for water quality restoration projects in the WRB and information to help bridge the science:policy gap.",
        "DOI": "10.1016/j.envres.2022.113843",
        "paper_author": "Xu G.",
        "affiliation_name": "Nanjing Institute of Geography and Limnology Chinese Academy of Sciences",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60027277",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "Influence of hydroclimatic variability on dengue incidence in a tropical dryland area",
        "publication": "Acta Tropica",
        "citied_by": "8",
        "cover_date": "2022-11-01",
        "Abstract": "Dengue is an endemic disease in more than 100 countries, but there are few studies about the effects of hydroclimatic variability on dengue incidence (DI) in tropical dryland areas. This study investigates the association between hydroclimatic variability and DI (2008-2018) in a large tropical dryland area. The area studied comprehends seven municipalities with populations ranging from 32,879 to 2,545,419 inhabitants. First, the precipitation and temperature impacts on interannual and seasonal DI were investigated. Then, the monthly association between DI and hydroclimatic variables was analyzed using generalized least squares (GLS) regression. The model's capability to reproduce DI given the current hydroclimatic conditions and DI seasonality over the entire time period studied were assessed. No association between the interannual variation of precipitation and DI was found. However, seasonal variation of DI was shaped by precipitation and temperature. February-July was the main dengue season period. A precipitation threshold, usually above 100 mm, triggers the rapid DI rising. Precipitation and minimum air temperature were the main explanatory variables. A two-month-lagged predictor was relevant for modeling, occurring in all regressions, followed by a non-lagged predictor. The climate predictors differed among the regression models, revealing the high spatial DI variability driven by hydroclimatic variability. GLS regressions were able to reproduce the beginning, development, and end of the dengue season, although we found underestimation of DI peaks and overestimation of low DI. These model limitations are not an issue for climate change impact assessment on DI at the municipality scale since historical DI seasonality was well simulated. However, they may not allow seasonal DI forecasting for some municipalities. These findings may help not only public health policies in the studied municipalities but also have the potential to be reproducible for other dryland regions with similar data availability.",
        "DOI": "10.1016/j.actatropica.2022.106657",
        "paper_author": "Costa A.C.",
        "affiliation_name": "Universidade da Integração Internacional da Lusofonia Afro-Brasileira - UNILAB",
        "affiliation_city": "Redencao",
        "affiliation_country": "Brazil",
        "affiliation_id": "60271971",
        "affiliation_state": "CE"
    },
    {
        "paper_title": "Examining the asymmetric impact of macroeconomic policy in the UAE: Evidence from quartile impulse responses and machine learning",
        "publication": "Journal of Economic Asymmetries",
        "citied_by": "4",
        "cover_date": "2022-11-01",
        "Abstract": "The current paper examines the asymmetric effects of changes to monetary and fiscal variables on different types of firms in the UAE. We compute impulse responses based on local projections and select shock and switching variables using machine learning. We examine 180 firms listed in the UAE exchanges and find significant asymmetries among financial and non-financial firms and among low- and high-debt firms when there is a shock to macroeconomic monetary or fiscal variables. Quartile analysis shows that firms belonging to the first and last quartile of debt respond negatively to expansionary policies, while middle-quartile firms respond more positively. Our results demonstrate the importance of comprehending the heterogeneity in the micro characteristics of the underlying corporate environment when evaluating macroeconomic policies. Our work can facilitate the design and implementation of policy in the UAE and helps explain the transmission mechanisms towards corporations.",
        "DOI": "10.1016/j.jeca.2022.e00267",
        "paper_author": "Polyzos E.",
        "affiliation_name": "Zayed University",
        "affiliation_city": "Dubai",
        "affiliation_country": "United Arab Emirates",
        "affiliation_id": "60070818",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "How can machine learning be used in stress management: A systematic literature review of applications in workplaces and education",
        "publication": "International Journal of Information Management Data Insights",
        "citied_by": "33",
        "cover_date": "2022-11-01",
        "Abstract": "In today's competitive and rival world, stress has emerged to be an integral part of every person's life which affects an individual directly or indirectly in many ways. The COVID-19 pandemic even glorified the importance and cruciality of managing stress, anxiety and depression as these created a massive impact on the economy, education, healthcare, business areas and other aspects of society in every possible manner. This study determines to find all the feasible contributing factors to stress, anxiety and depression which influence individuals coming from vivid occupational backgrounds due to personal, work-related, psychological and interpersonal reasons. Our research aims to define and describe the impact of the rise in technology and the COVID-19 pandemic on the stress levels of an individual. It includes variously supervised and unsupervised machine learning algorithms in detecting stress efficiently and effectively among a huge population. The objective of this paper is to make those millions of people aware of the early detection and treatment of stress before it becomes life-threatening to them. The paper finally throws light on how stress-related research will help policymakers in the education field and general industry sector to rebuild the policies on stress and countermeasures to avoid stress.",
        "DOI": "10.1016/j.jjimei.2022.100110",
        "paper_author": "Mittal S.",
        "affiliation_name": "Narsee Monjee Institute of Management Studies, Mumbai",
        "affiliation_city": "Mumbai",
        "affiliation_country": "India",
        "affiliation_id": "60079592",
        "affiliation_state": "MH"
    },
    {
        "paper_title": "Marginal reduction in surface NO<inf>2</inf> attributable to airport shutdown: A machine learning regression-based approach",
        "publication": "Environmental Research",
        "citied_by": "3",
        "cover_date": "2022-11-01",
        "Abstract": "Emissions from aviation and airport-related activities degrade surface air quality but received limited attention relative to regular transportation sectors like road traffic and waterborne vessels. Statistically, assessing the impact of airport-related emissions remains a challenge due to the fact that its signal in the air quality time series data is largely dwarfed by meteorology and other emissions. Flight-ban policy has been implemented in a number of cities in response to the COVID-19 spread since early 2020, which provides an unprecedented opportunity to examine the changes in air quality attributable to airport closure. It would also be interesting to know whether such an intervention produces extra marginal air quality benefits, in addition to road traffic. Here we investigated the impact of airport-related emissions from a civil airport on nearby NO2 air quality by applying machine learning predictive model to observational data collected from this unique quasi-natural experiment. The whole lockdown-attributable change in NO2 was 16.7 μg/m3, equals to a drop of 73% in NO2 with respect to the business-as-usual level. Meanwhile, the airport flight-ban aviation-attributable NO2 was 3.1 μg/m3, accounting for a marginal reduction of 18.6% of the overall NO2 change that driven by the whole lockdown effect. The airport-related emissions contributed up to 24% of the local ambient NO2 under normal conditions. Additionally, the average impact of airport-related emissions on the nearby air quality was ∼0.01 ± 0.001 μg/m3 NO2 per air-flight. Our results highlight that attention needs to be paid to such a considerable emission source in many places where regular air quality regulatory measures were insufficient to bring NO2 concentration into compliance with the health-based limit.",
        "DOI": "10.1016/j.envres.2022.114117",
        "paper_author": "Han B.",
        "affiliation_name": "Civil Aviation University of China",
        "affiliation_city": "Tianjin",
        "affiliation_country": "China",
        "affiliation_id": "60028512",
        "affiliation_state": "Tianjin"
    },
    {
        "paper_title": "From academia to policy makers: a methodology for real-time forecasting of infrequent events",
        "publication": "Journal of Computational Social Science",
        "citied_by": "0",
        "cover_date": "2022-11-01",
        "Abstract": "The field of conflict forecasting has matured greatly over the last decade. Advances in machine learning have allowed researchers to forecast rare political and social events in near real time. Yet the maturity of the field has led to a proliferation of diverse platforms for forecasting, divergent results across forecasts, and an explosion of forecasting methodologies. While the field has done much to establish some baseline results, true, consensual benchmarks against which future forecasts may be evaluated remain elusive, and thus, agreed upon empirical results are still rare. The aim of this work is to address these concerns and provide the field of conflict forecasting with a standardized analysis pipeline to evaluate future forecasts of political violence. We aim to open the black box of the conflict forecasting pipeline and provide empirical evidence on how modeling decisions along all steps of the pipeline affect end results. In this way, we empirically demonstrate best practices that conflict forecasting researchers may utilize in future endeavors. We employ forecasts of targeted mass killings and genocides to support our methodological claims.",
        "DOI": "10.1007/s42001-022-00176-6",
        "paper_author": "Krzywicki A.",
        "affiliation_name": "UNSW Sydney",
        "affiliation_city": "Sydney",
        "affiliation_country": "Australia",
        "affiliation_id": "60028333",
        "affiliation_state": "NSW"
    },
    {
        "paper_title": "Correlates of quality of life, happiness and life satisfaction among European adults older than 50 years: A machine‐learning approach",
        "publication": "Archives of Gerontology and Geriatrics",
        "citied_by": "26",
        "cover_date": "2022-11-01",
        "Abstract": "Background and objectives: Previous research has documented the role of different categories of psychosocial factors (i.e., sociodemographic factors, personality, subjective life circumstances, activity, physical health, and childhood circumstances) in predicting subjective well-being and quality of life among older adults. No previous study has simultaneously modeled a large number of these psychosocial factors using a well-powered sample and machine learning algorithms to predict quality of life, happiness, and life satisfaction among older adults. The aim of this paper was to investigate the correlates of quality of life, happiness, and life satisfaction among European adults older than 50 years using machine learning techniques. Research design and methods: Data drawn from the Survey of Health, Ageing and Retirement in Europe (SHARE) Wave 7 were used. Participants were 62,500 persons aged 50 years and over living in 26 Continental EU Member States, Switzerland, and Israel. Multiple machine learning regression approaches were used. Results: The algorithms captured 53%, 33%, and 18% of the variance of quality of life, life satisfaction, and happiness, respectively. The most important categories of correlates of quality of life and life satisfaction were physical health and subjective life circumstances. Sociodemographic factors (mostly country of residence) and psychological variables were the most important categories of correlates of happiness. Discussion and implications: This study highlights subjective poverty, self-perceived health, country of residence, subjective survival probability, and personality factors (especially neuroticism) as important correlates of quality of life, happiness, and life satisfaction. These findings provide evidence-based recommendations for practice and/or policy implications.",
        "DOI": "10.1016/j.archger.2022.104791",
        "paper_author": "Prati G.",
        "affiliation_name": "Alma Mater Studiorum Università di Bologna, Cesena",
        "affiliation_city": "Cesena",
        "affiliation_country": "Italy",
        "affiliation_id": "60000686",
        "affiliation_state": "FC"
    },
    {
        "paper_title": "Adaptive Edge Offloading for Image Classification Under Rate Limit",
        "publication": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
        "citied_by": "14",
        "cover_date": "2022-11-01",
        "Abstract": "This article considers a setting where embedded devices are used to acquire and classify images. Because of limited computing capacity, embedded devices rely on a parsimonious classification model with uneven accuracy. When local classification is deemed inaccurate, devices can decide to offload the image to an edge server with a more accurate but resource-intensive model. Resource constraints, e.g., network bandwidth, however, require regulating such transmissions to avoid congestion and high latency. This article investigates this offloading problem when transmissions regulation is through a token bucket, a mechanism commonly used for such purposes. The goal is to devise a lightweight, online offloading policy that optimizes an application-specific metric (e.g., classification accuracy) under the constraints of the token bucket. This article develops a policy based on a deep $Q$ -network (DQN), and demonstrates both its efficacy and the feasibility of its deployment on embedded devices. Of note is the fact that the policy can handle complex input patterns, including correlation in image arrivals and classification accuracy. The evaluation is carried out by performing image classification over a local testbed using synthetic traces generated from the ImageNet image classification benchmark. Implementation of this work is available at https://github.com/qiujiaming315/edgeml-dqn.",
        "DOI": "10.1109/TCAD.2022.3197533",
        "paper_author": "Qiu J.",
        "affiliation_name": "McKelvey School of Engineering",
        "affiliation_city": "St. Louis",
        "affiliation_country": "United States",
        "affiliation_id": "60105336",
        "affiliation_state": "MO"
    },
    {
        "paper_title": "High robustness energy management strategy of hybrid electric vehicle based on improved soft actor-critic deep reinforcement learning",
        "publication": "Energy",
        "citied_by": "52",
        "cover_date": "2022-11-01",
        "Abstract": "As a hybrid electric vehicle (HEV) key control technology, intelligent energy management strategies (EMSs) directly affect fuel consumption. Investigating the robustness of EMSs to maximize the advantages of energy savings and emission reduction in different driving environments is necessary. This article proposes a soft actor-critic (SAC) deep reinforcement learning (DRL) EMS for hybrid electric tracked vehicles (HETVs). Munchausen reinforcement learning (MRL) is adopted in the SAC algorithm, and the Munchausen SAC (MSAC) algorithm is constructed to achieve lower fuel consumption than the traditional SAC method. The prioritized experience replay (PER) is proposed to achieve more reasonable experience sampling and improve the optimization effect. To enhance the “cold start” performance, a dynamic programming (DP)-assisted training method is proposed that substantially improves the training efficiency. The proposed method optimization result is compared with the traditional SAC and deep deterministic policy gradient (DDPG) with PER through the simulation. The result shows that the proposed strategy improves both fuel consumption and possesses excellent robustness under different driving cycles.",
        "DOI": "10.1016/j.energy.2022.124806",
        "paper_author": "Sun W.",
        "affiliation_name": "Beijing Institute of Technology",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60016835",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Dynamic and heterogeneity assessment of carbon efficiency in the manufacturing industry in China: Implications for formulating carbon policies",
        "publication": "Environmental Impact Assessment Review",
        "citied_by": "26",
        "cover_date": "2022-11-01",
        "Abstract": "In 2019, the Chinese manufacturing industry (CMI) accounted for 13.46% of the world's energy consumption and 12.24% of carbon dioxide emissions. Such high emissions and pollution pose a challenge for China towards becoming carbon neutral. Through the panel data of 30 provinces, 283 key cities, and 30 sub-sectors from 2006 to 2019, combined with econometrics and machine learning, the dynamic evolution trend and spatial heterogeneity of CMI were comprehensively analyzed. The notable results are fourfold. (1) Carbon efficiency of CMI in most provinces is between 0.4881 and 0.846, which can be divided into three clusters (high, medium, and low); the difference between clusters is the primary source of the overall difference in carbon efficiency, and carbon efficiency distribution converges tend to a higher level; (2) The promotion of TC is generally greater than that of EC; both TC and EC have certain spatial agglomeration characteristics among three city clubs; (3) High energy consumption sectors shows low carbon efficiency, high technology, and emerging sectors demonstrate high carbon efficiency; TC has a greater enhancement effect on medium and high energy consumption sub-sectors, while EC demonstrates a greater promotion on low energy consumption sub-sectors. This study has implications for policy development regarding promoting carbon efficiency, carbon neutralization, and the world's carbon emission reduction.",
        "DOI": "10.1016/j.eiar.2022.106885",
        "paper_author": "Peng H.",
        "affiliation_name": "Huazhong University of Science and Technology",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60025761",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Assessing water quality for cropping management practices: A new approach for dissolved inorganic nitrogen discharged to the Great Barrier Reef",
        "publication": "Journal of Environmental Management",
        "citied_by": "5",
        "cover_date": "2022-11-01",
        "Abstract": "Applications of nitrogen (N) fertiliser to agricultural lands impact many marine and aquatic ecosystems, and improved N fertiliser management is needed to reduce these water quality impacts. Government policies need information on water quality and risk associated with improved practices to evaluate the benefits of their adoption. Policies protecting Great Barrier Reef (GBR) ecosystems are an example of this situation. We developed a simple metric for assessing the risk of N discharge from sugarcane cropping, the biggest contributor of dissolved inorganic N to the GBR. The metric, termed NiLRI, is the ratio of N fertiliser applied to crops and the cane yield achieved (i.e. kg N (t cane)−1). We defined seven classes of water quality risk using NiLRI values derived from first principles reasoning. NiLRI values calculated from (1) results of historical field experiments and (2) survey data on the management of 170,177 ha (or 53%) of commercial sugarcane cropping were compared to the classes. The NiLRI values in both the experiments and commercial crops fell into all seven classes, showing that the classes were both biophysically sensible (c.f. the experiments) and relevant to farmers’ experience. We then used machine learning to explore the association between crop management practices recorded in the surveys and associated NiLRI values. Practices that most influenced NiLRI values had little apparent direct impact on N management. They included improving fallow management and reducing tillage and compaction, practices that have been promoted for production rather than N discharge benefits. The study not only provides a metric for the change in N water quality risk resulting from adoption of improved practices, it also gives the first clear empirical evidence of the agronomic practices that could be promoted to reduce water quality risk while maintaining or improving yields of sugarcane crops grown in catchments adjacent to the GBR. Our approach has relevance to assessing the environmental risk of N fertiliser management in other countries and cropping systems.",
        "DOI": "10.1016/j.jenvman.2022.115932",
        "paper_author": "Thorburn P.J.",
        "affiliation_name": "Commonwealth Scientific and Industrial Research Organisation",
        "affiliation_city": "Canberra",
        "affiliation_country": "Australia",
        "affiliation_id": "60029470",
        "affiliation_state": "ACT"
    },
    {
        "paper_title": "Heuristic Reward Design for Deep Reinforcement Learning-Based Routing, Modulation and Spectrum Assignment of Elastic Optical Networks",
        "publication": "IEEE Communications Letters",
        "citied_by": "21",
        "cover_date": "2022-11-01",
        "Abstract": "In this letter, we study the deep reinforcement learning (DRL)-based routing, modulation and spectrum assignment problem in the elastic optical networks. We emphasize the importance of proper reward design of the DRL framework and we propose to include some heuristic information to the reward design. This introduction of human knowledge to the machine learning, is to reduce the exploration blindness of the latter and lead to more efficient learning of better policies. We make it clear that what kind of heuristic information should be included in the reward design is an open question. Specifically, we propose to consider the spectrum fragmentation level of each candidate path as the heuristic information. As a result, the DRL agent is more inclined to choose the candidate path that leads to lower spectrum fragmentation level, which is more friendly for future traffic requests. Simulation results show that the proposed heuristic reward design scheme outperforms both the simple-reward DRL based approaches and the heuristic rule-based approaches.",
        "DOI": "10.1109/LCOMM.2022.3195778",
        "paper_author": "Tang B.",
        "affiliation_name": "South China Normal University",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60005816",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "How do green energy investment, economic policy uncertainty, and natural resources affect greenhouse gas emissions? A Markov-switching equilibrium approach",
        "publication": "Environmental Impact Assessment Review",
        "citied_by": "93",
        "cover_date": "2022-11-01",
        "Abstract": "Energy efficiency has enormous potential for boosting economic development while simultaneously reducing greenhouse gas emissions. Energy efficiency improvements are widely accepted as a necessary corollary of China's decarbonisation efforts. This study analysed key factors in green energy investment and its determinants, i.e. natural resources and uncertainty of economic policy on greenhouse gas emissions, from 1987 to 2019. This work conduct a thorough empirical analysis and used innovative econometric techniques, such as the Markov-switching equilibrium correction model for long-run estimation and the kernel-based regularised least squares machine-learning approach, to establish the direction of causality for the variables in this study. The results indicate that green energy investment is significantly correlated with greenhouse gas emissions and helps to support environmental quality. However, although natural resources and economic policy uncertainty support economic growth, they are harmful to environmental quality. This conclusion implies that the Chinese government should exploit green energy investment to strengthen environmental performance as its long-term strategy.",
        "DOI": "10.1016/j.eiar.2022.106887",
        "paper_author": "Hassan S.T.",
        "affiliation_name": "Nanjing University of Information Science &amp; Technology",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60064143",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "BahiaRT Setplays Collecting Toolkit and BahiaRT Gym",
        "publication": "Software Impacts",
        "citied_by": "3",
        "cover_date": "2022-11-01",
        "Abstract": "One of the challenges in the research using machine learning is the requirement to build realistic datasets using common sense knowledge from domain experts. The BahiaRT Setplays Collecting Toolkit is a software that supports research where soccer fans or experts can watch robots playing soccer and capture situations where they want to demonstrate a better setplay to that robot team. All demonstrations were gathered in a dataset used to feed a reinforcement learning mechanism that produced a control policy for setplays selection in a robot soccer team. This software impacts many research areas such as autonomous vehicles, unmanned aerial vehicles, dataset organization, reinforcement learning, and learning from demonstration. The BahiaRT Gym is an open-source tool designed to integrate the OpenAI Gym toolkit with the RoboCup Soccer Simulation 3D server(rcssserver3d), in order to make available an easier way to create training environments for the soccer teams while also facilitating the development of other RoboCup leagues's environments as well.",
        "DOI": "10.1016/j.simpa.2022.100401",
        "paper_author": "Simōes M.A.C.",
        "affiliation_name": "Universidade do Estado da Bahia",
        "affiliation_city": "Salvador",
        "affiliation_country": "Brazil",
        "affiliation_id": "60024874",
        "affiliation_state": "BA"
    },
    {
        "paper_title": "One size fits all? Using machine learning to study heterogeneity and dominance in the determinants of early-stage entrepreneurship",
        "publication": "Journal of Business Research",
        "citied_by": "14",
        "cover_date": "2022-11-01",
        "Abstract": "Despite the vast number of studies exploring the determinants of entrepreneurship, few have been able to distinguish the relative importance of these factors. Traditional regression-based approaches, upon which such studies are based, are unable to fully capture heterogeneous and complex non-linear patterns in the determinants of entrepreneurship. To address these limitations, we adopt a novel approach, using machine learning to study heterogeneity and dominance in the social-cognitive determinants of early-stage entrepreneurship. We apply decision tree algorithms to a large-scale dataset from the Global Entrepreneurship Monitor. Our results reveal that the dominant determinants, irrespective of entrepreneurial pathway, are individual entrepreneurial self-efficacy and networks, with factors such as cultural perceptions being relatively unimportant, despite substantial attention in the literature. The results also show considerable heterogeneity in the factors contributing to entrepreneurship, highlighting the need for academics and policy makers to consider the likelihood that there is no single set of motivating factors.",
        "DOI": "10.1016/j.jbusres.2022.07.043",
        "paper_author": "Graham B.",
        "affiliation_name": "Queen's Management School",
        "affiliation_city": "Belfast",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60177646",
        "affiliation_state": "Northern Ireland"
    },
    {
        "paper_title": "Learning to accelerate globally optimal solutions to the AC Optimal Power Flow problem",
        "publication": "Electric Power Systems Research",
        "citied_by": "7",
        "cover_date": "2022-11-01",
        "Abstract": "We propose machine learning-based (ML) methods to accelerate convergence to global solutions for the AC Optimal Power Flow (AC-OPF) problem. In particular, for the non-convex AC-OPF problem, optimality-based bound tightening (OBBT) has been observed to be a very effective approach for tightening the variable domains, thus leading to tight convex relaxations that are nearly global optimum solutions. However, by construction, OBBT is computationally expensive even on medium-scaled power networks. To address this issue, we propose a novel ML-based policy to replace the exhaustive algorithm of OBBT by choosing a subset of variables whose tightening of bounds can still contribute to the best improvement of the convex relaxation of the AC-OPF problem. To this end, we leverage historical data of load profiles for a test system to learn a map between the system loading and subset selection of variables which will need to participate in the OBBT algorithm, thus enabling us to find near-global optimal solutions at faster run-times. Finally, we present detailed numerical studies on a few medium-sized benchmark instances, on which we observe up to 6.3× speed-up in OBBT run times.",
        "DOI": "10.1016/j.epsr.2022.108275",
        "paper_author": "Cengil F.",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Fayetteville",
        "affiliation_country": "United States",
        "affiliation_id": "60158796",
        "affiliation_state": "AR"
    },
    {
        "paper_title": "Carbon neutrality prediction of municipal solid waste treatment sector under the shared socioeconomic pathways",
        "publication": "Resources, Conservation and Recycling",
        "citied_by": "51",
        "cover_date": "2022-11-01",
        "Abstract": "To mitigate greenhouse gas (GHG) emissions from municipal solid waste (MSW) sector, it is crucial to describe its GHG emission patterns and propose suitable mitigation measures. Therefore, this study forecasts GHG emissions from MSW treatment by combining IPCC, logarithmic mean divisia index and machine learning models. Carbon neutrality potentials of China towards 2060 are analyzed under assumed policy scenarios and shared socioeconomic pathways (SSPs). Results showed that GHG emissions from China's MSW treatment increased nearly 37 megatons in the past decade, with incineration emissions increasing fast. Economic development was the dominant and positive driving force of MSW GHG emissions. Scenario analysis revealed that carbon neutrality from MSW treatment could be achieved only after implementing MSW classification, reducing approximately 125 megatons GHG emissions. Fossil-fueled development pathway (SSP5) will generate the most GHG emissions among SSPs. Finally, policy recommendations on priority regions, MSW treatment transition and circular economy schemes are proposed.",
        "DOI": "10.1016/j.resconrec.2022.106528",
        "paper_author": "Zhang C.",
        "affiliation_name": "Shanghai Jiao Tong University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60025084",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Multi-Agent Deep Reinforcement Learning for Voltage Control with Coordinated Active and Reactive Power Optimization",
        "publication": "IEEE Transactions on Smart Grid",
        "citied_by": "92",
        "cover_date": "2022-11-01",
        "Abstract": "The increasing penetration of distributed renewable energy resources causes voltage fluctuations in distribution networks. The controllable active and reactive power resources such as energy storage (ES) systems and electric vehicles (EVs) in active distribution networks play an important role in mitigating the voltage excursions. This paper proposes a two-timescale hybrid voltage control strategy based on a mixed-integer optimization method and multi-agent reinforcement learning (MARL) to reduce power loss and mitigate voltage violations. In the slow-timescale, the active and reactive power optimization problem involving capacitor banks (CBs), on-load tap changers (OLTC), and ES systems is formulated as a mixed-integer second-order cone programming problem. In the fast-timescale, the reactive power of smart inverters connected to solar photovoltaic systems and active power of EVs are adjusted to mitigate short-term voltage fluctuations with a MARL algorithm. Specifically, we propose an experience augmented multi-agent actor-critic (EA-MAAC) algorithm with an attention mechanism to learn high-quality control policies. The control policies are executed online in a decentralized manner. The proposed hybrid voltage control strategy is validated on an IEEE testing distribution feeder. The numerical results show that our proposed control strategy is not only sample-efficient and robust but also effective in mitigating voltage fluctuations.",
        "DOI": "10.1109/TSG.2022.3185975",
        "paper_author": "Hu D.",
        "affiliation_name": "Zhejiang University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60003970",
        "affiliation_state": "Zhejiang"
    },
    {
        "paper_title": "Toward Secure Federated Learning for IoT Using DRL-Enabled Reputation Mechanism",
        "publication": "IEEE Internet of Things Journal",
        "citied_by": "8",
        "cover_date": "2022-11-01",
        "Abstract": "Federated learning (FL) has emerged to leverage datasets from multiple devices to improve the performance of a machine learning (ML) model while providing privacy preservation for devices. The training data is collected at the devices, also known as FL workers, which collaboratively train a global learning model and share their local model updates with a central entity or server without sharing their data. However, FL can be susceptible to various adversarial attacks that target its security and privacy. In particular, the workers can upload unreliable local model updates, leading to corruption of the main FL task. Workers may intentionally contribute unreliable local updates by launching poisoning attacks or unintentionally by updating low-quality models caused by high device mobility, limited device resources, or unstable network connection. Consequently, identifying reliable and trustworthy workers becomes critical for FL security. In this article, the concept of reputation is adopted as a metric to evaluate workers' reliability and trustworthiness. In addition, deep reinforcement learning (DRL)-based reputation mechanism is proposed for optimal selection and evaluation of reliable FL workers. Due to the dynamic nature of worker behavior in the FL environment, the DRL-based algorithm deep deterministic policy gradient (DDPG) is employed to improve the FL model accuracy and stability. We compare the performance of our proposed method with a conventional reputation method and deep $Q$ -networks (DQNs)-based reputation method. Our simulation results demonstrate that our proposed method can improve FL accuracy by more than 30% under various scenarios and achieves better convergence than the other methods.",
        "DOI": "10.1109/JIOT.2022.3184812",
        "paper_author": "Al-Maslamani N.M.",
        "affiliation_name": "Hamad Bin Khalifa University, College of Science and Engineering",
        "affiliation_city": "Doha",
        "affiliation_country": "Qatar",
        "affiliation_id": "60113885",
        "affiliation_state": "Ad-Dawhah"
    },
    {
        "paper_title": "A multi-action deep reinforcement learning framework for flexible Job-shop scheduling problem",
        "publication": "Expert Systems with Applications",
        "citied_by": "133",
        "cover_date": "2022-11-01",
        "Abstract": "This paper presents an end-to-end deep reinforcement framework to automatically learn a policy for solving a flexible Job-shop scheduling problem (FJSP) using a graph neural network. In the FJSP environment, the reinforcement agent needs to schedule an operation belonging to a job on an eligible machine among a set of compatible machines at each timestep. This means that an agent needs to control multiple actions simultaneously. Such a problem with multi-actions is formulated as a multiple Markov decision process (MMDP). For solving the MMDPs, we propose a multi-pointer graph networks (MPGN) architecture and a training algorithm called multi-Proximal Policy Optimization (multi-PPO) to learn two sub-policies, including a job operation action policy and a machine action policy to assign a job operation to a machine. The MPGN architecture consists of two encoder-decoder components, which define the job operation action policy and the machine action policy for predicting probability distributions over different operations and machines, respectively. We introduce a disjunctive graph representation of FJSP and use a graph neural network to embed the local state encountered during scheduling. The computational experiment results show that the agent can learn a high-quality dispatching policy and outperforms handcrafted heuristic dispatching rules in solution quality and meta-heuristic algorithm in running time. Moreover, the results achieved on random and benchmark instances demonstrate that the learned policies have a good generalization performance on real-world instances and significantly larger scale instances with up to 2000 operations.",
        "DOI": "10.1016/j.eswa.2022.117796",
        "paper_author": "Lei K.",
        "affiliation_name": "Southwest Jiaotong University",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China",
        "affiliation_id": "60010421",
        "affiliation_state": "Sichuan"
    },
    {
        "paper_title": "HETEROGENEOUS TREATMENT EFFECTS OF NUDGE AND REBATE: CAUSAL MACHINE LEARNING IN A FIELD EXPERIMENT ON ELECTRICITY CONSERVATION",
        "publication": "International Economic Review",
        "citied_by": "4",
        "cover_date": "2022-11-01",
        "Abstract": "This study investigates the different impacts of monetary and nonmonetary incentives on energy-saving behaviors using a field experiment conducted in Japan. We find that the average reduction in electricity consumption from the rebate is 4%, whereas that from the nudge is not significantly different from zero. Applying a novel machine learning method for causal inference (causal forest) to estimate heterogeneous treatment effects at the household level, we demonstrate that the nudge intervention's treatment effects generate greater heterogeneity among households. These findings suggest that selective targeting for treatment increases the policy efficiency of monetary and nonmonetary interventions.",
        "DOI": "10.1111/iere.12589",
        "paper_author": "Murakami K.",
        "affiliation_name": "Kobe University",
        "affiliation_city": "Kobe",
        "affiliation_country": "Japan",
        "affiliation_id": "60011418",
        "affiliation_state": "Hyogo"
    },
    {
        "paper_title": "Machine Learning for Hypertension Prediction: a Systematic Review",
        "publication": "Current Hypertension Reports",
        "citied_by": "57",
        "cover_date": "2022-11-01",
        "Abstract": "Purpose of Review: To provide an overview of the literature regarding the use of machine learning algorithms to predict hypertension. A systematic review was performed to select recent articles on the subject. Recent Findings: The screening of the articles was conducted using a machine learning algorithm (ASReview). A total of 21 articles published between January 2018 and May 2021 were identified and compared according to variable selection, train-test split, data balancing, outcome definition, final algorithm, and performance metrics. Overall, the articles achieved an area under the ROC curve (AUROC) between 0.766 and 1.00. The algorithms most frequently identified as having the best performance were support vector machines (SVM), extreme gradient boosting (XGBoost), and random forest. Summary: Machine learning algorithms are a promising tool to improve preventive clinical decisions and targeted public health policies for hypertension. However, technical factors such as outcome definition, availability of the final code, predictive performance, explainability, and data leakage need to be consistently and critically evaluated.",
        "DOI": "10.1007/s11906-022-01212-6",
        "paper_author": "Silva G.F.S.",
        "affiliation_name": "Universidade de São Paulo",
        "affiliation_city": "Sao Paulo",
        "affiliation_country": "Brazil",
        "affiliation_id": "60008088",
        "affiliation_state": "SP"
    },
    {
        "paper_title": "Multi-step air quality index forecasting via data preprocessing, sequence reconstruction, and improved multi-objective optimization algorithm",
        "publication": "Journal of Forecasting",
        "citied_by": "2",
        "cover_date": "2022-11-01",
        "Abstract": "This research presents a hybrid model for multi-step, interval forecasting of air quality indices. An efficient preprocessing module is applied to split the raw data into various sub-series, and the optimal mode of data input is determined through feature selection. A multi-objective optimization algorithm is proposed to tune the parameters of kernel extreme learning machine to achieve high accuracy and stability. An evaluation with several error criteria, benchmark models, and critique is conducted using three daily air quality index datasets from three cities of China. Empirical results indicate that the developed model achieves superior predictions of air quality indices, which may be useful in policies for mitigating air pollution.",
        "DOI": "10.1002/for.2872",
        "paper_author": "Wang Y.",
        "affiliation_name": "Hainan University",
        "affiliation_city": "Haikou",
        "affiliation_country": "China",
        "affiliation_id": "60017716",
        "affiliation_state": "Hainan"
    },
    {
        "paper_title": "Learning-based model predictive current control for synchronous machines: An LSTM approach",
        "publication": "European Journal of Control",
        "citied_by": "12",
        "cover_date": "2022-11-01",
        "Abstract": "In this work, a data-driven model predictive control (MPC) approach for the current control of synchronous machines is presented. The model of the motor is represented via a long-short term memory (LSTM) neural network (NN). The model is obtained purely from collected data and doesn't include any physical knowledge. As an online optimization using the obtained data-driven model is not easily implementable in the available sampling time, the neural model is used to solve an MPC problem offline. Finally, the control policy is learned via another computationally implementable NN that runs in real-time as a current controller. The proposed data-driven MPC controller is tested experimentally, and is bench-marked against MPC schemes that incorporate the well-known physically-based first-principles linear and nonlinear model1 of the machine.",
        "DOI": "10.1016/j.ejcon.2022.100663",
        "paper_author": "Hammoud I.",
        "affiliation_name": "IAV GmbH Ingenieurgesellschaft Auto und Verkehr",
        "affiliation_city": "Berlin",
        "affiliation_country": "Germany",
        "affiliation_id": "60085259",
        "affiliation_state": "Berlin"
    },
    {
        "paper_title": "Global Groundwater Solute Composition and Concentrations",
        "publication": "Groundwater",
        "citied_by": "10",
        "cover_date": "2022-11-01",
        "Abstract": "Informed analysis of policies related to food security, global climate change, wetland ecology, environmental nutrient flux, element cycling, groundwater weathering, continental denudation, human health, and others depends to a large extent on quantitative estimates of solute mass fluxes into and out of all global element pools including the enigmatic global aquifer systems. Herein for the first time, we proffer the mean global solute concentration of all major and selected minor and trace solutes in the active groundwater that represents 99% of liquid fresh water on Earth. Concentrations in this significant element pool have yielded to a geospatial machine learning kNN-nearest neighbors' algorithm with numerous geospatial predictors utilizing a large new lithology/climate/aquifer age/elevation based solute database. The predicted concentrations are consistent with traditional solute ratios, concentrations, and thermodynamic saturation indices.",
        "DOI": "10.1111/gwat.13205",
        "paper_author": "Wood W.W.",
        "affiliation_name": "Michigan State University",
        "affiliation_city": "East Lansing",
        "affiliation_country": "United States",
        "affiliation_id": "60031707",
        "affiliation_state": "MI"
    },
    {
        "paper_title": "Probability estimation via policy restrictions, convexification, and approximate sampling",
        "publication": "Mathematical Programming",
        "citied_by": "3",
        "cover_date": "2022-11-01",
        "Abstract": "This paper develops various optimization techniques to estimate probability of events where the optimal value of a convex program, satisfying certain structural assumptions, exceeds a given threshold. First, we relate the search of affine/polynomial policies for the robust counterpart to existing relaxation hierarchies in MINLP (Lasserre in Proceedings of the international congress of mathematicians (ICM 2018), 2019; Sherali and Adams in A reformulation–linearization technique for solving discrete and continuous nonconvex problems, Springer, Berlin). Second, we leverage recent advances in Dworkin et al. (in: Kaski, Corander (eds) Proceedings of the seventeenth international conference on artificial intelligence and statistics, Proceedings of machine learning research, PMLR, Reykjavik, 2014), Gawrychowski et al. (in: ICALP, LIPIcs, Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2018) and Rizzi and Tomescu (Inf Comput 267:135–144, 2019) to develop techniques to approximately compute the probability binary random variables from Bernoulli distributions belong to a specially-structured union of sets. Third, we use convexification, robust counterpart, and chance-constrained optimization techniques to cover the event set of interest with such set unions. Fourth, we apply our techniques to the network reliability problem, which quantifies the probability of failure scenarios that cause network utilization to exceed one. Finally, we provide preliminary computational evaluation of our techniques on test instances for network reliability.",
        "DOI": "10.1007/s10107-022-01823-6",
        "paper_author": "Chandra A.",
        "affiliation_name": "Mitch Daniels School of Business",
        "affiliation_city": "West Lafayette",
        "affiliation_country": "United States",
        "affiliation_id": "60116251",
        "affiliation_state": "IN"
    },
    {
        "paper_title": "Dual-Space Co-Evolutionary Memetic Algorithm for Scheduling Hybrid Differentiation Flowshop With Limited Buffer Constraints",
        "publication": "IEEE Transactions on Systems, Man, and Cybernetics: Systems",
        "citied_by": "20",
        "cover_date": "2022-11-01",
        "Abstract": "This article proposes a novel and efficient dual-space co-evolutionary memetic algorithm (DCMA) to tackle a practical hybrid differentiation flowshop scheduling problem with limited buffer constraints. In this scheduling problem, jobs are divided into different types and each job consists of multiple parts. The manufacturing of a job involves three stages: 1) parts fabrication on first-stage parallel machines; 2) parts assembly on second-stage single machine; and 3) job differentiation on one of third-stage dedicated machines. Due to the assembly operation and limited buffers between adjacent stages, deadlock and blocking will occur. We formulate the problem and present a deadlock handling policy to guarantee all schedules feasible. Then, we propose the DCMA metaheuristic to approximate the optimal solutions in acceptable time. Global exploration of DCMA is performed by a hybrid of three parts: 1) a continuous optimizer to be executed in continuous search space; 2) a discrete optimizer to be executed in combinatorial solution space; and 3) a meta-Lamarckian learning-based selection mechanism for adaptively determining that which optimizer is more suitable for current global exploration campaign. To balance exploration and exploitation, three problem-special local search strategies are presented, which collaborates with each other and are adaptively started to avoid high computational costs. The effect of parameter setting on DCMA is checked by the Taguchi method of design of experiment. The computational experiments demonstrate the effectiveness of DCMA special designs, and show that DCMA performs better than the existing algorithms for the considered problem.",
        "DOI": "10.1109/TSMC.2021.3102658",
        "paper_author": "Zhang G.",
        "affiliation_name": "Hebei Agricultural University",
        "affiliation_city": "Baoding",
        "affiliation_country": "China",
        "affiliation_id": "60015407",
        "affiliation_state": "Hebei"
    },
    {
        "paper_title": "Classifying papers into subfields using Abstracts, Titles, Keywords and KeyWords Plus through pattern detection and optimization procedures: An application in Physics",
        "publication": "Journal of the Association for Information Science and Technology",
        "citied_by": "8",
        "cover_date": "2022-11-01",
        "Abstract": "Classifying papers according to the fields of knowledge is critical to clearly understand the dynamics of scientific (sub)fields, their leading questions, and trends. Most studies rely on journal categories defined by popular databases such as WoS or Scopus, but some experts find that those categories may not correctly map the existing subfields nor identify the subfield of a specific article. This study addresses the classification problem using data from each paper (Abstract, Title, Keywords, and the KeyWords Plus) and the help of experts to identify the existing subfields and journals exclusive of each subfield. These “exclusive journals” are critical to obtain, through a pattern detection procedure that uses machine learning techniques (from software NVivo), a list of the frequent terms that are specific to each subfield. With that list of terms and with the help of optimization procedures, we can identify to which subfield each paper most likely belongs. This study can contribute to support scientific policy-makers, funding, and research institutions—via more accurate academic performance evaluations—, to support editors in their tasks to redefine the scopes of journals, and to support popular databases in their processes of refining categories.",
        "DOI": "10.1002/asi.24655",
        "paper_author": "Pech G.",
        "affiliation_name": "Universidade do Estado do Rio de Janeiro",
        "affiliation_city": "Rio de Janeiro",
        "affiliation_country": "Brazil",
        "affiliation_id": "60005499",
        "affiliation_state": "RJ"
    },
    {
        "paper_title": "Crash mitigation controller for unavoidable T-bone collisions using reinforcement learning",
        "publication": "ISA Transactions",
        "citied_by": "13",
        "cover_date": "2022-11-01",
        "Abstract": "T-bone collision constitutes an emergency crash scenario that results in casualties and heavy losses; it is an excessively complicated scenario that cannot be handled by conventional control systems. This paper presents an innovative crash mitigation controller for application during unavoidable T-bone collisions to expand the vehicle-maneuverability envelope and minimize crash severity; this controller combines prior knowledge using an optimum expert-behavior policy and drift-operation mechanism based on an improved reinforcement learning algorithm, TD3. Vehicle and tire modeling are performed considering the nonlinear and coupled dynamics characteristics to improve control accuracy. Unlike conventional control systems and other reinforcement learning algorithms, the proposed controller realizes the optimum crash mitigation effect under different scenarios. It is expected to afford autonomous driving technologies with enhanced operating capabilities under extreme conditions.",
        "DOI": "10.1016/j.isatra.2022.03.021",
        "paper_author": "Hou X.",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60025278",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Before and beyond trust: reliance in medical AI",
        "publication": "Journal of Medical Ethics",
        "citied_by": "46",
        "cover_date": "2022-11-01",
        "Abstract": "Artificial intelligence (AI) is changing healthcare and the practice of medicine as data-driven science and machine-learning technologies, in particular, are contributing to a variety of medical and clinical tasks. Such advancements have also raised many questions, especially about public trust. As a response to these concerns there has been a concentrated effort from public bodies, policy-makers and technology companies leading the way in AI to address what is identified as a \"public trust deficit\". This paper argues that a focus on trust as the basis upon which a relationship between this new technology and the public is built is, at best, ineffective, at worst, inappropriate or even dangerous, as it diverts attention from what is actually needed to actively warrant trust. Instead of agonising about how to facilitate trust, a type of relationship which can leave those trusting vulnerable and exposed, we argue that efforts should be focused on the difficult and dynamic process of ensuring reliance underwritten by strong legal and regulatory frameworks. From there, trust could emerge but not merely as a means to an end. Instead, as something to work in practice towards; that is, the deserved result of an ongoing ethical relationship where there is the appropriate, enforceable and reliable regulatory infrastructure in place for problems, challenges and power asymmetries to be continuously accounted for and appropriately redressed.",
        "DOI": "10.1136/medethics-2020-107095",
        "paper_author": "Kerasidou C.",
        "affiliation_name": "Department of Sociology, Lancaster University",
        "affiliation_city": "Lancaster",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60117756",
        "affiliation_state": "Lancashire"
    },
    {
        "paper_title": "Mapping from proximity traces to socio-spatial behaviours and student progression at the school",
        "publication": "British Journal of Educational Technology",
        "citied_by": "10",
        "cover_date": "2022-11-01",
        "Abstract": "Identifying students facing difficulties and providing them with timely support is one of the educator's key responsibilities. Yet, this task is becoming increasingly challenging as the complexity of physical learning spaces grows, along with the emergence of novel educational technologies and classroom designs. There has been substantial research and development work focused on identifying student social behaviours in digital platforms (eg, the learning management system) as predictors of academic progression. However, little work has investigated such relationships in physical learning spaces. This study explores the potential of using wearable trackers for the early detection of low-progress students based on their social and spatial (socio-spatial) behaviours at the school. Positioning data from 98 primary school students and six teachers were automatically captured over a period of eight weeks. Fourteen socio-spatial behavioural features were extracted and processed using a set of machine learning classifiers to model students’ learning progression. Results illustrate the potential of prospectively identifying low-progress students from these features and the importance of adapting classroom learning analytics to differences in pedagogical designs. Practitioner notes What is already known about this topic Learning analytics research on predicting students’ academic progression is emerging in both digital and physical learning spaces. Students’ social behaviours in learning activities is a key factor in predicting their academic progression. Emerging sensing technologies can provide opportunities to study students’ real-time social behaviours in physical learning spaces. What this paper adds Fourteen progression-related socio-spatial behavioural features are extracted from students’ physical (x-y) positioning traces. Predictive learning analytics that achieved 81% accuracy in prospectively identifying low-progress students from their real-time socio-spatial behaviours. Empirical evidence to support the need for classroom learning analytics to have instructional sensitivity (ie, be calibrated according to the learning design). Implications for practice and/or policy Sensing technologies and machine learning algorithms can be used to capture and generate valuable insights about higher-order learning constructs (eg, performance and collaboration) from students' physical positioning traces in classrooms. Researchers and practitioners should be cautious with generalised classification algorithms and predictive learning analytics that do not account for the pedagogical differences between different subjects or learning designs. Researchers and practitioners should consider the potentially unforeseen ethical issues that can emerge in using sensing technologies and predictive learning analytics in authentic, physical classroom settings.",
        "DOI": "10.1111/bjet.13203",
        "paper_author": "Yan L.",
        "affiliation_name": "Monash University",
        "affiliation_city": "Melbourne",
        "affiliation_country": "Australia",
        "affiliation_id": "60019578",
        "affiliation_state": "VIC"
    },
    {
        "paper_title": "Adaptive DRL-Based Virtual Machine Consolidation in Energy-Efficient Cloud Data Center",
        "publication": "IEEE Transactions on Parallel and Distributed Systems",
        "citied_by": "57",
        "cover_date": "2022-11-01",
        "Abstract": "The dramatic increasing of data and demands for computing capabilities may result in excessive use of resources in cloud data centers, which not only causes the raising of energy consumption, but also leads to the violation of Service Level Agreement (SLA). Dynamic consolidation of virtual machines (VMs) is proven to be an efficient way to tackle this issue. In this paper, we present an Adaptive Deep Reinforcement Learning (DRL)-based Virtual Machine Consolidation (ADVMC) framework for energy-efficient cloud data centers. ADVMC has two phases. In the first phase, Influence Coefficient is introduced to measure the impact of a VM on producing host overload, and a dynamic Influence Coefficient-based VM selection algorithm (ICVMS) is proposed to preferentially choose those VMs with the greatest impact for migration in order to remove the excessive workloads of the overloaded host quickly and accurately. In the second phase, a Prediction Aware DRL-based VM placement method (PADRL) is further proposed to automatically find suitable hosts for VMs to be migrated, in which a state prediction network is designed based on LSTM to provide DRL-based model more reasonable environment states so as to accelerate the convergence of DRL. Simulation experiments on the real-world workload provided by Google Cluster Trace have shown that our ADVMC approach can largely cut down system energy consumption and reduce SLA violation of users as compared to many other VM consolidation policies.",
        "DOI": "10.1109/TPDS.2022.3147851",
        "paper_author": "Zeng J.",
        "affiliation_name": "Beijing Jiaotong University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60022381",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Tools and Techniques for Collection and Analysis of Internet-of-Things malware: A systematic state-of-art review",
        "publication": "Journal of King Saud University - Computer and Information Sciences",
        "citied_by": "23",
        "cover_date": "2022-11-01",
        "Abstract": "IoT devices which include wireless sensors, software, actuators, and computer devices operated through the Internet, enable the transfer of data among objects or people automatically without human intervention. Since these devices are resource constraint embedded devices, security policies are not implemented adequately upon these devices. The connectivity with the Internet, diversity of hardware, varied operating platforms, and surge in attack surface increases the target space for malicious cyber actors. The threat probability increases substantially since the attacker takes advantage of less secure, vulnerable devices to perform the massive-scale attack on the critical infrastructure. It has been observed that the majority of embedded IoT devices operate upon Linux-flavoured operating environments. This paper reviews the Linux-based IoT malware analysis techniques and tools employed for malware detection, analysis, and classification. Various threat data collection methods have been discussed at length and a thorough study of tools and techniques used in static and dynamic analysis of the Linux malware has been provided. A review of the machine learning methods developed using discrete features to classify the malicious program is one of the essential components of this paper. The paper concluded with a discussion on various open issues and challenges that need to be addressed by the research community at large.",
        "DOI": "10.1016/j.jksuci.2021.12.016",
        "paper_author": "Madan S.",
        "affiliation_name": "Punjab Engineering College (Deemed to be University)",
        "affiliation_city": "Chandigarh",
        "affiliation_country": "India",
        "affiliation_id": "60022458",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Adaptive, Optimal, Virtual Synchronous Generator Control of Three-Phase Grid-Connected Inverters Under Different Grid Conditions - An Adaptive Dynamic Programming Approach",
        "publication": "IEEE Transactions on Industrial Informatics",
        "citied_by": "28",
        "cover_date": "2022-11-01",
        "Abstract": "This article proposes an adaptive, optimal, data-driven control approach based on reinforcement learning and adaptive dynamic programming to the three-phase grid-connected inverter employed in virtual synchronous generators (VSGs). This article takes into account unknown system dynamics and different grid conditions, including balanced/unbalanced grids, voltage drop/sag, and weak grids. The proposed method is based on value iteration, which does not rely on an initial admissible control policy for learning. Considering the premise that the VSG control should stabilize the closed-loop dynamics, the VSG outputs are optimally regulated through the adaptive, optimal control strategy proposed in this article. Comparative simulations and experimental results validate the proposed method's effectiveness and reveal its practicality and implementation.",
        "DOI": "10.1109/TII.2021.3138893",
        "paper_author": "Wang Z.",
        "affiliation_name": "Fuzhou Institute of Technology",
        "affiliation_city": "Fuzhou",
        "affiliation_country": "China",
        "affiliation_id": "124621326",
        "affiliation_state": "Fujian"
    },
    {
        "paper_title": "AI-Enabled Automation for Completeness Checking of Privacy Policies",
        "publication": "IEEE Transactions on Software Engineering",
        "citied_by": "34",
        "cover_date": "2022-11-01",
        "Abstract": "Technological advances in information sharing have raised concerns about data protection. Privacy policies contain privacy-related requirements about how the personal data of individuals will be handled by an organization or a software system (e.g., a web service or an app). In Europe, privacy policies are subject to compliance with the General Data Protection Regulation (GDPR). A prerequisite for GDPR compliance checking is to verify whether the content of a privacy policy is complete according to the provisions of GDPR. Incomplete privacy policies might result in large fines on violating organization as well as incomplete privacy-related software specifications. Manual completeness checking is both time-consuming and error-prone. In this paper, we propose AI-based automation for the completeness checking of privacy policies. Through systematic qualitative methods, we first build two artifacts to characterize the privacy-related provisions of GDPR, namely a conceptual model and a set of completeness criteria. Then, we develop an automated solution on top of these artifacts by leveraging a combination of natural language processing and supervised machine learning. Specifically, we identify the GDPR-relevant information content in privacy policies and subsequently check them against the completeness criteria. To evaluate our approach, we collected 234 real privacy policies from the fund industry. Over a set of 48 unseen privacy policies, our approach detected 300 of the total of 334 violations of some completeness criteria correctly, while producing 23 false positives. The approach thus has a precision of 92.9% and recall of 89.8%. Compared to a baseline that applies keyword search only, our approach results in an improvement of 24.5% in precision and 38% in recall.",
        "DOI": "10.1109/TSE.2021.3124332",
        "paper_author": "Amaral O.",
        "affiliation_name": "University of Luxembourg",
        "affiliation_city": "Esch-sur-Alzette",
        "affiliation_country": "Luxembourg",
        "affiliation_id": "60072562",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "BEHAVE: Behavior-Aware, Intelligent and Fair Resource Management for Heterogeneous Edge-IoT Systems",
        "publication": "IEEE Transactions on Mobile Computing",
        "citied_by": "6",
        "cover_date": "2022-11-01",
        "Abstract": "Edge computing is an emerging solution to support the future Internet of Things (IoT) applications that are delay-sensitive, processing-intensive or that require closer intelligence. Machine intelligence and data-driven approaches are envisioned to build future Edge-IoT systems that satisfy IoT devices' demands for edge resources. However, significant challenges and technical barriers exist which complicate the resource management for such Edge-IoT systems. IoT devices running various applications can demonstrate a wide range of behaviors in the devices' resource demand that are extremely difficult to manage. In addition, the management of multidimensional resources fairly and efficiently by the edge in such a setting is a challenging task. In this paper, we develop a novel data-driven resource management framework named BEHAVE that intelligently and fairly allocates edge resources to heterogeneous IoT devices with consideration of their behavior of resource demand (BRD). BEHAVE aims to holistically address the management technical barriers by: 1) building an efficient scheme for modeling and assessment of the BRD of IoT devices based on their resource requests and resource usage; 2) expanding a new Rational, Fair, and Truthful Resource Allocation (RFTA) model that binds the devices' BRD and resource allocation to achieve fair allocation and encourage truthfulness in resource demand; and 3) developing an enhanced deep reinforcement learning (EDRL) scheme to achieve the RFTA goals. The evaluation results demonstrate BEHAVE's capability to analyze the IoT devices' BRD and adjust its resource management policy accordingly.",
        "DOI": "10.1109/TMC.2021.3068632",
        "paper_author": "Alqerm I.",
        "affiliation_name": "University of Missouri-St. Louis",
        "affiliation_city": "St. Louis",
        "affiliation_country": "United States",
        "affiliation_id": "60030171",
        "affiliation_state": "MO"
    },
    {
        "paper_title": "Understanding dialogue for human communication",
        "publication": "Handbook of Cognitive Mathematics",
        "citied_by": "1",
        "cover_date": "2022-10-31",
        "Abstract": "Dialogue is a peculiar activity of humans and a crucial characteristic of human cognition. It is not surprising that dialogue have been investigated, under different perspectives, by linguists, cognitive scientists, and philosophers and, in the last decades, by computer scientists. This chapter shows the progress achieved in computational linguistics to design formal models of dialogues and exploit them in human-machine systems. We highlight that collaborative dialogues follow sequences of turns characterized by speech acts and that they show an internal coherence based on conversational goals. Analysis carried on dialogue collections reveals the importance of modeling mixed-initiative schema, various types of subdialogues, and grounding among interlocutors, as they help to achieve the speakers' communicative goals. On the computational side, both knowledge-driven and machine learning technologies are nowadays used to model a pipeline of dialogue components, particularly for task-oriented situations, including automatic speech recognition, utterance understanding, dialogue state tracking, dialogue policy making, and response generation. In recent years, research on dialogue systems has moved toward the so-called conversational AI, which takes advantage of the power of neural architectures to induce models from annotated dialogues. Neural models have achieved state-of-the-art performance, and end-to-end solutions are now proposed in place of traditional dialogue pipelines. However, we argue that current models are applied to relatively narrow tasks and still scratch the surface of capturing human collaborative dialogues' effectiveness and cognitive abilities.",
        "DOI": "10.1007/978-3-031-03945-4_20",
        "paper_author": "Magnini B.",
        "affiliation_name": "Bruno Kessler Foundation",
        "affiliation_city": "Trento",
        "affiliation_country": "Italy",
        "affiliation_id": "60083112",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Predicting happiness levels of European immigrants and natives: An application of Artificial Neural Network and Ordinal Logistic Regression",
        "publication": "Frontiers in Psychology",
        "citied_by": "1",
        "cover_date": "2022-10-31",
        "Abstract": "The main purpose of this paper is to investigate the happiness factors and assess the performance of machine learning techniques on predicting the happiness levels of European immigrants and natives. Two types of machine learning methods, Ordinal Logistic Regression (OLR) and Artificial Neural Network (ANN), are employed for analytical modeling. Our results with a total sample size of 196,724 respondents from nine rounds of the European Social Survey (ESS) indicate that the determinants of happiness for immigrants and natives are significantly inconsistent. Therefore, variables should be specifically selected to predict the happiness levels of these two different groups. The sensitivity analysis shows that satisfaction with life, subjective general health, and the highest level of education are the three most prominent determinants that contribute to the happiness of immigrants and natives. The overall accuracies of OLR and ANN baseline models are >80%. This can be further improved by building models for each individual country. The application of OLR and ANN implies that machine learning algorithms can be a useful tool for predicting happiness levels. The greater knowledge of migration and happiness will allow us to better understand the decision-making processes and construct more effective policies.",
        "DOI": "10.3389/fpsyg.2022.1012796",
        "paper_author": "Chen S.",
        "affiliation_name": "Guangzhou City University of Technology",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China",
        "affiliation_id": "128263351",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Amalgamation of deep learning in healthcare systems",
        "publication": "Deep Learning for Healthcare Decision Making",
        "citied_by": "1",
        "cover_date": "2022-10-28",
        "Abstract": "Deep learning's massive powers are transforming healthcare. In recent years, AI and machine learning have grown in popularity and acceptability. The situation became much more convoluted when the COVID-19 outbreak broke out. During the crisis, we witnessed a rapid digital renovation and the adoption of disruptive technology across different industries. Healthcare was one of the potential sectors that gained many benefits from deploying disruptive technologies. Artificial intelligence, machine learning, and deep learning have all become the most vital mechanisms of the business. Deep learning had a significant influence in healthcare, allowing the industry to progress patient monitoring and diagnosis. Drug development, medical imaging and diagnostics, personalized treatments, and patient monitoring improved the health record management, health insurance, and fraud detection. These are some of the most ground-breaking solicitations of deep learning in healthcare. Deep learning and machine learning models can process and analyze various medical and healthcare data, both structured and unstructured. Document classification and maintaining up-to-date health records might become manually difficult. As a result, smart health records can be well-maintained using machine learning with its subsection deep learning. With the advent of telemedicine, wearable, and remote patient monitoring, there is now abundant real-time data on health and deep learning which help in perceptively monitoring the patients and predict risks. Deep learning can efficiently detect insurance frauds and predict future risks. Health insurance benefactors are also an advantage if they use deep learning because the models can predict future trends and behavior to recommend smart insurance policies to the clients. Natural language processing (NLP) forces deep learning algorithms for classification and identification. These two technologies can be used in recognizing and categorizing health data and can also be leveraged to develop chatbots and voice bots. In the current scenario of telehealth, chatbots play a crucial role. It makes the interface with patients. This chapter provides a detail summary about the role of artificial intelligence, machine learning, and deep learning in extemporizing the technological improvements in healthcare. It also gives a brief introduction about the various deep learning models, spots out some of the important radiological applications of deep learning algorithms, and finally explains the various applications of deep learning in healthcare.",
        "DOI": "NA",
        "paper_author": "Gnanasankaran N.",
        "affiliation_name": "Thiagarajar College",
        "affiliation_city": null,
        "affiliation_country": "India",
        "affiliation_id": "126066046",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Health informatics system using machine learning techniques",
        "publication": "Deep Learning for Healthcare Decision Making",
        "citied_by": "0",
        "cover_date": "2022-10-28",
        "Abstract": "A global pandemic is the cause of concern for humanity. The data collection and their analytics are a critical part of research and clinical studies for decision-making activities in the healthcare sector. Healthcare informatics systems and analytics (HCI&A) is a rapidly emerging technology in the medical domain that could be explored for analyzing pandemics like coronavirus disease 2019 (COVID-19). The ethical, legal, and privacy issues to be considered during data collection for research activities. Data governance and data stewardship are required to be addressed during interoperability and interpretation while sharing and reusing the data in collaborative research. The sharing of comprehensive records of clinical data collected by EHRs, also known as electronic health records, to be stored and analyzed on a time-to-time basis. The emerging area of information technology, represented by big data and artificial intelligence (AI) technology, has been widely studied in recent circumstances like COVID-19 for pandemic management. The possibility of using machine learning is explored for better predictive diagnostics and treatment. This chapter discusses the application of artificial intelligence in pandemic management including prevention, diagnosis, treatment, and also critical policy decisions in the COVID-19 pandemic. The methods to collect the digital data of health records are categorized along with few constraints as most of the electronic records related to clinical and epidemiological data are obtained through a shared database such as national and international collaborative informatics infrastructure. The necessity of digital technologies for pandemic emergencies including medical infrastructure reorganization and data workflow model is highlighted. A comparative study of different machine learning models is discussed in the subsequent sections. The digital healthcare informatics envisage a decentralized network architecture and better privacy and security such as blockchain and heterogeneous data collection with machine learning capability are also emphasized.",
        "DOI": "NA",
        "paper_author": "Rajendran S.",
        "affiliation_name": "R.V.College of Engineering",
        "affiliation_city": "Bengaluru",
        "affiliation_country": "India",
        "affiliation_id": "60070217",
        "affiliation_state": "KA"
    },
    {
        "paper_title": "FarmCan: a physical, statistical, and machine learning model to forecast crop water deficit for farms",
        "publication": "Hydrology and Earth System Sciences",
        "citied_by": "3",
        "cover_date": "2022-10-27",
        "Abstract": "In the coming decades, a changing climate, the loss of high-quality land, the slowing in the annual yield of cereals, and increasing fertilizer use indicate that better agricultural water management strategies are needed. In this study, we designed FarmCan, a novel, robust remote sensing and machine learning (ML) framework to forecast farms' needed daily crop water quantity or needed irrigation (NI). We used a diverse set of simulated and observed near-real-time (NRT) remote sensing data coupled with a random forest (RF) algorithm and inputs about farm-specific situations to predict the amount and timing of evapotranspiration (ET), potential ET (PET), soil moisture (SM), and root zone soil moisture (RZSM). Our case study of four farms in the Canadian Prairies Ecozone (CPE) shows that 8 d composite precipitation (P) has the highest correlation with changes (Δ) of RZSM and SM. In contrast, 8 d PET and 8 d ET do not offer a strong correlation with 8 d P. Using R2, root mean square error (RMSE), and Kling-Gupta efficiency (KGE) indicators, our algorithm could reasonably calculate daily NI up to 14 d in advance. From 2015 to 2020, the R2 values between predicted and observed 8 d ET and 8 d PET were the highest (80 % and 54 %, respectively). The 8 d NI also had an average R2 of 68%. The KGE of the 8 d ET and 8 d PET in four study farms showed an average of 0.71 and 0.50, respectively, with an average KGE of 0.62. FarmCan can be used in any region of the world to help stakeholders make decisions during prolonged periods of drought or waterlogged conditions, schedule cropping and fertilization, and address local government policy concerns. Copyright:",
        "DOI": "10.5194/hess-26-5373-2022",
        "paper_author": "Sadri S.",
        "affiliation_name": "Global Institute for Water Security",
        "affiliation_city": "Saskatoon",
        "affiliation_country": "Canada",
        "affiliation_id": "60189135",
        "affiliation_state": "SK"
    },
    {
        "paper_title": "BIM leadership theory for organisational BIM transformation",
        "publication": "Frontiers in Built Environment",
        "citied_by": "5",
        "cover_date": "2022-10-26",
        "Abstract": "Construction firms are struggling to stay in business as a result of BIM’s new and compelling business model and potential. As a result, businesses must adapt their current operations to the BIM paradigm by developing new organisational leadership capabilities. While such BIM leadership capacity is critical in designing, advancing, and driving a competitive and successful BIM transformation, BIM researchers have largely ignored it. Thus, this research explores the leadership capacities to drive BIM transformation in construction organisations in order to determine whether it will provide a leadership model as a solution to the leadership demands of organisations undergoing BIM transformation. The study design was a confirmatory sequential mixed methods research strategy that included a theoretical framework established through literature synthesis, qualitative grounded theory, and artificial intelligence (AI)-based modelling. Following the GT analysis, leadership capacity to drive organisational BIM transformation equates to the capability to develop a BIM-friendly leadership orientation, build a BIM-focused leadership procedure, perform BIM-related leadership responsibilities, generate a BIM-enabled leadership environment, and reach maturity within the realms of these strengths. The overall results of the AI-based modelling demonstrated that the acceptable capacity needed by the leadership that is pivoting organisational BIM transformation is the capability to coordinate functions, individuals, and transition alignment; produce BIM-related policies and a positive atmosphere for BIM implementation; and impartiality in recruiting individuals for BIM leadership roles. The study’s findings have implications for targeting key initiatives that might aid leaders in constructing adaptation strategies for organisational BIM transformation.",
        "DOI": "10.3389/fbuil.2022.1030403",
        "paper_author": "Olugboyega O.",
        "affiliation_name": "Obafemi Awolowo University",
        "affiliation_city": "Ife",
        "affiliation_country": "Nigeria",
        "affiliation_id": "60011031",
        "affiliation_state": "Ogun"
    },
    {
        "paper_title": "Threats to Terrestrial Plants from Emerging Nanoplastics",
        "publication": "ACS Nano",
        "citied_by": "52",
        "cover_date": "2022-10-25",
        "Abstract": "Nanoplastics are ubiquitous in ecosystems and impact planetary health. However, our current understanding on the impacts of nanoplastics upon terrestrial plants is fragmented. The lack of systematic approaches to evaluating these impacts limits our ability to generalize from existing studies and perpetuates regulatory barriers. Here, we undertook a meta-analysis to quantify the overall strength of nanoplastic impacts upon terrestrial plants and developed a machine learning approach to predict adverse impacts and identify contributing features. We show that adverse impacts are primarily associated with toxicity metrics, followed by plant species, nanoplastic mass concentration and size, and exposure time and medium. These results highlight that the threats of nanoplastics depend on a diversity of reactions across molecular to ecosystem scales. These reactions are rooted in both the spatial and functional complexities of nanoplastics and, as such, are specific to both the plastic characteristics and environmental conditions. These findings demonstrate the utility of interrogating the diversity of toxicity data in the literature to update both risk assessments and evidence-based policy actions.",
        "DOI": "10.1021/acsnano.2c07627",
        "paper_author": "Dang F.",
        "affiliation_name": "Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60019499",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Using Low-Resourced Language in Social Media Platforms Towards Disease Surveillance for Public Health Monitoring using Artificial Intelligence",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "2",
        "cover_date": "2022-10-21",
        "Abstract": "Early detection of diseases is crucial to control and prevent the emergence of infectious diseases. The disease monitoring and tracking of digital public health refer to social media data. The proposed study pulls data from a Twitter user who posts on various topics. The social media data will be continuously collected and performed computing analysis and interpretation. The results and assessment refer to the trends in emerging infectious diseases disseminated to those who have the right to know to act. This project aims to detect trends in social media posts on emerging infectious diseases in the Philippines, using the low-resourced languages Filipino and Cebuano, to understand the context of a social media post. Machine learning and natural language processing principles shall be used in the study. The insights from this study's dashboard can assist health professionals, officials, and the public have informed decisions and policies for better public health services for all Filipinos. It is well anticipated that policies based on the dashboard analytics will help prevent the surge or emergence of an outbreak or pandemic and, therefore, decrease the risk of economic loss. Thus, this ongoing research can result in fewer social and economic disruptions.",
        "DOI": "10.1145/3571513.3571527",
        "paper_author": "Abisado M.",
        "affiliation_name": "National University, Philippines",
        "affiliation_city": "Manila",
        "affiliation_country": "Philippines",
        "affiliation_id": "60105238",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Digital-era Propaganda: A Credible Threat to National and Global Security",
        "publication": "Arab World Geographer",
        "citied_by": "0",
        "cover_date": "2022-10-21",
        "Abstract": "This paper seeks to define and analyze the difference between public policy and propaganda, the mechanisms of disseminating both disinformation and misinformation and the possible consequences, taking into account both technological and psychological factors. As regards the technological factors, particular attention is given to “deep fakes” that allow the creation of audio and video of real people saying and doing things they have never said or done, sheer invention. Machine learning techniques are accelerating technology's sophistication, making deep fakes increasingly more realistic and resistant to detection. Psychological factors include the tendency of average people to fell prey to conspiracy theories and the confusion created by the plethora of “news sources” which are both easily available and unregulated. Strategies and tactics of digital-era propaganda will be examined based on contemporary case studies (including Russian campaigns to damage EU-Ukraine relations, and President Trump's accusations against his adversaries for fake news). Recommendations will be given as how to counter the threat to the best possible extent. Moreover, focus is also placed on the danger of using the suppression of propaganda as a pretext to suppress media pluralism and control dissident voices that criticize the established status quo, especially in non-liberal democracies (like Russia) or in autocratic regimes (like China).",
        "DOI": "10.5555/1480-6800-25.1.51",
        "paper_author": "Kyriakidis K.",
        "affiliation_name": "American University in the Emirates",
        "affiliation_city": "Dubai",
        "affiliation_country": "United Arab Emirates",
        "affiliation_id": "60110955",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Interpreting global variations in the toll of COVID-19: The case for context and nuance in hypothesis generation and testing",
        "publication": "Frontiers in Public Health",
        "citied_by": "0",
        "cover_date": "2022-10-19",
        "Abstract": "Key points: As of January 2022, the COVID-19 pandemic was on-going, affecting populations worldwide. The potential risks of the Omicron variant (and future variants) still remain an area of active investigation. Thus, the ultimate human toll of SARS-CoV-2, and, by extension, the variations in that toll among diverse populations, remain unresolved. Nonetheless, an extensive literature on causal factors in the observed patterns of COVID-19 morbidity and cause-specific mortality has emerged—particularly at the aggregate level of analysis. This article explores potential pitfalls in the attribution of COVID outcomes to specific factors in isolation by examining a diverse set of potential factors and their interactions. Methods: We sourced published data to establish a global database of COVID-19 outcomes for 68 countries and augmented these with an array of potential explanatory covariates from a diverse set of sources. We sought population-level aggregate factors from both health- and (traditionally) non-health domains, including: (a) Population biomarkers (b) Demographics and infrastructure (c) Socioeconomics (d) Policy responses at the country-level. We analyzed these data using (OLS) regression and more flexible non-parametric methods such as recursive partitioning, that are useful in examining both potential joint factor contributions to variations in pandemic outcomes, and the identification of possible interactions among covariates across these domains. Results: Using the national obesity rates of 68 countries as an illustrative predictor covariate of COVID-19 outcomes, we observed marked inconsistencies in apparent outcomes by population. Importantly, we also documented important variations in outcomes, based on interactions of health factors with covariates in other domains that are traditionally not related to biomarkers. Finally, our results suggest that single-factor explanations of population-level COVID-19 outcomes (e.g., obesity vs. cause-specific mortality) appear to be confounded substantially by other factors. Conclusions/implications: Our methods and findings suggest that a full understanding of the toll of the COVID-19 pandemic, as would be central to preparing for similar future events, requires analysis within and among diverse variable domains, and within and among diverse populations. While this may seem apparent, the bulk of the recent literature on the pandemic has focused on one or a few of these drivers in isolation. Hypothesis generation and testing related to pandemic outcomes will benefit from accommodating the nuance of covariate interactions, in an epidemiologic context. Finally, our results add to the literature on the ecological fallacy: the attempt to infer individual drivers and outcomes from the study of population-level aggregates.",
        "DOI": "10.3389/fpubh.2022.1010011",
        "paper_author": "Stein R.M.",
        "affiliation_name": "Leonard N. Stern School of Business",
        "affiliation_city": "New York",
        "affiliation_country": "United States",
        "affiliation_id": "60108316",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Water chemical oxygen demand prediction model based on the CNN and ultraviolet-visible spectroscopy",
        "publication": "Frontiers in Environmental Science",
        "citied_by": "13",
        "cover_date": "2022-10-18",
        "Abstract": "Excessive levels of organic matter in water threaten ecological safety and endanger human health. As the water resource environment is deteriorating, accurate and rapid determination of water quality parameters has become a current research hotspot. In recent years, the ultraviolet spectrometry method has been more widely used in the detection of chemical oxygen demand (COD), which is convenient and without chemical reagents. However, this method tends to use absorbance at 254 nm to measure COD. It has a good detection effect when the composition of pollutants is single, but in real life, the complex composition of pollutants will seriously affect the accuracy of measurement. Therefore, a COD prediction model based on ultraviolet-visible (UV-Vis) spectrometry and the convolutional neural network (CNN) is proposed. Compared with other traditional COD prediction models, this model makes full use of the absorbance of all ultraviolet and visible wavelengths, avoiding the information loss caused by using specific wavelengths. Meanwhile, this model is constructed based on the shallow CNN, using convolutional layers with different step lengths instead of the traditional pooling layers, which reduces computation and enhances the capture of spectral feature peaks. Additionally, with the powerful feature extraction capability of the CNN, this model reduces the reliance on pre-processing methods and improves the utilization of spectral information. Experiments have shown that our model has better fitting results and accuracy than other traditional COD prediction models such as the principal component analysis (PCA), partial least squares regression (PLSR), and backpropagation (BP) neural network. This study provides a better solution for improving the accuracy of UV-Vis water quality COD detection, which is conducive to real-time monitoring of the water quality, providing data support of water pollution and its development trend for the government’s water resource protection policy and promoting biodiversity development.",
        "DOI": "10.3389/fenvs.2022.1027693",
        "paper_author": "Ye B.",
        "affiliation_name": "Chongqing University of Technology",
        "affiliation_city": "Chongqing",
        "affiliation_country": "China",
        "affiliation_id": "60031991",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Offline Reinforcement Learning for Mobile Notifications",
        "publication": "International Conference on Information and Knowledge Management, Proceedings",
        "citied_by": "4",
        "cover_date": "2022-10-17",
        "Abstract": "Mobile notification systems have taken a major role in driving and maintaining user engagement for online platforms. They are interesting recommender systems to machine learning practitioners with more sequential and long-term feedback considerations. Most machine learning applications in notification systems are built around response-prediction models, trying to attribute both short-term impact and long-term impact to a notification decision. However, a user's experience depends on a sequence of notifications and attributing impact to a single notification is not always accurate, if not impossible. In this paper, we argue that reinforcement learning is a better framework for notification systems in terms of performance and iteration speed. We propose an offline reinforcement learning framework to optimize sequential notification decisions for driving user engagement. We describe a state-marginalized importance sampling policy evaluation approach, which can be used to evaluate the policy offline and tune learning hyperparameters. Through simulations that approximate the notifications ecosystem, we demonstrate the performance and benefits of the offline evaluation approach as a part of the reinforcement learning modeling approach. Finally, we collect data through online exploration in the production system, train an offline Double Deep Q-Network and launch a successful policy online. We also discuss the practical considerations and results obtained by deploying these policies for a large-scale recommendation system use-case.",
        "DOI": "10.1145/3511808.3557083",
        "paper_author": "Yuan Y.",
        "affiliation_name": "LinkedIn Corporation",
        "affiliation_city": "Sunnyvale",
        "affiliation_country": "United States",
        "affiliation_id": "60121308",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Stop&amp;Hop: Early Classification of Irregular Time Series",
        "publication": "International Conference on Information and Knowledge Management, Proceedings",
        "citied_by": "9",
        "cover_date": "2022-10-17",
        "Abstract": "Early classification algorithms help users react faster to their machine learning model's predictions. Early warning systems in hospitals, for example, let clinicians improve their patients' outcomes by accurately predicting infections. While early classification systems are advancing rapidly, a major gap remains: existing systems do not consider irregular time series, which have uneven and often-long gaps between their observations. Such series are notoriously pervasive in impactful domains like healthcare. We bridge this gap and study early classification of irregular time series, a new setting for early classifiers that opens doors to more real-world problems. Our solution, Stop&Hop, uses a continuous-time recurrent network to model ongoing irregular time series in real time, while an irregularity-aware halting policy, trained with reinforcement learning, predicts when to stop and classify the streaming series. By taking real-valued step sizes, the halting policy flexibly decides exactly when to stop ongoing series in real time. This way, Stop&Hop seamlessly integrates information contained in the timing of observations, a new and vital source for early classification in this setting, with the time series values to provide early classifications for irregular time series. Using four synthetic and three real-world datasets, we demonstrate that Stop&Hop consistently makes earlier and more-accurate predictions than state-of-the-art alternatives adapted to this new problem. Our code is publicly available at https://github.com/thartvigsen/StopAndHop.",
        "DOI": "10.1145/3511808.3557460",
        "paper_author": "Hartvigsen T.",
        "affiliation_name": "Massachusetts Institute of Technology",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States",
        "affiliation_id": "60022195",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "WARNER: Weakly-Supervised Neural Network to Identify Eviction Filing Hotspots in the Absence of Court Records",
        "publication": "International Conference on Information and Knowledge Management, Proceedings",
        "citied_by": "2",
        "cover_date": "2022-10-17",
        "Abstract": "The widespread eviction of tenants across the United States has metamorphosed into a challenging public-policy problem. In particular, eviction exacerbates several income-based, educational, and health inequities in society, e.g., eviction disproportionately affects low-income renting families, many of whom belong to underrepresented minority groups. Despite growing interest in understanding and mitigating the eviction crisis, there are several legal and infrastructural obstacles to data acquisition at scale that limit our understanding of the distribution of eviction across the United States. To circumvent existing challenges in data acquisition, we propose WARNER, a novel Machine Learning (ML) framework that predicts eviction filing hotspots in US counties from unlabeled satellite imagery dataset. We account for the lack of labeled training data in this domain by leveraging sociological insights to propose a novel approach to generate probabilistic labels for a subset of an unlabeled dataset of satellite imagery, which is then used to train a neural network model to identify eviction filing hotspots. Our experimental results show that WARNER acheives a higher predictive performance than several strong baselines. Further, the superiority of WARNER can be generalized to different counties across the United States. Our proposed framework has the potential to assist NGOs and policymakers in designing well-informed (data-driven) resource allocation plans to improve the nationwide housing stability. This work is conducted in collaboration with The Child Poverty Action Lab (a leading non-profit leveraging data-driven approaches to inform actions for relieving poverty and relevant problems in Dallas County, TX). The code can be accessed via https://github.com/maryam-tabar/WARNER.",
        "DOI": "10.1145/3511808.3557128",
        "paper_author": "Tabar M.",
        "affiliation_name": "Pennsylvania State University",
        "affiliation_city": "University Park",
        "affiliation_country": "United States",
        "affiliation_id": "60001439",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "Risk-Aware Bid Optimization for Online Display Advertisement",
        "publication": "International Conference on Information and Knowledge Management, Proceedings",
        "citied_by": "3",
        "cover_date": "2022-10-17",
        "Abstract": "This research focuses on the bid optimization problem in the real-time bidding setting for online display advertisements, where an advertiser, or the advertiser's agent, has access to the features of the website visitor and the type of ad slots, to decide the optimal bid prices given a predetermined total advertisement budget. We propose a risk-aware data-driven bid optimization model that maximizes the expected profit for the advertiser by exploiting historical data to design upfront a bidding policy, mapping the type of advertisement opportunity to a bid price, and accounting for the risk of violating the budget constraint during a given period of time. After employing a Lagrangian relaxation, we derive a parametrized closed-form expression for the optimal bidding strategy. Using a real-world dataset, we demonstrate that our risk-averse method can effectively control the risk of overspending the budget while achieving a competitive level of profit compared with the risk-neutral model and a state-of-the-art data-driven risk-aware bidding approach.",
        "DOI": "10.1145/3511808.3557436",
        "paper_author": "Fan R.",
        "affiliation_name": "HEC Montréal",
        "affiliation_city": "Montreal",
        "affiliation_country": "Canada",
        "affiliation_id": "60002970",
        "affiliation_state": "QC"
    },
    {
        "paper_title": "Towards Automated Imbalanced Learning with Deep Hierarchical Reinforcement Learning",
        "publication": "International Conference on Information and Knowledge Management, Proceedings",
        "citied_by": "13",
        "cover_date": "2022-10-17",
        "Abstract": "Imbalanced learning is a fundamental challenge in data mining, where there is a disproportionate ratio of training samples in each class. Over-sampling is an effective technique to tackle imbalanced learning through generating synthetic samples for the minority class. While numerous over-sampling algorithms have been proposed, they heavily rely on heuristics, which could be sub-optimal since we may need different sampling strategies for different datasets and base classifiers, and they cannot directly optimize the performance metric. Motivated by this, we investigate developing a learning-based over-sampling algorithm to optimize the classification performance, which is a challenging task because of the huge and hierarchical decision space. At the high level, we need to decide how many synthetic samples to generate. At the low level, we need to determine where the synthetic samples should be located, which depends on the high-level decision since the optimal locations of the samples may differ for different numbers of samples. To address the challenges, we propose AutoSMOTE, an automated over-sampling algorithm that can jointly optimize different levels of decisions. Motivated by the success of SMOTE and its extensions, we formulate the generation process as a Markov decision process (MDP) consisting of three levels of policies to generate synthetic samples within the SMOTE search space. Then we leverage deep hierarchical reinforcement learning to optimize the performance metric on the validation data. Extensive experiments on six real-world datasets demonstrate that AutoSMOTE significantly outperforms the state-of-the-art resampling algorithms. The code is at https://github.com/daochenzha/autosmote",
        "DOI": "10.1145/3511808.3557474",
        "paper_author": "Zha D.",
        "affiliation_name": "Rice University",
        "affiliation_city": "Houston",
        "affiliation_country": "United States",
        "affiliation_id": "60005286",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "Innovations in freight transport: a systematic literature evaluation and COVID implications",
        "publication": "International Journal of Logistics Management",
        "citied_by": "25",
        "cover_date": "2022-10-17",
        "Abstract": "Purpose: This paper systematically evaluates the existing literature of innovations in freight transport, including all modes, to uncover the key research themes and methodologies employed by researchers to study innovations and their implications in this industry. It analyses the role of transport and the impact of innovations during crises, such as COVID-19. Design/methodology/approach: Qualitative and quantitative analysis of the innovations in freight transport unravels the pre-requisites of such endeavours in achieving a resilient and sustainable transport network that effectively and efficiently operates during a crisis. The authors performed keyword co-occurrence network (KCON) analysis and research focus parallelship network (RFPN) analysis using BibExcel and Gephi to determine the major resulting research streams in freight transport. Findings: The RFPN identified five emerging themes: transport operations, technological innovation, transport economics, transport policy and resilience and disaster management. Optimisation and simulation techniques, and more recently, artificial intelligence and machine learning (ML) approaches, have been used to model and solve freight transport problems. Automation innovations have also penetrated freight and supply chains. Information and communication technology (ICT)-based innovations have also been found to be effective in building resilient supply chains. Research limitations/implications: Given the growth of e-commerce during COVID-19 and the resulting logistics demand, along with the need for transporting food and medical emergency products, the role of automation, optimisation, monitoring systems and risk management in the transport industry has become more salient. Transport companies need to improve their operational efficiency using innovative technologies and data science for informed decision-making. Originality/value: This paper advises researchers and practitioners involved in freight transport and innovation about main directions and gaps in the field through an integrated approach for evaluating research undertaken in the area. This paper also highlights the role of crisis, e.g. COVID-19, and its impacts on freight transport. Major contributions of this paper are as follows: (1) a qualitative and quantitative, systematic and effective assessment of the literature on freight transport through a network analysis of keywords supplemented by a review of the text of 148 papers; (2) unravelling major research areas; (3) identifying innovations in freight transport and their classification as technological and non-technological and (4) investigating the impact of crises and disruptions in freight transport.",
        "DOI": "10.1108/IJLM-07-2021-0360",
        "paper_author": "Kiani Mavi R.",
        "affiliation_name": "Edith Cowan University",
        "affiliation_city": "Perth",
        "affiliation_country": "Australia",
        "affiliation_id": "60105210",
        "affiliation_state": "WA"
    },
    {
        "paper_title": "Cholecystectomy for asymptomatic gallstones: Markov decision tree analysis",
        "publication": "World Journal of Clinical Cases",
        "citied_by": "12",
        "cover_date": "2022-10-16",
        "Abstract": "Gallstones are a common public health problem, especially in developed countries. There are an increasing number of patients who are diagnosed with gallstones due to increasing awareness and liberal use of imaging, with 22.6%- 80% of gallstone patients being asymptomatic at the time of diagnosis. Despite being asymptomatic, this group of patients are still at life-long risk of developing symptoms and complications such as acute cholangitis and acute biliary pancreatitis. Hence, while early prophylactic cholecystectomy may have some benefits in selected groups of patients, the current standard practice is to recommend cholecystectomy only after symptoms or complications occur. After reviewing the current evidence about the natural course of asymptomatic gallstones, complications of cholecystectomy, quality of life outcomes, and economic outcomes, we recommend that the option of cholecystectomy should be discussed with all asymptomatic gallstone patients. Disclosure of material information is essential for patients to make an informed choice for prophylactic cholecystectomy. It is for the patient to decide on watchful waiting or prophylactic cholecystectomy, and not for the medical community to make a blanket policy of watchful waiting for asymptomatic gallstone patients. For patients with high-risk profiles, it is clinically justifiable to advocate cholecystectomy to minimize the likelihood of morbidity due to complications.",
        "DOI": "10.12998/wjcc.v10.i29.10399",
        "paper_author": "Lee B.J.H.",
        "affiliation_name": "Lee Kong Chian School of Medicine",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore",
        "affiliation_id": "60118633",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A MINING FRAMEWORK FOR REAL BURST LOCATION ESTIMATION AND PORTABILITYOF THE WATER USING DEEP LEARNING",
        "publication": "Journal of Theoretical and Applied Information Technology",
        "citied_by": "0",
        "cover_date": "2022-10-15",
        "Abstract": "Good health policy requires that all people have access to safe drinking water as a basic human right. In terms of national, regional, and local health and development, this is critical. Water and sanitation improvements have been found to provide a positive return on investment in certain areas, since the reductions in health risks and medical expenses much surpass the costs of making the improvements. To check whether that water is safe or not we have some parameters which need to be checked like pH value which ranges from 6.52 - 6.83 and Hardness, Chloramines, Sulphate, Conductivity, Organic carbon, Trihalomethanes, Turbidity, and at last portability. When we acquire a result of 1, we know that the water is safe to drink. If we get a portability value of 0 it is not safe for water consumption Before checking the quality of water, we need to collect all water bodies' images from Google earth maps and mask them and check their pot ability.The project involves data analysis of the different parameters which are involved in checking the portability of water with proper dataset using data processing methods. Random Forest, Decision Tree and other machine learning algorithms are used to make predictions. With the use of VGG image Annotator and leakage location estimate algorithms such as cross correlation of sinusoidal waves and water bodies are masked out of water distribution pipes.",
        "DOI": "NA",
        "paper_author": "Vasanthsena P.",
        "affiliation_name": "Chaitanya Bharathi Institute of Technology",
        "affiliation_city": "Hyderabad",
        "affiliation_country": "India",
        "affiliation_id": "60094091",
        "affiliation_state": "TS"
    },
    {
        "paper_title": "Machine learning can identify the sources of heavy metals in agricultural soil: A case study in northern Guangdong Province, China",
        "publication": "Ecotoxicology and Environmental Safety",
        "citied_by": "36",
        "cover_date": "2022-10-15",
        "Abstract": "Source tracing of heavy metals in agricultural soils is of critical importance for effective pollution control and targeting policies. It is a great challenge to identify and apportion the complex sources of soil heavy metal pollution. In this study, a traditional analysis method, positive matrix fraction (PMF), and three machine learning methodologies, including self-organizing map (SOM), conditional inference tree (CIT) and random forest (RF), were used to identify and apportion the sources of heavy metals in agricultural soils from Lianzhou, Guangdong Province, China. Based on PMF, the contribution of the total loadings of heavy metals in soil were 19.3% for atmospheric deposition, 65.5% for anthropogenic and geogenic sources, and 15.2% for soil parent materials. Based on SOM model, As, Cd, Hg, Pb and Zn were attributed to mining and geogenic sources; Cr, Cu and Ni were derived from geogenic sources. Based on CIT results, the influence of altitude on soil Cr, Cu, Hg, Ni and Zn, as well as soil pH on Cd indicated their primary origin from natural processes. Whereas As and Pb were related to agricultural practices and traffic emissions, respectively. RF model further quantified the importance of variables and identified potential control factors (altitude, soil pH, soil organic carbon) in heavy metal accumulation in soil. This study provides an integrated approach for heavy metals source apportionment with a clear potential for future application in other similar regions, as well as to provide the theoretical basis for undertaking management and assessment of soil heavy metal pollution.",
        "DOI": "10.1016/j.ecoenv.2022.114107",
        "paper_author": "Shi T.",
        "affiliation_name": "Nanjing University of Information Science &amp; Technology",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60064143",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "Machine learning aided meta-analysis of microplastic polymer composition in global marine environment",
        "publication": "Journal of Hazardous Materials",
        "citied_by": "32",
        "cover_date": "2022-10-15",
        "Abstract": "Conventional meta-analysis of the literature on marine microplastic monitoring is not effective at producing patterns and trends in microplastic polymer distribution on a global level, mainly due to the limited number of studies considered for the re-analysis. As a solution, for the first time, we adopted a machine learning-based approach to demonstrate the distribution of microplastic polymers in four different compartments (viz: beach sediment, bottom sediment, water column, and biota) of global oceans. Polyethylene (79.9%), Polypropylene (77.2%), and Polyamide (52.3%) were identified as the most abundant polymers in the marine environment. The abundance of microfibres reported by previous studies was found to be underestimated; moreover, biota contained a disproportionately high amount of microfibres. The morphological characteristics of microplastics, the practice of picking up large particles for spectroscopic identification, and the lower resolution of FTIR found to be inducting bias in polymer characterization. Importantly, this work also illustrates the prominent role of ocean currents in the transport of microplastics. In essence, our study proposes machine learning-aided meta-analysis as an effective technique to facilitate the large-scale analysis of microplastic data to help formulate data-driven policies for combating microplastic pollution.",
        "DOI": "10.1016/j.jhazmat.2022.129801",
        "paper_author": "Kannankai M.P.",
        "affiliation_name": "Cochin University of Science and Technology",
        "affiliation_city": "Kochi",
        "affiliation_country": "India",
        "affiliation_id": "60031544",
        "affiliation_state": "KL"
    },
    {
        "paper_title": "Estimation of global and diffuse horizontal irradiance by machine learning techniques based on variables from the Heliosat model",
        "publication": "Journal of Cleaner Production",
        "citied_by": "6",
        "cover_date": "2022-10-15",
        "Abstract": "The increased interest in the sources of renewable electricity has drawn attention to the rapidly developing solar energy sector owing to its high cost-benefit ratio. To accurately calculate the potential electricity output of photovoltaic (PV) panels, the global horizontal irradiance (GHI) and diffuse horizontal irradiance (DHI) must be known. The worldwide presence of satellite imagery provides an efficient way to estimate current and historical GHI instead of using the high time- and cost-consuming in-situ measurement. The most of past studies utilized more satellite bands as the inputs for machine learning algorithms to estimate GHI. The further estimation of DHI was usually based on weather-related parameters. Thus, there is a research gap in using only one satellite band for both GHI and DHI estimations. Therefore, this study used machine learning algorithms to estimate GHI and DHI, with inputs delivered from the Heliosat model based on band 3 of Himawari-8 satellite imageries. The results were compared with the original and site-adapted Heliosat models and seven DHI separation models. The results indicated that the machine learning models were capable of performing with the same accuracy as the Heliosat models. However, their performance was better while estimating DHI, in which case they outperformed even the best separation model. Higher accuracy and precision were observed in those models where the additional solar zenith at time t+1h was used together with other input features. This highlighted the possibility of using only one satellite band together with the calculated solar position variables as the input. Overall, this research has established a new method for estimating GHI and DHI with high confidence based on satellite imageries and the Heliosat model through the application of machine learning techniques.",
        "DOI": "10.1016/j.jclepro.2022.133696",
        "paper_author": "Han J.Y.",
        "affiliation_name": "College of Engineering, National Taiwan University",
        "affiliation_city": "Taipei",
        "affiliation_country": "Taiwan",
        "affiliation_id": "60118890",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Long-term trends of atmospheric hot-and-polluted episodes (HPE) and the public health implications in the Pearl River Delta region of China",
        "publication": "Environmental Pollution",
        "citied_by": "4",
        "cover_date": "2022-10-15",
        "Abstract": "Air pollution and extreme heat have been responsible for more than a million deaths in China every year, especially in densely urbanized regions. While previous studies intensively evaluated air pollution episodes and extreme heat events, a limited number of studies comprehensively assessed atmospheric hot-and-polluted-episodes (HPE) – an episode with simultaneously high levels of air pollution and temperature – which have potential adverse synergic impacts on human health. This study focused on the Pearl River Delta (PRD) region of China due to its high temperature in summer and poor air quality throughout a year. We employed geostatistical downscaling to model meteorology at a spatial resolution of 1 km, and applied a machine learning algorithm (XGBoost) to estimate a high-resolution (1 km) daily concentration of particulate matter with an aerodynamic diameter ≤2.5 μm (PM2.5) and ozone (O3) for June to October over 20 years (2000–2019). Our results indicate an increasing trend (∼50%) in the frequency of HPE occurrence in the first decade (2000–2010). Conversely, the annual frequency of HPE occurrence reduced (16.7%), but its intensity increased during the second decade (2010–2019). The northern cities in the PRD region had higher levels of PM2.5 and O3 than their southern counterparts. During HPEs, regional daily PM2.5 exceeded the World Health Organization (WHO) and Chinese guideline levels by 75% and 25%, respectively, while the O3 exceeded the WHO O3 standard by up to 69%. Overall, 567,063 (95% confidence interval (CI): 510,357–623,770) and 52,231 (95%CI: 26,116–78,346) excessive deaths were respectively attributable to exposure to PM2.5 and O3 in the PRD region. Our findings imply the necessity and urgency to formulate co-benefit policies to mitigate the region's air pollution and heat problems.",
        "DOI": "10.1016/j.envpol.2022.119782",
        "paper_author": "Nduka I.C.",
        "affiliation_name": "Chinese University of Hong Kong",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong",
        "affiliation_id": "60002798",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Path planning for underwater gliders in time-varying ocean current using deep reinforcement learning",
        "publication": "Ocean Engineering",
        "citied_by": "29",
        "cover_date": "2022-10-15",
        "Abstract": "The objective of this paper is to solve the application research of underwater glider (UG) and UGs formation, it is aiming to solve the path planning of gliders in ocean current environment by deep deterministic policy gradient (DDPG). Gliders can be deployed individually or collectively to execute ocean missions. Using the existing glider model and the interactions between gliders and environment, models close to the practical application of UGs are established. The deep reinforcement learning (DRL) based planning algorithm by integrating artificial intelligence, and solution to planning problem of UGs is provided. For a single UG planning, the designed RL algorithm can solve the compliance of UG motion constraints. The algorithm can calculate the appropriate path for the UGs formation, and change the shape of formation as necessary, which is useful for navigation in the environment of dense obstacles. With the same reward function, the improved DDPG outperforms the deep Q-network (DQN). Based on Tokyo Bay geography and unacquainted ocean, the developed algorithm is tested in ocean current environments.",
        "DOI": "10.1016/j.oceaneng.2022.112226",
        "paper_author": "Lan W.",
        "affiliation_name": "Dalian Maritime University",
        "affiliation_city": "Dalian",
        "affiliation_country": "China",
        "affiliation_id": "60029322",
        "affiliation_state": "Liaoning"
    },
    {
        "paper_title": "Multi-agent energy management optimization for integrated energy systems under the energy and carbon co-trading market",
        "publication": "Applied Energy",
        "citied_by": "54",
        "cover_date": "2022-10-15",
        "Abstract": "With the development trends of carbon neutrality, carbon trading is graduating embedded into the energy management of integrated energy systems (IESs). The dual benefices of carbon emission reduction and economics can be achieved by coordinatively optimizing the complementarity and flexibility between multiple IESs. However, this leads to the increased complexity of the market transactions, which poses significant challenges in terms of the benefits distribution among multiple entities, the convergence of trading processes, and the privacy-preserving issues. The multi-agent reinforcement learning (MARL) is capable of solving complex sequential-decision problems and acquiring the optimal strategies for each entity through the interactions between the multiple agents and the market. The MARL deployed on the local agent can provide online trading decisions for individual market entities considering their own interests, which offers new potential to solve the abovementioned difficulties. In this paper, we proposed an IESs co-trading market including electricity, natural gas and carbon trading. The multi-agent energy management coordinative optimization problem is solved by an improved Multi-agent Deep Deterministic Policy Gradient (MADDPG) algorithm to achieve fair trade and entity privacy protection. The case study results verify that the proposed optimal energy management strategy based on the improved MADDPG algorithm can efficiently guide the IESs in the energy and carbon co-trading market.",
        "DOI": "10.1016/j.apenergy.2022.119646",
        "paper_author": "Sun Q.",
        "affiliation_name": "Beijing Jiaotong University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60022381",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Disentangling the impact of childhood abuse and neglect on depressive affect in adulthood: A machine learning approach in a general population sample",
        "publication": "Journal of Affective Disorders",
        "citied_by": "6",
        "cover_date": "2022-10-15",
        "Abstract": "Background: Different types of childhood maltreatment (CM) are key risk factors for psychopathology. Specifically, there is evidence for a unique role of emotional abuse in affective psychopathology in children and youth; however, its predictive power for depressive symptomatology in adulthood is still unknown. Additionally, emotional abuse encompasses several facets, but the strength of their individual contribution to depressive affect has not been examined. Method: Here, we used a machine learning (ML) approach based on Random Forests to assess the performance of domain scores and individual items from the Childhood Trauma Questionnaire (CTQ) in predicting self-reported levels of depressive affect in an adult general population sample. Models were generated in a training sample (N = 769) and validated in an independent test sample (N = 466). Using state-of-the-art methods from interpretable ML, we identified the most predictive domains and facets of CM for adult depressive affect. Results: Models based on individual CM items explained more variance in the independent test sample than models based on CM domain scores (R2 = 7.6 % vs. 6.4 %). Emotional abuse, particularly its more subjective components such as reactions to and appraisal of the abuse, emerged as the strongest predictors of adult depressive affect. Limitations: Assessment of CM was retrospective and lacked information on timing and duration. Moreover, reported rates of CM and depressive affect were comparatively low. Conclusions: Our findings corroborate the strong role of subjective experience in CM-related psychopathology across the lifespan that necessitates greater attention in research, policy, and clinical practice.",
        "DOI": "10.1016/j.jad.2022.07.042",
        "paper_author": "Betz L.T.",
        "affiliation_name": "Medizinische Fakultät",
        "affiliation_city": "Koln",
        "affiliation_country": "Germany",
        "affiliation_id": "60204358",
        "affiliation_state": "Nordrhein-Westfalen"
    },
    {
        "paper_title": "A hybrid prototype selection-based deep learning approach for anomaly detection in industrial machines",
        "publication": "Expert Systems with Applications",
        "citied_by": "37",
        "cover_date": "2022-10-15",
        "Abstract": "Anomaly detection in time series is an important task to many applications, e.g, the maintenance policies of rotating machines within industries strongly rely on time series monitoring. Rotating machines are vital elements within industries. Therefore, maintenance policies on these critical elements concern the quality of products and safety issues. Condition-based maintenance is an example of those policies. In this context, we propose a novel method to train a deep learning-based feature extractor for the anomaly detection problem on rotating machinery. It consists of using a prototype selection algorithm to improve the training process of a randomly initialized feature extractor. We perform this process iteratively using data belonging to one probability distribution, i.e., the normal class. We carried the prototype selection out with the Nearest Neighbors algorithm, and the feature extractor was a Convolutional Neural Network. We validate the method on three datasets of spectrograms related to gearbox and compressors faults and achieved promising results. We obtained detection rates in anomalous data close to 100%, and the anomaly detectors classified normal instances with accuracy values superior to 95%. Those results were competitive concerning other deep learning-based anomaly detectors in the literature, with the advantage of being an integrated solution.",
        "DOI": "10.1016/j.eswa.2022.117528",
        "paper_author": "de Paula Monteiro R.",
        "affiliation_name": "Universidade Catolica de Pernambuco",
        "affiliation_city": "Recife",
        "affiliation_country": "Brazil",
        "affiliation_id": "60005352",
        "affiliation_state": "PE"
    },
    {
        "paper_title": "Intelligent Fault Quantitative Identification for Industrial Internet of Things (IIoT) via a Novel Deep Dual Reinforcement Learning Model Accompanied With Insufficient Samples",
        "publication": "IEEE Internet of Things Journal",
        "citied_by": "20",
        "cover_date": "2022-10-15",
        "Abstract": "Industrial Internet of Things (IIoT) is mainly a data-oriented network, so intelligent processing of massive data is desiderated to realize the interconnection between machines. Currently, deep-learning-based methods are widely applied for intelligent construction of the IIoT, so as to maximize the self-monitoring and self-management capabilities of various machines. However, the quantity and quality of data and the optimization of parameters greatly limit the properties of such methods. As a breakthrough of artificial intelligence (AI), deep reinforcement learning (DRL) provides inspiration and direction, which combines the advantages of deep learning and reinforcement learning to construct an end-to-end fault identification system. Therefore, a novel deep dual reinforcement learning model was proposed, which consisted of an actor model and a critic model. The dual structures avoid the over-self-optimization of the network. The action model continually learns the knowledge of identifying unknown samples by the $\\varepsilon $ - $greedy$ algorithm, while the critic model dynamically adjusts the policy to guide the action model in right training direction. The effectiveness of the proposed method was verified by three bearing data sets. The results indicate that the proposed method enables agents to independently realize precise fault quantitative identification. The establishment of an experience storage unit overcomes the problem of insufficient samples, which avoids blind trial and error of the proposed mode.",
        "DOI": "10.1109/JIOT.2022.3168317",
        "paper_author": "Chang Y.",
        "affiliation_name": "State Key Laboratory for Manufacturing Systems Engineering",
        "affiliation_city": "Xi'an",
        "affiliation_country": "China",
        "affiliation_id": "60089930",
        "affiliation_state": "Shaanxi"
    },
    {
        "paper_title": "Decentralized Adaptive Spectrum Learning in Wireless IoT Networks Based on Channel Quality Information",
        "publication": "IEEE Internet of Things Journal",
        "citied_by": "7",
        "cover_date": "2022-10-15",
        "Abstract": "In the Internet of Things (IoT) context such as low power wide area network (LPWAN), it is essential to reduce the packet losses, e.g., to save energy. Decentralized artificial intelligence (AI) techniques have been proposed to combat radio collisions, but the approach here is extended to deal additionally with the channel propagation effects. In this article, a Quality of Channel Allocation (QoC-A) learning technique based on bandit algorithms is proposed in order to choose the transmission channel. This aims to reduce the effect of the propagation impairments between the radio channels while using the effective signal power (ESP) as a quality metric. In addition, a discounted QoC-A (DQoC-A) algorithm is proposed to adapt rapidly to any abrupt change in the channels' conditions. An experimental campaign on a real IoT device is carried out to demonstrate the low complexity and efficiency of these proposed decentralized algorithms. In the given results, QoC-A outperforms the classical upper confidence bound (UCB) policy with a more accelerated learning process. On the other hand, the feasibility of using the DQoC-A in nonstationary scenarios is illustrated by its rapid convergence when abrupt changes in the channels' conditions occur. At the end of the process, these proposed learning techniques give 4.1 and 2.4 times fewer packet losses than the traditional ones with a random channel assignment scheme, in the stationary and nonstationary scenarios, respectively.",
        "DOI": "10.1109/JIOT.2022.3167016",
        "paper_author": "Abdelghany A.",
        "affiliation_name": "Université de Rennes",
        "affiliation_city": "Rennes",
        "affiliation_country": "France",
        "affiliation_id": "60030553",
        "affiliation_state": "Brittany"
    },
    {
        "paper_title": "Security technologies and social implications",
        "publication": "Security Technologies and Social Implications",
        "citied_by": "2",
        "cover_date": "2022-10-14",
        "Abstract": "Security Technologies and Social Implications focuses on the development and application of new technologies that police and homeland security officers can leverage as a tool for both predictive and intelligence-led investigations. The book recommends the best practices for incorporation of these technologies into day-to-day activities by law enforcement agencies and counter-terrorism units. Practically, it addresses legal, technological, and organizational challenges (e.g. resource limitation and privacy concerns) combined with challenges related to the adoption of innovative technologies. In contrast to classic tools, modern policing and security requires the development and implementation of new technologies using AI, machine learning, social media tracking, drones, robots, GIS, computer vision, and more. As crime (and cybercrime in particular) becomes more and more sophisticated, security requires a complex mix of social measures, including prevention, detection, investigation, and prosecution. Key topics related to these developments and their implementations covered in Security Technologies and Social Implications include: New security technologies and how these technologies can be implemented in practice, plus associated social, ethical or policy issues. Expertise and commentary from individuals developing and testing new technologies and individuals using the technologies within their everyday roles. The latest advancements in commercial and professional law enforcement technologies and platforms. Commentary on how technologies can advance humanity by making policing and security more efficient and keeping citizens safe. Security Technologies and Social Implications serves as a comprehensive resource for defense personnel and law enforcement staff, practical security engineers, and trainee staff in security and police colleges to understand the latest security technologies, with a critical look at their uses and limitations regarding potential ethical, regulatory, or legal issues.",
        "DOI": "10.1002/9781119834175",
        "paper_author": "Markarian G.",
        "affiliation_name": "School of Computing and Communications, Lancaster University",
        "affiliation_city": "Lancaster",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60117772",
        "affiliation_state": "Lancashire"
    },
    {
        "paper_title": "Handbook of energy transitions",
        "publication": "Handbook of Energy Transitions",
        "citied_by": "15",
        "cover_date": "2022-10-14",
        "Abstract": "The global energy scenario is undergoing an unprecedented transition. In the wake of enormous challenges-such as increased population, higher energy demands, increasing greenhouse gas emissions, depleting fossil fuel reserves, volatile energy prices, geopolitical concerns, and energy insecurity issues-the energy sector is experiencing a transition in terms of energy resources and their utilization. This modern transition is historically more dynamic and multidimensional compared to the past considering the vast technological advancements, socioeconomic implications and political responses, and ever-evolving global policies and regulations. Energy insecurity in terms of its critical dimensions-access, affordability, and reliability-remains a major problem hindering the socioeconomic progress in developing countries. The Handbook of Energy Transitions presents a holistic account of the 21st-century energy transition away from fossil fuels. It provides an overview of the unfolding transition in terms of overall dimensions, drivers, trends, barriers, policies, and geopolitics, and then discusses transition in terms of particular resources or technologies, such as renewable energy systems, solar energy, hydropower, hydrogen and fuel cells, electric vehicles, energy storage systems, batteries, digitalization, smart grids, blockchain, and machine learning. It also discusses the present energy transition in terms of broader policy and developmental perspectives. Further, it examines sustainable development, the economics of energy and green growth, and the role of various technologies and initiatives like renewables, nuclear power, and electrification in promoting energy security and energy transition worldwide. Key Features Includes technical, economic, social, and policy perspectives of energy transitions Features practical case studies and comparative assessments Examines the latest renewable energy and low-carbon technologies Explains the connection between energy transition and global climate chang.",
        "DOI": "10.1201/9781003315353",
        "paper_author": "Asif M.",
        "affiliation_name": "King Fahd University of Petroleum and Minerals",
        "affiliation_city": "Dhahran",
        "affiliation_country": "Saudi Arabia",
        "affiliation_id": "60009506",
        "affiliation_state": "Ash Sharqiyah"
    },
    {
        "paper_title": "A framework for identification and classification of liver diseases based on machine learning algorithms",
        "publication": "Frontiers in Oncology",
        "citied_by": "9",
        "cover_date": "2022-10-14",
        "Abstract": "Hepatocellular carcinoma (HCC) is one of the most commonly seen liver disease. Most of HCC patients are diagnosed as Hepatitis B related cirrhosis simultaneously, especially in Asian countries. HCC is the fifth most common cancer and the second most common cause of cancer-related death in the World. HCC incidence rates have been rising in the past 3 decades, and it is expected to be doubled by 2030, if there is no effective means for its early diagnosis and management. The improvement of patient’s care, research, and policy is significantly based on accurate medical diagnosis, especially for malignant tumor patients. However, sometimes it is really difficult to get access to advanced and expensive diagnostic tools such as computed tomography (CT), magnetic resonance imaging (MRI) and positron emission tomography (PET-CT)., especially for people who resides in poverty-stricken area. Therefore, experts are searching for a framework for predicting of early liver diseases based on basic and simple examinations such as biochemical and routine blood tests, which are easily accessible all around the World. Disease identification and classification has been significantly enhanced by using artificial intelligence (AI) and machine learning (ML) in conjunction with clinical data. The goal of this research is to extract the most significant risk factors or clinical parameters for liver diseases in 525 patients based on clinical experience using machine learning algorithms, such as regularized regression (RR), logistic regression (LR), random forest (RF), decision tree (DT), and extreme gradient boosting (XGBoost). The results showed that RF classier had the best performance (accuracy = 0.762, recall = 0.843, F1-score = 0.775, and AUC = 0.999) among the five ML algorithms. And the important orders of 14 significant risk factors are as follows: Total bilirubin, gamma-glutamyl transferase (GGT), direct bilirubin, hemoglobin, age, platelet, alkaline phosphatase (ALP), aspartate transaminase (AST), creatinine, alanine aminotransferase (ALT), cholesterol, albumin, urea nitrogen, and white blood cells. ML classifiers might aid medical organizations in the early detection and classification of liver disease, which would be beneficial in low-income regions, and the relevance of risk factors would be helpful in the prevention and treatment of liver disease patients.",
        "DOI": "10.3389/fonc.2022.1048348",
        "paper_author": "Ding H.",
        "affiliation_name": "First Affiliated Hospital of Zhengzhou University",
        "affiliation_city": "Zhengzhou",
        "affiliation_country": "China",
        "affiliation_id": "60102075",
        "affiliation_state": "Henan"
    },
    {
        "paper_title": "MOBILITY RESILIENCE OF COMMUTE TRIPS DURING THE COVID-19 PANDEMIC IN SEOUL, KOREA",
        "publication": "ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences",
        "citied_by": "1",
        "cover_date": "2022-10-14",
        "Abstract": "Since early 2020, the number of COVID-19 cases has continued its rise and fall worldwide, greatly impacting sectors such as health outcomes, economics, housing, and transportation. To mitigate the spread of the pandemic, governments implemented various measures to reduce the mobility of the population, restricting international travel, hierarchical lockdowns, stay-at-home mandates, and work-from-home orders. In this aspect, early studies in the transportation field showed large changes in travel behaviour. However, we know less about the long-term impact of COVID-19 on people's travel behaviour. This paper explores the change in commute behaviour during the pandemic, focusing on the resilience index of transit users and its determining factors. The hist gradient boosting model was the most precise when compared with linear and other machine learning models (considering R2, MSE, MAE). The results suggested the following: (1) commuters' trips decreased unevenly in Seoul. Through machine learning algorithms, social-economic factors, and accessibility, 50% of the heterogeneity can be explained. (2) Consumer and Service Industry and Foreigner Tourism were impacted negatively continually. Neighbourhoods with higher car ownership and a higher percentage of female residents show long term weak public transit resilience. (2) Short distance commuters (less than 20 minutes) and commuters visiting city centres, returned to public transport in the second year after avoiding it during the first year of the pandemic. Considering the uneven negative results of COVID-19, this research can be a reference for policy design and effective decision making.",
        "DOI": "10.5194/isprs-annals-X-4-W3-2022-135-2022",
        "paper_author": "Li X.",
        "affiliation_name": "Hanyang University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea",
        "affiliation_id": "60024872",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "PREDICTING OF URBAN EXPANSION USING CONVOLUTIONAL LSTM NETWORK MODEL: THE CASE OF SEOUL METROPOLITAN AREA, KOREA",
        "publication": "ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences",
        "citied_by": "0",
        "cover_date": "2022-10-14",
        "Abstract": "As urbanization progresses, many studies about the analysis and prediction of land-use change and urban sprawl have been conducted recently. As the sprawl phenomenon progresses rapidly, the urban expansion phenomenon became uncontrolled and it has affected negatively on the city's environment and transportation finally. So, it is essential to identify lands likely to be urbanized in the future because it aids in establishing land use plans and policies pre-acting the negative impact of spatially urban expansion the sprawl by determining factors affecting the urban sprawl. Previous studies based on statistical models are limited to identifying determining factors, so the prediction performance is low compared to deep learning. On the other hand, existing studies using machine learning and deep learning overlook selecting specific region-focused variables. Therefore, this study aims to analyze and predict changes in the Seoul Metropolitan Area's sprawl in Korea using the Convolutional Long Short-Term Memory Network (ConvLSTM) with factors at the city scale and neighboring factors at the local scale in the Seoul Metropolitan Area (SMA). ConvLSTM is a type of combination model: combining Recurrent Neural Network(RNN) and Convolutional Neural Network(CNN). This study showed that ConvLSTM with factors at the city and neighboring factors at the local scale predicted the urbanized land. The determinants contain population and roads ratio at the city scale, and neighboring urban lands, distance to the nearest subway stations, slope, and elevation at the local scale. The results reveal that predicted urban lands in 2020 increase over the entire region. In particular, the expected urban lands in 2020 increase by reducing farmlands in the southern part of the SMA. It is consistent with the trend of urbanized lands from 1980 to 2010. In addition, urbanization occurs in areas adjacent to Seoul due to the well-established urban infrastructure. The results of this study can be used as evidence to establish sustainable land use plans and regulations in the future.",
        "DOI": "10.5194/isprs-annals-X-4-W3-2022-113-2022",
        "paper_author": "Kim J.M.",
        "affiliation_name": "Hanyang University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea",
        "affiliation_id": "60024872",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Monitoring the marine invasive alien species Rugulopteryx okamurae using unmanned aerial vehicles and satellites",
        "publication": "Frontiers in Marine Science",
        "citied_by": "26",
        "cover_date": "2022-10-13",
        "Abstract": "Rugulopteryx okamurae is a species of brown macroalgae belonging to the Dictyotaceae family and native to the north-western Pacific. As an Invasive Alien Species (IAS), it was first detected in the Strait of Gibraltar in 2015. Since then, R. okamurae has been spreading rapidly through the submerged euphotic zone, colonizing from 0 to 50 m depth and generating substantial economic and environmental impacts on the Andalusian coasts (southern Spain). More than 40% of marine IAS in the European Union (EU) are macroalgae, representing one of the main threats to biodiversity and ecosystem functioning in coastal habitats. This study presents a monitoring pilot of beached R. okamurae and fresh R. okamurae down to 5 m depth in Tarifa (Cadiz, Spain), combining multispectral remote sensing data collected by sensors on-board Unmanned Aerial Vehicles (UAVs) and satellites, and how this information can be used to support decision-making and policy. We used an UAV flight carried out at Bolonia beach (Tarifa, Spain) on 1st July 2021 and Sentinel-2 (S2) and Landsat-8 (L8) image acquisitions close to the drone flight date. In situ data were also measured on the same date of the flight, and they were used to train the supervised classification Super Vector Machine (SVM) method based on the spectral information obtained for each substrate cover. The results obtained show how multispectral images allow the detection of beached R. okamurae, and the classification accuracy for water, land vegetation, sand and R. okamurae depending on the image resolution (8.3 cm/pixel for UAV flight, 10 m/pixel for S2 and 30 m/pixel for L8). While the UAV imagery precisely delimited the area occupied by this macroalgae, satellite data were capable of detecting its presence, and able to generate early warnings. This study demonstrates the usefulness of multispectral remote sensing techniques to be incorporated in continuous monitoring programmes of the marine IAS R. okamurae in coastal areas. This information is also key to supporting regional, national and European policies in order to adapt strategic management of invasive marine macrophytes.",
        "DOI": "10.3389/fmars.2022.1004012",
        "paper_author": "Roca M.",
        "affiliation_name": "CSIC - Instituto de Ciencias Marinas de Andalucia (ICMAN)",
        "affiliation_city": "Cadiz",
        "affiliation_country": "Spain",
        "affiliation_id": "60009410",
        "affiliation_state": "Cadiz"
    },
    {
        "paper_title": "Data sharing: standards are on the rise",
        "publication": "Nature",
        "citied_by": "0",
        "cover_date": "2022-10-13",
        "Abstract": "NA",
        "DOI": "10.1038/d41586-022-03232-3",
        "paper_author": "Postlethwaite C.F.",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Predicting the number of covid-19 cases in Surabaya using hybrid extreme machine learning with particle swarm optimization",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2022-10-11",
        "Abstract": "Covid-19 has spread to various countries in the world, including Indonesia. Surabaya becomes one of the big cities in Indonesia where the spread of Covid-19 is very fast, so the number of positive cases of Covid-19 is very large and more than 1000 people die because of this disease until November 2020. Prediction of the number of positive cases of Covid-19 can be used to regulate the availability of facilities in hospitals and make plans and policies to overcome this disease outbreak. Many prediction methods have been found, one of which is the Extreme Learning Machine (ELM). ELM has high training speed and accuracy. However, the performance of ELM depends on the number of neurons. When the number of neurons is not precisely determined, the accuracy of prediction becomes worst. Particle Swarm Optimization (PSO) is used to decide the number of neurons. For this reason, this paper proposes a prediction of the Covid-19 cases in the City of Surabaya using the hybrid of ELM and PSO (ELM-PSO). The experiments show that the comparative performance of the proposed methods with several activation functions in the prediction of the Covid-19 cases in the City of Surabaya.",
        "DOI": "10.1063/5.0112007",
        "paper_author": "Tuloli M.H.",
        "affiliation_name": "Brawijaya University",
        "affiliation_city": "Malang",
        "affiliation_country": "Indonesia",
        "affiliation_id": "60069392",
        "affiliation_state": "East Java"
    },
    {
        "paper_title": "Policy interventions and competing management paradigms shape the long-term distribution of forest harvesting across the landscape",
        "publication": "Proceedings of the National Academy of Sciences of the United States of America",
        "citied_by": "2",
        "cover_date": "2022-10-11",
        "Abstract": "Industrial economic models of natural resource management often incentivize the sequential harvesting of resources based on profitability, disproportionately targeting the higher-value elements of the environment. In fisheries, this issue is framed as a problem of “fishing down the food chain” when these elements represent different trophic levels or sequential depletion more generally. Harvesting that focuses on high grading the most profitable, productive, and accessible components of environmental gradients is also thought to occur in the forestry sector. Such a paradigm is inconsistent with a stewardship ethic, entrenched in the forestry literature, that seeks to maintain or enhance forest condition over time. We ask 1) how these conflicting paradigms have influenced patterns of forest harvesting over time and 2) whether more recent conservation-oriented policies influenced these historical harvesting patterns. We use detailed harvest data over a 47-y period and aggregated time series data that span over a century on the central coast of British Columbia, Canada to assess temporal changes in how logging is distributed among various classes of site productivity and terrain accessibility, corresponding to timber value. Most of this record shows a distinct trend of harvesting shifting over time to less productive stands, with some evidence of harvesting occurring in increasingly less accessible forests. However, stewardship-oriented policy changes enacted in the mid-1990s appear to have strongly affected these trends. This illustrates both a profit-maximizing tendency to log down the value chain when choices are unconstrained and the potential of policy choices to impose a greater stewardship ethic on harvesting behavior.",
        "DOI": "10.1073/pnas.2208360119",
        "paper_author": "Benner J.",
        "affiliation_name": "Simon Fraser University",
        "affiliation_city": "Burnaby",
        "affiliation_country": "Canada",
        "affiliation_id": "60018491",
        "affiliation_state": "BC"
    },
    {
        "paper_title": "Output synchronization of multi-agent systems via reinforcement learning",
        "publication": "Neurocomputing",
        "citied_by": "3",
        "cover_date": "2022-10-07",
        "Abstract": "In this paper, the measured input–output data sequences with pinning gain are proposed via the topology of multi-agent systems (MAS), where the requirement of reinforcement learning algorithm on internal state is avoided by pinning gain and the measured data of leader and neighbors. Besides, a data-based tracking state is given, which can be applied to MAS with different control matrices. According to the sequences and tracking state, a distributed control policy and corresponding reinforcement learning algorithm are proposed for the output synchronization. The proposed algorithm overcomes the shortcoming that previous algorithms can not be applied to MAS with different control matrices in the absence of model information and full-state vector. Finally, the effectiveness of proposed algorithm is verified by simulation examples.",
        "DOI": "10.1016/j.neucom.2022.08.006",
        "paper_author": "Liu Y.",
        "affiliation_name": "The State Key Laboratory of Synthetical Automation for Process Industries",
        "affiliation_city": "Shenyang",
        "affiliation_country": "China",
        "affiliation_id": "60119041",
        "affiliation_state": "Liaoning"
    },
    {
        "paper_title": "Real-Time Analysis of Predictors of COVID-19 Infection Spread in Countries in the European Union Through a New Tool",
        "publication": "International Journal of Public Health",
        "citied_by": "4",
        "cover_date": "2022-10-06",
        "Abstract": "Objectives: Real-time data analysis during a pandemic is crucial. This paper aims to introduce a novel interactive tool called Covid-Predictor-Tracker using several sources of COVID-19 data, which allows examining developments over time and across countries. Exemplified here by investigating relative effects of vaccination to non-pharmaceutical interventions on COVID-19 spread. Methods: We combine >100 indicators from the Global COVID-19 Trends and Impact Survey, Johns Hopkins University, Our World in Data, European Centre for Disease Prevention and Control, National Centers for Environmental Information, and Eurostat using random forests, hierarchical clustering, and rank correlation to predict COVID-19 cases. Results: Between 2/2020 and 1/2022, we found among the non-pharmaceutical interventions “mask usage” to have strong effects after the percentage of people vaccinated at least once, followed by country-specific measures such as lock-downs. Countries with similar characteristics share ranks of infection predictors. Gender and age distribution, healthcare expenditures and cultural participation interact with restriction measures. Conclusion: Including time-aware machine learning models in COVID-19 infection dashboards allows to disentangle and rank predictors of COVID-19 cases per country to support policy evaluation. Our open-source tool can be updated daily with continuous data streams, and expanded as the pandemic evolves.",
        "DOI": "10.3389/ijph.2022.1604974",
        "paper_author": "Balogh A.",
        "affiliation_name": "Mannheim Business School gGmbH",
        "affiliation_city": "Mannheim",
        "affiliation_country": "Germany",
        "affiliation_id": "60116646",
        "affiliation_state": "Baden-Wurttemberg"
    },
    {
        "paper_title": "Policy Radar: Creation of a tool for monitoring Planning Instruments in Portugal",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2022-10-04",
        "Abstract": "Planning Instruments (PI) are textual documents in the form of plans or strategies that articulate public policies with objectives and goals by the actions of public authorities. PIs offer a large volume of textual information that can change from time to time. Each PI can contain hundreds of objectives and goals with its own indicators, execution rates, and time limits. The volume of PI's information makes it difficult to monitor the execution of all plans and carry out cross-sectional analyses to perceive parallel activities and possible synergies in addressing public policy problems. The present study seeks to respond to the challenge of systematizing these PIs with a dual purpose: On the one hand, it aims to develop a decision support system that allows policymakers to monitor the execution of the different IPs and identify areas with potential for convergence. At the same time, in an open electronic governance model, the system is intended to be available on a public portal, where citizens and stakeholders can research and follow the various public policy indicators. The project will build an algorithm based on natural language processing (NLP) and machine learning. Through text mining, the algorithm will learn how to extract, categorize and compare information from different PIs, such as operational objectives, goals, and execution rates. The last step will be to feed a search engine that will simplify the navigation among other PIs.",
        "DOI": "10.1145/3560107.3560201",
        "paper_author": "Melo W.M.C.D.",
        "affiliation_name": "Policy and Prospective of Public Administration",
        "affiliation_city": null,
        "affiliation_country": "Portugal",
        "affiliation_id": "128910460",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Fake News Goes Viral! Determination and Analysis of Virality of Socially Relevant Events in Digital Governance",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2022-10-04",
        "Abstract": "Governments, public policy makers and public institutions are utilizing the potential of emerging digital technologies and their products and services for better governance services for citizens. Digital technologies integrated with Artificial Intelligence(AI)/Machine Learning(ML), Social media platforms empower the citizens to share information, emotions, opinions, feedback to masses without regulation instantly. Governments, public institutions and individuals also utilized these benefits of a large citizen base and speed of information diffusion for creating awareness about the government policies and programmes leading to the improvement in public services and product delivery in a more productive and efficient manner. In this information-driven ecosystem, misinformation, and fake news has the potential to influence citizens at very high speed and to a large population resulting in the disruption of governance and political system, law and order situations, challenges to socio and economic ecosystem, and the well-being of the social fabric of the region. This paper makes an attempt to study and quantify the social engagement of content, and its virality in comparison to fake news and real news in governance events and provides a measure of virality. The paper also proposes a model to formulate the social media content to calculate virality score of events. Results show that False/Fake news traveled 5.29 times faster than genuine/real news. The virality score with bot accounts and real accounts are also analyzed. The proposed study will help the government and administration in knowing the speed of fake news/false information and help them to respond swiftly in the digital governance sphere.",
        "DOI": "10.1145/3560107.3560165",
        "paper_author": "Kumar S.",
        "affiliation_name": "University of Delhi",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India",
        "affiliation_id": "60029284",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Automatic structural elucidation of vacancies in materials by active learning",
        "publication": "Physical Chemistry Chemical Physics",
        "citied_by": "9",
        "cover_date": "2022-10-04",
        "Abstract": "Finding the optimum structures of non-stoichiometric or berthollide materials, such as (1D, 2D, 3D) materials or nanoparticles (0D), is challenging due to the huge chemical/structural search space. Computational methods coupled with global optimization algorithms have been used successfully for this purpose. In this work, we have developed an artificial intelligence method based on active learning (AL) or Bayesian optimization for the automatic structural elucidation of vacancies in solids and nanoparticles. AL uses machine learning regression algorithms and their uncertainties to take decisions (from a policy) on the next unexplored structures to be computed, increasing the probability of finding the global minimum with few calculations. The methodology allows an accurate and automated structural elucidation for vacancies, which are common in non-stoichiometric (berthollide) materials, helping to understand chemical processes in catalysis and environmental sciences, for instance. The AL vacancies method was implemented in the quantum machine learning software/agent for material design and discovery (QMLMaterial). Also, two additional acquisition functions for decision making were implemented, besides the expected improvement (EI): the lower confidence bound (LCB) and the probability of improvement (PI). The new software was applied for the automatic structural search for graphite (C36) with 3 (C36-3) and 4 (C36-4) carbon vacancies and C60 (C60-4) fullerene with 4 carbon vacancies. DFTB calculations were used to build the complex search surfaces with reasonably low computational cost. Furthermore, with the AL method for vacancies, it was possible to elucidate the optimum oxygen vacancy distribution in CaTiO3 perovskite by DFT, where a semiconductor behavior results from oxygen vacancies. Throughout the work, a Gaussian process with its uncertainty was employed in the AL framework using different acquisition functions (EI, LCB and PI), and taking into account different descriptors: Ewald sum matrix and sine matrix. Finally, the performance of the proposed AL method was compared to random search and genetic algorithm.",
        "DOI": "10.1039/d2cp02585j",
        "paper_author": "Lourenço M.P.",
        "affiliation_name": "Universidade Federal do Espírito Santo",
        "affiliation_city": "Vitoria",
        "affiliation_country": "Brazil",
        "affiliation_id": "60028426",
        "affiliation_state": "ES"
    },
    {
        "paper_title": "Daily Local-Level Estimates of Ambient Wildfire Smoke PM<inf>2.5</inf>for the Contiguous US",
        "publication": "Environmental Science and Technology",
        "citied_by": "80",
        "cover_date": "2022-10-04",
        "Abstract": "Smoke from wildfires is a growing health risk across the US. Understanding the spatial and temporal patterns of such exposure and its population health impacts requires separating smoke-driven pollutants from non-smoke pollutants and a long time series to quantify patterns and measure health impacts. We develop a parsimonious and accurate machine learning model of daily wildfire-driven PM2.5concentrations using a combination of ground, satellite, and reanalysis data sources that are easy to update. We apply our model across the contiguous US from 2006 to 2020, generating daily estimates of smoke PM2.5over a 10 km-by-10 km grid and use these data to characterize levels and trends in smoke PM2.5. Smoke contributions to daily PM2.5concentrations have increased by up to 5 μg/m3in the Western US over the last decade, reversing decades of policy-driven improvements in overall air quality, with concentrations growing fastest for higher income populations and predominantly Hispanic populations. The number of people in locations with at least 1 day of smoke PM2.5above 100 μg/m3per year has increased 27-fold over the last decade, including nearly 25 million people in 2020 alone. Our data set can bolster efforts to comprehensively understand the drivers and societal impacts of trends and extremes in wildfire smoke.",
        "DOI": "10.1021/acs.est.2c02934",
        "paper_author": "Childs M.L.",
        "affiliation_name": "Stanford University",
        "affiliation_city": "Stanford",
        "affiliation_country": "United States",
        "affiliation_id": "60012708",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "MEMSYS 2022 - Proceedings of the International Symposium on Memory Systems",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2022-10-03",
        "Abstract": "The proceedings contain 12 papers. The topics discussed include: evaluating HPC kernels for processing in memory; dynamic page policy using perceptron learning; a case for amplifying row hammer attacks via cell-coupling in DRAM devices; using many small 1T1C memory arrays in a large and dense multicore processor; in-memory bulk bitwise logic operation for multi-level cell non-volatile memories; a framework for formal verification of DRAM controllers; hybrid refresh: improving DRAM performance by handling weak rows smartly; FPGA-accelerated simulation of variable latency memory systems; unveiling the real performance of LPDDR5 memories; Cronus: computer vision-based machine intelligent hybrid memory management; exploiting data source distribution to enhance NVM reliability; and toward classification of phase change memory and 3D NAND flash SSDs using power-based side-channel analysis in the time-domain.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Learning attention models for resource-constrained, self-adaptive visual sensing applications",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "2",
        "cover_date": "2022-10-03",
        "Abstract": "Resource constraints are one of the main design challenges for wireless sensor network applications and visual sensing networks that employ cameras in particular. The objective in this paper is to enable the sensors to be context-aware by utilizing application-level information, to prioritize parts of an image, and only transmit those parts that contribute most to the utility of the application. We, therefore, study online-learning of visual attention models for the use case of person detection and counting. We analyze how the resulting models can prioritize relevant elements of a partial image, so that object detection remains accurate compared to a random selection strategy when resources for transmission get scarce. Results show that such attention models can be learned also under constraints and converge towards the true models. For the application performance, we observed an average reduction of errors (the number of undetected persons) of 55% compared to policies without a corresponding attention model.",
        "DOI": "10.1145/3538641.3561505",
        "paper_author": "Asad H.A.",
        "affiliation_name": "Norges Teknisk-Naturvitenskapelige Universitet",
        "affiliation_city": "Trondheim",
        "affiliation_country": "Norway",
        "affiliation_id": "60013141",
        "affiliation_state": "Trondelag"
    },
    {
        "paper_title": "Econometric Evaluation of Macro Prudential Policy Effects on Financial Stability",
        "publication": "China Journal of Econometrics",
        "citied_by": "0",
        "cover_date": "2022-10-01",
        "Abstract": "The main goal of macro prudential policies is to maintain financial stability. This paper proposes adopting the macro-econometric policy evaluation method under the Rubin causal effect framework to evaluate the impact of China’s macro prudential policies on financial stability during the sample period 2007-2020. First, the paper constructs a macro prudential policy index to quantitatively measure the intensity of China’s macro prudential policies. Second, the paper uses the systemic financial risk index, evaluates termed the macro as SRISK prudential to measure policies China ’ effects ’s systemic on the financial systemic risk. financial Finally, risk, the cross paper sectoral contagion of systemic financial risk and important intermediate variables in the credit channel. Our empirical findings indicate that loose macro prudential policies can increase the risks of intermediate variables in the credit channel, and the risks lead to a significant rise in SRISK of house sector, but for the SRISK of financial and manufacturing addition to sectors, a significant the cumulative rise in commercial effects in banks 24 periods ’ capital are adequacy not significant. ratio growth, However, tight in macro prudential policies have no significant effects on the other intermediate variables in the credit channel, and further have no obvious effects on SRISK of financial, house and manufacturing sectors. Based on the conclusions, we suggest that systemic risk indicators should be further researched to provide more comprehensive and systematic targets for macro prudential authorities. Moreover, the transmission channel of macro prudential policies on financial stability should be improved to enhance the efficiency of regulation. Finally, more attentions should be paid to the cross-sectoral contagion of systemic financial risk so as to prevent systemic financial risk from a systemic perspective.",
        "DOI": "10.12012/CJoE2022-0069",
        "paper_author": "Ying F.",
        "affiliation_name": "Xiamen University",
        "affiliation_city": "Xiamen",
        "affiliation_country": "China",
        "affiliation_id": "60018205",
        "affiliation_state": "Fujian"
    },
    {
        "paper_title": "THE DEATH OF LAW? COMPUTATIONALLY PERSONALIZED NORMS AND THE RULE OF LAW",
        "publication": "University of Toronto Law Journal",
        "citied_by": "7",
        "cover_date": "2022-10-01",
        "Abstract": "The emergent power of big data analytics makes it possible to replace impersonal general legal rules with personalized, particular norms. We consider arguments that such a move would be generally beneficial, replacing crude, general laws with more efficiently targeted ways of meeting public policy goals and satisfying personal preferences. Those proposals pose a radical, new challenge to the rule of law. Data-driven legal personalization offers some benefits that are worth pursuing, but we argue that the benefits can only legitimately be pursued where doing so is consistent with the agency that the law ought to accord to individuals and with the agency that the law ought to accord to public bodies. The principle of public agency is a prerequisite for the rule of law. The principle of private agency depends on the rule of law. Each is incompatible with the unrestrained computational personalization of law.",
        "DOI": "10.3138/utlj-2021-0011",
        "paper_author": "Endicott T.",
        "affiliation_name": "University of Oxford",
        "affiliation_city": "Oxford",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60026851",
        "affiliation_state": "Oxfordshire"
    },
    {
        "paper_title": "Continuous Heartbeat Prediction Using a Face Recognition Algorithm",
        "publication": "Traitement du Signal",
        "citied_by": "0",
        "cover_date": "2022-10-01",
        "Abstract": "Health providers use the ECG machine to get information about the heart. This information plays a significant role since it tells them about the status of the heart. The ECG machine presents this information in a waveform. During the Covid-19 pandemic, all governments have placed numerous rules and policies to protect people from the virus and from spreading it. One of the rules and policies is to prevent touching surfaces in public places. However, in health care centers, touching surfaces can't be avoided completely since there is a need to touch them or place some wires on the human body such as placing wires to use the ECG machine. In Saudi Arabia, the government has placed a policy in all its buildings, public places, and the private sector to measure the temperature at the entrance. Due to this situation, the idea has come into mind to have a touchless method to measure the heartbeat rate. In this paper, proposing a feasible and reliable method to estimate a continuous heartbeat rate is presented. It uses a face recognition approach to predict the heart pulse continuously in real-time according to colors intensity measurement. Using a segmentation algorithm is involved since the approach takes its input from a video or an image. Several experiments have been conducted on volunteers to verify the obtained results and measure their relative errors. Consequently, the errors are less than 7% which is quite acceptable. At the end of this article, a comparative assessment is performed between the presented approach and some works from literature. This assessment is conducted based on the methodologies being utilized and applied and Mean Absolute Error (MAE). Furthermore, it shows whether those methods require physical contact or not. The obtained results indicate that the implemented system herein outperforms other state-of-the-art methods.",
        "DOI": "10.18280/ts.390506",
        "paper_author": "Alsheikhy A.A.",
        "affiliation_name": "Northern Border University",
        "affiliation_city": "Arar",
        "affiliation_country": "Saudi Arabia",
        "affiliation_id": "60104697",
        "affiliation_state": "Al Hudud ash Shamaliyah"
    },
    {
        "paper_title": "Predicting COVID-19 county-level case number trend by combining demographic characteristics and social distancing policies",
        "publication": "JAMIA Open",
        "citied_by": "4",
        "cover_date": "2022-10-01",
        "Abstract": "Objective: Predicting daily trends in the Coronavirus Disease 2019 (COVID-19) case number is important to support individual decisions in taking preventative measures. This study aims to use COVID-19 case number history, demographic characteristics, and social distancing policies both independently/interdependently to predict the daily trend in the rise or fall of county-level cases. Materials and Methods: We extracted 2093 features (5 from the US COVID-19 case number history, 1824 from the demographic characteristics independently/interdependently, and 264 from the social distancing policies independently/interdependently) for 3142 US counties. Using the top selected 200 features, we built 4 machine learning models: Logistic Regression, Naïve Bayes, Multi-Layer Perceptron, and Random Forest, along with 4 Ensemble methods: Average, Product, Minimum, and Maximum, and compared their performances. Results: The Ensemble Average method had the highest area-under the receiver operator characteristic curve (AUC) of 0.692. The top ranked features were all interdependent features. Conclusion: The findings of this study suggest the predictive power of diverse features, especially when combined, in predicting county-level trends of COVID-19 cases and can be helpful to individuals in making their daily decisions. Our results may guide future studies to consider more features interdependently from conventionally distinct data sources in county-level predictive models. Our code is available at: https://doi.org/10.5281/zenodo.6332944.",
        "DOI": "10.1093/jamiaopen/ooac056",
        "paper_author": "Li M.M.",
        "affiliation_name": "University of California, San Diego",
        "affiliation_city": "La Jolla",
        "affiliation_country": "United States",
        "affiliation_id": "60030612",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Independent double DQN-based multi-agent reinforcement learning approach for online two-stage hybrid flow shop scheduling with batch machines",
        "publication": "Journal of Manufacturing Systems",
        "citied_by": "23",
        "cover_date": "2022-10-01",
        "Abstract": "Two-stage hybrid flow shop scheduling with batch machines and jobs arriving over time is complex and challenging in various real-world production scenarios. For the online scheduling problem, traditional heuristic rules can quickly respond to dynamically arrived jobs, while with poor and unstable performance. To close the research gap in the problem, this paper proposes an independent double deep-q-network-based multi-agent reinforcement learning (MA-IDDQN) approach to produce an adaptive rule for batch forming and scheduling. Specifically, the online scheduling problem is transformed into a cooperative Markov decision process by defining state space, action space, and reward function for different agents. Then, two agents are constructed and trained via double DQN to address the batch forming task and scheduling task respectively. Meanwhile, multi-agent cooperates through the behavior analysis mechanism among agents. Moreover, we designed a ε-greedy policy considering waiting in batch forming to make a reasonable decision through historical data. To validate the proposed approach, 27 instances with different scales are settled and contrasted. By comparing with frequently-used heuristic rules and other deep reinforcement learning methods, the experimental results demonstrate that the MA-IDDQN can integrate online batch forming and scheduling to minimize the total tardiness time effectively.",
        "DOI": "10.1016/j.jmsy.2022.11.001",
        "paper_author": "Wang M.",
        "affiliation_name": "Donghua University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60010953",
        "affiliation_state": "Shanghai"
    },
    {
        "paper_title": "Winter Wheat Yield Prediction Using an LSTM Model from MODIS LAI Products",
        "publication": "Agriculture (Switzerland)",
        "citied_by": "45",
        "cover_date": "2022-10-01",
        "Abstract": "Yield estimation using remote sensing data is a research priority in modern agriculture. The rapid and accurate estimation of winter wheat yields over large areas is an important prerequisite for food security policy formulation and implementation. In most county-level yield estimation processes, multiple input data are used for yield prediction as much as possible, however, in some regions, data are more difficult to obtain, so we used the single-leaf area index (LAI) as input data for the model for yield prediction. In this study, the effects of different time steps as well as the LAI time series on the estimation results were analyzed for the properties of long short-term memory (LSTM), and multiple machine learning methods were compared with yield estimation models constructed by the LSTM networks. The results show that the accuracy of the yield estimation results using LSTM did not show an increasing trend with the increasing step size and data volume, while the yield estimation results of the LSTM were generally better than those of conventional machine learning methods, with the best R2 and RMSE results of 0.87 and 522.3 kg/ha, respectively, in the comparison between predicted and actual yields. Although the use of LAI as a single input factor may cause yield uncertainty in some extreme years, it is a reliable and promising method for improving the yield estimation, which has important implications for crop yield forecasting, agricultural disaster monitoring, food trade policy, and food security early warning.",
        "DOI": "10.3390/agriculture12101707",
        "paper_author": "Wang J.",
        "affiliation_name": "Henan Agricultural University",
        "affiliation_city": "Zhengzhou",
        "affiliation_country": "China",
        "affiliation_id": "60002833",
        "affiliation_state": "Henan"
    },
    {
        "paper_title": "CPG-RL: Learning Central Pattern Generators for Quadruped Locomotion",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "56",
        "cover_date": "2022-10-01",
        "Abstract": "In this letter, we present a method for integrating central pattern generators (CPGs), i.e. systems of coupled oscillators, into the deep reinforcement learning (DRL) framework to produce robust and omnidirectional quadruped locomotion. The agent learns to directly modulate the intrinsic oscillator setpoints (amplitude and frequency) and coordinate rhythmic behavior among different oscillators. This approach also allows the use of DRL to explore questions related to neuroscience, namely the role of descending pathways, interoscillator couplings, and sensory feedback in gait generation. We train our policies in simulation and perform a sim-to-real transfer to the Unitree A1 quadruped, where we observe robust behavior to disturbances unseen during training, most notably to a dynamically added 13.75 kg load representing 115% of the nominal quadruped mass. We test several different observation spaces based on proprioceptive sensing and show that our framework is deployable with no domain randomization and very little feedback, where along with the oscillator states, it is possible to provide only contact booleans in the observation space.",
        "DOI": "10.1109/LRA.2022.3218167",
        "paper_author": "Bellegarda G.",
        "affiliation_name": "École Polytechnique Fédérale de Lausanne",
        "affiliation_city": "Lausanne",
        "affiliation_country": "Switzerland",
        "affiliation_id": "60028186",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Robust Adaptive Ensemble Adversary Reinforcement Learning",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "4",
        "cover_date": "2022-10-01",
        "Abstract": "Reinforcement learning needs to learn policies through trial and error. The unstable policies in the early stage of training make it expensive (and time-consuming) to train directly in the real environment, which may cause disastrous consequences. The popular solution is to use the simulator to train the policy and deploy it in a real environment. However, the modeling error and external disturbance between the simulation and the real environment may fail the physical deployment, resulting in the sim2real transfer problem. In this letter, we propose a novel robust adversarial reinforcement learning framework, which uses the ensemble training of multi-adversarial agents that can adaptively adjust adversaries' strength to enhance RL policy's robustness. More specifically, we take the accumulative reward as feedback and construct a PID controller to adjust the adversary's output magnitude to perform the adversarial training well. Experiments in the simulated and the real environment show that our algorithm improves the generalization ability of the policy for the modeling error and the uncertain disturbance simultaneously, outperforming the next best prior methods across all domains. The algorithm was further proven to be effective in a sim2real transfer task through the load experiment of a real racing drone, and the tracking performance is better than the PID-based flight controller.",
        "DOI": "10.1109/LRA.2022.3220531",
        "paper_author": "Zhai P.",
        "affiliation_name": "Fudan University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60009860",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Sharing is Caring: Guidelines for Sharing in the Electronic Laboratory Notebook (ELN) Chemotion as applied by a Synthesis-oriented Working Group**",
        "publication": "Chemistry-Methods",
        "citied_by": "4",
        "cover_date": "2022-10-01",
        "Abstract": "The documentation and storage of experimental data is crucial in research data management and science in general. With regard to automated data curation and the generation of data for machine learning processes, the collection and sharing of machine-readable data, including negative results, is a key step. The electronic laboratory notebook (ELN) Chemotion provides the possibility to share synthesis data with other scientists taking the mentioned aspects into account. In these guidelines, we offer general information on how to share data in Chemotion and present our sharing policy as a best practice example on how to use Chemotion's sharing functions in a working group with several group members on various hierarchy levels.",
        "DOI": "10.1002/cmtd.202200026",
        "paper_author": "Fink F.",
        "affiliation_name": "Rheinisch-Westfälische Technische Hochschule Aachen",
        "affiliation_city": "Aachen",
        "affiliation_country": "Germany",
        "affiliation_id": "60016653",
        "affiliation_state": "Nordrhein-Westfalen"
    },
    {
        "paper_title": "DAILY STREAMFLOW FORECASTING USING EXTREME LEARNING MACHINE AND OPTIMIZATION ALGORITHM. CASE STUDY: TRA KHUC RIVER IN VIETNAM",
        "publication": "Geographia Technica",
        "citied_by": "1",
        "cover_date": "2022-10-01",
        "Abstract": "Accurate prediction of streamflow plays an important role in water resource management and sustainability. Recent years have seen increased interest in data-based models, compared to the more established physics-based models, due to the accuracy of their predictions. Better results mean greater support for those who are tasked with formulating strategies and writing policy around water resource management. The objective of this study is the development of a state-of-the-art streamflow prediction method based on extreme learning machine (ELM), optimized by both hunger games search (HGS) and social spider optimization (SSO) to make accurate predictions for the Tra Khuc River in Vietnam. Rainfall and flow from 2000 to 2020 at Son Giang station on the Tra Khuc River were used to build the streamflow prediction model. The statistical indices root-mean-square error, mean absolute error, and the coefficient of determination (R²) were applied to assess the predictive ability of the proposed models. The results showed that both optimization algorithms successfully improved the ELM model to predict the streamflow for one day and six days ahead by using data from one day and three days before the day in question. Of the proposed models, the ELM-SSO model scored highest, with R²=0.891 for the one-day-ahead prediction and R²=0.701 for six days ahead. Second was ELM-HGS (R²=0.889 and R²=0.699 for one day and six days respectively), and third was ELM (R²=0.883, R²=0.696). The results demonstrate ELM to be a robust data-driven method for simulating time series regimes that is appropriate for various hydrological applications. The models proposed in this study can be generalized to predict streamflow in rivers around the world.",
        "DOI": "10.21163/GT_2022.172.13",
        "paper_author": "Nguyen H.D.",
        "affiliation_name": "Vietnam National University, Hanoi",
        "affiliation_city": "Hanoi",
        "affiliation_country": "Viet Nam",
        "affiliation_id": "60071364",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Spatial Predictive Modeling of the Burning of Sugarcane Plots in Northeast Thailand with Selection of Factor Sets Using a GWR Model and Machine Learning Based on an ANN-CA",
        "publication": "Symmetry",
        "citied_by": "11",
        "cover_date": "2022-10-01",
        "Abstract": "The main purpose of the study is to apply symmetry principles to general mathematical modelling based on multi-criteria decision making (MCDM) approach for use in development in conjunction with geographic weighted regression (GWR) model and optimize the artificial neural network-cellular automaton (ANN-CA) model for forecasting the sugarcane plot burning area of Northeast Thailand. First, to calculate the service area boundaries of sugarcane transport that caused the burning of sugarcane with a fire radiative power (FRP) values using spatial correlation analysis approach. Second, the analysis of the spatial factors influencing sugarcane burning. The study uses the approach of symmetry in the design of algorithm for finding the optimal service boundary distance (called as cut-off) in the analysis of hot-spot clustering and uses calculations with the geographic information system (GIS) approach, and the final stage is the use of screened independent variable factors to predict the plots of burned sugarcane in 2031. The results showed that the positively related factors for the percentage of cane plot sintering in the sub-area units of each sugar plant’s service were the distance to transport sugarcane plots index and percentage of sugarcane plantations in service areas, while the negative coefficients were FRP differences and density of sugarcane yield factors, according to the analysis with a total of seven spatial variables. The best GWR models display local R2 values at levels of 0.902 to 0.961 in the service zones of Khonburi and Saikaw. An influential set of independent variables can increase the accuracy of the ANN-CA model in forecasting with kappa statistical estimates in the range of 0.81 to 0.85 The results of the study can be applied to other regions of Thailand, including countries with similar sugarcane harvesting industries, to formulate policies to reduce the exposure of sugarcane harvested by burning methods and to support the transportation of sugarcane within the appropriate scope of service so that particulate matter less than 2.5 microns ((Formula presented.)) can be reduced.",
        "DOI": "10.3390/sym14101989",
        "paper_author": "Littidej P.",
        "affiliation_name": "Mahasarakham University",
        "affiliation_city": "Maha Sarakham",
        "affiliation_country": "Thailand",
        "affiliation_id": "60002875",
        "affiliation_state": "Maha Sarakham"
    },
    {
        "paper_title": "Development and Evaluation of Machine Learning-Based High-Cost Prediction Model Using Health Check-Up Data by the National Health Insurance Service of Korea",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "8",
        "cover_date": "2022-10-01",
        "Abstract": "In this study, socioeconomic, medical treatment, and health check-up data from 2010 to 2017 of the National Health Insurance Service (NHIS) of Korea were analyzed. This year’s socioeconomic, treatment, and health check-up data are used to develop a predictive model for high medical expenses in the next year. The characteristic of this study is to derive important variables related to the high cost of domestic medical expenses users by using data on health check-up items conducted by the country. In this study, we tried to classify data and evaluate its performance using classification supervised learning algorithms for high-cost medical expense prediction. Supervised learning for predicting high-cost medical expenses was performed using the logistic regression model, random forest, and XGBoost, which have been known to result the best performance and explanatory power among the machine learning algorithms used in previous studies. Our experimental results show that the XGBoost model had the best performance with 77.1% accuracy. The contribution of this study is to identify the variables that affect the prediction of high-cost medical expenses by analyzing the medical bills using the health check-up variables and the Korea Classification Disease (KCD) large group as input variables. Through this study, it was confirmed that musculoskeletal disorders (M) and respiratory diseases (J), which are the most frequently treated diseases, as important KCD disease groups for high-cost prediction in Korea, affect the future high cost prediction. In addition, it was confirmed that malignant neoplasia diseases (C) with high medical cost per treatment are a group of diseases related to high future medical cost prediction. Unlike previous studies, it is the result of analyzing all disease data, so it is expected that the study will be more meaningful when compared with the results of other national health check-up data.",
        "DOI": "10.3390/ijerph192013672",
        "paper_author": "Choi Y.",
        "affiliation_name": "Kyung Hee University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea",
        "affiliation_id": "60001873",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Machine Learning Method for Prediction of Stock Market Using Real-Time Twitter Data",
        "publication": "Electronics (Switzerland)",
        "citied_by": "17",
        "cover_date": "2022-10-01",
        "Abstract": "Finances represent one of the key requirements to perform any useful activity for humanity. Financial markets, e.g., stock markets, forex, and mercantile exchanges, etc., provide the opportunity to anyone to invest and generate finances. However, to reap maximum benefits from these financial markets, effective decision making is required to identify the trade directions, e.g., going long/short by analyzing all the influential factors, e.g., price action, economic policies, and supply/demand estimation, in a timely manner. In this regard, analysis of the financial news and Twitter posts plays a significant role to predict the future behavior of financial markets, public sentiment estimation, and systematic/idiosyncratic risk estimation. In this paper, our proposed work aims to analyze the Twitter posts and Google Finance data to predict the future behavior of the stock markets (one of the key financial markets) in a particular time frame, i.e., hourly, daily, weekly, etc., through a novel StockSentiWordNet (SSWN) model. The proposed SSWN model extends the standard opinion lexicon named SentiWordNet (SWN) through the terms specifically related to the stock markets to train extreme learning machine (ELM) and recurrent neural network (RNN) for stock price prediction. The experiments are performed on two datasets, i.e., Sentiment140 and Twitter datasets, and achieved the accuracy value of 86.06%. Findings show that our work outperforms the state-of-the-art approaches with respect to overall accuracy. In future, we plan to enhance the capability of our method by adding other popular social media, e.g., Facebook and Google News etc.",
        "DOI": "10.3390/electronics11203414",
        "paper_author": "Albahli S.",
        "affiliation_name": "Qassim University",
        "affiliation_city": "Al-Mulida",
        "affiliation_country": "Saudi Arabia",
        "affiliation_id": "60025518",
        "affiliation_state": "Al Qasim"
    },
    {
        "paper_title": "Graph Neural Networks for Intelligent Modelling in Network Management and Orchestration: A Survey on Communications",
        "publication": "Electronics (Switzerland)",
        "citied_by": "24",
        "cover_date": "2022-10-01",
        "Abstract": "The advancing applications based on machine learning and deep learning in communication networks have been exponentially increasing in the system architectures of enabled software-defined networking, network functions virtualization, and other wired/wireless networks. With data exposure capabilities of graph-structured network topologies and underlying data plane information, the state-of-the-art deep learning approach, graph neural networks (GNN), has been applied to understand multi-scale deep correlations, offer generalization capability, improve the accuracy metrics of prediction modelling, and empower state representation for deep reinforcement learning (DRL) agents in future intelligent network management and orchestration. This paper contributes a taxonomy of recent studies using GNN-based approaches to optimize the control policies, including offloading strategies, routing optimization, virtual network function orchestration, and resource allocation. The algorithm designs of converged DRL and GNN are reviewed throughout the selected studies by presenting the state generalization, GNN-assisted action selection, and reward valuation cooperating with GNN outputs. We also survey the GNN-empowered application deployment in the autonomous control of optical networks, Internet of Healthcare Things, Internet of Vehicles, Industrial Internet of Things, and other smart city applications. Finally, we provide a potential discussion on research challenges and future directions.",
        "DOI": "10.3390/electronics11203371",
        "paper_author": "Tam P.",
        "affiliation_name": "Soonchunhyang University",
        "affiliation_city": "Asan",
        "affiliation_country": "South Korea",
        "affiliation_id": "60025615",
        "affiliation_state": "Chungcheongnam-do"
    },
    {
        "paper_title": "Smart Homes and Families to Enable Sustainable Societies: A Data-Driven Approach for Multi-Perspective Parameter Discovery Using BERT Modelling",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "25",
        "cover_date": "2022-10-01",
        "Abstract": "Homes are the building block of cities and societies and therefore smart homes are critical to establishing smart living and are expected to play a key role in enabling smart, sustainable cities and societies. The current literature on smart homes has mainly focused on developing smart functions for homes such as security and ambiance management. Homes are composed of families and are inherently complex phenomena underlined by humans and their relationships with each other, subject to individual, intragroup, intergroup, and intercommunity goals. There is a clear need to understand, define, consolidate existing research, and actualize the overarching roles of smart homes, and the roles of smart homes that will serve the needs of future smart cities and societies. This paper introduces our data-driven parameter discovery methodology and uses it to provide, for the first time, an extensive, fairly comprehensive, analysis of the families and homes landscape seen through the eyes of academics and the public, using over a hundred thousand research papers and nearly a million tweets. We developed a methodology using deep learning, natural language processing (NLP), and big data analytics methods (BERT and other machine learning methods) and applied it to automatically discover parameters that capture a comprehensive knowledge and design space of smart families and homes comprising social, political, economic, environmental, and other dimensions. The 66 discovered parameters and the knowledge space comprising 100 s of dimensions are explained by reviewing and referencing over 300 articles from the academic literature and tweets. The knowledge and parameters discovered in this paper can be used to develop a holistic understanding of matters related to families and homes facilitating the development of better, community-specific policies, technologies, solutions, and industries for families and homes, leading to strengthening families and homes, and in turn, empowering sustainable societies across the globe.",
        "DOI": "10.3390/su142013534",
        "paper_author": "Alqahtani E.",
        "affiliation_name": "King Abdulaziz University",
        "affiliation_city": "Jeddah",
        "affiliation_country": "Saudi Arabia",
        "affiliation_id": "60004582",
        "affiliation_state": "Makkah al Mukarramah"
    },
    {
        "paper_title": "Actuarial Credibility Approach in Adjusting Initial Cost Estimates of Transport Infrastructure Projects",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "4",
        "cover_date": "2022-10-01",
        "Abstract": "This paper presents a novel methodology based on the modified actuarial credibility approach. It allows for the adjustment of initial cost estimates of public infrastructure projects by accounting for the additional risk/uncertainty factor. Hence, it offers an interesting alternative to other existing forecasting methods. We test our approach by applying data for over 300 major infrastructure projects implemented in Poland between 2004 and 2020. We prove that, despite its simplicity, the actuarial credibility approach can deliver accurate cost estimates compared to more complex methods such as regression analysis (OLS) or machine learning (LASSO). In particular, we show that, although the forecasting accuracy varies among different project categories, actuarial credibility outperforms other forecasting approaches in the majority of cases. As a result, we argue that actuarial credibility should be considered as a relatively simple tool with very modest data requirements that can be easily applied by investors and policy makers in order to improve project planning and avoid cost overruns.",
        "DOI": "10.3390/su142013371",
        "paper_author": "Rokicki B.",
        "affiliation_name": "University of Warsaw",
        "affiliation_city": "Warsaw",
        "affiliation_country": "Poland",
        "affiliation_id": "60013756",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "A Transformer-Based Machine Learning Approach for Sustainable E-Waste Management: A Comparative Policy Analysis between the Swiss and Canadian Systems",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "7",
        "cover_date": "2022-10-01",
        "Abstract": "Efficient e-waste management is crucial to successfully achieve sustainable urban growth universally. The upsurge in e-waste has resulted in countries, including Canada, adopting a wide array of policies associated with sustainable management. In this study, we conducted a mixed-method analysis of Canadian e-waste management policies to showcase the opportunities and limitations of the current system. We examine and compare the effectiveness of electronic waste management strategies in Canada and Switzerland using a comparative policy evaluation and by quantitatively measuring their efficiencies through two efficiency methods, namely a transformer-based, bidirectional, unsupervised machine learning model for natural language processing (NLP) and data envelopment analysis (DEA). Switzerland is utilized as a comparison case due to its robust legal framework that has been in place for proper management e-waste in order to enhance Canada’s electronic waste management system. The policy considerations presented in this study are directed toward urban planners, policy makers, and corporate strategists. These involve a mix of political, economic, social, and environmental planning tools concerning how to communicate and foster competent e-waste management in these countries. This is the first study to incorporate DEA and NLP-based BERT analysis to identify the most efficient policy deployment concerning e-waste management.",
        "DOI": "10.3390/su142013220",
        "paper_author": "Ali S.",
        "affiliation_name": "Ted Rogers School of Management",
        "affiliation_city": "Toronto",
        "affiliation_country": "Canada",
        "affiliation_id": "60189774",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "A Fairer Renewable Energy Policy for Aged Care Communities: Data Driven Insights across Climate Zones",
        "publication": "Buildings",
        "citied_by": "0",
        "cover_date": "2022-10-01",
        "Abstract": "Communal living for older people exists in many different forms, such as suburban communities, lifestyle communities, retirement villages and residential aged care communities (RAC) where electricity is supplied via a main gate meter to the whole community. Australia’s Small-scale Renewable Energy Scheme incentivizes individuals and businesses to install renewable energy systems up to 100 kW peak. A system of this size, however, may not meet a community’s energy needs or sustainability goals. In contrast, other residential dwellings are allowed to install a minimum solar inverter of 5 kW. Therefore, this paper investigates small-scale renewable energy targets on a per bed basis for RACs and the impact of a change from the current 100 kW peak small-scale renewable energy policy. A data driven clustering-based method has been implemented to identify financially optimal photovoltaic (PV) system ratings for ten RACs across four climate zones. Explored are 100 kW peak PV and net zero electricity scenarios. Results show RACs with 5 kW PV per bed can move closer to a net zero electricity goal and generate 800 to 1400 GWh of renewable electricity each year with significant financial savings. A fairer renewable policy, based on kilowatts per bed, is advocated to improve communities’ energy resilience, financial sustainability, and environmental sustainability.",
        "DOI": "10.3390/buildings12101631",
        "paper_author": "Liu A.",
        "affiliation_name": "Queensland University of Technology",
        "affiliation_city": "Brisbane",
        "affiliation_country": "Australia",
        "affiliation_id": "60011019",
        "affiliation_state": "QLD"
    },
    {
        "paper_title": "Food Outlet Access and the Healthiness of Food Available ‘On-Demand’ via Meal Delivery Apps in New Zealand",
        "publication": "Nutrients",
        "citied_by": "9",
        "cover_date": "2022-10-01",
        "Abstract": "Access to unhealthy commodities is a key factor determining consumption, and therefore influences the prevalence of non-communicable diseases. Recently, there has been an increase in the availability of food ‘on-demand’ via meal delivery apps (MDAs). However, the public health and equity impacts of this shift are not yet well understood. This study focused on three MDAs in New Zealand and aimed to answer (1) what is the health profile of the foods being offered on-demand, (2) how many food outlets are available and does this differ by physical access or neighbourhood demographics and (3) does the health profile of foods offered differ by physical access or neighbourhood demographics? A dataset was created by sampling a set of street addresses across a range of demographic variables, and recording the menu items and number of available outlets offered to each address. Machine learning was utilised to evaluate the healthiness of menu items, and we examined if healthiness and the number of available outlets varied by neighbourhood demographics. Over 75% of menu items offered by all MDAs were unhealthy and approximately 30% of all menu items across the three MDAs scored at the lowest level of healthiness. Statistically significant differences by demographics were identified in one of the three MDAs in this study, which suggested that the proportion of unhealthy foods offered was highest in areas with the greatest socioeconomic deprivation and those with a higher proportion of Māori population. Policy and regulatory approaches need to adapt to this novel mode of access to unhealthy foods, to mitigate public health consequences and the effects on population groups already more vulnerable to non-communicable diseases.",
        "DOI": "10.3390/nu14204228",
        "paper_author": "Norriss D.",
        "affiliation_name": "University of Otago, Christchurch",
        "affiliation_city": "Christchurch",
        "affiliation_country": "New Zealand",
        "affiliation_id": "60002143",
        "affiliation_state": "CAN"
    },
    {
        "paper_title": "Electricity Management Policy Applying Data Science and Machine Learning Techniques to Improve Electricity Costs",
        "publication": "Symmetry",
        "citied_by": "0",
        "cover_date": "2022-10-01",
        "Abstract": "This paper studies the actual electricity case of a national university in northern Taiwan, pointing out that many schools will face certain asymmetrical information and practical problems in the development of power systems, such as energy-savings and carbon-reduction policies, collecting electricity fees in each division, reducing the loss of power outages, expanding the power system capacity, and maintaining power distribution equipment. These problems are closely related to electricity costs, which include general electricity fees, unexpected losses caused by power outages, purchases of replacement power equipment, and maintenance fees of distribution equipment. This paper proposes corresponding improvement plans for each of the problems in the above-mentioned actual case studies and assists school power managers in using symmetrical information to formulate the best strategies to improve electricity costs.",
        "DOI": "10.3390/sym14102104",
        "paper_author": "Lee C.Y.",
        "affiliation_name": "Chung Yuan Christian University",
        "affiliation_city": "Taoyuan",
        "affiliation_country": "Taiwan",
        "affiliation_id": "60029740",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Estimating Carbon Sink Strength of Norway Spruce Forests Using Machine Learning",
        "publication": "Forests",
        "citied_by": "4",
        "cover_date": "2022-10-01",
        "Abstract": "Forests sequester atmospheric carbon dioxide (CO2) which is important for climate mitigation. Net ecosystem production (NEP) varies significantly across forests in different regions depending on the dominant tree species, stand age, and environmental factors. Therefore, it is important to evaluate forest NEP and its potential changes under climate change in different regions to inform forestry policy making. Norway spruce (Picea abies) is the most prevalent species in conifer forests throughout Europe. Here, we focused on Norway spruce forests and used eddy covariance-based observations of CO2 fluxes and other variables from eight sites to build a XGBoost machine learning model for NEP estimation. The NEP values from the study sites varied between −296 (source) and 1253 (sink) g C m−2 yr−1. Overall, among the tested variables, air temperature was the most important factor driving NEP variations, followed by global radiation and stand age, while precipitation had a very limited contribution to the model. The model was used to predict the NEP of mature Norway spruce forests in different regions within Europe. The NEP median value was 494 g C m−2 yr−1 across the study areas, with higher NEP values, up to >800 g C m−2 yr−1, in lower latitude regions. Under the “middle-of-the-road” SSP2-4.5 scenario, the NEP values tended to be greater in almost all the studied regions by 2060 with the estimated median of NEP changes in 2041–2060 to be +45 g C m−2 yr−1. Our results indicate that Norway spruce forests show high productivity in a wide area of Europe with potentially future NEP enhancement. However, due to the limitations of the data, the potential decrease in NEP induced by temperature increases beyond the photosynthesis optima and frequent ecosystem disturbances (e.g., drought, bark beetle infestation, etc.) still needs to be evaluated.",
        "DOI": "10.3390/f13101721",
        "paper_author": "Zhao J.",
        "affiliation_name": "Norsk institutt for bioøkonomi",
        "affiliation_city": "As",
        "affiliation_country": "Norway",
        "affiliation_id": "60108626",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Determinants of Electricity Prices in Turkey: An Application of Machine Learning and Time Series Models",
        "publication": "Energies",
        "citied_by": "7",
        "cover_date": "2022-10-01",
        "Abstract": "The study compares the prediction performance of alternative machine learning algorithms and time series econometric models for daily Turkish electricity prices and defines the determinants of electricity prices by considering seven global, national, and electricity-related variables as well as the COVID-19 pandemic. Daily data that consist of the pre-pandemic (15 February 2019–10 March 2020) and the pandemic (11 March 2020–31 March 2021) periods are included. Moreover, various time series econometric models and machine learning algorithms are applied. The findings reveal that (i) machine learning algorithms present higher prediction performance than time series models for both periods, (ii) renewable sources are the most influential factor for the electricity prices, and (iii) the COVID-19 pandemic caused a change in the importance order of influential factors on the electricity prices. Thus, the empirical results highlight the consideration of machine learning algorithms in electricity price prediction. Based on the empirical results obtained, potential policy implications are also discussed.",
        "DOI": "10.3390/en15207512",
        "paper_author": "Ertuğrul H.M.",
        "affiliation_name": "Anadolu Üniversitesi",
        "affiliation_city": "Eskisehir",
        "affiliation_country": "Türkiye",
        "affiliation_id": "60016238",
        "affiliation_state": "Eskisehir"
    },
    {
        "paper_title": "Effective Evaluation of Green and High-Quality Development Capabilities of Enterprises Using Machine Learning Combined with Genetic Algorithm Optimization",
        "publication": "Systems",
        "citied_by": "6",
        "cover_date": "2022-10-01",
        "Abstract": "Studying the impact of green and high-quality development is of great significance to the healthy growth and sustainable development of enterprises. This paper discusses the influencing factors of the green and high-quality development of enterprises from the perspective of ownership structure and innovation ability, aiming to clarify the impact mechanism of these influencing factors on the green development of enterprises, and combined with emerging machine learning technologies, to propose a novel and effective corporate green high-quality development using a regression prediction model for quality development. Linear regression and one-way ANOVA were used to analyze the influence of each variable on the green and high-quality development of the enterprise, and the weight proportions of each influencing factor under the linear model were obtained. Two machine learning models based on the random forest (RF) algorithm and support vector machine algorithm were established, and the random parameters in the two machine learning algorithms were optimized by a genetic algorithm (GA). The reliability and accuracy of machine learning models and multivariate linear models were compared. The results show that the GA–RF model has superior regression performance compared with other prediction models. This paper provides a convenient machine learning model, which can quickly and effectively predict the green and high-quality development of enterprises, and provide help for enterprise decision-making and government policy formulation.",
        "DOI": "10.3390/systems10050128",
        "paper_author": "Zhai D.",
        "affiliation_name": "Harbin Institute of Technology",
        "affiliation_city": "Harbin",
        "affiliation_country": "China",
        "affiliation_id": "60019616",
        "affiliation_state": "Heilongjiang"
    },
    {
        "paper_title": "Applying the Geostatistical Eigenvector Spatial Filter Approach into Regularized Regression for Improving Prediction Accuracy for Mass Appraisal",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "9",
        "cover_date": "2022-10-01",
        "Abstract": "Prediction accuracy for mass appraisal purposes has evolved substantially over the last few decades, facilitated by the evolution in big data, data availability and open source software. Accompanying these advances, newer forms of geo-spatial approaches and machine learning (ML) algorithms have been shown to help improve house price prediction and mass appraisal assessment. Nonetheless, the adoption a of ML within mass appraisal has been protracted and subject to scrutiny by assessment jurisdictions due to their failure to account for spatial autocorrelation and limited practicality in terms of value significant estimates needed for tribunal defense and explainability. Existing research comparing traditional regression approaches has tended to examine unsupervised ML methods such as Random Forest (RF) models which remain more esoteric and less transparent in producing value significant estimates necessary for mass appraisal explainability and defense. Therefore, the purpose of this study is to apply the supervised Regularized regression technique which offers a more transparent alternative, and integrate this with a more nuanced geo-statistical technique, the Eigenvector Spatial Filter (ESF) approach, to more accurately account for spatial autocorrelation and enhance prediction accuracy whilst improving explainability needed for mass appraisal exercises. By undertaking such an approach, the research demonstrates the application of this method can be easily adopted for property tax jurisdictions in a framework which is more interpretable, transparent and useable within mass appraisal given its simple and appealing approach. The findings reveal that the integration of the ESFs improves model explainability, prediction accuracy and spatial residual error compared to baseline classical regression and Elastic-net regularized regression architectures, whilst offering the necessary ‘front-facing’ and flexible structure for in-sample and out-of-sample assessment needed by the assessment community for valuing the unsold housing stock. In terms of policy and practice, the study demonstrates some important considerations for mass appraisal tax assessment and for the improvement of taxation assessment and the alleviation of horizontal and vertical inequity.",
        "DOI": "10.3390/app122010660",
        "paper_author": "McCord M.",
        "affiliation_name": "Ulster University",
        "affiliation_city": "Coleraine",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60020730",
        "affiliation_state": "Londonderry, Northern Ireland"
    },
    {
        "paper_title": "Meta Reinforcement Learning for Optimal Design of Legged Robots",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "12",
        "cover_date": "2022-10-01",
        "Abstract": "The process of robot design is a complex task and the majority of design decisions are still based on human intuition or tedious manual tuning. A more informed way of facing this task is computational design methods where design parameters are concurrently optimized with corresponding controllers. Existing approaches, however, are strongly influenced by predefined control rules or motion templates and cannot provide end-to-end solutions. In this paper, we present a design optimization framework using model-free meta reinforcement learning, and its application to the optimizing kinematics and actuator parameters of quadrupedal robots. We use meta reinforcement learning to train a locomotion policy that can quickly adapt to different designs. This policy is used to evaluate each design instance during the design optimization. We demonstrate that the policy can control robots of different designs to track random velocity commands over various rough terrains. With controlled experiments, we show that the meta policy achieves close-to-optimal performance for each design instance after adaptation. Lastly, we compare our results against a model-based baseline and show that our approach allows higher performance while not being constrained by predefined motions or gait patterns.",
        "DOI": "10.1109/LRA.2022.3211785",
        "paper_author": "Belmonte-Baeza A.",
        "affiliation_name": "ETH Zürich",
        "affiliation_city": "Zurich",
        "affiliation_country": "Switzerland",
        "affiliation_id": "60025858",
        "affiliation_state": "ZH"
    },
    {
        "paper_title": "Forecasting Crop Residue Fires in Northeastern China Using Machine Learning",
        "publication": "Atmosphere",
        "citied_by": "3",
        "cover_date": "2022-10-01",
        "Abstract": "With repeated changes to local crop residue disposal policies in recent years, the distribution and density of crop residue fire events have been irregular in both space and time. A nonlinear and complex relationship between natural and anthropogenic factors often affects the occurrence of crop residue field fires. To overcome this difficulty, we used the Himawari-8 wildfire data for 2018–2021 to verify the likelihood of crop residue fires against the results of three machine learning methods: logistic regression, backpropagation neural network (BPNN), and decision tree (DT). The results showed the verified accuracies of BPNN and DT methods were 68.59 and 79.59%. Meantime, the sensitivity and specificity of DT performed the best, with the value of area under the curve (AUC) 0.82. Furthermore, among all the influencing factors, open burning prohibition constraints, relative humidity and air pressure showed significant correlations with open burning events. As such, BPNN and DT could accurately forecast the occurrence of agricultural fires. The results presented here may improve the ability to forecast agricultural field fires and provide important advances in understanding fire formation in Northeastern China. They would also provide scientific and technical support for crop fire control and air quality forecasting.",
        "DOI": "10.3390/atmos13101616",
        "paper_author": "Bai B.",
        "affiliation_name": "Northeast Institute of Geography and Agroecology",
        "affiliation_city": "Changchun",
        "affiliation_country": "China",
        "affiliation_id": "60007333",
        "affiliation_state": "Jilin"
    },
    {
        "paper_title": "Systematic Machine Translation of Social Network Data Privacy Policies †",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "4",
        "cover_date": "2022-10-01",
        "Abstract": "With the growing popularity of online social networks, one common desire of people is to use of social networking services for establishing social relations with others. The boom of social networking has transformed common users into content (data) contributors. People highly rely on social sites to share their ideas and interests and express opinions. Social network sites store all such activities in a data form and exploit the data for various purposes, e.g., marketing, advertisements, product delivery, product research, and even sentiment analysis, etc. Privacy policies primarily defined in Natural Language (NL) specify storage, usage, and sharing of the user’s data and describe authorization, obligation, or denial of specific actions under specific contextual conditions. Although these policies expressed in Natural Language (NL) allow users to read and understand the allowed (or obliged or denied) operations on their data, the described policies cannot undergo automatic control of the actual use of the data by the entities that operate on them. This paper proposes an approach to systematically translate privacy statements related to data from NL into a controlled natural one, i.e., CNL4DSA to improve the machine processing. The methodology discussed in this work is based on a combination of standard Natural Language Processing (NLP) techniques, logic programming, and ontologies. The proposed technique is demonstrated with a prototype implementation and tested with policy examples. The system is tested with a number of data privacy policies from five different social network service providers. Predominantly, this work primarily takes into account two key aspects: (i) The translation of social networks’ data privacy policy and (ii) the effectiveness and efficiency of the developed system. It is concluded that the proposed system can successfully and efficiently translate any common data policy based on an empirical analysis performed of the obtained results.",
        "DOI": "10.3390/app122010499",
        "paper_author": "Tanoli I.K.",
        "affiliation_name": "Shaheed Zulfikar Ali Bhutto Institute of Science and Technology, Karachi",
        "affiliation_city": "Karachi",
        "affiliation_country": "Pakistan",
        "affiliation_id": "60107419",
        "affiliation_state": "Sindh"
    },
    {
        "paper_title": "Classification and Prediction of Nitrogen Dioxide in a Portuguese Air Quality Critical Zone",
        "publication": "Atmosphere",
        "citied_by": "1",
        "cover_date": "2022-10-01",
        "Abstract": "This study presents classification and prediction exercises to evaluate the future behavior of nitrogen dioxide in a critical air quality zone located in Portugal using a dataset, the time span of which covers the period between 1 September 2021 and 23 July 2022. Three main results substantiate the importance of this research. First, the classification analysis corroborates the idea of a neutrality principle of road traffic on the target since the respective coefficient is significant, but quantitatively close to zero. This result, which may be the first sign of a paradigm shift regarding the adoption of electric vehicles in addition to reflect the success of previously implemented measures in the city of Lisbon, is reinforced by evidence that the carbon monoxide emitted mostly by diesel vehicles exhibits a significant, negative and permanent effect on satisfying the hourly limit value associated with the target. Second, robustness checks confirm that the period between 8 h and 16 h is particularly remarkable for influencing the target. Finally, the predictive exercise demonstrates that the internationally patented Variable Split Convolutional Attention model has the best predictive performance among several deep learning neural network alternatives. Results indicate that the concentration of nitrogen dioxide is expected to be volatile and only a redundant downward trend is likely to be observed. Therefore, in terms of policy recommendations, additional measures to avoid exceeding the legal nitrogen dioxide ceiling at the local level should be focused on reducing carbon monoxide emissions, rather than just being concerned about halting the intensity of road traffic.",
        "DOI": "10.3390/atmos13101672",
        "paper_author": "Ribeiro V.M.",
        "affiliation_name": "Universidade do Porto",
        "affiliation_city": "Porto",
        "affiliation_country": "Portugal",
        "affiliation_id": "60007249",
        "affiliation_state": "Porto"
    },
    {
        "paper_title": "Antimicrobial Challenge in Acute Care Surgery",
        "publication": "Antibiotics",
        "citied_by": "3",
        "cover_date": "2022-10-01",
        "Abstract": "The burden of infections in acute care surgery (ACS) is huge. Surgical emergencies alone account for three million admissions per year in the United States (US) with estimated financial costs of USD 28 billion per year. Acute care facilities and ACS patients represent boost sanctuaries for the emergence, development and transmission of infections and multi-resistant organisms. According to the World Health Organization, healthcare-associated infections affected around 4 million cases in Europe and 1.7 million in the US alone in 2011 with 39,000 and 99,000 directly attributable deaths, respectively. In this scenario, antimicrobial resistance arose as a public-health emergency that worsens patients’ morbidity and mortality and increases healthcare costs. The optimal patient care requires the application of comprehensive evidence-based policies and strategies aiming at minimizing the impact of healthcare associated infections and antimicrobial resistance, while optimizing the treatment of intra-abdominal infections. The present review provides a snapshot of two hot topics, such as antimicrobial resistance and systemic inflammatory response, and three milestones of infection management, such as source control, infection prevention, and control and antimicrobial stewardship.",
        "DOI": "10.3390/antibiotics11101315",
        "paper_author": "Schena C.A.",
        "affiliation_name": "Hôpital Henri Mondor",
        "affiliation_city": "Creteil",
        "affiliation_country": "France",
        "affiliation_id": "60023970",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "An Optimized Data Analysis on a Real-Time Application of PEM Fuel Cell Design by Using Machine Learning Algorithms",
        "publication": "Algorithms",
        "citied_by": "20",
        "cover_date": "2022-10-01",
        "Abstract": "In recent years, machine learning algorithms have been applied in many real-time applications. Crises in the energy sector are the primary challenges experienced today among all countries across the globe, regardless of their economic status. There is a huge demand to acquire and produce environmentally friendly renewable energy and to distribute and utilize it efficiently because of its huge production cost. PEMFC are known for their energy efficiency and comparatively low cost, and can be an alternative energy source. The efficiency of these PEMFC can still be enhanced with the help of advanced technologies like machine learning and artificial intelligence, as they provide an optimal solution to explore the hidden knowledge from the generated data. The proposed model attempts to compare several design techniques with varied humidity levels. To enhance the performance of PEMFC, the various humidification processes were considered during the experimental study. The humidification reduces the heat during energy generation and increases the performance of PEM fuel cell. The humidity levels such as 100%, 50%, and 10% were considered to be tested with the machine learning models. The SVMR, LR, and KNN algorithms were tested and observed with the RMSE value as the evaluation parameters. The observed results show that SVMR has an RMSE rate of 0.0046, the LR method has an RMSE rate of 0.0034, and KNN has an RMSE rate of 0.004. The analysis shows that the LR model provides better accuracy than other models. The LR model enhances the PEMFC performance.",
        "DOI": "10.3390/a15100346",
        "paper_author": "Saco A.",
        "affiliation_name": "Sri Venkateswara College of Engineering &amp; Technology, Chittoor",
        "affiliation_city": "Chittoor",
        "affiliation_country": "India",
        "affiliation_id": "60107508",
        "affiliation_state": "AP"
    },
    {
        "paper_title": "Forecasting SARS-CoV-2 transmission and clinical risk at small spatial scales by the application of machine learning architectures to syndromic surveillance data",
        "publication": "Nature Machine Intelligence",
        "citied_by": "9",
        "cover_date": "2022-10-01",
        "Abstract": "Timely and well-informed syndromic surveillance is essential for effective public health policy. The monitoring of traditional epidemiological indicators can be lagged and misleading, which hampers efforts to identify hotspot locations. The increasing predominance of digitalized healthcare-seeking behaviour necessitates that it is fully exploited for the public benefit of effective pandemic management. Using the highest-resolution spatial data for Google Trends relative search volumes, Google mobility, telecoms mobility, National Health Service Pathways calls and website testing journeys, we have developed a machine learning early indicator modelling approach of SARS-CoV-2 transmission and clinical risk at small geographic scales. We trained shallow learning algorithms as the baseline against a geospatial neural network architecture that we termed the spatio-integrated long short-term memory (SI-LSTM) algorithm. The SI-LSTM algorithm was able to—for the assessed temporal periods—accurately identify hotspot locations over time horizons of a month or more with an accuracy in excess of 99%, and an improved performance of up to 15% against the shallow learning algorithms. Furthermore, in public health operational use, this model highlighted the localized exponential growth of the Alpha variant in late 2020, the Delta variant in April 2021 and the Omicron variant in November 2021 within the United Kingdom prior to their spatial dispersion and growth being confirmed by clinical data.",
        "DOI": "10.1038/s42256-022-00538-9",
        "paper_author": "Ward T.",
        "affiliation_name": "UK Health Security Agency",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60267654",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Unsupervised Representation Learning of GRACE Improves Groundwater Predictions",
        "publication": "Water (Switzerland)",
        "citied_by": "3",
        "cover_date": "2022-10-01",
        "Abstract": "Groundwater is a crucial source of the world’s drinking and irrigation water. Nonetheless, it is being rapidly depleted in many parts of the world. To enact policy decisions to preserve this precious resource, policymakers need real-time data on the groundwater levels in their local area. However, groundwater monitoring wells are costly and scarce in supply. The use of satellite imagery is a promising alternative with its ability to provide continuous information over a large area. Machine learning has also emerged as an alternative to computationally intensive physics-based models. However, advancements in machine learning such as unsupervised learning methods have never been translated to groundwater modeling. Thus, in this paper, learned representations were generated for the GRACE satellite for the first time. When used as an input to groundwater prediction models, the learned representations reduce the root mean square error (RMSE) by up to 19% and improve the Nash–Sutcliffe efficiency (NSE) by up to 8x compared to traditional satellite data inputs at three different spatial scales: national, state, and county. The learned representations are able to discern fine-grained patterns from the coarse satellite data, globally downscaling the GRACE satellite. Crucially, the globally trained representations have the potential to improve the performance of virtually every machine learning-based groundwater prediction model. With accurate measurements, local officials are empowered to make proactive decisions to ensure the stability of their region’s water.",
        "DOI": "10.3390/w14192947",
        "paper_author": "Ram A.P.",
        "affiliation_name": "Lexington High School",
        "affiliation_city": "Lexington",
        "affiliation_country": "United States",
        "affiliation_id": "100473575",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Solving a Simple Geduldspiele Cube with a Robotic Gripper via Sim-to-Real Transfer",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "1",
        "cover_date": "2022-10-01",
        "Abstract": "Geduldspiele cubes (also known as patience cubes in English) are interesting problems to solve with robotic systems on the basis of machine learning approaches. Generally, highly dexterous hand and finger movement is required to solve them. In this paper, we propose a reinforcement-learning-based approach to solve simple geduldspiele cubes of a flat plane, a convex plane, and a concave plane. The key idea of the proposed approach is that we adopt a sim-to-real framework in which a robotic agent is virtually trained in simulation environment based on reinforcement learning, then the virtually trained robotic agent is deployed into a physical robotic system and evaluated for tasks in the real world. We developed a test bed which consists of a dual-arm robot with a patience cube in a gripper and the virtual avatar system to be trained in the simulation world. The experimental results showed that the virtually trained robotic agent was able to solve simple patience cubes in the real world as well. Based on the results, we could expect to solve the more complex patience cubes by augmenting the proposed approach with versatile reinforcement learning algorithms.",
        "DOI": "10.3390/app121910124",
        "paper_author": "Yoo J.H.",
        "affiliation_name": "Yonsei University Mirae Campus",
        "affiliation_city": "Wonju",
        "affiliation_country": "South Korea",
        "affiliation_id": "60107290",
        "affiliation_state": "Gangwon-do"
    },
    {
        "paper_title": "Predicting Flood Hazards in the Vietnam Central Region: An Artificial Neural Network Approach",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "9",
        "cover_date": "2022-10-01",
        "Abstract": "Flooding as a hazard has negatively impacted Vietnam’s agriculture, economy, and infrastructure with increasing intensity because of climate change. Flood hazards in Vietnam are difficult to combat, as Vietnam is densely populated with rivers and canals. While there are attempts to lessen the damage through hazard mitigation policies, such as early evacuation warnings, these attempts are made heavily reliant on short-term traditional statistical models and physical hydrology modeling, which provide suboptimal results. The current situation is caused by the fragmented approach from the Vietnamese government and exacerbates a need for more centralized and robust flood predictive systems. Local governments need to employ their own prediction models which often lack the capacity to draw key insights from limited flood occurrences. Given the robustness of machine learning, especially in low data settings, in this study, we attempt to introduce an artificial neural network model with the aim to create long-term forecast and compare it with other machine learning approaches. We trained the models using different variables evaluated under three characteristics: climatic, hydrological, and socio-economic. We found that our artificial neural network model performed substantially better both in performance metrics (91% accuracy) and relative to other models and can predict well flood hazards in the long term.",
        "DOI": "10.3390/su141911861",
        "paper_author": "Pham Quang M.",
        "affiliation_name": "Viet Nam National University Ho Chi Minh City",
        "affiliation_city": "Ho Chi Minh City",
        "affiliation_country": "Viet Nam",
        "affiliation_id": "60078566",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Improving Estimates and Change Detection of Forest Above-Ground Biomass Using Statistical Methods",
        "publication": "Remote Sensing",
        "citied_by": "14",
        "cover_date": "2022-10-01",
        "Abstract": "Forests store approximately as much carbon as is in the atmosphere, with potential to take in or release carbon rapidly based on growth, climate change and human disturbance. Above-ground biomass (AGB) is the largest carbon pool in most forest systems, and the quickest to change following disturbance. Quantifying AGB on a global scale and being able to reliably map how it is changing, is therefore required for tackling climate change by targeting and monitoring policies. AGB can be mapped using remote sensing and machine learning methods, but such maps have high uncertainties, and simply subtracting one from another does not give a reliable indication of changes. To improve the quantification of AGB changes it is necessary to add advanced statistical methodology to existing machine learning and remote sensing methods. This review discusses the areas in which techniques used in statistical research could positively impact AGB quantification. Nine global or continental AGB maps, and a further eight local AGB maps, were investigated in detail to understand the limitations of techniques currently used. It was found that both modelling and validation of maps lacked spatial consideration. Spatial cross validation or other sampling methods, which specifically account for the spatial nature of this data, are important to introduce into AGB map validation. Modelling techniques which capture the spatial nature should also be used. For example, spatial random effects can be included in various forms of hierarchical statistical models. These can be estimated using frequentist or Bayesian inference. Strategies including hierarchical modelling, Bayesian inference, and simulation methods can also be applied to improve uncertainty estimation. Additionally, if these uncertainties are visualised using pixelation or contour maps this could improve interpretation. Improved uncertainty, which is commonly between 30% and 40%, is in addition needed to produce accurate change maps which will benefit policy decisions, policy implementation, and our understanding of the carbon cycle.",
        "DOI": "10.3390/rs14194911",
        "paper_author": "Turton A.E.",
        "affiliation_name": "The University of Edinburgh",
        "affiliation_city": "Edinburgh",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60027272",
        "affiliation_state": "Scotland"
    },
    {
        "paper_title": "The Impact of Reduced Fire Risk Cigarettes Regulation on Residential Fire Incidents, Mortality and Health Service Utilisation in New South Wales, Australia",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "6",
        "cover_date": "2022-10-01",
        "Abstract": "Smoking materials are a common ignition source for residential fires. In Australia, reduced fire risk (RFR) cigarettes regulation was implemented in 2010. However, the impact of this regulation on residential fires is unknown. This paper examines the impact of the RFR cigarettes regulation on the severity and health outcomes of fire incidents in New South Wales (NSW), Australia, from 2005 to 2014. Fire department data from 2005 to 2014 were linked with ambulance, emergency department, hospital, outpatient burns clinic and mortality datasets for NSW. Negative binomial regression analysis was performed to assess the changes to fire incidents’ severity pre- and post-RFR cigarettes regulation. There was an 8% reduction in total fire incidents caused by smokers’ materials post-RFR cigarettes regulation. Smokers’ materials fire incidents that damaged both contents and structure of the building, where fire flames extended beyond the room of fire origin, with over AUD 1000 monetary damage loss, decreased by 18, 22 and 12%, respectively. RFR cigarettes regulation as a fire risk mitigation has positively impacted the residential fire incident outcomes. This provides support for regulation of fire risk protective measures and bestows some direction for other fire safety policies and regulations.",
        "DOI": "10.3390/ijerph191912481",
        "paper_author": "Ghassempour N.",
        "affiliation_name": "Western Sydney University",
        "affiliation_city": "Penrith",
        "affiliation_country": "Australia",
        "affiliation_id": "60017803",
        "affiliation_state": "NSW"
    },
    {
        "paper_title": "Integrated Framework to Assess the Extent of the Pandemic Impact on the Size and Structure of the E-Commerce Retail Sales Sector and Forecast Retail Trade E-Commerce",
        "publication": "Electronics (Switzerland)",
        "citied_by": "10",
        "cover_date": "2022-10-01",
        "Abstract": "With customers’ increasing reliance on e-commerce and multimedia content after the outbreak of COVID-19, it has become crucial for companies to digitize their business methods and models. Consequently, COVID-19 has highlighted the prominence of e-commerce and new business models while disrupting conventional business activities. Hence, assessing and forecasting e-commerce growth is currently paramount for e-market planners, market players, and policymakers alike. This study sources data for the global e-commerce market leader, the US, and proposes an integrated framework that encompasses automated algorithms able to estimate six statistical and machine-learning univariate methods in order to accomplish two main tasks: (i) to produce accurate forecasts for e-commerce retail sales (e-sale) and the share of e-commerce in total retail sales (e-share); and (ii) to assess in quantitative terms the pandemic impact on the size and structure of the e-commerce retail sales sector. The results confirm that COVID-19 has significantly impacted the trend and structure of the US retail sales sector, producing cumulative excess (or abnormal) retail e-sales of $227.820 billion and a cumulative additional e-share of 10.61 percent. Additionally, estimations indicate a continuation of the increasing trend, with point estimates of $378.691 billion for US e-commerce retail sales that are projected to account for 16.72 percent of total US retail sales by the end of 2025. Nonetheless, the current findings also document that the growth of e-commerce is not a consequence of the COVID-19 crisis, but that the pandemic has accelerated the evolution of the e-commerce sector by at least five years. Overall, the study concludes that the shift towards e-commerce is permanent and, thus, governments (especially in developing countries) should prioritize policies aimed at harnessing e-commerce for sustainable development. Furthermore, in light of the research findings, digital transformation should constitute a top management priority for retail businesses.",
        "DOI": "10.3390/electronics11193194",
        "paper_author": "Tudor C.",
        "affiliation_name": "Bucharest University of Economic Studies",
        "affiliation_city": "Bucharest",
        "affiliation_country": "Romania",
        "affiliation_id": "60107810",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Drivers of energy efficiency for manufacturing SMEs in Eurasian countries: a profiling analysis using machine learning techniques",
        "publication": "Energy Efficiency",
        "citied_by": "4",
        "cover_date": "2022-10-01",
        "Abstract": "This study profiles manufacturing small and medium-sized enterprises (SMEs) in Eurasian countries regarding their practices of energy efficiency investments and energy management techniques. Given that the energy efficiency gap could be larger for SMEs because of the barriers identified in the related literature, the profiling of SMEs regarding their energy efficiency practices could help design specific policies that could be adopted for SMEs with a higher likelihood of insufficient energy efficiency investments. Advanced machine learning techniques, such as the random forest algorithm, enable us to perform such profiling. In profiling SMEs, the article uses the group enterprise survey collected by the European Bank for Reconstruction and Development-European Investment Bank-World Bank. The results of the random forest algorithm suggest that the most important input variable to identify the firm behavior to make an effort to enhance energy efficiency or adopt any energy management method is the sector of the firm, followed by firm size, number of skilled workers, the expertise of the top manager, and the firm’s experience. Contrary to the main findings in the literature, the firm’s ownership structure is the least important factor in forecasting its energy efficiency efforts. The elements of a clean energy strategy do not matter for efforts to enhance the energy efficiency, either. These results suggest that if policymakers in Eurasia were to design policies for manufacturing SMEs to make them invest more in energy efficiency, they should address smaller, younger enterprises with relatively less human capital when giving public subsidies.",
        "DOI": "10.1007/s12053-022-10060-x",
        "paper_author": "Ozbugday F.C.",
        "affiliation_name": "Ankara Yildirim Beyazit University",
        "affiliation_city": "Ankara",
        "affiliation_country": "Turkey",
        "affiliation_id": "60106507",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Consolidation of structure of high noise data by a new noise index and reinforcement learning",
        "publication": "Information Sciences",
        "citied_by": "3",
        "cover_date": "2022-10-01",
        "Abstract": "Data denoising is an essential issue in machine learning and computer vision. However, most existing denoising methods can handle only low-noise data, because it is difficult for these methods to obtain the true structures of high-noise data. An existing partial solution is to consolidate the structures of data by moving each sample to its near high density region. It actually regards all non-noisy samples as noisy ones, but non-noisy samples are important for constructing the structures of high-noise data so they should not be moved. To address this problem, we propose a new denoising method called Denoising by a new Noise index and Reinforcement learning (DNR). Firstly, it detects each noisy sample by its density and the distance between this sample and the center of its neighbors. A noisy sample usually has a low density and most neighbors of this sample will be on the same side of it, leading to the center of its neighbors far from it, especially for high-noise data. Secondly, for each detected noisy sample, DNR models its movement as a Markov decision process to store the experience in this movement. Finally, NDR learns a policy to iteratively move each detected noisy sample to its near high density region by learns the experience of this movement in reinforcement learning. The learned experience can effectively help the movement adapt to the high-noise in real-world cases. In DNR, the structures of high-noise data can be well consolidated by our detection and movement of noisy samples. To prove the rationality of DNR, we theoretically analyze its convergence. Then, we perform the experiment to illustrate that DNR can better denoise high-noise data than existing denoising methods. The source code can be downloaded from https://github.com/TianyiHuang2022.",
        "DOI": "10.1016/j.ins.2022.10.008",
        "paper_author": "Huang T.",
        "affiliation_name": "University of Electronic Science and Technology of China",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China",
        "affiliation_id": "60005465",
        "affiliation_state": "Sichuan"
    },
    {
        "paper_title": "Optimization of Dry Weight Assessment in Hemodialysis Patients via Reinforcement Learning",
        "publication": "IEEE Journal of Biomedical and Health Informatics",
        "citied_by": "5",
        "cover_date": "2022-10-01",
        "Abstract": "Dry weight (DW), defined as the lowest tolerated postdialysis weight following the ultrafiltration (UF) of excess fluid volume, is essential for any dialysis prescription for hemodialysis (HD) patients. However, there is no gold standard for DW assessment, and the difficulty of its accurate assessment increases given individual variations and the dynamic changes caused by the uncertainty of patients' condition. Therefore, the current empirical evaluation process is often crude, imprecise, experience-dependent, and energy-consuming. Here, we highlight the personalized dynamic changes in DW over time rather than the more accurate DW assessments at some point in time and formulate the DW evaluation problem into a sequential decision-making process using the Markov decision process (MDP) framework. A reinforcement learning (RL) algorithm based on a dueling double deep Q-network (Duel-DDQN) is proposed to optimize the DW assessment policy, and a multifaceted inspection is applied to assess policy effectiveness and safety. We utilize ten years of data from the Kidney Disease Center, enrolling 750 HD patients and 243,287 dialysis sessions. Good model calibration is confirmed, and off-policy evaluation demonstrates that our policy outperforms other policies, suggesting a decrease of 7.71% in the expected 5-year mortality rate and of 13.44% in the incidence of intradialytic symptoms compared with those of clinicians' strategy. The RL policy adjusts DW more frequently, responds to DW changes more actively, and observes a larger feature space. It is hoped that the proposed solution will help clinicians assess and monitor DW dynamically, making the estimation process more refined, personalized, and intelligent.",
        "DOI": "10.1109/JBHI.2022.3192021",
        "paper_author": "Yang Z.",
        "affiliation_name": "College of Biomedical Engineering &amp; Instrument Science, Zhejiang University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60117749",
        "affiliation_state": "Zhejiang"
    },
    {
        "paper_title": "Edge-enhanced graph neural network for DU-CU placement and lightpath provision in X-Haul networks",
        "publication": "Journal of Optical Communications and Networking",
        "citied_by": "21",
        "cover_date": "2022-10-01",
        "Abstract": "The emerging mobile services have imposed several new challenges on the radio access network (RAN), which stimulates its evolution from cloud-RAN to next-generation RAN (NG-RAN). In NG-RAN, baseband functions are redefined as the radio unit (RU), distributed unit (DU), and central unit (CU), which are, respectively, connected by optical front/mid/backhaul (X-Haul) networks. The DU-CU placement and lightpath provision need an elaborate strategy for allocating resources tailored to user requests. However, most existing methods fail to fully perceive network conditions and then make inappropriate solutions, which may result in over-consumption of processing and transmission resources. Therefore, we propose a reinforcement-learning-based DU-CU placement and lightpath provision strategy using an edge-enhanced graph neural network, i.e, EGNN-RL. The EGNN is leveraged to adequately exploit graph-structured features in X-Haul networks, while proximal-policy-optimization-based RL is introduced to maintain policy stability. In addition, this problem is formulated as an integer linear programming model to find the optimal solution. We validate the proposed strategy under different service types (i.e., enhanced mobile broadband, ultra-reliable low latency communication, and massive machine connections) and different numbers of services, respectively. The results show that our scheme can achieve higher resource efficiency compared with existing methods. Moreover, it also adapts to new networks with the same nodes through fine-tuning the model. This can decrease nearly 15% of the retaining time and obtain similar performance with retaining.",
        "DOI": "10.1364/JOCN.465369",
        "paper_author": "Wang R.",
        "affiliation_name": "Beijing University of Posts and Telecommunications",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60016930",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Opioid Use Disorder, Cannabis Use Disorder, and a Mindfulness Intervention Affecting Pain-Related Neural Substrates",
        "publication": "American Journal of Psychiatry",
        "citied_by": "0",
        "cover_date": "2022-10-01",
        "Abstract": "NA",
        "DOI": "10.1176/appi.ajp.20220713",
        "paper_author": "Kalin N.H.",
        "affiliation_name": "University of Wisconsin School of Medicine and Public Health",
        "affiliation_city": "Madison",
        "affiliation_country": "United States",
        "affiliation_id": "60025553",
        "affiliation_state": "WI"
    },
    {
        "paper_title": "Towards optimized TBM cutter changing policies with reinforcement learning",
        "publication": "Geomechanik und Tunnelbau",
        "citied_by": "4",
        "cover_date": "2022-10-01",
        "Abstract": "In tunnel boring machine (TBM) excavation, cutter maintenance is necessary, but the time for it has to be minimized for efficiency. Although there is extensive literature on TBM cutter wear and predictive maintenance for different industrial applications, there is no optimized policy for cutter changes in TBM tunnelling today. This study aims to investigate the application of reinforcement learning (RL) – a branch of machine learning – for finding optimized policies for cutter changing that maximize the number of working cutters and minimize the maintenance effort. A simulation of a TBM excavation process is developed that focuses on the cutter wear and an agent that controls when cutters must be changed. The simulation uses generated parameters that indicate the cutter life, but the results could be transferred to real sensor data in future excavations once that level of development is reached. The article presents the first results from this RL scenario which can give valuable insights into TBM excavation logistics and presents a challenging multiaction-selection RL problem.",
        "DOI": "10.1002/geot.202200032",
        "paper_author": "Erharter G.H.",
        "affiliation_name": "Norges Geotekniske Institutt",
        "affiliation_city": "Oslo",
        "affiliation_country": "Norway",
        "affiliation_id": "60016961",
        "affiliation_state": "Oslo"
    },
    {
        "paper_title": "Reprint of The new paradigm of economic complexity",
        "publication": "Research Policy",
        "citied_by": "26",
        "cover_date": "2022-10-01",
        "Abstract": "Economic complexity offers a potentially powerful paradigm to understand key societal issues and challenges of our time. The underlying idea is that growth, development, technological change, income inequality, spatial disparities, and resilience are the visible outcomes of hidden systemic interactions. The study of economic complexity seeks to understand the structure of these interactions and how they shape various socioeconomic processes. This emerging field relies heavily on big data and machine learning techniques. This brief introduction to economic complexity has three aims. The first is to summarize key theoretical foundations and principles of economic complexity. The second is to briefly review the tools and metrics developed in the economic complexity literature that exploit information encoded in the structure of the economy to find new empirical patterns. The final aim is to highlight the insights from economic complexity to improve prediction and political decision-making. Institutions including the World Bank, the European Commission, the World Economic Forum, the OECD, and a range of national and regional organizations have begun to embrace the principles of economic complexity and its analytical framework. We discuss policy implications of this field, in particular the usefulness of building recommendation systems for major public investment decisions in a complex world.",
        "DOI": "10.1016/j.respol.2022.104568",
        "paper_author": "Balland P.A.",
        "affiliation_name": "Universiteit Utrecht",
        "affiliation_city": "Utrecht",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60007989",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Stochastic parallel machine scheduling using reinforcement learning",
        "publication": "Journal of Advanced Manufacturing and Processing",
        "citied_by": "12",
        "cover_date": "2022-10-01",
        "Abstract": "In a high-mix and low-volume manufacturing facility, heterogeneous jobs introduce frequent reconfiguration of machines which increases the chance of unplanned machine breakdowns. As machines are often nonidentical and their performance degrades over time, it is critical to consider the heterogeneity and non-stationarity of the machines during scheduling. We propose a reinforcement learning-based framework with a novel sampling method to train the agent to schedule heterogeneous jobs on non-stationary unreliable parallel machines to minimize weighted tardiness. The results indicate that the new sampling approach expedites the learning process and the resulting policy significantly outperforms static dispatching rules.",
        "DOI": "10.1002/amp2.10119",
        "paper_author": "Julaiti J.",
        "affiliation_name": "Penn State College of Engineering",
        "affiliation_city": "University Park",
        "affiliation_country": "United States",
        "affiliation_id": "60147936",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "What Do Twitter Users Think about Climate Change? Characterization of Twitter Interactions Considering Geographical, Gender, and Account Typologies Perspectives",
        "publication": "Weather, Climate, and Society",
        "citied_by": "5",
        "cover_date": "2022-10-01",
        "Abstract": "Climate change (CC) is a topical issue of profound social interest. This paper aims to analyze the sentiments expressed in Twitter interactions in relation to CC. The study is performed considering the geographical and gender perspectives as well as different user typologies (individual users or companies). A total of 92 474 Twitter messages were utilized for the study. These are characterized by analyzing sentiment polarity and identifying the underlying topics related to climate change. Polarity is examined utilizing different commercial algorithms such as Valence Aware Dictionary and Sentiment Reasoner (VADER) and TextBlob, in conjunction with a procedure that uses word embedding and clustering techniques in an unsupervised machine learning approach. In addition, hypothesis testing is applied to inspect whether a gender independence exists or not. The topics are identified using latent Dirichlet allocation (LDA) and the usage of n-grams is explored. The topics identified are (in descending order of importance) CC activism, biodiversity, CC evidence, sustainability, CC awareness, pandemic, net zero, CC policies and finances, government action, and climate emergency. Moreover, globally speaking, it is found that the interactions on all topics are predominantly negative, and they are maintained as such for both men and women. If the polarity by topic and country is considered, it is also negative in most countries, although there are several notable exceptions. Finally, the presence of organizations and their perspective is studied, and results suggest that organizations post with more frequency when addressing topics such as sustainability, CC awareness, and net zero topics.",
        "DOI": "10.1175/WCAS-D-21-0163.1",
        "paper_author": "Mouronte-López M.L.",
        "affiliation_name": "Universidad Francisco de Vitoria",
        "affiliation_city": "Pozuelo de Alarcon",
        "affiliation_country": "Spain",
        "affiliation_id": "60108692",
        "affiliation_state": "Madrid"
    },
    {
        "paper_title": "Nonlinear and threshold effects of the built environment on e-scooter sharing ridership",
        "publication": "Journal of Transport Geography",
        "citied_by": "49",
        "cover_date": "2022-10-01",
        "Abstract": "Understanding the relationship between the built environment and e-scooter sharing (ESS) usage is important because it could help planners determine the high-demand area and design an effective investment plan to promote the use of micromobility. Previous studies usually assume that the relationship is linear, which may lead to inaccurate ridership prediction and ineffective policy. Thus, this study explores the nonlinear and threshold effects of the built environment on ESS ridership in Los Angeles using the gradient boosting decision tree. Fourteen built environment and ten demographic variables are selected as independent variables. We find that the built environment accounts for 91.66% of the total relative importance. The ten most important variables are intersection density, road density, public transit station density, restaurant density, employment density, distance to the center, population density, proportion of park area, parking density, and bike lane density. The nonlinear and threshold effects of the built environment on ESS ridership are determined. By using two spatial analysis units (census tract and census block group) and four temporal analysis units (spring, summer, autumn, and winter), the modifiable areal unit problem and the modifiable temporal unit problem are revealed.",
        "DOI": "10.1016/j.jtrangeo.2022.103453",
        "paper_author": "Yang H.",
        "affiliation_name": "Southwest Jiaotong University",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China",
        "affiliation_id": "60010421",
        "affiliation_state": "Sichuan"
    },
    {
        "paper_title": "Density-based clustering multiple linear regression model of energy consumption for electric vehicles",
        "publication": "Sustainable Energy Technologies and Assessments",
        "citied_by": "12",
        "cover_date": "2022-10-01",
        "Abstract": "With the increasing popularity of electric vehicles, energy consumption has become a key performance indicator for electric vehicle drivers, automakers and policy makers. Accurate and real-time prediction of energy consumption under real-world driving conditions is critical to reducing “range anxiety” and can support optimization of battery size, energy-saving route planning and charging facility operation. In this paper, data collected from 988 electric vehicles of the same model for one year in Zhengzhou, China, are obtained to study the energy consumption of electric vehicles in actual driving conditions. An improved Density-Based Spatial Clustering of Applications with Noise (DBSCAN) model were established to classify the driving behaviors of the drivers. Then the key factors of energy consumption including velocity, accelerated velocity and temperature are studied and modeled. With that, an improved density-based clustering multiple linear regression model for energy prediction were established with driving behavior classification. The density-based clustering multiple linear regression model (DBC-MLR) has better prediction accuracy and can grasp the training features in energy consumption prediction in real driving. The proposed method shows a root mean error (RMSE) of 3.008 kwh/100 km, which is reduced by 11.3 % and 18.4 % compared to conventional machine learning method and multiple linear regression method respectively.",
        "DOI": "10.1016/j.seta.2022.102614",
        "paper_author": "Chen Y.",
        "affiliation_name": "Wuhan University of Technology",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60022414",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Using explainable machine learning to understand how urban form shapes sustainable mobility",
        "publication": "Transportation Research Part D: Transport and Environment",
        "citied_by": "37",
        "cover_date": "2022-10-01",
        "Abstract": "Municipalities are increasingly acknowledging the importance of urban form interventions that can reduce intra-city car travel in achieving more sustainable cities. Current academic knowledge for supporting such policies falls short in providing the spatial details required to plan specific interventions. Here, we develop an explainable machine learning framework to identify location-specific relevance of built environment for urban motorised travel, using a sample of 3.5 million car commutes over one year in Berlin and high-resolution urban form data. Results demonstrate that subcenters play a vital role in reducing commuting-related travel distance, giving support to the 15-minute city hypothesis. Observed threshold effects of induced CO2 emissions require low-carbon-policies targeted towards densifying the inner city while releasing peripheral low income communities from car dependence. This research provides a starting point for increasingly rich big data analyses of urban form for creating low-carbon and inclusive urban planning strategies.",
        "DOI": "10.1016/j.trd.2022.103442",
        "paper_author": "Wagner F.",
        "affiliation_name": "Mercator Research Institute on Global Commons and Climate Change",
        "affiliation_city": "Berlin",
        "affiliation_country": "Germany",
        "affiliation_id": "60159447",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Meta-reinforcement learning for the tuning of PI controllers: An offline approach",
        "publication": "Journal of Process Control",
        "citied_by": "20",
        "cover_date": "2022-10-01",
        "Abstract": "Meta-learning is a branch of machine learning which trains neural network models to synthesize a wide variety of data in order to rapidly solve new problems. In process control, many systems have similar and well-understood dynamics, which suggests it is feasible to create a generalizable controller through meta-learning. In this work, we formulate a meta reinforcement learning (meta-RL) control strategy that can be used to tune proportional–integral controllers. Our meta-RL agent has a recurrent structure that accumulates “context” to learn a system's dynamics through a hidden state variable in closed-loop. This architecture enables the agent to automatically adapt to changes in the process dynamics. In tests reported here, the meta-RL agent was trained entirely offline on first order plus time delay systems, and produced excellent results on novel systems drawn from the same distribution of process dynamics used for training. A key design element is the ability to leverage model-based information offline during training in simulated environments while maintaining a model-free policy structure for interacting with novel processes where there is uncertainty regarding the true process dynamics. Meta-learning is a promising approach for constructing sample-efficient intelligent controllers.",
        "DOI": "10.1016/j.jprocont.2022.08.002",
        "paper_author": "McClement D.G.",
        "affiliation_name": "The University of British Columbia",
        "affiliation_city": "Vancouver",
        "affiliation_country": "Canada",
        "affiliation_id": "60010365",
        "affiliation_state": "BC"
    },
    {
        "paper_title": "Land use change and climate dynamics in the Rift Valley Lake Basin, Ethiopia",
        "publication": "Environmental Monitoring and Assessment",
        "citied_by": "26",
        "cover_date": "2022-10-01",
        "Abstract": "Land use and climate dynamics have a pronounced impact on water resources, biodiversity, land degradation, and productivity at all scales. Thus, in this study, we present the spatio-temporal dynamics of land use change and climate aiming to provide a scientific evidence about gains and losses in major land use categories and associated drivers and significancy and homogeneity of climate change. To this end, Landsat images and historical climate data have been used to determine the dynamics. In addition, population census data and land use policy have been considered to assess the potential drivers of land use change. The spatio-temporal land use dynamics have been evaluated using transition matrix and dynamics index. Likewise, shifts in the climate data were analyzed using change point analysis and three homogenous climate zones have been identified using principal component analysis. The results show that, from 1989 to 2019, the areal percentage of agricultural land increased by 27.5%, settlement by 0.8%, and barren land 0.4% while the natural vegetation, wetland, water body, and grass land decreased by 24.5%, 1.6%, 0.5%, and 2.1%, respectively. The land use dynamics have been stronger in the first decade of the study period. An abrupt shift of climate has occurred in the 1980s. In the last four decades, rainfall shows a not significant decreasing trend. However, a significant increasing trend has been observed for temperature. Rapid population growth, agricultural expansion policy, and climate variability have been identified as the underlying drivers of land use dynamics.",
        "DOI": "10.1007/s10661-022-10393-1",
        "paper_author": "Ayalew A.D.",
        "affiliation_name": "Christian-Albrechts-Universität zu Kiel",
        "affiliation_city": "Kiel",
        "affiliation_country": "Germany",
        "affiliation_id": "60012345",
        "affiliation_state": "Schleswig-Holstein"
    },
    {
        "paper_title": "Predicting agri-food quality across space: A Machine Learning model for the acknowledgment of Geographical Indications",
        "publication": "Food Policy",
        "citied_by": "14",
        "cover_date": "2022-10-01",
        "Abstract": "Geographical Indications (GIs), as Protected Designation of Origin (PDO)and Protected Geographical Indication (PGI), offer a unique protection scheme to preserve high-quality agri-food productions and support sustainable rural development at the territorial level. However, not all the areas with traditional agri-food products are acknowledged with a GI. Examining the Italian wine sector by a geo-referenced database and a machine learning framework, we show that municipalities which obtain a GI within the subsequent 10 year period (2002–2011) can be predicted using a large set of (lagged) municipality-level data (1981–2001). We find that the Random Forest algorithm is the best model to make out-of-sample predictions of municipalities which obtain GIs. Results show that there is a sort of optimal territorial condition characterized by the successful matching of wine-growing profession (vineyards), local actors involved (number of farmers), and physical dimension of farms (middle farms). Being in a vital economic system and the distance from major urban centers also emerges among the main relevant features in predicting the success of GIs. The methodology adopted and the evidence provided lead to policy reflections, in the light of the future Common Agricultural Policy (CAP) programming period and the scheduled reform of the GI's quality scheme.",
        "DOI": "10.1016/j.foodpol.2022.102345",
        "paper_author": "Resce G.",
        "affiliation_name": "Università degli Studi del Molise",
        "affiliation_city": "Campobasso",
        "affiliation_country": "Italy",
        "affiliation_id": "60014096",
        "affiliation_state": "CB"
    },
    {
        "paper_title": "Exploring nonlinear built environment effects on driving with a mixed-methods approach",
        "publication": "Transportation Research Part D: Transport and Environment",
        "citied_by": "25",
        "cover_date": "2022-10-01",
        "Abstract": "Recent studies have been exploring the complex nonlinear relationships between built environment attributes and driving using machine learning approaches. However, these nonlinear relationships lack causal explanations. This study applied a mixed-methods approach to data from a smaller European city, Stavanger, Norway. Our results showed that transport rationales for choosing activity locations and travel modes, along with configurations of the jobs and other facilities, provide causal explanations for the nonlinear and threshold effects of built environment attributes on people's driving-related behavior. Distance to city center plays the most important role and its nonlinear relationship reflects the influence of the polycentric city structure of Stavanger on driving. For Stavanger and similar cities, compact development around the city center helps to rein the auto dependence. Furthermore, the thresholds of nonlinear relationships provide planning guidelines to support compact development policies.",
        "DOI": "10.1016/j.trd.2022.103443",
        "paper_author": "Tao T.",
        "affiliation_name": "University of Minnesota Twin Cities",
        "affiliation_city": "Minneapolis",
        "affiliation_country": "United States",
        "affiliation_id": "60029445",
        "affiliation_state": "MN"
    },
    {
        "paper_title": "Using artificial intelligence to identify administrative errors in unemployment insurance",
        "publication": "Government Information Quarterly",
        "citied_by": "8",
        "cover_date": "2022-10-01",
        "Abstract": "Administrative errors in unemployment insurance (UI) decisions give rise to a public values conflict between efficiency and efficacy. We analyze whether artificial intelligence (AI) – in particular, methods in machine learning (ML) – can be used to detect administrative errors in UI claims decisions, both in terms of accuracy and normative tradeoffs. We use 16 years of US Department of Labor audit and policy data on UI claims to analyze the accuracy of 7 different random forest and deep learning models. We further test weighting schemas and synthetic data approaches to correcting imbalances in the training data. A random forest model using gradient descent boosting is more accurate, along several measures, and preferable in terms of public values, than every deep learning model tested. Adjusting model weights produces significant recall improvements for low-n outcomes, at the expense of precision. Synthetic data produces attenuated improvements and drawbacks relative to weights.",
        "DOI": "10.1016/j.giq.2022.101758",
        "paper_author": "Young M.M.",
        "affiliation_name": "Universiteit Leiden",
        "affiliation_city": "Leiden",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60019816",
        "affiliation_state": "Zuid-Holland"
    },
    {
        "paper_title": "Reinforcement learning with algorithms from probabilistic structure estimation",
        "publication": "Automatica",
        "citied_by": "4",
        "cover_date": "2022-10-01",
        "Abstract": "Reinforcement learning (RL) algorithms aim to learn optimal decisions in unknown environments through the experience of taking actions and observing the rewards gained. In some cases, the environment is not influenced by the actions of the RL agent, in which case the problem can be modeled as a contextual multi-armed bandit, and lightweight myopic algorithms can be employed. On the other hand, when the RL agent's actions affect the environment, the problem must be modeled as a Markov decision process, and more complex RL algorithms are required, which take the future effects of actions into account. Moreover, in practice, it is often unknown from the outset whether or not the agent's actions will impact the environment, and it is therefore not possible to determine which RL algorithm is most fitting. In this work, we propose to avoid this difficult decision entirely and incorporate a choice mechanism into our RL framework. Rather than assuming a specific problem structure, we use a probabilistic structure estimation procedure based on a likelihood-ratio (LR) test to make a more informed selection of the learning algorithm. We derive a sufficient condition under which myopic policies are optimal, present an LR test for this condition, and derive a bound on the regret of our framework. We provide examples of real-world scenarios where our framework is needed and provide extensive simulations to validate our approach.",
        "DOI": "10.1016/j.automatica.2022.110483",
        "paper_author": "Epperlein J.P.",
        "affiliation_name": "IBM Research Europe, Ireland",
        "affiliation_city": "Dublin",
        "affiliation_country": "Ireland",
        "affiliation_id": "60108035",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A new crude oil futures forecasting method based on fusing quadratic forecasting with residual forecasting",
        "publication": "Digital Signal Processing: A Review Journal",
        "citied_by": "7",
        "cover_date": "2022-10-01",
        "Abstract": "As an essential energy commodity, crude oil plays a vital role in the global economy. Accurate forecasting of crude oil is a critical guide in determining economic policies. This study proposes a hybrid forecasting model SEL-FR-XGBoost-GNB based on fusing quadratic forecasting with residual forecasting to achieve high accuracy in forecasting crude oil futures. The model-building process includes three stages. In stage I, the crude oil futures series are predicted using SVM, ELM, and LSTM models, respectively. In stage II, the prediction results of the above three single models are first reconstructed using FR. And then, the XGBoost method is used to make a secondary prediction of the crude oil futures series. In stage III, the residual sequences of the second prediction results are trained and predicted using the GNB method. The residual prediction result and the second prediction result are added as the final prediction result. Through the forecasting study of OPEC's historical crude oil futures series, the following conclusions can be drawn: (a) the proposed FR-XGBoost-based quadratic forecasting method can make the single model SVM, ELM, and LSTM form a complementary advantage and effectively improve crude oil futures forecasting accuracy; (b) extracting the positive and negative attributes of the residual sequence and transforming the regression prediction problem into a classification prediction problem can significantly improve the predictability of the residual sequence; (c) the proposed GNB residual sequence prediction method helps to improve the performance of the hybrid model; and (d) the proposed hybrid prediction model SEL-FR-XGBoost-GNB has the best performance among the 16 general models and 4 recent existing models.",
        "DOI": "10.1016/j.dsp.2022.103691",
        "paper_author": "Su M.",
        "affiliation_name": "Central South University",
        "affiliation_city": "Changsha",
        "affiliation_country": "China",
        "affiliation_id": "60017060",
        "affiliation_state": "Hunan"
    },
    {
        "paper_title": "Beating the odds: Identifying the top predictors of resilience among Hong Kong students",
        "publication": "Child Indicators Research",
        "citied_by": "22",
        "cover_date": "2022-10-01",
        "Abstract": "Students from disadvantaged socioeconomic backgrounds generally have worse academic outcomes than their more advantaged peers. However, some resilient students beat the odds and achieve academic success despite socioeconomic adversity. Identifying the factors that promote resilience is of critical theoretical and practical importance. Hence, this study aims to examine the different personal and social-contextual factors that predict resilience. We utilized the 2018 Program for International Student Assessment (PISA) data from Hong Kong and focused specifically on the 1,459 students in the bottom socioeconomic quartile. Of these, 251 were identified as resilient students as they demonstrated a high level of achievement despite being from disadvantaged backgrounds. Machine learning (i.e., random forest classification) was adopted to understand the relative importance of 30 different personal and social-contextual factors in classifying students into those who are deemed resilient versus those who are not. Eight top variables that best predicted resilience were identified, including the use of meta-cognitive strategies, joy of reading, teacher-directed instruction, perception of difficulty of the PISA test, sense of belonging to school, discriminating school climate, self-efficacy, and perceived teacher’s interest. This study sheds light on the factors that underpin resilience, providing important theoretical and policy implications.",
        "DOI": "10.1007/s12187-022-09939-z",
        "paper_author": "Wang F.",
        "affiliation_name": "University of Macau",
        "affiliation_city": "Taipa",
        "affiliation_country": "Macao",
        "affiliation_id": "60022317",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Contributions of meteorology to ozone variations: Application of deep learning and the Kolmogorov-Zurbenko filter",
        "publication": "Environmental Pollution",
        "citied_by": "32",
        "cover_date": "2022-10-01",
        "Abstract": "From hourly ozone observations obtained from three regions⸻Houston, Dallas, and West Texas⸻we investigated the contributions of meteorology to changes in surface daily maximum 8-h average (MDA8) ozone from 2000 to 2019. We applied a deep convolutional neural network and Shapely additive explanation (SHAP) to examine the complex underlying nonlinearity between variations of surface ozone and meteorological factors. Results of the models showed that between 2000 and 2019, specific humidity (38% and 27%) and temperature (28% and 37%) contributed the most to ozone formation over the Houston and Dallas metropolitan areas, respectively. On the other hand, the results show that solar radiation (50%) strongly impacted ozone variation over West Texas during this time. Using a combination of the Kolmogorov-Zurbenko (KZ) filter and multiple linear regression, we also evaluated the influence of meteorology on ozone and quantified the contributions of meteorological parameters to trends in surface ozone formation. Our findings showed that in Houston and Dallas, meteorology influenced ozone variations to a large extent. The impacts of meteorology on West Texas, however, showed meteorological factors had fewer influences on ozone variabilities from 2000 to 2019. This study showed that SHAP analysis and the KZ approach can investigate the contributions of the meteorological factors on ozone concentrations and help policymakers enact effective ozone mitigation policies.",
        "DOI": "10.1016/j.envpol.2022.119863",
        "paper_author": "Sadeghi B.",
        "affiliation_name": "College of Natural Sciences and Mathematics",
        "affiliation_city": "Houston",
        "affiliation_country": "United States",
        "affiliation_id": "60151391",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "Estimation of rainfall–runoff using SCS-CN method and GIS techniques in drought-prone area of Upper Kangsabati Watershed, India",
        "publication": "Sustainable Water Resources Management",
        "citied_by": "5",
        "cover_date": "2022-10-01",
        "Abstract": "Purulia is one of the most intense drought-prone districts of the western part of West Bengal, India. Acute form of water scarcity is a common phenomenon in this area during the hot-summer period. The water scarcity in this study region is due to the presence of monsoonal vagaries, unfavorable lithological condition and availability of poor groundwater. Therefore, watershed management is the primary concern for sustainable development of natural resources like water and land for optimal development of watershed and economic activities. Therefore, optimal measurement of rainfall-induced runoff is indeed necessary to understand the hydrological behavior. Several traditional statistical and advanced machine learning methods has been used previously to measure surface runoff among the researchers. It is difficult to simulate the required runoff with physical-based models due to the complexity and non-linear behavior of the runoff phenomena, as well as the absence of relevant historical data in all places. Thus, in the present research study of gravelly dominated drought-prone area of Upper Kangsabati Watershed (UKW) is considered to assess rainfall-induced surface runoff using most reliable method of Soil Conservation Service Curve Number (SCS-CN), and considering remote sensing and geographic information system platform. The SCS-CN method is very much reliable and till now has been frequently used among the global hydrological community to optimal assessment of surface runoff and adaptation of proper watershed management strategies. Henceforth, in this study SSC-CN method is used in the hard rock terrain landscape of extended plateau fringe of western West Bengal. The estimated result of runoff depth and runoff volume is 979.45 mm and 280.85 m3, respectively, and the rainfall–runoff is strongly positively correlated with (r) value being 0.98. Additionally, the applied statistical methods and the outcomes of this study will be helpful among the hydrological communities, different stakeholders and policy makers for sustainable watershed management in terms of optimal conserve of water resources and reduced threating of drought condition.",
        "DOI": "10.1007/s40899-022-00731-z",
        "paper_author": "Saha A.",
        "affiliation_name": "The University of Burdwan",
        "affiliation_city": "Bardhaman",
        "affiliation_country": "India",
        "affiliation_id": "60030482",
        "affiliation_state": "WB"
    },
    {
        "paper_title": "Fare adjustment's impacts on travel patterns and farebox revenue: An empirical study based on longitudinal smartcard data",
        "publication": "Transportation Research Part A: Policy and Practice",
        "citied_by": "11",
        "cover_date": "2022-10-01",
        "Abstract": "Fare policy plays an important role in transit operations and management. To better coordinate and achieve the multidimensional goals of a proposed fare adjustment policy (e.g., increasing revenue, managing demand, and improving equity), a fundamental step is to evaluate its travel pattern impacts, which helps us consider the policy in a bigger socioeconomic context. Existing studies rarely investigate the impacts of such a policy on different users’ and user groups’ travel patterns and transit operators’ farebox revenue using longitudinal data from sources such as smartcard data. To fill this gap, we exploit 24 weeks’ smartcard data from Wuhan, China, to empirically quantify those impacts. We find that (a) the fare increase had significant but varying impacts on travel patterns across users and user groups; (b) confronting the fare increase, commuter groups identified by the topic model reduced their trip frequency more but later as compared to other groups; (c) low-accessibility, long-distance, and single-destination metro riders were less sensitive to the fare increase; (d) when there was a system-wide fare increase with a distance-based structure, trip purposes and socioeconomic statuses could better predict the impacts on the travel demand and farebox revenue than spatiality. These findings indicate that increasing average fares while offering discounted tickets for frequent and/or captive riders could maintain the existing ridership and farebox revenue and possibly increase additional ridership.",
        "DOI": "10.1016/j.tra.2022.08.003",
        "paper_author": "Chen R.",
        "affiliation_name": "Peking University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60014966",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Understanding leptospirosis: application of state-of-the-art molecular typing tools with a One Health lens",
        "publication": "American Journal of Veterinary Research",
        "citied_by": "10",
        "cover_date": "2022-10-01",
        "Abstract": "Leptospirosis is an archetypal One Health problem as described in the companion Currents in One Health article in the October 2022 issue of the Journal of the American Veterinary Medical Association by Sykes et al. A thorough understanding of leptospirosis requires a detailed analysis of the elaborate interplay among pathogenic leptospiral strains, host species, and the environment. Such an understanding is required to inform appropriate preventative measures including vaccine design, prophylaxis efforts, educational programs that help to reduce exposure to pathogenic spirochetes, as well as policy development. Because of the complex epidemiology of leptospirosis, a One Health approach as defined by the One Health Initiative Task Force is critical—an approach that calls for “the collaborative efforts of multiple disciplines working locally, nationally, and globally, to attain optimal health for people, animals and our environment.” Over the last three decades, progressive advances in cutting-edge molecular typing techniques, as well as our ability to rapidly generate and share large amounts of sequence data through establishment and growth of databases, have been central to accelerating a One Health understanding of the epidemiology of leptospirosis. Nevertheless, our dependence on serotype information because of the serovar-specific nature of current vaccines means that laborious serotyping efforts continue. With the advent of new approaches such as mRNA vaccines that are based on lipopolysac-charide immunogens, sequence-and/or proteomics-based typing methods may replace these methods.",
        "DOI": "10.2460/ajvr.22.06.0104",
        "paper_author": "Sykes J.E.",
        "affiliation_name": "School of Veterinary Medicine",
        "affiliation_city": "Davis",
        "affiliation_country": "United States",
        "affiliation_id": "60000018",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Cost-effective occupation dependant infrared zonal heating system for operational university buildings",
        "publication": "Energy and Buildings",
        "citied_by": "0",
        "cover_date": "2022-10-01",
        "Abstract": "Recent legislations have necessitated policies for carbon print reduction. Buildings and in particular space heating are major energy consumers and responsible for over 34% of carbon print. This work presents a method of heating only certain parts of the building using far infrared (FIR) heating. This study gives an overview on the application of infrared radiation in heating by modern methods in tune and compatibility with climate developments for the public spaces in this decade. The case study is on a university lecture theatre and the space is split up into varyingly sized zones which enable different parts of the room to be heated depending on the time and occupation of the zone. The potential to heat each zone with FIR is implemented which runs according to the machine learning algorithm (MLA) through a practical study of real CO2 data collection and validation. This allows heating to start running before the zone(s) is occupied to optimise thermal comfort. Results show that occupation zone FIR heating saves an average of 11.175kWh through various occupations compared to the currently equipped convection wall mounted radiators. Occupation forecasting of the room using random forest machine learning has an accuracy of 97.75% for 15-minutes intervals of a day. Cost analysis for the proposed occupation heating show savings of up to 76% and 14.6% compared to convection electric and gas heating respectively. FIR provides a more efficient method of heating with the capacity for zonal implementation. The results in this research demonstrate the feasibility of FIR zonal heating for non-domestic applications.",
        "DOI": "10.1016/j.enbuild.2022.112362",
        "paper_author": "Scott C.",
        "affiliation_name": "Manchester Metropolitan University",
        "affiliation_city": "Manchester",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60022046",
        "affiliation_state": "Greater Manchester"
    },
    {
        "paper_title": "Forecasting the realized variance of oil-price returns using machine learning: Is there a role for U.S. state-level uncertainty?",
        "publication": "Energy Economics",
        "citied_by": "17",
        "cover_date": "2022-10-01",
        "Abstract": "Predicting the variance of oil-price returns is of paramount importance for policymakers and investors. Recent research has focused on whether disaggregate measures of economic-policy uncertainty provide better forecasts. Given that the United States (U.S.) is a major player in the international oil market, we extend this line of research by exploring by means of machine-learning techniques whether accounting for U.S. state-level measures of economic-policy uncertainty results in more accurate forecasts. We find improvements in forecast accuracy, especially when we study intermediate and long forecast horizons. This finding is robust to various changes in the model configuration (realized variance vs. realized volatility, sample period, recursive vs. rolling-estimation window, loss function of forecast consumers). Understandably, our findings have important implications for oil traders and policy authorities.",
        "DOI": "10.1016/j.eneco.2022.106229",
        "paper_author": "Çepni O.",
        "affiliation_name": "Copenhagen Business School",
        "affiliation_city": "Frederiksberg",
        "affiliation_country": "Denmark",
        "affiliation_id": "60020188",
        "affiliation_state": "Hovedstaden"
    },
    {
        "paper_title": "A comparative analysis of local similarity metrics and machine learning approaches: application to link prediction in author citation networks",
        "publication": "Scientometrics",
        "citied_by": "11",
        "cover_date": "2022-10-01",
        "Abstract": "Understanding the evolution of paper and author citations is of paramount importance for the design of research policies and evaluation criteria that can promote and accelerate scientific discoveries. Recently many studies on the evolution of science have been conducted in the context of the emergent Science of Science field. While many studies have probed the link problem in citation networks, only a few works have analyzed the temporal nature of link prediction in author citation networks. In this study we compared the performance of 10 well-known local network similarity measurements with four machine learning models to predict future links in author citations networks. Differently from traditional link prediction methods, the temporal nature of the predict links is relevant for our approach. Our analysis revealed that the Jaccard coefficient was found to be among the most relevant measurements. The preferential attachment measurement, conversely, displayed the worst performance. We also found that the extension of local measurements to their weighted version do not significantly improved the performance of predicting citations. Finally, we also found that a XGBoost and neural network approach summarizing the information from all 10 considered similarity measurements was able to provide the highest AUC performance and competitive precision values.",
        "DOI": "10.1007/s11192-022-04484-6",
        "paper_author": "Vital A.",
        "affiliation_name": "Universidade de São Paulo",
        "affiliation_city": "Sao Paulo",
        "affiliation_country": "Brazil",
        "affiliation_id": "60008088",
        "affiliation_state": "SP"
    },
    {
        "paper_title": "Optimum condition-based maintenance policy with dynamic inspections based on reinforcement learning",
        "publication": "Ocean Engineering",
        "citied_by": "8",
        "cover_date": "2022-10-01",
        "Abstract": "During the service life, inspections and repairs should be applied timely to maintain the reliability level of deteriorating structures. Condition-based maintenance (CBM) is an effective maintenance policy to reduce the life cycle cost. When the number of inspections does not change regardless of the performance, the CBM is categorize as fixed inspection (FI), otherwise, the inspection policy is denoted as dynamic inspection (DI). Compared with FI policy, DI policy performs the inspections based on the actual state and can avoid the unnecessary or insufficient inspections. Reinforcement learning is an effective and advanced decision-making tool and provides a useful method to optimize DI policy. Meanwhile, reinforcement learning has two methods (model free and model based) distinguished by the interaction method of environment. Comparison of two methods can help select an appropriate method to derive DI policy. Here, model based dynamic inspection (MBDI) and model free dynamic inspection (MFDI) are investigated for their performances in integrity management of fatigue structures. A fatigue details of ship structure is applied to illustrate the proposed framework and comparison study between DI and FI is performed. Results show that dynamic inspections can effectively reduce the expected life cycle costs. Furthermore, MFDI has a better performance than MBDI under different deteriorating rate and cost conditions.",
        "DOI": "10.1016/j.oceaneng.2022.112058",
        "paper_author": "Cheng J.",
        "affiliation_name": "Huazhong University of Science and Technology",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60025761",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Learning Variable Impedance Control for Aerial Sliding on Uneven Heterogeneous Surfaces by Proprioceptive and Tactile Sensing",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "15",
        "cover_date": "2022-10-01",
        "Abstract": "The recent development of novel aerial vehicles capable of physically interacting with the environment leads to new applications such as contact-based inspection. These tasks require the robotic system to exchange forces with partially-known environments, which may contain uncertainties including unknown spatially-varying friction properties and discontinuous variations of the surface geometry. Finding a solution that senses, adapts, and remains robust against these environmental uncertainties remains an open challenge. This letter presents a learning-based adaptive control strategy for aerial sliding tasks. In particular, the gains of a standard impedance controller are adjusted in real-time by a neural network policy based on proprioceptive and tactile sensing. This policy is trained in simulation with simplified actuator dynamics in a student-teacher learning setup. The real-world performance of the proposed approach is verified using a tilt-arm omnidirectional flying vehicle. The proposed controller structure combines data-driven and model-based control methods, enabling our approach to successfully transfer directly and without adaptation from simulation to the real platform. We attribute the success of the sim-to-real transfer to the inclusion of feedback control in the training and deployment. We achieved tracking performance and disturbance rejection that cannot be achieved using fine-tuned state of the art interaction control method.",
        "DOI": "10.1109/LRA.2022.3194315",
        "paper_author": "Zhang W.",
        "affiliation_name": "ETH Zürich",
        "affiliation_city": "Zurich",
        "affiliation_country": "Switzerland",
        "affiliation_id": "60025858",
        "affiliation_state": "ZH"
    },
    {
        "paper_title": "Shedding light on the decline of Iberian freshwater fish species over the period 1980–2020",
        "publication": "Freshwater Biology",
        "citied_by": "1",
        "cover_date": "2022-10-01",
        "Abstract": "Freshwater fish biodiversity is experiencing an alarming decline worldwide. Understanding the main factors behind its deterioration is a key step for ecosystem restoration. In this work, large-scale and long-term data were used to identify the causes of the decline of native species richness in Castilla-La Mancha. This region in central Spain covers part of six river basins belonging to four of the 11 biogeographical provinces for freshwater fish in the Iberian Peninsula. Firstly, we built a dataset that associates the presence of several fish species and a wide range of environmental variables (e.g. hydrological and hydromorphological indicators, land use classes, presence of alien fish species) at selected river sites for two different time periods (1980–2000 and 2001–2020). Secondly, we conducted an exploratory data analysis to identify possible temporal trends in the dataset. Finally, we applied the random forest algorithm to predict the response of different ecological guild-based metrics of fish richness to the selected variables. The exploratory data analysis revealed a decrease in native fish species richness in 74% of the area studied. There was no sustained temporal trend for stressor variables, except for the number of alien species, which increased in most river sites (63%). The models of the richness of native rheophilic, native intolerant, alien rheophilic, and alien limnophilic species performed satisfactorily. Magnitude of maximum discharge, presence of alien species, land use in the catchment area and altitude were the most important predictors of richness of native intolerant and rheophilic species. Alien limnophilic species proved to be sensitive to variables related to flow regime alteration, such as the presence of dams and the number of river flow reversals, while a less degraded habitat was found to be favourable to alien rheophilic species. The results suggest that the cumulative effect of persistent altered flow regimes and water pollution, coupled with a strong increase in the number of alien species, have led to the decline of native species in the area studied. The restoration of near-natural magnitudes of high flows when implementing environmental flows emerged as a key measure to restore ecosystem integrity. Starting from a long-term and large-scale dataset, this study provides new, quantitative insights into stressor–ecosystem relationships in rivers and could inform future environmental policy initiatives because it has identified the main factors leading to native fish decline and alien fish proliferation. Our findings emphasise the importance of considering metrics based on fish assemblage composition and ecological functional groups in order to disentangle the effects of stressors on fish communities.",
        "DOI": "10.1111/fwb.13963",
        "paper_author": "Valerio C.",
        "affiliation_name": "Universidad Complutense de Madrid",
        "affiliation_city": "Madrid",
        "affiliation_country": "Spain",
        "affiliation_id": "60027282",
        "affiliation_state": "Madrid"
    },
    {
        "paper_title": "Model-Based Imitation Learning Using Entropy Regularization of Model and Policy",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "3",
        "cover_date": "2022-10-01",
        "Abstract": "Approaches based on generative adversarial networks for imitation learning are promising because they are sample efficient in terms of expert demonstrations. However, training a generator requires many interactions with the actual environment because model-free reinforcement learning is adopted to update a policy. To improve the sample efficiency using model-based reinforcement learning, we propose model-based Entropy-Regularized Imitation Learning (MB-ERIL) under the entropy-regularized Markov decision process to reduce the number of interactions with the actual environment. MB-ERIL uses two discriminators. A policy discriminator distinguishes the actions generated by a robot from expert ones, and a model discriminator distinguishes the counterfactual state transitions generated by the model from the actual ones. We derive structured discriminators so that the learning of the policy and the model is efficient. Computer simulations and real robot experiments show that MB-ERIL achieves a competitive performance and significantly improves the sample efficiency compared to baseline methods.",
        "DOI": "10.1109/LRA.2022.3196139",
        "paper_author": "Uchibe E.",
        "affiliation_name": "Advanced Telecommunications Research Institute International (ATR)",
        "affiliation_city": "Kyoto",
        "affiliation_country": "Japan",
        "affiliation_id": "60001271",
        "affiliation_state": "Kyoto"
    },
    {
        "paper_title": "What Matters in Language Conditioned Robotic Imitation Learning Over Unstructured Data",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "46",
        "cover_date": "2022-10-01",
        "Abstract": "A long-standing goal in robotics is to build robots that can perform a wide range of daily tasks from perceptions obtained with their onboard sensors and specified only via natural language. While recently substantial advances have been achieved in language-driven robotics by leveraging end-to-end learning from pixels, there is no clear and well-understood process for making various design choices due to the underlying variation in setups. In this letter, we conduct an extensive study of the most critical challenges in learning language conditioned policies from offline free-form imitation datasets. We further identify architectural and algorithmic techniques that improve performance, such as a hierarchical decomposition of the robot control learning, a multimodal transformer encoder, discrete latent plans and a self-supervised contrastive loss that aligns video and language representations. By combining the results of our investigation with our improved model components, we are able to present a novel approach that significantly outperforms the state of the art on the challenging language conditioned long-horizon robot manipulation CALVIN benchmark. We have open-sourced our implementation to facilitate future research in learning to perform many complex manipulation skills in a row specified with natural language.",
        "DOI": "10.1109/LRA.2022.3196123",
        "paper_author": "Mees O.",
        "affiliation_name": "Universität Freiburg",
        "affiliation_city": "Freiburg im Breisgau",
        "affiliation_country": "Germany",
        "affiliation_id": "60025641",
        "affiliation_state": "Baden-Wurttemberg"
    },
    {
        "paper_title": "Plastic consumption in urban municipalities: Characteristics and policy implications of Vietnamese consumers’ plastic bag use",
        "publication": "Environmental Science and Policy",
        "citied_by": "12",
        "cover_date": "2022-10-01",
        "Abstract": "Plastic waste pollution remains a major problem across the developing world. In Vietnam, the situation is particularly serious as many plastic consumption behaviours remain under-analysed and pertinent policies have produced limited impact. Accordingly, this paper examines the patterns and predictors of consumer plastic bag use when shopping in Da Nang, Vietnam. It does so by drawing on an original household survey and key informant interviews. Moreover, it applies the latest behavioural theory research and machine learning techniques. Subsequently, this paper observes Vietnamese consumers’ plastic bag use is prevalent and often entrenched as a habit. Additionally, two socio-demographic and seven socio-psychological predictors are significant to the frequency of using plastic bags. These results, then, inform Vietnam's plastic consumption policies and, more broadly, emphasise the (1) heterogeneity of influences on consumer behaviour; (2) contingency of many widely-accepted behavioural predictors; and (3) shortcomings of purely regulatory solutions.",
        "DOI": "10.1016/j.envsci.2022.07.015",
        "paper_author": "Makarchev N.",
        "affiliation_name": "University of Oxford",
        "affiliation_city": "Oxford",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60026851",
        "affiliation_state": "Oxfordshire"
    },
    {
        "paper_title": "Estimating optimal ABC zone sizes in manual warehouses",
        "publication": "International Journal of Production Economics",
        "citied_by": "20",
        "cover_date": "2022-10-01",
        "Abstract": "The ABC storage is the most popular class-based policy for the storage location assignment in warehouses. It divides a storage area into three zones and assigns the most demanded products to the best-located zone. Despite the policy's popularity, arbitrary zone sizes are commonly used, which can lead to major efficiency losses. We investigate how several factors, such as the warehouse layout, the demand characteristics, and the storage and routing policies, impact the solutions for the zone sizing problem. We propose a new methodology to solve it using machine learning models to predict the optimal zone sizes considering the mentioned factors. We simulate many common manual warehouse settings, such as the multi-block layout, demand distributions, and several operating policies, to observe which zone sizes lead to the best performance in each one. The data generated is used to train four regression models – ordinary least squares, regression tree, random forest, and multilayer perceptron – to predict the optimal zone sizes from the best ones observed. Computational experiments show that zone sizes provided by all models significantly improve the order picking efficiency when compared to the arbitrary zone sizes commonly used, notably for the one-zone (random policy), the two-zone (20/80 rule), and the three-zone (20/30/50) systems. The proposed methodology is easily adaptable for different warehousing systems and problems when enough data is available to train the models. The resulting linear functions and decision trees are made available and can be used by practitioners for determining zone sizes for their particular warehouse.",
        "DOI": "10.1016/j.ijpe.2022.108579",
        "paper_author": "Silva A.",
        "affiliation_name": "Centre Interuniversitaire de Recherche sur les Réseaux d‘Entreprise, la Logistique et le Transport",
        "affiliation_city": "Montreal",
        "affiliation_country": "Canada",
        "affiliation_id": "60113917",
        "affiliation_state": "QC"
    },
    {
        "paper_title": "Infusing Model Predictive Control Into Meta-Reinforcement Learning for Mobile Robots in Dynamic Environments",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "11",
        "cover_date": "2022-10-01",
        "Abstract": "The successful operation of mobile robots requires them to adapt rapidly to environmental changes. To develop an adaptive decision-making tool for mobile robots, we propose a novel algorithm that combines meta-reinforcement learning (meta-RL) with model predictive control (MPC). Our method employs an off-policy meta-RL algorithm as a baseline to train a policy using transition samples generated by MPC when the robot detects certain events that can be effectively handled by MPC, with its explicit use of robot dynamics. The key idea of our method is to switch between the meta-learned policy and the MPC controller in a randomized and event-triggered fashion to make up for suboptimal MPC actions caused by the limited prediction horizon. During meta-testing, the MPC module is deactivated to significantly reduce computation time in motion control. We further propose an online adaptation scheme that enables the robot to infer and adapt to a new task within a single trajectory. The performance of our method has been demonstrated through simulations using a nonlinear car-like vehicle model with (i) synthetic movements of obstacles, and (ii) real-world pedestrian motion data. The simulation results indicate that our method outperforms other algorithms in terms of learning efficiency and navigation quality.",
        "DOI": "10.1109/LRA.2022.3191234",
        "paper_author": "Shin J.",
        "affiliation_name": "Automation and Systems Research Institute",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea",
        "affiliation_id": "60120116",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Reinforcement Learning of Impedance Policies for Peg-in-Hole Tasks: Role of Asymmetric Matrices",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "19",
        "cover_date": "2022-10-01",
        "Abstract": "Robotic manipulators are playing an increasing role in a wide range of industries. However, their application to assembly tasks is hampered by the need for precise control over the environment and for task-specific coding. Cartesian impedance control is a well-established method for interacting with the environment and handling uncertainties. With the advance of Reinforcement Learning (RL) it has been suggested to learn the impedance matrices. However, most of the current work is limited to learning diagonal impedance matrices in addition to the trajectory itself. We argue that asymmetric impedance matrices enhance the ability to properly correct reference trajectories generated by a baseline planner, alleviating the need for learning the trajectory. Moreover, a task-specific set of asymmetric impedance matrices can be sufficient for simple tasks, alleviating the need for learning variable impedance control. We learn impedance policies for small (few mm) peg-in-hole using model-free RL, and investigate the advantage of using asymmetric impedance matrices and their space-invariance. Finally, we demonstrate zero-shot policy transfer from the simulation to a real robot, and generalization to new real-world environments, with larger parts and semi-flexible pegs.",
        "DOI": "10.1109/LRA.2022.3191070",
        "paper_author": "Kozlovsky S.",
        "affiliation_name": "Technion - Israel Institute of Technology",
        "affiliation_city": "Haifa",
        "affiliation_country": "Israel",
        "affiliation_id": "60022403",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Reward prediction errors, not sensory prediction errors, play a major role in model selection in human reinforcement learning",
        "publication": "Neural Networks",
        "citied_by": "6",
        "cover_date": "2022-10-01",
        "Abstract": "Model-based reinforcement learning enables an agent to learn in variable environments and tasks by optimizing its actions based on the predicted states and outcomes. This mechanism has also been considered in the brain. However, exactly how the brain selects an appropriate model for confronting environments has remained unclear. Here, we investigated the model selection algorithm in the human brain during a reinforcement learning task. One primary theory of model selection in the brain is based on sensory prediction errors. Here, we compared this theory with an alternative possibility of internal model selection with reward prediction errors. To compare these two theories, we devised a switching experiment from a first-order Markov decision process to a second-order Markov decision process that provides either reward- or sensory prediction error regarding environmental change. We tested two representative computational models driven by different prediction errors. One is the sensory prediction-error-driven Bayesian algorithm, which has been discussed as a representative internal model selection algorithm in the animal reinforcement learning task. The other is the reward-prediction-error-driven policy gradient algorithm. We compared the simulation results of these two computational models with human reinforcement learning behaviors. The model fitting result supports that the policy gradient algorithm is preferable to the Bayesian algorithm. This suggests that the human brain employs the reward prediction error to select an appropriate internal model in the reinforcement learning task.",
        "DOI": "10.1016/j.neunet.2022.07.002",
        "paper_author": "Wu Y.",
        "affiliation_name": "University of Tsukuba",
        "affiliation_city": "Tsukuba",
        "affiliation_country": "Japan",
        "affiliation_id": "60014256",
        "affiliation_state": "Ibaraki"
    },
    {
        "paper_title": "Deep reinforcement learning and adaptive policy transfer for generalizable well control optimization",
        "publication": "Journal of Petroleum Science and Engineering",
        "citied_by": "15",
        "cover_date": "2022-10-01",
        "Abstract": "Well control optimization is a challenging task but plays a critical role in reservoir management. Traditional methods independently solve each task from scratch and the obtained scheme is only applicable to the environment where the optimization process is run. In stark contrast, human experts are adept at learning and building generalizable skills and using them to efficiently draw inferences and make decisions for similar scenarios. Inspired by the recently proposed generalizable field development optimization approach, this work presents an adaptive and robust deep learning-based Representation-Decision-Transfer (DLRDT) framework to deal with the generalization problem in well control optimization. Specifically, DLRDT uses a three-stage workflow to train an artificial agent. First, the agent develops its vision and understands its surroundings by learning a latent state representation with domain adaptation techniques. Second, the agent is tasked with using high-performance deep reinforcement learning algorithms to train the optimal control policy in the latent state space. Finally, the agent is transferred and evaluated in several environments that were not seen during the training. Compared with previous methods that optimize a solution for a specific scenario, our approach trains a policy that is not only robust to variations in their environments but can adapt to unseen (but similar) environments without additional training. For a demonstration, we validate the proposed framework on waterflooding well control optimization problems. Experimental evaluations on two three-dimensional reservoir models demonstrate the trained agent has excellent optimization efficiency and generalization performance. Our approach is particularly favorable when considering the deployment of schemes in the real world as it can handle unforeseen situations.",
        "DOI": "10.1016/j.petrol.2022.110868",
        "paper_author": "Wang Z.",
        "affiliation_name": "China University of Petroleum-Beijing",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60016087",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Enabling intelligent onboard guidance, navigation, and control using reinforcement learning on near-term flight hardware",
        "publication": "Acta Astronautica",
        "citied_by": "3",
        "cover_date": "2022-10-01",
        "Abstract": "Future space missions require technological advances to meet more stringent requirements. Next generation guidance, navigation, and control systems must safely operate autonomously in hazardous and uncertain environments. While these developments often focus on flight software, spacecraft hardware also creates computational limitations for onboard algorithms. Intelligent control methods combine theories from automatic control, artificial intelligence, and operations research to derive control systems capable of handling large uncertainties. While this can be beneficial for spacecraft control, such control systems often require substantial computational power. Recent improvements in single board computers have created physically lighter and less power-intensive processors that are suitable for spaceflight and purpose built for machine learning. In this study, we implement a reinforcement learning based controller on NVIDIA Jetson Nano hardware and apply this controller to a simulated Mars powered descent problem. The proposed approach uses optimal trajectories and guidance laws under nominal environment conditions to initialise a reinforcement learning agent. This agent learns a control policy to cope with environmental uncertainties and updates its control policy online using a novel update mechanism called Extreme Q-Learning Machine. We show that this control system performs well on flight suitable hardware, which demonstrates the potential for intelligent control onboard spacecraft.",
        "DOI": "10.1016/j.actaastro.2022.07.013",
        "paper_author": "Wilson C.",
        "affiliation_name": "University of Strathclyde",
        "affiliation_city": "Glasgow",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60024724",
        "affiliation_state": "Scotland"
    },
    {
        "paper_title": "Oil futures volatility predictability: New evidence based on machine learning models<sup>1</sup>",
        "publication": "International Review of Financial Analysis",
        "citied_by": "27",
        "cover_date": "2022-10-01",
        "Abstract": "This paper comprehensively examines the connection between oil futures volatility and the financial market based on a model-rich environment, which contains traditional predicting models, machine learning models, and combination models. The results highlight the efficiency of machine learning models for oil futures volatility forecasting, particularly the ensemble models and neural network models. Most interestingly, we consider the “forecast combination puzzle” in machine learning models, and find that combination models continue to have more satisfactory performances in all types of situations. We also discuss the model interpretability and each indicator's contribution to the prediction. Our paper provides new insights for machine learning methods' applications in futures market volatility prediction, which is helpful for academics, policy-makers, and investors.",
        "DOI": "10.1016/j.irfa.2022.102299",
        "paper_author": "Lu X.",
        "affiliation_name": "Southwest Jiaotong University",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China",
        "affiliation_id": "60010421",
        "affiliation_state": "Sichuan"
    },
    {
        "paper_title": "A comparison of performance measures of three machine learning algorithms for flood susceptibility mapping of river Silabati (tropical river, India)",
        "publication": "Physics and Chemistry of the Earth",
        "citied_by": "34",
        "cover_date": "2022-10-01",
        "Abstract": "Flood is the most common phenomenon causing extensive disruption to the environment, socio-economy, infrastructure and many other aspects of human life. Flood susceptibility mapping (FSM) is a crucial step for policymakers in disaster management. However, in the present study, we applied three ensemble machine learning models, namely, Random Forest (RF), Naive Bayes (NB), and Extreme Gradient Boosting (XGB) for FSM of Silabati river (tropical river, India). A total of 500 historical flood points and field observations with considering set of twelve flood influencing factors (rainfall, elevation, slope, curvature, stream power index (SPI), Sediment Transport Index (STI), Terrain ruggedness index (TRI), topographic wetness index (TWI), clay content in soil (SC), distance from the river (DFR), drainage density (DD), and land use and land cover (LULC) for generating the training and validation datasets. To investigate and perceive the flood vulnerability of the study basin, five factors, such as elevation, DD, rainfall, DFR and SC turn out to be the most dominating factors out of the adopted twelve factors considered for the present study in all models. It is found that an area of around 36.08% of the total basin comes under the very high to high FSM. The prediction ability and performance efficiency of three models were comparison and validation measures by statistical techniques such as multicollinearity diagnosis test, Kappa index, MAE, (Mean absolute error), RMSE (Root mean square error), Pearson's correlation coefficients and receiver operating characteristic (ROC). The highest ROC was achieved by the RF model (84.7%), followed by the XGB model (83.1%), and NB model (82.1%) respectively. The RF model performs better for FSM then the other models.",
        "DOI": "10.1016/j.pce.2022.103198",
        "paper_author": "Hasanuzzaman M.",
        "affiliation_name": "Raja Narendra Lal Khan Women's College",
        "affiliation_city": "Purba Midnapore",
        "affiliation_country": "India",
        "affiliation_id": "60094047",
        "affiliation_state": "WB"
    },
    {
        "paper_title": "Retro-RL: Reinforcing Nominal Controller with Deep Reinforcement Learning for Tilting-Rotor Drones",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "6",
        "cover_date": "2022-10-01",
        "Abstract": "Studies that broaden drone applications into complex tasks require a stable control framework. Recently, deep reinforcement learning (RL) algorithms have been exploited in many studies for robot control to accomplish complex tasks. Unfortunately, deep RL algorithms might not be suitable for being deployed directly into a real-world robot platform due to the difficulty in interpreting the learned policy and lack of stability guarantee, especially for a complex task such as a wall-climbing drone. This letter proposes a novel hybrid architecture that reinforces a nominal controller with a robust policy learned using a model-free deep RL algorithm. The proposed architecture employs an uncertainty-aware control mixer to preserve guaranteed stability of a nominal controller while using the extended robust performance of the learned policy. The policy is trained in a simulated environment with thousands of domain randomizations to achieve robust performance over diverse uncertainties. The performance of the proposed method was verified through real-world experiments and then compared with a conventional controller and the state-of-the-art learning-based controller trained with a vanilla deep RL algorithm.",
        "DOI": "10.1109/LRA.2022.3189446",
        "paper_author": "Nahrendra I.M.A.",
        "affiliation_name": "Korea Advanced Institute of Science and Technology",
        "affiliation_city": "Daejeon",
        "affiliation_country": "South Korea",
        "affiliation_id": "60032144",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Elastic O-RAN Slicing for Industrial Monitoring and Control: A Distributed Matching Game and Deep Reinforcement Learning Approach",
        "publication": "IEEE Transactions on Vehicular Technology",
        "citied_by": "36",
        "cover_date": "2022-10-01",
        "Abstract": "In this work, we design an elastic open radio access network (O-RAN) slicing for the Industrial Internet of things (IIoT). Due to the rapid spread of IoT in the industrial use-cases such as safety and mobile robot communications, the IIoT landscape has been shifted from static manufacturing processes towards dynamic manufacturing workflows (e.g., Modular Production System). But unlike IoT, IIoT poses additional challenges such as severe communication environment, network-slice resource demand variations, and on-time information update from the IIoT devices during industrial production. First, we formulate the O-RAN slicing problem for on-time industrial monitoring and control where the objective is to minimize the cost of fresh information updates (i.e., age of information (AoI)) from the IIoT devices (i.e., sensors) with the device energy consumption and O-RAN slice isolation constraints. Second, we propose the intelligent O-RAN framework based on game theory and machine learning to mitigate the problem's complexity. We propose a two-sided distributed matching game in the O-RAN control layer that captures the IIoT channel characteristics and the IIoT service priorities to create IIoT device and small cell base station (SBS) preference lists. We then employ an actor-critic model with a deep deterministic policy gradient (DDPG) in the O-RAN service management layer to solve the resource allocation problem for optimizing the network slice configuration policy under time-varying slicing demand. Furthermore, the proposed matching game within the actor-critic model training helps to enforce the long-term policy-based guidance for resource allocation that reflects the trends of all IIoT devices and SBSs satisfactions with the assignment. Finally, the simulation results show that the proposed solution enhances the performance gain for the IIoT services by serving an average of 50% and 43.64% more IIoT devices than the baseline approaches.",
        "DOI": "10.1109/TVT.2022.3188217",
        "paper_author": "Abedin S.F.",
        "affiliation_name": "Mid Sweden University, Sundsvall",
        "affiliation_city": "Sundsvall",
        "affiliation_country": "Sweden",
        "affiliation_id": "60106597",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Soil-Adaptive Excavation Using Reinforcement Learning",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "26",
        "cover_date": "2022-10-01",
        "Abstract": "In this letter, we present an excavation controller for a full-sized hydraulic excavator that can adapt online to different soil characteristics. Soil properties are hard to predict and can vary even within one scoop, which requires a controller that can adapt online to the encountered soil conditions. The objective is to fill the bucket with excavation material while respecting machine limitations to prevent stalling or lifting of the machine. To this end, we train a control policy in simulation using Reinforcement Learning (RL). The soil interactions are modeled based on the Fundamental Equation of Earth-Moving (FEE) with heavily randomized soil parameters to expose the agent to a wide range of different conditions. The agent learns to output joint velocity commands, which can be directly applied to the standard proportional valves of the real machine. We test the controller on a 12-ton excavator in different types of soils. The experiments demonstrate that the controller can adapt online to changing conditions without the explicit knowledge of the soil parameters, solely from proprioceptive observations, which are easily measurable.",
        "DOI": "10.1109/LRA.2022.3189834",
        "paper_author": "Egli P.",
        "affiliation_name": "ETH Zürich",
        "affiliation_city": "Zurich",
        "affiliation_country": "Switzerland",
        "affiliation_id": "60025858",
        "affiliation_state": "ZH"
    },
    {
        "paper_title": "Robust Enhancement of Intrusion Detection Systems Using Deep Reinforcement Learning and Stochastic Game",
        "publication": "IEEE Transactions on Vehicular Technology",
        "citied_by": "42",
        "cover_date": "2022-10-01",
        "Abstract": "The incorporation of advanced networking technologies makes modern systems vulnerable to cyber-attacks that can result in a number of harmful outcomes. Due to the increase of security incidents and massive activities on networks, existing works have mainly focused on designing Intrusion Detection Systems (IDSs) based on traditional machine learning and deep learning models. In recent times, state of the art performance has been achieved in various fields through Deep Reinforcement Learning (DRL), which combines deep learning with reinforcement learning. In this paper, we propose a new DRL-based IDS for network traffics using Markov decision process (MDP) to improve the IDS decision-making performance. In addition, an extensive analysis of the IDS behavior is provided through modeling the interaction between the well-behaving IDS and attacker players using Stochastic Game Theory. Specifically, we used a non-zero-sum stochastic game, where the transitions between states depend on both the IDS and the attacker's actions at each stage of the game. We show that our game reaches a Nash Equilibrium upon convergence to seek the optimal solution, which corresponds to the optimal decision policy where both players maximize their profits. We compared the performance of our proposed DRL-IDS to the baseline benchmark of standard reinforcement learning (RL) and several machine learning algorithms using NSL-KDD dataset. As a result, our proposed DRL-IDS outperforms the existing models by improving both the detection rate and the accuracy while reducing false alarms. Results were provided to demonstrate the convergence of the game theory-based IDS under various settings toward equilibrium. This equilibrium corresponds to the safe state where both players are playing their respective best strategies.",
        "DOI": "10.1109/TVT.2022.3186834",
        "paper_author": "Benaddi H.",
        "affiliation_name": "Faculty of Science, Ibn Tofail University",
        "affiliation_city": "Kenitra",
        "affiliation_country": "Morocco",
        "affiliation_id": "60030441",
        "affiliation_state": "Rabat-Sale-Kenitra"
    },
    {
        "paper_title": "Machine learning algorithms for predicting smokeless tobacco status among women in Northeastern States, India",
        "publication": "International Journal of System Assurance Engineering and Management",
        "citied_by": "0",
        "cover_date": "2022-10-01",
        "Abstract": "Use of smokeless tobacco (SLT) in women is very high and serious public health issue in the northeast states, India. Prediction on status of SLT use among women is a key to policy making and resource planning at district and community level in this region. This study aims to predict the status of smokeless tobacco use among women in northeast states of India by applying several machine learning (ML) algorithms. We used publicly available National Family Health Survey, 2015–16 data. Eight ML algorithms were used for the prediction on status of SLT use. Precision, specificity, sensitivity, accuracy, and Cohen’s kappa statistic were performed as a part of the systematic assessment of the algorithms. Result of this study reveals that the best classification performance was accomplished with random forest (RF) algorithm accuracy of 79.51% [77.65–81.37], sensitivity of 87.75% [86.55–88.95], specificity of 65.19% [65.18–65.20], precision of 81.39%, F-measure of 84.35 and Cohen’s Kappa was 0.545 [0.529–0.558]. It was concluded that the algorithm of random forest was found superior and performed much better as compared to the rest ML algorithms in predicting the status on smokeless tobacco use in women of northeast states, India. Finally, this research finding recommends application of RF algorithm for classification and feature selection to predict the status of smokeless tobacco as a core interest.",
        "DOI": "10.1007/s13198-022-01720-3",
        "paper_author": "Jitenkumar Singh K.",
        "affiliation_name": "Indian Council of Medical Research",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India",
        "affiliation_id": "60025666",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A reinforcement and imitation learning method for pricing strategy of electricity retailer with customers’ flexibility",
        "publication": "Applied Energy",
        "citied_by": "11",
        "cover_date": "2022-10-01",
        "Abstract": "The effective pricing of retail broker in competitive electricity market constitutes a key problem toward four goals: (1) the maximization of the broker's economic benefits; (2) the balance between customers’ energy supply and demand; (3) the realization of the energy supply and demand flexibility potential of customers; (4) the constraint that prevents the retail prices from too high or too low. Unfortunately, few studies can achieve four goals simultaneously. Moreover, the complicated electricity trading environment with continuous states and actions also increases the difficulty of learning optimal pricing strategy. To solve these problems, a reinforcement and imitation learning approach is proposed to develop the optimal pricing strategy of retail broker in this paper. Specifically, the proposed approach consists of a demand prediction method to predict customers’ energy demand and supply volume, a self-generated expert knowledge imitation learning mechanism to instruct the agent to imitate given expert policy with generated expert knowledge, and an action policy learning method. Different from existing studies, our approach achieves all four goals and exploits the generated transition tuples fully to learn a more effective pricing strategy. The proposed scheme is verified by experiments using real-world market data, the experimental results illustrate our proposed approach gains 9.71%, 3.32%, and 15.94% higher economic profits than three state-of-the-art pricing strategies, respectively. Meanwhile, the total needed computation time for our method to learn an effectiveness pricing strategy is only 4102 s. The results show that our method gains the highest economic profits for the broker with acceptable computation cost. Moreover, the changing curves of customers’ consumption/production habits demonstrate that the proposed method could achieve demand/supply response of customers.",
        "DOI": "10.1016/j.apenergy.2022.119543",
        "paper_author": "Zhang Y.",
        "affiliation_name": "Xi'an Jiaotong University",
        "affiliation_city": "Xi'an",
        "affiliation_country": "China",
        "affiliation_id": "60018308",
        "affiliation_state": "Shaanxi"
    },
    {
        "paper_title": "Metamodeling for Policy Simulations with Multivariate Outcomes",
        "publication": "Medical Decision Making",
        "citied_by": "4",
        "cover_date": "2022-10-01",
        "Abstract": "Purpose: Metamodels are simplified approximations of more complex models that can be used as surrogates for the original models. Challenges in using metamodels for policy analysis arise when there are multiple correlated outputs of interest. We develop a framework for metamodeling with policy simulations to accommodate multivariate outcomes. Methods: We combine 2 algorithm adaptation methods—multitarget stacking and regression chain with maximum correlation—with different base learners including linear regression (LR), elastic net (EE) with second-order terms, Gaussian process regression (GPR), random forests (RFs), and neural networks. We optimize integrated models using variable selection and hyperparameter tuning. We compare the accuracy, efficiency, and interpretability of different approaches. As an example application, we develop metamodels to emulate a microsimulation model of testing and treatment strategies for hepatitis C in correctional settings. Results: Output variables from the simulation model were correlated (average ρ = 0.58). Without multioutput algorithm adaptation methods, in-sample fit (measured by R2) ranged from 0.881 for LR to 0.987 for GPR. The multioutput algorithm adaptation method increased R2 by an average 0.002 across base learners. Variable selection and hyperparameter tuning increased R2 by 0.009. Simpler models such as LR, EE, and RF required minimal training and prediction time. LR and EE had advantages in model interpretability, and we considered methods for improving the interpretability of other models. Conclusions: In our example application, the choice of base learner had the largest impact on R2; multioutput algorithm adaptation and variable selection and hyperparameter tuning had a modest impact. Although advantages and disadvantages of specific learning algorithms may vary across different modeling applications, our framework for metamodeling in policy analyses with multivariate outcomes has broad applicability to decision analysis in health and medicine.",
        "DOI": "10.1177/0272989X221105079",
        "paper_author": "Zhong H.",
        "affiliation_name": "Stanford Engineering",
        "affiliation_city": "Stanford",
        "affiliation_country": "United States",
        "affiliation_id": "60141508",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Has the Three-Year Action Plan improved the air quality in the Fenwei Plain of China? Assessment based on a machine learning technique",
        "publication": "Atmospheric Environment",
        "citied_by": "16",
        "cover_date": "2022-10-01",
        "Abstract": "A Three-Year Action Plan was implemented in 2018 to improve air quality in the Fenwei Plain. Yet, the effectiveness of this action plan is not clear, which is of great concern to the public and policymakers. Meteorological differences between years may lead us to misestimate the effect of pollutant reduction policies. Here we quantitatively assess the effectiveness of the Three-Year Action Plan in the Fenwei Plain using a meteorological normalization technique based on a machine learning model. To reduce the uncertainty of the model itself, we performed the optimization of hyperparameters and the selection of the optimal model. Ultimately, our study results showed that after removing the impact of meteorology on air quality, the true concentrations of PM2.5, PM10, O3, SO2, NO2, and CO in the Fenwei Plain decreased by 29, 29, 2, 61, 20, and 42% from 2017 to 2020, respectively. The normalized concentrations of all pollutants decreased less than the observed concentrations, implying that if we focus only on the observed data, we could overestimate the effectiveness of the Three-Year Action Plan in the Fenwei Plain. Besides, combining the different decreases in concentrations of six pollutants and the emission characteristics of different pollution sources, we can conclude the improvement in air quality in the Fenwei Plain during 2017–2020 was mainly driven by the residential and industrial sectors. Our work could help the public and policymakers review the effectiveness of the Three Action Plan in the Fenwei Plain and provided a useful reference for the evaluation and formulation of pollutant emission reduction policies.",
        "DOI": "10.1016/j.atmosenv.2022.119204",
        "paper_author": "Dai X.",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60025278",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Slower than expected reduction in annual PM<inf>2.5</inf> in Xi'an revealed by machine learning-based meteorological normalization",
        "publication": "Science of the Total Environment",
        "citied_by": "21",
        "cover_date": "2022-10-01",
        "Abstract": "To evaluate the effectiveness of air pollution control policies, trend analysis of the air pollutants is often performed. However, trend analysis of air pollutants over multiple years is complicated by the fact that changes in meteorology over time can also affect the levels of air pollutants in addition to changes in emissions or atmospheric chemistry. To decouple the meteorological effect, this study performed a trend analysis of the hourly fine particulate matter (PM2.5) observed at an urban background site in Xi'an city over 5 years from 2015 to 2019 using the machine learning algorithm. As a novel way of meteorological normalization, the meteorological parameters were used as constant input for 5 consecutive years. In this way, the impact of meteorological parameters was excluded, providing insights into the “real” changes in PM2.5 due to changes in emission strength or atmospheric chemistry. After meteorological normalization, a decreasing trend of −3.3 % year−1 (−1.9 μg m−3 year−1) in PM2.5 was seen, instead of −4.4 % year−1 from direct PM2.5 observation. Assuming the rate of −1.9 μg m−3 year−1 were kept constant for the next few decades in Xi'an, it would take approximately 25 years (in the year 2045) to reduce the annual PM2.5 level to 5 μg m−3, the new guideline value from World Health Organization. We also show that PM2.5 is primarily associated with anthropogenic emissions, which, underwent aqueous phase chemistry in winter and photochemical oxidation in summer as suggested by partial dependence of RH and Ox in different seasons. Therefore, reducing the anthropogenic secondary aerosol precursors at a higher rate, such as NOx and VOCs is expected to reduce the particulate pollution in this region more effectively than the current −3.3 % year−1 found in this study.",
        "DOI": "10.1016/j.scitotenv.2022.156740",
        "paper_author": "Wang M.",
        "affiliation_name": "The Hong Kong Polytechnic University",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong",
        "affiliation_id": "60008928",
        "affiliation_state": "Hong Kong"
    },
    {
        "paper_title": "Barking up the wrong tree? Can forest expansion help meet climate goals?",
        "publication": "Environmental Science and Policy",
        "citied_by": "7",
        "cover_date": "2022-10-01",
        "Abstract": "Forest expansion can make an important contribution to the 2015 Paris Agreement, through offsetting Greenhouse gas (GHG) emissions. EU, UK and Scottish forest policy encourages substantial forest expansion. Unfortunately, policy is still inadequately informed by high resolution data, and often assumes a fairly homogenous landscape, uniformly suitable soil types and idealised ‘average’ tree timber yields, while carbon emissions caused by soil disturbance during planting, and changes in climate are rarely adequately considered. Also, the proportional contribution of afforestation targets to national mitigation needs is often overlooked which could lead to over-reliance on tree planting. We address these shortcomings through an integrated modelling approach which estimates net carbon gain for eleven tree species accounting for the interactions between climate, soil and planting practices. We present detailed spatial results for a case study area (Scotland), showing where forest expansion would be likely to result in overall carbon gains, accounting for the differentiated spatial variability of timber yield classes for each one of the species considered including present and future climate. The results showed that upland ecosystems, whose soils are rich in carbon, were vulnerable to net carbon loss, particularly with intensive ground preparation and planting practices. While the prevalence of mineral soils in the lowlands makes them a safer option for planting in theory, these are also areas which might conflict with agricultural activities. Our findings strongly support the notion that both “the right tree in the right place” and “no trees in the wrong place” are important messages for practitioners. In terms of the total UK and Scottish carbon footprints, the magnitude of the offset obtained in 30 years if afforestation goals were fully reached would likely be around 1% of the UK total business as usual GHG footprint and around 10% of the Scottish footprint. Our results can help to improve the targeting of incentives and investments in forest and woodland expansion, but also reinforce the need to pursue emissions reductions in a variety of ways throughout all sectors.",
        "DOI": "10.1016/j.envsci.2022.05.011",
        "paper_author": "Baggio-Compagnucci A.",
        "affiliation_name": "The James Hutton Institute",
        "affiliation_city": "Aberdeen",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60106782",
        "affiliation_state": "Scotland"
    },
    {
        "paper_title": "Socially CompliAnt Navigation Dataset (SCAND): A Large-Scale Dataset of Demonstrations for Social Navigation",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "46",
        "cover_date": "2022-10-01",
        "Abstract": "Social navigation is the capability of an autonomous agent, such as a robot, to navigate in a 'socially compliant' manner in the presence of other intelligent agents such as humans. With the emergence of autonomously navigating mobile robots in human-populated environments (e.g., domestic service robots in homes and restaurants and food delivery robots on public sidewalks), incorporating socially compliant navigation behaviors on these robots becomes critical to ensuring safe and comfortable human-robot coexistence. To address this challenge, imitation learning is a promising framework, since it is easier for humans to demonstrate the task of social navigation rather than to formulate reward functions that accurately capture the complex multi-objective setting of social navigation. The use of imitation learning and inverse reinforcement learning to social navigation for mobile robots, however, is currently hindered by a lack of large-scale datasets that capture socially compliant robot navigation demonstrations in the wild. To fill this gap, we introduce Socially CompliAnt Navigation Dataset ( SCAND )-a large-scale, first-person-view dataset of socially compliant navigation demonstrations. Our dataset contains 8.7 hours, 138 trajectories, 25 miles of socially compliant, human tele-operated driving demonstrations that comprises multi-modal data streams including 3D lidar, joystick commands, odometry, visual and inertial information, collected on two morphologically different mobile robots-a Boston Dynamics Spot and a Clearpath Jackal-by four different human demonstrators in both indoor and outdoor environments. We additionally perform preliminary analysis and validation through real-world robot experiments and show that navigation policies learned by imitation learning on SCAND generate socially compliant behaviors.",
        "DOI": "10.1109/LRA.2022.3184025",
        "paper_author": "Karnan H.",
        "affiliation_name": "Cockrell School of Engineering",
        "affiliation_city": "Austin",
        "affiliation_country": "United States",
        "affiliation_id": "60150401",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "Do young people really engage in sustainable behaviors in their lifestyles?",
        "publication": "Social Indicators Research",
        "citied_by": "30",
        "cover_date": "2022-10-01",
        "Abstract": "In recent years, environmental problems have become a serious issue worldwide due to the increasing damage caused by climate change. People’s environmental awareness has grown, and public opinion is now demanding effective action from governments. Young people around the world are playing an important role in this, with the Fridays For Future movement, calling on policymakers to make environmental protection one of their political priorities. Through a survey of 1,975 high school students, this paper aims to contribute to the study of young people’s sustainable behaviors and their awareness to take effective action against environmental degradation, to explore their concerns and opinions about environmental issues, and to find out what ecological practices they are willing to adopt in their daily lives. Data analysis is conducted using tree-based methods to examine the sustainable behaviors and define the key practices that constitute them. The results of classification tree show that sustainable behaviors impact lifestyles, whether through less demanding actions such as turning off the faucet or appliances, or willingness to work as a volunteer, among others. The Random Forest provides us with a ranked list of sustainable behaviors that young people engage in to reduce and stop environmental degradation. The results of this study may be of interest to policy makers who need to plan educational pathways for students from elementary school to university, as environmental culture must be a cornerstone of our society.",
        "DOI": "10.1007/s11205-022-02955-0",
        "paper_author": "Piscitelli A.",
        "affiliation_name": "Università degli Studi di Napoli Federico II",
        "affiliation_city": "Naples",
        "affiliation_country": "Italy",
        "affiliation_id": "60017293",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Anthropogenic activities amplify wildfire occurrence in the Zagros eco-region of western Iran",
        "publication": "Natural Hazards",
        "citied_by": "8",
        "cover_date": "2022-10-01",
        "Abstract": "The aim of this study was to improve our understanding of factors that affect the spatial distribution of wildfire occurrences at the regional scale. We employed the random forest, boosted regression tree, and genetic algorithm rule-set production models to assess the spatial interplay between fire events and climate, topography, and anthropogenic factors in order to characterize wildfire occurrence in the Zagros eco-region of western Iran. We constructed a geospatial database using the historical fires from the period 2007–2020 and topography, climate, and human related factors. The results demonstrated that human activities (i.e., land use and distance from the settlements and roads) contributed 45% to the probability model of wildfire occurrence in the study region. The models ranked the climate factors (rainfall, temperature, and wind effect) as the second most influential drivers of fire occurrences, whereas topographic features (slope, elevation, and aspect) did not significantly influence fire probability in the landscape. Overall model performance was assessed with the area under the receiver operating characteristic (AUROC) method that showed the superior performance of the RF model in the training phase (AUROC = 0.92) and in its ability to predict upcoming fires (AUROC = 0.90). The insights obtained from this research can bring into focus both the locations and the types of suppression policies that are required to alleviate the effects of the upcoming wildfires in the early twenty-first century.",
        "DOI": "10.1007/s11069-022-05397-6",
        "paper_author": "Jaafari A.",
        "affiliation_name": "Agricultural Research, Education &amp; Extension Organization, Iran",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran",
        "affiliation_id": "60089324",
        "affiliation_state": "Tehran"
    },
    {
        "paper_title": "Improving the performance of micro-simulation models with machine learning: The case of Australian farms",
        "publication": "Economic Modelling",
        "citied_by": "5",
        "cover_date": "2022-10-01",
        "Abstract": "Micro-simulation models are widely used to measure the effects on businesses or individuals of policy changes or other shocks, including the effects on farms of changes in weather conditions and prices. Typically, economic micro-simulation involves econometric analysis of microdata to estimate parametric models. In contrast with the existing literature, this paper presents a non-parametric machine learning based micro-simulation model. In this study, a multi-target regression tree algorithm is combined with farm and weather panel data, to produce an economic micro-simulation model of Australian farm businesses. This approach captures the complex non-linear and farm specific effects of weather and price shocks on profits, with out-of-sample tests showing performance gains over conventional methods. Model results demonstrate the sensitivity of Australian farm profits to weather risk, particularly drought, and show an increase in weather risk exposure over the last 20 years.",
        "DOI": "10.1016/j.econmod.2022.105957",
        "paper_author": "Hughes N.",
        "affiliation_name": "Australian Government",
        "affiliation_city": "Parkes",
        "affiliation_country": "Australia",
        "affiliation_id": "60088313",
        "affiliation_state": "ACT"
    },
    {
        "paper_title": "Forecasting and explaining emergency department visits in a public hospital",
        "publication": "Journal of Intelligent Information Systems",
        "citied_by": "19",
        "cover_date": "2022-10-01",
        "Abstract": "Emergency Departments (EDs) are the most overcrowded places in public hospitals. Machine learning can support decisions on effective ED resource management by accurately forecasting the number of ED visits. In addition, Explainable Artificial Intelligence (XAI) techniques can help explain decisions from forecasting models and address challenges like lack of trust in machine learning results. The objective of this paper is to use machine learning and XAI to forecast and explain the ED visits on the next on duty day. Towards this end, a case study is presented that uses the XGBoost algorithm to create a model that forecasts the number of patient visits to the ED of the University Hospital of Ioannina in Greece, based on historical data from patient visits, time-based data, dates of holidays and special events, and weather data. The SHapley Additive exPlanations (SHAP) framework is used to explain the model. The evaluation of the forecasting model resulted in an MAE value of 18.37, revealing a more accurate model than the baseline, with an MAE of 29.38. The number of patient visits is mostly affected by the day of the week of the on duty day, the mean number of visits in the previous four on duty days, and the maximum daily temperature. The results of this work can help policy makers in healthcare make more accurate and transparent decisions that increase the trust of people affected by them (e.g., medical staff).",
        "DOI": "10.1007/s10844-022-00716-6",
        "paper_author": "Petsis S.",
        "affiliation_name": "University of Macedonia",
        "affiliation_city": "Thessaloniki",
        "affiliation_country": "Greece",
        "affiliation_id": "60001086",
        "affiliation_state": "Central Macedonia"
    },
    {
        "paper_title": "Deep reinforcement learning for improving competitive cycling performance",
        "publication": "Expert Systems with Applications",
        "citied_by": "2",
        "cover_date": "2022-10-01",
        "Abstract": "Developing expert systems that make use of artificial intelligence (AI) to provide predictive analytics as well as targeted recommendations for decision support has been gaining momentum in recent years. Both academia and industry are looking into creating such systems to solve real-world problems and tackle specific challenges. In our work, we investigate the potential application of different machine learning approaches to solutions around competitive cycling. Specifically, we build and evaluate prediction models that are capable of accurately predicting a cyclist's speed and heart rate using sensory information collected during bike rides. In addition, we create a recommendation module that is able to provide real-time action suggestions to cyclists regarding their posture with the goal of improving their overall performance. We achieve this using a combination of model-based reinforcement learning (RL) and deep RL. In particular, we use model-based RL to learn a “simulator” of bike rides using the prediction models and action profiles extracted from sensors placed on the cyclists’ back. We then use deep Q-learning in the simulator to extract policies that improve a cyclist's behavior during a bike ride. Our evaluation shows that by recommending specific actions throughout the ride, cyclists can increase their overall average speed with only a minimal impact on their heart rate. The results presented in this paper constitute clear evidence that advanced AI techniques are a prime candidate for further developing intelligent solutions in competitive cycling and other similar areas.",
        "DOI": "10.1016/j.eswa.2022.117311",
        "paper_author": "Demosthenous G.",
        "affiliation_name": "CYENS Center of Excellence",
        "affiliation_city": "Nicosia",
        "affiliation_country": "Cyprus",
        "affiliation_id": "125868465",
        "affiliation_state": "Nicosia"
    },
    {
        "paper_title": "Robot learning towards smart robotic manufacturing: A review",
        "publication": "Robotics and Computer-Integrated Manufacturing",
        "citied_by": "110",
        "cover_date": "2022-10-01",
        "Abstract": "Robotic equipment has been playing a central role since the proposal of smart manufacturing. Since the beginning of the first integration of industrial robots into production lines, industrial robots have enhanced productivity and relieved humans from heavy workloads significantly. Towards the next generation of manufacturing, this review first introduces the comprehensive background of smart robotic manufacturing within robotics, machine learning, and robot learning. Definitions and categories of robot learning are summarised. Concretely, imitation learning, policy gradient learning, value function learning, actor-critic learning, and model-based learning as the leading technologies in robot learning are reviewed. Training tools, benchmarks, and comparisons amongst different robot learning methods are delivered. Typical industrial applications in robotic grasping, assembly, process control, and industrial human-robot collaboration are listed and discussed. Finally, open problems and future research directions are summarised.",
        "DOI": "10.1016/j.rcim.2022.102360",
        "paper_author": "Liu Z.",
        "affiliation_name": "Wuhan University of Technology",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60022414",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Utility Fairness for the Differentially Private Federated-Learning-Based Wireless IoT Networks",
        "publication": "IEEE Internet of Things Journal",
        "citied_by": "10",
        "cover_date": "2022-10-01",
        "Abstract": "Federated learning (FL) allows predictive model training on the sensed data in a wireless Internet of Things (IoT) network evading data collection cost in terms of energy, time, and privacy. In this article, for an FL setting, we model the learning gain achieved by an IoT device against its participation cost as its utility. The local model quality and the associated cost differ from device to device due to the device heterogeneity, which could be time varying. We identify that this results in utility unfairness because the same global model is shared among the devices. In the vanilla FL setting, the master is unaware of devices' local model computation and transmission costs, thus, it is unable to address the utility unfairness problem. In addition, a device may exploit this lack of knowledge at the master to intentionally reduce its expenditure and thereby boost its utility. We propose to control the quality of the global model shared with the devices, in each round, based on their contribution and expenditure. This is achieved by employing differential privacy (DP) to curtail global model divulgence based on the learning contribution. Furthermore, we devise adaptive computation and transmission policies for each device to control its expenditure in order to mitigate utility unfairness. Our results show that the proposed scheme reduces the standard deviation of the energy cost of devices by 99% in comparison to the benchmark scheme, while the standard deviation of the training loss of devices varies around 0.103.",
        "DOI": "10.1109/JIOT.2022.3165596",
        "paper_author": "Alvi S.A.",
        "affiliation_name": "The Australian National University",
        "affiliation_city": "Canberra",
        "affiliation_country": "Australia",
        "affiliation_id": "60008950",
        "affiliation_state": "ACT"
    },
    {
        "paper_title": "Path Design and Resource Management for NOMA Enhanced Indoor Intelligent Robots",
        "publication": "IEEE Transactions on Wireless Communications",
        "citied_by": "14",
        "cover_date": "2022-10-01",
        "Abstract": "A communication enabled indoor intelligent robots (IRs) service framework is proposed, where non-orthogonal multiple access (NOMA) technique is adopted to enable highly reliable communications. In cooperation with the ultramodern indoor channel model recently proposed by the International Telecommunication Union (ITU), the Lego modeling method is proposed, which can deterministically describe the indoor layout and channel state in order to construct the radio map. The investigated radio map is invoked as a virtual environment to train the reinforcement learning agent, which can save training time and hardware costs. Build on the proposed communication model, motions of IRs who need to reach designated mission destinations and their corresponding down-link power allocation policy are jointly optimized to maximize the mission efficiency and communication reliability of IRs. In an effort to solve this optimization problem, a novel reinforcement learning approach named deep transfer deterministic policy gradient (DT-DPG) algorithm is proposed. Our simulation results demonstrate in the following: 1) with the aid of NOMA techniques, the communication reliability of IRs is effectively improved; 2) radio map is qualified to be a virtual training environment, and its statistical channel state information improves training efficiency by about 30%; 3) proposed DT-DPG algorithm is superior to the conventional deep deterministic policy gradient (DDPG) algorithm in terms of optimization performance, training time, and anti-local optimum ability.",
        "DOI": "10.1109/TWC.2022.3163422",
        "paper_author": "Zhong R.",
        "affiliation_name": "Queen Mary University of London",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60022109",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "From multiple aspect trajectories to predictive analysis: a case study on fishing vessels in the Northern Adriatic sea",
        "publication": "GeoInformatica",
        "citied_by": "11",
        "cover_date": "2022-10-01",
        "Abstract": "In this paper we model spatio-temporal data describing the fishing activities in the Northern Adriatic Sea over four years. We build, implement and analyze a database based on the fusion of two complementary data sources: trajectories from fishing vessels (obtained from terrestrial Automatic Identification System, or AIS, data feed) and fish catch reports (i.e., the quantity and type of fish caught) of the main fishing market of the area. We present all the phases of the database creation, starting from the raw data and proceeding through data exploration, data cleaning, trajectory reconstruction and semantic enrichment. We implement the database by using MobilityDB, an open source geospatial trajectory data management and analysis platform. Subsequently, we perform various analyses on the resulting spatio-temporal database, with the goal of mapping the fishing activities on some key species, highlighting all the interesting information and inferring new knowledge that will be useful for fishery management. Furthermore, we investigate the use of machine learning methods for predicting the Catch Per Unit Effort (CPUE), an indicator of the fishing resources exploitation in order to drive specific policy design. A variety of prediction methods, taking as input the data in the database and environmental factors such as sea temperature, waves height and Clorophill-a, are put at work in order to assess their prediction ability in this field. To the best of our knowledge, our work represents the first attempt to integrate fishing ships trajectories derived from AIS data, environmental data and catch data for spatio-temporal prediction of CPUE – a challenging task.",
        "DOI": "10.1007/s10707-022-00463-4",
        "paper_author": "Brandoli B.",
        "affiliation_name": "Dalhousie University",
        "affiliation_city": "Halifax",
        "affiliation_country": "Canada",
        "affiliation_id": "60015913",
        "affiliation_state": "NS"
    },
    {
        "paper_title": "EU-27 bank failure prediction with C5.0 decision trees and deep learning neural networks",
        "publication": "Research in International Business and Finance",
        "citied_by": "31",
        "cover_date": "2022-10-01",
        "Abstract": "This article provides evidence that machine learning methods are suitable for reliably predicting the failure risk of European Union-27 banks from the experiences of the past decade. It demonstrates that earnings, capital adequacy, and management capability are the strongest predictors of bank failure. Critical and relevant field research is presented in the context of economic uncertainties arising from the COVID-19 pandemic. The results suggest that the developed models possess high predictive power, with the C5.0 decision tree model providing the best performance. The findings have policy implications for bank supervisory authorities, bank executives, risk management professionals, and policymakers working in finance. The models can be used to recognize bank weaknesses in time to take appropriate mitigating actions.",
        "DOI": "10.1016/j.ribaf.2022.101644",
        "paper_author": "Kristóf T.",
        "affiliation_name": "Budapesti Corvinus Egyetem",
        "affiliation_city": "Budapest",
        "affiliation_country": "Hungary",
        "affiliation_id": "60001434",
        "affiliation_state": "Budapest"
    },
    {
        "paper_title": "Real-time power system generator tripping control based on deep reinforcement learning",
        "publication": "International Journal of Electrical Power and Energy Systems",
        "citied_by": "10",
        "cover_date": "2022-10-01",
        "Abstract": "In case of faults or severe disturbances, the power system will enter an emergency operation state. After the system instability is detected, oscillation and blackout will occur in the system if effective control measures are not taken in time. Generator tripping control (GTC) is the most effective emergency control measure. In view of the mismatch between the traditional GTC algorithm and the transient stability assessment method based on machine learning, a new real-time GTC method is needed. In this paper, a three-part control framework is designed for the GTC problem. The control agent is endowed with decision-making ability by interacting with the simulation environment in the offline pre-learning part. Then the trained agent is transplanted to the online application which can help system operators make decisions. Meanwhile, the agent is updated with real data to be better adapted to the actual system in the online learning part. A deep reinforcement learning algorithm, deep deterministic policy gradient (DDPG) is employed to train the control agent in this framework. A modified DDPG algorithm and the corresponding reward function are designed for the GTC problem. Convolution neural network (CNN) is added to the DDPG network, by which the training time of the agent is shortened and the generalization ability of the algorithm is improved. Trained with simulation data and real system experience, the control agent can determine control strategies timely according to the system operating conditions. Simulation results on the IEEE-39 bus system and the realistic regional power system of Eastern China show the effectiveness, generalizability, and timeliness of the decision algorithm.",
        "DOI": "10.1016/j.ijepes.2022.108127",
        "paper_author": "Lin B.",
        "affiliation_name": "Fuzhou University",
        "affiliation_city": "Fuzhou",
        "affiliation_country": "China",
        "affiliation_id": "60017605",
        "affiliation_state": "Fujian"
    },
    {
        "paper_title": "Autonomous Vehicle Cut-In Algorithm for Lane-Merging Scenarios via Policy-Based Reinforcement Learning Nested Within Finite-State Machine",
        "publication": "IEEE Transactions on Intelligent Transportation Systems",
        "citied_by": "41",
        "cover_date": "2022-10-01",
        "Abstract": "Lane-merging scenarios pose highly challenging problems for autonomous vehicles due to conflicts of interest between the human-driven and cutting-in autonomous vehicles. Such conflicts become severe when traffic increases, and cut-in algorithms suffer from a steep trade-off between safety and cut-in performance. In this study, a reinforcement learning (RL)-based cut-in policy network nested within a finite state machine (FSM) - which is a high-level decision maker, is proposed to achieve high cut-in performance without sacrificing safety. This FSM-RL hybrid approach is proposed to obtain 1) a strategic and adjustable algorithm, 2) optimal safety and cut-in performance, and 3) robust and consistent performance. In the high-level decision making algorithm, the FSM provides a framework for four cut-in phases (ready for safe gap selection, gap approach, negotiation, and lane-change execution) and handles the transitions between these phases by calculating the collision risks associated with target vehicles. For the lane-change phase, a policy-based deep-RL approach with a soft actor-critic network is employed to get optimal cut-in performance. The results of simulations show that the proposed FSM-RL cut-in algorithm consistently achieves a high cut-in success rate without sacrificing safety. In particular, as the traffic increases, the cut-in success rate and safety are significantly improved over existing optimized rule-based cut-in algorithms and end-to-end RL algorithm.",
        "DOI": "10.1109/TITS.2022.3153848",
        "paper_author": "Hwang S.",
        "affiliation_name": "Naver Labs Corporation",
        "affiliation_city": "Seongnam",
        "affiliation_country": "South Korea",
        "affiliation_id": "60121135",
        "affiliation_state": "Gyeonggi-do"
    },
    {
        "paper_title": "Solving job scheduling problems in a resource preemption environment with multi-agent reinforcement learning",
        "publication": "Robotics and Computer-Integrated Manufacturing",
        "citied_by": "66",
        "cover_date": "2022-10-01",
        "Abstract": "In smart manufacturing, robots gradually replace traditional machines as new processing units, which have significantly liberated laborers and reduced manufacturing expenditure. However, manufacturing resources are usually limited so that the preemption relationship exists among robots. Under this circumstance, job scheduling puts forward higher requirements on accuracy and generalization. To this end, this paper proposes a scheduling algorithm to solve job scheduling problems in a resource preemption environment with multi-agent reinforcement learning. The resource preemption environment is modeled as a decentralized partially observable Markov decision process, where each job is regarded as an intelligent agent that chooses an available robot according to its current partial observation. Based on this modeling, a multi-agent scheduling architecture is constructed to handle the high-dimension action space issue caused by multi-task simultaneous scheduling. Besides, multi-agent reinforcement learning is employed to learn both the decision-making policy of each agent and the cooperation between job agents. This paper is novel in addressing the scheduling problem in a resource preemption environment and solving the job shop scheduling problem with multi-agent reinforcement learning. The experiments of the case study indicate that our proposed method outperforms the traditional rule-based methods and the distributed-agent reinforcement learning method in total makespan, training stability, and model generalization.",
        "DOI": "10.1016/j.rcim.2022.102324",
        "paper_author": "Wang X.",
        "affiliation_name": "Beihang University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60013789",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Tax-Induced Inequalities in the Sharing Economy",
        "publication": "Management Science",
        "citied_by": "16",
        "cover_date": "2022-10-01",
        "Abstract": "The growth of sharing economy marketplaces like Airbnb has generated discussions on their socioeconomic impact and lack of regulation. As a result, most major cities in the United States have started to collect an “occupancy tax” for Airbnb bookings. In this study, we investigate the heterogeneous treatment effects of the occupancy tax policy on Airbnb listings, using a combination of a generalized causal forest methodology and a difference-in-differences framework. While we find that the introduction of the tax significantly reduces both listing revenues and sales, more importantly, these effects are disproportionately more pronounced for residential hosts with single shared-space (nontarget) listings versus commercial hosts with multiple properties or entire-space (target) listings. We further show that this unintended consequence is caused by customers' discriminatory tax aversion against nontarget listings. We then leverage these empirical results by prescribing how hosts should optimally set prices in response to the occupancy tax and identify the discriminatory tax rates that would equalize the tax's effect across nontarget and target listings.",
        "DOI": "10.1287/mnsc.2021.4277",
        "paper_author": "Cui Y.",
        "affiliation_name": "Cornell SC Johnson College of Business",
        "affiliation_city": "Ithaca",
        "affiliation_country": "United States",
        "affiliation_id": "60116635",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "A Novel Ensemble Machine Learning and an Evolutionary Algorithm in Modeling the COVID-19 Epidemic and Optimizing Government Policies",
        "publication": "IEEE Transactions on Systems, Man, and Cybernetics: Systems",
        "citied_by": "12",
        "cover_date": "2022-10-01",
        "Abstract": "The spread of the COVID-19 disease has prompted a need for immediate reaction by governments to curb the pandemic. Many countries have adopted different policies and studies are performed to understand the effect of each of the policies on the growth rate of the infected cases. In this article, the data about the policies taken by all countries at each date, and the effect of the policies on the growth rate of the pandemic are used to build a model of the pandemic's behavior. The model takes as input a set of policies and predicts the growth rate of the pandemic. Then, a population-based multiobjective optimization algorithm is developed, which uses the model to search through the policy space and finds a set of policies that minimize the cost induced to the society due to the policies and the growth rate of the pandemic. Because of the complexity of the modeling problem and the uncertainty in measuring the growth rate of the pandemic via the models, an ensemble learning algorithm is proposed in this article to improve the performance of individual learning algorithms. The ensemble consists of ten learning algorithms and a metamodel algorithm that is built to predict the accuracy of each learning algorithm for a given data record. The metamodel is a set of support vector machine (SVM) algorithms that is used in the aggregation phase of the ensemble algorithm. Because there is uncertainty in measuring the growth rate via the models, a landscape smoothing operator is proposed in the optimization process, which aims at reducing uncertainty. The algorithm is tested on open access data online and experiments on the ensemble learning and the policy optimization algorithms are performed.",
        "DOI": "10.1109/TSMC.2022.3143955",
        "paper_author": "Tayarani-Najaran M.H.",
        "affiliation_name": "University of Hertfordshire",
        "affiliation_city": "Hatfield",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60032760",
        "affiliation_state": "Hertfordshire"
    },
    {
        "paper_title": "Predicting household resilience with machine learning: preliminary cross-country tests",
        "publication": "Empirical Economics",
        "citied_by": "5",
        "cover_date": "2022-10-01",
        "Abstract": "Using a unique cross-country sample from 10 impact evaluations of development projects, we test the out-of-sample performance of machine learning algorithms in predicting non-resilient households, where resilience is a subjective metrics defined as the perceived ability to recover from shocks. We report preliminary evidence of the potential of these data-driven techniques to identify the main predictors of household resilience and inform the targeting of resilience-oriented policy interventions.",
        "DOI": "10.1007/s00181-022-02199-4",
        "paper_author": "Garbero A.",
        "affiliation_name": "International Fund for Agricultural Development",
        "affiliation_city": "Rome",
        "affiliation_country": "Italy",
        "affiliation_id": "60091230",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "The (real) need for a human touch: testing a human–machine hybrid topic classification workflow on a New York Times corpus",
        "publication": "Quality and Quantity",
        "citied_by": "6",
        "cover_date": "2022-10-01",
        "Abstract": "The classification of the items of ever-increasing textual databases has become an important goal for a number of research groups active in the field of computational social science. Due to the increased amount of text data there is a growing number of use-cases where the initial effort of human classifiers was successfully augmented using supervised machine learning (SML). In this paper, we investigate such a hybrid workflow solution classifying the lead paragraphs of New York Times front-page articles from 1996 to 2006 according to policy topic categories (such as education or defense) of the Comparative Agendas Project (CAP). The SML classification is conducted in multiple rounds and, within each round, we run the SML algorithm on n samples and n times if the given algorithm is non-deterministic (e.g., SVM). If all the SML predictions point towards a single label for a document, then it is classified as such (this approach is also called a “voting ensemble\"). In the second step, we explore several scenarios, ranging from using the SML ensemble without human validation to incorporating active learning. Using these scenarios, we can quantify the gains from the various workflow versions. We find that using human coding and validation combined with an ensemble SML hybrid approach can reduce the need for human coding while maintaining very high precision rates and offering a modest to a good level of recall. The modularity of this hybrid workflow allows for various setups to address the idiosyncratic resource bottlenecks that a large-scale text classification project might face.",
        "DOI": "10.1007/s11135-021-01287-4",
        "paper_author": "Sebők M.",
        "affiliation_name": "Centre for Social Sciences",
        "affiliation_city": "Budapest",
        "affiliation_country": "Hungary",
        "affiliation_id": "60182614",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Voting intentions on social media and political opinion polls",
        "publication": "Government Information Quarterly",
        "citied_by": "9",
        "cover_date": "2022-10-01",
        "Abstract": "Opinion polls play an important role in modern democratic processes: they are known to not only affect the outcomes of elections, but also have a significant influence on government policy after elections. Recent years have seen large discrepancies between polls and outcomes at several major elections and referendums, stemming from decreased participation in polls and an increasingly volatile electorate. This calls for new ways to measure public support for political parties. In this paper, we propose a method for measuring the popularity of election candidates on social media using Machine Learning-based Natural Language Processing techniques. The method is based on detecting voting intentions in the data. This is a considerable advance upon earlier work using automatic sentiment analysis. We evaluate the method both intrinsically on a set of hand-labelled social media posts, and extrinsically – by forecasting daily election polls. In the extrinsic evaluation, we analyze data from the 2016 US presidential election, and find that voting intentions measured from social media provide significant additional predictive value for forecasting daily polls. Thus, we demonstrate that the proposed method can be used to interpolate polls both spatially and temporally, thus providing reliable, continuous and fine-grained information about public opinion on current political issues.",
        "DOI": "10.1016/j.giq.2021.101658",
        "paper_author": "Pekar V.",
        "affiliation_name": "Aston Business School",
        "affiliation_city": "Birmingham",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60116225",
        "affiliation_state": "West Midlands"
    },
    {
        "paper_title": "SACC: A Size Adaptive Content Caching Algorithm in Fog/Edge Computing Using Deep Reinforcement Learning",
        "publication": "IEEE Transactions on Emerging Topics in Computing",
        "citied_by": "12",
        "cover_date": "2022-10-01",
        "Abstract": "Edge/Fog caching is promising to mitigate the data traffic problem in both traditional wireline/wireless networks and the 5G network. Recently, deep reinforcement learning (DRL) has been adopted to provide a more powerful content caching policy. The current DRL-based scheme considers the requests for the same size and updates the caching for each request. However, the real-world data delivery systems usually refresh the content cache periodically, with different sizes of requests. To satisfy the real-world requirements, this study proposes a novel size adaptive content caching algorithm using DRL, termed SACC. SACC models the requests with random sizes and updates the cache after a batch of requests. Technically, SACC utilizes the Actor-Critic framework, which is able to process large discrete action space. SACC comprehensively considers the short-, medium- and long-term requests as the state to train the actor network. The reward is modeled as the cache hit rate. Once an action is selected from the policy network, it is expended to its k nearest neighbors. The critic network finds the action with the best reward from the k actions. The performance of the proposed SACC is evaluated through computer simulation. The experimental results showed that SACC could train the network much more efficiently and improve the cache hit rate by as much as 4% when comparing to the state-of-art DRL-based scheme.",
        "DOI": "10.1109/TETC.2021.3115793",
        "paper_author": "Zhou X.",
        "affiliation_name": "Beijing University of Civil Engineering and Architecture",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60092860",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Decentralized Learning for Optimality in Stochastic Dynamic Teams and Games With Local Control and Global State Information",
        "publication": "IEEE Transactions on Automatic Control",
        "citied_by": "17",
        "cover_date": "2022-10-01",
        "Abstract": "Stochastic dynamic teams and games are rich models for decentralized systems and challenging testing grounds for multiagent learning. Previous work that guaranteed team optimality assumed stateless dynamics, or an explicit coordination mechanism, or joint-control sharing. In this article, we present an algorithm with guarantees of convergence to team optimal policies in teams and common interest games. The algorithm is a two-timescale method that uses a variant of Q-learning on the finer timescale to perform policy evaluation while exploring the policy space on the coarser timescale. Agents following this algorithm are 'independent learners': they use only local controls, local cost realizations, and global state information, without access to controls of other agents. The results presented here are the first, to the best of our knowledge, to give formal guarantees of convergence to team optimality using independent learners in stochastic dynamic teams and common interest games.",
        "DOI": "10.1109/TAC.2021.3121228",
        "paper_author": "Yongacoglu B.",
        "affiliation_name": "Queen’s University",
        "affiliation_city": "Kingston",
        "affiliation_country": "Canada",
        "affiliation_id": "60016005",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "Real-Time Scheduling for Dynamic Partial-No-Wait Multiobjective Flexible Job Shop by Deep Reinforcement Learning",
        "publication": "IEEE Transactions on Automation Science and Engineering",
        "citied_by": "109",
        "cover_date": "2022-10-01",
        "Abstract": "In modern discrete flexible manufacturing systems, dynamic disturbances frequently occur in real time and each job may contain several special operations in partial-no-wait constraint due to technological requirements. In this regard, a hierarchical multiagent deep reinforcement learning (DRL)-based real-time scheduling method named hierarchical multi-agent proximal policy optimization (HMAPPO) is developed to address the dynamic partial-no-wait multiobjective flexible job shop scheduling problem (DMOFJSP-PNW) with new job insertions and machine breakdowns. The proposed HMAPPO contains three proximal policy optimization (PPO)-based agents operating in different spatiotemporal scales, namely, objective agent, job agent, and machine agent. The objective agent acts as a higher controller periodically determining the temporary objectives to be optimized. The job agent and machine agent are lower actuators, respectively, choosing a job selection rule and machine assignment rule to achieve the temporary objective at each rescheduling point. Five job selection rules and six machine assignment rules are designed to select an uncompleted job and assign the next operation of which together with its successors in no-wait constraint on the corresponding processing machines. A hierarchical PPO-based training algorithm is developed. Extensive numerical experiments have confirmed the effectiveness and superiority of the proposed HMAPPO compared with other well-known dynamic scheduling methods. Note to Practitioners-The motivation of this article stems from the need to develop real-time scheduling methods for modern discrete flexible manufacturing factories, such as aerospace product manufacturing and steel manufacturing, where dynamic events frequently occur, and each job may contain several operations subjected to the no-wait constraint. Traditional dynamic scheduling methods, such as metaheuristics or dispatching rules, either suffer from poor time efficiency or fail to ensure good solution quality for multiple objectives in the long-term run. Meanwhile, few of the previous studies have considered the partial-no-wait constraint among several operations from the same job, which widely exists in many industries. In this article, we propose a hierarchical multiagent deep reinforcement learning (DRL)-based real-time scheduling method named HMAPPO to address the dynamic partial-no-wait multiobjective flexible job shop scheduling problem (DMOFJSP-PNW) with new job insertions and machine breakdowns. The proposed HMAPPO uses three DRL-based agents to adaptively select the temporary objectives and choose the most feasible dispatching rules to achieve them at different rescheduling points, through which the rescheduling can be made in real time and a good compromise among different objectives can be obtained in the long-term schedule. Extensive experimental results have demonstrated the effectiveness and superiority of the proposed HMAPPO. For industrial applications, this method can be extended to many other production scheduling problems, such as hybrid flow shops and open shop with different uncertainties and objectives.",
        "DOI": "10.1109/TASE.2021.3104716",
        "paper_author": "Luo S.",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60025278",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Multimodal Data Processing System for LiDAR-Based Human Activity Recognition",
        "publication": "IEEE Transactions on Cybernetics",
        "citied_by": "36",
        "cover_date": "2022-10-01",
        "Abstract": "Increasingly, the task of detecting and recognizing the actions of a human has been delegated to some form of neural network processing camera or wearable sensor data. Due to the degree to which the camera can be affected by lighting and wearable sensors scantiness, neither one modality can capture the required data to perform the task confidently. That being the case, range sensors, like light detection and ranging (LiDAR), can complement the process to perceive the environment more robustly. Most recently, researchers have been exploring ways to apply convolutional neural networks to 3-D data. These methods typically rely on a single modality and cannot draw on information from complementing sensor streams to improve accuracy. This article proposes a framework to tackle human activity recognition by leveraging the benefits of sensor fusion and multimodal machine learning. Given both RGB and point cloud data, our method describes the activities being performed by subjects using regions with a convolutional neural network (R-CNN) and a 3-D modified Fisher vector network. Evaluated on a custom captured multimodal dataset demonstrates that the model outputs remarkably accurate human activity classification (90%). Furthermore, this framework can be used for sports analytics, understanding social behavior, surveillance, and perhaps most notably by autonomous vehicles (AVs) to data-driven decision-making policies in urban areas and indoor environments.",
        "DOI": "10.1109/TCYB.2021.3085489",
        "paper_author": "Roche J.",
        "affiliation_name": "Loughborough University London",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60123703",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Predicting Blood Donors Using Machine Learning Techniques",
        "publication": "Information Systems Frontiers",
        "citied_by": "6",
        "cover_date": "2022-10-01",
        "Abstract": "The United States’ blood supply chain is experiencing market decline due to recent innovations in surgical practice, transfusion management, and hospital policy. These innovations strain US blood centers, resulting in cuts to surge capacities, consolidation, and reduced funding for research and outreach programs. In this study, we use data from a regional blood center to explore the application of contemporary machine learning algorithms for modeling donor retention. Such predictive models of donor retention can be used to design more cost effective donor outreach programs. Using data from a large US blood center paired with random forest classifiers, we are able to build a model of donor retention with a Mathews correlation of coefficient of 0.851.",
        "DOI": "10.1007/s10796-021-10149-1",
        "paper_author": "Kauten C.",
        "affiliation_name": "Samuel Ginn College of Engineering",
        "affiliation_city": "Auburn",
        "affiliation_country": "United States",
        "affiliation_id": "60009974",
        "affiliation_state": "AL"
    },
    {
        "paper_title": "Reinforcement Learning Control of Robotic Knee With Human-in-the-Loop by Flexible Policy Iteration",
        "publication": "IEEE Transactions on Neural Networks and Learning Systems",
        "citied_by": "30",
        "cover_date": "2022-10-01",
        "Abstract": "We are motivated by the real challenges presented in a human-robot system to develop new designs that are efficient at data level and with performance guarantees, such as stability and optimality at system level. Existing approximate/adaptive dynamic programming (ADP) results that consider system performance theoretically are not readily providing practically useful learning control algorithms for this problem, and reinforcement learning (RL) algorithms that address the issue of data efficiency usually do not have performance guarantees for the controlled system. This study fills these important voids by introducing innovative features to the policy iteration algorithm. We introduce flexible policy iteration (FPI), which can flexibly and organically integrate experience replay and supplemental values from prior experience into the RL controller. We show system-level performances, including convergence of the approximate value function, (sub)optimality of the solution, and stability of the system. We demonstrate the effectiveness of the FPI via realistic simulations of the human-robot system. It is noted that the problem we face in this study may be difficult to address by design methods based on classical control theory as it is nearly impossible to obtain a customized mathematical model of a human-robot system either online or offline. The results we have obtained also indicate the great potential of RL control to solving realistic and challenging problems with high-dimensional control inputs.",
        "DOI": "10.1109/TNNLS.2021.3071727",
        "paper_author": "Gao X.",
        "affiliation_name": "Ira A. Fulton Schools of Engineering",
        "affiliation_city": "Tempe",
        "affiliation_country": "United States",
        "affiliation_id": "60139200",
        "affiliation_state": "AZ"
    },
    {
        "paper_title": "Adaptive Observation-Based Efficient Reinforcement Learning for Uncertain Systems",
        "publication": "IEEE Transactions on Neural Networks and Learning Systems",
        "citied_by": "14",
        "cover_date": "2022-10-01",
        "Abstract": "This article develops an adaptive observation-based efficient reinforcement learning (RL) approach for systems with uncertain drift dynamics. A novel concurrent learning adaptive extended observer (CL-AEO) is first designed to jointly estimate the system state and parameter. This observer has a two-time-scale structure and does not require any additional numerical techniques to calculate the state derivative information. The idea of concurrent learning (CL) is leveraged to use the recorded data, which leads to a relaxed verifiable excitation condition for the convergence of parameter estimation. Based on the estimated state and parameter provided by the CL-AEO, a simulation of experience-based RL scheme is developed to online approximate the optimal control policy. Rigorous theoretical analysis is given to show that the practical convergence of the system state to the origin and the developed policy to the ideal optimal policy can be achieved without the persistence of excitation (PE) condition. Finally, the effectiveness and superiority of the developed methodology are demonstrated via comparative simulations.",
        "DOI": "10.1109/TNNLS.2021.3070852",
        "paper_author": "Ran M.",
        "affiliation_name": "School of Electrical and Electronic Engineering",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore",
        "affiliation_id": "60118454",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Efficient Online Scheduling for Coflow-Aware Machine Learning Clusters",
        "publication": "IEEE Transactions on Cloud Computing",
        "citied_by": "12",
        "cover_date": "2022-10-01",
        "Abstract": "Distributed machine learning (DML) is an increasingly important workload. In a DML job, each communication phase can comprise a coflow, and there are dependencies among its coflows. Thus, efficient coflow scheduling becomes critical for DML jobs. However, the majority of existing solutions focus on scheduling single-stage coflows with no dependencies. While there are a few studies schedule dependent coflows of multi-stage jobs, they suffer from either practical or theoretical issues. Motivated by this situation, we study how to schedule dependent coflows of multiple DML jobs to minimize the total JCT in a shared cluster. We present a formal mathematical formulation for this problem and prove its NP-hardness. To solve this problem without job size information, we present an online coflow-aware optimization framework called Parrot. The core idea in Parrot is to infer the job with the shortest remaining processing time (SRPT) each time and dynamically control the inferred job's bandwidth based on how confident it is an SRPT job while being mindful of not starving any other job. Specifically, in the design of Parrot, we present a least per-coflow attained service (LPCAS) policy to infer the SRPT job. We further propose a dynamic job weight assignment mechanism and a linear program (LP) based weighted bandwidth scaling strategy for sharing bandwidth among DML jobs. We have proved that Parrot algorithm has a non-trivial competitive ratio. The results from large-scale trace-driven simulations further demonstrate that our Parrot can reduce the total JCT by up to 58.4 percent, compared to the state-of-the-art Aalo solution.",
        "DOI": "10.1109/TCC.2020.3040312",
        "paper_author": "Li W.",
        "affiliation_name": "Hong Kong University of Science and Technology",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong",
        "affiliation_id": "60008592",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Automation, Algorithms, and Beyond: Why Work Design Matters More Than Ever in a Digital World",
        "publication": "Applied Psychology",
        "citied_by": "330",
        "cover_date": "2022-10-01",
        "Abstract": "We propose a central role for work design in understanding the effects of digital technologies. We give examples of how new technologies can—depending on various factors—positively and negatively affect job resources (autonomy/control, skill use, job feedback, relational aspects) and job demands (e.g., performance monitoring), with consequences for employee well-being, safety, and performance. We identify four intervention strategies. First, work design choices need to be proactively considered during technology implementation, consistent with the sociotechnical systems principle of joint optimization. Second, human-centred design principles should be explicitly considered in the design and procurement of new technologies. Third, organizationally oriented intervention strategies need to be supported by macro-level policies. Fourth, there is a need to go beyond a focus on upskilling employees to help them adapt to technology change, to also focus on training employees, as well as other stakeholders, in work design and related topics. Finally, we identify directions for moving the field forward, including new research questions (e.g., job autonomy in the context of machine learning; understanding designers’ work design mindsets; investigating how job crafting applies to technology); a reorientation of methods (e.g., interdisciplinary, intervention studies); and steps for achieving practical impact.",
        "DOI": "10.1111/apps.12241",
        "paper_author": "Parker S.K.",
        "affiliation_name": "Curtin University",
        "affiliation_city": "Perth",
        "affiliation_country": "Australia",
        "affiliation_id": "60031226",
        "affiliation_state": "WA"
    },
    {
        "paper_title": "Integrating equipment investment strategy with maintenance operations under uncertain failures",
        "publication": "Annals of Operations Research",
        "citied_by": "2",
        "cover_date": "2022-10-01",
        "Abstract": "This paper studies the issue of coordinating equipment maintenance operations with capital investment strategy in the presence of random equipment failures. The traditional approach, developed by Kamien and Schwartz (KS) in their celebrated paper published in 1971, is to formulate the problem as a deterministic optimal control problem with the probability of machine failure as the state variable. With this approach, the optimal policy is deterministic. As a major departure from the KS approach, we explicitly model the underlying stochastic process of machine failures. Our analysis of the stochastic dynamic programming model offers new insights into the problem. Under a long planning horizon with a limited replacement opportunity, each individual machine serves as a revenue generator and contributes a significant amount to the profit of the system. In contrast, when the replacement budget is quite generous over a relatively short planning horizon, adding one extra machine only helps as a backup for unexpected failures of the machines purchased before it. An interesting result derived from this comparison is that a deterministic policy turns out to be optimal for the former, while a state-contingent policy must be applied to the latter. In other words, the deterministic KS approach does not work in general when a chain of machine replacement is considered. We further characterize the effects of the discount rate, productivity deterioration, learning, decision delay, and technology advancement on the optimal policy.",
        "DOI": "10.1007/s10479-015-1862-0",
        "paper_author": "Bensoussan A.",
        "affiliation_name": "The Naveen Jindal School of Management",
        "affiliation_city": "Richardson",
        "affiliation_country": "United States",
        "affiliation_id": "60116070",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "FLoBC: A Decentralized Blockchain-Based Federated Learning Framework",
        "publication": "2022 4th International Conference on Blockchain Computing and Applications, BCCA 2022",
        "citied_by": "8",
        "cover_date": "2022-09-30",
        "Abstract": "The rapid expansion of data worldwide invites the need for more distributed solutions in order to apply machine learning on a much wider scale. The resultant distributed learning systems can have various degrees of centralization. In this work, we demonstrate our solution FLoBC for building a generic decentralized federated learning system using the blockchain technology, accommodating any machine learning model that is compatible with gradient descent optimization. We present our system design comprising the two decentralized actors: trainer and validator, alongside our methodology for ensuring reliable and efficient operation of said system. Finally, we utilize FLoBC as an experimental sandbox to compare and contrast the effects of trainer-to-validator ratio, reward-penalty policy, and model synchronization schemes on the overall system performance, ultimately showing by example that a decentralized federated learning system is indeed a feasible alternative to more centralized architectures.",
        "DOI": "10.1109/BCCA55292.2022.9922258",
        "paper_author": "Ghanem M.",
        "affiliation_name": "The American University in Cairo",
        "affiliation_city": "New Cairo",
        "affiliation_country": "Egypt",
        "affiliation_id": "60019087",
        "affiliation_state": "Cairo"
    },
    {
        "paper_title": "Predicting the behavioral intentions of hospice and palliative care providers from real-world data using supervised learning: A cross-sectional survey study",
        "publication": "Frontiers in Public Health",
        "citied_by": "2",
        "cover_date": "2022-09-30",
        "Abstract": "Background: Hospice and palliative care (HPC) aims to improve end-of-life quality and has received much more attention through the lens of an aging population in the midst of the coronavirus disease pandemic. However, several barriers remain in China due to a lack of professional HPC providers with positive behavioral intentions. Therefore, we conducted an original study introducing machine learning to explore individual behavioral intentions and detect factors of enablers of, and barriers to, excavating potential human resources and improving HPC accessibility. Methods: A cross-sectional study was designed to investigate healthcare providers' behavioral intentions, knowledge, attitudes, and practices in hospice care (KAPHC) with an indigenized KAPHC scale. Binary Logistic Regression and Random Forest Classifier (RFC) were performed to model impacting and predict individual behavioral intentions. Results: The RFC showed high sensitivity (accuracy = 0.75; F1 score = 0.84; recall = 0.94). Attitude could directly or indirectly improve work enthusiasm and is the most efficient approach to reveal behavioral intentions. Continuous practice could also improve individual confidence and willingness to provide HPC. In addition, scientific knowledge and related skills were the foundation of implementing HPC. Conclusion: Individual behavioral intention is crucial for improving HPC accessibility, particularly at the initial stage. A well-trained RFC can help estimate individual behavioral intentions to organize a productive team and promote additional policies.",
        "DOI": "10.3389/fpubh.2022.927874",
        "paper_author": "Chu T.",
        "affiliation_name": "Shanghai University of Traditional Chinese Medicine",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60010170",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A new approach to impact case study analytics",
        "publication": "Data and Policy",
        "citied_by": "0",
        "cover_date": "2022-09-28",
        "Abstract": "The 2014 Research Excellence Framework (REF) assessed the quality of university research in the UK. 20% of the assessment was allocated according to peer review of the impact of research, reflecting the growing importance of impact in UK government policy. Beyond academia, impact is defined as a change or benefit to the economy, society, culture, public policy or services, health, the environment, or quality of life. Each institution submitted a set of four-page impact case studies. These are predominantly free-form descriptions and evidences of the impact of study. Numerous analyses of these case studies have been conducted, but they have utilised either qualitative methods or primary forms of text searching. These approaches have limitations, including the time required to manually analyse the data and the frequently inferior quality of the answers provided by applying computational analysis to unstructured, context-less free text data. This paper describes a new system to address these problems. At its core is a structured, queryable representation of the case study data. We describe the ontology design used to structure the information and how semantic web related technologies are used to store and query the data. Experiments show that this gives two significant advantages over existing techniques: improved accuracy in question answering and the capability to answer a broader range of questions, by integrating data from external sources. Then we investigate whether machine learning can predict each case study's grade using this structured representation. The results provide accurate predictions for computer science impact case studies.",
        "DOI": "10.1017/dap.2022.21",
        "paper_author": "Zhang J.",
        "affiliation_name": "Newcastle University",
        "affiliation_city": "Newcastle",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60006222",
        "affiliation_state": "Tyne and Wear"
    },
    {
        "paper_title": "Machine learning can improve the development of evidence-based dietary guidelines",
        "publication": "Public Health Nutrition",
        "citied_by": "3",
        "cover_date": "2022-09-27",
        "Abstract": "NA",
        "DOI": "10.1017/S1368980022001392",
        "paper_author": "Bodnar L.M.",
        "affiliation_name": "University of Pittsburgh Graduate School of Public Health",
        "affiliation_city": "Pittsburgh",
        "affiliation_country": "United States",
        "affiliation_id": "60003742",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "Learning to branch with Tree-aware Branching Transformers",
        "publication": "Knowledge-Based Systems",
        "citied_by": "9",
        "cover_date": "2022-09-27",
        "Abstract": "Machine learning techniques have attracted increasing attention in learning Branch-and-Bound (B&B) variable selection policies, but most of the existing methods lack extensions to heterogeneous problems. Though parameterizing search trees has recently shown a promising alternative for heterogeneous scenarios, it remains challenging to maintain good performance when generalizing to instances comparatively harder to solve than those seen during training. To fill this gap, we propose a tree-aware transformer-based branching framework for branching efficiently and effectively. Specifically, the transformer-based branching is conducted, in which the mutual connections between candidate variables can be evaluated by the self-attention mechanism. Then, we novelly encode the empirical data in the search tree, i.e., branching history, with a binary tree representation. In this way, we can fully utilize features exploited from the parameterized B&B search trees and stronger branching policies can be attained thereby. The proposed models are evaluated on multiple benchmark instances and achieve a significant boost on performance, in terms of smaller B&B search trees and lower primal–dual integrals and gaps for harder problems within a given time limit. Ablation studies further validate the effectiveness of our method.",
        "DOI": "10.1016/j.knosys.2022.109455",
        "paper_author": "Lin J.",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60025278",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Privacy-preserving deep learning for electricity consumer characteristics identification",
        "publication": "Frontiers in Energy Research",
        "citied_by": "1",
        "cover_date": "2022-09-26",
        "Abstract": "Deep learning models trained from smart meter data have proven to be effective in predicting socio-demographic characteristics of electricity consumers, which can help retailers provide personalized service to electricity customers. Traditionally, deep learning models are trained in a centralized manner to gather large amounts of data to ensure effectiveness and efficiency. However, gathering smart meter data in plaintext may raise privacy concerns since the data is privately owned by different retailers. This indicates an imminent need for privacy-preserving deep learning. This paper proposes several secure multi-party computation (MPC) protocols that enable deep learning training and inference for electricity consumer characteristics identification while keeping the retailer’s raw data confidential. In our protocols, the retailers secret-share their raw data to three computational servers, which implement deep learning training and inference through lightweight replicated secret sharing techniques. We implement and benchmark multiple neural network models and optimization strategies. Comprehensive experiments are conducted on the Irish Commission for Energy Regulation (CER) dataset to verify that our MPC-based protocols have comparable performance.",
        "DOI": "10.3389/fenrg.2022.992117",
        "paper_author": "Zhang Z.",
        "affiliation_name": "Qingdao University",
        "affiliation_city": "Qingdao",
        "affiliation_country": "China",
        "affiliation_id": "60030434",
        "affiliation_state": "Shandong"
    },
    {
        "paper_title": "Can Mental Illness Lead to Dismissal? From a Causal Machine Learning Perspective",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2022-09-23",
        "Abstract": "Causal inference has been used extensively in health, economics, policy research, and other fields. With the introduction of the Neyman-Rubin framework in 1974, more scholars began to realize that correlation between variables is not equivalent to causation, and therefore, relying too heavily on statistical correlation methods to model can lead to serious theoretical flaws. In this paper, we use data on the work of people with mental illness to analyze whether society treats people with mental illness equally, use propensity score matching (PSM) method to reduce the dimensionality of covariates, and estimate the causal effect of having a mental illness on hiring rates. Our study shows that the covariates can all be well balanced after the implementation of PSM and that employees with mental illness have a 5.8% greater likelihood of leading to dismissal compared to employees in the general population.",
        "DOI": "10.1145/3573942.3573950",
        "paper_author": "Feng Y.",
        "affiliation_name": "Northeastern University",
        "affiliation_city": "Boston",
        "affiliation_country": "United States",
        "affiliation_id": "60028628",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Investigation of independent reinforcement learning algorithms in multi-agent environments",
        "publication": "Frontiers in Artificial Intelligence",
        "citied_by": "5",
        "cover_date": "2022-09-20",
        "Abstract": "Independent reinforcement learning algorithms have no theoretical guarantees for finding the best policy in multi-agent settings. However, in practice, prior works have reported good performance with independent algorithms in some domains and bad performance in others. Moreover, a comprehensive study of the strengths and weaknesses of independent algorithms is lacking in the literature. In this paper, we carry out an empirical comparison of the performance of independent algorithms on seven PettingZoo environments that span the three main categories of multi-agent environments, i.e., cooperative, competitive, and mixed. For the cooperative setting, we show that independent algorithms can perform on par with multi-agent algorithms in fully-observable environments, while adding recurrence improves the learning of independent algorithms in partially-observable environments. In the competitive setting, independent algorithms can perform on par or better than multi-agent algorithms, even in more challenging environments. We also show that agents trained via independent algorithms learn to perform well individually, but fail to learn to cooperate with allies and compete with enemies in mixed environments.",
        "DOI": "10.3389/frai.2022.805823",
        "paper_author": "Lee K.M.",
        "affiliation_name": "University of Waterloo",
        "affiliation_city": "Waterloo",
        "affiliation_country": "Canada",
        "affiliation_id": "60014171",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "The Future of Diagnostic Excellence",
        "publication": "JAMA",
        "citied_by": "1",
        "cover_date": "2022-09-20",
        "Abstract": "NA",
        "DOI": "10.1001/jama.2022.12205",
        "paper_author": "Fineberg H.V.",
        "affiliation_name": "Gordon E. and Betty I. Moore Foundation",
        "affiliation_city": "Palo Alto",
        "affiliation_country": "United States",
        "affiliation_id": "60079649",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Scrutinizing Privacy Policy Compliance of Virtual Personal Assistant Apps",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "20",
        "cover_date": "2022-09-19",
        "Abstract": "A large number of functionality-rich and easily accessible applications have become popular among various virtual personal assistant (VPA) services such as Amazon Alexa. VPA applications (or VPA apps for short) are accompanied by a privacy policy document that informs users of their data handling practices. These documents are usually lengthy and complex for users to comprehend, and developers may intentionally or unintentionally fail to comply with them. In this work, we conduct the first systematic study on the privacy policy compliance issue of VPA apps. We develop Skipper, which targets Amazon Alexa skills. It automatically depicts the skill into the declared privacy profile by analyzing their privacy policy documents with Natural Language Processing (NLP) and machine learning techniques, and derives the behavioral privacy profile of the skill through a black-box testing. We conduct a large-scale analysis on all skills listed on Alexa store, and find that a large number of skills suffer from the privacy policy noncompliance issues.",
        "DOI": "10.1145/3551349.3560416",
        "paper_author": "Xie F.",
        "affiliation_name": "The University of Queensland",
        "affiliation_city": "Brisbane",
        "affiliation_country": "Australia",
        "affiliation_id": "60031004",
        "affiliation_state": "QLD"
    },
    {
        "paper_title": "Using Machine Learning to Enhance Archival Processing of Social Media Archives",
        "publication": "Journal on Computing and Cultural Heritage",
        "citied_by": "1",
        "cover_date": "2022-09-16",
        "Abstract": "This article reports on a study using machine learning to identify incidences and shifting dynamics of hate speech in social media archives. To better cope with the archival processing need for such large-scale and fast evolving archives, we propose the Data-driven and Circulating Archival Processing (DCAP) method. As a proof-of-concept, our study focuses on an English language Twitter archive relating to COVID-19: Tweets were repeatedly scraped between February and June 2020, ingested and aggregated within the COVID-19 Hate Speech Twitter Archive (CHSTA), and analyzed for hate speech using the Generative Adversarial Network–inspired DCAP method. Outcomes suggest that it is possible to use machine learning and data analytics to surface and substantiate trends from CHSTA and similar social media archives that could provide immediately useful knowledge for crisis response, in controversial situations, or for public policy development, as well as for subsequent historical analysis. The approach shows potential for integrating multiple aspects of the archival workflow and supporting automatic iterative redescription and reappraisal activities in ways that make them more accountable and more rapidly responsive to changing societal interests and unfolding developments.",
        "DOI": "10.1145/3547146",
        "paper_author": "Fan L.",
        "affiliation_name": "University of Michigan, Ann Arbor",
        "affiliation_city": "Ann Arbor",
        "affiliation_country": "United States",
        "affiliation_id": "60025778",
        "affiliation_state": "MI"
    },
    {
        "paper_title": "Identifying kidney trade networks using web scraping data",
        "publication": "BMJ Global Health",
        "citied_by": "2",
        "cover_date": "2022-09-16",
        "Abstract": "Kidney trade has been on the rise despite the domestic and international law enforcement aiming to protect the vulnerable population from potential exploitation. Regional hubs are emerging in several parts of the world including South Asia, Central America, the Middle East and East Asia. Kidney trade networks reported in these hot spots are often complex systems involving several players such as buyers, sellers and surgery countries operating across international borders so that they can bypass domestic laws in sellers and buyers' countries. The exact patterns of the country networks are, however, largely unknown due to the lack of a systematic approach to collect the data. Most of the kidney trade information is currently available in the form of case studies, court materials and news articles or reports, and no comprehensive database exists at this time. The present study thus explored online newspaper scraping to systematically collect 10 419 news articles from 24 major English newspapers in South Asia (January 2016 to May 2019) and build transnational kidney trade networks at the country level. Additionally, this study applied text mining techniques to extract words from each news article and developed machine learning algorithms to identify kidney trade and non-kidney trade news articles. Our findings suggest that online newspaper scraping coupled with the machine learning method is a promising approach to compile such data, especially in the dire shortage of empirical data.",
        "DOI": "10.1136/bmjgh-2022-009803",
        "paper_author": "Li M.H.",
        "affiliation_name": "Mason Square",
        "affiliation_city": "Arlington",
        "affiliation_country": "United States",
        "affiliation_id": "60023279",
        "affiliation_state": "VA"
    },
    {
        "paper_title": "The \"water machine\" of Bengal",
        "publication": "Science",
        "citied_by": "2",
        "cover_date": "2022-09-16",
        "Abstract": "NA",
        "DOI": "10.1126/science.ade0393",
        "paper_author": "Mukherji A.",
        "affiliation_name": "International Water Management Institute",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India",
        "affiliation_id": "100630860",
        "affiliation_state": "DL"
    },
    {
        "paper_title": "Assessment of long-term particulate nitrate air pollution and its health risk in China",
        "publication": "iScience",
        "citied_by": "11",
        "cover_date": "2022-09-16",
        "Abstract": "Air pollution is a major environmental and public health challenge in China and the Chinese government has implemented a series of strict air quality policies. However, particulate nitrate (NO3−) concentration remains high or even increases at monitoring sites despite the total PM2.5 concentration has decreased. Unfortunately, it has been difficult to estimate NO3− concentration across China due to the lack of a PM2.5 speciation monitoring network. Here, we use a machine learning model incorporating ground measurements and satellite data to characterize the spatiotemporal patterns of NO3−, thereby understanding the disease burden associated with long-term NO3− exposure in China. Our results show that existing air pollution control policies are effective, but increased NO3− of traffic emissions offset reduced NO3− of industrial emissions. In 2018, the national mean mortality burden attributable to NO3− was as high as 0.68 million, indicating that targeted regulations are needed to control NO3− pollution in China.",
        "DOI": "10.1016/j.isci.2022.104899",
        "paper_author": "Hang Y.",
        "affiliation_name": "Rollins School of Public Health",
        "affiliation_city": "Atlanta",
        "affiliation_country": "United States",
        "affiliation_id": "60025315",
        "affiliation_state": "GA"
    },
    {
        "paper_title": "The future of cyber-enabled influence operations: Emergent technologies, disinformation, and the destruction of democracy",
        "publication": "The Great Power Competition",
        "citied_by": "1",
        "cover_date": "2022-09-15",
        "Abstract": "Nation-states have been embracing online influence campaigns through disinformation at breakneck speeds. Countries such as China and Russia have completely revamped their military doctrine to information-first platforms [1, 2] (Mattis, Peter. (2018). China's Three Warfares in Perspective. War on the Rocks. Special Series: Ministry of Truth. https://warontherocks.com/2018/01/chinas-three-warfares-perspective/, Cunningham, C. (2020). A Russian Federation Information Warfare Primer. Then Henry M. Jackson School of International Studies. Washington University. https://jsis.washington.edu/news/a-russian-federation-information-warfare-primer/.) to compete with the United States and the West. The Chinese principle of \"Three Warfares\" and Russian Hybrid Warfare have been used and tested across the spectrum of operations ranging from competition to active conflict. With the COVID19 pandemic limiting most means of face-to-face interpersonal communication, many other nations have transitioned to online tools to influence audiences both domestically and abroad [3] (Strick, B. (2020). COVID-19 Disinformation: Attempted Influence in Disguise. Australian Strategic Policy Institute. International Cyber Policy Center. https://www.aspi.org.au/report/covid-19-disinformation.) to create favorable environments for their geopolitical goals and national objectives. This chapter focuses on the landscape that allows nations like China and Russia to attack democratic institutions and discourse within the United States, the strategies and tactics employed in these campaigns, and the emergent technologies that will enable these nations to gain an advantage with key populations within their spheres of influence or to create a disadvantage to their competitors within their spheres of influence. Advancements in machine learning through generative adversarial networks [4] (Creswell, A; White, T; Dumoulin, V; Arulkumaran, K; Sengupta, B; Bharath, A. (2017) Generative Adversarial Networks: An Overview. IEE-SPM. April 2017. https://arxiv.org/pdf/1710.07035.pdf.) that create deepfakes [5] (Whittaker, L; Letheren, K; Mulcahy, R. (2021). The Rise of Deepfakes: A Conceptual Framework and Research Agenda for Marketing. https://journals.sagepub.com/doi/abs/10.1177/1839334921999479.) and attention-based transformers [6] (https://arxiv.org/abs/1810.04805.) (Devlin et al., 2018) that create realistic speech patterns and interaction will continue to plague online discussion and information spread, attempting to cause further partisan divisions and decline of U.S. stature on the world stage and democracy as a whole.",
        "DOI": "10.1007/978-3-031-04586-8_10",
        "paper_author": "Littell J.",
        "affiliation_name": "The U.S. Military Academy at West Point",
        "affiliation_city": "West Point",
        "affiliation_country": "United States",
        "affiliation_id": "60027596",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Exploring the impacts of street-level greenspace on stroke and cardiovascular diseases in Chinese adults",
        "publication": "Ecotoxicology and Environmental Safety",
        "citied_by": "18",
        "cover_date": "2022-09-15",
        "Abstract": "In recent years, cardiovascular diseases (CVDs) have become the primary cause of death in the world. Existing studies have found that greenspace is important for the prevention of CVDs and stroke. However, since they only focus on large green infrastructure (e.g., urban parks) or the general greenspace (usually being evaluated through normalized difference vegetation index), little information exists regarding the association between street-level greenspace and CVDs (stroke). In this study, the CVDs and stroke data of participants were retrieved from the 33 Chinese Community Health Study. We measured participants’ exposure to street-level greenspace exposure using street view images and machine learning technique. Multilevel logistic regressions were applied. While controlling for confounders, we found that higher level of street-level greenspace exposure was associated with lower CVDs prevalence. However, street-level greenspace exposure was associated with stroke prevalence only for females. The associations were stronger among females, younger adults, participants with educational attainment above high school, physically active participants and participants who were not overweight. None of the mediators (air pollution, physical exercise, and BMI) can explain the associations between street-level greenspace exposure and CVDs (stroke) prevalence. Our findings suggest that street-level vegetation should be increased to cope with the rapid growth of the CVDs burdens. Also, the differences between the effect of street-level trees and grasses should be noted before formulating specific urban planning policies.",
        "DOI": "10.1016/j.ecoenv.2022.113974",
        "paper_author": "Wang R.",
        "affiliation_name": "Queen's University Belfast",
        "affiliation_city": "Belfast",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60029738",
        "affiliation_state": "Northern Ireland"
    },
    {
        "paper_title": "Generative adversarial interactive imitation learning for path following of autonomous underwater vehicle",
        "publication": "Ocean Engineering",
        "citied_by": "13",
        "cover_date": "2022-09-15",
        "Abstract": "Autonomous underwater vehicle (AUV) is playing a more and more important role in marine scientific research and resource exploration due to its flexibility. Recently, deep reinforcement learning (DRL) has been used to improve the autonomy of AUV. However, it is very time-consuming and even unpractical to define efficient reward functions for DRL to learn control policies in various tasks. In this paper, we implemented the generative adversarial imitation learning (GAIL) algorithm learning from demonstrated trajectories and proposed GA2IL learning from demonstrations and additional human rewards for AUV path following. We evaluated GAIL and our GA2IL method in a straight line following task and a sinusoids curve following task on the Gazebo platform extended to simulated underwater environments with AUV simulator of our lab. Both methods were compared to PPO—a classic traditional deep reinforcement learning from a predefined reward function, and a well-tuned PID controller. In addition, to evaluate the generalization of GAIL and our GA2IL method, we tested the trained control policies of the previous two tasks via GAIL and GA2IL in a new complex comb scan following task and a different sinusoids curve following task respectively. Our simulation results show AUV path following with GA2IL and GAIL can obtain a performance at a similar level to PPO and PID controller in both tasks. Moreover, GA2IL can generalize as well as PPO, adapting better to complex and different tasks than traditional PID controller.",
        "DOI": "10.1016/j.oceaneng.2022.111971",
        "paper_author": "Jiang D.",
        "affiliation_name": "Ocean University of China",
        "affiliation_city": "Qingdao",
        "affiliation_country": "China",
        "affiliation_id": "60022422",
        "affiliation_state": "Shandong"
    },
    {
        "paper_title": "Predicting waste management system performance from city and country attributes",
        "publication": "Journal of Cleaner Production",
        "citied_by": "23",
        "cover_date": "2022-09-15",
        "Abstract": "Supporting good waste management practices is crucial for the sustainable development of cities. Transforming the practices of cities is a complex problem that requires understanding their societal, technological, and economic processes. Machine learning is a branch of artificial intelligence techniques that generates models from patterns in data. Many of these models are difficult to present to city planners because of poor transparency. To provide insights for policymaking, interpretable machine learning models between city attributes and waste management performance are needed. Country attributes have a top-down influence on the sustainability of cities. Their inclusion provides deeper insights in addition to city-wide scope analysis. This work develops a rule-based machine learning model in the impact of city and country attributes on waste management. Rough set-based machine learning is used to generate models consisting of if-then rules with data from 100 cities in 41 countries. The results identify local governance, employment, and technological research as core attributes that influence sustainable waste management. The rough set-based machine learning models attained binary classification accuracies of 89%–91%. The implications on waste management and Circular Economy transition policies are discussed in this study.",
        "DOI": "10.1016/j.jclepro.2022.132951",
        "paper_author": "Gue I.H.V.",
        "affiliation_name": "De La Salle University",
        "affiliation_city": "Manila",
        "affiliation_country": "Philippines",
        "affiliation_id": "60071464",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Application of data-mining technique and hydro-chemical data for evaluating vulnerability of groundwater in Indo-Gangetic Plain",
        "publication": "Journal of Environmental Management",
        "citied_by": "23",
        "cover_date": "2022-09-15",
        "Abstract": "Vulnerability of groundwater is critical for the sustainable development of groundwater resources, especially in freshwater-limited coastal Indo-Gangetic plains. Here, we intend to develop an integrated novel approach for delineating groundwater vulnerability using hydro-chemical analysis and data-mining methods, i.e., Decision Tree (DT) and K-Nearest Neighbor (KNN) via k-fold cross-validation (CV) technique. A total of 110 of groundwater samples were obtained during the dry and wet seasons to generate an inventory map. Four K-fold CV approach was used to delineate the vulnerable region from sixteen vulnerability causal factors. The statistical error metrics i.e., receiver operating characteristic-area under the curve (AUC-ROC) and other advanced metrices were adopted to validate model outcomes. The results demonstrated the excellent ability of the proposed models to recognize the vulnerability of groundwater zones in the Indo-Gangetic plain. The DT model revealed higher performance (AUC = 0.97) followed by KNN model (AUC = 0.95). The north-central and north-eastern parts are more vulnerable due to high salinity, Nitrate (NO3−), Fluoride (F−) and Arsenic (As) concentrations. Policy-makers and groundwater managers can utilize the proposed integrated novel approach and the outcome of groundwater vulnerability maps to attain sustainable groundwater development and safeguard human-induced activities at the regional level.",
        "DOI": "10.1016/j.jenvman.2022.115582",
        "paper_author": "Chandra Pal S.",
        "affiliation_name": "The University of Burdwan",
        "affiliation_city": "Bardhaman",
        "affiliation_country": "India",
        "affiliation_id": "60030482",
        "affiliation_state": "WB"
    },
    {
        "paper_title": "Building a top-down method based on machine learning for evaluating energy intensity at a fine scale",
        "publication": "Energy",
        "citied_by": "9",
        "cover_date": "2022-09-15",
        "Abstract": "Energy intensity is an important representative of energy efficiency. Currently, most countries lack fine-scale energy intensity data, taking China as an example, it only published provincial energy intensity data. However, the published large-scale energy intensity cannot support the formulation of local policies. What's more, the research work about the evaluation of fine-scale energy intensity is rare. To solve this problem, a “top-down” method based on machine learning is proposed to evaluate the fine-scale energy intensity. Appropriate features were extracted from multi-source satellite data, then the performances of multiple machine learning models were compared. It is found that deep neural network reaches the highest level among these models. Therefore, it was selected to estimate city-scale energy intensity from the year of 2001–2017. It turns out that the energy efficiency of southeast cities is higher than that of northwest cities in China, and most cities are developing towards the direction of improving energy efficiency. Among all cities, the central ones are the fastest to improve energy efficiency. However, the energy efficiency of a few cities is found to reduce during this period. The proposed method can also be used in other countries to help governments save energy and reduce emissions.",
        "DOI": "10.1016/j.energy.2022.124505",
        "paper_author": "Guo J.",
        "affiliation_name": "Anhui Normal University",
        "affiliation_city": "Wuhu",
        "affiliation_country": "China",
        "affiliation_id": "60009559",
        "affiliation_state": "Anhui"
    },
    {
        "paper_title": "A survey on AI and decision support systems in psychiatry – Uncovering a dilemma",
        "publication": "Expert Systems with Applications",
        "citied_by": "19",
        "cover_date": "2022-09-15",
        "Abstract": "Every year, healthcare specialists collect more and more data about patients but struggle to use it to optimize disease prevention, diagnosis, or treatment processes. While a manual use of this medical data is virtually impossible considering the vast growth rate, automation with artificial intelligence (AI) and digital decision support systems (DDSSs) has still not yielded any large-scale success in healthcare. We aim to investigate possible obstacles, the trustworthiness based on potential biases, and the adoption of new technology by AI and DDSSs in psychiatry based on a systematic literature review. We screened 520 papers about AI or DDSSs in psychiatry. We added results from a literature screening of 65 articles about AI or DDSSs for post-traumatic stress disorder as one specific psychiatric disease to our research, given that literature possibly deviates from general decision support systems for psychiatry. Out of 80 articles, we extract algorithms, data collection method and sample size of the used training data, and testing process including accuracy metrics. The results show that sample sizes are small (median of 151,5), a focus on algorithm development without real-world interaction, and methodological shortcomings when it comes to the evaluation of DDSSs. Our survey concludes that DDSSs in psychiatry are not ready for the often-promised “AI revolution in healthcare”.",
        "DOI": "10.1016/j.eswa.2022.117464",
        "paper_author": "Bertl M.",
        "affiliation_name": "Tallinna Tehnikaülikool",
        "affiliation_city": "Tallinn",
        "affiliation_country": "Estonia",
        "affiliation_id": "60068861",
        "affiliation_state": "Harjumaa"
    },
    {
        "paper_title": "Qauxi: Cooperative multi-agent reinforcement learning with knowledge transferred from auxiliary task",
        "publication": "Neurocomputing",
        "citied_by": "5",
        "cover_date": "2022-09-14",
        "Abstract": "Deep multi-agent reinforcement learning (MARL) can efficiently learn decentralized policies for real-world applications. However, current MARL methods suffer from the difficulty of transferring knowledge from already learned tasks to improve its exploration. In this paper, we propose a novel MARL method called Qauxi, which forms coordinated exploration scheme to improve the traditional MARL algorithms by reusing the meta-experience transferred from auxiliary task. We also use the weighting function to weight the importance of the joint action in monotonic loss function in order to focus on more important joint actions and thus avoid yielding suboptimal policies. Furthermore, we prove the convergence of Qauxi based on contraction mapping theorem. Qauxi is evaluated on the widely adopted StarCraft benchmarks (SMAC) across easy, hard, and super hard scenarios. Experimental results show that the proposed method outperforms the state-of-the-art MARL methods by a large margin in the most challenging super hard scenarios.",
        "DOI": "10.1016/j.neucom.2022.06.091",
        "paper_author": "Liang W.",
        "affiliation_name": "National University of Defense Technology China",
        "affiliation_city": "Changsha",
        "affiliation_country": "China",
        "affiliation_id": "60024350",
        "affiliation_state": "Hunan"
    },
    {
        "paper_title": "Reasoning and interaction for social artificial intelligence",
        "publication": "AI Communications",
        "citied_by": "4",
        "cover_date": "2022-09-12",
        "Abstract": "Current work on multi-agent systems at King's College London is extensive, though largely based in two research groups within the Department of Informatics: the Distributed Artificial Intelligence (DAI) thematic group and the Reasoning & Planning (RAP) thematic group. DAI combines AI expertise with political and economic theories and data, to explore social and technological contexts of interacting intelligent entities. It develops computational models for analysing social, political and economic phenomena to improve the effectiveness and fairness of policies and regulations, and combines intelligent agent systems, software engineering, norms, trust and reputation, agent-based simulation, communication and provenance of data, knowledge engineering, crowd computing and semantic technologies, and algorithmic game theory and computational social choice, to address problems arising in autonomous systems, financial markets, privacy and security, urban living and health. RAP conducts research in symbolic models for reasoning involving argumentation, knowledge representation, planning, and other related areas, including development of logical models of argumentation-based reasoning and decision-making, and their usage for explainable AI and integration of machine and human reasoning, as well as combining planning and argumentation methodologies for strategic argumentation.",
        "DOI": "10.3233/AIC-220133",
        "paper_author": "Black E.",
        "affiliation_name": "King's College London",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60011520",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "REVEAL 2022: Reinforcement Learning-Based Recommender Systems at Scale",
        "publication": "RecSys 2022 - Proceedings of the 16th ACM Conference on Recommender Systems",
        "citied_by": "1",
        "cover_date": "2022-09-12",
        "Abstract": "Recommendation systems are increasingly modelled as a sequential decision making process, where the system decides which items to recommend to a given user. Each decision to recommend an item or slate of items has a significant impact on immediate and future user responses, long-term satisfaction or engagement with the system, and possibly valuable exposure for the item provider. The REVEAL workshop will focus on how to optimise this multi-step decision-making process, where a stream of interactions occurs between the user and the system. Deriving reward signals from these interactions, and creating a scalable, performant, and maintainable recommendation model to use for inference is a key challenge for machine learning teams, both in industry and academia. We will discuss the following challenges at the workshop: How can recommendation system models take into account the delayed effects of each recommendation? What are the right ways to reason and plan for longer-term user satisfaction? How can we leverage techniques such as Reinforcement Learning (RL) at scale?",
        "DOI": "10.1145/3523227.3547418",
        "paper_author": "Liaw R.",
        "affiliation_name": "Anyscale",
        "affiliation_city": "Berkeley",
        "affiliation_country": "United States",
        "affiliation_id": "127401459",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Study on China coal Price forecasting based on CEEMDAN-GWO-CatBoost hybrid forecasting model under Carbon Neutral Target",
        "publication": "Frontiers in Environmental Science",
        "citied_by": "13",
        "cover_date": "2022-09-12",
        "Abstract": "The emission peak and carbon neutrality targets pose a great challenge to carbon emission reduction in the coal industry, and the coal industry will face an all-around deep adjustment. The forecast of coal price is crucial for reducing carbon emissions in the coal industry in an orderly manner under the premise of ensuring national energy security. The volatility and instability of coal prices are a result of multiple influencing factors, making it very difficult to make accurate predictions of coal price changes. We propose in this paper an innovative hybrid forecasting method (CEEMDAN-GWO-CatBoost) for forecasting coal price indexes by combining machine learning models, feature selections, data decomposition, and model interpretation. By combining high forecasting accuracy with good interpretability, this method fills a gap in the field of coal price forecasting. Initially, we examine the factors that influence coal prices from five angles: Supply, demand, macroeconomic factors, freight costs, and substitutes; and we employ Spearman correlation analysis to reduce the complexity of the attribute set and devise a coal price forecasting index system. Secondly, the CEEMDAN method is used to decompose the raw coal price index data into seven intrinsic modal functions and one residual term in order to weaken the volatility of the data caused by complex factors. Next, the CatBoost model hyperparameters are optimized using the Grey Wolf Optimizer algorithm, while the coal price data is fed into the combined forecasting model. Lastly, the SHAP interpretation method is introduced for studying the important indicators affecting coal prices. The experimental results show that the combined CEEMDAN-GWO-CatBoost forecasting model proposed in this paper has significantly better forecasting performance than other comparative models, and the SHAP method employed in this study identifies the macroeconomic environment, freight costs, and coal import volume as significant factors affecting coal prices. As part of the contribution of this paper, specific recommendations are made to the government regarding the formulation of a regulatory policy for the coal industry in the context of carbon neutrality based on the findings of this research.",
        "DOI": "10.3389/fenvs.2022.1014021",
        "paper_author": "Wang X.",
        "affiliation_name": "Zhengzhou University of Aeronautics",
        "affiliation_city": "Zhengzhou",
        "affiliation_country": "China",
        "affiliation_id": "60073735",
        "affiliation_state": "Henan"
    },
    {
        "paper_title": "A machine learning algorithm to analyse the effects of vaccination on COVID-19 mortality",
        "publication": "Epidemiology and Infection",
        "citied_by": "43",
        "cover_date": "2022-09-12",
        "Abstract": "The coronavirus disease 2019 (COVID-19), with new variants, continues to be a constant pandemic threat that is generating socio-economic and health issues in manifold countries. The principal goal of this study is to develop a machine learning experiment to assess the effects of vaccination on the fatality rate of the COVID-19 pandemic. Data from 192 countries are analysed to explain the phenomena under study. This new algorithm selected two targets: the number of deaths and the fatality rate. Results suggest that, based on the respective vaccination plan, the turnout in the participation in the vaccination campaign, and the doses administered, countries under study suddenly have a reduction in the fatality rate of COVID-19 precisely at the point where the cut effect is generated in the neural network. This result is significant for the international scientific community. It would demonstrate the effective impact of the vaccination campaign on the fatality rate of COVID-19, whatever the country considered. In fact, once the vaccination has started (for vaccines that require a booster, we refer to at least the first dose), the antibody response of people seems to prevent the probability of death related to COVID-19. In short, at a certain point, the fatality rate collapses with increasing doses administered. All these results here can help decisions of policymakers to prepare optimal strategies, based on effective vaccination plans, to lessen the negative effects of the COVID-19 pandemic crisis in socioeconomic and health systems.",
        "DOI": "10.1017/S0950268822001418",
        "paper_author": "Magazzino C.",
        "affiliation_name": "Università degli Studi Roma Tre",
        "affiliation_city": "Rome",
        "affiliation_country": "Italy",
        "affiliation_id": "60012630",
        "affiliation_state": "RM"
    },
    {
        "paper_title": "Probabilistic forecasting of remotely sensed cropland vegetation health and its relevance for food security",
        "publication": "Science of the Total Environment",
        "citied_by": "13",
        "cover_date": "2022-09-10",
        "Abstract": "In a world where climate change, population growth, and global diseases threaten economic access to food, policies and contingency plans can strongly benefit from reliable forecasts of agricultural vegetation health. To inform decisions, it is also crucial to quantify the forecasting uncertainty and prove its relevance for food security. Yet, in previous studies both these aspects have been largely overlooked. This paper develops a methodology to anticipate the agricultural Vegetation Health Index (VHI) while making the underlying prediction uncertainty explicit. To achieve this aim, a probabilistic machine learning framework modelling weather and climate determinants is introduced and implemented through Quantile Random Forests. In a second step, a statistical link between VHI forecasts and monthly food price variations is established. As a pilot implementation, the framework is applied to nine countries of South-East Asia (SEA) with consideration of national monthly rice prices. Model benchmarks show satisfactory accuracy metrics, suggesting that the probabilistic VHI predictions can provide decision-makers with reliable information about future cropland health and its impact on food price variation weeks or even months ahead, albeit with increasing uncertainty as the forecasting horizon grows. These results - ultimately allowing to anticipate the impact of weather shocks on household food expenditure - contribute to advancing the multidisciplinary literature linking vegetation health, probabilistic forecasting models, and food security policy.",
        "DOI": "10.1016/j.scitotenv.2022.156157",
        "paper_author": "Hammad A.T.",
        "affiliation_name": "Decatab Pte. Ltd.",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore",
        "affiliation_id": "125579300",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Deep reinforcement learning for personalized treatment recommendation",
        "publication": "Statistics in Medicine",
        "citied_by": "28",
        "cover_date": "2022-09-10",
        "Abstract": "In precision medicine, the ultimate goal is to recommend the most effective treatment to an individual patient based on patient-specific molecular and clinical profiles, possibly high-dimensional. To advance cancer treatment, large-scale screenings of cancer cell lines against chemical compounds have been performed to help better understand the relationship between genomic features and drug response; existing machine learning approaches use exclusively supervised learning, including penalized regression and recommender systems. However, it would be more efficient to apply reinforcement learning to sequentially learn as data accrue, including selecting the most promising therapy for a patient given individual molecular and clinical features and then collecting and learning from the corresponding data. In this article, we propose a novel personalized ranking system called Proximal Policy Optimization Ranking (PPORank), which ranks the drugs based on their predicted effects per cell line (or patient) in the framework of deep reinforcement learning (DRL). Modeled as a Markov decision process, the proposed method learns to recommend the most suitable drugs sequentially and continuously over time. As a proof-of-concept, we conduct experiments on two large-scale cancer cell line data sets in addition to simulated data. The results demonstrate that the proposed DRL-based PPORank outperforms the state-of-the-art competitors based on supervised learning. Taken together, we conclude that novel methods in the framework of DRL have great potential for precision medicine and should be further studied.",
        "DOI": "10.1002/sim.9491",
        "paper_author": "Liu M.",
        "affiliation_name": "University of Minnesota Twin Cities",
        "affiliation_city": "Minneapolis",
        "affiliation_country": "United States",
        "affiliation_id": "60029445",
        "affiliation_state": "MN"
    },
    {
        "paper_title": "Nonlinear forces in urban thermal environment using Bayesian optimization-based ensemble learning",
        "publication": "Science of the Total Environment",
        "citied_by": "36",
        "cover_date": "2022-09-10",
        "Abstract": "Urbanization witnessed unprecedented development globally, which causes citizens and urban temperature to become increasingly intertwined. Although researchers were interested in the field, most studies focused on holistic linear links between the characteristics of the urban built-up environment and temperature. The study used Bayesian optimization ensemble learning and Shapley value to decouple the urban thermal environment by Landsat satellite data. This work's novelties reveal the specific driving effect of different value ranges of urban features in the overall process on the urban thermal environment and advancing an optimum observation buffer zone of the urban surface temperature. The study's results were only for daytime and Beijing scope. The following are the main findings: (1) The 2 km observation buffer zone is best to analyze the urban thermal environment for this dataset. (2) The ecological environment factors have a more significant effect on the urban temperature than the urban morphology factors. (3) In summer, when the vegetation coverage exceeds 58.1%, every 10% increase could reduce the temperature by 0.84 °C. In contrast to summer, when vegetation coverage exceeds 64.7% and 73.2%, respectively, in spring and fall, there will be a significant marginal utility. (4) The effect of the building height has seasonal variations. It has the greatest cooling effect in the spring when the height is between 18 m and 75 m, and the daytime surface temperature at the time of Landsat overpass will drop by 1.25 °C. These findings will aid in understanding how building construction influences urban surface temperature and provide statistical support for planners.",
        "DOI": "10.1016/j.scitotenv.2022.156348",
        "paper_author": "Wu Z.",
        "affiliation_name": "Tongji University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60073652",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Carbon neutrality cognition, environmental value, and consumption preference of low-carbon products",
        "publication": "Frontiers in Environmental Science",
        "citied_by": "8",
        "cover_date": "2022-09-08",
        "Abstract": "It is now the mainstream scientific consensus that carbon emissions cause global climate change. Achieving the goal of China’s carbon neutrality is essential for environmental protection and economic sustainable development worldwide. In the above context, this paper aims to explore the carbon neutrality cognition, environmental value, and consumption preference for low-carbon products from the perspective of consumption end. Thus, we built and checked a new conceptual model of consumers’ carbon neutrality cognition and the consumption preference for low-carbon products. The TF-IDF algorithm in machine learning was used to confirm the dimensions of carbon neutrality cognition based on text data collected from an academic database CNKI. Then, we used data from a social investigation (N = 405) to test hypotheses and models using bootstrapping and independent sample t-tests. The results showed that altruistic (β = 0.168, 95% CI: [−0.54514, 0.8819]) and egoistic values (β = −0.066, 95% confidence interval [CI]: [−0.6361, 0.6772]) mediated the impact of carbon neutrality cognition on the consumption of low-carbon products, whereas the egoistic value did not (β = −0.066, 95% CI: [−0.6361, 0.6772]). Additionally, based on the characteristics of current Chinese consumers and the market, we argue for two boundary factors: face consciousness and carbon footprint label. The moderation of face consciousness (Mhigh = 5.395 vs. Mlow = 3.312) and carbon footprint label (Mwith = 6.394 vs. Mwithout = 5.432) were revealed. The empirical results support our conceptual model, and our findings provide insights to policymakers and enterprises regarding people’s carbon neutrality cognition, which will allow them to develop more appropriate policies and sustainable development strategies.",
        "DOI": "10.3389/fenvs.2022.979783",
        "paper_author": "Li B.",
        "affiliation_name": "Liaoning Technical University",
        "affiliation_city": "Fuxin",
        "affiliation_country": "China",
        "affiliation_id": "60032758",
        "affiliation_state": "Liaoning"
    },
    {
        "paper_title": "Development and training of a machine learning algorithm to identify patients at risk for recurrence following an arthroscopic Bankart repair (CLEARER): protocol for a retrospective, multicentre, cohort study",
        "publication": "BMJ Open",
        "citied_by": "4",
        "cover_date": "2022-09-08",
        "Abstract": "Introduction Shoulder instability is a common injury, with a reported incidence of 23.9 per 100 000 person-years. There is still an ongoing debate on the most effective treatment strategy. Non-operative treatment has recurrence rates of up to 60%, whereas operative treatments such as the Bankart repair and bone block procedures show lower recurrence rates (16% and 2%, respectively) but higher complication rates (<2% and up to 30%, respectively). Methods to determine risk of recurrence have been developed; however, patient-specific decision-making tools are still lacking. Artificial intelligence and machine learning algorithms use self-learning complex models that can be used to make patient-specific decision-making tools. The aim of the current study is to develop and train a machine learning algorithm to create a prediction model to be used in clinical practice - as an online prediction tool - to estimate recurrence rates following a Bankart repair. Methods and analysis This is a multicentre retrospective cohort study. Patients with traumatic anterior shoulder dislocations that were treated with an arthroscopic Bankart repair without remplissage will be included. This study includes two parts. Part 1, collecting all potential factors influencing the recurrence rate following an arthroscopic Bankart repair in patients using multicentre data, aiming to include data from >1000 patients worldwide. Part 2, the multicentre data will be re-evaluated (and where applicable complemented) using machine learning algorithms to predict outcomes. Recurrence will be the primary outcome measure. Ethics and dissemination For safe multicentre data exchange and analysis, our Machine Learning Consortium adhered to the WHO regulation Policy on Use and Sharing of Data Collected by WHO in Member States Outside the Context of Public Health Emergencies'. The study results will be disseminated through publication in a peer-reviewed journal. No Institutional Review Board is required for this study.",
        "DOI": "10.1136/bmjopen-2021-055346",
        "paper_author": "Van Spanning S.H.",
        "affiliation_name": "OLVG",
        "affiliation_city": "Amsterdam",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60022969",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Measuring the Uncertainty of Environmental Good Preferences with Bayesian Deep Learning",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2022-09-07",
        "Abstract": "Due to climate change and resulting natural disasters, there has been a growing interest in measuring the value of social goods to our society, like environmental conservation. Traditionally, the stated preference, such as contingent valuation, captures an economics-perspective on the value of environmental goods through the willingness to pay (WTP) paradigm. Where the economics theory to estimate the WTP using machine learning is the random utility model. However, the estimation of WTP depends on rather simple preference assumptions based on a linear functional form. These models are therefore unable to capture the complex uncertainty in the human decision-making process. Further, contingent valuation only uses the mean or median estimation of WTP. Yet it has been recognized that other quantiles of the WTP would be valuable to ensure the provision of social goods. In this work, we propose to leverage the Bayesian Deep Learning (BDL) models to capture the uncertainty in stated preference estimation. We focus on the probability of paying for an environmental good and the conditional distribution of WTP. The Bayesian deep learning model connects with the economics theory of the random utility model through the stochastic component on the individual preferences. For testing our proposed model, we work with both synthetic and real world data. The results on synthetic data suggest the BDL can capture the uncertainty consistently with different distribution of WTP. For the real world data, a forest conservation contingent valuation survey, we observed a high variability in the distribution of the WTP, suggesting high uncertainty in the individual preferences for social goods. Our research can be used to inform environmental policy, including the preservation of natural resources and other social good.",
        "DOI": "10.1145/3524458.3547250",
        "paper_author": "Flores R.",
        "affiliation_name": "Worcester Polytechnic Institute",
        "affiliation_city": "Worcester",
        "affiliation_country": "United States",
        "affiliation_id": "60011410",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Feasibility study of personalized speed adaptation method based on mental state for teleoperated robots",
        "publication": "Frontiers in Neuroscience",
        "citied_by": "2",
        "cover_date": "2022-09-02",
        "Abstract": "The teleoperated robotic system can support humans to complete tasks in high-risk, high-precision and difficult special environments. Because this kind of special working environment is easy to cause stress, high mental workload, fatigue and other mental states of the operator, which will reduce the quality of operation and even cause safety accidents, so the mental state of the people in this system has received extensive attention. However, the existence of individual differences and mental state diversity is often ignored, so that most of the existing adjustment strategy is out of a match between mental state and adaptive decision, which cannot effectively improve operational quality and safety. Therefore, a personalized speed adaptation (PSA) method based on policy gradient reinforcement learning was proposed in this paper. It can use electroencephalogram and electro-oculogram to accurately perceive the operator’s mental state, and adjust the speed of the robot individually according to the mental state of different operators, in order to perform teleoperation tasks efficiently and safely. The experimental results showed that the PSA method learns the mapping between the mental state and the robot’s speed regulation action by means of rewards and punishments, and can adjust the speed of the robot individually according to the mental state of different operators, thereby improving the operating quality of the system. And the feasibility and superiority of this method were proved. It is worth noting that the PSA method was validated on 6 real subjects rather than a simulation model. To the best of our knowledge, the PSA method is the first implementation of online reinforcement learning control of teleoperated robots involving human subjects.",
        "DOI": "10.3389/fnins.2022.976437",
        "paper_author": "Zhang T.",
        "affiliation_name": "Xi'an Jiaotong University",
        "affiliation_city": "Xi'an",
        "affiliation_country": "China",
        "affiliation_id": "60018308",
        "affiliation_state": "Shaanxi"
    },
    {
        "paper_title": "Open questions and research gaps for monitoring and updating AI-enabled tools in clinical settings",
        "publication": "Frontiers in Digital Health",
        "citied_by": "12",
        "cover_date": "2022-09-02",
        "Abstract": "As the implementation of artificial intelligence (AI)-enabled tools is realized across diverse clinical environments, there is a growing understanding of the need for ongoing monitoring and updating of prediction models. Dataset shift—temporal changes in clinical practice, patient populations, and information systems—is now well-documented as a source of deteriorating model accuracy and a challenge to the sustainability of AI-enabled tools in clinical care. While best practices are well-established for training and validating new models, there has been limited work developing best practices for prospective validation and model maintenance. In this paper, we highlight the need for updating clinical prediction models and discuss open questions regarding this critical aspect of the AI modeling lifecycle in three focus areas: model maintenance policies, performance monitoring perspectives, and model updating strategies. With the increasing adoption of AI-enabled tools, the need for such best practices must be addressed and incorporated into new and existing implementations. This commentary aims to encourage conversation and motivate additional research across clinical and data science stakeholders.",
        "DOI": "10.3389/fdgth.2022.958284",
        "paper_author": "Davis S.E.",
        "affiliation_name": "Vanderbilt University Medical Center",
        "affiliation_city": "Nashville",
        "affiliation_country": "United States",
        "affiliation_id": "60030769",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Methodology for the Collection and Analysis of Real Estate Data Using Alternative Sources: Case Study in Three Medium-Sized Cities of Colombia",
        "publication": "Ingenieria (Colombia)",
        "citied_by": "0",
        "cover_date": "2022-09-01",
        "Abstract": "Context: The Multipurpose Cadastre public policy needs to consolidate real estate information from different sources for analysis, such as offers, transactions, and construction costs, among others. Real estate websites are part of these sources of information, although they have not yet been included in commercial analysis. In light of the above, it is necessary to review a methodology that allows optimal access to these web platforms and facilitates the analysis of the variables provided therein, which are crucial to a property’s commercial value. A study case was carried out in three Colombian cities: Fusagasugá, Manizales, and Villavicencio. Method: The method is implemented in two stages: (i) web scraping, which allows obtaining the information links from real estate web pages and downloading their data, and (ii) analyzing real estate data by developing a workflow that starts with data exploration and cleaning, continues with pre-modeling, and ends by modeling the crucial variables in the determination of real estate value using machine learning techniques. Results: By applying machine learning techniques, it was possible to automate the collection, cleaning, storage, and analysis of real estate data from web platforms, as well as to outline two models (Ridge Regression and Random Forest), which, according to their mean absolute percentage error (0,34 and 0,35, respectively), allow predicting the commercial value of a property while considering internal and external explanatory variables. Conclusions: Obtaining and analyzing real estate data from alternative sources such as web platforms through machine learning techniques contributes significantly to addressing the high information demand of the country’s cadastre. However, it is necessary to expand the supply of this information to rural areas, which have less access and availability to it.",
        "DOI": "10.14483/23448393.17952",
        "paper_author": "Rosso-Mateus A.E.",
        "affiliation_name": "Centro de Investigación y Desarrollo en Información Geográfica-CIAF",
        "affiliation_city": null,
        "affiliation_country": "Colombia",
        "affiliation_id": "131053985",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Hierarchical DDPG for Manipulator Motion Planning in Dynamic Environments",
        "publication": "AI (Switzerland)",
        "citied_by": "3",
        "cover_date": "2022-09-01",
        "Abstract": "In this paper, a hierarchical reinforcement learning (HRL) architecture, namely a “Hierarchical Deep Deterministic Policy Gradient (HDDPG)” has been proposed and studied. A HDDPG utilizes manager and worker formation similar to other HRL structures. However, unlike others, the HDDPG enables sharing an identical environment and state among workers and managers, while a unique reward system is required for each Deep Deterministic Policy Gradient (DDPG) agent. Therefore, the HDDPG allows easy structural expansion with probabilistic action selection of a worker by the manager. Due to its innate structural advantage, the HDDPG has a merit in building a general AI to deal with a complex time-horizon tasks with various conflicting sub-goals. The experimental results demonstrated its usefulness with a manipulator motion planning problem in a dynamic environment, where path planning and collision avoidance conflict each other. The proposed HDDPG is compared with an HAM and a single DDPG for performance evaluation. The result shows that the HDDPG demonstrated more than 40% of reward gain and more than two times the reward improvement rate. Another important feature of the proposed HDDPG is the biased manager training capability. By adding a preference factor to each worker, the manager can be trained to prefer a certain worker to achieve better success rate for a specific objective if needed.",
        "DOI": "10.3390/ai3030037",
        "paper_author": "Um D.",
        "affiliation_name": "Texas A and M University - Corpus Christi",
        "affiliation_city": "Corpus Christi",
        "affiliation_country": "United States",
        "affiliation_id": "60002208",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "Ushering Human Dignity into the Era of Globalized, Human-less Technology",
        "publication": "Business and Professional Ethics Journal",
        "citied_by": "0",
        "cover_date": "2022-09-01",
        "Abstract": "As our work is ever evolving from agrarian to more service-oriented tasks, the rise of machine learning is the advent of an intelligence that contrasts with the natural intelligence exhibited by humans. Many see the emergence of artificial intelligence (AI) as simply another opportunity for business to exploit. Additionally, as coding becomes the new language of the business world, the challenge of using data and analytics to help foster a new generation of human flourishing lessens with organizations solidifying their protocols for the use of AI. As our work changes, it is vital for business to recognize that being a force for good requires policies, procedures, and programs that will respect and promote human dignity at all levels, even amidst the changes brought by AI initiatives. This person-first philosophy needs to be a critical component of any future strategies business utilizes to uphold the well-being of all stakeholders.",
        "DOI": "10.5840/bpej20221227127",
        "paper_author": "Sovak K.",
        "affiliation_name": "University of Mary",
        "affiliation_city": null,
        "affiliation_country": "United States",
        "affiliation_id": "102072758",
        "affiliation_state": "WA"
    },
    {
        "paper_title": "Debiased machine learning of global and local parameters using regularized Riesz representers",
        "publication": "Econometrics Journal",
        "citied_by": "10",
        "cover_date": "2022-09-01",
        "Abstract": "We provide adaptive inference methods, based on ℓ regularization, for regular (semiparametric) and nonregular (nonparametric) linear functionals of the conditional expectation function. Examples of regular functionals include average treatment effects, policy effects, and derivatives. Examples of nonregular functionals include average treatment effects, policy effects, and derivatives conditional on a covariate subvector fixed at a point. We construct a Neyman orthogonal equation for the target parameter that is approximately invariant to small perturbations of the nuisance parameters. To achieve this property, we include the Riesz representer for the functional as an additional nuisance parameter. Our analysis yields weak 'double sparsity robustness': either the approximation to the regression or the approximation to the representer can be 'completely dense' as long as the other is sufficiently 'sparse'. Our main results are nonasymptotic and imply asymptotic uniform validity over large classes of models, translating into honest confidence bands for both global and local parameters.",
        "DOI": "10.1093/ectj/utac002",
        "paper_author": "Chernozhukov V.",
        "affiliation_name": "Massachusetts Institute of Technology",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States",
        "affiliation_id": "60022195",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Retraction notice to'Financial investor information impact based on FPGA and machine learning (Microprocessors and Microsystems (2021) 80, (S0141933120307006), (10.1016/j.micpro.2020.103550))",
        "publication": "Microprocessors and Microsystems",
        "citied_by": "0",
        "cover_date": "2022-09-01",
        "Abstract": "This article has been retracted: please see Elsevier Policy on Article Withdrawal (https://www.elsevier.com/about/our-business/policies/article-withdrawal). This article has been retracted at the request of the first author and the Editor-in-Chief. Following an investigation conducted at her institution, the first author has initially contacted the journal to report that she was not aware of the submission of the article. Furthermore, the corresponding author has denied to the journal that he submitted the article and has also denied that he owned the email address which was used for the submission of the article. Consequently, the Editor-in-Chief no longer has confidence in the integrity of the data and decided to retract the article.",
        "DOI": "10.1016/j.micpro.2022.104594",
        "paper_author": "Wang H.",
        "affiliation_name": "Nanjing University of Finance and Economics",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60014724",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "Predictive Soil Mapping of Key Soil Properties in Western Ghats, India",
        "publication": "Journal of the Indian Society of Soil Science",
        "citied_by": "2",
        "cover_date": "2022-09-01",
        "Abstract": "A study was conducted to map the key soil properties such as pH, organic carbon (OC), cation exchange capacity (CEC), sand, silt, clay and bulk density (BD) in part of Western Ghats, South India using three machine learning algorithms (random forest, cubist and support vector machine). Primary and secondary terrain attributes, vegetation indices and bioclimatic variables were used as environmental variables for prediction of soil properties. Equal-area quadratic splines were fitted to 173 soil profile datasets collected over the study area to estimate the soil properties at six soil depths (0-5, 5-15, 15-30, 30-60, 60-100 and 100-200 cm) as per GlobalSoilMap specifications. The models were calibrated using 80% of the samples (138) and validated using remaining 20% of the samples (35). The accuracy of the performance was assessed based on coefficient of determination (R2), concordance correlation coefficient (CCC), root mean square error (RMSE) and mean error (bias). The random forest model outperformed other two models with high R2 and minimal RMSE for most of the soil properties. The model explained 41, 42, 31 and 36% of variation for surface pH, CEC, OC and BD, respectively. The high resolution (250 m) predicted soil properties aid the policy makers to revert the land degradation process and to preserve soil quality by executing suitable land use policies.",
        "DOI": "10.5958/0974-0228.2022.00032.9",
        "paper_author": "Dharumarajan S.",
        "affiliation_name": "ICAR - National Bureau of Soil Survey and Land Use Planning, Nagpur",
        "affiliation_city": "Nagpur",
        "affiliation_country": "India",
        "affiliation_id": "60114679",
        "affiliation_state": "MH"
    },
    {
        "paper_title": "Variable Impedance-based Human-machine Interaction Method Using Reinforcement Learning for Shared Steering Control of Intelligent Vehicle",
        "publication": "Jixie Gongcheng Xuebao/Journal of Mechanical Engineering",
        "citied_by": "2",
        "cover_date": "2022-09-01",
        "Abstract": "Human-Machine interaction has become one of the focuses of intelligent vehicle design. Aiming at the problem of human-machine shared steering control, a human-machine interaction method with variable impedance based on reinforcement learning is proposed. Firstly, A human-machine interaction framework for shared steering is built based on virtual impedance, which can describe the continuous process of control authority distribution. Secondly, on this basis, a variable impedance-based human-machine shared steering control method is designed, which can dynamically distribute control authority by changing virtual impedance. Thirdly, an impedance tuning strategy based on deep deterministic policy gradient (DDPG) is developed to determine the virtual impedance according to the driver's steering behavior. The driver-in-the-loop experiment shows that, compared with the conventional method, the method proposed can make the automation system yield a certain degree of control authority to the driver according to the driver’s steering behavior, and make the interaction process smooth and easy for the driver to adapt to. The method has a less bad influence on the driver and reduces the driver's steering load, and meanwhile, the automation system can also generate an appropriate control torque to express its intention to the driver, and realize the effective human-machine interaction.",
        "DOI": "10.3901/JME.2022.18.141",
        "paper_author": "Han J.",
        "affiliation_name": "Jilin University",
        "affiliation_city": "Changchun",
        "affiliation_country": "China",
        "affiliation_id": "60007711",
        "affiliation_state": "Jilin"
    },
    {
        "paper_title": "Exploring the nonlinear impact of air pollution on housing prices: A machine learning approach",
        "publication": "Economics of Transportation",
        "citied_by": "17",
        "cover_date": "2022-09-01",
        "Abstract": "Air pollution has profoundly impacted residents’ lifestyles as well as their willingness to pay for real estate. Exploring the relationship between air pollution and housing prices has become increasingly prominent. Current research on housing prices mainly uses the hedonic pricing model and the spatial econometric model, which are both linear methods. However, it is difficult to use these methods to model the nonlinear relationship between housing price and its determinants. In addition, most of the existing studies neglect the effects of multiple pollutants on housing prices. To fill these gaps, this study uses a machine learning approach, the gradient boosting decision tree (GBDT) model to analyze the nonlinear impacts of air pollution and the built environment on housing prices in Shanghai. The experimental results show that the GBDT can better fit the nonlinear relationship between housing prices and various explanatory variables compared with traditional linear models. Furthermore, the relative importance rankings of the built environment and air pollution variables are analyzed based on the GBDT model. It indicates that built environment variables contribute 97.21% of the influences on housing prices, whereas the contribution of air pollution variables is 2.79%. Although the impact of air pollution is relatively small, the marginal willingness of residents to pay for clean air is significant. With an improvement of 1 μg/m3 in the average concentrations of PM2.5 and NO2, the average housing price increases by 155.93 Yuan/m2 and 278.03 Yuan/m2, respectively. Therefore, this study can improve our understanding of the nonlinear impact of air pollution on housing prices and provide a basis for formulating and revising policies related to housing prices.",
        "DOI": "10.1016/j.ecotra.2022.100272",
        "paper_author": "Zou G.",
        "affiliation_name": "Key Laboratory of Road and Traffic Engineering of the State Ministry of Education",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60129238",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "An Automatic IP Autoconfiguration Scheme using Harmony Search Algorithm Optimized Hybrid Extreme Learning Machine Model",
        "publication": "SSRG International Journal of Electrical and Electronics Engineering",
        "citied_by": "1",
        "cover_date": "2022-09-01",
        "Abstract": "IP autoconfiguration is the process of allocating unique network settings, flows, policies, and controls. Each node in the network must be configured to connect with other nodes in the system. The centralized DHCP server can accomplish it. Meanwhile, in the mobile ad hoc network (MANET), there are no centralized servers; hence, the autoconfiguration of nodes to the network is arduous. It can be achieved by distributed address autoconfiguration approach, and we proposed a novel Harmony Search Algorithm (HSA) based hybrid structure-adaptive radial basis function-extreme learning machine (HSARBF-ELM) (HSHSAELM) approach for MANET. The proposed HSA is used to detect the duplicate address node. An HSARBF-ELM classifier effectively assigns a reserved address to the new node by classifying the nodes. The proposed HSHSAELM approach effectively achieves the address allocation and duplicate address nodes. Further, it is also used to minimize the communication overhead and ensures unique address allocation to the new nodes. Experimental analysis of our proposed method with state-of-art works shows that our proposed method outperforms all the other approaches.",
        "DOI": "10.14445/23488379/IJEEE-V9I9P102",
        "paper_author": "Devi M.S.",
        "affiliation_name": "Mother Teresa Women's University",
        "affiliation_city": "Kodaikanal",
        "affiliation_country": "India",
        "affiliation_id": "60015537",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Nowcasting the Australian Labour Market at Disaggregated Levels",
        "publication": "Australian Economic Review",
        "citied_by": "0",
        "cover_date": "2022-09-01",
        "Abstract": "Detailed labour market and economic data are often released infrequently and with considerable time lags between collection and release, making it difficult for policy-makers to accurately assess current conditions. Nowcasting is an emerging technique in the field of economics that seeks to address this gap by ‘predicting the present’. While nowcasting has primarily been used to derive timely estimates of economy-wide indicators such as GDP and unemployment, this article extends this literature to show how big data and machine-learning techniques can be utilised to produce nowcasting estimates at detailed disaggregated levels. A range of traditional and real-time data sources were used to produce, for the first time, a useful and timely indicator—or nowcast—of employment by region and occupation. The resulting Nowcast of Employment by Region and Occupation (NERO) will complement existing sources of labour market information and improve Australia's capacity to understand labour market trends in a more timely and detailed manner.",
        "DOI": "10.1111/1467-8462.12464",
        "paper_author": "Shamiri S.",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Deep reinforcement learning for multi-agent interaction",
        "publication": "AI Communications",
        "citied_by": "7",
        "cover_date": "2022-09-01",
        "Abstract": "The development of autonomous agents which can interact with other agents to accomplish a given task is a core area of research in artificial intelligence and machine learning. Towards this goal, the Autonomous Agents Research Group develops novel machine learning algorithms for autonomous systems control, with a specific focus on deep reinforcement learning and multi-agent reinforcement learning. Research problems include scalable learning of coordinated agent policies and inter-agent communication; reasoning about the behaviours, goals, and composition of other agents from limited observations; and sample-efficient learning based on intrinsic motivation, curriculum learning, causal inference, and representation learning. This article provides a broad overview of the ongoing research portfolio of the group and discusses open problems for future directions.",
        "DOI": "10.3233/AIC-220116",
        "paper_author": "Ahmed I.H.",
        "affiliation_name": "The University of Edinburgh",
        "affiliation_city": "Edinburgh",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60027272",
        "affiliation_state": "Scotland"
    },
    {
        "paper_title": "Path and future of artificial intelligence in the field of justice: a systematic literature review and a research agenda",
        "publication": "SN Social Sciences",
        "citied_by": "8",
        "cover_date": "2022-09-01",
        "Abstract": "Studies on the use of Artificial Intelligence (AI) in the public sector are on the rise. However, there is still a lack of depth concerning specific government segments, such as in the field of justice. In this sense, this work seeks to understand the evolution of the use of AI in Justice and its future perspectives. The authors carried out a Systematic Literature Review (SLR) with a bibliometric analysis of 69 articles collected in the Scopus and Web of Science databases, without a time frame. The categorized results demonstrate stability in productivity between 1988 and 2014 and substantial growth from 2015 onwards. There is also a clear interaction between sub-themes relating to AI and judicialization, including Knowledge-based Systems, Online Dispute Resolution, Algorithmic Surveillance, Decision Support Systems, and Machine Learning Explainable. Thus, the authors expect that this SLR will contribute to the advancement of studies on AI in Justice, subsidize the management of public policies aimed at the justice system, and guide managers in the production chain.",
        "DOI": "10.1007/s43545-022-00482-w",
        "paper_author": "de Oliveira L.F.",
        "affiliation_name": "Universidade de Brasília",
        "affiliation_city": "Brasilia",
        "affiliation_country": "Brazil",
        "affiliation_id": "60024989",
        "affiliation_state": "DF"
    },
    {
        "paper_title": "Double machine learning-based programme evaluation under unconfoundedness",
        "publication": "Econometrics Journal",
        "citied_by": "32",
        "cover_date": "2022-09-01",
        "Abstract": "This paper reviews, applies, and extends recently proposed methods based on double machine learning (DML) with a focus on programme evaluation under unconfoundedness. DML-based methods leverage flexible prediction models to adjust for confounding variables in the estimation of (a) standard average effects, (b) different forms of heterogeneous effects, and (c) optimal treatment assignment rules. An evaluation of multiple programmes of the Swiss Active Labour Market Policy illustrates how DML-based methods enable a comprehensive programme evaluation. Motivated by extreme individualised treatment effect estimates of the DR-learner, we propose the normalised DR-learner (NDR-learner) to address this issue. The NDR-learner acknowledges that individualised effect estimates can be stabilised by an individualised normalisation of inverse probability weights.",
        "DOI": "10.1093/ectj/utac015",
        "paper_author": "Knaus M.C.",
        "affiliation_name": "University of St. Gallen",
        "affiliation_city": "St Gallen",
        "affiliation_country": "Switzerland",
        "affiliation_id": "60027786",
        "affiliation_state": "SG"
    },
    {
        "paper_title": "Best proxy to determine firm performance using financial ratios: A CHAID approach",
        "publication": "Review of Economic Perspectives",
        "citied_by": "11",
        "cover_date": "2022-09-01",
        "Abstract": "The main purpose of this study is to investigate the best predictor of firm performance among different proxies. A sample of 287 Czech firms was taken from automobile, construction, and manufacturing sectors. Panel data of the firms was acquired from the Albertina database for the time period from 2016 to 2020. Three different proxies of firm performance, return of assets (RoA), return of equity (RoE), and return of capital employed (RoCE) were used as dependent variables. Including three proxies of firm's performance, 16 financial ratios were measured based on the previous literature. A machine learning-based decision tree algorithm, Chi-squared Automatic Interaction Detector (CHAID), was deployed to gauge each proxy's efficacy and examine the best proxy of the firm performance. A partitioning rule of 70:30 was maintained, which implied that 70% of the dataset was used for training and the remaining 30% for testing. The results revealed that return on assets (RoA) was detected to be a robust proxy to predict financial performance among the targeted indicators. The results and the methodology will be useful for policy-makers, stakeholders, academics and managers to take strategic business decisions and forecast financial performance.",
        "DOI": "10.2478/revecp-2022-0010",
        "paper_author": "Yousaf M.",
        "affiliation_name": "Tomas Bata University in Zlin",
        "affiliation_city": "Zlin",
        "affiliation_country": "Czech Republic",
        "affiliation_id": "60008287",
        "affiliation_state": "Zlin Region"
    },
    {
        "paper_title": "Huddling with families after disaster: Human resilience and social disparity",
        "publication": "PLoS ONE",
        "citied_by": "3",
        "cover_date": "2022-09-01",
        "Abstract": "Disasters, from hurricanes to pandemics, tremendously impact human lives and behaviors. Physical closeness to family post-disaster plays a critical role in mental healing and societal sustainability. Nonetheless, little is known about whether and how family colocation alters after a disaster, a topic of immense importance to a post-disaster society. We analyze 1 billion records of population-scale, granular, individual-level mobile location data to quantify family colocation, and examine the magnitude, dynamics, and socioeconomic heterogeneity of the shift in family colocation from the pre- to post-disaster period. Leveraging Hurricane Florence as a natural experiment, and Geographic Information System (GIS), machine learning, and statistical methods to investigate the shift across the landfall (treated) city of Wilmington, three partially treated cites on the hurricane’s path, and two control cities off the path, we uncover dramatic (18.9%), widespread (even among the partially treated cities), and enduring (over at least 3 months) escalations in family colocation. These findings reveal the powerful psychological and behavioral impacts of the disaster upon the broader populations, and simultaneously remarkable human resilience via behavioral adaptations during disastrous times. Importantly, the disaster created a gap across socioeconomic groups nonexistent beforehand, with the disadvantaged displaying weaker lifts in family colocation. This sheds important lights on policy making and policy communication to promote sustainable family colocation, healthy coping strategies against traumatic experiences, social parity, and societal recovery.",
        "DOI": "10.1371/journal.pone.0273307",
        "paper_author": "Wang W.",
        "affiliation_name": "Simon Business School",
        "affiliation_city": "Rochester",
        "affiliation_country": "United States",
        "affiliation_id": "60122753",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "RegBR: A novel Brazilian government framework to classify and analyze industry-specific regulations",
        "publication": "PLoS ONE",
        "citied_by": "4",
        "cover_date": "2022-09-01",
        "Abstract": "Government transparency and openness are key factors to bring forth the modernization of the state. The combination of transparency and digital information has given rise to the concept of Open Government, that increases citizen understanding and monitoring of government actions, which in turn improves the quality of public services and of the government decision making process. With the goal of improving legislative transparency and the understanding of the Brazilian regulatory process and its characteristics, this paper introduces RegBR, the first national framework to centralize, classify and analyze regulations from the Brazilian government. A centralized database of Brazilian federal legislation built from automated ETL routines and processed with data mining and machine learning techniques was created. Our framework evaluates different NLP models in a text classification task on our novel Portuguese legal corpus and performs regulatory analysis based on metrics that concern linguistic complexity, restrictiveness, law interest, and industry-specific citation relevance. Our results were examined over time and validated by correlating them with known episodes of regulatory changes in Brazilian history, such as the implementation of new economic plans or the emergence of an energy crisis. Methods and metrics proposed by this framework can be used by policy makers to measure their own work and serve as inputs for future studies that could analyze government changes and their relationship with federal regulations.",
        "DOI": "10.1371/journal.pone.0275282",
        "paper_author": "Valle L.M.",
        "affiliation_name": "Universidade de Brasília",
        "affiliation_city": "Brasilia",
        "affiliation_country": "Brazil",
        "affiliation_id": "60024989",
        "affiliation_state": "DF"
    },
    {
        "paper_title": "Transfer and Reinforcement Learning Based Production Control",
        "publication": "ZWF Zeitschrift fuer Wirtschaftlichen Fabrikbetrieb",
        "citied_by": "0",
        "cover_date": "2022-09-01",
        "Abstract": "Constantly increasing complexity and growing information densities in production systems open up potentials for the application of machine learning methods. So-called reinforcement learning is particularly suitable for implementing autonomous agentbased control systems. However, the application of this becomes more difficult in changing production systems. It is shown for the first time that the transfer learning approach is useful for production control systems with reinforcement learning.",
        "DOI": "10.1515/zwf-2022-1111",
        "paper_author": "Steinbacher L.",
        "affiliation_name": "Universität Bremen",
        "affiliation_city": "Bremen",
        "affiliation_country": "Germany",
        "affiliation_id": "60008293",
        "affiliation_state": "Bremen"
    },
    {
        "paper_title": "Risk Disclosure in Crowdfunding",
        "publication": "Information Systems Research",
        "citied_by": "36",
        "cover_date": "2022-09-01",
        "Abstract": "How should crowdfunding platforms alleviate information asymmetry between creators and crowdfunders? In traditional financial markets, public companies are required to disclose potential risks to their investors, and such risk disclosure requirements are enforced by legal and fiduciary regulations. In the crowdfunding context, however, such information asymmetry concerns are often addressed by crowd-based platforms. In this study, we examine whether and how a regulation to increase the salience of project risks in crowdfunding affects crowdfunders’ funding decisions. Leveraging a policy change as an exogenous event, we adopt a difference-in-differences empirical strategy with a matching sample to compare funding decisions before and after the regulation was mandated and show differential effects between high- and low-risk projects. In addition, to strengthen the causality, we directly test individuals’ intention to pledge after reading project descriptions with and without risk disclosure in online experiments. We find that increasing the awareness of project risks is associated with inferior funding outcomes of crowdfunding projects, and the effect exists mainly for high-risk projects but not much for low-risk projects. In addition, high-risk projects benefit from a risk disclosure with relevant information, authentic language, and balanced tones that are not overly negative or optimistic. Despite the negative short-term effects, technology funders tend to interpret risk disclosures rationally over time, suggesting a positive long-term effect. Implications for research and insights for practitioners are discussed, particularly the fact that disclosure policies may make crowdfunding markets more sustainable by reducing information asymmetry and helping crowdfunders make more informed decisions.",
        "DOI": "10.1287/isre.2021.1096",
        "paper_author": "Kim K.",
        "affiliation_name": "CUHK Business School",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong",
        "affiliation_id": "60256697",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Mitigating Bias in Radiology Machine Learning: 1. Data Handling",
        "publication": "Radiology: Artificial Intelligence",
        "citied_by": "65",
        "cover_date": "2022-09-01",
        "Abstract": "Minimizing bias is critical to adoption and implementation of machine learning (ML) in clinical practice. Systematic mathematical biases produce consistent and reproducible differences between the observed and expected performance of ML systems, resulting in suboptimal performance. Such biases can be traced back to various phases of ML development: data handling, model development, and performance evaluation. This report presents 12 suboptimal practices during data handling of an ML study, explains how those practices can lead to biases, and describes what may be done to mitigate them. Authors employ an arbitrary and simplified framework that splits ML data handling into four steps: data collection, data investigation, data splitting, and feature engineering. Examples from the available research literature are provided. A Google Colaboratory Jupyter notebook includes code examples to demonstrate the sub-optimal practices and steps to prevent them.",
        "DOI": "10.1148/ryai.210290",
        "paper_author": "Rouzrokh P.",
        "affiliation_name": "Mayo Clinic",
        "affiliation_city": "Rochester",
        "affiliation_country": "United States",
        "affiliation_id": "60005558",
        "affiliation_state": "MN"
    },
    {
        "paper_title": "Using machine learning to understand determinants of IUD use in India: Analyses of the National Family Health Surveys (NFHS-4)",
        "publication": "SSM - Population Health",
        "citied_by": "2",
        "cover_date": "2022-09-01",
        "Abstract": "Intra-uterine devices (IUDs) are a safe and effective method to delay or space pregnancies and are available for free or at low cost in the Indian public health system; yet, IUD uptake in India remains low. Limited quantitative research using national data has explored factors that may affect IUD use. Machine Learning (ML) techniques allow us to explore determinants of low prevalence behaviors in survey research, such as IUD use. We applied ML to explore the determinants of IUD use in India among married women in the 4th National Family Health Survey (NFHS-4; N = 499,627), which collects data on demographic and health indicators among women of childbearing age. We conducted ML logistic regression (lasso and ridge) and neural network approaches to assess significant determinants and used iterative thematic analysis (ITA) to offer insight into related variable constructs generated from a series of regularized models. We found that couples’ shared family planning (FP) goals were the strongest determinants of IUD use, followed by receipt of FP services and desire for no more children, higher wealth and education, and receipt of maternal and child health services. Findings highlight the importance of male engagement and family planning services for IUD uptake and the need for more targeted efforts to support awareness of IUD as an option for spacing, especially for those of lower SES and with lower access to care.",
        "DOI": "10.1016/j.ssmph.2022.101234",
        "paper_author": "Dey A.K.",
        "affiliation_name": "Department of Medicine",
        "affiliation_city": "La Jolla",
        "affiliation_country": "United States",
        "affiliation_id": "60121547",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "A novel machine learning approach to predict the export price of seafood products based on competitive information: The case of the export of Vietnamese shrimp to the US market",
        "publication": "PLoS ONE",
        "citied_by": "8",
        "cover_date": "2022-09-01",
        "Abstract": "Predicting the export price of shrimp is important for Vietnam's fisheries. It not only promotes product quality but also helps policy makers determine strategies to develop the national shrimp industry. Competition in global markets is considered to be an important factor, one that significantly influences price. In this study, we predicted trends in the export price of Vietnamese shrimp based on competitive information from six leading exporters (China, India, Indonesia, Thailand, Ecuador, and Chile) who, alongside Vietnam, also export shrimp to the US. The prediction was based on a dataset collected from the US Department of Agriculture (USDA), the Food and Agriculture Organization of the United Nations (FAO), and the World Trade Organization (WTO) (May-1995 to May-2019) that included price, required farming certificates, and disease outbreak data. A super learner technique, which combined 10 single algorithms, was used to make predictions in selected base periods (3, 6, 9, and 12 months). It was found that the super learner obtained results in all base periods that were more accurate and stable than any candidate algorithms. The impacts of variables in the predictive model were interpreted by a SHapley Additive exPlanations (SHAP) analysis to determine their influence on the price of Vietnamese exports. The price of Indian, Thai, and Chinese exports highlighted the advantages of being a World Trade Organization member and the disadvantages of the prevalence of shrimp disease in Vietnam, which has had a significant impact on the Vietnamese shrimp export price.",
        "DOI": "10.1371/journal.pone.0275290",
        "paper_author": "Khiem N.M.",
        "affiliation_name": "Hokkaido University",
        "affiliation_city": "Sapporo",
        "affiliation_country": "Japan",
        "affiliation_id": "60014652",
        "affiliation_state": "Hokkaido"
    },
    {
        "paper_title": "Counteracting French Fake News on Climate Change Using Language Models",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "8",
        "cover_date": "2022-09-01",
        "Abstract": "The unprecedented scale of disinformation on the Internet for more than a decade represents a serious challenge for democratic societies. When this process is focused on a well-established subject such as climate change, it can subvert measures and policies that various governmental bodies have taken to mitigate the phenomenon. It is therefore essential to effectively identify and counteract fake news on climate change. To do this, our main contribution represents a novel dataset with more than 2300 articles written in French, gathered using web scraping from all types of media dealing with climate change. Manual labeling was performed by two annotators with three classes: “fake”, “biased”, and “true”. Machine Learning models ranging from bag-of-words representations used by an SVM to Transformer-based architectures built on top of CamemBERT were built to automatically classify the articles. Our results, with an F1-score of 84.75% using the BERT-based model at the article level coupled with hand-crafted features specifically tailored for this task, represent a strong baseline. At the same time, we highlight perceptual properties as text sequences (i.e., fake, biased, and irrelevant text fragments) at the sentence level, with a macro F1 of 45.01% and a micro F1 of 78.11%. Based on these results, our proposed method facilitates the identification of fake news, and thus contributes to better education of the public.",
        "DOI": "10.3390/su141811724",
        "paper_author": "Meddeb P.",
        "affiliation_name": "Mines Paris - PSL",
        "affiliation_city": "Paris",
        "affiliation_country": "France",
        "affiliation_id": "60030506",
        "affiliation_state": "Ile-de-France"
    },
    {
        "paper_title": "Control of quasi-equilibrium state of annular flow through reinforcement learning",
        "publication": "Physics of Fluids",
        "citied_by": "2",
        "cover_date": "2022-09-01",
        "Abstract": "Stability control of the convection flow field has always been a focal issue. The annular flow discussed in this work is a typical research model of microgravity fluid physics, which is extracted from the industrial crystal growth by the Czochralski method. It is believed that the instability of thermal convection is the key factor affecting the quality of crystal growth. Combining the reinforcement learning algorithm with the neural network, this paper proposes a control policy that makes forced convection compete with thermocapillary convection by changing the dynamic boundary conditions of the system. This control policy is successfully applied to the control of the quasi-equilibrium state of annular flow, and the global stability of the flow field is well maintained. It first experimentally makes the annular flow field under low and medium M a numbers achieve a quasi-equilibrium state, which is different from that before the onset of flow oscillations. Then, a simulation environment is created to imitate the experimental conditions. After training in the simulation environment, with the self-optimized algorithm, the machine learning approach can successfully maintain the simulation environment in a quasi-equilibrium state for a long period of time. Finally, the learning method is validated in the experimental environment, and a quasi-equilibrium state control policy is completely optimized by using the same optimization policy and similar neural network structure. This work demonstrates that the model can understand the physical environment and the author's control objectives through reinforcement learning. It is an important application of reinforcement learning in the real world and a clear demonstration of the research value of microgravity fluid physics.",
        "DOI": "10.1063/5.0102668",
        "paper_author": "Chen Y.",
        "affiliation_name": "Institute of Mechanics Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60011040",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "LiDAR as a Tool for Assessing Timber Assortments: A Systematic Literature Review",
        "publication": "Remote Sensing",
        "citied_by": "11",
        "cover_date": "2022-09-01",
        "Abstract": "Forest ecosystems strongly contribute to the mitigation of climate change impacts through the carbon stored in forests and through harvested wood products, such as sawed wood and furniture, which are obtained from many types of timber assortments. Timber assortments are defined as log sections of specific dimensions (log length and maximum/minimum end diameters), gathered from felled trunks, that have both specific commercial timber utilisation and economic value. However, it is challenging to discriminate and assess timber assortment types, especially within a forest stand before the forest has been harvested. Accurate estimations of timber assortments are a fundamental prerequisite in supporting forest holdings and assisting practitioners in the optimisation of harvesting activities and promoting forest wood chains, in addition to forest policy and planning. Based on the georeferenced points cloud tool, light detection and ranging (LiDAR) is a powerful technology for rapidly and accurately depicting forest structure, even if the use of LiDAR for timber assortments estimation is lacking and poorly explored. This systematic literature review aimed to highlight the state-of-the-art applications of the LiDAR systems (spaceborne; airborne, including unmanned aerial UASs; and terrestrial) to quantify and classify different timber assortment types. A total of 304 peer-reviewed papers were examined. The results highlight a constant increment of published articles using LiDAR systems for forest-related aspects in the period between 2000 and 2021. The most recurring investigation topics in LiDAR studies were forest inventory and forest productivity. No studies were found that used spaceborne LiDAR systems for timber assortment assessments, as these were conditioned by the time and sample size (sample size = ~12 m/~25 m of laser footprint and 0.7 m/60 m of space along the track for ICESat-2, GEDI and time = since 2018). Terrestrial LiDAR systems demonstrated a higher performance in successfully characterising the trees belonging to an understory layer. Combining airborne/UAS systems with terrestrial LiDAR systems is a promising approach to obtain detailed data concerning the timber assortments of large forest covers. Overall, our results reveal that the interest of scientists in using machine and deep learning algorithms for LiDAR processes is steadily increasing.",
        "DOI": "10.3390/rs14184466",
        "paper_author": "Alvites C.",
        "affiliation_name": "Università degli Studi del Molise",
        "affiliation_city": "Campobasso",
        "affiliation_country": "Italy",
        "affiliation_id": "60014096",
        "affiliation_state": "CB"
    },
    {
        "paper_title": "Designing Efficient and Sustainable Predictions of Water Quality Indexes at the Regional Scale Using Machine Learning Algorithms",
        "publication": "Water (Switzerland)",
        "citied_by": "21",
        "cover_date": "2022-09-01",
        "Abstract": "Water quality and scarcity are key topics considered by the Sustainable Development Goals (SDGs), institutions, policymakers and stakeholders to guarantee human safety, but also vital to protect natural ecosystems. However, conventional approaches to deciding the suitability of water for drinking purposes are often costly because multiple characteristics are required, notably in low-income countries. As a result, building right and trustworthy models is mandatory to correctly manage available groundwater resources. In this research, we propose to check multiple classification techniques such as Decision Trees (DT), K-Nearest Neighbors (KNN), Discriminants Analysis (DA), Support Vector Machine (SVM), and Ensemble Trees (ET) to design the best strategy allowing the forecast a Water Quality Index (WQI). To achieve this goal, an extended dataset characterized by water samples collected in a total of twelve municipalities of the Wilaya of Naâma in Algeria was considered. Among them, 151 samples were examined as training samples, and 18 were used to test and confirm the prediction model. Later, data samples were classified based on the WQI into four states: excellent water quality, good water quality, poor water quality, and very poor or unsafe water. The main results revealed that the SVM classifier obtained the highest forecast accuracy, with 95.4% of prediction accuracy when the data are standardized and 88.9% for the accuracy of the test samples. The results confirmed that the use of machine learning models are powerful tools for forecasting drinking water as larger scales to promote the design of efficient and sustainable water quality control and support decision-plans.",
        "DOI": "10.3390/w14182801",
        "paper_author": "Derdour A.",
        "affiliation_name": "Centre Universitaire Salhi Ahmed -Naama",
        "affiliation_city": "Naama",
        "affiliation_country": "Algeria",
        "affiliation_id": "60105423",
        "affiliation_state": "Naâma"
    },
    {
        "paper_title": "Evaluation of Machine Learning Algorithm on Drinking Water Quality for Better Sustainability",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "40",
        "cover_date": "2022-09-01",
        "Abstract": "Water has become intricately linked to the United Nations’ sixteen sustainable development goals. Access to clean drinking water is crucial for health, a fundamental human right, and a component of successful health protection policies. Clean water is a significant health and development issue on a national, regional, and local level. Investments in water supply and sanitation have been shown to produce a net economic advantage in some areas because they reduce adverse health effects and medical expenses more than they cost to implement. However, numerous pollutants are affecting the quality of drinking water. This study evaluates the efficiency of using machine learning (ML) techniques in order to predict the quality of water. Thus, in this paper, a machine learning classifier model is built to predict the quality of water using a real dataset. First, significant features are selected. In the case of the used dataset, all measured characteristics are chosen. Data are split into training and testing subsets. A set of existing ML algorithms is applied, and the results are compared in terms of precision, recall, F1 score, and ROC curve. The results show that support vector machine and k-nearest neighbor are better according to F1-score and ROC AUC values. However, The LASSO LARS and stochastic gradient descent are better based on recall values.",
        "DOI": "10.3390/su141811478",
        "paper_author": "Kaddoura S.",
        "affiliation_name": "Zayed University",
        "affiliation_city": "Dubai",
        "affiliation_country": "United Arab Emirates",
        "affiliation_id": "60070818",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Early Prediction of At-Risk Students in Secondary Education: A Countrywide K-12 Learning Analytics Initiative in Uruguay",
        "publication": "Information (Switzerland)",
        "citied_by": "14",
        "cover_date": "2022-09-01",
        "Abstract": "This paper describes a nationwide learning analytics initiative in Uruguay focused on the future implementation of governmental policies to mitigate student retention and dropouts in secondary education. For this, data from a total of 258,440 students were used to generate automated models to predict students at risk of failure or dropping out. Data were collected from primary and secondary education from different sources and for the period between 2015 and 2020. Such data contains demographic information about the students and their trajectories from the first grade of primary school to the second grade of secondary school (e.g., student assessments in different subjects over the years, the amount of absences, participation in social welfare programs, and the zone of the school, among other factors). Predictive models using the random forest algorithm were trained, and their performances were evaluated with F1-Macro and AUROC measures. The models were planned to be applied in different periods of the school year for the regular secondary school and for the technical secondary school ((before the beginning of the school year and after the first evaluation meeting for each grade). A total of eight predictive models were developed considering this temporal approach, and after an analysis of bias considering three protected attributes (gender, school zone, and social welfare program participation), seven of them were approved to be used for prediction. The models achieved outstanding performances according to the literature, with an AUROC higher than 0.90 and F1-Macro higher than 0.88. This paper describes in depth the characteristics of the data gathered, the specifics of data preprocessing, and the methodology followed for model generation and bias analysis, together with the architecture developed for the deployment of the predictive models. Among other findings, the results of the paper corroborate the importance given in the literature of using the previous performances of the students in order to predict their future performances.",
        "DOI": "10.3390/info13090401",
        "paper_author": "Queiroga E.M.",
        "affiliation_name": "Instituto Federal de Educação, Ciência e Tecnologia Sul-rio-grandense (IFSul)",
        "affiliation_city": "Pelotas",
        "affiliation_country": "Brazil",
        "affiliation_id": "60069359",
        "affiliation_state": "RS"
    },
    {
        "paper_title": "Extraction of Urban Quality of Life Indicators Using Remote Sensing and Machine Learning: The Case of Al Ain City, United Arab Emirates (UAE)",
        "publication": "ISPRS International Journal of Geo-Information",
        "citied_by": "10",
        "cover_date": "2022-09-01",
        "Abstract": "Urban quality of life (UQoL) study is very important for many applications such as services distribution, urban planning, and socioeconomic analysis. The objective of this study is to create an urban quality of life index map for Al Ain city in the United Arab Emirates (UAE). The research aligns with the United Nations Sustainable Development Goals number ten (reduce inequalities) and eleven (sustainable cities and communities). In this study, remote sensing images and GIS vector datasets were used to extract biophysical and infrastructure facility indicators. The biophysical indicators are normalized difference vegetation index (NDVI), normalized difference water index (NDWI), modified normalized difference water index (MNDWI), soil adjusted vegetation index (SAVI), enhanced normalized difference impervious surfaces index (ENDISI), normalized difference built-up index (NDBI), land surface temperature (LST), slope, and land use land cover (LULC). In addition, infrastructure facility indicators such as distances to main roads, parks, schools, and hospitals were obtained. Additional infrastructure facility variables namely built-up to green area and build-up to bare soil area ratio were extracted from the LULC map. Machine learning was used to classify satellite images and generate LULC map. Random Forest (RF) was found as the best machine learning classifier for this study. The overall classification and Kappa hat accuracy was 95.3 and 0.92, respectively. Both biophysical and infrastructure facility indicators were integrated using principal component analysis (PCA). The PCA analysis identified four components that explain 75% of the variance among the indicators. The four factors were interpreted as the effect of LULC, infrastructure facility, ecological, and slope. Finally, the components were assigned weights based on the percentage of variance they explained and developed the UQoL map. Overall, the result showed that greenness has a greater effect on the spatial pattern of UQoL in Al Ain city. The study could be of a value to policy makers in urban planning and socioeconomic departments.",
        "DOI": "10.3390/ijgi11090458",
        "paper_author": "Yagoub M.M.",
        "affiliation_name": "United Arab Emirates University",
        "affiliation_city": "Al Ain",
        "affiliation_country": "United Arab Emirates",
        "affiliation_id": "60008665",
        "affiliation_state": "Abu Dhabi"
    },
    {
        "paper_title": "Using Social Media to Monitor Conflict-Related Migration: A Review of Implications for A.I. Forecasting",
        "publication": "Social Sciences",
        "citied_by": "7",
        "cover_date": "2022-09-01",
        "Abstract": "Following the large-scale 2015–2016 migration crisis that shook Europe, deploying big data and social media harvesting methods became gradually popular in mass forced migration monitoring. These methods have focused on producing ‘real-time’ inferences and predictions on individual and social behavioral, preferential, and cognitive patterns of human mobility. Although the volume of such data has improved rapidly due to social media and remote sensing technologies, they have also produced biased, flawed, or otherwise invasive results that made migrants’ lives more difficult in transit. This review article explores the recent debate on the use of social media data to train machine learning classifiers and modify thresholds to help algorithmic systems monitor and predict violence and forced migration. Ultimately, it identifies and dissects five prevalent explanations in the literature on limitations for the use of such data for A.I. forecasting, namely ‘policy-engineering mismatch’, ‘accessibility/comprehensibility’, ‘legal/legislative legitimacy’, ‘poor data cleaning’, and ‘difficulty of troubleshooting’. From this review, the article suggests anonymization, distributed responsibility, and ‘right to reasonable inferences’ debates as potential solutions and next research steps to remedy these problems.",
        "DOI": "10.3390/socsci11090395",
        "paper_author": "Unver H.A.",
        "affiliation_name": "Özyeğin Üniversitesi",
        "affiliation_city": "Istanbul",
        "affiliation_country": "Turkey",
        "affiliation_id": "60086558",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Dataset for the Vietnamese Banking System (2002–2021)",
        "publication": "Data",
        "citied_by": "10",
        "cover_date": "2022-09-01",
        "Abstract": "This data article describes a dataset that consists of key statistics on the activities of 45 Vietnamese banks (e.g., deposits, loans, assets, and labor productivity), operated during the 2002–2021 period, yielding a total of 644 bank-year observations. This is the first systematic compilation of data on the splits of state vs. private ownership, foreign vs. domestic banks, commercial vs. policy banks, and listed vs. nonlisted banks. Consequently, this arrives at a unique set of variables and indicators that allow us to capture the development and performance of the Vietnamese banking sector over time along many different dimensions. This can play an important role for financial analysts, researchers, and educators in banking efficiency and performance, risk and profit/revenue management, machine learning, and other fields. Dataset: https://doi.org/10.7910/DVN/RIWA3B Dataset License: CC0",
        "DOI": "10.3390/data7090120",
        "paper_author": "Le T.D.Q.",
        "affiliation_name": "VNUHCM-University of Economics and Law",
        "affiliation_city": "Ho Chi Minh City",
        "affiliation_country": "Viet Nam",
        "affiliation_id": "60283216",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Analysis of Atmospheric Pollutants and Meteorological Factors on PM<inf>2.5</inf> Concentration and Temporal Variations in Harbin",
        "publication": "Atmosphere",
        "citied_by": "15",
        "cover_date": "2022-09-01",
        "Abstract": "With rapid economic development, the problem of air pollution has become increasingly prominent. Countries have paid attention to PM2.5, one of the main air pollutants, and have gradually addressed this issue. Based on the 2015–2019 air quality data, meteorological data, and aerosol optical depth data from Harbin, China, this study investigated the relationship between PM2.5, a number of influencing factors, and their temporal changes using a machine-learning method. It can be seen from the analysis that the random forest model can predict PM2.5 concentration. In this model, the mean RH and AOD have a high impact on PM2.5 concentration, but there was negligent correlation with PM2.5. The results indicated that the level of PM2.5 pollution continuously decreased from 2015 to 2019, and there were significant seasonal differences in PM2.5 concentration and its variations. In 2019, due to the impact of heating and adverse meteorological conditions, PM2.5 pollution during the heating period increased significantly. This study provides theoretical and data support for the analysis of PM2.5 pollution in Harbin and formulation of air pollution control policies.",
        "DOI": "10.3390/atmos13091426",
        "paper_author": "Gao X.",
        "affiliation_name": "Harbin Institute of Technology",
        "affiliation_city": "Harbin",
        "affiliation_country": "China",
        "affiliation_id": "60019616",
        "affiliation_state": "Heilongjiang"
    },
    {
        "paper_title": "Monitoring the Influence of Industrialization and Urbanization on Spatiotemporal Variations of AQI and PM<inf>2.5</inf> in Three Provinces, China",
        "publication": "Atmosphere",
        "citied_by": "12",
        "cover_date": "2022-09-01",
        "Abstract": "With the rapid development of industrialization and urbanization, atmospheric pollution research is vital for regional sustainable development and related policies formulated by the government. Previous studies have mainly studied a single evaluation method to analyze the air quality index (AQI) or single air pollutant. This research integrated the Spearman coefficient (SC) correlation analysis, a random search (RS) algorithm and an excellent extreme gradient boosting (XGBoost) algorithm to evaluate the air pollution influence of industrialization and urbanization (APIIU). Industrialization, urbanization and meteorological indicators were used to measure the influence degree of APIIU on AQI and particulate matter 2.5 (PM2.5), respectively. The main findings were: (1) the APIIU-AQI and APIIU-PM2.5 of Henan Province, Hubei Province and Hunan Province had significant changes from 2017 to 2019; (2) the value of square of determination coefficient of real value (R2), the root mean square error (RMSE) and the mean absolute percentage error (MAPE) of APIIU-AQI and APIIU-PM2.5 in three provinces predicted by the SC-RS-XGBoost were 0.945, 0.103, 4.25% and 0.897, 0.205, 4.84%, respectively; (3) the predicted results were more accurate than using a SC-XGBoost, RS-XGBoost, traditional XGBoost, support vector regression (SVR) and extreme learning machine (ELM).",
        "DOI": "10.3390/atmos13091377",
        "paper_author": "Chen H.",
        "affiliation_name": "Henan University of Science and Technology",
        "affiliation_city": "Luoyang",
        "affiliation_country": "China",
        "affiliation_id": "60073587",
        "affiliation_state": "Henan"
    },
    {
        "paper_title": "Secure Smart Communication Efficiency in Federated Learning: Achievements and Challenges",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "19",
        "cover_date": "2022-09-01",
        "Abstract": "Federated learning (FL) is known to perform machine learning tasks in a distributed manner. Over the years, this has become an emerging technology, especially with various data protection and privacy policies being imposed. FL allows for performing machine learning tasks while adhering to these challenges. As with the emergence of any new technology, there will be challenges and benefits. A challenge that exists in FL is the communication costs: as FL takes place in a distributed environment where devices connected over the network have to constantly share their updates, this can create a communication bottleneck. This paper presents the state-of-the-art of the conducted works on communication constraints of FL while maintaining the secure and smart properties that federated learning is known for. Overall, current challenges and possible methods for enhancing the FL models’ efficiency with a perspective on communication are discussed. This paper aims to bridge the gap in all conducted review papers by solely focusing on communication aspects in FL environments.",
        "DOI": "10.3390/app12188980",
        "paper_author": "Pouriyeh S.",
        "affiliation_name": "Kennesaw State University",
        "affiliation_city": "Kennesaw",
        "affiliation_country": "United States",
        "affiliation_id": "60019740",
        "affiliation_state": "GA"
    },
    {
        "paper_title": "Machine Learning Applications in Surface Transportation Systems: A Literature Review",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "22",
        "cover_date": "2022-09-01",
        "Abstract": "Surface transportation has evolved through technology advancements using parallel knowledge areas such as machine learning (ML). However, the transportation industry has not yet taken full advantage of ML. To evaluate this gap, we utilized a literature review approach to locate, categorize, and synthesize the principal concepts of research papers regarding surface transportation systems using ML algorithms, and we then decomposed them into their fundamental elements. We explored more than 100 articles, literature review papers, and books. The results show that 74% of the papers concentrate on forecasting, while multilayer perceptions, long short-term memory, random forest, supporting vector machine, XGBoost, and deep convolutional neural networks are the most preferred ML algorithms. However, sophisticated ML algorithms have been minimally used. The root-cause analysis revealed a lack of effective collaboration between the ML and transportation experts, resulting in the most accessible transportation applications being used as a case study to test or enhance a given ML algorithm and not necessarily to enhance a mobility or safety issue. Additionally, the transportation community does not define transportation issues clearly and does not provide publicly available transportation datasets. The transportation sector must offer an open-source platform to showcase the sector’s concerns and build spatiotemporal datasets for ML experts to accelerate technology advancements.",
        "DOI": "10.3390/app12189156",
        "paper_author": "Behrooz H.",
        "affiliation_name": "Charles V. Schaefer, Jr. School of Engineering and Science",
        "affiliation_city": "Hoboken",
        "affiliation_country": "United States",
        "affiliation_id": "60148800",
        "affiliation_state": "NJ"
    },
    {
        "paper_title": "Examining the Prevailing Negative Sentiments Related to COVID-19 Vaccination: Unsupervised Deep Learning of Twitter Posts over a 16 Month Period",
        "publication": "Vaccines",
        "citied_by": "34",
        "cover_date": "2022-09-01",
        "Abstract": "Despite the demonstrated efficacy, safety, and availability of COVID-19 vaccines, efforts in global mass vaccination have been met with widespread scepticism and vaccine hesitancy or refusal. Understanding the reasons for the public’s negative opinions towards COVID-19 vaccination using Twitter may help make new headways in improving vaccine uptake. This study, therefore, examined the prevailing negative sentiments towards COVID-19 vaccination via the analysis of public twitter posts over a 16 month period. Original tweets (in English) from 1 April 2021 to 1 August 2022 were extracted. A bidirectional encoder representations from transformers (BERT)-based model was applied, and only negative sentiments tweets were selected. Topic modelling was used, followed by manual thematic analysis performed iteratively by the study investigators, with independent reviews of the topic labels and themes. A total of 4,448,314 tweets were analysed. The analysis generated six topics and three themes related to the prevailing negative sentiments towards COVID-19 vaccination. The themes could be broadly understood as either emotional reactions to perceived invidious policies or safety and effectiveness concerns related to the COVID-19 vaccines. The themes uncovered in the present infodemiology study fit well into the increasing vaccination model, and they highlight important public conversations to be had and potential avenues for future policy intervention and campaign efforts.",
        "DOI": "10.3390/vaccines10091457",
        "paper_author": "Ng Q.X.",
        "affiliation_name": "Singapore General Hospital",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore",
        "affiliation_id": "60017958",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Time series-based PM<inf>2.5</inf> concentration prediction in Jing-Jin-Ji area using machine learning algorithm models",
        "publication": "Heliyon",
        "citied_by": "8",
        "cover_date": "2022-09-01",
        "Abstract": "Globally all countries encounter air pollution problems along their development path. As a significant indicator of air quality, PM2.5 concentration has long been proven to be affecting the population's death rate. Machine learning algorithms proven to outperform traditional statistical approaches are widely used in air pollution prediction. However research on the model selection discussion and environmental interpretation of model prediction results is still scarce and urgently needed to lead the policy making on air pollution control. Our research compared four types of machine learning algorisms LinearSVR, K-Nearest Neighbor, Lasso regression, Gradient boosting by looking into their performance in predicting PM2.5 concentrations among different cities and seasons. The results show that the machine learning model is able to forecast the next day PM2.5 concentration based on the previous five days' data with better accuracy. The comparative experiments show that based on city level the Gradient Boosting prediction model has better prediction performance with mean absolute error (MAE) of 9 ug/m3 and root mean square error (RMSE) of 10.25–16.76 ug/m3, lower compared with the other three models, and based on season level four models have the best prediction performances in winter time and the worst in summer time. And more importantly the demonstration of models' different performances in each city and each season is of great significance in environmental policy implications.",
        "DOI": "10.1016/j.heliyon.2022.e10691",
        "paper_author": "Ma X.",
        "affiliation_name": "North China University of Water Resources and Electric Power",
        "affiliation_city": "Zhengzhou",
        "affiliation_country": "China",
        "affiliation_id": "60103148",
        "affiliation_state": "Henan"
    },
    {
        "paper_title": "Analysis of changes in geographical factors affecting sales in commercial alleys after COVID-19 using machine learning techniques",
        "publication": "Heliyon",
        "citied_by": "3",
        "cover_date": "2022-09-01",
        "Abstract": "Social restrictions, such as social distancing and self-isolation, imposed owing to the coronavirus disease-19 (COVID-19) pandemic have resulted in a decreased demand of commodities and manufactured products. However, the factors influencing sales in commercial districts in the pre- and post-COVID-19 periods have not yet been fully understood. Thus, this study uses machine learning techniques to identify the changes in important geographical factors among both periods that have affected sales in commercial alleys. It was discovered that, in the post-COVID-19 period, the number of pharmacies, age groups of the working population, average monthly income, and number of families living in apartments priced higher than $600k in the catchment areas had relatively high importance after COVID-19 in the prediction of a high level of sales. Moreover, the percentage of deciduous forests appeared to be a important factor in the post-COVID-19 period. As the average monthly income and worker population in their 60s and numbers of pharmacies and banks increased after the pandemic, sales in commercial alleys also increased. The survival of commercial alleys has become a critical social problem in the post-COVID-19 era; therefore, this study is meaningful in that it suggests a policy direction that could contribute to the revitalization of commercial alley sales in the future and boost the local economy.",
        "DOI": "10.1016/j.heliyon.2022.e10708",
        "paper_author": "Kangjae L.",
        "affiliation_name": "Kyungpook National University (KNU)",
        "affiliation_city": "Daegu",
        "affiliation_country": "South Korea",
        "affiliation_id": "60012704",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Estimating the effect of donor sex on red blood cell transfused patient mortality: A retrospective cohort study using a targeted learning and emulated trials-based approach",
        "publication": "eClinicalMedicine",
        "citied_by": "9",
        "cover_date": "2022-09-01",
        "Abstract": "Background: Observational studies determining the effect of red blood cell (RBC) donor sex on recipient mortality have been inconsistent. Emulating hypothetical randomized target trials using large real-world data and targeted learning may clarify potential adverse effects. Methods: In this retrospective cohort study, a RBC transfusion database from the Capital Region of Denmark comprising more than 900,000 transfusion events defined the observational data. Eligible patients were minimum 18 years, had received a leukocyte-reduced RBC transfusion, and had no history of RBC transfusions within the past year at baseline. The doubly robust targeted maximum likelihood estimation method coupled with ensembled machine learning was used to emulate sex-stratified target trials determining the comparative effectiveness of exclusively transfusing RBC units from either male or female donors. The outcome was all-cause mortality within 28 days of the baseline-transfusion. Estimates were adjusted for the total number of transfusions received on each day k, hospital of transfusion, calendar period, patient age and sex, ABO/RhD blood group of the patient, Charlson comorbidity score, the total number of transfusions received prior to day k, and the number of RBC units received on each day k from donors younger than 40 years of age. Findings: Among 98,167 adult patients who were transfused between Jan. 1, 2008, and Apr. 10, 2018, a total of 90,917 patients (54.6% female) were eligible. For male patients, the 28-day survival was 2.06 percentage points (pp) (95 % confidence interval [CI]: 1.81-2.32, P<0.0001) higher under treatment with RBC units exclusively from male donors compared with exclusively from female donors. In female patients, exclusively transfusing RBC units from either male or female donors increased the 28-day survival with 0.64pp (0.52-0.76, P<0.0001), and 0.62pp (0.49-0.75, P<0.0001) compared with the current practice, respectively. No evidence of a sex-specific donor effect was found for female patients (0.02pp [-0.18-0.22]). The sensitivity analyses showed that a large unknown causal bias would have to be present to affect the conclusions. Interpretation: The results suggest that a sex-matched transfusion policy may benefit patients. However, a causal interpretation of the findings relies on the assumption of no unmeasured confounding, treatment consistency, positivity, and minimal model misspecifications. Funding: Novo Nordisk Foundation and the Innovation Fund Denmark.",
        "DOI": "10.1016/j.eclinm.2022.101628",
        "paper_author": "Bruun-Rasmussen P.",
        "affiliation_name": "Rigshospitalet",
        "affiliation_city": "Copenhagen",
        "affiliation_country": "Denmark",
        "affiliation_id": "60006564",
        "affiliation_state": "Hovedstaden"
    },
    {
        "paper_title": "Comparison of Reinforcement and Imitation Learning algorithms in autonomous sailboat Digital Twins",
        "publication": "IEEE Latin America Transactions",
        "citied_by": "5",
        "cover_date": "2022-09-01",
        "Abstract": "This project aims to study the performance of two reinforcement machine learning algorithms, namely the Proximal Policy Optimization and Soft Actor Critic, in the simulation of autonomous sailboats and their response to different wind directions while avoiding obstacles detected by image analysis and following defined target check-points. Also, the effect of the imitation learning algorithms Behavioral Cloning and Generative Adversarial Imitation Learning combined with the first mentioned algorithms is studied. The proposed scenarios consist of areas filled with random static or moving obstacles and with the presence of favorable or crosswinds. The motivation for the project comes from the lack of studies of the mentioned algorithms in autonomous sailboats, issue which the current study tries to address. The Unity platform and ML-Agents machine learning toolkit are used for development and the methodology that guides the project can be similarly applied to other reinforcement learning problems. Through agent training, it is possible to compare the results and observe that the Proximal Policy Optimization obtains better performance within the proposed scenarios, both with and without the support of imitation learning algorithms.",
        "DOI": "10.1109/TLA.2022.9878171",
        "paper_author": "Mexas R.P.",
        "affiliation_name": "Universidade Federal Fluminense",
        "affiliation_city": "Niteroi",
        "affiliation_country": "Brazil",
        "affiliation_id": "60001865",
        "affiliation_state": "RJ"
    },
    {
        "paper_title": "Federated Learning and Its Role in the Privacy Preservation of IoT Devices",
        "publication": "Future Internet",
        "citied_by": "44",
        "cover_date": "2022-09-01",
        "Abstract": "Federated learning (FL) is a cutting-edge artificial intelligence approach. It is a decentralized problem-solving technique that allows users to train using massive data. Unprocessed information is stored in advanced technology by a secret confidentiality service, which incorporates machine learning (ML) training while removing data connections. As researchers in the field promote ML configurations containing a large amount of private data, systems and infrastructure must be developed to improve the effectiveness of advanced learning systems. This study examines FL in-depth, focusing on application and system platforms, mechanisms, real-world applications, and process contexts. FL creates robust classifiers without requiring information disclosure, resulting in highly secure privacy policies and access control privileges. The article begins with an overview of FL. Then, we examine technical data in FL, enabling innovation, contracts, and software. Compared with other review articles, our goal is to provide a more comprehensive explanation of the best procedure systems and authentic FL software to enable scientists to create the best privacy preservation solutions for IoT devices. We also provide an overview of similar scientific papers and a detailed analysis of the significant difficulties encountered in recent publications. Furthermore, we investigate the benefits and drawbacks of FL and highlight comprehensive distribution scenarios to demonstrate how specific FL models could be implemented to achieve the desired results.",
        "DOI": "10.3390/fi14090246",
        "paper_author": "Alam T.",
        "affiliation_name": "Islamic University of Madinah",
        "affiliation_city": "Medina",
        "affiliation_country": "Saudi Arabia",
        "affiliation_id": "60105081",
        "affiliation_state": "Al Madinah al Munawwarah"
    },
    {
        "paper_title": "A machine learning approach to evaluate the state of hypertension care coverage: From 2016 STEPs survey in Iran",
        "publication": "PLoS ONE",
        "citied_by": "5",
        "cover_date": "2022-09-01",
        "Abstract": "Background The increasing burden of hypertension in low- to middle-income countries necessitates the assessment of care coverage to monitor progress and guide future policies. This study uses an ensemble learning approach to evaluate hypertension care coverage in a nationally representative Iranian survey. Methods The data source was the cross-sectional 2016 Iranian STEPwise approach to risk factor surveillance (STEPs). Hypertension was based on blood pressure ≥140/90 mmHg, reported use of anti-hypertensive medications, or a previous hypertension diagnosis. The four steps of care were screening (irrespective of blood pressure value), diagnosis, treatment, and control. The proportion of patients reaching each step was calculated, and a random forest model was used to identify features associated with progression to each step. After model optimization, the six most important variables at each step were considered to demonstrate population-based marginal effects. Results The total number of participants was 30541 (52.3% female, median age: 42 years). Overall, 9420 (30.8%) had hypertension, among which 89.7% had screening, 62.3% received diagnosis, 49.3% were treated, and 7.9% achieved control. The random forest model indicated that younger age, male sex, lower wealth, and being unmarried/divorced were consistently associated with a lower probability of receiving care in different levels. Dyslipidemia was associated with reaching diagnosis and treatment steps; however, patients with other cardiovascular comorbidities were not likely to receive more intensive blood pressure management. Conclusion Hypertension care was mostly missing the treatment and control stages. The random forest model identified features associated with receiving care, indicating opportunities to improve effective coverage.",
        "DOI": "10.1371/journal.pone.0273560",
        "paper_author": "Tavolinejad H.",
        "affiliation_name": "Non-Communicable Diseases Research Center",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran",
        "affiliation_id": "60120846",
        "affiliation_state": "Tehran"
    },
    {
        "paper_title": "Food Security: 3D Dynamic Display and Early Warning Platform Construction and Security Strategy",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "2",
        "cover_date": "2022-09-01",
        "Abstract": "Since it affects a nation’s economy and people’s wellbeing, food security is a crucial national security requirement. In order to realize multi-angle grain data presentation and analysis and achieve the goal of deep mining, we propose a 3D dynamic visualization analysis method of multidimensional agricultural spatial–temporal data based on the self-organizing map. This method realizes the multi-angle display and analysis of grain data and achieves the purpose of deep mining. With the outbreak of COVID-19, the global food security situation is not optimistic, so it is necessary to use the food security early warning system to solve the food security issue. Machine learning has emerged widely in recent years and has been applied in various fields. Therefore, it is an excellent way to solve food security to apply the model in machine learning to construct a food security early warning system. Afterward, a food security early warning platform is developed with a support vector regression (SVR) model to ensure food security. Finally, we analyze China’s medium and long-term food security policy in line with modernization objectives. The experimental results show that the food security early warning platform based on the SVR model from 2007 to 2016 is effective compared with the actual situation every year. Through analyses, we should improve the stability, reliability, and sustainability of food supply, firmly hold the food security initiative, and construct a national food security guarantee system matching the goal of modernization.",
        "DOI": "10.3390/ijerph191811169",
        "paper_author": "Sun N.",
        "affiliation_name": "Harbin Institute of Technology",
        "affiliation_city": "Harbin",
        "affiliation_country": "China",
        "affiliation_id": "60019616",
        "affiliation_state": "Heilongjiang"
    },
    {
        "paper_title": "Unveiling the Relationship between Economic Growth and Equality for Developing Countries",
        "publication": "China and World Economy",
        "citied_by": "5",
        "cover_date": "2022-09-01",
        "Abstract": "This study investigates the relationship between economic growth and inequality by employing the artificial neural network approach. There are many important findings. First, this work reveals the underlying functional form of economic growth and inequality by using three-dimensional diagrams. Second, the findings show that there was an inverted-U relationship between economic growth and inequality. This explains apparent contradictions in research findings in the literature. Third, the optimal level of inequality, which corresponds to the highest level of economic growth, is computed for different economies. Our findings were confirmed by the development processes in many developing countries and also in China in recent years, thereby highlighting the importance of inequality alleviation in promoting further economic growth. These findings enable us to derive pragmatic policy implications for other developing countries at different stages of economic development in achieving sustainable growth with equity.",
        "DOI": "10.1111/cwe.12435",
        "paper_author": "Cheong T.S.",
        "affiliation_name": "The Hang Seng University of Hong Kong",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong",
        "affiliation_id": "60086451",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Does credit rating agency reputation matter in China’s local government bond market?",
        "publication": "PLoS ONE",
        "citied_by": "1",
        "cover_date": "2022-09-01",
        "Abstract": "All issuers in China’s local government bond market, which is nascent but growing rapidly, have the same AAA ratings. However, we provide evidence that the credit rating agency’s reputation can certify differences in ratings’ reliability and further impact bond pricing. On the basis of a sample of 7941 local government bonds issued from 2015 to 2021, results show that risk premium is significantly low for bonds rated by prestigious credit rating agencies, which means that issuers can save borrowing costs. Moreover, local governments regarded as less transparent in fiscal information disclosure enjoy more cost savings for their bonds by hiring more reputable agencies. Regression results are affirmed with the Heckman two-stage model, difference-in-differences regression, and machine learning method to solve the potential endogeneity issue. This paper’s findings contribute to the debate on the credit rating agency’s reputation hypothesis and present three implications. First, investors can rely on the credit rating agency’s reputation to complement credit risk analysis. Second, local government policymakers should implement appropriate policies to reduce debt costs and improve public finance sustainability. Lastly, regulators should considerably focus on the supervision of credit rating agencies, given their substantial impact on bond pricing and the market’s information asymmetry.",
        "DOI": "10.1371/journal.pone.0274828",
        "paper_author": "Xie C.",
        "affiliation_name": "Universiti Malaya",
        "affiliation_city": "Kuala Lumpur",
        "affiliation_country": "Malaysia",
        "affiliation_id": "60029157",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Machine learning can guide food security efforts when primary data are not available",
        "publication": "Nature Food",
        "citied_by": "49",
        "cover_date": "2022-09-01",
        "Abstract": "Estimating how many people are food insecure and where they are is of fundamental importance for governments and humanitarian organizations to make informed and timely decisions on relevant policies and programmes. In this study, we propose a machine learning approach to predict the prevalence of people with insufficient food consumption and of people using crisis or above-crisis food-based coping when primary data are not available. Making use of a unique global dataset, the proposed models can explain up to 81% of the variation in insufficient food consumption and up to 73% of the variation in crisis or above food-based coping levels. We also show that the proposed models can nowcast the food security situation in near real time and propose a method to identify which variables are driving the changes observed in predicted trends—which is key to make predictions serviceable to decision-makers.",
        "DOI": "10.1038/s43016-022-00587-8",
        "paper_author": "Martini G.",
        "affiliation_name": "World Food Program, Italy",
        "affiliation_city": "Rome",
        "affiliation_country": "Italy",
        "affiliation_id": "60077708",
        "affiliation_state": "RM"
    },
    {
        "paper_title": "Analysing privacy policies",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2022-09-01",
        "Abstract": "Privacy statements are legal documents designed to protect the interests of companies and individuals, most commonly in web services and are widely adopted way of informing users how data will be collected, stored, shared. However, the average Internet user either does not read these statements or it takes considerable amount of time and yet it's difficult to fully understand the statement content and meaning. On the other hand, the length and complexity of the privacy statements are an obstacle for regulators to verify that the requirements are applied. Utilization of natural language processing allows document's processing to be automated to some extent and thus to reduce the gap in understanding the content by the end users. A model for automated analyses of privacy policy documents based on natural language processing and machine learning is suggested in the paper. The model comprises a pre-processing stage for data retrieval and fusion of information from different sources as well as data processing stage that utilizes neural network for data classification. The neural network architecture and several groups of model parameters have been experimentally evaluated using a dataset created using five information sources of both publicly available annotated privacy policy documents and self-annotated documents.",
        "DOI": "10.1063/5.0091928",
        "paper_author": "Stoykov K.",
        "affiliation_name": "Technical University of Sofia",
        "affiliation_city": "Sofia",
        "affiliation_country": "Bulgaria",
        "affiliation_id": "60004554",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Reinforcement learning algorithms using for agent behaviour modelling and researching",
        "publication": "AIP Conference Proceedings",
        "citied_by": "1",
        "cover_date": "2022-09-01",
        "Abstract": "Achieving autonomy and rational behaviour, especially when the environment is complex, partially observable, dynamic or unfamiliar, requires the inclusion of machine learning methods. This article presents the latest and most widely used model-based and model-free reinforcement learning algorithms and their applications. In model-based reinforcement learning algorithms, the learning agents have knowledge of their environment, which leads to faster learning and faster building of optimal policies. In model-free reinforcement learning algorithms, agents build optimal policies by interacting with the environment, which makes their training slower. On the other hand, their advantage is that they can work with unfamiliar and dynamically changing environments. Some hybrid reinforcement learning algorithms are also considered.",
        "DOI": "10.1063/5.0090866",
        "paper_author": "Petrova-Dimitrova V.",
        "affiliation_name": "Technical University of Sofia",
        "affiliation_city": "Sofia",
        "affiliation_country": "Bulgaria",
        "affiliation_id": "60004554",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Reinforcement Learning: Theory and Applications in HEMS",
        "publication": "Energies",
        "citied_by": "17",
        "cover_date": "2022-09-01",
        "Abstract": "The steep rise in reinforcement learning (RL) in various applications in energy as well as the penetration of home automation in recent years are the motivation for this article. It surveys the use of RL in various home energy management system (HEMS) applications. There is a focus on deep neural network (DNN) models in RL. The article provides an overview of reinforcement learning. This is followed with discussions on state-of-the-art methods for value, policy, and actor–critic methods in deep reinforcement learning (DRL). In order to make the published literature in reinforcement learning more accessible to the HEMS community, verbal descriptions are accompanied with explanatory figures as well as mathematical expressions using standard machine learning terminology. Next, a detailed survey of how reinforcement learning is used in different HEMS domains is described. The survey also considers what kind of reinforcement learning algorithms are used in each HEMS application. It suggests that research in this direction is still in its infancy. Lastly, the article proposes four performance metrics to evaluate RL methods.",
        "DOI": "10.3390/en15176392",
        "paper_author": "Al-Ani O.",
        "affiliation_name": "Carl R. Ice College of Engineering",
        "affiliation_city": "Manhattan",
        "affiliation_country": "United States",
        "affiliation_id": "60146075",
        "affiliation_state": "KS"
    },
    {
        "paper_title": "Machine Learning and Sustainable Mobility: The Case of the University of Foggia (Italy)",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "2",
        "cover_date": "2022-09-01",
        "Abstract": "Thanks to the development of increasingly sophisticated machine-learning techniques, it is possible to improve predictions of a particular phenomenon. In this paper, after analyzing data relating to the mobility habits of University of Foggia (UniFG) community members, we apply logistic regression and cross validation to determine the information that is missing in the dataset (so-called imputation process). Our goal is to make it possible to obtain the missing information that can be useful for calculating sustainability indicators and that allow the UniFG Rectorate to improve its sustainable mobility policies by encouraging methods that are as appropriate as possible to the users’ needs.",
        "DOI": "10.3390/app12178774",
        "paper_author": "Cappelletti G.M.",
        "affiliation_name": "Università degli Studi di Foggia",
        "affiliation_city": "Foggia",
        "affiliation_country": "Italy",
        "affiliation_id": "60022956",
        "affiliation_state": "FG"
    },
    {
        "paper_title": "Remote Sensing for Land Administration 2.0",
        "publication": "Remote Sensing",
        "citied_by": "4",
        "cover_date": "2022-09-01",
        "Abstract": "Contemporary land administration (LA) systems incorporate the concepts of cadastre and land registration. Conceptually, LA is part of a global land management paradigm incorporating LA functions such as land value, land tenure, land development, and land use. The implementation of land-related policies integrated with well-maintained spatial information reflects the aim set by the United Nations to deliver tenure security for all (Sustainable Development Goal target 1.4, amongst many others). Innovative methods for data acquisition, processing, and maintaining spatial information are needed in response to the global challenges of urbanization and complex urban infrastructure. Current technological developments in remote sensing and geo-spatial information science provide enormous opportunities in this respect. Over the past decade, the increasing usage of unmanned aerial vehicles (UAVs), satellite and airborne-based acquisitions, as well as active remote sensing sensors such as LiDAR, resulted in high spatial, spectral, radiometric, and temporal resolution data. Moreover, significant progress has also been achieved in automatic image orientation, surface reconstruction, scene analysis, change detection, classification, and automatic feature extraction with the help of artificial intelligence, spatial statistics, and machine learning. These technology developments, applied to LA, are now being actively demonstrated, piloted, and scaled. This Special Issue hosts papers focusing on the usage and integration of emerging remote sensing techniques and their potential contribution to the LA domain.",
        "DOI": "10.3390/rs14174359",
        "paper_author": "Koeva M.",
        "affiliation_name": "Universiteit Twente",
        "affiliation_city": "Enschede",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60020599",
        "affiliation_state": "Overijssel"
    },
    {
        "paper_title": "Deep Adversarial Imitation Reinforcement Learning for QoS-Aware Cloud Job Scheduling",
        "publication": "IEEE Systems Journal",
        "citied_by": "38",
        "cover_date": "2022-09-01",
        "Abstract": "Although cloud computing is one of the promising technologies for online business services, how to schedule real-time cloud jobs with high quality of service (QoS) is still challenging current techniques. With the advancing of machine learning, deep reinforcement learning (DRL) has demonstrated its outstanding capability in dispatching time-sensitive tasks. However, the reinforced rewards in DRL are typically unavailable until the completion of the scheduling for all the jobs. Considering the fact that the trajectory of jobs in cloud is always long, current DRL-based solutions will meet challenges in finding the trajectories with high rewards, and thus would have issues such that the finally trained scheduling policy is suboptimal. To improve the problem, in this article, we propose a more advanced approach called a deep adversarial imitation reinforcement learning (AIRL) framework for scheduling time-sensitive cloud jobs. Specifically, we focus on scheduling user requests in a way to maximize job successful rate along with a significant reduction on job response time. We present the detailed design of our method, and our experimental results demonstrate that AIRL can generally outperform the existing cloud job scheduling approaches, including the DRL-based method, in the presence of different real-time workload and computing resource configurations.",
        "DOI": "10.1109/JSYST.2021.3122126",
        "paper_author": "Huang Y.",
        "affiliation_name": "North China Electric Power University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60021227",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Visual Pretraining via Contrastive Predictive Model for Pixel-Based Reinforcement Learning",
        "publication": "Sensors",
        "citied_by": "3",
        "cover_date": "2022-09-01",
        "Abstract": "In an attempt to overcome the limitations of reward-driven representation learning in vision-based reinforcement learning (RL), an unsupervised learning framework referred to as the visual pretraining via contrastive predictive model (VPCPM) is proposed to learn the representations detached from the policy learning. Our method enables the convolutional encoder to perceive the underlying dynamics through a pair of forward and inverse models under the supervision of the contrastive loss, thus resulting in better representations. In experiments with a diverse set of vision control tasks, by initializing the encoders with VPCPM, the performance of state-of-the-art vision-based RL algorithms is significantly boosted, with 44% and 10% improvement for RAD and DrQ at 100 steps, respectively. In comparison to the prior unsupervised methods, the performance of VPCPM matches or outperforms all the baselines. We further demonstrate that the learned representations successfully generalize to the new tasks that share a similar observation and action space.",
        "DOI": "10.3390/s22176504",
        "paper_author": "Luu T.M.",
        "affiliation_name": "Korea Advanced Institute of Science and Technology",
        "affiliation_city": "Daejeon",
        "affiliation_country": "South Korea",
        "affiliation_id": "60032144",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Energy System 4.0: Digitalization of the Energy Sector with Inclination towards Sustainability",
        "publication": "Sensors",
        "citied_by": "87",
        "cover_date": "2022-09-01",
        "Abstract": "The United Nations’ sustainable development goals have emphasized implementing sustainability to ensure environmental security for the future. Affordable energy, clean energy, and innovation in infrastructure are the relevant sustainable development goals that are applied to the energy sector. At present, digital technologies have a significant capability to realize the target of sustainability in energy. With this motivation, the study aims to discuss the significance of different digital technologies such as the Internet of Things (IoT), artificial intelligence (AI), edge computing, blockchain, and big data and their implementation in the different stages of energy such as generation, distribution, transmission, smart grid, and energy trading. The study also discusses the different architecture that has been implemented by previous studies for smart grid computing. Additionally, we addressed IoT-based microgrids, IoT services in electrical equipment, and blockchain-based energy trading. Finally, the article discusses the challenges and recommendations for the effective implementation of digital technologies in the energy sector for meeting sustainability. Big data for energy analytics, digital twins in smart grid modeling, virtual power plants with Metaverse, and green IoT are the major vital recommendations that are discussed in this study for future enhancement.",
        "DOI": "10.3390/s22176619",
        "paper_author": "Singh R.",
        "affiliation_name": "Uttaranchal University",
        "affiliation_city": "Dehradun",
        "affiliation_country": "India",
        "affiliation_id": "60115862",
        "affiliation_state": "UK"
    },
    {
        "paper_title": "Village level identification of sugarcane in Sangali, Maharashtra using open source dat",
        "publication": "Journal of Agrometeorology",
        "citied_by": "6",
        "cover_date": "2022-09-01",
        "Abstract": "Agriculture and crop monitoring are very important for an agrarian country like India. This study is done in June Khed village in the Sangli district of Maharashtra, India to assessing the efficiency of an open source cloud-based remote sensing platform Google Earth Engine (GEE), in the village-scale identification of sugarcane. The ground-truth data was collected and the efficiency of Landsat-8 and Sentinel-2 satellite data was assessed in driving GEE’s inbuilt Machine Learning classifiers: Classification and Regression Tree (CART), Support Vector Machine (SVM) and Random Forest (RF). Results were validated with the ground truth data and the official data. Of the methods used, SVM outperformed RF and CART with the lowest relative deviation (+9.2%), highest F1-score (0.8) and overall accuracy (78%), using the Sentinel-2 data. Results also indicated the in-situ use of observation data with high spatio-temporal resolution data. The validated model was then up-scaled for the Walwa Taluka level, to map sugarcane area that can be used for further agriculture tasks such as crop monitoring and yield prediction, leading to better management of crop and better formulating of sugarcane farmer policy.",
        "DOI": "10.54386/jam.v24i3.1688",
        "paper_author": "Lonare A.",
        "affiliation_name": "Indian Institute of Technology Bombay",
        "affiliation_city": "Mumbai",
        "affiliation_country": "India",
        "affiliation_id": "60014153",
        "affiliation_state": "MH"
    },
    {
        "paper_title": "Comparison of weather-based wheat yield forecasting models for different districts of Uttarakhand using statistical and machine learning techniques",
        "publication": "Journal of Agrometeorology",
        "citied_by": "12",
        "cover_date": "2022-09-01",
        "Abstract": "The prediction of crop yield before harvest is crucial for facilitating the formulation and implementation of policies about food safety, transportation cost, and import-export, storage and marketing of agro-products. The weather plays a crucial role in crop growth and development. Therefore, models using weather variables can provide reliable forecasts for crop yield and choosing the right model for crop production forecasts can be difficult. Therefore in the present study, an attempt was made to find the best model for wheat yield forecast by using five different techniques viz. Stepwise Multiple Linear Regression (SMLR), Artificial Neural Network (ANN), Least Absolute Shrinkage and Selection Operator (LASSO), Elastic Net (ELNET) and Ridge regression. Historical wheat yield data (taken from the Directorate of Economics and Statistics, Ministry of Agriculture and Farmers Welfare) and weather data of past 18-20 years were collected for seven different districts of Uttarakhand. Analysis was carried out by fixing 80% of the data for calibration and remaining dataset for validation. The present study concluded that the performance of ANN was good for crop yield forecasting as compared to the other models based on the value of RMSE (0.005-0.474) and nRMSE (0.166-26.171).",
        "DOI": "10.54386/jam.v24i3.1571",
        "paper_author": "Setiya P.",
        "affiliation_name": "Govind Ballabh Pant University of Agriculture and Technology",
        "affiliation_city": "Pantnagar",
        "affiliation_country": "India",
        "affiliation_id": "60012351",
        "affiliation_state": "UK"
    },
    {
        "paper_title": "Alzheimer's disease risk biomarkers: progress and challenges",
        "publication": "The Lancet Healthy Longevity",
        "citied_by": "7",
        "cover_date": "2022-09-01",
        "Abstract": "NA",
        "DOI": "10.1016/S2666-7568(22)00191-X",
        "paper_author": "Adkins-Jackson P.B.",
        "affiliation_name": "Mailman School of Public Health",
        "affiliation_city": "New York",
        "affiliation_country": "United States",
        "affiliation_id": "60012769",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Deep reinforcement learning-based balancing and sequencing approach for mixed model assembly lines",
        "publication": "IET Collaborative Intelligent Manufacturing",
        "citied_by": "4",
        "cover_date": "2022-09-01",
        "Abstract": "A multi-agent iterative optimisation method based on deep reinforcement learning is proposed for the balancing and sequencing problem in mixed model assembly lines. Based on the Markov decision process model for balancing and sequencing, a balancing agent using a deep deterministic policy gradient algorithm, a sequencing agent using an Actor–Critic algorithm, as well as an iterative interaction mechanism between these agents' output solutions are designed for realising the global optimisation of mixed model assembly lines. The exchange of solution information including assembly time and station workload in the iterative interaction realises the coordination of the worker assignment policy at the balancing stage and the production arrangement policy at the sequencing stage for the minimisation of work overload and idle time at stations. Through the comparative experiments with heuristic rules, genetic algorithms, and the original deep reinforcement learning algorithm, the effectiveness of the proposed method is demonstrated and discussed for small-scale instances as well as large-scale ones.",
        "DOI": "10.1049/cim2.12061",
        "paper_author": "Lv Y.",
        "affiliation_name": "Donghua University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60010953",
        "affiliation_state": "Shanghai"
    },
    {
        "paper_title": "Public Discourse and Sentiment Toward Dementia on Chinese Social Media: Machine Learning Analysis of Weibo Posts",
        "publication": "Journal of Medical Internet Research",
        "citied_by": "12",
        "cover_date": "2022-09-01",
        "Abstract": "Background: Dementia is a global public health priority due to rapid growth of the aging population. As China has the world’s largest population with dementia, this debilitating disease has created tremendous challenges for older adults, family caregivers, and health care systems on the mainland nationwide. However, public awareness and knowledge of the disease remain limited in Chinese society. Objective: This study examines online public discourse and sentiment toward dementia among the Chinese public on a leading Chinese social media platform Weibo. Specifically, this study aims to (1) assess and examine public discourse and sentiment toward dementia among the Chinese public, (2) determine the extent to which dementia-related discourse and sentiment vary among different user groups (ie, government, journalists/news media, scientists/experts, and the general public), and (3) characterize temporal trends in public discourse and sentiment toward dementia among different user groups in China over the past decade. Methods: In total, 983,039 original dementia-related posts published by 347,599 unique users between 2010 and 2021, together with their user information, were analyzed. Machine learning analytical techniques, including topic modeling, sentiment analysis, and semantic network analyses, were used to identify salient themes/topics and their variations across different user groups (ie, government, journalists/news media, scientists/experts, and the general public). Results: Topic modeling results revealed that symptoms, prevention, and social support are the most prevalent dementia-related themes on Weibo. Posts about dementia policy/advocacy have been increasing in volume since 2018. Raising awareness is the least discussed topic over time. Sentiment analysis indicated that Weibo users generally attach negative attitudes/emotions to dementia, with the general public holding a more negative attitude than other user groups. Conclusions: Overall, dementia has received greater public attention on social media since 2018. In particular, discussions related to dementia advocacy and policy are gaining momentum in China. However, disparaging language is still used to describe dementia in China; therefore, a nationwide initiative is needed to alter the public discourse on dementia. The results contribute to previous research by providing a macrolevel understanding of the Chinese public’s discourse and attitudes toward dementia, which is essential for building national education and policy initiatives to create a dementia-friendly society. Our findings indicate that dementia is associated with negative sentiments, and symptoms and prevention dominate public discourse. The development",
        "DOI": "10.2196/39805",
        "paper_author": "Kong D.",
        "affiliation_name": "Chinese University of Hong Kong",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong",
        "affiliation_id": "60002798",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Economic development, weather shocks and child marriage in South Asia: A machine learning approach",
        "publication": "PLoS ONE",
        "citied_by": "2",
        "cover_date": "2022-09-01",
        "Abstract": "Globally, 21 percent of young women are married before their 18th birthday. Despite some progress in addressing child marriage, it remains a widespread practice, in particular in South Asia. While household predictors of child marriage have been studied extensively in the literature, the evidence base on macro-economic factors contributing to child marriage and models that predict where child marriage cases are most likely to occur remains limited. In this paper we aim to fill this gap and explore region-level indicators to predict the persistence of child marriage in four countries in South Asia, namely Bangladesh, India, Nepal and Pakistan. We apply machine learning techniques to child marriage data and develop a prediction model that relies largely on regional and local inputs such as droughts, floods, population growth and nightlight data to model the incidence of child marriages. We find that our gradient boosting model is able to identify a large proportion of the true child marriage cases and correctly classifies 77% of the true marriage cases, with a higher accuracy in Bangladesh (92% of the cases) and a lower accuracy in Nepal (70% of cases). In addition, all countries contain in their top 10 variables for classification nighttime light growth, a shock index of drought over the previous and the last two years and the regional level of education, suggesting that income shocks, regional economic activity and regional education levels play a significant role in predicting child marriage. Given the accuracy of the model to predict child marriage, our model is a valuable tool to support policy design in countries where household-level data remains limited.",
        "DOI": "10.1371/journal.pone.0271373",
        "paper_author": "Dietrich S.",
        "affiliation_name": "Universiteit Maastricht",
        "affiliation_city": "Maastricht",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60018869",
        "affiliation_state": "Limburg"
    },
    {
        "paper_title": "Estimation of Ebola's spillover infection exposure in Sierra Leone based on sociodemographic and economic factors",
        "publication": "PLoS ONE",
        "citied_by": "2",
        "cover_date": "2022-09-01",
        "Abstract": "Zoonotic diseases spread through pathogens-infected animal carriers. In the case of Ebola Virus Disease (EVD), evidence supports that the main carriers are fruit bats and non-human primates. Further, EVD spread is a multi-factorial problem that depends on sociodemographic and economic (SDE) factors. Here we inquire into this phenomenon and aim at determining, quantitatively, the Ebola spillover infection exposure map and try to link it to SDE factors. To that end, we designed and conducted a survey in Sierra Leone and implement a pipeline to analyze data using regression and machine learning techniques. Our methodology is able (1) to identify the features that are best predictors of an individual's tendency to partake in behaviors that can expose them to Ebola infection, (2) to develop a predictive model about the spillover risk statistics that can be calibrated for different regions and future times, and (3) to compute a spillover exposure map for Sierra Leone. Our results and conclusions are relevant to identify the regions in Sierra Leone at risk of EVD spillover and, consequently, to design and implement policies for an effective deployment of resources (e.g., drug supplies) and other preventative measures (e.g., educational campaigns).",
        "DOI": "10.1371/journal.pone.0271886",
        "paper_author": "Mursel S.",
        "affiliation_name": "P.C. Rossin College of Engineering &amp; Applied Science",
        "affiliation_city": "Bethlehem",
        "affiliation_country": "United States",
        "affiliation_id": "60146240",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "Spatiotemporal dynamics of global population and heat exposure (2020-2100): based on improved SSP-consistent population projections",
        "publication": "Environmental Research Letters",
        "citied_by": "31",
        "cover_date": "2022-09-01",
        "Abstract": "To address future environmental change and consequent social vulnerability, a better understanding of future population (FPOP) dynamics is critical. In this regard, notable progress has been made in producing FPOP projections that are consistent with the Shared Socioeconomic Pathways (SSPs) at low resolutions for the globe and high resolutions for specific regions. Building on existing endeavors, here we contribute a new set of 1 km SSP-consistent global population projections (FPOP in short for the dataset) under a machine learning framework. Our approach incorporates a recently available SSP-consistent global built-up land dataset under the Coupled Model Intercomparison Project 6, with the aim to address the misestimation of future built-up land dynamics underlying existing datasets of future global population projections. We show that the overall accuracy of our FPOP outperforms five existing datasets at multiple scales and especially in densely-populated areas (e.g. cities and towns). Followingly, FPOP-based assessments of future global population dynamics suggest a similar trend by population density and a spatial Matthew effect of regional population centralization. Furthermore, FPOP-based estimates of global heat exposure are around 300 billion person-days in 2020 under four SSP-Representative Concentration Pathway (RCPs), which by 2100 could increase to as low as 516 billion person-days under SSP5-RCP4.5 and as high as 1626 billion person-days under SSP3-RCP8.5—with Asia and Africa contributing 64%-68% and 21%-25%, respectively. While our results shed lights on proactive policy interventions for addressing future global heat hazard, FPOP will enable future-oriented assessments of a wide range of environmental hazards, e.g. hurricanes, droughts, and flooding.",
        "DOI": "10.1088/1748-9326/ac8755",
        "paper_author": "Li M.",
        "affiliation_name": "East China Normal University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60021200",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Multi-tier archetypes to characterise British landscapes, farmland and farming practices",
        "publication": "Environmental Research Letters",
        "citied_by": "12",
        "cover_date": "2022-09-01",
        "Abstract": "Due to rising demand for both food and environmental services, agriculture is increasingly required to deliver multiple outcomes. Characterising differences, across agricultural landscapes, via the identification of broad archetypal groupings, is an important step in exploring spatial patterns in the capacity of land to deliver these potentially competing functions. Creating characterisations at multiple levels, for landscape and farm management, can allow policy-makers and land managers to harmonise delivery of ecosystem services at different intervention scales. This can identify ways to increase the complementarity of public goods and the sustainability of farmed landscapes. We used data-driven machine learning to create landscape and agricultural management archetypes (1 km resolution) at three levels, defined by opportunities for adaptation. Tier 1 archetypes quantify broad differences in soil, land cover and population across Great Britain, which cannot be readily influenced by the actions of land managers; Tier 2 archetypes capture more nuanced variations within farmland-dominated landscapes of Great Britain, over which land managers may have some degree of influence. Tier 3 archetypes are built at national levels for England and Wales and focus on socioeconomic and agro-ecological characteristics within farmland-dominated landscapes, characterising differences in farm management. By using a non-nested hierarchy, we identified which types of management are restricted to certain landscape settings, and which are applicable across multiple landscape contexts. Understanding variation within and between agricultural landscapes and farming practices has implications for planning environmental sustainability and food security. It can also aid understanding of the scale at which interventions could be most effective, from incentivising changes in farmer behaviour to policy drivers of large-scale land use change.",
        "DOI": "10.1088/1748-9326/ac810e",
        "paper_author": "Goodwin C.E.D.",
        "affiliation_name": "UK Centre for Ecology &amp; Hydrology",
        "affiliation_city": "Wallingford",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60004708",
        "affiliation_state": "Oxfordshire"
    },
    {
        "paper_title": "A model-based hybrid soft actor-critic deep reinforcement learning algorithm for optimal ventilator settings",
        "publication": "Information Sciences",
        "citied_by": "21",
        "cover_date": "2022-09-01",
        "Abstract": "A ventilator is a device that mechanically assists in pumping air into the lungs, which is a life-saving supportive therapy in an intensive care unit (ICU). In clinical scenarios, each patient has unique physiological circumstances and specific respiratory diseases, thus requiring individualized ventilator settings. Long-term supervision by experienced clinicians is essential to perform the task of precisely adjusting ventilator parameters and making timely modifications. Moreover, a tiny clinical error can result in severe lung injury, induce multi-system organ dysfunction, and increase mortality. To reduce the workload of clinicians and prevent medical errors, machine learning (ML), or more specifically, reinforcement learning (RL) methods, have been developed to automatically adjust the ventilator's parameters and select optimal strategies. However, the ventilator settings contain both continuous (e.g., frequency) and discrete parameters (e.g., ventilation mode), making it challenging for conventional RL-based approaches to handle such problems. Meanwhile, it is necessary to develop models with high data efficiency to overcome medical data insufficiency. In this paper, we propose a model-based hybrid soft actor-critic (MHSAC) algorithm that is developed based on the classic soft actor-critic (SAC) and model-based policy optimization (MBPO) framework. This algorithm can learn both continuous and discrete policies according to the current and predictive state of patient's physiological information with high data efficiency. Results reveal that our proposed model significantly outperforms the baseline models, achieving superior efficiency and high accuracy in the OpenAI Gym simulation environment. Our proposed model is capable of resolving mixed action space problems, enhancing data efficiency, and accelerating convergence, which can generate practical optimal ventilator settings, minimize possible medical errors, and provide clinical decision support.",
        "DOI": "10.1016/j.ins.2022.08.028",
        "paper_author": "Chen S.",
        "affiliation_name": "Shanghai University of Engineering Science",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60032504",
        "affiliation_state": "Shanghai"
    },
    {
        "paper_title": "When Effects Cannot be Estimated: Redefining Estimands to Understand the Effects of Naloxone Access Laws",
        "publication": "Epidemiology",
        "citied_by": "20",
        "cover_date": "2022-09-01",
        "Abstract": "Violations of the positivity assumption (also called the common support condition) challenge health policy research and can result in significant bias, large variance, and invalid inference. We define positivity in the single- and multiple-timepoint (i.e., longitudinal) health policy evaluation setting, and discuss real-world threats to positivity. We show empirical evidence of the practical positivity violations that can result when attempting to estimate the effects of health policies (in this case, Naloxone Access Laws). In such scenarios, an alternative is to estimate the effect of a shift in law enactment (e.g., the effect if enactment had been delayed by some number of years). Such an effect corresponds to what is called a modified treatment policy, and dramatically weakens the required positivity assumption, thereby offering a means to estimate policy effects even in scenarios with serious positivity problems. We apply the approach to define and estimate the longitudinal effects of Naloxone Access Laws on opioid overdose rates.",
        "DOI": "10.1097/EDE.0000000000001502",
        "paper_author": "Rudolph K.E.",
        "affiliation_name": "Mailman School of Public Health",
        "affiliation_city": "New York",
        "affiliation_country": "United States",
        "affiliation_id": "60012769",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Leveraging multidimensional features for policy opinion sentiment prediction",
        "publication": "Information Sciences",
        "citied_by": "4",
        "cover_date": "2022-09-01",
        "Abstract": "Previous online policy opinion analyses based on social media data have focused on topic detection and sentiment classification of policy opinion after a given period following policy implementation. These approaches are limited and inefficient because they provide no opportunity to change citizens’ opinions once they have been formed. Furthermore, incorporating auxiliary information to enrich semantic representations is vital and challenging due to limited texts, and a lack of both semantic information and strict syntactic structure. Therefore, we propose a novel framework to extract and integrate multidimensional features from user-related and policy-related social media information and predict policy comment polarity in the policy release phase. First, we construct four machine learning models for model-induced features to capture topic-related and opinion-related features and identify the policy-opinion nexus. In addition, we integrate basic and behavioral user features. Then, we leverage multidimensional features to construct a stacked learning model for predicting the policy opinion. Finally, we conduct experiments on 20 policy comment datasets to demonstrate that our prediction framework can effectively predict public opinion about a policy once it is released. Our model provides key insights into policy opinions in advance and can enable policymakers to engage in better policy communication before opinion formation.",
        "DOI": "10.1016/j.ins.2022.08.004",
        "paper_author": "Hou W.",
        "affiliation_name": "Jilin University",
        "affiliation_city": "Changchun",
        "affiliation_country": "China",
        "affiliation_id": "60007711",
        "affiliation_state": "Jilin"
    },
    {
        "paper_title": "Quantifying the social impacts of the London Night Tube with a double/debiased machine learning based difference-in-differences approach",
        "publication": "Transportation Research Part A: Policy and Practice",
        "citied_by": "20",
        "cover_date": "2022-09-01",
        "Abstract": "There is a worldwide trend toward a growing number of people involved in various night-time activities. The night-time public transport service is of central importance for the urban night-time mobility. In London, the Night Tube service was launched in 2016 to meet the constantly growing night-time travel demand and support London's night-time economy. Yet limited empirical evidence on the ex-post impacts of the London Night Tube has been provided. In this study, we conduct a causal analysis on such impacts using a double/debiased machine learning based difference-in-differences approach. Specifically, we quantify the impacts of the Night Tube on London's night-time economy, house prices, road crashes and related casualties, and crimes. We further investigate the spatial variations in such impacts. Our results indicate a rise in house prices associated with the announcement and the implementation of the service. The number of night-time workplaces showed a limited response. Regarding the safety dimension, we find that the Night Tube service led to a small reduction in the frequency of road crashes but a substantial reduction in crash-related casualties. However, the crime rate in areas served by the Night Tube was increased, especially for the following two categories, robbery of personal property and violence against the person. Moreover, the impact on the crime rate is found to be larger in the inner London area. These findings provide practical implications for urban planners and policy makers, and reveal the need for monitoring the social impacts of the Night Tube service from a long-term perspective.",
        "DOI": "10.1016/j.tra.2022.07.015",
        "paper_author": "Zhang Y.",
        "affiliation_name": "Southeast University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60005244",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "Using Machine Learning to Identify Heterogeneous Impacts of Agri-Environment Schemes in the EU: A Case Study",
        "publication": "European Review of Agricultural Economics",
        "citied_by": "23",
        "cover_date": "2022-09-01",
        "Abstract": "Legislators in the European Union have long been concerned with the environmental impact of farming activities and introduced so-called agri-environment schemes (AES) to mitigate adverse environmental effects and foster desirable ecosystem services in agriculture. This study combines economic theory with a novel machine learning method to identify the environmental effectiveness of AES at the farm level. We develop a set of more than 130 contextual predictors to assess the individual impact of participating in AES. Results from our empirical application for Southeast Germany suggest the existence of heterogeneous, but limited effects of agri-environment measures in several environmental dimensions such as climate change mitigation, clean water and soil health. By making use of Shapley values, we demonstrate the importance of considering the individual farming context in agricultural policy evaluation and provide important insights into the improved targeting of AES along several domains.",
        "DOI": "10.1093/erae/jbab057",
        "paper_author": "Stetter C.",
        "affiliation_name": "Technische Universität München",
        "affiliation_city": "Munich",
        "affiliation_country": "Germany",
        "affiliation_id": "60019722",
        "affiliation_state": "Bayern"
    },
    {
        "paper_title": "Gender differences in serious police misconduct: A machine-learning analysis of the New York Police Department (NYPD)",
        "publication": "Journal of Criminal Justice",
        "citied_by": "9",
        "cover_date": "2022-09-01",
        "Abstract": "Purpose: Despite a considerable body of research on police misconduct, findings have been mixed, with little consensus regarding its causes and best practices for prevention. Emerging research has focused on the role of gender in understanding and preventing misconduct. The current study examines the extent to which the features associated with serious misconduct differ between male and female officers. Methods: Using a unique complaint dataset from the NYPD, we apply a sequence of machine learning analytics to consider if it is possible to predict serious misconduct among either group, and whether key predictors differ between groups. Results: The results show that it was possible to predict serious misconduct among each group with considerable confidence, while there were notable differences in prevalence, and type of misconduct between sexes. Conclusions: Findings hold important implications for policy, prevention and analytical approaches to police misconduct.",
        "DOI": "10.1016/j.jcrimjus.2022.101976",
        "paper_author": "Cubitt T.I.C.",
        "affiliation_name": "University of Tasmania",
        "affiliation_city": "Hobart",
        "affiliation_country": "Australia",
        "affiliation_id": "60015356",
        "affiliation_state": "TS"
    },
    {
        "paper_title": "Dissecting drivers of immune activation in chronic HIV-1 infection",
        "publication": "eBioMedicine",
        "citied_by": "11",
        "cover_date": "2022-09-01",
        "Abstract": "Background: Immune activation is a significant contributor to HIV pathogenesis and disease progression. In virally-suppressed individuals on ART, low-level immune activation has been linked to several non-infectious comorbid diseases. However, studies have not been systematically performed in sub-Saharan Africa and thus the impact of demographics, ART and regional endemic co-infections on immune activation is not known. We therefore comprehensively evaluated in a large multinational African cohort markers for immune activation and its distribution in various settings. Methods: 2747 specimens from 2240 people living with HIV (PLWH) and 477 without HIV from the observational African Cohort Study (AFRICOS) were analyzed for 13 immune parameters. Samples were collected along with medical history, sociodemographic and comorbidity data at 12 HIV clinics across 5 programs in Uganda, Kenya, Tanzania and Nigeria. Data were analyzed with univariate and multivariate methods such as random forests and principal component analysis. Findings: Immune activation was markedly different between PLWH with detectable viral loads, and individuals without HIV across sites. Among viremic PLWH, we found that all immune parameters were significantly correlated with viral load except for IFN-α. The overall inflammatory profile was distinct between men and women living with HIV, in individuals off ART and with HIV viremia. We observed stronger differences in the immune activation profile with increasing viremia. Using machine learning methods, we found that geographic differences contributed to unique inflammatory profiles. We also found that among PLWH, age and the presence of infectious and/or noninfectious comorbidities showed distinct inflammatory patterns, and biomarkers may be used to predict the presence of some comorbidities. Interpretation: Our findings show that chronic immune activation in HIV-1 infection is influenced by HIV viral load, sex, age, region and ART use. These predictors, as well as associations among some biomarkers and coinfections, influence biomarkers associated with noncommunicable diseases. Funding: This work was supported by the President's Emergency Plan for AIDS Relief via a cooperative agreement between the Henry M. Jackson Foundation for the Advancement of Military Medicine, Inc., and the U.S. Department of Defense [W81XWH-11-2-0174, W81XWH-18-2-0040]. The investigators have adhered to the policies for protection of human subjects as prescribed in AR 70–25. This article was prepared while Michael A. Eller was employed at Henry M. Jackson Foundation for the Advancement of Military Medicine for the U.S. Military HIV Research Program. The views expressed are those of the authors and should not be construed to represent the positions of the US Army or the Department of Defense. The opinions expressed in this article are the author's own, and do not reflect the view of the National Institutes of Health, the U.S. Department of Health and Human Services, or the U.S. government.",
        "DOI": "10.1016/j.ebiom.2022.104182",
        "paper_author": "Streeck H.",
        "affiliation_name": "Medizinischen Fakultät der Universität Bonn",
        "affiliation_city": "Bonn",
        "affiliation_country": "Germany",
        "affiliation_id": "60243126",
        "affiliation_state": "Nordrhein-Westfalen"
    },
    {
        "paper_title": "Partially autoregressive machine learning: Development and testing of methods to predict United States Air Force retention",
        "publication": "Computers and Industrial Engineering",
        "citied_by": "0",
        "cover_date": "2022-09-01",
        "Abstract": "Establishing effective personnel management policies in the United States Air Force (USAF) requires methods to predict the number of personnel remaining in the USAF for different lengths of time in the future. Defined as the Personnel Retention Problem (PRP), determining this type of aggregate survival rate is a time series regression problem that shares many characteristics with binary classification problems. The limitations of this particular structure are particularly difficult to overcome for problems with limited data like the USAF PRP. We develop and test several machine learning models to produce improved retention predictions compared to the USAF's current Kaplan Meier model. In addition to traditional random forest models and feedforward neural networks, we propose the inclusion of a partially autoregressive feature to extend the benefits of low-capacity autoregressive techniques to higher-capacity machine learning techniques. We present a Partially Autoregressive Neural Network (PARNet) and a Partially Autoregressive Random Forest (PARFor) and test the performance of each technique across a range of hyperparameter values. We select the superlative model using a validation dataset, compare results to the existing benchmark model, and find a 62.8% reduction in aggregate prediction error for the baseline neural network and 34.8% reduction for the PARNet.",
        "DOI": "10.1016/j.cie.2022.108424",
        "paper_author": "Hoecherl J.C.",
        "affiliation_name": "AFIT Graduate School of Engineering and Management",
        "affiliation_city": "Dayton",
        "affiliation_country": "United States",
        "affiliation_id": "60279872",
        "affiliation_state": "OH"
    },
    {
        "paper_title": "A Loss of Decency – and a Call for Voluntary Academic Integrity",
        "publication": "Global Spine Journal",
        "citied_by": "1",
        "cover_date": "2022-09-01",
        "Abstract": "NA",
        "DOI": "10.1177/21925682221115789",
        "paper_author": "Chapman J.R.",
        "affiliation_name": "Swedish Medical Center, Seattle",
        "affiliation_city": "Seattle",
        "affiliation_country": "United States",
        "affiliation_id": "60011874",
        "affiliation_state": "WA"
    },
    {
        "paper_title": "HETEROGENEOUS CAUSAL EFFECTS WITH IMPERFECT COMPLIANCE: A BAYESIAN MACHINE LEARNING APPROACH",
        "publication": "Annals of Applied Statistics",
        "citied_by": "6",
        "cover_date": "2022-09-01",
        "Abstract": "This paper introduces an innovative Bayesian machine learning algorithm to draw interpretable inference on heterogeneous causal effects in the presence of imperfect compliance (e.g., under an irregular assignment mechanism). We show, through Monte Carlo simulations, that the proposed Bayesian Causal Forest with Instrumental Variable (BCF-IV) methodology outperforms other machine learning techniques tailored for causal inference in discovering and estimating the heterogeneous causal effects while controlling for the familywise error rate (or, less stringently, for the false discovery rate) at leaves’ level. BCF-IV sheds a light on the heterogeneity of causal effects in instrumental variable scenarios and, in turn, provides the policy-makers with a relevant tool for targeted policies. Its empirical application evaluates the effects of additional funding on students’ performances. The results indicate that BCF-IV could be used to enhance the effectiveness of school funding on students’ performance.",
        "DOI": "10.1214/21-AOAS1579",
        "paper_author": "Bargagli-Stoffi F.J.",
        "affiliation_name": "Harvard T.H. Chan School of Public Health",
        "affiliation_city": "Boston",
        "affiliation_country": "United States",
        "affiliation_id": "60032499",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "A neural-embedded discrete choice model: Learning taste representation with strengthened interpretability",
        "publication": "Transportation Research Part B: Methodological",
        "citied_by": "25",
        "cover_date": "2022-09-01",
        "Abstract": "Discrete choice models (DCMs) require a priori knowledge of the utility functions, especially how tastes vary across individuals. Utility misspecification may lead to biased estimates, inaccurate interpretations and limited predictability. In this paper, we utilize a neural network to learn taste representation. Our formulation consists of two modules: a neural network (TasteNet) that learns taste parameters (e.g., time coefficient) as flexible functions of individual characteristics; and a multinomial logit (MNL) model with utility functions defined with expert knowledge. Taste parameters learned by the neural network are fed into the choice model and link the two modules. Our approach extends the L-MNL model (Sifringer et al., 2020) by allowing the neural network to learn the interactions between individual characteristics and alternative attributes. Moreover, we formalize and strengthen the interpretability condition — requiring realistic estimates of behavior indicators (e.g., value-of-time, elasticity) at the disaggregated level, which is crucial for a model to be suitable for scenario analysis and policy decisions. Through a unique network architecture and parameter transformation, we incorporate prior knowledge and guide the neural network to output realistic behavior indicators at the disaggregated level. We show that TasteNet-MNL reaches the ground-truth model's predictability and recovers the nonlinear taste functions on synthetic data. Its estimated value-of-time and choice elasticities at the individual level are close to the ground truth. In contrast, exemplary logit models with misspecified systematic utility lead to biased parameter estimates and lower prediction accuracy. On a publicly available Swissmetro dataset, TasteNet-MNL outperforms benchmarking MNLs and Mixed Logit model's predictability. It learns a broader spectrum of taste variations within the population and suggests a higher average value-of-time. Our source code is available for research and application.",
        "DOI": "10.1016/j.trb.2022.07.001",
        "paper_author": "Han Y.",
        "affiliation_name": "MIT School of Engineering",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States",
        "affiliation_id": "60140949",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Turnover intention among Indian police: Do organizational and community stressors matter?",
        "publication": "Journal of Criminal Justice",
        "citied_by": "7",
        "cover_date": "2022-09-01",
        "Abstract": "Purpose: This study investigates the influence of organizational (abusive supervision, organizational support, and interpersonal trust) and community (political interference and perceived crime rate) stressors on police turnover intention via the mediating role of burnout. Methods: A total of 492 police personnel from India participated in the study. Structured Equation Modeling (SEM) was performed for testing hypotheses. Additionally, a machine learning model was built to develop concurrent predictive capability for burnout and turnover intention. Results: While abusive supervision and political interference positively influenced police turnover intention, co-worker trust, perceived organizational support and perceived crime rate negatively contributed to turnover intention. Burnout was found to significantly mediate the relationship between stressors and turnover intention. The machine learning model developed for the study that indicated an accuracy of 85% asserted that about 60% of the sample who suffered from burnout were also prone to developing a positive attitude towards turnover intention. Conclusion: This study is among the first to empirically explore the precursors to police turnover intention in India and establish the role of burnout in the turnover process. Based on the findings, the police organization can introduce and modify policies and practices to help control high employee turnover.",
        "DOI": "10.1016/j.jcrimjus.2022.101969",
        "paper_author": "Anand V.",
        "affiliation_name": "Indian Institute of Management Indore",
        "affiliation_city": "Indore",
        "affiliation_country": "India",
        "affiliation_id": "60105397",
        "affiliation_state": "MP"
    },
    {
        "paper_title": "From Policy to Prediction: Forecasting COVID-19 Dynamics Under Imperfect Vaccination",
        "publication": "Bulletin of Mathematical Biology",
        "citied_by": "16",
        "cover_date": "2022-09-01",
        "Abstract": "Understanding the joint impact of vaccination and non-pharmaceutical interventions on COVID-19 development is important for making public health decisions that control the pandemic. Recently, we created a method in forecasting the daily number of confirmed cases of infectious diseases by combining a mechanistic ordinary differential equation (ODE) model for infectious classes and a generalized boosting machine learning model (GBM) for predicting how public health policies and mobility data affect the transmission rate in the ODE model (Wang et al. in Bull Math Biol 84:57, 2022). In this paper, we extend the method to the post-vaccination period, accordingly obtain a retrospective forecast of COVID-19 daily confirmed cases in the US, and identify the relative influence of the policies used as the predictor variables. In particular, our ODE model contains both partially and fully vaccinated compartments and accounts for the breakthrough cases, that is, vaccinated individuals can still get infected. Our results indicate that the inclusion of data on non-pharmaceutical interventions can significantly improve the accuracy of the predictions. With the use of policy data, the model predicts the number of daily infected cases up to 35 days in the future, with an average mean absolute percentage error of 20.15 % , which is further improved to 14.88 % if combined with human mobility data. Moreover, the most influential predictor variables are the policies of restrictions on gatherings, testing and school closing. The modeling approach used in this work can help policymakers design control measures as variant strains threaten public health in the future.",
        "DOI": "10.1007/s11538-022-01047-x",
        "paper_author": "Wang X.",
        "affiliation_name": "University of Alberta",
        "affiliation_city": "Edmonton",
        "affiliation_country": "Canada",
        "affiliation_id": "60030835",
        "affiliation_state": "AB"
    },
    {
        "paper_title": "How Do Economic Sanctions Affect Public Opinion and Consumer Behavior in Target States? Evidence from China's Economic Sanctions on South Korea",
        "publication": "International Studies Quarterly",
        "citied_by": "2",
        "cover_date": "2022-09-01",
        "Abstract": "Previous research shows that economic sanctions affect three facets of public opinion in target states: support for the policy at issue, support for the target government, and hostility toward the sanctioner. We explore the dynamics between the facets of opinion and link them to consumer behavior. How do supportive opinions of the policy and the target government lead to hostility? How does this hostility affect consumers' propensity to buy the sanctioner's branded products? We examine a case in which China imposed economic sanctions on South Korea in opposition to South Korea's decision to deploy Terminal High Altitude Area Defense. We collect comments from relevant newspaper articles and conduct attitude analysis using machine learning. We conduct difference-in-differences analyses using barcode-level data regarding monthly beer sales, for which we argue boycotts are more likely to occur. We find that a backlash effect in public opinion occurred with respect to two facets of opinion. However, despite the public antagonism, we observe no significant backlash in consumer behavior. These findings imply that effects of economic sanctions in target states are multidimensional and thus it is too simplistic to assess the effectiveness of economic sanctions only by looking at public opinion in target states.",
        "DOI": "10.1093/isq/sqac023",
        "paper_author": "Sung R.",
        "affiliation_name": "University of Pittsburgh",
        "affiliation_city": "Pittsburgh",
        "affiliation_country": "United States",
        "affiliation_id": "60015543",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "What changed in the cyber-security after COVID-19?",
        "publication": "Computers and Security",
        "citied_by": "14",
        "cover_date": "2022-09-01",
        "Abstract": "This paper examines the transition in the cyber-security discipline induced by the ongoing COVID-19 pandemic. Using the classical information retrieval techniques, a more than twenty thousand documents are analyzed for the cyber content. In particular, we build the topic models using the Latent Dirichlet Allocation (LDA) unsupervised machine learning algorithm. The literature corpus is build through a uniform keyword search process made on the scholarly and the non-scholarly platforms filtered through the years 2010-2021. To qualitatively know the impact of COVID-19 pandemic on cyber-security, and perform a trend analysis of key themes, we organize the entire corpus into various (combination of) categories based on time period and whether the literature has undergone peer review process. Based on the weighted distribution of keywords in the aggregated corpus, we identify the key themes. While in the pre-COVID-19 period, the topics of cyber-threats to technology, privacy policy, blockchain remain popular, in the post-COVID-19 period, focus has shifted to challenges directly or indirectly brought by the pandemic. In particular, we observe post-COVID-19 cyber-security themes of privacy in healthcare, cyber insurance, cyber risks in supply chain gaining recognition. Few cyber-topics such as of malware, control system security remain important in perpetuity. We believe our work represents the evolving nature of the cyber-security discipline and reaffirms the need to tailor appropriate interventions by noting the key trends.",
        "DOI": "10.1016/j.cose.2022.102821",
        "paper_author": "Kumar R.",
        "affiliation_name": "Birla Institute of Technology and Science, Pilani",
        "affiliation_city": "Pilani",
        "affiliation_country": "India",
        "affiliation_id": "60000414",
        "affiliation_state": "RJ"
    },
    {
        "paper_title": "The language and targets of online trolling: A psycholinguistic approach for social cybersecurity",
        "publication": "Information Processing and Management",
        "citied_by": "10",
        "cover_date": "2022-09-01",
        "Abstract": "This paper posits and tests a social cybersecurity framework to detect and characterize online trolling. Using a dataset of online trolling obtained through active learning, we empirically find that troll messages are significantly associated with more abusive language (p<.001), lower cognitive complexity (p<.01), and greater targeting of named entities (p<.05) and identities (p<.05). These effects are robust to the likelihood that these messages come from bots. We then train and evaluate TrollHunter, a theory-driven and interpretable machine learning model using the derived psycholinguistic features. TrollHunter achieves 89% accuracy and F1 score in detecting trolling messages, with an average 12.25% improvement in performance when relationally modeling conversational context. Explorations of convergent and discriminant validity reveal that our measure of trolling is more closely related to non-hateful offensive speech over hate speech, aggressive over non-aggressive speech, and that Chinese state-sponsored accounts engage in higher levels of trolling than Russian state-sponsored accounts (p<.001). Finally, we apply TrollHunter in a field study to compare the media targets of trolling activity compared to bots as a reference group. Bots dominate replies to exclusive right-leaning media outlets like Breitbart and Newsmax, while trolls disproportionately target outlets with mixed partisan trust like BBC and ABC. This bifurcation suggests that not only are trolls and bots different entities, but they also have different impacts in relation to driving polarization and disinformation in society. Echoing recent calls for interdisciplinary approaches that link computational models with social theory, we conclude with implications for platform regulation and policy-making to curtail the actions of diverse agents of disinformation.",
        "DOI": "10.1016/j.ipm.2022.103012",
        "paper_author": "Uyheng J.",
        "affiliation_name": "School of Computer Science",
        "affiliation_city": "Pittsburgh",
        "affiliation_country": "United States",
        "affiliation_id": "60136640",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "Digital twin oriented architecture for secure and QoS aware intelligent communications in industrial environments",
        "publication": "Pervasive and Mobile Computing",
        "citied_by": "15",
        "cover_date": "2022-09-01",
        "Abstract": "In modern networking industrial environments, characterized by the integration of Operation Technology and Information Technology, there is a strong need to ensure both safety and security of operations and communications. In this regard, IEC 62443 zones and conduits represent powerful high-level abstractions stressing the importance of clearly separating machines in relation to safety requirements and of clearly defining inter-machine communication security requirements. However, their actual implementation is still demanded to human-centric error-prone procedures performed by technicians directly on network elements, without any integrated plant-wide point of view. To overcome these issues, first of all we originally state the need of applying the Digital Twin approach to zones and conduits, making easier the definition and management of inter-machine security requirements. For instance, industrial technicians can specify that communication among two zones should always flows through a ciphered conduit with a given algorithm and key length, at the cost of increased latency. Secondly, we state the need of exploiting an intelligent reasoner to monitor the current state of the environment (represented by asset and network Digital Twins), actively reconfiguring them in case desired requirements are not satisfied. Then, the reasoner allows to enforce requirements while also considering the fulfillment of a proper trade-off between security and performance, e.g., by reducing the ciphering complexity to ensure prompt packet dispatching whenever required. Performance results based on our working prototype demonstrate the feasibility and efficiency of the proposed solution under stringent requirements typical of industrial environments. In particular, in terms of better flexibility we proved that our orchestrator is able to create a new Digital Twin in less than 2.5 s in a typical edge node with a medium load. In addition, proposed routing policies based on our machine learning reasoner led to the satisfaction of well-defined low latency requirements (250 ms) while avoiding packet dropping.",
        "DOI": "10.1016/j.pmcj.2022.101646",
        "paper_author": "Bellavista P.",
        "affiliation_name": "Alma Mater Studiorum Università di Bologna",
        "affiliation_city": "Bologna",
        "affiliation_country": "Italy",
        "affiliation_id": "60028218",
        "affiliation_state": "BO"
    },
    {
        "paper_title": "Deep Reinforcement Learning-Enabled Bridge Management Considering Asset and Network Risks",
        "publication": "Journal of Infrastructure Systems",
        "citied_by": "10",
        "cover_date": "2022-09-01",
        "Abstract": "Bridges deteriorate over time due to various environmental and mechanical stressors. Deterioration is a significant risk to bridge owners (asset risk) and the traveling public (network risk). To tackle this issue, transportation agencies carry out bridge management under limited resources to preserve bridge conditions and control the risks of bridge failure. Nonetheless, existing network-level analysis for bridge management cannot explicitly consider the effects of preservation actions on network risk, measured directly by functionality indicators such as network capacity. In this paper, a novel method based on deep reinforcement learning is proposed to devise network-level preservation policies that can reflect bridge importance to network functionality. The proposed method is based on the proximal policy optimization algorithm adapted for bridge management problems and improved via distributed computing and architecture. The method is applied to an illustrative bridge network. The results indicate that the proposed method can produce significantly better preservation policies in terms of minimizing long-term costs that include asset and network risks. The devised policies are also investigated in depth to allow for transparent interpretation and easy integration with existing bridge management systems.",
        "DOI": "10.1061/(ASCE)IS.1943-555X.0000704",
        "paper_author": "Yang D.Y.",
        "affiliation_name": "Maseeh College of Engineering and Computer Science",
        "affiliation_city": "Portland",
        "affiliation_country": "United States",
        "affiliation_id": "60147807",
        "affiliation_state": "OR"
    },
    {
        "paper_title": "Marine route optimization using reinforcement learning approach to reduce fuel consumption and consequently minimize CO<inf>2</inf> emissions",
        "publication": "Ocean Engineering",
        "citied_by": "29",
        "cover_date": "2022-09-01",
        "Abstract": "To meet the 2050 CO2 targets, the shipping industry which is responsible for about 3% of global CO2 emissions needs to be optimized in several aspects. Obviously, alternative fuels constitute the main measure in this respect. However, relatively high fuel prices in combination with increasing political and economic pressure may raise the need for more efficient ship operation. Ship route optimization can make an indispensable contribution to achieving this goal. In this sense, this paper applies an innovative approach for route optimization using Reinforcement Learning (RL). For this purpose, a generic ship model is first developed using Artificial Neural Networks (ANNs) to predict the fuel consumption of the ship. Moreover, various RL methods, namely Deep Q-Network (DQN), Deep Deterministic Policy Gradient (DDPG), and Proximal Policy Optimization (PPO) are applied. The application of RL enables continuous action space and simultaneous optimization of ship speed and heading. DDPG demonstrates the best results as an off-policy and policy gradient method which allows a continuous action space. For example, in the fuel consumption minimization scenario without time limitation, this method can achieve savings of 6.64%. For DQN as a method with discrete action space, this value is 1.07%.",
        "DOI": "10.1016/j.oceaneng.2022.111882",
        "paper_author": "Moradi M.H.",
        "affiliation_name": "Karlsruher Institut für Technologie",
        "affiliation_city": "Karlsruhe",
        "affiliation_country": "Germany",
        "affiliation_id": "60102538",
        "affiliation_state": "Baden-Wurttemberg"
    },
    {
        "paper_title": "The estimation of the carbon dioxide emission and driving factors in China based on machine learning methods",
        "publication": "Sustainable Production and Consumption",
        "citied_by": "52",
        "cover_date": "2022-09-01",
        "Abstract": "Global warming can be reduced and the ecological environment can be enhanced by reducing carbon dioxide emissions. Therefore, it is imperative to determine how to calculate urban carbon dioxide emissions. Besides, as the country with the largest carbon dioxide emissions, exploring the influencing factors of carbon dioxide emissions is conducive to providing support for emission reduction actions. In the first place, the paper makes use of China's provincial carbon dioxide emissions data and nighttime light data to build an inversion model from 2000 to 2019 that calculates carbon emission data for prefecture-level cities. Furthermore, this paper employs machine learning methods such as decision tree and random forest to determine the factors affecting carbon dioxide emissions. The main conclusion is that carbon dioxide emissions are highest in the eastern regions with higher economic development. Additionally, cities which are dependent on resources to develop have higher carbon dioxide emissions and a rising trend. Factors such as gross domestic production, financial general budget revenue and foreign investment can influence carbon dioxide emissions. According to the random forest results, the feature importance of GDP, financial general budget revenue and foreign investment is 0.45, 0.12 and 0.08, respectively. Accordingly, different regions cannot ignore carbon dioxide emissions when developing their economies. As the growth rate of emissions has slowed in recent years in part due to government policies, China's ongoing implementation of low-carbon transformation must continue to be implemented, including low-carbon city pilot programs, carbon trading markets, etc. As well, areas with serious carbon dioxide emissions, such as Shanghai, Tianjin, and Chongqing, should be prioritized as areas for low-carbon economic development.",
        "DOI": "10.1016/j.spc.2022.06.027",
        "paper_author": "Qin J.",
        "affiliation_name": "Shanghai University of Finance and Economics",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60032744",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A universal adversarial policy for text classifiers",
        "publication": "Neural Networks",
        "citied_by": "7",
        "cover_date": "2022-09-01",
        "Abstract": "Discovering the existence of universal adversarial perturbations had large theoretical and practical impacts on the field of adversarial learning. In the text domain, most universal studies focused on adversarial prefixes which are added to all texts. However, unlike the vision domain, adding the same perturbation to different inputs results in noticeably unnatural inputs. Therefore, we introduce a new universal adversarial setup – a universal adversarial policy, which has many advantages of other universal attacks but also results in valid texts – thus making it relevant in practice. We achieve this by learning a single search policy over a predefined set of semantics preserving text alterations, on many texts. This formulation is universal in that the policy is successful in finding adversarial examples on new texts efficiently. Our approach uses text perturbations which were extensively shown to produce natural attacks in the non-universal setup (specific synonym replacements). We suggest a strong baseline approach for this formulation which uses reinforcement learning. Its ability to generalise (from as few as 500 training texts) shows that universal adversarial patterns exist in the text domain as well.",
        "DOI": "10.1016/j.neunet.2022.06.018",
        "paper_author": "Maimon G.",
        "affiliation_name": "Ben-Gurion University of the Negev",
        "affiliation_city": "Beer-Sheva",
        "affiliation_country": "Israel",
        "affiliation_id": "60027161",
        "affiliation_state": "Southern District"
    },
    {
        "paper_title": "Which are the most favourable conditions for reducing soil CO<inf>2</inf> emissions with no-tillage? Results from a meta-analysis",
        "publication": "International Soil and Water Conservation Research",
        "citied_by": "15",
        "cover_date": "2022-09-01",
        "Abstract": "No-tillage practices have a recognised beneficial impact on soil and water conservation while reducing erosion processes and enhancing soil organic matter content. However, scientists continue to debate the effectiveness of no-tillage in reducing soil carbon dioxide (CO2) emissions from farming. Following the same line of inquiry pursued by the authors who reviewed the impact of conservative practices on direct CO2 emissions, we applied meta-analytic and machine learning techniques to unravel the effect of no-tillage under contrasting pedo-environmental conditions and agricultural management. We analysed fifty-six experimental studies investigating direct soil CO2 emissions from no-tillage and conventional tillage practices (102 paired observations), considering pedological (soil texture, soil organic carbon content), environmental (climate type), and management (crop rotation, experiment duration) factors. We estimated the effect of different practices on the daily amount of soil CO2 emissions, and the impact of tillage in the period immediately following the event. The main insights of this study are: (i) the conditions leading to the highest reduction of CO2 emissions due to no-tillage were long-term experiments (standardised mean difference βˆ = 0.64) conducted in arid environments (βˆ = 0.76) and clay soils (βˆ = 0.81), with low organic carbon content (βˆ = 0.79) where crop rotations (βˆ = 0.65) were performed; (ii) the same conditions were associated with the lowest absolute CO2 emissions, irrespective of soil management; (iii) the highest contribution to the variability of absolute soil CO2 emissions was associated with soil texture (mean decrease in accuracy of Random Forest models, MDA = 4.57), rotation (MDA = 3.07), experiment duration (MDA = 2.93) and soil organic carbon content (MDA = 2.24), rather than to tillage practices; (iv) soil CO2 emissions almost doubled in the first day after a tillage event, consistently across studies (p = 0.001). This meta-analysis offers quantitative figures on the impact of tillage practices on soil CO2 emissions and releases data for informing policies aimed at promoting climate change mitigation.",
        "DOI": "10.1016/j.iswcr.2022.05.003",
        "paper_author": "Bregaglio S.",
        "affiliation_name": "Research Centre for Agriculture and Environment",
        "affiliation_city": "Bologna",
        "affiliation_country": "Italy",
        "affiliation_id": "121362389",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Reward criteria impact on the performance of reinforcement learning agent for autonomous navigation",
        "publication": "Applied Soft Computing",
        "citied_by": "19",
        "cover_date": "2022-09-01",
        "Abstract": "In reinforcement learning, an agent takes action at every time step (follows a policy) in an environment to maximize the expected cumulative reward. Therefore, the shaping of a reward function plays a crucial role in an agent's learning. Designing an optimal reward function is not a trivial task. In this article, we propose a reward criterion using which we develop different reward functions. The reward criterion chosen is based on the percentage of positive and negative rewards received by an agent. This reward criteria further gives rise to three different classes, ‘Balanced Class,’ ‘Skewed Positive Class,’ and ‘Skewed Negative Class.’ We train a Deep Q-Network agent on a point-goal based navigation task using the different reward classes. We also compare the performance of the proposed classes with a benchmark class. Based on the experiments, the skewed negative class outperforms the benchmark class by achieving very less variance. On the other hand, the benchmark class converges relatively faster than the skewed negative class.",
        "DOI": "10.1016/j.asoc.2022.109241",
        "paper_author": "Dayal A.",
        "affiliation_name": "Universitetet i Agder",
        "affiliation_city": "Kristiansand",
        "affiliation_country": "Norway",
        "affiliation_id": "60080184",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "ECG beat classification based on discriminative multilevel feature analysis and deep learning approach",
        "publication": "Biomedical Signal Processing and Control",
        "citied_by": "18",
        "cover_date": "2022-09-01",
        "Abstract": "Extraction of significant features from Electrocardiogram (ECG) signal is the primary concern for accurate diagnosis of cardiac arrhythmia. This work presents a novel approach of multilevel feature analysis and deep learning strategy for efficient ECG beat classification. The multilayer characteristics of ECG signal obtained from Empirical mode decomposition (EMD) are explored to extract discriminative feature vectors. The multilayer similarity coefficients are obtained by applying Dynamic time warping (DTW) metric and Pearson correlation coefficient (PCC) as diagnostic features. Furthermore, discrete orthonormal Stockwell transform (DOST) is employed for time–frequency representation of ECG data in multilayer aspect. The sublet changes in time–frequency spaces due to the presence of cardiac abnormalities are captured by estimating various nonlinear parameters. Interlayer deviations of these nonlinear parameters are estimated as the significant characteristics of arrhythmia detection. In addition, this study shows that the phase synchrony (PS) coefficients are prominent index for quantifying the crucial phase variation between normal and abnormal heart conditions. Hence multilevel PS coefficients are employed as the predictors of arrhythmia detection. Finally, the extracted feature vectors are fed to various classifiers to identify the heart anomalies. The proposed technique attains average accuracy of 98.82% and 98.14% using support vector machine (SVM) and k-nearest neighbors (k-NN) classifier respectively. The improved classification accuracy of 99.05% is obtained with the strategy of combining deep neural network (DNN) with the proposed feature extraction policy. Present work delivers satisfactory and superior performances for arrhythmia classification compare to other existing approaches.",
        "DOI": "10.1016/j.bspc.2022.103943",
        "paper_author": "Sinha N.",
        "affiliation_name": "University of Calcutta",
        "affiliation_city": "Kolkata",
        "affiliation_country": "India",
        "affiliation_id": "60024232",
        "affiliation_state": "WB"
    },
    {
        "paper_title": "Machine Learning for Enhanced Regional Seismic Risk Assessments",
        "publication": "Journal of Structural Engineering (United States)",
        "citied_by": "32",
        "cover_date": "2022-09-01",
        "Abstract": "The ability to conduct accurate regional seismic risk assessments is key to informing a risk-reduction policy and fostering community resilience. This paper presents a machine learning-based framework to predict a building's postearthquake damage state using structural properties and ground motion intensity measures as model inputs. The machine learning techniques assessed, namely, logistic regression, k-nearest neighbors, decision tree, random forest, AdaBoost, and gradient boosting, are trained using a dataset of nonlinear response history analysis results from 36 detailed structural models of modern reinforced concrete shear wall buildings ranging from four to 24 stories and subjected to approximately 500 ground motion records with a range of shaking intensities. The results indicate that the gradient boosting classifier is the most efficient algorithm by achieving a prediction success (F1-score) of 87%. The proposed framework also leverages synthetic data samples to support the prediction of severe damage state instances, that is, collapse. The percentage of observed collapse cases correctly classified by the gradient boosting algorithm is increased from 76% to 93% when synthetic data are also used for training. The framework is implemented in a portfolio of reinforced concrete shear wall buildings across the Metro Seattle region to quantify earthquake-induced damage and collapse risk. The framework shows great potential for enhancing regional seismic risk assessments by leveraging datasets of detailed nonlinear response history analysis results.",
        "DOI": "10.1061/(ASCE)ST.1943-541X.0003421",
        "paper_author": "Kourehpaz P.",
        "affiliation_name": "The University of British Columbia",
        "affiliation_city": "Vancouver",
        "affiliation_country": "Canada",
        "affiliation_id": "60010365",
        "affiliation_state": "BC"
    },
    {
        "paper_title": "Deep Reinforcement Learning for Personalized Driving Recommendations to Mitigate Aggressiveness and Riskiness: Modeling and Impact Assessment",
        "publication": "Transportation Research Part C: Emerging Technologies",
        "citied_by": "17",
        "cover_date": "2022-09-01",
        "Abstract": "Most driving recommendation and assistance systems, such as Advanced Driving Assistance Systems (ADAS), are usually designed based on the behavior of an average driver. Nevertheless, personalized driving systems that can be adapted to different driving styles and recognize individual needs and preferences, may be key to the sensitization of drivers and the adoption of safer driving habits. In this paper, an enhanced self-aware driving recommendation system is developed using a Deep Reinforcement Learning algorithm, which produces personalized driving recommendations with a view to improving driving safety, while respecting individual driving styles and preferences. The impact of applying this recommendation system is evaluated through microscopic simulation; findings revealed that, in case all drivers follow the suggestions, there is a significant improvement in road safety and some minor changes in traffic flow properties. The outputs of this work may be useful within the framework of an advanced active cruise control system, can be exploited in the development of enhanced behavioral models or even lead to the revision of policy measures that utilize driving behavior as a key controller of traffic management.",
        "DOI": "10.1016/j.trc.2022.103770",
        "paper_author": "Mantouka E.G.",
        "affiliation_name": "National Technical University of Athens (NTUA)",
        "affiliation_city": "Athens",
        "affiliation_country": "Greece",
        "affiliation_id": "60002947",
        "affiliation_state": "Attica"
    },
    {
        "paper_title": "An empirical analysis of high school students' practices of modelling with unstructured data",
        "publication": "British Journal of Educational Technology",
        "citied_by": "17",
        "cover_date": "2022-09-01",
        "Abstract": "To date, many AI initiatives (eg, AI4K12, CS for All) developed standards and frameworks as guidance for educators to create accessible and engaging Artificial Intelligence (AI) learning experiences for K-12 students. These efforts revealed a significant need to prepare youth to gain a fundamental understanding of how intelligence is created, applied, and its potential to perpetuate bias and unfairness. This study contributes to the growing interest in K-12 AI education by examining student learning of modelling real-world text data. Four students from an Advanced Placement computer science classroom at a public high school participated in this study. Our qualitative analysis reveals that the students developed nuanced and in-depth understandings of how text classification models—a type of AI application—are trained. Specifically, we found that in modelling texts, students: (1) drew on their social experiences and cultural knowledge to create predictive features, (2) engineered predictive features to address model errors, (3) described model learning patterns from training data and (4) reasoned about noisy features when comparing models. This study contributes to an initial understanding of student learning of modelling unstructured data and offers implications for scaffolding in-depth reasoning about model decision making. Practitioner notes: What is already known about this topic Scholarly attention has turned to examining Artificial Intelligence (AI) literacy in K-12 to help students understand the working mechanism of AI technologies and critically evaluate automated decisions made by computer models. While efforts have been made to engage students in understanding AI through building machine learning models with data, few of them go in-depth into teaching and learning of feature engineering, a critical concept in modelling data. There is a need for research to examine students' data modelling processes, particularly in the little-researched realm of unstructured data. What this paper adds Results show that students developed nuanced understandings of models learning patterns in data for automated decision making. Results demonstrate that students drew on prior experience and knowledge in creating features from unstructured data in the learning task of building text classification models. Students needed support in performing feature engineering practices, reasoning about noisy features and exploring features in rich social contexts that the data set is situated in. Implications for practice and/or policy It is important for schools to provide hands-on model building experiences for students to understand and evaluate automated decisions from AI technologies. Students should be empowered to draw on their cultural and social backgrounds as they create models and evaluate data sources. To extend this work, educators should consider opportunities to integrate AI learning in other disciplinary subjects (ie, outside of computer science classes).",
        "DOI": "10.1111/bjet.13253",
        "paper_author": "Jiang S.",
        "affiliation_name": "NC State University",
        "affiliation_city": "Raleigh",
        "affiliation_country": "United States",
        "affiliation_id": "60004923",
        "affiliation_state": "NC"
    },
    {
        "paper_title": "How do machines predict energy use? Comparing machine learning approaches for modeling household energy demand in the United States",
        "publication": "Energy Research and Social Science",
        "citied_by": "10",
        "cover_date": "2022-09-01",
        "Abstract": "This paper illustrates the use of different machine learning techniques to estimate household energy demand. To demonstrate the performance of the techniques, we discuss how the different machine learning algorithms select a model or models of energy usage and we explore how well the models predict usage. Our study employs a high-dimensional dataset of housing, socioeconomic, and behavioral characteristics, provided by the U.S. Energy Information Administration's ongoing Residential Energy Consumption Survey. In addition to discussing the machine learning models, we estimate energy price elasticities, which are important indicators of how sensitive households are to changes in residential energy prices. Given the broad set of data in the survey, we compare and contrast various machine learning techniques to see which model provides the best overall fit to the data. We find that a random forest algorithm performs better than the other machine learning approaches, which include a step-wise Akaike Information Criterion, partial least squares, ordinary least squares, k-nearest neighbors, penalized regression, and gradient boosting methods. Finally, we discuss how machine learning can be used to inform residential energy policies and predict household energy consumption.",
        "DOI": "10.1016/j.erss.2022.102715",
        "paper_author": "Burnett J.W.",
        "affiliation_name": "United States Department of Agriculture",
        "affiliation_city": "Washington, D.C.",
        "affiliation_country": "United States",
        "affiliation_id": "60032280",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Modelling and mapping of soil erosion susceptibility using machine learning in a tropical hot sub-humid environment",
        "publication": "Journal of Cleaner Production",
        "citied_by": "42",
        "cover_date": "2022-09-01",
        "Abstract": "Sobha watershed, located in the Puruliya district of West Bengal, India, is experiencing severe soil erosion due to specific geo-environmental settings and unscientific land practices. It poses serious threats to agricultural and natural resource development, resulting in land degradation and desertification. This study attempts to identify soil erosion susceptible zones (SESZ) of the Sobha watershed by utilising remote sensing and GIS data products in different machine learning algorithms i.e., Support Vector Machine (SVM), Classification and Regression Tree (CART), Boosted Regression Tree (BRT), and Random Forest (RF)) considering sixteen soil erosion controlling factors (SECFs). In addition, the efficiency of the chosen machine learning models was evaluated using known soil erosion and non-erosion data. The results showed that elevation, drainage density (DD), and normalised difference vegetation index (NDVI) factors contribute the most to soil erosion. The ROC (receiver operating curve) AUC (area under the curve) is used to compare each model, and it was reveals that the RF model performed and predicted the best among them. However, all the models exhibit an outstanding capacity with AUC >85% (RF = 0.97, BRT = 0.96, SVM = 0.95, and CART = 0.88). The RF model results show that the Northeastern portion of the catchment (upper part) is most vulnerable to erosion, and about 14.48% of the basin areas are under the severe erosion zone. Thereby, the findings based on machine learning algorithms and intensive field visits are utilised to assess the soil erosion risk zones, and this work will give insight into implementing suitable policies to mitigate this issue. Furthermore, the approaches utilised in this study could be useful in predicting soil erosion risk in other regions as well. In addition, the study has also recommended some appropriate policies and management approaches that would be immensely useful for the local government and policymakers in initiating strategic planning to combat soil erosion.",
        "DOI": "10.1016/j.jclepro.2022.132428",
        "paper_author": "Bag R.",
        "affiliation_name": "Jawaharlal Nehru University",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India",
        "affiliation_id": "60030622",
        "affiliation_state": "DL"
    },
    {
        "paper_title": "Supervised Optimal Chemotherapy Regimen Based on Offline Reinforcement Learning",
        "publication": "IEEE Journal of Biomedical and Health Informatics",
        "citied_by": "15",
        "cover_date": "2022-09-01",
        "Abstract": "In recent years, reinforcement learning (RL) has achieved a remarkable achievement and it has attracted researchers' attention in modeling real-life scenarios by expanding its research beyond conventional complex games. Prediction of optimal treatment regimens from observational real clinical data is being popularized, and more advanced versions of RL algorithms are being implemented in the literature. However, RL-generated medications still need careful supervision of expertise parties or doctors in healthcare. Hence, in this paper, a Supervised Optimal Chemotherapy Regimen (SOCR) approach to investigate optimal chemotherapy-dosing schedule for cancer patients was presented by using Offline Reinforcement Learning. The optimal policy suggested by the RL approach was supervised by incorporating previous treatment decisions of oncologists, which could add clinical expertise knowledge on algorithmic results. Presented SOCR approach followed a model-based architecture using conservative Q-Learning (CQL) algorithm. The developed model was tested using a manually constructed database of forty Stage-IV colon cancer patients, receiving line-1 chemotherapy treatments, who were clinically classified as 'Bevacizumab based patient' and 'Cetuximab based patient'. Experimental results revealed that the supervision from the oncologists has considered the effect to stabilize chemotherapy regimen and it was suggested that the proposed framework could be successfully used as a supportive model for oncologists in deciding their treatment decisions.",
        "DOI": "10.1109/JBHI.2022.3183854",
        "paper_author": "Shiranthika C.",
        "affiliation_name": "National Taipei University",
        "affiliation_city": "Taipei",
        "affiliation_country": "Taiwan",
        "affiliation_id": "60016334",
        "affiliation_state": "Taipei"
    },
    {
        "paper_title": "An ensemble random forest tree with SVM, ANN, NBT, and LMT for landslide susceptibility mapping in the Rangit River watershed, India",
        "publication": "Natural Hazards",
        "citied_by": "31",
        "cover_date": "2022-09-01",
        "Abstract": "This study examined landslide susceptibility, an increasingly common problem in mountainous regions across the world as a result of urbanization, deforestation, and various natural processes. The Rangit River watershed in Sikkim Himalaya is one of the most landslide-prone areas in India. The main objective of this study was to produce landslide susceptibility maps of the Rangit River watershed using novel ensembles of random forest tree (RFT) with support vector machine (RFT-SVM), artificial neural network (RFT-ANN), naïve Bayes tree (RFT-NBT), and logistic model tree (RFT-LMT). An inventory of landslides was created using historical landslide data, government and scientific studies, and Google Earth’s high-resolution satellite images. The landslide/non-landslide locations were split 70/30 for training and validating the models, respectively. Eleven landslide conditioning factors were selected based on their predictive ability, determined using the information gain method, and each factor’s importance was derived. A landslide susceptibility index was then estimated by weighted overlay using a model builder in a GIS (Geographic Information System) environment. Based on the area under the curve and statistical metrics, RFT-LMT was identified as the best model. The results showed that approximately 40% of the Rangit River watershed has high to very high susceptibility to landslides. This study’s findings will be useful for policy-makers and land use planners in managing and mitigating future landslides in the study area.",
        "DOI": "10.1007/s11069-022-05360-5",
        "paper_author": "Ali S.A.",
        "affiliation_name": "Aligarh Muslim University",
        "affiliation_city": "Aligarh",
        "affiliation_country": "India",
        "affiliation_id": "60032269",
        "affiliation_state": "UP"
    },
    {
        "paper_title": "The financial market impact of ECB monetary policy press conferences — A text based approach",
        "publication": "European Journal of Political Economy",
        "citied_by": "11",
        "cover_date": "2022-09-01",
        "Abstract": "Using natural language processing techniques I create two measures of the monetary policy tilt of the ECB, that outline the beliefs of the ECB on the current state of the economy and the outlook for growth and inflation. These measures closely track interest rate expectations over the tightening and loosening cycle, and can provide a useful measure of monetary policy tilt at the zero lower bound and contains information about the state of the economy. I exploit the time lag between the announcement of the monetary policy decision, and the ECB's monetary policy press conference to assess the immediate financial market impact of changes in communication within the press conference, free from the effects of the shock from the monetary policy decision. Consistent with the literature on the information channel of monetary policy, I find a non-negligible positive (negative) effect on stock prices of a more hawkish (dovish) tone in the press conference. This indicates that the ECB reveals “private information” during these press conferences, and that market participants internalise this as good (bad) news regarding the future state of the economy, rather than internalising a future potential increase (decrease) in interest rates. This effect is stronger prior to the introduction of formal forward guidance in July 2013, suggesting that since then ECB communication has been less surprising to markets in recent times.",
        "DOI": "10.1016/j.ejpoleco.2022.102230",
        "paper_author": "Parle C.",
        "affiliation_name": "Banc Ceannais na hÉireann",
        "affiliation_city": "Dublin",
        "affiliation_country": "Ireland",
        "affiliation_id": "60106701",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Leveraging Machine Learning to Automatically Derive Robust Decision Strategies from Imperfect Knowledge of the Real World",
        "publication": "Computational Brain and Behavior",
        "citied_by": "2",
        "cover_date": "2022-09-01",
        "Abstract": "Teaching people clever heuristics is a promising approach to improve decision-making under uncertainty. The theory of resource rationality makes it possible to leverage machine learning to discover optimal heuristics automatically. One bottleneck of this approach is that the resulting decision strategies are only as good as the model of the decision problem that the machine learning methods were applied to. This is problematic because even domain experts cannot give complete and fully accurate descriptions of the decisions they face. To address this problem, we develop strategy discovery methods that are robust to potential inaccuracies in the description of the scenarios in which people will use the discovered decision strategies. The basic idea is to derive the strategy that will perform best in expectation across all possible real-world problems that could have given rise to the likely erroneous description that a domain expert provided. To achieve this, our method uses a probabilistic model of how the description of a decision problem might be corrupted by biases in human judgment and memory. Our method uses this model to perform Bayesian inference on which real-world scenarios might have given rise to the provided descriptions. We applied our Bayesian approach to robust strategy discovery in two domains: planning and risky choice. In both applications, we find that our approach is more robust to errors in the description of the decision problem and that teaching the strategies it discovers significantly improves human decision-making in scenarios where approaches ignoring the risk that the description might be incorrect are ineffective or even harmful. The methods developed in this article are an important step towards leveraging machine learning to improve human decision-making in the real world because they tackle the problem that the real world is fundamentally uncertain.",
        "DOI": "10.1007/s42113-022-00141-6",
        "paper_author": "Mehta A.",
        "affiliation_name": "Max Planck Institute for Intelligent Systems",
        "affiliation_city": "Tubingen",
        "affiliation_country": "Germany",
        "affiliation_id": "60030569",
        "affiliation_state": "Baden-Wurttemberg"
    },
    {
        "paper_title": "Sneaking Through Security: Mutating Live Network Traffic to Evade Learning-Based NIDS",
        "publication": "IEEE Transactions on Network and Service Management",
        "citied_by": "13",
        "cover_date": "2022-09-01",
        "Abstract": "Machine learning based network intrusion system (NIDS) is known to be vulnerable to evasions. Attackers conceal intrusion activities to make them undetected. Researching evasion techniques contributes to evaluating and increasing the robustness of NIDS. Previous evasion approaches modify feature values or packets of an offline network trace as a whole. However, in real scenarios, attackers are constrained to manipulate only outbound packets on the fly. To bridge this assumption gap, we present the first evasion solution for live network traffic against learning based NIDSs. The solution consists of three components: A devised Kalman filter based algorithm to predicate the feature values of live flows, a set of formally constructed atomic packet mutation operators, and a proposed Strength Enhanced Deep Q-learning (SE-DQN) to determine effective mutation operators on outbound packets according to the predicted features. A defense scheme based on adaptive decision threshold adjustment is also provided. Experimental evaluation is presented on various NIDS classifiers and cyber attacks. Results show that SE-DQN achieves an evasion rate of at least 64.2% on most classifiers and even more than 90% on certain ones, and it is three times faster than DQN on learning mutation policy. The defense scheme shows an improvement of at least 76.4% on recall measurement.",
        "DOI": "10.1109/TNSM.2022.3173933",
        "paper_author": "Tan S.",
        "affiliation_name": "Peng Cheng Laboratory",
        "affiliation_city": "Shenzhen",
        "affiliation_country": "China",
        "affiliation_id": "60271961",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "The framing of green innovations—a comparative topic modeling study on the public frames of the electric vehicle in Germany and UK",
        "publication": "Journal of Cleaner Production",
        "citied_by": "13",
        "cover_date": "2022-09-01",
        "Abstract": "In the innovation framing literature, scholars argue that green innovations are being challenged by legitimacy barriers associated with strong lock-in effects on the fossil-based economy. To break down barriers and create legitimacy, we stress the role of demarcation frames. Demarcation frames we argue are an important supplement to the established framing mechanisms that signal alignment and similarity with existing systems. Building on a machine-learning topic modeling approach, we investigate the development of the perception and meaning of the electric vehicle over a period of 27 years—i.e., its framing. By using public media data to undertake a systematic cross-country study in Germany and the UK, we show how and through which combinations of framing mechanisms the electric vehicle overcame the initial skepticism that was closely linked to the internal combustion vehicle. Hence, our research contributes to a better understanding of the framing processes around green innovations in the carbon-based economy. Firstly, we offer a novel analytical perspective focusing on the overarching public framing of green innovations. Secondly, we contribute to the literature by describing the theoretical implications and functionality of demarcation frames to overcome lock-ins. And thirdly, we discuss policy implications to support the dissemination of green innovations and propose future research avenues relevant for the green innovation and innovation framing field.",
        "DOI": "10.1016/j.jclepro.2022.132499",
        "paper_author": "Bohn S.",
        "affiliation_name": "Alexander von Humboldt Institute for Internet and Society",
        "affiliation_city": "Berlin",
        "affiliation_country": "Germany",
        "affiliation_id": "116367687",
        "affiliation_state": "Berlin"
    },
    {
        "paper_title": "Random forest modelling of multi-scale, multi-species habitat associations within KAZA transfrontier conservation area using spoor data",
        "publication": "Journal of Applied Ecology",
        "citied_by": "8",
        "cover_date": "2022-09-01",
        "Abstract": "As landscape-scale conservation models grow in prominence, assessments of how wildlife utilise multiple-use landscapes are required to inform effective conservation and management planning. Such efforts should incorporate multi-species perspectives to maximise value for conservation, and should account for scale to accurately capture species-environment relationships. We show that the random forest machine learning algorithm can be used to model large-scale sign-based data in a multi-scale framework. We used this method to investigate scale-dependent habitat associations for 16 mammal species of high conservation importance across the southern Kavango Zambezi (KAZA) Transfrontier Conservation Area in Botswana and Zimbabwe. Our findings revealed substantial variation in factors shaping habitat use across species, and illustrate that different species often have divergent responses to the same environmental and anthropogenic factors, and differ in the scales at which they respond to them. For all variables across all species, scale optimisation most often selected our largest scale. Precipitation, soil nutrients, and vegetation appeared to be the most important factors determining mammal distributions, likely through their associations with food resources for herbivores and, in turn, prey availability for carnivores. Anthropogenic pressures also had an important influence, with many species selecting against areas with high cattle density. The variety of relationships with human density indicated that species vary in their tolerance of humans. We found a consistent positive relationship with areas under high protection, and negative relationship with unprotected and less-strictly protected areas. Policy implications. Through a novel application of random forest modelling to spoor data from 16 mammal species, this study highlights the importance of adopting a multi-scale, multi-species approach for decision-making processes that depend on understanding wildlife distributions and habitat associations, such as protected area and corridor prioritisation. The findings identify changing rainfall patterns and increasing livestock numbers as emerging trends that may impact wildlife distributions, both within sub-Saharan Africa and on a global scale. Wildlife management authorities should use modelling exercises and adaptive management to ensure that protected area networks remain fit for purpose under anticipated changes in rainfall under climate change, and explore initiatives that promote coexistence of wildlife and livestock.",
        "DOI": "10.1111/1365-2664.14234",
        "paper_author": "Searle C.E.",
        "affiliation_name": "University of Oxford",
        "affiliation_city": "Oxford",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60026851",
        "affiliation_state": "Oxfordshire"
    },
    {
        "paper_title": "Reinforcement Learning-Based Sequential Batch-Sampling for Bayesian Optimal Experimental Design",
        "publication": "Journal of Mechanical Design",
        "citied_by": "2",
        "cover_date": "2022-09-01",
        "Abstract": "Engineering problems that are modeled using sophisticated mathematical methods or are characterized by expensive-To-conduct tests or experiments are encumbered with limited budget or finite computational resources. Moreover, practical scenarios in the industry, impose restrictions, based on logistics and preference, on the manner in which the experiments can be conducted. For example, material supply may enable only a handful of experiments in a single-shot or in the case of computational models one may face significant wait-Time based on shared computational resources. In such scenarios, one usually resorts to performing experiments in a manner that allows for maximizing one's state-of-knowledge while satisfying the above-mentioned practical constraints. Sequential design of experiments (SDOE) is a popular suite of methods that have yielded promising results in recent years across different engineering and practical problems. A common strategy that leverages Bayesian formalism is the Bayesian SDOE, which usually works best in the one-step-Ahead or myopic scenario of selecting a single experiment at each step of a sequence of experiments. In this work, we aim to extend the SDOE strategy, to query the experiment or computer code at a batch of inputs. To this end, we leverage deep reinforcement learning (RL)-based policy gradient methods, to propose batches of queries that are selected taking into account the entire budget in hand. The algorithm retains the sequential nature, inherent in the SDOE while incorporating elements of reward based on task from the domain of deep RL. A unique capability of the proposed methodology is its ability to be applied to multiple tasks, for example, optimization of a function, once its trained. We demonstrate the performance of the proposed algorithm on a synthetic problem and a challenging high-dimensional engineering problem.",
        "DOI": "10.1115/1.4054631",
        "paper_author": "Ashenafi Y.",
        "affiliation_name": "University of Alberta",
        "affiliation_city": "Edmonton",
        "affiliation_country": "Canada",
        "affiliation_id": "60030835",
        "affiliation_state": "AB"
    },
    {
        "paper_title": "Online Feature Selection for Efficient Learning in Networked Systems",
        "publication": "IEEE Transactions on Network and Service Management",
        "citied_by": "1",
        "cover_date": "2022-09-01",
        "Abstract": "Current AI/ML methods for data-driven engineering use models that are mostly trained offline. Such models can be expensive to build in terms of communication and computing costs, and they rely on data that is collected over extended periods of time. Further, they become out-of-date when changes in the system occur. To address these challenges, we investigate online learning techniques that automatically reduce the number of available data sources for model training. We present an online algorithm called Online Stable Feature Set Algorithm (OSFS), which selects a small feature set from a large number of available data sources after receiving a small number of measurements. The algorithm is initialized with a feature ranking algorithm, a feature set stability metric, and a search policy. We perform an extensive experimental evaluation of this algorithm using traces from an in-house testbed and from two external datasets. We find that OSFS achieves a massive reduction in the size of the feature set by 1-3 orders of magnitude on all investigated datasets. Most importantly, we find that the accuracy of a predictor trained on a OSFS-produced feature set is somewhat better than when the predictor is trained on a feature set obtained through offline feature selection. OSFS is thus shown to be effective as an online feature selection algorithm and robust regarding the sample interval used for feature selection. We also find that, when concept drift in the data underlying the model occurs, its effect can be mitigated by recomputing the feature set and retraining the prediction model.",
        "DOI": "10.1109/TNSM.2022.3180936",
        "paper_author": "Wang X.",
        "affiliation_name": "The Royal Institute of Technology (KTH)",
        "affiliation_city": "Stockholm",
        "affiliation_country": "Sweden",
        "affiliation_id": "60002014",
        "affiliation_state": "Stockholms"
    },
    {
        "paper_title": "D<inf>2</inf>A: Operating a Service Function Chain Platform with Data-Driven Scheduling Policies",
        "publication": "IEEE Transactions on Network and Service Management",
        "citied_by": "7",
        "cover_date": "2022-09-01",
        "Abstract": "Realizing Service Function Chaining with a micro-service-based architecture results in an increased number of computationally cheap Virtual Network Functions (VNFs). Pinning cheap VNFs to dedicated CPU cores can waste resources since not every VNF fully utilizes its core. Thus, cheap VNFs should share CPU cores to improve resource utilization. However, sharing cores can result in degraded performance due to interference between VNFs, even in mildly loaded scenarios. We propose D2A, a system that combines Neural Combinatorial Optimization, Machine Learning (ML)-based Digital Twins (DTs), and Game Theory to optimize VNF assignments. Measurements in a testbed show that D2A increases throughput by up to 46% and reduces latency by up to 93%, compared to three baseline algorithms. Using an ML-based DT to model VNF interference increases throughput by up to 11%, and reduces latency by up to 90% compared to an analytical model of the system.",
        "DOI": "10.1109/TNSM.2022.3177694",
        "paper_author": "Kramer P.",
        "affiliation_name": "Technische Universität München",
        "affiliation_city": "Munich",
        "affiliation_country": "Germany",
        "affiliation_id": "60019722",
        "affiliation_state": "Bayern"
    },
    {
        "paper_title": "Intake monitoring in free-living conditions: Overview and lessons we have learned",
        "publication": "Appetite",
        "citied_by": "2",
        "cover_date": "2022-09-01",
        "Abstract": "The progress in artificial intelligence and machine learning algorithms over the past decade has enabled the development of new methods for the objective measurement of eating, including both the measurement of eating episodes as well as the measurement of in-meal eating behavior. These allow the study of eating behavior outside the laboratory in free-living conditions, without the need for video recordings and laborious manual annotations. In this paper, we present a high-level overview of our recent work on intake monitoring using a smartwatch, as well as methods using an in-ear microphone. We also present evaluation results of these methods in challenging, real-world datasets. Furthermore, we discuss use-cases of such intake monitoring tools for advancing research in eating behavior, for improving dietary monitoring, as well as for developing evidence-based health policies. Our goal is to inform researchers and users of intake monitoring methods regarding (i) the development of new methods based on commercially available devices, (ii) what to expect in terms of effectiveness, and (iii) how these methods can be used in research as well as in practical applications.",
        "DOI": "10.1016/j.appet.2022.106096",
        "paper_author": "Diou C.",
        "affiliation_name": "Harokopio University of Athens",
        "affiliation_city": "Athens",
        "affiliation_country": "Greece",
        "affiliation_id": "60012296",
        "affiliation_state": "Attica"
    },
    {
        "paper_title": "Do natural resources, economic growth, human capital, and urbanization affect the ecological footprint? A modified dynamic ARDL and KRLS approach",
        "publication": "Resources Policy",
        "citied_by": "94",
        "cover_date": "2022-09-01",
        "Abstract": "The interaction between the abundance of natural resources and environmental depletion has significant ecological consequences. Nonetheless, this area is not adequately studied, and numerous results are apparent throughout the literature. For massive economic development, it is vital to recognize the role of human capital, urbanization, and natural resources. Hence it is important to consider various factors that can play a constructive role in environmental sustainability. Therefore, this study investigates the relationship between total natural resources (TNR), gross domestic product (GDP), human capital index (HCI) and urbanization (URB) with ecological footprint (EFP) in Pakistan from 1980 to 2018. The research uses the latest versions of dynamic Autoregressive Distributed Lag (ARDL) simulations model. The key benefit of dynamic ARDL is to estimate positive and negative shifts between the selected variables with an immediate visual illustration over the short and long period. In addition, the Kernel-based Regularized Least Squares (KRLS) machine learning method is used to test robustness. The results show that the rise in TNR has a long-term detrimental influence on EFP. However, upsurge in GDP and HCI increases EFP in the long-term. Lastly, URB observed an important and detrimental long-term impact on the EFP. The KRLS also support the hypothesis. This study suggest a policies to the planners and government officials for managing rapid urbanization and minimizing its urban, environmental and economic challenges.",
        "DOI": "10.1016/j.resourpol.2022.102782",
        "paper_author": "Zhou R.",
        "affiliation_name": "Anhui University of Finance and Economics",
        "affiliation_city": "Bengbu",
        "affiliation_country": "China",
        "affiliation_id": "60108842",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Editorial special issue addictive behaviors, networks, complexity and addictive behaviors",
        "publication": "Addictive Behaviors",
        "citied_by": "1",
        "cover_date": "2022-09-01",
        "Abstract": "NA",
        "DOI": "10.1016/j.addbeh.2022.107369",
        "paper_author": "Wiers R.W.",
        "affiliation_name": "Universiteit van Amsterdam",
        "affiliation_city": "Amsterdam",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60002483",
        "affiliation_state": "Noord-Holland"
    },
    {
        "paper_title": "Recent advances in black box and white-box models for urban heat island prediction: Implications of fusing the two methods",
        "publication": "Renewable and Sustainable Energy Reviews",
        "citied_by": "31",
        "cover_date": "2022-09-01",
        "Abstract": "The urban heat island (UHI) phenomenon is a serious concern for urban planners and policymakers, requiring effective and efficient mitigation policies. To develop such policies, accurate and pre-emptive estimations of current and future UHI manifestations are vital elements that help determine efficient policies and mitigation techniques. There are two fundamental approaches for modelling overheating in an urban environment: white-box and black-box based methods. The first one is characterized by the easily interpretable working process, while the unclear working procedure defines the second one. The present study comprehensively reviews the commonly used white-box and black-box based approaches applied for UHI predictions, analyses the existing literature adopting these tools for UHI prediction, and discusses the effectiveness of fusing both methods at the design and operation stages of the urban area for effective prediction and mitigation of UHI effect. The literature analysis showed that the transparent working process and high prediction accuracy of the physical-based white-box models make them a popular and reliable tool for UHI evaluation. Nevertheless, some white-box based simulation tools are too complex and require a high level of expertise to operate, leading to potential inaccuracies in the obtained outcomes. Black-box models, in turn, despite their opaque working process, are more straightforward in use and require less computation time. The fusion of these two methods is a novel approach that may benefit both UHI prediction and mitigation at the design and operation stages, respectively.",
        "DOI": "10.1016/j.rser.2022.112520",
        "paper_author": "Adilkhanova I.",
        "affiliation_name": "Kyung Hee University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea",
        "affiliation_id": "60001873",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Assessing myocardial infarction severity from the urban environment perspective in Wuhan, China",
        "publication": "Journal of Environmental Management",
        "citied_by": "13",
        "cover_date": "2022-09-01",
        "Abstract": "Health inequalities are globally widespread due to the regional socioeconomic inequalities. Myocardial infarction (MI) is a leading health problem causing deaths worldwide. Yet medical services for it are often inequitably distributed by region. Moreover, studies concerning MI's potential spatial risk factors generally suffer from difficulties in focusing on too few factors, inappropriate models, and coarse spatial grain of data. To address these issues, this paper integrates registered 1098 MI cases and urban multi-source spatio-temporal big data, and spatially analyses the risk factors for MI severity by applying an advanced interpretable model, the random forest algorithm (RFA)-based SHapley Additive exPlanations (SHAP) model. In addition, a community-scale model between spatio-temporal risk factors and MI cases is constructed to predict the MI severity of all communities in Wuhan, China. The results suggest that those risk factors (i.e., age of patients, medical quality, temperature changes, air pollution and urban habitat) affect the MI severity at the community scale. We found that Wuhan residents in the downtown area are at risk for high MI severity, and the surrounding suburb areas show a donut-shape pattern of risk for medium-to-high MI severity. These patterns draw our attention to the impact of spatial environmental risk factors on MI severity. Thus, this paper provides three recommendations for urban planning to reduce the risk and mortality from severe MI in the aspect of policy implication.",
        "DOI": "10.1016/j.jenvman.2022.115438",
        "paper_author": "Yao Y.",
        "affiliation_name": "China University of Geosciences",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60006019",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Microgrid Operational Planning using a Hybrid Neural Network with Resource-aware Scenario Selection",
        "publication": "Simulation Modelling Practice and Theory",
        "citied_by": "9",
        "cover_date": "2022-09-01",
        "Abstract": "Modern power systems continue to increase in complexity due to the intermittent behavior of distributed energy resources (DERs) and stochastic climate factors. Considering these factors simultaneously generates enumerable scenarios for decision-making within a microgrid (MG) dispatch policy. An MG dispatch policy can be formulated as a mixed integer linear program (MILP) and solved using traditional solution methods such as simplex, graphical method, etc. However, given the number of potential scenarios, these traditional solution methods incur a high computational burden to obtain near real-time solutions. Thus, dimension reduction techniques are needed to reduce the MILP's complexity. To address the need for a scalable solution method and reduced model complexity, we propose a 2-stage reconfigurable framework for near real-time MG operational planning. The proposed 2-stage reconfigurable framework is comprised of four modules: i) a resource-aware scenario selection (RSS) algorithm to identify the most probable scenarios, ii) an MG dispatch policy to obtain solutions or dispatch decisions for an MG system, iii) a neural network (NN) to map the most probable scenarios from RSS to their corresponding solutions from the MG dispatch policy and iv) a rule-based policy to monitor depreciating predictions from the NN. The RSS module is a probabilistic variate of a full factorial design (FFD) that assigns weights to test points defining the design space based on stochastic climate factors and the number of quartiles used to partition each factor's likelihood function. The most probable scenarios from RSS and their corresponding solutions from the proposed MG dispatch policy were both synthesized into a training set. NNs were then trained using this dataset to predict dispatch decisions for the IEEE 18-bus and the IEEE 33-bus. The proposed RSS reduced the state space by 77%, where the remaining states reflect 98% of the original state space. Furthermore, the NNs applied for MG operational planning showed robust predictive performance with loss function values of 0.0228 and 0.0563 for the IEEE 18-bus and the IEEE 33-bus, respectively.",
        "DOI": "10.1016/j.simpat.2022.102583",
        "paper_author": "Darville J.",
        "affiliation_name": "University of Miami",
        "affiliation_city": "Coral Gables",
        "affiliation_country": "United States",
        "affiliation_id": "60029251",
        "affiliation_state": "FL"
    },
    {
        "paper_title": "Planted forest is catching up with natural forest in China in terms of carbon density and carbon storage",
        "publication": "Fundamental Research",
        "citied_by": "25",
        "cover_date": "2022-09-01",
        "Abstract": "Over the last several decades, China has taken multiple measures for afforestation and natural forest protection, including setting the goal of carbon neutrality by the middle of 21th century. In order to support the practice of relevant policies from the scientific perspective, it is essential to precisely estimate the carbon storage of arbor forest, as it plays an important role in the carbon cycle of ecosystems. In this study, we first used the latest four phases of national forest inventory data to investigate the variation of carbon storage for both natural and planted arbor forest in China during the covered period (1999-2018). Then we used machine leaning methods to simulate the carbon density based on various kinds of environmental factors and analyzed the contribution of each influencing factor. Our results demonstrate that the total carbon storage for arbor forest in China kept increasing over the last two decades, but this increment was mainly brought about by the continuous expansion of forest land. The gap of carbon sequestration between natural forest and planted forest showed a significant trend of reduction. Additionally, tree age was identified as the dominant factor for influencing the spatiotemporal variation of carbon density among all the independent variables while the impact of climatic factor was limited. Therefore, the future improvement of carbon sequestration of arbor forest in should mainly rely on additional projects of afforestation, reforestation, green space conservation and reduction of emissions in China. Conclusions of this study have important implications for policy makers and other stakeholders to evaluate the previous achievement of environmental projects and can also help to set future plans and finally realize the goals of carbon neutrality.",
        "DOI": "10.1016/j.fmre.2022.04.008",
        "paper_author": "Liang B.",
        "affiliation_name": "Beijing Forestry University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60006782",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "The Bank of Korea watch",
        "publication": "Journal of International Money and Finance",
        "citied_by": "0",
        "cover_date": "2022-09-01",
        "Abstract": "Traders closely monitor the Bank of Korea (BOK) base-rate decisions since the short rate is the primary factor in bond and currency valuations. The Survey of Professional Forecasters(SPF) has been widely used and is considered the most reliable BOK base-rate decision forecast. In this study, we investigate whether the SPF's prediction ability can be further improved. To this end, we use a dynamic multinomial ordered probit prediction model of the BOK base rate with a large number of predictors and apply a Bayesian variable selection algorithm. Through an empirical exercise, we show that our approach substantially outperforms the SPF in terms of out-of-sample prediction. The key predictors found are SPF, short-term bond yields, lagged base rate, federal funds rate, and inflation expectation survey data. Furthermore, allowing the prediction ability to change over time is essential for improving predictive accuracy.",
        "DOI": "10.1016/j.jimonfin.2022.102668",
        "paper_author": "Kim H.",
        "affiliation_name": "Bank of Korea",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea",
        "affiliation_id": "60175871",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "CO<inf>2</inf> emissions in the USA: new insights based on ANN approach",
        "publication": "Environmental Science and Pollution Research",
        "citied_by": "16",
        "cover_date": "2022-09-01",
        "Abstract": "The paper’s main aim is to forecast the carbon dioxide (CO2) emissions in the USA and its related components, analysing the contributions of each of those components to CO2 total volume. The empirical ground is a mix of non-linear tools, combining the artificial neural network (ANN) parametric method with a vector autoregressive (VAR) estimator. ANN includes 1 layer and 20 neurons, forecasting being based on the economic growth and net trade effects doubled by different types of renewable energy consumption. The accuracy of estimations for 14 targeted categories of CO2 emissions is ensured by 4360 observations, with 10 types of inputs over 1984M01–2020M04. ANN seems to offer superior forecasting accuracy compared with the widely used autoregressive methods, such as VAR model, but seems to be weak in capturing the output ‘spike’ forms. The main findings show that, although economic growth and net trade have an important contribution to the targeted outputs, the more prominent ones are wind, solar and total biomass energy consumption. Therefore, the CO2 emissions can be better controlled through non-polluting capacities, in parallel with the use of wind, solar and total biomass energies. The tool excellently predicts the CO2 emissions during pandemic crises being a good instrument in policy decisions. Modest contributions to CO2 prediction seem to have energy consumption generated by waste, hydroelectric power and renewable geothermal systems. This underlines an unclear current status given their collateral effects in environmental damages and high investment costs. The paper contributes to the literature in several ways. It is one of the first works focused on CO2 emissions forecasting in the USA based on a mixed approach by ANN and VAR types, considering an extended pallet of inputs to predict the volume of total CO2 emissions but also its components. As a novelty, the inputs combine both economic and environmental determinants. Not at least, the estimations are performed based on a large span, with monthly frequency.",
        "DOI": "10.1007/s11356-022-20615-1",
        "paper_author": "Mutascu M.",
        "affiliation_name": "Zeppelin University",
        "affiliation_city": "Friedrichshafen",
        "affiliation_country": "Germany",
        "affiliation_id": "60019062",
        "affiliation_state": "Baden-Wurttemberg"
    },
    {
        "paper_title": "A digital twin hierarchy for metal additive manufacturing",
        "publication": "Computers in Industry",
        "citied_by": "62",
        "cover_date": "2022-09-01",
        "Abstract": "Digital twins present a conceptual framework for product life-cycle monitoring and control using a simulated replica of the physical system. Since their emergence, they have garnered particular attention as a shift away from costly physical testing and towards the use of high fidelity simulations, sensor data and intelligent control. Metal additive manufacturing (AM), a 3D printing technology prone to defects, requires a digital twin capable of tackling issues of printed part qualification, certification and optimisation. In this paper, we evaluate the key features specific to metal AM and review the current literature of modelling, sensing, control and machine intelligence. We find that the body of research toward the development of an metal additive manufacturing (AM) digital twin can be organised logically into a hierarchy of four levels of increasing complexity. The elements composing each level require deep integration and we highlight the key enabling technologies: surrogate modelling, in-situ sensing, hardware control systems and intelligent control policies. Our proposed digital twin hierarchy for AM provides a developer framework for engineering digital twins, both for AM and other intelligent manufacturing systems.",
        "DOI": "10.1016/j.compind.2022.103667",
        "paper_author": "Phua A.",
        "affiliation_name": "Commonwealth Scientific and Industrial Research Organisation",
        "affiliation_city": "Canberra",
        "affiliation_country": "Australia",
        "affiliation_id": "60029470",
        "affiliation_state": "ACT"
    },
    {
        "paper_title": "An assessment of meteorological parameters effects on COVID-19 pandemic in Bangladesh using machine learning models",
        "publication": "Environmental Science and Pollution Research",
        "citied_by": "9",
        "cover_date": "2022-09-01",
        "Abstract": "Coronavirus (COVID-19) is a highly contagious virus (SARS-CoV-2) that has caused a global pandemic since January 2020. Scientists around the world are doing extensive research to control this disease. They are working tirelessly to find out the origin and causes of the disease. Several studies and experiments mentioned that there are some meteorological parameters which are highly correlated with COVID-19 transmission. In this work, we studied the effects of 11 meteorological parameters on the transmission of COVID-19 in Bangladesh. We first applied statistical analysis and observed that there is no significant effect of these parameters. Therefore, we proposed a novel technique to analyze the insight effects of these parameters by using a combination of Random Forest, CART, and Lasso feature selection techniques. We observed that 4 parameters are highly influential for COVID-19 where Tmin and Cloud have positive association whereas WS and AQ have negative impact. Among them, Cloud has the highest positive impact which is 0.063 and WS has the highest negative association which is - 0.021. Moreover, we have validated our performance using DLNM technique. The result of this investigation can be used to develop an alert system that will assist the policymakers to know the characteristics of COVID-19 against meteorological parameters and can impose different policies based on the weather conditions.",
        "DOI": "10.1007/s11356-022-20196-z",
        "paper_author": "Karmokar J.",
        "affiliation_name": "Bangladesh Agricultural University",
        "affiliation_city": "Mymensingh",
        "affiliation_country": "Bangladesh",
        "affiliation_id": "60015047",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Development of methods for identifying an appropriate benchmarking peer to establish information security policy",
        "publication": "Expert Systems with Applications",
        "citied_by": "6",
        "cover_date": "2022-09-01",
        "Abstract": "Benchmarking methodology provides organizations with appropriate information security policy. However, selecting an appropriate organization as a benchmarking peer can be a challenge due to firms’ heterogeneous implementation and usage of information systems. Our goal is to develop and propose methods to appropriately identify a benchmarking peer organization by incorporating machine learning methods and mathematics set theory. We incorporate vague soft set, entropy, dynamic time warping, and Gaussian process. We use log data from information security management systems in multiple companies to validate our methods. Our experimental results indicate that the combined use of Gaussian process, vague soft set, and dynamic time warping can be more effective in identifying an appropriate benchmarking peer than conventional machine learning methods.",
        "DOI": "10.1016/j.eswa.2022.117028",
        "paper_author": "Kang M.",
        "affiliation_name": "Mississippi State University",
        "affiliation_city": "Mississippi State",
        "affiliation_country": "United States",
        "affiliation_id": "60001526",
        "affiliation_state": "MS"
    },
    {
        "paper_title": "A Novel Data-Driven Method for Behind-the-Meter Solar Generation Disaggregation with Cross-Iteration Refinement",
        "publication": "IEEE Transactions on Smart Grid",
        "citied_by": "9",
        "cover_date": "2022-09-01",
        "Abstract": "Photovoltaic (PV) generation is increasing in distribution systems following policies and incentives to promote zero-carbon emission societies. Most residential PV systems are installed behind-the-meter (BTM). Due to single meter deployment that measures the net load only, this PV generation is invisible to distribution system operators causing a negative impact on the distribution system planning and local supply and demand balance. This paper proposes a novel data-driven BTM PV generation disaggregation method using only net load and weather data, without relying on other PV proxies and PV panels' physical models. Long Short-Term Memory (LSTM) is employed to build a generation difference fitted model (GDFM) and a consumption difference fitted model (CDFM) derived from weather data. Both difference fitted models are refined by a cross-iteration with mutual output. Finally, considering the photoelectric conversion properties, the disaggregated generation results are acquired by the refined GDFM of changing input. The proposed method has been tested with actual smart meter data of Austin, Texas and proves to increase the disaggregated accuracy as compared to current state-of-the-art methods. The proposed method is also applicable to disaggregate BTM PV systems of different manufacturing processes and types.",
        "DOI": "10.1109/TSG.2022.3171656",
        "paper_author": "Pan K.",
        "affiliation_name": "Guangdong University of Technology",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60007155",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Low-carbon economy and policy implications: a systematic review and bibliometric analysis",
        "publication": "Environmental Science and Pollution Research",
        "citied_by": "27",
        "cover_date": "2022-09-01",
        "Abstract": "In the face of the rapid increase of carbon emissions, climate warming, and an epidemic situation, low-carbon economy is attracting growing attention. Using bibliometric analysis and machine learning methods, the paper conducts a systematic review in the low-carbon economy. Using the Web of Science Core Collection database, 1433 articles from 1990 to 2021 were selected for review. We find that the trajectories of the low-carbon economy research can be divided into four phases: exploration, fermentation, rising, and flourishing. The low-carbon economy research can be categorized into five clusters: low-carbon energy policy, carbon footprint and carbon trading, energy–economy–environment system, energy efficiency and its decomposition, and carbon emission drivers. The findings of this review study shed light on the role and effects of low-carbon economic policies on energy futures.",
        "DOI": "10.1007/s11356-022-20381-0",
        "paper_author": "Wang J.",
        "affiliation_name": "Renmin University of China",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60014402",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "RAFALE: Rethinking the provisioning of virtuAl network services using a Fast and scAlable machine LEarning approach",
        "publication": "Journal of Supercomputing",
        "citied_by": "0",
        "cover_date": "2022-09-01",
        "Abstract": "Network Function Virtualization (NFV) has been identified to revamp the provisioning of next-generation network services. This new paradigm allows cloud and network/service providers to compose their network services, also known as service function chains (SFCs), in an agile way since the software of the network function is decoupled from the legacy hardware. To reap the benefits of this new technology, there is a need for novel mechanisms that help cloud and network/service providers deploy the increasingly complex virtual network services seamlessly, efficiently, and in a time-efficient way. Existing state-of-the-art techniques often rely on the Integer Linear Programming framework, heuristics/metaheuristics, and greedy methods to deploy the services function chains. However, these techniques although reasonable and acceptable, still suffer from several key limitations: convergence time and scalability. To this end, we propose RAFALE, a suite of solution techniques, to tame this complexity by leveraging the concept of similarity from machine learning and skip-gram modeling framework. To the best of our knowledge, we are the first to tackle these key limitations and propose a suite of solutions to them. RAFALE, a novel approach proposed to find the similarity between the new incoming virtual network service request and all the already-deployed services to learn from the previous experience of deploying techniques and use the same or close similar provisioning techniques. RAFALE is the first and the only method that develops the idea of detecting the similarity between virtual network services. Experimental results show that RAFALE reduces greatly the convergence time needed for provisioning virtual network services and can scale to 100 virtual network functions per virtual network service compared to the state-of-the-art. The Experimental results prove that RAFALE accomplished the NFV promises; decreasing the time and complexity of managing and deploying the virtual services, and providing a solution that is agile, faster, and scalable to deploy the new service requests by skipping one or more service provisioning steps (i.e., detecting and resolving the conflicts among policies, placement, and chaining) while satisfying the validated NFV policies.",
        "DOI": "10.1007/s11227-022-04492-6",
        "paper_author": "Suwi H.",
        "affiliation_name": "École de Technologie Supérieure",
        "affiliation_city": "Montreal",
        "affiliation_country": "Canada",
        "affiliation_id": "60026786",
        "affiliation_state": "QC"
    },
    {
        "paper_title": "Recommendations for developing a lifecycle, multidimensional assessment framework for mobile medical apps",
        "publication": "Health Economics (United Kingdom)",
        "citied_by": "21",
        "cover_date": "2022-09-01",
        "Abstract": "Digital health and mobile medical apps (MMAs) have shown great promise in transforming health care, but their adoption in clinical care has been unsatisfactory, and regulatory guidance and coverage decisions have been lacking or incomplete. A multidimensional assessment framework for regulatory, policymaking, health technology assessment, and coverage purposes based on the MMA lifecycle is needed. A targeted review of relevant policy documents from international sources was conducted to map current MMA assessment frameworks, to formulate 10 recommendations, subsequently shared amongst an expert panel of key stakeholders. Recommendations go beyond economic dimensions such as cost and economic evaluation and also include MMA development and update, classification and evidentiary requirements, performance and maintenance monitoring, usability testing, clinical evidence requirements, safety and security, equity considerations, organizational assessment, and additional outcome domains (patient empowerment and environmental impact). The COVID-19 pandemic greatly expanded the use of MMAs, but temporary policies governing their use and oversight need consolidation through well-developed frameworks to support decision-makers, producers and introduction into clinical care processes, especially in light of the strong international, cross-border character of MMAs, the new EU medical device and health technology assessment regulations, and the Next Generation EU funding earmarked for health digitalization.",
        "DOI": "10.1002/hec.4505",
        "paper_author": "Tarricone R.",
        "affiliation_name": "Università Bocconi",
        "affiliation_city": "Milan",
        "affiliation_country": "Italy",
        "affiliation_id": "60021796",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Regressive Class Modelling for Predicting Trajectories of COVID-19 Fatalities Using Statistical and Machine Learning Models",
        "publication": "Bulletin of the Malaysian Mathematical Sciences Society",
        "citied_by": "0",
        "cover_date": "2022-09-01",
        "Abstract": "The COVID-19 (SARS-CoV-2 virus) pandemic has led to a substantial loss of human life worldwide by providing an unparalleled challenge to the public health system. The economic, psychological, and social disarray generated by the COVID-19 pandemic is devastating. Public health experts and epidemiologists worldwide are struggling to formulate policies on how to control this pandemic as there is no effective vaccine or treatment available which provide long-term immunity against different variants of COVID-19 and to eradicate this virus completely. As the new cases and fatalities are recorded daily or weekly, the responses are likely to be repeated or longitudinally correlated. Thus, studying the impact of available covariates and new cases on deaths from COVID-19 repeatedly would provide significant insights into this pandemic’s dynamics. For a better understanding of the dynamics of spread, in this paper, we study the impact of various risk factors on the new cases and deaths over time. To do that, we propose a marginal-conditional based joint modelling approach to predict trajectories, which is crucial to the health policy planners for taking necessary measures. The conditional model is a natural choice to study the underlying property of dependence in consecutive new cases and deaths. Using this model, one can examine the relationship between outcomes and predictors, and it is possible to calculate risks of the sequence of events repeatedly. The advantage of repeated measures is that one can see how individual responses change over time. The predictive accuracy of the proposed model is also compared with various machine learning techniques. The machine learning algorithms used in this paper are extended to accommodate repeated responses. The performance of the proposed model is illustrated using COVID-19 data collected from the Texas Health and Human Services.",
        "DOI": "10.1007/s40840-022-01287-z",
        "paper_author": "Chowdhury R.I.",
        "affiliation_name": "University of Prince Edward Island",
        "affiliation_city": "Charlottetown",
        "affiliation_country": "Canada",
        "affiliation_id": "60007655",
        "affiliation_state": "PE"
    },
    {
        "paper_title": "Does a Carbon Tax Reduce CO<inf>2</inf> Emissions? Evidence from British Columbia",
        "publication": "Environmental and Resource Economics",
        "citied_by": "47",
        "cover_date": "2022-09-01",
        "Abstract": "Using difference-in-differences, synthetic control, and introducing a new break-detection approach, I show that the introduction of North America’s first major carbon tax has reduced transportation emissions but not ‘yet’ led to large statistically significant reductions in aggregate CO2 emissions. Proposing a new method to assess policy based on breaks in difference-in-differences using machine learning, I demonstrate that neither carbon pricing nor trading schemes in other provinces are detected as large and statistically significant interventions. Instead, closures and efficiency-improvements in emission-intense industries in untaxed provinces have reduced emissions. Overall, the results show that existing carbon taxes (and prices) are likely too low to be effective in the time frame since their introduction.",
        "DOI": "10.1007/s10640-022-00679-w",
        "paper_author": "Pretis F.",
        "affiliation_name": "University of Victoria",
        "affiliation_city": "Victoria",
        "affiliation_country": "Canada",
        "affiliation_id": "60003122",
        "affiliation_state": "BC"
    },
    {
        "paper_title": "Meta-Reinforcement Learning for Reliable Communication in THz/VLC Wireless VR Networks",
        "publication": "IEEE Transactions on Wireless Communications",
        "citied_by": "22",
        "cover_date": "2022-09-01",
        "Abstract": "In this paper, the problem of enhancing the quality of virtual reality (VR) services is studied for an indoor terahertz (THz)/visible light communication (VLC) wireless network. In the studied model, small base stations (SBSs) transmit high-quality VR images to VR users over THz bands and light-emitting diodes (LEDs) provide accurate indoor positioning services for them using VLC. Here, VR users move in real time and their movement patterns change over time according to their applications, where both THz and VLC links can be blocked by the bodies of VR users. To control the energy consumption of the studied THz/VLC wireless VR network, VLC access points (VAPs) must be selectively turned on so as to ensure accurate and extensive positioning for VR users. Based on the user positions, each SBS must generate corresponding VR images and establish THz links without body blockage to transmit the VR content. The problem is formulated as an optimization problem whose goal is to maximize the average number of successfully served VR users by selecting the appropriate VAPs to be turned on and controlling the user association with SBSs. To solve this problem, a policy gradient-based reinforcement learning (RL) algorithm that adopts a meta-learning approach is proposed. The proposed meta policy gradient (MPG) algorithm enables the trained policy to quickly adapt to new user movement patterns. In order to solve the problem of maximizing the average number of successfully served users for VR scenarios with large numbers of users, a low-complexity dual method based MPG algorithm (D-MPG) with a low complexity is proposed. Simulation results demonstrate that, compared to a baseline trust region policy optimization algorithm (TRPO), the proposed MPG and D-MPG algorithms yield up to 26.8% and 21.9% improvement in the average number of successfully served users as well as 81.2% and 87.5% gains in the convergence speed, respectively.",
        "DOI": "10.1109/TWC.2022.3161970",
        "paper_author": "Wang Y.",
        "affiliation_name": "Beijing University of Posts and Telecommunications",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60016930",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Application of convolutional neural network fused with machine learning modeling framework for geospatial comparative analysis of landslide susceptibility",
        "publication": "Natural Hazards",
        "citied_by": "20",
        "cover_date": "2022-09-01",
        "Abstract": "Landslides in mountain settlements are among the most widespread and dangerous geohazards. In this study, we aimed to assess landslide susceptibility using Wenchuan, southwest China, as a case. For this purpose, we constructed an optimization method that combines a convolutional neural network with the machine learning algorithm of support vector machines, quadratic discriminant analysis, Bayesian optimized gradient boosting tree, and Bayesian optimized random forest. The model inputs were 13,886 historical seismic-induced landslide events interpreted from remote sensing imagery and ten evaluation features: elevation, slope angle, slope aspect, plan curvature, profile curvature, distance to roads, distance to rivers, distance to faults, land use pattern, and soil texture. The output was the probability of landslide occurrence for each prediction unit. Finally, we evaluated the assessed outcomes using both the receiver operating characteristic curve and 1074 latest recorded landslide dataset (2013–2020). The calculations showed that the overall susceptibility values to landslides in the high–very high interval produced by the hybrid convolutional neural networks was 9.95%–16.91%, which is close to the actual landslide susceptibility of the region. The receiver operating characteristic curve and statistical analysis of the latest landslide event outcomes demonstrated that the hybrid Bayesian optimized gradient boosting tree model had a higher classification accuracy than the other classifiers presented in this study. The research findings are available to local governments and disaster management authorities in guiding disaster prevention, mitigation policy formulation, and land use and provide reference value for evaluating landslide susceptibility in other mountainous areas.",
        "DOI": "10.1007/s11069-022-05326-7",
        "paper_author": "Gao Z.",
        "affiliation_name": "Southwest Jiaotong University",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China",
        "affiliation_id": "60010421",
        "affiliation_state": "Sichuan"
    },
    {
        "paper_title": "Development of machine learning multi-city model for municipal solid waste generation prediction",
        "publication": "Frontiers of Environmental Science and Engineering",
        "citied_by": "35",
        "cover_date": "2022-09-01",
        "Abstract": "Integrated management of municipal solid waste (MSW) is a major environmental challenge encountered by many countries. To support waste treatment/management and national macroeconomic policy development, it is essential to develop a prediction model. With this motivation, a database of MSW generation and feature variables covering 130 cities across China is constructed. Based on the database, advanced machine learning (gradient boost regression tree) algorithm is adopted to build the waste generation prediction model, i.e., WGMod. In the model development process, the main influencing factors on MSW generation are identified by weight analysis. The selected key influencing factors are annual precipitation, population density and annual mean temperature with the weights of 13%, 11% and 10%, respectively. The WGMod shows good performance with R2 = 0.939. Model prediction on MSW generation in Beijing and Shenzhen indicates that waste generation in Beijing would increase gradually in the next 3–5 years, while that in Shenzhen would grow rapidly in the next 3 years. The difference between the two is predominately driven by the different trends of population growth. [Figure not available: see fulltext.].",
        "DOI": "10.1007/s11783-022-1551-6",
        "paper_author": "Lu W.",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60025278",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Drivers and forecasts of multiple waves of the coronavirus disease 2019 pandemic: A systematic analysis based on an interpretable machine learning framework",
        "publication": "Transboundary and Emerging Diseases",
        "citied_by": "6",
        "cover_date": "2022-09-01",
        "Abstract": "Coronavirus disease 2019 (COVID-19) has become a global pandemic and continues to prevail with multiple rebound waves in many countries. The driving factors for the spread of COVID-19 and their quantitative contributions, especially to rebound waves, are not well studied. Multidimensional time-series data, including policy, travel, medical, socioeconomic, environmental, mutant and vaccine-related data, were collected from 39 countries up to 30 June 2021, and an interpretable machine learning framework (XGBoost model with Shapley Additive explanation interpretation) was used to systematically analyze the effect of multiple factors on the spread of COVID-19, using the daily effective reproduction number as an indicator. Based on a model of the pre-vaccine era, policy-related factors were shown to be the main drivers of the spread of COVID-19, with a contribution of 60.81%. In the post-vaccine era, the contribution of policy-related factors decreased to 28.34%, accompanied by an increase in the contribution of travel-related factors, such as domestic flights, and contributions emerged for mutant-related (16.49%) and vaccine-related (7.06%) factors. For single-peak countries, the dominant ones were policy-related factors during both the rising and fading stages, with overall contributions of 33.7% and 37.7%, respectively. For double-peak countries, factors from the rebound stage contributed 45.8% and policy-related factors showed the greatest contribution in both the rebound (32.6%) and fading (25.0%) stages. For multiple-peak countries, the Delta variant, domestic flights (current month) and the daily vaccination population are the three greatest contributors (8.12%, 7.59% and 7.26%, respectively). Forecasting models to predict the rebound risk were built based on these findings, with accuracies of 0.78 and 0.81 for the pre- and post-vaccine eras, respectively. These findings quantitatively demonstrate the systematic drivers of the spread of COVID-19, and the framework proposed in this study will facilitate the targeted prevention and control of the ongoing COVID-19 pandemic.",
        "DOI": "10.1111/tbed.14492",
        "paper_author": "Cao Z.",
        "affiliation_name": "Sun Yat-Sen University",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60021182",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Context-Aware Wireless Connectivity and Processing Unit Optimization for IoT Networks",
        "publication": "IEEE Internet of Things Journal",
        "citied_by": "6",
        "cover_date": "2022-09-01",
        "Abstract": "A novel approach is presented in this work for context-aware connectivity and processing optimization of Internet of Things (IoT) networks. Different from the state-of-the-art approaches, the proposed approach simultaneously selects the best connectivity and processing unit (e.g., device, fog, and cloud) along with the percentage of data to be offloaded by jointly optimizing energy consumption, response time, security, and monetary cost. The proposed scheme employs a reinforcement learning algorithm and manages to achieve significant gains compared to deterministic solutions. In particular, the requirements of IoT devices in terms of response time and security are taken as inputs along with the remaining battery level of the devices, and the developed algorithm returns an optimized policy. The results obtained show that only our method is able to meet the holistic multiobjective optimization criteria, albeit, the benchmark approaches may achieve better results on a particular metric at the cost of failing to reach the other targets. Thus, the proposed approach is a device-centric and context-aware solution that accounts for the monetary and battery constraints.",
        "DOI": "10.1109/JIOT.2022.3152381",
        "paper_author": "Ozturk M.",
        "affiliation_name": "Ankara Yildirim Beyazit University",
        "affiliation_city": "Ankara",
        "affiliation_country": "Turkey",
        "affiliation_id": "60106507",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Federated Learning Over Wireless IoT Networks With Optimized Communication and Resources",
        "publication": "IEEE Internet of Things Journal",
        "citied_by": "90",
        "cover_date": "2022-09-01",
        "Abstract": "To leverage massive distributed data and computation resources, machine learning in the network edge is considered to be a promising technique, especially for large-scale model training. Federated learning (FL), as a paradigm of collaborative learning techniques, has obtained increasing research attention with the benefits of communication efficiency and improved data privacy. Due to the lossy communication channels and limited communication resources (e.g., bandwidth and power), it is of interest to investigate fast responding and accurate FL schemes over wireless systems. Hence, we investigate the problem of jointly optimized communication efficiency and resources for FL over wireless Internet of Things (IoT) networks. To reduce complexity, we divide the overall optimization problem into two subproblems, i.e., the client scheduling problem and the resource allocation problem. To reduce the communication costs for FL in wireless IoT networks, a new client scheduling policy is proposed by reusing stale local model parameters. To maximize successful information exchange over networks, a Lagrange multiplier method is first leveraged by decoupling variables, including power variables, bandwidth variables, and transmission indicators. Then, a linear-search-based power and bandwidth allocation method is developed. Given appropriate hyperparameters, we show that the proposed communication-efficient FL (CEFL) framework converges at a strong linear rate. Through extensive experiments, it is revealed that the proposed CEFL framework substantially boosts both the communication efficiency and learning performance of both training loss and test accuracy for FL over wireless IoT networks compared to a basic FL approach with uniform resource allocation.",
        "DOI": "10.1109/JIOT.2022.3151193",
        "paper_author": "Chen H.",
        "affiliation_name": "The Royal Institute of Technology (KTH)",
        "affiliation_city": "Stockholm",
        "affiliation_country": "Sweden",
        "affiliation_id": "60002014",
        "affiliation_state": "Stockholms"
    },
    {
        "paper_title": "Linking cognitive processes and learning outcomes: The influence of cognitive presence on learning performance in MOOCs",
        "publication": "British Journal of Educational Technology",
        "citied_by": "11",
        "cover_date": "2022-09-01",
        "Abstract": "Massively open online courses (MOOCs) offer learners opportunities for self-improvement and knowledge development. Linking cognitive processes and learning outcomes is helpful in supporting students learning with the help of MOOCs. Based on the cognitive presence of the Community of Inquiry framework, this study quantified the effect of cognitive presence on students’ learning performance by analysing more than 400,000 posts of 13 MOOCs. First, the study built a highly predictive classification model using a machine learning algorithm to automatically identify the phases of cognitive presence in MOOC forum posts. Subsequently, multilevel modelling was used to analyse the influence of learners’ cognitive presence on their learning performance. The results showed that different phases of cognitive presence influenced students’ learning differently. The findings help us understand the phases and depth of students’ cognitive presence and can be used as the basis for MOOCs to automatically provide appropriate cognitive feedback and support to students. Practitioner notes What is already known about this topic Massively open online courses (MOOCs) offer learners opportunities for self-improvement and knowledge development. Linking cognitive processes and learning outcomes is helpful in supporting student learning in MOOCs. Little is known so far about the role of cognitive presence in influencing students’ learning performance. What this paper adds This study built a highly predictive classification model using a machine learning algorithm to automatically identify the phases of cognitive presence in MOOC forum posts. Multilevel modelling was used to quantify the influence of learners’ cognitive presence on their learning performance. The results showed that different phases of cognitive presence influenced students’ learning differently. Implications for practice and/or policy This study built an effective, high-performance machine learning model with easy feature extraction, which could be used to automatically identify students’ cognitive processes in MOOC forums and beyond. The combination of cognitive presence and learning performance helps us understand the phases and depth of students’ cognitive presence. The findings can be used as the basis for MOOCs to automatically provide appropriate cognitive feedback and support to students.",
        "DOI": "10.1111/bjet.13193",
        "paper_author": "Liu B.",
        "affiliation_name": "Central China Normal University",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60010591",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Prediction of combustion reactivity for lignocellulosic fuels by means of machine learning",
        "publication": "Journal of Thermal Analysis and Calorimetry",
        "citied_by": "10",
        "cover_date": "2022-09-01",
        "Abstract": "Energy demand and environmental concerns made biomass a sustainable energy source that can be used as a substitute for coal in many applications. Therefore, the combustion efficiency of biomass is a major concern for policy makers and engineers. Thermogravimetric analysis (TGA) is a robust method for determining the combustion characteristics of biomass using combustion index. TGA instruments, on the other hand, are quite expensive, and performing the experiments themselves requires a lot of time and a trained operator. Developing a method that is both faster and more reliable to obtain combustion characteristics without the use of TGA is therefore very important. In this study, a machine learning approach based on artificial neural network (ANN) was developed to predict the instantaneous combustion index defined for biomass combustion process with the help of biomass properties and combustion conditions, without using instruments and complex equations. Thus, a total of 6721 data sets were generated by using the 24 thermogravimetric experiments which conducted in this work. The Bayesian regularization optimization algorithm was used to train the developed ANN model, which is based on a multilayer perceptron architecture. The results showed that there was good agreement between the predicted and measured values of the combustion index for the training, testing, and external validation data sets. The mean absolute percentage error (MAPE), regression coefficient (R2), root-mean-square error (RMSE) for each biomass sample under the different experimental conditions were investigated, and the results were found to be satisfactory with R2 > 0.99.",
        "DOI": "10.1007/s10973-022-11208-8",
        "paper_author": "Sezer S.",
        "affiliation_name": "Marmara Üniversitesi",
        "affiliation_city": "Istanbul",
        "affiliation_country": "Türkiye",
        "affiliation_id": "60027981",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Survey on Imitation Learning Techniques for End-to-End Autonomous Vehicles",
        "publication": "IEEE Transactions on Intelligent Transportation Systems",
        "citied_by": "127",
        "cover_date": "2022-09-01",
        "Abstract": "The state-of-the-art decision and planning approaches for autonomous vehicles have moved away from manually designed systems, instead focusing on the utilisation of large-scale datasets of expert demonstration via Imitation Learning (IL). In this paper, we present a comprehensive review of IL approaches, primarily for the paradigm of end-to-end based systems in autonomous vehicles. We classify the literature into three distinct categories: 1) Behavioural Cloning (BC), 2) Direct Policy Learning (DPL) and 3) Inverse Reinforcement Learning (IRL). For each of these categories, the current state-of-the-art literature is comprehensively reviewed and summarised, with future directions of research identified to facilitate the development of imitation learning based systems for end-to-end autonomous vehicles. Due to the data-intensive nature of deep learning techniques, currently available datasets and simulators for end-to-end autonomous driving are also reviewed.",
        "DOI": "10.1109/TITS.2022.3144867",
        "paper_author": "Le Mero L.",
        "affiliation_name": "Warwick Manufacturing Group",
        "affiliation_city": "Coventry",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60163100",
        "affiliation_state": "West Midlands"
    },
    {
        "paper_title": "Hierarchical Human-robot Cooperative Control Based on GPR and Deep Reinforcement Learning",
        "publication": "Zidonghua Xuebao/Acta Automatica Sinica",
        "citied_by": "5",
        "cover_date": "2022-09-01",
        "Abstract": "In this paper, a hierarchical human-robot collaboration control problem is investigated by Gaussian process regression and deep reinforcement learning approaches, and a ball and beam system controlled jointly by human and robot is used to verify the proposed method. The main contributions are as follows: 1) To deal with the problem that the classical control method can not be directly used in the human-robot collaboration scenario without a known model, a deep reinforcement learning algorithm is adopted to design an effective nonlinear suboptimal policy without the system model, and this suboptimal policy is considered as the expected control policy to guide the Human-robot collaboration process; 2) To weaken the negative influences caused by the unknown and random human-control strategies, the Gaussian process regression method is used to fit the human-control strategies and build the cognitive model of robot for human control behaviors, which can improve the efficiency of collaboration by enhancing the initiative of the robot through the Human-robot collaboration process; 3) A controller for the end-effector velocity is designed based on the cognitive model and the expected control policy, and the effectiveness of the proposed method is verified by experimental comparison.",
        "DOI": "10.16383/j.aas.c190451",
        "paper_author": "Jin Z.H.",
        "affiliation_name": "Zhejiang University of Technology",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60026282",
        "affiliation_state": "Zhejiang"
    },
    {
        "paper_title": "Post-purchase Federal Financial Aid: How (in)Effective is the IRS’s Student Loan Interest Deduction (SLID) in Reaching Lower-Income Taxpayers and Students?",
        "publication": "Research in Higher Education",
        "citied_by": "5",
        "cover_date": "2022-09-01",
        "Abstract": "Federal financial aid policies for higher education may be classified based on their “for-purchase” and “post-purchase” natures. The former include grants, loans, and workstudy and intend to help students finance or afford college attendance, persistence, and graduation. Post-purchase policies are designed to minimize financial burdens associated with having invested in college attendance and are granted as tax incentives/expenditures. One of these expenditures is the IRS’s Student Loan Interest Deduction (SLID)—which offers up to $2500 as an adjustment for taxable income based on having paid interest on student loans and has an annual cost of $12.81 billion—about 45.7% of the Pell grant cost. Despite this high cost, SLID has remained virtually unstudied. Accordingly, the study’s purpose is to assess how (in)effective SLID may be in reaching lower-income taxpayers. To address this purpose, we relied on an innovative analytic framework “multilevel modelling with spatial interaction effects” that allowed controlling for contextual and systemic observed and unobserved factors that may both affect college participation and may be related with SLID disbursements over and above income prospects. Data sources included the IRS, ACS, FBI, IPEDS, and the NPSAS:2015–2016. Findings revealed that SLID is regressive at the top, wealthier taxpayers and students attending more expensive colleges realize higher tax benefits than lower income taxpayers and students. Indeed, 75% of community college students were found to not be eligible to receive SLID—data and replication code (https://cutt.ly/COyfdKC) are provided. Is this the best use of this multibillion tax incentive? Is SLID designed to exclude the poorest, neediest students? A policy similar to Education Credits, focused on outstanding debt rather than on interest, that targets below-poverty line students with up to $5000 in debt, would represent a true commitment, and better use of public funds, to close socioeconomic gaps, by helping those more prone to default.",
        "DOI": "10.1007/s11162-021-09672-6",
        "paper_author": "González Canché M.S.",
        "affiliation_name": "University of Pennsylvania",
        "affiliation_city": "Philadelphia",
        "affiliation_country": "United States",
        "affiliation_id": "60006297",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "Finding nash equilibrium for imperfect information games via fictitious play based on local regret minimization",
        "publication": "International Journal of Intelligent Systems",
        "citied_by": "7",
        "cover_date": "2022-09-01",
        "Abstract": "Finding Nash equilibrium in the domain of imperfect information games as a challenging problem has received much attention. Neural Fictitious Self-Play (NFSP) is a popular model-free machine learning algorithm and has computed approximate Nash equilibrium on such games. However, the deep reinforcement learning method used to approximate the best response in NFSP requires reaching a fully observable Markov state, while the states in imperfect information games are partially observable and non-Markovian, which results in a poor approximation of the best response. Thus, NFSP needs more iterations to converge. In this study, we present a new reinforcement learning method that is inspired by counterfactual regret minimization to relax the Markov requirement by iteratively updating policy according to the regret matching process. Combining this new reinforcement learning algorithm with fictitious play, we further present a novel algorithm to find approximate Nash equilibrium in zero-sum imperfect information games. Experimental results in three benchmark games show that this new algorithm can find approximate Nash equilibrium effectively and converge much faster compared with baseline.",
        "DOI": "10.1002/int.22837",
        "paper_author": "He K.",
        "affiliation_name": "Sichuan University",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China",
        "affiliation_id": "60016521",
        "affiliation_state": "Sichuan"
    },
    {
        "paper_title": "Data distribution debugging in machine learning pipelines",
        "publication": "VLDB Journal",
        "citied_by": "15",
        "cover_date": "2022-09-01",
        "Abstract": "Machine learning (ML) is increasingly used to automate impactful decisions, and the risks arising from this widespread use are garnering attention from policy makers, scientists, and the media. ML applications are often brittle with respect to their input data, which leads to concerns about their correctness, reliability, and fairness. In this paper, we describe mlinspect, a library that helps diagnose and mitigate technical bias that may arise during preprocessing steps in an ML pipeline. We refer to these problems collectively as data distribution bugs. The key idea is to extract a directed acyclic graph representation of the dataflow from a preprocessing pipeline and to use this representation to automatically instrument the code with predefined inspections. These inspections are based on a lightweight annotation propagation approach to propagate metadata such as lineage information from operator to operator. In contrast to existing work, mlinspect operates on declarative abstractions of popular data science libraries like estimator/transformer pipelines and does not require manual code instrumentation. We discuss the design and implementation of the mlinspect library and give a comprehensive end-to-end example that illustrates its functionality.",
        "DOI": "10.1007/s00778-021-00726-w",
        "paper_author": "Grafberger S.",
        "affiliation_name": "Universiteit van Amsterdam",
        "affiliation_city": "Amsterdam",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60002483",
        "affiliation_state": "Noord-Holland"
    },
    {
        "paper_title": "The True Human Cost of the Novel Coronavirus 2019 (COVID-19) Pandemic",
        "publication": "Indian Journal of Medical and Paediatric Oncology",
        "citied_by": "0",
        "cover_date": "2022-09-01",
        "Abstract": "NA",
        "DOI": "10.1055/s-0041-1740320",
        "paper_author": "Chaubal R.",
        "affiliation_name": "Tata Memorial Hospital",
        "affiliation_city": "Mumbai",
        "affiliation_country": "India",
        "affiliation_id": "60018517",
        "affiliation_state": "MH"
    },
    {
        "paper_title": "An Adaptive Hierarchical Energy Management Strategy for Hybrid Electric Vehicles Combining Heuristic Domain Knowledge and Data-Driven Deep Reinforcement Learning",
        "publication": "IEEE Transactions on Transportation Electrification",
        "citied_by": "31",
        "cover_date": "2022-09-01",
        "Abstract": "With the development of artificial intelligence, there has been a growing interest in machine learning-based control strategy, among which reinforcement learning (RL) has opened up a new direction in the field of hybrid electric vehicle (HEV) energy management. However, the issues of the current RL setting ranging from inappropriate battery state-of-charge (SOC) constraint to ineffective and risky exploration make it inapplicable to many industrial energy management strategy (EMS) tasks. To address this, an adaptive hierarchical EMS combining heuristic equivalent consumption minimization strategy (ECMS) knowledge and deep deterministic policy gradient (DDPG), which is a state-of-the-art data-driven RL algorithm, is proposed in this work. For comparison purposes, the proposed strategy is contrasted with dynamic programming (DP), proportion integration differentiation (PID)-based adaptive ECMS, and rule-based and standard RL-based counterparts, and the results show that the fuel consumption after SOC correction for the proposed strategy is very close to that of the DP-based control and lower than that of the other three benchmark strategies. Considering that the proposed strategy can make better use of the RL techniques while realizing an effective, efficient, and safe exploration in a data-driven manner, it may become a strong foothold for future RL-based EMS to build on, especially when the controller has to be trained directly and from scratch in a real-world environment.",
        "DOI": "10.1109/TTE.2021.3132773",
        "paper_author": "Hu B.",
        "affiliation_name": "Chongqing University of Technology",
        "affiliation_city": "Chongqing",
        "affiliation_country": "China",
        "affiliation_id": "60031991",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Multimodal Perception-Driven Self Evolving Autonomous Ground Vehicle",
        "publication": "IEEE Transactions on Cybernetics",
        "citied_by": "9",
        "cover_date": "2022-09-01",
        "Abstract": "Increasingly complex automated driving functions, specifically those associated with free space detection (FSD), are delegated to convolutional neural networks (CNNs). If the dataset used to train the network lacks diversity, modality, or sufficient quantities, the driver policy that controls the vehicle may induce safety risks. Although most autonomous ground vehicles (AGVs) perform well in structured surroundings, the need for human intervention significantly rises when presented with unstructured niche environments. To this end, we developed an AGV for seamless indoor and outdoor navigation to collect realistic multimodal data streams. We demonstrate one application of the AGV when applied to a self-evolving FSD framework that leverages online active machine-learning (ML) paradigms and sensor data fusion. In essence, the self-evolving AGV queries image data against a reliable data stream, ultrasound, before fusing the sensor data to improve robustness. We compare the proposed framework to one of the most prominent free space segmentation methods, DeepLabV3+ [1]. DeepLabV3+ [1] is a state-of-the-art semantic segmentation model composed of a CNN and an autodecoder. In consonance with the results, the proposed framework outperforms DeepLabV3+ [1]. The performance of the proposed framework is attributed to its ability to self-learn free space. This combination of online and active ML removes the need for large datasets typically required by a CNN. Moreover, this technique provides case-specific free space classifications based on the information gathered from the scenario at hand.",
        "DOI": "10.1109/TCYB.2021.3113804",
        "paper_author": "Roche J.",
        "affiliation_name": "Loughborough University London",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60123703",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Generation of Game Stages With Quality and Diversity by Reinforcement Learning in Turn-Based RPG",
        "publication": "IEEE Transactions on Games",
        "citied_by": "8",
        "cover_date": "2022-09-01",
        "Abstract": "Many recent studies in procedural content generation (PCG) are based on machine learning. One of the promising approaches is generative models, which have shown impressive results in generating new pictures and videos from existing ones. However, it is usually costly to collect sufficient content for training on PCG. To address this issue, we consider reinforcement learning (RL), which does not need to collect training data in advance but learns from its interaction with an environment. In this work, RL agents are trained to generate stages, which we define as series of events in turn-based role playing games. It is a challenging task since several events in a stage are usually highly correlated to each other. We first formulate the stage generation problem into a Markov decision process. A hand-crafted evaluation function, which simulates players' enjoyment, is defined to evaluate generated stages. Two RL algorithms are selected in the experiments, which are deep Q-network for discrete action space and deep deterministic policy gradient for continuous action space. The generated stages from both models receive evaluation values indicating good quality. To solve the delayed reward problem and further improve the quality of the stages, we employ virtual simulations (VS) to give rewards to intermediate actions and get stages with higher average scores. In addition, we introduce noise to avoid generating similar stages while trying to keep the quality as high as possible. The proposed methods succeed in generating good and diverse stages.",
        "DOI": "10.1109/TG.2021.3113313",
        "paper_author": "Nam S.G.",
        "affiliation_name": "Japan Advanced Institute of Science and Technology",
        "affiliation_city": "Nomi",
        "affiliation_country": "Japan",
        "affiliation_id": "60011375",
        "affiliation_state": "Ishikawa"
    },
    {
        "paper_title": "Artificial intelligence, firms and consumer behavior: A survey",
        "publication": "Journal of Economic Surveys",
        "citied_by": "42",
        "cover_date": "2022-09-01",
        "Abstract": "The current advances in Artificial Intelligence (AI) are likely to have profound economic implications and bring about new trade-offs, thereby posing new challenges from a policymaking point of view. What is the impact of these technologies on the labor market and firms? Will algorithms reduce consumers' biases or will they rather originate new ones? How competition will be affected by AI-powered agents? This study is a first attempt to survey the growing literature on the multi-faceted economic effects of the recent technological advances in AI that involve machine learning applications. We first review research on the implications of AI on firms, focusing on its impact on labor market, productivity, skill composition and innovation. Then we examine how AI contributes to shaping consumer behavior and market competition. We conclude by discussing how public policies can deal with the radical changes that AI is already producing and is going to generate in the future for firms and consumers.",
        "DOI": "10.1111/joes.12455",
        "paper_author": "Abrardi L.",
        "affiliation_name": "Politecnico di Torino",
        "affiliation_city": "Turin",
        "affiliation_country": "Italy",
        "affiliation_id": "60012162",
        "affiliation_state": "TO"
    },
    {
        "paper_title": "Party Messaging in the U.S. House of Representatives",
        "publication": "Political Research Quarterly",
        "citied_by": "2",
        "cover_date": "2022-09-01",
        "Abstract": "Both Congressional parties compete to promote their own reputations while damaging the opposition party’s brand. This behavior affects both policy-making agendas and the party members’ communications with the media and constituents. While there has been ample study of partisan influence on legislative agenda-setting and roll call voting behavior, much less is known about the parties’ efforts to shape the public debate. This paper analyzes two strategic decisions of parties: the timing of collective efforts to influence the public policy debate and the substantive content of these “party messaging” events. These dynamics are analyzed using a unique dataset of 50,195 one-minute speeches delivered on the floor of the U.S. House of Representatives from 1989 to 2016. We find a pattern of strategic matching—both parties are more likely to engage in concurrent messaging efforts, often on the same issue.",
        "DOI": "10.1177/10659129211029712",
        "paper_author": "Hughes T.",
        "affiliation_name": "California State University, Northridge",
        "affiliation_city": "Northridge",
        "affiliation_country": "United States",
        "affiliation_id": "60020975",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Certifiable Robustness to Adversarial State Uncertainty in Deep Reinforcement Learning",
        "publication": "IEEE Transactions on Neural Networks and Learning Systems",
        "citied_by": "24",
        "cover_date": "2022-09-01",
        "Abstract": "Deep neural network-based systems are now state-of-the-art in many robotics tasks, but their application in safety-critical domains remains dangerous without formal guarantees on network robustness. Small perturbations to sensor inputs (from noise or adversarial examples) are often enough to change network-based decisions, which was recently shown to cause an autonomous vehicle to swerve into another lane. In light of these dangers, numerous algorithms have been developed as defensive mechanisms from these adversarial inputs, some of which provide formal robustness guarantees or certificates. This work leverages research on certified adversarial robustness to develop an online certifiably robust for deep reinforcement learning algorithms. The proposed defense computes guaranteed lower bounds on state-action values during execution to identify and choose a robust action under a worst case deviation in input space due to possible adversaries or noise. Moreover, the resulting policy comes with a certificate of solution quality, even though the true state and optimal action are unknown to the certifier due to the perturbations. The approach is demonstrated on a deep Q-network (DQN) policy and is shown to increase robustness to noise and adversaries in pedestrian collision avoidance scenarios, a classic control task, and Atari Pong. This article extends our prior work with new performance guarantees, extensions to other reinforcement learning algorithms, expanded results aggregated across more scenarios, an extension into scenarios with adversarial behavior, comparisons with a more computationally expensive method, and visualizations that provide intuition about the robustness algorithm.",
        "DOI": "10.1109/TNNLS.2021.3056046",
        "paper_author": "Everett M.",
        "affiliation_name": "MIT School of Engineering",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States",
        "affiliation_id": "60140949",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Assessing the impacts, trends and responses to COVID-19 in developing countries using machine learning techniques",
        "publication": "An Interdisciplinary Approach in the Post-COVID-19 Pandemic Era",
        "citied_by": "0",
        "cover_date": "2022-08-31",
        "Abstract": "On March 11, 2020, the World Health Organization (WHO) declared the novel coronavirus, otherwise known as COVID-19, a pandemic. Originating from Wuhan in Hubei province, China in December 2019, the pandemic has spread to more than 200 countries across the globe. The number of people infected globally by the disease has risen to more than 2.2 million, and over 154,219 people have died as a result of this disease as of April 18, 2020. Although China, the epicentre of the pandemic, has in recent weeks recorded only a few new cases of the disease and very few deaths, The number of cases and deaths has continued to upsurge in mostly developed nations such as the United States of America (USA), Spain, Italy, France, and Germany. There is, however, speculation that developing countries, many of which have continued to record lower cases of the disease, will likely see a rise in the near future. In this chapter, we apply machine learning models for the assessment of the trend and spread of the virus in Ethiopia, Nigeria, Pakistan, and India. The analysis showed a similar trend in the rise of the epidemic in these countries and an increase in the number of cases forecast for the near future in all four countries. We evaluated the social, cultural, and economic impacts of this pandemic on these countries and how the disease is impacting the mental well-being of the masses. Finally, we analysed how government response policies have affected the spread of the pandemic and the contributions of social media to the dissemination of vital information to facilitate the containment of the global spread of the virus.",
        "DOI": "NA",
        "paper_author": "Pandey D.",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Eco-FL: Adaptive Federated Learning with Efficient Edge Collaborative Pipeline Training",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "8",
        "cover_date": "2022-08-29",
        "Abstract": "Federated Learning (FL) has been a promising paradigm in distributed machine learning that enables in-situ model training and global model aggregation. While it can well preserve private data for end users, to apply it efficiently on IoT devices yet suffer from their inherent variants: their available computing resources are typically constrained, heterogeneous, and changing dynamically. Existing works deploy FL on IoT devices by pruning a sparse model or adopting a tiny counterpart, which alleviates the workload but may have negative impacts on model accuracy. To address these issues, we propose Eco-FL, a novel Edge Collaborative pipeline based Federated Learning framework. On the client side, each IoT device collaborates with trusted available devices in proximity to perform pipeline training, enabling local training acceleration with efficient augmented resource orchestration. On the server side, Eco-FL adopts a novel grouping-based hierarchical architecture that combines synchronous intra-group aggregation and asynchronous inter-group aggregation, where a heterogeneity-aware dynamic grouping strategy that jointly considers response latency and data distribution is developed. To tackle the resource fluctuation during the runtime, Eco-FL further applies an adaptive scheduling policy to judiciously adjust workload allocation and client grouping at different levels. Extensive experimental results using both prototype and simulation show that, compared to state-of-the-art methods, Eco-FL can upgrade the training accuracy by up to 26.3%, reduce the local training time by up to 61.5%, and improve the local training throughput by up to 2.6 ×.",
        "DOI": "10.1145/3545008.3545015",
        "paper_author": "Ye S.",
        "affiliation_name": "Sun Yat-Sen University",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60021182",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "LDPP: A Learned Directory Placement Policy in Distributed File Systems",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "3",
        "cover_date": "2022-08-29",
        "Abstract": "Load balance is a critical problem in distributed file systems. Previous works focus on how to distribute data evenly on different nodes or storage devices from the perspective of file level, but neglect to effectively take advantage of the directory's locality and the long duration of the directory's hotness, which may affect the degree of balance and cause performance degradation. To overcome this shortcoming, in this paper, we propose a learning-based directory placement policy, called LDPP, which determines the data layout by predicting the load. We first establish a relationship between directory request characteristics and state information to predict the state information of the directory (storage capacity, bandwidth, and IOPS). Then, the new directory is placed on different nodes in a multi-dimensional manner based on the Manhattan distance according to the predicted multidimensional state information. In addition, we also take into account the trade-off between the same category directory classified by the load prediction module and the peer directories and explore their influence on the balance. Extensive experiments demonstrate that LDPP not only efficiently alleviates load imbalance and increases the utilization of the resources but also improves DFS performance in practice, which can reduce service latency by up to 36 and increase IOPS and bandwidth by 8 and 9, respectively.",
        "DOI": "10.1145/3545008.3545057",
        "paper_author": "Wang Y.",
        "affiliation_name": "Huazhong University of Science and Technology",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60025761",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Forest protection and permanence of reduced emissions",
        "publication": "Frontiers in Forests and Global Change",
        "citied_by": "6",
        "cover_date": "2022-08-29",
        "Abstract": "Tropical forests are essential for climate change mitigation. With growing interest over the use of credits from reducing emissions from deforestation and forest degradation (REDD+) and other natural climate solutions within both voluntary and compliance carbon markets, key concerns about the long-term durability of the reductions, or their permanence, arise for countries, corporations, regulators, and policy makers. This paper seeks to analyze the longevity of emissions reductions from different policies to slow down and stop deforestation. To establish conditions of permanence, we conduct numerical analyses using a model based on a cellular automata algorithm that learns from historical deforestation patterns and other spatial features in the Brazilian state of Mato Grosso. First, we simulate increased law enforcement to curb deforestation at a jurisdictional scale from 2025 to 2034, followed by potential policy rollbacks from 2035 to 2050. Second, we consider alternative scenarios to avoid potentially legal deforestation coupled with reforestation. We find spatial and path dependence – a successful policy intervention may permanently change the deforestation trajectory even after potential policy reversals. Hence, permanence depends both on the probability of policy reversals and the risk of emissions overshooting. Our results are important for advancing the understanding around the unsettled debate on the permanence of avoided emissions. Further, this paper argues that as policies to prevent deforestation or reduce emissions otherwise are reversible, permanence should be understood and discussed in a probabilistic and time-dependent framework.",
        "DOI": "10.3389/ffgc.2022.928518",
        "paper_author": "McCallister M.",
        "affiliation_name": "Environmental Defense Fund",
        "affiliation_city": "Washington, D.C.",
        "affiliation_country": "United States",
        "affiliation_id": "60002913",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Stabilized BLDCM Speed Regulation Control Method for Electric Energy Vehicle based on Deep Reinforcement Learning",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2022-08-26",
        "Abstract": "Electric new energy vehicles are gradually replacing traditional fuel vehicles by their more environmental advantages and better control systems. In the normally used brushless DC motor speed regulation system for nowadays electric energy vehicles, the rising time, adjustment time and overshoot are most important performance parameters. Although some proportional-integral-derivative (PID) parameter tuning methods such as traditional PID, particle swarm optimization (PSO) algorithm, gray wolf optimization (GWO) algorithm, and sparrow search algorithm (SSA) were used to reduce the response time and overshoot, the control system performance still has much room to be improved. In this paper, a new PID parameter tuning algorithm based on deep reinforcement learning is proposed for electric energy vehicle brushless DC motor speed control. The model of the speed loop/current of the brushless DC motor is designed and the deep deterministic policy gradient (DDPG) algorithm is added to the speed loop to realize the tuning of the PID parameters. The simulation is carried out on the Simulink platform with the speed as the control variable. The results show that, compared with traditional PID, PSO, GWO and SSA, the overshoot of proposed DC speed control system has been significantly improved while keeping the shortest adjustment time.",
        "DOI": "10.1145/3562007.3562010",
        "paper_author": "Liu B.",
        "affiliation_name": "South China University of Technology",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60024542",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Neural Decoders Using Reinforcement Learning in Brain Machine Interfaces: A Technical Review",
        "publication": "Frontiers in Systems Neuroscience",
        "citied_by": "1",
        "cover_date": "2022-08-26",
        "Abstract": "Creating flexible and robust brain machine interfaces (BMIs) is currently a popular topic of research that has been explored for decades in medicine, engineering, commercial, and machine-learning communities. In particular, the use of techniques using reinforcement learning (RL) has demonstrated impressive results but is under-represented in the BMI community. To shine more light on this promising relationship, this article aims to provide an exhaustive review of RL’s applications to BMIs. Our primary focus in this review is to provide a technical summary of various algorithms used in RL-based BMIs to decode neural intention, without emphasizing preprocessing techniques on the neural signals and reward modeling for RL. We first organize the literature based on the type of RL methods used for neural decoding, and then each algorithm’s learning strategy is explained along with its application in BMIs. A comparative analysis highlighting the similarities and uniqueness among neural decoders is provided. Finally, we end this review with a discussion about the current stage of RLBMIs including their limitations and promising directions for future research.",
        "DOI": "10.3389/fnsys.2022.836778",
        "paper_author": "Girdler B.",
        "affiliation_name": "Stanley and Karen Pigman College of Engineering",
        "affiliation_city": "Lexington",
        "affiliation_country": "United States",
        "affiliation_id": "60025173",
        "affiliation_state": "KY"
    },
    {
        "paper_title": "COVID-19 severity is associated with population-level gut microbiome variations",
        "publication": "Frontiers in Cellular and Infection Microbiology",
        "citied_by": "7",
        "cover_date": "2022-08-23",
        "Abstract": "The human gut microbiome interacts with many diseases, with recent small studies suggesting a link with COVID-19 severity. Exploring this association at the population-level may provide novel insights and help to explain differences in COVID-19 severity between countries. We explore whether there is an association between the gut microbiome of people within different countries and the severity of COVID-19, measured as hospitalisation rate. We use data from the large (n = 3,055) open-access gut microbiome repository curatedMetagenomicData, as well as demographic and country-level metadata. Twelve countries were placed into two groups (high/low) according to COVID-19 hospitalisation rate before December 2020 (ourworldindata.com). We use an unsupervised machine learning method, Topological Data Analysis (TDA). This method analyses both the local geometry and global topology of a high-dimensional dataset, making it particularly suitable for population-level microbiome data. We report an association of distinct baseline population-level gut microbiome signatures with COVID-19 severity. This was found both with a PERMANOVA, as well as with TDA. Specifically, it suggests an association of anti-inflammatory bacteria, including Bifidobacteria species and Eubacterium rectale, with lower severity, and pro-inflammatory bacteria such as Prevotella copri with higher severity. This study also reports a significant impact of country-level confounders, specifically of the proportion of over 70-year-olds in the population, GDP, and human development index. Further interventional studies should examine whether these relationships are causal, as well as considering the contribution of other variables such as genetics, lifestyle, policy, and healthcare system. The results of this study support the value of a population-level association design in microbiome research in general and for the microbiome-COVID-19 relationship, in particular. Finally, this research underscores the potential of TDA for microbiome studies, and in identifying novel associations.",
        "DOI": "10.3389/fcimb.2022.963338",
        "paper_author": "Lymberopoulos E.",
        "affiliation_name": "UCL Queen Square Institute of Neurology",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60019953",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "YOU SHALL NOT COMPUTE on my Data: Access Policies for Privacy-Preserving Data Marketplaces and an Implementation for a Distributed Market using MPC",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "3",
        "cover_date": "2022-08-23",
        "Abstract": "Personal data is an attractive source of insights for a diverse field of research and business. While our data is highly valuable, it is often privacy-sensitive. Thus, regulations like the GDPR restrict what data can be legally published, and what a buyer may do with this sensitive data. While personal data must be protected, we can still sell some insights gathered from our data that do not hurt our privacy. A data marketplace is a platform that helps users to sell their data while assisting buyers in discovering relevant datasets. The major challenge such a marketplace faces is balancing between offering valuable insights into data while preserving privacy requirements. Private data marketplaces try to solve this challenge by offering privacy-preserving computations on personal data. Such computations allow for calculating statistics or training machine learning models on personal data without accessing the data in plain. However, the user selling the data cannot restrict who can buy or what type of computation the data is allowed. We close the latter gap by proposing a flexible access control architecture for private data marketplaces, which can be applied to existing data markets. Our architecture enables data sellers to define detailed policies restricting who can buy their data. Furthermore, a seller can control what computation a specific buyer can purchase on the data, and make constraints on its parameters to mitigate privacy breaches. The data market's computation system then enforces the policies before initiating a computation. To demonstrate the feasibility of our approach, we provide an implementation for the KRAKEN marketplace, a distributed data market using MPC. We show that our approach is practical since it introduces a negligible performance overhead and is secure against several adversaries.",
        "DOI": "10.1145/3538969.3544445",
        "paper_author": "More S.",
        "affiliation_name": "Technische Universitat Graz",
        "affiliation_city": "Graz",
        "affiliation_country": "Austria",
        "affiliation_id": "60019663",
        "affiliation_state": "Styria"
    },
    {
        "paper_title": "AutoPlex: Inter-Session Multiplexing Congestion Control for Large-Scale Live Video Services",
        "publication": "NAI 2022 - Proceedings of the ACM SIGCOMM 2022 Workshop on Network-Application Integration",
        "citied_by": "5",
        "cover_date": "2022-08-22",
        "Abstract": "The increasingly obvious advances in live video services introduce the urgent need for enhancing network transmission performance, especially by designing an efficient congestion control (CC) scheme. Unfortunately, the previous rule-based CC methods cannot adapt well to various network conditions and statuses while machine-learning-powered CC paradigms always suffer from non-trivial system overhead and unstable effects. In this paper, we first conduct a large-scale network measurement for 800+ million live video streams, and find that QoS metrics of better-performed sessions show similarity in the same user group. We then propose AutoPlex, an inter-session multiplexing CC framework that makes full use of this similarity and automatically adjusts CC parameters (i.e., pacing rate and congestion window size). AutoPlex supports user-defined policies that can act as standards to learn QoS features of better-performed sessions. We implement the proposed AutoPlex prototype based on QUIC protocol and BBR algorithm, and conduct experiments in the real live CDN proxy. The experimental results demonstrate the potentials of AutoPlex for the transmission optimization of live video applications, in which the average (or 90th-percentile) retransmission ratio can be reduced by 24% ∼ 27% (or 32% ∼ 40%) while the average value of goodput/rtt is promoted by 14% ∼ 32%.",
        "DOI": "10.1145/3538401.3546596",
        "paper_author": "Wu B.",
        "affiliation_name": "Tencent",
        "affiliation_city": "Shenzhen",
        "affiliation_country": "China",
        "affiliation_id": "60114181",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Spatial characteristics and multifactorial driving analysis of fly-tipping bulky waste in Beijing based on the random forest model",
        "publication": "Journal of Cleaner Production",
        "citied_by": "6",
        "cover_date": "2022-08-20",
        "Abstract": "The phenomenon of fly-tipping of bulky waste is becoming increasingly serious with the development of economy and the improvement of living standards. The fly-tipping of bulky waste causes a waste stream with a high proportion of good quality recyclable materials and a wide range of social and environmental problems including damaging the environment and aggravating traffic in a megacity like Beijing. In this study, we analyzed the quantitative spatial distribution of fly-tipping bulky waste in Beijing for the first time and identified its driving forces to explore the fly-tipping rule of bulky waste. We used Anselin's Local Moran I method to reveal the spatial characteristics, and Random Forest machine learning method to examine the driving factors based on multiple data sources such as geographical, population and survey data. The results showed that the spatial agglomeration of fly-tipping presented a typical core edge diffusion spatial distribution pattern. High-High clusters of cases were found in most regions of Dongcheng District and the east part of Xicheng District. In the multifactorial drivers, the three most important driving factors were found to be accommodation, floating population and income level. Surprisingly, any kind of road and educational level had small effect on the fly-tipping case. These results provide a theoretical basis for the government to advance comprehensive prevention and control strategies of bulky waste fly-tipping. And it is helpful to formulate management policies of fly-tipping bulky waste and provide targeted guidance and suggestions for senior decision makers and managers.",
        "DOI": "10.1016/j.jclepro.2022.132534",
        "paper_author": "Xiong N.",
        "affiliation_name": "Beijing Forestry University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60006782",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Leveraging machine learning methods to quantify 50 years of dwindling groundwater in India",
        "publication": "Science of the Total Environment",
        "citied_by": "21",
        "cover_date": "2022-08-20",
        "Abstract": "Global compilations and regional studies, indicative of the unsustainable extraction and subsequent unremittingly depleting groundwater (GW) in India, either provide bulk estimates or are confined to the river basins and therefore conceal inferences from a nationwide policymaking perspective. Here, we provide the state-wise past (2000−2020) and future (2030–2050) assessment of dwindling groundwater in India utilizing in-situ groundwater levels (GWL) from 54,112 wells, remote sensing products, and hydrological simulations. By employing three machine learning methods, we show a decline in GWL of over 80% in North India with a notable shift towards the eastern state of Uttar Pradesh and a cumulative groundwater loss (169.96 ± 19.67 km3) equivalent to the water storage capacity of the world's biggest dam (Kariba Dam, Zimbabwe). Its likely contribution to sea-level rise (0.47 ± 0.06 mm) is about 64% of that from annual global glacier melt. Our results typically contrast the GW recovery paradox in South India (e.g., a declining trend of −84.48 ± 38.81 mm/a (p < 0.05) in Andhra Pradesh during 2000–2020), reveal high seasonal variability (e.g., up to ~6 m in Maharashtra), and illustrate the skewed effect of survivor bias in the traditional assessments. We infer the significant impact of underlying hydrogeology and the implementation of water-related policies and projects on the GWL dynamic and variability in the region. Projected GWL reveals a likely water scarcity situation for about 2.8 million km2 area and one billion residents of the country up to 2050. Our observation-based analysis offers insights into the state-level monthly GW dynamics, which is critical for efficient interstate resource allocation, development plans, and policy interventions with broad methodological implications for the water-scarce countries.",
        "DOI": "10.1016/j.scitotenv.2022.155474",
        "paper_author": "Xiong J.",
        "affiliation_name": "Wuhan University",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60029306",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "A comprehensive analysis of evolution and underlying connections of water research themes in the 21st century",
        "publication": "Science of the Total Environment",
        "citied_by": "9",
        "cover_date": "2022-08-20",
        "Abstract": "This work aimed to reflect the advancements in water-related science, technology, and policy and shed light on future research opportunities related to water through a systematic overview of Water Research articles published in the first 21.5 years of the 21st century. Specific bibliometric analyses were performed to i) reveal the temporal and spatial trends of water-related research themes and ii) identify the underlying connections between research topics. The results showed that while top topics including wastewater (treatment), drinking water, adsorption, model, biofilm, and bioremediation remained constantly researched, there were clear shifts in topics over the years, leading to the identification of trending-up and emerging research topics. Compared to the first decade of the 21st century, the second decade not only experienced significant uptrends of disinfection by-products, anaerobic digestion, membrane bioreactor, advanced oxidation processes, and pharmaceuticals but also witnessed the emerging popularity of PFAS, anammox, micropollutants, emerging contaminants, desalination, waste activated sludge, microbial community, forward osmosis, antibiotic resistance genes, resource recovery, and transformation products. On top of the temporal evolution, distinct spatial evolution existed in water-related research topics. Microplastics and Covid-19 causing global concerns were hot topics detected, while metagenomics and machine learning were two technical approaches emerging in recent years. These consistently popular, trending-up and emerging research topics would most likely attract continuous/increasing research input and therefore constitute a major part of the prospective water-related research publications.",
        "DOI": "10.1016/j.scitotenv.2022.155411",
        "paper_author": "Chen X.",
        "affiliation_name": "Fuzhou University",
        "affiliation_city": "Fuzhou",
        "affiliation_country": "China",
        "affiliation_id": "60017605",
        "affiliation_state": "Fujian"
    },
    {
        "paper_title": "Statistical and machine learning methods for evaluating trends in air quality under changing meteorological conditions",
        "publication": "Atmospheric Chemistry and Physics",
        "citied_by": "21",
        "cover_date": "2022-08-19",
        "Abstract": "Evaluating the influence of anthropogenic-emission changes on air quality requires accounting for the influence of meteorological variability. Statistical methods such as multiple linear regression (MLR) models with basic meteorological variables are often used to remove meteorological variability and estimate trends in measured pollutant concentrations attributable to emission changes. However, the ability of these widely used statistical approaches to correct for meteorological variability remains unknown, limiting their usefulness in the real-world policy evaluations. Here, we quantify the performance of MLR and other quantitative methods using simulations from a chemical transport model, GEOS-Chem, as a synthetic dataset. Focusing on the impacts of anthropogenic-emission changes in the US (2011 to 2017) and China (2013 to 2017) on PM2.5 and O3, we show that widely used regression methods do not perform well in correcting for meteorological variability and identifying long-term trends in ambient pollution related to changes in emissions. The estimation errors, characterized as the differences between meteorology-corrected trends and emission-driven trends under constant meteorology scenarios, can be reduced by 30 %–42 % using a random forest model that incorporates both local- and regional-scale meteorological features. We further design a correction method based on GEOS-Chem simulations with constant-emission input and quantify the degree to which anthropogenic emissions and meteorological influences are inseparable, due to their process-based interactions. We conclude by providing recommendations for evaluating the impacts of anthropogenic-emission changes on air quality using statistical approaches.",
        "DOI": "10.5194/acp-22-10551-2022",
        "paper_author": "Qiu M.",
        "affiliation_name": "Massachusetts Institute of Technology",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States",
        "affiliation_id": "60022195",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Regional representation of wind stakeholders’ end-of-life behaviors and their impact on wind blade circularity",
        "publication": "iScience",
        "citied_by": "14",
        "cover_date": "2022-08-19",
        "Abstract": "The growing number of end-of-life (EOL) wind blades could further strain US landfills or be a valuable composite materials source, depending on stakeholders’ behaviors. Technical solutions based on circular economy (CE) principles have been proposed but are not guaranteed to solve the issue of EOL management. Transitioning to CE implies changing how business models, supply chains, and behaviors deal with products and waste. A spatially resolved agent-based modeling combined with a machine-learning metamodel shows that including behavioral factors is crucial to designing effective policies. Logistical barriers and transportation costs significantly affect the results: lowering blade shredding costs by a third before transportation makes EOL blades a source of valuable materials, decreasing the 2050 cumulative landfill rate below 50%. In another scenario, parameter settings simulating policy interventions aiming at boosting early adoption incites new social norms favorable to recycling, lowering the cumulative landfill rate below 10%.",
        "DOI": "10.1016/j.isci.2022.104734",
        "paper_author": "Walzberg J.",
        "affiliation_name": "National Renewable Energy Laboratory",
        "affiliation_city": "Golden",
        "affiliation_country": "United States",
        "affiliation_id": "60030451",
        "affiliation_state": "CO"
    },
    {
        "paper_title": "Predictive pricing models to classify potential customers using data-driven approaches",
        "publication": "AIP Conference Proceedings",
        "citied_by": "2",
        "cover_date": "2022-08-18",
        "Abstract": "In the vehicle insurance field, many traditional companies use actuarial methods to evaluate the possibility of the insurance claim, whereas more intelligent systems with predictive analytics models are earnestly growing. The article tries to review and analyze the data set provided by a private insurance company and build a predictive scoring model that can reduce possible costs on new customers. The problem is considered as a binary classification problem on a highly imbalanced data set. All required operations were performed for the data set, such as dealing with missing values, feature engineering, dimensionality reduction, and resampling techniques. Then, on the resulted data set, sophisticated and state-of-the-art machine learning algorithms were applied. When dealing with an imbalanced data set, the critical part is a proper choice of resampling techniques and evaluation metrics. As a result, the best performance was achieved by two models, XGBoost and KNN. The percentage of the correctly predicted cases using the recall metric with the best model was more than 92%. The last step, based on the results of the predictive, recommendations were offered to the company on possible improvements in pricing policies.",
        "DOI": "10.1063/5.0099796",
        "paper_author": "Yedilkhan D.",
        "affiliation_name": "Astana IT University",
        "affiliation_city": "Astana",
        "affiliation_country": "Kazakhstan",
        "affiliation_id": "60204069",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Mapping photovoltaic power plants in China using Landsat, random forest, and Google Earth Engine",
        "publication": "Earth System Science Data",
        "citied_by": "44",
        "cover_date": "2022-08-17",
        "Abstract": "Photovoltaic (PV) technology, an efficient solution for mitigating the impacts of climate change, has been increasingly used across the world to replace fossil fuel power to minimize greenhouse gas emissions. With the world’s highest cumulative and fastest built PV capacity, China needs to assess the environmental and social impacts of these established PV power plants. However, a comprehensive map regarding the PV power plants’ locations and extent remains scarce on the country scale. This study developed a workflow, combining machine learning and visual interpretation methods with big satellite data, to map PV power plants across China. We applied a pixel-based random forest (RF) model to classify the PV power plants from composite images in 2020 with a 30 m spatial resolution on the Google Earth Engine (GEE). The resulting classification map was further improved by a visual interpretation approach. Eventually, we established a map of PV power plants in China by 2020, covering a total area of 2917 km2. We found that most PV power plants were situated on cropland, followed by barren land and grassland, based on the derived national PV map. In addition, the installation of PV power plants has generally decreased the vegetation cover. This new dataset is expected to be conducive to policy management, environmental assessment, and further classification of PV power plants. The dataset of photovoltaic power plant distribution in China by 2020 is available to the public at https://doi.org/10.5281/zenodo.6849477 (Zhang et al., 2022).",
        "DOI": "10.5194/essd-14-3743-2022",
        "paper_author": "Zhang X.",
        "affiliation_name": "Henan University",
        "affiliation_city": "Kaifeng",
        "affiliation_country": "China",
        "affiliation_id": "60003146",
        "affiliation_state": "Henan"
    },
    {
        "paper_title": "Forecasting of total national health expenditure based on the Kalman fusion model",
        "publication": "Modern Preventive Medicine",
        "citied_by": "1",
        "cover_date": "2022-08-15",
        "Abstract": "Objective Through the description and analysis of the total health expenditure and composition in my country, the Kalman fusion model was used to analyze and study the total health expenditure and composition, the total health expenditure and compositions from 2021 to 2023 were forecasted to provide information for the adjustment and implementation of health policies. Methods The national total health expenditure and composition data from 1990 to 2020 was collected. ARIMA, GM (1,1) , and ELM models were established, and the above two or three models were integrated under the Kalman framework to analyze the total health expenditure and composition prediction. The fitting and prediction errors were compared respectively, and the average relative error was used to evaluate the prediction ability of the model, and the total health expenditure and compositions from 2021 to 2023 were predicted. Results In the prediction of total health expenditure, government expenditure, social expenditure, and personal expenditure based on the Kalman fusion model, three prediction results were better than the single model prediction results, and the average relative error was 0. 003 less than the best single model prediction result. The predicted results of the total health expenditure from 2021 to 2023 were : 84 002. 31, 94 799. 95, and 107 256. 90 billion yuan. Conclusion The prediction accuracy based on the Kalman fusion model is better than that of the single prediction model and is robust to a variety of data. The total health expenditure has maintained a relatively high growth rate, but its proportion to the GDP is steady. Among them, social expenditures have grown rapidly, and government and personal expenditures have maintained a synchronous and stable growth.",
        "DOI": "10.20043/j.cnki.MPM.202204181",
        "paper_author": "Yan G.",
        "affiliation_name": "Hengshui People's Hospital",
        "affiliation_city": "Hebei",
        "affiliation_country": "China",
        "affiliation_id": "126887517",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A \"Data-Feature-Policy\" Solution for Multi-scale Geological-Geophysical Intelligent Reservoir Characterization",
        "publication": "SEG Technical Program Expanded Abstracts",
        "citied_by": "0",
        "cover_date": "2022-08-15",
        "Abstract": "Innovative interpretation methods based on efficient measurements are the key to establishing accurate \"geological-geophysical\" mappings. The multi-solvability of petrophysical responses leads to the complex non-linear relationship between multi-scale geophysical data and the classification of strongly heterogeneous geological bodies, which makes it difficult for quantitative reservoir characterization. As multi-source geological-geophysical data have inherent differences in resolution and properties, how to effectively fuse these data becomes a real challenge in the big data era. Based on understanding the intrinsic correlation of different measurements on the given geo-body, a combination of machine learning algorithms was established for multi-scale geological-geophysical intelligent reservoir characterization, which was named \"Data-Feature-Policy\" solution. (1) Data-level fusion: The dataset was obtained from the conventional logging data of 921m deep carbonate rock in the Tarim Basin. Firstly, the Isolated Forest algorithm was used for quality control to remove outliers, and then, the multi-dimensional parameters were reduced to a 2-dimensional embedding vector by t-SNE algorithm. (2) Feature-level extraction: Based on the Density Peak Clustering algorithm, non-spherical clusters of 2-dimensional embedding vectors were clustered. Then, calibrated by the core-electrical imaging chart, each cluster was labeled electrofacies classification; (3) Policy-level characterization: Based on Deep Belief Network, the geological prediction model was established. It was optimized by a double-loop filtering mechanism that selected the parameter-cluster combination with the highest correct rate, provided that each accuracy is greater than 80%. And the final accuracy rate exceeded 93%. The model was used to quantitatively identify the 3176m carbonate reservoir, automatically count the development thickness and distribution range of reservoir classification, which clarified the reservoir vertical structure and distribution around the wellbore. The \"Data-Feature-Policy\" solution has the advantages of data-driven, objectivity, and compatibility. It can dig high-dimensional mapping relationships of geological-geophysical data at the feature level, which helps to better understand the multi-variate and multi-attribute data fusion. This effective workflow is suitable to characterize heterogeneous reservoirs intelligently.",
        "DOI": "10.1190/image2022-3748948.1",
        "paper_author": "Zheng W.",
        "affiliation_name": "Institute of Geology and Geophysics Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60014157",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Data-driven decision support system for building stocks energy retrofit policy",
        "publication": "Journal of Building Engineering",
        "citied_by": "19",
        "cover_date": "2022-08-15",
        "Abstract": "In most European countries, residential assets account for as much as 85% of the building stock floor area and are, on average, very outdated and energy inefficient. Moreover, the European Commission published the EU Green Deal invigorating higher retrofit of private and public buildings. Nowadays, public authorities collect extensive datasets to analyze the existing building stock; however, the complex and diverse scenario makes the definition of retrofit policies cumbersome. The biggest hurdle is often linked to the high cost of acquiring information. The presented research tries to overcome these issues by introducing a decision support system for retrofit policymaking from low-cost data-driven approaches. The method is based on: i) clustering techniques to divide building assets into groups with similar characteristics and energy consumption, and ii) Montecarlo simulation to compute each cluster's energy savings based on different retrofit scenarios. The proposed method has been successfully applied to an extensive portfolio of residential assets in Lombardy Region in Italy, called the CENED database, with over one million assets. As a result, the introduced method defines the optimum retrofit scenario with a low cost of information (e.g., without expensive surveys to gather data on existing assets' characteristics and performance indicators) and determines the number of assets to be retrofitted along with the expected energy savings. This data-driven approach can be easily updated given new renovations and status changes in the built environment, making it useable for the long term or in different regions. To summarize, data-driven solutions are now required to accomplish the European Union's decarbonization ambitions, and the proposed method helps decision-makers choose better energy retrofit policies for the built environment.",
        "DOI": "10.1016/j.jobe.2022.104633",
        "paper_author": "Re Cecconi F.",
        "affiliation_name": "Politecnico di Milano",
        "affiliation_city": "Milan",
        "affiliation_country": "Italy",
        "affiliation_id": "60023256",
        "affiliation_state": "MI"
    },
    {
        "paper_title": "Using satellite images of nighttime lights to predict the economic impact of COVID-19 in India",
        "publication": "Advances in Space Research",
        "citied_by": "16",
        "cover_date": "2022-08-15",
        "Abstract": "The outbreak of COVID-19 in early 2020 heralded a deep global recession not seen since the Second World War. With entire nations in lockdown, burgeoning economies of countries like India plunged into a downward spiral. The conventional instruments of estimating the short-term economic impact of a pandemic is limited, and as a result, it is challenging to implement timely monetary policies to mitigate the financial impact of such unforeseen events. This study investigates the promise of using nighttime images of lights on Earth, also known as nightlight (NTL), captured by the Visible Infrared Imaging Radiometer Suite (VIIRS) instrumentation onboard the Suomi National Polar-Orbiting Partnership (Suomi NPP) satellite mission to measure the economic cost of the pandemic in India. First, a novel data processing framework was developed for a recently released radiance dataset known as VNP46A1, part of NASA's Black Marble suite of NTL products. Second, the elasticity of nightlight to India's National Gross Domestic Product (GDP) was estimated using panel regression followed by machine learning to predict the Year-over-Year (YoY) change in GDP during Fiscal Year (FY) 2020Q1 (Apr-Jun, 2020). Electricity consumption, known to closely track economic output and precipitation were included as additional features to improve model performance. A strong relationship between both electricity usage and nightlight to GDP was observed. The model predicted a YoY contraction of 24% in FY2020Q1, almost identical to the official GDP decline of 23.9% later announced by the Indian Government. Based on the findings, the study concludes that nightlight along with electricity usage can be invaluable proxies for estimating the cost of short-term supply–demand shocks such as COVID-19, and should be explored further.",
        "DOI": "10.1016/j.asr.2022.05.039",
        "paper_author": "Dasgupta N.",
        "affiliation_name": "Imperial College Business School",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60116484",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Combining multi-indicators with machine-learning algorithms for maize yield early prediction at the county-level in China",
        "publication": "Agricultural and Forest Meteorology",
        "citied_by": "52",
        "cover_date": "2022-08-15",
        "Abstract": "The accurate and timely prediction of crop yield at a large scale is important for food security and the development of agricultural policy. An adaptable and robust method for estimating maize yield for the entire territory of China, however, is currently not available. The inherent trade-off between early estimates of yield and the accuracy of yield prediction also remains a confounding issue. To explore these challenges, we employ indicators such as GPP, ET, surface temperature (Ts), LAI, soil properties and maize phenological information with random forest regression (RFR) and gradient boosting decision tree (GBDT) machine learning approaches to provide maize yield estimates within China. The aims were to: (1) evaluate the accuracy of maize yield prediction obtained from multimodal data analysis using machine-learning; (2) identify the optimal period for estimating yield; and (3) determine the spatial robustness and adaptability of the proposed method. The results can be summarized as: (1) RFR estimated maize yield more accurately than GBDT; (2) Ts was the best single indicator for estimating yield, while the combination of GPP, Ts, ET and LAI proved best when multi-indicators were used (R2 = 0.77 and rRMSE = 16.15% for the RFR); (3) the prediction accuracy was lower with earlier lead time but remained relatively high within at least 24 days before maturity (R2 > 0.77 and rRMSE <16.92%); and (4) combining machine-learning algorithms with multi-indicators demonstrated a capacity to cope with the spatial heterogeneity. Overall, this study provides a reliable reference for managing agricultural production.",
        "DOI": "10.1016/j.agrformet.2022.109057",
        "paper_author": "Cheng M.",
        "affiliation_name": "Ministry of Agriculture of the People's Republic of China",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60087826",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "A comprehensive artificial neural network model for gasification process prediction",
        "publication": "Applied Energy",
        "citied_by": "56",
        "cover_date": "2022-08-15",
        "Abstract": "The viability and the relative merits of competing biomass and waste gasification schemes depends on a complex mix of interacting factors. Conventional analytical methods that are used to aid decision making rely on a plethora of poorly defined parameters. Here we develop a method that eschews the uncertainty in process representation by using a machine learning, data driven, approach to predicting a set of 10 key measures of gasification technology's performance. We develop an artificial neural network that is novel in its use of both categorical and continuous data inputs, which makes it flexible and broadly applicable in assessing gasification process designs. It is the first model applicable to a wide range of feedstock types, gasifying agents, and reactor options. A strong predictive performance, quantified by a coefficient of determination (R2) of 0.9310, was confirmed. The approach has the potential to generate accurate input data for e.g., cost-benefit analysis (CBA) and life cycle sustainability assessment (LCSA) and thus allow for more transparency in the decisions made by policy makers and investors.",
        "DOI": "10.1016/j.apenergy.2022.119289",
        "paper_author": "Ascher S.",
        "affiliation_name": "University of Glasgow",
        "affiliation_city": "Glasgow",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60001490",
        "affiliation_state": "Scotland"
    },
    {
        "paper_title": "Towards stochastic modeling for two-phase flow interfacial area predictions: A physics-informed reinforcement learning approach",
        "publication": "International Journal of Heat and Mass Transfer",
        "citied_by": "4",
        "cover_date": "2022-08-15",
        "Abstract": "The stochastic nature of turbulent two-phase flow determines that the deterministic modeling approaches always have limited predicting range and accuracy, which is due to the averaging and approximation made during the model developments. On the other hand, well-developed machine learning models that are suitable for complex tasks can be used as a surrogate to improve the predictions. In this paper, a physics-informed reinforcement learning-based method for interfacial area prediction is proposed. The method aims to capture the complexity of the two-phase flow using the advantage of reinforcement learning with the aid of the Interfacial Area Transport Equation. A Markov Decision Process (MDP) that describes the bubble transformation in the fluid flow is established by assuming that the development of two-phase flow is a stochastic process with Markov property. The details of the method's framework design are described, including the design of the MDP, environment setup, and the algorithms used to solve the RL problem. The performance of the newly proposed method is tested through experiments based on an experimental database for vertical upward bubbly air-water flows. The result shows that with a knowledge-based stochastic policy, good performance of the new method is achieved with the rRMSE of 0.1573, and it is a significant performance boost to the applied IATE model. The approaches to extending the capability of this new RL-based method are discussed, which is a reference for the further development of this approach.",
        "DOI": "10.1016/j.ijheatmasstransfer.2022.122919",
        "paper_author": "Dang Z.",
        "affiliation_name": "University of Calgary",
        "affiliation_city": "Calgary",
        "affiliation_country": "Canada",
        "affiliation_id": "60002306",
        "affiliation_state": "AB"
    },
    {
        "paper_title": "An adaptive feature selection schema using improved technical indicators for predicting stock price movements",
        "publication": "Expert Systems with Applications",
        "citied_by": "27",
        "cover_date": "2022-08-15",
        "Abstract": "Accurate stock market forecasts can bring high returns for investors. There have been a growing number of studies employing machine learning technology to perform stock prediction tasks with the development of machine learning and artificial intelligence technologies. However, accurately predicting stock price trends still is an elusive goal, not only because the stock market is affected by policies, market environment, market sentiment, etc., but also because stock price data is inherently complex, noisy, and nonlinear. Many technical indicators have been used as input features to stock prediction models, but the quality of technical indicators has always been a neglected issue, thus the application of feature engineering in stock prediction tasks needs to be further expanded. Using 18 technical indicators as the original features, this paper presents improved technical indicators based on wavelet denoising and a novel two-stage adaptive feature selection method. Finally, the random forest model is used as the stock prediction model. Experiments show that in contrast to the original technical indicators, the improved technical indicators significantly enhance the performance of the model (e.g., F1 scores increased by 34.48% on the SSE Composite Index (SSEC) data set, 41.56% on the Hang Seng Index (HSI) data set, 34.48% on the Dow Jones Industrial Average (DJI) data set, 32.75% on the Standard & Poor's 500 Index (S&P 500) data set). The experimental results verify the importance of the quality of technical indicators in the task of stock prediction. Meanwhile, the results also demonstrate the effectiveness of the feature selection method, which can achieve higher prediction accuracy with fewer features. In addition, we established multiple data sets according to the size-varied time windows to study the influence of the size-varied time windows. The results show that properly increasing the size of the time window can exert a positive impact on the model. Finally, by utilizing our two-stage adaptive feature selection method, we remove redundant features, and achieve excellent results on data sets from four different stock markets (e.g., F1 scores reached 0.754 on the SSEC data set, 0.794 on the HSI data set, 0.789 on the DJI data set, 0.821 on the S&P 500 data set). Overall, this study experimentally verifies that improving feature quality can positively impact model performance, and that choosing an appropriate combination of input features can not only improve model performance, but reduces the negative impact of the curse of dimensionality as well.",
        "DOI": "10.1016/j.eswa.2022.116941",
        "paper_author": "Ji G.",
        "affiliation_name": "Jiangnan University",
        "affiliation_city": "Wuxi",
        "affiliation_country": "China",
        "affiliation_id": "60007029",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "DRLR: A Deep-Reinforcement-Learning-Based Recruitment Scheme for Massive Data Collections in 6G-Based IoT Networks",
        "publication": "IEEE Internet of Things Journal",
        "citied_by": "49",
        "cover_date": "2022-08-15",
        "Abstract": "Recently, rapid deployment on the fifth-generation (5G) networks has brought great opportunities for enabling data-intensive applications and brings an extending expectation on the developments of 6G. A basic requirement to develop 6G networks is to reach data with low latency, low cost, and high coverage in smart Internet of Things (IoT). Therefore, this article proposes a novel machine learning-based approach to collect data from multiple sensor devices by cooperation between vehicle and unmanned aerial vehicle (UAV) in IoT. First, a genetic algorithm is utilized to select vehicular collectors to collect massive data from sensor devices, which aims to maximize coverage ratio and to minimize employment cost. Second, we design a novel deep reinforcement learning (DRL)-based route policy to plan collection routes of UAVs with constrain energy, which simplifies the network model, accelerates training speeds, and realizes dynamic planning of flight paths. The optimal collection route of a UAV is a series of outputs based on the proposed DRL-based route policy. Finally, our extensive experiments demonstrate that the proposed scheme can comprehensively improve the coverage ratio of massive data collections and reduce collection costs in smart IoT for the future 6G networks.",
        "DOI": "10.1109/JIOT.2021.3067904",
        "paper_author": "Li T.",
        "affiliation_name": "Central South University",
        "affiliation_city": "Changsha",
        "affiliation_country": "China",
        "affiliation_id": "60017060",
        "affiliation_state": "Hunan"
    },
    {
        "paper_title": "ILASR: Privacy-Preserving Incremental Learning for Automatic Speech Recognition at Production Scale",
        "publication": "Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "citied_by": "6",
        "cover_date": "2022-08-14",
        "Abstract": "Incremental learning is one paradigm to enable model building and updating at scale with streaming data. For end-to-end automatic speech recognition (ASR) tasks, the absence of human annotated labels along with the need for privacy preserving policies for model building makes it a daunting challenge. Motivated by these challenges, in this paper we use a cloud based framework for production systems to demonstrate insights from privacy preserving incremental learning for automatic speech recognition (ILASR). By privacy preserving, we mean, usage of ephemeral data which are not human annotated. This system is a step forward for production level ASR models for incremental/continual learning that offers near real-time test-bed for experimentation in the cloud for end-to-end ASR, while adhering to privacy-preserving policies. We show that the proposed system can improve the production models significantly (3%) over a new time period of six months even in the absence of human annotated labels with varying levels of weak supervision and large batch sizes in incremental learning. This improvement is 20% over test sets with new words and phrases in the new time period. We demonstrate the effectiveness of model building in a privacy-preserving incremental fashion for ASR while further exploring the utility of having an effective teacher model and use of large batch sizes.",
        "DOI": "10.1145/3534678.3539174",
        "paper_author": "Chennupati G.",
        "affiliation_name": "Amazon.com, Inc.",
        "affiliation_city": "Seattle",
        "affiliation_country": "United States",
        "affiliation_id": "60076757",
        "affiliation_state": "WA"
    },
    {
        "paper_title": "Deconfounding Actor-Critic Network with Policy Adaptation for Dynamic Treatment Regimes",
        "publication": "Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "citied_by": "3",
        "cover_date": "2022-08-14",
        "Abstract": "Despite intense efforts in basic and clinical research, an individualized ventilation strategy for critically ill patients remains a major challenge. Recently, dynamic treatment regime (DTR) with reinforcement learning (RL) on electronic health records (EHR) has attracted interest from both the healthcare industry and machine learning research community. However, most learned DTR policies might be biased due to the existence of confounders. Although some treatment actions non-survivors received may be helpful, if confounders cause the mortality, the training of RL models guided by long-term outcomes (e.g., 90-day mortality) would punish those treatment actions causing the learned DTR policies to be suboptimal. In this study, we develop a new deconfounding actor-critic network (DAC) to learn optimal DTR policies for patients. To alleviate confounding issues, we incorporate a patient resampling module and a confounding balance module into our actor-critic framework. To avoid punishing the effective treatment actions non-survivors received, we design a short-term reward to capture patients' immediate health state changes. Combining short-term with long-term rewards could further improve the model performance. Moreover, we introduce a policy adaptation method to successfully transfer the learned model to new-source small-scale datasets. The experimental results on one semi-synthetic and two different real-world datasets show the proposed model outperforms the state-of-the-art models. The proposed model provides individualized treatment decisions for mechanical ventilation that could improve patient outcomes.",
        "DOI": "10.1145/3534678.3539413",
        "paper_author": "Yin C.",
        "affiliation_name": "The Ohio State University",
        "affiliation_city": "Columbus",
        "affiliation_country": "United States",
        "affiliation_id": "60003500",
        "affiliation_state": "OH"
    },
    {
        "paper_title": "Group-wise Reinforcement Feature Generation for Optimal and Explainable Representation Space Reconstruction",
        "publication": "Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "citied_by": "13",
        "cover_date": "2022-08-14",
        "Abstract": "Representation (feature) space is an environment where data points are vectorized, distances are computed, patterns are characterized, and geometric structures are embedded. Extracting a good representation space is critical to address the curse of dimensionality, improve model generalization, overcome data sparsity, and increase the availability of classic models. Existing literature, such as feature engineering and representation learning, is limited in achieving full automation (e.g., over heavy reliance on intensive labor and empirical experiences), explainable explicitness (e.g., traceable reconstruction process and explainable new features), and flexible optimal (e.g., optimal feature space reconstruction is not embedded into downstream tasks). Can we simultaneously address the automation, explicitness, and optimal challenges in representation space reconstruction for a machine learning task? To answer this question, we propose a group-wise reinforcement generation perspective. We reformulate representation space reconstruction into an interactive process of nested feature generation and selection, where feature generation is to generate new meaningful and explicit features, and feature selection is to eliminate redundant features to control feature sizes. We develop a cascading reinforcement learning method that leverages three cascading Markov Decision Processes to learn optimal generation policies to automate the selection of features and operations and the feature crossing. We design a group-wise generation strategy to cross a feature group, an operation, and another feature group to generate new features and find the strategy that can enhance exploration efficiency and augment reward signals of cascading agents. Finally, we present extensive experiments to demonstrate the effectiveness, efficiency, traceability, and explicitness of our system.",
        "DOI": "10.1145/3534678.3539278",
        "paper_author": "Wang D.",
        "affiliation_name": "University of Central Florida",
        "affiliation_city": "Orlando",
        "affiliation_country": "United States",
        "affiliation_id": "60022144",
        "affiliation_state": "FL"
    },
    {
        "paper_title": "ASPIRE: Air Shipping Recommendation for E-commerce Products via Causal Inference Framework",
        "publication": "Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "citied_by": "3",
        "cover_date": "2022-08-14",
        "Abstract": "Speed of delivery is critical for the success of e-commerce platforms. Faster delivery promise to the customer results in increased conversion and revenue. There are typically two mechanisms to control the delivery speed - a) replication of products across warehouses, and b) air-shipping the product. In this paper, we present a machine learning based framework to recommend air-shipping eligibility for products. Specifically, we develop a causal inference framework (referred to as Air Shipping Recommendation or ASPIRE) that balances the trade-off between revenue or conversion and delivery cost to decide whether a product should be shipped via air. We propose a doubly-robust estimation technique followed by an optimization algorithm to determine air eligibility of products and calculate the uplift in revenue and shipping cost. We ran extensive experiments (both offline and online) to demonstrate the superiority of our technique as compared to the incumbent policies and baseline approaches. ASPIRE resulted in a lift of +79 bps of revenue as measured through an A/B experiment in an emerging marketplace on Amazon.",
        "DOI": "10.1145/3534678.3539197",
        "paper_author": "Mondal A.",
        "affiliation_name": "Amazon.com, Inc.",
        "affiliation_city": "Seattle",
        "affiliation_country": "United States",
        "affiliation_id": "60076757",
        "affiliation_state": "WA"
    },
    {
        "paper_title": "Data-driven Humanitarian Mapping and Policymaking: Toward Planetary-Scale Resilience, Equity, and Sustainability",
        "publication": "Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "citied_by": "1",
        "cover_date": "2022-08-14",
        "Abstract": "Human civilization faces existential threats in the forms of climate change, food insecurity, pandemics, international conflicts, forced displacements, and environmental injustice. These overarching humanitarian challenges disproportionately impact historically marginalized communities worldwide. UN OCHA estimates that 274 million people will need humanitarian support in 2022. Despite growing perils to human and environmental well-being, there remains a paucity of publicly-engaged computing research to inform the design of interventions. Data science efforts exist, but they remain isolated from socioeconomic, environmental, cultural, and policy contexts at local and international scales. Moreover, biases and privacy infringements in data-driven methods further amplify existing inequalities. The result is that proclaimed benefits of data-driven innovations may remain inaccessible to policymakers, practitioners, and underserved communities whose lives they intend to transform. To address gaps in knowledge and improve the livelihood of marginalized populations, we have established the Data-driven Humanitarian Mapping and Policymaking, an interdisciplinary initiative.",
        "DOI": "10.1145/3534678.3542918",
        "paper_author": "Gaikwad S.N.",
        "affiliation_name": "Massachusetts Institute of Technology",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States",
        "affiliation_id": "60022195",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Stacking based prediction of COVID-19 Pandemic by integrating infectious disease dynamics model and traditional machine learning",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "3",
        "cover_date": "2022-08-12",
        "Abstract": "Accurate prediction of 2019 novel coronavirus diseases (COVID-19) has been playing an important role in making more effective prevention and control policies during pandemic crises. The aim of this paper was to develop an innovative stacking based prediction of COVID-19 pandemic cumulative confirmed cases (StackCPPred) by integrating infectious disease dynamics model and traditional machine learning. Based on population migration characteristics, five feature indicators were first extracted from the population flow data in the early stage of this epidemic, which were collected from the National Health Commission of the People's Republic of China. Then, stacking based ensemble learning (SEL) model was established for COVID-19 prediction using traditional machine learning, including the multiple linear regression (MLR) and the tree regression model (XGBoost and LightGBM). By introducing the variable \"death state\", an improved Susceptible-Infected-Recovered (ISIR) model was established. Finally, a hybrid model, StackCPPred was proposed by incorporating the ISIR model outputs and the five feature indicators into the SEL model. Real data on population movements and daily cumulative number of newly confirmed cases across the country from January 23 to February 6 were used to validate our model. The results positively proved that the proposed StackCPPred model outperformed the existing models for COVID-19 prediction, as quantified by the root mean square error (RMSE), the root mean square logarithmic error (RMSLE) and the coefficient of determination (R2) (g1/41841 persons, g1/40.1 and >0.9, respectively). Furthermore, this study confirms the validity and usefulness of the StackCPPred model for COVID-19 prediction.",
        "DOI": "10.1145/3561801.3561805",
        "paper_author": "Fan X.R.",
        "affiliation_name": "Chongqing Technology and Business University",
        "affiliation_city": "Chongqing",
        "affiliation_country": "China",
        "affiliation_id": "60017236",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Public sentiments toward COVID-19 vaccines in South African cities: An analysis of Twitter posts",
        "publication": "Frontiers in Public Health",
        "citied_by": "16",
        "cover_date": "2022-08-12",
        "Abstract": "Amidst the COVID-19 vaccination, Twitter is one of the most popular platforms for discussions about the COVID-19 vaccination. These types of discussions most times lead to a compromise of public confidence toward the vaccine. The text-based data generated by these discussions are used by researchers to extract topics and perform sentiment analysis at the provincial, country, or continent level without considering the local communities. The aim of this study is to use clustered geo-tagged Twitter posts to inform city-level variations in sentiments toward COVID-19 vaccine-related topics in the three largest South African cities (Cape Town, Durban, and Johannesburg). VADER, an NLP pre-trained model was used to label the Twitter posts according to their sentiments with their associated intensity scores. The outputs were validated using NB (0.68), LR (0.75), SVMs (0.70), DT (0.62), and KNN (0.56) machine learning classification algorithms. The number of new COVID-19 cases significantly positively correlated with the number of Tweets in South Africa (Corr = 0.462, P < 0.001). Out of the 10 topics identified from the tweets using the LDA model, two were about the COVID-19 vaccines: uptake and supply, respectively. The intensity of the sentiment score for the two topics was associated with the total number of vaccines administered in South Africa (P < 0.001). Discussions regarding the two topics showed higher intensity scores for the neutral sentiment class (P = 0.015) than for other sentiment classes. Additionally, the intensity of the discussions on the two topics was associated with the total number of vaccines administered, new cases, deaths, and recoveries across the three cities (P < 0.001). The sentiment score for the most discussed topic, vaccine uptake, differed across the three cities, with (P = 0.003), (P = 0.002), and (P < 0.001) for positive, negative, and neutral sentiments classes, respectively. The outcome of this research showed that clustered geo-tagged Twitter posts can be used to better analyse the dynamics in sentiments toward community–based infectious diseases-related discussions, such as COVID-19, Malaria, or Monkeypox. This can provide additional city-level information to health policy in planning and decision-making regarding vaccine hesitancy for future outbreaks.",
        "DOI": "10.3389/fpubh.2022.987376",
        "paper_author": "Ogbuokiri B.",
        "affiliation_name": "York University",
        "affiliation_city": "Toronto",
        "affiliation_country": "Canada",
        "affiliation_id": "60033420",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "Erratum: Research on the driving factors and carbon emission reduction pathways of China's iron and steel industry under the vision of carbon neutrality (Journal of Cleaner Production (2022) 357, (S0959652622015980), (10.1016/j.jclepro.2022.131990))",
        "publication": "Journal of Cleaner Production",
        "citied_by": "27",
        "cover_date": "2022-08-10",
        "Abstract": "Under the vision of carbon neutrality, China's iron and steel industry (CISI) urgently needs to achieve low-carbon development. To formulate effective and targeted emission reduction policies for CISI, the driving forces of carbon dioxide (CO2) emissions and future emission reduction pathways in CISI are explored in this paper. The Logarithmic Mean Divisia Index (LMDI) method and the Mean Impact Value (MIV) technique are adopted to analyze the driving factors of CO2 emissions in CISI at historical and prospective dimensions, respectively. Furthermore, the extreme learning machine (ELM) model optimized by the bat algorithm (BA) is established to project the carbon emission reduction pathways of CISI during 2020–2050 under the business-as-usual (BAU) scenario, the low-speed, medium-speed, and high-speed development scenarios considering the constraint of the carbon neutrality target. The results reveal that production capacity and energy efficiency are essential drivers of CO2 emissions in CISI. Consequently, aimed at achieving carbon neutrality, CISI should focus on eliminating backward capacity and simultaneously accelerating the deployment of advanced technologies. Additionally, it is difficult to accomplish the carbon neutrality goal by 2060 under the BAU scenario. Conversely, under the optimal emission reduction pathway determined by the high-speed development scenario, CISI will reach its peak in 2022 with a peak value of 2143.42 million tons of CO2 (MtCO2). The average annual emission abatement rate during 2022–2050 is maintained at approximately 4.47% and the cumulative reduction rate in 2050 will exceed 70% compared to the base year 2019. CISI is required to develop more stringent emission reduction measures to achieve significant emission abatement. The crude steel production capacity should be reduced to 533 Mt in 2050 and the capacity utilization rate should be maintained beyond 80%. The energy consumption per ton of steel must be decreased to 264 Kg of coal equivalent (Kgce) in 2050.",
        "DOI": "10.1016/j.jclepro.2022.132237",
        "paper_author": "Li W.",
        "affiliation_name": "North China Electric Power University (Baoding)",
        "affiliation_city": "Baoding",
        "affiliation_country": "China",
        "affiliation_id": "60108757",
        "affiliation_state": "Hebei"
    },
    {
        "paper_title": "Geographic microtargeting of social assistance with high-resolution poverty maps",
        "publication": "Proceedings of the National Academy of Sciences of the United States of America",
        "citied_by": "13",
        "cover_date": "2022-08-09",
        "Abstract": "Hundreds of millions of poor families receive some form of targeted social assistance. Many of these antipoverty programs involve some degree of geographic targeting, where aid is prioritized to the poorest regions of the country. However, policy makers in many low-resource settings lack the disaggregated poverty data required to make effective geographic targeting decisions. Using several independent datasets from Nigeria, this paper shows that high-resolution poverty maps, constructed by applying machine learning algorithms to satellite imagery and other nontraditional geospatial data, can improve the targeting of government cash transfers to poor families. Specifically, we find that geographic targeting relying on machine learning-based poverty maps can reduce errors of exclusion and inclusion relative to geographic targeting based on recent nationally representative survey data. This result holds for antipoverty programs that target both the poor and the extreme poor and for initiatives of varying sizes. We also find no evidence that machine learning-based maps increase targeting disparities by demographic groups, such as gender or religion. Based in part on these findings, the Government of Nigeria used this approach to geographically target emergency cash transfers in response to the COVID-19 pandemic.",
        "DOI": "10.1073/pnas.2120025119",
        "paper_author": "Smythe I.S.",
        "affiliation_name": "Columbia University",
        "affiliation_city": "New York",
        "affiliation_country": "United States",
        "affiliation_id": "60030162",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Decarbonization of Indonesia’s Economy: An Analysis using Machine Learning Method",
        "publication": "AIP Conference Proceedings",
        "citied_by": "2",
        "cover_date": "2022-08-02",
        "Abstract": "A strong correlation between economic growth, energy consumption and carbon dioxide (CO2) emissions has made the increasing level of CO2 emissions becomes a perpetual problem worldwide. Therefore, the efficacy of current energy and environment related policies needs to be evaluated. In this regard, finding a reliable model to accurately forecast CO2 emissions is of importance, particularly for a developing country like Indonesia. By involving an unbalanced panel data of 77 countries covering the period of 1966 to 2019, this paper proposes a model which relies on the machine learning method to forecast CO2 emissions in Indonesia and to predict the feasibility for decoupling CO2 emissions from economic growth. This study finds the beneficial impacts of new and renewable energy on reducing CO2 emissions. However, the peak of CO2 emissions in Indonesia was not predicted. Hence, decarbonization of Indonesia’s economy is not likely to be achieved in the near future.",
        "DOI": "10.1063/5.0093879",
        "paper_author": "Sugiawan Y.",
        "affiliation_name": "Badan Tenaga Nuklir Nasional Indonesia",
        "affiliation_city": "Central Jakarta",
        "affiliation_country": "Indonesia",
        "affiliation_id": "60089122",
        "affiliation_state": "Jakarta"
    },
    {
        "paper_title": "The Estimation of the Long-Term Agricultural Output with a Robust Machine Learning Prediction Model",
        "publication": "Agriculture (Switzerland)",
        "citied_by": "7",
        "cover_date": "2022-08-01",
        "Abstract": "Recently, annual agricultural data have been highly volatile as a result of climate change and national economic trends. Therefore, such data might not be enough to develop good agricultural policies for stabilizing agricultural output. A good agricultural output prediction model to assist agricultural policymaking has thus become essential. However, the highly volatile data would affect the prediction model’s performance. For this reason, this study proposes a marriage in honey bees optimization/support vector regression (MBO/SVR) model to minimize the effects of highly volatile data (outliers) and enhance prediction accuracy. We verified the performance of the MBO/SVR model by using the annual total agricultural output collected from the official Agricultural Statistics Yearbook of the Council of Agriculture, Taiwan. Taiwan’s annual total agricultural output integrates agricultural, livestock and poultry, fishery, and forest products. The results indicated that the MBO/SVR model had a lower mean absolute percentage error (MAPE), root mean square percentage error (RMSPE), and relative root mean squared error (r-RMSE) than those of the models it was compared to. Furthermore, the MBO/SVR model predicted long-term agricultural output more accurately and achieved higher directional symmetry (DS) than the other models. Accordingly, the MBO/SVR model is a robust, high-prediction-accuracy model for predicting long-term agricultural output to assist agricultural policymaking.",
        "DOI": "10.3390/agriculture12081075",
        "paper_author": "Kuan C.H.",
        "affiliation_name": "National Taiwan University of Science and Technology",
        "affiliation_city": "Taipei",
        "affiliation_country": "Taiwan",
        "affiliation_id": "60027709",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Machine Learning-Based Beamforming for Unmanned Aerial Vehicles Equipped with Reconfigurable Intelligent Surfaces",
        "publication": "IEEE Wireless Communications",
        "citied_by": "15",
        "cover_date": "2022-08-01",
        "Abstract": "Unmanned aerial vehicles (UAVs) equipped with reconfigurable intelligent surfaces (RISs) have emerged as a promising technology for numerous applications involving aerial networks. However, the UAV-RIS concept faces challenges related to the deployment of the UAV-RIS, especially in cases, where UAV-RIS is combined with emerging technologies, such as beamforming, sensitive to propagation channel variation. In this article, we first overview various use-cases of UAV-RIS beam-forming considering practical scenarios. Aiming to improve the performance of communication channels, we propose a machine learning-based beamforming policy for UAV-RIS by employing prioritized experience replay (PER) based deep Q-Network (DQN). Compared to traditional approaches, the proposed PER DQN-based beamforming for UAV-RIS communication provides significant enhancements in performance. Finally, we highlight some potential directions for future research.",
        "DOI": "10.1109/MWC.004.2100694",
        "paper_author": "Ahmad I.",
        "affiliation_name": "Gomal University",
        "affiliation_city": "Dera Ismail Khan",
        "affiliation_country": "Pakistan",
        "affiliation_id": "60045003",
        "affiliation_state": "North-West Frontier Province"
    },
    {
        "paper_title": "Sentinel-2 Enables Nationwide Monitoring of Single Area Payment Scheme and Greening Agricultural Subsidies in Hungary",
        "publication": "Remote Sensing",
        "citied_by": "4",
        "cover_date": "2022-08-01",
        "Abstract": "The verification and monitoring of agricultural subsidy claims requires combined evaluation of several criteria at the scale of over a million cultivation units. Sentinel-2 satellite imagery is a promising data source and paying agencies are encouraged to test their pre-operational use. Here, we present the outcome of the Hungarian agricultural subsidy monitoring pilot: our goal was to propose a solution based on open-source components and evaluate the main strengths and weaknesses for Sentinel-2 in the framework of a complex set of tasks. These include the checking of the basic cultivation of grasslands and arable land and compliance to the criteria of ecological focus areas. The processing of the satellite data was conducted based on random forest for crop classification and the detection of cultivation events was conducted based on NDVI (Normalized Differential Vegetation Index) time series analysis results. The outputs of these processes were combined in a decision tree ruleset to provide the final results. We found that crop classification provided good performance (overall accuracy 88%) for 22 vegetation classes and cultivation detection was also reliable when compared to on-screen visual interpretation. The main limitation was the size of fields, which were frequently small compared to the spatial resolution of the images: more than 4% of the parcels had to be excluded, although these represent less than 3% of the cultivated area of Hungary. Based on these results, we find that operational satellite-based monitoring is feasible for Hungary, and expect further improvements from integration with Sentinel-1 due to additional temporal resolution.",
        "DOI": "10.3390/rs14163917",
        "paper_author": "Henits L.",
        "affiliation_name": "Ulyssys Software Development and Consulting Ltd",
        "affiliation_city": "Budapest",
        "affiliation_country": "Hungary",
        "affiliation_id": "128306213",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Design and Improvement of SD3-Based Energy Management Strategy for a Hybrid Electric Urban Bus",
        "publication": "Energies",
        "citied_by": "4",
        "cover_date": "2022-08-01",
        "Abstract": "With the rapid development of machine learning, deep reinforcement learning (DRL) algorithms have recently been widely used for energy management in hybrid electric urban buses (HEUBs). However, the current DRL-based strategies suffer from insufficient constraint capability, slow learning speed, and unstable convergence. In this study, a state-of-the-art continuous control DRL algorithm, softmax deep double deterministic policy gradients (SD3), is used to develop the energy management system of a power-split HEUB. In particular, an action masking (AM) technique that does not alter the SD3′s underlying principles is proposed to prevent the SD3-based strategy from outputting invalid actions that violate the system’s physical constraints. Additionally, the transfer learning (TL) method of the SD3-based strategy is explored to avoid repetitive training of neural networks in different driving cycles. The results demonstrate that the learning performance and learning stability of SD3 are unaffected by AM and that SD3 with AM achieves control performance that is highly comparable to dynamic planning for both the CHTC-B and WVUCITY driving cycles. Aside from that, TL contributes to the rapid development of SD3. TL can speed up SD3’s convergence by at least 67.61% without significantly affecting fuel economy.",
        "DOI": "10.3390/en15165878",
        "paper_author": "Wang K.",
        "affiliation_name": "Guangxi University",
        "affiliation_city": "Nanning",
        "affiliation_country": "China",
        "affiliation_id": "60030270",
        "affiliation_state": "Guangxi"
    },
    {
        "paper_title": "Assessing China’s Investment Risk of the Maritime Silk Road: A Model Based on Multiple Machine Learning Methods",
        "publication": "Energies",
        "citied_by": "4",
        "cover_date": "2022-08-01",
        "Abstract": "The maritime silk road policy of China brings opportunities to companies relating to overseas investment. Despite the investment potentials, the risks cannot be ignored and have still not been well assessed. Considering the fact that ICRG comprehensive risk has certain subjectivity, it is not completely applicable to China’s overseas investment. Therefore, based on the data of the China Statistical Yearbook and International Statistical Yearbook, a new indictor is adopted to better capture the Chinese investment risk and to make our prediction more objective. In order to acquire the ability to predict the investment risk in the future which is essential to stakeholders, machine learning techniques are applied by training the ICRG data of the previous year and Outward Foreign Direct Investment (OFDI) data of the next year together. Finally, a relative reliable link has been built between the OFDI indicator in the next year and the left ICRG indicators in the last year with both the best precision score of 86% and recall score of 86% (KNN method). Additionally, the KNN method has a better performance than the other algorithms even for high-level risk, which is more concerning for stakeholders. The selected model cannot only be used to predict an objective and reasonable investment risk level, but can also be used to provide investment risk predictions and suggestions for stakeholders.",
        "DOI": "10.3390/en15165780",
        "paper_author": "Xu J.",
        "affiliation_name": "National University of Defense Technology China",
        "affiliation_city": "Changsha",
        "affiliation_country": "China",
        "affiliation_id": "60024350",
        "affiliation_state": "Hunan"
    },
    {
        "paper_title": "ADMM-Based Differential Privacy Learning for Penalized Quantile Regression on Distributed Functional Data",
        "publication": "Mathematics",
        "citied_by": "2",
        "cover_date": "2022-08-01",
        "Abstract": "Alternating Direction Method of Multipliers (ADMM) is a widely used machine learning tool in distributed environments. In the paper, we propose an ADMM-based differential privacy learning algorithm (FDP-ADMM) on penalized quantile regression for distributed functional data. The FDP-ADMM algorithm can resist adversary attacks to avoid the possible privacy leakage in distributed networks, which is designed by functional principal analysis, an approximate augmented Lagrange function, ADMM algorithm, and privacy policy via Gaussian mechanism with time-varying variance. It is also a noise-resilient, convergent, and computationally effective distributed learning algorithm, even if for high privacy protection. The theoretical analysis on privacy and convergence guarantees is derived and offers a privacy–utility trade-off: a weaker privacy guarantee would result in better utility. The evaluations on simulation-distributed functional datasets have demonstrated the effectiveness of the FDP-ADMM algorithm even if under high privacy guarantee.",
        "DOI": "10.3390/math10162954",
        "paper_author": "Zhou X.",
        "affiliation_name": "Nanjing Audit University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60089945",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "Measuring the Impact of Language Models in Sentiment Analysis for Mexico’s COVID-19 Pandemic",
        "publication": "Electronics (Switzerland)",
        "citied_by": "2",
        "cover_date": "2022-08-01",
        "Abstract": "The world has been facing the COVID-19 pandemic, which has come with an unprecedented impact on general physical health and financial and social repercussions. The adopted mitigation measures also present significant challenges to the population’s mental health and health-related programs. It is complex for public organizations to measure the population’s mental health to incorporate its feedback into their decision-making process. A significant portion of the population has turned to social media to express the details of their daily life, making these public data a rich field for understanding emotional and mental well-being. To this end, by using open sentiment analysis tools, we analyzed 760,064,879 public domain tweets collected from a public access repository to examine the collective shifts in the general mood about the pandemic evolution, news cycles, and governmental policies. Several modern language models were evaluated and compared using intrinsic and extrinsic tasks, that is, the sentiment analysis evaluation of public domain tweets related to the COVID-19 pandemic in Mexico. This study provides a fair evaluation of state-of-the-art language models, such as BERT and VADER, showcasing their metrics and comparing their performance against a real-world task. Results show the importance of selecting the correct language model for large projects such as this one, for there is a need to balance costs with the model’s performance.",
        "DOI": "10.3390/electronics11162483",
        "paper_author": "León-Sandoval E.",
        "affiliation_name": "Tecnológico de Monterrey",
        "affiliation_city": "Monterrey",
        "affiliation_country": "Mexico",
        "affiliation_id": "60007966",
        "affiliation_state": "NLE"
    },
    {
        "paper_title": "Application of Tubular Reactor Technologies for the Acceleration of Biodiesel Production",
        "publication": "Bioengineering",
        "citied_by": "21",
        "cover_date": "2022-08-01",
        "Abstract": "The need to arrest the continued environmental contamination and degradation associated with the consumption of fossil-based fuels has continued to serve as an impetus for the increased utilization of renewable fuels. The demand for biodiesel has continued to escalate in the past few decades due to urbanization, industrialization, and stringent government policies in favor of renewable fuels for diverse applications. One of the strategies for ensuring the intensification, commercialization, and increased utilization of biodiesel is the adaptation of reactor technologies, especially tubular reactors. The current study reviewed the deployment of different types and configurations of tubular reactors for the acceleration of biodiesel production. The feedstocks, catalysts, conversion techniques, and modes of biodiesel conversion by reactor technologies are highlighted. The peculiarities, applications, merits, drawbacks, and instances of biodiesel synthesis through a packed bed, fluidized bed, trickle bed, oscillatory flow, and micro-channel tubular reactor technologies are discussed to facilitate a better comprehension of the mechanisms behind the technology. Indeed, the deployment of the transesterification technique in tubular reactor technologies will ensure the ecofriendly, low-cost, and large-scale production of biodiesel, a high product yield, and will generate high-quality biodiesel. The outcome of this study will enrich scholarship and stimulate a renewed interest in the application of tubular reactors for large-scale biodiesel production among biodiesel refiners and other stakeholders. Going forward, the use of innovative technologies such as robotics, machine learning, smart metering, artificial intelligent, and other modeling tools should be deployed to monitor reactor technologies for biodiesel production.",
        "DOI": "10.3390/bioengineering9080347",
        "paper_author": "Awogbemi O.",
        "affiliation_name": "University of Johannesburg",
        "affiliation_city": "Johannesburg",
        "affiliation_country": "South Africa",
        "affiliation_id": "60000717",
        "affiliation_state": "Gauteng"
    },
    {
        "paper_title": "Efficient Privacy-Preserving K-Means Clustering from Secret-Sharing-Based Secure Three-Party Computation",
        "publication": "Entropy",
        "citied_by": "8",
        "cover_date": "2022-08-01",
        "Abstract": "Privacy-preserving machine learning has become an important study at present due to privacy policies. However, the efficiency gap between the plain-text algorithm and its privacy-preserving version still exists. In this paper, we focus on designing a novel secret-sharing-based K-means clustering algorithm. Particularly, we present an efficient privacy-preserving K-means clustering algorithm based on replicated secret sharing with honest-majority in the semi-honest model. More concretely, the clustering task is outsourced to three semi-honest computing servers. Theoretically, the proposed privacy-preserving scheme can be proven with full data privacy. Furthermore, the experimental results demonstrate that our proposed privacy version reaches the same accuracy as the plain-text one. Compared to the existing privacy-preserving scheme, our proposed protocol can achieve about 16.5×–25.2× faster computation and 63.8×–68.0× lower communication. Consequently, the proposed privacy-preserving scheme is suitable for secret-sharing-based secure outsourced computation.",
        "DOI": "10.3390/e24081145",
        "paper_author": "Wei W.",
        "affiliation_name": "Guangzhou University",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60025345",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Full-Coverage PM<inf>2.5</inf> Mapping and Variation Assessment during the Three-Year Blue-Sky Action Plan Based on a Daily Adaptive Modeling Approach",
        "publication": "Remote Sensing",
        "citied_by": "6",
        "cover_date": "2022-08-01",
        "Abstract": "Owing to a series of air pollution prevention and control policies, China’s PM2.5 pollution has greatly improved; however, the long-term spatial contiguous products that facilitate the analysis of the distribution and variation of PM2.5 pollution are insufficient. Due to the limitations of missing values in aerosol optical depth (AOD) products, the reconstruction of full-coverage PM2.5 concentration remains challenging. In this study, we present a two-stage daily adaptive modeling framework, based on machine learning, to solve this problem. We built the annual models in the first stage, then daily models were constructed in the second stage based on the output of the annual models, which incorporated the parameter and feature adaptive tuning strategy. Within this study, PM2.5 concentrations were adaptively modeled and reconstructed daily based on the multi-angle implementation of atmospheric correction (MAIAC) AOD products and other ancillary data, such as meteorological factors, population, and elevation. Our model validation showed excellent performance with an overall R2 = 0.91 and RMSE = 9.91 μg/m3 for the daily models, along with the site-based cross-validation R2s and RMSEs of 0.86–0.87 and 12–12.33 μg/m3; these results indicated the reliability and feasibility of the proposed approach. The daily full-coverage PM2.5 concentrations at 1 km resolution across China during the Three-Year Blue-Sky Action Plan were reconstructed in this study. We analyzed the distribution and variations of reconstructed PM2.5 at three different time scales. Overall, national PM2.5 pollution has significantly improved with the annual average concentration dropping from 33.67–28.03 μg/m3, which demonstrated that air pollution control policies are effective and beneficial. However, some areas still have severe PM2.5 pollution problems that cannot be ignored. In conclusion, the approach proposed in this study can accurately present daily full-coverage PM2.5 concentrations and the research outcomes could provide a reference for subsequent air pollution prevention and control decision-making.",
        "DOI": "10.3390/rs14153571",
        "paper_author": "He W.",
        "affiliation_name": "Tongji University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60073652",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "NICU Admission for Term Neonates in a Large Single-Center Population: A Comprehensive Assessment of Risk Factors Using a Tandem Analysis Approach",
        "publication": "Journal of Clinical Medicine",
        "citied_by": "7",
        "cover_date": "2022-08-01",
        "Abstract": "Objective: Neonatal intensive care unit (NICU) admission among term neonates is associated with significant morbidity and mortality, as well as high healthcare costs. A comprehensive NICU admission risk assessment using an integrated statistical approach for this rare admission event may be used to build a risk calculation algorithm for this group of neonates prior to delivery. Methods: A single-center case–control retrospective study was conducted between August 2005 and December 2019, including in-hospital singleton live born neonates, born at ≥37 weeks’ gestation. Analyses included univariate and multivariable models combined with the machine learning gradient-boosting model (GBM). The primary aim of the study was to identify and quantify risk factors and causes of NICU admission of term neonates. Results: During the study period, 206,509 births were registered at the Shaare Zedek Medical Center. After applying the study exclusion criteria, 192,527 term neonates were included in the study; 5292 (2.75%) were admitted to the NICU. The NICU admission risk was significantly higher (ORs [95%CIs]) for offspring of nulliparous women (1.19 [1.07, 1.33]), those with diabetes mellitus or hypertensive complications of pregnancy (2.52 [2.09, 3.03] and 1.28 [1.02, 1.60] respectively), and for those born during the 37th week of gestation (2.99 [2.63, 3.41]; p < 0.001 for all), adjusted for congenital malformations and genetic syndromes. A GBM to predict NICU admission applied to data prior to delivery showed an area under the receiver operating characteristic curve of 0.750 (95%CI 0.743–0.757) and classified 27% as high risk and 73% as low risk. This risk stratification was significantly associated with adverse maternal and neonatal outcomes. Conclusion: The present study identified NICU admission risk factors for term neonates; along with the machine learning ranking of the risk factors, the highly predictive model may serve as a basis for individual risk calculation algorithm prior to delivery. We suggest that in the future, this type of planning of the delivery will serve different health systems, in both high- and low-resource environments, along with the NICU admission or transfer policy.",
        "DOI": "10.3390/jcm11154258",
        "paper_author": "Talisman S.",
        "affiliation_name": "Hebrew University-Hadassah Medical School",
        "affiliation_city": "Jerusalem",
        "affiliation_country": "Israel",
        "affiliation_id": "60072445",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A cross-country analysis of macroeconomic responses to COVID-19 pandemic using Twitter sentiments",
        "publication": "PLoS ONE",
        "citied_by": "14",
        "cover_date": "2022-08-01",
        "Abstract": "The COVID-19 pandemic has had a devastating impact on the global economy. In this paper, we use the Phillips curve to compare and analyze the macroeconomics of three different countries with distinct income levels, namely, lower-middle (Nigeria), upper-middle (South Africa), and high (Canada) income. We aim to (1) find macroeconomic changes in the three countries during the pandemic compared to pre-pandemic time, (2) compare the countries in terms of response to the COVID-19 economic crisis, and (3) compare their expected economic reaction to the COVID-19 pandemic in the near future. An advantage to our work is that we analyze macroeconomics on a monthly basis to capture the shocks and rapid changes caused by on and off rounds of lockdowns. We use the volume and social sentiments of the Twitter data to approximate the macroeconomic statistics. We apply four different machine learning algorithms to estimate the unemployment rate of South Africa and Nigeria on monthly basis. The results show that at the beginning of the pandemic the unemployment rate increased for all the three countries. However, Canada was able to control and reduce the unemployment rate during the COVID-19 pandemic. Nonetheless, in line with the Phillips curve short-run, the inflation rate of Canada increased to a level that has never occurred in more than fifteen years. Nigeria and South Africa have not been able to control the unemployment rate and did not return to the pre-COVID-19 level. Yet, the inflation rate has increased in both countries. The inflation rate is still comparable to the pre- COVID-19 level in South Africa, but based on the Phillips curve short-run, it will increase further, if the unemployment rate decreases. Unfortunately, Nigeria is experiencing a horrible stagflation and a wild increase in both unemployment and inflation rates. This shows how vulnerable lower-middle-income countries could be to lockdowns and economic restrictions. In the near future, the main concern for all the countries is the high inflation rate. This work can potentially lead to more targeted and publicly acceptable policies based on social media content.",
        "DOI": "10.1371/journal.pone.0272208",
        "paper_author": "Nia Z.M.",
        "affiliation_name": "York University",
        "affiliation_city": "Toronto",
        "affiliation_country": "Canada",
        "affiliation_id": "60033420",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "Artificial Intelligence-Augmented Propensity Score, Cost Effectiveness and Computational Ethical Analysis of Cardiac Arrest and Active Cancer with Novel Mortality Predictive Score",
        "publication": "Medicina (Lithuania)",
        "citied_by": "11",
        "cover_date": "2022-08-01",
        "Abstract": "Background and objectives: Little is known about outcome improvements and disparities in cardiac arrest and active cancer. We performed the first known AI and propensity score (PS)-augmented clinical, cost-effectiveness, and computational ethical analysis of cardio-oncology cardiac arrests including left heart catheterization (LHC)-related mortality reduction and related disparities. Materials and methods: A nationally representative cohort analysis was performed for mortality and cost by active cancer using the largest United States all-payer inpatient dataset, the National Inpatient Sample, from 2016 to 2018, using deep learning and machine learning augmented propensity score-adjusted (ML-PS) multivariable regression which informed cost-effectiveness and ethical analyses. The Cardiac Arrest Cardio-Oncology Score (CACOS) was then created for the above population and validated. The results informed the computational ethical analysis to determine ethical and related policy recommendations. Results: Of the 101,521,656 hospitalizations, 6,656,883 (6.56%) suffered cardiac arrest of whom 61,300 (0.92%) had active cancer. Patients with versus without active cancer were significantly less likely to receive an inpatient LHC (7.42% versus 20.79%, p < 0.001). In ML-PS regression in active cancer, post-arrest LHC significantly reduced mortality (OR 0.18, 95%CI 0.14–0.24, p < 0.001) which PS matching confirmed by up to 42.87% (95%CI 35.56–50.18, p < 0.001). The CACOS model included the predictors of no inpatient LHC, PEA initial rhythm, metastatic malignancy, and high-risk malignancy (leukemia, pancreas, liver, biliary, and lung). Cost-benefit analysis indicated 292 racial minorities and $2.16 billion could be saved annually by reducing racial disparities in LHC. Ethical analysis indicated the convergent consensus across diverse belief systems that such disparities should be eliminated to optimize just and equitable outcomes. Conclusions: This AI-guided empirical and ethical analysis provides a novel demonstration of LHC mortality reductions in cardio-oncology cardiac arrest and related disparities, along with an innovative predictive model that can be integrated within the digital ecosystem of modern healthcare systems to improve equitable clinical and public health outcomes.",
        "DOI": "10.3390/medicina58081039",
        "paper_author": "Monlezun D.J.",
        "affiliation_name": "The University of Texas MD Anderson Cancer Center",
        "affiliation_city": "Houston",
        "affiliation_country": "United States",
        "affiliation_id": "60015023",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "COVID-19 Vaccines Related User’s Response Categorization Using Machine Learning Techniques",
        "publication": "Computation",
        "citied_by": "17",
        "cover_date": "2022-08-01",
        "Abstract": "Respiratory viruses known as coronaviruses infect people and cause death. The multiple crown-like spikes on the virus’s surface give them the name “corona”. The pandemic has resulted in a global health crisis and it is expected that every year we will have to fight against different COVID-19 variants. In this critical situation, the existence of COVID-19 vaccinations provides hope for mankind. Despite severe vaccination campaigns and recommendations from health experts and the government, people have perceptions regarding vaccination risks and share their views and experiences on social media platforms. Social attitudes to these types of vaccinations are influenced by their positive and negative effects. The analysis of such opinions can help to determine social trends and formulate policies to increase vaccination acceptance. This study presents a methodology for sentiment analysis of the global perceptions and perspectives related to COVID-19 vaccinations. The research is performed on five vaccinations that include Sinopharm, Pfizer, Moderna, AstraZeneca, and Sinovac on the Twitter platform extracted using Twitter crawling. To effectively perform this research, tweets datasets are categorized into three groups, i.e., positive, negative and natural. For sentiment classification, different machine learning classifiers are used such as Random Forest (RF), Naive Bayes (NB), Decision Tree (DT), Logistic Regression (LR), and Support Vector Machine (SVM). It should be noted that the Decision tree classifier achieves the highest classification performance in all datasets as compared to the other machine learning algorithms. For COVID-19 Vaccine Tweets with Sentiment Annotation (CVSA), the highest accuracy obtained is 93.0%, for the AstraZeneca vaccine dataset 90.94%, for the Pfizer vaccine dataset 91.07%, 88.01% accuracy for the Moderna vaccine dataset, for the Sinovac vaccine dataset 92.8% accuracy, and 93.87% accuracy for the Sinopharm vaccine dataset, respectively. The quantitative comparisons demonstrate that the proposed research achieves better accuracy as compared to state-of-the-art research.",
        "DOI": "10.3390/computation10080141",
        "paper_author": "Shahzad A.",
        "affiliation_name": "Government College University Faisalabad",
        "affiliation_city": "Faisalabad",
        "affiliation_country": "Pakistan",
        "affiliation_id": "60070615",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Monitoring and Identification of Agricultural Crops through Multitemporal Analysis of Optical Images and Machine Learning Algorithms",
        "publication": "Sensors",
        "citied_by": "7",
        "cover_date": "2022-08-01",
        "Abstract": "The information about where crops are distributed is useful for agri-environmental assessments, but is chiefly important for food security and agricultural policy managers. The quickness with which this information becomes available, especially over large areas, is important for decision makers. Methodologies have been proposed for the study of crops. Most of them require field survey for ground truth data and a single crop map is generated for the whole season at the end of the crop cycle and for the next crop cycle a new field survey is necessary. Here, we present models for recognizing maize (Zea mays L.), beans (Phaseolus vulgaris L.), and alfalfa (Medicago sativa L.) before the crop cycle ends without current-year field survey for ground truth data. The models were trained with an exhaustive field survey at plot level in a previous crop cycle. The field surveys begin since days before the emergence of crops to maturity. The algorithms used for classification were support vector machine (SVM) and bagged tree (BT), and the spectral information captured in the visible, red-edge, near infrared, and shortwave infrared regions bands of Sentinel 2 images was used. The models were validated within the next crop cycle each fifteen days before the mid-season. The overall accuracies range from 71.9% (38 days after the begin of cycle) to 87.5% (81 days after the begin cycle) and a kappa coefficient ranging from 0.53 at the beginning to 0.74 at mid-season",
        "DOI": "10.3390/s22166106",
        "paper_author": "Espinosa-Herrera J.M.",
        "affiliation_name": "Colegio De Postgraduados Campus Montecillo",
        "affiliation_city": "Texcoco",
        "affiliation_country": "Mexico",
        "affiliation_id": "60033065",
        "affiliation_state": "MEX"
    },
    {
        "paper_title": "Predicting the Impact of Utility Lighting Rebate Programs on Promoting Industrial Energy Efficiency: A Machine Learning Approach",
        "publication": "Environments - MDPI",
        "citied_by": "11",
        "cover_date": "2022-08-01",
        "Abstract": "Implementation costs are a major factor in manufacturers’ decisions to invest in energy-efficient technologies. Emerging technologies in lighting systems, however, typically require small investment costs and offer short, simple payback periods, due, in part, to federal, state, and utility incentive programs. Recently, however, certain state and federal mandates have reduced the support for and efficacy of electricity utility incentivizing programs. To determine the impact of such support programs, this study examined historical data regarding lighting retrofit savings, implementation costs, and utility rebates gathered from 13 years of industrial energy audits by a U.S. Department of Energy Industrial Assessment Center in a midwestern state. It uses a machine learning approach to evaluate the industrial energy and cost-saving opportunities that may have been lost due to decisions attributable to legislative mandates, utility policies, and manufacturers’ calculations and to evaluate the potential effect of lighting rebates on manufacturers’ decisions to implement industrial energy-efficient lighting retrofits. The results indicate that the decision not to implement lighting energy efficiency recommendations resulted in a loss of more than USD800,000 in potential rebates by industries during the study period and that the implementation of lighting energy assessment recommendations could have increased by about 50% if electric utility rebates had been available. These findings can help industries evaluate the benefits of implementing lighting efficiency improvements, and help utilities determine feasible lighting retrofit rebate values for incentivizing such changes by the industries they serve.",
        "DOI": "10.3390/environments9080100",
        "paper_author": "Shook P.",
        "affiliation_name": "Ameresco",
        "affiliation_city": "Columbia",
        "affiliation_country": "United States",
        "affiliation_id": "128501672",
        "affiliation_state": "MD"
    },
    {
        "paper_title": "Analyzing Spanish-Language Public Sentiment in the Context of a Pandemic and Social Unrest: The Panama Case",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "6",
        "cover_date": "2022-08-01",
        "Abstract": "Over the past decade, an increase in global connectivity and social media users has changed the way in which opinions and sentiments are shared. Platforms such as Twitter can act as public forums for expressing opinions on non-personal matters, but often also as an outlet for individuals to share their feelings and personal thoughts. This becomes especially evident during times of crisis, such as a massive civil disorder or a pandemic. This study proposes the estimation and analysis of sentiments expressed by Twitter users of the Republic of Panama during the years 2019 and 2020. The proposed workflow is comprised of the extraction, quantification, processing and analysis of Spanish-language Twitter data based on Sentiment Analysis. This case of study highlights the importance of developing natural language processing resources explicitly devised for supporting opinion mining applications in Latin American countries, where language regionalisms can drastically change the lexicon on each country. A comparative analysis performed between popular machine learning algorithms demonstrated that a version of a distributed gradient boosting algorithm could infer sentiment polarity contained in Spanish text in an accurate and time-effective manner. This algorithm is the tool used to analyze over 20 million tweets produced between the years of 2019 and 2020 by residents of the Republic of Panama, accurately displaying strong sentiment responses to events occurred in the country over the two years that the analysis performed spanned. The obtained results highlight the potential that methodologies such as the one proposed in this study could have for transparent government monitoring of responses to public policies on a population scale.",
        "DOI": "10.3390/ijerph191610328",
        "paper_author": "Arias F.",
        "affiliation_name": "Universidad Tecnológica de Panamá",
        "affiliation_city": "Panama City",
        "affiliation_country": "Panama",
        "affiliation_id": "60103993",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "ULMR: An Unsupervised Learning Framework for Mismatch Removal",
        "publication": "Sensors",
        "citied_by": "3",
        "cover_date": "2022-08-01",
        "Abstract": "Due to radiometric and geometric distortions between images, mismatches are inevitable. Thus, a mismatch removal process is required for improving matching accuracy. Although deep learning methods have been proved to outperform handcraft methods in specific scenarios, including image identification and point cloud classification, most learning methods are supervised and are susceptible to incorrect labeling, and labeling data is a time-consuming task. This paper takes advantage of deep reinforcement leaning (DRL) and proposes a framework named unsupervised learning for mismatch removal (ULMR). Resorting to DRL, ULMR firstly scores each state–action pair guided by the output of classification network; then, it calculates the policy gradient of the expected reward; finally, through maximizing the expected reward of state–action pairings, the optimal network can be obtained. Compared to supervised learning methods (e.g., NM-Net and LFGC), unsupervised learning methods (e.g., ULCM), and handcraft methods (e.g., RANSAC, GMS), ULMR can obtain higher precision, more remaining correct matches, and fewer remaining false matches in testing experiments. Moreover, ULMR shows greater stability, better accuracy, and higher quality in application experiments, demonstrating reduced sampling times and higher compatibility with other classification networks in ablation experiments, indicating its great potential for further use.",
        "DOI": "10.3390/s22166110",
        "paper_author": "Deng C.",
        "affiliation_name": "Wuhan University",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60029306",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "A Bibliometric Analysis of Solar Energy Forecasting Studies in Africa",
        "publication": "Energies",
        "citied_by": "24",
        "cover_date": "2022-08-01",
        "Abstract": "Solar energy forecasting is considered an essential scientific aspect in supporting efforts to integrate solar energy into power grids. Moreover, solar energy forecasting plays an essential role in mitigating greenhouse gas emissions and conserving energy for future use. This study conducted a bibliometric analysis to assess solar energy forecasting research studies evolution at the continental (Africa) and southern Africa levels. Key aspects of analysis included (i) scientific research trends, (ii) nature of collaboration networks, (iii) co-occurrence of keywords and (iv) emerging themes in solar energy forecasting over the last two decades, between the years 2000–2021. The results indicate that solar energy forecasting research has, on average, expanded by 6.4% and 3.3% in Africa and southern Africa, respectively. Based on the study context, solar energy forecasting research only gained momentum in 2015, peaking in 2019, but it is generally still subtle. The scientific mapping illustrated that only South Africa ranks among the leading countries that have produced high numbers of published documents and also leads in contributions to the research area in both Africa and southern Africa. Three emerging topics were identified from the thematic map analysis—namely, “solar irradiance”, “artificial intelligence” and “clear sky”, which implies that researchers are paying attention to solar irradiance, using modelling techniques that incorporate machine learning techniques. Overall, this study contributes to scientific information on the potential bankability of renewable energy projects that could assist power utilities, governments and policymakers in Africa to enforce the green economy through accelerated decarbonisation of the energy systems and building relationships with developed countries for support and better transitioning to solar energy. From a Water–Energy–Food nexus perspective, the results of this work could assist the scientific community in Africa to take advantage of the inherent interconnectedness of water, energy and food resources, whilst also advancing the use of integrated solutions to shape the focus of solar energy research into a more systems thinking and transdisciplinary approach involving the interconnected primary resources and stakeholders pursuit of the Sustainable Development Goals.",
        "DOI": "10.3390/en15155520",
        "paper_author": "Zwane N.",
        "affiliation_name": "South African Weather Service",
        "affiliation_city": "Pretoria",
        "affiliation_country": "South Africa",
        "affiliation_id": "60028665",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Does Uncertainty Forecast Crude Oil Volatility before and during the COVID-19 Outbreak? Fresh Evidence Using Machine Learning Models",
        "publication": "Energies",
        "citied_by": "8",
        "cover_date": "2022-08-01",
        "Abstract": "This paper uses two competing machine learning models, namely the Support Vector Regression (SVR) and the eXtreme Gradient Boosting (XGBoost) against the Autoregressive Integrated Moving Average ARIMAX (p,d,q) model to identify their predictive performance of the crude oil volatility index before and during COVID-19. In terms of accuracy, forecasting results reveal that the SVR model dominates the XGBoost and ARIMAX models in predicting the crude oil volatility index before COVID-19. However, the XGBoost model provides more accurate predictions of the crude oil volatility index than the SVR and ARIMAX models during the pandemic. The inverse cumulative distribution of residuals suggests that both ML models produce good results in terms of convergence. Findings also indicate that there is a fast convergence to the optimal solution when using the XGBoost model. When analyzing the feature importance, the Shapley Additive Explanation Method reveals that the SVR performs significantly better than the XGBoost in terms of feature importance. During the pandemic, the predictive power of the CBOE Volatility Index and Economic Policy Uncertainty index for forecasting the crude oil volatility index is improved compared to the pre-COVID-19 period. These findings imply that investor fear-induced uncertainty in the financial market and economic policy uncertainty are the most significant features and hence represent substantial sources of uncertainty in the oil market.",
        "DOI": "10.3390/en15155744",
        "paper_author": "Tissaoui K.",
        "affiliation_name": "University of Ha'il",
        "affiliation_city": "Ha'il",
        "affiliation_country": "Saudi Arabia",
        "affiliation_id": "60104671",
        "affiliation_state": "Ha'il"
    },
    {
        "paper_title": "Public’s Mental Health Monitoring via Sentimental Analysis of Financial Text Using Machine Learning Techniques",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "19",
        "cover_date": "2022-08-01",
        "Abstract": "Public feelings and reactions associated with finance are gaining significant importance as they help individuals, public health, financial and non-financial institutions, and the government understand mental health, the impact of policies, and counter-response. Every individual sentiment linked with a financial text can be categorized, whether it is a headline or the detailed content published in a newspaper. The Guardian newspaper is considered one of the most famous and the biggest websites for digital media on the internet. Moreover, it can be one of the vital platforms for tracking the public’s mental health and feelings via sentimental analysis of news headlines and detailed content related to finance. One of the key purposes of this study is the public’s mental health tracking via the sentimental analysis of financial text news primarily published on digital media to identify the overall mental health of the public and the impact of national or international financial policies. A dataset was collected using The Guardian application programming interface and processed using the support vector machine, AdaBoost, and single layer convolutional neural network. Among all identified techniques, the single layer convolutional neural network with a classification accuracy of 0.939 is considered the best during the training and testing phases as it produced efficient performance and effective results compared to other techniques, such as support vector machine and AdaBoost with associated classification accuracies 0.677 and 0.761, respectively. The findings of this research would also benefit public health, as well as financial and non-financial institutions.",
        "DOI": "10.3390/ijerph19159695",
        "paper_author": "Alanazi S.A.",
        "affiliation_name": "Jouf University",
        "affiliation_city": "Sakakah",
        "affiliation_country": "Saudi Arabia",
        "affiliation_id": "60104126",
        "affiliation_state": "Al Jawf"
    },
    {
        "paper_title": "Long-Term Traffic Forecast Using Neural Network and Seasonal Autoregressive Integrated Moving Average: Case of a Container Port",
        "publication": "Transportation Research Record",
        "citied_by": "7",
        "cover_date": "2022-08-01",
        "Abstract": "Long-term insight into maritime traffic is critical for port authorities, logistics companies, and port operators to proactively formulate suitable policies, develop strategic plans, allocate budget, and preserve and improve competitiveness. Forecasting freight rate is a spotlight in port traffic literature, but relatively little research has been directed at forecasting long-term vessel traffic trends. Based on forecast long-term freight rate input provided by the recent 10-year strategic planning of the port of Rajaee, the largest port of Iran, the paper implements seasonal autoregressive integrated moving average (SARIMA) and neural network (NN) models to forecast its container vessel traffic between 2020 and 2025. A database consisting of monthly container traffic data for this port from 1999 to 2019 is utilized. The comparison between the two forecasting models is fulfilled by benchmarking the naïve method. The results reveal the superiority of the NN model over SARIMA in this practice. Considering NN model outputs, the port should expect a significant increase in Panamax and Over-Panamax vessels in the future, and, if not timely addressed, this would result in a systemic queue in the port of Rajaee. That said, the approach can be implemented in port planning and design to avoid under-or over-estimations in such capital-intensive projects.",
        "DOI": "10.1177/03611981221083311",
        "paper_author": "Gargari N.S.",
        "affiliation_name": "Cork University Business School (CUBS)",
        "affiliation_city": "Cork",
        "affiliation_country": "Ireland",
        "affiliation_id": "60231072",
        "affiliation_state": "Munster"
    },
    {
        "paper_title": "Leveraging Big Data and Coordinated Charging for Effective Taxi Fleet Electrification: The 100% EV Conversion of Shenzhen, China",
        "publication": "IEEE Transactions on Intelligent Transportation Systems",
        "citied_by": "11",
        "cover_date": "2022-08-01",
        "Abstract": "Taxis provide an important market for electric vehicles (EVs), but long charging durations and limited charger availability have prevented rapid adoption. Leveraging over two weeks of high-resolution GPS and battery data from almost 20,000 EVs in the all-electric Shenzhen taxi fleet, we analyze the potential to improve fleet-wide operations by optimizing both the location and timing of vehicle charging. We construct machine learning models to predict travel time, queuing time at charging stations, and charge consumption by time of day. Contrary to the emphasis on charging station siting in the literature, we find that optimizing charging locations would have a relatively limited impact. Instead, providing drivers with better real-time information about queuing times at charging stations, and enabling flexibility in battery charge during shift changes could reduce down-time per vehicle by over 30 minutes per day, while increasing the number of economically viable charging stations by over 50%. Moreover, taking full advantage of break periods and nighttime to charge could reduce downtime per vehicle by over one hour per day, reducing revenue losses due to charging by roughly 90%. These results are verified with evidence from real-time charging station data and driver shift-change data. Policy recommendations from this study include establishing citywide open data platforms to integrate real-time data on vehicle trajectory, battery charge, and charger availability, and providing drivers and companies with training on best charging practices. As a number of cities worldwide move toward fully electrified taxi fleets, this analysis has large-scale implications for decarbonized, cleaner urban areas.",
        "DOI": "10.1109/TITS.2021.3092276",
        "paper_author": "Bauer G.S.",
        "affiliation_name": "University of California, Berkeley",
        "affiliation_city": "Berkeley",
        "affiliation_country": "United States",
        "affiliation_id": "60025038",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Frailty: understanding the difference between age and ageing",
        "publication": "Age and Ageing",
        "citied_by": "46",
        "cover_date": "2022-08-01",
        "Abstract": "In the past, illness and dependence were viewed as inevitable consequences of old age. Now, we understand that there is a difference between age (the passing of chronological time) and ageing (the increased risk of adverse outcomes over time). Over the last 50 years, 'frailty' research has established that ageing is heterogeneous, variable and malleable. Significant advances have been made in frailty measurement (description of clinical features and development of clinical models), mechanisms (insights into pathogenesis) and management (development of interventions to reduce and/or prevent progression). Subsequently, the concept of frailty has informed health policy and clinical practice and started to change perceptions of older age held by the general public and the health sector. Here, we overview key achievements in frailty research and clinical practice and highlight the considerable number of known unknowns that may be addressed in the future.",
        "DOI": "10.1093/ageing/afac185",
        "paper_author": "Gordon E.H.",
        "affiliation_name": "The University of Queensland",
        "affiliation_city": "Brisbane",
        "affiliation_country": "Australia",
        "affiliation_id": "60031004",
        "affiliation_state": "QLD"
    },
    {
        "paper_title": "Integrating hypertension and HIV care in Namibia: A quality improvement collaborative approach",
        "publication": "PLoS ONE",
        "citied_by": "8",
        "cover_date": "2022-08-01",
        "Abstract": "Background Hypertension (HTN) is highly prevalent among people with HIV (PWH) in Namibia, but screening and treatment for HTN are not routinely offered as part of HIV care delivery. We report the implementation of a quality improvement collaborative (QIC) to accelerate integration of HTN and HIV care within public-sector health facilities in Namibia. Methods Twenty-four facilities participated in the QIC with the aim of increasing HTN screening and treatment among adult PWH (>15 years). HTN was defined according to national treatment guidelines (i.e., systolic blood pressure >140 and/or diastolic blood pressure >90 across three measurements and at least two occasions), and decisions regarding initiation of treatment were made by physicians only. Teams from participating hospitals used quality improvement methods, monthly measurement of performance indicators, and small-scale tests of change to implement contextually tailored interventions. Coaching of sites was performed on a monthly basis by clinical officers with expertise in QI and HIV, and sites were convened as part of learning sessions to facilitate diffusion of effective interventions. Results Between March 2017 and March 2018, hypertension screening occurred as part of 183,043 (86%) clinical encounters at participating facilities. Among 1,759 PWH newly diagnosed with HTN, 992 (56%) were initiated on first-line treatment. Rates of treatment initiation were higher in facilities with an on-site physician (61%) compared to those without one (51%). During the QIC, facility teams identified fourteen interventions to improve HTN screening and treatment. Among barriers to implementation, teams pointed to malfunctions of blood pressure machines and stock outs of antihypertensive medications as common challenges. Conclusions Implementation of a QIC provided a structured approach for integrating HTN and HIV services across 24 high-volume facilities in Namibia. As rates of HTN treatment remained low despite ongoing facility-level changes, policy-level interventions—such as task sharing and supply chain strengthening—should be pursued to further improve delivery of HTN care among PWH beyond initial screening.",
        "DOI": "10.1371/journal.pone.0272727",
        "paper_author": "Basenero A.",
        "affiliation_name": "Ministry of Health and Social Services",
        "affiliation_city": "Windhoek",
        "affiliation_country": "Namibia",
        "affiliation_id": "120614656",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Resource Allocation for Cellular Zero-Touch Deterministic Industrial M2M Networks: A Reinforcement Learning-Based Scheme",
        "publication": "IEEE Sensors Letters",
        "citied_by": "4",
        "cover_date": "2022-08-01",
        "Abstract": "In this letter, we investigate the resource allocation problem for cellular zero-touch deterministic industrial machine-to-machine networks. We consider a scenario in which multiple self-adaptive industrial machine-to-machine (M2M) networks are deployed underlaying cellular networks to emulate a variety of vertical use cases in future Industry 4.0 with the objective of guaranteeing the determinacy of latency by learning-inspired resource allocation policy. Specifically, in order to figure out the relationship between latency determinacy and the utilization of wireless resources meanwhile modeling the uncertainty of the stochastic environment, we first formulate the resource allocation problem as a Markov game by jointly considering transmission power control, frequency spectrum allocation, and the selection of base stations. Afterward, a random graph-based sparse long short-term memory network is proposed to solve the optimization problem while reducing the computational complexity. Finally, numerical result demonstrates the effectiveness of the proposed scheme.",
        "DOI": "10.1109/LSENS.2022.3194141",
        "paper_author": "Xu Y.H.",
        "affiliation_name": "Nanjing Forestry University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60025665",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "Trends in the Prevalence of Chronic Medication Use Among Children in Israel Between 2010 and 2019: Protocol for a Retrospective Cohort Study",
        "publication": "JMIR Research Protocols",
        "citied_by": "0",
        "cover_date": "2022-08-01",
        "Abstract": "Background: Prescription of psychostimulants has significantly increased in most countries worldwide for both preschool and school-aged children. Understanding the trends of chronic medication use among children in different age groups and from different sociodemographic backgrounds is essential. It is essential to distinguish between selected therapy areas to help decision-makers evaluate not only the relevant expected medication costs but also the specific services related to these areas. Objective: This study will analyze differences in trends regarding medications considered psychobehavioral treatments and medications considered nonpsychobehavioral treatments and will identify risk factors and predictors for chronic medication use among children. Methods: This is a retrospective study. Data will be extracted from the Clalit Health Services data warehouse. For each year between 2010 and 2019, there are approximately 1,500,000 children aged 0-18 years. All medication classes will be identified using the Anatomical Therapeutic Chemical code. A time-trend analysis will be performed to investigate if there is a significant difference between the trends of children's psychobehavioral and nonpsychobehavioral medication prescriptions. A logistic regression combined with machine learning models will be developed to identify variables that may increase the risk for specific chronic medication types and identify children likely to get such treatment. Results: The project was funded in 2019. Data analysis is currently underway, and the results are expected to be submitted for publication in 2022. Understanding trends regarding medications considered psychobehavioral treatments and medications considered nonpsychobehavioral treatments will support the identification of risk factors and predictors for chronic medication use among children. Conclusions: Analyzing the response of the patient (and their parents or caregivers) population over time will hopefully help improve policies for prescriptions and follow-up of chronic treatments in children",
        "DOI": "10.2196/36756",
        "paper_author": "Sadaka Y.",
        "affiliation_name": "Ministry of Health",
        "affiliation_city": "Jerusalem",
        "affiliation_country": "Israel",
        "affiliation_id": "60002021",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "NCCIH Priorities for Natural Products Research",
        "publication": "Planta Medica",
        "citied_by": "4",
        "cover_date": "2022-08-01",
        "Abstract": "The National Center for Complementary and Integrative Health (NCCIH) is the lead agency within the U.S. federal government for complementary and integrative health research, which includes natural products. Although NCCIH is one of the smaller components of the National Institutes of Health (NIH), NCCIH funds a disproportionately high percentage of natural products research at NIH. This stems from NCCIH being the only NIH grant issuing component that includes natural products as an explicit part of its mission. This perspective provides an overview of the NCCIH mission and summarizes NCCIH funding priorities for natural products research across basic and mechanistic as well as clinical sectors. These priorities are guided by the recently released NCCIH strategic plan. A primary element of this new plan is a focus on whole person health instead of the frequent focus on the treatment of diseases. The NCCIH focus on whole person health includes how natural products and multicomponent therapeutic approaches, which often include natural products, can help move individuals towards health restoration and promotion.",
        "DOI": "10.1055/a-1767-2226",
        "paper_author": "Still P.",
        "affiliation_name": "National Institutes of Health (NIH)",
        "affiliation_city": "Bethesda",
        "affiliation_country": "United States",
        "affiliation_id": "60006577",
        "affiliation_state": "MD"
    },
    {
        "paper_title": "Deep Reinforcement Learning for autonomous pre-failure tool life improvement",
        "publication": "International Journal of Advanced Manufacturing Technology",
        "citied_by": "6",
        "cover_date": "2022-08-01",
        "Abstract": "This paper develops an approach to improve a CNC machine’s tool performance and slow down its degradation rate automatically in the Pre-Failure stage. A Deep Reinforcement Learning (DRL) agent is developed to optimize the machining process performance online during the Pre-Failure interval of the tool’s life. The Pre-Failure agent that is presented in the proposed approach tunes the feed rate according to the optimal policy that is learned in order to slow down the tool’s degradation rate, while maintaining an acceptable Material Removal Rate (MRR) level. The machine learning techniques and pattern recognitions are implemented to monitor and detect the tool’s potential failure level. The proposed mechanism is applied to a CNC machine when turning Titanium Metal Matrix Composites (TiMMC). A CNC machine Digital Twin (DT) is developed to emulate the physical machine in the digital environment. It is validated with the physical machine’s measurements. The proposed pre-failure mechanism is a model-free approach, which can be implemented in any machining process with fewer online computational efforts. It also validated on a wide range of cutting speeds, up to 15,000 RPM. Deployment of the proposed machine learning approach for the particular case study improves the tool’s Time to Failure (T2F) by 40% and the MRR by 6%, on average, compared to the classical approach.",
        "DOI": "10.1007/s00170-022-09700-4",
        "paper_author": "Taha H.A.",
        "affiliation_name": "Polytechnique Montréal",
        "affiliation_city": "Montreal",
        "affiliation_country": "Canada",
        "affiliation_id": "60019141",
        "affiliation_state": "QC"
    },
    {
        "paper_title": "Traded Control of Human-Machine Systems for Sequential Decision-Making Based on Reinforcement Learning",
        "publication": "IEEE Transactions on Artificial Intelligence",
        "citied_by": "8",
        "cover_date": "2022-08-01",
        "Abstract": "Sequential decision-making (SDM) is a common type of decision-making problem with sequential and multistage characteristics. Among them, the learning and updating of policy are the main challenges in solving SDM problems. Unlike previous machine autonomy driven by artificial intelligence alone, we improve the control performance of SDM tasks by combining human intelligence and machine intelligence. Specifically, this article presents a paradigm of a human-machine traded control systems based on reinforcement learning methods to optimize the solution process of sequential decision problems. By designing the idea of autonomous boundary and credibility assessment, we enable humans and machines at the decision-making level of the systems to collaborate more effectively. And the arbitration in the human-machine traded control systems introduces the Bayesian neural network and the dropout mechanism to consider the uncertainty and security constraints. Finally, experiments involving machine traded control, human traded control were implemented. The preliminary experimental results of this article show that our traded control method improves decision-making performance and verifies the effectiveness for SDM problems.",
        "DOI": "10.1109/TAI.2021.3127857",
        "paper_author": "Zhang Q.",
        "affiliation_name": "University of Science and Technology of China",
        "affiliation_city": "Hefei",
        "affiliation_country": "China",
        "affiliation_id": "60019118",
        "affiliation_state": "Anhui"
    },
    {
        "paper_title": "Prediction with expert advice applied to the problem of prediction with expert advice",
        "publication": "Synthese",
        "citied_by": "3",
        "cover_date": "2022-08-01",
        "Abstract": "We often need to have beliefs about things on which we are not experts. Luckily, we often have access to expert judgements on such topics. But how should we form our beliefs on the basis of expert opinion when experts conflict in their judgments? This is the core of the novice/2-expert problem in social epistemology. A closely related question is important in the context of policy making: how should a policy maker use expert judgments when making policy in domains in which she is not herself an expert? This question is more complex, given the messy and strategic nature of politics. In this paper we argue that the prediction with expert advice (PWEA) framework from machine learning provides helpful tools for addressing these problems. We outline conditions under which we should expert PWEA to be helpful and those under which we should not expect these methods to perform well.",
        "DOI": "10.1007/s11229-022-03809-5",
        "paper_author": "Herrmann D.A.",
        "affiliation_name": "University of California, Irvine",
        "affiliation_city": "Irvine",
        "affiliation_country": "United States",
        "affiliation_id": "60007278",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Development of an operation trajectory design algorithm for control of multiple 0D parameters using deep reinforcement learning in KSTAR",
        "publication": "Nuclear Fusion",
        "citied_by": "22",
        "cover_date": "2022-08-01",
        "Abstract": "This work develops an artificially intelligent (AI) tokamak operation design algorithm that provides an adequate operation trajectory to control multiple plasma parameters simultaneously into different targets. An AI is trained with the reinforcement learning technique in the data-driven tokamak simulator, searching for the best action policy to get a higher reward. By setting the reward function to increase as the achieved β p, q 95, and l i are close to the given target values, the AI tries to properly determine the plasma current and boundary shape to reach the given targets. After training the AI with various targets and conditions in the simulation environment, we demonstrated that we could successfully achieve the target plasma states with the AI-designed operation trajectory in a real KSTAR experiment. The developed algorithm would replace the human task of searching for an operation setting for given objectives, provide clues for developing advanced operation scenarios, and serve as a basis for the autonomous operation of a fusion reactor.",
        "DOI": "10.1088/1741-4326/ac79be",
        "paper_author": "Seo J.",
        "affiliation_name": "Seoul National University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea",
        "affiliation_id": "60013682",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Distributional reinforcement learning with the independent learners for flexible job shop scheduling problem with high variability",
        "publication": "Journal of Computational Design and Engineering",
        "citied_by": "28",
        "cover_date": "2022-08-01",
        "Abstract": "Multi-agent scheduling algorithm is a useful method for the flexible job shop scheduling problem (FJSP). Also, the variability of the target system has to be considered in the scheduling problem that includes the machine failure, the setup change, etc. This study proposes the scheduling method that combines the independent learners with the implicit quantile network by modeling of the FJSP with high variability to the form of the multi-agent. The proposed method demonstrates superior performance compared to the several known heuristic dispatching rules. In addition, the trained model exhibits superior performance compared to the reinforcement learning algorithms such as proximal policy optimization and deep Q-network.",
        "DOI": "10.1093/jcde/qwac044",
        "paper_author": "Oh S.H.",
        "affiliation_name": "Department of Naval Architecture and Ocean Engineering",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea",
        "affiliation_id": "60120081",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Spatio-temporal estimation of wind speed and wind power using extreme learning machines: predictions, uncertainty and technical potential",
        "publication": "Stochastic Environmental Research and Risk Assessment",
        "citied_by": "13",
        "cover_date": "2022-08-01",
        "Abstract": "With wind power providing an increasing amount of electricity worldwide, the quantification of its spatio-temporal variations and the related uncertainty is crucial for energy planners and policy-makers. Here, we propose a methodological framework which (1) uses machine learning to reconstruct a spatio-temporal field of wind speed on a regular grid from spatially irregularly distributed measurements and (2) transforms the wind speed to wind power estimates. Estimates of both model and prediction uncertainties, and of their propagation after transforming wind speed to power, are provided without any assumptions on data distributions. The methodology is applied to study hourly wind power potential on a grid of 250 × 250 m2 for turbines of 100 m hub height in Switzerland, generating the first dataset of its type for the country. We show that the average annual power generation per turbine is 4.4 GWh. Results suggest that around 12,000 wind turbines could be installed on all 19,617 km2 of available area in Switzerland resulting in a maximum technical wind potential of 53 TWh. To achieve the Swiss expansion goals of wind power for 2050, around 1000 turbines would be sufficient, corresponding to only 8% of the maximum estimated potential.",
        "DOI": "10.1007/s00477-022-02219-w",
        "paper_author": "Amato F.",
        "affiliation_name": "École Polytechnique Fédérale de Lausanne",
        "affiliation_city": "Lausanne",
        "affiliation_country": "Switzerland",
        "affiliation_id": "60028186",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Temporal downscaling of precipitation from climate model projections using machine learning",
        "publication": "Stochastic Environmental Research and Risk Assessment",
        "citied_by": "9",
        "cover_date": "2022-08-01",
        "Abstract": "Increased greenhouse gas concentration in the atmosphere has led to significant climate warming and changes in precipitation and temperature characteristics. These trends, which are expected to continue, will affect water infrastructure and raise the need to update associated planning and design policies. The potential effects of climate change can be addressed, in part, by incorporating outputs of climate model projections into statistical assessments to develop the Intensity Duration Frequency (IDF) curves used in engineering design and analysis. The results of climate model projections are available at fixed temporal and spatial resolutions. Model results often need to be downscaled from a coarser to a finer grid spacing (spatial downscaling) and/or from a larger to a smaller time-step (temporal downscaling). Machine Learning (ML) models are among the methods used for spatial and temporal downscaling of climate model outputs. These methods are more frequently used for spatial downscaling; fewer studies explore temporal downscaling. In this study, multiple ML models are evaluated to temporally downscale precipitation time-series (available at 3-h time steps) generated by several regional climate models of the North American Regional Climate Change Assessment Program (NARCCAP) under a high-carbon-emission projection. The temporally downscaled time-series for 2-h, 1-h, 30-min, and 15-min durations are intended for subsequent statistical analysis to generate current- and future-climate IDF curves for Maryland. In this study, the behavior of the ML models is explored by assessing performance in predicting large target response quantities, identifying systematic trends in errors, investigating input/output relationships using response functions, and leveraging conventional performance metrics.",
        "DOI": "10.1007/s00477-022-02259-2",
        "paper_author": "Kajbaf A.A.",
        "affiliation_name": "A. James Clark School of Engineering",
        "affiliation_city": "College Park",
        "affiliation_country": "United States",
        "affiliation_id": "60078684",
        "affiliation_state": "MD"
    },
    {
        "paper_title": "Machine learning in handling disease outbreaks: a comprehensive review",
        "publication": "Bulletin of Electrical Engineering and Informatics",
        "citied_by": "10",
        "cover_date": "2022-08-01",
        "Abstract": "The changes in the global environment have made impact on the evolution of infectious diseases, virus mutations, or new diseases which are challenging to be tackled with new technological advances. This work aims to identify and analyze previous studies on machine learning applications in handling disease outbreaks. Bibliometric analysis was conducted on 3,447 scientific articles selected from the Scopus database. Further, latent dirichlet analysis (LDA) method was applied to identify the topic hotspots in attempting to deepen the analysis. The LDA results identified twelve topic hotspots that can be classified into three themes: COVID-19 disease, miscellaneous diseases, and public opinion on disease outbreaks for discussion. The study reveals that the scientific structure of this domain is dominated by machine learning research on COVID-19 diseases and miscellaneous diseases caused by pathogens or some genetic factors. A huge amount of multimodal medical data was used by previous studies for prediction, forecasting, classification, or screening purposes to resolve many problems of diseases, including epidemiological surveillance, diagnosis, treatment, health monitoring, epidemic management, viral infection, and pathogenesis. Public opinions toward new diseases are also an interesting topic in addition to the public perceptions in response to the health protocol and policies.",
        "DOI": "10.11591/eei.v11i4.3612",
        "paper_author": "Riswantini D.",
        "affiliation_name": "Badan Riset dan Inovasi Nasional",
        "affiliation_city": "Central Jakarta",
        "affiliation_country": "Indonesia",
        "affiliation_id": "60273350",
        "affiliation_state": "Jakarta"
    },
    {
        "paper_title": "Improving field boundary delineation in ResUNets via adversarial deep learning",
        "publication": "International Journal of Applied Earth Observation and Geoinformation",
        "citied_by": "30",
        "cover_date": "2022-08-01",
        "Abstract": "Field boundary data is often required to access digital agricultural services and tools that assist with field-level assessment and monitoring. In addition, policy-makers and researchers need field boundaries to accurately assess food security and impacts on climate change. Thus, scalable and efficient automatic field boundary detection algorithms on satellite images have direct, important implications for many stakeholders. Deep learning is one approach that has been successfully applied in recent years to field boundary detection. Qualitatively however, these boundaries are often broken or malformed, necessitating a dependence on fine-tuned post-processing methods with arbitrary thresholds obtained through trial-and-error. Prior work has explored various architectures for predicting field boundaries, but little has been done beyond traditional supervised learning regimes. Thus, in this work, we propose a new approach to improving field boundary prediction by using an adversarial training framework. In particular, we investigated the effects of training a ResUNet model (a standard fully convolutional network architecture) as a generator in a traditional generative adversarial network (GAN) setup, on 30 meter resolution satellite imagery from 2017 over the state of Illinois. We then explored whether or not our methods can be transferred to label-scarce regions in Brazil. Overall, our results showed that adversarial training substantially improved boundary quality and performance, but had a lesser effect when transferred to unseen, low-data agricultural landscapes. Based on these findings, we conclude that adversarial training is a promising way to improve boundary quality during prediction time, and we suggest several ideas for future improvements that may make adversarial training more viable in transfer learning.",
        "DOI": "10.1016/j.jag.2022.102877",
        "paper_author": "Jong M.",
        "affiliation_name": "Siebel School of Computing and Data Science",
        "affiliation_city": "Urbana",
        "affiliation_country": "United States",
        "affiliation_id": "60282642",
        "affiliation_state": "IL"
    },
    {
        "paper_title": "Variation in the Reporting of Elective Surgeries and Its Influence on Patient Safety Indicators",
        "publication": "Joint Commission Journal on Quality and Patient Safety",
        "citied_by": "2",
        "cover_date": "2022-08-01",
        "Abstract": "Background: US hospital safety is routinely measured via Patient Safety Indicators (PSIs). Receiving a score for most PSIs requires a minimum number of qualifying cases to meet specific criteria; for example, whether an admission was elective. Because admission type is determined by hospitals’ internal policies, the study team suspected that hospitals may be exempted from elective-based PSI scores as a result of their internal admission classification policies. Methods: Multiple regression was combined with machine learning to analyze Medicare inpatient claims data reported by 3,484 hospitals during the 2015–2017 PSI measurement period. The researchers examined the average percentage of elective admissions across surgical diagnosis-related groups (DRGs) (average percent elective [APE]) in relation to hospital characteristics, surgical claims volumes, and numbers and types of surgical DRGs. This study asked whether hospitals with exceptionally low APE shared particular characteristics, reported claims for similar DRGs, or were disproportionately exempted from elective-based PSIs. Results: Cross-validated multiple regression explained 73.9% of variation in APE among hospitals and identified surgical claims volume and 16 surgical DRGs as consistently important variables. However, the exceptionally low APE of 96 hospitals could not be explained by surgical claims volume, surgical DRGs among claims, or hospital characteristics. These outliers were disproportionately exempt from elective-based PSI scores. Conclusion: Some hospitals may have classified admissions in a way that exempted them from elective-based PSI scores. Transparency into admission classification policies is needed to ensure fair and reliable use of PSIs when ranking hospitals and adjusting payments. Alternatively, PSIs may need modifications to rely on externally validated criteria.",
        "DOI": "10.1016/j.jcjq.2022.05.002",
        "paper_author": "Locey K.J.",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A healthy migrant effect? Estimating health outcomes of the undocumented immigrant population in the United States using machine learning",
        "publication": "Social Science and Medicine",
        "citied_by": "13",
        "cover_date": "2022-08-01",
        "Abstract": "This paper investigated whether the commonly observed immigrant health advantage persists among undocumented immigrants in the U.S. and provides nationally representative evidence on the health of this vulnerable population. Data were derived from pooled cross-sections of the National Health Interview Survey (NHIS, 2000–2018). The legal status of foreign-born NHIS respondents is imputed using a non-parametric machine learning model built based on information from the 2004, 2008 and 2014 cohorts of the Survey of Income and Program Participation (SIPP). Multivariate logistic regression analysis indicated that, despite exposure to numerous additional risk factors, the undocumented population experienced a more pronounced Healthy Migrant Effect, with lower odds of reporting fair or poor self-rated health, any physician-diagnosed chronic conditions or being obese. The observed patterns in undocumented health outcomes may be related to the additional challenges and exclusionary policies associated with undocumented migration that could in turn lead to a more pronounced selection of healthy and resilient individuals.",
        "DOI": "10.1016/j.socscimed.2022.115177",
        "paper_author": "Ruhnke S.A.",
        "affiliation_name": "Berliner Institut für Empirische Integrations- und Migrationsforschung/BIM",
        "affiliation_city": "Berlin",
        "affiliation_country": "Germany",
        "affiliation_id": "115172001",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Mapping human perception of urban landscape from street-view images: A deep-learning approach",
        "publication": "International Journal of Applied Earth Observation and Geoinformation",
        "citied_by": "63",
        "cover_date": "2022-08-01",
        "Abstract": "Human perception of urban landscape, which signifies to what extent urban landscape is appreciated by local dwellers, informs human-oriented policies that reinforce public participation. Yet, conventional studies on human perception of urban landscape are largely dependent on individual experience, which may restrict the co-production of knowledge that can be operationalized across spatial scales and sectors. In this study, we mapped human perception of urban landscape in Shanghai by leveraging an advanced deep-learning approach and street-view images. Specifically, the ResNet50 model was employed to map four critical perceptions, i.e., security, depression, vitality, and aesthetic, at parcel level. We further explored the relationship between human perception and land-use types. Our results show that highly urbanized area (Puxi district encompassed by the Inner Ring Road) is perceived as more secure and vital, but more depressing. Besides, human perception varies substantially across different land-use types, among which administrative and service land is favored with regard to all the four perception types. This study advances our understanding of urban landscape through the lens of human perception, and provides nuanced insights into steering human settlement towards sustainability by strategically promoting mixed land use.",
        "DOI": "10.1016/j.jag.2022.102886",
        "paper_author": "Wei J.",
        "affiliation_name": "Zhejiang University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60003970",
        "affiliation_state": "Zhejiang"
    },
    {
        "paper_title": "Artificial Intelligence Applications in Healthcare Sector: Ethical and Legal Challenges",
        "publication": "Emerging Science Journal",
        "citied_by": "27",
        "cover_date": "2022-08-01",
        "Abstract": "Recently, artificial intelligence (AI) has been one of the hottest topics in the technological world. Although it is involved in many domains, it was recently involved in the healthcare sector. AI can be used for diagnostics, drug development, treatment personalization, gene editing, disease prediction, and many more. It helps to improve healthcare services by benefiting medical professionals, hospitals, and patients. Saudi Arabia has a particular interest in the healthcare sector, and it has a clear vision for the future, which points toward the development of AI-based technologies. Few studies investigated the use of AI in Saudi healthcare, and most of them focused on healthcare employees' perceptions. This study is beyond the focus of the existing works. It aims at: 1) presenting the main AI-based healthcare applications; 2) exploring the use of AI in the Saudi healthcare sector; 3) addressing their ethical and legal challenges, along with the policy questions in Saudi healthcare; 4) studying the benefits of these AI-based applications and the acceptance of professionals to use AI in daily practice; 5) introducing the new Personal Data Protection Law (PDPL) in Saudi Arabia; and 6) discussing the importance of AI to the future of Saudi healthcare. To this purpose, a survey was distributed among four main Saudi hospitals. The findings showed that AI should not only lead to better health but also save manpower and simplify the healthcare processes. The respondents agreed that AI helps reflect human intellectual competencies and pushes its limits.",
        "DOI": "10.28991/ESJ-2022-06-04-05",
        "paper_author": "Chikhaoui E.",
        "affiliation_name": "Prince Sultan University",
        "affiliation_city": "Riyadh",
        "affiliation_country": "Saudi Arabia",
        "affiliation_id": "60015723",
        "affiliation_state": "Ar Riyad"
    },
    {
        "paper_title": "A machine learning-driven spatio-temporal vulnerability appraisal based on socio-economic data for COVID-19 impact prevention in the U.S. counties",
        "publication": "Sustainable Cities and Society",
        "citied_by": "14",
        "cover_date": "2022-08-01",
        "Abstract": "A mature and hybrid machine-learning model is verified by mature empirical analysis to measure county-level COVID-19 vulnerability and track the impact of the imposition of pandemic control policies in the U.S. A total of 30 county-level social, economic, and medical variables and a timeline of the imposed policies constitutes a COVID-19 database. A hybrid feature-selection model composed of four machine-learning algorithms is developed to emphasize the regional impact of community features on the case fatality rate (CFR). A COVID-19 vulnerability index (COVULin) is proposed to measure the county's vulnerability, the effects of model's parameters on mortality, and the efficiency of control policies. The results showed that the dense counties in which minority groups represent more than 45% of the population and those with poverty rates greater than 24% were the most vulnerable counties during the first and the last pandemic peaks, respectively. Highly-correlated CFR and COVULin scores indicated a close agreement between the model outcomes and COVID-19 impacts. Counties with higher poverty and uninsured rates were the most resistant to government intervention. It is anticipated that the proposed model can play an essential role in identifying vulnerable communities and help reduce damages during long-term alike disasters.",
        "DOI": "10.1016/j.scs.2022.103990",
        "paper_author": "Moosazadeh M.",
        "affiliation_name": "Kyung Hee University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea",
        "affiliation_id": "60001873",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Spatio-Temporal autocorrelation model for designing a carshare system using historical heterogeneous Data: Policy suggestion",
        "publication": "Transportation Research Part C: Emerging Technologies",
        "citied_by": "8",
        "cover_date": "2022-08-01",
        "Abstract": "As an emerging urban mobility service, carsharing has become increasingly popular worldwide. To understand customers’ needs and optimize the design of service networks, the usage of carsharing vehicles whose trips are recorded by the operators has been applied in research estimating carsharing demand. However, as a form of spatio-temporal correlated data, the underlying spatio-temporal information included in such carsharing records has not been investigated in existing models of carsharing demand. Meanwhile, due to the supply limitation of carsharing stations, some demand cannot be fulfilled and thus remains unrecorded in the operational data. Unrealized demand may lead to underestimation of carsharing demand and therefore an incorrect vehicle deployment strategy by the service providers. In view of these issues, this paper develops an innovative approach to estimating the actual demand at a carsharing station with operational data from GoGet, the largest carsharing company in Australia. The accuracy of the estimation is improved by adding spatio-temporal correlated variables as well as variables from emerging data sources such as social media. To explore the latent space-and-time correlated information, spatio-temporal autoregressive and moving-average models have been applied. Based on the results of the analysis, the paper also provides recommendations related to the operation policies of the service providers.",
        "DOI": "10.1016/j.trc.2022.103758",
        "paper_author": "Cheng Z.",
        "affiliation_name": "UNSW Sydney",
        "affiliation_city": "Sydney",
        "affiliation_country": "Australia",
        "affiliation_id": "60028333",
        "affiliation_state": "NSW"
    },
    {
        "paper_title": "Driving the environmental extra mile – Car sharing and voluntary carbon dioxide offsetting",
        "publication": "Transportation Research Part D: Transport and Environment",
        "citied_by": "8",
        "cover_date": "2022-08-01",
        "Abstract": "Sharing seems a key feature of transforming linear consumption to a more environmentally friendly system. This is especially applicable to car sharing. The aim of this study is to find out which factors influence environmentally friendly behaviour and how strongly. 13,629 journeys of a German car sharing provider specialised in the transport of goods and larger groups of people are evaluated. The focus is on the possibility for customers to offset their carbon footprint by voluntarily making their journeys climate neutral. Considering socio-economic characteristics, a Light Gradient Boosting Machine (LightGBM) model is applied to analyse variables which influence environmentally friendly behaviour. Age, place of residence, mileage driven, and education level have a statistically significant influence in predicting whether a customer will voluntarily offset CO2 or not, in contrast to gender. These findings have societal and political implications which could be used for future policy making.",
        "DOI": "10.1016/j.trd.2022.103361",
        "paper_author": "Haase E.",
        "affiliation_name": "Technische Universität Dresden",
        "affiliation_city": "Dresden",
        "affiliation_country": "Germany",
        "affiliation_id": "60018353",
        "affiliation_state": "Sachsen"
    },
    {
        "paper_title": "LAP: Latency-aware automated pruning with dynamic-based filter selection",
        "publication": "Neural Networks",
        "citied_by": "16",
        "cover_date": "2022-08-01",
        "Abstract": "Model pruning is widely used to compress and accelerate convolutional neural networks (CNNs). Conventional pruning techniques only focus on how to remove more parameters while ensuring model accuracy. This work not only covers the optimization of model accuracy, but also optimizes the model latency during pruning. When there are multiple optimization objectives, the difficulty of algorithm design increases exponentially. So latency sensitivity is proposed to effectively guide the determination of layer sparsity in this paper. We present the latency-aware automated pruning (LAP) framework which leverages the reinforcement learning to automatically determine the layer sparsity. Latency sensitivity is used as a prior knowledge and involved into the exploration loop. Rather than relying on a single reward signal such as validation accuracy or floating-point operations (FLOPs), our agent receives the feedback on the accuracy error and latency sensitivity. We also provide a novel filter selection algorithm to accurately distinguish important filters within a layer based on their dynamic changes. Compared to the state-of-the-art compression policies, our framework demonstrated superior performances for VGGNet, ResNet, and MobileNet on CIFAR-10, ImageNet, and Food-101. Our LAP allowed the inference latency of MobileNet-V1 to achieve approximately 1.64 times speedup on the Titan RTX GPU, with no loss of ImageNet Top-1 accuracy. It significantly improved the pareto optimal curve on the accuracy and latency trade-off.",
        "DOI": "10.1016/j.neunet.2022.05.002",
        "paper_author": "Chen Z.",
        "affiliation_name": "Hunan University",
        "affiliation_city": "Changsha",
        "affiliation_country": "China",
        "affiliation_id": "60032356",
        "affiliation_state": "Hunan"
    },
    {
        "paper_title": "An approach to urban landscape character assessment: Linking urban big data and machine learning",
        "publication": "Sustainable Cities and Society",
        "citied_by": "27",
        "cover_date": "2022-08-01",
        "Abstract": "Diverse urban landscape is an important cultural driving force for urban sustainable development. Although characterizing landscape to protect landscape diversity is widely used in regional landscape and preservation practices, it is difficult to apply to urban landscape character assessment, which needs fine-scale data support, explicit study units, and effective clustering models. Therefore, this study uses urban big geospatial data and machine learning technology to establish a technical system for character assessment of urban landscape applicable to the block scale and complete the landscape assessment of urban areas of Beijing and Shanghai, China. A total of 64 landscape character types were identified in Beijing, and 61 in Shanghai. We find that (1) urban landscape characters are different with the ring road as the boundary, but each zone presents a combination of different proportions of landscape characters. (2) Beijing's city wall demolition policy is affected by historical protection policy. Landscape differentiation on both sides of the Huangpu River in Shanghai has yet to be realized. This study extends the theory of LCA and realizes the research exploration of urban built environment. It can also be used to guide urban zoning control, evaluate planning policy, and provide assistance in practice for sustainable urban development and management.",
        "DOI": "10.1016/j.scs.2022.103983",
        "paper_author": "LU Y.",
        "affiliation_name": "College of Computer Science and Technology, Zhejiang University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60117751",
        "affiliation_state": "Zhejiang"
    },
    {
        "paper_title": "Application of Machine Learning Methods for Asset Management on Power Distribution Networks",
        "publication": "Emerging Science Journal",
        "citied_by": "13",
        "cover_date": "2022-08-01",
        "Abstract": "This study aims to study the different kinds of Machine Learning (ML) models and their working principles for asset management in power networks. Also, it investigates the challenges behind asset management and its maintenance activities. In this review article, Machine Learning (ML) models are analyzed to improve the lifespan of the electrical components based on the maintenance management and assessment planning policies. The articles are categorized according to their purpose: 1) classification, 2) machine learning, and 3) artificial intelligence mechanisms. Moreover, the importance of using ML models for proper decision making based on the asset management plan is illustrated in a detailed manner. In addition to this, a comparative analysis between the ML models is performed, identifying the advantages and disadvantages of these techniques. Then, the challenges and managing operations of the asset management strategies are discussed based on the technical and economic factors. The proper functioning, maintenance and controlling operations of the electric components are key challenging and demanding tasks in the power distribution systems. Typically, asset management plays an essential role in determining the quality and profitability of the elements in the power network. Based on this investigation, the most suitable and optimal machine learning technique can be identified and used for future work.",
        "DOI": "10.28991/ESJ-2022-06-04-017",
        "paper_author": "Rajora G.L.",
        "affiliation_name": "Universidad Pontificia Comillas, Escuela Técnica Superior de Ingeniería, Instituto de Investigación Tecnológica",
        "affiliation_city": "Madrid",
        "affiliation_country": "Spain",
        "affiliation_id": "60110597",
        "affiliation_state": "Madrid"
    },
    {
        "paper_title": "Examining spatial disparities in electric vehicle charging station placements using machine learning",
        "publication": "Sustainable Cities and Society",
        "citied_by": "49",
        "cover_date": "2022-08-01",
        "Abstract": "Electric vehicles (EV) are an emerging mode of transportation that has the potential to reshape the transportation sector by significantly reducing carbon emissions thereby promoting a cleaner environment and pushing the boundaries of climate progress. Nevertheless, there remain significant hurdles to the widespread adoption of electric vehicles in the United States ranging from the high cost of EVs to the inequitable placement of EV charging stations (EVCS). A deeper understanding of the underlying complex interactions of social, economic, and demographic factors which may lead to such emerging disparities in EVCS placements is, therefore, necessary to mitigate accessibility issues and improve EV usage among people of all ages and abilities. In this study, we develop a machine learning framework to examine spatial disparities in EVCS placements by using a predictive approach. We first identify the essential socioeconomic factors that may contribute to spatial disparities in EVCS access. Second, using these factors along with ground truth data from existing EVCS placements we predict future ECVS density at multiple spatial scales using machine learning algorithms and compare their predictive accuracy to identify the most optimal spatial resolution for our predictions. Finally, we compare the most accurately predicted EVCS placement density with a spatial inequity indicator to quantify how equitably these placements would be for Orange County, California. Our method achieved the highest predictive accuracy (94.9%) of EVCS placement density at a spatial resolution of 3 km using Random Forests. Our results indicate that a total of 11.04% of predicted EVCS placements in Orange County will lie within a high spatial inequity zone – indicating populations with the lowest accessibility may require the greater investments in EVCS placements. 69.52% of the study area experience moderate accessibility issues and the remaining 19.11% face least accessibility issues w.r.t EV charging stations. Within least accessible areas, 7.8% of the area will require low density of predicted EVCS placements, 3.4% will require a medium density of predicted EVCS placements, and 0.55% will require high density of EVCS placements. The moderately accessible areas would require the highest placements of EVCS but mostly with low density placements covering 54.42% of the area. The findings from this study highlight a generalizable framework to quantify inequities in EVCS placements that will enable policymakers to identify underserved communities and facilitate targeted infrastructure investments for widespread EV usage and adoption for all. The findings from this study highlight a generalizable framework to quantify inequities in EVCS placements that will enable policymakers to identify underserved communities and facilitate targeted infrastructure investments for widespread EV usage and adoption for all.",
        "DOI": "10.1016/j.scs.2022.103978",
        "paper_author": "Roy A.",
        "affiliation_name": "University of California, Irvine",
        "affiliation_city": "Irvine",
        "affiliation_country": "United States",
        "affiliation_id": "60007278",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Development and External Validation of a Machine Learning Model for Progression of CKD",
        "publication": "Kidney International Reports",
        "citied_by": "38",
        "cover_date": "2022-08-01",
        "Abstract": "Introduction: Prediction of disease progression at all stages of chronic kidney disease (CKD) may help improve patient outcomes. As such, we aimed to develop and externally validate a random forest model to predict progression of CKD using demographics and laboratory data. Methods: The model was developed in a population-based cohort from Manitoba, Canada, between April 1, 2006, and December 31, 2016, with external validation in Alberta, Canada. A total of 77,196 individuals with an estimated glomerular filtration rate (eGFR) > 10 ml/min per 1.73 m2 and a urine albumin-to-creatinine ratio (ACR) available were included from Manitoba and 107,097 from Alberta. We considered >80 laboratory features, including analytes from complete blood cell counts, chemistry panels, liver enzymes, urine analysis, and quantification of urine albumin and protein. The primary outcome in our study was a 40% decline in eGFR or kidney failure. We assessed model discrimination using the area under the receiver operating characteristic curve (AUC) and calibration using plots of observed and predicted risks. Results: The final model achieved an AUC of 0.88 (95% CI 0.87–0.89) at 2 years and 0.84 (0.83–0.85) at 5 years in internal testing. Discrimination and calibration were preserved in the external validation data set with AUC scores of 0.87 (0.86–0.88) at 2 years and 0.84 (0.84–0.86) at 5 years. The top 30% of individuals predicted as high risk and intermediate risk represent 87% of CKD progression events in 2 years and 77% of progression events in 5 years. Conclusion: A machine learning model that leverages routinely collected laboratory data can predict eGFR decline or kidney failure with accuracy.",
        "DOI": "10.1016/j.ekir.2022.05.004",
        "paper_author": "Ferguson T.",
        "affiliation_name": "Max Rady College of Medicine, University of Manitoba",
        "affiliation_city": "Winnipeg",
        "affiliation_country": "Canada",
        "affiliation_id": "60010853",
        "affiliation_state": "MB"
    },
    {
        "paper_title": "Mass appraisal as affordable public policy: Open data and machine learning for mapping urban land values",
        "publication": "Land Use Policy",
        "citied_by": "15",
        "cover_date": "2022-08-01",
        "Abstract": "Updated cadastral land values are a matter of critical importance for local governments: higher revenue of property taxes, more equitable treatment to taxpayers, a fundamental input in the design of public policies related to access to land and housing for the most vulnerable and a key feature in land value capture strategies to finance public infrastructure, to name just a few public policies that require correct valuations of land. However, in Latin America, outdated cadastral values are common to most cities. The reasons for this can be found in the complexity of the mass appraisal process, lack of institutional and fiscal capacity to undertake it and bureaucratic resistance to its implementation. The objective of this paper is to present a mass appraisal methodology that uses only free and open data to achieve robust urban land valuations. Information from the OpenStreetMap Project is used to generate several land variables. In addition, the Global Human Settlement Layer of the European Commission is used to determine the level of consolidation of urban sprawl. Land value data were obtained from the Mapa de Valores de América Latina, a collaborative initiative that systemizes more than 68,000 data from more than 900 cities. This information is used to train three tree-based machine learning models: Random Forest, Quantile Random Forest and Gradient Boosting Model. The results support the viability of the proposed strategy, simplifying the mass appraisal process in terms of costs, time and complexity of the information used.",
        "DOI": "10.1016/j.landusepol.2022.106211",
        "paper_author": "Carranza J.P.",
        "affiliation_name": "Universidad Nacional de Córdoba",
        "affiliation_city": "Cordoba",
        "affiliation_country": "Argentina",
        "affiliation_id": "60000658",
        "affiliation_state": "Cordoba"
    },
    {
        "paper_title": "Identifying key risk factors for premature discontinuation of opioid use disorder treatment in the United States: A predictive modeling study",
        "publication": "Drug and Alcohol Dependence",
        "citied_by": "10",
        "cover_date": "2022-08-01",
        "Abstract": "Background: Treatment for opioid use disorder (OUD), particularly medication for OUD, is highly effective; however, retention in OUD treatment is a significant challenge. We aimed to identify key risk factors for premature exit from OUD treatment. Methods: We analyzed 2,381,902 cross-sectional treatment episodes for individuals in the U.S., discharged between Jan/1/2015 and Dec/31/2019. We developed classification models (Random Forest, Classification and Regression Trees (CART), Bagged CART, and Boosted CART), and analyzed 31 potential risk factors for premature treatment exit, including treatment characteristics, substance use history, socioeconomic status, and demographic characteristics. We stratified our analysis based on length of stay in treatment and service setting. Models were compared using cross-validation and the receiver operating characteristic area under the curve (ROC-AUC). Results: Random Forest outperformed other methods (ROC-AUC: 74%). The most influential risk factors included characteristics of service setting, geographic region, primary source of payment, and referral source. Race, ethnicity, and sex had far weaker predictive impacts. When stratified by treatment setting and length of stay, employment status and delay (days waited) to enter treatment were among the most influential factors. Their importance increased as treatment duration decreased. Notably, importance of referral source increased as the treatment duration increased. Finally, age and age of first use were important factors for lengths of stay of 2–7 days and in detox treatment settings. Conclusions: The key factors of OUD treatment attrition identified in this analysis should be more closely explored (e.g., in causal studies) to inform targeted policies and interventions to improve models of care.",
        "DOI": "10.1016/j.drugalcdep.2022.109507",
        "paper_author": "Stafford C.",
        "affiliation_name": "Harvard Medical School",
        "affiliation_city": "Boston",
        "affiliation_country": "United States",
        "affiliation_id": "60002746",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Automated floorplan generation in architectural design: A review of methods and applications",
        "publication": "Automation in Construction",
        "citied_by": "47",
        "cover_date": "2022-08-01",
        "Abstract": "Accommodating predicted population growth and urbanization within the UN Climate Goals poses a significant challenge for disciplines that engage with the built environment. High performing buildings of the future should offer spatial quality for their users while utilizing resources as efficiently as possible for both construction and operation. In this review, we survey the value proposition of automatic floorplan layout generation methods and their opportunities for design guidance, feedback, and optimization in the creation of new buildings, in addition to applications for inventory characterization to survey existing housing stock and guide building policy and code. We divide existing methods into three categories: bottom-up methods, top-down methods, and referential methods. We explore advantages and challenges for each approach and propose a hybrid method for future building layout automation that utilizes a new set of metrics to create sustainable buildings of the future.",
        "DOI": "10.1016/j.autcon.2022.104385",
        "paper_author": "Weber R.E.",
        "affiliation_name": "Massachusetts Institute of Technology",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States",
        "affiliation_id": "60022195",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "The roles of learning mechanisms in services: Evidence from US residential solar installations",
        "publication": "Energy Policy",
        "citied_by": "5",
        "cover_date": "2022-08-01",
        "Abstract": "Non-hardware costs are majority of the cost of producing solar photovoltaic (PV) electricity. We use matched data on patents and over 125,000 residential PV installations to estimate the effects of three learning mechanisms in reducing PV costs: learning by doing, searching, and interacting. While previous work in this area has focused predominantly on learning by doing, we find that learning by searching and interacting are also significant mechanisms to facilitate non-hardware cost reductions. Including these two mechanisms reduces the effect of learning by doing in explaining non-hardware cost reductions by 43%. Our results suggest that prior work may overemphasize the role of learning by doing and the policies that help generate learning by doing. Analysis of the supplier-network between installers and their suppliers shows that concentrated supplier networks are associated with lower non-hardware costs, although there are key differences between installer-panel and installer-inverter manufacturer networks. An important implication is that policies for reducing non-hardware costs need to take a more complete view of how different learning mechanisms engender cost reductions. They should particularly consider the important role of learning in supplier networks in cost reductions—an effect that until now has largely been missing in analyses of solar non-hardware costs.",
        "DOI": "10.1016/j.enpol.2022.113003",
        "paper_author": "Gao X.",
        "affiliation_name": "University of Miami",
        "affiliation_city": "Coral Gables",
        "affiliation_country": "United States",
        "affiliation_id": "60029251",
        "affiliation_state": "FL"
    },
    {
        "paper_title": "The role of ‘big data’ and ‘in silico’ New Approach Methodologies (NAMs) in ending animal use – A commentary on progress",
        "publication": "Computational Toxicology",
        "citied_by": "12",
        "cover_date": "2022-08-01",
        "Abstract": "In silico (computational) methods continue to evolve as part of a robust 21st century public health strategy in risk assessment, relevant to all sectors of chemical safety including preclinical drug discovery, industrial chemicals testing, food and cosmetics. Alongside in vitro methods as components of intelligent testing and pathway driven strategies, in silico models provide the potential for more human relevant solutions to the use of animals in safety testing and biomedical research. These are often termed ‘New Approach Methodologies’ (NAMs). Some NAMs incorporate the use of ‘big data’, for example the information provided from high throughput or high content in vitro screening assays or ‘omics’ technologies. Big data has increasing relevance to predictive toxicology but must be appropriately defined, particularly with regard to ‘quality vs quantity’. The purpose of this article is to provide a commentary on the progress of in silico human-based research methods within the context of NAMs, as well as discussion of the emerging use of big data with relevance to safety assessment. The current status of in silico methods is discussed, with input from researchers in the field. Scientific and legislative drivers for change are also considered, along with next steps to address challenges in funding and recognition, to achieve regulatory acceptance and uptake within the research community. To provide some wider context, the use of in silico methods alongside other relevant approaches (e.g., human-based in vitro) is also discussed.",
        "DOI": "10.1016/j.comtox.2022.100232",
        "paper_author": "Ram R.N.",
        "affiliation_name": "Safer Medicines Trust",
        "affiliation_city": "Kingsbridge",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60134864",
        "affiliation_state": "Devon"
    },
    {
        "paper_title": "Machine learning (ML)-centric resource management in cloud computing: A review and future directions",
        "publication": "Journal of Network and Computer Applications",
        "citied_by": "81",
        "cover_date": "2022-08-01",
        "Abstract": "Cloud computing has rapidly emerged as a model for delivering Internet-based utility computing services. Infrastructure as a Service (IaaS) is one of the most important and rapidly growing models in cloud computing. Scalability, quality of service, optimum utility, decreased overheads, higher throughput, reduced latency, specialised environment, cost-effectiveness, and a streamlined interface are some of the essential elements of cloud computing for IaaS. Traditionally, resource management has been done through static policies, which impose certain limitations in various dynamic scenarios, prompting cloud service providers to adopt data-driven, machine-learning-based approaches. Machine learning is being used to handle various resource management tasks, including workload estimation, task scheduling, VM consolidation, resource optimisation, and energy optimisation, among others. This paper provides a detailed review of machine learning-based resource management solutions. We begin by introducing background concepts of cloud computing like service models, deployment models, and machine learning use in cloud computing. Then we look at resource management challenges in cloud computing, categorise them based on various aspects of resource management types such as workload prediction, VM consolidation, resource provisioning, VM placement and thermal management, review current techniques for addressing these challenges, and evaluate their key benefits and drawbacks. Finally, we propose prospective future research directions based on observed resource management challenges and shortcomings in current approaches for solving these challenges.",
        "DOI": "10.1016/j.jnca.2022.103405",
        "paper_author": "Khan T.",
        "affiliation_name": "University of Electronic Science and Technology of China",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China",
        "affiliation_id": "60005465",
        "affiliation_state": "Sichuan"
    },
    {
        "paper_title": "Individual and program Characteristics May Drive Variability in Outcomes After Caregivers Participate in a Tailored Support Intervention",
        "publication": "Journal of Applied Gerontology",
        "citied_by": "0",
        "cover_date": "2022-08-01",
        "Abstract": "Critically needed programs designed to support family caregivers have shown inconsistent reductions in stress and burden. To explore drivers of improvement in caregiver outcomes after participation in a support intervention we analyzed data from a one-on-one, tailored problem-solving intervention targeting caregiver wellbeing (2015–2019, n = 503). We explored data patterns across 21 individual, household, and program-level variables using elastic net regression to identify drivers of improvements, and their relative importance. Baseline subjective burden, baseline depressive symptom scores, baseline caregiver problem solving, African American race, and site and coach fixed effects were the most consistent drivers of changes across the explored caregiver outcomes. Caregiver and program characteristics may be promising avenues to target to decrease distress and burden during intervention design. Interventions focusing on highly distressed caregivers may lead to greater improvements. More research is needed to identify how site or interventionists characteristics drive positive intervention effects.",
        "DOI": "10.1177/07334648221091564",
        "paper_author": "Shepherd-Banigan M.",
        "affiliation_name": "Durham VA Health Care System",
        "affiliation_city": "Durham",
        "affiliation_country": "United States",
        "affiliation_id": "60138754",
        "affiliation_state": "NC"
    },
    {
        "paper_title": "Social sustainability in the age of digitalization: A systematic literature Review on the social implications of industry 4.0",
        "publication": "Technology in Society",
        "citied_by": "118",
        "cover_date": "2022-08-01",
        "Abstract": "The fourth industrial revolution has turned into a reality during the past few years, and, as a result, the related literature has grown at an unprecedented rate, offering valuable insights into the possible impacts of Industry 4.0 at various analysis levels. Investigating the economic effects of Industry 4.0, especially at the corporate level, has been a cutting-edge research topic across various disciplines. Similarly, several studies have addressed the opportunities that Industry 4.0 might offer to environmental sustainability. On the contrary, the social sustainability implications of Industry 4.0 are less explored in the literature. Unlike the overoptimism around the economic benefits, academia remains quite inconsistent while interpreting the social aspects linked to Industry 4.0. Trying to shed some light on this issue, this research conducts a state-of-the-art systematic review of academic papers and a Machine Learning-based analysis of grey literature on the social implications of Industry 4.0. Contributing to this very relevant and fresh topic, the study summarizes the ongoing trends on social sustainability consequences of Industry 4.0, highlights the existing gaps, and proposes exciting avenues for future research.",
        "DOI": "10.1016/j.techsoc.2022.101997",
        "paper_author": "Grybauskas A.",
        "affiliation_name": "Kaunas University of Technology",
        "affiliation_city": "Kaunas",
        "affiliation_country": "Lithuania",
        "affiliation_id": "60042282",
        "affiliation_state": "Kaunas"
    },
    {
        "paper_title": "TD-RA policy-enforcement framework for an SDN-based IoT architecture",
        "publication": "Journal of Network and Computer Applications",
        "citied_by": "12",
        "cover_date": "2022-08-01",
        "Abstract": "Internet of Things (IoT) has been suffering from increasing security threats since many years which compromise the whole network security. Automating the management of IoT devices helps in implementing security measures within communication systems. Software Defined Networking (SDN) has been introduced as a new networking approach that enables this automation. Many approaches were developed to mitigate IoT attacks in SDN-based IoT networks. Some studies investigated the prevention of flooding attacks, while others tried to cover broader attack surfaces. However, their proposed methods are time consuming and resource-exhausting as they use complex algorithms. In this paper, we propose a lightweight secure Threat Detection (TD) and Rule Automation (RA) framework namely “TD-RA” to effectively detect and mitigate different cyber-security threats in an SDN-based IoT environment. The proposed solution is composed of a Binary and Multi-class Classification Modules (BCM/MCM) for IoT threat detection and a Policy-Enforcement Module (PEM) for attack mitigation. Different machine learning methods have been implemented and compared to solve the classification problems. It is shown that for binary classification, the Decision Tree method achieves the highest accuracy which is around 98.7%, while for multi-class classification, Random Forest achieves the highest accuracy which is around 91.1%. The experimental results show that the proposed framework can successfully detect abnormal traffic and prevent IoT threats through SDN with smaller network overhead and high performance. Moreover, the overall processing time of our security modules is significantly smaller than that of existing solutions by reaching a mean value of 6 ms. This paper also introduces a large-scale architecture that comprises clusters of controllers to maintain high availability of network services. Such an integrated security approach, including detection and mitigation techniques, provides IT industries with reliable security measures that can be implemented to increase SDN-based IoT system responsiveness to different IoT attacks.",
        "DOI": "10.1016/j.jnca.2022.103390",
        "paper_author": "Lahlou S.",
        "affiliation_name": "International University of Rabat",
        "affiliation_city": "Sale",
        "affiliation_country": "Morocco",
        "affiliation_id": "60111570",
        "affiliation_state": "Rabat-Sale-Kenitra"
    },
    {
        "paper_title": "Data misrepresentation detection for insurance underwriting fraud prevention",
        "publication": "Decision Support Systems",
        "citied_by": "9",
        "cover_date": "2022-08-01",
        "Abstract": "Premium fraud concerns data misrepresentation committed by an insurance customer with the intent to benefit from an unduly low premium at the underwriting of a policy. In this paper, we propose a novel approach for evaluating the risk of underwriting premium fraud at the time of application in the presence of potentially misrepresented self-reported information. The aim of the approach is to support insurance companies in identifying fraudulent applications and their decisions to underwrite insurance contract propositions. Likewise, it can be use to make straight-through processing (i.e. automated) underwriting systems more fraudproof, by e.g., triggering a validation on applications prone to misrepresentations. Our approach is based on conditional density estimates for a set of validated contracts. The proposed approach does not require historical fraud labels and can adapt to changes in pricing policy. Moreover, the approach can be used to detect outliers in addition to predicting underwriting fraud and is extended to multivariate self-reported data. We further demonstrate a link between Shapley values in common conditional expectation problems and conditional density estimations to make our approach explainable. We report a case study involving motor insurance underwriting, in which a driver's identity and driving record can be misrepresented to benefit from an unduly low premium; the results indicate the effectiveness of the proposed approach for detecting and preventing underwriting fraud.",
        "DOI": "10.1016/j.dss.2022.113798",
        "paper_author": "Vandervorst F.",
        "affiliation_name": "Data Office",
        "affiliation_city": "Brussels",
        "affiliation_country": "Belgium",
        "affiliation_id": "127066118",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Data-driven online interactive bidding strategy for demand response",
        "publication": "Applied Energy",
        "citied_by": "8",
        "cover_date": "2022-08-01",
        "Abstract": "Demand response (DR), as one of the important energy resources in the future's grid, provides the services of peak shaving, enhancing the efficiency of renewable energy utilization with a short response period, and low cost. Various categories of DR are established, e.g. automated DR, incentive DR, emergency DR, and demand bidding. However, with the practical issue of the unawareness of residential and commercial consumers’ utility models, the researches about demand bidding aggregator involved in the electricity market are just at the beginning stage. For this issue, the bidding price and bidding quantity are two required decision variables while considering the uncertainties due to the market and participants. In this paper, we determine the bidding and purchasing strategy simultaneously employing the smart meter data and functions. A two-agent deep deterministic policy gradient method is developed to optimize the decisions through learning historical bidding experiences. The online learning further utilizes the daily newest bidding experience attained to ensure trend tracing and self-adaptation. Two environment simulators are adopted for testifying the robustness of the model. The results prove that when facing diverse situations the proposed model can earn the optimal profit via off/online learning the bidding rules and robustly making the proper bid.",
        "DOI": "10.1016/j.apenergy.2022.119082",
        "paper_author": "Lee K.C.",
        "affiliation_name": "National Cheng Kung University",
        "affiliation_city": "Tainan",
        "affiliation_country": "Taiwan",
        "affiliation_id": "60014982",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Perspective view of autonomous control in unknown environment: Dual control for exploitation and exploration vs reinforcement learning",
        "publication": "Neurocomputing",
        "citied_by": "12",
        "cover_date": "2022-08-01",
        "Abstract": "This paper overviews and discusses the relationship between Reinforcement Learning (RL) and the recently developed Dual Control for Exploitation and Exploration (DCEE). It is argued that there are two related but quite distinctive approaches, namely, control and machine learning, in tackling intractability arising in optimal decision making/control problems. In the control approach, the original problems (of an infinite horizon) are approximated by finite horizon problems and solved online by taking advantage of the availability of computing power. In the machine learning approach, the optimal solutions are approximated through iterations, or (offline) training through trials when models are not available. When dealing with unknown environments, DCEE as a technique developed from the control approach could potentially solve similar problems as RL while offering a number of advantages, most notably, coping with uncertainty in environment/tasks, high efficiency in learning through balancing exploitation and exploration, and potential in establishing its formal properties like stability. The links between DCEE and other relevant methods like dual control, Model Predictive Control and particularly Active Inference in neuroscience are discussed. The latter provides a strong biological endorsement for DCEE. The methods and discussions are illustrated by autonomous source search using a robot. It is concluded that DCEE provides a promising, complementary approach to RL, and more research is required to develop it as a generic theory and fully realise its potential. The relationships revealed in this paper provide insights into these relevant methods and facilitate cross fertilisation between control, machine learning and neuroscience for developing autonomous control under uncertain environments.",
        "DOI": "10.1016/j.neucom.2022.04.131",
        "paper_author": "Chen W.H.",
        "affiliation_name": "Loughborough University",
        "affiliation_city": "Loughborough",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60000891",
        "affiliation_state": "Leicestershire"
    },
    {
        "paper_title": "APPL: Adaptive Planner Parameter Learning",
        "publication": "Robotics and Autonomous Systems",
        "citied_by": "28",
        "cover_date": "2022-08-01",
        "Abstract": "While current autonomous navigation systems allow robots to successfully drive themselves from one point to another in specific environments, they typically require extensive manual parameter re-tuning by human robotics experts in order to function in new environments. Furthermore, even for just one complex environment, a single set of fine-tuned parameters may not work well in different regions of that environment. These problems prohibit reliable mobile robot deployment by non-expert users. As a remedy, we propose Adaptive Planner Parameter Learning (APPL), a machine learning framework that can leverage non-expert human interaction via several modalities – including teleoperated demonstrations, corrective interventions, and evaluative feedback – and also unsupervised reinforcement learning to learn a parameter policy that can dynamically adjust the parameters of classical navigation systems in response to changes in the environment. APPL inherits safety and explainability from classical navigation systems while also enjoying the benefits of machine learning, i.e., the ability to adapt and improve from experience. We present a suite of individual APPL methods and also a unifying cycle-of-learning scheme that combines all the proposed methods in a framework that can improve navigation performance through continual, iterative human interaction and simulation training.",
        "DOI": "10.1016/j.robot.2022.104132",
        "paper_author": "Xiao X.",
        "affiliation_name": "Department of Computer Science",
        "affiliation_city": "Austin",
        "affiliation_country": "United States",
        "affiliation_id": "60150459",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "Riding the wave: Predicting the use of the bike-sharing system in Barcelona before and during COVID-19",
        "publication": "Sustainable Cities and Society",
        "citied_by": "30",
        "cover_date": "2022-08-01",
        "Abstract": "To simultaneously promote health, economic, and environmental sustainability, a number of cities worldwide have established bike-sharing systems (BSS) that complement the conventional public transport systems. As the rapid spread of COVID-19 becoming a global pandemic disrupted urban mobility due to government-imposed lockdowns and the heightened fear of infection in crowded spaces, populations were increasingly less likely to use public transportation and instead shifted toward alternative transport systems, including BSS. In this study, we use probabilistic machine learning in a quasi-experimental research design to identify how the relevance of a comprehensive set of factors to predict the use of Bicing (the BSS in Barcelona) may have changed as COVID-19 unfolded. We unpack the key factors in predicting the use of Bicing, uncovering evidence of increasing bike-related built infrastructure (e.g., tactical urbanism), trip distance, and the income levels of neighborhoods as the most relevant predictors. Moreover, we find that the relevance of the factors in predicting Bicing usage has generally decreased during the global pandemic, suggesting altered societal behavior. Our study enhances the understanding of BSS and societal behavior, which can have important implications for developing resilient programs for cities to adopt sustainable practices through transport policy, infrastructure planning, and urban development.",
        "DOI": "10.1016/j.scs.2022.103929",
        "paper_author": "Bustamante X.",
        "affiliation_name": "Universitat Politècnica de Catalunya",
        "affiliation_city": "Barcelona",
        "affiliation_country": "Spain",
        "affiliation_id": "60007592",
        "affiliation_state": "Barcelona"
    },
    {
        "paper_title": "OpenComm: Open community platform for data integration and privacy preserving for 311 calls",
        "publication": "Sustainable Cities and Society",
        "citied_by": "6",
        "cover_date": "2022-08-01",
        "Abstract": "Local governments are increasingly leveraging administrative data to drive performance. Likewise, cities are interested in improving responsiveness to citizens’ demands and cost savings through data analytics. However, city managers face many challenges when utilizing secondary data, such as 311 call records and the US Census. The challenge of interest to the current study is boundary issues as a result of data being collected at divergent geographic levels over different time horizons. Accordingly, an inductive analytical methodology was developed to create units of analysis that were both pragmatically and analytically appropriate for city managers and local policymakers. We created an open data analytics framework called OpenComm to harmonize administrative and secondary data using administrative data derived from Kansas City, Missouri. This framework produced robust inferences regarding the spatial and temporal aspects for the communities. Privacy-preserving technology, in particular, has been applied to public data to protect community privacy. The findings illustrate the power of inductive data aggregation, leading to empirical insights into hidden patterns of city service disparity over a decade-long time horizon. An application for the Open Data Platform is available at http://kc311.herokuapp.com/.",
        "DOI": "10.1016/j.scs.2022.103858",
        "paper_author": "Ho D.H.",
        "affiliation_name": "University of Missouri-Kansas City",
        "affiliation_city": "Kansas City",
        "affiliation_country": "United States",
        "affiliation_id": "60007056",
        "affiliation_state": "MO"
    },
    {
        "paper_title": "Integrated cloud computing and cost effective modelling to delineate the ecological corridors for Spectacled bears (Tremarctos ornatus) in the rural territories of the Peruvian Amazon",
        "publication": "Global Ecology and Conservation",
        "citied_by": "9",
        "cover_date": "2022-08-01",
        "Abstract": "Spectacled bears (SB) (Tremarctos ornatus) are the only bear species native to South America. This particular bear is the single species of its genus, and it is listed as vulnerable according to the IUCN red list. A critical SB conservation habitat is in the rural territories of the Peruvian Amazon, where anthropogenic land-use changes and landscape fragmentation threaten SB habitats. The following questions arise in this context: How much has land-use changed? How to design the establishment of ecological corridors (ECs) to support the conservation of SB?. We investigated the temporal land use and land cover changes for last 30 years (1990–2020) for a better projection of the ECs and to quantify the temporal landscape metrics. Furthermore, we integrated cloud computing, machine learning models with cost-effective techniques to delineate the ECs for SB within the rural territories. Ensemble Random Forest model associated with Google Earth Engine (GEE) was used to develop four land use and land cover (LULC) maps (for the years 1990, 2000, 2010 and 2020). The least cost path (LCP) model based on Dijkstra's shortest path algorithm was assembled based on six variables (altitude; slope; distance to roads; distance to population centers; land use map; inventory map of SB). Then, we calculated the ECs based on the multidirectional origin-destination points, we found that forest patches increased by 57% between 1990 and 2020. Results showed statistically significant agreement (R2 = 0.47; p < 0.05) between cost/ha* and percentage of forest cover. We observed that the higher the forest cover, the better the connectivity and the lower the cost of mobilization in the ECs. Our study outcomes validated through the images obtained from trap cameras that confirms that delineated routs for SB movements. The proposed model can be adopted for other parts of the global forest including other species of interest. To formulate a sustainable conservation action plan, we provided five recommendations that will support conservation practices, design cost-effective ECs for policy makers.",
        "DOI": "10.1016/j.gecco.2022.e02126",
        "paper_author": "Cotrina Sánchez A.",
        "affiliation_name": "Universidad Nacional Toribio Rodríguez de Mendoza de Amazonas",
        "affiliation_city": "Chachapoyas",
        "affiliation_country": "Peru",
        "affiliation_id": "60105265",
        "affiliation_state": "Amazonas"
    },
    {
        "paper_title": "Enhancing hybrid renewable energy performance through deep Q-learning networks improved by fuzzy reward control",
        "publication": "International Journal of Electrical and Computer Engineering",
        "citied_by": "3",
        "cover_date": "2022-08-01",
        "Abstract": "In a stand-alone system, the use of renewable energies, load changes, and interruptions to transmission lines can cause voltage drops, impacting its reliability. A way to offset a change in the nature of hybrid renewable energy immediately is to utilize energy storage without needing to turn on other plants. Photovoltaic panels, a wind turbine, and a wallbox unit (responsible for providing the vehicle’s electrical need) are the components of the proposed system; in addition to being a power source, batteries also serve as a storage unit. Taking advantage of deep learning, particularly convolutional neural networks, and this new system will take advantage of recent advances in machine learning. By employing algorithms for deep Q-learning, the agent learns from the data of the various elements of the system to create the optimal policy for enhancing performance. To increase the learning efficiency, the reward function is implemented using a fuzzy Mamdani system. Our proposed experimental results shows that the new system with fuzzy reward using deep Q-learning networks (DQN) keeps the battery and the wallbox unit optimally charged and less discharged. Moreover confirms the economic advantages of the proposed approach performs better approximate to +25% Moreover, it has dynamic response capabilities and is more efficient over the existing optimization approach using deep learning without fuzzy logic.",
        "DOI": "10.11591/ijece.v12i4.pp4302-4314",
        "paper_author": "Ameur C.",
        "affiliation_name": "Faculté des Sciences Dhar El Mahraz, Université Sidi Mohamed Ben Abdellah",
        "affiliation_city": "Fez",
        "affiliation_country": "Morocco",
        "affiliation_id": "60012964",
        "affiliation_state": "Fes-Meknes"
    },
    {
        "paper_title": "Assessment of energy market's progress towards achieving Sustainable Development Goal 7: A clustering approach",
        "publication": "Sustainable Energy Technologies and Assessments",
        "citied_by": "9",
        "cover_date": "2022-08-01",
        "Abstract": "The Sustainable Development agenda of 2030, which was adopted by all the member states of the United Nations provides a strategic blue print for peace and prosperity for all people on the planet. This blueprint provides 17 Sustainable Development Goals (SDGs), which call to action all countries regardless of economic status. The main interest of this paper is the goal on affordable and clean energy (SDG7). This requires developing an assessment model to evaluate economies on their progress towards achieving SGD7. Performing an effective assessment of energy markets in both developing and developed economies is cumbersome and challenging. The lack of a universal metric for assessing the health of energy markets further compounds this challenge. This study formulates an empirical structure to evaluate the advancements towards achieving SGD7. Using an unsupervised learning approach(ordinal K-Means clustering) we form eight health level statuses. In this study, health level status is the degree of closeness of an energy market to achieving Sustainable Development Goal 7. This approach is used to track the changes in the energy markets under three weight priority assignments; equal, access and quality. The data used is from the World Bank indicators between 1990 and 2019 provided by the World Bank. The findings illustrates how machine learning enables a holistic approach when assessing the health of energy markets by combining multiple indicators into a single score. Furthermore, the machine learning approach confirms the poor performance of energy markets in Sub-Saharan Africa in accessibility, and the lack of significant strides in achieving sustainability in some of the developed economies like the United States and China. This study highlights the dangers of a myopic approach to energy markets evaluation: A sole focus on access assigns the United States a health level status of 7, while a focus on quality shows a health level status of 2. The policy implications of the cluster classifications for selected countries were provided in detail where relevant.",
        "DOI": "10.1016/j.seta.2022.102224",
        "paper_author": "Matenga Z.",
        "affiliation_name": "Penn State College of Engineering",
        "affiliation_city": "University Park",
        "affiliation_country": "United States",
        "affiliation_id": "60147936",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "Data fusion of multiple machine intelligent systems for the condition assessment of subway structures",
        "publication": "Tunnelling and Underground Space Technology",
        "citied_by": "10",
        "cover_date": "2022-08-01",
        "Abstract": "Water intrusion through soil is considered the most significant structural issue and the major cause of concrete degradation in subway networks. An enormous amount of water infiltration may expedite the deterioration mechanisms, such as moisture marks, spalling, scaling, and cracks. These mechanisms can compromise the structural durability and jeopardize people's safety. The condition assessment of concrete infrastructure is predominantly conducted based of visual inspection techniques, which are costly, time-consuming, and error prone. In this research, two main models for the condition assessment of subway networks are proposed. First, image processing techniques and machine intelligent systems are streamlined through successive operations to detect and quantify multiple surface defects automatically. Spatial and frequency domain filters are used to enhance the image clues, in tandem with artificial neural networks (ANNs) and regression analysis (RA) for defect recognition. The Monte Carlo simulation (MCS) is then leveraged to deliver advanced optimization and accurate estimation for each defect's condition index in the subway element. The developed method was implemented on four stations in Montréal subway systems, whereby the performance of ANNs and RA was validated through R2 as 0.928 and 0.957, respectively. Moreover, the MCS forecast precision was recorded as 95% percentile, which proves the efficacy of the developed models. This research provides insights for infrastructure managers about maintenance and intervention plans in order to prioritize their spending policies.",
        "DOI": "10.1016/j.tust.2022.104512",
        "paper_author": "Dawood T.",
        "affiliation_name": "Purdue University",
        "affiliation_city": "West Lafayette",
        "affiliation_country": "United States",
        "affiliation_id": "60009254",
        "affiliation_state": "IN"
    },
    {
        "paper_title": "Discovering diverse solutions in deep reinforcement learning by maximizing state–action-based mutual information",
        "publication": "Neural Networks",
        "citied_by": "18",
        "cover_date": "2022-08-01",
        "Abstract": "Reinforcement learning algorithms are typically limited to learning a single solution for a specified task, even though diverse solutions often exist. Recent studies showed that learning a set of diverse solutions is beneficial because diversity enables robust few-shot adaptation. Although existing methods learn diverse solutions by using the mutual information as unsupervised rewards, such an approach often suffers from the bias of the gradient estimator induced by value function approximation. In this study, we propose a novel method that can learn diverse solutions without suffering the bias problem. In our method, a policy conditioned on a continuous or discrete latent variable is trained by directly maximizing the variational lower bound of the mutual information, instead of using the mutual information as unsupervised rewards as in previous studies. Through extensive experiments on robot locomotion tasks, we demonstrate that the proposed method successfully learns an infinite set of diverse solutions by learning continuous latent variables, which is more challenging than learning a finite number of solutions. Subsequently, we show that our method enables more effective few-shot adaptation compared with existing methods.",
        "DOI": "10.1016/j.neunet.2022.04.009",
        "paper_author": "Osa T.",
        "affiliation_name": "Kyushu Institute of Technology",
        "affiliation_city": "Kitakyushu",
        "affiliation_country": "Japan",
        "affiliation_id": "60031838",
        "affiliation_state": "Fukuoka"
    },
    {
        "paper_title": "Learning intraoperative organ manipulation with context-based reinforcement learning",
        "publication": "International Journal of Computer Assisted Radiology and Surgery",
        "citied_by": "5",
        "cover_date": "2022-08-01",
        "Abstract": "Purpose: Automation of sub-tasks during robotic surgery is challenging due to the high variability of the surgical scenes intra- and inter-patients. For example, the pick and place task can be executed different times during the same operation and for distinct purposes. Hence, designing automation solutions that can generalise a skill over different contexts becomes hard. All the experiments are conducted using the Pneumatic Attachable Flexible (PAF) rail, a novel surgical tool designed for robotic-assisted intraoperative organ manipulation. Methods: We build upon previous open-source surgical Reinforcement Learning (RL) training environment to develop a new RL framework for manipulation skills, rlman. In rlman, contextual RL agents are trained to solve different aspects of the pick and place task using the PAF rail system. rlman is implemented to support both low- and high-dimensional state information to solve surgical sub-tasks in a simulation environment. Results: We use rlman to train state of the art RL agents to solve four different surgical sub-tasks involving manipulation skills using the PAF rail. We compare the results with state-of-the-art benchmarks found in the literature. We evaluate the ability of the agent to be able to generalise over different aspects of the targeted surgical environment. Conclusion: We have shown that the rlman framework can support the training of different RL algorithms for solving surgical sub-task, analysing the importance of context information for generalisation capabilities. We are aiming to deploy the trained policy on the real da Vinci using the dVRK and show that the generalisation of the trained policy can be transferred to the real world.",
        "DOI": "10.1007/s11548-022-02630-2",
        "paper_author": "D’Ettorre C.",
        "affiliation_name": "University College London",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60022148",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Context meta-reinforcement learning via neuromodulation",
        "publication": "Neural Networks",
        "citied_by": "5",
        "cover_date": "2022-08-01",
        "Abstract": "Meta-reinforcement learning (meta-RL) algorithms enable agents to adapt quickly to tasks from few samples in dynamic environments. Such a feat is achieved through dynamic representations in an agent's policy network (obtained via reasoning about task context, model parameter updates, or both). However, obtaining rich dynamic representations for fast adaptation beyond simple benchmark problems is challenging due to the burden placed on the policy network to accommodate different policies. This paper addresses the challenge by introducing neuromodulation as a modular component to augment a standard policy network that regulates neuronal activities in order to produce efficient dynamic representations for task adaptation. The proposed extension to the policy network is evaluated across multiple discrete and continuous control environments of increasing complexity. To prove the generality and benefits of the extension in meta-RL, the neuromodulated network was applied to two state-of-the-art meta-RL algorithms (CAVIA and PEARL). The result demonstrates that meta-RL augmented with neuromodulation produces significantly better result and richer dynamic representations in comparison to the baselines.",
        "DOI": "10.1016/j.neunet.2022.04.003",
        "paper_author": "Ben-Iwhiwhu E.",
        "affiliation_name": "Loughborough University",
        "affiliation_city": "Loughborough",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60000891",
        "affiliation_state": "Leicestershire"
    },
    {
        "paper_title": "Smart City Data Science: Towards data-driven smart cities with open research issues",
        "publication": "Internet of Things (Netherlands)",
        "citied_by": "95",
        "cover_date": "2022-08-01",
        "Abstract": "Cities are undergoing huge shifts in technology and operations in recent days, and ‘data science’ is driving the change in the current age of the Fourth Industrial Revolution (Industry 4.0 or 4IR). Extracting useful knowledge or actionable insights from city data and building a corresponding data-driven model is the key to making a city system automated and intelligent. Data science is typically the scientific study and analysis of actual happenings with historical data using a variety of scientific methodologies, machine learning techniques, processes, and systems. In this paper, we concentrate on and explore “Smart City Data Science”, where city data collected from various sources such as sensors, Internet-connected devices, or other external sources, is being mined for insights and hidden correlations to enhance decision-making processes and deliver better and more intelligent services to citizens. To achieve this goal, artificial intelligence, particularly, machine learning analytical modeling can be employed to provide deeper knowledge about city data, which makes the computing process more actionable and intelligent in various real-world city services. Finally, we identify and highlight ten open research issues for future development and research in the context of data-driven smart cities. Overall, we aim to provide an insight into smart city data science conceptualization on a broad scale, which can be used as a reference guide for the researchers, industry professionals, as well as policy-makers of a country, particularly, from the technological point of view.",
        "DOI": "10.1016/j.iot.2022.100528",
        "paper_author": "Sarker I.H.",
        "affiliation_name": "Chittagong University of Engineering and Technology",
        "affiliation_city": "Chittagong",
        "affiliation_country": "Bangladesh",
        "affiliation_id": "60013024",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Deep Learning Approach to Detection and Mitigation of Distributed Denial of Service Attacks in High Availability Intelligent Transport Systems",
        "publication": "Mobile Networks and Applications",
        "citied_by": "10",
        "cover_date": "2022-08-01",
        "Abstract": "In the era of Internet of Things (IoT) powered by 5G technologies, Automobile Industry is headed towards a revolution. In Intelligent Transport Systems (ITS), vehicles act as connected entities, and exchange data with each other and with the back-end servers on the mobile network. These communications are often session based and require a light weight protocol for session establishment and continuity. Session Initiation Protocol (SIP) can act as the base for this kind of communication. However, its simplicity also makes the protocol vulnerable to various web attacks such as identity theft and Distributed Denial of Service (DDoS). As 5G technologies will enable high data rates to the users, this will also exponentially increase the threat of high-speed DDoS on the servers originating from different sources. Thus, appropriate solutions need to be developed for securing SIP systems from these threats. Machine Learning (ML) has transpired as a building block in cyber security solutions, and a large number of techniques are available to make quick and robust network defense systems by automating the identification of attack flows in the network. In this paper, a Deep Learning-based model is proposed for the identification and alleviation of DDoS attacks in SIP based networks. The work presented here uses a system that is scalable and highly available with load balancing and failover addressing capabilities. The datasets used for conducting experiments are created by emulating SIP sessions, generating DDoS attacks, capturing the normal and attack flows, and extracting time window-based features from the packets. A stacked autoencoder model is trained on the curated datasets to detect various types of DDoS attacks. Once an attack is detected, the Mitigation Policy Recommender module recommends various actions for threat mitigation. Performance of the system is assessed in terms of Accuracy, Precision, Recall and F1-Score. The proposed model obtains a significant improvement in the performance than the previously existing state-of-the-art techniques in terms of accuracy and detection rate.",
        "DOI": "10.1007/s11036-022-01973-z",
        "paper_author": "Mahajan N.",
        "affiliation_name": "University Institute of Engineering and Technology",
        "affiliation_city": "Chandigarh",
        "affiliation_country": "India",
        "affiliation_id": "60116744",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Unraveling the invisible leptospirosis in mainland Southeast Asia and its fate under climate change",
        "publication": "Science of the Total Environment",
        "citied_by": "17",
        "cover_date": "2022-08-01",
        "Abstract": "Leptospirosis is a neglected waterborne zoonosis of growing concern in tropical and low-income regions. Endemic in Southeast Asia, its distribution and environmental factors such as climate controlling its dynamics remain poorly documented. In this paper, we investigate for the first time the current and future leptospirosis burden at a local scale in mainland Southeast Asia. We adjusted machine-learning models on incidence reports from the Thai surveillance system to identify environmental determinants of leptospirosis. The explanatory variables tested in our models included climate, topographic, land cover and soil variables. The model performing the best in cross-validation was used to estimate the current incidence regionally in Thailand, Myanmar, Cambodia, Vietnam and Laos. It then allowed to predict the spatial distribution of leptospirosis future burden from 2021 to 2100 based on an ensemble of CMIP6 climate model projections and 4 Shared Socio-economics Pathways ranging from the most optimistic to the no-climate policy outcomes (SSP1–2.6, SSP2–4.5, SSP3–7.0 and SSP5–8.5). Leptospirosis incidence was best estimated by 10 environmental variables: four landscape-, four rainfall-, two temperature-related variables. Of all tested scenario, the worst-case scenario of climate change (SSP5–8.5) surprisingly appeared as the best-case scenario for the future of leptospirosis since it would induce a significant global decline in disease incidence in Southeast Asia mainly driven by the increasing temperatures. These global patterns are however contrasted regionally with some regions showing increased incidence in the future. Our work highlights climate and the environment as major drivers of leptospirosis incidence in Southeast Asia. Applying our model to regions where leptospirosis is not routinely monitored suggests an overlooked burden in the region. As our model focuses on leptospirosis responses to environmental drivers only, some other factors, such as poverty, lifestyle or behavioral changes, could further influence these estimated future patterns.",
        "DOI": "10.1016/j.scitotenv.2022.155018",
        "paper_author": "Douchet L.",
        "affiliation_name": "CNRS Centre National de la Recherche Scientifique",
        "affiliation_city": "Paris",
        "affiliation_country": "France",
        "affiliation_id": "60008134",
        "affiliation_state": "Ile-de-France"
    },
    {
        "paper_title": "Can the Framing of Climate Mitigation Actions into Government Policies Lead to Delivering Them? – Insights from Nepal’s Experience",
        "publication": "Environmental Management",
        "citied_by": "1",
        "cover_date": "2022-08-01",
        "Abstract": "Many low-income countries (LICs), including Nepal, endeavour to deliver climate mitigation by reducing greenhouse gas (GHG) emissions and achieving more sustainable resource consumption. However, their prospects of delivering on such goals alongside the rapid structural changes in the economy prevalent in the LICs are not clear. This research aims to better understand the underlying complexity in the linkage between the framing of climate mitigation actions into government policies and the prospects for their delivery. We use critical discourse analysis, post-structural discourse analysis, and thematic analysis of textual data corpus generated from government policies (n = 12) and semi-structured interviews (n = 12) with policy actors, such as government policymakers and private sector and non-government organisations’ representatives. We also develop energy and material consumption and GHG emissions models to predict their values up to 2050 via the R tools and machine learning algorithms that validate the accuracy of models. Our findings suggest that the social context of policymaking creates a knowledge structure on climate mitigation which is reflected in government policies. The policy actors and their institutions exchange their ideas and interests in a deliberative and collaborative environment to prioritise policies for the energy, forest, and transport sectors to deliver climate mitigation actions in Nepal. However, the energy sector, together with the agriculture sector, has insufficient climate mitigation actions. Reflecting on the high proportion of biomass in the energy mix and the rapid rise in fossil fuel and energy consumption per capita—both of which are driven by the remittance inflows—this research suggests measures to reduce these in an absolute sense.",
        "DOI": "10.1007/s00267-022-01643-6",
        "paper_author": "Baniya B.",
        "affiliation_name": "University of Technology Sydney",
        "affiliation_city": "Sydney",
        "affiliation_country": "Australia",
        "affiliation_id": "60023932",
        "affiliation_state": "NSW"
    },
    {
        "paper_title": "Dynamic Role-Based Access Control Policy for Smart Grid Applications: An Offline Deep Reinforcement Learning Approach",
        "publication": "IEEE Transactions on Human-Machine Systems",
        "citied_by": "44",
        "cover_date": "2022-08-01",
        "Abstract": "Role-based access control (RBAC) is adopted in the information and communication technology domain for authentication purposes. However, due to a very large number of entities within organizational access control (AC) systems, static RBAC management can be inefficient, costly, and can lead to cybersecurity threats. In this article, a novel hybrid RBAC model is proposed, based on the principles of offline deep reinforcement learning (RL) and Bayesian belief networks. The considered framework utilizes a fully offline RL agent, which models the behavioral history of users as a Bayesian belief-based trust indicator. Thus, the initial static RBAC policy is improved in a dynamic manner through off-policy learning while guaranteeing compliance of the internal users with the security rules of the system. By deploying our implementation within the smart grid domain and specifically within a Distributed Energy Resources (DER) ecosystem, we provide an end-To-end proof of concept of our model. Finally, detailed analysis and evaluation regarding the offline training phase of the RL agent are provided, while the online deployment of the hybrid RL-based RBAC model into the DER ecosystem highlights its key operation features and salient benefits over traditional RBAC models.",
        "DOI": "10.1109/THMS.2022.3163185",
        "paper_author": "Fragkos G.",
        "affiliation_name": "University of New Mexico School of Engineering",
        "affiliation_city": "Albuquerque",
        "affiliation_country": "United States",
        "affiliation_id": "60138187",
        "affiliation_state": "NM"
    },
    {
        "paper_title": "Evaluating the real changes of air quality due to clean air actions using a machine learning technique: Results from 12 Chinese mega-cities during 2013–2020",
        "publication": "Chemosphere",
        "citied_by": "29",
        "cover_date": "2022-08-01",
        "Abstract": "China has implemented two national clean air actions in 2013–2017 and 2018–2020, respectively, with the aim of reducing primary emissions and hence improving air quality at a national level. It is important to examine the effectiveness of such emission reductions and assess the resulting changes in air quality. However, such evaluation is difficult as meteorological factors can amplify, or obscure the changes of air pollutants, in addition to the emission reduction. In this study, we applied the random forest machine learning technique to decouple meteorological influences from emissions changes, and examined the deweathered trends of air pollutants in 12 Chinese mega-cities during 2013–2020. The observed concentrations of all criteria pollutants except O3 showed significant declines from 2013 to 2020, with PM2.5 annual decline rates of 6–9% in most cities. In contrast, O3 concentrations increased with annual growth rates of 1–9%. Compared with the observed results, all the pollutants showed smoothed but similar variation in trend and annual rate-of-change after weather normalization. The response of O3 to NO2 concentrations indicated significant regional differences in photochemical regimes, and the differences between observed and deweathered results provided implications for volatile organic compound emission reductions in O3 pollution mitigation. We further evaluated the effectiveness of first and second clean air actions by removing the meteorological influence. We found that the meteorology can make negative or positive contribution in reducing pollutant concentrations from emission reduction, depending on type of pollutants, locations, and time period. Among the 12 mega-cities, only Beijing showed a positive meteorological contribution in amplifying reductions in main pollutants except O3 during both clean air action periods. Considering the large and variable impact of meteorological effects in changing air quality, we suggest that similar deweathered analysis is needed as a routine policy evaluation tool on a regional basis.",
        "DOI": "10.1016/j.chemosphere.2022.134608",
        "paper_author": "Guo Y.",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60025278",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Machine learning-based monitoring and modeling for spatio-temporal urban growth of Islamabad",
        "publication": "Egyptian Journal of Remote Sensing and Space Science",
        "citied_by": "37",
        "cover_date": "2022-08-01",
        "Abstract": "LULC maps are important thematic maps that provide a baseline for monitoring, assessing, and planning activities. This study incorporates spatio-temporal land use/ land cover (LULC) monitoring (1991–2021) and urban growth modeling (2021–2041) of Islamabad, Pakistan to deduce the changes in various LULC classes in the past and the future by incorporating realistic influential thematic layers and Artificial Neural Network-Cellular Automata (ANN-CA) machine learning algorithms. Three decades of Landsat satellite imagery were used to classify LULC maps using a random forest algorithm with high Kappa indexes ranging from 0.93 to 0.97. Simulations for 2011 and 2021 were done for well-calibration of the model with Kappa (>0.85) and spatial similarity (>75%) using the MOLUSCE plugin in QGIS software. Future predictions were done for the years 2031 and 2041 to analyze and study the future urban growth patterns. The satellite-based LULC maps during 1991–2021 exhibited a 142.4 km2 increase in net urban growth. This had detrimental effects on other classes: net decrease of forests by 38.4 km2 and waterbodies by 2.9 km2. The projected increase of urban areas in 2021–2041 will be 58.2 km2. Visual urban sprawl assessment on LULC maps was done to highlight the type of sprawls. Overall, it was sensed that the city's urbanization has been unplanned and erratic; leading to dire consequences on the environmental and urban systems. Therefore, the study necessitates better monitoring and better planning of urbanization by enforcing policies and necessary measures.",
        "DOI": "10.1016/j.ejrs.2022.03.012",
        "paper_author": "Khan A.",
        "affiliation_name": "Capital University of Science &amp; Technology",
        "affiliation_city": "Islamabad",
        "affiliation_country": "Pakistan",
        "affiliation_id": "60039637",
        "affiliation_state": "Islamabad"
    },
    {
        "paper_title": "Learning to refine source representations for neural machine translation",
        "publication": "International Journal of Machine Learning and Cybernetics",
        "citied_by": "5",
        "cover_date": "2022-08-01",
        "Abstract": "Machine translation is one of the most classic application technologies in artificial intelligence and natural language processing. Neural machine translation models generally adopt an encoder–decoder architecture for modeling the entire translation process. However, without considering target context (e.g., decoding state) to guide the encoding, encoded source representations struggle to put great emphasis on important information for predicting some target word, yielding the weakness in generating more discriminative attentive representations across different decoding steps. Towards tackling this issue, we propose a novel encoder–refiner–decoder framework, which dynamically refines the source representations based on the generated target-side information at each decoding step. Since the refining operations are time-consuming, we propose a policy network to decide when to refine at specific decoding steps. We solve such a problem using the Gumbel-Softmax reparameterization, which makes our network differentiable and trainable through standard stochastic gradient methods. Experimental results on both Chinese–English and English–German translation tasks show that the proposed approach significantly and consistently improves translation performance over the standard encoder–decoder framework. Furthermore, when refining strategy is applied, experimental results still show a reasonable improvement over the baseline with much decrease in decoding speed.",
        "DOI": "10.1007/s13042-022-01515-9",
        "paper_author": "Geng X.",
        "affiliation_name": "Harbin Institute of Technology",
        "affiliation_city": "Harbin",
        "affiliation_country": "China",
        "affiliation_id": "60019616",
        "affiliation_state": "Heilongjiang"
    },
    {
        "paper_title": "Digital twin-enabled dynamic scheduling with preventive maintenance using a double-layer Q-learning algorithm",
        "publication": "Computers and Operations Research",
        "citied_by": "76",
        "cover_date": "2022-08-01",
        "Abstract": "Dynamic scheduling methods are essential and critical to manufacturing systems because of uncertain events in the production process, such as new job insertions, order cancellations, worker absences, and machine breakdowns. Emerging digital twin (DT) technology can help detect disturbances by continuously comparing physical space with virtual space and triggering a rescheduling policy immediately after a disturbance. This enables dynamic scheduling and greatly reduces the deviation between preschedules and actual schedules. This study focuses on a DT-enabled integrated optimisation problem of flexible job shop scheduling and flexible preventive maintenance (PM) considering both machine and worker resources. A double-layer Q-learning algorithm (DLQL) is designed as the underlying key optimisation method to simultaneously learn the selection process of machines and operations to achieve efficient real-time scheduling. The superior solution performance of DLQL was verified by comparing it with two well-known metaheuristic algorithms and a single-layer Q-learning algorithm under several benchmarks. Furthermore, different disturbance settings were designed to illustrate the DLQL-based dynamic scheduling process in detail. The proposed reinforcement learning (RL)-driven DT enables efficient collaborative scheduling between production and maintenance departments and helps manufacturing companies improve the real-time decision-making process under uncertain perturbations.",
        "DOI": "10.1016/j.cor.2022.105823",
        "paper_author": "Yan Q.",
        "affiliation_name": "College of Information Science and Engineering, Northeastern University",
        "affiliation_city": "Shenyang",
        "affiliation_country": "China",
        "affiliation_id": "60118697",
        "affiliation_state": "Liaoning"
    },
    {
        "paper_title": "Assessing the effect of future landslide on ecosystem services in Aqabat Al-Sulbat region, Saudi Arabia",
        "publication": "Natural Hazards",
        "citied_by": "11",
        "cover_date": "2022-08-01",
        "Abstract": "Ecosystem conservation requires monitoring natural resources, such as vegetation, freshwater, and wetland systems, as well as their adaptations to natural and anthropogenic stressors. As a result, the present study intends to assess the effect of future landslide on different ecosystem services in Aqabat Al-Sulbat regions. In the present study, landslide susceptibility maps (LSM) at different return periods as a proxy of future landslide have been generated with integration of ensemble machine learning algorithms. Stacking framework based on bagging, dagging, and artificial neural network (ANN) along with random forest has been proposed to generate LSMs. Also, bagging, dagging, and ANN have been applied individually. Receiver operational characteristics (ROC curve) were used to validate the LSM model against reality. The best LSM predicting model determined by the ROC curve was used to create future LSM maps by combining rainfall at 2–100 year return periods (using Gumble extreme value distribution). The potential loss of ecosystem services value in the Aqabat Al-Sulbat region was estimated. All models anticipate 6–16 km2 and 27–41 km2 of the study region as very high and high land susceptibility (LS) zones, respectively. According to the area under the curve (AUC) of the ROC curve, stacking-bagging outperformed all other models (AUC: 0.91). While the region encompassed by very high LS zones would steadily rise from 2 to 100 year return periods (6.1–20.44 km2). As a result, ecosystem services (ES) would be destroyed as well. The results revealed that the ES generating region with zero value will steadily expand with increasing return periods (39–42 km2) owing to the expansion of high LS zones. The research delivers reliable spatially explicit results in order to influence policy decisions on forest management investment, landslide management, and development activity management as well as replication possibilities in other data sparse places.",
        "DOI": "10.1007/s11069-022-05318-7",
        "paper_author": "Alqadhi S.",
        "affiliation_name": "King Khalid University",
        "affiliation_city": "Abha",
        "affiliation_country": "Saudi Arabia",
        "affiliation_id": "60027741",
        "affiliation_state": "Asir"
    },
    {
        "paper_title": "Using chains of bottleneck transitions to decompose and solve reinforcement learning tasks with hidden states",
        "publication": "Future Generation Computer Systems",
        "citied_by": "4",
        "cover_date": "2022-08-01",
        "Abstract": "Reinforcement learning is known to underperform in large and ambiguous problem domains under partial observability. In such cases, a proper decomposition of the task can improve and accelerate the learning process. Even ambiguous and complex problems that are not solvable by conventional methods turn out to be easier to handle by using a convenient problem decomposition, followed by the incorporation of machine learning methods for the sub-problems. Like in most real-life problems, the decomposition of a task usually stems from the sequence of sub-tasks that must be achieved in order to get the main task done. In this study, assuming that unambiguous states are provided in advance, a decomposition of the problem is constructed by the agent based on a set of chains of bottleneck transitions, which are sequences of unambiguous and critical transitions leading to the goal state. At the higher level, an agent trains its sub-agents to extract sub-policies corresponding to the sub-tasks, namely two successive transitions in any chain, and learns the value of each sub-policy at the abstract level. Experimental study demonstrates that an early decomposition based on useful bottleneck transitions eliminates the necessity for excessive memory and improves the learning performance of the agent. It is also shown that knowing the correct order of bottleneck transitions in the decomposition results in faster construction of the solution.",
        "DOI": "10.1016/j.future.2022.03.016",
        "paper_author": "Aydın H.",
        "affiliation_name": "Middle East Technical University (METU)",
        "affiliation_city": "Ankara",
        "affiliation_country": "Turkey",
        "affiliation_id": "60004305",
        "affiliation_state": "Ankara"
    },
    {
        "paper_title": "Predicting the changes in the WTI crude oil price dynamics using machine learning models",
        "publication": "Resources Policy",
        "citied_by": "56",
        "cover_date": "2022-08-01",
        "Abstract": "This study aims to use a monthly dataset from 1991 to 2021 to predict West Texas Intermediate (WTI) oil price dynamics using U.S. macroeconomic and financial factors, as well as a global crisis and crashes. We used advanced machine learning models such as Logistic Regression, Decision Tree, Random Forest, AdaBoost, and XgBoost in this study. According to the results, the XgBoost and Random Forest models outperform traditional models. We also used DeLong statistical test procedures to accurately compare machine learning models' performance. In addition, the study used SHAP - SHapley Additive exPlanations values to support model evaluation and interpretability. This new outline highlights the critical features of the WTI crude oil price prediction and provides appropriate model explanations by utilizing the practical SHAP values. The empirical findings showed that machine learning models could successfully and accurately predict the trend of WTI crude oil price changes. Our findings are important for policymakers, companies, and investors, as well as long-term energy-based economic development.",
        "DOI": "10.1016/j.resourpol.2022.102664",
        "paper_author": "Guliyev H.",
        "affiliation_name": "Azerbaijan State University of Economics (UNEC)",
        "affiliation_city": "Baku",
        "affiliation_country": "Azerbaijan",
        "affiliation_id": "60086039",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Quantifying the impact of environment factors on the risk of medical responders’ stress-related absenteeism",
        "publication": "Risk Analysis",
        "citied_by": "2",
        "cover_date": "2022-08-01",
        "Abstract": "Medical emergency response staff are exposed to incidents which may involve high-acuity patients or some intractable or traumatic situations. Previous studies on emergency response staff stress-related absence have focused on perceived factors and their impacts on absence leave. To date, analytical models on absenteeism risk prediction use past absenteeism to predict risk of future absenteeism. We show that these approaches ignore environment data, such as stress factors. The increased use of digital systems in emergency services allows us to gather data that were not available in the past and to apply a data-driven approach to quantify the effect of environment variables on the risk of stress-related absenteeism. We propose a two-stage data-driven framework to identify the variables of importance and to quantify their impact on medical staff stress-related risk of absenteeism. First, machine learning techniques are applied to identify the importance of different stressors on staff stress-related risk of absenteeism. Second, the Cox proportional-hazards model is applied to estimate the relative risk of each stressor. Four significant stressors are identified, these are the average night shift, past stress leave, the squared term of death confirmed by the Emergency Services and completion of the safeguarding form. We discuss counterintuitive results and implications to policy.",
        "DOI": "10.1111/risa.13909",
        "paper_author": "Brito M.P.",
        "affiliation_name": "University of Southampton",
        "affiliation_city": "Southampton",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60025225",
        "affiliation_state": "Hampshire"
    },
    {
        "paper_title": "Deep reinforcement learning based parameter self-tuning control strategy for VSG",
        "publication": "Energy Reports",
        "citied_by": "17",
        "cover_date": "2022-08-01",
        "Abstract": "With the development of new energy technology, the distributed generation has attracted more and more attention. In order to enhance the inertia of distributed generator system to improve its stability, the control technology of virtual synchronous generator (VSG) is proposed. However, the traditional VSG control technology often has poor flexibility and long dynamic adjustment time. In this context, a deep deterministic policy gradient (DDPG) algorithm based adaptive controller is designed to realize the adaptive control of inertia and damping coefficient in the system, so that the parameters can be adjusted adaptively under different operating conditions. Next, a simulation model of VSG isolated island single machine operation model is built in MATLAB Simulink, and the implementation of DDPG algorithm is given and verified by simulation. The results show that the VSG parameter adaptive system controlled by DDPG has stronger ability to resist disturbance and achieves better performance than the traditional VSG adaptive system. The DRL model only takes 0.448 s to return to stable, but the other two models need 0.632 s and 0.818 s respectively.",
        "DOI": "10.1016/j.egyr.2022.02.147",
        "paper_author": "Xiong K.",
        "affiliation_name": "University of Electronic Science and Technology of China",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China",
        "affiliation_id": "60005465",
        "affiliation_state": "Sichuan"
    },
    {
        "paper_title": "Understanding knowledge management and upskilling in Fourth Industrial Revolution: transformational shift and SECI model",
        "publication": "VINE Journal of Information and Knowledge Management Systems",
        "citied_by": "67",
        "cover_date": "2022-08-01",
        "Abstract": "Purpose: The adoption of knowledge management (KM) to steer new skills and capabilities among people provides evidence that KM not only offers competitive advantages but also provides a means for organisational survival, by improvising core capabilities or generate new ones that can drive people in the Fourth Industrial Revolution (4IR) era. This paperaim to identify critical new skills and capabilities among people within an organisation to stay competitive, innovative and relevant. Design/methodology/approach: The paper presents the findings on new skills assessment for Fourth Industrial Revolution. The study was carried out through an interview with a focus group discussion technique to gather data on the role of KM in creating new set of skills or capabilities in Fourth Industrial Revolution’s landscape. The study also reports a bibliographic study of critical skills based on more than a decade of related academic and industry publications to portray research trends and future directions. Findings: There is a demand in “must-have” skills related to Industry 4.0 such as capability for complex decision-making, complex problem-solving, collaborative innovation, project management, creativity and critical thinking, social skill and social responsibility. While these skills are critical enablers to aiding individuals in the scenarios of plausible 4IR futures, several important new research trends that emerge have also not been adequately explored including KM and Industry 4.0 skill gap, skill evolution, machine knowledge, intuitive decision-making, rational decision-making, technostress, digital fluency, collaborative innovation, industrial policies, human–machine interaction and societal systems. Research limitations/implications: This research provides a roadmap for the next research trends and topics in the area of Fourth Industrial Revolution and new skills requirements. The study discusses some of the essential issues and challenges with upskilling required for Industry 4.0. It also focuses on how upskilling learning initiatives influence new knowledge creation. This primarily contributes to the educational field in deciding how and when to adopt appropriate strategies and identify which initiatives to best meet the needs of its community. Practical implications: KM enables individuals to utilise their existing core capabilities or generate new ones for immediate investment in upskilling to meet current and future skills needs required by an organisation. Simply put, KM will improve the organisation’s talent-driven learning strategy and increase individuals’ ability to learn faster and attain sustainable competitive advantage in a fast-paced ever changing environment. Originality/value: This paper is useful to academics, practitioners and policymakers in the fields of KM. The research provides initial insight into new skills mapping in the context of Fourth Industrial Revolution and the needs for researchers to understand the recent research trends in KM.",
        "DOI": "10.1108/VJIKMS-09-2021-0203",
        "paper_author": "Anshari M.",
        "affiliation_name": "Universiti Brunei Darussalam",
        "affiliation_city": "Bandar Seri Begawan",
        "affiliation_country": "Brunei Darussalam",
        "affiliation_id": "60072089",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Classification of Poverty Condition Using Natural Language Processing",
        "publication": "Social Indicators Research",
        "citied_by": "3",
        "cover_date": "2022-08-01",
        "Abstract": "This work introduces a methodology to classify between poor and extremely poor people through Natural Language Processing. The approach serves as a baseline to understand and classify poverty through the people’s discourses using machine learning algorithms. Based on classical and modern word vector representations we propose two strategies for document level representations: (1) document-level features based on the concatenation of descriptive statistics and (2) Gaussian mixture models. Three classification methods are systematically evaluated: Support Vector Machines, Random Forest, and Extreme Gradient Boosting. The fourth best experiments yielded around 55% of accuracy, while the embeddings based on GloVe word vectors yielded a sensitivity of 79.6% which could be of great interest for the public policy makers to accurately find people who need to be prioritized in social programs.",
        "DOI": "10.1007/s11205-022-02883-z",
        "paper_author": "Muñetón-Santa G.",
        "affiliation_name": "Universidad de Antioquia",
        "affiliation_city": "Medellin",
        "affiliation_country": "Colombia",
        "affiliation_id": "60055833",
        "affiliation_state": "Antioquia"
    },
    {
        "paper_title": "Identifying Important Factors to Prevent Juvenile Delinquency among Male and Female Adolescents: an Exploratory Analysis Using the LASSO Regression Algorithm in the Korean Children and Youth Panel Survey (KCYPS)",
        "publication": "Child Indicators Research",
        "citied_by": "5",
        "cover_date": "2022-08-01",
        "Abstract": "Juvenile delinquency is the outcome of complex interactions with multiple factors. However, few studies have explored what factors most likely contribute to delinquent behavior among female and male adolescents when all possible levels of factors are included in one model. To fill this gap, the current study investigated what factors were associated with juvenile delinquency and which factors were appeared to be significant in both female and male adolescents using machine learning algorithms. This information can be particularly informative for policymakers and researchers to capture the overall feature of delinquency. Data were derived from three-time points (8th grade, 9th grade, and 10th grade) of the nationally-representative Korean Children and Youth Panel Survey (KCYPS). The sample consisted of an almost equal number of males and females. This study employed the Least Absolute Shrinkage and Selection Operator (LASSO) regression as an exploratory analysis to identify important factors related to juvenile delinquency and compared coefficients of each factor in the model. The results showed that individual factors, including cyber delinquency, aggression, romantic relationships, following school rules, engagement in learning activities at school, academic confidence in Korean, English, and math, relationships with teachers, peer relationships, social withdrawal, and mobile phone dependency, were relatively important factors of delinquency for both females and males. These findings suggest that using LASSO regression to identify the most important factors for juvenile delinquency will provide an opportunity to understand the complex phenomenon of delinquency among female and male adolescents and can be a useful source in delinquency prevention policies in Korea.",
        "DOI": "10.1007/s12187-022-09916-6",
        "paper_author": "Choi J.",
        "affiliation_name": "Sungkyunkwan University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea",
        "affiliation_id": "60007511",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Classification of Rice Diseases using Convolutional Neural Network Models",
        "publication": "Journal of The Institution of Engineers (India): Series B",
        "citied_by": "45",
        "cover_date": "2022-08-01",
        "Abstract": "Automatic diagnosis and control of rice plant disease are highly desired by agricultural experts. Many machine learning approaches have been proposed in automating rice disease identification, where deep learning has generated significant outcomes. In the present study, state-of-the-art deep learning models based on transfer learning approach are deployed for the classification of various disease symptoms in rice plant images. The efficiency of the leading pre-trained VGG-16 and GoogleNet convolutional neural network (CNN) models on the held-out dataset is evaluated using a threefold cross-validation method. The trained VGG-16 and GoogleNet CNN models achieved an average classification accuracy of 92.24% and 91.28%, respectively. The experimental results demonstrate the practical usefulness of utilizing the deep learning methodology employing 12,000 labeled images of three different rice diseases with 24 different symptoms. The proposed work finds applications in on-field identification of rice disease symptoms providing actionable information to farmers and policy makers in many aspects of crop handling and management practices.",
        "DOI": "10.1007/s40031-021-00704-4",
        "paper_author": "Yakkundimath R.",
        "affiliation_name": "KLE Institute of Technology",
        "affiliation_city": "Hubli",
        "affiliation_country": "India",
        "affiliation_id": "60283220",
        "affiliation_state": "KA"
    },
    {
        "paper_title": "Multi-Stage Hybrid Federated Learning Over Large-Scale D2D-Enabled Fog Networks",
        "publication": "IEEE/ACM Transactions on Networking",
        "citied_by": "67",
        "cover_date": "2022-08-01",
        "Abstract": "Federated learning has generated significant interest, with nearly all works focused on a 'star' topology where nodes/devices are each connected to a central server. We migrate away from this architecture and extend it through the network dimension to the case where there are multiple layers of nodes between the end devices and the server. Specifically, we develop multi-stage hybrid federated learning (MH-FL), a hybrid of intra-and inter-layer model learning that considers the network as a multi-layer cluster-based structure. MH-FL considers the topology structures among the nodes in the clusters, including local networks formed via device-to-device (D2D) communications, and presumes a semi-decentralized architecture for federated learning. It orchestrates the devices at different network layers in a collaborative/cooperative manner (i.e., using D2D interactions) to form local consensus on the model parameters and combines it with multi-stage parameter relaying between layers of the tree-shaped hierarchy. We derive the upper bound of convergence for MH-FL with respect to parameters of the network topology (e.g., the spectral radius) and the learning algorithm (e.g., the number of D2D rounds in different clusters). We obtain a set of policies for the D2D rounds at different clusters to guarantee either a finite optimality gap or convergence to the global optimum. We then develop a distributed control algorithm for MH-FL to tune the D2D rounds in each cluster over time to meet specific convergence criteria. Our experiments on real-world datasets verify our analytical results and demonstrate the advantages of MH-FL in terms of resource utilization metrics.",
        "DOI": "10.1109/TNET.2022.3143495",
        "paper_author": "Hosseinalipour S.",
        "affiliation_name": "Purdue University",
        "affiliation_city": "West Lafayette",
        "affiliation_country": "United States",
        "affiliation_id": "60009254",
        "affiliation_state": "IN"
    },
    {
        "paper_title": "Extracting built-up areas from spectro-textural information using machine learning",
        "publication": "Soft Computing",
        "citied_by": "5",
        "cover_date": "2022-08-01",
        "Abstract": "Extraction of built-up area (BUA) is essential for proper city planning and management. It enables the concerned authorities to formulate better city development policies and manage emergent disasters. However, the traditionally used optical data present spectral confusion where BUAs are mixed with other features adding to management complexities. Therefore, an advanced automated method is required to extract the spectral and textural features from satellite data for the pattern recognition of BUA. Landsat-8 Operational Land Imager (OLI) has been used in the current study to identify the pattern and extract BUA of Gujranwala, Pakistan. First, eight textural features based on the gray-level co-occurrence matrix (GLCM) are selected and combined with multispectral data. Then, feature selection methods are applied to select optimal features used to train the proposed support vector machine (SVM) classifier. Finally, the results from SVM classifiers are compared with k-nearest neighbor (k-NN) and backpropagation neural network (BP-NN) to highlight any improvements in results. The comparisons show that the proposed approach increases the overall accuracy of linear-SVM by 8.41%, radial basis function SVM by 8.3%, BP-NN by 7.63%, and k-NN by 6.6%. This can help city managers and planners to extract critical BUA information in otherwise unplanned and rapidly expanding cities to move toward smart and sustainable cities.",
        "DOI": "10.1007/s00500-022-06794-6",
        "paper_author": "Maqsoom A.",
        "affiliation_name": "COMSATS University Islamabad, Wah Campus",
        "affiliation_city": "Rawalpindi",
        "affiliation_country": "Pakistan",
        "affiliation_id": "60212764",
        "affiliation_state": "Punjab"
    },
    {
        "paper_title": "Optimal Charging Infrastructure Portfolio for Minimizing Grid Impact of Plug-In Electric Vehicles",
        "publication": "IEEE Transactions on Industrial Informatics",
        "citied_by": "11",
        "cover_date": "2022-08-01",
        "Abstract": "Charging infrastructure (CI) is crucial for the widespread adoption of plug-in electric vehicles (PEVs). The optimal portfolio of CI is beneficial for PEV owners, power systems, and policymakers. Therefore, an optimal CI portfolio estimation method is proposed to minimize the grid impacts of PEVs. All possible CI types (residential, workplace, public, and commercial) along with the three commonly used charging levels are considered and seven PEV charging profiles are estimated. Then, an optimization model is developed to determine the optimal portfolio of CI. An index is proposed to compare the performance of the proposed method with other CI portfolios. Various cases are simulated to analyze the performance under diverse conditions such as the presence of renewables and specific policy scenarios. The proposed method can be used by policymakers for making new policies, in new PEV markets, and to incentivize the use of specific CI types, in mature/developing markets.",
        "DOI": "10.1109/TII.2022.3146292",
        "paper_author": "Almutairi A.",
        "affiliation_name": "Majmaah University",
        "affiliation_city": "Al-Majmaah",
        "affiliation_country": "Saudi Arabia",
        "affiliation_id": "60110525",
        "affiliation_state": "Ar Riyad"
    },
    {
        "paper_title": "Weather Forecasting for Renewable Energy System: A Review",
        "publication": "Archives of Computational Methods in Engineering",
        "citied_by": "78",
        "cover_date": "2022-08-01",
        "Abstract": "Energy crisis and climate change are the major concerns which has led to a significant growth in the renewable energy resources which includes mainly the solar and wind power generation. In smart grid, there is a increase in the penetration level of solar PV and wind power generation. The solar radiation received at the earth surface is greatly dependent on various atmospheric parameters. Forecasting of solar radiation and photovoltaic power is a major concern in terms of efficient integration of solar PV plants in the power grid. There are significant challenges in smart grid energy management due to the variability of large-scale renewable energy generation. Renewable energy forecasting is critical to reduce the uncertainty related to renewable energy generation for a wide range of planning, investment and decision-making purposes. As renewable energy sources are highly intermittent and variable, all the forecasting models available in the literature contain errors. This paper presents an overview of current and new development of weather forecasting such as solar and wind forecasting techniques for renewable energy system in smart grid. Many forecasting models such as physical models, statistical models, artificial intelligence based models, machine learning and deep learning based models were discussed. It is observed that, despite having no clear understanding on atmospheric physics, the artificial intelligence based methods such as machine learning and deep learning method produces reasonable weather forecasting results.",
        "DOI": "10.1007/s11831-021-09695-3",
        "paper_author": "Meenal R.",
        "affiliation_name": "Karunya Institute of Technology and Sciences",
        "affiliation_city": "Coimbatore",
        "affiliation_country": "India",
        "affiliation_id": "60100082",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Predicting financial crises with machine learning methods",
        "publication": "Journal of Forecasting",
        "citied_by": "27",
        "cover_date": "2022-08-01",
        "Abstract": "Countries must establish an effective early warning system to predict financial crises in order to avoid their catastrophic effects. To this end, we construct early warning systems based on the logistic model and seven machine learning methods, and we also use the Shapley value decomposition and Shapley regression to explore the causality of the machine learning methods. By comparing the performance of different early warning models in out-of-sample tests, we find that the machine learning models, especially the random forest, gradient boosting decision tree, and ensemble models, outperform the logistic model in terms of providing early predictions of financial crises. In addition, the Shapley value can be used to find more effective predictive indicators and analyze the causes of risks in different countries to a certain extent, enabling policymakers to supplement the policy toolbox to deal with such crises. Thus, we suggest that machine learning methods should be considered when establishing early warning systems to predict financial crises in the future.",
        "DOI": "10.1002/for.2840",
        "paper_author": "Liu L.",
        "affiliation_name": "Nankai University",
        "affiliation_city": "Tianjin",
        "affiliation_country": "China",
        "affiliation_id": "60018038",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Toward a New Rural Typology: Mapping Resources, Opportunities, and Challenges",
        "publication": "Economic Development Quarterly",
        "citied_by": "11",
        "cover_date": "2022-08-01",
        "Abstract": "While the concept of rurality has been debated in academic and professional literature for decades, less research has been done on a practical typology that can guide localized economic development strategies. This paper adds to the growing body of literature in search of a more nuanced definition of rural by applying unsupervised machine learning (ML) to the abundance of existing county-level data in the United States. The authors illustrate how this method can lead to a new county typology, named after economic development strategies, that accounts for idiosyncrasies in resources, opportunities, and challenges. This research serves as a practical step toward tractable, heterogeneous classifications that can inform the work of federal, state, and local policy makers, economic development practitioners, and many others.",
        "DOI": "10.1177/08912424211069122",
        "paper_author": "Khalaf C.",
        "affiliation_name": "University of Wyoming",
        "affiliation_city": "Laramie",
        "affiliation_country": "United States",
        "affiliation_id": "60008827",
        "affiliation_state": "WY"
    },
    {
        "paper_title": "FedGraph: Federated Graph Learning with Intelligent Sampling",
        "publication": "IEEE Transactions on Parallel and Distributed Systems",
        "citied_by": "62",
        "cover_date": "2022-08-01",
        "Abstract": "Federated learning has attracted much research attention due to its privacy protection in distributed machine learning. However, existing work of federated learning mainly focuses on Convolutional Neural Network (CNN), which cannot efficiently handle graph data that are popular in many applications. Graph Convolutional Network (GCN) has been proposed as one of the most promising techniques for graph learning, but its federated setting has been seldom explored. In this article, we propose FedGraph for federated graph learning among multiple computing clients, each of which holds a subgraph. FedGraph provides strong graph learning capability across clients by addressing two unique challenges. First, traditional GCN training needs feature data sharing among clients, leading to risk of privacy leakage. FedGraph solves this issue using a novel cross-client convolution operation. The second challenge is high GCN training overhead incurred by large graph size. We propose an intelligent graph sampling algorithm based on deep reinforcement learning, which can automatically converge to the optimal sampling policies that balance training speed and accuracy. We implement FedGraph based on PyTorch and deploy it on a testbed for performance evaluation. The experimental results of four popular datasets demonstrate that FedGraph significantly outperforms existing work by enabling faster convergence to higher accuracy.",
        "DOI": "10.1109/TPDS.2021.3125565",
        "paper_author": "Chen F.",
        "affiliation_name": "The University of Aizu",
        "affiliation_city": "Aizuwakamatsu",
        "affiliation_country": "Japan",
        "affiliation_id": "60031404",
        "affiliation_state": "Fukushima"
    },
    {
        "paper_title": "How Much Information Do Monetary Policy Committees Disclose? Evidence from the FOMC's Minutes and Transcripts",
        "publication": "Journal of Money, Credit and Banking",
        "citied_by": "20",
        "cover_date": "2022-08-01",
        "Abstract": "The purpose of central bank minutes is to give an account of monetary policy meeting discussions to outside observers, thereby enabling them to draw informed conclusions about future policy. However, minutes are by necessity a shortened and edited representation of a broader discussion. Consequently, they may omit information that is predictive of future policy decisions. To investigate this, we compare the predictive content of the Federal Open Market Committee's (FOMC) minutes and transcripts, focusing on dimensions that are likely to be excluded from the minutes, such as the committee's degree of hawkishness, the chairperson's degree of hawkishness, and the level of agreement between committee members. We measure committee and chairperson hawkishness with a new dictionary that is constructed using the FOMC's minutes and transcripts. Agreement is measured using a technique we import from the machine learning literature. We also show that transcripts contain predictive content that is not included in FOMC minutes, macroeconomic variables, financial variables, forecasts, or federal funds rate futures.",
        "DOI": "10.1111/jmcb.12885",
        "paper_author": "Apel M.",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Identifying Unexpected Deaths in Long-Term Care Homes",
        "publication": "Journal of the American Medical Directors Association",
        "citied_by": "3",
        "cover_date": "2022-08-01",
        "Abstract": "Objectives: Predicting unexpected deaths among long-term care (LTC) residents can provide valuable information to clinicians and policy makers. We study multiple methods to predict unexpected death, adjusting for individual and home-level factors, and to use as a step to compare mortality differences at the facility level in the future work. Design: We conducted a retrospective cohort study using Resident Assessment Instrument Minimum Data Set assessment data for all LTC residents in Ontario, Canada, from April 2017 to March 2018. Setting and Participants: All residents in Ontario long-term homes. We used data routinely collected as part of administrative reporting by health care providers to the funder: Ontario Ministry of Health and Long-Term Care. This project is a component of routine policy development to ensure safety of the LTC system residents. Methods: Logistic regression (LR), mixed-effect LR (mixLR), and a machine learning algorithm (XGBoost) were used to predict individual mortality over 5 to 95 days after the last available RAI assessment. Results: We identified 22,419 deaths in the cohort of 106,366 cases (mean age: 83.1 years; female: 67.7%; dementia: 68.8%; functional decline: 16.6%). XGBoost had superior calibration and discrimination (C-statistic 0.837) over both mixLR (0.819) and LR (0.813). The models had high correlation in predicting death (LR-mixLR: 0.979, LR-XGBoost: 0.885, mixLR-XGBoost: 0.882). The inter-rater reliability between the models LR-mixLR and LR-XGBoost was 0.56 and 0.84, respectively. Using results in which all 3 models predicted probability of actual death of a resident at <5% yielded 210 unexpected deaths or 0.9% of the observed deaths. Conclusions and Implications: XGBoost outperformed other models, but the combination of 3 models provides a method to detect facilities with potentially higher rates of unexpected deaths while minimizing the possibility of false positives and could be useful for ongoing surveillance and quality assurance at the facility, regional, and national levels.",
        "DOI": "10.1016/j.jamda.2021.09.025",
        "paper_author": "Rangrej J.",
        "affiliation_name": "Ministère de la Santé Ministère des Soins de Longue durée",
        "affiliation_city": "Toronto",
        "affiliation_country": "Canada",
        "affiliation_id": "60002534",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "‘Whatever it takes’ to change belief: evidence from Twitter",
        "publication": "Review of World Economics",
        "citied_by": "4",
        "cover_date": "2022-08-01",
        "Abstract": "The sovereign debt literature suggests the possibility that a self-fulfilling default crisis might be avoided if markets believe the central bank will act as lender of last resort. This paper investigates the extent to which changes in belief about an intervention of the European Central Bank (ECB) explain the sudden reduction of government bond spreads for the distressed countries in summer 2012. The authors study Twitter data and extract belief using machine learning techniques. They find evidence of strong increases in the perceived likelihood of ECB intervention and show that those increases explain subsequent decreases in the bond spreads of the distressed countries.",
        "DOI": "10.1007/s10290-021-00443-0",
        "paper_author": "Stiefel M.",
        "affiliation_name": "Universität Zürich",
        "affiliation_city": "Zurich",
        "affiliation_country": "Switzerland",
        "affiliation_id": "60012614",
        "affiliation_state": "ZH"
    },
    {
        "paper_title": "Individual Mobility Prediction in Mass Transit Systems Using Smart Card Data: An Interpretable Activity-Based Hidden Markov Approach",
        "publication": "IEEE Transactions on Intelligent Transportation Systems",
        "citied_by": "24",
        "cover_date": "2022-08-01",
        "Abstract": "Individual mobility is driven by demand for activities with diverse spatiotemporal patterns, but existing methods for mobility prediction often overlook the underlying activity patterns. Knowledge of activity patterns can improve the performance and interpretability of existing individual mobility models, leading to more informed policy design and better user experience in intelligent transportation systems. This study develops an activity-based modeling framework for individual mobility prediction in mass transit systems. Specifically, an input-output hidden Markov model (IOHMM) approach is proposed to simultaneously predict the (continuous) time and (discrete) location of an individual's next trip using transit smart card data. The prediction task can be transformed into predicting the hidden activity duration and end location. Based on a case study of Hong Kong's metro system, we show that the proposed model can achieve similar prediction performance as the state-of-the-art long short-term memory (LSTM) model. Unlike LSTM, the proposed IOHMM approach can also be used to analyze hidden activity patterns, which provides meaningful behavioral interpretation for why an individual makes a certain trip. Therefore, the activity-based prediction framework offers a way to preserve the predictive power of advanced machine learning methods while enhancing our ability to generate insightful behavioral explanations, which is useful for user-centric policy design and intelligent transportation applications such as personalized traveler information.",
        "DOI": "10.1109/TITS.2021.3109428",
        "paper_author": "Mo B.",
        "affiliation_name": "MIT School of Engineering",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States",
        "affiliation_id": "60140949",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Systematizing serendipity for big science infrastructures: The ATTRACT project",
        "publication": "Technovation",
        "citied_by": "17",
        "cover_date": "2022-08-01",
        "Abstract": "Big Science Research Infrastructures (BSRIs) are tremendous sources of ‘deep-tech’ with the potential to foment alternative commercial applications in diverse industries. Yet, cultivating novel applications of BSRI technologies is not straightforward due to misalignment between their scientific mission, large technological risks, market uncertainties, and long development times. Given these challenges, research is needed to understand if- and how-serendipitous innovations can be purposefully developed from BSRIs. In this study, we analyse ATTRACT, a novel initiative funded by the European Commission's Horizon 2020 program, which funded 170 projects with €100,000 each to develop a proof-of-concept commercial application of BSRI technologies within one year. Our analysis of this dataset identifies three modes employed by researchers to come up with alternate applications: (1) combining different technologies, (2) applying technology into a different field, and (3) using artificial intelligence or machine learning. In a second step, we conducted multinomial logistic regressions using the project data, expert evaluations, and a questionnaire to identify the antecedents associated with the pursuit of each of the three modes. Our findings suggest that scientists and engineers develop many new ideas about novel potential applications of BSRI technologies in their daily work. The main value of ATTRACT is in facilitating project development through financial resources, brokering relationships with industrial partners, and facilitating the applications of technologies in domains outside of the immediate purview of BSRIs.",
        "DOI": "10.1016/j.technovation.2021.102374",
        "paper_author": "Wareham J.",
        "affiliation_name": "Universitat Ramon Llull, ESADE",
        "affiliation_city": "Barcelona",
        "affiliation_country": "Spain",
        "affiliation_id": "60113192",
        "affiliation_state": "Barcelona"
    },
    {
        "paper_title": "Optimizing City-Scale Traffic Through Modeling Observations of Vehicle Movements",
        "publication": "IEEE Transactions on Intelligent Transportation Systems",
        "citied_by": "2",
        "cover_date": "2022-08-01",
        "Abstract": "The capability of traffic-information systems to sense the movement of millions of users and offer trip plans through mobile phones has enabled a new way of optimizing city traffic dynamics, turning transportation big data into insights and actions in a closed-loop and evaluating this approach in the real world. Existing research has applied dynamic Bayesian networks and deep neural networks to make traffic predictions from floating car data, utilized dynamic programming and simulation approaches to identify how people normally travel with dynamic traffic assignment for policy research, and introduced Markov decision processes and reinforcement learning to optimally control traffic signals. However, none of these works utilized floating car data to suggest departure times and route choices in order to optimize city traffic dynamics. In this paper, we present a study showing that floating car data can lead to lower average trip time, higher on-time arrival ratio, and higher Charypar-Nagel score compared with how people normally travel. The study is based on optimizing a partially observable discrete-time decision process and is evaluated in one synthesized scenario, one partly synthesized scenario, and three real-world scenarios. This study points to the potential of a 'living lab' approach where we learn, predict, and optimize behaviors in the real world.",
        "DOI": "10.1109/TITS.2021.3094758",
        "paper_author": "Yang F.",
        "affiliation_name": "School of Engineering and Applied Sciences",
        "affiliation_city": "Buffalo",
        "affiliation_country": "United States",
        "affiliation_id": "60153573",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Data-Based Optimal Consensus Control for Multiagent Systems With Policy Gradient Reinforcement Learning",
        "publication": "IEEE Transactions on Neural Networks and Learning Systems",
        "citied_by": "38",
        "cover_date": "2022-08-01",
        "Abstract": "This article investigates the optimally distributed consensus control problem for discrete-time multiagent systems with completely unknown dynamics and computational ability differences. The problem can be viewed as solving nonzero-sum games with distributed reinforcement learning (RL), and each agent is a player in these games. First, to guarantee the real-time performance of learning algorithms, a data-based distributed control algorithm is proposed for multiagent systems using offline system interaction data sets. By utilizing the interactive data produced during the run of a real-time system, the proposed algorithm improves system performance based on distributed policy gradient RL. The convergence and stability are guaranteed based on functional analysis and the Lyapunov method. Second, to address asynchronous learning caused by computational ability differences in multiagent systems, the proposed algorithm is extended to an asynchronous version in which executing policy improvement or not of each agent is independent of its neighbors. Furthermore, an actor-critic structure, which contains two neural networks, is developed to implement the proposed algorithm in synchronous and asynchronous cases. Based on the method of weighted residuals, the convergence and optimality of the neural networks are guaranteed by proving the approximation errors converge to zero. Finally, simulations are conducted to show the effectiveness of the proposed algorithm.",
        "DOI": "10.1109/TNNLS.2021.3054685",
        "paper_author": "Yang X.",
        "affiliation_name": "Tongji University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60073652",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Novel Resilient Control Scheme for a Class of Markovian Jump Systems With Partially Unknown Information",
        "publication": "IEEE Transactions on Cybernetics",
        "citied_by": "38",
        "cover_date": "2022-08-01",
        "Abstract": "In the complex practical engineering systems, many interferences and attacking signals are inevitable in industrial applications. This article investigates the reinforcement learning (RL)-based resilient control algorithm for a class of Markovion jump systems with completely unknown transition probability information. Based on the Takagi-Sugeno logical structure, the resilient control problem of the nonlinear Markovion systems is converted into solving a set of local dynamic games, where the control policy and attacking signal are considered as two rival players. Combining the potential learning and forecasting abilities, the new integral RL (IRL) algorithm is designed via system data to compute the zero-sum games without using the information of stationary transition probability. Besides, the matrices of system dynamics can also be partially unknown, and the new architecture requires less transmission and computation during the learning process. The stochastic stability of the system dynamics under the developed overall resilient control is guaranteed based on the Lyapunov theory. Finally, the designed IRL-based resilient control is applied to a typical multimode robot arm system, and implementing results demonstrate the practicality and effectiveness.",
        "DOI": "10.1109/TCYB.2021.3050619",
        "paper_author": "Zhang K.",
        "affiliation_name": "Academy of Mathematics and System Sciences Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60003707",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Online Reinforcement Learning Control by Direct Heuristic Dynamic Programming: From Time-Driven to Event-Driven",
        "publication": "IEEE Transactions on Neural Networks and Learning Systems",
        "citied_by": "14",
        "cover_date": "2022-08-01",
        "Abstract": "In this work, time-driven learning refers to the machine learning method that updates parameters in a prediction model continuously as new data arrives. Among existing approximate dynamic programming (ADP) and reinforcement learning (RL) algorithms, the direct heuristic dynamic programming (dHDP) has been shown an effective tool as demonstrated in solving several complex learning control problems. It continuously updates the control policy and the critic as system states continuously evolve. It is therefore desirable to prevent the time-driven dHDP from updating due to insignificant system event such as noise. Toward this goal, we propose a new event-driven dHDP. By constructing a Lyapunov function candidate, we prove the uniformly ultimately boundedness (UUB) of the system states and the weights in the critic and the control policy networks. Consequently, we show the approximate control and cost-to-go function approaching Bellman optimality within a finite bound. We also illustrate how the event-driven dHDP algorithm works in comparison to the original time-driven dHDP.",
        "DOI": "10.1109/TNNLS.2021.3053037",
        "paper_author": "Zhao Q.",
        "affiliation_name": "Beijing Institute of Technology",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60016835",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Frame-Correlation Transfers Trigger Economical Attacks on Deep Reinforcement Learning Policies",
        "publication": "IEEE Transactions on Cybernetics",
        "citied_by": "12",
        "cover_date": "2022-08-01",
        "Abstract": "Adversarial attack can be deemed as a necessary prerequisite evaluation procedure before the deployment of any reinforcement learning (RL) policy. Most existing approaches for generating adversarial attacks are gradient based and are extensive, viz., perturbing every pixel of every frame. In contrast, recent advances show that gradient-free selective perturbations (i.e., attacking only selected pixels and frames) could be a more realistic adversary. However, these attacks treat every frame in isolation, ignoring the relationship between neighboring states of a Markov decision process; thus resulting in high computational complexity that tends to limit their real-world plausibility due to the tight time constraint in RL. Given the above, this article showcases the first study of how transferability across frames could be exploited for boosting the creation of minimal yet powerful attacks in image-based RL. To this end, we introduce three types of frame-correlation transfers (FCTs) (i.e., anterior case transfer, random projection-based transfer, and principal components-based transfer) with varying degrees of computational complexity in generating adversaries via a genetic algorithm. We empirically demonstrate the tradeoff between the complexity and potency of the transfer mechanism by exploring four fully trained state-of-the-art policies on six Atari games. Our FCTs dramatically speed up the attack generation compared to existing methods, often reducing the computation time required to nearly zero; thus, shedding light on the real threat of real-time attacks in RL.",
        "DOI": "10.1109/TCYB.2020.3041265",
        "paper_author": "Qu X.",
        "affiliation_name": "School of Computer Science and Engineering",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore",
        "affiliation_id": "60078616",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "The Impact of Green Finance on the Ecologicalization of Urban Industrial Structure—Based on GMM of Dynamic Panel System",
        "publication": "Journal of Artificial Intelligence and Technology",
        "citied_by": "18",
        "cover_date": "2022-07-29",
        "Abstract": "Although a number of studies have been published in the general area on various factors affecting the ecologicalization of urban industrial structure, little work has been carried out for empirical studies quantitatively analyzing the relevance between green finance development and the ecologicalization of urban industrial structure. Therefore, based on a comprehensive index of green finance development, this research employs panel data of target cities1 for the period 2012–2020 to explore the influence of green finance on the ecologicalization of urban industrial structure. The empirical results show that green finance development significantly improves the ecologicalization level of urban industrial structure. In addition, it is found that green finance plays a stronger role in promoting the ecologicalization of industrial structure in economically developed regions than in economically underdeveloped regions. The research results can provide a valuable policy reference for urban green financial market planning and green product innovation.",
        "DOI": "10.37965/jait.2022.0115",
        "paper_author": "Lin K.",
        "affiliation_name": "Qilu University of Technology",
        "affiliation_city": "Jinan",
        "affiliation_country": "China",
        "affiliation_id": "60011592",
        "affiliation_state": "Shandong"
    },
    {
        "paper_title": "A Study of Dementia Prediction Models Based on Machine Learning with Survey Data of Community-Dwelling Elderly People in China",
        "publication": "Journal of Alzheimer's Disease",
        "citied_by": "1",
        "cover_date": "2022-07-29",
        "Abstract": "Background: For community-dwelling elderly individuals without enough clinical data, it is important to develop a method to predict their dementia risk and identify risk factors for the formulation of reasonable public health policies to prevent dementia. Objective: A community elderly survey data was used to establish machine learning prediction models for dementia and analyze the risk factors. Methods: In a cluster-sample community survey of 9,387 elderly people in 5 subdistricts of Wuxi City, data on sociodemographics and neuropsychological self-rating scales for depression, anxiety, and cognition evaluation were collected. Machine learning models were developed to predict their dementia risk and identify risk factors. Results: The random forest model (AUC = 0.686) had slightly better dementia prediction performance than logistic regression model (AUC = 0.677) and neural network model (AUC = 0.664). The sociodemographic data and psychological evaluation revealed that depression (OR = 3.933, 95% CI = 2.995-5.166); anxiety (OR = 2.352, 95% CI = 1.577-3.509); multiple physical diseases (OR = 2.486, 95% CI = 1.882-3.284 for three or above); 'disability, poverty or no family member' (OR = 1.859, 95% CI = 1.337-2.585) and 'empty nester' (OR = 1.339, 95% CI = 1.125-1.595) in special family status; 'no spouse now' (OR = 1.567, 95% CI = 1.118-2.197); age older than 80 years (OR = 1.645, 95% CI = 1.335-2.026); and female (OR = 1.214, 95% CI = 1.048-1.405) were risk factors for suspected dementia, while a higher education level (OR = 0.365, 95% CI = 0.245-0.546 for college or above) was a protective factor. Conclusion: The machine learning models using sociodemographic and psychological evaluation data from community surveys can be used as references for the prevention and control of dementia in large-scale community populations and the formulation of public health policies.",
        "DOI": "10.3233/JAD-220316",
        "paper_author": "Xu Q.",
        "affiliation_name": "Nanjing Medical University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60018310",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "Healthy and sustainable development of sports economy based on artificial intelligence and mental model",
        "publication": "Frontiers in Psychology",
        "citied_by": "1",
        "cover_date": "2022-07-28",
        "Abstract": "In recent years, sports have achieved rapid development worldwide, and the global economy has been significantly improved and improved. With the in-depth development of the two, the connection between sports and the economy has also become closer. Sports economy is a new type of economic form bred by specialization of sports organization, participation in consumerization, and profit-oriented operation under the condition of market economy. And the development of sports economy cannot be developed at once; it needs healthy and sustainable development. In order to find a better way to study the healthy and sustainable development of sports economy, this paper uses deep learning network algorithm and supports vector machine learning algorithm to build a mental model. It then uses the model to analyze various indicators of the sports industry in a province in China. This article is looking for information and summarizes the province’s sports data from 2017 to 2021. The sports indicators of this experiment include regional GDP, total output of sports industry, sports practitioners, local financial sports expenditures, the number of policies, the number of people participating in physical exercise, and fitness venues and facilities. The realization results show that these variables develop at a relatively small rate under normal conditions, and then predict the data in the next few years under the healthy and sustainable development of the next few years through the mental model. The growth rates of various indicators of the sports economy have increased significantly, and they have been optimized by about 20% compared with the normal development.",
        "DOI": "10.3389/fpsyg.2022.956682",
        "paper_author": "Liu Y.",
        "affiliation_name": "Liaoning Normal University",
        "affiliation_city": "Dalian",
        "affiliation_country": "China",
        "affiliation_id": "60017010",
        "affiliation_state": "Liaoning"
    },
    {
        "paper_title": "Towards Robust Off-Policy Evaluation via Human Inputs",
        "publication": "AIES 2022 - Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",
        "citied_by": "0",
        "cover_date": "2022-07-26",
        "Abstract": "Off-policy Evaluation (OPE) methods are crucial tools for evaluating policies in high-stakes domains such as healthcare, where direct deployment is often infeasible, unethical, or expensive. When deployment environments are expected to undergo changes (that is, dataset shifts), it is important for OPE methods to perform robust evaluation of the policies amidst such changes. Existing approaches consider robustness against a large class of shifts that can arbitrarily change any observable property of the environment. This often results in highly pessimistic estimates of the utilities, thereby invalidating policies that might have been useful in deployment. In this work, we address the aforementioned problem by investigating how domain knowledge can help provide more realistic estimates of the utilities of policies. We leverage human inputs on which aspects of the environments may plausibly change, and adapt the OPE methods to only consider shifts on these aspects. Specifically, we propose a novel framework, Robust OPE (ROPE), which considers shifts on a subset of covariates in the data based on user inputs, and estimates worst-case utility under these shifts. We then develop computationally efficient algorithms for OPE that are robust to the aforementioned shifts for contextual bandits and Markov decision processes. We also theoretically analyze the sample complexity of these algorithms. Extensive experimentation with synthetic and real world datasets from the healthcare domain demonstrates that our approach not only captures realistic dataset shifts accurately, but also results in less pessimistic policy evaluations.",
        "DOI": "10.1145/3514094.3534198",
        "paper_author": "Singh H.",
        "affiliation_name": "New York University",
        "affiliation_city": "New York",
        "affiliation_country": "United States",
        "affiliation_id": "60021784",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Achievement and Fragility of Long-Term Equitability",
        "publication": "AIES 2022 - Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",
        "citied_by": "1",
        "cover_date": "2022-07-26",
        "Abstract": "Equipping current decision-making tools with notions of fairness, equitability, or other ethically motivated outcomes, is one of the top priorities in recent research efforts in machine learning, AI, and optimization. In this paper, we investigate how to allocate limited resources to locally interacting communities in a way to maximize a pertinent notion of equitability. In particular, we look at the dynamic setting where the allocation is repeated across multiple periods (e.g., yearly), the local communities evolve in the meantime (driven by the provided allocation), and the allocations are modulated by feedback coming from the communities themselves. We employ recent mathematical tools stemming from data-driven feedback online optimization, by which communities can learn their (possibly unknown) evolution, satisfaction, as well as they can share information with the deciding bodies. We design dynamic policies that converge to an allocation that maximize equitability in the long term. We further demonstrate our model and methodology with realistic examples of healthcare and education subsidies design in Sub-Saharian countries. One of the key empirical takeaways from our setting is that long-Term equitability is fragile, in the sense that it can be easily lost when deciding bodies weigh in other factors (e.g., equality in allocation) in the allocation strategy. Moreover, a naive compromise, while not providing significant advantage to the communities, can promote inequality in social outcomes.",
        "DOI": "10.1145/3514094.3534132",
        "paper_author": "Simonetto A.",
        "affiliation_name": "ENSTA",
        "affiliation_city": "Palaiseau",
        "affiliation_country": "France",
        "affiliation_id": "60288614",
        "affiliation_state": "Ile-de-France"
    },
    {
        "paper_title": "Examining Responsibility and Deliberation in AI Impact Statements and Ethics Reviews",
        "publication": "AIES 2022 - Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",
        "citied_by": "7",
        "cover_date": "2022-07-26",
        "Abstract": "The artificial intelligence research community is continuing to grapple with the ethics of its work by encouraging researchers to discuss potential positive and negative consequences. Neural Information Processing Systems (NeurIPS), a top-Tier conference for machine learning and artificial intelligence research, first required a statement of broader impact in 2020. In 2021, NeurIPS updated their call for papers such that 1) the impact statement focused on negative societal impacts and was not required but encouraged, 2) a paper checklist and ethics guidelines were provided to authors, and 3) papers underwent ethics reviews and could be rejected on ethical grounds. In light of these changes, we contribute a qualitative analysis of 231 impact statements and all publicly-Available ethics reviews. We describe themes arising around the ways in which authors express agency (or lack thereof) in identifying or mitigating negative consequences and assign responsibility for mitigating negative societal impacts. We also characterize ethics reviews in terms of the types of issues raised by ethics reviewers (falling into categories of policy-oriented and non-policy-oriented), recommendations ethics reviewers make to authors (e.g., in terms of adding or removing content), and interaction between authors, ethics reviewers, and original reviewers (e.g., consistency between issues flagged by original reviewers and those discussed by ethics reviewers). Finally, based on our analysis we make recommendations for how authors can be further supported in engaging with the ethical implications of their work.",
        "DOI": "10.1145/3514094.3534155",
        "paper_author": "Liu D.",
        "affiliation_name": "Northeastern University",
        "affiliation_city": "Boston",
        "affiliation_country": "United States",
        "affiliation_id": "60028628",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Generating Deontic Obligations from Utility-Maximizing Systems",
        "publication": "AIES 2022 - Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",
        "citied_by": "0",
        "cover_date": "2022-07-26",
        "Abstract": "This work gives a logical characterization of the (ethical and social) obligations of an agent trained with Reinforcement Learning (RL). An RL agent takes actions by following a utility-maximizing policy. We maintain that the choice of utility function embeds ethical and social values implicitly, and that it is necessary to make these values explicit. This work provides a basis for doing so. First, we propose a probabilistic deontic logic that is suited for formally specifying the obligations of a stochastic system, including its ethical obligations. We prove some useful validities about this logic, and how its semantics are compatible with those of Markov Decision Processes (MDPs). Second, we show that model checking allows us to prove that an agent has a given obligation to bring about some state of affairs-meaning that by acting optimally, it is seeking to reach that state of affairs. We develop a model checker for our logic against MDPs. Third, we observe that it is useful for a system designer to obtain a logical characterization of her system's obligations, which is potentially more interpretable and helpful in debugging than the expression of a utility function. Enumerating all the obligations of an agent is impractical, so we propose a Bayesian optimization routine that learns to generate a system's obligations that the system designer deems interesting. We implement the model checking and Bayesian optimization routines, and demonstrate their effectiveness with an initial pilot study. This work provides a rigorous method to characterize utility-maximizing agents in terms of the (ethical and social) obligations that they implicitly seek to satisfy.",
        "DOI": "10.1145/3514094.3534163",
        "paper_author": "Shea-Blymyer C.",
        "affiliation_name": "Oregon State University",
        "affiliation_city": "Corvallis",
        "affiliation_country": "United States",
        "affiliation_id": "60013402",
        "affiliation_state": "OR"
    },
    {
        "paper_title": "Equalizing credit opportunity in algorithms: Aligning algorithmic fairness research with U.S. fair lending regulation",
        "publication": "AIES 2022 - Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",
        "citied_by": "11",
        "cover_date": "2022-07-26",
        "Abstract": "Credit is an essential component of financial wellbeing in America, and unequal access to it is a large factor in the economic disparities between demographic groups that exist today. Today, machine learning algorithms, sometimes trained on alternative data, are increasingly being used to determine access to credit, yet research has shown that machine learning can encode many different versions of \"unfairness,\"thus raising the concern that banks and other financial institutions could-potentially unwittingly-engage in illegal discrimination through the use of this technology. In the US, there are laws in place to make sure discrimination does not happen in lending and agencies charged with enforcing them. However, conversations around fair credit models in computer science and in policy are often misaligned: fair machine learning research often lacks legal and practical considerations specific to existing fair lending policy, and regulators have yet to issue new guidance on how, if at all, credit risk models should be utilizing practices and techniques from the research community. This paper aims to better align these sides of the conversation. We describe the current state of credit discrimination regulation in the United States, contextualize results from fair ML research to identify the specific fairness concerns raised by the use of machine learning in lending, and discuss regulatory opportunities to address these concerns.",
        "DOI": "10.1145/3514094.3534154",
        "paper_author": "Kumar I.E.",
        "affiliation_name": "Brown University",
        "affiliation_city": "Providence",
        "affiliation_country": "United States",
        "affiliation_id": "60011460",
        "affiliation_state": "RI"
    },
    {
        "paper_title": "Fair, Robust, and Data-Efficient Machine Learning in Healthcare",
        "publication": "AIES 2022 - Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",
        "citied_by": "1",
        "cover_date": "2022-07-26",
        "Abstract": "While machine learning systems have shown improvements, often, in carefully curated settings, challenges still exist to their wider deployment, especially for making consequential decisions. The research described here explores three challenges, particularly, emphasizing the interesting issues that arise at their intersection. How do we design machine learning systems to account for the systemic biases of the world, to act reliably under unseen settings, and to handle limited availability of data? Human-facing applications of machine learning such as personalized health commonly encounter these challenges, thus, these are important to address. The research has three components addressing different parts of the above central question. Here, we describe the work done on two components of the above central question and highlight the future work planned as part of the third one. We draw from methods in causal inference, algorithmic fairness, and interactive learning, and apply them to applications in health.",
        "DOI": "10.1145/3514094.3539552",
        "paper_author": "Singh H.",
        "affiliation_name": "New York University",
        "affiliation_city": "New York",
        "affiliation_country": "United States",
        "affiliation_id": "60021784",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Stock Market Forecasting Using the Random Forest and Deep Neural Network Models Before and During the COVID-19 Period",
        "publication": "Frontiers in Environmental Science",
        "citied_by": "17",
        "cover_date": "2022-07-25",
        "Abstract": "Stock market forecasting is considered the most challenging problem to solve for analysts. In the past 2 years, Covid-19 has severely affected stock markets globally, which, in turn, created a great problem for investors. The prime objective of this study is to use a machine learning model to effectively forecast stock index prices in three time frames: the whole period, the pre-Covid-19 period, and the Covid-19 period. The model accuracy testing results of mean absolute error, root mean square error, mean absolute percentage error, and r2 suggest that the proposed machine learning models autoregressive deep neural network (AR-DNN(1, 3, 10)), autoregressive deep neural network (AR-DNN(3, 3, 10)), and autoregressive random forest (AR-RF(1)) are the best forecasting models for stock index price forecasting for the whole period, for the pre-Covid-19 period, and during the Covid-19 period, respectively, under high stock price fluctuations compared to traditional time-series forecasting models such as autoregressive moving average models. In particular, AR-DNN(1, 3, 10) is suggested when the number of observations is large, whereas AR-RF(1) is suggested for a series with a low number of observations. Our study has a practical implication as they can be used by investors and policy makers in their investment decisions and in formulating financial decisions and policies, respectively.",
        "DOI": "10.3389/fenvs.2022.917047",
        "paper_author": "Omar A.B.",
        "affiliation_name": "National College of Business Administration and Economics",
        "affiliation_city": "Lahore",
        "affiliation_country": "Pakistan",
        "affiliation_id": "100614449",
        "affiliation_state": "Punjab"
    },
    {
        "paper_title": "DEVELOPMENT of GEOSPATIAL INFORMATION INTEGRATED with BIG DATA to AGRICULTURAL HAZARD MONITORING in WEST JAVA",
        "publication": "International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",
        "citied_by": "4",
        "cover_date": "2022-07-25",
        "Abstract": "Food security is highly dependent on three aspects, namely food availability, food access, and food utilization. The availability aspect depends on food supply which is identical to agricultural productivity. West Java Province is the third national rice producer with 16.6%, but West Java Province is the most extensive rice consumer, around 21.1% of the total national rice consumption. Agricultural productivity can decline due to natural hazards such as floods and droughts. Monitoring floods and droughts in paddy fields are necessary to prevent decreased agricultural productivity. This study aims to monitor the rice fields from the dangers of flooding and drought every month. Agricultural hazard monitoring is divided into two parameters, namely static parameters and dynamic parameters. Dynamic parameters are observed every month so that the hazard index is generated on a monthly scale. GIS and Remote sensing data are integrated to perform agricultural hazard modelling. Furthermore, this agricultural hazard modelling results will be strengthened by using big to provide information about an almost real-Time event that can be accessed through the Application Program Interface (API) service. This study uses a data mining system from Drone Emprit that performs data mining on Twitter and news portals with machine learning technology (probabilistic classifier) and Natural Learning Process. The results obtained are around 15,000 data from January 1 to November 1, 2021, and 37.9% of them are identified by location based on the city or district level in West Java Province. It is hoped that the policy-maker can consider the area of agricultural land that requires assistance to increase productivity and plan a policy to support agriculture in West Java in the future.",
        "DOI": "10.5194/isprs-Archives-XLVI-M-2-2022-209-2022",
        "paper_author": "Virtriana R.",
        "affiliation_name": "Institut Teknologi Bandung",
        "affiliation_city": "Bandung",
        "affiliation_country": "Indonesia",
        "affiliation_id": "60069382",
        "affiliation_state": "West Java"
    },
    {
        "paper_title": "Spatially Explicit Seagrass Extent Mapping Across the Entire Mediterranean",
        "publication": "Frontiers in Marine Science",
        "citied_by": "25",
        "cover_date": "2022-07-22",
        "Abstract": "The seagrass Posidonia oceanica is the main habitat-forming species of the coastal Mediterranean, providing millennial-scale ecosystem services including habitat provisioning, biodiversity maintenance, food security, coastal protection, and carbon sequestration. Meadows of this endemic seagrass species represent the largest carbon storage among seagrasses around the world, largely contributing to global blue carbon stocks. Yet, the slow growth of this temperate species and the extreme projected temperature and sea-level rise due to climate change increase the risk of reduction and loss of these services. Currently, there are knowledge gaps in its basin-wide spatially explicit extent and relevant accounting, therefore accurate and efficient mapping of its distribution and trajectories of change is needed. Here, we leveraged contemporary advances in Earth Observation—cloud computing, open satellite data, and machine learning—with field observations through a cloud-native geoprocessing framework to account the spatially explicit ecosystem extent of P. oceanica seagrass across its full bioregional scale. Employing 279,186 Sentinel-2 satellite images between 2015 and 2019, and a human-labeled training dataset of 62,928 pixels, we mapped 19,020 km2 of P. oceanica meadows up to 25 m of depth in 22 Mediterranean countries, across a total seabed area of 56,783 km2. Using 2,480 independent, field-based points, we observe an overall accuracy of 72%. We include and discuss global and region-specific seagrass blue carbon stocks using our bioregional seagrass extent estimate. As reference data collections, remote sensing technology and biophysical modelling improve and coalesce, such spatial ecosystem extent accounts could further support physical and monetary accounting of seagrass condition and ecosystem services, like blue carbon and coastal biodiversity. We envisage that effective policy uptake of these holistic seagrass accounts in national climate strategies and financing could accelerate transparent natural climate solutions and coastal resilience, far beyond the physical location of seagrass beds.",
        "DOI": "10.3389/fmars.2022.871799",
        "paper_author": "Traganos D.",
        "affiliation_name": "Deutsches Zentrum für Luft- und Raumfahrt (DLR)",
        "affiliation_city": "Koln",
        "affiliation_country": "Germany",
        "affiliation_id": "60007798",
        "affiliation_state": "Nordrhein-Westfalen"
    },
    {
        "paper_title": "Mixed Impact of Climate Change on Cold Season Residential Electricity: A Case Study of Lanzhou and Lhasa",
        "publication": "Frontiers in Earth Science",
        "citied_by": "3",
        "cover_date": "2022-07-22",
        "Abstract": "Extreme weather induced by climate change has triggered large-scale power outages worldwide, particularly during the cold season. More insight into the climatic impacts (especially those of precipitation) on cold season residential electricity consumption (REC) is needed. This study quantified the climatic impacts on REC, with a focus on precipitation, and projected the associated changes under representative concentration pathways (RCPs) 2.6, 4.5, and 8.5 climate change scenarios in Lanzhou and Lhasa, two cities in China with distinctive cold season climates. The climatic impacts on REC in both cities are driven by heating-related demand. A stronger precipitation impact during the cold season was observed in both cities, since precipitation (particularly snowfall) boosts electricity consumption during the cold season. As the two cities become warmer and wetter, increased precipitation will offset the impact of warming on REC, with Lanzhou being more strongly affected. Based on the projections for Lanzhou, the offsetting effect will be more pronounced during the cold season across all scenarios, and will be particularly strong under RCP 2.6. For the remainder of the year, the effects of increased precipitation and warming will have competing importances under the RCP 4.5 scenario, whereas temperature effects will generally dominate the climatic impacts under the RCP 8.5 scenario. These results provide new insights for future cold season climate–energy studies and can be used to improve regional climate adaptation policies. We also propose a method to project climate-based REC changes which is compatible with potential early-warning projects to protect against extreme weather-induced power outages.",
        "DOI": "10.3389/feart.2022.908259",
        "paper_author": "Xia C.",
        "affiliation_name": "Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60019499",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Consumer Demand for Environmental, Social, and Ethical Information in Fishery and Aquaculture Product Labels",
        "publication": "Frontiers in Marine Science",
        "citied_by": "9",
        "cover_date": "2022-07-22",
        "Abstract": "Customers’ attention to sustainability labels in fishery and aquaculture products (FAPs) has been increasing in the last decades, and the industry has adapted to this growing interest by adopting fish ecolabels. However, there is a growing interest to widen the sustainability concept to include the social and ethical information of the fishery and aquaculture industry and to go further from the voluntary approach on the labeling of these aspects in FAPs. For this reason, using data from 2021 Eurobarometer and using machine learning techniques, we disentangle the characteristics of the FAP buyers that consider the importance of environmental impact, ethical, and social information appearing on FAP labeling. The results confirmed that most of the consumers who consider environmental, social, and ethical aspects when buying FAPs also think that this information should be labeled. In line with other works, young, educated, and environmentally aware consumers in high-income countries are more likely to request this information in the FAP label. One interesting finding of the study relates with the asymmetric impact of the variables and the important group of respondents who do not consider these aspects but also advocate to include them in the FAP label. The study outcomes can be beneficial for policymakers to design future public policies regarding FAP labeling, as well as to be taken into consideration in the marketing policies of fishery and aquaculture producers and retailers.",
        "DOI": "10.3389/fmars.2022.948437",
        "paper_author": "Peiró-Signes A.",
        "affiliation_name": "Universitat Politècnica de València",
        "affiliation_city": "Valencia",
        "affiliation_country": "Spain",
        "affiliation_id": "60011476",
        "affiliation_state": "Valencia"
    },
    {
        "paper_title": "Enhanced Weather-Based Index Insurance Design for Hedging Crop Yield Risk",
        "publication": "Frontiers in Plant Science",
        "citied_by": "1",
        "cover_date": "2022-07-22",
        "Abstract": "This study proposes an optimization-based weather-yield model to reduce the basis risk of weather-based index insurance. This weather-yield model helps us capture the growing season's monthly variation as it involves monthly explanatory weather indices. In addition, it can capture additional extreme weather effects by including extreme cooling or heating weather indices. This study presents an innovative machine learning framework incorporating optimization approaches to ensure the parsimony of weather index models and the accuracy of crop yield predictions, which can be integrated into the conventional policy design and pricing process. The advantages of this modeling approach and the effectiveness of weather index-based insurance based on this approach in reducing basis risk and revenue risk are demonstrated by applying county-level yield data for mid-season rice in the Anhui province, China.",
        "DOI": "10.3389/fpls.2022.895183",
        "paper_author": "Sun Y.",
        "affiliation_name": "Institute of Disaster Prevention Science and Technology",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60108798",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A novel data-driven multi-energy load forecasting model",
        "publication": "Frontiers in Energy Research",
        "citied_by": "9",
        "cover_date": "2022-07-22",
        "Abstract": "With the increasing concern on energy crisis, the coordination of multiple energy sources and low-carbon economic operation of integrated energy system (IES) have drawn more and more attention in recent years. In IES, accurate and effective multi-energy load forecasting becomes a research hotspot, especially using the high-performance data mining and machine learning algorithms. However, due to the huge difference in energy utilization between IES and traditional energy systems, the load forecasting of IES is more difficult and complex. In fact, in IES, load forecasting is not only related to external factors such as meteorological parameters and different seasons, but the correlation between energy consumption of different types of loads also plays an important role. In order to deal with the strong coupling and high uncertainty issues in IES, a novel data-driven multi-energy load forecasting model is proposed in this paper. Firstly, a feature extraction method based on Uniform Manifold Approximation and Projection (UMAP) for multi-energy load of the IES is developed, which reduces the dimension of the complex nonlinear input data. Then, considering multi-energy coupling correlation, a combined TCN-NBeats model is proposed for the joint prediction of multi-energy loads, aiming to improve the prediction accuracy through ensemble learning. Finally, the numerical case analysis using the multi-energy consumption data of an actual campus verifies the effectiveness and accuracy of the proposed data-driven multi-energy load forecasting model.",
        "DOI": "10.3389/fenrg.2022.955851",
        "paper_author": "Yao Y.",
        "affiliation_name": "Guangdong Energy Group Science and Technology Research Institute Co. Ltd.",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China",
        "affiliation_id": "128287266",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A spatiotemporal ensemble machine learning framework for generating land use/land cover time-series maps for Europe (2000-2019) based on LUCAS, CORINE and GLAD Landsat",
        "publication": "PeerJ",
        "citied_by": "23",
        "cover_date": "2022-07-21",
        "Abstract": "A spatiotemporal machine learning framework for automated prediction and analysis of long-term Land Use/Land Cover dynamics is presented. The framework includes: (1) harmonization and preprocessing of spatial and spatiotemporal input datasets (GLAD Landsat, NPP/VIIRS) including five million harmonized LUCAS and CORINE Land Cover-derived training samples, (2) model building based on spatial k-fold cross-validation and hyper-parameter optimization, (3) prediction of the most probable class, class probabilities and model variance of predicted probabilities per pixel, (4) LULC change analysis on time-series of produced maps. The spatiotemporal ensemble model consists of a random forest, gradient boosted tree classifier, and an artificial neural network, with a logistic regressor as meta-learner. The results show that the most important variables for mapping LULC in Europe are: seasonal aggregates of Landsat green and near-infrared bands, multiple Landsat-derived spectral indices, long-term surface water probability, and elevation. Spatial cross-validation of the model indicates consistent performance across multiple years with overall accuracy (a weighted F1-score) of 0.49, 0.63, and 0.83 when predicting 43 (level-3), 14 (level-2), and five classes (level-1). Additional experiments show that spatiotemporal models generalize better to unknown years, outperforming single-year models on known-year classification by 2.7% and unknown-year classification by 3.5%. Results of the accuracy assessment using 48,365 independent test samples shows 87% match with the validation points. Results of time-series analysis (time-series of LULC probabilities and NDVI images) suggest forest loss in large parts of Sweden, the Alps, and Scotland. Positive and negative trends in NDVI in general match the land degradation and land restoration classes, with ''urbanization'' showing the most negative NDVI trend. An advantage of using spatiotemporal ML is that the fitted model can be used to predict LULC in years that were not included in its training dataset, allowing generalization to past and future periods, e.g. to predict LULC for years prior to 2000 and beyond 2020. The generated LULC time-series data stack (ODSE-LULC), including the training points, is publicly available via the ODSE Viewer. Functions used to prepare data and run modeling are available via the eumap library for Python.",
        "DOI": "10.7717/peerj.13573",
        "paper_author": "Witjes M.",
        "affiliation_name": "OpenGeoHub Foundation",
        "affiliation_city": "Wageningen",
        "affiliation_country": "Netherlands",
        "affiliation_id": "128065480",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Africa: regulate surveillance technologies and personal data",
        "publication": "Nature",
        "citied_by": "5",
        "cover_date": "2022-07-21",
        "Abstract": "CCTV cameras and spyware are proliferating in the continent without checks and balances. Governments must legislate locally to prevent civil-rights abuses.",
        "DOI": "10.1038/d41586-022-01949-9",
        "paper_author": "Jili B.",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Sustainable Participatory Governance: Data-Driven Discovery of Parameters for Planning Online and In-Class Education in Saudi Arabia During COVID-19",
        "publication": "Frontiers in Sustainable Cities",
        "citied_by": "16",
        "cover_date": "2022-07-19",
        "Abstract": "Everything about our life is complex. It should not be so. New approaches to governance are needed to tackle these complexities and the rising global challenges. Smartization of cities and societies has the potential to unite us, humans, on a sustainable future for us through its focus on the triple bottom line (TBL) – social, environmental, and economic sustainability. Data-driven analytics are at the heart of this smartization. This study provides a case study on sustainable participatory governance using a data-driven parameter discovery for planning online, in-class, and blended learning in Saudi Arabia evidenced during the COVID-19 pandemic. For this purpose, we developed a software tool comprising a complete machine learning pipeline and used a dataset comprising around 2 million tweets in the Arabic language collected during a period of over 14 months (October 2020 to December 2021). We discovered fourteen governance parameters grouped into four governance macro parameters. These discovered parameters by the tool demonstrate the possibility and benefits of our sustainable participatory planning and governance approach, allowing the discovery and grasp of important dimensions of the education sector in Saudi Arabia, the complexity of the policy, the procedural and practical issues in continuing learning during the pandemic, the factors that have contributed to the success of teaching and learning during the pandemic times, both its transition to online learning and its return to in-class learning, the challenges public and government have faced related to learning during the pandemic times, and the new opportunities for social, economical, and environmental benefits that can be drawn out of the situation created by the pandemic. The parameters and information learned through the tool can allow governments to have a participatory approach to governance and improve their policies, procedures, and practices, perpetually through public and stakeholder feedback. The data-driven parameter discovery approach we propose is generic and can be applied to the governance of any sector. The specific case study is used to elaborate on the proposed approach.",
        "DOI": "10.3389/frsc.2022.871171",
        "paper_author": "Alswedani S.",
        "affiliation_name": "King Abdulaziz University",
        "affiliation_city": "Jeddah",
        "affiliation_country": "Saudi Arabia",
        "affiliation_id": "60004582",
        "affiliation_state": "Makkah al Mukarramah"
    },
    {
        "paper_title": "Data Science for Advancing Environmental Science, Engineering, and Technology: Upcoming Special and Virtual Issues in ES &amp;T and ES &amp;T Letters",
        "publication": "Environmental Science and Technology",
        "citied_by": "4",
        "cover_date": "2022-07-19",
        "Abstract": "NA",
        "DOI": "10.1021/acs.est.2c03735",
        "paper_author": "Lowry G.V.",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Can Robots Understand Welfare? Exploring Machine Bureaucracies in Welfare-to-Work",
        "publication": "Journal of Social Policy",
        "citied_by": "36",
        "cover_date": "2022-07-16",
        "Abstract": "The exercise of administrative discretion by street-level workers plays a key role in shaping citizens' access to welfare and employment services. Governance reforms of social services delivery, such as performance-based contracting, have often been driven by attempts to discipline this discretion. In several countries, these forms of market governance are now being eclipsed by new modes of digital governance that seek to reshape the delivery of services using algorithms and machine learning. Australia, a pioneer of marketisation, is one example, proposing to deploy digitalisation to fully automate most of its employment services rather than as a supplement to face-to-face case management. We examine the potential and limits of this project to replace human-to-human with 'machine bureaucracies'. To what extent are welfare and employment services amenable to digitalisation? What trade-offs are involved? In addressing these questions, we consider the purported benefits of machine bureaucracies in achieving higher levels of efficiency, accountability, and consistency in policy delivery. While recognising the potential benefits of machine bureaucracies for both governments and jobseekers, we argue that trade-offs will be faced between enhancing the efficiency and consistency of services and ensuring that services remain accessible and responsive to highly personalised circumstances.",
        "DOI": "10.1017/S0047279422000174",
        "paper_author": "Considine M.",
        "affiliation_name": "School of Social and Political Sciences",
        "affiliation_city": "Melbourne",
        "affiliation_country": "Australia",
        "affiliation_id": "60118560",
        "affiliation_state": "VIC"
    },
    {
        "paper_title": "Informed Deep Learning for Epidemics Forecasting",
        "publication": "Frontiers in Artificial Intelligence and Applications",
        "citied_by": "0",
        "cover_date": "2022-07-15",
        "Abstract": "The SARS-CoV-2 pandemic has galvanized the interest of the scientific community toward methodologies apt at predicting the trend of the epidemiological curve, namely, the daily number of infected individuals in the population. One of the critical issues, is providing reliable predictions based on interventions enacted by policy-makers, which is of crucial relevance to assess their effectiveness. In this paper, we provide a novel data-driven application incorporating sub-symbolic knowledge to forecast the spreading of an epidemic depending on a set of interventions. More specifically, we focus on the embedding of classical epidemiological approaches, i.e., compartmental models, into Deep Learning models, to enhance the learning process and provide higher predictive accuracy.",
        "DOI": "10.3233/FAIA220067",
        "paper_author": "Baldo F.",
        "affiliation_name": "Alma Mater Studiorum Università di Bologna",
        "affiliation_city": "Bologna",
        "affiliation_country": "Italy",
        "affiliation_id": "60028218",
        "affiliation_state": "BO"
    },
    {
        "paper_title": "Automated design of phononic crystals under thermoelastic wave propagation through deep reinforcement learning",
        "publication": "Engineering Structures",
        "citied_by": "24",
        "cover_date": "2022-07-15",
        "Abstract": "This article presents a novel concept of deep reinforcement learning (DRL) to facilitate the reverse design of layered phononic crystal (PC) beams with anticipated band structures focusing on the band structure analysis of thermoelastic waves propagating. To this end, we define the reverse design of phononic crystals (PCs) as a game for the DRL agent. To achieve the desired band structure, the DRL agent needs to obtain the topological system of PC. We trained a DRL agent called deep deterministic policy gradient (DDPG). An environment is developed and used to simulate the reverse design of layered PCs with the acquisition of a reward function. The presented reward function encourages the agent to achieve the desired bandgaps. The trained DDPG agent can maximize the game's score by attaining the desired bandgap. The presented concept allows the user to instantly generate the design parameters through the trained DDPG agent without unnecessary search over the design space. We demonstrated that the DRL agent could perform very well for the automated design of PCs with hundred design cases.",
        "DOI": "10.1016/j.engstruct.2022.114385",
        "paper_author": "Maghami A.",
        "affiliation_name": "Ferdowsi University of Mashhad",
        "affiliation_city": "Mashhad",
        "affiliation_country": "Iran",
        "affiliation_id": "60001800",
        "affiliation_state": "Razavi Khorasan"
    },
    {
        "paper_title": "AUV path tracking with real-time obstacle avoidance via reinforcement learning under adaptive constraints",
        "publication": "Ocean Engineering",
        "citied_by": "34",
        "cover_date": "2022-07-15",
        "Abstract": "In this paper, the methods are proposed for underactuated autonomous underwater vehicle (AUV) to address three-dimensional (3D) path tracking and real-time obstacle avoidance. The errors of path tracking are generated based on the Serret–Frenet frame and line-of-sight (LOS) guidance law, while the errors in obstacle avoidance are obtained based on the carrier coordinate system to filter irrelevant environment information. On this basis, to deal with the complicated target path and unknown obstacles, the controller is designed by deep deterministic policy gradient (DDPG) algorithm and adaptive multi-constraints. The safety constraints are adopted in reward functions to avoid useless explorations and facilitate convergence. The training proceeds for path tracking and obstacle avoidance respectively. Compared to the original DDPG algorithm in the training, the proposed algorithm shows faster convergence. Various simulations are conducted under different initial conditions, and the results demonstrate the effectiveness of the proposed algorithm.",
        "DOI": "10.1016/j.oceaneng.2022.111453",
        "paper_author": "Zhang C.",
        "affiliation_name": "Shanghai Jiao Tong University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60025084",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Valuation of compound real options for co-investment in residential battery systems",
        "publication": "Applied Energy",
        "citied_by": "10",
        "cover_date": "2022-07-15",
        "Abstract": "The process of evaluating investments by electricity utilities to subsidize residential battery installations in distribution networks is challenging, because finding efficient investment plans involves reasoning over various sources of uncertainty and consideration of different sizing, location and timing options. This work proposes a novel financial framework using real options valuation (ROV) to solve the optimal investment problem for subsidizing residential battery installations, considering compound options including the option to delay the investment and its subsequent option (expand). We incorporate random variations in PV generation and load behaviour, making use of Monte Carlo (MC) power flow analysis that incorporates battery scheduling profiles. Specifically, the ROV framework solves stochastic power flow analysis with battery scheduling in a forward-looking MC model, and determines the optimal plan for executing the options that maximize the investment value using backward induction over all MC paths. However, this approach to MC becomes computationally prohibitive when battery operational profiles are generated by optimization-based home energy management. To reduce the computational burden, we use policy function approximation to provide fast battery operational profiles, drawing on machine learning methods that reduce MC computation time by 95%. Compared to standard financial analysis, the proposed model allows ROV to capture all uncertainties while allowing utilities to make contingent decisions to maximize the benefits from subsidizing residential battery storage.",
        "DOI": "10.1016/j.apenergy.2022.119111",
        "paper_author": "Ma Y.",
        "affiliation_name": "Faculty of Engineering",
        "affiliation_city": "Sydney",
        "affiliation_country": "Australia",
        "affiliation_id": "60090755",
        "affiliation_state": "NSW"
    },
    {
        "paper_title": "Improving neural network classification of indigenous forest in New Zealand with phenological features",
        "publication": "Journal of Environmental Management",
        "citied_by": "3",
        "cover_date": "2022-07-15",
        "Abstract": "Accurate and up-to-date land cover maps inform and support effective management and policy decisions. Describing phenological changes in spectral response using time-series data may help to distinguish vegetation types, thereby allowing for more specificity within vegetation classification. In this research, we test this by classifying indigenous forest vegetation in New Zealand, using PlanetScope (PS) and Sentinel-2 (S-2) satellite time-series data. The study was undertaken in a podocarp forest in New Zealand's central north island, which was classified into nine land cover classes. Phenological features, based on S-2 imagery, were extracted, including the enhanced vegetation index (EVI), enhanced vegetation index 2 (EVI2) and normalised difference vegetation index (NDVI). Google Earth Engine (GEE) harmonic analysis and TIMESAT double logistic fitting function were used to extract phenological features. Pixel-based classifications were performed using a Neural Network on six different scenarios. The accuracy of the classification scenarios was determined and the importance score for each feature was evaluated. Using only the fused PS and S-2 bands, the land cover in the study area was classified with 90.1% accuracy. Adding phenological features increased the classification accuracy to 93.1%. When combined with VIs, texture features, and a digital terrain model, the addition of phenological features increased the classification accuracy to 96.6%. Including GEE-generated phenological features resulted in better classification accuracies than TIMESAT features. In terms of feature importance evaluation, EVI2- and NDVI-generated phenological features all had high scores; the effectiveness of EVI features could potentially have been limited by the quality of the blue band. The results demonstrate that it is possible to produce a more accurate classification of New Zealand's native vegetation by using phenological features. This method offers important cost-savings as the platforms for phenological analysis are free to use.",
        "DOI": "10.1016/j.jenvman.2022.115134",
        "paper_author": "Ye N.",
        "affiliation_name": "University of Canterbury",
        "affiliation_city": "Christchurch",
        "affiliation_country": "New Zealand",
        "affiliation_id": "60020585",
        "affiliation_state": "CAN"
    },
    {
        "paper_title": "Measurements and determinants of extreme multidimensional energy poverty using machine learning",
        "publication": "Energy",
        "citied_by": "32",
        "cover_date": "2022-07-15",
        "Abstract": "The contribution of this study is twofold. First, it calculates the depth, intensity, and degrees of energy poverty in developing countries using a multidimensional approach. The data analysis of 59 developing countries of Asia and Africa confirmed a widespread ‘severe’ energy poverty across multiple dimensions. The results revealed that Afghanistan, Yemen, Nepal, India, Bangladesh, and the Philippines in Asia and DR Congo, Chad, Madagascar, Niger, Sierre Leone, Tanzania, and Burundi in Africa were the most susceptible countries to extreme multidimensional energy poverty. Second, the study employed supervised machine learning algorithms to identify the most pertinent socioeconomic determinants of extreme multidimensional energy poverty in the developing world. The results of machine learning identified the accumulated wealth of a household, size and ownership status of a house, marital status of the main breadwinner, and place of residence of the main breadwinner to be the five most influential socioeconomic determinants of extreme multidimensional energy poverty. Therefore, the robust findings of an accurate assessment of extreme energy poverty and its socioeconomic determinants have policy significance to eradicate severe energy poverty by announcing additional incentives, allocating resources, and providing special assistance to those who are at the bottom.",
        "DOI": "10.1016/j.energy.2022.123977",
        "paper_author": "Abbas K.",
        "affiliation_name": "China University of Geosciences",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60006019",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Joint Cluster Head Selection and Trajectory Planning in UAV-Aided IoT Networks by Reinforcement Learning with Sequential Model",
        "publication": "IEEE Internet of Things Journal",
        "citied_by": "29",
        "cover_date": "2022-07-15",
        "Abstract": "Employing unmanned aerial vehicles (UAVs) has attracted growing interests and emerged as the state-of-The-Art technology for data collection in Internet of Things (IoT) networks. In this article, with the objective of minimizing the total energy consumption of the UAV-IoT system, we formulate the problem of jointly designing the UAV's trajectory and selecting cluster heads in the IoT network as a constrained combinatorial optimization problem, which is classified as NP-hard, and challenging to solve. We propose a novel deep reinforcement learning (DRL) with a sequential model strategy that can effectively learn the policy represented by a sequence-To-sequence neural network for the UAV's trajectory design in an unsupervised manner. Through extensive simulations, the obtained results show that the proposed DRL method can find the UAV's trajectory that requires much less energy consumption when compared to other baseline algorithms and achieves close-To-optimal performance. In addition, simulation results show that the trained model by our proposed DRL algorithm has an excellent generalization ability to larger problem sizes without the need to retrain the model.",
        "DOI": "10.1109/JIOT.2021.3133278",
        "paper_author": "Zhu B.",
        "affiliation_name": "University of Saskatchewan",
        "affiliation_city": "Saskatoon",
        "affiliation_country": "Canada",
        "affiliation_id": "60015186",
        "affiliation_state": "SK"
    },
    {
        "paper_title": "Potential Opportunities and Challenges of Deploying Next Generation Sequencing and CRISPR-Cas Systems to Support Diagnostics and Surveillance Towards Malaria Control and Elimination in Africa",
        "publication": "Frontiers in Cellular and Infection Microbiology",
        "citied_by": "13",
        "cover_date": "2022-07-13",
        "Abstract": "Recent developments in molecular biology and genomics have revolutionized biology and medicine mainly in the developed world. The application of next generation sequencing (NGS) and CRISPR-Cas tools is now poised to support endemic countries in the detection, monitoring and control of endemic diseases and future epidemics, as well as with emerging and re-emerging pathogens. Most low and middle income countries (LMICs) with the highest burden of infectious diseases still largely lack the capacity to generate and perform bioinformatic analysis of genomic data. These countries have also not deployed tools based on CRISPR-Cas technologies. For LMICs including Tanzania, it is critical to focus not only on the process of generation and analysis of data generated using such tools, but also on the utilization of the findings for policy and decision making. Here we discuss the promise and challenges of NGS and CRISPR-Cas in the context of malaria as Africa moves towards malaria elimination. These innovative tools are urgently needed to strengthen the current diagnostic and surveillance systems. We discuss ongoing efforts to deploy these tools for malaria detection and molecular surveillance highlighting potential opportunities presented by these innovative technologies as well as challenges in adopting them. Their deployment will also offer an opportunity to broadly build in-country capacity in pathogen genomics and bioinformatics, and to effectively engage with multiple stakeholders as well as policy makers, overcoming current workforce and infrastructure challenges. Overall, these ongoing initiatives will build the malaria molecular surveillance capacity of African researchers and their institutions, and allow them to generate genomics data and perform bioinformatics analysis in-country in order to provide critical information that will be used for real-time policy and decision-making to support malaria elimination on the continent.",
        "DOI": "10.3389/fcimb.2022.757844",
        "paper_author": "Lyimo B.M.",
        "affiliation_name": "National Institute for Medical Research Tanga",
        "affiliation_city": "Tanga",
        "affiliation_country": "Tanzania",
        "affiliation_id": "60071565",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Benchmarking Perturbation-Based Saliency Maps for Explaining Atari Agents",
        "publication": "Frontiers in Artificial Intelligence",
        "citied_by": "9",
        "cover_date": "2022-07-13",
        "Abstract": "One of the most prominent methods for explaining the behavior of Deep Reinforcement Learning (DRL) agents is the generation of saliency maps that show how much each pixel attributed to the agents' decision. However, there is no work that computationally evaluates and compares the fidelity of different perturbation-based saliency map approaches specifically for DRL agents. It is particularly challenging to computationally evaluate saliency maps for DRL agents since their decisions are part of an overarching policy, which includes long-term decision making. For instance, the output neurons of value-based DRL algorithms encode both the value of the current state as well as the expected future reward after doing each action in this state. This ambiguity should be considered when evaluating saliency maps for such agents. In this paper, we compare five popular perturbation-based approaches to create saliency maps for DRL agents trained on four different Atari 2,600 games. The approaches are compared using two computational metrics: dependence on the learned parameters of the underlying deep Q-network of the agents (sanity checks) and fidelity to the agents' reasoning (input degradation). During the sanity checks, we found that a popular noise-based saliency map approach for DRL agents shows little dependence on the parameters of the output layer. We demonstrate that this can be fixed by tweaking the algorithm such that it focuses on specific actions instead of the general entropy within the output values. For fidelity, we identify two main factors that influence which saliency map approach should be chosen in which situation. Particular to value-based DRL agents, we show that analyzing the agents' choice of action requires different saliency map approaches than analyzing the agents' state value estimation.",
        "DOI": "10.3389/frai.2022.903875",
        "paper_author": "Huber T.",
        "affiliation_name": "Universität Augsburg",
        "affiliation_city": "Augsburg",
        "affiliation_country": "Germany",
        "affiliation_id": "60016060",
        "affiliation_state": "Bayern"
    },
    {
        "paper_title": "Random Forest Classification Method for Predicting Intertidal Wetland Migration Under Sea Level Rise",
        "publication": "Frontiers in Environmental Science",
        "citied_by": "14",
        "cover_date": "2022-07-13",
        "Abstract": "Intertidal wetlands such as mangrove and saltmarsh are increasingly susceptible to areal losses related to sea level rise. This exposure is potentially offset by processes that might enable wetlands to accrete in situ or migrate landward under sea level rise, and planning policies that might open new opportunities for migration. We present and demonstrate a method to predict intertidal wetland distribution in the present-day landscape using random forest classification models, and use these models to predict the intertidal wetland distribution in future landscapes under specified sea level scenarios. The method is demonstrably robust in predicting present-day intertidal wetland distribution, with moderate correlation or better between predicted and mapped wetland distributions occurring in nearly all estuaries and strong correlation or better occurring in more than half of the estuaries. Given the accuracy in predicting present-day wetland distribution the method is assumed to be informative in predicting potential future wetland distribution when combined with best available models of future sea level. The classification method uses a variety of hydro-geomorphological surrogates that are derived from digital elevation models, Quaternary geology or soils mapping and land use mapping, which is then constrained by a representation of the future sea level inside estuaries. It is anticipated that the outputs from applying the method would inform assessments of intertidal wetland vulnerability to sea level rise and guide planning for potential wetland migration pathways.",
        "DOI": "10.3389/fenvs.2022.749950",
        "paper_author": "Hughes M.G.",
        "affiliation_name": "New South Wales Department of Planning &amp; Environment",
        "affiliation_city": "Sydney",
        "affiliation_country": "Australia",
        "affiliation_id": "60112807",
        "affiliation_state": "NSW"
    },
    {
        "paper_title": "Urban-Rural Disparities in Air Quality Responses to Traffic Changes in a Megacity of China Revealed Using Machine Learning",
        "publication": "Environmental Science and Technology Letters",
        "citied_by": "13",
        "cover_date": "2022-07-12",
        "Abstract": "Assessing the disparities of urban-rural air quality response to changes in emissions is essential for the development of effective air pollution mitigation strategies in megacities. However, meteorology and nonlinear atmospheric chemistry complicate the determination of emission-air quality responses. Here, we established a machine learning (ML)-based air quality simulator based on hourly air quality, meteorology, traffic activity, and other relevant indicators for Chengdu, a megacity in Southwest China. The ML-based simulator exhibits high fidelity in reproducing hourly pollutant concentrations (with cross validation R2 > 0.6 for NO2, O3, and PM2.5). The results indicated similar trends of meteorological impacts but various effects from traffic activities on air quality between urban and rural areas. Truck restriction policies have significantly reduced the impacts of truck traffic on air quality in the urban area. Repartitioning between NO2 and O3 is observed in both urban and rural areas, indicating a VOC-limited regime in winter across Chengdu. Total gaseous oxidant (i.e., OX = NO2 + O3) and PM2.5 concentrations are more sensitive to changes in nontruck (which emit more VOC) traffic than truck (which emit more NOX) traffic. We suggest that effective mitigation policies of OX should be developed according to local features to improve and alleviate winter haze simultaneously.",
        "DOI": "10.1021/acs.estlett.2c00246",
        "paper_author": "Wen Y.",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60025278",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Optimizing quantum circuit placement via machine learning",
        "publication": "Proceedings - Design Automation Conference",
        "citied_by": "15",
        "cover_date": "2022-07-10",
        "Abstract": "Quantum circuit placement (QCP) is the process of mapping the synthesized logical quantum programs on physical quantum machines, which introduces additional SWAP gates and affects the performance of quantum circuits. Nevertheless, determining the minimal number of SWAP gates has been demonstrated to be an NP-complete problem. Various heuristic approaches have been proposed to address QCP, but they suffer from suboptimality due to the lack of exploration. Although exact approaches can achieve higher optimality, they are not scalable for large quantum circuits due to the massive design space and expensive runtime. By formulating QCP as a bilevel optimization problem, this paper proposes a novel machine learning (ML)-based framework to tackle this challenge. To address the lower-level combinatorial optimization problem, we adopt a policy-based deep reinforcement learning (DRL) algorithm with knowledge transfer to enable the generalization ability of our framework. An evolutionary algorithm is then deployed to solve the upper-level discrete search problem, which optimizes the initial mapping with a lower SWAP cost. The proposed ML-based approach provides a new paradigm to overcome the drawbacks in both traditional heuristic and exact approaches while enabling the exploration of optimality-runtime trade-off. Compared with the leading heuristic approaches, our ML-based method significantly reduces the SWAP cost by up to 100%. In comparison with the leading exact search, our proposed algorithm achieves the same level of optimality while reducing the runtime cost by up to 40 times.",
        "DOI": "10.1145/3489517.3530403",
        "paper_author": "Fan H.",
        "affiliation_name": "Imperial College London",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60015150",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "SS-LRU: A Smart Segmented LRU Caching",
        "publication": "Proceedings - Design Automation Conference",
        "citied_by": "5",
        "cover_date": "2022-07-10",
        "Abstract": "Many caching policies use machine learning to predict data reuse, but they ignore the impact of incorrect prediction on cache performance, especially for large-size objects. In this paper, we propose a smart segmented LRU (SS-LRU) replacement policy, which adopts a size-aware classifier designed for cache scenarios and considers the cache cost caused by misprediction. Besides, SS-LRU enhances the migration rules of segmented LRU (SLRU) and implements a smart caching with unequal priorities and segment sizes based on prediction and multiple access patterns. We conducted Extensive experiments under the real-world workloads to demonstrate the superiority of our approach over state-of-the-art caching policies.",
        "DOI": "10.1145/3489517.3530469",
        "paper_author": "Li C.",
        "affiliation_name": "Huazhong University of Science and Technology",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60025761",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Interpretable AI for policy-making in pandemics",
        "publication": "GECCO 2022 Companion - Proceedings of the 2022 Genetic and Evolutionary Computation Conference",
        "citied_by": "3",
        "cover_date": "2022-07-09",
        "Abstract": "Since the first wave of the COVID-19 pandemic, governments have applied restrictions in order to slow down its spreading. However, creating such policies is hard, especially because the government needs to trade-off the spreading of the pandemic with the economic losses. For this reason, several works have applied machine learning techniques, often with the help of special-purpose simulators, to generate policies that were more effective than the ones obtained by governments. While the performance of such approaches are promising, they suffer from a fundamental issue: since such approaches are based on black-box machine learning, their real-world applicability is limited, because these policies cannot be analyzed, nor tested, and thus they are not trustable. In this work, we employ a recently developed hybrid approach, which combines reinforcement learning with evolutionary computation, for the generation of interpretable policies for containing the pandemic. These policies, trained on an existing simulator, aim to reduce the spreading of the pandemic while minimizing the economic losses. Our results show that our approach is able to find solutions that are extremely simple, yet very powerful. In fact, our approach has significantly better performance (in simulated scenarios) than both previous work and government policies.",
        "DOI": "10.1145/3520304.3533959",
        "paper_author": "Custode L.L.",
        "affiliation_name": "Università di Trento",
        "affiliation_city": "Trento",
        "affiliation_country": "Italy",
        "affiliation_id": "60015986",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "QDax: On the Benefits of Massive Parallelization for Quality-Diversity",
        "publication": "GECCO 2022 Companion - Proceedings of the 2022 Genetic and Evolutionary Computation Conference",
        "citied_by": "1",
        "cover_date": "2022-07-09",
        "Abstract": "Quality-Diversity (QD) algorithms are a well-known approach to generate large collections of diverse and high-quality policies. However, QD algorithms are also known to be data-inefficient, requiring large amounts of computational resources and are slow when used in practice for robotics tasks. Policy evaluations are already commonly performed in parallel to speed up QD algorithms but have limited capabilities on a single machine as most physics simulators run on CPUs. With recent advances in simulators that run on accelerators, thousands of evaluations can be performed in parallel on single GPU/TPU. In this paper, we present QDax, an implementation of MAP-Elites which leverages massive parallelism on accelerators to make QD algorithms more accessible. We show that QD algorithms are ideal candidates and can scale with massive parallelism to be run at interactive timescales. The increase in parallelism does not significantly affect the performance of QD algorithms, while reducing experiment runtimes by two factors of magnitudes, turning days of computation into minutes. These results show that QD can now benefit from hardware acceleration, which contributed significantly to the bloom of deep learning.",
        "DOI": "10.1145/3520304.3528927",
        "paper_author": "Lim B.",
        "affiliation_name": "Imperial College London",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60015150",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Policy network for solving flexible job shop scheduling problem with setup times and rescoure constraints",
        "publication": "GECCO 2022 Companion - Proceedings of the 2022 Genetic and Evolutionary Computation Conference",
        "citied_by": "1",
        "cover_date": "2022-07-09",
        "Abstract": "The flexible job shop scheduling problem (FJSP) is one of the most popular scheduling problems which allows an operation to be completed on several machines in a specific order and is well known as NP-hardness. And in today's industrial manufacturing environment, this problem is becoming more complex. We should take sequence-dependent setup times and resource constraints into account during the manufacturing process. In this article, a policy network is designed to solve the extended FJSP, with the goal of minimizing the total completion time (makespan). Three dispatching rules are proposed to simultaneously select an operation and assign it on a feasible machine at each scheduling point. To describe the manufacturing status at a scheduling stage, six state features are extracted as the input of the network. After the calculation of the network, the distribution of choosing each dispatching rule is obtained. After all of the operations are completed, the policy network is trained using policy gradient, a well-known reinforcement learning (RL) method. Finally, we generate ten benchmark data instances with different scales to compare the performance and efficiency of policy network with other algorithms. The proposed policy network outperforms its competitors, according to the results of the experiments.",
        "DOI": "10.1145/3520304.3529084",
        "paper_author": "Xu N.",
        "affiliation_name": "East China Normal University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60021200",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Reinforcement learning based adaptive metaheuristics",
        "publication": "GECCO 2022 Companion - Proceedings of the 2022 Genetic and Evolutionary Computation Conference",
        "citied_by": "4",
        "cover_date": "2022-07-09",
        "Abstract": "Parameter adaptation, that is the capability to automatically adjust an algorithm's hyperparameters depending on the problem being faced, is one of the main trends in evolutionary computation applied to numerical optimization. While several handcrafted adaptation policies have been proposed over the years to address this problem, only few attempts have been done so far at apply machine learning to learn such policies. Here, we introduce a general-purpose framework for performing parameter adaptation in continuous-domain metaheuristics based on state-of-the-art reinforcement learning algorithms. We demonstrate the applicability of this framework on two algorithms, namely Covariance Matrix Adaptation Evolution Strategies (CMA-ES) and Differential Evolution (DE), for which we learn, respectively, adaptation policies for the step-size (for CMA-ES), and the scale factor and crossover rate (for DE). We train these policies on a set of 46 benchmark functions at different dimensionalities, with various inputs to the policies, in two settings: one policy per function, and one global policy for all functions. Compared, respectively, to the Cumulative Step-size Adaptation (CSA) policy and to two well-known adaptive DE variants (iDE and jDE), our policies are able to produce competitive results in the majority of cases, especially in the case of DE.",
        "DOI": "10.1145/3520304.3533983",
        "paper_author": "Tessari M.",
        "affiliation_name": "Università di Trento",
        "affiliation_city": "Trento",
        "affiliation_country": "Italy",
        "affiliation_id": "60015986",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Pittsburgh Learning Classifier Systems for Explainable Reinforcement Learning: Comparing with XCS",
        "publication": "GECCO 2022 - Proceedings of the 2022 Genetic and Evolutionary Computation Conference",
        "citied_by": "5",
        "cover_date": "2022-07-08",
        "Abstract": "Interest in reinforcement learning (RL) has recently surged due to the application of deep learning techniques, but these connectionist approaches are opaque compared with symbolic systems. Learning Classifier Systems (LCSs) are evolutionary machine learning systems that can be categorised as eXplainable AI (XAI) due to their rule-based nature. Michigan LCSs are commonly used in RL domains as the alternative Pittsburgh systems (e.g. SAMUEL) suffer from complex algorithmic design and high computational requirements; however they can produce more compact/interpretable solutions than Michigan systems. We aim to develop two novel Pittsburgh LCSs to address RL domains: PPL-DL and PPL-ST. The former acts as a \"zeroth-level\"system, and the latter revisits SAMUEL's core Monte Carlo learning mechanism for estimating rule strength. We compare our two Pittsburgh systems to the Michigan system XCS across deterministic and stochastic FrozenLake environments. Results show that PPL-ST performs on-par or better than PPL-DL and outperforms XCS in the presence of high levels of environmental uncertainty. Rulesets evolved by PPL-ST can achieve higher performance than those evolved by XCS, but in a more parsimonious and therefore more interpretable fashion, albeit with higher computational cost. This indicates that PPL-ST is an LCS well-suited to producing explainable policies in RL domains.",
        "DOI": "10.1145/3512290.3528767",
        "paper_author": "Bishop J.T.",
        "affiliation_name": "The University of Queensland",
        "affiliation_city": "Brisbane",
        "affiliation_country": "Australia",
        "affiliation_id": "60031004",
        "affiliation_state": "QLD"
    },
    {
        "paper_title": "Machine learning-based active flutter suppression for a flexible flying-wing aircraft",
        "publication": "Journal of Sound and Vibration",
        "citied_by": "16",
        "cover_date": "2022-07-07",
        "Abstract": "It is challenging to synthesize controller parameters for high-dimensional aeroservoelastic systems, such as a flexible aircraft, so that the controller cannot work effectively. This paper presents a novel design approach of machine learning-based control law for the problem of active flutter suppression. The approach is able to automatically tune the controller parameters via machine learning and avoid the conventional and tedious procedure of manual tuning. As such, the approach leads to a controller with better performance synthesized. The paper deals with a case study of active flutter suppression for a flexible flying-wing aircraft and demonstrates the control performance and efficiency of the machine learning-based control scheme in expanding the flutter boundaries. Based on the environment/agent interface of reinforcement learning, the proposed approach takes the closed-loop aeroservoelastic system as an environment and the actor-critic neural networks as an agent. The approach trains the policy of synthesizing the optimal controller parameters through the interaction between the environment and the agent. In the numerical simulation, with the active flutter suppression controller synthesized via the well-trained policy automatically, the critical flutter speed of the closed-loop aeroservoelastic system increases by about 36.6% compared to the open-loop system robustly. Moreover, the stability and the robustness of the closed-loop aeroservoelastic system designed via the proposed approach are better than that with a conventional robust H∞ controller.",
        "DOI": "10.1016/j.jsv.2022.116916",
        "paper_author": "Mu X.",
        "affiliation_name": "Nanjing University of Aeronautics and Astronautics",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60021666",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "Fostering Coopetition While Plugging Leaks: The Design and Implementation of the MS MARCO Leaderboards",
        "publication": "SIGIR 2022 - Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "citied_by": "6",
        "cover_date": "2022-07-06",
        "Abstract": "We articulate the design and implementation of the MS MARCO document ranking and passage ranking leaderboards. In contrast to \"standard\"community-wide evaluations such as those at TREC, which can be characterized as simultaneous games, leaderboards represent sequential games, where every player move is immediately visible to the entire community. The fundamental challenge with this setup is that every leaderboard submission leaks information about the held-out evaluation set, which conflicts with the fundamental tenant in machine learning about separation of training and test data. These \"leaks\", accumulated over long periods of time, threaten the validity of the insights that can be derived from the leaderboards. In this paper, we share our experiences grappling with this issue over the past few years and how our considerations are operationalized into a coherent submission policy. Our work provides a useful guide to help the community understand the design choices made in the popular MS MARCO leaderboards and offers lessons for designers of future leaderboards.",
        "DOI": "10.1145/3477495.3531725",
        "paper_author": "Lin J.",
        "affiliation_name": "University of Waterloo",
        "affiliation_city": "Waterloo",
        "affiliation_country": "Canada",
        "affiliation_id": "60014171",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "Multi-Armed Bandits in Brain-Computer Interfaces",
        "publication": "Frontiers in Human Neuroscience",
        "citied_by": "2",
        "cover_date": "2022-07-05",
        "Abstract": "The multi-armed bandit (MAB) problem models a decision-maker that optimizes its actions based on current and acquired new knowledge to maximize its reward. This type of online decision is prominent in many procedures of Brain-Computer Interfaces (BCIs) and MAB has previously been used to investigate, e.g., what mental commands to use to optimize BCI performance. However, MAB optimization in the context of BCI is still relatively unexplored, even though it has the potential to improve BCI performance during both calibration and real-time implementation. Therefore, this review aims to further describe the fruitful area of MABs to the BCI community. The review includes a background on MAB problems and standard solution methods, and interpretations related to BCI systems. Moreover, it includes state-of-the-art concepts of MAB in BCI and suggestions for future research.",
        "DOI": "10.3389/fnhum.2022.931085",
        "paper_author": "Heskebeck F.",
        "affiliation_name": "Department of Automatic Control",
        "affiliation_city": "Lund",
        "affiliation_country": "Sweden",
        "affiliation_id": "60118609",
        "affiliation_state": "Skane"
    },
    {
        "paper_title": "Interaction Patterns between Climate Action and Air Cleaning in China: A Two-Way Evaluation Based on an Ensemble Learning Approach",
        "publication": "Environmental Science and Technology",
        "citied_by": "15",
        "cover_date": "2022-07-05",
        "Abstract": "China will attempt to achieve its simultaneous goals in 2060, whereby carbon neutrality will be accomplished and the PM2.5(fine particulate matter) level is expected to remain below 10 μg/m3. Identifying interaction patterns between air cleaning and climate action represents an important step to obtain cobenefits. Here, we used a random sampling strategy through the combination of chemical transport modeling and machine learning approach to capture the interaction effects from two perspectives in which the driving forces of both climate action and air cleaning measures were compared. We revealed that climate action where carbon emissions were decreased to 1.9 Bt (billion tons) could lead to a PM2.5level of 12.4 μg/m3(95% CI (confidence interval): 10.2-14.6 μg/m3) in 2060, while air cleaning could force carbon emissions to reach 1.93 Bt (95% CI: 0.79-3.19 Bt) to achieve net carbon neutrality based on the potential carbon sinks in 2060. Additional controls targeting primary PM2.5, ammonia, and volatile organic compounds were required as supplements to overcome the partial lack of climate action. Our study provides novel insights into the cobenefits of air-quality improvement and climate change mitigation, indicating that the effect of air cleaning on the simultaneous goals might have been underestimated before.",
        "DOI": "10.1021/acs.est.2c01966",
        "paper_author": "Liu Z.",
        "affiliation_name": "College of Environmental and Resource Sciences",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60117779",
        "affiliation_state": "Zhejiang"
    },
    {
        "paper_title": "Dramatic changes in atmospheric pollution source contributions for a coastal megacity in northern China from 2011 to 2020",
        "publication": "Atmospheric Chemistry and Physics",
        "citied_by": "17",
        "cover_date": "2022-07-05",
        "Abstract": "Understanding the effectiveness of long-term air pollution regulatory measures is important for control policy formulation. Efforts have been made using chemical transport modelling and statistical approaches to evaluate the efficacy of the Clean Air Action Plan (CAAP; 2013-2017) and the Blue Sky Protection Campaign (BSPC; 2018-2020) enacted in China. Changes in air quality due to reduction in emissions can be masked by meteorology, making it highly challenging to reveal the real effects of control measures. A knowledge gap still existed with respect to how sources changed before and after the CAAP and BSPC were implemented, respectively, particularly in coastal areas where anthropogenic emissions mixed with additional natural sources (e.g. marine aerosol). This work applied a machine-learning-based meteorological normalization approach to decouple the meteorological effects from air quality trend in a coastal city in northern China (Qingdao). Secondly, the relative changes in source contributions to ambient PM2.5 with a ∼10-year observation interval (2011-2012, 2016, and 2019) were also investigated. We discovered that the largest emission reduction section was likely from coal combustion as the meteorologically normalized SO2 dropped by ∼15.5 %yr-1, and the annual average dispersion-normalized SO42- decreased by ∼41.5 %. Change in the meteorologically normalized NO2 was relatively stable (∼1.0 %yr-1), and NO3- changed inappreciably in 2016-2019 but was significantly higher than that prior to the CAAP. Crustal dust decreased remarkably after the CAAP began. Industrial emissions, for example, steel-related smelting, decreased after 2016 due to the relocation of steel-making enterprises. Note that vehicle emissions were increased in importance as opposed to the other primary sources. Similar to other megacities, Qingdao is also at risk of increased ozone pollution that in turn facilitates secondary-particle formation in the future. The policy assessment approaches applied in this work also work for other places where air quality management is highly in demand to reduce air pollution. Copyright:",
        "DOI": "10.5194/acp-22-8597-2022",
        "paper_author": "Liu B.",
        "affiliation_name": "Nankai University",
        "affiliation_city": "Tianjin",
        "affiliation_country": "China",
        "affiliation_id": "60018038",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Improvements in learning to control perched landings",
        "publication": "Aeronautical Journal",
        "citied_by": "4",
        "cover_date": "2022-07-04",
        "Abstract": "Reinforcement learning has previously been applied to the problem of controlling a perched landing manoeuvre for a custom sweep-wing aircraft. Previous work showed that the use of domain randomisation to train with atmospheric disturbances improved the real-world performance of the controllers, leading to increased reward. This paper builds on the previous project, investigating enhancements and modifications to the learning process to further improve performance, and reduce final state error. These changes include modifying the observation by adding information about the airspeed to the standard aircraft state vector, employing further domain randomisation of the simulator, optimising the underlying RL algorithm and network structure, and changing to a continuous action space. Simulated investigations identified hyperparameter optimisation as achieving the most significant increase in reward performance. Several test cases were explored to identify the best combination of enhancements. Flight testing was performed, comparing a baseline model against some of the best performing test cases from simulation. Generally, test cases that performed better than the baseline in simulation also performed better in the real world. However, flight tests also identified limitations with the current numerical model. For some models, the chosen policy performs well in simulation yet stalls prematurely in reality, a problem known as the reality gap.",
        "DOI": "10.1017/aer.2022.48",
        "paper_author": "Fletcher L.",
        "affiliation_name": "University of Bristol",
        "affiliation_city": "Bristol",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60020650",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "ARTIFICIAL INTELLIGENCE A HUMAN MIND TOOL SAVING FROM CYBERCRIMES AND CYBER THREATS",
        "publication": "International Journal of Medical Toxicology and Legal Medicine",
        "citied_by": "0",
        "cover_date": "2022-07-01",
        "Abstract": "Cyber security has become a major concern in the digital era. The breach in data, cracking the captcha, identity theft, and other such offences has affected millions of peoples in their personal and professional life. More so the effect on organizational levels has touched various companies’ research and development programmes / progress. There has been a tremendous increase in potential for cyberattacks and these attacks has affected in almost every field of world including research and development, medicinal health, engineering, countries economics policies etc. There has been a real perfect challenges in inventing techniques, procedures and also methods of implementations of methodologies for tackling cyber attacks and crimes. In this paper the various specific techniques in solving cyber offences using artificial intelligence are been discussed so as to take early actions before the damages are been done.",
        "DOI": "10.5958/0974-4614.2022.00047.X",
        "paper_author": "Singh B.",
        "affiliation_name": "Shri Guru Ram Das Institute of Medical Sciences and Research",
        "affiliation_city": "Amritsar",
        "affiliation_country": "India",
        "affiliation_id": "60011669",
        "affiliation_state": "PB"
    },
    {
        "paper_title": "Navigation of Robotic-Arms using Policy Gradient Reinforcement Learning",
        "publication": "International Journal of Computing and Digital Systems",
        "citied_by": "1",
        "cover_date": "2022-07-01",
        "Abstract": "In this paper, the Deep Deterministic Policy Gradient (DDPG) reinforcement-learning algorithm is employed to enable a double-jointed robot arm to reach continuously changing target locations. The experimentation of the algorithm is carried out by training an agent to control the movement of this double-jointed robot arm. The architectures of the actor and critic networks are meticulously designed and the DDPG hyperparameters are carefully tuned. An enhanced version of the DDPG is also presented to handle multiple robot arms simultaneously. The trained agents are successfully tested in the Unity Machine Learning Agents environment for controlling both a single robot arm as well as multiple simultaneous robot arms. The testing shows the robust performance of the DDPG algorithm for empowering robot arm maneuvering in complex environments.",
        "DOI": "10.12785/IJCDS/120171",
        "paper_author": "Farag W.",
        "affiliation_name": "American University of the Middle East",
        "affiliation_city": "Al Ahmadi",
        "affiliation_country": "Kuwait",
        "affiliation_id": "60105846",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Using reinforcement learning to manage massive access in NB-IoT networks",
        "publication": "Intelligent Security Management and Control in the IoT",
        "citied_by": "0",
        "cover_date": "2022-07-01",
        "Abstract": "Communications between objects in the Internet of Things (IoT) and particularly machine-to-machine communications are considered as one of the most important evolutions of the Internet. In this chapter, the authors utilizes the deep reinforcement learning algorithm Twin Delayed Deep Deterministic policy gradient algorithm (TD3) to produce the optimal blocking factor from these past estimations. They briefly present the NB-IoT standard and provide an overview of the main congestion control techniques in IoT networks, and more especially cellular IoT networks. The authors describe the model for IoT terminals to access the network and the suggested control solution, based on the TD3 algorithm, adapted to solve the problem of calculating the blocking factor. They also describe the environment for simulating the suggested approach and show its effectiveness compared to the existing approach.",
        "DOI": "10.1002/9781394156030.ch2",
        "paper_author": "Hadjadj-Aoul Y.",
        "affiliation_name": "Université de Rennes",
        "affiliation_city": "Rennes",
        "affiliation_country": "France",
        "affiliation_id": "60030553",
        "affiliation_state": "Brittany"
    },
    {
        "paper_title": "Politics of data reuse in machine learning systems: Theorizing reuse entanglements",
        "publication": "Big Data and Society",
        "citied_by": "19",
        "cover_date": "2022-07-01",
        "Abstract": "Policy discussions and corporate strategies on machine learning are increasingly championing data reuse as a key element in digital transformations. These aspirations are often coupled with a focus on responsibility, ethics and transparency, as well as emergent forms of regulation that seek to set demands for corporate conduct and the protection of civic rights. And the Protective measures include methods of traceability and assessments of ‘good’ and ‘bad’ datasets and algorithms that are considered to be traceable, stable and contained. However, these ways of thinking about both technology and ethics obscure a fundamental issue, namely that machine learning systems entangle data, algorithms and more-than-human environments in ways that challenge a well-defined separation. This article investigates the fundamental fallacy of most data reuse strategies as well as their regulation and mitigation strategies that data can somehow be followed, contained and controlled in machine learning processes. Instead, the article argues that we need to understand the reuse of data as an inherently entangled phenomenon. To examine this tension between the discursive regimes and the realities of data reuse, we advance the notion of reuse entanglements as an analytical lens. The main contribution of the article is the conceptualization of reuse that places entanglements at its core and the articulation of its relevance using empirical illustrations. This is important, we argue, for our understanding of the nature of data and algorithms, for the practical uses of data and algorithms and our attitudes regarding ethics, responsibility and regulation.",
        "DOI": "10.1177/20539517221139785",
        "paper_author": "Thylstrup N.B.",
        "affiliation_name": "Copenhagen Business School",
        "affiliation_city": "Frederiksberg",
        "affiliation_country": "Denmark",
        "affiliation_id": "60020188",
        "affiliation_state": "Hovedstaden"
    },
    {
        "paper_title": "Dynamic Path Planning for Mobile Robots with Deep Reinforcement Learning",
        "publication": "IFAC-PapersOnLine",
        "citied_by": "17",
        "cover_date": "2022-07-01",
        "Abstract": "Traditional path planning algorithms for mobile robots are not effective to solve high-dimensional problems, and suffer from slow convergence and complex modelling. Therefore, it is highly essential to design a more efficient algorithm to realize intelligent path planning of mobile robots. This work proposes an improved path planning algorithm, which is based on the algorithm of Soft Actor-Critic (SAC). It attempts to solve a problem of poor robot performance in complicated environments with static and dynamic obstacles. This work designs an improved reward function to enable mobile robots to quickly avoid obstacles and reach targets by using state dynamic normalization and priority replay buffer techniques. To evaluate its performance, a Pygame-based simulation environment is constructed. The proposed method is compared with a Proximal Policy Optimization (PPO) algorithm in the simulation environment. Experimental results demonstrate that the cumulative reward of the proposed method is much higher than that of PPO, and it is also more robust than PPO.",
        "DOI": "10.1016/j.ifacol.2022.08.042",
        "paper_author": "Yang L.",
        "affiliation_name": "Beijing University of Technology",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60022281",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "An empirical analysis of homicides in Mexico through Machine Learning and statistical design of experiments",
        "publication": "Poblacion y Salud en Mesoamerica",
        "citied_by": "0",
        "cover_date": "2022-07-01",
        "Abstract": "Homicide is one of the most important mortality causes that has reduced the Mexican life expectancy. That is why the aim of this work is to identify some sociodemographic and economic factors that can help explain homicides in Mexico and measure their impact, assuming the current conditions prevail. To do that, several Machine Learning (ML) methods were evaluated. The C5.0 model is best suited for the data at hand. After fine-tuning the algorithm, we used the estimated model to identify the main factors that explain homicides. Among these factors, eleven were selected that can be influenced by direct changes in domestic public policy, laws and/or regulations. These were used as input in a two-level fractional factorial Statistical Design of Experiments (DOE) to estimate their main effects and possible interactions. Although several of these factors had statistically significant effects on homicide rate, the one that had the biggest and direct impact from a practical perspective, was the Rule of Law Index (RLI). In fact, if we assumed that all states had the median RLI of 0.37, implementing domestic policies and procedures to move them all to the best RLI level could significantly reduce homicide rates.",
        "DOI": "10.15517/psm.v20i1.48217",
        "paper_author": "Urrutia J.E.S.",
        "affiliation_name": "Universidad Anáhuac México",
        "affiliation_city": "Huixquilucan",
        "affiliation_country": "Mexico",
        "affiliation_id": "60027461",
        "affiliation_state": "MEX"
    },
    {
        "paper_title": "Algorithmic empowerment: A comparative ethnography of two open-source algorithmic platforms – Decide Madrid and vTaiwan",
        "publication": "Big Data and Society",
        "citied_by": "10",
        "cover_date": "2022-07-01",
        "Abstract": "Scholars of critical algorithmic studies, including those from geography, anthropology, science talent search, and communication studies, have begun to consider how algorithmic devices and platforms facilitate democratic practices. In this article, I draw on a comparative ethnography of two alternative open-source algorithmic platforms – Decide Madrid and vTaiwan – to consider how they are dynamically constituted by differing algorithmic–human relationships. I compare how different algorithmic–human relationships empower citizens to influence political decision-making through proposing, commenting, and voting on the urban issues that should receive political resources in Taipei and Madrid. I argue that algorithmic empowerment is an emerging process in which algorithmic–human relationships orient away from limitations and towards conditions of plurality, actionality, and power decentralisation. This argument frames algorithmic empowerment as bringing about empowering conditions that allow (underrepresented) individuals to shape policy-making and consider plural perspectives for political change and action, not as an outcome-driven, binary assessment (i.e. yes/no). This article contributes a novel, situated, and comparative conceptualisation of algorithmic empowerment that moves beyond technological determinism and universalism.",
        "DOI": "10.1177/20539517221123505",
        "paper_author": "Tseng Y.S.",
        "affiliation_name": "Helsingin Yliopisto",
        "affiliation_city": "Helsinki",
        "affiliation_country": "Finland",
        "affiliation_id": "60002952",
        "affiliation_state": "Uusimaa"
    },
    {
        "paper_title": "Predicting attitudinal and behavioral responses to COVID-19 pandemic using machine learning",
        "publication": "PNAS Nexus",
        "citied_by": "26",
        "cover_date": "2022-07-01",
        "Abstract": "At the beginning of 2020, COVID-19 became a global problem. Despite all the efforts to emphasize the relevance of preventive measures, not everyone adhered to them. Thus, learning more about the characteristics determining attitudinal and behavioral responses to the pandemic is crucial to improving future interventions. In this study, we applied machine learning on the multinational data collected by the International Collaboration on the Social and Moral Psychology of COVID-19 (N= 51,404) to test the predictive efficacy of constructs from social, moral, cognitive, and personality psychology, as well as socio-demographic factors, in the attitudinal and behavioral responses to the pandemic. The results point to several valuable insights. Internalized moral identity provided the most consistent predictive contribution—individuals perceiving moral traits as central to their self-concept reported higher adherence to preventive measures. Similar results were found for morality as cooperation, symbolized moral identity, self-control, open-mindedness, and collective narcissism, while the inverse relationship was evident for the endorsement of conspiracy theories. However, we also found a non-neglible variability in the explained variance and predictive contributions with respect to macro-level factors such as the pandemic stage or cultural region. Overall, the results underscore the importance of morality-related and contextual factors in understanding adherence to public health recommendations during the pandemic.",
        "DOI": "10.1093/pnasnexus/pgac093",
        "paper_author": "Pavlović T.",
        "affiliation_name": "Institut Drustvenih Znanosti Ivo Pilar",
        "affiliation_city": "Zagreb",
        "affiliation_country": "Croatia",
        "affiliation_id": "60013147",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "An Enhanced Proximal Policy Optimization-Based Reinforcement Learning Method with Random Forest for Hyperparameter Optimization",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "4",
        "cover_date": "2022-07-01",
        "Abstract": "For most machine learning and deep learning models, the selection of hyperparameters has a significant impact on the performance of the model. Therefore, deep learning and data analysis experts have to spend a lot of time on hyperparameter tuning when building a model for accomplishing a task. Although there are many algorithms used to solve hyperparameter optimization (HPO), these methods require the results of the actual trials at each epoch to help perform the search. To reduce the number of trials, model-based reinforcement learning adopts multilayer perceptron (MLP) to capture the relationship between hyperparameter settings and model performance. However, MLP needs to be carefully designed because there is a risk of overfitting. Thus, we propose a random forest-enhanced proximal policy optimization (RFEPPO) reinforcement learning algorithm to solve the HPO problem. In addition, reinforcement learning as a solution to HPO will encounter the sparse reward problem, eventually leading to slow convergence. To address this problem, we employ the intrinsic reward, which introduces the prediction error as the reward signal. Experiments carried on nine tabular datasets and two image classification datasets demonstrate the effectiveness of our model.",
        "DOI": "10.3390/app12147006",
        "paper_author": "Ma Z.",
        "affiliation_name": "Hanyang University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea",
        "affiliation_id": "60024872",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "STUN: Reinforcement-Learning-Based Optimization of Kernel Scheduler Parameters for Static Workload Performance",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "1",
        "cover_date": "2022-07-01",
        "Abstract": "Modern Linux operating systems are being used in a wide range of fields, from small IoT embedded devices to supercomputers. However, most machines use the default Linux scheduler parameters implemented for general-purpose environments. The problem is that the Linux scheduler cannot utilize the features of the various hardware and software environments, and it is therefore, difficult to achieve optimal performance in the machines. In this paper, we propose STUN, an automatic scheduler optimization framework. STUN modifies the five scheduling policies of the Linux kernel and 10 parameters automatically to optimize for each workload environment. STUN decreases the training time and enhances the efficiency through a filtering mechanism and training reward algorithms. Using STUN, users can optimize the performance of their machines at the OS scheduler level without manual control of the scheduler. STUN showed an execution time and improved FPS of 18.3% and 22.4% on a face detection workload, respectively. In addition, STUN showed 26.97%, 54.42%, and 256.13% performance improvements for microbenchmarks with 4, 44, and 120 cores for each.",
        "DOI": "10.3390/app12147072",
        "paper_author": "Lee H.",
        "affiliation_name": "Chungbuk National University",
        "affiliation_city": "Cheongju",
        "affiliation_country": "South Korea",
        "affiliation_id": "60004739",
        "affiliation_state": "Chungcheongbuk-do"
    },
    {
        "paper_title": "Protecting Steppe Birds by Monitoring with Sentinel Data and Machine Learning under the Common Agricultural Policy",
        "publication": "Agronomy",
        "citied_by": "0",
        "cover_date": "2022-07-01",
        "Abstract": "This paper shows the work carried out to obtain a methodology capable of monitoring the Common Agricultural Policy (CAP) aid line for the protection of steppe birds, which aims to improve the feeding and breeding conditions of these species and contribute to the improvement of their overall biodiversity population. Two methodologies were initially defined, one based on remote sensing (BirdsEO) and the other on Machine Learning (BirdsML). Both use Sentinel-1 and Sentinel-2 data as a basis. BirdsEO encountered certain impediments caused by the land’s slope and the crop’s height. Finally, the methodology based on Machine Learning offered the best results. It evaluated the performance of up to 7 different Machine Learning classifiers, the most optimal being RandomForest. Fourteen different datasets were generated, and the results they offered were evaluated, the most optimal being the one with more than 150 features, including a time series of 8 elements with Sentinel-1, Sentinel-2 data and derived products, among others. The generated model provided values higher than 97% in metrics such as accuracy, recall and Area under the ROC Curve, and 95% in precision and recall. The methodology is transformed into a tool that continuously monitors 100% of the area requesting aid, continuously over time, which contributes positively to optimizing the use of administrative resources and a fairer distribution of CAP funds.",
        "DOI": "10.3390/agronomy12071674",
        "paper_author": "López-Andreu F.J.",
        "affiliation_name": "Institute of Agricultural and Food Research",
        "affiliation_city": "La Alberca, Murcia",
        "affiliation_country": "Spain",
        "affiliation_id": "128542076",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Fair Models for Impartial Policies: Controlling Algorithmic Bias in Transport Behavioural Modelling",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "1",
        "cover_date": "2022-07-01",
        "Abstract": "The increasing use of new data sources and machine learning models in transport modelling raises concerns with regards to potentially unfair model-based decisions that rely on gender, age, ethnicity, nationality, income, education or other socio-economic and demographic data. We demonstrate the impact of such algorithmic bias and explore the best practices to address it using three different representative supervised learning models of varying levels of complexity. We also analyse how the different kinds of data (survey data vs. big data) could be associated with different levels of bias. The methodology we propose detects the model’s bias and implements measures to mitigate it. Specifically, three bias mitigation algorithms are implemented, one at each stage of the model development pipeline—before the classifier is trained (pre-processing), when training the classifier (in-processing) and after the classification (post-processing). As these debiasing techniques have an inevitable impact on the accuracy of predicting the behaviour of individuals, the comparison of different types of models and algorithms allows us to determine which techniques provide the best balance between bias mitigation and accuracy loss for each case. This approach improves model transparency and provides an objective assessment of model fairness. The results reveal that mode choice models are indeed affected by algorithmic bias, and it is proven that the implementation of off-the-shelf mitigation techniques allows us to achieve fairer classification models.",
        "DOI": "10.3390/su14148416",
        "paper_author": "Vega-Gonzalo M.",
        "affiliation_name": "Universidad Politécnica de Madrid",
        "affiliation_city": "Madrid",
        "affiliation_country": "Spain",
        "affiliation_id": "60028442",
        "affiliation_state": "Madrid"
    },
    {
        "paper_title": "Review on Machine Learning Methods for Motion Planning and Control Policy of Intelligent Vehicles",
        "publication": "Beijing Ligong Daxue Xuebao/Transaction of Beijing Institute of Technology",
        "citied_by": "14",
        "cover_date": "2022-07-01",
        "Abstract": "Intelligent vehicles have achieved a considerable development in technologies and can fulfill the basic functions of autonomous driving in a limited closed environment. However, results of actual road tests show that the current technologies of intelligent vehicles still have many limitations and their large-scale application in complex urban and off-road environments still faces many challenges. As one of the key technologies, the motion planning and control technology has basically formed a complete theoretical system and has been widely applied in engineering. However, the traditional methods still have some defects in practical application, such as the inability of understanding dynamic and complex scenes, poor adaptability for different scenes, high complexity of the model, and difficulty in parameter tuning. Due to the strong ability in knowledge representation and model fitting, machine learning methods have been widely applied in perception and navigation technology for intelligent vehicles. In order to solve the problems of generalization and applicability in traditional motion planning and control techniques, many researchers have also devoted themselves to exploring the usage of deep learning, reinforcement learning, and so on machine learning methods in motion planning and control policy for intelligent vehicles. In this paper, machine learning-based methods were reviewed for motion planning and control in intelligent vehicles, analyzing the existing policy learning methods for motion planning and control from three aspects, including basic framework, basic learning paradigms, and different planning and control methods based on learning. Finally, the research status and future development directions were summarized and prospected.",
        "DOI": "10.15918/j.tbit1001-0645.2022.095",
        "paper_author": "Gong J.",
        "affiliation_name": "Beijing Institute of Technology",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60016835",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Detection and Classification of Floating Plastic Litter Using a Vessel-Mounted Video Camera and Deep Learning",
        "publication": "Remote Sensing",
        "citied_by": "26",
        "cover_date": "2022-07-01",
        "Abstract": "Marine plastic pollution is a major environmental concern, with significant ecological, economic, public health and aesthetic consequences. Despite this, the quantity and distribution of marine plastics is poorly understood. Better understanding of the global abundance and distribution of marine plastic debris is vital for global mitigation and policy. Remote sensing methods could provide substantial data to overcome this issue. However, developments have been hampered by the limited availability of in situ data, which are necessary for development and validation of remote sensing methods. Current in situ methods of floating macroplastics (size greater than 1 cm) are usually conducted through human visual surveys, often being costly, time-intensive and limited in coverage. To overcome this issue, we present a novel approach to collecting in situ data using a trained object-detection algorithm to detect and quantify marine macroplastics from video footage taken from vessel-mounted general consumer cameras. Our model was able to successfully detect the presence or absence of plastics from real-world footage with an accuracy of 95.2% without the need to pre-screen the images for horizon or other landscape features, making it highly portable to other environmental conditions. Additionally, the model was able to differentiate between plastic object types with a Mean Average Precision of 68% and an F1-Score of 0.64. Further analysis suggests that a way to improve the separation among object types using only object detection might be through increasing the proportion of the image area covered by the plastic object. Overall, these results demonstrate how low-cost vessel-mounted cameras combined with machine learning have the potential to provide substantial harmonised in situ data of global macroplastic abundance and distribution.",
        "DOI": "10.3390/rs14143425",
        "paper_author": "Armitage S.",
        "affiliation_name": "University of Exeter",
        "affiliation_city": "Exeter",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60026479",
        "affiliation_state": "Devon"
    },
    {
        "paper_title": "Supervisory control of the hybrid off-highway vehicle for fuel economy improvement using predictive double Q-learning with backup models",
        "publication": "Journal of Central South University",
        "citied_by": "4",
        "cover_date": "2022-07-01",
        "Abstract": "This paper studied a supervisory control system for a hybrid off-highway electric vehicle under the charge-sustaining (CS) condition. A new predictive double Q-learning with backup models (PDQL) scheme is proposed to optimize the engine fuel in real-world driving and improve energy efficiency with a faster and more robust learning process. Unlike the existing “model-free” methods, which solely follow on-policy and off-policy to update knowledge bases (Q-tables), the PDQL is developed with the capability to merge both on-policy and off-policy learning by introducing a backup model (Q-table). Experimental evaluations are conducted based on software-in-the-loop (SiL) and hardware-in-the-loop (HiL) test platforms based on real-time modelling of the studied vehicle. Compared to the standard double Q-learning (SDQL), the PDQL only needs half of the learning iterations to achieve better energy efficiency than the SDQL at the end learning process. In the SiL under 35 rounds of learning, the results show that the PDQL can improve the vehicle energy efficiency by 1.75% higher than SDQL. By implementing the PDQL in HiL under four predefined real-world conditions, the PDQL can robustly save more than 5.03% energy than the SDQL scheme.",
        "DOI": "10.1007/s11771-022-5004-y",
        "paper_author": "Shuai B.",
        "affiliation_name": "University of Birmingham",
        "affiliation_city": "Birmingham",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60019702",
        "affiliation_state": "West Midlands"
    },
    {
        "paper_title": "Ethics for Artificial Intelligence: Focus on the Use of Radiology Images",
        "publication": "Journal of the Korean Society of Radiology",
        "citied_by": "0",
        "cover_date": "2022-07-01",
        "Abstract": "The importance of ethics in research and the use of artificial intelligence (AI) is increasingly recognized not only in the field of healthcare but throughout society. This article intends to provide domestic readers with practical points regarding the ethical issues of using radiological images for AI research, focusing on data security and privacy protection and the right to data. Therefore, this article refers to related domestic laws and government policies. Data security and privacy protection is a key ethical principle for AI, in which proper de-identification of data is crucial. Sharing healthcare data to develop AI in a way that minimizes business interests is another ethical point to be highlighted. The need for data sharing makes the data security and privacy protection even more important as data sharing increases the risk of data breach.",
        "DOI": "10.3348/jksr.2022.0036",
        "paper_author": "Park S.H.",
        "affiliation_name": "University of Ulsan College of Medicine",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea",
        "affiliation_id": "60006240",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Reinforcement learning of optimal active particle navigation",
        "publication": "New Journal of Physics",
        "citied_by": "24",
        "cover_date": "2022-07-01",
        "Abstract": "The development of self-propelled particles at the micro- and the nanoscale has sparked a huge potential for future applications in active matter physics, microsurgery, and targeted drug delivery. However, while the latter applications provoke the quest on how to optimally navigate towards a target, such as e.g. a cancer cell, there is still no simple way known to determine the optimal route in sufficiently complex environments. Here we develop a machine learning-based approach that allows us, for the first time, to determine the asymptotically optimal path of a self-propelled agent which can freely steer in complex environments. Our method hinges on policy gradient-based deep reinforcement learning techniques and, crucially, does not require any reward shaping or heuristics. The presented method provides a powerful alternative to current analytical methods to calculate optimal trajectories and opens a route towards a universal path planner for future intelligent active particles.",
        "DOI": "10.1088/1367-2630/ac8013",
        "paper_author": "Nasiri M.",
        "affiliation_name": "Technische Universität Darmstadt",
        "affiliation_city": "Darmstadt",
        "affiliation_country": "Germany",
        "affiliation_id": "60011226",
        "affiliation_state": "Hessen"
    },
    {
        "paper_title": "Monitoring and Analysis of Urban Sprawl Based on Road Network Data and High-Resolution Remote Sensing Imagery: A Case Study of China’s Provincial Capitals",
        "publication": "Photogrammetric Engineering and Remote Sensing",
        "citied_by": "2",
        "cover_date": "2022-07-01",
        "Abstract": "The primary prerequisite for sustainable urban development is to accurately grasp the development of the city. The dynamic changes in the urban area can reflect the urban expansion process and spatial development model. Carrying out urban expansion monitoring and extracting urban areas is of great importance for grasping the law of urban development and promoting the sustainable development of cities. However, the related research reveals several problems such as insufficient accuracy and low intelligence of urban boundary extraction. In response to these problems, this paper proposes a new method for urban area extraction based on the progressive learning model. By combining prior knowledge and image features, the number of training samples required for machine learning is reduced, and the problem of using high-level semantic information expression in the process of urban areas is avoided by using the classification method, and thus the accuracy of urban area extraction is improved. The method uses urban road network data to divide the city into blocks. It applies the scene classification method to extract the urban areas and uses the pyramid layer-by-layer to carry out the space constraint method to integrate the urban extraction principle into the machine learning process, which can be obtained and kept artificial under a small sample condition. Extracting the effect of the urban area is very close, greatly reducing the workload and providing a new solution for high-precision automatic extraction of urban areas. Through the analysis of urban expansion, the following results were obtained: (1) from 2000 to 2015, China’s provincial capital cities maintained a high-speed growth trend with a total area increased by 90%; (2) urban expansion showed significant regional differences. The eastern expansion rate gradually slowed down, the western and northeast regions accelerated their expansion, and the central region expanded steadily; (3) 61% of the provincial capital cities expanded exponentially; (4) the development of China’s provincial capital cities was highly correlated with national urban development policies and regional development strategies.",
        "DOI": "10.14358/PERS.22-00017R2",
        "paper_author": "Ding L.",
        "affiliation_name": "Wuhan University",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60029306",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "A Machine Learning–Enabled Partially Observable Markov Decision Process Framework for Early Sepsis Prediction",
        "publication": "INFORMS Journal on Computing",
        "citied_by": "8",
        "cover_date": "2022-07-01",
        "Abstract": "Sepsis is a life-threatening condition, caused by the body’s extreme response to an infection. In the United States, 1.7 million cases of sepsis occur annually, resulting in 265,000 deaths. Delayed diagnosis and treatment are associated with higher mortality rates. An exponential rise in the availability of medical data has allowed for the development of sophisticated machine learning algorithms to predict sepsis earlier than the onset. However, these models often underperform, as the training data are retrospective and do not fully capture the uncertain future. In this study, we develop a novel framework, which we refer to as MLePOMDP, to leverage and combine the underlying, high-level knowledge about sepsis progression and machine learning (ML) for classification. Specifically, we use a hidden Markov model to describe sepsis development at a high level, where the ML model makes the higher-order “observations” from temporal data. Consequently, a partially observable Markov decision process (POMDP) model is developed to make classification decisions. We analytically establish that the optimal policy is of threshold-type, which we exploit to efficiently optimize MLePOMDP. MLePOMDP is calibrated and tested using high-frequency physiological data collected from bedside monitors. Different from past POMDP-based frameworks, MLePOMDP is developed for a prediction task using a very small state definition, produces highly interpretable results, and accounts for a novel and clinically meaningful action space. Our results show that MLePOMDP outperforms machine learning–based benchmarks by up to 8% in precision. Importantly, MLePOMDP is able to reduce false alarms by up to 28%. An additional experiment is conducted to show the generalizability of MLePOMDP to different patient cohorts.",
        "DOI": "10.1287/ijoc.2022.1176",
        "paper_author": "Liu Z.",
        "affiliation_name": "Tickle College of Engineering",
        "affiliation_city": "Knoxville",
        "affiliation_country": "United States",
        "affiliation_id": "60150705",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Commodity price forecasting via neural networks for coffee, corn, cotton, oats, soybeans, soybean oil, sugar, and wheat",
        "publication": "Intelligent Systems in Accounting, Finance and Management",
        "citied_by": "60",
        "cover_date": "2022-07-01",
        "Abstract": "Agricultural commodity price forecasting represents a key concern for market participants. We explore the usefulness of neural network modeling for forecasting problems in datasets of daily prices over periods of greater than 50 years for coffee, corn, cotton, oats, soybeans, soybean oil, sugar, and wheat. By investigating different model settings across the algorithm, delay, hidden neuron, and data-splitting ratio, we arrive at models leading to a decent performance for each commodity, with the overall relative root mean square error ranging from 1.70% to 3.19%. These results have small advantages over no-change models due to particular price adjustments in the prices considered here. Our results can be used on a standalone basis or combined with fundamental forecasts in forming perspectives of commodity price trends and conducting policy analysis. Our empirical framework should not be diffucult to implement, which is a critical consideration for many decision-makers and has the potential to be generalized for price forecasts of more commodities.",
        "DOI": "10.1002/isaf.1519",
        "paper_author": "Xu X.",
        "affiliation_name": "NC State University",
        "affiliation_city": "Raleigh",
        "affiliation_country": "United States",
        "affiliation_id": "60004923",
        "affiliation_state": "NC"
    },
    {
        "paper_title": "Big Data, Decision Models, and Public Health",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "4",
        "cover_date": "2022-07-01",
        "Abstract": "NA",
        "DOI": "10.3390/ijerph19148543",
        "paper_author": "Chan C.L.",
        "affiliation_name": "Yuan Ze University",
        "affiliation_city": "Taoyuan",
        "affiliation_country": "Taiwan",
        "affiliation_id": "60013395",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Consumer perceptions of telehealth for mental health or substance abuse: A Twitter-based topic modeling analysis",
        "publication": "JAMIA Open",
        "citied_by": "20",
        "cover_date": "2022-07-01",
        "Abstract": "Objective: The objective of this study is to understand the primary topics of consumer discussion on Twitter associated with telehealth for mental health or substance abuse for prepandemic versus during-pandemic time-periods, using a state-of-the-art machine learning (ML) natural language processing (NLP) method. Materials and Methods: The primary methodological phases of this project were: (1) collecting, cleaning, and filtering data (tweets) from January 2014 to June 2021, (2) describing the final corpus, (3) running and optimizing Bidirectional Encoder Representations from Transformers (BERT; using BERTopic in Python) models, and (4) human refinement of topic model results and thematic classification of topics. Results: The number of tweets in this context increased by 4 times during the pandemic (2017 tweets prepandemic vs 8672 tweets during the pandemic). During the pandemic topics were more frequently mental health related than substance abuse related. Top during-pandemic topics were therapy, suicide, pain (associated with burnout and drinking), and mental health diagnoses such as ADHD and autism. Anxiety was a key topic of discussion both pre- and during the pandemic. Discussion: Telehealth for mental health and substance abuse is being discussed more frequently online, which implies growing demand. Given the topics extracted as proxies for demand, the most demand is currently for telehealth for mental health primarily, especially for children, parents, and therapy for those with anxiety or depression, and substance abuse secondarily. Conclusions: Scarce telehealth resources can be allocated more efficiently if topics of consumer discussion are included in resource allocation decision- and policy-making processes.",
        "DOI": "10.1093/jamiaopen/ooac028",
        "paper_author": "Baird A.",
        "affiliation_name": "Georgia State University",
        "affiliation_city": "Atlanta",
        "affiliation_country": "United States",
        "affiliation_id": "60012387",
        "affiliation_state": "GA"
    },
    {
        "paper_title": "Implementation approaches and barriers for rule-based and machine learning-based sepsis risk prediction tools: A qualitative study",
        "publication": "JAMIA Open",
        "citied_by": "19",
        "cover_date": "2022-07-01",
        "Abstract": "Objective: Many options are currently available for sepsis surveillance clinical decision support (CDS) from electronic medical record (EMR) vendors, third party, and homegrown models drawing on rule-based (RB) and machine learning (ML) algorithms. This study explores sepsis CDS implementation from the perspective of implementation leads by describing the motivations, tool choices, and implementation experiences of a diverse group of implementers. Materials and Methods: Semi-structured interviews were conducted with and a questionnaire was administered to 21 hospital leaders overseeing CDS implementation at 15 US medical centers. Participants were recruited via convenience sampling. Responses were coded by 2 coders with consensus approach and inductively analyzed for themes. Results: Use of sepsis CDS is motivated in part by quality metrics for sepsis patients. Choice of tool is driven by ease of integration, customization capability, and perceived predictive potential. Implementation processes for these CDS tools are complex, time-consuming, interdisciplinary undertakings resulting in heterogeneous choice of tools and workflow integration. To improve clinician acceptance, implementers addressed both optimization of the alerts as well as clinician understanding and buy in. More distrust and confusion was reported for ML models, as compared to RB models. Respondents described a variety of approaches to overcome implementation barriers; these approaches related to alert firing, content, integration, and buy-in. Discussion: While there are shared socio-technical challenges of implementing CDS for both RB and ML models, attention to user education, support, expectation management, and dissemination of effective practices may improve feasibility and effectiveness of ML models in quality improvement efforts. Conclusion: Further implementation science research is needed to determine real world efficacy of these tools. Clinician acceptance is a significant barrier to sepsis CDS implementation. Successful implementation of less clinically intuitive ML models may require additional attention to user confusion and distrust.",
        "DOI": "10.1093/jamiaopen/ooac022",
        "paper_author": "Joshi M.",
        "affiliation_name": "Stanford University School of Medicine",
        "affiliation_city": "Stanford",
        "affiliation_country": "United States",
        "affiliation_id": "60032838",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Predicting age and gender from network telemetry: Implications for privacy and impact on policy",
        "publication": "PLoS ONE",
        "citied_by": "1",
        "cover_date": "2022-07-01",
        "Abstract": "The systematic monitoring of private communications through the use of information technology pervades the digital age. One result of this is the potential availability of vast amount of data tracking the characteristics of mobile network users. Such data is becoming increasingly accessible for commercial use, while the accessibility of such data raises questions about the degree to which personal information can be protected. Existing regulations may require the removal of personally-identifiable information (PII) from datasets before they can be processed, but research now suggests that powerful machine learning classification methods are capable of targeting individuals for personalized marketing purposes, even in the absence of PII. This study aims to demonstrate how machine learning methods can be deployed to extract demographic characteristics. Specifically, we investigate whether key demographics—gender and age—of mobile users can be accurately identified by third parties using deep learning techniques based solely on observations of the user’s interactions within the network. Using an anonymized dataset from a Latin American country, we show the relative ease by which PII in terms of the age and gender demographics can be inferred; specifically, our neural networks model generates an estimate for gender with an accuracy rate of 67%, outperforming decision tree, random forest, and gradient boosting models by a significant margin. Neural networks achieve an even higher accuracy rate of 78% in predicting the subscriber age. These results suggest the need for a more robust regulatory framework governing the collection of personal data to safeguard users from predatory practices motivated by fraudulent intentions, prejudices, or consumer manipulation. We discuss in particular how advances in machine learning have chiseled away a number of General Data Protection Regulation (GDPR) articles designed to protect consumers from the imminent threat of privacy violations.",
        "DOI": "10.1371/journal.pone.0271714",
        "paper_author": "Kuang L.",
        "affiliation_name": "College of Computing",
        "affiliation_city": "Chicago",
        "affiliation_country": "United States",
        "affiliation_id": "60280460",
        "affiliation_state": "IL"
    },
    {
        "paper_title": "The Executive Branch decisions in Brazil: A study of administrative decrees through machine learning and network analysis",
        "publication": "PLoS ONE",
        "citied_by": "4",
        "cover_date": "2022-07-01",
        "Abstract": "This paper dissects the potential of state-of-the-art computational analysis to promote the investigation of government’s administrative decisions and politics. The Executive Branch generates massive amounts of textual data comprising daily decisions in several levels and stages of the law and decree-making processes. The use of automated text analysis to explore this data based on the substantive interests of scholars runs into computational challenges. Computational methods have been applied to texts from the Legislative and Judicial Branches; however, there barely are suitable taxonomies to automate the classification and analysis of the Executive’s administrative decrees. To solve this problem, we put forward a computational framework to analyze the Brazilian administrative decrees from 2000 to 2019. Our strategy to uncover the contents and patterns of the presidential decree-making is developed in three main steps. First, we conduct an unsupervised text analysis through the LDA algorithm for topic modeling. Second, building upon the LDA results, we propose two taxonomies for the classification of decrees: (a) the ministerial coauthorship of the decrees to map policy areas and (b) the decrees’ fields of law based on a tagging system provided by the Brazilian Senate. Using these taxonomies, we compare the performance of three supervised text classification algorithms: SVM, Convolutional Neural Network, and Hierarchical Attention Network, achieving F1-scores of up to 80% when automatically classifying decrees. Third, we analyze the network generated by links between decrees through centrality and clustering approaches, distinguishing a set of administrative decisions related to the president’s priorities in the economic policy area. Our findings confirm the potential of our computational framework to explore N-large datasets, advance exploratory studies, and generate testable propositions in different research areas. They advance the monitoring of Brazil’s administrative decree-making process that is shaped by the president’s priorities and by the interplay among cabinet members.",
        "DOI": "10.1371/journal.pone.0271741",
        "paper_author": "Ribeiro A.L.",
        "affiliation_name": "Universidade Federal de Minas Gerais",
        "affiliation_city": "Belo Horizonte",
        "affiliation_country": "Brazil",
        "affiliation_id": "60030074",
        "affiliation_state": "MG"
    },
    {
        "paper_title": "The impact of institutional delivery on neonatal and maternal health outcomes: Evidence from a road upgrade programme in India",
        "publication": "BMJ Global Health",
        "citied_by": "4",
        "cover_date": "2022-07-01",
        "Abstract": "Introduction Persistently high rates of neonatal and maternal mortality have been associated with home births in many low-income and middle-income countries (LMICs). However, causal evidence of the effect of institutional deliveries on neonatal and maternal health outcomes is limited in these settings. Methods We investigate the effect of institutional deliveries on neonatal mortality and maternal postpartum complications in rural India using data from the 2015-2016 Indian Demographic and Health Survey and an instrumental variable methodology to overcome selection bias issues inherent in observational studies. Specifically, we exploit plausibly exogenous variation in exposure to a road upgrade programme that quasi-randomly upgraded roads to villages across India. Results We find large effects of the road construction programme on the probability that a woman delivered in a health facility: moving from an unconnected village to a connected village increased the probability of an institutional delivery by 13 percentage points, with the biggest increases in institutional delivery observed in public hospitals and among women with lower levels of education and from poorer households. However, we find no evidence that increased institutional delivery rates improved rates of neonatal mortality or postpartum complications, regardless of whether the delivery occurred in a public or private facility, or if it was with a skilled birth attendant. Conclusion Policies that encourage institutional delivery do not always translate into increased health outcomes and should thus be complemented with efforts to improve the quality of care to improve neonatal and maternal health outcomes in LMICs.",
        "DOI": "10.1136/bmjgh-2021-007926",
        "paper_author": "Shajarizadeh A.",
        "affiliation_name": "Independent Researcher",
        "affiliation_city": "Vancouver",
        "affiliation_country": "Canada",
        "affiliation_id": "123470329",
        "affiliation_state": "BC"
    },
    {
        "paper_title": "Application of non-negative matrix factorization in oncology: One approach for establishing precision medicine",
        "publication": "Briefings in Bioinformatics",
        "citied_by": "24",
        "cover_date": "2022-07-01",
        "Abstract": "The increase in the expectations of artificial intelligence (AI) technology has led to machine learning technology being actively used in the medical field. Non-negative matrix factorization (NMF) is a machine learning technique used for image analysis, speech recognition, and language processing; recently, it is being applied to medical research. Precision medicine, wherein important information is extracted from large-scale medical data to provide optimal medical care for every individual, is considered important in medical policies globally, and the application of machine learning techniques to this end is being handled in several ways. NMF is also introduced differently because of the characteristics of its algorithms. In this review, the importance of NMF in the field of medicine, with a focus on the field of oncology, is described by explaining the mathematical science of NMF and the characteristics of the algorithm, providing examples of how NMF can be used to establish precision medicine, and presenting the challenges of NMF. Finally, the direction regarding the effective use of NMF in the field of oncology is also discussed.",
        "DOI": "10.1093/bib/bbac246",
        "paper_author": "Hamamoto R.",
        "affiliation_name": "National Cancer Center Research Institute",
        "affiliation_city": "Chuo",
        "affiliation_country": "Japan",
        "affiliation_id": "127105071",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Where Are Potential Areas for Transit-Oriented Development (TOD)—Exploring the Demands for Built Environment for TOD Planning",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "10",
        "cover_date": "2022-07-01",
        "Abstract": "Sustainable urban growth advocates the implementation of transit-oriented development (TOD) to optimize urban spatial structure. The bilateral planning concept of TOD emphasizes the importance of discovering areas with existing TOD features but poor public transit service (potential TOD areas) and further introducing transit connectivity or conducting TOD policy in such areas to facilitate sustainable transportation. However, current studies that are devoted to discovering potential TOD areas remain scarce. In this study, we find that random forest (RF) is an optimal algorithm that can effectively identify potential TOD regions in Hong Kong. We propose an RF-mediated machine learning model (RF-TPI model) and reveal underlying mechanisms of specific indicators. After iteratively learning the typical features of TOD areas in Hong Kong, the developed RF-TPI model shows great capacity to identify potential TOD areas, with satisfactory model performances (accuracy score: 0.89, precision score: 0.81). Further investigation on manifestations of indicators by the SHapley Additive exPlanations (SHAP) interpreter demonstrates the intricate, significant nonlinear and threshold effects of distinct indicators. Conclusively, we highlight that random forest would be a prospective tool for identifying potential TOD areas to aid TOD strategy in urban sustainable endeavors.",
        "DOI": "10.3390/su14148364",
        "paper_author": "Xia J.",
        "affiliation_name": "College of Civil Engineering and Architecture Zhejiang University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60117841",
        "affiliation_state": "Zhejiang"
    },
    {
        "paper_title": "Effects of Human Activities on Urban Vegetation: Explorative Analysis of Spatial Characteristics and Potential Impact Factors",
        "publication": "Remote Sensing",
        "citied_by": "4",
        "cover_date": "2022-07-01",
        "Abstract": "Since the 21st century, large cities around the world have experienced the transition from economically destructive development to a harmonious eco-environment. Understanding the dynamic relationships between human activities and urban eco-environment in this transition is a challenging and essential topic. The normalized difference vegetation index (NDVI) can reflect the urban vegetation cover status well. Socio-economic indexes can present the intensity and spatiality of human activities quantitatively. This work aims to use traditional regression models and machine learning algorithms to analyze the impact of socio-economic factors on NDVI accurately. Random forest regression (RFR) was performed to initially assess the contributions of all factors on NDVI, which was the numerical basis for feature selection. Subsequently, detailed dynamic relationship simulations were implemented using geographically weighted regression. In the case of Wuhan in China, the results showed that the goodness-of-fit of NDVI with socio-economic factors generally exceeded 50%. The influence coefficients changed from negative to positive, and 2010 was the turning point, indicating that human activities gradually played a favorable role in protecting vegetation during this transition period. The urban–rural interface, which was located between urban centers and marginal urban suburbs, was the area where human activities contributed most to vegetation. Thus, policy makers should focus on planning and managing housing construction and vegetation planting in urban–rural interface to relieve the population burden of the central area and improve the environmental conditions of the urban eco-environment subconsciously.",
        "DOI": "10.3390/rs14132999",
        "paper_author": "Li X.",
        "affiliation_name": "China University of Geosciences",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60006019",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Unconditional quantile regression with high-dimensional data",
        "publication": "Quantitative Economics",
        "citied_by": "7",
        "cover_date": "2022-07-01",
        "Abstract": "This paper considers estimation and inference for heterogeneous counterfactual effects with high-dimensional data. We propose a novel robust score for debiased estimation of the unconditional quantile regression (Firpo, Fortin, and Lemieux (2009)) as a measure of heterogeneous counterfactual marginal effects. We propose a multiplier bootstrap inference and develop asymptotic theories to guarantee the size control in large sample. Simulation studies support our theories. Applying the proposed method to Job Corps survey data, we find that a policy, which counterfactually extends the duration of exposures to the Job Corps training program, will be effective especially for the targeted subpopulations of lower potential wage earners.",
        "DOI": "10.3982/QE1896",
        "paper_author": "Sasaki Y.",
        "affiliation_name": "Vanderbilt University",
        "affiliation_city": "Nashville",
        "affiliation_country": "United States",
        "affiliation_id": "60003915",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "EpyNN: Educational python for Neural Networks",
        "publication": "SoftwareX",
        "citied_by": "1",
        "cover_date": "2022-07-01",
        "Abstract": "Artificial Neural Networks (ANNs) have achieved unequaled performance for numerous problems in many areas of Science, Business, Public Policy, and more. While experts are familiar with performance-oriented software and underlying theory, ANNs are difficult to comprehend for non-experts because it requires skills in programming, background in mathematics and knowledge of terminology and concepts. In this work, we release EpyNN, an educational Python resource meant for a public willing to understand key concepts and practical implementation of scalable ANN architectures from concise, homogeneous and idiomatic source code. EpyNN contains an educational Application Programming Interface (API), educational workflows from data preparation to ANN training and a documentation website setting side-by-side code, mathematics, graphical representation and text to facilitate learning and provide teaching material. Overall, EpyNN provides basics in Python for individuals who wish to learn, teach or develop from scratch.",
        "DOI": "10.1016/j.softx.2022.101140",
        "paper_author": "Malard F.",
        "affiliation_name": "Medical College of Wisconsin",
        "affiliation_city": "Milwaukee",
        "affiliation_country": "United States",
        "affiliation_id": "60012553",
        "affiliation_state": "WI"
    },
    {
        "paper_title": "Survey of Deep Reinforcement Learning Methods with Evolutionary Algorithms",
        "publication": "Jisuanji Xuebao/Chinese Journal of Computers",
        "citied_by": "8",
        "cover_date": "2022-07-01",
        "Abstract": "Deep reinforcement learning is one of the most important branches in the field of machine learning, which can achieve end-to-end learning through direct interaction with the environment and is capable of solving high-dimensional and large-scale problems. Although deep reinforcement learning has achieved remarkable results, it still faces problems such as insufficient exploration of the environment, poor robustness, and susceptibility of gradients caused by deceptive rewards. In general, evolutionary algorithms have good global search ability, robustness, parallelism and other advantages. Therefore, the methods combining evolutionary algorithms with deep reinforcement learning to compensate the inadequacy of deep reinforcement learning methods have become a research hotspot recently. This paper focuses on the applications of evolutionary algorithms in model-free deep reinforcement learning methods. We introduce evolutionary algorithms and basic methods of reinforcement learning firstly. After that, we introduce the characteristics, advantages, disadvantages, and applicable tasks of evolutionary algorithms, deep reinforcement learning algorithms, and combined methods of evolutionary algorithms and deep reinforcement learning, showing the necessity of combined methods from a different aspect. Then, two types of reinforcement learning methods with evolutionary algorithms are elaborated, which are reinforcement learning with evolutionary algorithms guided policy search and combination of evolutionary algorithms and deep reinforcement learning. In reinforcement learning with evolutionary algorithms guided policy search methods, we categorize the different policy search methods into parameter distribution search methods, policy gradient approximation methods, and policy population search methods. Parameter distribution search methods regard the parameters of a policy as a distribution and sample the parameters from this distribution to form a new policy. Policy gradient approximation methods use the fitness of the policy as an approximation of the gradient to update the parameters. Policy population search methods search directly from individuals in the policy population and select the individual with higher fitness. Then, we focus on the combined methods of evolutionary algorithms and deep reinforcement learning which attracts the interest of scholars currently, including evolutionary algorithm experience-guided deep reinforcement learning methods and evolutionary algorithm modules-embedded deep reinforcement learning methods. The evolutionary algorithm experience-guided deep reinforcement learning methods use experience obtained from individuals by continually interacting with the environment to guide the value network of reinforcement learning, while the evolutionary algorithm module-embedded deep reinforcement learning methods embed the evolutionary algorithm as an auxiliary module in the learning process of reinforcement learning. Furthermore, we compare and analyze these methods in detail. In particular, we compare the characteristics of various algorithms in the methods combing evolutionary algorithms and deep reinforcement learning, including without-feedback guidance methods and with-feedback guidance methods. We also compare the performance of various widely-used algorithms in with-feedback guidance methods on the continuous control tasks of MuJoCo and give a detailed analysis and future directions for improvement and research. Finally, we summarize all the combined methods of evolutionary algorithms and deep reinforcement learning mentioned in the paper, and we study the research emphasis and development trend of this field. Although evolutionary deep reinforcement learning frameworks have been proposed, we think these methods still require further theoretical study to balance the issues of exploration and exploitation.",
        "DOI": "10.11897/SP.J.1016.2022.01478",
        "paper_author": "Lü S.",
        "affiliation_name": "Jilin University",
        "affiliation_city": "Changchun",
        "affiliation_country": "China",
        "affiliation_id": "60007711",
        "affiliation_state": "Jilin"
    },
    {
        "paper_title": "Contrasting Forest Loss and Gain Patterns in Subtropical China Detected Using an Integrated LandTrendr and Machine‐Learning Method",
        "publication": "Remote Sensing",
        "citied_by": "14",
        "cover_date": "2022-07-01",
        "Abstract": "China has implemented a series of forestry law, policies, regulations, and afforestation projects since the 1970s. However, their impacts on the spatial and temporal patterns of forests have not been fully assessed yet. The lack of an accurate, high‐resolution, and long‐term forest disturbance and recovery dataset has impeded this assessment. Here we improved the forest loss and gain detections by integrating the LandTrendr change detection algorithm with the Random Forest (RF) machine‐learning method and applied it to assess forest loss and gain patterns in the Zhejiang, Jiangxi, and Guangxi Provinces of the subtropical vegetation in China. The accuracy evaluation indicated that our approach can adequately detect the spatial and temporal distribution patterns in forest gain and loss, with an overall accuracy of 93% and the Kappa coefficient of 0.89. The forest loss area was 8.30 × 104 km2 in the Zhejiang, Jiangxi, and Guangxi Provinces during 1986–2019, accounting for 43.52% of total forest area in 1986, while the forest gain area was 20.25 × 104 km2, accounting for 106.19% of total forest area in 1986. Although the interannual variation patterns were similar among three provinces, the forest loss and gain area and the magnitude of change trends were significantly different. Guangxi has the largest forest loss and gain area and increasing trends, followed by Jiangxi, and the least in Zhejiang. The variations in annual forest loss and gain area can be mostly explained by the timelines of major forestry policies and regulations. Our study would provide an applicable method and data for assessing the impacts of forest disturbance events and forestry policies and regulations on the spatial and temporal patterns of forest loss and gain in China, and further contributing to regional and national forest carbon and greenhouse gases budget estimations.",
        "DOI": "10.3390/rs14133238",
        "paper_author": "Shen J.",
        "affiliation_name": "Zhejiang Agriculture and Forestry University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60103970",
        "affiliation_state": "Zhejiiang"
    },
    {
        "paper_title": "Variable Impedance Skill Learning for Contact-Rich Manipulation",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "13",
        "cover_date": "2022-07-01",
        "Abstract": "Contact-rich manipulation tasks remain a hard problem in robotics that requires interaction with unstructured environments. Reinforcement Learning (RL) is one potential solution to such problems, as it has been successfully demonstrated on complex continuous control tasks. Nevertheless, current state-of-The-Art methods require policy training in simulation to prevent undesired behavior and later domain transfer even for simple skills involving contact. In this paper, we address the problem of learning contact-rich manipulation policies by extending an existing skill-based RL framework with a variable impedance action space. Our method leverages a small set of suboptimal demonstration trajectories and learns from both position, but also crucially impedance-space information. We evaluate our method on a number of peg-in-hole task variants with a Franka Panda arm and demonstrate that learning variable impedance actions for RL in Cartesian space can be deployed directly on the real robot, without resorting to learning in simulation.",
        "DOI": "10.1109/LRA.2022.3187276",
        "paper_author": "Yang Q.",
        "affiliation_name": "Örebro Universitet",
        "affiliation_city": "Orebro",
        "affiliation_country": "Sweden",
        "affiliation_id": "60008141",
        "affiliation_state": "Orebro"
    },
    {
        "paper_title": "Learning Pneumatic Non-Prehensile Manipulation with a Mobile Blower",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "6",
        "cover_date": "2022-07-01",
        "Abstract": "We investigate pneumatic non-prehensile manipulation (i.e., blowing) as a means of efficiently moving scattered objects into a target receptacle. Due to the chaotic nature of aerodynamic forces, a blowing controller must i) continually adapt to unexpected changes from its actions, ii) maintain fine-grained control, since the slightest misstep can result in large unintended consequences (e.g., scatter objects already in a pile), and iii) infer long-range plans (e.g., move the robot to strategic blowing locations). We tackle these challenges in the context of deep reinforcement learning, introducing a multi-frequency version of the spatial action maps framework. This allows for efficient learning of vision-based policies that effectively combine high-level planning and low-level closed-loop control for dynamic mobile manipulation. Experiments show that our system learns efficient behaviors for the task, demonstrating in particular that blowing achieves better downstream performance than pushing, and that our policies improve performance over baselines. Moreover, we show that our system naturally encourages emergent specialization between the different subpolicies spanning low-level fine-grained control and high-level planning. On a real mobile robot equipped with a miniature air blower, we show that our simulation-trained policies transfer well to a real environment and can generalize to novel objects.",
        "DOI": "10.1109/LRA.2022.3187833",
        "paper_author": "Wu J.",
        "affiliation_name": "School of Engineering and Applied Science",
        "affiliation_city": "Princeton",
        "affiliation_country": "United States",
        "affiliation_id": "60141284",
        "affiliation_state": "NJ"
    },
    {
        "paper_title": "Deep Reinforcement Learning for Integrated Non-Linear Control of Autonomous UAVs",
        "publication": "Processes",
        "citied_by": "27",
        "cover_date": "2022-07-01",
        "Abstract": "In this research, an intelligent control architecture for an experimental Unmanned Aerial Vehicle (UAV) bearing unconventional inverted V-tail design, is presented. To handle UAV’s inherent control complexities, while keeping them computationally acceptable, a variant of distinct Deep Reinforcement Learning (DRL) algorithm, namely Deep Deterministic Policy Gradient (DDPG) is proposed. Conventional DDPG algorithm after being modified in its learning architecture becomes capable of intelligently handling the continuous state and control space domains besides controlling the platform in its entire flight regime. Nonlinear simulations were then performed to analyze UAV performance under different environmental and launch conditions. The effectiveness of the proposed strategy is further demonstrated by comparing the results with the linear controller for the same UAV whose feedback loop gains are optimized by employing technique of optimal control theory. Results indicate the significance of the proposed control architecture and its inherent capability to adapt dynamically to the changing environment, thereby making it of significant utility to airborne UAV applications.",
        "DOI": "10.3390/pr10071307",
        "paper_author": "Din A.F.U.",
        "affiliation_name": "Air University Islamabad",
        "affiliation_city": "Islamabad",
        "affiliation_country": "Pakistan",
        "affiliation_id": "60070604",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Cluster Analysis of US COVID‐19 Infected States for Vaccine Distribution",
        "publication": "Healthcare (Switzerland)",
        "citied_by": "2",
        "cover_date": "2022-07-01",
        "Abstract": "Since December 2019, COVID‐19 has been raging worldwide. To prevent the spread of COVID‐19 infection, many countries have proposed epidemic prevention policies and quickly ad-ministered vaccines, However, under facing a shortage of vaccines, the United States did not put forward effective epidemic prevention policies in time to prevent the infection from expanding, re-sulting in the epidemic in the United States becoming more and more serious. Through “The COVID Tracking Project”, this study collects medical indicators for each state in the United States from 2020 to 2021, and through feature selection, each state is clustered according to the epidemic’s severity. Furthermore, through the confusion matrix of the classifier to verify the accuracy of the cluster anal-ysis, the study results show that the Cascade K‐means cluster analysis has the highest accuracy. This study also labeled the three clusters of the cluster analysis results as high, medium, and low infection levels. Policymakers could more objectively decide which states should prioritize vaccine allocation in a vaccine shortage to prevent the epidemic from continuing to expand. It is hoped that if there is a similar epidemic in the future, relevant policymakers can use the analysis procedure of this study to determine the allocation of relevant medical resources for epidemic prevention according to the severity of infection in each state to prevent the spread of infection.",
        "DOI": "10.3390/healthcare10071235",
        "paper_author": "Shih D.H.",
        "affiliation_name": "National Yunlin University of Science and Technology",
        "affiliation_city": "Douliou",
        "affiliation_country": "Taiwan",
        "affiliation_id": "60014261",
        "affiliation_state": "Yunlin"
    },
    {
        "paper_title": "Source Term Estimation Using Deep Reinforcement Learning With Gaussian Mixture Model Feature Extraction for Mobile Sensors",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "9",
        "cover_date": "2022-07-01",
        "Abstract": "This paper proposes a deep reinforcement learning method for mobile sensors to estimate the properties of the source of the hazardous gas release. The problem of estimating the properties of the released gas is generally termed as the source term estimation (STE) problem. Since the sensor measurements from atmospheric gas dispersion are sparse, intermittent, and time-varying due to the turbulence and the sensor noise, STE is considered to be a challenging problem. The particle filter is adopted to estimate the source term under such stochastic noise conditions. The deep deterministic policy gradient (DDPG) is also employed to find the best source search policy in terms of successful estimation and traveled distance. Through ablation studies, we demonstrate that the use of the Gaussian mixture model, which clusters the potential source positions from the particle filter, as an input to the DDPG and the gated recurrent unit functioning as a memory in DDPG help to improve the STE performance. Besides, simulation results in randomized source term conditions and previously-unseen environments show the superior STE performance of the proposed algorithm compared with the existing information-theoretic STE algorithm.",
        "DOI": "10.1109/LRA.2022.3184787",
        "paper_author": "Park M.",
        "affiliation_name": "Ulsan National Institute of Science and Technology",
        "affiliation_city": "Ulsan",
        "affiliation_country": "South Korea",
        "affiliation_id": "60103153",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Multiple-UAV Reinforcement Learning Algorithm Based on Improved PPO in Ray Framework",
        "publication": "Drones",
        "citied_by": "32",
        "cover_date": "2022-07-01",
        "Abstract": "Distributed multi-agent collaborative decision-making technology is the key to general artificial intelligence. This paper takes the self-developed Unity3D collaborative combat environment as the test scenario, setting a task that requires heterogeneous unmanned aerial vehicles (UAVs) to perform a distributed decision-making and complete cooperation task. Aiming at the problem of the traditional proximal policy optimization (PPO) algorithm’s poor performance in the field of complex multi-agent collaboration scenarios based on the distributed training framework Ray, the Critic network in the PPO algorithm is improved to learn a centralized value function, and the muti-agent proximal policy optimization (MAPPO) algorithm is proposed. At the same time, the inheritance training method based on course learning is adopted to improve the generalization performance of the algorithm. In the experiment, MAPPO can obtain the highest average accumulate reward compared with other algorithms and can complete the task goal with the fewest steps after convergence, which fully demonstrates that the MAPPO algorithm outperforms the state-of-the-art.",
        "DOI": "10.3390/drones6070166",
        "paper_author": "Zhan G.",
        "affiliation_name": "Northwestern Polytechnical University",
        "affiliation_city": "Xi'an",
        "affiliation_country": "China",
        "affiliation_id": "60003977",
        "affiliation_state": "Shaanxi"
    },
    {
        "paper_title": "Autonomous State-Based Flipper Control for Articulated Tracked Robots in Urban Environments",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "7",
        "cover_date": "2022-07-01",
        "Abstract": "We demonstrate a hybrid approach to autonomous flipper control, focusing on a fusion of hard-coded and learned knowledge. The result is a sample-efficient and modifiable control structure that can be used in conjunction with a mapping/navigation stack. The backbone of the control policy is formulated as a state machine whose states define various flipper action templates and local control behaviors. It is also used as an interface that facilitates the gathering of demonstrations to train the transitions of the state machine. We propose a soft-differentiable state machine neural network that mitigates the shortcomings of its naively implemented counterpart and improves over a multi-layer perceptron baseline in the task of state-transition classification. We show that by training on several minutes of user-gathered demonstrations in simulation, our approach is capable of a zero-shot domain transfer to a wide range of obstacles on a similar real robotic platform. Our results show a considerable increase in performance over a previous competing approach in several essential criteria. A subset of this work was successfully used in the Defense Advanced Research Projects Agency (DARPA) Subterranean Challenge to alleviate the operator of manual flipper control. We autonomously traversed stairs and other obstacles, improving map coverage.",
        "DOI": "10.1109/LRA.2022.3185762",
        "paper_author": "Azayev T.",
        "affiliation_name": "Czech Technical University in Prague",
        "affiliation_city": "Prague",
        "affiliation_country": "Czech Republic",
        "affiliation_id": "60013323",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Forecasting Crude Oil Consumption in Poland Based on LSTM Recurrent Neural Network",
        "publication": "Energies",
        "citied_by": "18",
        "cover_date": "2022-07-01",
        "Abstract": "Primary fuels, i.e., crude oil, natural gas, and power coal, dominate the total global demand for primary energy. Among them, crude oil plays a particularly important role due to the universality of applications and the practical lack of substitutes in transport. Crude oil is also one of the main sources of primary energy in Poland and accounts for around 30% of the energy consumed. Poland covers only 3% of its needs from domestic deposits. The rest is imported from Russia, Saudi Arabia, Nigeria, Great Britain, Kazakhstan, and Norway. Due to such a high import of raw material, Poland must anticipate future demand. On the one hand, this article aims to analyze the current (2020) and future (2040) crude oil consumption on the Polish market. The study analyzes the geo-political and economic foundations of the functioning of the energy raw-materials market, the crude oil supply, the structure of Poland’s energy mix, and assumptions about the energy policy until 2040. On the other hand, conclusions from the research were used to build a model of crude oil consumption for the internal market. It has been also shown that the consumption of crude oil on the Polish market is a nonlinear phenomenon with a small set of statistical data, which makes it difficult to build an accurate model. This paper proposes a new model based on artificial neural networks that includes long-term memory (LSTM). The accuracy of the constructed model was assessed using the MSE, Theil, and Janus coefficients. The results show that LSTM models can be used to forecast crude oil consumption, and they cope with the nonstationary and nonlinear time series. Many important contemporary problems posed in the field of energy economy are also discussed, and it is proposed to solve them with the use of modern machine-learning tools.",
        "DOI": "10.3390/en15134885",
        "paper_author": "Manowska A.",
        "affiliation_name": "Silesian University of Technology",
        "affiliation_city": "Gliwice",
        "affiliation_country": "Poland",
        "affiliation_id": "60009081",
        "affiliation_state": "Silesian"
    },
    {
        "paper_title": "Big Health Data Research and Group Harm: The Scope of IRB Review",
        "publication": "Ethics and Human Research",
        "citied_by": "20",
        "cover_date": "2022-07-01",
        "Abstract": "Much of precision medicine is driven by big health data research—the analysis of massive datasets representing the complex web of genetic, behavioral, environmental, and other factors that impact human well-being. There are some who point to the Common Rule, the regulation governing federally funded human subjects research, as a regulatory panacea for all types of big health data research. But how well does the Common Rule fit the regulatory needs of this type of research? This article suggests that harms that may arise from artificial intelligence and machine-learning technologies used in big health data research—and the increased likelihood that this research will affect public policy—mean it is time to consider whether the current human research regulations prohibit comprehensive, ethical review of big health data research that may result in group harm.",
        "DOI": "10.1002/eahr.500130",
        "paper_author": "Doerr M.",
        "affiliation_name": "Sage Bionetworks",
        "affiliation_city": "Seattle",
        "affiliation_country": "United States",
        "affiliation_id": "60111926",
        "affiliation_state": "WA"
    },
    {
        "paper_title": "MPR-RL: Multi-Prior Regularized Reinforcement Learning for Knowledge Transfer",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "7",
        "cover_date": "2022-07-01",
        "Abstract": "In manufacturing, assembly tasks have been a challenge for learning algorithms due to variant dynamics of different environments. Reinforcement learning (RL) is a promising framework to automatically learn these tasks, yet it is still not easy to apply a learned policy or skill, that is the ability of solving a task, to a similar environment even if the deployment conditions are only slightly different. In this letter, we address the challenge of transferring knowledge within a family of similar tasks by leveraging multiple skill priors. We propose to learn prior distribution over the specific skill required to accomplish each task and compose the family of skill priors to guide learning the policy for a new task by comparing the similarity between the target task and the prior ones. Our method learns a latent action space representing the skill embedding from demonstrated trajectories for each prior task. We have evaluated our method on a task in simulation and a set of peg-in-hole insertion tasks and demonstrate better generalization to new tasks that have never been encountered during training. Our Multi-Prior Regularized RL (MPR-RL) method is deployed directly on a real world Franka Panda arm, requiring only a set of demonstrated trajectories from similar, but crucially not identical, problem instances.",
        "DOI": "10.1109/LRA.2022.3184805",
        "paper_author": "Yang Q.",
        "affiliation_name": "Örebro Universitet",
        "affiliation_city": "Orebro",
        "affiliation_country": "Sweden",
        "affiliation_id": "60008141",
        "affiliation_state": "Orebro"
    },
    {
        "paper_title": "Incorporating a Machine Learning Model into a Web-Based Administrative Decision Support Tool for Predicting Workplace Absenteeism",
        "publication": "Information (Switzerland)",
        "citied_by": "3",
        "cover_date": "2022-07-01",
        "Abstract": "Productivity losses caused by absenteeism at work cost U.S. employers billions of dollars each year. In addition, employers typically spend a considerable amount of time managing employees who perform poorly. By using predictive analytics and machine learning algorithms, organizations can make better decisions, thereby increasing organizational productivity, reducing costs, and im-proving efficiency. Thus, in this paper we propose hybrid optimization methods in order to find the most parsimonious model for absenteeism classification. We utilized data from a Brazilian courier company. In order to categorize absenteeism classes, we preprocessed the data, selected the attributes via multiple methods, balanced the dataset using the synthetic minority over-sampling method, and then employed four methods of machine learning classification: Support Vector Machine (SVM), Multinomial Logistic Regression (MLR), Artificial Neural Network (ANN), and Random Forest (RF). We selected the best model based on several validation scores, and compared its performance against the existing model. Furthermore, project managers may lack experience in machine learning, or may not have the time to spend developing machine learning algorithms. Thus, we propose a web-based interactive tool supported by cognitive analytics management (CAM) theory. The web-based decision tool enables managers to make more informed decisions, and can be used without any prior knowledge of machine learning. Understanding absenteeism patterns can assist managers in revising policies or creating new arrangements to reduce absences in the workplace, financial losses, and the probability of economic insolvency.",
        "DOI": "10.3390/info13070320",
        "paper_author": "Nath G.",
        "affiliation_name": "Murray State University",
        "affiliation_city": "Murray",
        "affiliation_country": "United States",
        "affiliation_id": "60005783",
        "affiliation_state": "KY"
    },
    {
        "paper_title": "Weekly Nowcasting of New COVID-19 Cases Using Past Viral Load Measurements",
        "publication": "Viruses",
        "citied_by": "5",
        "cover_date": "2022-07-01",
        "Abstract": "The rapid spread of the coronavirus disease COVID-19 has imposed clinical and financial burdens on hospitals and governments attempting to provide patients with medical care and implement disease-controlling policies. The transmissibility of the disease was shown to be correlated with the patient’s viral load, which can be measured during testing using the cycle threshold (Ct). Previous models have utilized Ct to forecast the trajectory of the spread, which can provide valuable information to better allocate resources and change policies. However, these models combined other variables specific to medical institutions or came in the form of compartmental models that rely on epidemiological assumptions, all of which could impose prediction uncertainties. In this study, we overcome these limitations using data-driven modeling that utilizes Ct and previous number of cases, two institution-independent variables. We collected three groups of patients (n = 6296, n = 3228, and n = 12,096) from different time periods to train, validate, and independently validate the models. We used three machine learning algorithms and three deep learning algorithms that can model the temporal dynamic behavior of the number of cases. The endpoint was 7-week forward number of cases, and the prediction was evaluated using mean square error (MSE). The sequence-to-sequence model showed the best prediction during validation (MSE = 0.025), while polynomial regression (OLS) and support vector machine regression (SVR) had better performance during independent validation (MSE = 0.1596, and MSE = 0.16754, respectively), which exhibited better generalizability of the latter. The OLS and SVR models were used on a dataset from an external institution and showed promise in predicting COVID-19 incidences across institutions. These models may support clinical and logistic decision-making after prospective validation.",
        "DOI": "10.3390/v14071414",
        "paper_author": "Khalil A.",
        "affiliation_name": "CASE School of Medicine",
        "affiliation_city": "Cleveland",
        "affiliation_country": "United States",
        "affiliation_id": "60006404",
        "affiliation_state": "OH"
    },
    {
        "paper_title": "Novel Insights in Spatial Epidemiology Utilizing Explainable AI (XAI) and Remote Sensing",
        "publication": "Remote Sensing",
        "citied_by": "30",
        "cover_date": "2022-07-01",
        "Abstract": "The COVID-19 pandemic has affected many aspects of human life around the world, due to its tremendous outcomes on public health and socio-economic activities. Policy makers have tried to develop efficient responses based on technologies and advanced pandemic control methodologies, to limit the wide spreading of the virus in urban areas. However, techniques such as social isolation and lockdown are short-term solutions that minimize the spread of the pandemic in cities and do not invert long-term issues that derive from climate change, air pollution and urban planning challenges that enhance the spreading ability. Thus, it seems crucial to understand what kind of factors assist or prevent the wide spreading of the virus. Although AI frameworks have a very efficient predictive ability as data-driven procedures, they often struggle to identify strong correlations among multidimensional data and provide robust explanations. In this paper, we propose the fusion of a heterogeneous, spatio-temporal dataset that combine data from eight European cities spanning from 1 January 2020 to 31 December 2021 and describe atmospheric, socio-economic, health, mobility and environmental factors all related to potential links with COVID-19. Remote sensing data are the key solution to monitor the availability on public green spaces between cities in the study period. So, we evaluate the benefits of NIR and RED bands of satellite images to calculate the NDVI and locate the percentage in vegetation cover on each city for each week of our 2-year study. This novel dataset is evaluated by a tree-based machine learning algorithm that utilizes ensemble learning and is trained to make robust predictions on daily cases and deaths. Comparisons with other machine learning techniques justify its robustness on the regression metrics RMSE and MAE. Furthermore, the explainable frameworks SHAP and LIME are utilized to locate potential positive or negative influence of the factors on global and local level, with respect to our model’s predictive ability. A variation of SHAP, namely treeSHAP, is utilized for our tree-based algorithm to make fast and accurate explanations.",
        "DOI": "10.3390/rs14133074",
        "paper_author": "Temenos A.",
        "affiliation_name": "National Technical University of Athens (NTUA)",
        "affiliation_city": "Athens",
        "affiliation_country": "Greece",
        "affiliation_id": "60002947",
        "affiliation_state": "Attica"
    },
    {
        "paper_title": "Analysis of Renewable Energy Policies through Decision Trees",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "4",
        "cover_date": "2022-07-01",
        "Abstract": "This paper presents an alternative way of making predictions on the effectiveness and efficacy of Renewable Energy (RE) policies using Decision Trees (DT). As a data-driven process for decision-making, the analysis uses the Renewable Energy (RE) target achievement, predicting whether or not a RE target will likely be achieved (efficacy) and to what degree (effectiveness), depending on the different criteria, including geographical context, characterizing concerns, and policy characteristics. The results suggest different criteria that could help policymakers in designing policies with a higher propensity to achieve the desired goal. Using this tool, the policy decision-makers can better test/predict whether the target will be achieved and to what degree. The novelty in the present paper is the application of Machine Learning methods (through the Decision Trees) for energy policy analysis. Machine learning methodologies present an alternative way to pilot RE policies before spending lots of time, money, and other resources. We also find that using Machine Learning techniques underscores the importance of data availability. A general summary for policymakers has been included.",
        "DOI": "10.3390/su14137720",
        "paper_author": "Ortiz D.",
        "affiliation_name": "Universidade do Porto",
        "affiliation_city": "Porto",
        "affiliation_country": "Portugal",
        "affiliation_id": "60007249",
        "affiliation_state": "Porto"
    },
    {
        "paper_title": "Learning-Oriented QoS- and Drop-Aware Task Scheduling for Mixed-Criticality Systems",
        "publication": "Computers",
        "citied_by": "5",
        "cover_date": "2022-07-01",
        "Abstract": "In Mixed-Criticality (MC) systems, multiple functions with different levels of criticality are integrated into a common platform in order to meet the intended space, cost, and timing requirements in all criticality levels. To guarantee the correct, and on-time execution of higher criticality tasks in emergency modes, various design-time scheduling policies have been recently presented. These techniques are mostly pessimistic, as the occurrence of worst-case scenario at run-time is a rare event. Nevertheless, they lead to an under-utilized system due to frequent drops of Low-Criticality (LC) tasks, and creation of unused slack times due to the quick execution of high-criticality tasks. Accordingly, this paper proposes a novel optimistic scheme, that introduces a learning-based drop-aware task scheduling mechanism, which carefully monitors the alterations in the behaviour of the MC system at run-time, to exploit the generated dynamic slacks for reducing the LC tasks penalty and preventing frequent drops of LC tasks in the future. Based on an extensive set of experiments, our observations have shown that the proposed approach exploits accumulated dynamic slack generated at run-time, by 9.84% more on average compared to existing works, and is able to reduce the deadline miss rate by up to 51.78%, and 33.27% on average, compared to state-of-the-art works.",
        "DOI": "10.3390/computers11070101",
        "paper_author": "Ranjbar B.",
        "affiliation_name": "Technische Universität Dresden",
        "affiliation_city": "Dresden",
        "affiliation_country": "Germany",
        "affiliation_id": "60018353",
        "affiliation_state": "Sachsen"
    },
    {
        "paper_title": "A Social Media Infodemic-Based Prediction Model for the Number of Severe and Critical COVID-19 Patients in the Lockdown Area",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "4",
        "cover_date": "2022-07-01",
        "Abstract": "Accurately predicting the number of severe and critical COVID-19 patients is critical for the treatment and control of the epidemic. Social media data have gained great popularity and wide-spread application in various research domains. The viral-related infodemic outbreaks have oc-curred alongside the COVID-19 outbreak. This paper aims to discover trustworthy sources of social media data to improve the prediction performance of severe and critical COVID-19 patients. The innovation of this paper lies in three aspects. First, it builds an improved prediction model based on machine learning. This model helps predict the number of severe and critical COVID-19 patients on a specific urban or regional scale. The effectiveness of the prediction model, shown as accuracy and satisfactory robustness, is verified by a case study of the lockdown in Hubei Province. Second, it finds the transition path of the impact of social media data for predicting the number of severe and critical COVID-19 patients. Third, this paper provides a promising and powerful model for COVID-19 prevention and control. The prediction model can help medical organizations to realize a prediction of COVID-19 severe and critical patients in multi-stage with lead time in specific areas. This model can guide the Centers for Disease Control and Prevention and other clinic institutions to expand the monitoring channels and research methods concerning COVID-19 by using web-based social media data. The model can also facilitate optimal scheduling of medical resources as well as prevention and control policy formulation.",
        "DOI": "10.3390/ijerph19138109",
        "paper_author": "Yan Q.",
        "affiliation_name": "Beihang University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60013789",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Assessing the Yield of Wheat Using Satellite Remote Sensing-Based Machine Learning Algorithms and Simulation Modeling",
        "publication": "Remote Sensing",
        "citied_by": "39",
        "cover_date": "2022-07-01",
        "Abstract": "Globally, estimating crop acreage and yield is one of the most critical issues that policy and decision makers need for assessing annual crop productivity and food supply. Nowadays, satellite remote sensing and geographic information system (GIS) can enable the estimation of these crop production parameters over large geographic areas. The present work aims to estimate the wheat (Triticum aestivum) acreage and yield of Maharajganj, Uttar Pradesh, India, using satellite-based data products and the Carnegie-Ames-Stanford Approach (CASA) model. Uttar Pradesh is the largest wheat-producing state in India, and this district is well known for its quality organic wheat. India is the leader in wheat grain export, and, hence, its monitoring of growth and yield is one of the top economic priorities of the country. For the calculation of wheat acreage, we performed supervised classification using the Random Forest (RF) and Support Vector Machine classifiers and compared their classification accuracy based on ground-truthing. We found that RF performed a significantly accurate acreage assessment (kappa coefficient 0.84) compared to SVM (0.68). The CASA model was then used to calculate the winter crop (Rabi, winter-sown, and summer harvested) wheat net primary productivity (NPP) in the study area for the 2020–2021 growth season using the RF-based acreage product. The model used for wheat NPP-yield conversion (CASA) showed 3100.27 to 5000.44 kg/ha over 148,866 ha of the total wheat area. The results showed that in the 2020–2021 growing season, all the districts of Uttar Pradesh had similar wheat growth trends. A total of 30 observational data points were used to verify the CASA model-based estimates of wheat yield. Field-based verification shows that the estimated yield correlates well with the observed yield (R2 = 0.554, RMSE = 3.36 Q/ha, MAE −0.56 t ha−1, and MRE = −4.61%). Such an accuracy for assessing regional wheat yield can prove to be one of the promising methods for calculating the whole region’s agricultural yield. The study concludes that RF classifier-based yield estimation has shown more accurate results and can meet the requirements of a regional-scale wheat grain yield estimation and, thus, can prove highly beneficial in policy and decision making.",
        "DOI": "10.3390/rs14133005",
        "paper_author": "Meraj G.",
        "affiliation_name": "Suresh Gyan Vihar University",
        "affiliation_city": "Jaipur",
        "affiliation_country": "India",
        "affiliation_id": "60114286",
        "affiliation_state": "RJ"
    },
    {
        "paper_title": "High-Resolution Urban Air Quality Mapping for Multiple Pollutants based on Dense Monitoring Data and Machine Learning",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "7",
        "cover_date": "2022-07-01",
        "Abstract": "Spatially explicit urban air quality information is important for urban fine‐management and public life. However, existing air quality measurement methods still have some limitations on spatial coverage and system stability. A micro station is an emerging monitoring system with multiple sensors, which can be deployed to provide dense air quality monitoring data. Here, we proposed a method for urban air quality mapping at high‐resolution for multiple pollutants. By using the dense air quality monitoring data from 448 micro stations in Lanzhou city, we developed a decision tree model to infer the distribution of citywide air quality at a 500 m × 500 m × 1 h resolution, with a coefficient of determination (R2) value of 0.740 for PM2.5, 0.754 for CO and 0.716 for SO2. Meanwhile, we also show that the deployment density of the monitoring stations can have a significant impact on the air quality inference results. Our method is able to show both short‐term and long‐term distribution of multiple important pollutants in the city, which demonstrates the potential and feasibility of dense monitoring data combined with advanced data science methods to support urban atmospheric environment fine‐management, policy making, and public health studies.",
        "DOI": "10.3390/ijerph19138005",
        "paper_author": "Guo R.",
        "affiliation_name": "Northwest Normal University China",
        "affiliation_city": "Lanzhou",
        "affiliation_country": "China",
        "affiliation_id": "60003640",
        "affiliation_state": "Gansu"
    },
    {
        "paper_title": "Monitoring Spatiotemporal Distribution of the GDP of Major Cities in China during the COVID-19 Pandemic",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "13",
        "cover_date": "2022-07-01",
        "Abstract": "Monitoring the fine spatiotemporal distribution of urban GDP is a critical research topic for assessing the impact of the COVID-19 outbreak on economic and social growth. Based on nighttime light (NTL) images and urban land use data, this study constructs a GDP machine learning and linear estimation model. Based on the linear model with better effect, the monthly GDP of 34 cities in China is estimated and the GDP spatialization is realized, and finally the GDP spatiotemporal correction is processed. This study analyzes the fine spatiotemporal distribution of GDP, reveals the spatiotemporal change trend of GDP in China’s major cities during the current COVID-19 pandemic, and explores the differences in the economic impact of the COVID-19 pandemic on China’s major cities. The result shows: (1) There is a significant linear association between the total value of NTL and the GDP of subindustries, with R2 models generated by the total value of NTL and the GDP of secondary and tertiary industries being 0.83 and 0.93. (2) The impact of the COVID-19 pandemic on the GDP of cities with varied degrees of development and industrial structures obviously varies across time and space. The GDP of economically developed cities such as Beijing and Shanghai are more affected by COVID-19, while the GDP of less developed cities such as Xining and Lanzhou are less affected by COVID-19. The GDP of China’s major cities fell significantly in February. As the COVID-19 outbreak was gradually brought under control in March, different cities achieved different levels of GDP recovery. This study establishes a fine spatial and temporal distribution estimation model of urban GDP by industry; it accurately monitors and assesses the spatial and temporal distribution characteristics of urban GDP during the COVID-19 pandemic, reveals the impact mechanism of the COVID-19 pandemic on the economic development of major Chinese cities. Moreover, economically developed cities should pay more attention to the spread of the COVID-19 pandemic. It should do well in pandemic prevention and control in airports and stations with large traffic flow. At the same time, after the COVID-19 pandemic is brought under control, they should speed up the resumption of work and production to achieve economic recovery. This study provides scientific references for COVID-19 pandemic prevention and control measures, as well as for the formulation of urban economic development policies.",
        "DOI": "10.3390/ijerph19138048",
        "paper_author": "Wang Y.",
        "affiliation_name": "Hunan University of Science and Technology",
        "affiliation_city": "Xiangtan",
        "affiliation_country": "China",
        "affiliation_id": "60024617",
        "affiliation_state": "Hunan"
    },
    {
        "paper_title": "Identifying adverse childhood experiences with electronic health records of linked mothers and children in England: a multistage development and validation study",
        "publication": "The Lancet Digital Health",
        "citied_by": "10",
        "cover_date": "2022-07-01",
        "Abstract": "Background: Electronic health records (EHRs) of mothers and children provide an opportunity to identify adverse childhood experiences (ACEs) during crucial periods of childhood development, yet well developed indicators of ACEs remain scarce. We aimed to develop clinically relevant indicators of ACEs for linked EHRs of mothers and children using a multistage prediction model of child maltreatment and maternal intimate partner violence (IPV). Methods: In this multistage development and validation study, we developed a representative population-based birth cohort of mothers and children in England, followed from up to 2 years before birth to up to 5 years after birth across the Clinical Practice Research Datalink (CPRD) GOLD (primary care), Hospital Episode Statistics (secondary care), and the Office for National Statistics mortality register. We included livebirths in England between July 1, 2004, and June 30, 2016, to mothers aged 16–55 years, who had registered with a general practitioner (GP) that met CPRD quality standards before 21 weeks of gestation. The primary outcome (reference standard) was any child maltreatment or maternal IPV in either the mother's or child's record from 2 years before birth (maternal IPV only) to 5 years after birth. We used seven prediction models, combined with expert ratings, to systematically develop indicators. We validated the final indicators by integrating results from machine learning models, survival analyses, and clustering analyses in the validation cohort. Findings: We included data collected between July 1, 2002, and June 27, 2018. Of 376 006 eligible births, we included 211 393 mother–child pairs (422 786 patients) from 400 practices, of whom 126 837 mother–child pairs (60·0%; 240 practices) were randomly assigned to a derivation cohort and 84 556 pairs (40·0%; 160 practices) to a validation cohort. We included 63 indicators in six ACE domains: maternal mental health problems, maternal substance misuse, adverse family environments, child maltreatment, maternal IPV, and high-risk presentations of child maltreatment. Excluding the seven indicators in the reference standard, 56 indicators showed high discriminative validity for the reference standard of any child maltreatment or maternal IPV between 2 years before and 5 years after birth (validation cohort, area under the receiver operating characteristic curve 0·85 [95% CI 0·84–0·86]). During the 2 years before birth and 5 years after birth, the overall period prevalence of maternal IPV and child maltreatment (reference standard) was 2·3% (2876 of 126 837 pairs) in the derivation cohort and 2·3% (1916 of 84 556 pairs) in the validation cohort. During the 2 years before and after birth, the period prevalence was 39·1% (95% CI 38·7–39·5; 34 773 pairs) for any of the 63 ACE indicators, 22·2% (21·8–22·5%; 20 122 pairs) for maternal mental health problems, 15·7% (15·4–16·0%; 14 549 pairs) for adverse family environments, 8·1% (7·8–8·3%; 6808 pairs) for high-risk presentations of child maltreatment, 6·9% (6·7–7·2%; 7856 pairs) for maternal substance misuse, and 3·0% (2·9–3·2%; 2540 pairs) for any child maltreatment (2·4% [2·3–5·6%; 2051 pairs]) and maternal IPV (1·0% [0·8–1·0%; 875 pairs]). 62·6% (21 785 of 34 773 pairs) of ACEs were recorded in primary care only, and 72·3% (25 140 cases) were recorded in the maternal record only. Interpretation: We developed clinically relevant indicators for identifying ACEs using the EHRs of mothers and children presenting to general practices and hospital admissions. Over 70% of ACEs were identified via maternal records and were recorded in primary care by GPs within 2 years of birth, reinforcing the importance of reviewing parental and carer records to inform clinical responses to children. ACE indicators can contribute to longitudinal surveillance informing public health policy and resource allocation. Further evaluation is required to determine how ACE indicators can be used in clinical practice. Funding: None.",
        "DOI": "10.1016/S2589-7500(22)00061-9",
        "paper_author": "Syed S.",
        "affiliation_name": "UCL Great Ormond Street Institute of Child Health",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60012662",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Federated Deep Reinforcement Learning-Based Task Offloading and Resource Allocation for Smart Cities in a Mobile Edge Network",
        "publication": "Sensors",
        "citied_by": "33",
        "cover_date": "2022-07-01",
        "Abstract": "Mobile edge computing (MEC) has become an indispensable part of the era of the intelligent manufacturing industry 4.0. In the smart city, computation-intensive tasks can be offloaded to the MEC server or the central cloud server for execution. However, the privacy disclosure issue may arise when the raw data is migrated to other MEC servers or the central cloud server. Since federated learning has the characteristics of protecting the privacy and improving training performance, it is introduced to solve the issue. In this article, we formulate the joint optimization problem of task offloading and resource allocation to minimize the energy consumption of all Internet of Things (IoT) devices subject to delay threshold and limited resources. A two-timescale federated deep reinforcement learning algorithm based on Deep Deterministic Policy Gradient (DDPG) framework (FL-DDPG) is proposed. Simulation results show that the proposed algorithm can greatly reduce the energy consumption of all IoT devices.",
        "DOI": "10.3390/s22134738",
        "paper_author": "Chen X.",
        "affiliation_name": "Xi'an Jiaotong University",
        "affiliation_city": "Xi'an",
        "affiliation_country": "China",
        "affiliation_id": "60018308",
        "affiliation_state": "Shaanxi"
    },
    {
        "paper_title": "Carbon Emissions Estimation and Spatiotemporal Analysis of China at City Level Based on Multi‐Dimensional Data and Machine Learning",
        "publication": "Remote Sensing",
        "citied_by": "14",
        "cover_date": "2022-07-01",
        "Abstract": "Carbon emissions caused by the massive consumption of energy have brought enormous pressure on the Chinese government. Accurately and rapidly characterizing the spatiotemporal characteristics of Chinese city‐level carbon emissions is crucial for policy decision making. Based on multi‐dimensional data, including nighttime light (NTL) data, land use (LU) data, land surface temperature (LST) data, and added‐value secondary industry (AVSI) data, a deep neural network en-semble (DNNE) model was built to analyze the nonlinear relationship between multi‐dimensional data and province‐level carbon emission statistics (CES) data. The city‐level carbon emissions data were estimated, and the spatiotemporal characteristics were analyzed. As compared to the energy statistics released by partial cities, the results showed that the DNNE model based on multi‐dimen-sional data could well estimate city‐level carbon emissions data. In addition, according to a linear trend analysis and standard deviational ellipse (SDE) analysis of China from 2001 to 2019, we con-cluded that the spatiotemporal changes in carbon emissions at the city level were in accordance with the development of China’s economy. Furthermore, the results can provide a useful reference for the scientific formulation, implementation, and evaluation of carbon emissions reduction poli-cies.",
        "DOI": "10.3390/rs14133014",
        "paper_author": "Lin X.",
        "affiliation_name": "Anhui Normal University",
        "affiliation_city": "Wuhu",
        "affiliation_country": "China",
        "affiliation_id": "60009559",
        "affiliation_state": "Anhui"
    },
    {
        "paper_title": "Article Machine Learning-Based Regression Framework to Predict Health Insurance Premiums",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "36",
        "cover_date": "2022-07-01",
        "Abstract": "Artificial intelligence (AI) and machine learning (ML) in healthcare are approaches to make people’s lives easier by anticipating and diagnosing diseases more swiftly than most medical experts. There is a direct link between the insurer and the policyholder when the distance between an insurance business and the consumer is reduced to zero with the use of technology, especially digital health insurance. In comparison with traditional insurance, AI and machine learning have altered the way insurers create health insurance policies and helped consumers receive services faster. Insurance businesses use ML to provide clients with accurate, quick, and efficient health insurance coverage. This research trained and evaluated an artificial intelligence network-based regression-based model to predict health insurance premiums. The authors predicted the health insurance cost incurred by individuals on the basis of their features. On the basis of various parameters, such as age, gender, body mass index, number of children, smoking habits, and geolocation, an artificial neural network model was trained and evaluated. The experimental results displayed an accuracy of 92.72%, and the authors analyzed the model’s performance using key performance metrics.",
        "DOI": "10.3390/ijerph19137898",
        "paper_author": "Kaushik K.",
        "affiliation_name": "University of Petroleum and Energy Studies",
        "affiliation_city": "Dehradun",
        "affiliation_country": "India",
        "affiliation_id": "60107631",
        "affiliation_state": "UK"
    },
    {
        "paper_title": "Deep Koopman Operator With Control for Nonlinear Systems",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "57",
        "cover_date": "2022-07-01",
        "Abstract": "Recently Koopman operator has become a promising data-driven tool to facilitate real-time control for unknown nonlinear systems. It maps nonlinear systems into equivalent linear systems in embedding space, ready for real-time linear control methods. However, designing an appropriate Koopman embedding function remains a challenging task. Furthermore, most Koopman-based algorithms only consider nonlinear systems with linear control input, resulting in lousy prediction and control performance when the system is fully nonlinear with the control input. In this work, we propose an end-to-end deep learning framework to learn the Koopman embedding function and Koopman Operator together to alleviate such difficulties. We first parameterize the embedding function and Koopman Operator with the neural network and train them end-to-end with the K-steps loss function. Then, an auxiliary control network is augmented to encode the nonlinear state-dependent control term to model the nonlinearity in the control input. This encoded term is considered the new control variable instead to ensure linearity of the modeled system in the embedding system. We next deploy Linear Quadratic Regulator (LQR) on the linear embedding space to derive the optimal control policy and decode the actual control input from the control net. Experimental results demonstrate that our approach outperforms other existing methods, reducing the prediction error by order of magnitude and achieving superior control performance in several nonlinear dynamic systems like damping pendulum, CartPole, and the seven DOF robotic manipulator.",
        "DOI": "10.1109/LRA.2022.3184036",
        "paper_author": "Shi H.",
        "affiliation_name": "Chinese University of Hong Kong",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong",
        "affiliation_id": "60002798",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "AI trading and the limits of EU law enforcement in deterring market manipulation",
        "publication": "Computer Law and Security Review",
        "citied_by": "14",
        "cover_date": "2022-07-01",
        "Abstract": "As in many other sectors of EU economies, ‘Artificial Intelligence’ (AI) has entered the scene of the financial services industry as a game-changer. A growing number of investment firms have been adopting AI, and particularly ‘Machine Learning’ (ML) methods, within the ramification of algorithmic trading. While AI/ML trading is expected to deliver several efficiency gains for capital markets, it also brings unprecedented risks for their safety and integrity due to some of its technical specificities and related additional uncertainties. With a focus on new and emerging risks of AI-driven market manipulation, this study critically assesses the ability of the EU anti-manipulation law and enforcement regime to achieve credible deterrence. It argues that AI trading is currently left operating within a (quasi-)lawless market environment with the ultimate risk of jeopardising EU capital markets’ integrity and stability. It shows how ‘deterrence theory’, as a normative framework, can allow us to think of innovative solutions to fix the many shortcomings of the EU legal framework in the fight against AI-driven market manipulation. In concluding, this study suggests improving the existing EU anti-manipulation law and enforcement regime with a number of policy proposals. Namely, (i) an improved, ‘harm-centric’ definition of manipulation; (ii) an improved, ‘multi-layered’ liability regime for AI-driven manipulation; and (iii) a novel, ‘hybrid’ public-private enforcement institutional architecture through the introduction of market manipulation ‘bounty-hunters’.",
        "DOI": "10.1016/j.clsr.2022.105690",
        "paper_author": "Azzutti A.",
        "affiliation_name": "Universität Hamburg",
        "affiliation_city": "Hamburg",
        "affiliation_country": "Germany",
        "affiliation_id": "60028229",
        "affiliation_state": "Hamburg"
    },
    {
        "paper_title": "Supervised Learning for Arrival Time Estimations in Restaurant Meal Delivery",
        "publication": "Transportation Science",
        "citied_by": "16",
        "cover_date": "2022-07-01",
        "Abstract": "Restaurant meal delivery companies have begun to provide customers with meal arrival time estimations to inform the customers’ selection. Accurate estimations increase customer experience, whereas inaccurate estimations may lead to dissatisfaction. Estimating arrival times is a challenging prediction problem because of uncertainty in both delivery and meal preparation process. To account for both processes, we present an offline and online-offline estimation approaches. Our offline method uses supervised learning to map state features directly to expected arrival times. Our online-offline method pairs online simulations with an offline approximation of the delivery vehicles’ routing policy, again achieved via supervised learning. Our computational study shows that both methods perform comparably to a full near-optimal online simulation at a fraction of the computational time. We present an extensive analysis on how arrival time estimation changes the experience for customers, restaurants, and the platform. Our results indicate that accurate arrival times not only raise service perception but also improve the overall delivery system by guiding customer selections, effectively resulting in faster delivery and fresher food.",
        "DOI": "10.1287/trsc.2021.1095",
        "paper_author": "Hildebrandt F.D.",
        "affiliation_name": "Otto-von-Guericke-Universität Magdeburg",
        "affiliation_city": "Magdeburg",
        "affiliation_country": "Germany",
        "affiliation_id": "60018362",
        "affiliation_state": "Sachsen-Anhalt"
    },
    {
        "paper_title": "Bypassing Web Application Firewalls Using Deep Reinforcement Learning<sup>**</sup>",
        "publication": "ISeCure",
        "citied_by": "3",
        "cover_date": "2022-07-01",
        "Abstract": "Web application firewalls (WAFs) are used for protecting web applications from attacks such as SQL injection, cross-site request forgery, and cross-site scripting. As a result of the growing complexity of web attacks, WAFs need to be tested and updated on a regular basis. There are various tools and techniques to verify the correct performance of a WAF. But most of the techniques are manual or use brute-force attacks, so suffer from poor efficacy. In this work, we propose a solution based on Reinforcement Learning (RL) to discover malicious payloads, which are able to bypass WAFs. We provide an RL framework with an environment compatible with OpenAI gym toolset standards. The environment is employed for training agents to implement WAF circumvention tasks. The agent mutates the syntax of a malicious payload using a set of modification operators as actions, without changes to its semantic. Then, upon WAF’s reaction to the payload, the environment ascertains a reward for the agent. Eventually, based on these rewards, the agent learns a suitable sequence of mutations for any malicious payload. The payloads, which bypass the WAF determine rules defects, which can be further used in rule tuning for rule-based WAFs. Also, it can enrich the machine learning-based WAFs datasets for retraining. We use Q-Learning, Advantage Actor-Critic (A2C), and Proximal Policy Optimization (PPO) algorithms with the deep neural network. Our solution is successful in evading signature-based and machine learning-based WAFs. While our focus in this work is on SQL injection, the method can be simply extended to use for any string-based injection attacks.",
        "DOI": "10.22042/isecure.2022.323140.744",
        "paper_author": "Hemmati M.",
        "affiliation_name": "Malek Ashtar University of Technology",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran",
        "affiliation_id": "60005710",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "72",
        "cover_date": "2022-07-01",
        "Abstract": "General-purpose robots coexisting with humans in their environment must learn to relate human language to their perceptions and actions to be useful in a range of daily tasks. Moreover, they need to acquire a diverse repertoire of general-purpose skills that allow composing long-horizon tasks by following unconstrained language instructions. In this letter, we present Composing Actions from Language and Vision (CALVIN) (Composing Actions from Language and Vision), an open-source simulated benchmark to learn long-horizon language-conditioned tasks. Our aim is to make it possible to develop agents that can solve many robotic manipulation tasks over a long horizon, from onboard sensors, and specified only via human language. CALVIN tasks are more complex in terms of sequence length, action space, and language than existing vision-and-language task datasets and supports flexible specification of sensor suites. We evaluate the agents in zero-shot to novel language instructions and to novel environments. We show that a baseline model based on multi-context imitation learning performs poorly on CALVIN, suggesting that there is significant room for developing innovative agents that learn to relate human language to their world models with this benchmark.",
        "DOI": "10.1109/LRA.2022.3180108",
        "paper_author": "Mees O.",
        "affiliation_name": "Universität Freiburg",
        "affiliation_city": "Freiburg im Breisgau",
        "affiliation_country": "Germany",
        "affiliation_id": "60025641",
        "affiliation_state": "Baden-Wurttemberg"
    },
    {
        "paper_title": "Learning on the Job: Long-Term Behavioural Adaptation in Human-Robot Interactions",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "5",
        "cover_date": "2022-07-01",
        "Abstract": "In this work, we propose a framework for allowing autonomous robots deployed for extended periods of time in public spaces to adapt their own behaviour online from user interactions. The robot behaviour planning is embedded in a Reinforcement Learning (RL) framework, where the objective is maximising the level of overall user engagement during the interactions. We use the Upper-Confidence-Bound Value-Iteration (UCBVI) algorithm, which gives a helpful way of managing the exploration-exploitation trade-off for real-time interactions. An engagement model trained end-to-end generates the reward function in real-time during policy execution. We test this approach in a public museum in Lincoln (U.K.), where the robot is deployed as a tour guide for the visitors. Results show that after a couple of months of exploration, the robot policy learned to maintain the engagement of users for longer, with an increase of 22.8% over the initial static policy in the number of items visited during the tour and a 30% increase in the probability of completing the tour. This work is a promising step toward behavioural adaptation in long-term scenarios for robotics applications in social settings.",
        "DOI": "10.1109/LRA.2022.3178807",
        "paper_author": "Duchetto F.D.",
        "affiliation_name": "University of Lincoln",
        "affiliation_city": "Lincoln",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60026830",
        "affiliation_state": "Lincolnshire"
    },
    {
        "paper_title": "Multiplicity and dynamics of social representations of the COVID-19 pandemic on Chinese social media from 2019 to 2020",
        "publication": "Information Processing and Management",
        "citied_by": "14",
        "cover_date": "2022-07-01",
        "Abstract": "Documenting the emergent social representations of COVID-19 in public communication is necessary for critically reflecting on pandemic responses and providing guidance for global pandemic recovery policies and practices. This study documents the dynamics of changing social representations of the COVID-19 pandemic on one of the largest Chinese social media, Weibo, from December 2019 to April 2020. We draw on the social representation theory (SRT) and conceptualize topics and topic networks as a form of social representation. We analyzed a dataset of 40 million COVID-19 related posts from 9.7 million users (including the general public, opinion leaders, and organizations) using machine learning methods. We identified 12 topics and found an expansion in social representations of COVID-19 from a clinical and epidemiological perspective to a broader perspective that integrated personal illness experiences with economic and sociopolitical discourses. Discussions about COVID-19 science did not take a prominent position in the representations, suggesting a lack of effective science and risk communication. Further, we found the strongest association of social representations existed between the public and opinion leaders and the organizations’ representations did not align much with the other two groups, suggesting a lack of organizations’ influence in public representations of COVID-19 on social media in China.",
        "DOI": "10.1016/j.ipm.2022.102990",
        "paper_author": "Chen A.",
        "affiliation_name": "University of Science and Technology of China",
        "affiliation_city": "Hefei",
        "affiliation_country": "China",
        "affiliation_id": "60019118",
        "affiliation_state": "Anhui"
    },
    {
        "paper_title": "Graph neural network and multi-agent reinforcement learning for machine-process-system integrated control to optimize production yield",
        "publication": "Journal of Manufacturing Systems",
        "citied_by": "23",
        "cover_date": "2022-07-01",
        "Abstract": "In this paper, an integrated control framework is proposed for the optimization of the production yield by integrating different levels of a manufacturing system, including system, process, and machine levels. The manufacturing system is modeled as a graph by treating machines as nodes and material flows as links. The graph model enjoys high flexibility and is able to incorporate all relevant real-time information across all levels of the manufacturing system in the dynamic node feature. Since the real-time tool state is essential for decision making, Recursive Bayesian Estimation (RBE) is adopted to reduce the tool state observations through sensors and machine learning models and provide more accurate tool state estimation to be included into the graph node feature. With the graph model, Graph Neural Network (GNN) is applied to process the node features to generate node embedding that reflects both local and global information. For the integrated control purpose, each machine node is then be treated as a distributed agent in Multi-Agent Reinforcement Learning (MARL) that conditions its policy on the node embedding from GNN. State-of-the-art GNN and MARL algorithms, namely Graph Attention Network (GAT) and Value Decomposition Actor Critic (VDAC), are implemented to train learnable parameters in GNN-MARL networks to learn the optimal multi-agent policy. Extensive numerical experiments and analysis proves the effectiveness of the proposed integrated control framework.",
        "DOI": "10.1016/j.jmsy.2022.05.018",
        "paper_author": "Huang J.",
        "affiliation_name": "University of Virginia School of Engineering and Applied Science",
        "affiliation_city": "Charlottesville",
        "affiliation_country": "United States",
        "affiliation_id": "60152865",
        "affiliation_state": "VA"
    },
    {
        "paper_title": "Support vector regression and ANN approach for predicting the ground water quality",
        "publication": "Journal of the Indian Chemical Society",
        "citied_by": "9",
        "cover_date": "2022-07-01",
        "Abstract": "The current study investigates the potential of well-known artificial neural network (ANN), Support vector regression (SVR), multilinear and multi-nonlinear regression techniques to predict total dissolve solids (TSO) and electrical conductivity (ECO), which are essential water quality indicators. To develop the anticipated models, seven effective parameters: Ca2+ Mg2+ Na+ Cl- SO42- HCO3- and pH were used as input variables. The external validation criteria were employed to address the modeling overfitting. The outcome of the study demonstrated a strong association between experimental and models predicted data. The coefficient of determination was 0.97, 0.96, 0.92, and 0.94 for SVR, ANN, MLR, and MNLR models, respectively. The lowest error value of 5.37 and 7.92 was attained by SVR model for training and testing data, respectively. Performance of the proposed techniques showed relative dominance of SVR compared to ANN, MLR and MNLR. Sensitivity analysis demonstrated that the HCO3- is the most sensitive parameter for both TSO and ECO followed by Cl- and SO42-. The models assessment on external criteria ensured generalized results. Conclusively, the outcome of the present research indicated that formulation of machine learning models for prediction of water quality parameters are cost effective and helpful in river water quality assessment, management and policy making.",
        "DOI": "10.1016/j.jics.2022.100538",
        "paper_author": "Alnuwaiser M.A.",
        "affiliation_name": "Princess Nourah Bint Abdulrahman University",
        "affiliation_city": "Riyadh",
        "affiliation_country": "Saudi Arabia",
        "affiliation_id": "60105146",
        "affiliation_state": "Ar Riyad"
    },
    {
        "paper_title": "SafeAPT: Safe Simulation-to-Real Robot Learning Using Diverse Policies Learned in Simulation",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "8",
        "cover_date": "2022-07-01",
        "Abstract": "The framework of sim-to-real learning, i.e., training policies in simulation and transferring them to real-world systems, is one of the most promising approaches towards data-efficient learning in robotics. However, due to the inevitable reality gap between the simulation and the real world, a policy learned in the simulation may not always generate a safe behaviour on the real robot. As a result, during policy adaptation in the real world, the robot may damage itself or cause harm to its surroundings. In this work, we introduce SafeAPT, a multi-goal robot learning algorithm that leverages a diverse repertoire of policies evolved in simulation and transfers the most promising safe policy to the real robot through episodic interaction. To achieve this, SafeAPT iteratively learns probabilistic reward and safety models from real-world observations using simulated experiences as priors. Then, it performs Bayesian optimization to select the best policy from the repertoire with the reward model, while maintaining the specified safety constraint using the safety model. SafeAPT allows a robot to adapt to a wide range of goals safely with the same repertoire of policies evolved in the simulation. We compare SafeAPT with several baselines, both in simulated and real robotic experiments, and show that SafeAPT finds high-performing policies within a few minutes of real-world operation while minimizing safety violations during the interactions.",
        "DOI": "10.1109/LRA.2022.3177294",
        "paper_author": "Kaushik R.",
        "affiliation_name": "Aalto University",
        "affiliation_city": "Espoo",
        "affiliation_country": "Finland",
        "affiliation_id": "60103653",
        "affiliation_state": "Uusimaa"
    },
    {
        "paper_title": "Chang impact analysis of level 3 COVID-19 alert on air pollution indicators using artificial neural network",
        "publication": "Ecological Informatics",
        "citied_by": "8",
        "cover_date": "2022-07-01",
        "Abstract": "In this study, mean monthly and diurnal variations in fine particulate matters (PM2.5), nitrate, sulfate, and gaseous precursors were investigated during the Level 3 COVID-19 alert from May 19 to July 27 in 2021. For comparison, the historical data during the identical period in 2019 and 2020 were also provided to determine the effect of the Level 3 COVID-19 alert on aerosols and gaseous pollutants concentrations in Taichung City. A machine learning model using the artificial neural network technique coupled with a kinetic model was applied to predict NOx, O3, nitrate (NO3−), and sulfate (SO42−) to investigate potential emission sources and chemical reaction mechanism. D during the Level 3 COVID-19 alert, a decrease in NOx concentration due to a decrease in traffic flow under the NOx-saturated regime was observed to enhance the secondary NO3− and O3 formation. The present models were shown to predict 80.1, 77.0, 72.6, and 67.2% concentrations of NOx, O3, NO3−, and SO42−, respectively, which could help decision-makers for pollutant emissions reduction policies development and air pollution control strategies. It is recommended that more long-term datasets, including water soluble inorganic salts (WIS), precursors including OH radicals, NH3, HNO3, and H2SO4, be provided by regulatory air quality monitoring stations to further improve the prediction model accuracy.",
        "DOI": "10.1016/j.ecoinf.2022.101674",
        "paper_author": "Lin G.Y.",
        "affiliation_name": "Tunghai University",
        "affiliation_city": "Taichung",
        "affiliation_country": "Taiwan",
        "affiliation_id": "60021062",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Discussing street tree planning based on pedestrian volume using machine learning and computer vision",
        "publication": "Building and Environment",
        "citied_by": "12",
        "cover_date": "2022-07-01",
        "Abstract": "An increasing number of cities are advocating people-oriented street tree planning to make cities more walkable, livable, and sustainable. People-oriented planning can be further divided into resident-oriented planning and pedestrian-oriented. Numerous studies and policies have started to include the distribution of residents as an essential factor while planning street trees. However, due to various reasons, existing studies failed to enhance the street tree planning based on pedestrian volume, which has a higher exposure. Our study is the first research that combines street tree and pedestrian volume in discussing existing patterns between them and proposes additional scientific planning suggestions. To achieve the research goal, we proposed a methodology framework called LightGBM with K-fold Max variance Semi-Supervised Learning and DeepLab v3+ (KMSSL-DL). KMSSL-DL combines machine learning and computer vision technology to estimate pedestrian volume with unlabeled data from high dimensional urban features, and extract tree crowns from satellite imagery in the Central Business District, City of Melbourne, Australia. KMSSL part achieved an excellent prediction effect (R2 score = 0.8360, RMSE score = 0.2304). We also used DeepLab v3+ to recognize and extract street trees from Google Earth satellite imagery with good performance (mIoU = 84.37). Lastly, we combined the two results to conduct a pattern analysis, enabling us to find four patterns between street trees and pedestrian volume: more trees - more pedestrians (MTMP), more trees - fewer pedestrians (MTFP), fewer trees - more pedestrians (FTMP), fewer trees - fewer pedestrians (FTFP). We discussed the four patterns and presented planning suggestions, including urban planners giving the most attention to FTMP.",
        "DOI": "10.1016/j.buildenv.2022.109178",
        "paper_author": "Li Z.",
        "affiliation_name": "The University of Hong Kong",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong",
        "affiliation_id": "60006541",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Differential impacts of urbanization characteristics on city-level carbon emissions from passenger transport on road: Evidence from 360 cities in China",
        "publication": "Building and Environment",
        "citied_by": "20",
        "cover_date": "2022-07-01",
        "Abstract": "Although it's well known that the carbon intensity from passenger transport of cities varies widely, few studies assessed the disparities of that in city-level and its underlying factors due to the limited availability of data, and thus developed effective strategies for different types of cities. This study is the first to present a comprehensive inventory of emissions from passenger transport on road for 360 cities in mainland China for 2018, based on the data from 5 transport modes and evaluated by combining distance-based and top-down fuel-based methods. In 2018, passenger transport on road in China emitted 1076 MtC. A large portion of CO2 emissions was identified in the southern and eastern coastal areas and capital cities. GDP, population, and policy were the major factors determining the total CO2 emissions, but not carbon intensity. Clustering analysis of carbon intensity and 9 socio-economic predictors, using a tree-based regression model, clustered the 360 cities into 6 groups and showed that higher carbon intensities occurred in both affluent city groups with a high active population share and less affluent city groups with a low population density but high density of trip destinations. Forward-and-backward stepwise multiple regression analysis indicated that constructing a compact city is more effective for city groups with a high income and high active population share. Enhancing land-use mixed degree is more critical for city groups with a high income and low active population share, while shortening travel distance by intensifying infrastructure construction is more important for the less affluent city groups.",
        "DOI": "10.1016/j.buildenv.2022.109165",
        "paper_author": "Su Y.",
        "affiliation_name": "Guangdong Academy of Sciences",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60281259",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Guiding Army Commanders’ Decision-making Process in Managing Their Suicide Prevention Programs",
        "publication": "Current Psychiatry Reports",
        "citied_by": "0",
        "cover_date": "2022-07-01",
        "Abstract": "Purpose of Review: This paper focuses on how mental health professionals working with Army commanders can help them make decisions based on valid population-based metrics. We first summarize the scope of the impact of suicides on the Army. We then describe the process by which decision-making can be optimized. Recent Findings: The currently available tools in the US Army including BH Pulse, Unit Risk Inventory, The Azimuth Check, and the Army Readiness Assessment Program have a role in assisting mental health professionals. The specific advantages of BH Pulse over the other tools are highlighted. Summary: The US Army has been committed to enhancing its suicide prevention program through comprehensive policies, procedures, and provisions of resources. Commanders are expected to interact with the suicide prevention programs in their units and maximize the systems in place to prevent suicides and other negative mental health outcomes. Commanders are expected to receive cues and signals from a variety of data sources to assist their decision-making process. We discuss the specific advantages of BH Pulse and recommend its routine use for primary prevention and utilizing this tool after incidents to make data-driven, justifiable decisions. Finally, recommendations are provided on enhancing a unit’s suicide prevention program.",
        "DOI": "10.1007/s11920-022-01341-4",
        "paper_author": "Amin R.",
        "affiliation_name": "Walter Reed National Military Medical Center",
        "affiliation_city": "Bethesda",
        "affiliation_country": "United States",
        "affiliation_id": "60006130",
        "affiliation_state": "MD"
    },
    {
        "paper_title": "Data driven predictive maintenance applications for industrial systems with temporal convolutional networks",
        "publication": "Computers and Industrial Engineering",
        "citied_by": "29",
        "cover_date": "2022-07-01",
        "Abstract": "Cyber-physical systems (CPS) are an indispensable aspect of the modern age's data driven industrial systems. These systems can be controlled and monitored with the help of computer-oriented devices and software that are responsible for integrating the physical environment with cyber frameworks. Owing to the nature of operations in any physical process industry, it becomes imperative to deal with potential failures before they occur. To avoid downtime and losses, predictive maintenance is one relevant policy that utilizes prior information and domain knowledge to help in scheduling operations and maintenance. Predictive maintenance (PdM) in industrial applications is known to improve the efficiency, lifetime, and reliability of the machines and thereby reducing the maintenance cost. With the advances in machine learning approaches in cyber physical systems, reliable predictions can be performed to significantly reduce downtime and operational losses associated with the physical processes. In this paper, usefulness of Temporal Convolutional Networks (TCNs) is investigated with the aim of forecasting the remaining useful life (RUL) for Turbofan engines. This paper demonstrates the effectiveness of using TCNs for prognosis under various evaluation conditions and also provides comparison of their performance with hybrid architectures like CNN-LSTM networks and meta-heuristically optimized LSTM networks. The proposed methods were able to achieve upto 94.47% accuracy in case of binary classification tasks and upto 98.7% precision in case of multi-label classification. The cumulative results in accordance to elaborated test cases are presented with the conclusion of the study.",
        "DOI": "10.1016/j.cie.2022.108213",
        "paper_author": "Kumar Sharma D.",
        "affiliation_name": "Netaji Subhas University of Technology",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India",
        "affiliation_id": "60010633",
        "affiliation_state": "DL"
    },
    {
        "paper_title": "Who was at risk for COVID-19 late in the US pandemic? Insights from a population health machine learning model",
        "publication": "Medical and Biological Engineering and Computing",
        "citied_by": "1",
        "cover_date": "2022-07-01",
        "Abstract": "Notable discrepancies in vulnerability to COVID-19 infection have been identified between specific population groups and regions in the USA. The purpose of this study was to estimate the likelihood of COVID-19 infection using a machine-learning algorithm that can be updated continuously based on health care data. Patient records were extracted for all COVID-19 nasal swab PCR tests performed within the Providence St. Joseph Health system from February to October of 2020. A total of 316,599 participants were included in this study, and approximately 7.7% (n = 24,358) tested positive for COVID-19. A gradient boosting model, LightGBM (LGBM), predicted risk of initial infection with an area under the receiver operating characteristic curve of 0.819. Factors that predicted infection were cough, fever, being a member of the Hispanic or Latino community, being Spanish speaking, having a history of diabetes or dementia, and living in a neighborhood with housing insecurity. A model trained on sociodemographic, environmental, and medical history data performed well in predicting risk of a positive COVID-19 test. This model could be used to tailor education, public health policy, and resources for communities that are at the greatest risk of infection. Graphical abstract: [Figure not available: see fulltext.].",
        "DOI": "10.1007/s11517-022-02549-5",
        "paper_author": "Adeoye E.A.",
        "affiliation_name": "Providence St. Joseph Health",
        "affiliation_city": "Renton",
        "affiliation_country": "United States",
        "affiliation_id": "120016258",
        "affiliation_state": "WA"
    },
    {
        "paper_title": "A literature review of Artificial Intelligence applications in railway systems",
        "publication": "Transportation Research Part C: Emerging Technologies",
        "citied_by": "127",
        "cover_date": "2022-07-01",
        "Abstract": "Nowadays it is widely accepted that Artificial Intelligence (AI) is significantly influencing a large number of domains, including railways. In this paper, we present a systematic literature review of the current state-of-the-art of AI in railway transport. In particular, we analysed and discussed papers from a holistic railway perspective, covering sub-domains such as maintenance and inspection, planning and management, safety and security, autonomous driving and control, revenue management, transport policy, and passenger mobility. This review makes an initial step towards shaping the role of AI in future railways and provides a summary of the current focuses of AI research connected to rail transport. We reviewed about 139 scientific papers covering the period from 2010 to December 2020. We found that the major research efforts have been put in AI for rail maintenance and inspection, while very limited or no research has been found on AI for rail transport policy and revenue management. The remaining sub-domains received mild to moderate attention. AI applications are promising and tend to act as a game-changer in tackling multiple railway challenges. However, at the moment, AI research in railways is still mostly at its early stages. Future research can be expected towards developing advanced combined AI applications (e.g. with optimization), using AI in decision making, dealing with uncertainty and tackling newly rising cybersecurity challenges.",
        "DOI": "10.1016/j.trc.2022.103679",
        "paper_author": "Tang R.",
        "affiliation_name": "University of Leeds",
        "affiliation_city": "Leeds",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60012070",
        "affiliation_state": "West Yorkshire"
    },
    {
        "paper_title": "Capricious opinions: A study of polarization of social media groups",
        "publication": "Government Information Quarterly",
        "citied_by": "27",
        "cover_date": "2022-07-01",
        "Abstract": "Experts claim that the world is increasingly polarized by emerging social media platforms. The political actors amplify the polarization through their agents' user-generated content. The extreme political ideologies sway the people sitting on the fence on these social media platforms. Using tweets on a recent policy change on identity in India, the present study seeks to perform a scientific analysis of the polarization of the debates within ordinary citizens' groups from a theoretical lens. We further highlight some of the crucial trends that triggered these polarized discussions in general. Through the lens of Echo chambers and Herd behavior, this study provides valuable insights surrounding the influencers and individuals involved in this discussion where the polarization of preferences is witnessed. Proposing a novel design of a root-level influencer, this study establishes them as polarization actors on a social media platform (Twitter). Through various engagement metrics, we also identify the role of targeted communication (hashtags) and similarity in the users' discussion across the political domain as potential behavioral explanations for opinion polarization on Twitter.",
        "DOI": "10.1016/j.giq.2022.101709",
        "paper_author": "Kushwaha A.K.",
        "affiliation_name": "Indian Institute of Technology Delhi",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India",
        "affiliation_id": "60032730",
        "affiliation_state": "DL"
    },
    {
        "paper_title": "A survey on Zero touch network and Service Management (ZSM) for 5G and beyond networks",
        "publication": "Journal of Network and Computer Applications",
        "citied_by": "93",
        "cover_date": "2022-07-01",
        "Abstract": "Faced with the rapid increase in smart Internet-of-Things (IoT) devices and the high demand for new business-oriented services in the fifth-generation (5G) and beyond network, the management of mobile networks is getting complex. Thus, traditional Network Management and Orchestration (MANO) approaches cannot keep up with rapidly evolving application requirements. This challenge has motivated the adoption of the Zero-touch network and Service Management (ZSM) concept to adapt the automation into network services management. By automating network and service management, ZSM offers efficiency to control network resources and enhance network performance visibility. The ultimate target of the ZSM concept is to enable an autonomous network system capable of self-configuration, self-monitoring, self-healing, and self-optimization based on service-level policies and rules without human intervention. Thus, the paper focuses on conducting a comprehensive survey of E2E ZSM architecture and solutions for 5G and beyond networks. The article begins by presenting the fundamental ZSM architecture and its essential components and interfaces. Then, a comprehensive review of the state-of-the-art for key technical areas, i.e., ZSM automation, cross-domain E2E service lifecycle management, and security aspects, are presented. Furthermore, the paper contains a summary of recent standardization efforts and research projects towards the ZSM realization in 5G and beyond networks. Finally, several lessons learned from the literature and open research problems related to ZSM realization are also discussed in this paper.",
        "DOI": "10.1016/j.jnca.2022.103362",
        "paper_author": "Liyanage M.",
        "affiliation_name": "University College Dublin",
        "affiliation_city": "Dublin",
        "affiliation_country": "Ireland",
        "affiliation_id": "60005141",
        "affiliation_state": "Leinster"
    },
    {
        "paper_title": "Economic policy uncertainty and bankruptcy filings",
        "publication": "International Review of Financial Analysis",
        "citied_by": "17",
        "cover_date": "2022-07-01",
        "Abstract": "Applying machine learning techniques to predict bankruptcy in the sample of French, Italian, Russian and Spanish firms, the study demonstrates that the inclusion of economic policy uncertainty (EPU) indicator into bankruptcy prediction models notably increases their accuracy. This effect is more pronounced when we use novel Twitter-based version of EPU index instead of original news-based index. We further compare the prediction accuracy of machine learning techniques and conclude that stacking ensemble method outperforms (though marginally) machine learning methods, which are more commonly used for bankruptcy prediction, such as single classifiers and bagging.",
        "DOI": "10.1016/j.irfa.2022.102174",
        "paper_author": "Fedorova E.",
        "affiliation_name": "Financial University under the Government of the Russian Federation",
        "affiliation_city": "Moscow",
        "affiliation_country": "Russian Federation",
        "affiliation_id": "60032982",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Improving the Robustness of Reinforcement Learning Policies With L<inf>1</inf>Adaptive Control",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "22",
        "cover_date": "2022-07-01",
        "Abstract": "A reinforcement learning (RL) control policy could fail in a new/perturbed environment that is different from the training environment, due to the presence of dynamic variations. For controlling systems with continuous state and action spaces, we propose an add-on approach to robustifying a pre-trained RL policy by augmenting it with an ${\\mathcal {L}_{1}}$ adaptive controller (${\\mathcal {L}_{1}}$AC). Leveraging the capability of an ${\\mathcal {L}_{1}}$AC for fast estimation and active compensation of dynamic variations, the proposed approach can improve the robustness of an RL policy which is trained either in a simulator or in the real world without consideration of a broad class of dynamic variations. Numerical and real-world experiments empirically demonstrate the efficacy of the proposed approach in robustifying RL policies trained using both model-free and model-based methods.",
        "DOI": "10.1109/LRA.2022.3169309",
        "paper_author": "Cheng Y.",
        "affiliation_name": "The Grainger College of Engineering",
        "affiliation_city": "Urbana",
        "affiliation_country": "United States",
        "affiliation_id": "60158506",
        "affiliation_state": "IL"
    },
    {
        "paper_title": "Artificial intelligence and machine learning for early detection and diagnosis of colorectal cancer in sub-Saharan Africa",
        "publication": "Gut",
        "citied_by": "30",
        "cover_date": "2022-07-01",
        "Abstract": "With the growing resources and investments in AI/ML-based tools in SSA, one could envision a CRC surveillance and diagnosis pipeline that employs MAAA for population-based surveillance and pattern recognition and computer vision algorithms to guide diagnostic recommendations and prognosis. These tools will need to be tailored to local needs based on available resources and testing approaches (eg, sequential testing with MAAA and then FIT) and key stakeholders will need to engage in the codesign of widespread implementation strategies (eg, community-based screening programmes, practitioner education, health policies). Future studies are required to compare the efficacy of these tools to existing CRC surveillance and diagnosis tools (eg, FIT) in SSA populations. Furthermore, these innovative solutions provide opportunities for the adaption and adoption of these approaches in high-income countries. While CRC was used as the use case, these tools could be expanded to other prevalent and emergent cancers (eg, liver, breast and cervical) or other non-communicable diseases that would benefit from lab-based MAAA and computer vision AI-based methods for automated objective assessment of disease diagnosis and prognosis.",
        "DOI": "10.1136/gutjnl-2022-327211",
        "paper_author": "Waljee A.K.",
        "affiliation_name": "VA Medical Center",
        "affiliation_city": null,
        "affiliation_country": "United States",
        "affiliation_id": "60014232",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Fuzzy Deep Deterministic Policy Gradient-Based Motion Controller for Humanoid Robot",
        "publication": "International Journal of Fuzzy Systems",
        "citied_by": "7",
        "cover_date": "2022-07-01",
        "Abstract": "In conventional robot arm control, inverse kinematics (IK) is used as the basis for computing arm joint angles. However, IK can be used to compute joint angles only after the terminal point has been reached, and it cannot optimize arm movements. Furthermore, the singularity problem is sometimes encountered when using IK. For example, if a robot arm in motion passes through a singularity, the next step’s movement is incomputable, which results in errors. Therefore, this study did not use IK for computing the joint angles of humanoid robot arms. Instead, this paper proposes a motion controller based on machine learning and fuzzy logic for the aforementioned purpose. Conventional reinforcement learning can provide satisfactory results for a single state but cannot be used to perform optimized calculations for infinite states. To solve this problem, this study used the deep deterministic policy gradient (DDPG) algorithm and allowed a humanoid robot to self-learn and autonomously plan the movement of and joint angles in its arm. A state and its action can be calculated in a hyperplane by using the developed neural network. A continuous mapping relationship exists between the state and its action in this hyperplane. Thus, the humanoid robot obtained optimal learning experiences in multiple self-learning processes. Finally, the concept was incorporated into a visual feedback system to achieve object grasping by the humanoid robot. The humanoid robot exhibited satisfactory learning outcomes—as well as satisfactory motion control performance—in experiments when combining fuzzy logic with the DDPG algorithm.",
        "DOI": "10.1007/s40815-022-01293-0",
        "paper_author": "Kuo P.H.",
        "affiliation_name": "National Chung Cheng University",
        "affiliation_city": "Min-Hsiung",
        "affiliation_country": "Taiwan",
        "affiliation_id": "60007954",
        "affiliation_state": "Chiayi"
    },
    {
        "paper_title": "How do the existing fairness metrics and unfairness mitigation algorithms contribute to ethical learning analytics?",
        "publication": "British Journal of Educational Technology",
        "citied_by": "39",
        "cover_date": "2022-07-01",
        "Abstract": "With the widespread use of learning analytics (LA), ethical concerns about fairness have been raised. Research shows that LA models may be biased against students of certain demographic subgroups. Although fairness has gained significant attention in the broader machine learning (ML) community in the last decade, it is only recently that attention has been paid to fairness in LA. Furthermore, the decision on which unfairness mitigation algorithm or metric to use in a particular context remains largely unknown. On this premise, we performed a comparative evaluation of some selected unfairness mitigation algorithms regarded in the fair ML community to have shown promising results. Using a 3-year program dropout data from an Australian university, we comparatively evaluated how the unfairness mitigation algorithms contribute to ethical LA by testing for some hypotheses across fairness and performance metrics. Interestingly, our results show how data bias does not always necessarily result in predictive bias. Perhaps not surprisingly, our test for fairness-utility tradeoff shows how ensuring fairness does not always lead to drop in utility. Indeed, our results show that ensuring fairness might lead to enhanced utility under specific circumstances. Our findings may to some extent, guide fairness algorithm and metric selection for a given context. Practitioner notes What is already known about this topic LA is increasingly being used to leverage actionable insights about students and drive student success. LA models have been found to make discriminatory decisions against certain student demographic subgroups—therefore, raising ethical concerns. Fairness in education is nascent. Only a few works have examined fairness in LA and consequently followed up with ensuring fair LA models. What this paper adds A juxtaposition of unfairness mitigation algorithms across the entire LA pipeline showing how they compare and how each of them contributes to fair LA. Ensuring ethical LA does not always lead to a dip in performance. Sometimes, it actually improves performance as well. Fairness in LA has only focused on some form of outcome equality, however equality of outcome may be possible only when the playing field is levelled. Implications for practice and/or policy Based on desired notion of fairness and which segment of the LA pipeline is accessible, a fairness-minded decision maker may be able to decide which algorithm to use in order to achieve their ethical goals. LA practitioners can carefully aim for more ethical LA models without trading significant utility by selecting algorithms that find the right balance between the two objectives. Fairness enhancing technologies should be cautiously used as guides—not final decision makers. Human domain experts must be kept in the loop to handle the dynamics of transcending fair LA beyond equality to equitable LA.",
        "DOI": "10.1111/bjet.13217",
        "paper_author": "Deho O.B.",
        "affiliation_name": "University of South Australia",
        "affiliation_city": "Adelaide",
        "affiliation_country": "Australia",
        "affiliation_id": "60031846",
        "affiliation_state": "SA"
    },
    {
        "paper_title": "A machine learning algorithm for predicting prolonged postoperative opioid prescription after lumbar disc herniation surgery. An external validation study using 1,316 patients from a Taiwanese cohort",
        "publication": "Spine Journal",
        "citied_by": "22",
        "cover_date": "2022-07-01",
        "Abstract": "BACKGROUND CONTEXT: Preoperative prediction of prolonged postoperative opioid prescription helps identify patients for increased surveillance after surgery. The SORG machine learning model has been developed and successfully tested using 5,413 patients from the United States (US) to predict the risk of prolonged opioid prescription after surgery for lumbar disc herniation. However, external validation is an often-overlooked element in the process of incorporating prediction models in current clinical practice. This cannot be stressed enough in prediction models where medicolegal and cultural differences may play a major role. PURPOSE: The authors aimed to investigate the generalizability of the US citizens prediction model SORG to a Taiwanese patient cohort. STUDY DESIGN: Retrospective study at a large academic medical center in Taiwan. PATIENT SAMPLE: Of 1,316 patients who were 20 years or older undergoing initial operative management for lumbar disc herniation between 2010 and 2018. OUTCOME MEASURES: The primary outcome of interest was prolonged opioid prescription defined as continuing opioid prescription to at least 90 to 180 days after the first surgery for lumbar disc herniation at our institution. METHODS: Baseline characteristics were compared between the external validation cohort and the original developmental cohorts. Discrimination (area under the receiver operating characteristic curve and the area under the precision-recall curve), calibration, overall performance (Brier score), and decision curve analysis were used to assess the performance of the SORG ML algorithm in the validation cohort. This study had no funding source or conflict of interests. RESULTS: Overall, 1,316 patients were identified with sustained postoperative opioid prescription in 41 (3.1%) patients. The validation cohort differed from the development cohort on several variables including 93% of Taiwanese patients receiving NSAIDS preoperatively compared with 22% of US citizens patients, while 30% of Taiwanese patients received opioids versus 25% in the US. Despite these differences, the SORG prediction model retained good discrimination (area under the receiver operating characteristic curve of 0.76 and the area under the precision-recall curve of 0.33) and good overall performance (Brier score of 0.028 compared with null model Brier score of 0.030) while somewhat overestimating the chance of prolonged opioid use (calibration slope of 1.07 and calibration intercept of -0.87). Decision-curve analysis showed the SORG model was suitable for clinical use. CONCLUSIONS: Despite differences at baseline and a very strict opioid policy, the SORG algorithm for prolonged opioid use after surgery for lumbar disc herniation has good discriminative abilities and good overall performance in a Han Chinese patient group in Taiwan. This freely available digital application can be used to identify high-risk patients and tailor prevention policies for these patients that may mitigate the long-term adverse consequence of opioid dependence: https://sorg-apps.shinyapps.io/lumbardiscopioid/.",
        "DOI": "10.1016/j.spinee.2022.02.009",
        "paper_author": "Yen H.K.",
        "affiliation_name": "National Taiwan University Hospital",
        "affiliation_city": "Taipei",
        "affiliation_country": "Taiwan",
        "affiliation_id": "60073385",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Identifying critical energy-water paths and clusters within the urban agglomeration using machine learning algorithm",
        "publication": "Energy",
        "citied_by": "9",
        "cover_date": "2022-07-01",
        "Abstract": "Energy and water shortages are two major problems in the process of urban development, and meeting the demands for energy and fresh water has become the key to global sustainable development. In this study, we developed a structure-based singular value decomposition (SSVD) method through incorporating techniques of multi-regional input-output (MRIO), structural path analysis (SPA), and singular value decomposition (SVD) within a general framework. The SSVD method is used to explore and track the system properties and flow paths of energy-water nexus network in the Pearl River Delta urban agglomeration (PUA) from 2012 to 2015. Our main findings are: (i) the largest final demand of inducing energy-related water (E-water) and water-related energy (W-energy) is the exports; (ii) Shenzhen mainly depends on other cities for E-water and W-energy, and Huizhou is the provider of E-water and W-energy; (iii) we identified over 10,000 energy-water clusters and found that Guangzhou's electricity and equipment manufacture drive the largest energy-water clusters, respectively. Our findings suggest that monitoring key paths and clusters of major energy-water consumption in the supply chains of urban agglomerations can provide new insights into energy and water policies.",
        "DOI": "10.1016/j.energy.2022.123880",
        "paper_author": "Ding Y.",
        "affiliation_name": "Beijing Normal University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60023237",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Earth observation and geospatial data can predict the relative distribution of village level poverty in the Sundarban Biosphere Reserve, India",
        "publication": "Journal of Environmental Management",
        "citied_by": "15",
        "cover_date": "2022-07-01",
        "Abstract": "There is increasing interest in leveraging Earth Observation (EO) and geospatial data to predict and map aspects of socioeconomic conditions to support survey and census activities. This is particularly relevant for the frequent monitoring required to assess progress towards the UNs' Sustainable Development Goals (SDGs). The Sundarban Biosphere Reserve (SBR) is a region of international ecological importance, containing the Indian portion of the world's largest mangrove forest. The region is densely populated and home to over 4.4 million people, many living in chronic poverty with a strong dependence on nature-based rural livelihoods. Such livelihoods are vulnerable to frequent natural hazards including cyclone landfall and storm surges. In this study we examine associations between environmental variables derived from EO and geospatial data with a village level multidimensional poverty metric using random forest machine learning, to provide evidence in support of policy formulation in the field of poverty reduction. We find that environmental variables can predict up to 78% of the relative distribution of the poorest villages within the SBR. Exposure to cyclone hazard was the most important variable for prediction of poverty. The poorest villages were associated with relatively small areas of rural settlement (<∼30%), large areas of agricultural land (>∼50%) and moderate to high cyclone hazard. The poorest villages were also associated with less productive agricultural land than the wealthiest. Analysis suggests villages with access to more diverse livelihood options, and a smaller dependence on agriculture may be more resilient to cyclone hazard. This study contributes to the understanding of poverty-environment dynamics within Low-and middle-income countries and the associations found can inform policy linked to socio-environmental scenarios within the SBR and potentially support monitoring of work towards SDG1 (No Poverty) across the region.",
        "DOI": "10.1016/j.jenvman.2022.114950",
        "paper_author": "Marcinko C.L.J.",
        "affiliation_name": "University of Southampton",
        "affiliation_city": "Southampton",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60025225",
        "affiliation_state": "Hampshire"
    },
    {
        "paper_title": "Collaborative multi-agents in dynamic industrial internet of things using deep reinforcement learning",
        "publication": "Environment, Development and Sustainability",
        "citied_by": "5",
        "cover_date": "2022-07-01",
        "Abstract": "Sustainable cities are envisioned to have economic and industrial steps toward reducing pollution. Many real-world applications such as autonomous vehicles, transportation, traffic signals, and industrial automation can now be trained using deep reinforcement learning (DRL) techniques. These applications are designed to take benefit of DRL in order to improve the monitoring as well as measurements in industrial internet of things for automation identification system. The complexity of these environments means that it is more appropriate to use multi-agent systems rather than a single-agent. However, in non-stationary environments multi-agent systems can suffer from increased number of observations, limiting the scalability of algorithms. This study proposes a model to tackle the problem of scalability in DRL algorithms in transportation domain. A partition-based approach is used in the proposed model to reduce the complexity of the environment. This partition-based approach helps agents to stay in their working area. This reduces the complexity of the learning environment and the number of observations for each agent. The proposed model uses generative adversarial imitation learning and behavior cloning, combined with a proximal policy optimization algorithm, for training multiple agents in a dynamic environment. We present a comparison of PPO, soft actor-critic, and our model in reward gathering. Our simulation results show that our model outperforms SAC and PPO in cumulative reward gathering and dramatically improved training multiple agents.",
        "DOI": "10.1007/s10668-021-01836-9",
        "paper_author": "Raza A.",
        "affiliation_name": "COMSATS University Islamabad",
        "affiliation_city": "Islamabad",
        "affiliation_country": "Pakistan",
        "affiliation_id": "60089631",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Optimal policy trees",
        "publication": "Machine Learning",
        "citied_by": "18",
        "cover_date": "2022-07-01",
        "Abstract": "We propose an approach for learning optimal tree-based prescription policies directly from data, combining methods for counterfactual estimation from the causal inference literature with recent advances in training globally-optimal decision trees. The resulting method, Optimal Policy Trees, yields interpretable prescription policies, is highly scalable, and handles both discrete and continuous treatments. We conduct extensive experiments on both synthetic and real-world datasets and demonstrate that these trees offer best-in-class performance across a wide variety of problems.",
        "DOI": "10.1007/s10994-022-06128-5",
        "paper_author": "Amram M.",
        "affiliation_name": "Interpretable AI",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States",
        "affiliation_id": "125702848",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "State-of-health estimation of Li-ion batteries in the early phases of qualification tests: An interpretable machine learning approach",
        "publication": "Expert Systems with Applications",
        "citied_by": "37",
        "cover_date": "2022-07-01",
        "Abstract": "Reducing the time and cost associated with lithium-ion (Li-ion) battery qualification tests is critical to developing electronic devices and establishing their quality assurance policies. In this study, we develop an interpretable machine learning model for estimating the future state-of-health (SOH) of Li-ion batteries in the early phases of qualification tests. First, a window-moving technique is used to extract the statistical features that represent battery capacity-fading behaviors over certain cycles. Second, a machine learning model is developed to estimate a battery's future SOH value at a certain cycle. Third, the performance and reliability of the machine learning model are assessed using multiple experiments with varying forecast horizons for SOH estimation. Finally, the SHapley Additive exPlanation (SHAP) method is applied to the model to identify which statistical features are important when estimating a battery's SOH value. The experimental results confirm that the proposed approach can reduce the time required for qualification tests to 100 cycles, i.e., less than a month in practice, with less than a 5% mean absolute percentage error (MAPE) and a 0.002 mean squared error (MSE). The results of model interpretation by SHAP demonstrate that the changes in the SOH values of Li-ion batteries are more important than the values themselves to the SOH estimation. Moreover, the SOH degradation trends near the 100th cycle during the qualification tests are proved to have a significant impact on the future SOH values of the batteries.",
        "DOI": "10.1016/j.eswa.2022.116817",
        "paper_author": "Lee G.",
        "affiliation_name": "Ulsan National Institute of Science and Technology",
        "affiliation_city": "Ulsan",
        "affiliation_country": "South Korea",
        "affiliation_id": "60103153",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Forecasting the realized variance of oil-price returns: a disaggregated analysis of the role of uncertainty and geopolitical risk",
        "publication": "Environmental Science and Pollution Research",
        "citied_by": "15",
        "cover_date": "2022-07-01",
        "Abstract": "We contribute to the empirical literature on the predictability of oil-market volatility by comparing the predictive role of aggregate versus several disaggregated metrics of policy-related and equity-market uncertainties of the USA and geopolitical risks for forecasting the future realized volatility of oil-price (WTI) returns over the monthly period from 1985:01 to 2021:08. Using machine-learning techniques, we find that adding the disaggregated metrics to the array of predictors improves the accuracy of forecasts at intermediate and long forecast horizons, and mainly when we use random forests to estimate our forecasting model.",
        "DOI": "10.1007/s11356-022-19152-8",
        "paper_author": "Gupta R.",
        "affiliation_name": "University of Pretoria",
        "affiliation_city": "Pretoria",
        "affiliation_country": "South Africa",
        "affiliation_id": "60021902",
        "affiliation_state": "Gauteng"
    },
    {
        "paper_title": "Groundwater level prediction using machine learning algorithms in a drought-prone area",
        "publication": "Neural Computing and Applications",
        "citied_by": "109",
        "cover_date": "2022-07-01",
        "Abstract": "Groundwater resources (GWR) play a crucial role in agricultural crop production, daily life, and economic progress. Therefore, accurate prediction of groundwater (GW) level will aid in the sustainable management of GWR. A comparative study was conducted to evaluate the performance of seven different ML models, such as random tree (RT), random forest (RF), decision stump, M5P, support vector machine (SVM), locally weighted linear regression (LWLR), and reduce error pruning tree (REP Tree) for GW level (GWL) prediction. The long-term prediction was conducted using historical GWL, mean temperature, rainfall, and relative humidity datasets for the period 1981–2017 obtained from two wells in the northwestern region of Bangladesh. The whole dataset was divided into training (1981–2008) and testing (2008–2017) datasets. The output of the seven proposed models was evaluated using the root mean square error (RMSE), mean absolute error (MAE), relative absolute error (RAE), root relative squared error (RRSE), correlation coefficient (CC), and Taylor diagram. The results revealed that the Bagging-RT and Bagging-RF models outperformed other ML models. The Bagging-RT models can effectively improve prediction precision as compared to other models with RMSE of 0.60 m, MAE of 0.45 m, RAE of 27.47%, RRSE of 30.79%, and CC of 0.96 for Rajshahi and RMSE of 0.26 m, MAE of 0.18 m, RAE of 19.87%, RRSE of 24.17%, and 0.97 for Rangpur during training, and RMSE of 0.60 m, MAE of 0.40 m, RAE of 24.25%, RRSE of 29.99%, and CC of 0.96 for Rajshahi and RMSE of 0.38 m, MAE of 0.24 m, RAE of 23.55%, RRSE of 31.77%, and CC of 0.95 for Rangpur during testing stages, respectively. Our study offers an effective and practical approach to the forecast of GWL that could help to formulate policies for sustainable GWR management.",
        "DOI": "10.1007/s00521-022-07009-7",
        "paper_author": "Pham Q.B.",
        "affiliation_name": "Thu Dau Mot University",
        "affiliation_city": "Thu Dau Mot",
        "affiliation_country": "Viet Nam",
        "affiliation_id": "60159895",
        "affiliation_state": "Binh Duong"
    },
    {
        "paper_title": "Hindsight-aware deep reinforcement learning algorithm for multi-agent systems",
        "publication": "International Journal of Machine Learning and Cybernetics",
        "citied_by": "3",
        "cover_date": "2022-07-01",
        "Abstract": "Classic reinforcement learning algorithms generate experiences by the agent's constant trial and error, which leads to a large number of failure experiences stored in the replay buffer. As a result, the agents can only learn through these low-quality experiences. In the case of multi-agent systems, this problem is more serious. MADDPG (Multi-Agent Deep Deterministic Policy Gradient) has achieved significant results in solving multi-agent problems by using a framework of centralized training with decentralized execution. Nevertheless, the problem of too many failure experiences in the replay buffer has not been resolved. In this paper, we propose HMADDPG (Hindsight Multi-Agent Deep Deterministic Policy Gradient) to mitigate the negative impact of failure experience. HMADDPG has a hindsight unit, which allows the agents to reflect and produces pseudo experiences that tend to succeed. Pseudo experiences are stored in the replay buffer, so that the agents can combine two kinds of experiences to learn. We have evaluated our algorithm on a number of environments. The results show that the algorithm can guide agents to learn better strategies and can be applied in multi-agent systems which are cooperative, competitive, or mixed cooperative and competitive.",
        "DOI": "10.1007/s13042-022-01505-x",
        "paper_author": "Li C.",
        "affiliation_name": "Taiyuan University of Technology",
        "affiliation_city": "Taiyuan",
        "affiliation_country": "China",
        "affiliation_id": "60013268",
        "affiliation_state": "Shanxi"
    },
    {
        "paper_title": "Time to See the Difference: Video Capture for Patient-Centered Clinical Trials",
        "publication": "Patient",
        "citied_by": "7",
        "cover_date": "2022-07-01",
        "Abstract": "Developing therapeutics for the treatment of rare diseases usually requires a strong understanding of the natural history of the disease. Often, it also requires the creation of novel assessment tools and clinical trial endpoints. In diseases where mobility is impacted, the use of video to capture the impact of the disease and the assessment of specific parameters, such as gait and stride length, can help design sensitive endpoints. Video as an assessment tool also allows the use of historical videos or videos filmed by non-experts outside of clinical settings. Given the increased use of telemedicine, the use of video may be a useful addition to clinical trial assessments. Two cases are presented: (1) the use of video in the development of asfotase alfa (Strensiq®) in hypophosphatasia is detailed as an example of the utility of this type of assessment in rare diseases; and (2) a home-setting video tool that was developed and validated (SARAhome) from a commonly used clinical scale (Scale for the Assessment and Rating of Ataxia [SARA]), allowing patients to record their own severity of ataxia. While there are certain limitations associated with video assessment, advancing technologies such as automated analysis and machine learning provide a tremendous opportunity for automated analysis of video recordings, reducing the bias associated with human assessment.",
        "DOI": "10.1007/s40271-021-00569-1",
        "paper_author": "Davies E.H.",
        "affiliation_name": "Aparito Limited",
        "affiliation_city": "Wrexham",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60275922",
        "affiliation_state": "Clwyd, Wales"
    },
    {
        "paper_title": "Reinforcement Learning Versus PDE Backstepping and PI Control for Congested Freeway Traffic",
        "publication": "IEEE Transactions on Control Systems Technology",
        "citied_by": "21",
        "cover_date": "2022-07-01",
        "Abstract": "We develop reinforcement learning (RL) boundary controllers to mitigate stop-and-go traffic congestion on a freeway segment. The traffic dynamics of the freeway segment are governed by a macroscopic Aw-Rascle-Zhang (ARZ) model, consisting of 2 × 2 quasi-linear partial differential equations (PDEs) for traffic density and velocity. The boundary stabilization of the linearized ARZ PDE model has been solved by PDE backstepping, guaranteeing spatial L2 norm regulation of the traffic state to uniform density and velocity and ensuring that traffic oscillations are suppressed. Collocated proportional (P) and proportional-integral (PI) controllers also provide stability guarantees for allowable control gains and are always applicable as model-free control options through gain tuning by trial and error, or by model-free optimization. Although these approaches are mathematically elegant, the stabilization result only holds locally and is usually affected by the change of model parameters. Therefore, we reformulate the PDE boundary control problem as an RL problem that pursues stabilization without knowing the system dynamics, simply by observing the state values. The proximal policy optimization (PPO), a neural network-based policy gradient algorithm, is employed to obtain RL controllers by interacting with a numerical simulator of the ARZ PDE. Being stabilization-inspired, the RL state-feedback boundary controllers are compared and evaluated against the rigorously stabilizing controllers in two cases: 1) in a system with perfect knowledge of the traffic flow dynamics and then 2) in one with only partial knowledge. We obtain RL controllers that nearly recover the performance of the backstepping, P, and PI controllers with perfect knowledge and outperform them in some cases with partial knowledge. It must be noted, however, that the RL controllers are obtained by conducting about one thousand episodes of iterative training on a simulation model. This training cannot be performed in a collision-free fashion in real traffic, nor convergence guaranteed when training. Thus, we demonstrate that the RL approach has learning (i.e., adaptation) potential for traffic PDE systems under uncertain and changing conditions, but RL is neither simple nor a fully safe substitute for model-based control in real traffic systems.",
        "DOI": "10.1109/TCST.2021.3116796",
        "paper_author": "Yu H.",
        "affiliation_name": "Hong Kong University of Science and Technology",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong",
        "affiliation_id": "60008592",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Online Improvement of Condition-Based Maintenance Policy via Monte Carlo Tree Search",
        "publication": "IEEE Transactions on Automation Science and Engineering",
        "citied_by": "14",
        "cover_date": "2022-07-01",
        "Abstract": "Often in manufacturing systems, scenarios arise where the demand for maintenance exceeds the capacity of maintenance resources. This results in the problem of allocating the limited resources among machines competing for them. This maintenance scheduling problem can be formulated as a Markov decision process (MDP) with the goal of finding the optimal dynamic maintenance action given the current system state. However, as the system becomes more complex, solving an MDP suffers from the curse of dimensionality. To overcome this issue, we propose a two-stage approach that first optimizes a static condition-based maintenance (CBM) policy using a genetic algorithm (GA) and then improves the policy online via Monte Carlo tree search (MCTS). The static policy significantly reduces the state space of the online problem by allowing us to ignore machines that are not sufficiently degraded. Furthermore, we formulate MCTS to seek a maintenance schedule that maximizes the long-term production volume of the system to reconcile the conflict between maintenance and production objectives. We demonstrate that the resulting online policy is an improvement over the static CBM policy found by GA. Note to Practitioners - This article proposes a method of scheduling maintenance in complex manufacturing systems in scenarios where there is frequent competition for maintenance resources. We use a condition-based maintenance policy that prescribes maintenance actions based on a machine's current health. However, when several machines are due for maintenance, a maintenance technician must choose between multiple competing jobs. While a common approach is to establish rules that dictate how maintenance jobs should be prioritized, such as the first-in, first-out rule, the goal of this work is to improve upon static policies in real time. We do this by strategically evaluating sequences of maintenance actions and playing out many 'what-if' scenarios to see how the system will behave in the future. Implementation of the proposed method relies on the construction of a simulation model of the target system. This model is capable of retrieving the current state of the physical system, including the degradation state of machines, the availability of maintenance resources, and the distribution of parts throughout buffers in the system. We present several simulation experiments that demonstrate the improvement in system performance that our approach provides. Future work will aim to improve the efficiency of maintenance prioritization through online learning as well as more accurately identify manufacturing system configurations that will yield the greatest benefit of these methods.",
        "DOI": "10.1109/TASE.2021.3088603",
        "paper_author": "Hoffman M.",
        "affiliation_name": "Penn State College of Engineering",
        "affiliation_city": "University Park",
        "affiliation_country": "United States",
        "affiliation_id": "60147936",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "Assessment of renewable energy: Status, challenges, COVID-19 impacts, opportunities, and sustainable energy solutions in Africa",
        "publication": "Energy and Built Environment",
        "citied_by": "72",
        "cover_date": "2022-07-01",
        "Abstract": "Today's more focus and efforts are being put by all the energy leaders towards power generation using renewable energy resources. Fortunately, these resources are becoming affordable to facilitate a swift shift towards green and clean energy. Possible strategic assets are an add-on for all the developing nations in terms of economy. The technological advancement and power market revolution resulting in an adequate reduction of renewable energy cost and affordability. This paper mainly focusing on Covid-19 impacts in the African energy sector. Also, analyzing recent developments in African renewable energy generation that holds the immense capacity for improvisation. This paper highlighting the recommendations in response to the COVID-19 pandemic for the African renewable energy sector. This paper is a result of rigorous analysis based on major issues governing sustainable solutions for Africa. This review paper comes up with effective conclusions to address the challenges in the current pandemic situation. In Africa abundance of resources is found with huge potential for the generation of power. But still, Africa undergoing a phase of serious crises because they are not able to tap its huge capital of renewable energies. There is a subsequent need for power grid restructuring, energy storage technologies, and parallel mitigation of environmental factors with seasonal variations. Proposed review analysis bringing a better opportunity for all issues towards sustainable solutions, that will ease the renewable energy status in Africa. It is observed that there is an inevitable need to focus on having strong government policy frameworks and proper regulations. The various recommendations are required to swing towards renewable energy development. Combined efforts are required in luring foreign investments and to address feasible issues like setting-up targets. This paper demonstrated a smart energy system using a proposed machine learning-based framework for enhancing the PV forecasting and up-gradation in available technologies.",
        "DOI": "10.1016/j.enbenv.2021.03.002",
        "paper_author": "Amir M.",
        "affiliation_name": "Jamia Millia Islamia",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India",
        "affiliation_id": "60020458",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Predicting Human Intentions in Human-Robot Hand-Over Tasks Through Multimodal Learning",
        "publication": "IEEE Transactions on Automation Science and Engineering",
        "citied_by": "54",
        "cover_date": "2022-07-01",
        "Abstract": "In human-robot shared manufacturing contexts, product parts or tools hand-over between the robot and the human is an important collaborative task. Facilitating the robot to figure out and predict human hand-over intentions correctly to improve the task efficiency in human-robot collaboration is therefore a necessary issue to be addressed. In this study, a teaching-learning-prediction (TLP) framework is proposed for the robot to learn from its human partner's multimodal demonstrations and predict human hand-over intentions. In this approach, the robot can be programmed by the human through demonstrations utilizing natural language and wearable sensors according to task requirements and the human's working preferences. Then the robot learns from human hand-over demonstrations online via extreme learning machine (ELM) algorithms to update its cognition capacity, allowing the robot to use its learned policy to predict human intentions actively and assist its human companion in hand-over tasks. Experimental results and evaluations suggest that the human may program the robot easily by the proposed approach when the task changes, as the robot can effectively predict hand-over intentions with competitive accuracy to complete the hand-over tasks. Note to Practitioners - This article is motivated by human-robot hand-over problems in smart manufacturing contexts. Product parts or tools delivery in worker-robot partnerships is an important collaborative task. We develop a teaching-learning-prediction (TLP) framework for the robot to learn from its human partner's multimodal demonstrations and predict human hand-over intentions. The robot can be taught by human through natural language and wearable sensing information. The extreme learning machine (ELM) approach is employed for the robot to build its cognition capacity to predict human intentions actively and assist its human companion in hand-over tasks. We demonstrate that the proposed approach presents distinct and effective advantages to facilitate human-robot hand-over tasks in collaborative manufacturing contexts.",
        "DOI": "10.1109/TASE.2021.3074873",
        "paper_author": "Wang W.",
        "affiliation_name": "Montclair State University",
        "affiliation_city": "Montclair",
        "affiliation_country": "United States",
        "affiliation_id": "60012621",
        "affiliation_state": "NJ"
    },
    {
        "paper_title": "An Empirical Assessment of the Contagion Determinants in the Euro Area in a Period of Sovereign Debt Risk",
        "publication": "Italian Economic Journal",
        "citied_by": "2",
        "cover_date": "2022-07-01",
        "Abstract": "This paper uses learning methods and optimization techniques to investigate the determinants of shock propagation in the Euro area for the period 2001–2015. First, principal component analyses are used with country bond yields to identify sub-periods and country groups; second, influencing factors for country bond yields are investigated with random forest models; lastly, shock propagation among groups are examined with impulse response functions. Models in steps two and three are improved by using simulated annealing algorithm. The empirical findings achieved can be particularly relevant for both investors and policymakers. Shedding light on the determinants of financial contagion may be in fact useful for investors who can derive relevant information about countries which are less sensitive to be affected by shocks, orienting thus their investment strategies. At the same time, policymakers could draw worthwhile and preventive hedging strategies and design the most suitable crisis management policies.",
        "DOI": "10.1007/s40797-021-00147-2",
        "paper_author": "Altınbaş H.",
        "affiliation_name": "T.C. Beykent Üniversitesi",
        "affiliation_city": "Istanbul",
        "affiliation_country": "Turkey",
        "affiliation_id": "60012119",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Applying Reinforcement Learning towards automating energy efficient virtual machine consolidation in cloud data centers",
        "publication": "Information Systems",
        "citied_by": "80",
        "cover_date": "2022-07-01",
        "Abstract": "Energy awareness presents an immense challenge for cloud computing infrastructure and the development of next generation data centers. Virtual Machine (VM) consolidation is one technique that can be harnessed to reduce energy related costs and environmental sustainability issues of data centers. In recent times intelligent learning approaches have proven to be effective for managing resources in cloud data centers. In this paper we explore the application of Reinforcement Learning (RL) algorithms for the VM consolidation problem demonstrating their capacity to optimize the distribution of virtual machines across the data center for improved resource management. Determining efficient policies in dynamic environments can be a difficult task, however, the proposed RL approach learns optimal behavior in the absence of complete knowledge due to its innate ability to reason under uncertainty. Using real workload data we provide a comparative analysis of popular RL algorithms including SARSA and Q-learning. Our empirical results demonstrate how our approach improves energy efficiency by 25% while also reducing service violations by 63% over the popular Power-Aware heuristic algorithm.",
        "DOI": "10.1016/j.is.2021.101722",
        "paper_author": "Shaw R.",
        "affiliation_name": "University of Galway",
        "affiliation_city": "Galway",
        "affiliation_country": "Ireland",
        "affiliation_id": "60008539",
        "affiliation_state": "Connacht"
    },
    {
        "paper_title": "vrAIn: Deep Learning Based Orchestration for Computing and Radio Resources in vRANs",
        "publication": "IEEE Transactions on Mobile Computing",
        "citied_by": "30",
        "cover_date": "2022-07-01",
        "Abstract": "The virtualization of radio access networks (vRAN) is the last milestone in the NFV revolution. However, the complex relationship between computing and radio dynamics make vRAN resource control particularly daunting. We present vrAIn, a resource orchestrator for vRANs based on deep reinforcement learning. First, we use an autoencoder to project high-dimensional context data (traffic and channel quality patterns) into a latent representation. Then, we use a deep deterministic policy gradient (DDPG) algorithm based on an actor-critic neural network structure and a classifier to map contexts into resource control decisions. We have evaluated vrAIn experimentally, using an open-source LTE stack over different platforms, and via simulations over a production RAN. Our results show that: (i) vrAIn provides savings in computing capacity of up to 30 percent over CPU-agnostic methods; (ii) it improves the probability of meeting QoS targets by 25 percent over static policies; (iii) upon computing capacity under-provisioning, vrAIn improves throughput by 25 percent over state-of-the-art schemes; and (iv) it performs close to an optimal offline oracle. To our knowledge, this is the first work that thoroughly studies the computational behavior of vRANs and the first approach to a model-free solution that does not need to assume any particular platform or context.",
        "DOI": "10.1109/TMC.2020.3043100",
        "paper_author": "Ayala-Romero J.A.",
        "affiliation_name": "Trinity College Dublin",
        "affiliation_city": "Dublin",
        "affiliation_country": "Ireland",
        "affiliation_id": "60011149",
        "affiliation_state": "Leinster"
    },
    {
        "paper_title": "Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022",
        "publication": "Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022",
        "citied_by": "0",
        "cover_date": "2022-06-30",
        "Abstract": "The proceedings contain 1624 papers. The topics discussed include: maximizing Nash social welfare in 2-value instances; truth-tracking via approval voting: size matters; DeformRS: certifying input deformations with randomized smoothing; amplitude spectrum transformation for open compound domain adaptive semantic segmentation; renovate yourself: calibrating feature representation of misclassified pixels for semantic segmentation; certified symmetry and dominance breaking for combinatorial optimization; scaling up influence functions; chaining value functions for off-policy learning; partner-aware algorithms in decentralized cooperative bandit teams; optimized potential initialization for low-latency spiking neural networks; classifying emails into human vs machine category; pinpointing fine-grained relationships between hateful tweets and replies; cross-modal coherence for text-to-image retrieval; conditional synthetic data generation for robust machine learning applications with limited pandemic data; and explainable and local correction of classification models using decision trees.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022",
        "publication": "Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022",
        "citied_by": "0",
        "cover_date": "2022-06-30",
        "Abstract": "The proceedings contain 1624 papers. The topics discussed include: maximizing Nash social welfare in 2-value instances; truth-tracking via approval voting: size matters; DeformRS: certifying input deformations with randomized smoothing; amplitude spectrum transformation for open compound domain adaptive semantic segmentation; renovate yourself: calibrating feature representation of misclassified pixels for semantic segmentation; certified symmetry and dominance breaking for combinatorial optimization; scaling up influence functions; chaining value functions for off-policy learning; partner-aware algorithms in decentralized cooperative bandit teams; optimized potential initialization for low-latency spiking neural networks; classifying emails into human vs machine category; pinpointing fine-grained relationships between hateful tweets and replies; cross-modal coherence for text-to-image retrieval; conditional synthetic data generation for robust machine learning applications with limited pandemic data; and explainable and local correction of classification models using decision trees.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022",
        "publication": "Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022",
        "citied_by": "0",
        "cover_date": "2022-06-30",
        "Abstract": "The proceedings contain 1624 papers. The topics discussed include: maximizing Nash social welfare in 2-value instances; truth-tracking via approval voting: size matters; DeformRS: certifying input deformations with randomized smoothing; amplitude spectrum transformation for open compound domain adaptive semantic segmentation; renovate yourself: calibrating feature representation of misclassified pixels for semantic segmentation; certified symmetry and dominance breaking for combinatorial optimization; scaling up influence functions; chaining value functions for off-policy learning; partner-aware algorithms in decentralized cooperative bandit teams; optimized potential initialization for low-latency spiking neural networks; classifying emails into human vs machine category; pinpointing fine-grained relationships between hateful tweets and replies; cross-modal coherence for text-to-image retrieval; conditional synthetic data generation for robust machine learning applications with limited pandemic data; and explainable and local correction of classification models using decision trees.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022",
        "publication": "Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022",
        "citied_by": "0",
        "cover_date": "2022-06-30",
        "Abstract": "The proceedings contain 1624 papers. The topics discussed include: maximizing Nash social welfare in 2-value instances; truth-tracking via approval voting: size matters; DeformRS: certifying input deformations with randomized smoothing; amplitude spectrum transformation for open compound domain adaptive semantic segmentation; renovate yourself: calibrating feature representation of misclassified pixels for semantic segmentation; certified symmetry and dominance breaking for combinatorial optimization; scaling up influence functions; chaining value functions for off-policy learning; partner-aware algorithms in decentralized cooperative bandit teams; optimized potential initialization for low-latency spiking neural networks; classifying emails into human vs machine category; pinpointing fine-grained relationships between hateful tweets and replies; cross-modal coherence for text-to-image retrieval; conditional synthetic data generation for robust machine learning applications with limited pandemic data; and explainable and local correction of classification models using decision trees.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022",
        "publication": "Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022",
        "citied_by": "0",
        "cover_date": "2022-06-30",
        "Abstract": "The proceedings contain 1624 papers. The topics discussed include: maximizing Nash social welfare in 2-value instances; truth-tracking via approval voting: size matters; DeformRS: certifying input deformations with randomized smoothing; amplitude spectrum transformation for open compound domain adaptive semantic segmentation; renovate yourself: calibrating feature representation of misclassified pixels for semantic segmentation; certified symmetry and dominance breaking for combinatorial optimization; scaling up influence functions; chaining value functions for off-policy learning; partner-aware algorithms in decentralized cooperative bandit teams; optimized potential initialization for low-latency spiking neural networks; classifying emails into human vs machine category; pinpointing fine-grained relationships between hateful tweets and replies; cross-modal coherence for text-to-image retrieval; conditional synthetic data generation for robust machine learning applications with limited pandemic data; and explainable and local correction of classification models using decision trees.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022",
        "publication": "Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022",
        "citied_by": "0",
        "cover_date": "2022-06-30",
        "Abstract": "The proceedings contain 1624 papers. The topics discussed include: maximizing Nash social welfare in 2-value instances; truth-tracking via approval voting: size matters; DeformRS: certifying input deformations with randomized smoothing; amplitude spectrum transformation for open compound domain adaptive semantic segmentation; renovate yourself: calibrating feature representation of misclassified pixels for semantic segmentation; certified symmetry and dominance breaking for combinatorial optimization; scaling up influence functions; chaining value functions for off-policy learning; partner-aware algorithms in decentralized cooperative bandit teams; optimized potential initialization for low-latency spiking neural networks; classifying emails into human vs machine category; pinpointing fine-grained relationships between hateful tweets and replies; cross-modal coherence for text-to-image retrieval; conditional synthetic data generation for robust machine learning applications with limited pandemic data; and explainable and local correction of classification models using decision trees.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022",
        "publication": "Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022",
        "citied_by": "0",
        "cover_date": "2022-06-30",
        "Abstract": "The proceedings contain 1624 papers. The topics discussed include: maximizing Nash social welfare in 2-value instances; truth-tracking via approval voting: size matters; DeformRS: certifying input deformations with randomized smoothing; amplitude spectrum transformation for open compound domain adaptive semantic segmentation; renovate yourself: calibrating feature representation of misclassified pixels for semantic segmentation; certified symmetry and dominance breaking for combinatorial optimization; scaling up influence functions; chaining value functions for off-policy learning; partner-aware algorithms in decentralized cooperative bandit teams; optimized potential initialization for low-latency spiking neural networks; classifying emails into human vs machine category; pinpointing fine-grained relationships between hateful tweets and replies; cross-modal coherence for text-to-image retrieval; conditional synthetic data generation for robust machine learning applications with limited pandemic data; and explainable and local correction of classification models using decision trees.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022",
        "publication": "Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022",
        "citied_by": "0",
        "cover_date": "2022-06-30",
        "Abstract": "The proceedings contain 1624 papers. The topics discussed include: maximizing Nash social welfare in 2-value instances; truth-tracking via approval voting: size matters; DeformRS: certifying input deformations with randomized smoothing; amplitude spectrum transformation for open compound domain adaptive semantic segmentation; renovate yourself: calibrating feature representation of misclassified pixels for semantic segmentation; certified symmetry and dominance breaking for combinatorial optimization; scaling up influence functions; chaining value functions for off-policy learning; partner-aware algorithms in decentralized cooperative bandit teams; optimized potential initialization for low-latency spiking neural networks; classifying emails into human vs machine category; pinpointing fine-grained relationships between hateful tweets and replies; cross-modal coherence for text-to-image retrieval; conditional synthetic data generation for robust machine learning applications with limited pandemic data; and explainable and local correction of classification models using decision trees.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022",
        "publication": "Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022",
        "citied_by": "0",
        "cover_date": "2022-06-30",
        "Abstract": "The proceedings contain 1624 papers. The topics discussed include: maximizing Nash social welfare in 2-value instances; truth-tracking via approval voting: size matters; DeformRS: certifying input deformations with randomized smoothing; amplitude spectrum transformation for open compound domain adaptive semantic segmentation; renovate yourself: calibrating feature representation of misclassified pixels for semantic segmentation; certified symmetry and dominance breaking for combinatorial optimization; scaling up influence functions; chaining value functions for off-policy learning; partner-aware algorithms in decentralized cooperative bandit teams; optimized potential initialization for low-latency spiking neural networks; classifying emails into human vs machine category; pinpointing fine-grained relationships between hateful tweets and replies; cross-modal coherence for text-to-image retrieval; conditional synthetic data generation for robust machine learning applications with limited pandemic data; and explainable and local correction of classification models using decision trees.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022",
        "publication": "Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022",
        "citied_by": "0",
        "cover_date": "2022-06-30",
        "Abstract": "The proceedings contain 1624 papers. The topics discussed include: maximizing Nash social welfare in 2-value instances; truth-tracking via approval voting: size matters; DeformRS: certifying input deformations with randomized smoothing; amplitude spectrum transformation for open compound domain adaptive semantic segmentation; renovate yourself: calibrating feature representation of misclassified pixels for semantic segmentation; certified symmetry and dominance breaking for combinatorial optimization; scaling up influence functions; chaining value functions for off-policy learning; partner-aware algorithms in decentralized cooperative bandit teams; optimized potential initialization for low-latency spiking neural networks; classifying emails into human vs machine category; pinpointing fine-grained relationships between hateful tweets and replies; cross-modal coherence for text-to-image retrieval; conditional synthetic data generation for robust machine learning applications with limited pandemic data; and explainable and local correction of classification models using decision trees.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022",
        "publication": "Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022",
        "citied_by": "0",
        "cover_date": "2022-06-30",
        "Abstract": "The proceedings contain 1624 papers. The topics discussed include: maximizing Nash social welfare in 2-value instances; truth-tracking via approval voting: size matters; DeformRS: certifying input deformations with randomized smoothing; amplitude spectrum transformation for open compound domain adaptive semantic segmentation; renovate yourself: calibrating feature representation of misclassified pixels for semantic segmentation; certified symmetry and dominance breaking for combinatorial optimization; scaling up influence functions; chaining value functions for off-policy learning; partner-aware algorithms in decentralized cooperative bandit teams; optimized potential initialization for low-latency spiking neural networks; classifying emails into human vs machine category; pinpointing fine-grained relationships between hateful tweets and replies; cross-modal coherence for text-to-image retrieval; conditional synthetic data generation for robust machine learning applications with limited pandemic data; and explainable and local correction of classification models using decision trees.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Reinforcement Learning Based Dynamic Model Combination for Time Series Forecasting",
        "publication": "Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022",
        "citied_by": "37",
        "cover_date": "2022-06-30",
        "Abstract": "Time series data appears in many real-world fields such as energy, transportation, communication systems. Accurate modelling and forecasting of time series data can be of significant importance to improve the efficiency of these systems. Extensive research efforts have been taken for time series problems. Different types of approaches, including both statistical-based methods and machine learning-based methods, have been investigated. Among these methods, ensemble learning has shown to be effective and robust. However, it is still an open question that how we should determine weights for base models in the ensemble. Sub-optimal weights may prevent the final model from reaching its full potential. To deal with this challenge, we propose a reinforcement learning (RL) based model combination (RLMC) framework for determining model weights in an ensemble for time series forecasting tasks. By formulating model selection as a sequential decision-making problem, RLMC learns a deterministic policy to output dynamic model weights for non-stationary time series data. RLMC further leverages deep learning to learn hidden features from raw time series data to adapt fast to the changing data distribution. Extensive experiments on multiple real-world datasets have been implemented to showcase the effectiveness of the proposed method.",
        "DOI": "10.1609/aaai.v36i6.20618",
        "paper_author": "Fu Y.",
        "affiliation_name": "Université McGill",
        "affiliation_city": "Montreal",
        "affiliation_country": "Canada",
        "affiliation_id": "60002494",
        "affiliation_state": "QC"
    },
    {
        "paper_title": "Reinforcement Learning with Stochastic Reward Machines",
        "publication": "Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022",
        "citied_by": "16",
        "cover_date": "2022-06-30",
        "Abstract": "Reward machines are an established tool for dealing with reinforcement learning problems in which rewards are sparse and depend on complex sequences of actions. However, existing algorithms for learning reward machines assume an overly idealized setting where rewards have to be free of noise. To overcome this practical limitation, we introduce a novel type of reward machines, called stochastic reward machines, and an algorithm for learning them. Our algorithm, based on constraint solving, learns minimal stochastic reward machines from the explorations of a reinforcement learning agent. This algorithm can easily be paired with existing reinforcement learning algorithms for reward machines and guarantees to converge to an optimal policy in the limit. We demonstrate the effectiveness of our algorithm in two case studies and show that it outperforms both existing methods and a naive approach for handling noisy reward functions.",
        "DOI": "10.1609/aaai.v36i6.20594",
        "paper_author": "Corazza J.",
        "affiliation_name": "University of Zagreb",
        "affiliation_city": "Zagreb",
        "affiliation_country": "Croatia",
        "affiliation_id": "60008408",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Machine Learning for Utility Prediction in Argument-Based Computational Persuasion",
        "publication": "Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022",
        "citied_by": "11",
        "cover_date": "2022-06-30",
        "Abstract": "Automated persuasion systems (APS) aim to persuade a user to believe something by entering into a dialogue in which arguments and counterarguments are exchanged. To maximize the probability that an APS is successful in persuading a user, it can identify a global policy that will allow it to select the best arguments it presents at each stage of the dialogue whatever arguments the user presents. However, in real applications, such as for healthcare, it is unlikely the utility of the outcome of the dialogue will be the same, or the exact opposite, for the APS and user. In order to deal with this situation, games in extended form have been harnessed for argumentation in Bi-party Decision Theory. This opens new problems that we address in this paper: (1) How can we use Machine Learning (ML) methods to predict utility functions for different subpopulations of users? and (2) How can we identify for a new user the best utility function from amongst those that we have learned? To this extent, we develop two ML methods, EAI and EDS, that leverage information coming from the users to predict their utilities. EAI is restricted to a fixed amount of information, whereas EDS can choose the information that best detects the subpopulations of a user. We evaluate EAI and EDS in a simulation setting and in a realistic case study concerning healthy eating habits. Results are promising in both cases, but EDS is more effective at predicting useful utility functions.",
        "DOI": "10.1609/aaai.v36i5.20499",
        "paper_author": "Donadello I.",
        "affiliation_name": "Free University of Bozen-Bolzano",
        "affiliation_city": "Bolzano",
        "affiliation_country": "Italy",
        "affiliation_id": "60009914",
        "affiliation_state": "BZ"
    },
    {
        "paper_title": "Generalization in Mean Field Games by Learning Master Policies",
        "publication": "Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022",
        "citied_by": "12",
        "cover_date": "2022-06-30",
        "Abstract": "Mean Field Games (MFGs) can potentially scale multi-agent systems to extremely large populations of agents. Yet, most of the literature assumes a single initial distribution for the agents, which limits the practical applications of MFGs. Machine Learning has the potential to solve a wider diversity of MFG problems thanks to generalizations capacities. We study how to leverage these generalization properties to learn policies enabling a typical agent to behave optimally against any population distribution. In reference to the Master equation in MFGs, we coin the term \"Master policies\"to describe them and we prove that a single Master policy provides a Nash equilibrium, whatever the initial distribution. We propose a method to learn such Master policies. Our approach relies on three ingredients: adding the current population distribution as part of the observation, approximating Master policies with neural networks, and training via Reinforcement Learning and Fictitious Play. We illustrate on numerical examples not only the efficiency of the learned Master policy but also its generalization capabilities beyond the distributions used for training.",
        "DOI": "10.1609/aaai.v36i9.21173",
        "paper_author": "Perrin S.",
        "affiliation_name": "Université de Lille",
        "affiliation_city": "Lille",
        "affiliation_country": "France",
        "affiliation_id": "60104665",
        "affiliation_state": "Hauts-de-France"
    },
    {
        "paper_title": "The Pransky interview: Harry Kloor, PhD, PhD – CEO and Co-Founder, Beyond Imagination Inc.; scientist; entrepreneur; inventor; filmmaker",
        "publication": "Industrial Robot",
        "citied_by": "0",
        "cover_date": "2022-06-30",
        "Abstract": "Purpose: The following article is a “Q&A interview” conducted by Joanne Pransky of Industrial Robot Journal as a method to impart the combined technological, business and personal experience of a prominent, robotic industry PhD-turned successful innovator and entrepreneur regarding turning his lifelong dream into an invention and commercialized product. This paper aims to discuss these issues. Design/methodology/approach: Harry Kloor is a successful serial entrepreneur, scientist, technologist, educator, policy advisor, author and Hollywood filmmaker. He is the CEO and co-founder of Beyond Imagination, a company that has developed a suite of exponential technology solutions that deploys artificial intelligence (AI), AR, robotics, machine learning and human–computer interaction technology to enhance and revolutionize the world’s workforce. The company early in 2021 completed BEOMNI 1.0, the world’s first fully functional humanoid robotic system with an AI evolving brain, enabling remote work at a high level of fidelity to be done from around the globe. Kloor describes how he transformed his childhood dream into his brainchild and tangible reality. Findings: Kloor was born a groundbreaker who did not take no for an answer. He was born partially crippled with his legs facing backwards. The doctors said that he would spend his life in braces and would never be able to run. His parents told him not to let those ideas limit him and by the age of seven he ran for the first time and went on to become a martial arts master. Kloor’s childhood dream was to create ways to leave his body and inhabit a robotic body so that he could physically be free from his limited mobility. Kloor built his first computer at the age of seven and invented his first product at the age of eight. Kloor's inspiration to study science came largely from science fiction and his 20,000-plus collection of comic books. Knowing the nature of exponential growth, he spent the next 40 years building the expertise, relationships, networks and experience in all areas of exponential technology. Kloor obtained a BA from Southern Oregon State College, an MEd from Southern Oregon University and two simultaneous PhDs, one in chemistry and one in physics, from Purdue University. Kloor co-founded the company Universal Consultants, where he served as chief science consultant, providing guidance to clients in the development of new technological products, patents and policy positions. Kloor was the founder of Stem CC Inc. – a stem cell company that was sold in 2018 to Celularity, one of the world’s most cutting edge clinical-stage cell therapeutics company. Kloor is also the founder and president of Jupiter 9 Productions and is a credited film writer, director and producer. Since his graduation from Purdue University, he has written for Star Trek: Voyager and was the story editor for Gene Roddenberry’s Earth: Final Conflict, a series he co-created/developed. Kloor helped create Taiwan’s animation industry, bringing Quantum Quest: A Cassini Space Odyssey, the first big animation film that starred major Hollywood stars, to Taiwan. Kloor also sits on the board of Brain Mapping and Therapeutics Society and serves as their Chief Scientific Advisor and Educational Outreach Coordinator. Originality/value: Kloor is a “creative consultant and universal problem solver, with an emphasis in technology and education.” Kloor has worked with Dr Peter Diamandis since the first class of the International Space University in 1988. Kloor was one of the five founding team members of XPRIZE serving as its CSO until 2005 and was one of the founders of the Rocket Racing League. He was on the founding team of Singularity University and taught at Singularity’s first summer program. In 2016 he created the $10m Avatar XPRIZE, and in 2018 he co-created the Carbon Extraction XPRIZE which obtained the largest incentive prize in history, a $100m, funded by Elon Musk and the Musk Foundation. Kloor is the only person in world history to earn two PhDs simultaneously in two distinct academic disciplines. In recognition of this achievement, he was named ABC World News’ Person of the Week in August 1994. Kloor has received numerous awards, including The Golden Axon Award from the Society for Brain Mapping & Therapeutics. He has recently created the Kloor Cycle, a four-stage experiential autonomous learning process within Beomni’s “AI Brain,” adapted from Kolb’s Learning Cycles.",
        "DOI": "10.1108/IR-06-2022-0148",
        "paper_author": "Pransky J.",
        "affiliation_name": "Robotic Psychiatrist",
        "affiliation_city": "California",
        "affiliation_country": "United States",
        "affiliation_id": "128283279",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Causal inference in medicine and in health policy: A summary",
        "publication": "Handbook On Computer Learning And Intelligence",
        "citied_by": "0",
        "cover_date": "2022-06-29",
        "Abstract": "A data science task can be deemed as making sense of the data or testing a hypothesis about it. The conclusions inferred from the data can greatly guide us in making informative decisions. Big Data have enabled us to carry out countless prediction tasks in conjunction with machine learning, such as to identify high-risk patients suffering from a certain disease and take preventable measures. However, healthcare practitioners are not content with mere predictions; they are also interested in the cause-effect relation between input features and clinical outcomes. Understanding such relations will help doctors to treat patients and reduce the risk effectively. Causality is typically identified by randomized controlled trials. Often, such trials are not feasible when scientists and researchers turn to observational studies and attempt to draw inferences. However, observational studies may also be affected by selection and/or confounding biases that can result in wrong causal conclusions. In this chapter, we will try to highlight some of the drawbacks that may arise in traditional machine learning and statistical approaches to analyzing the observational data, particularly in the healthcare data analytics domain. We will discuss causal inference and ways to discover the cause-effect from observational studies in the healthcare domain. Moreover, we will demonstrate the application of causal inference in tackling some common machine learning issues such as missing data and model transportability. Finally, we will discuss the possibility of integrating reinforcement learning with causality as a way to counter confounding bias.",
        "DOI": "10.1142/9789811247323_0006",
        "paper_author": "Zhang W.",
        "affiliation_name": "University of California, Los Angeles",
        "affiliation_city": "Los Angeles",
        "affiliation_country": "United States",
        "affiliation_id": "60027550",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Note: Home Location Detection from Mobile Phone Data: Evidence from Togo",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2022-06-29",
        "Abstract": "Algorithms for home location inference from mobile phone data are frequently used to make high-stakes policy decisions, particularly when traditional sources of location data are unreliable or out of date. This paper documents analysis we performed in support of the government of Togo during the COVID-19 pandemic, using location information from mobile phone data to direct emergency humanitarian aid to individuals in specific geographic regions. This analysis, based on mobile phone records from millions of Togolese subscribers, highlights three main results. First, we show that a simple algorithm based on call frequencies performs reasonably well in identifying home locations, and may be suitable in contexts where machine learning methods are not feasible. Second, when machine learning algorithms can be trained with reliable and representative data, we find that they generally out-perform simpler frequency-based approaches. Third, we document considerable heterogeneity in the accuracy of home location inference algorithms across population subgroups, and discuss strategies to ensure that vulnerable mobile phone subscribers are not disadvantaged by home location inference algorithms.",
        "DOI": "10.1145/3530190.3534830",
        "paper_author": "Warren R.",
        "affiliation_name": "University of California, Berkeley",
        "affiliation_city": "Berkeley",
        "affiliation_country": "United States",
        "affiliation_id": "60025038",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Classifying Anti-Mask Tweets into Misclassification vs. Rejection: A Year-Long Study",
        "publication": "Proceedings of the 7th International Workshop on Social Media World Sensors, SIDEWAYS 2022",
        "citied_by": "2",
        "cover_date": "2022-06-28",
        "Abstract": "The debate over masks has played out vigorously over social media platforms such as Twitter over the course of the Covid-19 pandemic. Anti-maskers oppose the use of face masks on two philosophical grounds. First, they question their effectiveness and second, they reject them as an infringement of their personal liberties and freedoms. Both these narratives can be damaging in their own respective ways; misinformation can mislead people to abandon this simple public health measure, and rejection can incite unrest, disobedience and violence. Different policies, ranging from completely removing the tweet to simply placing a warning label, may be applied to these two types of anti-mask tweets to mitigate their damage. To facilitate these differentiated policy decisions, driven by the state of the pandemic and the surrounding social and political circumstances, this paper proposes a machine learning approach to separate anti-mask tweets into misinformation and rejection. Linguistic, social, auxiliary, and sentiment features are extracted from this corpus of tweets collected over the first year. A combination of these features is used to train ensemble and neural network classifiers. The results show that our machine learning framework can separate between misinformation and rejection tweets with a F1-score of around 0.90. These results are noteworthy because the framework can classify between two groups of tweets that share a common overall theme of anti-masking yet have only subtle differences. Moreover, the data collected over a period of one year implies that this separation is achieved even when the anti-masking rhetoric is embedded in widely varying social and political contexts.",
        "DOI": "10.1145/3544795.3544845",
        "paper_author": "Warnken J.",
        "affiliation_name": "UConn College of Engineering",
        "affiliation_city": "Storrs",
        "affiliation_country": "United States",
        "affiliation_id": "60151092",
        "affiliation_state": "CT"
    },
    {
        "paper_title": "Digital displacement of youth offending: addressing the issue",
        "publication": "Journal of Forensic Practice",
        "citied_by": "1",
        "cover_date": "2022-06-28",
        "Abstract": "Purpose: Global evidence suggests a potential displacement of youth offending from the physical to the digital landscape, requiring revision of existing detection and intervention methods. This study aims to explore pathways from harmful to illegal online activity perpetrated by young people, legislation and police perspectives, current detection methods and interventions. Design/methodology/approach: This perspective paper examines issues observed within a larger systematic literature review on digital youth offending. Findings: A trajectory from acceptable to harmful and subsequently illegal behaviour was identified, with a particular pathway from unethical video game activity to digitally dependent offending. Legislation and police perspectives vary by jurisdiction, with a common theme that increased officer education is key to the level of preparedness to investigate cases. Machine learning and automatic prevention show promise as detection and disruption processes, with education recommended for young people as a deterrent and redirection of skills to positive outcomes. Research limitations/implications: Recommendations for further research include a broad survey of school students to include all identified areas of digital offending, which could drive the development of targeted education by law enforcement and partner agencies for young people. Practical implications: The shift in youth offending requires the justice and educational systems to adjust how they respond to youth crime. Policy and practise shifts can include further exploration of investigative hacking, education for law enforcement and educational prevention and redirection programmes aimed at youth. Originality/value: The digital displacement of youth offending is a progressively emerging concept. This paper examines the current state of response from educational and law enforcement agencies and discusses the next steps based on what is currently known.",
        "DOI": "10.1108/JFP-03-2022-0012",
        "paper_author": "McCord A.",
        "affiliation_name": "University of Technology Sydney",
        "affiliation_city": "Sydney",
        "affiliation_country": "Australia",
        "affiliation_id": "60023932",
        "affiliation_state": "NSW"
    },
    {
        "paper_title": "Machine Learning Assisted HPC Workload Trace Generation for Leadership Scale Storage Systems",
        "publication": "HPDC 2022 - Proceedings of the 31st International Symposium on High-Performance Parallel and Distributed Computing",
        "citied_by": "4",
        "cover_date": "2022-06-27",
        "Abstract": "Monitoring and analyzing a wide range of I/O activities in an HPC cluster is important in maintaining mission-critical performance in a large-scale, multi-user, parallel storage system. Center-wide I/O traces can provide high-level information and fine-grained activities per application or per user running in the system. Studying such large-scale traces can provide helpful insights into the system. It can be used to develop predictive methods for making predictive decisions, adjusting scheduling policies, or providing decisions for the design of next-generation systems. However, sharing real-world I/O traces to expedite such research efforts leaves a few concerns; i) the cost of sharing the large traces is expensive due to this large size, and ii) privacy concern is an issue. We address such issues by building an end-to-end machine learn- ing (ML) workflow that can generate I/O traces for large-scale HPC applications. We leverage ML based feature selection and gener- ative models for I/O trace generation. The generative models are trained on I/O traces collected by the darshan I/O characterization tool over a period of one year. We present a two-step generation process consisting of two deep-learning models, called the feature generator and the trace generator. The combination of two-step generative models provides robustness by reducing the bias of the model and accounting for the stochastic nature of the I/O traces across different runs of an application. We evaluate the performance of the generative models and show that the two-step model can generate time-series I/O traces with less than 20% root mean square error.",
        "DOI": "10.1145/3502181.3531457",
        "paper_author": "Paul A.K.",
        "affiliation_name": "Birla Institute of Technology and Science, Pilani",
        "affiliation_city": "Pilani",
        "affiliation_country": "India",
        "affiliation_id": "60000414",
        "affiliation_state": "RJ"
    },
    {
        "paper_title": "Institutional pressures as drivers of circular economy in firms: A machine learning approach",
        "publication": "Journal of Cleaner Production",
        "citied_by": "50",
        "cover_date": "2022-06-25",
        "Abstract": "This paper investigates how institutional pressures affect the development of Circular Economy (CE) in firms. Using Institutional Entrepreneurship as a theoretical framework, this paper considers three different levels of institutional pressures (coercive, normative, and mimetic) to examine the effect of each pressure and their interactions on the development of CE. Seeking to clarify the debate on the effect of institutional pressures, this paper considers that the main limitation arises from the fact that previous research has analysed the relationship between institutional pressures without considering the interaction between them and the non-linearity of the processes. Deviating from previous papers, our analysis combines regression methods with Machine learning (i.e. Artificial Neural Networks), and employs data from the EU survey on Public Consultation on the Circular Economy. This research finds that while coercive pressures have a compulsory effect on the development of CE, mimetic and normative pressures do not have an effect by themselves, but only in interaction with coercive pressures. Moreover, this paper shows that the application of machine learning tools has an important contribution in solving interaction problems. From the perspective of environmental policy, this means that a comprehensive policy is required, which implies the coexistence or interaction of the three types of pressures.",
        "DOI": "10.1016/j.jclepro.2022.131738",
        "paper_author": "Carlos F.A.A.",
        "affiliation_name": "Essex Business School",
        "affiliation_city": "Colchester",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60170457",
        "affiliation_state": "Essex"
    },
    {
        "paper_title": "A data-augmentation approach to deriving long-term surface SO<inf>2</inf> across Northern China: Implications for interpretable machine learning",
        "publication": "Science of the Total Environment",
        "citied_by": "20",
        "cover_date": "2022-06-25",
        "Abstract": "Until recently, Northern China was one of the most SO2 polluted regions in the world. The lack of long-term and spatially resolved surface SO2 data hinders retrospective evaluation of relevant environmental policies and human health effects. This study aims to derive the spatiotemporal distribution of surface SO2 across Northern China during 2005–2019. As “concept drift” causes substantial estimation bias in back-extrapolation, we propose a new approach named the robust back-extrapolation via data augmentation approach (RBE-DA) to model the long-term surface SO2. The results show that the population-weighted regional SO2 ([SO2]pw) increased from 2005 to 2007 and decreased steadily afterwards. The [SO2]pw decreased by 80.4% from 74.2 ± 28.4 μg/m3 in 2007 to 14.6 ± 4.8 μg/m3 in 2019. The predicted spatial distributions for each year show that the SO2 pollution was severe (more than 20 μg/m3) in most areas of Northern China until 2017. By using model interpretation methods, we visually reveal the mechanism of estimation bias in the back-extrapolation. Specifically, the training data is severely imbalanced with respect to the satellite-retrieved SO2 column densities (i.e., it is short on high-value samples), so the benchmark model is unable to extrapolate the effects of this important predictor. This study provides long-term surface SO2 data for post hoc evaluation and human exposure assessment in Northern China, while demonstrating that the interpretable machine learning approach is critical for model diagnostics and refinement. Leveraging satellite retrievals, the RBE-DA approach can be applied worldwide to back-extrapolate various measures of air quality.",
        "DOI": "10.1016/j.scitotenv.2022.154278",
        "paper_author": "Zhang S.",
        "affiliation_name": "Sichuan University",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China",
        "affiliation_id": "60016521",
        "affiliation_state": "Sichuan"
    },
    {
        "paper_title": "Digitalization policy and practice towards the comprehensive and sustainable green growth of the industrial sector with special reference to the Indian Economy",
        "publication": "Handbook of Research on Building Greener Economics and Adopting Digital Tools in the Era of Climate Change",
        "citied_by": "0",
        "cover_date": "2022-06-24",
        "Abstract": "This chapter highlights the significance of digital policy framework and the practice carried out by the manufacturing industrial sector in India for the sustainable green growth during the digitalization era. The Industry 4.0 has been put forward for an innovative path for industrial rebellion that would support the industry through connectivity, automation, robotics, and machine learning. In this juncture, the industry has to adopt the various technology which brings the comprehensive and sustainable green growth through digitalization in their production process. This chapter follows the descriptive research method through the conceptual framework of the various existing related works done at the national and international levels. A well-defined and comprehensive degree of literature has been collected and structured on the basis of significance of the study. On the whole, the study reveals that the digitalization supports the industries to reduce the emission and maximizes output and profitability of the manufacturing sector with sustainability.",
        "DOI": "10.4018/978-1-6684-4610-2.ch001",
        "paper_author": "Sivasubramanian K.",
        "affiliation_name": "Kristu Jayanti College",
        "affiliation_city": "Bengaluru",
        "affiliation_country": "India",
        "affiliation_id": "60115704",
        "affiliation_state": "KA"
    },
    {
        "paper_title": "Crop Recognition Method Based on Gradient Features and Multilayer Perceptron with Application to Maize Recognition",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2022-06-24",
        "Abstract": "At present, using time-series remote sensing data to identify large-scale crop planting types is of great significance for the government to formulate macro policies and guide agricultural production. As one of the main food crops in Inner Mongolia, it is very necessary to identify maize from large regional crops. Based on MODIS (Moderate-resolution Imaging Spectroradiometer) remote sensing images and artificial field sampling data, this paper constructs a unique crop temporal vegetation index data set along the Yellow Plain in Inner Mongolia; A multi-layer perceptron algorithm integrating NDVI (Normalized Difference Vegetation Index) gradient features is proposed for the first time to realize the intelligent recognition of corn in the Yellow plain of Inner Mongolia. The experimental results show that the accuracy of the model can reach 85.2%, which is better than the traditional machine learning method using the original NDVI features.",
        "DOI": "10.1145/3548608.3559275",
        "paper_author": "Xu L.",
        "affiliation_name": "Neimenggu Agricultural University",
        "affiliation_city": "Hohhot",
        "affiliation_country": "China",
        "affiliation_id": "60028205",
        "affiliation_state": "Nei Mongol"
    },
    {
        "paper_title": "Editorial: Application of Big Data, Deep Learning, Machine Learning, and Other Advanced Analytical Techniques in Environmental Economics and Policy",
        "publication": "Frontiers in Environmental Science",
        "citied_by": "0",
        "cover_date": "2022-06-24",
        "Abstract": "NA",
        "DOI": "10.3389/fenvs.2022.953659",
        "paper_author": "Cheong T.S.",
        "affiliation_name": "The Hang Seng University of Hong Kong",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong",
        "affiliation_id": "60086451",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Reinforcement Learning Based Dance Movement Generation",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "1",
        "cover_date": "2022-06-22",
        "Abstract": "Generating genuinely creative and novel artifacts with machine learning is still a challenge in the world of computational science. A creative machine learning agent can be beneficial for applications where novel solutions are desired and may also optimize search. Reinforcement Learnings' (RL) interactive properties can make it an effective tool to investigate these possibilities in creative contexts. This paper shows how a Reinforcement learning-based technique, in combination with Principal Component Analysis (PCA), can be utilized for generating varying movements based on a goal picking policy. The proposed model is trained on a data set of motion capture recordings of dance improvisation. Our study shows that the trained RL agent can learn to pick sequences of dance poses that are coherent, have compound movement, and can resemble dance.",
        "DOI": "10.1145/3537972.3538007",
        "paper_author": "Toverud Ruud M.",
        "affiliation_name": "Universitetet i Oslo",
        "affiliation_city": "Oslo",
        "affiliation_country": "Norway",
        "affiliation_id": "60010348",
        "affiliation_state": "Oslo"
    },
    {
        "paper_title": "Algorithms Off-limits?: If digital trade law restricts access to source code of software then accountability will suffer",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "7",
        "cover_date": "2022-06-21",
        "Abstract": "Free trade agreements are increasingly used to construct an additional layer of protection for source code of software. This comes in the shape of a new prohibition for governments to require access to, or transfer of, source code of software, subject to certain exceptions. A clause on software source code is also part and parcel of an ambitious set of new rules on trade-related aspects of electronic commerce currently negotiated by 86 members of the World Trade Organization. Our understanding to date of how such a commitment inside trade law impacts on governments right to regulate digital technologies and the policy space that is allowed under trade law is limited. Access to software source code is for example necessary to meet regulatory and judicial needs in order to ensure that digital technologies are in conformity with individuals' human rights and societal values. This article will unpack and analyze the implications of such a source code clause for current and future digital policies by governments that aim to ensure transparency, fairness and accountability of computer and machine learning algorithms.",
        "DOI": "10.1145/3531146.3533212",
        "paper_author": "Irion K.",
        "affiliation_name": "Universiteit van Amsterdam",
        "affiliation_city": "Amsterdam",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60002483",
        "affiliation_state": "Noord-Holland"
    },
    {
        "paper_title": "Adversarial Scrutiny of Evidentiary Statistical Software",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "3",
        "cover_date": "2022-06-21",
        "Abstract": "The U.S. criminal legal system increasingly relies on software output to convict and incarcerate people. In a large number of cases each year, the government makes these consequential decisions based on evidence from statistical software - such as probabilistic genotyping, environmental audio detection and toolmark analysis tools - that the defense counsel cannot fully cross-examine or scrutinize. This undermines the commitments of the adversarial criminal legal system, which relies on the defense's ability to probe and test the prosecution's case to safeguard individual rights. Responding to this need to adversarially scrutinize output from such software, we propose robust adversarial testing as a framework to examine the validity of evidentiary statistical software. We define and operationalize this notion of robust adversarial testing for defense use by drawing on a large body of recent work in robust machine learning and algorithmic fairness. We demonstrate how this framework both standardizes the process for scrutinizing such tools and empowers defense lawyers to examine their validity for instances most relevant to the case at hand. We further discuss existing structural and institutional challenges within the U.S. criminal legal system which may create barriers for implementing this framework and close with a discussion on policy changes that could help address these concerns.",
        "DOI": "10.1145/3531146.3533228",
        "paper_author": "Abebe R.",
        "affiliation_name": "Department of Electrical Engineering and Computer Sciences",
        "affiliation_city": "Berkeley",
        "affiliation_country": "United States",
        "affiliation_id": "60121438",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Beyond Fairness: Reparative Algorithms to Address Historical Injustices of Housing Discrimination in the US",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "16",
        "cover_date": "2022-06-21",
        "Abstract": "Fairness in Machine Learning (ML) has mostly focused on interrogating the fairness of a particular decision point with assumptions made that the people represented in the data have been fairly treated throughout history. However, fairness cannot be ultimately achieved if such assumptions are not valid. This is the case for mortgage lending discrimination in the US, which should be critically understood as the result of historically accumulated injustices that were enacted through public policies and private practices including redlining, racial covenants, exclusionary zoning, and predatory inclusion, among others. With the erroneous assumptions of historical fairness in ML, Black borrowers with low income and low wealth are considered as a given condition in a lending algorithm, thus rejecting loans to them would be considered a \"fair\"decision even though Black borrowers were historically excluded from homeownership and wealth creation. To emphasize such issues, we introduce case studies using contemporary mortgage lending data as well as historical census data in the US. First, we show that historical housing discrimination has differentiated each racial group's baseline wealth which is a critical input for algorithmically determining mortgage loans. The second case study estimates the cost of housing reparations in the algorithmic lending context to redress historical harms because of such discriminatory housing policies. Through these case studies, we envision what reparative algorithms would look like in the context of housing discrimination in the US. This work connects to emerging scholarship on how algorithmic systems can contribute to redressing past harms through engaging with reparations policies and programs.",
        "DOI": "10.1145/3531146.3533160",
        "paper_author": "So W.",
        "affiliation_name": "Massachusetts Institute of Technology",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States",
        "affiliation_id": "60022195",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "It's Just Not That Simple: An Empirical Study of the Accuracy-Explainability Trade-off in Machine Learning for Public Policy",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "54",
        "cover_date": "2022-06-21",
        "Abstract": "To achieve high accuracy in machine learning (ML) systems, practitioners often use complex \"black-box\"models that are not easily understood by humans. The opacity of such models has resulted in public concerns about their use in high-stakes contexts and given rise to two conflicting arguments about the nature - and even the existence - of the accuracy-explainability trade-off. One side postulates that model accuracy and explainability are inversely related, leading practitioners to use black-box models when high accuracy is important. The other side of this argument holds that the accuracy-explainability trade-off is rarely observed in practice and consequently, that simpler interpretable models should always be preferred. Both sides of the argument operate under the assumption that some types of models, such as low-depth decision trees and linear regression are more explainable, while others such as neural networks and random forests, are inherently opaque. Our main contribution is an empirical quantification of the trade-off between model accuracy and explainability in two real-world policy contexts. We quantify explainability in terms of how well a model is understood by a human-in-the-loop (HITL) using a combination of objectively measurable criteria, such as a human's ability to anticipate a model's output or identify the most important feature of a model, and subjective measures, such as a human's perceived understanding of the model. Our key finding is that explainability is not directly related to whether a model is a black-box or interpretable and is more nuanced than previously thought. We find that black-box models may be as explainable to a HITL as interpretable models and identify two possible reasons: (1) that there are weaknesses in the intrinsic explainability of interpretable models and (2) that more information about a model may confuse users, leading them to perform worse on objectively measurable explainability tasks. In summary, contrary to both positions in the literature, we neither observed a direct trade-off between accuracy and explainability nor found interpretable models to be superior in terms of explainability. It's just not that simple!",
        "DOI": "10.1145/3531146.3533090",
        "paper_author": "Bell A.",
        "affiliation_name": "New York University",
        "affiliation_city": "New York",
        "affiliation_country": "United States",
        "affiliation_id": "60021784",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Adaptive Sampling Strategies to Construct Equitable Training Datasets",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "17",
        "cover_date": "2022-06-21",
        "Abstract": "In domains ranging from computer vision to natural language processing, machine learning models have been shown to exhibit stark disparities, often performing worse for members of traditionally underserved groups. One factor contributing to these performance gaps is a lack of representation in the data the models are trained on. It is often unclear, however, how to operationalize representativeness in specific applications. Here we formalize the problem of creating equitable training datasets, and propose a statistical framework for addressing this problem. We consider a setting where a model builder must decide how to allocate a fixed data collection budget to gather training data from different subgroups. We then frame dataset creation as a constrained optimization problem, in which one maximizes a function of group-specific performance metrics based on (estimated) group-specific learning rates and costs per sample. This flexible approach incorporates preferences of model-builders and other stakeholders, as well as the statistical properties of the learning task. When data collection decisions are made sequentially, we show that under certain conditions this optimization problem can be efficiently solved even without prior knowledge of the learning rates. To illustrate our approach, we conduct a simulation study of polygenic risk scores on synthetic genomic data - an application domain that often suffers from non-representative data collection. When optimizing policies for overall or group-specific average health, we find that our adaptive approach outperforms heuristic strategies, including equal and representative sampling. In this sense, equal treatment with respect to sampling decisions does not guarantee equal or equitable outcomes.",
        "DOI": "10.1145/3531146.3533203",
        "paper_author": "Cai W.",
        "affiliation_name": "Stanford University",
        "affiliation_city": "Stanford",
        "affiliation_country": "United States",
        "affiliation_id": "60012708",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Social Inclusion in Curated Contexts: Insights from Museum Practices",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "9",
        "cover_date": "2022-06-21",
        "Abstract": "Artificial intelligence literature suggests that minority and fragile communities in society can be negatively impacted by machine learning algorithms due to inherent biases in the design process, which lead to socially exclusive decisions and policies. Faced with similar challenges in dealing with an increasingly diversified audience, the museum sector has seen changes in theory and practice, particularly in the areas of representation and meaning-making. While rarity and grandeur used to be at the centre stage of the early museum practices, folk life and museums' relationships with the diverse communities they serve become a widely integrated part of the contemporary practices. These changes address issues of diversity and accessibility in order to offer more socially inclusive services. Drawing on these changes and reflecting back on the AI world, we argue that the museum experience provides useful lessons for building AI with socially inclusive approaches, especially in situations in which both a collection and access to it will need to be curated or filtered, as frequently happens in search engines, recommender systems and digital libraries. We highlight three principles: (1) Instead of upholding the value of neutrality, practitioners are aware of the influences of their own backgrounds and those of others on their work. By not claiming to be neutral but practising cultural humility, the chances of addressing potential biases can be increased. (2) There should be room for situational interpretation beyond the stages of data collection and machine learning. Before applying models and predictions, the contexts in which relevant parties exist should be taken into account. (3) Community participation serves the needs of communities and has the added benefit of bringing practitioners and communities together.",
        "DOI": "10.1145/3531146.3533095",
        "paper_author": "Huang H.Y.",
        "affiliation_name": "Delft University of Technology",
        "affiliation_city": "Delft",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60006288",
        "affiliation_state": "Zuid-Holland"
    },
    {
        "paper_title": "Promoting Fairness in Learned Models by Learning to Active Learn under Parity Constraints",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "9",
        "cover_date": "2022-06-21",
        "Abstract": "Machine learning models can have consequential effects when used to automate decisions, and disparities between groups of people in the error rates of those decisions can lead to harms suffered more by some groups than others. Past algorithmic approaches aim to enforce parity across groups given a fixed set of training data; instead, we ask: what if we can gather more data to mitigate disparities? We develop a meta-learning algorithm for parity-constrained active learning that learns a policy to decide which labels to query so as to maximize accuracy subject to parity constraints. To optimize the active learning policy, our proposed algorithm formulates the parity-constrained active learning task as a bi-level optimization problem. The inner level corresponds to training a classifier on a subset of labeled examples. The outer level corresponds to updating the selection policy choosing this subset to achieve a desired fairness and accuracy behavior on the trained classifier. To solve this constrained bi-level optimization problem, we employ the Forward-Backward Splitting optimization method. Empirically, across several parity metrics and classification tasks, our approach outperforms alternatives by a large margin.",
        "DOI": "10.1145/3531146.3534632",
        "paper_author": "Sharaf A.",
        "affiliation_name": "Microsoft Corporation",
        "affiliation_city": "Redmond",
        "affiliation_country": "United States",
        "affiliation_id": "60026532",
        "affiliation_state": "WA"
    },
    {
        "paper_title": "Robots Enact Malignant Stereotypes",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "26",
        "cover_date": "2022-06-21",
        "Abstract": "Stereotypes, bias, and discrimination have been extensively documented in Machine Learning (ML) methods such as Computer Vision (CV) [18, 80], Natural Language Processing (NLP) [6], or both, in the case of large image and caption models such as OpenAI CLIP [14]. In this paper, we evaluate how ML bias manifests in robots that physically and autonomously act within the world. We audit one of several recently published CLIP-powered robotic manipulation methods, presenting it with objects that have pictures of human faces on the surface which vary across race and gender, alongside task descriptions that contain terms associated with common stereotypes. Our experiments definitively show robots acting out toxic stereotypes with respect to gender, race, and scientifically-discredited physiognomy, at scale. Furthermore, the audited methods are less likely to recognize Women and People of Color. Our interdisciplinary sociotechnical analysis synthesizes across fields and applications such as Science Technology and Society (STS), Critical Studies, History, Safety, Robotics, and AI. We find that robots powered by large datasets and Dissolution Models (sometimes called \"foundation models\", e.g. CLIP) that contain humans risk physically amplifying malignant stereotypes in general; and that merely correcting disparities will be insufficient for the complexity and scale of the problem. Instead, we recommend that robot learning methods that physically manifest stereotypes or other harmful outcomes be paused, reworked, or even wound down when appropriate, until outcomes can be proven safe, effective, and just. Finally, we discuss comprehensive policy changes and the potential of new interdisciplinary research on topics like Identity Safety Assessment Frameworks and Design Justice to better understand and address these harms.",
        "DOI": "10.1145/3531146.3533138",
        "paper_author": "Hundt A.",
        "affiliation_name": "College of Computing",
        "affiliation_city": "Atlanta",
        "affiliation_country": "United States",
        "affiliation_id": "60097290",
        "affiliation_state": "GA"
    },
    {
        "paper_title": "Algorithmic Fairness and Vertical Equity: Income Fairness with IRS Tax Audit Models",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "15",
        "cover_date": "2022-06-21",
        "Abstract": "This study examines issues of algorithmic fairness in the context of systems that inform tax audit selection by the United States Internal Revenue Service (IRS). While the field of algorithmic fairness has developed primarily around notions of treating like individuals alike, we instead explore the concept of vertical equity - appropriately accounting for relevant differences across individuals - which is a central component of fairness in many public policy settings. Applied to the design of the U.S. individual income tax system, vertical equity relates to the fair allocation of tax and enforcement burdens across taxpayers of different income levels. Through a unique collaboration with the Treasury Department and IRS, we use access to detailed, anonymized individual taxpayer microdata, risk-selected audits, and random audits from 2010-14 to study vertical equity in tax administration. In particular, we assess how the adoption of modern machine learning methods for selecting taxpayer audits may affect vertical equity. Our paper makes four contributions. First, we show how the adoption of more flexible machine learning (classification) methods - as opposed to simpler models - shapes vertical equity by shifting audit burdens from high to middle-income taxpayers. Second, given concerns about high audit rates of low-income taxpayers, we investigate how existing algorithmic fairness techniques would change the audit distribution. We find that such methods can mitigate some disparities across income buckets, but that these come at a steep cost to performance. Third, we show that the choice of whether to treat risk of underreporting as a classification or regression problem is highly consequential. Moving from a classification approach to a regression approach to predict the expected magnitude of underreporting shifts the audit burden substantially toward high income individuals, while increasing revenue. Last, we investigate the role of differential audit cost in shaping the distribution of audits. Audits of lower income taxpayers, for instance, are typically conducted by mail and hence pose much lower cost to the IRS. We show that a narrow focus on return-on-investment can undermine vertical equity. Our results have implications for ongoing policy debates and the design of algorithmic tools across the public sector.",
        "DOI": "10.1145/3531146.3533204",
        "paper_author": "Black E.",
        "affiliation_name": "School of Computer Science",
        "affiliation_city": "Pittsburgh",
        "affiliation_country": "United States",
        "affiliation_id": "60136640",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "Dynamic Privacy Budget Allocation Improves Data Efficiency of Differentially Private Gradient Descent",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "3",
        "cover_date": "2022-06-21",
        "Abstract": "Protecting privacy in learning while maintaining the model performance has become increasingly critical in many applications that involve sensitive data. A popular private learning framework is differentially private learning composed of many privatized gradient iterations by noising and clipping. Under the privacy constraint, it has been shown that the dynamic policies could improve the final iterate loss, namely the quality of published models. In this talk, we will introduce these dynamic techniques for learning rate, batch size, noise magnitude and gradient clipping. Also, we discuss how the dynamic policy could change the convergence bounds which further provides insight of the impact of dynamic methods.",
        "DOI": "10.1145/3531146.3533070",
        "paper_author": "Hong J.",
        "affiliation_name": "Michigan State University",
        "affiliation_city": "East Lansing",
        "affiliation_country": "United States",
        "affiliation_id": "60031707",
        "affiliation_state": "MI"
    },
    {
        "paper_title": "AI Ethics Statements: Analysis and Lessons Learnt from NeurIPS Broader Impact Statements",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "15",
        "cover_date": "2022-06-21",
        "Abstract": "Ethics statements have been proposed as a mechanism to increase transparency and promote reflection on the societal impacts of published research. In 2020, the machine learning (ML) conference NeurIPS broke new ground by requiring that all papers include a broader impact statement. This requirement was removed in 2021, in favour of a checklist approach. The 2020 statements therefore provide a unique opportunity to learn from the broader impact experiment: to investigate the benefits and challenges of this and similar governance mechanisms, as well as providing an insight into how ML researchers think about the societal impacts of their own work. Such learning is needed as NeurIPS and other venues continue to question and adapt their policies. To enable this, we have created a dataset containing the impact statements from all NeurIPS 2020 papers, along with additional information such as affiliation type, location and subject area, and a simple visualisation tool for exploration. We also provide an initial quantitative analysis of the dataset, covering representation, engagement, common themes, and willingness to discuss potential harms alongside benefits. We investigate how these vary by geography, affiliation type and subject area. Drawing on these findings, we discuss the potential benefits and negative outcomes of ethics statement requirements, and their possible causes and associated challenges. These lead us to several lessons to be learnt from the 2020 requirement: (i) the importance of creating the right incentives, (ii) the need for clear expectations and guidance, and (iii) the importance of transparency and constructive deliberation. We encourage other researchers to use our dataset to provide additional analysis, to further our understanding of how researchers responded to this requirement, and to investigate the benefits and challenges of this and related mechanisms.",
        "DOI": "10.1145/3531146.3533780",
        "paper_author": "Ashurst C.",
        "affiliation_name": "The Alan Turing Institute",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60111768",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Making the Unaccountable Internet: The Changing Meaning of Accounting in the Early ARPANET",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "4",
        "cover_date": "2022-06-21",
        "Abstract": "Contemporary concerns over the governance of technological systems often run up against narratives about the technical infeasibility of designing mechanisms for accountability. While in recent AI ethics literature these concerns have been deliberated predominantly in relation to machine learning, other instances in the history of computing also presented circumstances in which computer scientists needed to un-muddle what it means to design accountable systems. One such compelling narrative can frequently be found in canonical histories of the Internet that highlight how its original designers' commitment to the \"End-to-End\"architectural principle precluded other features from being implemented, resulting in the fast-growing, generative, but ultimately unaccountable network we have today. This paper offers a critique of such technologically essentialist notions of accountability and the characterization of the \"unaccountable Internet\"as an unintended consequence. It explores the changing meaning of accounting and its relationship to accountability in a selected corpus of requests for comments (RFCs) concerning the early Internet's design from the 1970s and 80s. We characterize four ways of conceptualizing accounting: as billing, as measurement, as management, and as policy, and demonstrate how an understanding of accountability was constituted through these shifting meanings. We link together the administrative and technical mechanisms of accounting for shared resources in a distributed system and an emerging notion of accountability as a social, political, and technical category, arguing that the former is constitutive of the latter. Recovering this history is not only important for understanding the processes that shaped the Internet, but also serves as a starting point for unpacking the complicated political choices that are involved in designing accountability mechanisms for other technological systems today.",
        "DOI": "10.1145/3531146.3533137",
        "paper_author": "Cooper A.F.",
        "affiliation_name": "Cornell Ann S. Bowers College of Computing and Information Science",
        "affiliation_city": "Ithaca",
        "affiliation_country": "United States",
        "affiliation_id": "60278093",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "A data-driven framework for abnormally high building energy demand detection with weather and block morphology at community scale",
        "publication": "Journal of Cleaner Production",
        "citied_by": "13",
        "cover_date": "2022-06-20",
        "Abstract": "Buildings are one of the most important energy use sectors in cities, and forecasting the abnormal increase in building energy demand in certain climatic conditions is necessary to adjust building energy operations and implement energy policy. Accordingly, this research proposes a data-driven abnormally high energy demand detection framework in urban buildings based on their design parameters and local weather data, with the support of machine learning techniques. In this study, 71 public buildings with energy records in Jianhu city, Jiangsu province, China, were selected to abstract urban morphologies at community scale. The weather profile for the city was obtained from year 2015–2018 to create weather characteristics. Three machine learning algorithms—random forest, support vector machine, and artificial neural network—were applied to identify the months of abnormally high electricity consumption in different building types. This framework also explores key variables in the data and provides the basis for a system that prioritizes the acquisition of variables when complete data is unavailable. The results show that, with complete data, the accuracy score of the system in this study can reach 0.854 with the SVM algorithm, and the model returned an accuracy of 0.865 with the RF model after the key variable selection. Based on those results, the framework in this study can generate preemptive warnings for months with an expected abnormally high energy consumption in target buildings as a prerequisite of energy policy.",
        "DOI": "10.1016/j.jclepro.2022.131602",
        "paper_author": "Lin Q.",
        "affiliation_name": "College of Arts &amp; Sciences",
        "affiliation_city": "Chapel Hill",
        "affiliation_country": "United States",
        "affiliation_id": "60142023",
        "affiliation_state": "NC"
    },
    {
        "paper_title": "Sibyl: Adaptive and Extensible Data Placement in Hybrid Storage Systems Using Online Reinforcement Learning",
        "publication": "Proceedings - International Symposium on Computer Architecture",
        "citied_by": "19",
        "cover_date": "2022-06-18",
        "Abstract": "Hybrid storage systems (HSS) use multiple different storage devices to provide high and scalable storage capacity at high performance. Data placement across different devices is critical to maximize the benefts of such a hybrid system. Recent research proposes various techniques that aim to accurately identify performance-critical data to place it in a \"best-ft\"storage device. Unfortunately, most of these techniques are rigid, which (1) limits their adaptivity to perform well for a wide range of workloads and storage device confgurations, and (2) makes it difcult for designers to extend these techniques to different storage system confgurations (e.g., with a different number or different types of storage devices) than the confguration they are designed for. Our goal is to design a new data placement technique for hybrid storage systems that overcomes these issues and provides: (1) adaptivity, by continuously learning from and adapting to the workload and the storage device characteristics, and (2) easy extensibility to a wide range of workloads and HSS confgurations. We introduce Sibyl, the frst technique that uses reinforcement learning for data placement in hybrid storage systems. Sibyl observes different features of the running workload as well as the storage devices to make system-aware data placement decisions. For every decision it makes, Sibyl receives a reward from the system that it uses to evaluate the long-term performance impact of its decision and continuously optimizes its data placement policy online. We implement Sibyl on real systems with various HSS confgurations, including dual-and tri-hybrid storage systems, and extensively compare it against four previously proposed data placement techniques (both heuristic-and machine learning-based) over a wide range of workloads. Our results show that Sibyl provides 21.6%/19.9% performance improvement in a performanceoriented/cost-oriented HSS confguration compared to the best previous data placement technique. Our evaluation using an HSS confguration with three different storage devices shows that Sibyl outperforms the state-of-the-art data placement policy by 23.9%-48.2%, while signifcantly reducing the system architect's burden in designing a data placement mechanism that can simultaneously incorporate three storage devices. We show that Sibyl achieves 80% of the performance of an oracle policy that has complete knowledge of future access patterns while incurring a very modest storage overhead of only 124.4 KiB.",
        "DOI": "10.1145/3470496.3527442",
        "paper_author": "Singh G.",
        "affiliation_name": "ETH Zürich",
        "affiliation_city": "Zurich",
        "affiliation_country": "Switzerland",
        "affiliation_id": "60025858",
        "affiliation_state": "ZH"
    },
    {
        "paper_title": "Conceptual challenges of researching Artificial Intelligence in public administrations",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "3",
        "cover_date": "2022-06-15",
        "Abstract": "Research has been advancing on the development and deployment of Artificial Intelligence (AI) in public administrations. However, there is limited consensus and agreement on what is considered Artificial Intelligence, as different understandings and approaches in research and practice exist. This paper explores and compares the varying ways AI has been described and understood in previous Information Systems and eGovernment research. Following, a survey amongst Belgium civil servants is analysed to assess what they associate with the term Artificial Intelligence. The findings show that many civil servants tend to associate AI with being able to conduct intelligent tasks, have certain capabilities or are specific applications they are familiar with. Specific algorithms or learning methods, often included in research papers, are not associated with the term AI. These results show that researchers and policymakers may have opposite or even paradoxical views on what is or is not AI, which could have significant consequences for researching the adoption of AI in government, as well as comparing different research findings. In this respect, the paper proposes to use an integrative lens to study AI in government, by including different dimensions and understandings.",
        "DOI": "10.1145/3543434.3543441",
        "paper_author": "Van Noordt C.",
        "affiliation_name": "Tallinna Tehnikaülikool",
        "affiliation_city": "Tallinn",
        "affiliation_country": "Estonia",
        "affiliation_id": "60068861",
        "affiliation_state": "Harjumaa"
    },
    {
        "paper_title": "Prediction of suicidal ideation in shift workers compared to non-shift workers using machine learning techniques",
        "publication": "Journal of Affective Disorders",
        "citied_by": "5",
        "cover_date": "2022-06-15",
        "Abstract": "Background: Shift work can affect sleep and increase the risk of suicide. This study attempted to predict suicidal ideation according to shift work by using machine learning techniques. Methods: We analyzed a total of 43,095 data conducted by using the 10-year Korean National Health and Nutrition Examination Survey (KHANES). Shift workers and daytime workers were categorized and analyzed using random forest (RF) and decision tree (DT) techniques of machine learning techniques. Results: Shift workers were more than twice as likely to have suicidal ideation as daytime workers. The RF model showed an accuracy of 91.6% for shift workers and 98% for daytime workers. In the DT technique, the rate of suicidal ideation was the highest among shift workers (82.7%) when they were depressed and had an EuroQol-5 Dimension (EQ-5D) score of less than 0.71. Limitations: Shift work type was evaluated questionnaire and based on screening data, it was not possible to reflect recent changes in the work type and we evaluated for only suicidal ideation for suicide risk factors. Conclusion: The variables influencing the suicide risk of shift workers and daytime workers differ. In the case of shift workers, negative factors such as depression and low quality of life are risk factors for suicide. Efforts are needed to reduce risk factors through administrative and policy interventions to manage workers' health by early screening.",
        "DOI": "10.1016/j.jad.2022.03.076",
        "paper_author": "Park H.",
        "affiliation_name": "Kangbuk Samsung Hospital",
        "affiliation_city": "Jongno-gu",
        "affiliation_country": "South Korea",
        "affiliation_id": "60280060",
        "affiliation_state": "Seoul"
    },
    {
        "paper_title": "Machine learning based prediction for China's municipal solid waste under the shared socioeconomic pathways",
        "publication": "Journal of Environmental Management",
        "citied_by": "56",
        "cover_date": "2022-06-15",
        "Abstract": "Reliable forecast of municipal solid waste (MSW) generation is crucial for sustainable and efficient waste management. Big data analysis is a novel method to forecast MSW more accurately. Thus, this study employs five kinds of supervised machine learning approaches including linear regression, polynomial regression, support vector machine, random forest, and extreme gradient boosting (XGBoost) to examine their forecast performances. China's MSW generation from 2020 to 2060 under five shared socioeconomic pathways (SSPs) is further predicted and the mechanisms between MSW generation and socioeconomic features are explored. Results show that population and GDP are two dominant indicators in MSW prediction, and XGBoost model is proved to be effective in MSW forecast. MSW generation of China in 2060 is estimated to be 464–688 megatons under different SSPs scenarios, about four to six times of that in 2000. SSP3 that has the most population, least GDP and the highest climate change challenges is the only scenario showing a potential of MSW peak during the study period. The key for MSW increase is mainly the increase of per capita MSW caused by GDP. Finally, several policy recommendations are raised to reduce the overall MSW generation.",
        "DOI": "10.1016/j.jenvman.2022.114918",
        "paper_author": "Zhang C.",
        "affiliation_name": "Shanghai Jiao Tong University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60025084",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Learning-Aided Intermittent Cooperative Jamming Scheme for Nonslotted Wireless Transmission in an IoT System",
        "publication": "IEEE Internet of Things Journal",
        "citied_by": "6",
        "cover_date": "2022-06-15",
        "Abstract": "The boom of the Internet of Things (IoT) has exposed many security issues in recent years. Cooperative jamming, including the continuous jamming strategy (CJS) and intermittent jamming strategy (IJS), is an effective approach toward secure wireless communication in the physical layer. CJS used to be a primary physical-layer security technology that sends cooperative jamming signals at the expense of energy consumption. Different from CJS, IJS is more energy efficient. The feasibility of IJS has been proved in a slotted scenario, which motivates us to design IJS in a nonslotted scenario. In this article, we discuss the feasibility of IJS for a nonslotted transmission IoT system and formulate an optimization problem based on a sense-harvest-jam policy. This problem is to find the optimal matching precision between durations of artificial noise and legitimate signals. To solve this problem, we exploit a backpropagation-neural-network model to analyze jamming duration proportion and derive the optimal proportion for the binary phase-shift keying modulation. Finally, we design a matching precision optimization algorithm to achieve the optimal nonslotted secure transmission. Simulation results on jamming efficiency demonstrate that the proposed IJS has preferable secure performance than the CJS under energy constraints.",
        "DOI": "10.1109/JIOT.2021.3085275",
        "paper_author": "Huo Y.",
        "affiliation_name": "Beijing Jiaotong University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60022381",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "An End-to-End Automatic Cache Replacement Policy Using Deep Reinforcement Learning",
        "publication": "Proceedings International Conference on Automated Planning and Scheduling, ICAPS",
        "citied_by": "9",
        "cover_date": "2022-06-13",
        "Abstract": "In the past few decades, much research has been conducted on the design of cache replacement policies. Prior work frequently relies on manually-engineered heuristics to capture the most common cache access patterns, or predict the reuse distance and try to identify the blocks that are either cache-friendly or cache-averse. Researchers are now applying recent advances in machine learning to guide cache replacement policy, augmenting or replacing traditional heuristics and data structures. However, most existing approaches depend on the certain environment which restricted their application, e.g, most of the approaches only consider the on-chip cache consisting of program counters (PCs). Moreover, those approaches with attractive hit rates are usually unable to deal with modern irregular workloads, due to the limited feature used. In contrast, we propose a pervasive cache replacement framework to automatically learn the relationship between the probability distribution of different replacement policies and workload distribution by using deep reinforcement learning. We train an end-to-end cache replacement policy only on the past requested address through two simple and stable cache replacement policies. Furthermore, the overall framework can be easily plugged into any scenario that requires cache. Our simulation results on 8 production storage traces run against 3 different cache configurations confirm that the proposed cache replacement policy is effective and outperforms several state-of-the-art approaches.",
        "DOI": "10.1609/icaps.v32i1.19840",
        "paper_author": "Zhou Y.",
        "affiliation_name": "Huazhong University of Science and Technology",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60025761",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Inferring Probabilistic Reward Machines from Non-Markovian Reward Signals for Reinforcement Learning",
        "publication": "Proceedings International Conference on Automated Planning and Scheduling, ICAPS",
        "citied_by": "12",
        "cover_date": "2022-06-13",
        "Abstract": "The success of reinforcement learning in typical settings is predicated on Markovian assumptions on the reward signal by which an agent learns optimal policies. In recent years, the use of reward machines has relaxed this assumption by enabling a structured representation of non-Markovian rewards. In particular, such representations can be used to augment the state space of the underlying decision process, thereby facilitating non-Markovian reinforcement learning. However, these reward machines cannot capture the semantics of stochastic reward signals. In this paper, we make progress on this front by introducing probabilistic reward machines (PRMs) as a representation of non-Markovian stochastic rewards. We present an algorithm to learn PRMs from the underlying decision process and prove results around its correctness and convergence.",
        "DOI": "10.1609/icaps.v32i1.19844",
        "paper_author": "Dohmen T.",
        "affiliation_name": "University of Colorado Boulder",
        "affiliation_city": "Boulder",
        "affiliation_country": "United States",
        "affiliation_id": "60000221",
        "affiliation_state": "CO"
    },
    {
        "paper_title": "LSched: A Workload-Aware Learned Query Scheduler for Analytical Database Systems",
        "publication": "Proceedings of the ACM SIGMOD International Conference on Management of Data",
        "citied_by": "12",
        "cover_date": "2022-06-10",
        "Abstract": "Query scheduling is a crucial task for analytical database systems that can greatly affect the query latency. However, existing scheduling approaches are based on heuristics and not optimal. A recent trial proposed to use reinforcement learning for automatically learning end-to-end scheduling policies. However, such trial was not capable of considering the database-specific characteristics (e.g., operator types, pipelining), and hence becomes not efficient for analytical database systems. In this paper, we try to fill this gap and introduce LSched (Learned Scheduler), a fully learned workload-aware query scheduler for in-memory analytical database systems. LSched provides an efficient inter-query and intra-query scheduling for dynamic analytical workloads (i.e., different queries can arrive/depart at any time). We integrated LSched with an efficient in-memory analytical database system, and evaluated it with TPCH, SSB, and JOB benchmarks. Our evaluation shows that LSched improves over the performance of existing state-of-the-art query schedulers and heuristic-based ones by at least 35% and 50% in both streaming and batching query workloads.",
        "DOI": "10.1145/3514221.3526158",
        "paper_author": "Sabek I.",
        "affiliation_name": "Massachusetts Institute of Technology",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States",
        "affiliation_id": "60022195",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Exploring the effects of land management change on productivity, carbon and nutrient balance: Application of an Ensemble Modelling Approach to the upper River Taw observatory, UK",
        "publication": "Science of the Total Environment",
        "citied_by": "8",
        "cover_date": "2022-06-10",
        "Abstract": "Agriculture is challenged to produce healthy food and to contribute to cleaner energy whilst mitigating climate change and protecting ecosystems. To achieve this, policy-driven scenarios need to be evaluated with available data and models to explore trade-offs with robust accounting for the uncertainty in predictions. We developed a novel model ensemble using four complementary state-of-the-art agroecosystems models to explore the impacts of land management change. The ensemble was used to simulate key agricultural and environmental outputs under various scenarios for the upper River Taw observatory, UK. Scenarios assumed (i) reducing livestock production whilst simultaneously increasing the area of arable where it is feasible to cultivate (PG2A), (ii) reducing livestock production whilst simultaneously increasing bioenergy production in areas of the catchment that are amenable to growing bioenergy crops (PG2BE) and (iii) increasing both arable and bioenergy production (PG2A + BE). Our ensemble approach combined model uncertainty using the tower property of expectation and the law of total variance. Results show considerable uncertainty for predicted nutrient losses with different models partitioning the uncertainty into different pathways. Bioenergy crops were predicted to produce greatest yields from Miscanthus in lowland and from SRC-willow (cv. Endurance) in uplands. Each choice of management is associated with trade-offs; e.g. PG2A results in a significant increase of edible calories (6736 Mcal ha−1) but reduced soil C (−4.32 t C ha−1). Model ensembles in the agroecosystem context are difficult to implement due to challenges of model availability and input and output alignment. Despite these challenges, we show that ensemble modelling is a powerful approach for applications such as ours, offering benefits such as capturing structural as well as data uncertainty and allowing greater combinations of variables to be explored. Furthermore, the ensemble provides a robust means for combining uncertainty at different scales and enables us to identify weaknesses in system understanding.",
        "DOI": "10.1016/j.scitotenv.2022.153824",
        "paper_author": "Hassall K.L.",
        "affiliation_name": "Rothamsted Research",
        "affiliation_city": "Harpenden",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60002793",
        "affiliation_state": "Hertfordshire"
    },
    {
        "paper_title": "Governance of Emerging Technologies in Health and Medicine - Creating a New Framework",
        "publication": "New England Journal of Medicine",
        "citied_by": "19",
        "cover_date": "2022-06-09",
        "Abstract": "NA",
        "DOI": "10.1056/NEJMms2200907",
        "paper_author": "Mathews D.J.H.",
        "affiliation_name": "Johns Hopkins University",
        "affiliation_city": "Baltimore",
        "affiliation_country": "United States",
        "affiliation_id": "60005248",
        "affiliation_state": "MD"
    },
    {
        "paper_title": "A Cluster-Based Technique for Identifying and Grouping Oily Waste Types Generated From Marine Oil Spill Response Operations",
        "publication": "Frontiers in Environmental Science",
        "citied_by": "2",
        "cover_date": "2022-06-08",
        "Abstract": "In the event of a marine oil spill and its subsequent response operations, different types of oily wastes are generated in large quantities, and their management is a significant challenge that oil spill responders face. The goal of this study is to develop a comprehensive pattern recognition modeling framework for deriving and grouping a set of unique clusters that separate different types of oily wastes from each other. The main idea is to group oily wastes based on their unique characteristics, such as the percentage of oil, percentage of water, percentage of mineral matter, and percentage of organic matter. Each cluster has a relatively homogeneous pattern of pollution characteristics. Prior to implementing the cluster analysis technique, it is important to evaluate and transform the raw oily waste data using well-defined criteria. An advanced machine learning technique, fuzzy C-means clustering algorithm, is employed to classify the oily wastes. The Kolmogorov–Smirnov tests are employed to examine the statistical significance of clustered data. Results show a heterogeneous diversity in seven identified clusters in relation to different types of oily wastes. The cluster-based analysis method presented in this article is an integral part of an integrated optimization-based model which will provide valuable inputs for adjustment of the existing management practices, enhancement of short-term pollution control strategies, and development of long-term oily waste management policies. The output of this study would provide a better tool to waste characterization and sorting steps that are required to immediately separate recovered waste to support downstream response efforts. This result of this study also supports the overall goal of minimizing impact on the environment by ensuring the maximum amount of recovered waste can be recycled or disposed in an environmentally friendly fashion. Moreover, properly classified, sorted, and labeled waste will greatly help with downstream steps of packaging, transportation, and tracking of waste, and as a result, it will minimize total waste management time and costs, under the constraints involving waste storage and transport capacities, waste pre-treatment and treatment facility capacities, and environmental regulatory compliance, as well as other operational and logistic constraints.",
        "DOI": "10.3389/fenvs.2022.910214",
        "paper_author": "Hafezi M.H.",
        "affiliation_name": "Dalhousie University",
        "affiliation_city": "Halifax",
        "affiliation_country": "Canada",
        "affiliation_id": "60015913",
        "affiliation_state": "NS"
    },
    {
        "paper_title": "Applying AI Deep Learning to the DOD's big simulation and training projects",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2022-06-08",
        "Abstract": "All military services have invested heavily in creating, deploying, and training with large simulation systems. Historically, these have applied artificial intelligence algorithms to controlling non-player characters (NPCs) or semi-Automated forces (SAF) during the execution phase of the events. This has allowed a few human role players to control a much larger collection of virtual/simulated entities or aggregated units. Structured algorithms like finite state machines and knowledge-based systems have typically been limited to this single execution phase of the entire training process. However, \"the new AI\", deep learning and machine learning algorithms, operate much differently from the previous generation. These models configure themselves (or learn) from massive amounts of collected data (both labeled and unlabeled). As such, real applications require the collection of massive historical data. Luckily, military simulation events regularly generate multiple gigabytes of data as a by-product of the training event. Previously a tiny portion of this was saved and curated to perform after action review, and the remainder was deleted since it had no practical use and consumed scarce and expensive storage. To leverage deep learning, the military services need to reinvent their policies, relationships, and processes for handling these huge volumes of previously worthless, but now priceless, data. Additionally, deep learning models are much more widely applicable than the previous generation of algorithms. DL models can learn to process huge volumes of data to contribute to the analysis or after action review stage of an exercise. They can also be used to auto generate variations on giant scenario databases. They can match exercise plans against exercise results to determine whether training objectives have been met. And they can animate the NPC and SAF units during the execution phase. We stand at the edge of the application of deep learning to every phase of large military training simulation events.",
        "DOI": "10.1145/3518997.3534118",
        "paper_author": "Smith R.",
        "affiliation_name": "Modelbenders Llc",
        "affiliation_city": "Orlando",
        "affiliation_country": "United States",
        "affiliation_id": "127606448",
        "affiliation_state": "FL"
    },
    {
        "paper_title": "Signature for Pain Recovery IN Teens (SPRINT): protocol for a multisite prospective signature study in chronic musculoskeletal pain",
        "publication": "BMJ Open",
        "citied_by": "1",
        "cover_date": "2022-06-08",
        "Abstract": "Introduction Current treatments for chronic musculoskeletal (MSK) pain are suboptimal. Discovery of robust prognostic markers separating patients who recover from patients with persistent pain and disability is critical for developing patient-specific treatment strategies and conceiving novel approaches that benefit all patients. Given that chronic pain is a biopsychosocial process, this study aims to discover and validate a robust prognostic signature that measures across multiple dimensions in the same adolescent patient cohort with a computational analysis pipeline. This will facilitate risk stratification in adolescent patients with chronic MSK pain and more resourceful allocation of patients to costly and potentially burdensome multidisciplinary pain treatment approaches. Methods and analysis Here we describe a multi-institutional effort to collect, curate and analyse a high dimensional data set including epidemiological, psychometric, quantitative sensory, brain imaging and biological information collected over the course of 12 months. The aim of this effort is to derive a multivariate model with strong prognostic power regarding the clinical course of adolescent MSK pain and function. Ethics and dissemination The study complies with the National Institutes of Health policy on the use of a single internal review board (sIRB) for multisite research, with Cincinnati Children's Hospital Medical Center Review Board as the reviewing IRB. Stanford's IRB is a relying IRB within the sIRB. As foreign institutions, the University of Toronto and The Hospital for Sick Children (SickKids) are overseen by their respective ethics boards. All participants provide signed informed consent. We are committed to open-access publication, so that patients, clinicians and scientists have access to the study data and the signature(s) derived. After findings are published, we will upload a limited data set for sharing with other investigators on applicable repositories. Trial registration number NCT04285112.",
        "DOI": "10.1136/bmjopen-2022-061548",
        "paper_author": "Simons L.",
        "affiliation_name": "Stanford University School of Medicine",
        "affiliation_city": "Stanford",
        "affiliation_country": "United States",
        "affiliation_id": "60032838",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "RACE: A Reinforcement Learning Framework for Improved Adaptive Control of NoC Channel Buffers",
        "publication": "Proceedings of the ACM Great Lakes Symposium on VLSI, GLSVLSI",
        "citied_by": "3",
        "cover_date": "2022-06-06",
        "Abstract": "Network-on-chip (NoC) architectures rely on buffers to store flits to cope with contention for router resources during packet switching. Recently, reversible multi-function channel (RMC) buffers have been proposed to simultaneously reduce power and enable adaptive NoC buffering between adjacent routers. While adaptive buffering can improve NoC performance by maximizing buffer utilization, controlling the RMC buffer allocations requires a congestion-aware, scalable, and proactive policy. In this work, we present RACE, a novel reinforcement learning (RL) framework that utilizes better awareness of network congestion and a new reward metric (\"falsefulls\") to help guide the RL agent towards better RMC buffer control decisions. We show that RACE reduces NoC latency by up to 48.9%, and energy consumption by up to 47.1% against state-of-the-art NoC buffer control policies.",
        "DOI": "10.1145/3526241.3530335",
        "paper_author": "Khan K.",
        "affiliation_name": "Walter Scott, Jr. College of Engineering",
        "affiliation_city": "Fort Collins",
        "affiliation_country": "United States",
        "affiliation_id": "60136737",
        "affiliation_state": "CO"
    },
    {
        "paper_title": "Associations Between Aggregate NLP-Extracted Conflicts of Interest and Adverse Events by Drug Product",
        "publication": "Studies in Health Technology and Informatics",
        "citied_by": "8",
        "cover_date": "2022-06-06",
        "Abstract": "This study evaluates associations between aggregate conflicts of interest (COI) and drug safety. We used a machine-learning system to extract and classify COI from PubMed-indexed disclosure statements. Individual conflicts were classified as Type 1 (personal fees, travel, board memberships, and non-financial support), Type 2 (grants and research support), or Type 3 (stock ownership and industry employment). COI were aggregated by type compared to adverse events by product. Type 1 COI are associated with a 1.1-1.8% increase in the number of adverse events, serious events, hospitalizations, and deaths. Type 2 COI are associated with a 1.7-2% decrease in adverse events across severity levels. Type 3 COI are associated with an approximately 1% increase in adverse events, serious events, and hospitalizations, but have no significant association with adverse events resulting in death. The findings suggest that COI policies might be adapted to account the relative risks of different types of financial relationships.",
        "DOI": "10.3233/SHTI220106",
        "paper_author": "Graham S.S.",
        "affiliation_name": "The University of Texas at Austin",
        "affiliation_city": "Austin",
        "affiliation_country": "United States",
        "affiliation_id": "60013372",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "Gradient-Free Neural Network Training via Synaptic-Level Reinforcement Learning",
        "publication": "AppliedMath",
        "citied_by": "3",
        "cover_date": "2022-06-01",
        "Abstract": "An ongoing challenge in neural information processing is the following question: how do neurons adjust their connectivity to improve network-level task performance over time (i.e., actualize learning)? It is widely believed that there is a consistent, synaptic-level learning mechanism in specific brain regions, such as the basal ganglia, that actualizes learning. However, the exact nature of this mechanism remains unclear. Here, we investigate the use of universal synaptic-level algorithms in training connectionist models. Specifically, we propose an algorithm based on reinforcement learning (RL) to generate and apply a simple biologically-inspired synaptic-level learning policy for neural networks. In this algorithm, the action space for each synapse in the network consists of a small increase, decrease, or null action on the connection strength. To test our algorithm, we applied it to a multilayer perceptron (MLP) neural network model. This algorithm yields a static synaptic learning policy that enables the simultaneous training of over 20,000 parameters (i.e., synapses) and consistent learning convergence when applied to simulated decision boundary matching and optical character recognition tasks. The trained networks yield character-recognition performance comparable to identically shaped networks trained with gradient descent. The approach has two significant advantages in comparison to traditional gradient-descent-based optimization methods. First, the robustness of our novel method and its lack of reliance on gradient computations opens the door to new techniques for training difficult-to-differentiate artificial neural networks, such as spiking neural networks (SNNs) and recurrent neural networks (RNNs). Second, the method’s simplicity provides a unique opportunity for further development of local information-driven multiagent connectionist models for machine intelligence analogous to cellular automata.",
        "DOI": "10.3390/appliedmath2020011",
        "paper_author": "Bhargava A.",
        "affiliation_name": "University of Toronto",
        "affiliation_city": "Toronto",
        "affiliation_country": "Canada",
        "affiliation_id": "60016849",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "Using machine learning to revalue the regional coordinated development policy —analysis of heterogeneous treatment effects from causal forest",
        "publication": "China Journal of Economics",
        "citied_by": "1",
        "cover_date": "2022-06-01",
        "Abstract": "Although under the dual mechanism of “promising government” and “effective market”, the development gap between the East China and the West China has been narrowing and the economy is still growing at a high speed. For a long time in the past, the two phenomena coexist at the same time. However, restraining the spatial division of labor will lead to resource mismatch, and this way of narrowing the regional development gap will usually sacrifice economic efficiency, so the effect of regional coordination policy aimed at promoting the rapid development of underdeveloped areas is controversial. In view of this, this paper uses causal forest based machine learning for the first time to evaluate the impact of regional policies on urban production efficiency and relative public welfare since 2003,the results show that: (1) from 2003 to 2017, the role of regional policy in narrowing the relative public welfare gap between regions is not obvious, but it significantly improves the per capita GDP growth of less developed regions, with an annual contribution of 2. 2%; (2) the role of regional policy in improving the production efficiency of less developed regions is declining, and at present, many cities have gradually dropped to near the “zero” effect; (3) the effect of preferential policies on promoting the production efficiency in southern cities is greater than that of northern regions; (4) regional policy intervention can not weaken the restriction of port factors on the economic development of underdeveloped regions, especially in southern regions. Ridge regression, Lasso and regularization test were used to test the validity and robustness of the causal forest results.",
        "DOI": "NA",
        "paper_author": "Hu Z.",
        "affiliation_name": "Changsha University of Science and Technology",
        "affiliation_city": "Changsha",
        "affiliation_country": "China",
        "affiliation_id": "60001338",
        "affiliation_state": "Hunan"
    },
    {
        "paper_title": "Human-machine integration method for steering decision-making of intelligent vehicle based on reinforcement learning",
        "publication": "Jiaotong Yunshu Gongcheng Xuebao/Journal of Traffic and Transportation Engineering",
        "citied_by": "0",
        "cover_date": "2022-06-01",
        "Abstract": "In terms of the continuous dynamic allocation problem of driving weights between human and autonomous driving systems in the human-machine integration (HMD driving system of intelligent vehicles, especially the low adaptability problem of weight allocation methods caused by modeling errors, a HMI steering decision-making method based on the reinforcement learning was proposed. In view of drivers' steering characteristics, a driver model based on the two-point preview was built, and an autonomous steering control model of intelligent vehicles was established by adopting the predictive control theory. On this basis, a steering control framework of simultaneous human-machine in-loop for intelligent vehicles was constructed. According to the Actor-Critic reinforcement learning framework, a deep deterministic policy gradient (DDPG) agent for the human-machine driving weight allocation was designed, and a model-based gain function was proposed with the curvature adaptability, tracking accuracy, and ride comfort as targets. A reinforcement learning framework for the HMI driving weight allocation was constructed, which contains a driver model, an autonomous steering model, a driving weight allocation agent, and a gain function. To verify the effectiveness of the proposed method, eight drivers were recruited, and a total of 48 simulated driving experiments were carried out. Research results show that in the verification of curvature adaptability, the HMI-DDPG method is superior to the manned driving and HMI-Fuzzy methods. The trackability improves by an average of 70. 69% and 39. 67%, respectively, and the comfortability increases by an average of 18. 34% and 7. 55%, respectively. In the verification of speed adaptability, under the conditions of a vehicle speed of 40, 60, and 80 km · h-1, the time proportion is 90.00%, 85.76%, and 60.74%, respectively, when the driver's weight is greater than 0. 5. The phase trajectories of both the trackability and the comfort can effectively converge. Therefore, the proposed method can adapt to changes in curvature and vehicle speed and improve the trackability and comfort on the premise of ensuring safety. 5 tabs, 14 figs, 31 refs.",
        "DOI": "10.19818/j.cnki.1671-1637.2022.03.004",
        "paper_author": "Wu C.Z.",
        "affiliation_name": "Wuhan University of Technology",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60022414",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "The Role of Influential Actors in Fostering the Polarized COVID-19 Vaccine Discourse on Twitter: Mixed Methods of Machine Learning and Inductive Coding",
        "publication": "JMIR Infodemiology",
        "citied_by": "7",
        "cover_date": "2022-06-01",
        "Abstract": "Background: Since COVID-19 vaccines became broadly available to the adult population, sharp divergences in uptake have emerged along partisan lines. Researchers have indicated a polarized social media presence contributing to the spread of mis- or disinformation as being responsible for these growing partisan gaps in uptake. Objective: The major aim of this study was to investigate the role of influential actors in the context of the community structures and discourse related to COVID-19 vaccine conversations on Twitter that emerged prior to the vaccine rollout to the general population and discuss implications for vaccine promotion and policy. Methods: We collected tweets on COVID-19 between July 1, 2020, and July 31, 2020, a time when attitudes toward the vaccines were forming but before the vaccines were widely available to the public. Using network analysis, we identified different naturally emerging Twitter communities based on their internal information sharing. A PageRank algorithm was used to quantitively measure the level of \"influentialness\"of Twitter accounts and identifying the \"influencers,\"followed by coding them into different actor categories. Inductive coding was conducted to describe discourses shared in each of the 7 communities. Results: Twitter vaccine conversations were highly polarized, with different actors occupying separate \"clusters.\"The antivaccine cluster was the most densely connected group. Among the 100 most influential actors, medical experts were outnumbered both by partisan actors and by activist vaccine skeptics or conspiracy theorists. Scientists and medical actors were largely absent from the conservative network, and antivaccine sentiment was especially salient among actors on the political right. Conversations related to COVID-19 vaccines were highly polarized along partisan lines, with \"trust\"in vaccines being manipulated to the political advantage of partisan actors. Conclusions: These findings are informative for designing improved vaccine information communication strategies to be delivered on social media especially by incorporating influential actors. Although polarization and echo chamber effect are not new in political conversations in social media, it was concerning to observe these in health conversations on COVID-19 vaccines during the vaccine development process.",
        "DOI": "10.2196/34231",
        "paper_author": "Hagen L.",
        "affiliation_name": "University of South Florida, Tampa",
        "affiliation_city": "Tampa",
        "affiliation_country": "United States",
        "affiliation_id": "60007740",
        "affiliation_state": "FL"
    },
    {
        "paper_title": "Can Interpretable Reinforcement Learning Manage Prosperity Your Way?",
        "publication": "AI (Switzerland)",
        "citied_by": "2",
        "cover_date": "2022-06-01",
        "Abstract": "Personalisation of products and services is fast becoming the driver of success in banking and commerce. Machine learning holds the promise of gaining a deeper understanding of and tailoring to customers’ needs and preferences. Whereas traditional solutions to financial decision problems frequently rely on model assumptions, reinforcement learning is able to exploit large amounts of data to improve customer modelling and decision-making in complex financial environments with fewer assumptions. Model explainability and interpretability present challenges from a regulatory perspective which demands transparency for acceptance; they also offer the opportunity for improved insight into and understanding of customers. Post-hoc approaches are typically used for explaining pretrained reinforcement learning models. Based on our previous modeling of customer spending behaviour, we adapt our recent reinforcement learning algorithm that intrinsically characterizes desirable behaviours and we transition to the problem of prosperity management. We train inherently interpretable reinforcement learning agents to give investment advice that is aligned with prototype financial personality traits which are combined to make a final recommendation. We observe that the trained agents’ advice adheres to their intended characteristics, they learn the value of compound growth, and, without any explicit reference, the notion of risk as well as improved policy convergence.",
        "DOI": "10.3390/ai3020030",
        "paper_author": "Maree C.",
        "affiliation_name": "Sparebank 1 SR-Bank",
        "affiliation_city": "Stavanger",
        "affiliation_country": "Norway",
        "affiliation_id": "127430628",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A hybrid spatiotemporal convolution-based cellular automata model (ST-CA) for land-use/cover change simulation",
        "publication": "International Journal of Applied Earth Observation and Geoinformation",
        "citied_by": "28",
        "cover_date": "2022-06-01",
        "Abstract": "Accurate land-use/-cover change (LUCC) simulation is of great significance to issues closely related to regional planning and policy-making. Many models have been committed to conducting LUCC simulations for better decision-making. However, LUCC is a nonlinear spatiotemporal process with complex links and feedback as well as latent dependencies in both spatial and temporal neighborhoods. They are challenging to be integrally utilized using existing models that employ conventional statistical or machine learning methods, inevitably leading to inaccurate LUCC simulations. Aiming to handle this problem, this paper innovatively proposed a hybrid spatiotemporal convolution-based cellular automata model (ST-CA) by coupling nonlinear spatiotemporal dependency learning and CA-based spatial allocation. A three-dimensional convolutional neural network (3D-CNN) was introduced in the model to assimilate both the nonlinear driving mechanism and spatiotemporal dependencies. It contributes to generating more elaborate development potentials to increase simulation accuracy. To evaluate the model performance, an LUCC simulation was applied on a national scale in China by ST-CA. Four traditional CA models, namely, logistic regression (LR)-CA, random forest (RF)-CA, full-connected neural network (FCN)-CA, and convolutional neural network (CNN)-CA, were also developed for accuracy comparisons. The results demonstrate that the simulation by ST-CA reached an FoM of 18.42%, which outperformed the other models with accuracy increases of 11.65%, 13.11%, 7.01%, and 2.29%, respectively. The proposed model incorporating 3D-CNN effectively captured the nonlinear spatiotemporal properties in the LUCC process, which is promising for more accurate LUCC simulations.",
        "DOI": "10.1016/j.jag.2022.102789",
        "paper_author": "Geng J.",
        "affiliation_name": "Beijing Normal University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60023237",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Multistakeholder platforms for natural resource governance: lessons from eight landscape-level cases",
        "publication": "Ecology and Society",
        "citied_by": "15",
        "cover_date": "2022-06-01",
        "Abstract": "Multistakeholder platforms (MSPs) are the subject of increasing attention and investment in the domain of collaborative natural resource governance, yet evidence-based guidance is slim on policy and investment priorities to leverage the MSP approach. We provide a comparative analysis of eight landscape-level MSPs spanning seven countries (Peru, Brazil, India, Tanzania, Ethiopia, and a cross-border case from Kenya and Somalia), representing a diversity of resource systems covering forests, rangelands, and multiuse agricultural landscapes. Applying an adapted social-ecological systems framework, our synthesis identifies the influence of these MSPs on patterns of stakeholder interaction and draws implications for the design and organization of MSPs that are both appropriate and effective. From the cases, we distill lessons addressing: (1) how to design an MSP in relation to the governance context, including the fit between institutional and ecological dimensions of the system and with attention to cross-scale linkages; (2) how to implement inclusive processes that address power inequities, including through capacity building and procedural rules; and (3) how to support adaptive learning to expand the MSP’s influence over time, including monitoring outcomes, adapting the scope of stakeholder engagement, and investing in MSP durability.",
        "DOI": "10.5751/ES-13168-270202",
        "paper_author": "Ratner B.D.",
        "affiliation_name": "Collaborating for Resilience (CoRe)",
        "affiliation_city": null,
        "affiliation_country": "United States",
        "affiliation_id": "128463962",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Machine-Learning Analysis of the Impacts of the COVID-19 Pandemic on Small Business Owners and Implications for Canadian Government Policy Response",
        "publication": "Canadian Public Policy",
        "citied_by": "8",
        "cover_date": "2022-06-01",
        "Abstract": "This study applies a machine-learning technique to a dataset of 38,000 textual comments from Canadian small business owners on the impacts of coronavirus disease 2019 (COVID-19). Topic modelling revealed seven topics covering the short- and longer-term impacts of the pandemic, government relief programs and loan eligibility issues, mental health, and other impacts on business owners. The results emphasize the importance of policy response in aiding small business crisis management and offer implications for theory and policy. Moreover, the study provides an example of using a machine-learning–based automated content analysis in the fields of crisis management, small business, and public policy.",
        "DOI": "10.3138/cpp.2021-018",
        "paper_author": "Isabelle D.A.",
        "affiliation_name": "Sprott School of Business",
        "affiliation_city": "Ottawa",
        "affiliation_country": "Canada",
        "affiliation_id": "60189772",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "A deeper look at the 2020 Facebook boycott and related themes of misinformation: A text mining analysis of topics, emotion and sentiment",
        "publication": "Journal of Brand Strategy",
        "citied_by": "2",
        "cover_date": "2022-06-01",
        "Abstract": "Facebook has repeatedly come under fire from consumers, companies and government agencies in recent years owing to the prevalence of misinformation on its platform as well as the way it handles information related to social justice and public health, among other things. Using a large data set of 604,269 social media mentions sourced from popular social platforms (eg Twitter and Reddit), this study set out to discover themes associated with misinformation and the Facebook advertising boycott that occurred in July 2020. To understand the discourse, a linguistic analysis approach called theme extraction was used. This method employs machine learning and natural language processing to reveal relationships in the data that may otherwise be buried in the mass of social media mentions. The most prominent theme identified among social media users was the desire that Facebook and other social media platforms actively stop the spread of misinformation. Other trending topics included #StopHateForProfit, lockdown protests, hate speech policy, news as spam, right-wing politics and suspended ads. Managerial implications for advertisers are discussed as they relate to social media management and how misinformation impacts brand engagement on social platforms.",
        "DOI": "NA",
        "paper_author": "Bright L.F.",
        "affiliation_name": "The University of Texas at Austin",
        "affiliation_city": "Austin",
        "affiliation_country": "United States",
        "affiliation_id": "60013372",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "Explainable Artificial Intelligence (XAI)",
        "publication": "Information Technology and Libraries",
        "citied_by": "27",
        "cover_date": "2022-06-01",
        "Abstract": "The field of explainable artificial intelligence (XAI) advances techniques, processes, and strategies that provide explanations for the predictions, recommendations, and decisions of opaque and complex machine learning systems. Increasingly academic libraries are providing library users with systems, services, and collections created and delivered by machine learning. Academic libraries should adopt XAI as a tool set to verify and validate these resources, and advocate for public policy regarding XAI that serves libraries, the academy, and the public interest.",
        "DOI": "10.6017/ITAL.V41I2.14683",
        "paper_author": "Ridley M.",
        "affiliation_name": "University of Guelph",
        "affiliation_city": "Guelph",
        "affiliation_country": "Canada",
        "affiliation_id": "60015881",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "Ethical and Legal Aspects of Technology-Assisted Care in Neurodegenerative Disease",
        "publication": "Journal of Personalized Medicine",
        "citied_by": "4",
        "cover_date": "2022-06-01",
        "Abstract": "Technological solutions are increasingly seen as a way to respond to the demands of managing complex chronic conditions, especially neurodegenerative diseases such as Parkinson’s Disease. All of these new possibilities provide a variety of chances to improve the lives of affected persons and their families, friends, and caregivers. However, there are also a number of challenges that should be considered in order to safeguard the interests of affected persons. In this article, we discuss the ethical and legal considerations associated with the use of technology-assisted care in the context of neurodegenerative conditions.",
        "DOI": "10.3390/jpm12061011",
        "paper_author": "Schmitz-Luhn B.",
        "affiliation_name": "Universität Bonn",
        "affiliation_city": "Bonn",
        "affiliation_country": "Germany",
        "affiliation_id": "60007493",
        "affiliation_state": "Nordrhein-Westfalen"
    },
    {
        "paper_title": "Predicting 30-Day Readmission Risk for Patients with Chronic Obstructive Pulmonary Disease through a Federated Machine Learning Architecture on Findable, Accessible, Interoperable, and Reusable (FAIR) Data: Development and Validation Study",
        "publication": "JMIR Medical Informatics",
        "citied_by": "7",
        "cover_date": "2022-06-01",
        "Abstract": "Background: Owing to the nature of health data, their sharing and reuse for research are limited by legal, technical, and ethical implications. In this sense, to address that challenge and facilitate and promote the discovery of scientific knowledge, the Findable, Accessible, Interoperable, and Reusable (FAIR) principles help organizations to share research data in a secure, appropriate, and useful way for other researchers. Objective: The objective of this study was the FAIRification of existing health research data sets and applying a federated machine learning architecture on top of the FAIRified data sets of different health research performing organizations. The entire FAIR4Health solution was validated through the assessment of a federated model for real-time prediction of 30-day readmission risk in patients with chronic obstructive pulmonary disease (COPD). Methods: The application of the FAIR principles on health research data sets in 3 different health care settings enabled a retrospective multicenter study for the development of specific federated machine learning models for the early prediction of 30-day readmission risk in patients with COPD. This predictive model was generated upon the FAIR4Health platform. Finally, an observational prospective study with 30 days follow-up was conducted in 2 health care centers from different countries. The same inclusion and exclusion criteria were used in both retrospective and prospective studies. Results: Clinical validation was demonstrated through the implementation of federated machine learning models on top of the FAIRified data sets from different health research performing organizations. The federated model for predicting the 30-day hospital readmission risk was trained using retrospective data from 4.944 patients with COPD. The assessment of the predictive model was performed using the data of 100 recruited (22 from Spain and 78 from Serbia) out of 2070 observed (records viewed) patients during the observational prospective study, which was executed from April 2021 to September 2021. Significant accuracy (0.98) and precision (0.25) of the predictive model generated upon the FAIR4Health platform were observed. Therefore, the generated prediction of 30-day readmission risk was confirmed in 87% (87/100) of cases. Conclusions: Implementing a FAIR data policy in health research performing organizations to facilitate data sharing and reuse is relevant and needed, following the discovery, access, integration, and analysis of health research data. The FAIR4Health project proposes a technological solution in the health domain to facilitate alignment with the FAIR principles.",
        "DOI": "10.2196/35307",
        "paper_author": "Alvarez-Romero C.",
        "affiliation_name": "Hospital Universitario Virgen del Rocío",
        "affiliation_city": "Sevilla",
        "affiliation_country": "Spain",
        "affiliation_id": "60033267",
        "affiliation_state": "Seville"
    },
    {
        "paper_title": "Private and public efforts infuse artificial intelligence into materials research",
        "publication": "MRS Bulletin",
        "citied_by": "1",
        "cover_date": "2022-06-01",
        "Abstract": "NA",
        "DOI": "10.1557/s43577-022-00357-8",
        "paper_author": "Dyatkin B.",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "\"Un\"Fair Machine Learning Algorithms",
        "publication": "Management Science",
        "citied_by": "27",
        "cover_date": "2022-06-01",
        "Abstract": "Ensuring fairness in algorithmic decision making is a crucial policy issue. Current legislation ensures fairness by barring algorithm designers fromusing demographic information in their decision making. As a result, to be legally compliant, the algorithms need to ensure equal treatment. However, in many cases, ensuring equal treatment leads to disparate impact particularly when there are differences among groups based on demographic classes. In response, several \"fair\"machine learning (ML) algorithms that require impact parity (e.g., equal opportunity) at the cost of equal treatment have recently been proposed to adjust for the societal inequalities. Advocates of fair ML propose changing the law to allow the use of protected class-specific decision rules.We show that the proposed fairML algorithms that require impact parity, while conceptually appealing, can make everyone worse off, including the very class they aim to protect. Compared with the current law, which requires treatment parity, the fairML algorithms, which require impact parity, limit the benefits of a more accurate algorithm for a firm. As a result, profit maximizing firms could underinvest in learning, that is, improving the accuracy of their machine learning algorithms. We show that the investment in learning decreases when misclassification is costly, which is exactly the case when greater accuracy is otherwise desired. Our paper highlights the importance of considering strategic behavior of stake holders when developing and evaluating fair ML algorithms. Overall, our results indicate that fair ML algorithms that require impact parity, if turned into law,may not be able to deliver some of the anticipated benefits.",
        "DOI": "10.1287/mnsc.2021.4065",
        "paper_author": "Fu R.",
        "affiliation_name": "Carnegie Mellon University",
        "affiliation_city": "Pittsburgh",
        "affiliation_country": "United States",
        "affiliation_id": "60027950",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "A new automatic convolutional neural network based on deep reinforcement learning for fault diagnosis",
        "publication": "Frontiers of Mechanical Engineering",
        "citied_by": "8",
        "cover_date": "2022-06-01",
        "Abstract": "Convolutional neural network (CNN) has achieved remarkable applications in fault diagnosis. However, the tuning aiming at obtaining the well-trained CNN model is mainly manual search. Tuning requires considerable experiences on the knowledge on CNN training and fault diagnosis, and is always time consuming and labor intensive, making the automatic hyper parameter optimization (HPO) of CNN models essential. To solve this problem, this paper proposes a novel automatic CNN (ACNN) for fault diagnosis, which can automatically tune its three key hyper parameters, namely, learning rate, batch size, and L2-regulation. First, a new deep reinforcement learning (DRL) is developed, and it constructs an agent aiming at controlling these three hyper parameters along with the training of CNN models online. Second, a new structure of DRL is designed by combining deep deterministic policy gradient and long short-term memory, which takes the training loss of CNN models as its input and can output the adjustment on these three hyper parameters. Third, a new training method for ACNN is designed to enhance its stability. Two famous bearing datasets are selected to evaluate the performance of ACNN. It is compared with four commonly used HPO methods, namely, random search, Bayesian optimization, tree Parzen estimator, and sequential model-based algorithm configuration. ACNN is also compared with other published machine learning (ML) and deep learning (DL) methods. The results show that ACNN outperforms these HPO and ML/DL methods, validating its potential in fault diagnosis. [Figure not available: see fulltext.].",
        "DOI": "10.1007/s11465-022-0673-7",
        "paper_author": "Wen L.",
        "affiliation_name": "China University of Geosciences",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60006019",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Coevolution of machine learning and process-based modelling to revolutionize Earth and environmental sciences: A perspective",
        "publication": "Hydrological Processes",
        "citied_by": "38",
        "cover_date": "2022-06-01",
        "Abstract": "Machine learning (ML) applications in Earth and environmental sciences (EES) have gained incredible momentum in recent years. However, these ML applications have largely evolved in ‘isolation’ from the mechanistic, process-based modelling (PBM) paradigms, which have historically been the cornerstone of scientific discovery and policy support. In this perspective, we assert that the cultural barriers between the ML and PBM communities limit the potential of ML, and even its ‘hybridization’ with PBM, for EES applications. Fundamental, but often ignored, differences between ML and PBM are discussed as well as their strengths and weaknesses in light of three overarching modelling objectives in EES, (1) nowcasting and prediction, (2) scenario analysis, and (3) diagnostic learning. The paper ponders over a ‘coevolutionary’ approach to model building, shifting away from a borrowing to a co-creation culture, to develop a generation of models that leverage the unique strengths of ML such as scalability to big data and high-dimensional mapping, while remaining faithful to process-based knowledge base and principles of model explainability and interpretability, and therefore, falsifiability.",
        "DOI": "10.1002/hyp.14596",
        "paper_author": "Razavi S.",
        "affiliation_name": "Global Institute for Water Security",
        "affiliation_city": "Saskatoon",
        "affiliation_country": "Canada",
        "affiliation_id": "60189135",
        "affiliation_state": "SK"
    },
    {
        "paper_title": "A Structural Model of a Multitasking Salesforce: Incentives, Private Information, and Job Design",
        "publication": "Management Science",
        "citied_by": "9",
        "cover_date": "2022-06-01",
        "Abstract": "This paper broadens the focus of empirical research on salesforce management to include multitasking settings with multidimensional incentives, where salespeople have private information about customers. This allows us to ask novel substantive questions around multidimensional incentive design and job design while managing the costs and benefits of private information. To this end, the paper introduces the first structural model of a multitasking salesforce in response to multidimensional incentives. The model also accommodates (i) dynamic intertemporal tradeoffs in effort choice across the tasks and (ii) salesperson’s private information about customers. We apply our model in a rich empirical setting in microfinance and illustrate how to address various identification and estimation challenges. We extend two-step estimation methods used for unidimensional compensation plans by embedding a flexible machine learning (random forest) model in the first-stage multitasking policy function estimation within an iterative procedure that accounts for salesperson heterogeneity and private information. Estimates reveal two latent segments of salespeople—a hunter segment that is more efficient in loan acquisition and a farmer segment that is more efficient in loan collection. Counterfactuals reveal heterogeneous effects: hunters’ private information hurts the firm as they engage in adverse selection; farmers’ private information helps the firm as they use it to better collect loans. The payoff complementarity induced by multiplicative incentive aggregation softens adverse specialization by hunters relative to additive aggregation but hurts performance among farmers. Overall, task specialization in job design for hunters (acquisition) and farmers (collection) hurts the firm as adverse selection harm overwhelms efficiency gain.",
        "DOI": "10.1287/mnsc.2021.4079",
        "paper_author": "Kim M.",
        "affiliation_name": "UNC Kenan-Flagler Business School",
        "affiliation_city": "Chapel Hill",
        "affiliation_country": "United States",
        "affiliation_id": "60122501",
        "affiliation_state": "NC"
    },
    {
        "paper_title": "Smart Emergency EV-to-EV Portable Battery Charger",
        "publication": "Inventions",
        "citied_by": "16",
        "cover_date": "2022-06-01",
        "Abstract": "With the increase in the number of electric vehicles (EVs) and developments in their re-lated charging infrastructures, consumers have still some concerns about some limiting factors in the EV industry such as battery life, charging station availability, electric grid capacity, limited driving range, and slow charging of batteries. Although some solutions are proposed for these limitations, they are not sufficiently efficient and cost-effective. Moreover, charging of EVs on-the-road is still a challenging issue which requires more innovation. This paper proposes a novel battery charger, known as an Emergency EV-to-EV Portable Battery Charger (EPBC), which provides a cost-effective solution for charging EVs on-the-road in emergency mode. The suggested smart charger can charge another EV based on the state of charge (SOC), capacity, and other important technical specifications of batteries in a safe and reliable manner. The smart charger can regulate the output voltage and the injected current to the EV simultaneously. To realize these features, a model free nonlinear integral backstepping control (MF-NIBC) is adopted to regulate the output voltage of the battery charger. By utilizing the actor and critic networks, a deep deterministic policy gradient (DDPG) is adopted to adjust the MF-NIBC controller. Finally, real-time tests based on the OPAL-RT setup are conducted to confirm the applicability and feasibility of the proposed EV-to-EV portable battery charger.",
        "DOI": "10.3390/inventions7020045",
        "paper_author": "Mosayebi M.",
        "affiliation_name": "Aarhus Universitet",
        "affiliation_city": "Aarhus",
        "affiliation_country": "Denmark",
        "affiliation_id": "60029616",
        "affiliation_state": "Midtjylland"
    },
    {
        "paper_title": "Satellite Imagery to Map Topsoil Organic Carbon Content over Cultivated Areas: An Overview",
        "publication": "Remote Sensing",
        "citied_by": "47",
        "cover_date": "2022-06-01",
        "Abstract": "There is a need to update soil maps and monitor soil organic carbon (SOC) in the upper horizons or plough layer for enabling decision support and land management, while complying with several policies, especially those favoring soil carbon storage. This review paper is dedicated to the satellite-based spectral approaches for SOC assessment that have been achieved from several satellite sensors, study scales and geographical contexts in the past decade. Most approaches relying on pure spectral models have been carried out since 2019 and have dealt with temperate croplands in Europe, China and North America at the scale of small regions, of some hundreds of km2: dry combustion and wet oxidation were the analytical determination methods used for 50% and 35% of the satellite-derived SOC studies, for which measured topsoil SOC contents mainly referred to mineral soils, typically cambisols and luvisols and to a lesser extent, regosols, leptosols, stagnosols and chernozems, with annual cropping systems with a SOC value of ~15 g·kg−1 and a range of 30 g·kg−1 in median. Most satellite-derived SOC spectral prediction models used limited preprocessing and were based on bare soil pixel retrieval after Normalized Difference Vegetation Index (NDVI) thresholding. About one third of these models used partial least squares regression (PLSR), while another third used random forest (RF), and the remaining included machine learning methods such as support vector machine (SVM). We did not find any studies either on deep learning methods or on all-performance evaluations and uncertainty analysis of spatial model predictions. Nevertheless, the literature examined here identifies satellite-based spectral information, especially derived under bare soil conditions, as an interesting approach that deserves further investigations. Future research includes considering the simultaneous analysis of imagery acquired at several dates i.e., temporal mosaicking, testing the influence of possible disturbing factors and mitigating their effects fusing mixed models incorporating non-spectral ancillary information.",
        "DOI": "10.3390/rs14122917",
        "paper_author": "Vaudour E.",
        "affiliation_name": "Université Paris-Saclay",
        "affiliation_city": "Gif-sur-Yvette",
        "affiliation_country": "France",
        "affiliation_id": "60106017",
        "affiliation_state": "Ile-de-France"
    },
    {
        "paper_title": "Determinants of Qualified Investor Sentiment during the COVID-19 Pandemic in North America, Asia and Europe",
        "publication": "Economies",
        "citied_by": "1",
        "cover_date": "2022-06-01",
        "Abstract": "This work delineates the factors determining investor sentiment in specific regions during the pandemic and the influence of attitudes towards vaccination. The findings show that the reac-tions of knowledgeable investors in different regions to the economic effects of the pandemic were not uniform but depended on a variety of individual factors. Risk perception varied widely due to idiosyncrasies in specific countries and regions, the level of pandemic information, reaction to case reports and deaths, attitudes towards vaccination, lockdown compliance, and government measures to support businesses. These various elements combined to create different outlooks in the minds of investors that strongly influenced their investment strategies. For this investigation, we tested three estimation models: the classic robust standard error for time series regression, the new robust standard errors regression, and the Prais robust estimation. This study applied the lasso system of machine learning to select relevant explanatory variables. The novelty of our work resides in its analysis of the conduct of informed investors, using a reliable proxy, and the discussion of how government policies and different pandemic-related factors, specifically the vaccination status, affected investor sentiment in different regions. As for practical implications, an understanding of how the various economic factors related to the pandemic influenced the behavior of qualified investors in different regions can help regulators, government leaders, fund managers, and investors deal with a future virus outbreak.",
        "DOI": "10.3390/economies10060143",
        "paper_author": "Reis P.M.N.",
        "affiliation_name": "Instituto Politécnico de Viseu",
        "affiliation_city": "Viseu",
        "affiliation_country": "Portugal",
        "affiliation_id": "60013042",
        "affiliation_state": "Viseu"
    },
    {
        "paper_title": "County-Level Irrigation Water Demand Estimation Using Machine Learning: Case Study of California",
        "publication": "Water (Switzerland)",
        "citied_by": "7",
        "cover_date": "2022-06-01",
        "Abstract": "Irrigated agriculture is the largest consumer of freshwater globally. Despite the clarity of influential factors and deriving forces, estimation of the volumetric irrigation demand using biophysical models is prohibitively difficult. Data-driven models have proven their ability to predict geophysical and hydrological phenomena with only a handful of influential input variables; however, the lack of reliable input data in most agricultural regions of the world hinders the effectiveness of these approaches. Attempting to estimate the irrigation water demand, we first analyze the correlation of potential influencing variables with irrigation water. We develop machine learning models to predict California’s annual, county-level irrigation water demand based on the statistical analysis findings over an 18-year time span. Input variables are different combinations of deriving meteorological forces, geographical characteristics, cropped area, and crop category. After testing various regression machine learning approaches, the result shows that Gaussian process regression produces the best results. Our findings suggest that irrigated cropped area, air temperature, and vapor pressure deficit are the most significant variables in predicting irrigation water demand. This research also shows that Gaussian process regression can predict irrigation water demand with high accuracy (R2 higher than 0.97 and RMSE as low as 0.06 km3) with different input variable combinations. An accurate estimation of irrigation water use of various crop categories and areas can assist decision-making processes and improve water management strategies. The proposed model can help water policy makers evaluate climatological and agricultural scenarios and hence be used as a decision support tool for agricultural water management at a regional scale.",
        "DOI": "10.3390/w14121937",
        "paper_author": "Emami M.",
        "affiliation_name": "Semnan University",
        "affiliation_city": "Semnan",
        "affiliation_country": "Iran",
        "affiliation_id": "60018870",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Management of Smart and Sustainable Cities in the Post-COVID-19 Era: Lessons and Implications",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "41",
        "cover_date": "2022-06-01",
        "Abstract": "Nowadays, the concept of smart sustainable governance is wrapped around basic principles such as: (i) transparency, (ii) accountability, (iii) stakeholders’ involvement, and iv) citizens’ participation. It is through these principles that are influenced by information and communication technologies (ICT), Internet of Things (IoT), and artificial intelligence, that the practices employed by citizens and their interaction with electronic government (e-government) are diversified. Previously, the misleading concepts of the smart city implied only the objective of the local level or public officials to utilize technology. However, the recent European experience and research studies have led to a more comprehensive notion that refers to the search for intelligent solutions which allow modern sustainable cities to enhance the quality of services provided to citizens and to improve the management of urban mobility. The smart city is based on the usage of connected sensors, data management, and analytics platforms to improve the quality and functioning of built-environment systems. The aim of this paper is to understand the effects of the pandemic on smart cities and to accentuate major exercises that can be learned for post-COVID sustainable urban management and patterns. The lessons and implications outlined in this paper can be used to enforce social distancing community measures in an effective and timely way, and to optimize the use of resources in smart and sustainable cities in critical situations. The paper offers a conceptual overview and serves as a stepping-stone to extensive research and the deployment of sustainable smart city platforms and intelligent transportation systems (a sub-area of smart city applications) after the COVID-19 pandemic using a case study from Russia. Overall, our results demonstrate that the COVID-19 crisis encompasses an excellent opportunity for urban planners and policy makers to take transformative actions towards creating cities that are more intelligent and sustainable.",
        "DOI": "10.3390/su14127267",
        "paper_author": "Strielkowski W.",
        "affiliation_name": "Department of Agricultural and Resource Economics",
        "affiliation_city": "Berkeley",
        "affiliation_country": "United States",
        "affiliation_id": "60121449",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Commodity Prices after COVID-19: Persistence and Time Trends",
        "publication": "Risks",
        "citied_by": "11",
        "cover_date": "2022-06-01",
        "Abstract": "Since December 2019 we have been living with the virus known as SARS-CoV-2, a situation which has led to health policies being given prevalence over economic ones and has caused a paralysis in the demand for raw materials for several months due to the number confinements put in place around the world. Since the worst days of the pandemic caused by COVID-19, most commodity prices have been recovering. The main objective of this research work is to learn about the evolution and impact of COVID-19 on the prices of raw materials in order to understand how it will affect the behavior of the economy in the coming quarters. To this end, we use fractionally integrated methods and an Artificial Neural Network (ANN) model. During the COVID-19 pandemic episode, we observe that commodity prices have a mean reverting behavior, indicating that it will not be necessary to take additional measures since the series will return, by themselves, to their long term projections. Moreover, in our forecast using ANN algorithms, we observe that the Bloomberg Spot Commodity Index will recover its upward trend, increasing some 56.67% to the price from before the start of the COVID-19 pandemic episode.",
        "DOI": "10.3390/risks10060128",
        "paper_author": "Monge M.",
        "affiliation_name": "Universidad Francisco de Vitoria",
        "affiliation_city": "Pozuelo de Alarcon",
        "affiliation_country": "Spain",
        "affiliation_id": "60108692",
        "affiliation_state": "Madrid"
    },
    {
        "paper_title": "A Geographically Weighted Random Forest Approach to Predict Corn Yield in the US Corn Belt",
        "publication": "Remote Sensing",
        "citied_by": "34",
        "cover_date": "2022-06-01",
        "Abstract": "Crop yield prediction before the harvest is crucial for food security, grain trade, and policy making. Previously, several machine learning methods have been applied to predict crop yield using different types of variables. In this study, we propose using the Geographically Weighted Random Forest Regression (GWRFR) approach to improve crop yield prediction at the county level in the US Corn Belt. We trained the GWRFR and five other popular machine learning algorithms (Multiple Linear Regression (MLR), Partial Least Square Regression (PLSR), Support Vector Regression (SVR), Decision Tree Regression (DTR), and Random Forest Regression (RFR)) with the following different sets of features: (1) full length features; (2) vegetation indices; (3) gross primary production (GPP); (4) climate data; and (5) soil data. We compared the results of the GWRFR with those of the other five models. The results show that the GWRFR with full length features (R2 = 0.90 and RMSE = 0.764 MT/ha) outperforms other machine learning algorithms. For individual categories of features such as GPP, vegetation indices, climate, and soil features, the GWRFR also outperforms other models. The Moran’s I value of the residuals generated by GWRFR is smaller than that of other models, which shows that GWRFR can better address the spatial non-stationarity issue. The proposed method in this article can also be potentially used to improve yield prediction for other types of crops in other regions.",
        "DOI": "10.3390/rs14122843",
        "paper_author": "Khan S.N.",
        "affiliation_name": "South Dakota State University",
        "affiliation_city": "Brookings",
        "affiliation_country": "United States",
        "affiliation_id": "60014826",
        "affiliation_state": "SD"
    },
    {
        "paper_title": "Climate policy support as a tool to control others’ (but not own) environmental behavior?",
        "publication": "PLoS ONE",
        "citied_by": "2",
        "cover_date": "2022-06-01",
        "Abstract": "Drastic reductions in greenhouse gas emissions are necessary to successfully mitigate climate change. Individual environmental behavior is central to this change. Given that environmental behavior necessitates 1) effortful individual self-control and 2) cooperation by others, public policy may constitute an attractive instrument for regulating one’s own as well as others’ environmental behavior. Framing climate change mitigation as a cooperative self-control problem, we explore the incremental predictive power of self-control and beliefs surrounding others’ cooperation beyond established predictors of policy support in study 1 using machine-learning (N = 610). In study 2, we systematically test and confirm the effects of self-control and beliefs surrounding others’ cooperation (N = 270). Both studies showed that personal importance of climate change mitigation and perceived insufficiency of others’ environmental behavior predict policy support, while there was no strong evidence for a negative association between own-self control success and policy support. These results emerge beyond the effects of established predictors, such as environmental attitudes and beliefs, risk perception (study 1), and social norms (study 2). Results are discussed in terms of leveraging policy as a behavioral enactment constraint to control others’ but not own environmental behavior.",
        "DOI": "10.1371/journal.pone.0269030",
        "paper_author": "Kukowski C.A.",
        "affiliation_name": "Universität Zürich",
        "affiliation_city": "Zurich",
        "affiliation_country": "Switzerland",
        "affiliation_id": "60012614",
        "affiliation_state": "ZH"
    },
    {
        "paper_title": "Popular and Scientific Discourse on Autism: Representational Cross-Cultural Analysis of Epistemic Communities to Inform Policy and Practice",
        "publication": "Journal of Medical Internet Research",
        "citied_by": "6",
        "cover_date": "2022-06-01",
        "Abstract": "Background: Social media provide a window onto the circulation of ideas in everyday folk psychiatry, revealing the themes and issues discussed both by the public and by various scientific communities. Objective: This study explores the trends in health information about autism spectrum disorder within popular and scientific communities through the systematic semantic exploration of big data gathered from Twitter and PubMed. Methods: First, we performed a natural language processing by text-mining analysis and with unsupervised (machine learning) topic modeling on a sample of the last 10,000 tweets in English posted with the term #autism (January 2021). We built a network of words to visualize the main dimensions representing these data. Second, we performed precisely the same analysis with all the articles using the term “autism” in PubMed without time restriction. Lastly, we compared the results of the 2 databases. Results: We retrieved 121,556 terms related to autism in 10,000 tweets and 5.7x109 terms in 57,121 biomedical scientific articles. The 4 main dimensions extracted from Twitter were as follows: integration and social support, understanding and mental health, child welfare, and daily challenges and difficulties. The 4 main dimensions extracted from PubMed were as follows: diagnostic and skills, research challenges, clinical and therapeutical challenges, and neuropsychology and behavior. Conclusions: This study provides the first systematic and rigorous comparison between 2 corpora of interests, in terms of lay representations and scientific research, regarding the significant increase in information available on autism spectrum disorder and of the difficulty to connect fragments of knowledge from the general population. The results suggest a clear distinction between the focus of topics used in the social media and that of scientific communities. This distinction highlights the importance of knowledge mobilization and exchange to better align research priorities with personal concerns and to address dimensions of well-being, adaptation, and resilience. Health care professionals and researchers can use these dimensions as a framework in their consultations to engage in discussions on issues that matter to beneficiaries and develop clinical approaches and research policies in line with these interests. Finally, our study can inform policy makers on the health and social needs and concerns of individuals with autism and their caregivers, especially to define health indicators based on important issues for beneficiaries.",
        "DOI": "10.2196/32912",
        "paper_author": "Gauld C.",
        "affiliation_name": "Université de Lyon",
        "affiliation_city": "Lyon",
        "affiliation_country": "France",
        "affiliation_id": "60102126",
        "affiliation_state": "Auvergne-Rhone-Alpes"
    },
    {
        "paper_title": "COASTAL EROSION DETECTION USING LANDSAT SATELLITE IMAGERY AND SUPPORT VECTOR MACHINE ALGORITHM",
        "publication": "Journal of Ocean Technology",
        "citied_by": "2",
        "cover_date": "2022-06-01",
        "Abstract": "Erosion of coastal dunes and embankments has been an ongoing issue of the smallest Canadian province (i.e., P.E.I.) because it is largely composed of easily damaged sandstone. As more coast is eroded every year, more buildings, homes, environmental habitat, and provincial roads are at risk of being lost. This study is focused on the use of optical imagery acquired by Landsat-5 and Landsat-8, captured in September 1985 and June 2020, respectively, to determine the magnitude of coastal erosion on Robinson Island, P.E.I., Canada. The Support Vector Machine (SVM) algorithm, along with several ArcGIS software tools, were employed to map and interpret the erosion in this coastal area. The classification results, with Overall Accuracies (OA) of 97.86% and 98.46% respectively in 1985 and 2020, indicated that approximately 50.37% (i.e., from 1.35 km2 to 0.68 km2) coastal area was eroded. The results of this study are important to understand the magnitude of the erosion and adjust the required policies and mitigation actions for adaption to avoid possible environmental hazards.",
        "DOI": "NA",
        "paper_author": "Schellekens J.",
        "affiliation_name": "Marine Institute of Memorial University of Newfoundland",
        "affiliation_city": "St John's",
        "affiliation_country": "Canada",
        "affiliation_id": "60082745",
        "affiliation_state": "NL"
    },
    {
        "paper_title": "Digital soil mapping outputs on soil classification and sugarcane production in Brazil",
        "publication": "Journal of South American Earth Sciences",
        "citied_by": "5",
        "cover_date": "2022-06-01",
        "Abstract": "Soil maps at regional and farm levels are vital for the best management agricultural practices (BMAP). The soil is the substrate for plant growth and essential to ensure food security. In this context, soil maps require a detailed cartographic scale for BMAP. This study (i) investigated the use of digital soil mapping (DSM) products, such as soil chemical and physical attributes, indices, mineralogy, and properties to extrapolate late soil survey maps at 1:20,000 scale; (ii) created the digital yield environment for sugarcane based on the DSM products; and (iii) evaluated qualitatively the predict soil maps and relationship with previous studies and the predicted yield environment. The region of interest covers eight municipalities and almost 2598 km2 in São Paulo State, Brazil. The soil survey at farm level conducted covered almost 86.52 km2, ∼3.33% of the total area (96.67% of the unmapped area). We created a point grid (centroid) with the same spatial resolution (30 m) of the rasters used as covariates for soil mapping unit (SMU) predictions. This grid intended to retrieve the representative soil mapping unit of each geometric polygon. It was retrieved 117,413 points representing 27 SMU of seven soil orders at a first categorical level, according to the Brazilian Classification System, and seven yield environment for sugarcane production. SMU predictions and their respective soil orders were performed using the random forest machine learning regression method. The level of association between SMU and yield environments was 0.34 (α = 0.01) by the Cramer's V coefficient with a very strong relationship. Our approach could provide the first digital yield environment for sugarcane based on the DSM products. Furthermore, a qualitative evaluation of our framework was substantiated with previous research in the same study site. This framework could be replicated and fulfil the need for DSM at regional and farm levels for policy-makers and farmers.",
        "DOI": "10.1016/j.jsames.2022.103881",
        "paper_author": "Mendes W.d.S.",
        "affiliation_name": "Leibniz-Zentrum für Agrarlandschaftsforschung (ZALF) e. V.",
        "affiliation_city": "Muncheberg",
        "affiliation_country": "Germany",
        "affiliation_id": "60075693",
        "affiliation_state": "Brandenburg"
    },
    {
        "paper_title": "Modeling &amp; implementation of DRLA based partially shaded solar system integration with 3-ϕ conventional grid using constant current controller",
        "publication": "Heliyon",
        "citied_by": "2",
        "cover_date": "2022-06-01",
        "Abstract": "Renewable Energy Resources (RERs) are widely used on the concern of global environment protection. Solar energy systems play an important role in the generation of electrical energy, remarkably minimize the utilization of nonrenewable fuel sources. Solar energy can be extracted and transformed into electrical energy via solar photovoltaic process. Several traditional, soft computing, heuristic, and meta-heuristic maximum power point tracking (MPPT) techniques have been developed to extract Maximum Energy Point (MEP) from the solar photovoltaic modules under different atmospheric conditions. In this manuscript, the combination of reinforcement learning algorithm (RLA) and deep learning algorithm (DLA) called deep Reinforcement Learning Algorithm based MPPT (DRLAMPPT) is proposed under partial shading conditions (PSC) of the solar system. DRLAMPPT can deal with continuous state spaces, in contrast to RL it can be operated only with discrete action state spaces. In this proposed DRLAMPPT, deep deterministic policy gradient (DDPG) solves the problem of continuous state spaces are involved to reach the GMEP in photovoltaic systems especially under PSC. In DRLAMPPT, the representative's strategy is parameterized by an artificial neural network (ANN), which uses sensory information as input and directly sends out control signals. This work develops a 2 kW solar photovoltaic power plant comprises of a photovoltaic array, DC/DC step-up converter, 3-Φ Pulse Width Modulated Voltage Source Inverter (PWM-VSI) integrated with conventional power grid using Constant Current Controller (CCC Effectiveness of the proposed DRLAMPPT with CCC can be validated through an experimental setup and with MATLAB. Simulation and tested at different input conditions of solar irradiance. Experimental results prove that, in comparison to existing MPPTs, suggested DRLAMPPT not only attains the best efficiency and also adopts the change in environmental conditions of the photovoltaic system at a much faster rate and able to reach the GMEP within 0.8 s under PSC. Experimental and simulation results also prove that suggested CCC with LC filter makes the inverter output voltage and the grid voltage are in phase at the lower value of THD i.e. 1.1% and 0.98% respectively.",
        "DOI": "10.1016/j.heliyon.2022.e09669",
        "paper_author": "Guntupalli R.",
        "affiliation_name": "Pondicherry Technological University",
        "affiliation_city": "Puducherry",
        "affiliation_country": "India",
        "affiliation_id": "113973609",
        "affiliation_state": "PY"
    },
    {
        "paper_title": "Meta-Learning Approaches for Recovery Rate Prediction",
        "publication": "Risks",
        "citied_by": "6",
        "cover_date": "2022-06-01",
        "Abstract": "While previous academic research highlights the potential of machine learning and big data for predicting corporate bond recovery rates, the operations management challenge is to identify the relevant predictive variables and the appropriate model. In this paper, we use meta-learning to combine the predictions from 20 candidates of linear, nonlinear and rule-based algorithms, and we exploit a data set of predictors including security-specific factors, macro-financial indicators and measures of economic uncertainty. We find that the most promising approach consists of model combinations trained on security-specific characteristics and a limited number of well-identified, theoretically sound recovery rate determinants, including uncertainty measures. Our research provides useful indications for practitioners and regulators targeting more reliable risk measures in designing micro-and macro-prudential policies.",
        "DOI": "10.3390/risks10060124",
        "paper_author": "Gambetti P.",
        "affiliation_name": "CRIF S.p.A.",
        "affiliation_city": "Bologna",
        "affiliation_country": "Italy",
        "affiliation_id": "101929537",
        "affiliation_state": "BO"
    },
    {
        "paper_title": "Export Controls and Securitization of Economic Policy: Comparative Analysis of the Practice of the United States, the European Union, China, and Russia",
        "publication": "Journal of World Trade",
        "citied_by": "8",
        "cover_date": "2022-06-01",
        "Abstract": "National security rhetoric has gained prominence due to increasingly pervasive digitalization, the emergence of cutting-edge technologies, and developments in artificial intelligence and machine learning. Increased reliance on these areas fuels industrial development but also renders national economies vulnerable to foreign interference. Ultimately, the current wave of technological development with its potential threats intensifies competition between states and redefines their economic and military advantages over potential global rivals. Against this background, certain states have expanded the scope of their export control regimes by extending the lists of controlled items and/or imposing ‘catch-all’ control. Used in conjunction with economic sanctions, weap-onized tariffs, and extensive investment screening mechanisms aimed to protect national security interests, such measures go beyond conventional non-proliferation purposes to address economic security, technological supremacy, and human rights concerns for which those states are willing to sacrifice the economic efficiency that accompanies trade liberalization. Using the United States, the European Union, China, and Russia as case studies, this article discusses to which extent different export control objectives of these international actors have been securitized. Securitization of certain states’ interests is inevitable, even if not desirable. Yet, this article argues that international law can be managed to control and limit the level of securitization of domestic policies in order to strengthen the international legal system as a whole.",
        "DOI": "10.54648/trad2022026",
        "paper_author": "Hrynkiv O.",
        "affiliation_name": "Tilburg University",
        "affiliation_city": "Tilburg",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60017145",
        "affiliation_state": "Noord-Brabant"
    },
    {
        "paper_title": "Efficient reinforcement learning with partial observables for fluid flow control",
        "publication": "Physical Review E",
        "citied_by": "5",
        "cover_date": "2022-06-01",
        "Abstract": "Even if the trajectory in a viscous flow system stays within a low dimensional subspace in the state space, reinforcement learning (RL) requires many observables in the active control problem. This is because the observables are assumed to follow a policy-independent Markov decision process in the usual RL framework and full observation of the system is required to satisfy this assumption. Although RL with a partially observable condition is generally a difficult task, we construct a consistent algorithm with the condition using the low dimensional property of viscous flow. Using typical examples of active flow control, we show that our algorithm is more stable and efficient than the existing RL algorithms, even under a small number of observables.",
        "DOI": "10.1103/PhysRevE.105.065101",
        "paper_author": "Kubo A.",
        "affiliation_name": "Osaka University",
        "affiliation_city": "Suita",
        "affiliation_country": "Japan",
        "affiliation_id": "60024322",
        "affiliation_state": "Osaka"
    },
    {
        "paper_title": "Predicting nitrate leaching loss in temperate rainfed cereal crops: Relative importance of management and environmental drivers",
        "publication": "Environmental Research Letters",
        "citied_by": "15",
        "cover_date": "2022-06-01",
        "Abstract": "Nitrate (NO3) leaching from agriculture represents the primary source of groundwater contamination and freshwater ecosystem degradation. At the field level, NO3 leaching is highly variable due to interactions among soil, weather and crop management factors, but the relative effects of these drivers have not been quantified on a global scale. Using a global database of 82 field studies in temperate rainfed cereal crops with 961 observations, our objectives were to (a) quantify the relative importance of environmental and management variables to identify key leverage points for NO3 mitigation and (b) determine associated changes in crop productivity and potential tradeoffs for high and low NO3 loss scenarios. Machine learning algorithms (XGboost) and feature importance analysis showed that the amount and intensity of rainfall explained the most variability in NO3 leaching (up to 24 kg N ha-1), followed by nitrogen (N) fertilizer rate and crop N removal. In contrast, other soil and management variables such as soil texture, crop type, tillage and N source, timing and placement had less importance. To reduce N losses from global agriculture under changing weather and climatic conditions, these results highlight the need for better targeting and increased adoption of science-based, locally adapted management practices for improving N use efficiency. Future policy discussions should support this transition through different instruments while also promoting more advanced weather prediction analytics, especially in areas susceptible to extreme climatic variation.",
        "DOI": "10.1088/1748-9326/ac70ee",
        "paper_author": "Tamagno S.",
        "affiliation_name": "University of California, Davis",
        "affiliation_city": "Davis",
        "affiliation_country": "United States",
        "affiliation_id": "60014439",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "A Review of Image Processing Techniques for Deepfakes",
        "publication": "Sensors",
        "citied_by": "37",
        "cover_date": "2022-06-01",
        "Abstract": "Deep learning is used to address a wide range of challenging issues including large data analysis, image processing, object detection, and autonomous control. In the same way, deep learning techniques are also used to develop software and techniques that pose a danger to privacy, democracy, and national security. Fake content in the form of images and videos using digital manipulation with artificial intelligence (AI) approaches has become widespread during the past few years. Deepfakes, in the form of audio, images, and videos, have become a major concern during the past few years. Complemented by artificial intelligence, deepfakes swap the face of one person with the other and generate hyper-realistic videos. Accompanying the speed of social media, deepfakes can immediately reach millions of people and can be very dangerous to make fake news, hoaxes, and fraud. Besides the well-known movie stars, politicians have been victims of deepfakes in the past, especially US presidents Barak Obama and Donald Trump, however, the public at large can be the target of deepfakes. To overcome the challenge of deepfake identification and mitigate its impact, large efforts have been carried out to devise novel methods to detect face manipulation. This study also discusses how to counter the threats from deepfake technology and alleviate its impact. The outcomes recommend that despite a serious threat to society, business, and political institutions, they can be combated through appropriate policies, regulation, individual actions, training, and education. In addition, the evolution of technology is desired for deepfake identification, content authentication, and deepfake prevention. Different studies have performed deepfake detection using machine learning and deep learning techniques such as support vector machine, random forest, multilayer perceptron, k-nearest neighbors, convolutional neural networks with and without long short-term memory, and other similar models. This study aims to highlight the recent research in deepfake images and video detection, such as deepfake creation, various detection algorithms on self-made datasets, and existing benchmark datasets.",
        "DOI": "10.3390/s22124556",
        "paper_author": "Shahzad H.F.",
        "affiliation_name": "Khwaja Fareed University of Engineering &amp; Information Technology",
        "affiliation_city": "Rahim Yar Khan",
        "affiliation_country": "Pakistan",
        "affiliation_id": "60194949",
        "affiliation_state": "Punjab"
    },
    {
        "paper_title": "Informer: Irregular traffic detection for containerized microservices RPC in the real world",
        "publication": "High-Confidence Computing",
        "citied_by": "5",
        "cover_date": "2022-06-01",
        "Abstract": "Containerized microservices have been widely deployed in the industry. Meanwhile, security issues also arise. Many security enhancement mechanisms for containerized microservices require predefined rules and policies. However, it is challenging when it comes to thousands of microservices and a massive amount of real-time unstructured data. Hence, automatic policy generation becomes indispensable. In this paper, we focus on the automatic solution for the security problem: irregular traffic detection for RPCs. We propose Informer, a two-phase machine learning framework to track the traffic of each RPC and automatically report anomalous points. We first identify RPC chain patterns using density-based clustering techniques and build a graph for each critical pattern. Next, we solve the irregular RPC traffic detection problem as a prediction problem for attributed graphs with time series by leveraging spatial-temporal graph convolution networks. Since the framework builds multiple models and makes individual predictions for each RPC chain pattern, it can be efficiently updated upon legitimate changes in any graphs. In evaluations, we applied Informer to a dataset containing more than 7 billion lines of raw RPC logs sampled from a large Kubernetes system for two weeks. We provide two case studies of detected real-world threats. As a result, our framework found fine-grained RPC chain patterns and accurately captured the anomalies in a dynamic and complicated microservice production scenario, which demonstrates the effectiveness of Informer. Furthermore, we extensively evaluated the risk of adversarial attacks for our prediction model under different reality constraints and showed that the model is robust to such attacks in most real-world scenarios.",
        "DOI": "10.1016/j.hcc.2022.100050",
        "paper_author": "Chen J.",
        "affiliation_name": "University of California, Davis",
        "affiliation_city": "Davis",
        "affiliation_country": "United States",
        "affiliation_id": "60014439",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Mapping Gully Erosion Variability and Susceptibility Using Remote Sensing, Multivariate Statistical Analysis, and Machine Learning in South Mato Grosso, Brazil",
        "publication": "Geosciences (Switzerland)",
        "citied_by": "21",
        "cover_date": "2022-06-01",
        "Abstract": "In Brazil, the development of gullies constitutes widespread land degradation, especially in the state of South Mato Grosso, where fighting against this degradation has become a priority for policy makers. However, the environmental and anthropogenic factors that promote gully development are multiple, interact, and present a complexity that can vary by locality, making their prediction difficult. In this framework, a database was constructed for the Rio Ivinhema basin in the southern part of the state, including 400 georeferenced gullies and 13 geo-environmental descriptors. Multivariate statistical analysis was performed using principal component analysis (PCA) to identify the processes controlling the variability in gully development. Susceptibility maps were created through four machine learning models: multivariate discriminant analysis (MDA), logistic regression (LR), classification and regression tree (CART), and random forest (RF). The predictive performance of the models was analyzed by five evaluation indices: accuracy (ACC), sensitivity (SST), specificity (SPF), precision (PRC), and Receiver Operating Characteristic curve (ROC curve). The results show the existence of two major processes controlling gully erosion. The first is the surface runoff process, which is related to conditions of slightly higher relief and higher rainfall. The second also reflects high surface runoff conditions, but rather related to high drainage density and downslope, close to the river network. Human activity represented by peri-urban areas, construction of small earthen dams, and extensive rotational farming contribute significantly to gully formation. The four machine learning models yielded fairly similar results and validated susceptibility maps (ROC curve > 0.8). However, we noted a better performance of the random forest (RF) model (86% and 89.8% for training and test, respectively, with an ROC curve value of 0.931). The evaluation of the contribution of the parameters shows that susceptibility to gully erosion is not governed primarily by a single factor, but rather by the interconnection between different factors, mainly elevation, geology, precipitation, and land use.",
        "DOI": "10.3390/geosciences12060235",
        "paper_author": "Bouramtane T.",
        "affiliation_name": "Faculté des Sciences Rabat",
        "affiliation_city": "Rabat",
        "affiliation_country": "Morocco",
        "affiliation_id": "60000784",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "GIS and Machine Learning for Analysing Influencing Factors of Bushfires Using 40-Year Spatio-Temporal Bushfire Data",
        "publication": "ISPRS International Journal of Geo-Information",
        "citied_by": "10",
        "cover_date": "2022-06-01",
        "Abstract": "The causes of bushfires are extremely complex, and their scale of burning and probability of occurrence are influenced by the interaction of a variety of factors such as meteorological factors, topography, human activity and vegetation type. An in-depth understanding of the combined mechanisms of factors affecting the occurrence and spread of bushfires is needed to support the development of effective fire prevention plans and fire suppression measures and aid planning for geographic, ecological maintenance and urban emergency management. This study aimed to explore how bushfires, meteorological variability and other natural factors have interacted over the past 40 years in NSW Australia and how these influencing factors synergistically drive bushfires. The CSIRO’s Spark toolkit has been used to simulate bushfire burning spread over 24 h. The study uses NSW wildfire data from 1981–2020, combined with meteorological factors (temperature, precipitation, wind speed), vegetation data (NDVI data, vegetation type) and topography (slope, soil moisture) data to analyse the relationship between bushfires and influencing factors quantitatively. Machine learning-random forest regression was then used to determine the differences in the influence of bushfire factors on the incidence and burn scale of bushfires. Finally, the data on each influence factor was imported into Spark, and the results of the random forest model were used to set different influence weights in Spark to visualise the spread of bushfires burning over 24 h in four hotspot regions of bushfire in NSW. Wind speed, air temperature and soil moisture were found to have the most significant influence on the spread of bushfires, with the combined contribution of these three factors exceeding 60%, determining the spread of bushfires and the scale of burning. Precipitation and vegetation showed a greater influence on the annual frequency of bushfires. In addition, burn simulations show that wind direction influences the main direction of fire spread, whereas the shape of the flame front is mainly due to the influence of land classification. Besides, the simulation results from Spark could predict the temporal and spatial spread of fire, which is a potential decision aid for fireproofing agencies. The results of this study can inform how fire agencies can better understand fire occurrence mechanisms and use bushfire prediction and simulation techniques to support both their operational (short-term) and strategic (long-term) fire management responses and policies.",
        "DOI": "10.3390/ijgi11060336",
        "paper_author": "He W.",
        "affiliation_name": "UNSW Sydney",
        "affiliation_city": "Sydney",
        "affiliation_country": "Australia",
        "affiliation_id": "60028333",
        "affiliation_state": "NSW"
    },
    {
        "paper_title": "Towards Secure and Intelligent Internet of Health Things: A Survey of Enabling Technologies and Applications",
        "publication": "Electronics (Switzerland)",
        "citied_by": "42",
        "cover_date": "2022-06-01",
        "Abstract": "With the growth of computing and communication technologies, the information processing paradigm of the healthcare environment is evolving. The patient information is stored electronically, making it convenient to store and retrieve patient information remotely when needed. However, evolving the healthcare systems into smart healthcare environments comes with challenges and additional pressures. Internet of Things (IoT) connects things, such as computing devices, through wired or wireless mediums to form a network. There are numerous security vulnerabilities and risks in the existing IoT-based systems due to the lack of intrinsic security technologies. For example, patient medical data, data privacy, data sharing, and convenience are considered imperative for collecting and storing electronic health records (EHR). However, the traditional IoT-based EHR systems cannot deal with these paradigms because of inconsistent security policies and data access structures. Blockchain (BC) technology is a decentralized and distributed ledger that comes in handy in storing patient data and encountering data integrity and confidentiality challenges. Therefore, it is a viable solution for addressing existing IoT data security and privacy challenges. BC paves a tremendous path to revolutionize traditional IoT systems by enhancing data security, privacy, and transparency. The scientific community has shown a variety of healthcare applications based on artificial intelligence (AI) that improve health diagnosis and monitoring practices. Moreover, technology companies and startups are revolutionizing healthcare with AI and related technologies. This study illustrates the implication of integrated technologies based on BC, IoT, and AI to meet growing healthcare challenges. This research study examines the integration of BC technology with IoT and analyzes the advancements of these innovative paradigms in the healthcare sector. In addition, our research study presents a detailed survey on enabling technologies for the futuristic, intelligent, and secure internet of health things (IoHT). Furthermore, this study comprehensively studies the peculiarities of the IoHT environment and the security, performance, and progression of the enabling technologies. First, the research gaps are identified by mapping security and performance benefits inferred by the BC technologies. Secondly, practical issues related to the integration process of BC and IoT devices are discussed. Third, the healthcare applications integrating IoT, BC, and ML in healthcare environments are discussed. Finally, the research gaps, future directions, and limitations of the enabling technologies are discussed.",
        "DOI": "10.3390/electronics11121893",
        "paper_author": "Zaman U.",
        "affiliation_name": "SCIREP Institute of Scientific Research and Entrepreneurship",
        "affiliation_city": "Islamabad",
        "affiliation_country": "Pakistan",
        "affiliation_id": "128224511",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Short quantum circuits in reinforcement learning policies for the vehicle routing problem",
        "publication": "Physical Review A",
        "citied_by": "8",
        "cover_date": "2022-06-01",
        "Abstract": "Quantum computing and machine learning have potential for symbiosis. However, in addition to the hardware limitations from current devices, there are still basic issues that must be addressed before quantum circuits can usefully incorporate with current machine learning tasks. We report a strategy for such an integration in the context of attention models used for reinforcement learning. Agents that implement attention mechanisms have successfully been applied to certain cases of combinatorial routing problems by first encoding nodes on a graph and then sequentially decoding nodes until a route is selected. We demonstrate that simple quantum circuits can be used in place of classical attention head layers while maintaining performance. Our method modifies attention mechanisms by replacing key and query vectors for every node with quantum states that are entangled before being measured. The resulting hybrid classical-quantum agent is tested in the context of vehicle routing problems where its performance is competitive with the original classical approach. We regard our model as a prototype that can be scaled up and as an avenue for further study on the role of quantum computing in reinforcement learning.",
        "DOI": "10.1103/PhysRevA.105.062403",
        "paper_author": "Sanches F.",
        "affiliation_name": "QC Ware Corporation",
        "affiliation_city": "Palo Alto",
        "affiliation_country": "United States",
        "affiliation_id": "121802977",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Aligning artificial intelligence with climate change mitigation",
        "publication": "Nature Climate Change",
        "citied_by": "168",
        "cover_date": "2022-06-01",
        "Abstract": "There is great interest in how the growth of artificial intelligence and machine learning may affect global GHG emissions. However, such emissions impacts remain uncertain, owing in part to the diverse mechanisms through which they occur, posing difficulties for measurement and forecasting. Here we introduce a systematic framework for describing the effects of machine learning (ML) on GHG emissions, encompassing three categories: computing-related impacts, immediate impacts of applying ML and system-level impacts. Using this framework, we identify priorities for impact assessment and scenario analysis, and suggest policy levers for better understanding and shaping the effects of ML on climate change mitigation.",
        "DOI": "10.1038/s41558-022-01377-7",
        "paper_author": "Kaack L.H.",
        "affiliation_name": "Hertie School",
        "affiliation_city": "Berlin",
        "affiliation_country": "Germany",
        "affiliation_id": "60102536",
        "affiliation_state": "Berlin"
    },
    {
        "paper_title": "New Wave of COVID-19 Vaccine Opinions in the Month the 3rd Booster Dose Arrived",
        "publication": "Vaccines",
        "citied_by": "7",
        "cover_date": "2022-06-01",
        "Abstract": "Vaccination has been proposed as one of the most effective methods to combat the COVID-19 pandemic. Since the day the first vaccine, with an efficiency of more than 90%, was announced, the entire vaccination process and its possible consequences in large populations have generated a series of discussions on social media. Whereas the opinions triggered by the administration of the initial COVID-19 vaccine doses have been discussed in depth in the scientific literature, the approval of the so-called 3rd booster dose has only been analyzed in country-specific studies, primarily using questionnaires. In this context, the present paper conducts a stance analysis using a transformer-based deep learning model on a dataset containing 3,841,594 tweets in English collected between 12 July 2021 and 11 August 2021 (the month in which the 3rd dose arrived) and compares the opinions (in favor, neutral and against) with the ones extracted at the beginning of the vaccination process. In terms of COVID-19 vaccination hesitance, an analysis based on hashtags, n-grams and latent Dirichlet allocation is performed that highlights the main reasons behind the reluctance to vaccinate. The proposed approach can be useful in the context of the campaigns related to COVID-19 vaccination as it provides insights related to the public opinion and can be useful in creating communication messages to support the vaccination campaign.",
        "DOI": "10.3390/vaccines10060881",
        "paper_author": "Delcea C.",
        "affiliation_name": "Bucharest University of Economic Studies",
        "affiliation_city": "Bucharest",
        "affiliation_country": "Romania",
        "affiliation_id": "60107810",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Representation learning in the artificial and biological neural networks underlying sensorimotor integration",
        "publication": "Science Advances",
        "citied_by": "10",
        "cover_date": "2022-06-01",
        "Abstract": "The integration of deep learning and theories of reinforcement learning (RL) is a promising avenue to explore novel hypotheses on reward-based learning and decision-making in humans and other animals. Here, we trained deep RL agents and mice in the same sensorimotor task with high-dimensional state and action space and studied representation learning in their respective neural networks. Evaluation of thousands of neural network models with extensive hyperparameter search revealed that learning-dependent enrichment of state-value and policy representations of the task-performance-optimized deep RL agent closely resembled neural activity of the posterior parietal cortex (PPC). These representations were critical for the task performance in both systems. PPC neurons also exhibited representations of the internally defined subgoal, a feature of deep RL algorithms postulated to improve sample efficiency. Such striking resemblance between the artificial and biological networks and their functional convergence in sensorimotor integration offers new opportunities to better understand respective intelligent systems.",
        "DOI": "10.1126/sciadv.abn0984",
        "paper_author": "Suhaimi A.",
        "affiliation_name": "Nanyang Technological University",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore",
        "affiliation_id": "60005510",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Trends and Insights from Transportation Congestion Pricing Policy Research: A Bibliometric Analysis",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "1",
        "cover_date": "2022-06-01",
        "Abstract": "Toll-based congestion pricing (CP) policies are increasingly implemented globally for alleviating road traffic congestion. Several interconnected factors affecting or induced by CP imple-mentation include air quality/emissions, travel time, and road user safety. We sought to examine and characterize research output and patterns across several domains (e.g., health, policy accept-ability) surrounding toll-based CP policies, in order to identify where research has focused and where gaps exist. We conducted a structured review and identified 2333 relevant publications, using semi-supervised and machine learning strategies combined with manual review. Annual publication counts peaked in 2015 (n = 122). Themes identified from title and abstract terms included policy im-plementation characteristics, advanced transportation modeling methods and approaches, and public perception and acceptability. Authorship networks indicated a lack of interdisciplinary research. Country analyses identified the US, China, and the UK as the most frequently represented countries, and underrepresentation from low-income countries. Findings indicate that research focused on specific road user types (e.g., pedestrians) and safety impacts, and equity considerations were relatively sparse compared to other topics (e.g., policy economics, public perception). Additional research on these critical topics is necessary to ensure that such policies are designed to promote positive and equitable effects on road user health and safety.",
        "DOI": "10.3390/ijerph19127189",
        "paper_author": "Singichetti B.",
        "affiliation_name": "The University of North Carolina at Chapel Hill",
        "affiliation_city": "Chapel Hill",
        "affiliation_country": "United States",
        "affiliation_id": "60025111",
        "affiliation_state": "NC"
    },
    {
        "paper_title": "Prediction Method of Beijing Electric-Energy Substitution Potential Based on a Grid-Search Support Vector Machine",
        "publication": "Energies",
        "citied_by": "7",
        "cover_date": "2022-06-01",
        "Abstract": "Recently, “power cuts” and “coal price surges” have been significant concerns of individuals and societies. The main reasons for a power cut are a recent rapid increase in power consumption, shortage of thermal coal or the large shutdown capacity of thermal power units, resulting in a tight power supply in the power grid. In recent years, the shortage of fossil resources has led to frequent energy crises. In the context of carbon peaks and carbon neutralization, how to better develop electric-energy substitution and eliminate the dependence on fossil energy has become a problem that needs to be solved at present. In this paper, the influencing factors of electric-energy substitution in Beijing are analyzed, and the indexes affecting the electric-energy substitution are outlined. By constructing various machine-learning models, the prediction is performed. The results show that the Gaussian kernel support vector machine model based on a grid search has a good prediction effect on the electric-energy substitution potential in Beijing, which has certain guiding significance for electric-energy substitution potential analysis.",
        "DOI": "10.3390/en15113897",
        "paper_author": "Chi Y.",
        "affiliation_name": "Beijing University of Technology",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60022281",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Digital Marketing: A Unique Multidisciplinary Approach towards the Elimination of Viral Hepatitis",
        "publication": "Pathogens",
        "citied_by": "12",
        "cover_date": "2022-06-01",
        "Abstract": "New technologies are supported by the global implementation of the internet. These improvements have deeply affected various disciplines of sciences and consequently changed services such as daily business, particularly health sectors. Innovative digital marketing strategies utilize the channels of social media and retrieved user data to analyze and improve relevant services. These multidisciplinary innovations can assist specialists, physicians and researchers in diagnostic, prophylaxis and treatment issues in the health sector. Accordingly, compared to recent decades, health decision makers are more accurate and trustful in defining new strategies. Interestingly, using social media and mobile health apps in current pandemics of SARS-CoV-2 could be an important instance of the key role of these platforms at the local and global level of health policies. These digital technologies provide platforms to connect public health sectors and health politicians for communicating and spreading relevant information. Adding influencers and campaigns to this toolbox strengthens the implementation of public health programs. In 2016, the WHO adopted a global program to eliminate viral hepatitis by 2030. Recent constructive measures that have been used in the battle against COVID-19 could be adopted for the elimination of viral hepatitis program. The presented evidence in our narrative review demonstrates that the application of digital marketing tools to create campaigns on social media, armed with professional influencers, can efficiently consolidate this program. The application of different strategies in using these popular tools will raise the public awareness about viral hepatitis. Subsequently, the availability of an effective vaccine for HBV and antiviral medication for HCV can motivate the audience to take steps towards prophylaxis and screening methods against these infectious illnesses. The encouragement of health policy makers to apply digital communication technologies and comprehensive roadmaps to implement this global program will certainly decrease the burden of viral hepatitis worldwide.",
        "DOI": "10.3390/pathogens11060626",
        "paper_author": "Pourkarim M.",
        "affiliation_name": "Islamic Azad University, Yazd Branch",
        "affiliation_city": "Yazd",
        "affiliation_country": "Iran",
        "affiliation_id": "60003957",
        "affiliation_state": "Yazd Province"
    },
    {
        "paper_title": "Landslide Susceptibility Mapping Using Machine Learning: A Danish Case Study",
        "publication": "ISPRS International Journal of Geo-Information",
        "citied_by": "16",
        "cover_date": "2022-06-01",
        "Abstract": "Mapping of landslides, conducted in 2021 by the Geological Survey of Denmark and Greenland (GEUS), revealed 3202 landslides in Denmark, indicating that they might pose a bigger problem than previously acknowledged. Moreover, the changing climate is assumed to have an impact on landslide occurrences in the future. The aim of this study is to conduct the first landslide susceptibility mapping (LSM) in Denmark, reducing the geographical bias existing in LSM studies, and to identify areas prone to landslides in the future following representative concentration pathway RCP8.5, based on a set of explanatory variables in an area of interest located around Vejle Fjord, Jutland, Denmark. A subset from the landslide inventory provided by GEUS is used as ground truth data. Three well-established machine learning (ML) algorithms—Random Forest, Support Vector Machine, and Logistic Regression—were trained to classify the data samples as landslide or non-landslide, treating the ML task as a binary classification and expressing the results in the form of a probability in order to produce susceptibility maps. The classification results were validated through the test data and through an external data set for an area located outside of the region of interest. While the high predictive performance varied slightly among the three models on the test data, the LR and SVM demonstrated inferior accuracy outside of the study area. The results show that the RF model has robustness and potential for applicability in landslide susceptibility mapping in low-lying landscapes of Denmark in the present. The conducted mapping can become a step forward towards planning for mitigative and protective measures in landslide-prone areas in Denmark, providing policy-makers with necessary decision support. However, the map of the future climate change scenario shows the reduction of the susceptible areas, raising the question of the choice of the climate models and variables in the analysis.",
        "DOI": "10.3390/ijgi11060324",
        "paper_author": "Ageenko A.",
        "affiliation_name": "Aalborg University",
        "affiliation_city": "Aalborg",
        "affiliation_country": "Denmark",
        "affiliation_id": "60022134",
        "affiliation_state": "Nordjylland"
    },
    {
        "paper_title": "Long-Term Landsat-Based Monthly Burned Area Dataset for the Brazilian Biomes Using Deep Learning",
        "publication": "Remote Sensing",
        "citied_by": "57",
        "cover_date": "2022-06-01",
        "Abstract": "Fire is a significant agent of landscape transformation on Earth, and a dynamic and ephemeral process that is challenging to map. Difficulties include the seasonality of native vegetation in areas affected by fire, the high levels of spectral heterogeneity due to the spatial and temporal variability of the burned areas, distinct persistence of the fire signal, increase in cloud and smoke cover surrounding burned areas, and difficulty in detecting understory fire signals. To produce a large-scale time-series of burned area, a robust number of observations and a more efficient sampling strategy is needed. In order to overcome these challenges, we used a novel strategy based on a machine-learning algorithm to map monthly burned areas from 1985 to 2020 using Landsat-based annual quality mosaics retrieved from minimum NBR values. The annual mosaics integrated year-round observations of burned and unburned spectral data (i.e., RED, NIR, SWIR-1, and SWIR-2), and used them to train a Deep Neural Network model, which resulted in annual maps of areas burned by land use type for all six Brazilian biomes. The annual dataset was used to retrieve the frequency of the burned area, while the date on which the minimum NBR was captured in a year, was used to reconstruct 36 years of monthly burned area. Results of this effort indicated that 19.6% (1.6 million km2 ) of the Brazilian territory was burned from 1985 to 2020, with 61% of this area burned at least once. Most of the burning (83%) occurred between July and October. The Amazon and Cerrado, together, accounted for 85% of the area burned at least once in Brazil. Native vegetation was the land cover most affected by fire, representing 65% of the burned area, while the remaining 35% burned in areas dominated by anthropogenic land uses, mainly pasture. This novel dataset is crucial for understanding the spatial and long-term temporal dynamics of fire regimes that are fundamental for designing appropriate public policies for reducing and controlling fires in Brazil.",
        "DOI": "10.3390/rs14112510",
        "paper_author": "Alencar A.A.C.",
        "affiliation_name": "Instituto de Pesquisa Ambiental da Amazônia",
        "affiliation_city": "Belem",
        "affiliation_country": "Brazil",
        "affiliation_id": "60086761",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "Multinomial modeling methods: Predicting four decades of international banking crises",
        "publication": "Economic Systems",
        "citied_by": "4",
        "cover_date": "2022-06-01",
        "Abstract": "This paper examines banking crises in a large sample of countries over a forty-year period. A multinomial modeling approach is applied to panel data in order to track and capture end-to-end cyclical crisis formations, which enhances the binary focus of previous research studies. Several macroeconomic and banking sector variables are shown to be emblematic of leading indicators across the idiosyncratic stages of a banking crisis. Gross domestic product is an early warning signal across all phases, and a concomitant deterioration in consumption spending and fixed capital formation, preceded by a credit boom, signal a banking crisis to come. Currency depreciation exemplifies ensuing financial distress, reinforced by developmental constructs and regional integration. Lower real interest rates, increasing imports, and rising deposits are frequently harbingers of a recovery. Period effects underscore the dynamic evolution of common contemporaneous precursors over time. Premised on pursuing cyclical movements through multiple outcomes, our findings on forecasting performance suggest enhanced predictive power. Several multinomial logistic models generate higher predictive accuracy in contrast to probit models. Compared to machine learning methods (which encompass artificial neural networks, gradient boost, k-nearest neighbors, and random forests methods), a multinomial logistic approach outperforms during pre-crisis periods and when crisis severity is modeled, whereas gradient boost has the highest predictive accuracy across numerous versions of the multinomial model. As investors and policy makers continue to confront banking crises, leading to high economic and social costs, enhanced multinomial modeling methods make a valuable contribution to improved forecasting performance.",
        "DOI": "10.1016/j.ecosys.2022.100979",
        "paper_author": "du Plessis E.",
        "affiliation_name": "Universität Hamburg",
        "affiliation_city": "Hamburg",
        "affiliation_country": "Germany",
        "affiliation_id": "60028229",
        "affiliation_state": "Hamburg"
    },
    {
        "paper_title": "Deep Learning Based Muti-Objective Reactive Power Optimization of Distribution Network with PV and EVs",
        "publication": "Sensors",
        "citied_by": "9",
        "cover_date": "2022-06-01",
        "Abstract": "With the high penetration of photovoltaic (PV) and electric vehicle (EV) charging and replacement power stations connected to the distribution network, problems such as the increase of line loss and voltage deviation of the distribution network are becoming increasingly prominent. The application of traditional reactive power compensation devices and the change of transformer taps has struggled to meet the needs of reactive power optimization of the distribution network. It is urgent to present new reactive power regulation methods which have a vital impact on the safe operation and cost control of the power grid. Hence, the idea that applying the reactive power regulation potential of PV and EV is proposed to reduce the pressure of reactive power optimization in the distribution network. This paper establishes the reactive power regulation models of PV and EV, and their own dynamic evaluation methods of reactive power adjustable capacity are put forward. The model proposed above is optimized via five different algorithms and approximated through the deep learning when the optimization objective is only set as line loss and voltage deviation. Simulation results show that the prediction of deep learning has an incredible ability to fit the Pareto front that the intelligent algorithms obtain in practical application.",
        "DOI": "10.3390/s22124321",
        "paper_author": "Wu R.",
        "affiliation_name": "Shandong University",
        "affiliation_city": "Jinan",
        "affiliation_country": "China",
        "affiliation_id": "60031031",
        "affiliation_state": "Shandong"
    },
    {
        "paper_title": "Detecting Anomalies in National Bridge Inventory Databases Using Machine Learning Methods",
        "publication": "Transportation Research Record",
        "citied_by": "4",
        "cover_date": "2022-06-01",
        "Abstract": "National Bridge Inventory (NBI) data is regularly collected for 617,000 + national bridges in the U.S. These data, which consist of 100 + fields related to bridges and culverts, have been shown to contain errors. These errors could reduce the effectiveness of the decisions made based on this data, and cause safety issues. For this reason, an anomaly detection platform is developed to identify data anomalies in NBI datasets more effectively than existing rule-based error-check tools can. First, the user provides groups of correlated NBI fields as input to the platform. Then, for each group, it utilizes two tools to detect anomalous data and determine errors. The first tool uses three machine learning algorithms to identify anomalous data points and categorizes them based on their degree of anomaly. The second tool visualizes the distributions of the NBI fields in the group with histograms, scatter plots, and so forth. These plots are used to analyze the data points that are identified from the first tool as anomalies. The results of these two tools, together with expert knowledge about the data fields, are then used to distinguish data errors from outliers. The proposed platform is applied to a state’s NBI dataset that was submitted to the Federal Highway Administration (FHWA) in 2020. For this dataset, two groups of correlated fields are considered. The results showed the platform could effectively pinpoint anomalous values of NBI fields that individually, or in conjunction with other fields, do not follow the patterns that characterize most of the data, prompting the identification of potential inconsistencies and errors.",
        "DOI": "10.1177/03611981221075028",
        "paper_author": "Fereshtehnejad E.",
        "affiliation_name": "LLC.",
        "affiliation_city": "North Brunswick",
        "affiliation_country": "United States",
        "affiliation_id": "128431427",
        "affiliation_state": "NJ"
    },
    {
        "paper_title": "Exploring the Effects of Transportation Supply on Mixed Land-Use at the Parcel Level",
        "publication": "Land",
        "citied_by": "11",
        "cover_date": "2022-06-01",
        "Abstract": "The interactive relationship between transportation and land use has become more difficult to understand and predict, due to the economic boom and corresponding fast-paced proliferation of private transportation and land-development activities. A lack of coordination between transportation and land-use planning has created an imbalanced provision of transportation infrastructure and land-use patterns; this is indicated by places where a high-density land-development pattern is supported by a low-capacity transport system or vice versa. With this, literature suggests that Mixed Land-Use (MLU) developments have the potential to provide relevant solutions for urban sustainability, smart growth, inclusive public transit use, and efficient land-use. Therefore, this study employed deep neural network models—Long Short-Term Memory (LSTM), and Multilayer Perceptron (MLP)—for forecasting the effect of transportation supply on the MLU pattern at the parcel level in the Jiang’an District, Wuhan, China. The findings revealed a strong relationship between the supply of public transportation and MLU. Moreover, the study results indicated that MLU is widely available in areas with high accessibility, high density, and proximity to the city center. The forecasting results from the MLP and LSTM models showed an average error of 5.55– 7.36% and 3.62–4.28% for mixed use, respectively, while most of their 90th percentile errors were less than 13.73% and 10.46% for mixed use, respectively. The proposed models and the findings from this study should be useful for stakeholders and policy makers for more precise forecasting of MLU at the urban level.",
        "DOI": "10.3390/land11060797",
        "paper_author": "Almansoub Y.",
        "affiliation_name": "Wuhan University of Technology",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60022414",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "A new active learning approach for adsorbate–substrate structural elucidation in silico",
        "publication": "Journal of Molecular Modeling",
        "citied_by": "8",
        "cover_date": "2022-06-01",
        "Abstract": "Adsorbate interactions with substrates (e.g. surfaces and nanoparticles) are fundamental for several technologies, such as functional materials, supramolecular chemistry, and solvent interactions. However, modeling these kinds of systems in silico, such as finding the optimum adsorption geometry and energy, is challenging, due to the huge number of possibilities of assembling the adsorbate on the surface. In the current work, we have developed an artificial intelligence (AI) approach based on an active learning (AL) method for adsorption optimization on the surface of materials. AL uses machine learning (ML) regression algorithms and their uncertainties to make a decision (based on a policy) for the next unexplored structures to be computed, increasing, though, the probability of finding the global minimum with a small number of calculations. The methodology allows an accurate and automated structural elucidation of the adsorbate on the surface, based on the minimization of the total electronic energy. The new AL method for adsorption optimization was developed and implemented in the quantum machine learning software/agent for material design and discovery (QMLMaterial) program and was applied for C60@TiO2 anatase (101). It marks another software extension with a new feature in addition to the automatic structural elucidation of defects in materials and of nanoparticles as well. SCC-DFTB calculations were used to build the complex search surfaces with a reasonably low computational cost. An artificial neural network (NN) was employed in the AL framework evaluated together with two uncertainty quantification methods: K-fold cross-validation and non-parametric bootstrap (BS) resampling. Also, two different acquisition functions for decision-making were used: expected improvement (EI) and the lower confidence bound (LCB). Graphical abstract: [Figure not available: see fulltext.]",
        "DOI": "10.1007/s00894-022-05173-0",
        "paper_author": "Lourenço M.P.",
        "affiliation_name": "Universidade Federal do Espírito Santo",
        "affiliation_city": "Vitoria",
        "affiliation_country": "Brazil",
        "affiliation_id": "60028426",
        "affiliation_state": "ES"
    },
    {
        "paper_title": "Reinforcement learning coupled with finite element modeling for facial motion learning",
        "publication": "Computer Methods and Programs in Biomedicine",
        "citied_by": "8",
        "cover_date": "2022-06-01",
        "Abstract": "Background and Objective: Facial palsy patients or patients with facial transplantation have abnormal facial motion due to altered facial muscle functions and nerve damage. Computer-aided system and physics-based models have been developed to provide objective and quantitative information. However, the predictive capacity of these solutions is still limited to explore the facial motion patterns with emerging properties. The present study aims to couple the reinforcement learning and the finite element modeling for facial motion learning and prediction. Methods: A novel modeling workflow for learning facial motion was developed. A physically-based model of the face within the Artisynth modeling platform was used. Information exchange protocol was proposed to link reinforcement learning and rigid multi-bodies dynamics outcomes. Two reinforcement learning algorithms (deep deterministic policy gradient (DDPG) and Twin-delayed DDPG (TD3)) were used and implemented to drive the simulations of symmetry-oriented and smile movements. Numerical outcomes were compared to experimental observations (Bosphorus database) for evaluation and validation purposes. Results: As result, after more than 100 episodes of exploring the environment, the agent starts to learn from previous trials and can find the optimal policy after more than 300 episodes of training. Regarding the symmetry-oriented motion, the muscle excitations predicted by the trained agent help to increase the value of reward from R = -2.06 to R = -0.23, which counts for ∼89% improvement of the symmetry value of the face. For smile-oriented motion, two points at the edge of the mouth move up 0.35 cm, which is within the range of movements estimated from the Bosphorus database (0.4 ± 0.32 cm). Conclusions: The present study explored the muscle excitation patterns by coupling reinforcement learning with a detailed finite element model of the face. We developed, for the first time, a novel coupling scheme to integrate the finite element simulation into the reinforcement learning process for facial motion learning. As perspectives, this present workflow will be applied for facial palsy and facial transplantation patients to guide and optimize the functional rehabilitation program.",
        "DOI": "10.1016/j.cmpb.2022.106904",
        "paper_author": "Nguyen D.P.",
        "affiliation_name": "CNRS Centre National de la Recherche Scientifique",
        "affiliation_city": "Paris",
        "affiliation_country": "France",
        "affiliation_id": "60008134",
        "affiliation_state": "Ile-de-France"
    },
    {
        "paper_title": "A Novel Approach on Deep Learning—Based Decision Support System Applying Multiple Output LSTM-Autoencoder: Focusing on Identifying Variations by PHSMs’ Effect over COVID-19 Pandemic",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "3",
        "cover_date": "2022-06-01",
        "Abstract": "Following the outbreak of the COVID-19 pandemic, the continued emergence of major variant viruses has caused enormous damage worldwide by generating social and economic ripple effects, and the importance of PHSMs (Public Health and Social Measures) is being highlighted to cope with this severe situation. Accordingly, there has also been an increase in research related to a decision support system based on simulation approaches used as a basis for PHSMs. However, previous studies showed limitations impeding utilization as a decision support system for policy establishment and implementation, such as the failure to reflect changes in the effectiveness of PHSMs and the restriction to short-term forecasts. Therefore, this study proposes an LSTM-Autoencoder-based decision support system for establishing and implementing PHSMs. To overcome the limitations of existing studies, the proposed decision support system used a methodology for predicting the number of daily confirmed cases over multiple periods based on multiple output strategies and a methodology for rapidly identifying varies in policy effects based on anomaly detection. It was confirmed that the proposed decision support system demonstrated excellent performance compared to models used for time series analysis such as statistical models and deep learning models. In addition, we endeavored to increase the usability of the proposed decision support system by suggesting a transfer learning-based methodology that can efficiently reflect variations in policy effects. Finally, the decision support system proposed in this study provides a methodology that provides multi-period forecasts, identifying variations in policy effects, and efficiently reflects the effects of variation policies. It was intended to provide reasonable and realistic information for the establishment and implementation of PHSMs and, through this, to yield information expected to be highly useful, which had not been provided in the decision support systems presented in previous studies.",
        "DOI": "10.3390/ijerph19116763",
        "paper_author": "Jang Y.J.",
        "affiliation_name": "Yonsei University Mirae Campus",
        "affiliation_city": "Wonju",
        "affiliation_country": "South Korea",
        "affiliation_id": "60107290",
        "affiliation_state": "Gangwon-do"
    },
    {
        "paper_title": "Uncovering the shape of neighborhoods: Harnessing data analytics for a smart governance of urban areas",
        "publication": "Journal of Urban Management",
        "citied_by": "5",
        "cover_date": "2022-06-01",
        "Abstract": "Urban scholars have made great advances to understand the reciprocal relations between households and their immediate environments as a means for the creation of efficient urban administrative systems. However, from an urban management perspective, reliance on geographical areas fixed for long periods of time as basic data collection constitutes a problem. Modern urban areas are in a permanent state of flux because of changing preferences, willingness to pay, location choices, and physical development. In this constantly changing context, what is the most appropriate delimitation of a “neighborhood”, defined as a small and relatively homogeneous area in a certain (and temporary) urban configuration? This paper contributes to the growing literature on the use of data analytic tools in urban studies and neighborhood delimitation in housing sub-markets, exploiting big data on real-estate transactions in England and Wales during a long period of time. The results shed light on the importance of organic urban features and the drawbacks of rigid geometric definitions. They also highlight the importance of the usage of deep Machine Learning (ML) tools such as Artificial Neural Network (ANN), alongside with traditional methods. The paper's contribution to urban governance is the suggestion of a smart and dynamic system aimed at defining the most appropriate areas for urban management given a specific period and situation. The suggested framework can be implemented periodically, helping to define homogeneous spatial units (neighborhoods) with large variances among them, allowing for designing urban policies tailored to each one of them.",
        "DOI": "10.1016/j.jum.2022.05.005",
        "paper_author": "Sagi A.",
        "affiliation_name": "Technion - Israel Institute of Technology",
        "affiliation_city": "Haifa",
        "affiliation_country": "Israel",
        "affiliation_id": "60022403",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Modeling population density guided by land use-cover change model: a case study of Bogotá",
        "publication": "Population and Environment",
        "citied_by": "7",
        "cover_date": "2022-06-01",
        "Abstract": "Population densities provide valuable spatial information to identify populations at risk, quantify mobility, and improve our understanding of future urban settlements. Advancements in machine learning algorithms open up new horizons to face these challenges. This research proposes a supervised machine learning approach, Random Forest, for population density appraisal in a large and dense developing city. We studied Bogotá, where functional integration with neighboring municipalities exists, although they have different governments and uncoordinated urban development plans. As a starting point, we use simulated residential land-use patterns, classified according to socioeconomic levels, from a cellular automata-based model. We estimate population density with reliable land-use change models and nine simple representations of the urban structure, such as land values and the distance to urban amenities. Therefore, combining a cellular automata model with a classification model, considering both continuous and categorical variables, demonstrates this methodology’s potential and promises a reliable assessment of population density. Finally, we present a trip generation model integrated with densities and spatial location. A comprehensive results discussion suggests this study’s importance in urban planning and the accuracy of the proposed methodology to support decision-making processes and policy evaluation.",
        "DOI": "10.1007/s11111-022-00400-5",
        "paper_author": "Guzman L.A.",
        "affiliation_name": "Universidad de Los Andes, Colombia",
        "affiliation_city": "Bogota",
        "affiliation_country": "Colombia",
        "affiliation_id": "60052106",
        "affiliation_state": "Distrito Capital"
    },
    {
        "paper_title": "A Comparative Study of Data-driven Models for Groundwater Level Forecasting",
        "publication": "Water Resources Management",
        "citied_by": "14",
        "cover_date": "2022-06-01",
        "Abstract": "Irregular rainfall patterns and limited freshwater availability have driven humans to increase their dependence on groundwater resources. An essential aspect of effective water resources management is forecasting groundwater levels to ensure that sufficient quantities are available for future generations. Prediction models have been widely used to forecast groundwater levels at the regional scale. This study compares the accuracy of five commonly used data-driven models–Holt–Winters’ Exponential Smoothing, Seasonal Autoregressive Integrated Moving Average, Multi-Layer Perceptron, Extreme Learning Machine, and Neural Network Autoregression for simulating the declining groundwater levels of three monitoring wells in the National Capital Territory of Delhi in India. The performance of the selected models was compared using coefficient of determination (R2), Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE). Results indicate that Multi-Layer Perceptron had high R2 while fitting the training data and least RMSE and MAE during testing, thus proving to be more accurate in forecasting than the other models. Multi-Layer Perceptron was used to forecast the groundwater level in the study wells for 2025. The results showed that the groundwater level will decline further if the current situation continues. Such studies help determine the appropriate model to be used for regions with limited available data. Additionally, predictions made for the future will help policymakers understand which areas need immediate attention in terms of groundwater management.",
        "DOI": "10.1007/s11269-022-03173-6",
        "paper_author": "Sarma R.",
        "affiliation_name": "Delhi Technological University",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India",
        "affiliation_id": "60002874",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Assessment of fire resilience in subtropical wetlands using high spatial resolution images",
        "publication": "Environmental Monitoring and Assessment",
        "citied_by": "3",
        "cover_date": "2022-06-01",
        "Abstract": "Resilience is the ability of a system to absorb disturbances, rearrange itself, and adapt in order to maintain its functionality, structure, identity, and feedback. Research involving fire resilience in subtropical wetlands (SW) allows us to understand the dynamics of these ecosystems, measure impacts on fauna and flora, and promote policies for the management and protection. The aim of the present study is to assess the fire resilience of SW. The study was divided into three steps: (i) burned area classification, (ii) vegetation pattern classification, and (iii) temporal analysis of SW fire resilience based on NDVI calculation. Our results show that (a) high resilience potential of emerging plants, which developed green leaves in less than 90 days after the fire; (b) poor recovery of peatlands with underground fire history. Daily coverage of high spatial resolution PlanetScope images has great potential for classification and monitoring of land use in areas where there are rapid changes, such as after a fire event, explosions, and dam ruptures with ore tailings, for example.",
        "DOI": "10.1007/s10661-022-09985-8",
        "paper_author": "Simioni J.P.D.",
        "affiliation_name": "Universidade Federal do Rio Grande do Sul",
        "affiliation_city": "Porto Alegre",
        "affiliation_country": "Brazil",
        "affiliation_id": "60006726",
        "affiliation_state": "RS"
    },
    {
        "paper_title": "Forecasting in a complex environment: Machine learning sales expectations in a stock flow consistent agent-based simulation model",
        "publication": "Journal of Economic Dynamics and Control",
        "citied_by": "5",
        "cover_date": "2022-06-01",
        "Abstract": "The aim of this paper is to investigate how different degrees of sophistication in agents’ behavioral rules may affect individual and macroeconomic performances. In particular, we analyze the effects of introducing into an agent-based macro model firms that are able to formulate effective sales forecasts by using simple machine learning algorithms. These techniques are able to provide predictions that are unbiased and present a certain degree of accuracy, especially in the case of a genetic algorithm. We observe that machine learning allows firms to increase profits, though this result in a declining wage share and a smaller long-run growth rate. Moreover, the predictive methods are able to formulate expectations that remain unbiased when shocks are not massive, thus providing firms with forecasting capabilities that to a certain extent may be consistent with the Lucas Critique.",
        "DOI": "10.1016/j.jedc.2022.104405",
        "paper_author": "Catullo E.",
        "affiliation_name": "Università degli Studi di Teramo",
        "affiliation_city": "Teramo",
        "affiliation_country": "Italy",
        "affiliation_id": "60019089",
        "affiliation_state": "TE"
    },
    {
        "paper_title": "Worldwide city transport typology prediction with sentence-BERT based supervised learning via Wikipedia",
        "publication": "Transportation Research Part C: Emerging Technologies",
        "citied_by": "6",
        "cover_date": "2022-06-01",
        "Abstract": "An overwhelming majority of the world's human population lives in urban areas and cities. Understanding a city's transportation typology is immensely valuable for planners and policy makers whose decisions can potentially impact millions of city residents. Despite the value of understanding a city's typology, labeled data (city and its typology) is scarce, and spans at most a few hundred cities in the current transportation literature. To break this barrier, we propose a supervised machine learning approach to predict a city's typology given the information in its Wikipedia page. Our method leverages recent breakthroughs in natural language processing, namely sentence-BERT, and shows how the text-based information from Wikipedia can be effectively used as a data source for city typology prediction tasks that can be applied to over 2000 cities worldwide. We propose a novel method for low-dimensional city representation using a city's Wikipedia page, which makes supervised learning of city typology labels tractable even with a few hundred labeled samples. These features are used with labeled city samples to train binary classifiers (logistic regression) for four different city typologies: (i) congestion, (ii) auto-heavy, (iii) transit-heavy, and (iv) bike-friendly cities resulting in reasonably high AUC scores of 0.87, 0.86, 0.61 and 0.94 respectively. Our approach provides sufficient flexibility for incorporating additional variables in the city typology models and can be applied to study other city typologies as well. Our findings can assist a diverse group of stakeholders in transportation and urban planning fields, and opens up new opportunities for using text-based information from Wikipedia (or similar platforms) as data sources in such fields.",
        "DOI": "10.1016/j.trc.2022.103661",
        "paper_author": "Rath S.",
        "affiliation_name": "NYU Tandon School of Engineering",
        "affiliation_city": "New York",
        "affiliation_country": "United States",
        "affiliation_id": "60108318",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Detection and estimation of behind-the-meter photovoltaic generation based on smart meter data analytics",
        "publication": "Electricity Journal",
        "citied_by": "5",
        "cover_date": "2022-06-01",
        "Abstract": "With the economic development and the implementation of national policies, the penetration rate of distributed photovoltaic (PV) panels, especially those behind the meter, has been greatly increased at the customers' side, which has been changing the operational landscape of the distribution network. In that regard, a critical challenge is that the PV generation behind the meter cannot be detected and measured directly by the utility company. There are even cases where customers install PV without authorization. To ensure the secure and reliable operation, the utility company has to perceive the total PV generation from smart meter data while making operational decisions to strike a balance of power supply and demand. This paper presents a novel two-stage PV detection and capacity estimation method purely based on smart meter data, which is essentially a data-driven model based on machine learning. First, given the net load curves of customers, the PV capacity features are extracted and the support vector machine (SVM) model is used to classify the customers with or without realistic PV installations. Second, based on the spatial correlation of original power demand, the total PV capacity curve is estimated by using the difference between daytime and nighttime data based on the aggregated load information. Finally, the capacity curve, along with the net load curve, is fed into Long-Short Term Memory (LSTM) model as input to estimate the real-time PV generation of individual customers.",
        "DOI": "10.1016/j.tej.2022.107132",
        "paper_author": "Wang J.",
        "affiliation_name": "South China University of Technology",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60024542",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Performance evaluation of China's photovoltaic poverty alleviation project using machine learning and satellite images",
        "publication": "Utilities Policy",
        "citied_by": "8",
        "cover_date": "2022-06-01",
        "Abstract": "Photovoltaic poverty alleviation project (PPAP) is one of China's essential targeted poverty alleviation projects. This study proposes a machine learning model and uses satellite images to evaluate the performance of PPAP in China. The trained deep convolutional neural network (DCNN) with transfer learning was first used to identify the scale of photovoltaic (PV) power stations. Then the PV power capacity for poverty alleviation and carbon emission mitigation were estimated. The results identified 38 large-scale centralized and approximately 5,063,293 m2PV power stations built in Jinzhai County, Anhui province, China, by November 2020. The main findings are as follows. (1) The power generation and carbon mitigation of PPAP in Jinzhai County is about 1.8×103 MWh and 1.389 Mt per year. (2) The PPAP in Jinzhai County can recover the total costs and get benefits within three years, at least after completion. (3) Dynamic subsidy policy is needed to prevent over-scale or excessive government investment in PPAP. (4) The utilization of PPAP needs to be strengthened to transform the current “blood transfusion type” of poverty alleviation that relies more on government subsidies into more sustainable “hematopoietic style” poverty alleviation. This study is of significance to more accurately and comprehensively evaluate the performance of PPAP and give better utilize the role of renewable energy in promoting energy conservation, carbon emission reduction, and economic development.",
        "DOI": "10.1016/j.jup.2022.101378",
        "paper_author": "Yin H.",
        "affiliation_name": "Hefei University of Technology",
        "affiliation_city": "Hefei",
        "affiliation_country": "China",
        "affiliation_id": "60002836",
        "affiliation_state": "Anhui"
    },
    {
        "paper_title": "Urban neighborhood socioeconomic status (SES) inference: A machine learning approach based on semantic and sentimental analysis of online housing advertisements",
        "publication": "Habitat International",
        "citied_by": "7",
        "cover_date": "2022-06-01",
        "Abstract": "Understanding the dynamic distribution of residents' socioeconomic status (SES) across neighborhoods within cities is essential for urban planning and policy-making aligning to the Sustainable Development Goals 2030. Whereas the promise in explicitly linking geographical features to SES has been highlighted fairly clear in previous works, scholars hold an eclectic attitude in their outlook, given the absence of theoretical ground, the heavy reliance on nontransparent proprietary data sources and the relatively coarse resolution predictions. Drawing on a case study of Hangzhou metropolitan in China, this paper aims to address these problems by demonstrating a novel approach to neighborhood SES inference based on online housing advertisements. We first revisit the theoretical debates on the linkage between neighborhood SES and online housing advertisements. Then, the Naïve Bayes classifier is employed to semantically identify the topics from online housing advertisements and the associated sentiments are quantified using the lexicon-based approach. Following that, seven commonly used machine learning algorithms are compared and utilized to infer the fine-grained neighborhood SES at residential quarters scale based on the housing attributes and extracted topics from online housing advertisements. Results show that machine learning algorithms vary with predictive ability and the tree-based algorithms are much more powerful in inferring neighborhood SES. More specifically, we distinguish 8 reliable features which not only present relative high importance estimated by all the machine learning algorithms but also exhibit great robustness in inferring neighborhood SES and show promising potential to being applied for unraveling social inequalities. We also observe noteworthy spatial heterogeneity in neighborhood SES across the research site. The demonstrated approach not only enables the policymakers to take stock of deprived neighborhoods in a timely manner, but also lays firm ground for framing contextualized strategies of urban governance. This study is among the first attempts to bridge the theoretical interpretation of housing attributes with the proxy indicator -based approach for fine-grained neighborhood SES measurement.",
        "DOI": "10.1016/j.habitatint.2022.102572",
        "paper_author": "Wang L.",
        "affiliation_name": "Wuhan University",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60029306",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Accident causes data-driven coal and gas outburst accidents prevention: Application of data mining and machine learning in accident path mining and accident case-based deduction",
        "publication": "Process Safety and Environmental Protection",
        "citied_by": "31",
        "cover_date": "2022-06-01",
        "Abstract": "Analyzing the causes of accidents, excavating accident paths, and applying accident prevention are important tasks in safety management. Focusing on coal and gas outburst accidents, this study examined the primary accident path and conducted applied research on the reasoning of the accident case. First, combined with the obtained accident causes, a coupling analysis of the causes of coal and gas outburst accidents was conducted. Second, using the method of data mining coupled with Apriori algorithm, the coupling relationship between each cause module of the coal and gas outburst accident was obtained, and consequently, a path map of the coal and gas outburst accident was drawn. Third, a Bayesian network model for the causes of coal and gas outburst accidents was established based on the accident path map and the probability of occurrence of each cause. Finally, considering the safety concept element (SC1) as an example, the Bayesian network model was used to conduct a sensitivity analysis of accident causes. Thereafter, considering the coal and gas outburst accident of the Sanjia Coal Mine in Guizhou Province as an example, probabilistic reasoning research on the cause of the accident was conducted. The application results showed that (1) under normal conditions, there are approximately 797,280 accident paths for coal and gas outbursts. Following data mining, 188 main accident paths were found. (2) Sensitivity analysis determined 19 factors that were sensitive to safety concept elements (SC1), of which the three most sensitive factors were (i) resource management system procedures (SM7), (ii) safety policy (SM1), and (iii) safety training system procedure (SM8). 13 paths exhibited a sensitivity ≥0.5%, of which 7 exhibited strong sensitivity. (3) The absolute accuracy rate of accident cause reasoning in the Sanjia Coal Mine in Guizhou Province was 71.43%, while the relative accuracy rate was close to 100%. Thus, it was concluded that: (1) the accident path mining method proposed in this paper is feasible for main accident path mining. (2) The Bayesian network model for the causes of coal and gas outburst accidents established in this study can be practically applied for the sensitivity analysis of accident causes and exhibits high reliability in the probabilistic reasoning of accident causes. The results of this study is expected to aid in the prevention of coal and gas outburst accidents, and provide reference and help for the path mining of other accident causes and the probabilistic reasoning of accident causes.",
        "DOI": "10.1016/j.psep.2022.04.059",
        "paper_author": "Xuecai X.",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60025278",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Machine learning in the service of policy targeting: The case of public credit guarantees",
        "publication": "Journal of Economic Behavior and Organization",
        "citied_by": "8",
        "cover_date": "2022-06-01",
        "Abstract": "Public credit guarantees should be provided to firms that are both creditworthy and credit constrained. We use Machine Learning (ML) predictive tools to propose a targeting rule that includes both objectives. The study elaborates on the case of Italy's Guarantee Fund and demonstrates, by means of ex-post evaluation methods, that the program effectiveness can be increased by ML targeting. We discuss some of the problems in using algorithms for the implementation of public policies, such as transparency and manipulation.",
        "DOI": "10.1016/j.jebo.2022.04.004",
        "paper_author": "Andini M.",
        "affiliation_name": "Banca d'Italia",
        "affiliation_city": "Rome",
        "affiliation_country": "Italy",
        "affiliation_id": "60082963",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Data Transparency and Fairness Analysis of the NYPD Stop-and-Frisk Program",
        "publication": "Journal of Data and Information Quality",
        "citied_by": "1",
        "cover_date": "2022-06-01",
        "Abstract": "Given the increased concern of racial disparities in the stop-and-frisk programs, the New York Police Department (NYPD) requires publicly displaying detailed data for all the stops conducted by police authorities, including the suspected offense and race of the suspects. By adopting a public data transparency policy, it becomes possible to investigate racial biases in stop-and-frisk data and demonstrate the benefit of data transparency to approve or disapprove social beliefs and police practices. Thus, data transparency becomes a crucial need in the era of Artificial Intelligence (AI), where police and justice increasingly use different AI techniques not only to understand police practices but also to predict recidivism, crimes, and terrorism. In this study, we develop a predictive analytics method, including bias metrics and bias mitigation techniques to analyze the NYPD Stop-and-Frisk datasets and discover whether underline bias patterns are responsible for stops and arrests. In addition, we perform a fairness analysis on two protected attributes, namely, the race and the gender, and investigate their impacts on arrest decisions. We also apply bias mitigation techniques. The experimental results show that the NYPD Stop-and-Frisk dataset is not biased toward colored and Hispanic individuals and thus law enforcement authorities can apply the bias predictive analytics method to inculcate more fair decisions before making any arrests.",
        "DOI": "10.1145/3460533",
        "paper_author": "Badr Y.",
        "affiliation_name": "Penn State Great Valley",
        "affiliation_city": "Malvern",
        "affiliation_country": "United States",
        "affiliation_id": "60073786",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "Scalable Virtual Machine Migration using Reinforcement Learning",
        "publication": "Journal of Grid Computing",
        "citied_by": "19",
        "cover_date": "2022-06-01",
        "Abstract": "Heuristic approaches require fixed knowledge of how resource allocation should be carried out, and this can be limiting when managing variable cloud workloads. Solutions based on Reinforcement Learning (RL) have been presented to manage cloud infrastructure, however, these tend to be centralized and suffer in their ability to maintain Quality of Service (QoS) for data centres with thousands of nodes. To address this, we propose a reinforcement learning management policy, which can run decentralized, and achieve fast convergence towards efficient resource allocation, resulting in lower SLA violations compared to centralized architectures. To address some of the common challenges in applying RL to cloud resource management, such as slow learning and state/action management, we use parallel learning and reduction of the state/action space. We apply a decision making approach to optimize the migration of a VM and choose a target node to host the VM in such a way that brings response time within SLA level. We have also demonstrate unique, multi-level reinforcement learning cooperation, that further reduces SLA violations. We use simulation to evaluate and demonstrate our proposal in practice, and compare the results obtained with an established heuristic, demonstrating significant improvement to SLA violations and higher scalability.",
        "DOI": "10.1007/s10723-022-09603-4",
        "paper_author": "Hummaida A.R.",
        "affiliation_name": "The University of Manchester",
        "affiliation_city": "Manchester",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60003771",
        "affiliation_state": "Greater Manchester"
    },
    {
        "paper_title": "National-scale predictions of plant assemblages via community distribution models: Leveraging published data to guide future surveys",
        "publication": "Journal of Applied Ecology",
        "citied_by": "4",
        "cover_date": "2022-06-01",
        "Abstract": "Species distribution models (SDMs) have been widely used to create maps of expected species incidence, often using citizen science (CS) occurrence data as inputs. Environmental policy is informed by knowledge of community distributions, but there have been fewer attempts to utilise the potential of community distribution models (CDMs) to predict these. Many countries have vegetation community classification systems which include phytosociological information on individual species. Within Great Britain, the National Vegetation Classification (NVC) is the primary standard for vegetation communities, and while maps have been produced at regional scales, cost-effective techniques are required for national scales. Published NVC occurrence records of 22 upland NVC communities in England and Wales were used as observed occurrences (presence-only data). Predictors for the CDMs were enhanced vegetation index (EVI), elevation, slope, aspect, temperature and rainfall. Five modelling methods were investigated: generalised linear models (GLMs), support vector machines (SVMs), random forests (RFs), maximum entropy (MaxEnt) and maximum likelihood (MaxLike). Model quality was assessed via bootstrapping via area under the curve (AUC), true skill statistic (TSS) and Kappa index. There were only small differences in the accuracy of the models (median TSS model accuracy 0.742; range 0.280 to 0.873) with RF models the best overall CDM method. Across all NVC communities, summer and winter maximum temperatures and annual rainfall were the most important predictor variables. NVCs with spatially disjunct distributions in both lowlands and uplands, or that responded to localised management or environmental conditions, were poorly predicted. Synthesis and applications. Vegetation communities can be reliably predicted at large spatial scales using CDMs from extant datasets. Management practitioners can use community-level predictions to design targeted field surveys for individual species typically associated with specific communities. Most existing CS survey schemes focus on species rather than communities. Hence future development of new CS schemes similar to the National Plant Monitoring Scheme (NPMS), that aligns with the NVC, will enable CS data to generate up-to-date maps of both communities and species.",
        "DOI": "10.1111/1365-2664.14166",
        "paper_author": "Butler L.",
        "affiliation_name": "Newcastle University",
        "affiliation_city": "Newcastle",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60006222",
        "affiliation_state": "Tyne and Wear"
    },
    {
        "paper_title": "Illusion of Truth: Analysing and Classifying COVID-19 Fake News in Brazilian Portuguese Language",
        "publication": "Big Data and Cognitive Computing",
        "citied_by": "10",
        "cover_date": "2022-06-01",
        "Abstract": "Public health interventions to counter the COVID-19 pandemic have accelerated and increased digital adoption and use of the Internet for sourcing health information. Unfortunately, there is evidence to suggest that it has also accelerated and increased the spread of false information relating to COVID-19. The consequences of misinformation, disinformation and misinterpretation of health information can interfere with attempts to curb the virus, delay or result in failure to seek or continue legitimate medical treatment and adherence to vaccination, as well as interfere with sound public health policy and attempts to disseminate public health messages. While there is a significant body of literature, datasets and tools to support countermeasures against the spread of false information online in resource-rich languages such as English and Chinese, there are few such resources to support Portuguese, and Brazilian Portuguese specifically. In this study, we explore the use of machine learning and deep learning techniques to identify fake news in online communications in the Brazilian Portuguese language relating to the COVID-19 pandemic. We build a dataset of 11,382 items comprising data from January 2020 to February 2021. Exploratory data analysis suggests that fake news about the COVID-19 vaccine was prevalent in Brazil, much of it related to government communications. To mitigate the adverse impact of fake news, we analyse the impact of machine learning to detect fake news based on stop words in communications. The results suggest that stop words improve the performance of the models when keeping them within the message. Random Forest was the machine learning model with the best results, achieving 97.91% of precision, while Bi-GRU was the best deep learning model with an F1 score of 94.03%.",
        "DOI": "10.3390/bdcc6020036",
        "paper_author": "Endo P.T.",
        "affiliation_name": "Universidade de Pernambuco",
        "affiliation_city": "Recife",
        "affiliation_country": "Brazil",
        "affiliation_id": "60021980",
        "affiliation_state": "PE"
    },
    {
        "paper_title": "Special report: The AgAID AI institute for transforming workforce and decision support in agriculture",
        "publication": "Computers and Electronics in Agriculture",
        "citied_by": "18",
        "cover_date": "2022-06-01",
        "Abstract": "Tackling the grand challenges of 21st century agriculture (Ag) will require a fundamental shift in the way we envision the role of artificial intelligence (AI) technologies, and in the way we build agricultural AI systems. This shift is needed especially for complex, high-value agricultural ecosystems such as those in the Western U.S., where 300+ crops are grown. Farmers and policy makers in this region face variable profitability, major crop loss and poor crop quality owing to several challenges, including increased labor costs and shortages of skilled workers, weather and management uncertainties, and water scarcity. While AI is expected to be a significant tool for addressing these challenges, AI capabilities must be expanded and will need to account for human input and human behavior – calling for a strong AI-Ag coalition that also creates new opportunities to achieve sustained innovation. Accomplishing this goal goes well beyond the scope of any specific research project or disciplinary silo and requires a more holistic transdisciplinary effort in research, development, and training. To respond to this need, we initiated the AgAID Institute, a multi-institution, transdisciplinary National AI Research Institute that will build new public-private partnerships involving a diverse range of stakeholders in both agriculture and AI. The institute focuses its efforts on providing AI solutions to specialty crop agriculture where the challenges pertaining to water availability, climate variability and extreme weather, and labor shortages, are all significantly pronounced. Our approach to all AgAID Institute activities is being guided by three cross-cutting principles: (i) adoption as a first principle in AI design; (ii) adaptability to changing environments and scales, and (iii) amplification of human skills and machine efficiency. The AgAID Institute is conducting a range of activities including: using agricultural AI applications as testbeds for developing innovative AI technologies and workflows; laying the technological foundations for climate-smart agriculture; serving as a nexus for culturally inclusive collaborative and transdisciplinary learning and knowledge co-production; preparing the next generation workforce for careers at the intersection of Ag and AI technology; and facilitating technology adoption and transfer.",
        "DOI": "10.1016/j.compag.2022.106944",
        "paper_author": "Kalyanaraman A.",
        "affiliation_name": "Washington State University Pullman",
        "affiliation_city": "Pullman",
        "affiliation_country": "United States",
        "affiliation_id": "60018208",
        "affiliation_state": "WA"
    },
    {
        "paper_title": "Monitoring global development aid with machine learning",
        "publication": "Nature Sustainability",
        "citied_by": "19",
        "cover_date": "2022-06-01",
        "Abstract": "Monitoring global development aid provides important evidence for policymakers financing the Sustainable Development Goals (SDGs). To overcome the limitations of existing monitoring, we develop a machine learning framework that enables a comprehensive and granular categorization of development aid activities based on their textual descriptions. Specifically, we cluster the descriptions of ~3.2 million aid activities conducted between 2000 and 2019 totalling US$2.8 trillion. As a result, we generated 173 activity clusters representing the topics of underlying aid activities. Among them, 70 activity clusters cover topics that have not yet been analysed empirically (for example, greenhouse gas emissions reduction and maternal health care). On the basis of our activity clusters, global development aid can be monitored for new topics and at new levels of granularity, allowing the identification of unexplored spatio-temporal disparities. Our framework can be adopted by development finance and policy institutions to promote evidence-based decisions targeting the SDGs.",
        "DOI": "10.1038/s41893-022-00874-z",
        "paper_author": "Toetzke M.",
        "affiliation_name": "ETH Zürich",
        "affiliation_city": "Zurich",
        "affiliation_country": "Switzerland",
        "affiliation_id": "60025858",
        "affiliation_state": "ZH"
    },
    {
        "paper_title": "Evaluation of e-scooters as transit last-mile solution",
        "publication": "Transportation Research Part C: Emerging Technologies",
        "citied_by": "38",
        "cover_date": "2022-06-01",
        "Abstract": "E-scooters are an alternative for short trips and are particularly suitable for solving the last-mile transit problem, yet their impact on transit is not well understood. There is a need to understand the e-scooter demand patterns and users’ characteristics to develop adequate policies and regulations. In this research, we consider the problem of modeling the interaction of e-scooters and bus transit services and provide an overview of e-scooter trips and user characteristics. We use a revealed-preference survey to evaluate the e-scooter usage in one of the highest-demand areas in the City of Austin, corresponding to a university campus. We explore population characteristics, mode shift, and mode interaction. Then, using publicly available datasets, we provide a causal analysis to evaluate the nature of the relationship between e-scooter and transit trips in the whole city. Assessing this relationship is challenging because several factors affect the demand of both types of trips (e.g., location of attractive zones), known as confounding variables. We develop a methodological framework to isolate the effects of confounding variables on transit trips using a two-stage regression procedure. The first stage aims to isolate confounding variables using a gradient boosting regression. The second stage models first and last-mile trips using a negative binomial and a zero-inflated negative binomial count model. The university survey indicated that 12 percent of the e-scooter users employed transit to complement their trips. Although small in magnitude, the data modeling results show that a statistically significant relationship was found on the university campus and downtown areas, supporting the survey results and extending the analysis to other areas of the city. However, the overall interaction between the two modes has a small magnitude. The proposed methodology can be used to identify areas with potential e-scooter and transit interaction.",
        "DOI": "10.1016/j.trc.2022.103660",
        "paper_author": "Zuniga-Garcia N.",
        "affiliation_name": "Cockrell School of Engineering",
        "affiliation_city": "Austin",
        "affiliation_country": "United States",
        "affiliation_id": "60150401",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "Intelligent multi-agent reinforcement learning model for resources allocation in cloud computing",
        "publication": "Journal of King Saud University - Computer and Information Sciences",
        "citied_by": "32",
        "cover_date": "2022-06-01",
        "Abstract": "Now more than ever, optimizing resource allocation in cloud computing is becoming more critical due to the growth of cloud computing consumers and meeting the computing demands of modern technology. Cloud infrastructures typically consist of heterogeneous servers, hosting multiple virtual machines with potentially different specifications, and volatile resource usage. This makes the resource allocation face many issues such as energy conservation, fault tolerance, workload balancing, etc. Finding a comprehensive solution that considers all these issues is one of the essential concerns of cloud service providers. This paper presents a new resource allocation model based on an intelligent multi-agent system and reinforcement learning method (IMARM). It combines the multi-agent characteristics and the Q-learning process to improve the performance of cloud resource allocation. IMARM uses the properties of multi-agent systems to dynamically allocate and release resources, thus responding well to changing consumer demands. Meanwhile, the reinforcement learning policy makes virtual machines move to the best state according to the current state environment. Also, we study the impact of IMARM on execution time. The experimental results showed that our proposed solution performs better than other comparable algorithms regarding energy consumption and fault tolerance, with reasonable load balancing and respectful execution time.",
        "DOI": "10.1016/j.jksuci.2022.03.016",
        "paper_author": "Belgacem A.",
        "affiliation_name": "Université de Boumerdes",
        "affiliation_city": "Boumerdes",
        "affiliation_country": "Algeria",
        "affiliation_id": "60087352",
        "affiliation_state": "Boumerdes"
    },
    {
        "paper_title": "Metalearning Approach Coupled with CMIP6 Multi-GCM for Future Monthly Streamflow Forecasting",
        "publication": "Journal of Hydrologic Engineering",
        "citied_by": "15",
        "cover_date": "2022-06-01",
        "Abstract": "Spatial and temporal variability of streamflow due to climate change affects hydrological processes and irrigation demands at a basin scale. This study investigated the impacts of climate change on the Kurau River in Malaysia using metalearning, an ensemble machine learning technique using support vector regression (SVR) and random forest (RF) coupled with the Coupled Model Intercomparison Project CMIP6 multi-Global Climate Model (GCM). Five global climate models and three shared socioeconomic pathways (SSP1-2.6, SSP2-4.5, and SSP5-8.5) were used. The climate sequences generated by the delta change factor method were applied as input to the metalearning model to predict the streamflow changes in the Kurau River from 2021 to 2080. The model fitted reasonably well, with Kling-Gupta efficiency (KGE), Nash-Sutcliffe efficiency (NSE), percent bias (PBias), and RMS Error (RMSE) of 0.79, 0.83, 2.52, and 4.51, respectively, for the training period (1976-1995) and 0.72, 0.72, 5.85, and 6.90, respectively, for the testing period (1995-2005). Future projections of multi-GCM over the 2021-2080 period under three SSPs predicted an increase in rainfall for all months except April-June during the dry period (off-season), with a higher increase occurring during the wet period (main season). Temperature projections indicated an increase in maximum and minimum temperatures under all SSP scenarios, with a higher increase of approximately 2.0°C under SSP5-8.5 predicted during the 2051-2080 period relative to the baseline period of 1976-2005. The model predicted that the seasonal changes in streamflow of two planting periods range between -7.5% and 7.1% and between 1.2% and 5.9% during the off-season and the main season, respectively. A significant streamflow decrease was predicted in April and May for all SSP scenarios due to high temperatures during the off-season, with SSP5-8.5 being the worst. The impact assessment of climate variabilities on the availability of water resources is vital to identify appropriate adaptation strategies to deal with an expected increase in irrigation demand due to global warming in the future. The predicted future streamflow under the potential climate change impacts is crucial for the Bukit Merah Reservoir to establish suitable operational policies for irrigation release.",
        "DOI": "10.1061/(ASCE)HE.1943-5584.0002176",
        "paper_author": "Adib M.N.M.",
        "affiliation_name": "Universiti Teknologi Malaysia",
        "affiliation_city": "Johor Bahru",
        "affiliation_country": "Malaysia",
        "affiliation_id": "60021005",
        "affiliation_state": "Johor"
    },
    {
        "paper_title": "On the clinical acceptance of black-box systems for EEG seizure prediction",
        "publication": "Epilepsia Open",
        "citied_by": "12",
        "cover_date": "2022-06-01",
        "Abstract": "Seizure prediction may be the solution for epileptic patients whose drugs and surgery do not control seizures. Despite 46 years of research, few devices/systems underwent clinical trials and/or are commercialized, where the most recent state-of-the-art approaches, as neural networks models, are not used to their full potential. The latter demonstrates the existence of social barriers to new methodologies due to data bias, patient safety, and legislation compliance. In the form of literature review, we performed a qualitative study to analyze the seizure prediction ecosystem to find these social barriers. With the Grounded Theory, we draw hypotheses from data, while with the Actor-Network Theory we considered that technology shapes social configurations and interests, being fundamental in healthcare. We obtained a social network that describes the ecosystem and propose research guidelines aiming at clinical acceptance. Our most relevant conclusion is the need for model explainability, but not necessarily intrinsically interpretable models, for the case of seizure prediction. Accordingly, we argue that it is possible to develop robust prediction models, including black-box systems to some extent, while avoiding data bias, ensuring patient safety, and still complying with legislation, if they can deliver human- comprehensible explanations. Due to skepticism and patient safety reasons, many authors advocate the use of transparent models which may limit their performance and potential. Our study highlights a possible path, by using model explainability, on how to overcome these barriers while allowing the use of more computationally robust models.",
        "DOI": "10.1002/epi4.12597",
        "paper_author": "Pinto M.F.",
        "affiliation_name": "University of Coimbra, Centre for Informatics and System",
        "affiliation_city": "Coimbra",
        "affiliation_country": "Portugal",
        "affiliation_id": "60106440",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "The National Inpatient Sample: A Primer for Neurosurgical Big Data Research and Systematic Review",
        "publication": "World Neurosurgery",
        "citied_by": "14",
        "cover_date": "2022-06-01",
        "Abstract": "Objective: The National Inpatient Sample (NIS) (the largest all-payer inpatient database in the United States) is an important instrument for big data analysis of neurosurgical inquiries. However, earlier research has determined that many NIS studies are limited by common methodological pitfalls. In this study, we provide the first primer of NIS methodological procedures in the setting of neurosurgical research and review all reported neurosurgical studies using the NIS. Methods: We designed a protocol for neurosurgical big data research using the NIS, based on our subject matter expertise, NIS documentation, and input and verification from the Healthcare Cost and Utilization Project. We subsequently used a comprehensive search strategy to identify all neurosurgical studies using the NIS in the PubMed and MEDLINE, Embase, and Web of Science databases from inception to August 2021. Studies underwent qualitative categorization (years of NIS studied, neurosurgical subspecialty, age group, and thematic focus of study objective) and analysis of longitudinal trends. Results: We identified a canonical, 4-step protocol for NIS analysis: study population selection; defining additional clinical variables; identification and coding of outcomes; and statistical analysis. Methodological nuances discussed include identifying neurosurgery-specific admissions, addressing missing data, calculating additional severity and hospital-specific metrics, coding perioperative complications, and applying survey weights to make nationwide estimates. Inherent database limitations and common pitfalls of NIS studies discussed include lack of disease process–specific variables and data after the index admission, inability to calculate certain hospital-specific variables after 2011, performing state-level analyses, conflating hospitalization charges and costs, and not following proper statistical methodology for performing survey-weighted regression. In a systematic review, we identified 647 neurosurgical studies using the NIS. Although almost 60% of studies were reported after 2015, <10% of studies analyzed NIS data after 2015. The average sample size of studies was 507,352 patients (standard deviation = 2,739,900). Most studies analyzed cranial procedures (58.1%) and adults (68.1%). The most prevalent topic areas analyzed were surgical outcome trends (35.7%) and health policy and economics (17.8%), whereas patient disparities (9.4%) and surgeon or hospital volume (6.6%) were the least studied. Conclusions: We present a standardized methodology to analyze the NIS, systematically review the state of the NIS neurosurgical literature, suggest potential future directions for neurosurgical big data inquiries, and outline recommendations to improve the design of future neurosurgical data instruments.",
        "DOI": "10.1016/j.wneu.2022.02.113",
        "paper_author": "Tang O.Y.",
        "affiliation_name": "The Warren Alpert Medical School",
        "affiliation_city": "Providence",
        "affiliation_country": "United States",
        "affiliation_id": "60009796",
        "affiliation_state": "RI"
    },
    {
        "paper_title": "Explaining spatial accessibility to high-quality nursing home care in the US using machine learning",
        "publication": "Spatial and Spatio-temporal Epidemiology",
        "citied_by": "2",
        "cover_date": "2022-06-01",
        "Abstract": "In this study we measure and map the system-wide spatial accessibility to good quality nursing home care for all counties in the contiguous United States, and use an ‘imputed post-lasso’ machine learning technique to systematically examine this accessibility measure's associations with a broad range of county-level socio-demographic variables. Both steps were carried out using publicly available datasets. Analyses found clear evidence of spatial patterning in accessibility, particularly by population density, state and the populations of specific racial minorities. This has implications for outcomes that extend beyond the care homes and we highlight a number of policy measures that may help to address these shortcomings. The ‘out-of-sample’ predictive performance of the machine learning approach highlights the method's usefulness in identifying systematic differences in accessibility to services.",
        "DOI": "10.1016/j.sste.2022.100503",
        "paper_author": "Reddy B.P.",
        "affiliation_name": "Novartis Ireland Limited",
        "affiliation_city": "Dublin",
        "affiliation_country": "Ireland",
        "affiliation_id": "60284780",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Identifying predictors for energy poverty in Europe using machine learning",
        "publication": "Energy and Buildings",
        "citied_by": "27",
        "cover_date": "2022-06-01",
        "Abstract": "In this paper we identify drivers for energy poverty in Europe using machine learning. The establishment of predictors for energy poverty valid across countries is a call made by many experts, since it could provide a basis to effectively target energy-poor households with adequate policy measures. We apply a “low income, high expenditure” framework that classifies households as being at risk of energy poverty to a dataset from a survey conducted at the household-level in 11 European countries with vastly different economies, cultures, and climates. A gradient boosting classifier is successfully trained on a set of socio-economic features hypothesized as predictors for energy poverty in this diverse set of countries. The classifier's internal model is analyzed, providing novel insights into the intricacies that underlie energy poverty. We find that besides the main driver - income - floor area and household size can be confirmed as predictors. Our results suggest the presence of universal predictors that are valid across Europe, and contextual ones that are governed by local characteristics. To facilitate advanced research into energy poverty in Europe, we recommend to increase and streamline household data collection efforts, both at the country- and EU-level.",
        "DOI": "10.1016/j.enbuild.2022.112064",
        "paper_author": "van Hove W.",
        "affiliation_name": "Technische Universität München",
        "affiliation_city": "Munich",
        "affiliation_country": "Germany",
        "affiliation_id": "60019722",
        "affiliation_state": "Bayern"
    },
    {
        "paper_title": "Climate-induced range shifts of invasive species (Diaphorina citri Kuwayama)",
        "publication": "Pest Management Science",
        "citied_by": "55",
        "cover_date": "2022-06-01",
        "Abstract": "BACKGROUND: The Asian citrus psyllid (ACP) Diaphorina citri Kuwayama (Hemiptera: Liviidae) is a destructive, invasive species that poses a serious threat to the citrus industry wherever it occurs. The psyllid vectors the phloem-limited bacteria ‘Candidatus Liberibacter americanus’ and ‘Ca. L. asiaticus’, causal agents of the incurable citrus greening disease or huanglongbing (HLB). It is essential to understand which regions and areas are suitable for colonization by ACP to formulate appropriate policy and preventive measures. Considering its biology and ecology, we used a machine learning algorithm based on the MaxEnt (Maximum Entropy) principle, to predict the potential global distribution of ACP using bioclimatic variables and elevation. RESULTS: The model predictions are consistent with the known distribution of ACP and also highlight the potential occurrence outside its current ecological range, that is, primarily in Africa, Asia and the Americas. The most important abiotic variables driving the global distribution of ACP were annual mean temperature, seasonality of temperature and annual precipitation. CONCLUSION: Our findings highlight the need for international collaboration in slowing the spread of invasive pests like D. citri. © 2022 Society of Chemical Industry.",
        "DOI": "10.1002/ps.6886",
        "paper_author": "Aidoo O.F.",
        "affiliation_name": "University of Environment and Sustainable Development",
        "affiliation_city": "Somanya",
        "affiliation_country": "Ghana",
        "affiliation_id": "126216413",
        "affiliation_state": "Eastern Region"
    },
    {
        "paper_title": "Machine Learning and Survey-based Predictors of InfoSec Non-Compliance",
        "publication": "ACM Transactions on Management Information Systems",
        "citied_by": "3",
        "cover_date": "2022-06-01",
        "Abstract": "Survey items developed in behavioral Information Security (InfoSec) research should be practically useful in identifying individuals who are likely to create risk by failing to comply with InfoSec guidance. The literature shows that attitudes, beliefs, and perceptions drive compliance behavior and has influenced the creation of a multitude of training programs focused on improving ones' InfoSec behaviors. While automated controls and directly observable technical indicators are generally preferred by InfoSec practitioners, difficult-to-monitor user actions can still compromise the effectiveness of automatic controls. For example, despite prohibition, doubtful or skeptical employees often increase organizational risk by using the same password to authenticate corporate and external services. Analysis of network traffic or device configurations is unlikely to provide evidence of these vulnerabilities but responses to well-designed surveys might. Guided by the relatively new IPAM model, this study administered 96 survey items from the Behavioral InfoSec literature, across three separate points in time, to 217 respondents. Using systematic feature selection techniques, manageable subsets of 29, 20, and 15 items were identified and tested as predictors of non-compliance with security policy. The feature selection process validates IPAM's innovation in using nuanced self-efficacy and planning items across multiple time frames. Prediction models were trained using several ML algorithms. Practically useful levels of prediction accuracy were achieved with, for example, ensemble tree models identifying 69% of the riskiest individuals within the top 25% of the sample. The findings indicate the usefulness of psychometric items from the behavioral InfoSec in guiding training programs and other cybersecurity control activities and demonstrate that they are promising as additional inputs to AI models that monitor networks for security events.",
        "DOI": "10.1145/3466689",
        "paper_author": "Marshall B.",
        "affiliation_name": "Oregon State University",
        "affiliation_city": "Corvallis",
        "affiliation_country": "United States",
        "affiliation_id": "60013402",
        "affiliation_state": "OR"
    },
    {
        "paper_title": "The Private and External Costs of Germany's Nuclear Phase-Out",
        "publication": "Journal of the European Economic Association",
        "citied_by": "18",
        "cover_date": "2022-06-01",
        "Abstract": "Many countries have phased out nuclear power in response to concerns about nuclear waste and the risk of nuclear accidents. This paper examines the shutdown of more than half of the nuclear production capacity in Germany after the Fukushima accident in 2011. We use hourly data on power plant operations and a machine learning approach to estimate the impacts of the phase-out policy. We find that reductions in nuclear electricity production were offset primarily by increases in coal-fired production and net electricity imports. Our estimates of the social cost of the phase-out range from €3 to €8 billion per year. The majority of this cost comes from the increased mortality risk associated with exposure to the local air pollution emitted when burning fossil fuels. Policymakers would have to significantly overestimate the risk or cost of a nuclear accident to conclude that the benefits of the phase-out exceed its social costs. We discuss the likely role of behavioral biases in this setting, and highlight the importance of ensuring that policymakers and the public are informed about the health effects of local air pollution.",
        "DOI": "10.1093/jeea/jvac007",
        "paper_author": "Jarvis S.",
        "affiliation_name": "London School of Economics and Political Science",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60003059",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Machine learning application to spatio-temporal modeling of urban growth",
        "publication": "Computers, Environment and Urban Systems",
        "citied_by": "16",
        "cover_date": "2022-06-01",
        "Abstract": "Understanding the dynamics of urban growth is among the most important tasks in urban planning due to their influence on policy decision-making. Specifically, prediction of urban growth at regional levels is crucial for regional policy makers. Making such predictions is difficult because of the existence of complex topological structures and the high-dimensional nature of data sets related to urban growth. Spatial and temporal auto-correlation and cross-correlations, together with regional social and physical covariates, need to be properly accounted for improving the forecasting power of any statistical or machine learning method. To that end, we develop novel machine learning methodologies to perform predictions of urban growth at regional levels by incorporating lead-lag non-linear relationships among past urban changes in each region and its neighbors. Based on this analysis, machine learning algorithms outperform more classical methods, such as a logistic regression, in terms of classifying low/high urban growth regions, and the random forest algorithm seems to have the best prediction accuracy among the selected machine learning methods. Moreover, the random forest method without any external covariates has still a high prediction accuracy which not only confirms that most of variability of urban growth can be described by past observations of self and neighboring changes, but also makes it possible to perform real forecasting of urban growth without accessing any external covariates. The latter makes this modeling framework useful for local policy makers in allocating budget and directing resources appropriately based on such predictions.",
        "DOI": "10.1016/j.compenvurbsys.2022.101801",
        "paper_author": "Kim Y.",
        "affiliation_name": "College of Liberal Arts and Sciences",
        "affiliation_city": "Gainesville",
        "affiliation_country": "United States",
        "affiliation_id": "60154289",
        "affiliation_state": "FL"
    },
    {
        "paper_title": "Artificial intelligence-based microfluidic platforms for the sensitive detection of environmental pollutants: Recent advances and prospects",
        "publication": "Trends in Environmental Analytical Chemistry",
        "citied_by": "55",
        "cover_date": "2022-06-01",
        "Abstract": "Environmental pollution and its drastic effects on human and animal health have urged governments to implement strict policies to minimize damage. The first step in applying such policies is to find reliable methods to detect pollution in various media, including water, food, soil, and air. In this regard, various approaches such as spectrophotometric, chromatographic, and electrochemical techniques have been proposed. To overcome the limitations associated with conventional analytical methods, microfluidic devices have emerged as sensitive technologies capable of generating high content information during the past few years. The passage of contaminant samples through the microfluidic channels provides essential details about the whole environment after detection by the detector. In the meantime, artificial intelligence is an ideal means to identify, classify, characterize, and even predict the data obtained from microfluidic systems. The development of microfluidic devices with integrated machine learning and artificial intelligence is promising for the development of next-generation monitoring systems. Combination of the two systems ensures time efficient setups with easy operation. This review article is dedicated to the recent developments in microfluidic chips coupled with artificial intelligence technology for the evolution of more convenient pollution monitoring systems.",
        "DOI": "10.1016/j.teac.2022.e00160",
        "paper_author": "Pouyanfar N.",
        "affiliation_name": "SBUMS School of Pharmacy",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran",
        "affiliation_id": "60113496",
        "affiliation_state": "Tehran"
    },
    {
        "paper_title": "Machine Learning in Real-Time Internet of Things (IoT) Systems: A Survey",
        "publication": "IEEE Internet of Things Journal",
        "citied_by": "47",
        "cover_date": "2022-06-01",
        "Abstract": "Over the last decade, machine learning (ML) and deep learning (DL) algorithms have significantly evolved and been employed in diverse applications, such as computer vision, natural language processing, automated speech recognition, etc. Real-time safety-critical embedded and Internet of Things (IoT) systems, such as autonomous driving systems, UAVs, drones, security robots, etc., heavily rely on ML/DL-based technologies, accelerated with the improvement of hardware technologies. The cost of a deadline (required time constraint) missed by ML/DL algorithms would be catastrophic in these safety-critical systems. However, ML/DL algorithm-based applications have more concerns about accuracy than strict time requirements. Accordingly, researchers from the real-time systems (RTSs) community address the strict timing requirements of ML/DL technologies to include in RTSs. This article will rigorously explore the state-of-the-art results emphasizing the strengths and weaknesses in ML/DL-based scheduling techniques, accuracy versus execution time tradeoff policies of ML algorithms, and security and privacy of learning-based algorithms in real-time IoT systems.",
        "DOI": "10.1109/JIOT.2022.3161050",
        "paper_author": "Bian J.",
        "affiliation_name": "College of Engineering and Computer Science",
        "affiliation_city": "Orlando",
        "affiliation_country": "United States",
        "affiliation_id": "60154598",
        "affiliation_state": "FL"
    },
    {
        "paper_title": "Machine learning-based early detection of IoT botnets using network-edge traffic",
        "publication": "Computers and Security",
        "citied_by": "44",
        "cover_date": "2022-06-01",
        "Abstract": "In this work, we present an IoT botnet detection solution, EDIMA, consisting of a set of lightweight modules designed to be deployed at the edge gateway installed in home networks with the remaining modules expected to be implemented on cloud servers. EDIMA targets early detection of IoT botnets prior to the launch of an attack and includes a novel two-stage Machine Learning (ML)-based detector developed specifically for IoT bot detection at the edge gateway. The ML-based bot detector first employs supervised ML algorithms for aggregate traffic classification and subsequently Autocorrelation Function (ACF)-based tests to detect individual bots. The EDIMA architecture also comprises a malware traffic database, a policy engine, a feature extractor and a traffic parser. Performance evaluation results using our testbed setup with real-world IoT malware traffic as well as other public IoT datasets show that EDIMA achieves high bot scanning and bot-CnC traffic detection accuracies with very low false positive rates. The detection performance is also shown to be robust to an increase in the number of IoT devices connected to the edge gateway where EDIMA is deployed. Further, the runtime performance analysis of a Python implementation of EDIMA deployed on a Raspberry Pi reveals low bot detection delays and low RAM consumption. EDIMA is also shown to outperform existing detection techniques for bot scanning traffic and bot-CnC server communication.",
        "DOI": "10.1016/j.cose.2022.102693",
        "paper_author": "Kumar A.",
        "affiliation_name": "National University of Singapore",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore",
        "affiliation_id": "60017161",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Continental United States may lose 1.8 petagrams of soil organic carbon under climate change by 2100",
        "publication": "Global Ecology and Biogeography",
        "citied_by": "20",
        "cover_date": "2022-06-01",
        "Abstract": "Aims: High-resolution information on soils’ vulnerability to climate-induced soil organic carbon (SOC) loss can enable environmental scientists, land managers, and policy makers to develop targeted mitigation strategies. This study aims to estimate baseline and decadal changes in continental US surface SOC stocks under future emission scenarios. Location: Continental United States. Time period: 2014–2100. Methods: We used recent SOC field observations (n = 6,213 sites), environmental factors (n = 32), and an ensemble machine learning (ML) approach to estimate baseline SOC stocks in surface soils across the continental United States at 100-m spatial resolution, and decadal changes under the projected climate scenarios of Coupled Model Intercomparison Project Phase Six (CMIP6) earth system models (ESMs). Results: Baseline SOC projections from ML approaches captured more than 50% of variability in SOC observations, whereas ESMs represented only 6–16% of observed SOC variability. ML estimates showed a mean total loss of 1.8 Pg C from US surface soils under the high-emission scenario by 2100, whereas ESMs showed no significant change in SOC stocks with wide variation among ESMs. Both ML and ESM predictions agree on the direction of SOC change (net emissions or sequestration) across 46–51% of continental US land area. These differences are attributable to the high-resolution site-specific data used in the ML models compared to the relatively coarse grid represented in CMIP6 ESMs. Main conclusions: Our high-resolution estimates of baseline SOC stocks, identification of key environmental controllers, and projection of SOC changes from US land cover types under future climate scenarios suggest the need for high-resolution simulations of SOC in ESMs to represent the heterogeneity of SOC. We found that the SOC change is sensitive to key soil related factors (e.g. soil drainage and soil order) that have not been historically considered as input parameters in ESMs, because currently more than 95% variability in the SOC of CMIP6 ESMs is controlled by net primary productivity, temperature, and precipitation. Using additional environmental factors to estimate the baseline SOC stocks and predict the future trajectory of SOC change can provide more accurate results.",
        "DOI": "10.1111/geb.13489",
        "paper_author": "Gautam S.",
        "affiliation_name": "Sandia National Laboratories, California",
        "affiliation_city": "Livermore",
        "affiliation_country": "United States",
        "affiliation_id": "60015900",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "ANN-Based traffic volume prediction models in response to COVID-19 imposed measures",
        "publication": "Sustainable Cities and Society",
        "citied_by": "20",
        "cover_date": "2022-06-01",
        "Abstract": "Many countries around the globe have imposed several response measures to suppress the rapid spread of the COVID-19 pandemic since the beginning of 2020. These measures have impacted routine daily activities, along with their impact on economy, education, social and recreational activities, and domestic and international travels. Intuitively, the different imposed policies and measures have indirect impacts on urban traffic mobility. As a result of those imposed measures and policies, urban traffic flows have changed. However, those impacts are neither measured nor quantified. Therefore, estimating the impact of these combined yet different policies and measures on urban traffic flows is a challenging task. This paper demonstrates the development of an artificial neural networks (ANN) model which correlates the impact of the imposed response measure and other factors on urban traffic flows. The results show that the adopted ANN model is capable of mapping the complex relationship between traffic flows and the response measures with a high level of accuracy and good performance. The predicted values are closed to the observed ones. They are clustered around the regression line, with a coefficient of determination (R2) of 0.9761. Furthermore, the developed model can be generalized to determine the anticipated demand levels resulted from imposing any of the response measures in the post-pandemic era. This model can be used to manage traffic during mega-events. It can be also utilized for disaster or emergency situations, where traffic flow estimates are highly required for operational and planning purposes.",
        "DOI": "10.1016/j.scs.2022.103830",
        "paper_author": "Ghanim M.S.",
        "affiliation_name": "Ministry of Transport",
        "affiliation_city": "Doha",
        "affiliation_country": "Qatar",
        "affiliation_id": "118260901",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A novel approach to examining urban housing market segmentation: Comparing the dynamics between sales submarkets and rental submarkets",
        "publication": "Computers, Environment and Urban Systems",
        "citied_by": "27",
        "cover_date": "2022-06-01",
        "Abstract": "Submarket segmentation outlines an essential prerequisite for monitoring housing market and formulating urban housing policies. Although examining segmentation based on a posteriori knowledge rather than a priori knowledge becomes the mainstream, it follows a data-driven approach without a solid theoretical foundation and involves subjective interventions. Additionally, earlier studies have overwhelmingly examined the dynamics of sales submarkets while overlooking those of rental submarkets. This paper demonstrates a novel approach to segmenting the housing market by integrating the hedonic model, geographically and temporally weighted regression (GTWR), and machine learning, and further applies it to unpack the dynamics of sales submarkets and rental submarkets from 2018 to 2020 in Shanghai, China. More specifically, using the home-fixed panel data of housing sales prices and rental prices for each residential quarter, we first establish a series of hedonic models using GTWR and then aggregate the residential quarters into a number of submarkets using an affinity propagation clustering algorithm based on the produced spatiotemporally explicit coefficients. To validate the identified submarkets, we compare them to the static submarkets delineated by the real estate industry with respect to the performances of hedonic models. Finally, the Jaccard and Rand indices are applied to compare the magnitude of spatiotemporal dynamics of sales submarkets and rental submarkets. Results show that hedonic models based on the identified submarkets through our proposed method perform better than those based on the static submarkets delineated by the real estate industry. We also discover that the submarkets present a complex change over three years, especially in central urban areas. The dynamics between sales submarkets and rental submarkets are of significant differences. In particular, rental submarkets are more stable than sales submarkets. Our approach provides a practical and efficient tool for urban housing market segmentation. Our study highlights that differentiated policies should be formulated for regulating sales submarkets and rental submarkets in order to enhance housing affordability.",
        "DOI": "10.1016/j.compenvurbsys.2022.101775",
        "paper_author": "Hu L.",
        "affiliation_name": "Wuhan University",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60029306",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Action-dependent bidirectional contrastive predictive coding for neural belief representations",
        "publication": "Neurocomputing",
        "citied_by": "2",
        "cover_date": "2022-06-01",
        "Abstract": "The key to solving the complex, partially observable Markov decision process (POMDP) with high-dimensional observations lies in explicit belief representations. However, the existing methods generally adopted the black-box model for shaping beliefs, which is inefficient and lacks interpretability. Due to this reason, the action-dependent bidirectional contrastive predictive coding (BCPC|Action) is proposed in this paper, in which the observation features are extracted efficiently through self-supervised contrastive learning. Owing to the bottleneck belief constraints in the bidirectional model, the upper bound of prediction errors is effectively reduced. Besides, the forward prediction is optimized by the guidance of an easier trainable backward prediction; thus, the bidirectional match regularization (BMR) could be derived for stabilizing the training process. More importantly, the interpretability of the learned belief representation is thoroughly explored based on the gradient truncation. Simulation results verify the effectiveness of the presented method; apart from achieving highly accurate belief tracking, the state uncertainties could be characterized reasonably, which provides a guarantee for solving the POMDP optimal policy for downstream tasks.",
        "DOI": "10.1016/j.neucom.2022.02.066",
        "paper_author": "Liu J.",
        "affiliation_name": "Henan University of Science and Technology",
        "affiliation_city": "Luoyang",
        "affiliation_country": "China",
        "affiliation_id": "60073587",
        "affiliation_state": "Henan"
    },
    {
        "paper_title": "Semi-supervised clustering-based method for fault diagnosis and prognosis: A case study",
        "publication": "Reliability Engineering and System Safety",
        "citied_by": "37",
        "cover_date": "2022-06-01",
        "Abstract": "Recent increased enthusiasm towards Industrial Artificial Intelligence (IAI) coupled with advancements in smart sensor technologies have resulted in simultaneous incorporation of several Condition Monitoring (CM) technologies within manufacturing/industrial sectors. Smart utilization of CM data leads to enhanced safety, reliability and availability of manufacturing systems. Conventional system monitoring techniques, however, cannot efficiently cope with such rich CM information content. In this regard, the paper proposes a novel hybrid Maintenance Decision Support System (MDSS) for fault diagnostic and prognostics considering event-triggered CM data. The proposed MDSS is a hybrid framework designed by coupling Machine Learning (ML)-based models and statistical techniques. More specifically, the MDSS is a time-dependent Proportional Hazard Model (PHM) augmented with semi-supervised ML approaches and Reinforcement Learning (RL) to find an optimal maintenance policy for systems subject to stochastic degradations with focus on cost minimization. The developed hybrid model is capable of inferring and fusing high-volume CM data sources in an adaptive and autonomous fashion to recommend optimal maintenance decisions without human intervention, which is a step-forward contribution in the maintenance context. To evaluate the structure and performance of the proposed model, comprehensive ML-based solutions are developed based on a dataset provided by NASA containing run-to-failure and CM data associated with aircraft engines.",
        "DOI": "10.1016/j.ress.2022.108405",
        "paper_author": "Azar K.",
        "affiliation_name": "Concordia Institute for Information Systems Engineering",
        "affiliation_city": "Montreal",
        "affiliation_country": "Canada",
        "affiliation_id": "60116756",
        "affiliation_state": "QC"
    },
    {
        "paper_title": "Rivers of waste: Anthropogenic litter in intermittent Sardinian rivers, Italy (Central Mediterranean)",
        "publication": "Environmental Pollution",
        "citied_by": "16",
        "cover_date": "2022-06-01",
        "Abstract": "While the increasing accumulation of anthropogenic litter in the marine environment has received considerable attention over the last decade, litter occurrence and distribution in rivers, the main source of marine litter, have been comparatively less investigated. Moreover, little information is available about the amount and typology of Riverine Anthropogenic Macro-litter (RAM) entering marine environments from intermittent rivers in low populated areas of the Mediterranean basin. To provide insights on this issue, we investigated density and composition of RAM accumulated over a total of 133 riverbanks, belonging to 37 river basins in the Sardinia Island (Mediterranean Sea). We report here that plastics, especially single-use items, represent the most frequent and abundant RAM category in all investigated basins. Statistical modelling revealed that occurrence of lightweight RAM (especially plastic) is mostly explained by levels of urban (12.3% of the relative contribution) and agricultural (12%) land use of the territory, whereas the proximity of bridges to the sampling point (21%) and the local population density (19.8%) are best predictors of heavy weighted RAM items (i.e., large metal items, appliances) occurrence. Our results confirm that plastics represent an important component of RAM and pinpoint that, beside plastic reduction policies and better waste management, actions aimed at abating and monitoring litter contamination should be localized on the proximity of bridges, whatever the local population density. Finally, to fill existing knowledge gaps in understanding the severity of litter discharge and accumulation in the Mediterranean Sea, land-to-sea systematic monitoring campaigns at appropriate spatial and temporal scales should be put in place.",
        "DOI": "10.1016/j.envpol.2022.119073",
        "paper_author": "Palmas F.",
        "affiliation_name": "Università degli Studi di Cagliari",
        "affiliation_city": "Cagliari",
        "affiliation_country": "Italy",
        "affiliation_id": "60032259",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Machine learning-aided pilot and power allocation in multi-cellular massive MIMO networks",
        "publication": "Physical Communication",
        "citied_by": "7",
        "cover_date": "2022-06-01",
        "Abstract": "This paper addresses the problem of power allocation (PA) and pilot assignment (PS) in the uplink (UL) of multi-user (MU) massive multiple-input multiple-output (M-MIMO) under pilot contamination (PC) regime. We analyzed the aforementioned problem with different constraints configurations representing practical and adverse conditions such as minimum quality of service (QoS) requirements. A machine learning (ML) approach based on the reward Q-Learning is deployed for implementing the resource allocation (RA) policy. The RA solution obtained with Q-Learning is compared with classical methods available in literature. We have considered different PC scenarios, generated by pilot reuse patterns. Several figures of merit, including spectral and energy efficiencies (SE, EE), bit error rate (BER) and convergence are analyzed considering different system and channel configurations. Extensive numerical simulation results demonstrate the effectiveness and efficiency in terms of performance-complexity trade-off of the proposed sequentially combined PS-PA method in multi-cellular massive MIMO systems.",
        "DOI": "10.1016/j.phycom.2022.101646",
        "paper_author": "dos Santos H.L.",
        "affiliation_name": "Universidade Estadual de Londrina",
        "affiliation_city": "Londrina",
        "affiliation_country": "Brazil",
        "affiliation_id": "60027866",
        "affiliation_state": "PR"
    },
    {
        "paper_title": "Improving the performance of batch schedulers using online job runtime classification",
        "publication": "Journal of Parallel and Distributed Computing",
        "citied_by": "14",
        "cover_date": "2022-06-01",
        "Abstract": "Job scheduling in high-performance computing platforms is a hard problem that involves uncertainties on both the job arrival process and their execution times. Users typically provide only loose upper bounds for job execution times, which are not so useful for scheduling heuristics based on processing times. Previous studies focused on applying regression techniques to obtain better execution time estimates, which worked reasonably well and improved scheduling metrics. However, these approaches require a long period of training data. In this work, we propose a simpler approach by classifying jobs as small or large and prioritizing the execution of small jobs over large ones. Indeed, small jobs are the most impacted by queuing delays, but they typically represent a light load and incur a small burden on the other jobs. The classifier operates online and learns by using data collected over the previous weeks, facilitating its deployment and enabling a fast adaptation to changes in the workload characteristics. We evaluate our approach using four scheduling policies on seven HPC platform workload traces. We show that: first, incorporating such classification reduces the average bounded slowdown of jobs in all scenarios, second, in most considered scenarios, the improvements are comparable to the ideal hypothetical situation where the scheduler would know in advance the exact running time of jobs.",
        "DOI": "10.1016/j.jpdc.2022.01.003",
        "paper_author": "Zrigui S.",
        "affiliation_name": "Université Grenoble Alpes",
        "affiliation_city": "Saint Martin d'Heres",
        "affiliation_country": "France",
        "affiliation_id": "60104653",
        "affiliation_state": "Auvergne-Rhone-Alpes"
    },
    {
        "paper_title": "Deep reinforcement learning based active safety control for distributed drive electric vehicles",
        "publication": "IET Intelligent Transport Systems",
        "citied_by": "7",
        "cover_date": "2022-06-01",
        "Abstract": "Distributed drive electric vehicles are regarded as the promising transportation due to the advanced power flow architecture. Optimizing the yaw motion to enhance vehicle safety is a challenging job. Besides, the nonlinear features in vehicles affect the control accuracy of the yaw motion controllers. To this end, a deep reinforcement learning (DRL) based direct yaw moment control (DYC) strategy is put forward here. Vehicle dynamics can be approximated with the DRL algorithm, which reduces the complex nonlinear solving process. Concretely, the DYC problem is formulated as Markov Decision Process in which the observed signals and external yaw moment are incorporated as the state and action sets. Thereupon, actor-critic network is exhibited to approximate action-value function and policy function for better control performance. Furthermore, to guarantee the continuous solution of external yaw moment, the deep deterministic policy gradient algorithm is employed, in which target and online network parameters are simultaneously trained to maintain learning process stability. The proposed DRL based DYC strategy is verified using the Carsim/Simulink platform under the typical lane change manoeuvres. Numerical test results demonstrate that the proposed DYC strategy outperforms the linear approaches on taking full advantage of understeer features and enhancing the lateral stability, especially under the critical steering manoeuvres.",
        "DOI": "10.1049/itr2.12176",
        "paper_author": "Wei H.",
        "affiliation_name": "Beijing Institute of Technology",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60016835",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Temporal and spatial simultaneity assessment of wind-solar energy resources in India by statistical analysis and machine learning clustering approach",
        "publication": "Energy",
        "citied_by": "25",
        "cover_date": "2022-06-01",
        "Abstract": "The performance of hybrid power projects significantly relies on simultaneity of energy sources considering the generation fluctuations and grid penetration requirements. The aim of present study is to develop a novel three-module methodology for temporal and spatial simultaneity assessment of wind-solar energy resources useful for technical and economic hybrid (TH and EH) projects. The first module comprises data procurement and system performance evaluation. The second module applies temporal and spatial statistical correlations (Pearson, Spearman, and Kendall) to determine comparative simultaneity (complementarity-synergy). The third module encompasses dimension reduction (principal component analysis) and machine learning classification (Elbow algorithm aided k-means clustering) to classify study region into optimum number of clusters. The proposed methodology is applied over Indian onshore region employing ERA5 reanalysis dataset. The results indicate that the islands of Andaman and Nicobar and south-western parts of India are preferable sites for TH projects. Similarly, the spatial simultaneity clusters signify that the western and south-western part of the country is comparatively preferable for all four types of EH projects. The findings of this study will facilitate project developers, system manufacturers, and policymakers for better understanding of the typical peculiarities of various resources across geographical locations well in advance to deploy hybrid projects.",
        "DOI": "10.1016/j.energy.2022.123586",
        "paper_author": "Jani H.K.",
        "affiliation_name": "Pandit Deendayal Energy University",
        "affiliation_city": "Gandhinagar",
        "affiliation_country": "India",
        "affiliation_id": "60106943",
        "affiliation_state": "GJ"
    },
    {
        "paper_title": "Predicting wasteful spending in tree planting programs in Indian Himalaya",
        "publication": "World Development",
        "citied_by": "19",
        "cover_date": "2022-06-01",
        "Abstract": "Tree planting is widely promoted as a cost-effective natural climate solution, yet there are few evaluations of the implementation of tree planting. Our analysis of a unique dataset on tree planting in the Indian Himalayan state of Himachal Pradesh shows that over half of the state's budget for tree planting is wasted on plantations that are unlikely to survive and/or are poorly designed to achieve the state's goal of increasing forest cover. Himachal Pradesh (and India more generally) has been identified as a high potential area for natural climate solutions due to high government capacity, adequate funding, and government agencies with extensive planting experience. We combine data on the location and financial outlay for plantations, which allow us to analyze the relationship between plantations and social and biophysical conditions, with a machine learning model, trained on past land cover change, which predicts the likelihood of future tree cover loss in plantation areas. Our finding that even in this high potential area tree planting programs involve considerable wasted expenditure on ineffective plantations raises questions about optimistic assessments of the potential for tree planting to serve as a cost-effective natural climate solution. We suggest deemphasizing the target-based approaches that dominate present policy-making and high-profile scientific publications, which we argue are the cause of wasted expenditures in Himachal Pradesh. Instead policy-makers and scientists interested in natural climate solutions should focus on developing solutions that respond to local biophysical, social, and economic realities, and are implemented through transparent procedures that increase accountability to and reinforce the rights of forest dependent people.",
        "DOI": "10.1016/j.worlddev.2022.105864",
        "paper_author": "Rana P.",
        "affiliation_name": "Indian Forest Service",
        "affiliation_city": "Shimla",
        "affiliation_country": "India",
        "affiliation_id": "127775576",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Machine learning for liquidity risk modelling: A supervisory perspective",
        "publication": "Economic Analysis and Policy",
        "citied_by": "14",
        "cover_date": "2022-06-01",
        "Abstract": "The purpose of an effective liquidity risk assessment policy is to ensure that any given credit institution can meet its cash flow obligations, even factoring in the uncertainty caused by external factors. As part of the Supervisory Review and Evaluation Process (SREP), the European Central Bank (ECB) has determined this assessment should take into consideration both the institution's ability to meet its short-term obligations and its long-term funding strategy. Due to the fast pace of financial markets and more demanding regulations, there is a structural need for a precise and widely accepted risk assessment methodology. Furthermore, the ability to foresee alternative scenarios by stressing the involved key risk indicators is of the utmost importance. This work investigates whether machine learning techniques can successfully model liquidity risk, thus providing insights for stress-testing scenarios. We have applied the Risk Assessment System (RAS) methodology to classify credit institutions from the Portuguese banking sector according to their liquidity risk, using real supervisory data (from 2014 until March 2021). We then studied the ability to model this risk classification, by comparing a series of well-established machine learning algorithms to a traditional statistical model for benchmarking. The results show that extreme gradient boosting (XGBoost) outperforms other methods for this classification problem. The resulting model can be set up for a production environment and provide scenarios for stress-testing, or as an early warning system (EWS), thus supporting the overall SREP exercise.",
        "DOI": "10.1016/j.eap.2022.02.001",
        "paper_author": "Guerra P.",
        "affiliation_name": "NOVA Information Management School, Universidade Nova de Lisboa",
        "affiliation_city": "Lisbon",
        "affiliation_country": "Portugal",
        "affiliation_id": "60105899",
        "affiliation_state": "Lisboa"
    },
    {
        "paper_title": "Spatiotemporal forecasting for dengue, chikungunya fever and Zika using machine learning and artificial expert committees based on meta-heuristics",
        "publication": "Research on Biomedical Engineering",
        "citied_by": "6",
        "cover_date": "2022-06-01",
        "Abstract": "Purpose: Dengue is considered one of the biggest public health problems in recent decades. Climate and demographic changes, the disorderly growth of cities and international trade have brought new arboviruses such as chikungunya and Zika. Control of arboviruses depends on control of the vector: the Aedes aegypti mosquito. Objective: In this work, we propose a methodology for building disease predictors capable of predicting infected cases and locations based on machine learning. We also propose an artificial experts committee based on meta-heuristic methods to detect the most relevant risk factors. Method As a case study, we applied the methodology to forecast dengue, chikungunya and Zika, with data from the City of Recife, Brazil, from 2013 to 2016. We used arboviruses cases data and climatic and environmental information: wind speeds, temperatures and precipitation. Results The best prediction results were obtained with 10-tree Random Forest regression, with Pearson’s correlation above 0.99 and RMSE (%) below 6%. Additionally, the artificial experts committee was able to present the most relevant factors for predicting cases in each two-month period. Conclusion: The spatiotemporal prediction results showed the evolution of arboviruses, pointing out as major focuses on both regions richer in urban green areas and low-income neighborhood with irregular water supply. Determining the most relevant factors for prediction, as well as the spatial distribution of cases, can be useful for the planning and execution of public policies aimed at improving the health infrastructure and planning and controlling the vector.",
        "DOI": "10.1007/s42600-022-00202-6",
        "paper_author": "da Silva C.C.",
        "affiliation_name": "Universidade Federal de Pernambuco",
        "affiliation_city": "Recife",
        "affiliation_country": "Brazil",
        "affiliation_id": "60031482",
        "affiliation_state": "PE"
    },
    {
        "paper_title": "The spectrum of data sharing policies in neuroimaging data repositories",
        "publication": "Human Brain Mapping",
        "citied_by": "27",
        "cover_date": "2022-06-01",
        "Abstract": "Sharing data is a scientific imperative that accelerates scientific discoveries, reinforces open science inquiry, and allows for efficient use of public investment and research resources. Considering these benefits, data sharing has been widely promoted in diverse fields and neuroscience has been no exception to this movement. For all its promise, however, the sharing of human neuroimaging data raises critical ethical and legal issues, such as data privacy. Recently, the heightened risks to data privacy posed by the rapid advances in artificial intelligence and machine learning techniques have made data sharing more challenging; the regulatory landscape around data sharing has also been evolving rapidly. Here we present an in-depth ethical and regulatory analysis that examines how neuroimaging data are currently shared against the backdrop of the relevant regulations and policies in the United States and how advanced software tools and algorithms might undermine subjects' privacy in neuroimaging data sharing. The implications of these novel technological threats to privacy in neuroimaging data sharing practices and policies will also be discussed. We then conclude with a proposal for a legal prohibition against malicious use of neuroscience data as a regulatory mechanism to address privacy risks associated with the data while maximizing the benefits of data sharing and open science practice in the field of neuroscience.",
        "DOI": "10.1002/hbm.25803",
        "paper_author": "Jwa A.S.",
        "affiliation_name": "Stanford University",
        "affiliation_city": "Stanford",
        "affiliation_country": "United States",
        "affiliation_id": "60012708",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Deep reinforcement learning approach for solving joint pricing and inventory problem with reference price effects",
        "publication": "Expert Systems with Applications",
        "citied_by": "26",
        "cover_date": "2022-06-01",
        "Abstract": "Reference prices, developed by consumers who frequently buy their desired products or services and form psychological price expectations as a benchmark, have significant impacts on customers’ purchase behaviors and firms’ operational strategies. Therefore, to determine an appropriate pricing and ordering strategy to maximize the total discounted revenues of a retailer, we consider joint pricing and inventory management system under reference price effects in an infinite horizon. Such a system involves uncertain market turbulence and customers’ sensitivities to gains and losses (loss-averse, gain-seeking and loss-neutral). We aggregate those factors into a general value function model with only a few realistic constraints on the variables and structure parameters. A deep reinforcement learning approach based on Double Deep Q-Networks with a target network (TN-DDQN) algorithm is proposed and forms the core of the expert decision system. Two ground truth algorithms (value iteration and real-demand-response policy) and two classical RL algorithms (Double Q-learning and Q-learning) are compared with the TN-DDQN algorithm in discrete and continuous state spaces respectively. Through a sequence of experiments, we find that the retailer should not ignore the impact of current prices on future demand, and a myopic policy will cause the retailer's profits to decrease through reference price effects. Moreover, we also find that if customers have high abilities to remember previous prices, the retailer must bring sales prices down and raise the order-up-to level accordingly. Our system with the TN-DDQN algorithm provides a new way to handle complicated behavioral science and operation problems, which can be applied in a broader field.",
        "DOI": "10.1016/j.eswa.2022.116564",
        "paper_author": "Zhou Q.",
        "affiliation_name": "Beijing Jiaotong University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60022381",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Assessing the Performance of Gradient-Boosting Models for Predicting the Travel Mode Choice Using Household Survey Data",
        "publication": "Journal of Urban Planning and Development",
        "citied_by": "21",
        "cover_date": "2022-06-01",
        "Abstract": "The importance of analyzing travel mode choice to understand travel behavior in urban areas is crucial in the formulation of mobility policies, especially considering the growing concern to reduce the use of private transport modes in favor of sustainable ones. Although multinomial logit models have been widely used in travel behavior research, machine learning models are becoming an interesting alternative to perform this task, in which tree-based ensemble models, such as random forest and gradient boosting models, have demonstrated superiority in accomplishing this goal, although they both have not been compared using household survey data. This paper compares different logit and machine learning models, with a specific emphasis on gradient boosting, random forest, and multinomial logit models to predict travel mode choice and to identify the determinants in travel behavior in an urban area for three transport modes (public transport, private transport, and walking/bike), using household survey data. Although the methodology is defined following the case and features of the metropolitan area of the Aburrá Valley-Colombia (MAAV), it can be applied to any urban area. The results show that an optimized gradient boosting model is able to predict travel mode choice in an urban area using household survey data, outperforming the other compared models. In addition, travel time, parking type at the destination, the number of motorized vehicles in the household (cars and motorbikes), age, and gender are features that explain the travel mode choice in the MAAV. The optimized gradient boosting model presented in this paper can be employed as a policy tool to study and analyze strategies to promote the reduction of the use of private transport modes in the MAAV and increase the use of more sustainable transport modes.",
        "DOI": "10.1061/(ASCE)UP.1943-5444.0000830",
        "paper_author": "Pineda-Jaramillo J.",
        "affiliation_name": "University of Luxembourg",
        "affiliation_city": "Esch-sur-Alzette",
        "affiliation_country": "Luxembourg",
        "affiliation_id": "60072562",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A reinforcement learning-based approach for imputing missing data",
        "publication": "Neural Computing and Applications",
        "citied_by": "15",
        "cover_date": "2022-06-01",
        "Abstract": "Missing data is a major problem in real-world datasets, which hinders the performance of data analytics. Conventional data imputation schemes such as univariate single imputation replace missing values in each column with the same approximated value. These univariate single imputation techniques underestimate the variance of the imputed values. On the other hand, multivariate imputation explores the relationships between different columns of data, to impute the missing values. Reinforcement Learning (RL) is a machine learning paradigm where the agent learns by taking actions and receiving rewards in response, to achieve its goal. In this work, we propose an RL-based approach to impute missing data by learning a policy to impute data through an action-reward-based experience. Our approach imputes missing values in a column by working only on the same column (similar to univariate single imputation) but imputes the missing values in the column with different values thus keeping the variance in the imputed values. We report superior performance of our approach, compared with other imputation techniques, on a number of datasets.",
        "DOI": "10.1007/s00521-022-06958-3",
        "paper_author": "Awan S.E.",
        "affiliation_name": "The University of Western Australia",
        "affiliation_city": "Perth",
        "affiliation_country": "Australia",
        "affiliation_id": "60031806",
        "affiliation_state": "WA"
    },
    {
        "paper_title": "Virtual machine migration policy for multi-tier application in cloud computing based on Q-learning algorithm",
        "publication": "Computing",
        "citied_by": "30",
        "cover_date": "2022-06-01",
        "Abstract": "Cloud computing technology provides shared computing which can be accessed over the Internet. When cloud data centers are flooded by end-users, how to efficiently manage virtual machines to balance both economical cost and ensure QoS becomes a mandatory work to service providers. Virtual machine migration feature brings a plenty of benefits to stakeholders such as cost, energy, performance, stability, availability. However, stakeholders’ objectives are usually conflict with each other. Furthermore, the optimal resource allocation problem in cloud infrastructure is usually NP-Hard or NP-Complete class. In this paper, the virtual migration problem is formulated by applying the game theory to ensure both load balance and resource utilization. The virtual machine migration algorithm, named V2PQL, is proposed based on Markov decision process and Q-learning algorithm. The results of the simulation demonstrate the efficiency of our proposal which are divided into training phase and extraction phase. The proposed V2PQL algorithm has been benchmarked to the Round-Robin, inverse Ant System, Max–Min Ant System, and Ant System algorithms in order to highlight its strength and feasibility in extraction phase.",
        "DOI": "10.1007/s00607-021-01047-0",
        "paper_author": "Tran C.H.",
        "affiliation_name": "Posts and Telecommunications Institute of Technology Vietnam",
        "affiliation_city": "Hanoi",
        "affiliation_country": "Viet Nam",
        "affiliation_id": "60071384",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "CPFinder: Finding an unknown caller's profession from anonymized mobile phone data",
        "publication": "Digital Communications and Networks",
        "citied_by": "2",
        "cover_date": "2022-06-01",
        "Abstract": "Identifying an unfamiliar caller's profession is important to protect citizens' personal safety and property. Owing to the limited data protection of various popular online services in some countries, such as taxi hailing and ordering takeouts, many users presently encounter an increasing number of phone calls from strangers. The situation may be aggravated when criminals pretend to be such service delivery staff, threatening the user individuals as well as the society. In addition, numerous people experience excessive digital marketing and fraudulent phone calls because of personal information leakage. However, previous works on malicious call detection only focused on binary classification, which does not work for the identification of multiple professions. We observed that web service requests issued from users' mobile phones might exhibit their application preferences, spatial and temporal patterns, and other profession-related information. This offers researchers and engineers a hint to identify unfamiliar callers. In fact, some previous works already leveraged raw data from mobile phones (which includes sensitive information) for personality studies. However, accessing users' mobile phone raw data may violate the more and more strict private data protection policies and regulations (e.g., General Data Protection Regulation). We observe that appropriate statistical methods can offer an effective means to eliminate private information and preserve personal characteristics, thus enabling the identification of the types of mobile phone callers without privacy concerns. In this paper, we develop CPFinder —- a system that exploits privacy-preserving mobile data to automatically identify callers who are divided into four categories of users: taxi drivers, delivery and takeouts staffs, telemarketers and fraudsters, and normal users (other professions). Our evaluation of an anonymized dataset of 1,282 users over a period of 3 months in Shanghai City shows that the CPFinder can achieve accuracies of more than 75.0% and 92.4% for multiclass and binary classifications, respectively.",
        "DOI": "10.1016/j.dcan.2021.08.003",
        "paper_author": "Zhang J.",
        "affiliation_name": "Georg-August-Universität Göttingen",
        "affiliation_city": "Gottingen",
        "affiliation_country": "Germany",
        "affiliation_id": "60031514",
        "affiliation_state": "Niedersachsen"
    },
    {
        "paper_title": "DECO: Joint Computation Scheduling, Caching, and Communication in Data-Intensive Computing Networks",
        "publication": "IEEE/ACM Transactions on Networking",
        "citied_by": "13",
        "cover_date": "2022-06-01",
        "Abstract": "Driven by technologies such as IoT-enabled health care, machine learning applications at the edge, and industrial automation, mobile edge and fog computing paradigms have reinforced a general trend toward decentralized computing, where any network node can route traffic, compute tasks, and store data, possibly at the same time. In many such computing environments, there is a need to cache significant amounts of data, which may include large data sets, machine learning models, or executable code. In this work, we propose a framework for joint computation scheduling, caching, and request forwarding within such decentralized computing environments. We first characterize the stability region of a 'genie-aided' computing network where data required by computation are instantly accessible, and develop a throughput optimal control policy for this model. Based on this, we develop a practically implementable distributed and adaptive algorithm, and show that it exhibits superior performance in terms of average task completion time, when compared to several baseline policies.",
        "DOI": "10.1109/TNET.2021.3136157",
        "paper_author": "Kamran K.",
        "affiliation_name": "Pinterest Inc.",
        "affiliation_city": "San Francisco",
        "affiliation_country": "United States",
        "affiliation_id": "60121301",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Modeling global real economic activity: Evidence from variable selection across quantiles",
        "publication": "Journal of Economic Asymmetries",
        "citied_by": "4",
        "cover_date": "2022-06-01",
        "Abstract": "We conduct an open search of predictors of global real economic activity. To this end, we apply a predictive quantile regression framework, using four alternative proxies of global real economic activity during February 1997–August 2019 and building on a combination of machine learning algorithms to identify their predictors out of 23 candidate explanatory variables. The contemporaneous level of global real economic activity, the Asian and US financial stress are found the most robust predictors. The effect of US financial shocks appears asymmetric, as they undermine global economic growth when the latter is below the median, but do not matter much when the world economy expands fast. Besides, US shadow interest rates are significantly and positively linked to global real economic activity. This effect holds in a high-growth regime of the world economy and suggests that rising US policy rates, contrary to the conventional wisdom, entail US dollar depreciation rather than appreciation. A weaker US dollar stimulates dollar-denominated cross-border bank inflows to the countries other than the USA, leading to a rise in real investment worldwide and industrial output growth. Thus, our empirical findings inform policymakers which indicators should be monitored more closely to predict future shifts in global economic growth and also provide certain insights about optimal policy responses to such shifts.",
        "DOI": "10.1016/j.jeca.2021.e00238",
        "paper_author": "Stolbov M.",
        "affiliation_name": "Moscow State Institute of International Relations (MGIMO)",
        "affiliation_city": "Moscow",
        "affiliation_country": "Russian Federation",
        "affiliation_id": "60032576",
        "affiliation_state": "Moscow Oblast"
    },
    {
        "paper_title": "Detecting market pattern changes: A machine learning approach",
        "publication": "Finance Research Letters",
        "citied_by": "7",
        "cover_date": "2022-06-01",
        "Abstract": "We train an artificial neural network (ANN) model to recognize the pattern of the financial market and use this model to detect whether and when the market pattern has changed. Over 2000–2021, we find that the market has experienced five significant changes. The timings of these changes coincide with critical historical events (e.g. Great Recession and COVID-19) and changes in the monetary policy regime.",
        "DOI": "10.1016/j.frl.2021.102621",
        "paper_author": "Mustafa A.A.",
        "affiliation_name": "Hiroshima University",
        "affiliation_city": "Higashihiroshima",
        "affiliation_country": "Japan",
        "affiliation_id": "60030788",
        "affiliation_state": "Hiroshima"
    },
    {
        "paper_title": "Proactive Content Caching Based on Actor-Critic Reinforcement Learning for Mobile Edge Networks",
        "publication": "IEEE Transactions on Cognitive Communications and Networking",
        "citied_by": "16",
        "cover_date": "2022-06-01",
        "Abstract": "Mobile edge caching/computing (MEC) has emerged as a promising approach for addressing the drastic increasing mobile data traffic by bringing high caching and computing capabilities to the edge of networks. Under MEC architecture, content providers (CPs) are allowed to lease some virtual machines (VMs) at MEC servers to proactively cache popular contents for improving users' quality of experience. The scalable cache resource model rises the challenge for determining the ideal number of leased VMs for CPs to obtain the minimum expected downloading delay of users at the lowest caching cost. To address these challenges, in this paper, we propose an actor-critic (AC) reinforcement learning based proactive caching policy for mobile edge networks without the prior knowledge of users' content demand. Specifically, we formulate the proactive caching problem under dynamical users' content demand as a Markov decision process and propose a AC based caching algorithm to minimize the caching cost and the expected downloading delay. Particularly, to reduce the computational complexity, a branching neural network is employed to approximate the policy function in the actor part. Numerical results show that the proposed caching algorithm can significantly reduce the total cost and the average downloading delay when compared with other popular algorithms.",
        "DOI": "10.1109/TCCN.2021.3130995",
        "paper_author": "Jiang W.",
        "affiliation_name": "Shenzhen University",
        "affiliation_city": "Shenzhen",
        "affiliation_country": "China",
        "affiliation_id": "60000937",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Machine learning for food security: Principles for transparency and usability",
        "publication": "Applied Economic Perspectives and Policy",
        "citied_by": "14",
        "cover_date": "2022-06-01",
        "Abstract": "Machine learning (ML) holds potential to predict hunger crises before they occur. Yet, ML models embed crucial choices that affect their utility. We develop a prototype model to predict food insecurity across three countries in sub-Saharan Africa. Readily available data on prices, assets, and weather all influence our model predictions. Our model obtains 55%–84% accuracy, substantially outperforming both a logit and ML models using only time and location. We highlight key principles for transparency and demonstrate how modeling choices between recall and accuracy can be tailored to policy-maker needs. Our work provides a path for future modeling efforts in this area.",
        "DOI": "10.1002/aepp.13214",
        "paper_author": "Zhou Y.",
        "affiliation_name": "University of Illinois Urbana-Champaign",
        "affiliation_city": "Urbana",
        "affiliation_country": "United States",
        "affiliation_id": "60000745",
        "affiliation_state": "IL"
    },
    {
        "paper_title": "Explainable artificial intelligence: a comprehensive review",
        "publication": "Artificial Intelligence Review",
        "citied_by": "362",
        "cover_date": "2022-06-01",
        "Abstract": "Thanks to the exponential growth in computing power and vast amounts of data, artificial intelligence (AI) has witnessed remarkable developments in recent years, enabling it to be ubiquitously adopted in our daily lives. Even though AI-powered systems have brought competitive advantages, the black-box nature makes them lack transparency and prevents them from explaining their decisions. This issue has motivated the introduction of explainable artificial intelligence (XAI), which promotes AI algorithms that can show their internal process and explain how they made decisions. The number of XAI research has increased significantly in recent years, but there lacks a unified and comprehensive review of the latest XAI progress. This review aims to bridge the gap by discovering the critical perspectives of the rapidly growing body of research associated with XAI. After offering the readers a solid XAI background, we analyze and review various XAI methods, which are grouped into (i) pre-modeling explainability, (ii) interpretable model, and (iii) post-modeling explainability. We also pay attention to the current methods that dedicate to interpret and analyze deep learning methods. In addition, we systematically discuss various XAI challenges, such as the trade-off between the performance and the explainability, evaluation methods, security, and policy. Finally, we show the standard approaches that are leveraged to deal with the mentioned challenges.",
        "DOI": "10.1007/s10462-021-10088-y",
        "paper_author": "Minh D.",
        "affiliation_name": "FPT University",
        "affiliation_city": "Hanoi",
        "affiliation_country": "Viet Nam",
        "affiliation_id": "60117455",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Estimating heterogeneous policy impacts using causal machine learning: a case study of health insurance reform in Indonesia",
        "publication": "Health Services and Outcomes Research Methodology",
        "citied_by": "13",
        "cover_date": "2022-06-01",
        "Abstract": "Policymakers seeking to target health policies efficiently towards specific population groups need to know which individuals stand to benefit the most from each of these policies. While traditional approaches for subgroup analyses are constrained to only consider a small number of pre-defined subgroups, recently proposed causal machine learning (CML) approaches help explore treatment-effect heterogeneity in a more flexible yet principled way. Causal forests use a generalisation of the random forest algorithm to estimate heterogenous treatment effects both at the individual and the subgroup level. Our paper aims to explore this approach in the setting of health policy evaluation with strong observed confounding, applied specifically to the context of mothers’ health insurance enrolment in Indonesia. Comparing two health insurance schemes (subsidised and contributory) against no insurance, we find beneficial average impacts of enrolment in contributory health insurance on maternal health care utilisation and infant mortality, but no impact of subsidised health insurance. The causal forest algorithm identified significant heterogeneity in the impacts of contributory insurance, not just along socioeconomic variables that we pre-specified (indicating higher benefits for poorer, less educated, and rural women), but also according to some other characteristics not foreseen prior to the analysis, suggesting in particular important geographical impact heterogeneity. Our study demonstrates the power of CML approaches to uncover unexpected heterogeneity in policy impacts. The findings from our evaluation of past health insurance expansions can potentially guide the re-design of the eligibility criteria for subsidised health insurance in Indonesia.",
        "DOI": "10.1007/s10742-021-00259-3",
        "paper_author": "Kreif N.",
        "affiliation_name": "University of York",
        "affiliation_city": "York",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60016418",
        "affiliation_state": "North Yorkshire"
    },
    {
        "paper_title": "A Cooperative Memetic Algorithm with Learning-Based Agent for Energy-Aware Distributed Hybrid Flow-Shop Scheduling",
        "publication": "IEEE Transactions on Evolutionary Computation",
        "citied_by": "129",
        "cover_date": "2022-06-01",
        "Abstract": "With increasing environmental awareness and energy requirement, sustainable manufacturing has attracted growing attention. Meanwhile, distributed manufacturing systems have become emerging due to the development of globalization. This article addresses the energy-aware distributed hybrid flow-shop scheduling (EADHFSP) with minimization of makespan and energy consumption simultaneously. We present a mixed-integer linear programming model and propose a cooperative memetic algorithm (CMA) with a reinforcement learning (RL)-based policy agent. First, an encoding scheme and a reasonable decoding method are designed, considering the tradeoff between two conflicting objectives. Second, two problem-specific heuristics are presented for hybrid initialization to generate diverse solutions. Third, solutions are refined with appropriate improvement operator selected by the RL-based policy agent. Meanwhile, an effective solution selection method based on the decomposition strategy is utilized to balance the convergence and diversity. Fourth, an intensification search with multiple problem-specific operators is incorporated to further enhance the exploitation capability. Moreover, two energy-saving strategies are designed for improving the nondominated solutions. The effect of parameter setting is investigated and extensive numerical tests are carried out. The comparative results demonstrate that the special designs are effective and the CMA is superior to the existing algorithms in solving the EADHFSP.",
        "DOI": "10.1109/TEVC.2021.3106168",
        "paper_author": "Wang J.J.",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60025278",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Methods for Small Area Population Forecasts: State-of-the-Art and Research Needs",
        "publication": "Population Research and Policy Review",
        "citied_by": "39",
        "cover_date": "2022-06-01",
        "Abstract": "Small area population forecasts are widely used by government and business for a variety of planning, research and policy purposes, and often influence major investment decisions. Yet, the toolbox of small area population forecasting methods and techniques is modest relative to that for national and large subnational regional forecasting. In this paper, we assess the current state of small area population forecasting, and suggest areas for further research. The paper provides a review of the literature on small area population forecasting methods published over the period 2001–2020. The key themes covered by the review are extrapolative and comparative methods, simplified cohort-component methods, model averaging and combining, incorporating socioeconomic variables and spatial relationships, ‘downscaling’ and disaggregation approaches, linking population with housing, estimating and projecting small area component input data, microsimulation, machine learning, and forecast uncertainty. Several avenues for further research are then suggested, including more work on model averaging and combining, developing new forecasting methods for situations which current models cannot handle, quantifying uncertainty, exploring methodologies such as machine learning and spatial statistics, creating user-friendly tools for practitioners, and understanding more about how forecasts are used.",
        "DOI": "10.1007/s11113-021-09671-6",
        "paper_author": "Wilson T.",
        "affiliation_name": "Melbourne School of Population and Global Health",
        "affiliation_city": "Melbourne",
        "affiliation_country": "Australia",
        "affiliation_id": "60118547",
        "affiliation_state": "VIC"
    },
    {
        "paper_title": "An Optimal Computing Budget Allocation Tree Policy for Monte Carlo Tree Search",
        "publication": "IEEE Transactions on Automatic Control",
        "citied_by": "14",
        "cover_date": "2022-06-01",
        "Abstract": "We analyze a tree search problem with an underlying Markov decision process, in which the goal is to identify the best action at the root that achieves the highest cumulative reward. We present a new tree policy that optimally allocates a limited computing budget to maximize a lower bound on the probability of correctly selecting the best action at each node. Compared to widely used upper confidence bound (UCB) tree policies, the new tree policy presents a more balanced approach to manage the exploration and exploitation tradeoff when the sampling budget is limited. Furthermore, UCB assumes that the support of reward distribution is known, whereas our algorithm relaxes this assumption. Numerical experiments demonstrate the efficiency of our algorithm in selecting the best action at the root.",
        "DOI": "10.1109/TAC.2021.3088792",
        "paper_author": "Li Y.",
        "affiliation_name": "A. James Clark School of Engineering",
        "affiliation_city": "College Park",
        "affiliation_country": "United States",
        "affiliation_id": "60078684",
        "affiliation_state": "MD"
    },
    {
        "paper_title": "Emerging technologies in Indian mining industry: an exploratory empirical investigation regarding the adoption challenges",
        "publication": "Journal of Science and Technology Policy Management",
        "citied_by": "23",
        "cover_date": "2022-06-01",
        "Abstract": "Purpose: Emerging technologies have been transforming most industries. A wide range of emerging technologies such as blockchain, internet of things (IoT), artificial intelligence (AI), machine learning (ML), robotics and many others have changed the way in which firm value chain activities or processes were executed traditionally. The mining industry has also witnessed the introduction of these emerging technologies in various processes from the exploration stage to the final processing of ores. The purpose of this paper is to understand the pace of adoption of emerging technologies in the Indian mining industry and identify the challenges that managers confront while adopting emerging technologies. Design/methodology/approach: The authors undertook qualitative research. Data collection was done in two stages. Secondary research was conducted to arrive at a repository of use cases of the adoption of emerging technologies in the global mining industry. Primary data collection was also done. The insights on emerging technology adoption and challenges faced in the Indian mining industry were captured by in-depth interviewing of subject matter experts. The authors interviewed 21 mining subject matter experts with a semi-structured open-ended questionnaire. The responses were content analyzed by thematic content analysis. Technological-organizational-environmental (TOE) and diffusion of innovation (DOI) frameworks were applied to segregate different factors affecting the adoption of emerging technologies in the Indian mining industry. Findings: Emerging technologies such as blockchain, IoT, AI, ML, robotics has been applied across various mining engineering value chain activities such as in drilling, blasting, excavation and ore hauling. However, emerging technologies adoption was hindered because of a lack of managerial awareness, cultural inertia, substantive upfront investments and the nature of intangible benefits in the short run. Research limitations/implications: The research applied technology adoption frameworks in the mining industry. The authors used TOE and DOI frameworks to understand the challenges faced by Indian mining firms. The research findings, thus added to the conversation of TOE and DOI frameworks in the context of the Indian mining industry. Practical implications: The research finding would help mining firm managers to anticipate the challenges with respect to technology adoption. This would allow mining executives to create a proper technology adoption plan and intervene proactively. The research would also provide information about the steps taken by competing firms with respect to emerging technologies adoption. The research would help managers to decide technology implementation steps in drilling, blasting, excavation and ore hauling to be undertaken for successful adoption of emerging technologies. Technology firms could gain insights into the issues faced by mining firms in adopting emerging technologies. This research would help managers to influence organizational technology policy and endorse the addition of pro-technology policies in mining activities. Policymakers involved in the mining sector could also incorporate industry-level policy decisions so as to facilitate the adoption of emerging technologies among mining firms and remove the barriers to the adoption of emerging technologies. This would create an opportunity for technology providers to redesign product offerings, which could be a good fit for Indian mining firms. Originality/value: Indian mining industry contributed significantly to the Indian economy. Despite this, limited focus has been put regarding the adoption of emerging technologies in the mining industry. Mining managers did not have any framework to understand the challenges faced in the adoption of technologies across the mining value chain that is in drilling, blasting, excavation and ore hauling. This study focused on identifying those challenges through the use of technology adoption frameworks. This research was one of the first studies to gain insights on emerging technologies adoption in the context of the mining industry through the theoretical lens of TOE and DOI frameworks.",
        "DOI": "10.1108/JSTPM-03-2021-0048",
        "paper_author": "Bhattacharyya S.S.",
        "affiliation_name": "Indian Institute of Management Mumbai",
        "affiliation_city": "Mumbai",
        "affiliation_country": "India",
        "affiliation_id": "60022123",
        "affiliation_state": "MH"
    },
    {
        "paper_title": "Integrated production and maintenance planning under uncertain demand with concurrent learning of yield rate",
        "publication": "Flexible Services and Manufacturing Journal",
        "citied_by": "10",
        "cover_date": "2022-06-01",
        "Abstract": "Strong interactions between decisions in the maintenance and production scheduling domains, and their impacts on the equipment yield rates necessitate maintenance and production decisions being optimized concurrently, with considerations of yield dependencies on the equipment conditions and production rates. This paper proposes an integrated decision-making policy for production and maintenance operations on a single machine under uncertain demand, with concurrent considerations and learning of yield dependencies on the equipment conditions and production rates. This policy is obtained through a two-stage stochastic programming model, which considers the variable demand, machine degradation, and maintenance times. This model incorporates outsourcing decisions and operational decisions regarding reworking, scraping of imperfect products to ensure the demand is adequately met. A closed-form reinforcement learning method is utilized to learn yield dependencies. Simulations confirm the necessity of yield learning and show the proposed method outperforms the traditional, fragmented approaches where the effects of production rates and machine conditions on the resulting yield rates are not considered. The two-stage stochastic setting is demonstrated by comparing with the traditional one-stage deterministic approach, where decisions are made based on the expected demand and production performance, with scrapping, reworking, and outsourcing decisions established before the demand and production performance are observed.",
        "DOI": "10.1007/s10696-021-09417-8",
        "paper_author": "Zhang H.",
        "affiliation_name": "The University of Texas at Austin",
        "affiliation_city": "Austin",
        "affiliation_country": "United States",
        "affiliation_id": "60013372",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "Future ecosystem service value modeling with land cover dynamics by using machine learning based Artificial Neural Network model for Jashore city, Bangladesh",
        "publication": "Physics and Chemistry of the Earth",
        "citied_by": "55",
        "cover_date": "2022-06-01",
        "Abstract": "Land Use/Land Cover (LULC) provides provisional, supporting, cultural, and regulating ecosystem services that contribute to ecological environments, enhance human health and living, have economic advantages for sustaining living organisms. LULC transformation due to enormous urban expansion diminishing Ecosystem Services Values (ESVs) and discouraging sustainability. Though unplanned LULC transformation practice became more prevalent in developing countries, comprehensive assessment of LULC changes and their influences in ESVs are rarely attempted. This study aimed to illustrate and forecast the LULC changes and their influences on ESVs change in Jashore using remote sensing technologies. ESVs estimation and change analysis were conducted by utilizing -derived LULC data of the year 2000, 2010, and 2020 with the corresponding global value coefficients of each LULC type which are previously published. For simulating future LULC and ESVs, Land Change Modeler of TerrSet Geospatial Monitoring and Modeling Software was used in Multi-Layer Perceptron-Markov Chain and Artificial Neural Network method. The decline of agricultural land by 13.13% and waterbody by 5.79% has resulted in the reduction of total ESVs US$0.23 million (24.47%) during 2000–2020. The forecasted result shows that the built-up area will be dominant LULC in the future, and ESVs of provisioning and cultural services will be diminished by $0.107 million, $63400.3 by 2050 with the declination of agricultural, waterbody, vegetation, and vacant land covers. The study signifies the importance of a strategic rational land-use plan to strictly monitor and control the encroachment of built-up areas into vegetation, waterbodies, and agricultural land in addition to scientific mitigative policies for ensuring ecological sustainability.",
        "DOI": "10.1016/j.pce.2021.103021",
        "paper_author": "Morshed S.R.",
        "affiliation_name": "Khulna University of Engineering and Technology",
        "affiliation_city": "Khulna",
        "affiliation_country": "Bangladesh",
        "affiliation_id": "60025301",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Comparison of Self-Play Algorithms Under a Generalized Framework",
        "publication": "IEEE Transactions on Games",
        "citied_by": "9",
        "cover_date": "2022-06-01",
        "Abstract": "The notion of self-play, albeit often cited in multiagent reinforcement learning as a process by which to train agent policies from scratch, has received little efforts to be taxonomized within a formal model. We present a formalized framework, with clearly defined assumptions, which encapsulates the meaning of self-play as abstracted from various existing self-play algorithms. This framework is framed as an approximation to a theoretical solution concept for multiagent training. Through a novel qualitative visualization metric, on a simple environment, we show that different self-play algorithms generate different distributions of episode trajectories, leading to different explorations of the policy space by the learning agents. Quantitatively, on two environments, we analyze the learning dynamics of policies trained under different self-play algorithms captured under our framework and perform cross self-play performance comparisons. Our results indicate that, throughout training, various widely used self-play algorithms exhibit cyclic policy evolutions and that the choice of self-play algorithm significantly affects the final performance of trained agents.",
        "DOI": "10.1109/TG.2021.3058898",
        "paper_author": "Hernandez D.",
        "affiliation_name": "University of York",
        "affiliation_city": "York",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60016418",
        "affiliation_state": "North Yorkshire"
    },
    {
        "paper_title": "Adaptive Lower-Level Driven Compaction to Optimize LSM-Tree Key-Value Stores",
        "publication": "IEEE Transactions on Knowledge and Data Engineering",
        "citied_by": "12",
        "cover_date": "2022-06-01",
        "Abstract": "Log-structured merge (LSM) tree key-value (KV) stores have been widely deployed in many NoSQL and SQL systems, serving online big data applications such as social networking, graph processing, machine learning, etc. The batch processing of sorted data merging (i.e., compaction) in LSM-tree key-value stores improves the write efficiency, and some lazy compaction methods have been proposed to accumulate more data within a batch. However, these batched writing methods lead to significant tail latency, which is unacceptable for online processing. Aiming to optimize both latency and throughput, we propose a novel Lower-level Driven Compaction (LDC) method which breaks the limitations of the traditional upper-level driven compaction manner and triggers practical compaction actions bottom-up, with the benefits of both decreasing the compaction granularity for smaller latency and reducing write amplification for higher throughput. Furthermore, we extend LDC to Adaptive LDC (ALDC) by adding an adaptive policy to adjust the key compaction threshold to fit the changes of workloads' features. The experimental results indicate that ALDC reduces the tail latency significantly and meanwhile achieves a much higher and stable throughput compared with existing approaches.",
        "DOI": "10.1109/TKDE.2020.3019264",
        "paper_author": "Chai Y.",
        "affiliation_name": "Key Laboratory of Data Engineering and Knowledge Engineering",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60125536",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Nipples, memes, and algorithmic failure: NSFW critique of Tumblr censorship",
        "publication": "New Media and Society",
        "citied_by": "34",
        "cover_date": "2022-06-01",
        "Abstract": "In November 2018, after being suspended from Apple’s App Store for hosting child pornography, Tumblr announced its decision to ban all NSFW (not safe/suitable for work) content with the aid of machine-learning classification. The decision to opt for strict terms of use governing nudity and sexual depiction was as fast as it was drastic, leading to the quick erasure of subcultural networks developed over a decade. This article maps out platform critiques of and on Tumblr through a combination of visual and digital methods. By analyzing 7306 posts made between November 2018 (when Tumblr announced its new content policy) and August 2019 (when Verizon sold Tumblr to Automattic), we explore the key stakes and forms of user resistance to Tumblr “porn ban” and the affective capacities of user-generated content to mobilize engagement.",
        "DOI": "10.1177/1461444820979280",
        "paper_author": "Pilipets E.",
        "affiliation_name": "Universität Klagenfurt",
        "affiliation_city": "Klagenfurt",
        "affiliation_country": "Austria",
        "affiliation_id": "60019660",
        "affiliation_state": "Carinthia"
    },
    {
        "paper_title": "Anomaly traffic detection and correlation in Smart Home automation IoT systems",
        "publication": "Transactions on Emerging Telecommunications Technologies",
        "citied_by": "12",
        "cover_date": "2022-06-01",
        "Abstract": "Smart building automation systems are increasingly the target of hacking attacks. Moreover, they may be used as a tool for attacks against targets located out of the native Home Area Network (HAN). These attacks are often resulted in changes in traffic volume, damaged packets, increased message traffic, and so on. Symptoms of attacks can be detected as anomalies in traffic model and recognized by a software agent run on Home Gateway. Although these anomalies are detected locally, it may help network provider to protect his resources as well as other resources of his clients. For that purpose, network operator should be able to recognize anomalies and correlate them on the network level. In this way, the network operator has the ability to protect both its own network and HANs of its clients. This article shows that Smart Home security might be coupled with the providers' network security policy. For that reason, security tasks should be performed both in HAN and providers' data center. This article describes a novel strategy for anomaly detection that provides shared responsibility between a service client and the network provider. It uses a machine learning approach for classifying the monitoring data and correlation in searching suspicious behavior across the network resources at the service provider's data center.",
        "DOI": "10.1002/ett.4053",
        "paper_author": "Gajewski M.",
        "affiliation_name": "Internet Technologies and Applications Department",
        "affiliation_city": "Warsaw",
        "affiliation_country": "Poland",
        "affiliation_id": "124893665",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "On the relationship between oil and gas markets: a new forecasting framework based on a machine learning approach",
        "publication": "Annals of Operations Research",
        "citied_by": "20",
        "cover_date": "2022-06-01",
        "Abstract": "Owing to the uncertainty around the coupling and decoupling of oil and gas prices, this study re-examines the relationship between oil and gas markets by modeling the price of one energy source based on the price of the other, both linearly and nonlinearly. We present an autoregressive exogenous model and three nonlinear frameworks with different patterns of asymmetry. Based on daily data from January 7, 1997, to December 29, 2017, our analysis reaches two main findings. First, the nonlinear frameworks outperform the linear model (i.e., the autoregressive exogenous model) in modeling the relationship between oil and gas prices. Second, the nature of asymmetry varies based on market direction. We show that when oil prices exhibit an extreme movement (i.e., beyond a threshold value in absolute value), gas prices react nonlinearly, and that there is no relationship otherwise. Our results are robust for other frequencies, mainly weekly and monthly. These findings explain the conflicting results in the literature on the complex relationship between these markets. The results might serve investors in term of hedging, portfolio diversification, and asset allocation as we show that in the calm period, there is no relationship between oil and gas prices; however, the interaction between markets is more pronounced during periods of extreme movement. Similarly, policymakers’ awareness of the nonlinear dynamic under extreme movements could inform the regulation policy and/or adjustment in case oil (gas) prices increase or decrease.",
        "DOI": "10.1007/s10479-020-03652-2",
        "paper_author": "Ftiti Z.",
        "affiliation_name": "PSB Paris School of Business",
        "affiliation_city": "Paris",
        "affiliation_country": "France",
        "affiliation_id": "60158043",
        "affiliation_state": "Ile-de-France"
    },
    {
        "paper_title": "A Strategy of Assessing Climate Factors' Influence for Agriculture Output",
        "publication": "KSII Transactions on Internet and Information Systems",
        "citied_by": "1",
        "cover_date": "2022-05-31",
        "Abstract": "Due to the Internet of Things popularity, many agricultural data are collected by sensors automatically. The abundance of agricultural data makes precise prediction of rice yield possible. Because the climate factors have an essential effect on the rice yield, we considered the climate factors in the prediction model. Accordingly, this paper proposes a machine learning model for rice yield prediction in Taiwan, including the genetic algorithm and support vector regression model. The dataset of this study includes the meteorological data from the Central Weather Bureau and rice yield of Taiwan from 2003 to 2019. The experimental results show the performance of the proposed model is nearly 30% better thanMARS, RF, ANN, and SVR models. The most important climate factors affecting the rice yield are the total sunshine hours, the number of rainfall days, and the temperature. The proposed model also offers three advantages: (a) the proposed model can be used in different geographical regions with high prediction accuracies; (b) the proposed model has a high explanatory ability because it could select the important climate factors which affect rice yield; (c) the proposed model is more suitable for predicting rice yield because it provides higher reliability and stability for predicting. The proposed model can assist the government in making sustainable agricultural policies.",
        "DOI": "10.3837/TIIS.2022.05.001",
        "paper_author": "Kuan C.H.",
        "affiliation_name": "National Taiwan University of Science and Technology",
        "affiliation_city": "Taipei",
        "affiliation_country": "Taiwan",
        "affiliation_id": "60027709",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Adaptive Malware Control: Decision-Based Attacks in the Problem Space of Dynamic Analysis",
        "publication": "WoRMA 2022 - Proceedings of the 1st Workshop on Robust Malware Analysis",
        "citied_by": "1",
        "cover_date": "2022-05-30",
        "Abstract": "Adversarial malware have been widely explored, most often on static analysis based detection and feature space manipulations. With the prevalence of encryption, obfuscation, and packing, dynamic behavior is considered much more revealing of a program's nature. At the same time, defining and performing attacks through the feature representation of malware faces several obstacles, especially in dynamic analysis. However, if program behavior is both malleable and indicative of malicious intent, we concern ourselves with the question of how it can be adaptively controlled in order to evade detection. In this work, we redefine adversarial attacks on malware behavior so that they can be performed directly by the original binary and thus obviate the need to compute gradients through feature representations. We theoretically prove that this can occur even in the fully black-box case where only the final, hard-label decision is disclosed. Furthermore, we empirically evaluate our approach by training state-of-the-art sequence models for detecting malware behavior, constructing several malware manipulation environments, and training a host of reinforcement learning (RL) agents on them that learn evasive policies through interaction. Finally, we utilize the adversarial behavior learned by the RL agents to adversarially train the original detection models and we show that while an indispensable approach, the degree of robustness it imparts can be deceptive; especially when we consider adversaries with broader action sets.",
        "DOI": "10.1145/3494110.3528243",
        "paper_author": "Tsingenopoulos I.",
        "affiliation_name": "KU Leuven",
        "affiliation_city": "Leuven",
        "affiliation_country": "Belgium",
        "affiliation_id": "60025063",
        "affiliation_state": "Vlaams-Brabant"
    },
    {
        "paper_title": "RANDOM FOREST FOR CLASSIFYING AND MONITORING 50 YEARS OF VEGETATION DYNAMICS IN THREE DESERT CITIES OF THE UAE",
        "publication": "International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",
        "citied_by": "5",
        "cover_date": "2022-05-30",
        "Abstract": "The United Arab Emirates (UAE), a dryland country, has since its independence, emphasized on giant greening projects. Monitoring the trend of greening progress in the UAE has gained importance for environmental management and carbon footprint monitoring. Hence, this study created and analysed a time-series (TS) vegetation map to track and analyse vegetation dynamics over an extended period of fifty years. Study area included three selected desert cities of the UAE, Abu Dhabi (AD) capital city, Dubai city and Al Ain city. Random Forest algorithm was applied on Landsat multi-temporal images from 1972 until 2021 for classifying and monitoring the vegetation dynamics and change trajectories. Four vegetation subclasses (coastal/wetland vegetation, urban vegetation, farms/crop fields, and natural/artificial forests), were assessed then grouped and mapped as one vegetation class. With the adopted approach, we achieved overall classification accuracy ranging from 86% to 94%, with kappa coefficients ranging from 0.7200 to 0.8800. Current study showed that the vegetation cover extent in the UAE was at a constant growth for the past five decades from only 1,231.1 ha in 1972 to 23,176.46 ha in 2021, 19 times folds. Furthermore, it showed that desert cities tend to increase their vegetation cover while continuing their steady urban growth. The other drivers found include demographic increase and governmental policies (granting farms to locals and environmental protection laws). Finally, the approach implemented in this research can effectively and reliably be used in other urban centres for future monitoring and management of the vegetation cover status in the country.",
        "DOI": "10.5194/isprs-archives-XLIII-B3-2022-69-2022",
        "paper_author": "Dahy B.",
        "affiliation_name": "United Arab Emirates University",
        "affiliation_city": "Al Ain",
        "affiliation_country": "United Arab Emirates",
        "affiliation_id": "60008665",
        "affiliation_state": "Abu Dhabi"
    },
    {
        "paper_title": "FILTERING LPIS DATA FOR BUILDING TRUSTWORTHY TRAINING DATASETS FOR CROP TYPE MAPPING: A CASE STUDY IN GREECE",
        "publication": "International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",
        "citied_by": "3",
        "cover_date": "2022-05-30",
        "Abstract": "The need for effective crop monitoring in large geographical scales has become increasingly important in recent years and constitutes a technological and scientific challenge for remote sensing applications. In Europe, member states of the European Union collect geospatial data in the framework of the Land Parcel Information System (LPIS) for agricultural management and subsidizing farmers. These data can be exploited as training datasets of machine learning classifiers for crop-type mapping applications. However, the way the LPIS data are being generated, concerning primarily errors in the farmers' declarations in terms of crop-type labels, exact geometries, etc, constrains their direct use in such classification frameworks. In this study, we present and assess a methodology for filtering LPIS data based on geometric and spectral criteria in order to build a trustworthy training dataset for machine learning crop-type classifiers. A new nomenclature was developed, oriented towards the spectral discrimination of crop-type classes and sub-classes in Greece. The filtering methodology was applied at national scale for the agricultural year of 2019 and resulted in the selection of a sub-set of the LPIS parcels that were assessed as the most suitable and reliable to represent the different levels of the nomenclature. The developed filtering procedure was validated against actual crop-type labels derived from field visits, while the resulted filtered data were successfully utilized on various crop-type mapping experiments in Greece.",
        "DOI": "10.5194/isprs-archives-XLIII-B3-2022-871-2022",
        "paper_author": "Gounari O.",
        "affiliation_name": "National Technical University of Athens (NTUA)",
        "affiliation_city": "Athens",
        "affiliation_country": "Greece",
        "affiliation_id": "60002947",
        "affiliation_state": "Attica"
    },
    {
        "paper_title": "China Carbon Neutralization Research Status and Research Frontier Tracking",
        "publication": "Frontiers in Environmental Science",
        "citied_by": "22",
        "cover_date": "2022-05-26",
        "Abstract": "In the context of global joint response to climate change, carbon neutralization has become one of the main measures for governments, enterprises, and even individuals around the world to deal with global warming. This article explores the current research status and frontier trends in the field of carbon neutralization, which can not only provide theoretical reference for the follow-up research of carbon neutralization but also provide useful ideas for the policy-making of carbon neutralization. The study is built on the China National Knowledge Infrastructure (CNKI) Core Journals Database, the Chinese Social Sciences Citation Index database, and the Chinese Science Citation Database as the literature retrieval database platform. Using CiteSpace optical measurement software, 370 pieces of literature in the field of carbon neutrality research in China were systematically analyzed to track the situation and impact of research by various research institutions and prominent authors in China in this field and to analyze the research hotspots in this field, which is of great significance for the follow-up research of carbon neutrality. The research results show that 1) the number of publications in China’s carbon-neutral field has increased significantly with the change of years and has strong development potential. 2) Journals with many articles are more concentrated and are generally published in three journals: Natural Gas Industry, Proceedings of the CSEE, and Chinese Science Bulletin. 3) In terms of the major research institutions, the Institute of National Conditions of Tsinghua University, the School of Public Administration of Tsinghua University, the School of Grammar of Beijing University of Technology, and the Economic Development Research Center of the State Forestry and Grassland Administration have published more articles and muscular scientific research strength in the field of carbon neutrality. 4) In terms of the principal authors, four typical research teams have been set up: low-carbon economy, low-carbon cycle, carbon trading, sewage carbon neutrality, environmental science, and resource utilization, and the authors of each research team cooperate closely. 5) In the future, the research hotspots in China’s carbon-neutral field are still the themes of carbon emission reduction, green and low carbon, new energy, etc., and the research hotspots have begun to shift to the concept of development, from the very beginning of environmental protection and carbon emission reduction to the current new energy development. Carbon neutrality is currently a hot research field in China, and in the future, special attention should be paid to the full use of machine learning and extensive data mining to solve complex social, economic, and ecological problems. It is suggested that China should introduce the concepts of “common but differentiated responsibilities” and “consumer responsibility” in the international climate change negotiations in the formulation of carbon neutralization policies, so as to fully protect the right to the development of China’s underdeveloped regions and avoid the injustice caused by the blind formulation of carbon neutralization policies. Systematic Review Registration: [https://review.frontiersin.org/review/896524/16/1295417/#tab/History].",
        "DOI": "10.3389/fenvs.2022.896524",
        "paper_author": "Shi X.",
        "affiliation_name": "Shenyang Agricultural University",
        "affiliation_city": "Shenyang",
        "affiliation_country": "China",
        "affiliation_id": "60002299",
        "affiliation_state": "Liaoning"
    },
    {
        "paper_title": "Copyright in generative deep learning",
        "publication": "Data and Policy",
        "citied_by": "35",
        "cover_date": "2022-05-25",
        "Abstract": "Machine-generated artworks are now part of the contemporary art scene: they are attracting significant investments and they are presented in exhibitions together with those created by human artists. These artworks are mainly based on generative deep learning (GDL) techniques, which have seen a formidable development and remarkable refinement in the very recent years. Given the inherent characteristics of these techniques, a series of novel legal problems arise. In this article, we consider a set of key questions in the area of GDL for the arts, including the following: is it possible to use copyrighted works as training set for generative models? How do we legally store their copies in order to perform the training process? Who (if someone) will own the copyright on the generated data? We try to answer these questions considering the law in force in both the United States and the European Union, and potential future alternatives. We then extend our analysis to code generation, which is an emerging area of GDL. Finally, we also formulate a set of practical guidelines for artists and developers working on deep learning generated art, as well as some policy suggestions for policymakers.",
        "DOI": "10.1017/dap.2022.10",
        "paper_author": "Franceschelli G.",
        "affiliation_name": "Alma Mater Studiorum Università di Bologna",
        "affiliation_city": "Bologna",
        "affiliation_country": "Italy",
        "affiliation_id": "60028218",
        "affiliation_state": "BO"
    },
    {
        "paper_title": "Evaluating Privacy Policy for Mobile Health APPs with Machine Learning",
        "publication": "Data Analysis and Knowledge Discovery",
        "citied_by": "0",
        "cover_date": "2022-05-25",
        "Abstract": "[Objective] This paper analyzes privacy policies for mobile health APPs in China with machine learning, aiming to improve the efficiency and accuracy of compliance evaluation. [Methods] First, we constructed the evaluation system for the privacy policy compliance of mobile health APPs according to relevant policies and regulations. Then, based on the hard voting classifier, we established the compliance evaluation model integrating three machine learning algorithms: CNN, RNN and LSTM. Finally, we examined our model using 1210 mobile health APPs from the Android APP market, and evaluated the compliance of their privacy policies. [Results] The overall compliance of the privacy policies for mobile health APPs was poor. There are many violations in the six evaluation criteria. The compliance scores of online medical APPs, medical service APPs, health management APPs, and medical information APPs were 0.63, 0.59, 0.61and 0.66. [Limitations] Due to the limited amount of annotated privacy policy data, the proposed model may not be able to fully learn the features of evaluation indicators. [Conclusions] This proposed model could conduct large-scale, fine-grained automatic evaluation of the compliance of APPs privacy policies. It also provides new ideas and methods for the government agencies and APP operators to improve decision making.",
        "DOI": "10.11925/infotech.2096-3467.2021.0897",
        "paper_author": "Yang Z.",
        "affiliation_name": "Wuhan University",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60029306",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "The biotechnology sector in a latecomer country: The case of Poland",
        "publication": "New Biotechnology",
        "citied_by": "4",
        "cover_date": "2022-05-25",
        "Abstract": "This paper offers a detailed firm-level analysis of innovation strategies, exporting activities and employment and sales dynamics of Polish biotechnology companies. The study is based on the unique dataset provided by three runs of the Polish edition of the Community Innovation Survey (CIS). Poland is a latecomer in the biotechnology industry, as are all the Central and Eastern European Countries (CEECs). In addition, the Polish biotechnology sector faced relatively unfavorable starting conditions: no partners in the pharmaceutical industry and, unlike some other CEECs, little government support. Nevertheless, biotechnology companies have developed in several Polish industries. It is shown that these companies implement innovation strategies that are typical for the biotechnology sector; however, they do not cooperate much with their clients. The innovation expenditure of biotechnology companies is distinctively higher than that of other companies. In addition, government and EU grants play a crucial role in funding the R&D activities of Polish biotechnology companies. This reflects the change in the innovation policy in Poland, but also raises the question of the sustainability of these efforts. Biotechnology companies are strongly export-oriented but, contrary to expectations, their dynamics are average: when controlling for firm characteristics and export performance, biotechnology companies neither outperform nor underperform other companies in terms of sales or employment growth.",
        "DOI": "10.1016/j.nbt.2022.01.008",
        "paper_author": "Szczygielski K.",
        "affiliation_name": "University of Warsaw",
        "affiliation_city": "Warsaw",
        "affiliation_country": "Poland",
        "affiliation_id": "60013756",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "The digital agricultural revolution: Innovations and challenges in agriculture through technology disruptions",
        "publication": "The Digital Agricultural Revolution: Innovations and Challenges in Agriculture through Technology Disruptions",
        "citied_by": "0",
        "cover_date": "2022-05-20",
        "Abstract": "The book integrates computational intelligence, applied artificial intelligence, and modern agricultural practices and will appeal to scientists, agriculturists, and those in plant and crop science management. There is a need for synergy between the application of modern scientific innovation in the area of artificial intelligence and agriculture, considering the major challenges from climate change consequences viz. rising temperatures, erratic rainfall patterns, the emergence of new crop pests, drought, flood, etc. This volume reports on high-quality research (theory and practice including prototype & conceptualization of ideas, frameworks, real-world applications, policy, standards, psychological concerns, case studies, and critical surveys) on recent advances toward the realization of the digital agriculture revolution as a result of the convergence of different disruptive technologies. The book touches upon the following topics which have contributed to revolutionizing agricultural practices. Applications of Artificial Intelligence in Agriculture (AI models and architectures, system design, real-world applications of AI, machine learning and deep learning in the agriculture domain, integration & coordination of systems and issues & challenges). IoT and Big Data Analytics Applications in Agriculture (theory & architecture and the use of various types of sensors in optimizing agriculture resources and final product, benefits in real-time for crop acreage estimation, monitoring & control of agricultural produce). Robotics & Automation in Agriculture Systems (Automation challenges, need and recent developments and real case studies). Intelligent and Innovative Smart Agriculture Applications (use of hybrid intelligence in better crop health and management). Privacy, Security, and Trust in Digital Agriculture (government framework & policy papers). Open Problems, Challenges, and Future Trends.",
        "DOI": "10.1002/9781119823469",
        "paper_author": "Bhatnagar R.",
        "affiliation_name": "Manipal University Jaipur",
        "affiliation_city": "Jaipur",
        "affiliation_country": "India",
        "affiliation_id": "60108737",
        "affiliation_state": "RJ"
    },
    {
        "paper_title": "A Machine Learning-Driven Analysis of Phantom E911 Calls",
        "publication": "WiseML 2022 - Proceedings of the 2022 ACM Workshop on Wireless Security and Machine Learning",
        "citied_by": "1",
        "cover_date": "2022-05-19",
        "Abstract": "Phantom Enhanced 911 (E911) calls are automatically generated 2 second calls, are a serious concern on cellular networks, and consume critical resources. As networks become increasingly complex, detecting and troubleshooting the causes of phantom E911 calls is becoming increasingly difficult. In this paper machine learning (ML) tools are used to analyze anonymized call detail record data collected by a major US telecom network service provider. The data is carefully pre-processed and encoded using an efficient encoding method. Classification algorithms K Nearest Neighbors (KNN) and Decision Trees (DTs) are then implemented to study correlations between device and network level features and a mobile device's ability to initiate phantom calls. Based on the results, this work also suggests certain policy changes for network operators that may decrease the high volume of phantom E911 calls or alleviate the pressure of phantom E911 calls on a cellular network.",
        "DOI": "10.1145/3522783.3529527",
        "paper_author": "Hu Y.",
        "affiliation_name": "Rutgers University–New Brunswick",
        "affiliation_city": "New Brunswick",
        "affiliation_country": "United States",
        "affiliation_id": "60119141",
        "affiliation_state": "NJ"
    },
    {
        "paper_title": "Digital Twin Virtualization with Machine Learning for IoT and Beyond 5G Networks: Research Directions for Security and Optimal Control",
        "publication": "WiseML 2022 - Proceedings of the 2022 ACM Workshop on Wireless Security and Machine Learning",
        "citied_by": "28",
        "cover_date": "2022-05-19",
        "Abstract": "Digital twin (DT) technologies have emerged as a solution for real-time data-driven modeling of cyber physical systems (CPS) using the vast amount of data available by Internet of Things (IoT) networks. In this position paper, we elucidate unique characteristics and capabilities of a DT framework that enables realization of such promises as online learning of a physical environment, real-time monitoring of assets, Monte Carlo heuristic search for predictive prevention, on-policy, and off-policy reinforcement learning in real-time. We establish a conceptual layered architecture for a DT framework with decentralized implementation on cloud computing and enabled by artificial intelligence (AI) services for modeling and decision-making processes. The DT framework separates the control functions, deployed as a system of logically centralized process, from the physical devices under control, much like software-defined networking (SDN) in fifth generation (5G) wireless networks. To clarify the significance of DT in lowering the risk of development and deployment of innovative technologies on existing system, we discuss the application of implementing zero trust architecture (ZTA) as a necessary security framework in future data-driven communication networks.",
        "DOI": "10.1145/3522783.3529519",
        "paper_author": "Jagannath J.",
        "affiliation_name": "Andro Computational Solutions, LLC.",
        "affiliation_city": "Rome",
        "affiliation_country": "United States",
        "affiliation_id": "60097888",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Fertility Intentions for a Second Child and Their Influencing Factors in Contemporary China",
        "publication": "Frontiers in Psychology",
        "citied_by": "8",
        "cover_date": "2022-05-19",
        "Abstract": "Although the Chinese government has shifted from a one-child policy to a two-child policy (allowing a couple to have up to two children) since 2016 in response to the aging population, the policy results have been unsatisfactory. This is the first paper to systematically investigate the factors influencing residents’ intentions to have a second child. The research focuses on the perspective of individual, family, and social characteristics based on the Chinese General Social Survey (CGSS) from 2017 to 2018. Three machine learning methods are used in conjunction with logistic regression to reveal that the intention of having a second child increases heavily with age, more siblings in the family of origin, and better health. The family income, which is currently the focus of the literature and is statistically significant, is only sixth most important. This study further reveals differences between genders: Women with a lower level of education and religious beliefs prefer to have a second child, whereas for men, non-agricultural hukou and marriage are the position factors. The results of this study also illustrate the importance of future research focusing on the relationship of individuals to their family of origin and districts.",
        "DOI": "10.3389/fpsyg.2022.883317",
        "paper_author": "Li M.",
        "affiliation_name": "Central European University",
        "affiliation_city": "Vienna",
        "affiliation_country": "Austria",
        "affiliation_id": "60009563",
        "affiliation_state": "Vienna"
    },
    {
        "paper_title": "A novel method for mobile cloud computing security system",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2022-05-19",
        "Abstract": "In the recent era, usage of mobile device is increasing rapidly. People are enjoying the services provided in the mobile device and mobile applications. Cloud services can incredibly upgrade the computing capability of portable gadgets so the mobile applications are depending on the cloud to provide the operation like machine learning algorithms of data mining, big data, multimedia and ad hoc network. There are many issues and challenges while providing mobile services such as security and protection are major concern to give more attention. The perspective of this article is to explore the difficulties in mobile computing architectures and to evaluate the applicability of current security systems and to achieve solution toward it. There are three phases as the new architectural data service mechanism is proposed in the first phase. Time and Identity based proxy re-encryption with all or nothing transform scheme (TIB-PRE-AONT) efficiently achieved both of the secrecy and access control of data, secondly Cipher text policy attribute based encryption (CP-ABE) access control scheme is used and implement with k medoid clustering algorithm. Thirdis an Efficient and Optimal Fixed-Length Ciphertext-Policy Attribute-Based Encryption used for security with fuzzy clustering based Data Encryption Strategy for Big Data in Mobile Cloud Computing as a scheme, which used Whale Optimization Algorithm (BS-WOA) is used for the security key. The efficiency of the proposed methods proved from the comparison analysis results.",
        "DOI": "10.1063/5.0075132",
        "paper_author": "Naveen P.V.",
        "affiliation_name": "Vels Institute of Science, Technology &amp; Advanced Studies",
        "affiliation_city": "Chennai",
        "affiliation_country": "India",
        "affiliation_id": "60105237",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Importance of sex and gender factors for COVID-19 infection and hospitalisation: A sex-stratified analysis using machine learning in UK Biobank data",
        "publication": "BMJ Open",
        "citied_by": "10",
        "cover_date": "2022-05-18",
        "Abstract": "Objective To examine sex and gender roles in COVID-19 test positivity and hospitalisation in sex-stratified predictive models using machine learning. Design Cross-sectional study. Setting UK Biobank prospective cohort. Participants Participants tested between 16 March 2020 and 18 May 2020 were analysed. Main outcome measures The endpoints of the study were COVID-19 test positivity and hospitalisation. Forty-two individuals' demographics, psychosocial factors and comorbidities were used as likely determinants of outcomes. Gradient boosting machine was used for building prediction models. Results Of 4510 individuals tested (51.2% female, mean age=68.5±8.9 years), 29.4% tested positive. Males were more likely to be positive than females (31.6% vs 27.3%, p=0.001). In females, living in more deprived areas, lower income, increased low-density lipoprotein (LDL) to high-density lipoprotein (HDL) ratio, working night shifts and living with a greater number of family members were associated with a higher likelihood of COVID-19 positive test. While in males, greater body mass index and LDL to HDL ratio were the factors associated with a positive test. Older age and adverse cardiometabolic characteristics were the most prominent variables associated with hospitalisation of test-positive patients in both overall and sex-stratified models. Conclusion High-risk jobs, crowded living arrangements and living in deprived areas were associated with increased COVID-19 infection in females, while high-risk cardiometabolic characteristics were more influential in males. Gender-related factors have a greater impact on females; hence, they should be considered in identifying priority groups for COVID-19 infection vaccination campaigns.",
        "DOI": "10.1136/bmjopen-2021-050450",
        "paper_author": "Azizi Z.",
        "affiliation_name": "Centre Universitaire de Santé McGill",
        "affiliation_city": "Montreal",
        "affiliation_country": "Canada",
        "affiliation_id": "60006558",
        "affiliation_state": "QC"
    },
    {
        "paper_title": "Stacking-based uncertainty modelling of statistical and machine learning methods for residential property valuation",
        "publication": "ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences",
        "citied_by": "2",
        "cover_date": "2022-05-17",
        "Abstract": "Estimating real estate prices helps to adapt informed policies to regulate the real estate market and assist sellers and buyers to have a fair business. This study aims to estimate the price of residential properties in District 5 of Tehran, Capital of Iran, and model its associated uncertainty. The study implements the Stacking technique to model uncertainties by integrating the outputs of basic models. Basic models must have a good performance for their combinations to have acceptable results. This study employs four statistical and machine learning models as basic models: Random Forest (RF), Ordinary Least Squares (OLS), Weighted K-Nearest Neighbour (WKNN), and Support Vector Regression (SVR) to estimate the price of residential properties. The results show that the integrated output is more accurate for the quadruple combination mode than for any of the binary and triple combinations of the basic models. Comparing the Stacking technique with the Voting technique, it is shown that the Mean Absolute Percentage Error (MAPE) reduces from 10.18% to 9.81%. Hence we conclude that our method performs better than the Voting technique.",
        "DOI": "10.5194/isprs-Annals-V-4-2022-49-2022",
        "paper_author": "Jafari A.",
        "affiliation_name": "University of Tehran",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran",
        "affiliation_id": "60022927",
        "affiliation_state": "Tehran"
    },
    {
        "paper_title": "Machine learning-based economic development mapping from multi-source open geospatial data",
        "publication": "ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences",
        "citied_by": "6",
        "cover_date": "2022-05-17",
        "Abstract": "Timely and accurate socioeconomic indicators are the prerequisite for smart social governance. For example, the level of economic development and the structure of population are important statistics for regional or national policy-making. However, the collection of these characteristics usually depends on demographic and social surveys, which are time-and labor-intensive. To address these issues, we propose a machine learning-based approach to estimate and map the economic development from multi-source open available geospatial data, including remote sensing imagery and OpenStreetMap road networks. Specifically, we first extract knowledge-based features from different data sources; then the multi-view graphs are constructed through different perspectives of spatial adjacency and feature similarity; and a multi-view graph neural network (MVGNN) model is built on them and trained in a self-supervised learning manner. Then, the handcrafted features and the learned graph representations are combined to estimate the regional economic development indicators via random forest models. Taking China's county-level gross domestic product (GDP) as an example, extensive experiments have been conducted and the results demonstrate the effectiveness of the proposed method, and the combination of the knowledge-based and learning-based features can significantly outperform baseline methods. Our proposed approach can advance the goal of acquiring timely and accurate socioeconomic variables through widely accessible geospatial data, which has the potential to extend to more social indicators and other geographic regions to support smart governance and policy-making in the future.",
        "DOI": "10.5194/isprs-Annals-V-4-2022-259-2022",
        "paper_author": "Cao R.",
        "affiliation_name": "The Hong Kong Polytechnic University",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong",
        "affiliation_id": "60008928",
        "affiliation_state": "Hong Kong"
    },
    {
        "paper_title": "Empowering geo-based ai algorithm to aid coastal flood risk analysis: A review and framework development",
        "publication": "ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences",
        "citied_by": "4",
        "cover_date": "2022-05-17",
        "Abstract": "Climate change and current susceptibilities exacerbated the coastal flood loss and damage resulting in livelihoods and property damage. Urban areas in the Low to Lower-Middle Income Countries are expected to be disproportionately impacted by the disaster, given a higher share of citizens living in the Low Elevation Coastal Zone, limited financial resources, and poorly constructed disaster protection. Documentation of historical coastal floods, population, and property affected, could advance the assessment by considering those parameters in risk analysis. Besides, incorporating such geographic features e.g., mangroves as the ecological solution for alternative coastal flood protection in the prediction is also essential. Mangrove is considered fit for the LLMIC primarily situated in the tropical zone. The prediction utilizing spatial Machine Learning (ML) could aid climate-related disaster risk analysis and contribute to risk reduction and policy suggestions to improve disaster resilience. The research aims to archive recent studies on the application of geospatial science empowering Artificial Intelligence, notably ML in coastal flood risk assessment, so-called GIS-based AI. Another aim is to document population, property, and mangrove distribution across the LLMIC. Artificial Neural Networks were mostly utilized for disaster risk assessment in past research. The number of 58 historical coastal flood events and 908 expected coastal flood hotspots for 2006 to 2021 has been documented. Over 1,2 million Km2 falls under vulnerable areas toward coastal flood in LLMIC under different settlement types where Large City (urban areas) dominates it. Mangrove distribution is mainly distributed across tropical regions mostly distributed along the Southeast Asia coast.",
        "DOI": "10.5194/isprs-Annals-V-3-2022-517-2022",
        "paper_author": "Atmaja T.",
        "affiliation_name": "The University of Tokyo",
        "affiliation_city": "Tokyo",
        "affiliation_country": "Japan",
        "affiliation_id": "60025272",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Editorial: Contemporary Medicine: Making Sense of Implementation Models and Methods",
        "publication": "Frontiers in Medicine",
        "citied_by": "0",
        "cover_date": "2022-05-17",
        "Abstract": "NA",
        "DOI": "10.3389/fmed.2022.912045",
        "paper_author": "Ciulla M.M.",
        "affiliation_name": "Università degli Studi di Milano",
        "affiliation_city": "Milan",
        "affiliation_country": "Italy",
        "affiliation_id": "60030318",
        "affiliation_state": "MI"
    },
    {
        "paper_title": "The Legacy of the TTASAAN Report – Premature Conclusions and Forgotten Promises About SPECT Neuroimaging: A Review of Policy and Practice Part II",
        "publication": "Frontiers in Neurology",
        "citied_by": "1",
        "cover_date": "2022-05-17",
        "Abstract": "Brain perfusion single photon emission computed tomography (SPECT) scans were initially developed in 1970s. A key radiopharmaceutical, hexamethylpropyleneamine oxime (HMPAO), was not stabilized until 1993 and most early SPECT scans were performed on single-head gamma cameras. These early scans were of inferior quality. In 1996, the Therapeutics and Technology Assessment Subcommittee of the American Academy of Neurology (TTASAAN) issued a report regarding the use of SPECT in the evaluation of neurological disorders. This two-part series explores the policies and procedures related to perfusion SPECT functional neuroimaging. In Part I, the comparison between the quality of the SPECT scans and the depth of the data for key neurological and psychiatric indications at the time of the TTASAAN report vs. the intervening 25 years were presented. In Part II, the technical aspects of perfusion SPECT neuroimaging and image processing will be explored. The role of color scales will be reviewed and the process of interpreting a SPECT scan will be presented. Interpretation of a functional brain scans requires not only anatomical knowledge, but also technical understanding on correctly performing a scan, regardless of the scanning modality. Awareness of technical limitations allows the clinician to properly interpret a functional brain scan. With this foundation, four scenarios in which perfusion SPECT neuroimaging, together with other imaging modalities and testing, lead to a narrowing of the differential diagnoses and better treatment. Lastly, recommendations for the revision of current policies and practices are made.",
        "DOI": "10.3389/fneur.2022.851609",
        "paper_author": "Pavel D.G.",
        "affiliation_name": "PathFinder Brain SPECT",
        "affiliation_city": "Deerfield",
        "affiliation_country": "United States",
        "affiliation_id": "114340933",
        "affiliation_state": "IL"
    },
    {
        "paper_title": "Permutation flow shop scheduling with multiple lines and demand plans using reinforcement learning",
        "publication": "European Journal of Operational Research",
        "citied_by": "29",
        "cover_date": "2022-05-16",
        "Abstract": "Existing studies on the permutation flow shop problem (PFSP) commonly assume that jobs are produced on a single line. However, manufacturers may speed up their production by employing multiple lines, where each line produces sub-parts of the final product; which must be assembled by a synchronization machine. This study presents a novel reinforcement learning (RL) approach for the PFSP with multiple lines and demand plans. Our approach differs from existing RL-based scheduling methods as we train the policy to directly generate the sequence in an iterative way, where actions denote the job type to be sequenced next. During cutoff time, we follow a multistart approach that generates sequences with the trained policy, which are subsequently optimized by local search. Our numerical evaluation based on 1050 problem instances with up to three production lines shows that our approach outperforms existing methods on the multi-line problems for short cutoff times, while there is a tie with existing methods for medium and long cutoff times. A further analysis suggests that our approach can also be applied to problems with imbalanced demand plans.",
        "DOI": "10.1016/j.ejor.2021.08.007",
        "paper_author": "Brammer J.",
        "affiliation_name": "Volkswagen AG",
        "affiliation_city": "Wolfsburg",
        "affiliation_country": "Germany",
        "affiliation_id": "60020755",
        "affiliation_state": "Niedersachsen"
    },
    {
        "paper_title": "Historical and future spatially-explicit climate change impacts on mycorrhizal and saprotrophic macrofungal productivity in Mediterranean pine forests",
        "publication": "Agricultural and Forest Meteorology",
        "citied_by": "7",
        "cover_date": "2022-05-15",
        "Abstract": "Fungi are responsible for many of the processes that occur in natural ecosystems and largely determine forest ecosystem dynamics, such as the ability of trees to access limiting nutrients and sequester carbon. Understanding and predicting climate change impacts on fungal dynamics over large scales is key in order to gain further insights into the effects of global change on natural ecosystem functioning and related ecosystem services. In this study, we use predictive models based on machine learning algorithms to estimate, in a spatially explicit way, the historical and future (1976–2100) evolution of mycorrhizal and saprotrophic macrofungal productivity in Mediterranean forest areas under climate change scenarios. The greatest changes in total productivity, as well as mycorrhizal fungi, are predicted to occur in subalpine and montane pine forests, where fungal productivity is estimated to decrease, and will be more pronounced under climate change scenarios with higher expected increase in temperature. In contrast to mycorrhizal species, saprotrophic fungi could benefit from pronounced changes in climate and increase their productivity in supra- and mesomediterranean regions at mid-range elevations. Moreover, we estimated that fungal productivity has also changed historically in some scattered areas where changes in climate over the years may have led to a decrease in productivity. This study contributes to raising awareness on the need for anticipating potential global change impacts on this key element of ecosystem functioning, and for deploying possible management policies oriented toward maintaining the important role of fungal productivity in both climate change mitigation and adaptation.",
        "DOI": "10.1016/j.agrformet.2022.108918",
        "paper_author": "Morera A.",
        "affiliation_name": "Universitat de Lleida",
        "affiliation_city": "Lleida",
        "affiliation_country": "Spain",
        "affiliation_id": "60032717",
        "affiliation_state": "Lleida"
    },
    {
        "paper_title": "A moving forest model to predict the building-level progression of ordinance-mandated seismic retrofits",
        "publication": "Journal of Building Engineering",
        "citied_by": "2",
        "cover_date": "2022-05-15",
        "Abstract": "An algorithm is formulated to dynamically predict the progression of policy-mandated building-specific seismic retrofits over time. Elements of the widely utilized moving forecast model is integrated with the random forest machine learning algorithm. An evolving duration-based window structure is adopted, which is partitioned into training and forecasting phases, each of which is further subdivided into smaller increments of time. Inside the training window, the time since the passage of the ordinance is combined with building, socioeconomic and demographic variables to create a feature matrix. The outcome vector comprises the state of progression of each building (as defined by the ordinance and/or modeler) in the affected inventory at pre-defined time instants. The data subset in the training window is used to construct the random forest classification model that predicts the retrofit progression states within the forecasting part of the window. This process is repeated while incrementally increasing the fraction of the ordinance time horizon that is encapsulated within each subsequent window. The “moving forest” algorithm is implemented on the wood frame building inventory that is under the purview of the Los Angeles Soft-Story Ordinance. For this specific application, the number of units in the building, the percentage of owner-occupied units (at the census-block level) and the time since the passage of the Ordinance, are found to be the most important features. The classification accuracy ranged from approximately 53% during the first six months to 95% in the final year of the considered four-year time-horizon. The proposed model can be used by departments of building and safety to manage the flow of permit and retrofit certification applications associated with an ordinance or for dynamic assessments that aim to model the risk reduction in the affected inventory over time.",
        "DOI": "10.1016/j.jobe.2022.104020",
        "paper_author": "Burton H.",
        "affiliation_name": "UCLA Samueli School of Engineering",
        "affiliation_city": "Los Angeles",
        "affiliation_country": "United States",
        "affiliation_id": "60153950",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "A spacecraft attitude manoeuvre planning algorithm based on improved policy gradient reinforcement learning",
        "publication": "Journal of Navigation",
        "citied_by": "2",
        "cover_date": "2022-05-14",
        "Abstract": "To solve the problem of spacecraft attitude manoeuvre planning under dynamic multiple mandatory pointing constraints and prohibited pointing constraints, a systematic attitude manoeuvre planning approach is proposed that is based on improved policy gradient reinforcement learning. This paper presents a succinct model of dynamic multiple constraints that is similar to a real situation faced by an in-orbit spacecraft. By introducing return baseline and adaptive policy exploration methods, the proposed method overcomes issues such as large variances and slow convergence rates. Concurrently, the required computation time of the proposed method is markedly reduced. Using the proposed method, the near optimal path of the attitude manoeuvre can be determined, making the method suitable for the control of micro spacecraft. Simulation results demonstrate that the planning results fully satisfy all constraints, including six prohibited pointing constraints and two mandatory pointing constraints. The spacecraft also maintains high orientation accuracy to the Earth and Sun during all attitude manoeuvres.",
        "DOI": "10.1017/S0373463321000813",
        "paper_author": "Hua B.",
        "affiliation_name": "Nanjing University of Aeronautics and Astronautics",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60021666",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "Artificial intelligence, machine learning, automation, robotics, future of work and future of humanity: A review and research agenda",
        "publication": "Research Anthology on Machine Learning Techniques, Methods, and Applications",
        "citied_by": "5",
        "cover_date": "2022-05-13",
        "Abstract": "The exponential advancement in artificial intelligence (AI), machine learning, robotics, and automation are rapidly transforming industries and societies across the world. The way we work, the way we live, and the way we interact with others are expected to be transformed at a speed and scale beyond anything we have observed in human history. This new industrial revolution is expected, on one hand, to enhance and improve our lives and societies. On the other hand, it has the potential to cause major upheavals in our way of life and our societal norms. The window of opportunity to understand the impact of these technologies and to preempt their negative effects is closing rapidly. Humanity needs to be proactive, rather than reactive, in managing this new industrial revolution. This article looks at the promises, challenges, and future research directions of these transformative technologies. Not only are the technological aspects investigated, but behavioral, societal, policy, and governance issues are reviewed as well. This research contributes to the ongoing discussions and debates about AI, automation, machine learning, and robotics. It is hoped that this article will heighten awareness of the importance of understanding these disruptive technologies as a basis for formulating policies and regulations that can maximize the benefits of these advancements for humanity and, at the same time, curtail potential dangers and negative impacts.",
        "DOI": "10.4018/978-1-6684-6291-1.ch076",
        "paper_author": "Wang W.",
        "affiliation_name": "Missouri University of Science and Technology",
        "affiliation_city": "Rolla",
        "affiliation_country": "United States",
        "affiliation_id": "60024728",
        "affiliation_state": "MO"
    },
    {
        "paper_title": "Performance evaluation of forecasting models based on time series and machine learning techniques: an application to light fuel consumption in Brazil",
        "publication": "International Journal of Energy Sector Management",
        "citied_by": "2",
        "cover_date": "2022-05-11",
        "Abstract": "Purpose: Fuel demand forecast is a fundamental tool to guide private planning actions and public policies aim to guarantee energy supply. This paper aims to evaluate different forecasting methods to project the consumption of light fuels in Brazil (fuel used by vehicles with internal combustion engine). Design/methodology/approach: Eight different methods were implemented, besides of ensemble learning technics that combine the different models. The evaluation was carried out based on the forecast error for a forecast horizon of 3, 6 and 12 months. Findings: The statistical tests performed indicated the superiority of the evaluated models compared to a naive forecasting method. As the forecast horizon increase, the heterogeneity between the accuracy of the models becomes evident and the classification by performance becomes easier. Furthermore, for 12 months forecast, it was found methods that outperform, with statistical significance, the SARIMA method, that is widely used. Even with an unprecedented event, such as the COVID-19 crisis, the results proved to be robust. Practical implications: Some regulation instruments in Brazilian fuel market requires the forecast of light fuel consumption to better deal with supply and environment issues. In that context, the level of accuracy reached allows the use of these models as tools to assist public and private agents that operate in this market. Originality/value: The study seeks to fill a gap in the literature on the Brazilian light fuel market. In addition, the methodological strategy adopted assesses projection models from different areas of knowledge using a robust evaluation procedure.",
        "DOI": "10.1108/IJESM-02-2021-0009",
        "paper_author": "Rodrigues L.",
        "affiliation_name": "Universidade de São Paulo",
        "affiliation_city": "Sao Paulo",
        "affiliation_country": "Brazil",
        "affiliation_id": "60008088",
        "affiliation_state": "SP"
    },
    {
        "paper_title": "Demystifying corona virus disease (COVID-19) using graph data science",
        "publication": "AIP Conference Proceedings",
        "citied_by": "1",
        "cover_date": "2022-05-09",
        "Abstract": "Data science is emerging as a novel domain in the area of not only computers but also medical, agriculture, machine learning, social networking, and health care. As the data increases every second, the success of any real-world data analytical application majorly depends on the type and efficiency of its storage and management system. In these applications where there is difficulty in organizing the data in structured form materializes the role of Graph databases. Graph databases are well-organized to manage and store the data in the real world. Often, the graph databases have the capability of representing trillions of relationships which exist in any of the web or social networking dataset. The world is suffering from COVID-19 pandemic. Many researchers are working on post lockdown strategies that will control the spread of the coronavirus as well as unlock some of our freedoms. This situation is quite tricky, but data science as technology can probably provide a solution. One of the major objectives of this paper is that how graph database like Neo4j can help us information of policies which leads to achieve social isolation and to provide a solution for contact tracing problems which is a hurdle to social isolation [18].",
        "DOI": "10.1063/5.0080620",
        "paper_author": "Kaur P.",
        "affiliation_name": "Chitkara University, Punjab",
        "affiliation_city": "Rajpura",
        "affiliation_country": "India",
        "affiliation_id": "60113205",
        "affiliation_state": "PB"
    },
    {
        "paper_title": "Neural architecture tuning with policy adaptation",
        "publication": "Neurocomputing",
        "citied_by": "3",
        "cover_date": "2022-05-07",
        "Abstract": "Neural architecture search (NAS) is to automatically design task-specific neural architectures, whose performance has already surpassed those of many manually designed neural networks. Existing NAS techniques focus on searching for the neural architecture and training the optimal network weights from the scratch. Nevertheless, it could be essential to study how to tune a given neural architecture instead of producing a completely new neural architecture in some scenarios, which may lead to a more optimal solution by combining human experience and the advantages of the machine's automatic searching. This paper proposes to learn to tune the architectures at hand to achieve better performance. The proposed Neural Architecture Tuning (NAT) algorithm trains a deep Q-network to tune neural architectures given a random architecture so that we can achieve better performance on a reduced space. We then apply adversarial autoencoder to make the learned policy be generalized to a different searching space in real-world applications. The proposed algorithm is evaluated on the NAS-Bench-101 dataset. The results indicate that our NAT framework can achieve state-of-the-art performance on the NAS-Bench-101 benchmark, and the learned policy can be adapted to a different search space while maintaining the performance.",
        "DOI": "10.1016/j.neucom.2021.10.095",
        "paper_author": "Li Y.",
        "affiliation_name": "School of Computer Science",
        "affiliation_city": "Sydney",
        "affiliation_country": "Australia",
        "affiliation_id": "60099659",
        "affiliation_state": "NSW"
    },
    {
        "paper_title": "Poster Abstract: Learning from Demonstrations with Temporal Logics",
        "publication": "HSCC 2022 - Proceedings of the 25th ACM International Conference on Hybrid Systems: Computation and Control, Part of CPS-IoT Week 2022",
        "citied_by": "0",
        "cover_date": "2022-05-04",
        "Abstract": "Learning-from-demonstrations (LfD) is a popular paradigm to obtain effective robot control policies for complex tasks via reinforcement learning without the need to explicitly design reward functions. However, it is susceptible to imperfections in demonstrations and also raises concerns of safety and interpretability in the learned control policies. To address these issues, we propose to use Signal Temporal Logic (STL) to express high-level robotic tasks and use its quantitative semantics to evaluate and rank the quality of demonstrations. Temporal logic-based specifications allow us to create non-Markovian rewards, and are also capable of defining interesting causal dependencies between tasks such as sequential task specifications. We present our completed work that proposed LfD-STL framework that learns from even suboptimal/imperfect demonstrations and STL specifications to infer rewards for reinforcement learning tasks. We have validated our approach through various experimental setups to show how our method outperforms prior LfD methods. We then discuss future directions for tackling the problem of explainability and interpretability in such learning-based systems.",
        "DOI": "10.1145/3501710.3524914",
        "paper_author": "Puranic A.",
        "affiliation_name": "USC Viterbi School of Engineering",
        "affiliation_city": "Los Angeles",
        "affiliation_country": "United States",
        "affiliation_id": "60143535",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Deep Reinforcement Learning Based Trajectory Planning Under Uncertain Constraints",
        "publication": "Frontiers in Neurorobotics",
        "citied_by": "35",
        "cover_date": "2022-05-02",
        "Abstract": "With the advance in algorithms, deep reinforcement learning (DRL) offers solutions to trajectory planning under uncertain environments. Different from traditional trajectory planning which requires lots of effort to tackle complicated high-dimensional problems, the recently proposed DRL enables the robot manipulator to autonomously learn and discover optimal trajectory planning by interacting with the environment. In this article, we present state-of-the-art DRL-based collision-avoidance trajectory planning for uncertain environments such as a safe human coexistent environment. Since the robot manipulator operates in high dimensional continuous state-action spaces, model-free, policy gradient-based soft actor-critic (SAC), and deep deterministic policy gradient (DDPG) framework are adapted to our scenario for comparison. In order to assess our proposal, we simulate a 7-DOF Panda (Franka Emika) robot manipulator in the PyBullet physics engine and then evaluate its trajectory planning with reward, loss, safe rate, and accuracy. Finally, our final report shows the effectiveness of state-of-the-art DRL algorithms for trajectory planning under uncertain environments with zero collision after 5,000 episodes of training.",
        "DOI": "10.3389/fnbot.2022.883562",
        "paper_author": "Chen L.",
        "affiliation_name": "Technische Universität München",
        "affiliation_city": "Munich",
        "affiliation_country": "Germany",
        "affiliation_id": "60019722",
        "affiliation_state": "Bayern"
    },
    {
        "paper_title": "International Conference on Recent Innovations in Science and Technology, RIST 2021",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2022-05-02",
        "Abstract": "The proceedings contain 72 papers. The topics discussed include: review on disease detection of plants using image processing and machine learning techniques; survey on various load balancing algorithms in cloud computing; implementing cyber security policy for big data system; 3D modelling and visualization of buildings using photogrammetry; text classification using NLP based machine learning approach; optical character recognition using localization techniques; secure deep learning model for disease prediction and diagnosis system in cloud based IoT; a survey on recognition and classification of paddy leaf diseases using image processing and machine learning techniques; and big data analysis of traditional Indian ayurveda medicine and treatment process.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "AppAxO: Designing Application-specific Approximate Operators for FPGA-based Embedded Systems",
        "publication": "ACM Transactions on Embedded Computing Systems",
        "citied_by": "19",
        "cover_date": "2022-05-01",
        "Abstract": "Approximate arithmetic operators, such as adders and multipliers, are increasingly used to satisfy the energy and performance requirements of resource-constrained embedded systems. However, most of the available approximate operators have an application-agnostic design methodology, and the efficacy of these operators can only be evaluated by employing them in the applications. Furthermore, the various available libraries of approximate operators do not share any standard approximation-induction policy to design new operators according to an application's accuracy and performance constraints. These limitations also hinder the utilization of machine learning models to explore and determine approximate operators according to an application's requirements. In this work, we present a generic design methodology for implementing FPGA-based application-specific approximate arithmetic operators. Our proposed technique utilizes lookup tables and carry-chains of FPGAs to implement approximate operators according to the input configurations. For instance, for an accurate multiplier utilizing K lookup tables, our methodology utilizes K-bit configurations to design approximate multipliers. We then utilize various machine learning models to evaluate and select configurations satisfying application accuracy and performance constraints. We have evaluated our proposed methodology for three benchmark applications, i.e., biomedical signal processing, image processing, and ANNs. We report more non-dominated approximate multipliers with better hypervolume contribution than state-of-the-art designs for these benchmark applications with the proposed design methodology.",
        "DOI": "10.1145/3513262",
        "paper_author": "Ullah S.",
        "affiliation_name": "Center for Advancing Electronics Dresden",
        "affiliation_city": "Dresden",
        "affiliation_country": "Germany",
        "affiliation_id": "60116451",
        "affiliation_state": "Sachsen"
    },
    {
        "paper_title": "A Learning-Based Optimization Approach for Autonomous Ridesharing Platforms with Service-Level Contracts and On-Demand Hiring of Idle Vehicles",
        "publication": "Transportation Science",
        "citied_by": "13",
        "cover_date": "2022-05-01",
        "Abstract": "Current mobility services cannot compete on equal terms with self-owned mobility products concerning service quality. Because of supply and demand imbalances, ridesharing users invariably experience delays, price surges, and rejections. Traditional approaches often fail to respond to demand fluctuations adequately because service levels are, to some extent, bounded by fleet size. With the emergence of autonomous vehicles, however, the characteristics of mobility services change and new opportunities to overcome the prevailing limitations arise. In this paper, we consider an autonomous ridesharing problem in which idle vehicles are hired on-demand in order to meet the service-level requirements of a heterogeneous user base. In the face of uncertain demand and idle vehicle supply, we propose a learning-based optimization approach that uses the dual variables of the underlying assignment problem to iteratively approximate the marginal value of vehicles at each time and location under different availability settings. These approximations are used in the objective function of the optimization problem to dispatch, rebalance, and occasionally hire idle third-party vehicles in a high-resolution transportation network of Manhattan, New York City. The results show that the proposed policy outperforms a reactive optimization approach in a variety of vehicle availability scenarios while hiring fewer vehicles. Moreover, we demonstrate that mobility services can offer strict service-level contracts to different user groups featuring both delay and rejection penalties.",
        "DOI": "10.1287/trsc.2021.1069",
        "paper_author": "Beirigo B.A.",
        "affiliation_name": "TU Delft - Department of Maritime and Transportation Technology",
        "affiliation_city": "Delft",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60118236",
        "affiliation_state": "Zuid-Holland"
    },
    {
        "paper_title": "GAN augmentation for multiclass image classification using hemorrhage detection as a case-study",
        "publication": "Journal of Medical Imaging",
        "citied_by": "3",
        "cover_date": "2022-05-01",
        "Abstract": "Purpose: In recent years, the development and exploration of deeper and more complex deep learning models has been on the rise. However, the availability of large heterogeneous datasets to support efficient training of deep learning models is lacking. While linear image transformations for augmentation have been used traditionally, the recent development of generative adversarial networks (GANs) could theoretically allow us to generate an infinite amount of data from the real distribution to support deep learning model training. Recently, the Radiological Society of North America (RSNA) curated a multiclass hemorrhage detection challenge dataset that includes over 800,000 images for hemorrhage detection, but all high-performing models were trained using traditional data augmentation techniques. Given a wide variety of selections, the augmentation for image classification often follows a trial-and-error policy. Approach: We designed conditional DCGAN (cDCGAN) and in parallel trained multiple popular GAN models to use as online augmentations and compared them to traditional augmentation methods for the hemorrhage case study. Results: Our experimentations show that the super-minority, epidural hemorrhages with cDCGAN augmentation presented a minimum of 2 × improvement in their performance against the traditionally augmented model using the same classifier configuration. Conclusion: This shows that for complex and imbalanced datasets, traditional data imbalancing solutions may not be sufficient and require more complex and diverse data augmentation methods such as GANs to solve.",
        "DOI": "10.1117/1.JMI.9.3.035504",
        "paper_author": "Jason Jeong J.",
        "affiliation_name": "Ira A. Fulton Schools of Engineering",
        "affiliation_city": "Tempe",
        "affiliation_country": "United States",
        "affiliation_id": "60139200",
        "affiliation_state": "AZ"
    },
    {
        "paper_title": "Machine Learning as Natural Monopoly",
        "publication": "Iowa Law Review",
        "citied_by": "2",
        "cover_date": "2022-05-01",
        "Abstract": "Machine learning is transforming the economy, reshaping operations in communications, law enforcement, and medicine, among other sectors. But all is not well: Many machine-learning-based applications harvest vast amounts of personal information and yield results that are systematically biased. In response, policy makers have begun to offer a range of incomplete solutions. In so doing, they have overlooked the possibility —suggested intuitively by scholars across disciplines—that these systems are natural monopolies and have thus neglected the long legal tradition of natural monopoly regulation. Drawing on the computer science, economics, and legal literatures, I find that some machine-learning-based applications may be natural monopolies, particularly where the fixed costs of developing these applications and the computational costs of optimizing these systems are especially high, and where network effects are especially strong. This conclusion yields concrete policy implications: Where natural monopolies exist, public oversight and regulation are typically superior to market discipline through competition. Hence, where machine-learning-based applications are natural monopolies, this regulatory tradition offers one framework for confronting a range of issues—from privacy to accuracy and bias—that attend to such systems. Just as prior natural monopolies—the railways, electric grids, and telephone networks—faced rate and service regulation to protect against extractive, anticompetitive, and undemocratic behaviors, so too might machine-learning-based applications face similar public regulation to limit intrusive data collection and protect against algorithmic redlining, among other harms.",
        "DOI": "NA",
        "paper_author": "Narechania T.N.",
        "affiliation_name": "University of California, Berkeley",
        "affiliation_city": "Berkeley",
        "affiliation_country": "United States",
        "affiliation_id": "60025038",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Tool for Predicting College Student Career Decisions: An Enhanced Support Vector Machine Framework",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "11",
        "cover_date": "2022-05-01",
        "Abstract": "The goal of this research is to offer an effective intelligent model for forecasting college students' career decisions in order to give a useful reference for career decisions and policy formation by relevant departments. The suggested prediction model is mainly based on a support vector machine (SVM) that has been modified using an enhanced butterfly optimization approach with a communication mechanism and Gaussian bare-bones mechanism (CBBOA). To get a better set of parameters and feature subsets, first, we added a communication mechanism to BOA to improve its global search capability and balance exploration and exploitation trends. Then, Gaussian bare-bones was added to increase the population diversity of BOA and its ability to jump out of the local optimum. The optimal SVM model (CBBOA-SVM) was then developed to predict the career decisions of college students based on the obtained parameters and feature subsets that are already optimized by CBBOA. In order to verify the effectiveness of CBBOA, we compared it with some advanced algorithms on all benchmark functions of CEC2014. Simulation results demonstrated that the performance of CBBOA is indeed more comprehensive. Meanwhile, comparisons between CBBOA-SVM and other machine learning approaches for career decision prediction were carried out, and the findings demonstrate that the provided CBBOA-SVM has better classification and more stable performance. As a result, it is plausible to conclude that the CBBOA-SVM is capable of being an effective tool for predicting college student career decisions.",
        "DOI": "10.3390/app12094776",
        "paper_author": "Wang Z.",
        "affiliation_name": "Wenzhou University",
        "affiliation_city": "Wenzhou",
        "affiliation_country": "China",
        "affiliation_id": "60020224",
        "affiliation_state": "Zhejiang"
    },
    {
        "paper_title": "Real-Time Delivery Time Forecasting and Promising in Online Retailing: When Will Your Package Arrive?",
        "publication": "Manufacturing and Service Operations Management",
        "citied_by": "26",
        "cover_date": "2022-05-01",
        "Abstract": "Problem definition: Providing fast and reliable delivery services is key to running a successful online retail business. To achieve a better delivery time guarantee policy, we study how to estimate and promise delivery time for new customer orders in real time. Academic/practical relevance: Delivery time promising is critical to managing customer expectations and improving customer satisfaction. Simply overpromising or underpromising is undesirable because of the negative impacts on short-/long-term sales. To the best of our knowledge, we are the first to develop a data-driven framework to predict the distribution of order delivery time and set promised delivery time to customers in a cost-effective way. Methodology: We apply and extend tree-based models to generate distributional forecasts by exploiting the complicated relationship between delivery time and relevant operational predictors. To account for the cost-sensitive decision-making problem structure, we develop a new split rule for quantile regression forests that incorporates an asymmetric loss function in split point selection. We further propose a cost-sensitive decision rule to decide the promised delivery day from the predicted distribution. Results: Our decision rule is proven to be optimal given certain cost structures. Tested on a real-world data set shared from JD.com, our proposed machine learning–based models deliver superior forecasting performance. In addition, we demonstrate that our framework has the potential to provide better promised delivery time in terms of sales, cost, and accuracy as compared with the conventional promised time set by JD.com. Specifically, our simulation results indicate that the proposed delivery time promise policy can improve the sales volume by 6.1% over the current policy. Managerial implications: Through a more accurate estimation of the delivery time distribution, online retailers can strategically set the promised time to maximize customer satisfaction and boost sales. Our data-driven framework reveals the importance of modeling fulfillment operations in delivery time forecasting and integrating the decision-making problem structure with the forecasting model.",
        "DOI": "10.1287/msom.2022.1081",
        "paper_author": "Salari N.",
        "affiliation_name": "Rotman School of Management",
        "affiliation_city": "Toronto",
        "affiliation_country": "Canada",
        "affiliation_id": "60112541",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "A Scalable Deep Learning Framework for Extracting Model Inventory of Roadway Element Intersection Control Types From Panoramic Images",
        "publication": "Transportation Research Record",
        "citied_by": "2",
        "cover_date": "2022-05-01",
        "Abstract": "In the United States, the Model Inventory of Roadway Element (MIRE) provides a comprehensive list of data that are needed to support states’ data-driven safety programs. The intersection control is part of the MIRE Fundamental Data Elements (FDE) for which state Departments of Transportation are required to complete the collection by September 30, 2026. It is essential roadway data that have been used widely in traffic safety studies. This study proposes a scalable and automated deep learning framework for detecting and classifying stop and yield intersection controls using panoramic street view images. The Faster Region-based Convolutional Neural Networks (Faster R-CNN) model architecture was used to detect and classify stop and yield signs from the images. A transfer learning process was deployed using the Inception-ResNet-v2 generic feature extractor to accelerate the training and performance of the deep learning model with less data collection effort. The effectiveness and scalability of the proposed framework were tested on a sample of road intersections in the state of Michigan. The proposed deep learning model achieved a recall value of 97.7% and 98.2% for detecting and classifying stop and yield signs respectively. The evaluation of the model performance at a county level suggests that the model can be scaled to a statewide level without a substantial increase in the demand for computational resources. As demonstrated in this study, state DOTs can leverage the advancement of deep learning techniques and the availability of imagery data to expedite the process of collecting the MIRE data.",
        "DOI": "10.1177/03611981211069066",
        "paper_author": "Kwayu K.M.",
        "affiliation_name": "Systems Monitoring &amp; Reporting Unit",
        "affiliation_city": "Lansing",
        "affiliation_country": "United States",
        "affiliation_id": "128224940",
        "affiliation_state": "MI"
    },
    {
        "paper_title": "An Eye for Artificial Intelligence: Insights Into the Governance of Artificial Intelligence and Vision for Future Research",
        "publication": "Business and Society",
        "citied_by": "26",
        "cover_date": "2022-05-01",
        "Abstract": "In this 60th anniversary of Business & Society essay, we seek to make three main contributions at the intersection of governance and artificial intelligence (AI). First, we aim to illuminate some of the deeper social, legal, organizational, and democratic challenges of rising AI adoption and resulting algorithmic power by reviewing AI research through a governance lens. Second, we propose an AI governance framework that aims to better assess AI challenges as well as how different governance modalities (architecture, laws, norms, and market) can support AI. At the heart of our framework lies the governance forces that apply to institutions, organizations, and individuals, who ultimately provide, regulate, and use AI decision-making. We discuss how businesses may harness AI’s economic power through governance solutions without creating or amplifying societal biases and inequalities. Third, as part of our section on future research, we identify a set of governance trade-offs in AI adoption, suggest future research avenues to conceptually strengthen research on the governance of AI, and lay out key policy recommendations.",
        "DOI": "10.1177/00076503221080959",
        "paper_author": "Chhillar D.",
        "affiliation_name": "University of Illinois Urbana-Champaign",
        "affiliation_city": "Urbana",
        "affiliation_country": "United States",
        "affiliation_id": "60000745",
        "affiliation_state": "IL"
    },
    {
        "paper_title": "Assessment of the Benefits of Targeted Interventions for Pandemic Control in China Based on Machine Learning Method and Web Service for COVID-19 Policy Simulation",
        "publication": "Biomedical and Environmental Sciences",
        "citied_by": "6",
        "cover_date": "2022-05-01",
        "Abstract": "Taking the Chinese city of Xiamen as an example, simulation and quantitative analysis were performed on the transmissions of the Coronavirus Disease 2019 (COVID-19) and the influence of intervention combinations to assist policymakers in the preparation of targeted response measures. A machine learning model was built to estimate the effectiveness of interventions and simulate transmission in different scenarios. The comparison was conducted between simulated and real cases in Xiamen. A web interface with adjustable parameters, including choice of intervention measures, intervention weights, vaccination, and viral variants, was designed for users to run the simulation. The total case number was set as the outcome. The cumulative number was 4,614,641 without restrictions and 78 under the strictest intervention set. Simulation with the parameters closest to the real situation of the Xiamen outbreak was performed to verify the accuracy and reliability of the model. The simulation model generated a duration of 52 days before the daily cases dropped to zero and the final cumulative case number of 200, which were 25 more days and 36 fewer cases than the real situation, respectively. Targeted interventions could benefit the prevention and control of COVID-19 outbreak while safeguarding public health and mitigating impacts on people's livelihood.",
        "DOI": "10.3967/bes2022.057",
        "paper_author": "WU J.W.",
        "affiliation_name": "Chinese Center for Disease Control and Prevention",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60031363",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Mission Accomplished? Reflecting on 60 Years of Business &amp; Society",
        "publication": "Business and Society",
        "citied_by": "3",
        "cover_date": "2022-05-01",
        "Abstract": "Business & Society’s 60th anniversary affords an opportunity to reflect on the journal’s achievements in the context of the wider field. We analyze editorial commentaries to map the evolving mission of the journal, assess the achievement of the journal’s mission through a thematic analysis of published articles, and examine Business & Society’s distinctiveness relative to peer journals using a machine learning approach. Our analysis highlights subtle shifts in Business & Society’s mission and content over time, reflecting variation in the relative emphasis on scholarly quality versus policy/practice relevance, and building the journal and its academic community versus addressing issues of concern to wider society. While Business & Society’s intended missions have been substantially and sequentially achieved, an increased emphasis on the society-business nexus and a critical approach to interdisciplinarity could further enhance Business & Society’s leading role within business and society research and attract new generations of contributors and readers.",
        "DOI": "10.1177/00076503221098911",
        "paper_author": "Brammer S.",
        "affiliation_name": "University of Bath",
        "affiliation_city": "Bath",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60030480",
        "affiliation_state": "Somerset"
    },
    {
        "paper_title": "Low-Overhead Reinforcement Learning-Based Power Management Using 2QoSM",
        "publication": "Journal of Low Power Electronics and Applications",
        "citied_by": "2",
        "cover_date": "2022-05-01",
        "Abstract": "With the computational systems of even embedded devices becoming ever more powerful, there is a need for more effective and pro-active methods of dynamic power management. The work presented in this paper demonstrates the effectiveness of a reinforcement-learning based dynamic power manager placed in a software framework. This combination of Q-learning for determining policy and the software abstractions provide many of the benefits of co-design, namely, good performance, responsiveness and application guidance, with the flexibility of easily changing policies or platforms. The Q-learning based Quality of Service Manager (2QoSM) is implemented on an autonomous robot built on a complex, powerful embedded single-board computer (SBC) and a high-resolution path-planning algorithm. We find that the 2QoSM reduces power consumption up to 42% compared to the Linux on-demand governor and 10.2% over a state-of-the-art situation aware governor. Moreover, the performance as measured by path error is improved by up to 6.1%, all while saving power.",
        "DOI": "10.3390/jlpea12020029",
        "paper_author": "Giardino M.",
        "affiliation_name": "ETH Zürich",
        "affiliation_city": "Zurich",
        "affiliation_country": "Switzerland",
        "affiliation_id": "60025858",
        "affiliation_state": "ZH"
    },
    {
        "paper_title": "COVID-19 Vaccination and Public Health Countermeasures on Variants of Concern in Canada: Evidence from a Spatial Hierarchical Cluster Analysis",
        "publication": "JMIR Public Health and Surveillance",
        "citied_by": "6",
        "cover_date": "2022-05-01",
        "Abstract": "Background: There is mounting evidence that the third wave of COVID-19 incidence is declining, yet variants of concern (VOCs) continue to present public health challenges in Canada. The emergence of VOCs has sparked debate on how to effectively control their impacts on the Canadian population. Objective: Provincial and territorial governments have implemented a wide range of policy measures to protect residents against community transmission of COVID-19, but research examining the specific impact of policy countermeasures on the VOCs in Canada is needed. Our study objective was to identify provinces with disproportionate prevalence of VOCs relative to COVID-19 mitigation efforts in provinces and territories in Canada. Methods: We analyzed publicly available provincial- and territorial-level data on the prevalence of VOCs in relation to mitigating factors, summarized in 3 measures: (1) strength of public health countermeasures (stringency index), (2) the extent to which people moved about outside their homes (mobility index), and (3) the proportion of the provincial or territorial population that was fully vaccinated (vaccine uptake). Using spatial agglomerative hierarchical cluster analysis (unsupervised machine learning), provinces and territories were grouped into clusters by stringency index, mobility index, and full vaccine uptake. The Kruskal-Wallis test was used to compare the prevalence of VOCs (Alpha, or B.1.1.7; Beta, or B.1.351; Gamma, or P.1; and Delta, or B.1.617.2 variants) across the clusters. Results: We identified 3 clusters of vaccine uptake and countermeasures. Cluster 1 consisted of the 3 Canadian territories and was characterized by a higher degree of vaccine deployment and fewer countermeasures. Cluster 2 (located in Central Canada and the Atlantic region) was typified by lower levels of vaccine deployment and moderate countermeasures. The third cluster, which consisted of provinces in the Pacific region, Central Canada, and the Prairies, exhibited moderate vaccine deployment but stronger countermeasures. The overall and variant-specific prevalences were significantly different across the clusters. Conclusions: This “up to the point” analysis found that implementation of COVID-19 public health measures, including the mass vaccination of populations, is key to controlling VOC prevalence rates in Canada. As of June 15, 2021, the third wave of COVID-19 in Canada is declining, and those provinces and territories that had implemented more comprehensive public health measures showed lower VOC prevalence. Public health authorities and governments need to continue to communicate the importance of sociobehavioural preventive measures, even as populations in Canada continue to receive their primary and booster doses of vaccines.",
        "DOI": "10.2196/31968",
        "paper_author": "Adeyinka D.A.",
        "affiliation_name": "University of Saskatchewan, College of Medicine",
        "affiliation_city": "Saskatoon",
        "affiliation_country": "Canada",
        "affiliation_id": "60011553",
        "affiliation_state": "SK"
    },
    {
        "paper_title": "A hybrid artificial neural network: An optimization-based framework for smart groundwater governance",
        "publication": "Water Supply",
        "citied_by": "4",
        "cover_date": "2022-05-01",
        "Abstract": "Given the growing scarcity and strong demand for water resources, the sustainability of water resource management requires an urgent policy of measures to ensure the rational use of these resources. The heterogeneous properties of groundwater systems are related to the dynamic temporal-spatial patterns that cause great difficulty in quantifying their complex processes, while good regional groundwater level forecasts are completely required for managing water resources to guarantee suitable support of water demands within any area. Water managers and farmers need intelligent groundwater and irrigation planning systems and effective mechanisms to benefit from the scientific and technological revolution, particularly the artificial intelligence engines, to enhance the water support in their water use planning practices. Therefore, this work aims to improve the groundwater level prediction based on the previous measures for better planning of hydraulic resource use. For this concern, the suggested method starts with data-preprocessing using the Principal Component Analysis method. Next, we validated the effectiveness of the hybrid artificial neural network, combined with an extended genetic algorithm for the hyperparameters and weight optimization, in predicting the groundwater levels in a selected monitoring well in California. The evaluation results have demonstrated the performance of the optimized ANN-GA model.",
        "DOI": "10.2166/ws.2022.165",
        "paper_author": "El Mezouari A.",
        "affiliation_name": "Université Cadi Ayyad",
        "affiliation_city": "Marakech",
        "affiliation_country": "Morocco",
        "affiliation_id": "60002714",
        "affiliation_state": "Marrakesh-Safi"
    },
    {
        "paper_title": "Organizational Geosocial Network: A Graph Machine Learning Approach Integrating Geographic and Public Policy Information for Studying the Development of Social Organizations in China",
        "publication": "ISPRS International Journal of Geo-Information",
        "citied_by": "6",
        "cover_date": "2022-05-01",
        "Abstract": "This study aims to give an insight into the development trends and patterns of social organizations (SOs) in China from the perspective of network science integrating geography and public policy information embedded in the network structure. Firstly, we constructed a first-of-its-kind database which encompasses almost all social organizations established in China throughout the past decade. Secondly, we proposed four basic structures to represent the homogeneous and heterogeneous networks between social organizations and related social entities, such as government administrations and community members. Then, we pioneered the application of graph models to the field of organizations and embedded the Organizational Geosocial Network (OGN) into a low-dimensional representation of the social entities and relations while preserving their semantic meaning. Finally, we applied advanced graph deep learning methods, such as graph attention networks (GAT) and graph convolutional networks (GCN), to perform exploratory classification tasks by training models with county-level OGNs dataset and make predictions of which geographic region the county-level OGN belongs to. The experiment proves that different regions possess a variety of development patterns and economic structures where local social organizations are embedded, thus forming differential OGN structures, which can be sensed by graph machine learning algorithms and make relatively accurate predictions. To the best of our knowledge, this is the first application of graph deep learning to the construction and representation learning of geosocial network models of social organizations, which has certain reference significance for research in related fields.",
        "DOI": "10.3390/ijgi11050318",
        "paper_author": "Zhao X.",
        "affiliation_name": "China Agricultural University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60013551",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "The Multi-Objective Optimization of Powertrain Design and Energy Management Strategy for Fuel Cell–Battery Electric Vehicle",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "51",
        "cover_date": "2022-05-01",
        "Abstract": "Considering the limited driving range and inconvenient energy replenishment way of battery electric vehicle, fuel cell electric vehicles (FC EVs) are taken as a promising way to meet the requirements for long-distance low-carbon driving. However, due to the limitation of FC power ability, a battery is usually adopted as the supplement power source to fill the gap between the requirement of driving and the serviceability of FC. In consequence, energy management is essential and crucial to an efficient power flow to the wheel. In this paper, a self-optimizing power matching strategy is proposed, considering the energy efficiency and battery degradation, via implementing a deep deterministic policy gradient. Based on the proposed strategy, less energy consumption and longer FC and battery life can be expected in FC EV powertrain with optimal hybridization degree.",
        "DOI": "10.3390/su14106320",
        "paper_author": "Zhou J.",
        "affiliation_name": "Beijing Institute of Technology",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60016835",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Body Calibration: Automatic Inter-Task Mapping between Multi-Legged Robots with Different Embodiments in Transfer Reinforcement Learning",
        "publication": "Actuators",
        "citied_by": "1",
        "cover_date": "2022-05-01",
        "Abstract": "Machine learning algorithms are effective in realizing the programming of robots that behave autonomously for various tasks. For example, reinforcement learning (RL) does not require supervision or data sets; the RL agent explores solutions by itself. However, RL requires a long learning time, particularly for actual robot learning situations. Transfer learning (TL) in RL has been proposed to address this limitation. TL realizes fast adaptation and decreases the problem-solving time by utilizing the knowledge of the policy, value function, and Q-function from RL. Taylor proposed TL using inter-task mapping that defines the correspondence between the state and action between the source and target domains. Inter-task mapping is defined based on human intuition and experience; therefore, the effect of TL may not be obtained. The difference in robot shapes for TL is similar to the cognition in the modification of human body composition, and automatic inter-task mapping can be performed by referring to the body representation that is assumed to be stored in the human brain. In this paper, body calibration is proposed, which refers to the physical expression in the human brain. It realizes automatic inter-task mapping by acquiring data modeled on a body diagram that illustrates human body composition and posture. The proposed method is evaluated in a TL situation from a computer simulation of RL to actual robot control with a multi-legged robot.",
        "DOI": "10.3390/act11050140",
        "paper_author": "Ikeda S.",
        "affiliation_name": "Tokyo Polytechnic University",
        "affiliation_city": "Atsugi",
        "affiliation_country": "Japan",
        "affiliation_id": "60005647",
        "affiliation_state": "Kanagawa"
    },
    {
        "paper_title": "The State of the Art of Data Mining Algorithms for Predicting the COVID-19 Pandemic",
        "publication": "Axioms",
        "citied_by": "3",
        "cover_date": "2022-05-01",
        "Abstract": "Current computer systems are accumulating huge amounts of information in several application domains. The outbreak of COVID-19 has increased rekindled interest in the use of data mining techniques for the analysis of factors that are related to the emergence of an epidemic. Data mining techniques are being used in the analysis and interpretation of information, which helps in the discovery of patterns, planning of isolation policies, and even predicting the speed of proliferation of contagion in a viral disease such as COVID-19. This research provides a comprehensive study of various data mining algorithms that are used in conjunction with epidemiological prediction models. The document considers that there is an opportunity to improve or develop tools that offer an accurate prognosis in the management of viral diseases through the use of data mining tools, based on a comparative study of 35 research papers.",
        "DOI": "10.3390/axioms11050242",
        "paper_author": "Cortés-Martínez K.V.",
        "affiliation_name": "Centro Nacional de Investigación y Desarrollo Tecnológico, Mexico",
        "affiliation_city": "Cuernavaca",
        "affiliation_country": "Mexico",
        "affiliation_id": "60007127",
        "affiliation_state": "MOR"
    },
    {
        "paper_title": "Automatic Debiased Machine Learning of Causal and Structural Effects",
        "publication": "Econometrica",
        "citied_by": "24",
        "cover_date": "2022-05-01",
        "Abstract": "Many causal and structural effects depend on regressions. Examples include policy effects, average derivatives, regression decompositions, average treatment effects, causal mediation, and parameters of economic structural models. The regressions may be high-dimensional, making machine learning useful. Plugging machine learners into identifying equations can lead to poor inference due to bias from regularization and/or model selection. This paper gives automatic debiasing for linear and nonlinear functions of regressions. The debiasing is automatic in using Lasso and the function of interest without the full form of the bias correction. The debiasing can be applied to any regression learner, including neural nets, random forests, Lasso, boosting, and other high-dimensional methods. In addition to providing the bias correction, we give standard errors that are robust to misspecification, convergence rates for the bias correction, and primitive conditions for asymptotic inference for estimators of a variety of estimators of structural and causal effects. The automatic debiased machine learning is used to estimate the average treatment effect on the treated for the NSW job training data and to estimate demand elasticities from Nielsen scanner data while allowing preferences to be correlated with prices and income.",
        "DOI": "10.3982/ECTA18515",
        "paper_author": "Chernozhukov V.",
        "affiliation_name": "Massachusetts Institute of Technology",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States",
        "affiliation_id": "60022195",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Carbon Emission Prediction Model and Analysis in the Yellow River Basin Based on a Machine Learning Method",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "41",
        "cover_date": "2022-05-01",
        "Abstract": "Excessive carbon emissions seriously threaten the sustainable development of society and the environment and have attracted the attention of the international community. The Yellow River Basin is an important ecological barrier and economic development zone in China. Studying the influencing factors of carbon emissions in the Yellow River Basin is of great significance to help China achieve carbon peaking. In this study, quadratic assignment procedure regression analysis was used to analyze the factors influencing carbon emissions in the Yellow River Basin from the perspective of regional differences. Accurate carbon emission prediction models can guide the formulation of emission reduction policies. We propose a machine learning prediction model, namely, the long short-term memory network optimized by the sparrow search algorithm, and apply it to carbon emission prediction in the Yellow River Basin. The results show an increasing trend in carbon emissions in the Yellow River Basin, with significant inter-provincial differences. The carbon emission intensity of the Yellow River Basin decreased from 5.187 t/10,000 RMB in 2000 to 1.672 t/10,000 RMB in 2019, showing a gradually decreasing trend. The carbon emissions of Qinghai are less than one-tenth of those in Shandong, the highest carbon emitter. The main factor contributing to carbon emissions in the Yellow River Basin from 2000 to 2010 was GDP per capita; after 2010, the main factor was population. Compared to the single long short-term memory network, the mean absolute percentage error of the proposed model is reduced by 44.38%.",
        "DOI": "10.3390/su14106153",
        "paper_author": "Zhao J.",
        "affiliation_name": "Zhengzhou University",
        "affiliation_city": "Zhengzhou",
        "affiliation_country": "China",
        "affiliation_id": "60018554",
        "affiliation_state": "Henan"
    },
    {
        "paper_title": "Prediction of Charging Demand of Electric City Buses of Helsinki, Finland by Random Forest",
        "publication": "Energies",
        "citied_by": "14",
        "cover_date": "2022-05-01",
        "Abstract": "Climate change, global warming, pollution, and energy crisis are the major growing concerns of this era, which have initiated the electrification of transport. The electrification of roadway transport has the potential to drastically reduce pollution and the growing demand for energy and to increase the load demand of the power grid, thereby giving a rise to technological and commercial challenges. Thus, charging load prediction is a crucial and demanding issue for maintaining the security and stability of power systems. During recent years, random forest has gained a lot of popularity as a powerful machine learning technique for classification as well as regression analysis. This work develops a random forest (RF)-based approach for predicting charging demand. The proposed method is validated for the prediction of public e-bus charging demand in the city of Helsinki, Finland. The simulation results demonstrate the effectiveness of our scheme.",
        "DOI": "10.3390/en15103679",
        "paper_author": "Deb S.",
        "affiliation_name": "Faculty of Science, Engineering and Medicine",
        "affiliation_city": "Coventry",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60163091",
        "affiliation_state": "West Midlands"
    },
    {
        "paper_title": "International Natural Gas Price Trends Prediction with Historical Prices and Related News",
        "publication": "Energies",
        "citied_by": "13",
        "cover_date": "2022-05-01",
        "Abstract": "Under the idea of low carbon economy, natural gas has drawn widely attention all over the world and becomes one of the fastest growing energies because of its clean, high calorific value, and environmental protection properties. However, policy and political factors, supply-demand relationship and hurricanes can cause the jump in natural gas prices volatility. To address this issue, a deep learning model based on oil and gas news is proposed to predict natural gas price trends in this paper. In this model, news text embedding is conducted by BERT-Base, Uncased on natural gas-related news. Attention model is adopted to balance the weight of the news vector. Meanwhile, corresponding natural gas price embedding is conducted by a BiLSTM module. The Attention-weighted news vectors and price embedding are the inputs of the fused network with transformer is built. BiLSTM is used to extract used price information related with news features. Transformer is employed to capture time series trend of mixed features. Finally, the network achieves an accuracy as 79%, and the performance is better than most traditional machine learning algorithms.",
        "DOI": "10.3390/en15103573",
        "paper_author": "Guan R.",
        "affiliation_name": "Jilin University",
        "affiliation_city": "Changchun",
        "affiliation_country": "China",
        "affiliation_id": "60007711",
        "affiliation_state": "Jilin"
    },
    {
        "paper_title": "Artificial Intelligence Potential in Higher Education Institutions Enhanced Learning Environment in Romania and Serbia",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "66",
        "cover_date": "2022-05-01",
        "Abstract": "In their struggle to offer a sustainable educational system and transversal competencies for market requests, significant transformations characterise the higher education system in Serbia and Romania. According to EU policy, these transformations are related to educational reforms and the introduction of new technology and methodologies in teaching and learning. They are expected to answer to the PISA requirements and to increase the DESI (Digital Economy and Society Index). They are also likely to mitigate the inequity of HEIs (higher education institutions), empowered by a structured, goal-oriented strategy towards agile management in HEIs that is also appropriate for new market demands. Our study is based on an exploratory survey applied to 139 Romanian and Serbian teachers from the Information Technology School—ITS, Belgrade, and Spiru Haret University, Romania. The survey let them provide their knowledge of AI or their perceptions of the difficulties and opportunities of these technologies in HEIs. Our study discovered how difficulties and opportunities associated with AI impact HEIs. This study aims to see how AI might assist higher education in Romania and Serbia. We also considered how they might be integrated with the educational system, and if instructors would utilise them. Developing creative and transversal skills is required to anticipate future breakthroughs and technological possibilitiesThe new methods of education focuses on ethics, values, problem-solving, and daily activities. Students’ learning material, how they might achieve critical abilities, and their educational changes must be addressed in the future. In this environment, colleges must create new digital skills in IA, machine learning, IoT, 5G, the cloud, big data, blockchain, data analysis, using MS Office and other applications, MOOCs, simulation applications, VR/AR, and gamification. They must also develop cross-disciplinary skills and a long-term mindset.",
        "DOI": "10.3390/su14105842",
        "paper_author": "Bucea-Manea-țoniş R.",
        "affiliation_name": "National University of Physical Education and Sports",
        "affiliation_city": "Bucharest",
        "affiliation_country": "Romania",
        "affiliation_id": "112737578",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Survey on Reinforcement Learning Methods in Character Animation",
        "publication": "Computer Graphics Forum",
        "citied_by": "28",
        "cover_date": "2022-05-01",
        "Abstract": "Reinforcement Learning is an area of Machine Learning focused on how agents can be trained to make sequential decisions, and achieve a particular goal within an arbitrary environment. While learning, they repeatedly take actions based on their observation of the environment, and receive appropriate rewards which define the objective. This experience is then used to progressively improve the policy controlling the agent's behavior, typically represented by a neural network. This trained module can then be reused for similar problems, which makes this approach promising for the animation of autonomous, yet reactive characters in simulators, video games or virtual reality environments. This paper surveys the modern Deep Reinforcement Learning methods and discusses their possible applications in Character Animation, from skeletal control of a single, physically-based character to navigation controllers for individual agents and virtual crowds. It also describes the practical side of training DRL systems, comparing the different frameworks available to build such agents.",
        "DOI": "10.1111/cgf.14504",
        "paper_author": "Kwiatkowski A.",
        "affiliation_name": "École Polytechnique",
        "affiliation_city": "Palaiseau",
        "affiliation_country": "France",
        "affiliation_id": "60013425",
        "affiliation_state": "Ile-de-France"
    },
    {
        "paper_title": "Cloud-based Condition Monitoring of Rail Tracks for Trams",
        "publication": "ZWF Zeitschrift fuer Wirtschaftlichen Fabrikbetrieb",
        "citied_by": "0",
        "cover_date": "2022-05-01",
        "Abstract": "The workload of local public passenger transport by rail will increase as a result of urbanization and energy policy trends. For this purpose, an innovative vibration-based method for diagnosing the condition of rail tracks for trams has been developed. The classified measurement data in form of rail meter objects are made available in a cloud-based map view. This allows the analysis of historical and latest measurement data in order to evaluate the necessity or success of rail grinding.",
        "DOI": "10.1515/zwf-2022-1058",
        "paper_author": "Wolf M.",
        "affiliation_name": "Hochschule für Technik, Wirtschaft und Kultur Leipzig",
        "affiliation_city": "Leipzig",
        "affiliation_country": "Germany",
        "affiliation_id": "60031613",
        "affiliation_state": "Sachsen"
    },
    {
        "paper_title": "Forecasting Carbon Price in China: A Multimodel Comparison",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "24",
        "cover_date": "2022-05-01",
        "Abstract": "With the global concern for carbon dioxide, the carbon emission trading market is becoming more and more important. An accurate forecast of carbon price plays a significant role in understanding the dynamics of the carbon trading market and achieving national emission reduction targets. Carbon prices are influenced by many factors, which makes carbon price forecasting a complicated problem. In recent years, deep learning models are widely used in price forecasting, because they have high forecasting accuracy when dealing with nonlinear time series data. In this paper, Multivariate Long Short-Term Memory (LSTM) in deep learning is used to forecast carbon prices in China, which takes into account the factors affecting the carbon price. The historical time series data of carbon prices in Hubei (HBEA) and Guangdong (GDEA) and three traditional energy prices affecting carbon prices from 5 May 2014 to 22 July 2021 are collected to form two data sets. To prove the forecast effect of our model, this paper not only uses Multivariate LSTM, Multilayer Perceptron (MLP), Support Vector Regression (SVR), and Recurrent Neural Network (RNN) to forecast the same data, but also compares the forecast results of Multivariate LSTM with the existing research on HBEA and GDEA forecast based on deep learning recently. The results show that the MAE, MSE, and RMSE obtained by the Multivariate LSTM are all smaller than other prediction models, which proves that the model is more suitable for carbon price forecast and offers a new approach to carbon prices forecast. This research conclusion also provides some policy implications.",
        "DOI": "10.3390/ijerph19106217",
        "paper_author": "Li H.",
        "affiliation_name": "Sichuan Agricultural University",
        "affiliation_city": "Ya'an",
        "affiliation_country": "China",
        "affiliation_id": "60029885",
        "affiliation_state": "Sichuan"
    },
    {
        "paper_title": "Quality Assurance for Performing Arts Education: A Multi‐Dimensional Analysis Approach",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "8",
        "cover_date": "2022-05-01",
        "Abstract": "Senior management in tertiary institutions desires an efficient system that could help them assess and evaluate learning outcomes so that effective policies can be implemented to enhance teaching and learning. This gets intensified as broader issues arise and higher expectations are put on tertiary education—build a creative workforce and adapt to new technologies to analyze the large volume of teaching and learning data. Government and higher education policymakers have to rapidly adjust relevant policies to surmount the challenges from the pandemic and also to keep up with technological advancement. This demands a novel and efficient way for policymakers and senior management to see and gain insights from a large volume of data (e.g., student course and teacher evaluation). In this study, the researchers present such a system through various examples. The findings generated from this study contribute to the scholarship, and they provide a solution to senior management in tertiary institutions wanting to implement effective policies efficiently. The use of online analytical processing, virtual campus, online, and machine learning in education is growing. However, the use of these technology‐enhanced approaches is rare in performing arts education. There has been no in‐depth study, especially on technology‐enhanced learning that leads to the improvement of teaching. This study utilizes a multi‐dimensional analysis approach on the course student evaluation, a key aspect of the teaching and learning quality assurance for higher education. A novel analytical framework is developed and implemented at a leading performing arts university in Asia. It analyzes the course evaluation data of all courses (669 courses and 2664 responses) in the academic year 2018/2019 to make evidence‐based recommendations. Such a framework provides an easy and effective visualization for senior management to identify courses that need closer scrutiny to ascertain whether and what areas of course enhancement measures are warranted.",
        "DOI": "10.3390/app12104813",
        "paper_author": "Li Q.",
        "affiliation_name": "Xuzhou University of Technology",
        "affiliation_city": "Xuzhou",
        "affiliation_country": "China",
        "affiliation_id": "60104265",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "A Safe and Efficient Lane Change Decision-Making Strategy of Autonomous Driving Based on Deep Reinforcement Learning",
        "publication": "Mathematics",
        "citied_by": "28",
        "cover_date": "2022-05-01",
        "Abstract": "As an indispensable branch of machine learning (ML), reinforcement learning (RL) plays a prominent role in the decision-making process of autonomous driving (AD), which enables autonomous vehicles (AVs) to learn an optimal driving strategy through continuous interaction with the environment. This paper proposes a deep reinforcement learning (DRL)-based motion planning strategy for AD tasks in the highway scenarios where an AV merges into two-lane road traffic flow and realizes the lane changing (LC) maneuvers. We integrate the DRL model into the AD system relying on the end-to-end learning method. An improved DRL algorithm based on deep deterministic policy gradient (DDPG) is developed with well-defined reward functions. In particular, safety rules (SR), safety prediction (SP) module and trauma memory (TM) as well as the dynamic potential-based reward shaping (DPBRS) function are adopted to further enhance safety and accelerate learning of the LC behavior. For validation, the proposed DSSTD algorithm is trained and tested on the dual-computer co-simulation platform. The comparative experimental results show that our proposal outperforms other benchmark algorithms in both driving safety and efficiency.",
        "DOI": "10.3390/math10091551",
        "paper_author": "Lv K.",
        "affiliation_name": "Wuhan University of Technology",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60022414",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Partitioning for “Common but Differentiated” Precise Air Pollution Governance: A Combined Machine Learning and Spatial Econometric Approach",
        "publication": "Energies",
        "citied_by": "0",
        "cover_date": "2022-05-01",
        "Abstract": "Effective governance of air pollution requires precise identification of its influencing factors. Most existing studies attempt to identify the socioeconomic factors but lack consideration of multidimensional heterogeneous characteristics. This paper fills this long-ignored research gap by differentiating governance regions with regard to multidimensional heterogeneity characteristics. Decision tree recursive analysis combined with a spatial autoregressive model is used to identify governance factors in China. Empirical results show several interesting findings. First, geographic location, administrative level, economic zones and regional planning are the main heterogeneous features of accurate air pollution governance in Chinese cities. Second, significant influencing factors of air pollution in different delineated regions are identified, especially significant differences between coastal and non-coastal cities. Third, the trends of heterogeneity in urban air governance in China are to some extent consistent with national policies. The approach identifies factors influencing air pollution, thus providing a basis for accurate air pollution governance that has wider applicability.",
        "DOI": "10.3390/en15093346",
        "paper_author": "Yi Y.",
        "affiliation_name": "China University of Geosciences",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60006019",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Predicting the target specialty of referral notes to estimate per-specialty wait times with machine learning",
        "publication": "PLoS ONE",
        "citied_by": "1",
        "cover_date": "2022-05-01",
        "Abstract": "Background Currently, in Canada, existing health administrative data and hospital-inputted portal systems are used to measure the wait times to receiving a procedure or therapy after a specialist visit. However, due to missing and inconsistent labelling, estimating the wait time prior to seeing a specialist physician requires costly manual coding to label primary care referral notes. Methods In this work, we represent the notes using word-count vectors and develop a logistic regression machine learning model to automatically label the target specialist physician from a primary care referral note. These labels are not available in the administrative system. We also study the effects of note length (measured in number of tokens) and dataset size (measured in number of notes per target specialty) on model performance to help other researchers determine if such an approach may be feasible for them. We then calculate the wait time by linking the specialist type from a primary care referral to a full consultation visit held in Ontario, Canada health administrative data. Results For many target specialties, we can reliably (F1Score ≥ 0.70) predict the target specialist type. Doing so enables the automated measurement of wait time from family physician referral to specialist physician visit. Of the six specialties with wait times estimated using both 2008 and 2015 data, two had a substantial increase (defined as a change such that the original value lay outside the 95% confidence interval) in both median and 75th percentile wait times, one had a substantial decrease in both median and 75th percentile wait times, and three has non-substantial increases. Conclusions Automating these wait time measurements, which had previously been too time consuming and costly to evaluate at a population level, can be useful for health policy researchers studying the effects of policy decisions on patient access to care.",
        "DOI": "10.1371/journal.pone.0267964",
        "paper_author": "Abdalla M.",
        "affiliation_name": "University of Toronto",
        "affiliation_city": "Toronto",
        "affiliation_country": "Canada",
        "affiliation_id": "60016849",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "Deep Learning for Predicting Winter Temperature in North China",
        "publication": "Atmosphere",
        "citied_by": "4",
        "cover_date": "2022-05-01",
        "Abstract": "It is difficult to improve the seasonal prediction skill of winter temperature over North China, owing to the complex dynamics of East Asian winter and the relatively low prediction skill level of current climate models. Deep learning (DL) may be an informative and promising tool to enhance seasonal prediction, particularly in regions where the underlying mechanisms are not clear. Here, using a DL model based on the Convolutional Neural Network (CNN), we have found that the prediction skill for North China winter temperature (NCWT) can be extended up to five months by considering the remote impact of the Northeast Pacific sea-surface temperature (SST) on North China. Based on historical simulations of winter temperatures in North China, we selected six CMIP5 models with relatively small deviations for training the CNN, and the period chosen for training was 1852–1991. The ERA5 data during 1995–2017 were utilized to evaluate the performance of the CNN. Our CNN shows the best performance in a recent 10-year period (2008–2017), showing a significantly improved level of NCWT prediction skill with a correlation skill of 0.65 at a 5-month lead time, which is much better than the forecast skill of the state-of-the-art dynamic seasonal prediction system. Heat map analysis was used to explore the possible physical mechanisms associated with the NCWT anomaly from the perspective of the CNN; the results showed that the SST over the Northeast Pacific is highly relevant to NCWT prediction. The Northeast Pacific warming in the boreal summer is related to the development of the El Niño event in the coming winter, which may induce NCWT anomalies by atmospheric teleconnection. Climate model experiments support the role of Northeast Pacific warming in the boreal summer on NCWT. The improved capability for prediction from using the CNN may help to establish the energy policy for the coming winter and reduce the economic losses from extremely cold in North China.",
        "DOI": "10.3390/atmos13050702",
        "paper_author": "Gao L.",
        "affiliation_name": "Nanjing University of Information Science &amp; Technology",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60064143",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "Are Pair Trading Strategies Profitable During COVID-19 Period?",
        "publication": "Journal of Information and Knowledge Management",
        "citied_by": "2",
        "cover_date": "2022-05-01",
        "Abstract": "Pair trading strategy is a well-known profitable strategy in stock, forex, and commodity markets. As most of the world stock markets declined during COVID-19 period, therefore this study is going to observe whether this strategy is still profitable after COVID-19 pandemic. One of the powerful algorithms of DBSCAN under the umbrella of unsupervised machine learning is applied and three clusters were formed by using market and accounting data. The formation of these three clusters was based on book value per share, earning per share, classification of sector, market capitalisation and with other factors formed from PCA on the returns of daily data of six months of the 80 sample firms for year 2019-2020. An average of-0.32% average excess monthly return with Sharpe ratio of-0.0012 and Treynor ratio of-0.0231 is to be observed in COVID-19 pandemic period. However, the result of risk-adjusted performance under Jensen's alpha is observed to be insignificant. The policy implication of this study, for different portfolios and fund managers is suggested to use machine learning approach to get positive and higher returns for their clients.",
        "DOI": "10.1142/S021964922240010X",
        "paper_author": "Sohail M.K.",
        "affiliation_name": "Bahria University",
        "affiliation_city": "Islamabad",
        "affiliation_country": "Pakistan",
        "affiliation_id": "60070613",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "One-Size-Fits-All Policies Are Unacceptable: A Sustainable Management and Decision-Making Model for Schools in the Post-COVID-19 Era",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "0",
        "cover_date": "2022-05-01",
        "Abstract": "This paper proposes a sustainable management and decision-making model for COVID-19 control in schools, which makes improvements to current policies and strategies. It is not a case study of any specific school or country. The term one-size-fits-all has two meanings: being blind to the pandemic, and conducting inflexible and harsh policies. The former strategy leads to more casualties and does potential harm to children. Conversely, under long-lasting strict policies, people feel exhausted. Therefore, some administrators pretend that they are working hard for COVID-19 control, and people pretend to follow pandemic control rules. The proposed model helps to alleviate these problems and improve management efficiency. A customized queue model is introduced to control social gatherings. An indoor–outdoor tracking system is established. Based on tracing data, we can assess people’s infection risk, and allocate medical resources more effectively in case of emergency. We consider both social and technical feasibility. Test results demonstrate the improvements and effectiveness of the model. In conclusion, the model has patched up certain one-size-fits-all strategies to balance pandemic control and normal life.",
        "DOI": "10.3390/ijerph19105913",
        "paper_author": "Yang C.",
        "affiliation_name": "Southwest University",
        "affiliation_city": "Chongqing",
        "affiliation_country": "China",
        "affiliation_id": "60122052",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Dual Heuristic Dynamic Programming Based Energy Management Control for Hybrid Electric Vehicles",
        "publication": "Energies",
        "citied_by": "33",
        "cover_date": "2022-05-01",
        "Abstract": "This paper investigates an adaptive dynamic programming (ADP)-based energy management control strategy for a series-parallel hybrid electric vehicle (HEV). This strategy can further minimize the equivalent fuel consumption while satisfying the battery level constraints and vehicle power demand. Dual heuristic dynamic programming (DHP) is one of the basic structures of ADP, combining reinforcement learning, dynamic programming (DP) optimization principle, and neural network approximation function, which has higher accuracy with a slightly more complex structure. In this regard, the DHP energy management strategy (EMS) is designed by the backpropagation neural network (BPNN) as an Action network and two Critic networks approximating the control policy and the gradient of value function concerning the state variable. By comparing with the existing results such as HDP-based and rule-based control strategies, the equivalent consumption minimum strategy (ECMS), and reinforcement learning (RL)-based strategy, simulation results verify the robustness of fuel economy and the adaptability of the power-split optimization of the proposed EMS to different driving conditions.",
        "DOI": "10.3390/en15093235",
        "paper_author": "Wang Y.",
        "affiliation_name": "Yanshan University",
        "affiliation_city": "Qinhuangdao",
        "affiliation_country": "China",
        "affiliation_id": "60018465",
        "affiliation_state": "Hebei"
    },
    {
        "paper_title": "A Systematic Literature Review of Learning-Based Traffic Accident Prediction Models Based on Heterogeneous Sources",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "20",
        "cover_date": "2022-05-01",
        "Abstract": "Statistics affirm that almost half of deaths in traffic accidents were vulnerable road users, such as pedestrians, cyclists, and motorcyclists. Despite the efforts in technological infrastructure and traffic policies, the number of victims remains high and beyond expectation. Recent research establishes that determining the causes of traffic accidents is not an easy task because their occurrence depends on one or many factors. Traffic accidents can be caused by, for instance, mechanical problems, adverse weather conditions, mental and physical fatigue, negligence, potholes in the road, among others. At present, the use of learning-based prediction models as mechanisms to reduce the number of traffic accidents is a reality. In that way, the success of prediction models depends mainly on how data from different sources can be integrated and correlated. This study aims to report models, algorithms, data sources, attributes, data collection services, driving simulators, evaluation metrics, percentages of data for training/validation/testing, and others. We found that the performance of a prediction model depends mainly on the quality of its data and a proper data split configuration. The use of real data predominates over data generated by simulators. This work made it possible to determine that future research must point to developing traffic accident prediction models that use deep learning. It must also focus on exploring and using data sources, such as driver data and light conditions, and solve issues related to this type of solution, such as high dimensionality in data and information imbalance.",
        "DOI": "10.3390/app12094529",
        "paper_author": "Marcillo P.",
        "affiliation_name": "Escuela Politécnica Nacional",
        "affiliation_city": "Quito",
        "affiliation_country": "Ecuador",
        "affiliation_id": "60072054",
        "affiliation_state": "Pichincha"
    },
    {
        "paper_title": "The Impact of Macroeconomic Sustainability on Exchange Rate: Hybrid Machine-Learning Approach",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "6",
        "cover_date": "2022-05-01",
        "Abstract": "This paper constructed a robust methodology to investigate the impact of news regarding macroeconomic policies on exchange rate fluctuations, and to examined the applicability of qualitative information alongside historical data to predict exchange rates. To do so, hybrid machine learning algorithms comprised of natural language processing, fuzzy logic, and support vector regression have been constructed. This study emphasizes the significance of qualitative information on investors’ subjective consideration, the decision-making process, and causality on exchange rate volatility. To perceive the causality of expected and unexpected macroeconomic news on exchange rate fluctuations, news regarding the inflation rate, interest rate, unemployment rate, balance of trade, and credit ratings has been extracted from the web. Learning automata has been adopted to construct a unique lexicon for textual analysis. Subjective considerations of decision makers based on news have been evaluated by processing using the prospect theory and composing fuzzy antecedents for the fuzzy logic phase. The fuzzy logic method attained the correlation value between the macroeconomic news and the exchange rate. Finally, support vector regression predicted the exchange rate on a daily basis. The statistical test results indicated a strong correlation between recently published macroeconomic news on daily exchange rate fluctuations and their usability for predicting exchange rates in the short term, while emphasizing the significance of sustainable macroeconomic policies on exchange rate stability.",
        "DOI": "10.3390/su14095357",
        "paper_author": "Erçen H.İ.",
        "affiliation_name": "Yakın Doğu Üniversitesi",
        "affiliation_city": "Nicosia",
        "affiliation_country": "Cyprus",
        "affiliation_id": "60006471",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Blockchain‐Based Reference Architecture for Automated, Transparent, and Notarized Attestation of Compliance Adaptations",
        "publication": "Applied Sciences (Switzerland)",
        "citied_by": "4",
        "cover_date": "2022-05-01",
        "Abstract": "With cloud computing, organizations must comply with applicable laws, policies, and best practices. Companies typically rely on cloud service providers to implement and adopt regulations. This consulting phase is often time‐consuming, costly, and not transparent. Organizations must trust the third party’s implementation and associated documentation processes. To resolve this dilemma, we present a blockchain‐based reference architecture for the automated, transparent, and notarized attestation of such compliance adaptations. Before proposing a solution, our approach is to understand the underlying research context. We conduct a machine‐learning‐supported systematic literature review to create a knowledge base. A reference architecture, including a prototype for configuring intrusion‐detection systems, is developed using design science research. A mixed‐methods‐based approach is used for the evaluation of the proposed architecture. A quantitative survey is then used to show that the user experience of the developed prototype can be rated as positive, with an average value of 0.7. Finally, two focus group discussions are used to analyze the presented prototype qualitatively. As a result, we demonstrate how to actively support secure and trustworthy communication between a cloud service provider and an organization applying blockchain configurations.",
        "DOI": "10.3390/app12094531",
        "paper_author": "Weber T.",
        "affiliation_name": "Universidad Católica de Murcia",
        "affiliation_city": "Murcia",
        "affiliation_country": "Spain",
        "affiliation_id": "60109394",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Thermal coal price forecasting via the neural network",
        "publication": "Intelligent Systems with Applications",
        "citied_by": "48",
        "cover_date": "2022-05-01",
        "Abstract": "Thermal coal price forecasts represent an essential issue to investors and policy makers, given its importance as a strategic energy source. The current work aims at exploring usefulness of non-linear auto-regressive neural networks for this forecast problem based upon a data-set of closing prices recorded on a daily basis of thermal coal traded in China Zhengzhou Commodity Exchange during January 4, 2016 – December 31, 2020, which is an important financial index not sufficiently explored in the literature in terms of its price forecasts. Through testing a variety of model settings over algorithms, delays, hidden neurons, and data splitting ratios, the model that produces performance of good accuracy and stabilities is reached. Particularly, the model has five delays and ten hidden neurons and is constructed with the Levenberg-Marquardt algorithm based on the ratio of 80%–10%–10% of the data for training–validation–testing. It leads to relative root mean square errors of 1.48%, 1.49%, and 1.47% for the training, validation, and testing phases, respectively. Usefulness of neural networks for the price forecast issue of thermal coal is demonstrated. Forecast results here could serve as standalone technical forecasts and be combined with other forecasts when conducting policy analysis that involves forming perspectives of trends in prices.",
        "DOI": "10.1016/j.iswa.2022.200084",
        "paper_author": "Xu X.",
        "affiliation_name": "NC State University",
        "affiliation_city": "Raleigh",
        "affiliation_country": "United States",
        "affiliation_id": "60004923",
        "affiliation_state": "NC"
    },
    {
        "paper_title": "Towards smart energy systems – A survey about the impact of COVID-19 pandemic on renewable energy research",
        "publication": "Energy Strategy Reviews",
        "citied_by": "20",
        "cover_date": "2022-05-01",
        "Abstract": "The COVID-19 pandemic has a significant impact on renewable energy. This work investigates the effect of pandemic on the renewable energy research from four aspects: the regional cooperation model of renewable energy research, the research hotspots of renewable energy during the pandemic, the development trend of renewable energy research hotspots in the post-pandemic, policy recommendations for development in the post-epidemic era. Systematic literature review (SLR), latent semantic analysis (LSA), and machine learning–based analysis (principle component analysis) are used to analyze the relevant literature on the COVID-19 and renewable energy in the Scopus database. The results of geographic visualization analysis show the COVID-19 pandemic has not hindered but promoted bilateral cooperation in the field of renewable energy among the “ the Belt and Road ” partner countries, with China at the core. The results of visual analysis of research hotspots show the research in the field of renewable energy during pandemics is divided into two categories: “opportunities” and “crisis”, and further obtained five categories: sustainable development, environmental management, carbon emission, solar photovoltaic power, and wind power. The results of the keyword evolution map indicate the two main directions of renewable energy research in the post-pandemic: (1) Clean energy investment has become an important measure to revitalize the economy after the epidemic. (2) Energy efficiency research will effectively promote the sustainable development of renewable energy. Finally, we put forward policy suggestions on how to build a smart energy system in the post-epidemic era.",
        "DOI": "10.1016/j.esr.2022.100845",
        "paper_author": "Wang Q.",
        "affiliation_name": "China University of Petroleum (East China)",
        "affiliation_city": "Qingdao",
        "affiliation_country": "China",
        "affiliation_id": "60105111",
        "affiliation_state": "Shandong"
    },
    {
        "paper_title": "Rapid Post-Earthquake Structural Damage Assessment Using Convolutional Neural Networks and Transfer Learning",
        "publication": "Sensors",
        "citied_by": "30",
        "cover_date": "2022-05-01",
        "Abstract": "The adoption of artificial intelligence in post-earthquake inspections and reconnaissance has received considerable attention in recent years, owing to its exponential increase in computation capabilities and inherent potential in addressing disadvantages associated with manual in-spections. Herein, we present the effectiveness of automated deep learning in enhancing the assessment of damage caused by the 2017 Pohang earthquake. Six classical pre-trained convolutional neural network (CNN) models are implemented through transfer learning (TL) on a small dataset, comprising 1780 manually labeled images of structural damage. Feature extraction and fine-tuning TL methods are trained on the image datasets. The performances of various CNN models are compared on a testing image dataset. Results confirm that the MobileNet fine-tuned model offers the best performance. Therefore, the model is further developed as a web-based application for classifying earthquake damage. The severity of damage is quantified by assigning damage assessment values, derived using the CNN model and gradient-weighted class activation mapping. The web-based application can effectively and automatically classify structural damage resulting from earthquakes, rendering it suitable for decision making, such as in resource allocation, policy development, and emergency response.",
        "DOI": "10.3390/s22093471",
        "paper_author": "Ogunjinmi P.D.",
        "affiliation_name": "Kyungpook National University (KNU)",
        "affiliation_city": "Daegu",
        "affiliation_country": "South Korea",
        "affiliation_id": "60012704",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Deterring Deepfake Attacks with an Electrical Network Frequency Fingerprints Approach",
        "publication": "Future Internet",
        "citied_by": "8",
        "cover_date": "2022-05-01",
        "Abstract": "With the fast development of Fifth-/Sixth-Generation (5G/6G) communications and the Internet of Video Things (IoVT), a broad range of mega-scale data applications emerge (e.g., all-weather all-time video). These network-based applications highly depend on reliable, secure, and real-time audio and/or video streams (AVSs), which consequently become a target for attackers. While modern Artificial Intelligence (AI) technology is integrated with many multimedia applications to help enhance its applications, the development of General Adversarial Networks (GANs) also leads to deepfake attacks that enable manipulation of audio or video streams to mimic any targeted person. Deepfake attacks are highly disturbing and can mislead the public, raising further challenges in policy, technology, social, and legal aspects. Instead of engaging in an endless AI arms race “fighting fire with fire”, where new Deep Learning (DL) algorithms keep making fake AVS more realistic, this paper proposes a novel approach that tackles the challenging problem of detecting deepfaked AVS data leveraging Electrical Network Frequency (ENF) signals embedded in the AVS data as a fingerprint. Under low Signal-to-Noise Ratio (SNR) conditions, Short-Time Fourier Transform (STFT) and Multiple Signal Classification (MUSIC) spectrum estimation techniques are investigated to detect the Instantaneous Frequency (IF) of interest. For reliable authentication, we enhanced the ENF signal embedded through an artificial power source in a noisy environment using the spectral combination technique and a Robust Filtering Algorithm (RFA). The proposed signal estimation workflow was deployed on a continuous audio/video input for resilience against frame manipulation attacks. A Singular Spectrum Analysis (SSA) approach was selected to minimize the false positive rate of signal correlations. Extensive experimental analysis for a reliable ENF edge-based estimation in deepfaked multimedia recordings is provided to facilitate the need for distinguishing artificially altered media content.",
        "DOI": "10.3390/fi14050125",
        "paper_author": "Nagothu D.",
        "affiliation_name": "The Thomas J. Watson College of Engineering and Applied Science",
        "affiliation_city": "Binghamton",
        "affiliation_country": "United States",
        "affiliation_id": "60136396",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Deep Spatiotemporal Model for COVID-19 Forecasting",
        "publication": "Sensors",
        "citied_by": "9",
        "cover_date": "2022-05-01",
        "Abstract": "COVID-19 has caused millions of infections and deaths over the last 2 years. Machine learning models have been proposed as an alternative to conventional epidemiologic models in an effort to optimize short-and medium-term forecasts that will help health authorities to optimize the use of policies and resources to tackle the spread of the SARS-CoV-2 virus. Although previous machine learning models based on time pattern analysis for COVID-19 sensed data have shown promising results, the spread of the virus has both spatial and temporal components. This manuscript proposes a new deep learning model that combines a time pattern extraction based on the use of a Long-Short Term Memory (LSTM) Recurrent Neural Network (RNN) over a preceding spatial analysis based on a Convolutional Neural Network (CNN) applied to a sequence of COVID-19 incidence images. The model has been validated with data from the 286 health primary care centers in the Comunidad de Madrid (Madrid region, Spain). The results show improved scores in terms of both root mean square error (RMSE) and explained variance (EV) when compared with previous models that have mainly focused on the temporal patterns and dependencies.",
        "DOI": "10.3390/s22093519",
        "paper_author": "Muñoz-Organero M.",
        "affiliation_name": "Universidad Carlos III de Madrid",
        "affiliation_city": "Getafe",
        "affiliation_country": "Spain",
        "affiliation_id": "60001741",
        "affiliation_state": "Madrid"
    },
    {
        "paper_title": "The AI Economist: Taxation policy design via two-level deep multiagent reinforcement learning",
        "publication": "Science Advances",
        "citied_by": "75",
        "cover_date": "2022-05-01",
        "Abstract": "Artificial intelligence (AI) and reinforcement learning (RL) have improved many areas but are not yet widely adopted in economic policy design, mechanism design, or economics at large. The AI Economist is a two-level, deep RL framework for policy design in which agents and a social planner coadapt. In particular, the AI Economist uses structured curriculum learning to stabilize the challenging two-level, coadaptive learning problem. We validate this framework in the domain of taxation. In one-step economies, the AI Economist recovers the optimal tax policy of economic theory. In spatiotemporal economies, the AI Economist substantially improves both utilitarian social welfare and the trade-off between equality and productivity over baselines. It does so despite emergent tax-gaming strategies while accounting for emergent labor specialization, agent interactions, and behavioral change. These results demonstrate that two-level, deep RL complements economic theory and unlocks an AI-based approach to designing and understanding economic policy.",
        "DOI": "10.1126/sciadv.abk2607",
        "paper_author": "Zheng S.",
        "affiliation_name": "Salesforce.com, Inc.",
        "affiliation_city": "San Francisco",
        "affiliation_country": "United States",
        "affiliation_id": "60079130",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Guiding Efficient, Effective, and Patient-Oriented Electrolyte Replacement in Critical Care: An Artificial Intelligence Reinforcement Learning Approach",
        "publication": "Journal of Personalized Medicine",
        "citied_by": "4",
        "cover_date": "2022-05-01",
        "Abstract": "Both provider-and protocol-driven electrolyte replacement have been linked to the over-prescription of ubiquitous electrolytes. Here, we describe the development and retrospective validation of a data-driven clinical decision support tool that uses reinforcement learning (RL) algorithms to recommend patient-tailored electrolyte replacement policies for ICU patients. We used electronic health records (EHR) data that originated from two institutions (UPHS; MIMIC-IV). The tool uses a set of patient characteristics, such as their physiological and pharmacological state, a pre-defined set of possible repletion actions, and a set of clinical goals to present clinicians with a recommendation for the route and dose of an electrolyte. RL-driven electrolyte repletion substantially reduces the frequency of magnesium and potassium replacements (up to 60%), adjusts the timing of interventions in all three electrolytes considered (potassium, magnesium, and phosphate), and shifts them towards orally administered repletion over intravenous replacement. This shift in recommended treatment limits risk of the potentially harmful effects of over-repletion and implies monetary savings. Overall, the RL-driven electrolyte repletion recommendations reduce excess electrolyte replacements and improve the safety, precision, efficacy, and cost of each electrolyte repletion event, while showing robust performance across patient cohorts and hospital systems.",
        "DOI": "10.3390/jpm12050661",
        "paper_author": "Prasad N.",
        "affiliation_name": "School of Engineering and Applied Science",
        "affiliation_city": "Princeton",
        "affiliation_country": "United States",
        "affiliation_id": "60141284",
        "affiliation_state": "NJ"
    },
    {
        "paper_title": "Predictive Models for Forecasting Public Health Scenarios: Practical Experiences Applied during the First Wave of the COVID-19 Pandemic",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "18",
        "cover_date": "2022-05-01",
        "Abstract": "Background: Forecasting the behavior of epidemic outbreaks is vital in public health. This makes it possible to anticipate the planning and organization of the health system, as well as possible restrictive or preventive measures. During the COVID-19 pandemic, this need for prediction has been crucial. This paper attempts to characterize the alternative models that were applied in the first wave of this pandemic context, trying to shed light that could help to understand them for future practical applications. Methods: A systematic literature search was performed in standardized bibliographic repertoires, using keywords and Boolean operators to refine the findings, and selecting articles according to the main PRISMA 2020 statement recommendations. Results: After identifying models used throughout the first wave of this pandemic (between March and June 2020), we begin by examining standard data-driven epidemiological models, including studies applying models such as SIR (Susceptible-Infected-Recovered), SQUIDER, SEIR, time-dependent SIR, and other alternatives. For data-driven methods, we identify experiences using autoregressive integrated moving average (ARIMA), evolutionary genetic programming machine learning, short-term memory (LSTM), and global epidemic and mobility models. Conclusions: The COVID-19 pandemic has led to intensive and evolving use of alternative infectious disease prediction models. At this point it is not easy to decide which prediction method is the best in a generic way. Moreover, although models such as the LSTM emerge as remarkably versatile and useful, the practical applicability of the alternatives depends on the specific context of the underlying variable and on the information of the target to be prioritized. In addition, the robustness of the assessment is conditioned by heterogeneity in the quality of information sources and differences in the characteristics of disease control interventions. Further comprehensive comparison of the performance of models in comparable situations, assessing their predictive validity, is needed. This will help determine the most reliable and practical methods for application in future outbreaks and eventual pandemics.",
        "DOI": "10.3390/ijerph19095546",
        "paper_author": "Martin-Moreno J.M.",
        "affiliation_name": "Universitat de València",
        "affiliation_city": "Valencia",
        "affiliation_country": "Spain",
        "affiliation_id": "60002644",
        "affiliation_state": "Valencia"
    },
    {
        "paper_title": "Improving Winter Wheat Yield Forecasting Based on Multi-Source Data and Machine Learning",
        "publication": "Agriculture (Switzerland)",
        "citied_by": "20",
        "cover_date": "2022-05-01",
        "Abstract": "To meet the challenges of climate change, population growth, and an increasing food demand, an accurate, timely and dynamic yield estimation of regional and global crop yield is critical to food trade and policy-making. In this study, a machine learning method (Random Forest, RF) was used to estimate winter wheat yield in China from 2014 to 2018 by integrating satellite data, climate data, and geographic information. The results show that the yield estimation accuracy of RF is higher than that of the multiple linear regression method. The yield estimation accuracy can be significantly improved by using climate data and geographic information. According to the model results, the estimation accuracy of winter wheat yield increases dramatically and then flattens out over months; it approached the maximum in March, with R2 and RMSE reaching 0.87 and 488.59 kg/ha, respectively; this model can achieve a better yield forecasting at a large scale two months in advance.",
        "DOI": "10.3390/agriculture12050571",
        "paper_author": "Sun Y.",
        "affiliation_name": "Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60031150",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Dynamic Scheduling Method for Job-Shop Manufacturing Systems by Deep Reinforcement Learning with Proximal Policy Optimization",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "42",
        "cover_date": "2022-05-01",
        "Abstract": "With the rapid development of Industrial 4.0, the modern manufacturing system has been experiencing profoundly digital transformation. The development of new technologies helps to improve the efficiency of production and the quality of products. However, for the increasingly complex production systems, operational decision making encounters more challenges in terms of having sustainable manufacturing to satisfy customers and markets’ rapidly changing demands. Nowadays, rule-based heuristic approaches are widely used for scheduling management in production systems, which, however, significantly depends on the expert domain knowledge. In this way, the efficiency of decision making could not be guaranteed nor meet the dynamic scheduling requirement in the job-shop manufacturing environment. In this study, we propose using deep reinforcement learning (DRL) methods to tackle the dynamic scheduling problem in the job-shop manufacturing system with unexpected machine failure. The proximal policy optimization (PPO) algorithm was used in the DRL framework to accelerate the learning process and improve performance. The proposed method was testified within a real-world dynamic production environment, and it performs better compared with the state-of-the-art methods.",
        "DOI": "10.3390/su14095177",
        "paper_author": "Zhang M.",
        "affiliation_name": "Aston University",
        "affiliation_city": "Birmingham",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60014551",
        "affiliation_state": "West Midlands"
    },
    {
        "paper_title": "Agroforestry Suitability for Planning Site-Specific Interventions Using Machine Learning Approaches",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "11",
        "cover_date": "2022-05-01",
        "Abstract": "Agroforestry in the form of intercropping, boundary plantation, and home garden are parts of traditional land management systems in India. Systematic implementation of agroforestry may help achieve various ecosystem benefits, such as reducing soil erosion, maintaining biodiversity and microclimates, mitigating climate change, and providing food fodder and livelihood. The current study collected ground data for agroforestry patches in the Belpada block, Bolangir district, Odisha state, India. The agroforestry site-suitability analysis employed 15 variables on climate, soil, topography, and proximity, wherein the land use land cover (LULC) map was referred to prescribe the appropriate interventions. The random forest (RF) machine learning model was applied to estimate the relative weight of the determinant variables. The results indicated high accuracy (average suitability >0.87 as indicated by the validation data) and highlighted the dominant influence of the socioeconomic variables compared to soil and climate variables. The results show that >90% of the agricultural land in the study area is suitable for various agroforestry interventions, such as bund plantation and intercropping, based on the cropping intensity. The settlement and wastelands were found to be ideal for home gardens and bamboo block plantations, respectively. The spatially explicit data on agroforestry suitability may provide a baseline map and help the managers and planners. Moreover, the adopted approach can be hosted in cloud-based platforms and applied in the different agro-ecological zones of India, employing the local ground data on various agroforestry interventions. The regional and national scale agroforestry suitability and appropriate interventions map would help the agriculture managers to implement and develop policies.",
        "DOI": "10.3390/su14095189",
        "paper_author": "Singh R.",
        "affiliation_name": "World Agroforestry Centre",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India",
        "affiliation_id": "60097725",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Determinants of Electric Cars Purchase Intention in Poland: Personal Attitudes v. Economic Arguments",
        "publication": "Energies",
        "citied_by": "25",
        "cover_date": "2022-05-01",
        "Abstract": "Urban e-mobility, seen as a part of complex and multidimensional European Green Deal plan, is essential for cities. However, it cannot be implemented without a common social commitment accompanied by a shared, strong belief in its advantages. Even if urban authorities and central governments would encourage their citizens to buy or share an electric vehicle (EV), the shift to EV will not be significant without people convinced that the idea of becoming zero-emission is economically viable and rational to them privately. This is especially true and important in countries like Poland—which is classified as an “EV readiness straggler”. The main purpose of this study is to develop a robust forecasting model with the aid of advanced machine learning methods. Based on the survey conducted, we identified factors useful for predicting consumer behaviour in terms of willingness to purchase an EV. The proposed machine-learning tool (specifically, the Random Forest algorithm) will allow automotive companies to more effectively target factors supporting the promulgation of urban individual e-mobility.",
        "DOI": "10.3390/en15093078",
        "paper_author": "Sobiech-Grabka K.",
        "affiliation_name": "Szkola Glówna Handlowa w Warszawie",
        "affiliation_city": "Warsaw",
        "affiliation_country": "Poland",
        "affiliation_id": "60016640",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "A Study on the Application of GIS and Machine Learning to Predict Flood Areas in Nigeria",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "27",
        "cover_date": "2022-05-01",
        "Abstract": "Floods are one of the most devastating forces in nature. Several approaches for identifying flood-prone locations have been developed to reduce the overall harmful impacts on humans and the environment. However, due to the increased frequency of flooding and related disasters, coupled with the continuous changes in natural and social-economic conditions, it has become vital to predict areas with the highest probability of flooding to ensure effective measures to mitigate impending disasters. This study predicted the flood susceptible areas in Nigeria based on historical flood records from 1985~2020 and various conditioning factors. To evaluate the link between flood incidence and the fifteen (15) explanatory variables, which include climatic, topographic, land use and proximity information, the artificial neural network (ANN) and logistic regression (LR) models were trained and tested to develop a flood susceptibility map. The receiver operating characteristic curve (ROC) and area under the curve (AUC) were used to evaluate both model accuracies. The results show that both techniques can model and predict flood-prone areas. However, the ANN model produced a higher performance and prediction rate than the LR model, 76.4% and 62.5%, respectively. In addition, both models highlighted that those areas with the highest susceptibility to flood are the low-lying regions in the southern extremities and around water areas. From the study, we can establish that machine learning techniques can effectively map and predict flood-prone areas and serve as a tool for developing flood mitigation policies and plans.",
        "DOI": "10.3390/su14095039",
        "paper_author": "Ighile E.H.",
        "affiliation_name": "Nagoya University",
        "affiliation_city": "Nagoya",
        "affiliation_country": "Japan",
        "affiliation_id": "60000264",
        "affiliation_state": "Aichi"
    },
    {
        "paper_title": "Diagnosing Physician Error: A Machine Learning Approach to Low-Value Health Care",
        "publication": "Quarterly Journal of Economics",
        "citied_by": "44",
        "cover_date": "2022-05-01",
        "Abstract": "We use machine learning as a tool to study decision making, focusing specifically on how physicians diagnose heart attack. An algorithmic model of a patient's probability of heart attack allows us to identify cases where physicians' testing decisions deviate from predicted risk. We then use actual health outcomes to evaluate whether those deviations represent mistakes or physicians' superior knowledge. This approach reveals two inefficiencies. Physicians overtest: predictably low-risk patients are tested, but do not benefit. At the same time, physicians undertest: predictably high-risk patients are left untested, and then go on to suffer adverse health events including death. A natural experiment using shift-to-shift testing variation confirms these findings. Simultaneous over- and undertesting cannot easily be explained by incentives alone, and instead point to systematic errors in judgment. We provide suggestive evidence on the psychology underlying these errors. First, physicians use too simple a model of risk. Second, they overweight factors that are salient or representative of heart attack, such as chest pain. We argue health care models must incorporate physician error, and illustrate how policies focused solely on incentive problems can produce large inefficiencies.",
        "DOI": "10.1093/qje/qjab046",
        "paper_author": "Mullainathan S.",
        "affiliation_name": "The University of Chicago",
        "affiliation_city": "Chicago",
        "affiliation_country": "United States",
        "affiliation_id": "60029278",
        "affiliation_state": "IL"
    },
    {
        "paper_title": "Identifying widely disseminated scientific papers on social media",
        "publication": "Information Processing and Management",
        "citied_by": "11",
        "cover_date": "2022-05-01",
        "Abstract": "Identifying widely disseminated papers (WDPs) on social media can help to understand dissemination mechanisms of scientific papers from academia to social media and assist in the formulation of public and science policy. This study applies machine learning methods to explore the possibility of identifying WDPs and to investigate the influence mechanisms of literature-related and social media-related features. A pre-task was first conducted to investigate whether the visibility of scientific papers on social media can be predicted, and the role of various features was analyzed. Then, we defined two predictive tasks for identifying WDPs before and after they are visible on social media. The performance of eight state-of-the-art algorithms was compared in three experiments against the dataset of the oncology field, and the contribution of literature-related and social media-related features in the tasks was explained based on the Shapley additional explanations (SHAP) value. The results show that XGBoost performs better than other algorithms, especially with an F1 score of 0.988 and AUC of 0.998 in the trend prediction task. Nearly all of the literature-related features have great effects on identifying long-term disseminated papers, and most social media-related features play more significant roles in identifying broadly mentioned papers. Moreover, journal features contribute more to identifying papers of social media visibility, while paper features, especially research topics, have a greater influence on identifying WDPs. The number and proportion of academic-related Twitter users have great impacts on the scale and duration of papers’ dissemination. The number and duration of first-generation tweets play critical roles in identifying broadly mentioned and long-term disseminated papers, respectively. This study provides profound insights into the influencing factors in the dissemination of papers from the scientific community to and across social media, and helps to understand the difference in knowledge propagation between academia and the public.",
        "DOI": "10.1016/j.ipm.2022.102945",
        "paper_author": "Ma Y.",
        "affiliation_name": "Nanjing University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60033100",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "A web-based tool for automatically linking clinical trials to their publications",
        "publication": "Journal of the American Medical Informatics Association",
        "citied_by": "9",
        "cover_date": "2022-05-01",
        "Abstract": "Objective: Evidence synthesis teams, physicians, policy makers, and patients and their families all have an interest in following the outcomes of clinical trials and would benefit from being able to evaluate both the results posted in trial registries and in the publications that arise from them. Manual searching for publications arising from a given trial is a laborious and uncertain process. We sought to create a statistical model to automatically identify PubMed articles likely to report clinical outcome results from each registered trial in ClinicalTrials.gov. Materials and Methods: A machine learning-based model was trained on pairs (publications known to be linked to specific registered trials). Multiple features were constructed based on the degree of matching between the PubMed article metadata and specific fields of the trial registry, as well as matching with the set of publications already known to be linked to that trial. Results: Evaluation of the model using known linked articles as gold standard showed that they tend to be top ranked (median best rank = 1.0), and 91% of them are ranked in the top 10. Discussion: Based on this model, we have created a free, public web-based tool that, given any registered trial in ClinicalTrials.gov, presents a ranked list of the PubMed articles in order of estimated probability that they report clinical outcome data from that trial. The tool should greatly facilitate studies of trial outcome results and their relation to the original trial designs.",
        "DOI": "10.1093/jamia/ocab290",
        "paper_author": "Smalheiser N.R.",
        "affiliation_name": "University of Illinois College of Medicine",
        "affiliation_city": "Chicago",
        "affiliation_country": "United States",
        "affiliation_id": "60023868",
        "affiliation_state": "IL"
    },
    {
        "paper_title": "A Hybrid Approach to Explore Public Sentiments on COVID-19",
        "publication": "SN Computer Science",
        "citied_by": "7",
        "cover_date": "2022-05-01",
        "Abstract": "Text processing methods like lexicon-based unsupervised approaches play important roles to quantify public opinions in the textual domain. While these methods have benefit to directly generate sentiment scores from text data based on the word-intensity scores, they perform poorly with shorter unstructured texts like tweets. Besides, these lexicon models often produce poor accuracy with the human annotated datasets. To overcome these limitations of lexicon models, a new hybrid approach has been proposed. This new approach capitalizes the prediction capabilities of two supervised machine learning models to revise the lexicon scores using Bipolar sigmoid function that confirm better accuracy in the sentiment analysis. Three pre-annotated datasets have been used to verify lexicon-based models and the proposed hybrid method. Finally, the proposed method has been applied to the corona-induced tweets, which were collected from Japan, USA, UK, and Australia during January–June 2020. Several sentiment and emotion timeseries have been constructed and evaluated using statistical analysis against three events namely the first declaration of lockdown (FLD), the first declaration of the economic support package (FEP), and the first death-severity (FDS) event. Results showed the significant reduction of the mean negative polarity (meanNeg) in the USA and the significant increase of the ratio between positive and negative tweets (pnRatio) in the UK after the FLD event. The UK people also showed significant reduction of the mean polarity (meanPol) after the FLD and FDS events, respectively. On the other hand, the sadness emotion in the UK after the FEP, the anger and sadness emotions in Australia after the FDS event, and the surprise emotion in the UK after the FDS event have shown significant changes. However, no emotional variables after the FLD event and no sentiment variables after the FEP event have shown any impact among the people in any of the four countries. Surpringly, no events including government responses (FLD, FEP) to COVID-19 showed significant changes to the emotions of Japanese people. Our results can help leader’s policy decisions and can also perform more accurate prediction of the disaster driven public sentiments.",
        "DOI": "10.1007/s42979-022-01112-1",
        "paper_author": "Bashar M.K.",
        "affiliation_name": "The Tokyo Foundation for Policy Research",
        "affiliation_city": "Tokyo",
        "affiliation_country": "Japan",
        "affiliation_id": "60273686",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Hypothesis Learning in Automated Experiment: Application to Combinatorial Materials Libraries",
        "publication": "Advanced Materials",
        "citied_by": "43",
        "cover_date": "2022-05-01",
        "Abstract": "Machine learning is rapidly becoming an integral part of experimental physical discovery via automated and high-throughput synthesis, and active experiments in scattering and electron/probe microscopy. This, in turn, necessitates the development of active learning methods capable of exploring relevant parameter spaces with the smallest number of steps. Here, an active learning approach based on conavigation of the hypothesis and experimental spaces is introduced. This is realized by combining the structured Gaussian processes containing probabilistic models of the possible system's behaviors (hypotheses) with reinforcement learning policy refinement (discovery). This approach closely resembles classical human-driven physical discovery, when several alternative hypotheses realized via models with adjustable parameters are tested during an experiment. This approach is demonstrated for exploring concentration-induced phase transitions in combinatorial libraries of Sm-doped BiFeO3 using piezoresponse force microscopy, but it is straightforward to extend it to higher-dimensional parameter spaces and more complex physical problems once the experimental workflow and hypothesis generation are available.",
        "DOI": "10.1002/adma.202201345",
        "paper_author": "Ziatdinov M.A.",
        "affiliation_name": "Oak Ridge National Laboratory",
        "affiliation_city": "Oak Ridge",
        "affiliation_country": "United States",
        "affiliation_id": "60024266",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "A Hypothesis-Free Bridging of Disease Dynamics and Non-pharmaceutical Policies",
        "publication": "Bulletin of Mathematical Biology",
        "citied_by": "11",
        "cover_date": "2022-05-01",
        "Abstract": "Accurate prediction of the number of daily or weekly confirmed cases of COVID-19 is critical to the control of the pandemic. Existing mechanistic models nicely capture the disease dynamics. However, to forecast the future, they require the transmission rate to be known, limiting their prediction power. Typically, a hypothesis is made on the form of the transmission rate with respect to time. Yet the real form is too complex to be mechanistically modeled due to the unknown dynamics of many influential factors. We tackle this problem by using a hypothesis-free machine-learning algorithm to estimate the transmission rate from data on non-pharmaceutical policies, and in turn forecast the confirmed cases using a mechanistic disease model. More specifically, we build a hybrid model consisting of a mechanistic ordinary differential equation (ODE) model and a gradient boosting model (GBM). To calibrate the parameters, we develop an “inverse method” that obtains the transmission rate inversely from the other variables in the ODE model and then feed it into the GBM to connect with the policy data. The resulting model forecasted the number of daily confirmed cases up to 35 days in the future in the USA with an averaged mean absolute percentage error of 27%. It can identify the most informative predictive variables, which can be helpful in designing improved forecasters as well as informing policymakers.",
        "DOI": "10.1007/s11538-022-01012-8",
        "paper_author": "Wang X.",
        "affiliation_name": "University of Alberta",
        "affiliation_city": "Edmonton",
        "affiliation_country": "Canada",
        "affiliation_id": "60030835",
        "affiliation_state": "AB"
    },
    {
        "paper_title": "Applications of machine learning and deep learning methods for climate change mitigation and adaptation",
        "publication": "Environment and Planning B: Urban Analytics and City Science",
        "citied_by": "11",
        "cover_date": "2022-05-01",
        "Abstract": "Climate change is a global issue that must be considered and addressed immediately. Many articles have been published on climate change mitigation and adaptation. However, new methods are required to explore the complexities of climate change and provide more efficient and effective adaptation and mitigation policies. With the advancement of technology, machine learning (ML) and deep learning (DL) methods have gained considerable popularity in many fields, including climate change. This paper aims to explore the most popular ML and DL methods that have been applied for climate change mitigation and adaptation. Another aim is to determine the most common mitigation and adaptation measures/actions in general, and in urban areas in particular, that have been studied using ML and DL methods. For this purpose, word frequency analysis and topic modeling, specifically the Latent Dirichlet allocation (LDA) as a ML algorithm, are used in this study. The results indicate that the most popular ML technique in both climate change mitigation and adaptation is the Artificial Neural Network. Moreover, among different research areas related to climate change mitigation and adaptation, geoengineering, and land surface temperature are the ones that have used ML and DL algorithms the most.",
        "DOI": "10.1177/23998083221085281",
        "paper_author": "Ladi T.",
        "affiliation_name": "The University of Toledo",
        "affiliation_city": "Toledo",
        "affiliation_country": "United States",
        "affiliation_id": "60021624",
        "affiliation_state": "OH"
    },
    {
        "paper_title": "Automatic inverse treatment planning of Gamma Knife radiosurgery via deep reinforcement learning",
        "publication": "Medical Physics",
        "citied_by": "4",
        "cover_date": "2022-05-01",
        "Abstract": "Purpose: Several inverse planning algorithms have been developed for Gamma Knife (GK) radiosurgery to determine a large number of plan parameters by solving an optimization problem, which typically consists of multiple objectives. The priorities among these objectives need to be repetitively adjusted to achieve a clinically good plan for each patient. This study aimed to achieve automatic and intelligent priority tuning by developing a deep reinforcement learning (DRL)-based method to model the tuning behaviors of human planners. Methods: We built a priority-tuning policy network using deep convolutional neural networks. Its input was a vector composed of multiple plan metrics that were used in our institution for GK plan evaluation. The network can determine which tuning action to take based on the observed quality of the intermediate plan. We trained the network using an end-to-end DRL framework to approximate the optimal action-value function. A scoring function was designed to measure the plan quality to calculate the received reward of a tuning action. Results: Vestibular schwannoma was chosen as the test bed in this study. The number of training, validation and testing cases were 5, 5, and 16, respectively. For these three datasets, the average scores of the initial plans obtained with the same initial priority set were 3.63 ± 1.34, 3.83 ± 0.86 and 4.20 ± 0.78, respectively, while they were improved to 5.28 ± 0.23, 4.97 ± 0.44 and 5.22 ± 0.26 through manual priority tuning by human expert planners. Our network achieved competitive results with 5.42 ± 0.11, 5.10 ± 0. 42, 5.28 ± 0.20, respectively. Conclusions: Our network can generate GK plans of comparable or slightly higher quality than the plans generated by human planners via manual priority tuning for vestibular schwannoma cases. The network can potentially be incorporated into the clinical workflow as planning assistance to improve GK planning efficiency and help to reduce plan quality variation caused by interplanner variability. We also hope that our method can reduce the workload of GK planners and allow them to spend more time on more challenging cases.",
        "DOI": "10.1002/mp.15576",
        "paper_author": "Liu Y.",
        "affiliation_name": "Emory University School of Medicine",
        "affiliation_city": "Atlanta",
        "affiliation_country": "United States",
        "affiliation_id": "60002339",
        "affiliation_state": "GA"
    },
    {
        "paper_title": "Evaluation of shelter dog activity levels before and during COVID-19 using automated analysis",
        "publication": "Applied Animal Behaviour Science",
        "citied_by": "5",
        "cover_date": "2022-05-01",
        "Abstract": "Animal shelters have been found to represent stressful environments for pet dogs, both affecting behavior and influencing welfare. The current COVID-19 pandemic has brought to light new uncertainties in animal sheltering practices which may affect shelter dog behavior in unexpected ways. To evaluate this, we analyzed changes in dog activity levels before COVID-19 and during COVID-19 using an automated video analysis within a large, open-admission animal shelter in New York City, USA. Shelter dog activity was analyzed during two two-week long time periods: (i) just before COVID-19 safety measures were put in place (Feb 26-Mar 17, 2020) and (ii) during the COVID-19 quarantine (July 10–23, 2020). During these two periods, video clips of 15.3 second, on average, were taken of participating kennels every hour from approximately 8 am to 8 pm. Using a two-step filtering approach, a matched sample (based on the number of days of observation) of 34 dogs was defined, consisting of 17 dogs in each group (N1/N2 = 17). An automated video analysis of active/non-active behaviors was conducted and compared to manual coding of activity. The automated analysis validated by comparison to manual coding reaching above 79% accuracy. Significant differences in the patterns of shelter dog activity were observed: less activity was observed in the afternoons before COVID-19 restrictions, while during COVID-19, activity remained at a constant average. Together, these findings suggest that 1) COVID-19 lockdown altered shelter dog in-kennel activity, likely due to changes in the shelter environment and 2) automated analysis can be used as a hands-off tool to monitor activity. While this method of analysis presents immense opportunity for future research, we discuss the limitations of automated analysis and guidelines in the context of shelter dogs that can increase accuracy of detection, as well as reflect on policy changes that might be helpful in mediating canine stress in changing shelter environments.",
        "DOI": "10.1016/j.applanim.2022.105614",
        "paper_author": "Byosiere S.E.",
        "affiliation_name": "Hunter College",
        "affiliation_city": "New York",
        "affiliation_country": "United States",
        "affiliation_id": "60002896",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Young Swiss men's risky single-occasion drinking: Identifying those who do not respond to stricter alcohol policy environments",
        "publication": "Drug and Alcohol Dependence",
        "citied_by": "1",
        "cover_date": "2022-05-01",
        "Abstract": "Background: Previous research has demonstrated a preventive effect of the alcohol policy environment on alcohol consumption. However, little is known about the heterogeneity of this effect. Our aim was to examine the extent of heterogeneity in the relationship between the strictness of alcohol policy environments and heavy drinking and to identify potential moderators of the relationship. Methods: Cross-sectional data from 5986 young Swiss men participating in the cohort study on substance use risk factors (C-SURF) were analysed. The primary outcome was self-reported risky single-occasion drinking in the past 12 months (RSOD, defined as 6 standard drinks or more on a single occasion at least monthly). A previously-used index of alcohol policy environment strictness across Swiss cantons was analysed in conjunction with 21 potential moderator variables. Random forest machine learning captured high-dimensional interaction effects, while individual conditional expectations captured the heterogeneity induced by the interaction effects and identified moderators. Results: Predicted subject-specific absolute risk reductions in RSOD risk ranged from 16.8% to − 4.2%, indicating considerable heterogeneity. Sensation seeking and antisocial personality disorder (ASPD) were major moderators that reduced the preventive relationship between stricter alcohol policy environments and RSOD risk. They also were associated with the paradoxical observation that some individuals displayed increased RSOD risk in stricter alcohol policy environments. Conclusion: Whereas stricter alcohol policy environments were associated with reduced average RSOD risk, additionally addressing the risk conveyed by sensation seeking and ASPD would deliver an interlocking prevention mix against young Swiss men's RSOD.",
        "DOI": "10.1016/j.drugalcdep.2022.109410",
        "paper_author": "Foster S.",
        "affiliation_name": "Universität Zürich",
        "affiliation_city": "Zurich",
        "affiliation_country": "Switzerland",
        "affiliation_id": "60012614",
        "affiliation_state": "ZH"
    },
    {
        "paper_title": "Understanding the role of urban social and physical environment in opioid overdose events using found geospatial data",
        "publication": "Health and Place",
        "citied_by": "24",
        "cover_date": "2022-05-01",
        "Abstract": "Opioid use disorder is a serious public health crisis in the United States. Manifestations such as opioid overdose events (OOEs) vary within and across communities and there is growing evidence that this variation is partially rooted in community-level social and economic conditions. The lack of high spatial resolution, timely data has hampered research into the associations between OOEs and social and physical environments. We explore the use of non-traditional, “found” geospatial data collected for other purposes as indicators of urban social-environmental conditions and their relationships with OOEs at the neighborhood level. We evaluate the use of Google Street View images and non-emergency “311” service requests, along with US Census data as indicators of social and physical conditions in community neighborhoods. We estimate negative binomial regression models with OOE data from first responders in Columbus, Ohio, USA between January 1, 2016, and December 31, 2017. Higher numbers of OOEs were positively associated with service request indicators of neighborhood physical and social disorder and street view imagery rated as boring or depressing based on a pre-trained random forest regression model. Perceived safety, wealth, and liveliness measures from the street view imagery were negatively associated with risk of an OOE. Age group 50–64 was positively associated with risk of an OOE but age 35–49 was negative. White population, percentage of individuals living in poverty, and percentage of vacant housing units were also found significantly positive however, median income and percentage of people with a bachelor's degree or higher were found negative. Our result shows neighborhood social and physical environment characteristics are associated with likelihood of OOEs. Our study adds to the scientific evidence that the opioid epidemic crisis is partially rooted in social inequality, distress and underinvestment. It also shows the previously underutilized data sources hold promise for providing insights into this complex problem to help inform the development of population-level interventions and harm reduction policies.",
        "DOI": "10.1016/j.healthplace.2022.102792",
        "paper_author": "Li Y.",
        "affiliation_name": "The Ohio State University",
        "affiliation_city": "Columbus",
        "affiliation_country": "United States",
        "affiliation_id": "60003500",
        "affiliation_state": "OH"
    },
    {
        "paper_title": "Predictive capacity of four machine learning models for in-hospital postoperative outcomes following total knee arthroplasty",
        "publication": "Journal of Orthopaedics",
        "citied_by": "4",
        "cover_date": "2022-05-01",
        "Abstract": "Background: Machine learning (ML) methods have shown promise in the development of patient-specific predictive models prior to surgical interventions. The purpose of this study was to develop, test, and compare four distinct ML models to predict postoperative parameters following primary total knee arthroplasty (TKA). Methods: Data from the Nationwide Inpatient Sample was used to identify patients undergoing TKA during 2016–2017. Four distinct ML models predictive of mortality, length of stay (LOS), and discharge disposition were developed and validated using 15 predictive patient and hospital-specific factors. Area under the curve of the receiver operating characteristic curve (AUCROC) and accuracy were used as validity metrics, and the strongest predictive variables under each model were assessed. Results: A total of 305,577 patients were included. For mortality, the XGBoost, neural network (NN), and LSVM models all had excellent responsiveness during validation, while random forest (RF) had fair responsiveness. For predicting LOS, all four models had poor responsiveness. For the discharge disposition outcome, the LSVM, NN, and XGBoost models had good responsiveness, while the RF model had poor responsiveness. LSVM and XGBoost had the highest responsiveness for predicting discharge disposition with an AUCROC of 0.747. Discussion: The ML models tested demonstrated a range of poor to excellent responsiveness and accuracy in the prediction of the assessed metrics, with considerable variability noted in the predictive precision between the models. The continued development of ML models should be encouraged, with eventual integration into clinical practice in order to inform patient discussions, management decision making, and health policy.",
        "DOI": "10.1016/j.jor.2022.03.006",
        "paper_author": "Zalikha A.K.",
        "affiliation_name": "Detroit Medical Center",
        "affiliation_city": null,
        "affiliation_country": "United States",
        "affiliation_id": "60032657",
        "affiliation_state": "MI"
    },
    {
        "paper_title": "Multi-landmark environment analysis with reinforcement learning for pelvic abnormality detection and quantification",
        "publication": "Medical Image Analysis",
        "citied_by": "14",
        "cover_date": "2022-05-01",
        "Abstract": "Morphological abnormalities of the femoroacetabular (hip) joint are among the most common human musculoskeletal disorders and often develop asymptomatically at early easily treatable stages. In this paper, we propose an automated framework for landmark-based detection and quantification of hip abnormalities from magnetic resonance (MR) images. The framework relies on a novel idea of multi-landmark environment analysis with reinforcement learning. In particular, we merge the concepts of the graphical lasso and Morris sensitivity analysis with deep neural networks to quantitatively estimate the contribution of individual landmark and landmark subgroup locations to the other landmark locations. Convolutional neural networks for image segmentation are utilized to propose the initial landmark locations, and landmark detection is then formulated as a reinforcement learning (RL) problem, where each landmark-agent can adjust its position by observing the local MR image neighborhood and the locations of the most-contributive landmarks. The framework was validated on T1-, T2- and proton density-weighted MR images of 260 patients with the aim to measure the lateral center-edge angle (LCEA), femoral neck-shaft angle (NSA), and the anterior and posterior acetabular sector angles (AASA and PASA) of the hip, and derive the quantitative abnormality metrics from these angles. The framework was successfully tested using the UNet and feature pyramid network (FPN) segmentation architectures for landmark proposal generation, and the deep Q-network (DeepQN), deep deterministic policy gradient (DDPG), twin delayed deep deterministic policy gradient (TD3), and actor-critic policy gradient (A2C) RL networks for landmark position optimization. The resulting overall landmark detection error of 1.5 mm and angle measurement error of 1.4° indicates a superior performance in comparison to existing methods. Moreover, the automatically estimated abnormality labels were in 95% agreement with those generated by an expert radiologist.",
        "DOI": "10.1016/j.media.2022.102417",
        "paper_author": "Bekkouch I.E.I.",
        "affiliation_name": "Sorbonne Université",
        "affiliation_city": "Paris",
        "affiliation_country": "France",
        "affiliation_id": "60001422",
        "affiliation_state": "Ile-de-France"
    },
    {
        "paper_title": "Lockdown impacts on residential electricity demand in India: A data-driven and non-intrusive load monitoring study using Gaussian mixture models",
        "publication": "Energy Policy",
        "citied_by": "12",
        "cover_date": "2022-05-01",
        "Abstract": "This study evaluates the effect of complete nationwide lockdown in 2020 on residential electricity demand across 13 Indian cities and the role of digitalisation using a public smart meter dataset. We undertake a data-driven approach to explore the energy impacts of work-from-home norms across five dwelling typologies. Our methodology includes climate correction, dimensionality reduction and machine learning-based clustering using Gaussian Mixture Models of daily load curves. Results show that during the lockdown, maximum daily peak demand increased by 150–200% as compared to 2018 and 2019 levels for one room-units (RM1), one bedroom-units (BR1) and two bedroom-units (BR2) which are typical for low- and middle-income families. While the upper-middle- and higher-income dwelling units (i.e., three (3BR) and more-than-three bedroom-units (M3BR)) saw night-time demand rise by almost 44% in 2020, as compared to 2018 and 2019 levels. Our results also showed that new peak demand emerged for the lockdown period for RM1, BR1 and BR2 dwelling typologies. We found that the lack of supporting socioeconomic and climatic data can restrict a comprehensive analysis of demand shocks using similar public datasets, which informed policy implications for India's digitalisation. We further emphasised improving the data quality and reliability for effective data-centric policymaking.",
        "DOI": "10.1016/j.enpol.2022.112886",
        "paper_author": "Debnath R.",
        "affiliation_name": "Cambridge Judge Business School",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60112768",
        "affiliation_state": "Cambridgeshire"
    },
    {
        "paper_title": "On the use of deep learning for fish species recognition and quantification on board fishing vessels",
        "publication": "Marine Policy",
        "citied_by": "29",
        "cover_date": "2022-05-01",
        "Abstract": "The development and effective compliance of efficient fishing policies that guarantee both the sustainability of marine resources and fishing activity is one of the main challenges that policymakers nowadays face. At EU level, successful implementation of the Common Fisheries Policy (CFP) depends, at a large extent, on the capacity to quantify catches on board commercial vessels. Because of the large number of fishing vessels and the high number of trips to be monitored classic, monitoring methods, mainly based on inspections, are not effective. Therefore, the use of electronic devices to quantify fishing catches is gaining relevance. The data provided by such devices, in combination with mathematical models, may be used to assess the state of the different fishing stocks and to optimize the fishing activity. In this work, we consider different algorithms based on Deep Learning (DL) for species identification and length estimation. On the one hand, for the instance segmentation task, we have adapted the Mask R-CNN algorithm to the problem of fish species identification. On the other hand, the MobileNet-V1 convolutional neural network is used for the estimation of the length of each individual. The results show that, when overlapping among individuals is moderate to low, both the identification and length estimation models are able to satisfactorily quantify the catch. In situations where overlapping among individuals is large, results need further improvements.",
        "DOI": "10.1016/j.marpol.2022.105015",
        "paper_author": "Ovalle J.C.",
        "affiliation_name": "CSIC - Instituto de Investigaciones Marinas (IIM)",
        "affiliation_city": "Vigo",
        "affiliation_country": "Spain",
        "affiliation_id": "60015192",
        "affiliation_state": "Galicia"
    },
    {
        "paper_title": "Investigation of a composite two-phase hedging rule policy for a multi reservoir system using streamflow forecast",
        "publication": "Agricultural Water Management",
        "citied_by": "13",
        "cover_date": "2022-05-01",
        "Abstract": "Long-term changes in reservoir inflow due to climate changes and human interference violate the assumptions of hydrologic stationarity especially in the reservoir design. Utilization of uncertain prediction into a reservoir operating rule curves somehow reflects the challenges that imposed by nonstationary conditions. This study proposes a hedging based policy incorporated forecast term to manage release decisions in two separate phases. Hedging is applied firstly regarding to reservoir water level similar to conventional hedging rules and secondary according to an extra simulation in the near future. To determine the time interval of future effects, an exterior optimization model is introduced to handle the trade-off between forecast uncertainty and future information which imposed by forecast horizon. Future inflows are forecasted introducing a model including a wrapper-based feature selection method and AdaBoost.RT as a learning algorithm. The results of applying the model to a real six reservoir system in IRAN showed that incorporating future inflows into the real time decisions significantly improves the total squared relative deficit about 20% and 10% compared to conventional hedging rule curve (CHRC) and standard operation policy as objective function. Also having a glance at the near future reduces the vulnerability of the system about 5% and 27% respectively against CHRC and SOP. The results also showed that, although the SOP reaches to a best reliability of satisfying water demands in total system as 31% and 27% better than CHRC and the proposed two-phase policy, but the number of intensified failures was higher than two others which somehow influences on volume-based indices like vulnerability.",
        "DOI": "10.1016/j.agwat.2022.107542",
        "paper_author": "Mostaghimzadeh E.",
        "affiliation_name": "Shahid Chamran University of Ahvaz",
        "affiliation_city": "Ahvaz",
        "affiliation_country": "Iran",
        "affiliation_id": "60022210",
        "affiliation_state": "Khuzestan"
    },
    {
        "paper_title": "Generative Design by Reinforcement Learning: Enhancing the Diversity of Topology Optimization Designs",
        "publication": "CAD Computer Aided Design",
        "citied_by": "64",
        "cover_date": "2022-05-01",
        "Abstract": "Generative design refers to computational design methods that can automatically conduct design exploration under constraints defined by designers. Among many approaches, topology optimization-based generative designs aim to explore diverse topology designs, which cannot be represented by conventional parametric design approaches. Recently, data-driven topology optimization research has started to exploit artificial intelligence, such as deep learning or machine learning, to improve the capability of design exploration. This study proposes a reinforcement learning (RL) based generative design process, with reward functions maximizing the diversity of topology designs. We formulate generative design as a sequential problem of finding optimal design parameter combinations in accordance with a given reference design. Proximal Policy Optimization is used as the learning framework, which is demonstrated in the case study of an automotive wheel design problem. To reduce the heavy computational burden of the wheel topology optimization process required by our RL formulation, we approximate the optimization process with neural networks. With efficient data preprocessing/augmentation and neural architecture, the neural networks achieve a generalized performance and symmetricity-reserving characteristics. We show that RL-based generative design produces a large number of diverse designs within a short inference time by exploiting GPU in a fully automated manner. It is different from the previous approach using CPU which takes much more processing time and involving human intervention.",
        "DOI": "10.1016/j.cad.2022.103225",
        "paper_author": "Jang S.",
        "affiliation_name": "Seoul National University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea",
        "affiliation_id": "60013682",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A hierarchical reserving model for reported non-life insurance claims",
        "publication": "Insurance: Mathematics and Economics",
        "citied_by": "5",
        "cover_date": "2022-05-01",
        "Abstract": "Traditional non-life reserving models largely neglect the vast amount of information collected over the lifetime of a claim. This information includes covariates describing the policy, claim cause as well as the detailed history collected during a claim's development over time. We present the hierarchical reserving model as a modular framework for integrating a claim's history and claim-specific covariates into the development process. Hierarchical reserving models decompose the joint likelihood of the development process over time. Moreover, they are tailored to the portfolio at hand by adding a layer to the model for each of the events registered during the development of a claim (e.g. settlement, payment). Layers are modelled with statistical learning (e.g. generalized linear models) or machine learning methods (e.g. gradient boosting machines) and use claim-specific covariates. As a result of its flexibility, this framework incorporates many existing reserving models, ranging from aggregate models designed for run-off triangles to individual models using claim-specific covariates. This connection allows us to develop a data-driven strategy for choosing between aggregate and individual reserving; an important decision for reserving practitioners. We illustrate our method with a case study on a real insurance data set and deduce new insights in the covariates driving the development of claims. Moreover, we evaluate the method's performance on a large number of simulated portfolios representing several realistic development scenarios and demonstrate the flexibility and robustness of the hierarchical reserving model.",
        "DOI": "10.1016/j.insmatheco.2022.02.005",
        "paper_author": "Crevecoeur J.",
        "affiliation_name": "KU Leuven",
        "affiliation_city": "Leuven",
        "affiliation_country": "Belgium",
        "affiliation_id": "60025063",
        "affiliation_state": "Vlaams-Brabant"
    },
    {
        "paper_title": "Interactive teaching using human-machine interaction for higher education systems",
        "publication": "Computers and Electrical Engineering",
        "citied_by": "26",
        "cover_date": "2022-05-01",
        "Abstract": "Advances in Interactive Teaching Methodology (ITM) quickly transformshigher education and learning where ITM incorporates web-based face-to-face instruction. IfITM policies are expanding, teachers, lecturers, and administrators must discuss the theoretical foundations of ITM Research. The intellectually enhanced ability makesmore learning choices in some adverse circumstances.Using methods such as hands-on demonstrations, audio-visual aids, and regular teacher-student interaction, teachers actively involve their students in their learning through interactive Teaching. Students are constantly urged to take an active role in class discussions.This paper examines the Interactive Teaching Framework using Human-Machine Interaction (ITF-HMI) for online education Higher Education Systems.In the literature survey, the critical hypotheses used in prior research are the ad hoc models for embracing technologies, the performance studies of knowledge systems, the unified ideology of ITM deployment, the utilization of technology, and the propagation of progress theories. In higher education institutions' instructional phase, sophisticated information and communication technologies with virtual technologies allow an integrated collaboration. This framework discusses how the MOOC platform allows organizing e-learning, taking electronic classroom classes, following online courses, and carrying out synchronous and asynchronous learning. The online tasks created permitted the students to track their progress in all educational activities. The experiment findings helped direct a follow-up analysis in this area with a high acceptance rate amongst the participants. The simulation result of the proposed method enhances performance analysis (98.27%), prediction analysis (97.45%), accuracy analysis (96.27%), resiliency analysis (94.9%), efficiency analysis (98.8%).",
        "DOI": "10.1016/j.compeleceng.2022.107811",
        "paper_author": "Shang H.",
        "affiliation_name": "Zhumadian Vocational and Technical College",
        "affiliation_city": "Zhumadian",
        "affiliation_country": "China",
        "affiliation_id": "112993258",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Managing forests for old-growth attributes better promotes the provision of ecosystem services than current age-based old-growth management",
        "publication": "Forest Ecology and Management",
        "citied_by": "6",
        "cover_date": "2022-05-01",
        "Abstract": "Old-growth forests with complex structural attributes and large trees are rapidly transformed to more homogenous secondary forests through logging, reducing ecosystem services such as carbon storage, water provision and biodiversity. In British Columbia (BC), Canada, a century of logging resulted in strong pressures in the perceived dichotomy of conserving or logging the remaining old-growth. There is an urgency to determine if the current conservation policy for old-growth forests (e.g. old-growth management areas - OGMAs) primarily based on stand age is adequate to protect old-growth structural attributes and their ecosystem services (ES) while leaving opportunities for timber harvesting within timber harvesting tenures. We applied a systematic conservation planning tool (PrioritizR) to design and evaluate attribute-based old-growth reserves as an alternative to the current age-based OGMAs, in a Community Forest (123,695 ha), managed primarily for timber. Old-growth, timber, carbon, tree diversity, and water services were mapped using Aerial Laser Scanning (ALS), and field measurements were used to ground truth with the random forest machine-learning algorithm. Then, “PrioritizR'' was used to identify optimal reserve's networks for age, old-growth attribute, carbon, water, and a combination thereof. We found that the attribute-based old-growth reserves had significantly higher ESs provisioning than the age-based OGMAs. In addition, tradeoffs with timber harvesting were reduced when we simultaneously prioritized old-growth attributes and water values. Finally, while timber harvesting affects the provision of ESs by up to 11.8% ES loss for each 1% timber harvested (∼283,150 m3 of timber), an increase in the area used for old-growth conservation did not affect priority areas for timber harvesting until 22.6% of the study area was set aside (∼30,316 ha) (more than threefold increase of the current OGMAs' area, 8,611 ha). Such results indicate that the conservation of old-growth via attribute-based OGMAs can help maintain the provision of multiple ESs in the landscape, including sustainable timber harvesting.",
        "DOI": "10.1016/j.foreco.2022.120130",
        "paper_author": "de Assis Barros L.",
        "affiliation_name": "University of Northern British Columbia",
        "affiliation_city": "Prince George",
        "affiliation_country": "Canada",
        "affiliation_id": "60011550",
        "affiliation_state": "BC"
    },
    {
        "paper_title": "Forecasting COVID-19 new cases using deep learning methods",
        "publication": "Computers in Biology and Medicine",
        "citied_by": "70",
        "cover_date": "2022-05-01",
        "Abstract": "After nearly two years since the first identification of SARS-CoV-2 virus, the surge in cases because of virus mutations is a cause of grave public health concern across the globe. As a result of this health crisis, predicting the transmission pattern of the virus is one of the most vital tasks for preparing and controlling the pandemic. In addition to mathematical models, machine learning tools, especially deep learning models have been developed for forecasting the trend of the number of patients affected by SARS-CoV-2 with great success. In this paper, three deep learning models, including CNN, LSTM, and the CNN-LSTM have been developed to predict the number of COVID-19 cases for Brazil, India and Russia. We also compare the performance of our models with the previously developed deep learning models and notice significant improvements in prediction performance. Although our models have been used only for forecasting cases in these three countries, the models can be easily applied to datasets of other countries. Among the models developed in this work, the LSTM model has the highest performance when forecasting and shows an improvement in the forecasting accuracy compared with some existing models. The research will enable accurate forecasting of the COVID-19 cases and support the global fight against the pandemic.",
        "DOI": "10.1016/j.compbiomed.2022.105342",
        "paper_author": "Xu L.",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Pittsburgh",
        "affiliation_country": "United States",
        "affiliation_id": "60104842",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "Developing a mental health index using a machine learning approach: Assessing the impact of mobility and lockdown during the COVID-19 pandemic",
        "publication": "Technological Forecasting and Social Change",
        "citied_by": "27",
        "cover_date": "2022-05-01",
        "Abstract": "Governments worldwide have implemented stringent restrictions to curtail the spread of the COVID-19 pandemic. Although beneficial to physical health, these preventive measures could have a profound detrimental effect on the mental health of the population. This study focuses on the impact of lockdowns and mobility restrictions on mental health during the COVID-19 pandemic. We first develop a novel mental health index based on the analysis of data from over three million global tweets using the Microsoft Azure machine learning approach. The computed mental health index scores are then regressed with the lockdown strictness index and Google mobility index using fixed-effects ordinary least squares (OLS) regression. The results reveal that the reduction in workplace mobility, reduction in retail and recreational mobility, and increase in residential mobility (confinement to the residence) have harmed mental health. However, restrictions on mobility to parks, grocery stores, and pharmacy outlets were found to have no significant impact. The proposed mental health index provides a path for theoretical and empirical mental health studies using social media.",
        "DOI": "10.1016/j.techfore.2022.121560",
        "paper_author": "Nanath K.",
        "affiliation_name": "Middlesex University Dubai",
        "affiliation_city": "Dubai",
        "affiliation_country": "United Arab Emirates",
        "affiliation_id": "60113269",
        "affiliation_state": "Dubai"
    },
    {
        "paper_title": "Guest Editorial: Security and Privacy of Federated Learning Solutions for Industrial IoT Applications",
        "publication": "IEEE Transactions on Industrial Informatics",
        "citied_by": "6",
        "cover_date": "2022-05-01",
        "Abstract": "The Industrial Internet of Things (IoT) typically consists of several thousands of heterogeneous devices, such as sensors, actuators, access points, machinery, end-users' handheld equipment, and supply chain. In such an industrial environment, a multitude of data is generated from massive IoT devices, e.g., sensors for monitoring the environment, reading temperature, and gauging pressure. Most of the data are from delay-sensitive and computation-intensive applications, such as real-time manufacturing and automated diagnostics, which require big data analytics with low latency. Machine learning (ML) has been witnessed as an efficient solution for big data analytics. The majority of such ML algorithms are centralized methods, meaning that they first gather data from different users for use as a training dataset, which is placed on the ML server, and then build a model to classify the new data samples by applying the ML algorithms to this training dataset. However, the access to these datasets in the centralized ML methods raises concerns about data privacy for users. Federated learning (FL) was designed to protect data privacy to address a part of these issues. In FL, each participant uses a global training model without uploading their private data to a third-party server. Compared with the conventional ML, FL can preserve data security, especially in terms of participant data during the learning process. In particular, FL can also help in updating server-side data for the global model, and the participant is not required to provide their data. However, in FL, individual computing units may show abnormal actions, such as faulty software, hardware invasions, unreliable communication channels, and malicious samples deliberately crafting the model. To mitigate these challenges, we require robust policies to control the learning phases in FL. Motivated by the abovementioned issues, this special section solicits original research and practical contributions that advance the security and privacy of the FL solutions for industrial IoT applications as follows.",
        "DOI": "10.1109/TII.2021.3128972",
        "paper_author": "Shojafar M.",
        "affiliation_name": "University of Surrey",
        "affiliation_city": "Guildford",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60021097",
        "affiliation_state": "Surrey"
    },
    {
        "paper_title": "Combining Geographical Information System (GIS) and machine learning to monitor and predict vegetation vulnerability: An Empirical Study on Nijhum Dwip, Bangladesh",
        "publication": "Ecological Engineering",
        "citied_by": "10",
        "cover_date": "2022-05-01",
        "Abstract": "Vegetation loss has become a global concern as it is directly and indirectly harmful to all living beings, specifically to humans. By realizing the dimension of this issue, we have developed an Artificial Neural Network (ANN) based model by combining GIS and machine learning to investigate and predict the vegetation vulnerability with reliable accuracy. This model incorporates a sequence of geostatistical analyses, i.e., Normalized Difference Vegetation Index (NDVI), Hot Spot Analysis (Gi*) and Inverse distance weighted (IDW). 8 drivers, used in this study, were shuffled differently to obtain the highest accuracy possible and investigate their influence on land shift. However, the model was implemented on Nijhum Dwip island to quantify its credibility and evaluate the ecological stability and vulnerability status of this island. According to the findings, the island has undergone significant change between 2001 and 2021. The overall vegetative area has increased in this time as a result of the ecological reforestation projects undertaken after 2001. Then, our developed hybrid model was used to simulate the hot spot map of 2021 to quantify the accuracy of the model. Anyway, the kappa statistics was found more than 0.86 with 88.75% overall correctness, and the same weight values were utilized to predict the hotspot map of 2026 and 2031. The predicted maps showed a gradual increase in the vulnerable zones, which is the outcome of the uncontrolled extracting of natural resources. Eventually, the methodological knowledge of this study can help researchers as well as policymakers to estimate vegetation vulnerability and legislate new policies that support sustainable development, and the quantitative knowledge on Nijhum Dwip can facilitate future planning of this island.",
        "DOI": "10.1016/j.ecoleng.2022.106577",
        "paper_author": "Abdullah S.",
        "affiliation_name": "Noakhali Science and Technology University",
        "affiliation_city": "Noakhali",
        "affiliation_country": "Bangladesh",
        "affiliation_id": "60108743",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Life cycle thinking and machine learning for urban metabolism assessment and prediction",
        "publication": "Sustainable Cities and Society",
        "citied_by": "31",
        "cover_date": "2022-05-01",
        "Abstract": "The real-world urban systems represent nonlinear, dynamical, and interconnected urban processes that require better management of their complexity. Thereby, we need to understand, measure, and assess the structure and functioning of the urban processes. We propose an innovative and novel evidence-based methodology to manage the complexity of urban processes, that can enhance their resilience as part of the concept of smart and regenerative urban metabolism with the overarching intention to better achieve sustainability. We couple Life Cycle Thinking and Machine Learning to measure and assess the metabolic processes of the urban core of Lisbon's functional urban area using multidimensional indicators and measures incorporating urban ecosystem services dynamics. We built and trained a multilayer perceptron (MLP) network to identify the metabolic drivers and predict the metabolic changes for the near future (2025). The prediction model's performance was validated using the standard deviations of the prediction errors of the data subsets and the network's training graph. The simulated results show that the urban processes related to employment and unemployment rates (17%), energy systems (10%), sewage and waste management/treatment/recycling, demography & migration, hard/soft cultural assets, and air pollution (7%), education and training, welfare, cultural participation, and habitat-ecosystems (5%), urban safety, water systems, economy, housing quality, urban void, urban fabric, and health services and infrastructure (2%), consists the salient drivers for the urban metabolic changes. The proposed research framework acts as a knowledge-based tool to support effective urban metabolism policies ensuring sustainable and resilient urban development.",
        "DOI": "10.1016/j.scs.2022.103754",
        "paper_author": "Peponi A.",
        "affiliation_name": "Czech University of Life Sciences Prague",
        "affiliation_city": "Prague",
        "affiliation_country": "Czech Republic",
        "affiliation_id": "60024445",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Machine learning integrated design and operation management for resilient circular manufacturing systems",
        "publication": "Computers and Industrial Engineering",
        "citied_by": "21",
        "cover_date": "2022-05-01",
        "Abstract": "The implementation of smart manufacturing devices in the manufacturing industry has provided an effective and intelligent way to monitor their facilities and thus control their integrated manufacturing processes. Despite the ongoing technological advancements, the same industry is often involved with complex problems that are associated with increasing costs related to the persistent deterioration of the manufacturing/remanufacturing systems. In literature, these systems are often called “circular manufacturing systems”. Furthermore, this deterioration affects the quality of the manufactured products. To address these problems, a novel integrated design and operation management-based framework is introduced in order to obtain joint policies for the authorization of production, recycling, maintenance and remanufacturing activities in the context of deteriorating circular manufacturing systems. This framework utilizes a reinforcement learning technique and ad-hoc production control policies, such as Base Stock and CONWIP, in an effort to improve the flexibility and the resilience of the examined systems to the ever-changing customer demand. Finally, a series of simulation experiments evaluate the behavior and the efficiency of the proposed mechanism within the context of single-stage and 2-stage deteriorating manufacturing/remanufacturing systems. Results suggest that the presented approach can effectively generate sufficient inventory of ready-to-be-sold products due to the enhanced awareness of the ongoing customer demand compared to the traditional reinforcement learning joint control.",
        "DOI": "10.1016/j.cie.2022.107971",
        "paper_author": "Paraschos P.D.",
        "affiliation_name": "Democritus University of Thrace",
        "affiliation_city": "Komotini",
        "affiliation_country": "Greece",
        "affiliation_id": "60030988",
        "affiliation_state": "Eastern Macedonia and Thrace"
    },
    {
        "paper_title": "Quantify the role of anthropogenic emission and meteorology on air pollution using machine learning approach: A case study of PM<inf>2.5</inf> during the COVID-19 outbreak in Hubei Province, China",
        "publication": "Environmental Pollution",
        "citied_by": "35",
        "cover_date": "2022-05-01",
        "Abstract": "Air pollution is becoming serious in developing country, and how to quantify the role of local emission and/or meteorological factors is very important for government to implement policy to control pollution. Here, we use a random forest model, a machine learning (ML) approach, combined with a de-weather method to analyze the PM2.5 level during the COVID-19 outbreak in Hubei Province. The results show that changes in anthropogenic emissions have reduced PM2.5 concentrations in February and March 2020 by about 33.3% compared to the same period in 2019, while changes in meteorological conditions have increased PM2.5 concentrations by about 8.8%. Moreover, the impact of meteorological conditions is more significant in the central region, which is likely to be related to regional transport. After excluding the contribution of meteorological conditions, the PM2.5 concentration in Hubei Province in February and March 2020 is lower than the secondary standard of China (35 μ g/m3). Our estimates also indicate that under similar meteorological conditions as in February and March 2019, an emission reduction intensity equivalent to about 48% of the emission reduction intensity during the lockdown may bring the annual average PM2.5 concentration to the standard (35 μ g/m3). Our study shows that machine learning is a powerful tool to quantify the influencing factors of PM2.5, and the results further emphasize the need for scientific emission reduction as well as joint regional control measures in future.",
        "DOI": "10.1016/j.envpol.2022.118932",
        "paper_author": "Liu H.",
        "affiliation_name": "University of Science and Technology of China",
        "affiliation_city": "Hefei",
        "affiliation_country": "China",
        "affiliation_id": "60019118",
        "affiliation_state": "Anhui"
    },
    {
        "paper_title": "Reconciling data-driven crime analysis with human-centered algorithms",
        "publication": "Cities",
        "citied_by": "14",
        "cover_date": "2022-05-01",
        "Abstract": "This study combines traditional statistical methods with machine learning to better understand locally relevant, contextual models for analyzing crime in two urban American cities. Using census tracts as the units of analysis and controlling for several structural characteristics associated with crime, we find that in Milwaukee, Wisconsin, violent crime is associated with concentrated disadvantage, residential stability, ethnic heterogeneity, total population, and spatial lag of violent crime. Yet, the most important variable is the spatial lag of violent crime, followed by residential stability, ethnic heterogeneity, total population, and concentrated disadvantage. In addition, we find that in Chicago, Illinois, violent crime is associated with immigration, owner-occupied housing, proportion in professional occupations, and proportion population with college degree or higher, as well as ethnic heterogeneity, total population, and the spatial lag for violent crime. Machine learning models suggest that for Chicago's violent crime, the most important variable is the spatial lag term for violent crime, followed by total population, immigration, college education or beyond, owner occupancy, ethnic heterogeneity, and employment in professional occupations. The findings for property crime are similar: in Milwaukee, we find that disadvantage, residential stability, ethnic heterogeneity, total population and spatial lag for property crime are significant predictors in the traditional regression models. However, the most important variable for property crime in Milwaukee is the spatial lag term, followed by total population, ethnic heterogeneity, residential stability and disadvantage. The statistically significant predictors of property crime in Chicago include immigration, owner-occupied housing units, living in the same house, proportion of workforce in professional occupations, college education and beyond, total population, and the spatial lag for property crime. In Chicago, the most important variable for property crime is the spatial lag term, then the total population, the proportion of individuals in professional occupations, concentrated immigration, college education and beyond, living in the same house, and the proportion of owner-occupied housing units. Urban planners must consider policies that can effectively reduce nearby crime and violence in all cities that experience high crime levels, but also design locally responsive policies that make sense within a local context: in Milwaukee, residential stability matters more for violent crime than for property crime, while in Chicago, total population is similarly important for both violent crime and property crime. In Milwaukee, ethnic heterogeneity is similarly important for violent and property crime, while in Chicago, ethnic heterogeneity is not a very important variable for violent crime and it is not a significant predictor of property crime. Therefore, urban policy must differently approach social disorganization indicators and support the nuances of the local context for urban planning and policy.",
        "DOI": "10.1016/j.cities.2022.103604",
        "paper_author": "Clancy K.",
        "affiliation_name": "Marquette University",
        "affiliation_city": "Milwaukee",
        "affiliation_country": "United States",
        "affiliation_id": "60015720",
        "affiliation_state": "WI"
    },
    {
        "paper_title": "A Modelling Framework for Evidence-Based Public Health Policy Making",
        "publication": "IEEE Journal of Biomedical and Health Informatics",
        "citied_by": "6",
        "cover_date": "2022-05-01",
        "Abstract": "It is widely recognised that the process of public health policy making (i.e., the analysis, action plan design, execution, monitoring and evaluation of public health policies) should be evidenced based, and supported by data analytics and decision-making tools tailored to it. This is because the management of health conditions and their consequences at a public health policy making level can benefit from such type of analysis of heterogeneous data, including health care devices usage, physiological, cognitive, clinical and medication, personal, behavioural, lifestyle data, occupational and environmental data. In this paper we present a novel approach to public health policy making in a form of an ontology, and an integrated platform for realising this approach. Our solution is model-driven and makes use of big data analytics technology. More specifically, it is based on public health policy decision making (PHPDM) models that steer the public health policy decision making process by defining the data that need to be collected, the ways in which they should be analysed in order to produce the evidence useful for public health policymaking, how this evidence may support or contradict various policy interventions (actions), and the stakeholders involved in the decision-making process. The resulted web-based platform has been implemented using Hadoop, Spark and HBASE, developed in the context of a research programme on public health policy making for the management of hearing loss called EVOTION, funded by the Horizon 2020.",
        "DOI": "10.1109/JBHI.2022.3142503",
        "paper_author": "Prasinos M.",
        "affiliation_name": "University of London",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60033387",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Auto uning of price prediction models for high-frequency trading via reinforcement learning",
        "publication": "Pattern Recognition",
        "citied_by": "13",
        "cover_date": "2022-05-01",
        "Abstract": "In this paper, we propose an online model optimization algorithm based on reinforcement learning for quantitative trading. The combination of prediction model and trading policy is the most commonly used framework in practical quantitative trading. Integrated with machine learning methods, this framework brings huge profits to quantified companies. In the framework, the prediction model is used to predict future trading price trend, and the trading policy is used to determine the price and number of orders. Even though, the shortcomings of machine learning models are obvious, mainly are, (1) Slow prediction speed. Huge human-craft features and model computing cost much time, which is ten times of pure trading policy without model. (2) Poor generalization. This kind of models can hardly adapt to market data in each period, because market traders will change time to time at micro level, thus the distribution of market data will change. But current model is trained on a long period dataset, it achieves best effect at average, but can not adapt to different market at each period. To address this problem, we propose a novel online model optimization algorithm. A light model library will be constructed. Each light model in this library corresponds to a different market distribution. By devising the appropriate reward function via inverse reinforcement learning algorithm, the algorithm can accurately estimate the profits of each model. Then the model can be selected automatically in real-time trading, so that the trading policies can automatically adapt to changes in trading market, overcoming previous shortcoming of manually updating model and slow prediction speed. Experimental results show that the proposed algorithm achieves state-of-the-art performance on China Commodity Futures Market Data.",
        "DOI": "10.1016/j.patcog.2022.108543",
        "paper_author": "Zhang W.",
        "affiliation_name": "Shanghai Jiao Tong University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60025084",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Nepal’s domestic material consumption—projection and causal impact of external financial inflows, services value-added, population, and economic growth",
        "publication": "Environmental Science and Pollution Research",
        "citied_by": "10",
        "cover_date": "2022-05-01",
        "Abstract": "Literature on material flow accounting has increasingly emphasized the need for an equitable resource allocation for least developed countries (LDCs) considering their future growth and the social outcomes (e.g., poverty alleviation) they intend to deliver. This paper aims to project Nepal’s domestic material consumption (DMC)—scale and structure for different economic growth scenarios. We also investigate the causal impact of exogenous factors: (1) external financial inflows, such as the remittance and official development assistance (ODA); (2) services value-added; (3) population; and (4) economic growth on DMC by material types (e.g. biomass, fossil fuels, non-metallic minerals, and metal ores). We use the R tools, ridge regression and its machine learning algorithms, the autoregressive-distributed lag approach, and the abovementioned variables’ time-series data between 1993 and 2017 as methodological and data tools. While Nepal’s absolute DMC will increase even in the low-growth scenario, we found that the biomass-based DMC prevalent in many LDCs, including Nepal, will be non-metallic minerals-based—a material consumption trait of existing middle-income and emerging economies. Despite this, the United Nations’ LDC graduation growth pathway, often assumed to deliver sustainable development objectives by policymakers in LDCs, including Nepal, is material intensive. The increase in the gross domestic product per capita, remittance, and ODA cause a rise in DMC because of their strong correlation and causal relationship. In these circumstances, we suggest policy measures that can leverage present consumption-oriented remittances as a source of investment in up-scaling small-scale modern renewable energy technologies across the residential sector, particularly in rural areas. We suggest this policy measure considering the future rise in non-metallic minerals and the challenges to reduce it because of the rising urbanization.",
        "DOI": "10.1007/s11356-021-18050-9",
        "paper_author": "Baniya B.",
        "affiliation_name": "University of Technology Sydney",
        "affiliation_city": "Sydney",
        "affiliation_country": "Australia",
        "affiliation_id": "60023932",
        "affiliation_state": "NSW"
    },
    {
        "paper_title": "Using evaluation data to predict loan performance among poor borrowers: The case of BRAC’s asset transfer and microcredit programmes",
        "publication": "Development Policy Review",
        "citied_by": "0",
        "cover_date": "2022-05-01",
        "Abstract": "Motivation: Anti-poverty programmes can work as a comprehensive data source of poor households’ economic behaviour and performance, a resource that is particularly scarce in environments without formal credit scores or households that have minimal involvement in economic activities. Purpose: This study examines the potential role of information generated by an anti-poverty programme on self-selection by borrowers (i.e., applying for a loan), screening applicants by lenders (i.e., loan approval), and borrower performance in the microcredit market. Methods and approach: We apply the logistic regression, Least Absolute Shrinkage and Selection Operator (LASSO), and Random Forest (RF) methods to predict self-selection, screening, and post-loan outcomes. Findings: We show that the rate of accurate prediction is about 70% for self-selection and screening. We find that the prediction accuracy rate is 68% for productive use and 91% for repayment difficulty. Objective indicators (e.g., income, assets, age of the household head, savings) stand as the most influential predictors of self-selection, screening, and post-loan outcomes. Policy implications: Development programmes can improve availability of data needed to predict creditworthiness, suggesting that there could be potential to expand credit access among poor borrowers through partnerships between development agencies and financial institutions.",
        "DOI": "10.1111/dpr.12579",
        "paper_author": "Hossain M.",
        "affiliation_name": "International Fund for Agricultural Development",
        "affiliation_city": "Rome",
        "affiliation_country": "Italy",
        "affiliation_id": "60091230",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Political Attacks in 280 Characters or Less: A New Tool for the Automated Classification of Campaign Negativity on Social Media",
        "publication": "American Politics Research",
        "citied_by": "11",
        "cover_date": "2022-05-01",
        "Abstract": "Negativity in election campaign matters. To what extent can the content of social media posts provide a reliable indicator of candidates' campaign negativity? We introduce and critically assess an automated classification procedure that we trained to annotate more than 16,000 tweets of candidates competing in the 2018 Senate Midterms. The algorithm is able to identify the presence of political attacks (both in general, and specifically for character and policy attacks) and incivility. Due to the novel nature of the instrument, the article discusses the external and convergent validity of these measures. Results suggest that automated classifications are able to provide reliable measurements of campaign negativity. Triangulations with independent data show that our automatic classification is strongly associated with the experts’ perceptions of the candidates’ campaign. Furthermore, variations in our measures of negativity can be explained by theoretically relevant factors at the candidate and context levels (e.g., incumbency status and candidate gender); theoretically meaningful trends are also found when replicating the analysis using tweets for the 2020 Senate election, coded using the automated classifier developed for 2018. The implications of such results for the automated coding of campaign negativity in social media are discussed.",
        "DOI": "10.1177/1532673X211055676",
        "paper_author": "Petkevic V.",
        "affiliation_name": "Universiteit van Amsterdam",
        "affiliation_city": "Amsterdam",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60002483",
        "affiliation_state": "Noord-Holland"
    },
    {
        "paper_title": "Effectiveness of embodied conversational agents for managing academic stress at an Indian University (ARU) during COVID-19",
        "publication": "British Journal of Educational Technology",
        "citied_by": "16",
        "cover_date": "2022-05-01",
        "Abstract": "Stress has become one of the major reasons for many mental health related issues among students of all age groups, which has resulted in devastating personal losses including suicide. Societal and familial pressure to succeed is high, particularly in developing countries where education is highly valued as a key enabler. As part of stress management during the COVID-19 pandemic, demand for online intelligent virtual advisors has risen and, consequently, the need for personalised explanation that is culturally sensitive to the user's context is essential to improve the user's understanding of and trust in the recommendations provided by the virtual advisor. This paper presents the mAnaging stRess at University embodied conversational agent (ECA) that has been adapted for Indian university students from an explainable agent that was found to help Western students reduce their stress by providing study tips with explanations based on the student's beliefs and/or goals. We conducted a research study with sixty students which measured the impact of providing three different patterns of tailored explanations (belief-based, goal-based, and belief and goal-based explanation) on the students' intentions to change the recommended behaviours and the relationship built with the ECA. The experimental results indicate that there was stress reduction across all student groups provided with different types of explanations. Further, the students showed trust and a good working alliance with the conversational agent, along with an intention to change behaviour across all types of explanations. However, it was observed that the user context played an important role in behaviour change intention and hence explanations could be tailored further, making them culturally more relevant to Indian students. Practitioner notes What is already known about this topic Embodied conversational agents (ECAs) have been mostly developed, applied and shown to be effective in developed countries. Hence, their design and development are mostly guided by the intended user's needs and preferences. In a Western context, ECAs have been found to be beneficial for reducing study stress in university students. There is a pertinent need for use of low cost, effective technology that can aid academic stress reduction in higher educational institutions in developing countries owing to their high youth populations, lack of adequate mental healthcare facilities and associated social stigma. What this paper adds The adaptation and use of ECAs to reduce study stress in higher education students in a developing country is evaluated. The ECA technology is adapted for an Indian context in terms of its physical appearance, colour, speech dialect and dialog content so that it is culturally more aligned to the target population. The ECA engages in an empathic conversation tailored for the Indian students and their COVID-19 context providing them with explanation-backed behaviour recommendations that take their beliefs and goals into account. The ECA provides three types of explanation: belief-only; goal-only; and both belief and goal. Results of a study carried out in an Indian university with 61 students, randomly assigned to one of the explanation types, to capture their demographics, study stress statistics, behaviour change intentions and trust/working alliance with the conversational agent. The major findings include stress reduction across all explanation groups, development of a positive relationship between the ECA and the students regardless of its explanation pattern, and changes in behaviour intentions across all types of explanations for all recommended behaviours. However, differences in change intentions for certain behaviours indicate further tailoring of explanations is required based on the user context. Implications for practice and/or policy The ECA technology has shown promise in terms of stress reduction amongst Indian students. Higher Education Institutions in developing countries could utilise low-cost and widely accessible ECAs to overcome lack of access to human-based support and reluctance to use available services due to stigmatized attitudes to mental health issues. This technology can be further improved and deployed into a larger number of Indian educational institutions leading to a widespread impact on overall student health and wellbeing. Digital technologies to support mental health have become more prominent during the COVID-19 pandemic, at least in Western countries. The ECA technology evaluated in our study demonstrates its viability and potential value for use in developing countries, with appropriate tailoring.",
        "DOI": "10.1111/bjet.13174",
        "paper_author": "Nelekar S.",
        "affiliation_name": "Birla Institute of Technology and Science, Pilani",
        "affiliation_city": "Pilani",
        "affiliation_country": "India",
        "affiliation_id": "60000414",
        "affiliation_state": "RJ"
    },
    {
        "paper_title": "Aspect Based Twitter Sentiment Analysis on Vaccination and Vaccine Types in COVID-19 Pandemic With Deep Learning",
        "publication": "IEEE Journal of Biomedical and Health Informatics",
        "citied_by": "66",
        "cover_date": "2022-05-01",
        "Abstract": "Due to the COVID-19 pandemic, vaccine development and community vaccination studies are carried out all over the world. At this stage, the opposition to the vaccine seen in the society or the lack of trust in the developed vaccine is an important factor hampering vaccination activities. In this study, aspect-base sentiment analysis was conducted for USA, U.K., Canada, Turkey, France, Germany, Spain and Italy showing the approach of twitter users to vaccination and vaccine types during the COVID-19 period. Within the scope of this study, two datasets in English and Turkish were prepared with 928,402 different vaccine-focused tweets collected by country. In the classification of tweets, 4 different aspects (policy, health, media and other) and 4 different BERT models (mBERT-base, BioBERT, ClinicalBERT and BERTurk) were used. 6 different COVID-19 vaccines with the highest frequency among the datasets were selected and sentiment analysis was made by using Twitter posts regarding these vaccines. To the best of our knowledge, this paper is the first attempt to understand people's views about vaccination and types of vaccines. With the experiments conducted, the results of the views of the people on vaccination and vaccine types were presented according to the countries. The success of the method proposed in this study in the F1 Score was between 84% and 88% in datasets divided by country, while the total accuracy value was 87%.",
        "DOI": "10.1109/JBHI.2021.3133103",
        "paper_author": "Aygun I.",
        "affiliation_name": "Manisa Celâl Bayar Üniversitesi",
        "affiliation_city": "Manisa",
        "affiliation_country": "Turkey",
        "affiliation_id": "60019064",
        "affiliation_state": "Manisa"
    },
    {
        "paper_title": "Deep reinforcement learning based active disturbance rejection control for ship course control",
        "publication": "Neurocomputing",
        "citied_by": "28",
        "cover_date": "2022-05-01",
        "Abstract": "The linear active disturbance rejection control (LADRC) has been applied in many control practices and achieved satisfactory results. However, the controller with fixed parameters cannot achieve the optimal control performance for the controlled plant. In order to optimize the control effort of LADRC with online parameter adjustment, a deep reinforcement learning algorithm, the deep deterministic policy gradient (DDPG), is applied to enhance the LADRC in this paper for the ship course control and to obtain the optimized parameters of LADRC in different conditions. Specifically, the proposed strategy adopts the deep neural network to adjust the control parameters according to the measured states. This makes the proposed method applicable to the nonlinear control systems with high dimensional continuous states and actions. In addition, the stability of the closed-loop system is analyzed based on the Lyapunov method. In simulations, comparisons with Q-learning based LADRC and conventional LADRC controllers are also presented. Simulation results are provided to demonstrate the effectiveness of the proposed method.",
        "DOI": "10.1016/j.neucom.2021.06.096",
        "paper_author": "Qin H.",
        "affiliation_name": "Nankai University",
        "affiliation_city": "Tianjin",
        "affiliation_country": "China",
        "affiliation_id": "60018038",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Machine Learning Empowered Resource Allocation in IRS Aided MISO-NOMA Networks",
        "publication": "IEEE Transactions on Wireless Communications",
        "citied_by": "33",
        "cover_date": "2022-05-01",
        "Abstract": "A novel framework of intelligent reflecting surface (IRS)-aided multiple-input single-output (MISO) non-orthogonal multiple access (NOMA) network is proposed, where a base station (BS) serves multiple clusters with unfixed number of users in each cluster. The goal is to maximize the sum-rate of all users by jointly optimizing the passive beamforming vector at the IRS, decoding order, power allocation coefficient vector and number of clusters, subject to the rate requirements of users. In order to tackle the formulated problem, a three-step approach is proposed. More particularly, a long short-term memory (LSTM) based algorithm is first adopted for predicting the mobility of users. Secondly, a K-means based Gaussian mixture model (K-GMM) algorithm is proposed for user clustering. Thirdly, a deep Q-network (DQN) based algorithm is invoked for jointly determining the phase shift matrix and power allocation policy. Simulation results are provided for demonstrating that the proposed algorithm outperforms the benchmarks, while the throughput gain of 35% can be achieved by invoking NOMA technique instead of orthogonal multiple access (OMA).",
        "DOI": "10.1109/TWC.2021.3122409",
        "paper_author": "Gao X.",
        "affiliation_name": "Queen Mary University of London",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60022109",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "The use of deep learning on endoscopic images to assess the response of rectal cancer after chemoradiation",
        "publication": "Surgical Endoscopy",
        "citied_by": "8",
        "cover_date": "2022-05-01",
        "Abstract": "Background: Accurate response evaluation is necessary to select complete responders (CRs) for a watch-and-wait approach. Deep learning may aid in this process, but so far has never been evaluated for this purpose. The aim was to evaluate the accuracy to assess response with deep learning methods based on endoscopic images in rectal cancer patients after neoadjuvant therapy. Methods: Rectal cancer patients diagnosed between January 2012 and December 2015 and treated with neoadjuvant (chemo)radiotherapy were retrospectively selected from a single institute. All patients underwent flexible endoscopy for response evaluation. Diagnostic performance (accuracy, area under the receiver operator characteristics curve (AUC), positive- and negative predictive values, sensitivities and specificities) of different open accessible deep learning networks was calculated. Reference standard was histology after surgery, or long-term outcome (>2 years of follow-up) in a watch-and-wait policy. Results: 226 patients were included for the study (117(52%) were non-CRs; 109(48%) were CRs). The accuracy, AUC, positive- and negative predictive values, sensitivity and specificity of the different models varied from 0.67–0.75%, 0.76–0.83%, 67–74%, 70–78%, 68–79% to 66–75%, respectively. Overall, EfficientNet-B2 was the most successful model with the highest diagnostic performance. Conclusions: This pilot study shows that deep learning has a modest accuracy (AUCs 0.76-0.83). This is not accurate enough for clinical decision making, and lower than what is generally reported by experienced endoscopists. Deep learning models can however be further improved and may become useful to assist endoscopists in evaluating the response. More well-designed prospective studies are required.",
        "DOI": "10.1007/s00464-021-08685-7",
        "paper_author": "Haak H.E.",
        "affiliation_name": "The Netherlands Cancer Institute",
        "affiliation_city": "Amsterdam",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60015205",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Human-Machine Reinforcement Learning Method for Cooperative Energy Management",
        "publication": "IEEE Transactions on Industrial Informatics",
        "citied_by": "27",
        "cover_date": "2022-05-01",
        "Abstract": "The increasing penetration of distributed energy resources and a large volume of unprecedented data from smart metering infrastructure can help consumers transit to an active role in the smart grid. In this article, we propose a human-machine reinforcement learning (RL) framework in the smart grid context to formulate an energy management strategy for electric vehicles and thermostatically controlled loads aggregators. The proposed model-free method accelerates the decision-making speed by substituting the conventional optimization process, and it is more capable of coping with the diverse system environment via online learning. The human intervention is coordinated with machine learning to: 1) prevent the huge loss during the learning process; 2) realize emergency control; and 3) find preferable control policy. The performance of the proposed human-machine RL framework is verified in case studies. It can be concluded that our proposed method performs better than the conventional deep Q-learning and deep deterministic policy gradient in terms of convergence capability and preferable result exploration. Besides, the proposed method can better deal with emergent events, such as a sudden drop of photovoltaic (PV) output. Compared with the conventional model-based method, there are slight deviations between our method and the optimal solution, but the decision-making time is significantly reduced.",
        "DOI": "10.1109/TII.2021.3105115",
        "paper_author": "Tao Y.",
        "affiliation_name": "Faculty of Engineering",
        "affiliation_city": "Sydney",
        "affiliation_country": "Australia",
        "affiliation_id": "60090755",
        "affiliation_state": "NSW"
    },
    {
        "paper_title": "Linear and non-linear dynamics of the epidemics: System identification based parametric prediction models for the pandemic outbreaks",
        "publication": "ISA Transactions",
        "citied_by": "28",
        "cover_date": "2022-05-01",
        "Abstract": "Coronavirus disease 2019 (COVID-19) has endured constituting formidable economic, social, educational, and phycological challenges for the societies. Moreover, during pandemic outbreaks, the hospitals are overwhelmed with patients requiring more intensive care units and intubation equipment. Therein, to cope with these urgent healthcare demands, the state authorities seek ways to develop policies based on the estimated future casualties. These policies are mainly non-pharmacological policies including the restrictions, curfews, closures, and lockdowns. In this paper, we construct three model structures of the SpInItIbD-N (suspicious Sp, infected In, intensive care It, intubated Ib, and dead D together with the non-pharmacological policies N) holding two key targets. The first one is to predict the future COVID-19 casualties including the intensive care and intubated ones, which directly determine the need for urgent healthcare facilities, and the second one is to analyse the linear and non-linear dynamics of the COVID-19 pandemic under the non-pharmacological policies. In this respect, we have modified the non-pharmacological policies and incorporated them within the models whose parameters are learned from the available data. The trained models with the data released by the Turkish Health Ministry confirmed that the linear SpInItIbD-N model yields more accurate results under the imposed non-pharmacological policies. It is important to note that the non-pharmacological policies have a damping effect on the pandemic casualties and this can dominate the non-linear dynamics. Herein, a model without pharmacological or non-pharmacological policies might have more dominant non-linear dynamics. In addition, the paper considers two machine learning approaches to optimize the unknown parameters of the constructed models. The results show that the recursive neural network has superior performance for learning nonlinear dynamics. However, the batch least squares outperforms in the presence of linear dynamics and stochastic data. The estimated future pandemic casualties with the linear SpInItIbD-N model confirm that the suspicious, infected, and dead casualties converge to zero from 200000, 1400, 200 casualties, respectively. The convergences occur in 120 days under the current conditions.",
        "DOI": "10.1016/j.isatra.2021.08.008",
        "paper_author": "Tutsoy O.",
        "affiliation_name": "Adana Alparslan Türkeş Science and Technology University",
        "affiliation_city": "Adana",
        "affiliation_country": "Turkey",
        "affiliation_id": "60106489",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Nonparametric segmentation methods: Applications of unsupervised machine learning and revealed preference",
        "publication": "American Journal of Agricultural Economics",
        "citied_by": "6",
        "cover_date": "2022-05-01",
        "Abstract": "Many recent efforts by econometricians have focused on supervised machine learning techniques to aid in empirical studies using experimental data. By contrast, this article explores the merits of unsupervised machine learning algorithms for informing ex ante policy design using observational data. We examine the extent to which groups of consumers with differing responses to economic incentives can be identified in a context of fruit and vegetable demand. Two classes of nonparametric algorithms—revealed preference and unsupervised machine learning—are compared for segmenting households in the National Consumer Panel. Nonlinear almost-ideal demand models are estimated for all segments to determine which methods group households into segments with different expenditure and price elasticities. In-sample comparisons and out-of-sample prediction results indicate methods using price-quantity data alone—without demographic, geographic, or other variables—perform better at segmenting households into groups with sizeable differences in price and expenditure responsiveness. These segmentation results suggest considerable heterogeneity in household purchasing behavior of fruits and vegetables.",
        "DOI": "10.1111/ajae.12257",
        "paper_author": "Blumberg J.",
        "affiliation_name": "Colorado State University",
        "affiliation_city": "Fort Collins",
        "affiliation_country": "United States",
        "affiliation_id": "60009226",
        "affiliation_state": "CO"
    },
    {
        "paper_title": "Semiclosed Greenhouse Climate Control Under Uncertainty via Machine Learning and Data-Driven Robust Model Predictive Control",
        "publication": "IEEE Transactions on Control Systems Technology",
        "citied_by": "62",
        "cover_date": "2022-05-01",
        "Abstract": "This work proposes a novel data-driven robust model predictive control (DDRMPC) framework for automatic control of greenhouse in-door climate. The framework integrates dynamic control models of greenhouse temperature, humidity, and CO2 concentration level with data-driven robust optimization models that accurately and rigorously capture uncertainty in weather forecast error. Data-driven uncertainty sets for ambient temperature, solar radiation, and humidity are constructed from historical data by leveraging a machine learning approach, namely, support vector clustering with weighted generalized intersection kernel. A training-calibration procedure that tunes the size of uncertainty sets is implemented to ensure that data-driven uncertainty sets attain an appropriate performance guarantee. In order to solve the optimization problem in DDRMPC, an affine disturbance feedback policy is utilized to obtain tractable approximations of optimal control. A case study of controlling temperature, humidity, and CO2 concentration of a semiclosed greenhouse in New York City is presented. The results show that the DDRMPC approach ends up with 14% and 4% lower total cost than rule-based control and robust model predictive control with L1-norm-based uncertainty set, respectively. The constraint violation probability, which is the percentage of time that the greenhouse system states violate the constraint throughout the whole growing period, for DDRMPC is only 0.39%. Hence, the proposed DDRMPC framework can prevent the greenhouse climate from becoming harmful to plants and fruits. In conclusion, the proposed DDRMPC approach can improve the greenhouse climate control performance and reduce cost compared with other control strategies.",
        "DOI": "10.1109/TCST.2021.3094999",
        "paper_author": "Chen W.H.",
        "affiliation_name": "Cornell University College of Engineering",
        "affiliation_city": "Ithaca",
        "affiliation_country": "United States",
        "affiliation_id": "60104946",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Assessing machine learning for forecasting economic risk: Evidence from an expanded Chinese financial information set",
        "publication": "Finance Research Letters",
        "citied_by": "30",
        "cover_date": "2022-05-01",
        "Abstract": "While data sets used for forecasting can now be greatly improved, expanding data and information size also exposes weaknesses in traditional forecast models. We assess machine learning methods for forecasting monetary policy actions and concomitant macroeconomic risks. We construct an expanded information set on Chinese systemic risk, confirming that this set contains additional information useful for macroeconomic forecasting. We find that machine learning processes offer significant improvement for macroeconomic forecasting, with quantile regression forest exhibiting superior out-of-sample prediction accuracy compared with traditional methodologies. These findings will be of great interest to policy makers and investors.",
        "DOI": "10.1016/j.frl.2021.102273",
        "paper_author": "Duan Y.",
        "affiliation_name": "Nankai University",
        "affiliation_city": "Tianjin",
        "affiliation_country": "China",
        "affiliation_id": "60018038",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Transport infrastructure connectivity and conflict resolution: a machine learning analysis",
        "publication": "Neural Computing and Applications",
        "citied_by": "91",
        "cover_date": "2022-05-01",
        "Abstract": "Transport infrastructure connectivity (TIC) has strong endogeneity issues, making it difficult to directly assess its impact on local conflict resolution. This study presents new evidence of the effects of TIC on conflict resolution by conducting a natural experiment and applying machine learning methods to overcome the endogeneity issue. Based on global conflict data from 2010 to 2017, the empirical results show the following: (1) TIC can significantly improve countries’ global ranking for conflict resolution; in particular, the marginal benefit of developed countries is greater than that of developing countries. (2) The mechanism behind this effect is the promotion of trade facilitation, a more balanced employment ratio across genders, and improved income levels through TIC, which further enhances the conflict governance capacity of countries. In light of the findings, policy-makers should consider the opportunity to combine TIC with greater security for the realization of economic and social benefits, taking into account the significant opportunities for developing countries and the importance of balance across genders and income levels.",
        "DOI": "10.1007/s00521-021-06015-5",
        "paper_author": "Luo J.",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60025278",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "ANFIS Based Reinforcement Learning Strategy for Control A Nonlinear Coupled Tanks System",
        "publication": "Journal of Electrical Engineering and Technology",
        "citied_by": "8",
        "cover_date": "2022-05-01",
        "Abstract": "In this paper, a novel algorithm based machine learning technique for control nonlinear coupled tanks system is presented. An intelligent controller using adaptive neuro-fuzzy inference system (ANFIS) based reinforcement learning is proposed (ANFIS-RL) by representing the nonlinear coupled tanks system as a Markov decision process. A model-free learning algorithm has been used to train a policy that controls the liquid level of the tanks system without the need to determine the dynamic model of the controlled system. Based on the optimal learned policy, which is approximated by ANFIS, the controlled system can perform the best action quickly based on the states of the system. Simulation results demonstrated the feasibility of the proposed algorithm.",
        "DOI": "10.1007/s42835-021-00753-1",
        "paper_author": "Mary A.H.",
        "affiliation_name": "University of Baghdad",
        "affiliation_city": "Baghdad",
        "affiliation_country": "Iraq",
        "affiliation_id": "60071147",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Reinforcement Learning-Based Resource Partitioning for Improving Responsiveness in Cloud Gaming",
        "publication": "IEEE Transactions on Computers",
        "citied_by": "13",
        "cover_date": "2022-05-01",
        "Abstract": "Cloud gaming has been very popular in recent years, but issues relating to maintaining low interaction delay to guarantee satisfactory user experience are still prevalent. We observe that the server-side processing delay in cloud gaming system could be heavily influenced by how the resources are partitioned among processes. However, finding the optimal partitioning policy that minimizes the response delay faces several critical challenges. First, fine-grained resource partitioning is non-trivial due to the limitations of hardwre-based resource isolation techniques. Second, game wokload is highly dynamic and unpredictable, making the design of efficient resource partitioning policy more challenging. In this article, we propose an online resource partitioning framework for reducing response delay in cloud gaming, which has several promising properties. First, we divide the processes into disjoint groups and partition resources among process groups, which greatly simplifies the resource partitioning problem while ensuring high partitioning effectiveness. Second, to tackle dynamic workload changes, we classify game workloads into several clusters and maintain separate process grouping plan for each cluster. Third, we leverage reinforcement learning to adaptively choose the best actions for minimizing response delay in real time. We evaluate the proposed framework in a real cloud gaming environment using several real games. The experimental results show that our approach can reduce the response delay by 22 to 41 percent compared to a system without resource partitioning, and outperforms other resource partitioning policies significantly.",
        "DOI": "10.1109/TC.2021.3070879",
        "paper_author": "Li Y.",
        "affiliation_name": "Nankai University",
        "affiliation_city": "Tianjin",
        "affiliation_country": "China",
        "affiliation_id": "60018038",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Nursing and Informatics for the 21st Century - Embracing a Digital World, 3rd Edition, Book 4: Nursing in an Integrated Digital World that Supports People, Systems, and the Planet",
        "publication": "Nursing and Informatics for the 21st Century - Embracing a Digital World, 3rd Edition, Book 4: Nursing in an Integrated Digital World that Supports People, Systems, and the Planet",
        "citied_by": "1",
        "cover_date": "2022-04-29",
        "Abstract": "In Nursing in an Integrated Digital World that Supports People, Systems, and the Planet, the leading-edge innovators in digital health applications, global thought leaders, and multinational, cooperative research initiatives are woven together against the backdrop of health equity and policy-setting bodies, such as the United Nations and the World Health Organization. As the authors prepared this book, the world is struggling with the core issues of access to care, access to needed medical equipment and supplies, and access to vaccines. This access theme is reflected throughout the policy and world health chapters with an emphasis on how this COVID-19 pandemic is exposing the fissures, divides, unfairness, and unpreparedness that are in play across our globe. Sustainability and global health policy are linked to the new digital technologies in the chapters that illustrate healthcare delivery modalities that nurse innovators are developing, leading, and using to deliver care to hard-to-reach populations for better population health. A trio of chapters focus on the underlying need for standards to underlie nursing care in order to capture the data needed to enable new science and knowledge discoveries. The authors give particular attention to the cautions, potential for harm, and biases that the artificial intelligence technologies of algorithms and machine learning pose in healthcare. Additionally, they have tapped legal experts to review the legal statues, government regulations, and civil rights law in place for patients' rights, privacy, and confidentiality, and consents for the United States, the United Kingdom, and the European Union. The book closes with a chapter written by the editors that envisions the near future-the impact that the new digital technologies will have on how care is delivered, expanding care settings into community and home, virtual monitoring, and patient generated data, as well as the numerous ways that nurses' roles and technology skill sets must increase to support the global goals of equal access to healthcare.",
        "DOI": "10.4324/9781003281047",
        "paper_author": "Delaney C.W.",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Predicting Universal Healthcare Through Health Financial Management for Sustainable Development in BRICS, GCC, and AUKUS Economic Blocks",
        "publication": "Frontiers in Artificial Intelligence",
        "citied_by": "7",
        "cover_date": "2022-04-29",
        "Abstract": "The majority of the world's population is still facing difficulties in getting access to primary healthcare facilities. Universal health coverage (UHC) proposes access to high-quality, affordable primary healthcare for all. The 17 UN sustainable development goals (SDGs) are expected to be executed and achieved by all the 193 countries through national sustainable development strategies and multi-stakeholder partnerships. This article addresses SDG 3.8—access to good quality and affordable healthcare and two subindicators related to societal impact (SDG 3.8.1 and 3.8.2) through two objectives. The first objective is to determine whether health expenditure indicators (HEIs) drive UHC, and the second objective is to analyze the importance of key determinants and their interactions with UHC in three economic blocks: emerging Gulf Cooperation Council (GCC); developing Brazil, Russia, India, China, and South Africa (BRICS) vis-à-vis the developed Australia, UK, and USA (AUKUS). We use the WHO Global Health Indicator database and UHC periodical surveys to evaluate the hypotheses. We apply state-of-the-art machine learning (ML) models and ordinary least square (traditional—OLS regression) methods to see the superiority of artificial intelligence (AI) over traditional ones. The ML Random Forest Tree method is found to be superior to the OLS model in terms of lower root mean square error (RMSE). The ML results indicate that domestic private health expenditure (PVT-D), out-of-pocket expenditure (OOPS) per Capita in US dollars, and voluntary health insurance (VHI) as a percentage of current health expenditure (CHE) are the key factors influencing UHC across the three economic blocks. Our findings have implications for drafting health and finance sector public policies, such as providing affordable social health insurance to the weaker sections of the population, making insurance premiums less expensive and affordable for the masses, and designing healthcare financing policies that are beneficial to the masses. UHC is an important determinant of health for all and requires an in-depth analysis of related factors. Policymakers are often faced with the challenge of prioritizing the economic needs of sectors such as education and food safety, making it difficult for healthcare to receive its due share. In this context, this article attempts to identify the key components that may influence the attainment of UHC and enable policy changes to address them more effectively and efficiently.",
        "DOI": "10.3389/frai.2022.887225",
        "paper_author": "Manoj Kumar M.V.",
        "affiliation_name": "Nitte Meenakshi Institute of Technology",
        "affiliation_city": "Bengaluru",
        "affiliation_country": "India",
        "affiliation_id": "60114322",
        "affiliation_state": "KA"
    },
    {
        "paper_title": "Daily Sales Forecasting for Variable-Priced Items in Retail Business",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "5",
        "cover_date": "2022-04-28",
        "Abstract": "Modern retail business manages products from various sources to serve consumers. To be able to respond to customers' needs, accurate sales forecasting is essential to prepare appropriate levels of stocks. This research aims to find methods and features to forecast the daily sales for a case study retail store chain having many categories of products with varied promotion prices. Three main models are considered: (1) Time series forecasting, i.e., TBATS model, (2) explanatory forecasting method, i.e., multiple linear regression, and (3) machine learning model, i.e., XGBoost. Different types of dependent and independent variables transformation in Stepwise regression are also considered in order to find the most accurate results. Since there were records of the number of Covid-19 cases in Thailand from 2020 and there was also government's welfare money policy during Covid-19 crisis, this paper attempts to find how these variables affects retail sales. Weighted absolute percentage error (WAPE) is used to compare the accuracy among different models.",
        "DOI": "10.1145/3535782.3535794",
        "paper_author": "Auppakorn C.",
        "affiliation_name": "Chulalongkorn University",
        "affiliation_city": "Bangkok",
        "affiliation_country": "Thailand",
        "affiliation_id": "60028190",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A New Neuro-Optimal Nonlinear Tracking Control Method via Integral Reinforcement Learning with Applications to Nuclear Systems",
        "publication": "Neurocomputing",
        "citied_by": "17",
        "cover_date": "2022-04-28",
        "Abstract": "In this paper, a new infinite horizon optimal tracking control method for continuous-time nonlinear systems is given using an actor-critic structure. This present integral reinforcement learning (IRL) method is a novelty method in adaptive dynamic programming (ADP) algorithms and an online policy iteration algorithm. For the optimal tracking problem, the cost function is defined by tracking errors. Consequently, the goal is to minimize tracking errors toward desired trajectories. Since it is hard to solve the Hamilton-Jacobi-Bellman (HJB) equation for continuous-time nonlinear systems control problems, leveraging the actor-critic architecture with neural networks (NNs) to approximate the tracking error performance index and error control law is necessary. Instead of using conventional neural networks, we employ higher-order polynomials in the whole actor-critic architecture. Finally, we apply this new neuro-optimal tracking method to the 2500MW pressurized water reactor (PWR) nuclear power plant, and simulation results are given to demonstrate the effectiveness of the developed method.",
        "DOI": "10.1016/j.neucom.2022.01.034",
        "paper_author": "Zhong W.",
        "affiliation_name": "Harbin University of Science and Technology",
        "affiliation_city": "Harbin",
        "affiliation_country": "China",
        "affiliation_id": "60024758",
        "affiliation_state": "Heilongjiang"
    },
    {
        "paper_title": "Semi-automated Literature Review for Scientific Assessment of Socioeconomic Climate Change Scenarios",
        "publication": "WWW 2022 - Companion Proceedings of the Web Conference 2022",
        "citied_by": "6",
        "cover_date": "2022-04-25",
        "Abstract": "Climate change is now recognized as a global threat, and the literature surrounding it continues to increase exponentially. Expert bodies such as the Intergovernmental Panel on Climate Change (IPCC) are tasked with periodically assessing the literature to extract policy-relevant scientific conclusions that might guide policymakers. However, concerns have been raised that climate change research may be too voluminous for traditional literature review to adequately cover. It has been suggested that practices for literature review for scientific assessment be updated/augmented with semi-automated approaches from bibliometrics or scientometrics. In this study, we explored the feasibility of such recommendations for the scientific assessment of literature around socioeconomic climate change scenarios, so-called Shared Socioeconomic Pathways (SSPs). For automated literature reviews, most methods can be subsumed under two broad categories of classification tasks that use either (1) Natural Language Processing (NLP) or (2) Citation Networks. We performed two levels of classification tasks: (1) identifying SSP articles from a large corpus of climate change research and developing a database of SSP-related articles; (2) classifying SSP articles into different sectoral categories. We applied three machine learning algorithms for the text classification task: Multinomial Naïve Bayes, Logistic Regression, and Linear Support Vector Classification. However, the vocabulary of the SSP literature too closely resembles the vocabulary of broader climate change research for an NLP approach to be effective. We then attempted a citation network approach. We compared two sets of different community detection algorithms (the Louvain algorithm and the Fluid community detection algorithm), with one iteration of each algorithm containing 8 clusters and the next set containing 16. The citation network approach outperformed NLP with respect to false negatives. It also provided the ability to assess the uptake of SSPs across different sectors of climate change research. We concluded that, at the time of the study, the SSP corpus may not yet be large enough or diverse enough from broader climate change research for applying machine learning techniques for automated literature review. However, our research suggests that until there is a critical mass of SSP studies, there is the potential to divide labor between human and machine readers. Some of the data collection tasks currently done by human author teams, such as assessing scenario research, could be semi-automated to ensure and enhance the coverage of the literature. We also drew conclusions about the uptake of the SSP framework over its first 5 years in the broader climate change research literature. We observed that the uptake of SSPs in certain sub-disciplines (e.g., food systems) progressed slowly. Hence, to keep SSPs relevant, it may be fruitful to target SSP studies to particular research communities (e.g., sectors with slower uptake).",
        "DOI": "10.1145/3487553.3524659",
        "paper_author": "Schweizer V.J.",
        "affiliation_name": "University of Waterloo",
        "affiliation_city": "Waterloo",
        "affiliation_country": "Canada",
        "affiliation_id": "60014171",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "Machine learning for accessible web navigation",
        "publication": "Proceedings of the 19th International Web for All Conference, W4A 2022",
        "citied_by": "1",
        "cover_date": "2022-04-25",
        "Abstract": "This research looks at the application of Machine Learning to Web Accessibility. It considers how Machine Learning (ML) can be used to help make the processes of Web Navigation more accessible in line with Web Content Accessibility Guideline (WCAG) 2.4 Navigable, which demands that ways be provided \"to help users navigate, find content, and determine where they are.\"ML techniques such as reinforcement learning have been applied to website navigation in diverse ways. These include goal-directed search to answer questions and task-oriented problems such as booking a flight. Related work includes Web Automation and Testing. These techniques typically involve a state space exploration where actions such as clicking links, filling forms, and pressing buttons move the agent to a new state. The search space can be represented as a directed graph whose nodes represent the state. The choice of action involves a reward in reinforcement learning, which is used to learn behavior. Sequences of actions are collected into policies and the objective is to identify the best policy. For many websites, the potential search space is exceptionally large, and these techniques can provide ways of navigating them more efficiently. While the outcomes of these approaches can help to make the processes of search and task completion easier and thereby help accessibility, it is not the primary focus. The question under consideration is whether these techniques can be adapted to improve the website's navigability from an accessibility perspective. The approach seems promising. The state-space can be explored automatically, and the state representation can then be mined using ML techniques for structure, content and other information, which has the potential to improve accessibility Examples of the application of these approaches, which reflect the goals of the success criteria around WCAG Guideline 2.1 would include the generation of good anchor text for links where this is not provided, optimizing pathways to website functionality, and optimizing the processes of querying the website. There are open questions about how state representation can be enhanced to improve the prospects of reaching goal states. For example, can the process of calculating reward exploit features such as Search Engine Optimization information, which is designed to reflect the purpose of the page directly? Indexing techniques from Information science can serve a similar purpose. From an accessibility standpoint, there are exciting directions to explore. For example, what role can accessibility features, e.g., Headings, Anchor Text, and alternative text, play in calculating rewards. By way of illustration, a case study on one of these page enhancement techniques, DocTTTTTQuery, will be presented. This enhances a document by generating potential queries from its content. The system is trained by matching these generated questions against a set of historical questions asked of the site. This knowledge can then be exploited to lead the user more directly to potential answers to queries. Other studies have shown the effectiveness of this approach. Augmenting the page with these queries has shown improved search performance.",
        "DOI": "10.1145/3493612.3520463",
        "paper_author": "Makati T.",
        "affiliation_name": "Technological University Dublin",
        "affiliation_city": "Dublin",
        "affiliation_country": "Ireland",
        "affiliation_id": "60012873",
        "affiliation_state": "Leinster"
    },
    {
        "paper_title": "Dexterous Manipulation for Multi-Fingered Robotic Hands With Reinforcement Learning: A Review",
        "publication": "Frontiers in Neurorobotics",
        "citied_by": "15",
        "cover_date": "2022-04-25",
        "Abstract": "With the increasing demand for the dexterity of robotic operation, dexterous manipulation of multi-fingered robotic hands with reinforcement learning is an interesting subject in the field of robotics research. Our purpose is to present a comprehensive review of the techniques for dexterous manipulation with multi-fingered robotic hands, such as the model-based approach without learning in early years, and the latest research and methodologies focused on the method based on reinforcement learning and its variations. This work attempts to summarize the evolution and the state of the art in this field and provide a summary of the current challenges and future directions in a way that allows future researchers to understand this field.",
        "DOI": "10.3389/fnbot.2022.861825",
        "paper_author": "Yu C.",
        "affiliation_name": "Institute of Automation Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60018486",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Consumption-Related Health Education Inequality in COVID-19: A Cross-Sectional Study in China",
        "publication": "Frontiers in Public Health",
        "citied_by": "2",
        "cover_date": "2022-04-25",
        "Abstract": "Background: The COVID-19 pandemic influences various aspects of society, especially for people with low socioeconomic status. Health education has been proven to be a critical strategy in preventing a pandemic. However, socioeconomic characteristics may limit health education among low socioeconomic status groups. This study explores consumption-related health education inequality and the factors that contribute to this, which are variable across China during COVID-19. Methods: The 2020 China COVID-19 Survey is a cross-sectional study in China, based on an anonymous online survey from 7,715 samples in 85 cities. It employed machine-learning methods to assess household consumption and other contributing variates associated with health education during the pandemic. Concentration Index (CI) and Horizontal Index (HI) were used to measure consumption-related inequalities in health education, respectively. Moreover, Wagstaff decomposition analysis was employed to identify other contributing variables to health education inequality. Results: The result indicates that participants with more education, better income, and positive consumption preferences undertake higher health education during COVID-19. The CI and HI of consumption-health education inequality are 0.0321 (P < 0.001) and 0.0416 (p < 0.001), respectively, which indicates that health education is concentrated in wealthy groups. We adapted Lasso regression to solve issues and omit variables. In terms of other socioeconomic characteristics, Annual Income was also a major contributor to health education inequalities, accounting for 27.1% (P < 0.001). The empirical results also suggests that education, health status, identification residence, and medical health insurance contribute to health education inequality. Conclusions: The difference in Household consumption, annual income, rural and urban disparity, and private healthcare insurance are critical drivers of health education inequality. The government should pay more attention to promoting health education, and healthcare subside policy among vulnerable people. Significantly to improve awareness of undertaking health education with lower education, rural residential, to enhance confidence in economic recovery and life after COVID-19.",
        "DOI": "10.3389/fpubh.2022.810488",
        "paper_author": "You J.",
        "affiliation_name": "Shanghai University of Finance and Economics",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60032744",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Investigating for bias in healthcare algorithms: A sex-stratified analysis of supervised machine learning models in liver disease prediction",
        "publication": "BMJ Health and Care Informatics",
        "citied_by": "39",
        "cover_date": "2022-04-25",
        "Abstract": "Objectives The Indian Liver Patient Dataset (ILPD) is used extensively to create algorithms that predict liver disease. Given the existing research describing demographic inequities in liver disease diagnosis and management, these algorithms require scrutiny for potential biases. We address this overlooked issue by investigating ILPD models for sex bias. Methods Following our literature review of ILPD papers, the models reported in existing studies are recreated and then interrogated for bias. We define four experiments, training on sex-unbalanced/balanced data, with and without feature selection. We build random forests (RFs), support vector machines (SVMs), Gaussian Naïve Bayes and logistic regression (LR) classifiers, running experiments 100 times, reporting average results with SD. Results We reproduce published models achieving accuracies of >70% (LR 71.31% (2.37 SD) - SVM 79.40% (2.50 SD)) and demonstrate a previously unobserved performance disparity. Across all classifiers females suffer from a higher false negative rate (FNR). Presently, RF and LR classifiers are reported as the most effective models, yet in our experiments they demonstrate the greatest FNR disparity (RF; -21.02%; LR; -24.07%). Discussion We demonstrate a sex disparity that exists in published ILPD classifiers. In practice, the higher FNR for females would manifest as increased rates of missed diagnosis for female patients and a consequent lack of appropriate care. Our study demonstrates that evaluating biases in the initial stages of machine learning can provide insights into inequalities in current clinical practice, reveal pathophysiological differences between the male and females, and can mitigate the digitisation of inequalities into algorithmic systems. Conclusion Our findings are important to medical data scientists, clinicians and policy-makers involved in the implementation medical artificial intelligence systems. An awareness of the potential biases of these systems is essential in preventing the digital exacerbation of healthcare inequalities.",
        "DOI": "10.1136/bmjhci-2021-100457",
        "paper_author": "Straw I.",
        "affiliation_name": "University College London",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60022148",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "The Ties That Bind: Text Similarities and Conditional Diffusion among Parties",
        "publication": "British Journal of Political Science",
        "citied_by": "11",
        "cover_date": "2022-04-25",
        "Abstract": "Comparative analyses of party policy diffusion are only just emerging. To better understand the conditions under which diffusion occurs, this article argues that three heuristics - availability, representativeness and anchoring - shape parties' efforts to gather information (from elsewhere), leading to differing diffusion effects. The study operationalizes the outcome as textual similarity of party manifestos in nineteen Western democracies from 1960 to 2016, applying a text-as-data approach and machine translation. Analyzing dyads, it assesses how commonalities and sender/receiver attributes impact diffusion. It finds that there is little room for cross-border diffusion as successful parties stick to their old program. Beyond the still-prevailing domestic context, 'learning from cultural reference groups' in a region is most important. In addition, diffusion appears within EP factions and transnational party organizations independently of the success/loss of the sender. The analysis thus sheds light on (un-)favorable conditions for party policy diffusion and paves the way for future studies applying machine translation and quantitative text analyses.",
        "DOI": "10.1017/S0007123420000617",
        "paper_author": "Düpont N.",
        "affiliation_name": "Universität Bremen",
        "affiliation_city": "Bremen",
        "affiliation_country": "Germany",
        "affiliation_id": "60008293",
        "affiliation_state": "Bremen"
    },
    {
        "paper_title": "Bright lights, big pity",
        "publication": "Science",
        "citied_by": "3",
        "cover_date": "2022-04-22",
        "Abstract": "NA",
        "DOI": "10.1126/science.abq4280",
        "paper_author": "Sokol J.",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Time series prediction using machine learning: a case of Bitcoin returns",
        "publication": "Studies in Economics and Finance",
        "citied_by": "13",
        "cover_date": "2022-04-22",
        "Abstract": "Purpose: The purpose of this study is to compare five data-driven-based ML techniques to predict the time series data of Bitcoin returns, namely, alternating model tree, random forest (RF), multiple linear regression, multi-layer perceptron regression and M5 Tree algorithms. Design/methodology/approach: The data used to forecast time series data of Bitcoin returns ranges from 8 July 2010 to 30 Aug 2020. This study used several predictors to predict bitcoin returns including economic policy uncertainty, equity market volatility index, S&P returns, USD/EURO exchange rates, oil and gold prices, volatilities and returns. Five statistical indexes, namely, correlation coefficient, mean absolute error, root mean square error, relative absolute error and root relative squared error are determined. The results of these metrices are used to develop colour intensity ranking. Findings: Among the machine learning (ML) techniques used in this study, RF models has shown superior predictive ability for estimating the Bitcoin returns. Originality/value: This study is first of its kind to use and compare ML models in the prediction of Bitcoins. More studies can be carried out by using further cryptocurrencies and other ML data-driven models in future.",
        "DOI": "10.1108/SEF-06-2021-0217",
        "paper_author": "Shakri I.H.",
        "affiliation_name": "Edith Cowan University",
        "affiliation_city": "Perth",
        "affiliation_country": "Australia",
        "affiliation_id": "60105210",
        "affiliation_state": "WA"
    },
    {
        "paper_title": "Point break: Using machine learning to uncover a critical mass in women's representation",
        "publication": "Political Science Research and Methods",
        "citied_by": "21",
        "cover_date": "2022-04-20",
        "Abstract": "Decades of research has debated whether women first need to reach a critical mass in the legislature before they can effectively influence legislative outcomes. This study contributes to the debate using supervised tree-based machine learning to study the relationship between increasing variation in women's legislative representation and the allocation of government expenditures in three policy areas: education, healthcare, and defense. We find that women's representation predicts spending in all three areas. We also find evidence of critical mass effects as the relationships between women's representation and government spending are nonlinear. However, beyond critical mass, our research points to a potential critical mass interval or critical limit point in women's representation. We offer guidance on how these results can inform future research using standard parametric models.",
        "DOI": "10.1017/psrm.2021.51",
        "paper_author": "Funk K.D.",
        "affiliation_name": "Arizona State University",
        "affiliation_city": "Tempe",
        "affiliation_country": "United States",
        "affiliation_id": "60003892",
        "affiliation_state": "AZ"
    },
    {
        "paper_title": "Sara McLanahan: Pioneering scholar focused on families and the wellbeing of children",
        "publication": "Proceedings of the National Academy of Sciences of the United States of America",
        "citied_by": "0",
        "cover_date": "2022-04-19",
        "Abstract": "NA",
        "DOI": "10.1073/pnas.2204143119",
        "paper_author": "Carlson M.J.",
        "affiliation_name": "University of Wisconsin-Madison",
        "affiliation_city": "Madison",
        "affiliation_country": "United States",
        "affiliation_id": "60032179",
        "affiliation_state": "WI"
    },
    {
        "paper_title": "Deep reinforcement learning for inventory control: A roadmap",
        "publication": "European Journal of Operational Research",
        "citied_by": "93",
        "cover_date": "2022-04-16",
        "Abstract": "Deep reinforcement learning (DRL) has shown great potential for sequential decision-making, including early developments in inventory control. Yet, the abundance of choices that come with designing a DRL algorithm, combined with the intense computational effort to tune and evaluate each choice, may hamper their application in practice. This paper describes the key design choices of DRL algorithms to facilitate their implementation in inventory control. We also shed light on possible future research avenues that may elevate the current state-of-the-art of DRL applications for inventory control and broaden their scope by leveraging and improving on the structural policy insights within inventory research. Our discussion and roadmap may also spur future research in other domains within operations management.",
        "DOI": "10.1016/j.ejor.2021.07.016",
        "paper_author": "Boute R.N.",
        "affiliation_name": "KU Leuven",
        "affiliation_city": "Leuven",
        "affiliation_country": "Belgium",
        "affiliation_id": "60025063",
        "affiliation_state": "Vlaams-Brabant"
    },
    {
        "paper_title": "Predicting dengue outbreaks in Brazil with manifold learning on climate data[Formula presented]",
        "publication": "Expert Systems with Applications",
        "citied_by": "7",
        "cover_date": "2022-04-15",
        "Abstract": "Tropical countries face urgent public health challenges regarding epidemic control of Dengue. Since effective vector-control efforts depend on the timing in which public policies take place, there is an enormous demand for accurate prediction tools. In this work, we improve upon a recent approach of coarsely predicting outbreaks in Brazilian urban centers based solely on their yearly climate data. Our methodological advancements encompass a judicious choice of data pre-processing steps and usage of modern computational techniques from signal-processing and manifold learning. Altogether, our results improved earlier prediction accuracy scores from 0.72 to 0.80, solidifying manifold learning on climate data alone as a viable way to make (coarse) dengue outbreak prediction in large urban centers. Ultimately, this approach has the potential of radically simplifying the data required to do outbreak analysis, as municipalities with limited public health funds may not monitor a large number of features needed for more extensive machine learning approaches.",
        "DOI": "10.1016/j.eswa.2021.116324",
        "paper_author": "Souza C.",
        "affiliation_name": "Institute for Pure and Applied Mathematics",
        "affiliation_city": "Rio de Janeiro",
        "affiliation_country": "Brazil",
        "affiliation_id": "101182806",
        "affiliation_state": "RJ"
    },
    {
        "paper_title": "Deep multi-agent reinforcement learning for multi-level preventive maintenance in manufacturing systems[Formula presented]",
        "publication": "Expert Systems with Applications",
        "citied_by": "55",
        "cover_date": "2022-04-15",
        "Abstract": "Designing preventive maintenance (PM) policies that ensure smooth and efficient production for large-scale manufacturing systems is non-trivial. Recent model-free reinforcement learning (RL) methods shed lights on how to cope with the non-linearity and stochasticity in such complex systems. However, the action space explosion impedes RL-based PM policies to be generalized to real applications. In order to obtain cost efficient PM policies for a serial production line that has multiple levels of PM actions, a novel multi-agent modeling is adopted to support adaptive learning by modeling each machine as cooperative agent. The evaluation of system-level production loss is leveraged to construct the reward function. An adaptive learning framework based on value-decomposition multi-agent actor–critic algorithm is utilized to obtain PM policies. In simulation study, the proposed framework demonstrates its effectiveness by leading other baselines on a comprehensive set of metrics whereas the centralized RL-based methods struggles to converge to stable policies. Our analysis further demonstrates that our multi-agent reinforcement learning based method learns effective PM policies without any knowledge about the environment and maintenance strategies.",
        "DOI": "10.1016/j.eswa.2021.116323",
        "paper_author": "Su J.",
        "affiliation_name": "University of Virginia",
        "affiliation_city": "Charlottesville",
        "affiliation_country": "United States",
        "affiliation_id": "60021918",
        "affiliation_state": "VA"
    },
    {
        "paper_title": "Stochastic intervention for causal inference via reinforcement learning",
        "publication": "Neurocomputing",
        "citied_by": "2",
        "cover_date": "2022-04-14",
        "Abstract": "Causal inference methods are widely applied in various decision-making domains such as precision medicine, optimal policy and economics. The main focus of causal inference is the treatment effect estimation of intervention strategies, such as changes in drug dosing and increases in financial aid. Existing methods are mostly restricted to the deterministic treatment and compare outcomes under different treatments. However, they are unable to address the substantial recent interests of treatment effect estimation under stochastic intervention, e.g., “how all units health status change if they adopt 50% dose reduction”. In other words, they lack the capability of addressing fine-grained treatment effect estimation to empower the decision-making applications. In this paper, we advance the causal inference research by proposing a new effective framework to estimate the treatment effect under the stochastic intervention. Particularly, we develop a stochastic intervention effect estimator (SIE) based on nonparametric influence function, with the theoretical guarantees of robustness and fast convergence rates. Additionally, we construct a customised reinforcement learning algorithm based on the random search solver which can effectively find the optimal policy to produce the greatest expected outcomes for the decision-making process. Finally, we conduct extensive empirical experiments to validate that our framework can achieve superior performance in comparison with state-of-the-art baselines. For reproducing experimental results, all the source codes and data are available at https://github.com/tridungduong16/Interpretable-Machine-Learning/tree/master/Source%20Code.",
        "DOI": "10.1016/j.neucom.2022.01.086",
        "paper_author": "Duong T.D.",
        "affiliation_name": "University of Technology Sydney",
        "affiliation_city": "Sydney",
        "affiliation_country": "Australia",
        "affiliation_id": "60023932",
        "affiliation_state": "NSW"
    },
    {
        "paper_title": "Explainable Machine Learning Approach Quantified the Long-Term (1981–2015) Impact of Climate and Soil Properties on Yields of Major Agricultural Crops Across CONUS",
        "publication": "Frontiers in Sustainable Food Systems",
        "citied_by": "14",
        "cover_date": "2022-04-13",
        "Abstract": "A comprehensive understanding of the long-term data on the crop, soils, environment, climate, and production management would facilitate efficient data-driven decision-making in agriculture production under changing climate. We have employed an explainable machine learning algorithm (random forest model coupled with LIME; Local Interpretable Model-Agnostic Explanations framework) using multi-decadal (1981–2015) data on climate variables, soil properties, and yield of major crops across the Coterminous United States (CONUS). This data-driven approach explained the multi-faceted factors of crop production for corn, soybean, cotton, and wheat under field conditions by leveraging agricultural informatics. We attempted to show how crop yields can better be correlated and explained when production input varies along with changing climatic/environmental and edaphic conditions. Our findings suggest Growing Degree Days (GDDs) as important climatic factors, while water holding capacity is one of the dominant soil properties in interpreting crop yield variability. Our findings will facilitate growers, crop production scientists, land management specialists, stakeholders, and policy makers in their future decision-making processes related to sustainable and long-term soil, water, and crop management practices.",
        "DOI": "10.3389/fsufs.2022.847892",
        "paper_author": "Sihi D.",
        "affiliation_name": "Emory University School of Medicine",
        "affiliation_city": "Atlanta",
        "affiliation_country": "United States",
        "affiliation_id": "60002339",
        "affiliation_state": "GA"
    },
    {
        "paper_title": "Integrated decision and control at multi-lane intersections with mixed traffic flow",
        "publication": "Journal of Physics: Conference Series",
        "citied_by": "11",
        "cover_date": "2022-04-12",
        "Abstract": "Autonomous driving at intersections is one of the most complicated and accident-prone traffic scenarios, especially with mixed traffic participants such as vehicles, bicycles and pedestrians. The driving policy should make safe decisions to handle the dynamic traffic conditions and meet the requirements of on-board computation. However, most of the current researches focuses on simplified intersections considering only the surrounding vehicles and idealized traffic lights. This paper improves the integrated decision and control framework and develops a learning-based algorithm to deal with complex intersections with mixed traffic flows, which can not only take account of realistic characteristics of traffic lights, but also learn a safe policy under different safety constraints. We first consider different velocity models for green and red lights in the training process and use a finite state machine to handle different modes of light transformation. Then we design different types of distance constraints for vehicles, traffic lights, pedestrians, bicycles respectively and formulize the constrained optimal control problems (OCPs) to be optimized. Finally, reinforcement learning (RL) with value and policy networks is adopted to solve the series of OCPs. In order to verify the safety and efficiency of the proposed method, we design a multi-lane intersection with the existence of large-scale mixed traffic participants and set practical traffic light phases. The simulation results indicate that the trained decision and control policy can well balance safety and tracking performance. Compared with model predictive control (MPC), the computational time is three orders of magnitude lower.",
        "DOI": "10.1088/1742-6596/2234/1/012015",
        "paper_author": "Jiang J.",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60025278",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Impact of financial subsidy schemes on climate goals in the residential building sector",
        "publication": "Journal of Cleaner Production",
        "citied_by": "9",
        "cover_date": "2022-04-10",
        "Abstract": "The international Paris agreement climate goals regarding the residential building sector were mainly incorporated into national legislation as CO2 emission reduction levels for specific years (e.g., 80% CO2 emission reduction until 2035). Financial subsidy schemes incentivizing early retrofitting can lead to lock-in effects, not realizing energy savings potential from technological advancements in the long run and potentially failing emission reduction goals. However, early retrofitting leads to CO2 emission reductions over longer periods, minimizing the combined total CO2 emissions. Depending on which of these two conflicting goals is pursued, differing subsidy schemes are suitable to incentivize respective retrofits. Knowledge about the effects of these subsidy schemes is relevant to setting correct incentives. We, therefore, investigate the difference in CO2 emission reductions of time-dependent subsidy schemes per monetary unit invested. In doing so, this study is the first to investigate time-dependent subsidy schemes. We apply an agent-based building stock model for a case study to the German residential building stock using an extensive real-world dataset. Results indicate that prioritizing early retrofits reduces the probability of achieving emission reduction goals while simultaneously minimizing total CO2 emissions. Total CO2 emission reductions per monetary unit invested differ up to 675% compared to static subsidy schemes. We conclude that political incentive mechanisms should not be designed to meet the climate goals but instead minimize total CO2 emissions.",
        "DOI": "10.1016/j.jclepro.2022.131040",
        "paper_author": "Wiethe C.",
        "affiliation_name": "Fraunhofer Institute for Applied Information Technology FIT",
        "affiliation_city": "Sankt Augustin",
        "affiliation_country": "Germany",
        "affiliation_id": "60002596",
        "affiliation_state": "Nordrhein-Westfalen"
    },
    {
        "paper_title": "How to Ask for Donations? Learning User-Specific Persuasive Dialogue Policies through Online Interactions",
        "publication": "UMAP2022 - Proceedings of the 30th ACM Conference on User Modeling, Adaptation and Personalization",
        "citied_by": "8",
        "cover_date": "2022-04-07",
        "Abstract": "Persuasive conversations are more effective when they are custom-tailored for the intended audience. Current persuasive dialogue systems rely heavily on advice-giving or focus on different framing policies in a constrained and less dynamic/flexible manner. In this paper, we argue for a new approach, in which the system can identify optimal persuasive strategies in context and persuade users through online interactions. We study two main questions (1) can a reinforcement-learning-based dialogue framework learn to exercise user-specific communicative strategies for persuading users? (2) How can we leverage the crowd-sourcing platforms to collect data for training, and evaluating such frameworks for human-AI(/machine) conversations? We describe a prototype system that interacts with users with the goal of persuading them to donate to a charity and use experiments with crowd workers and analyses of our learned policies to document that our approach leads to learning context-sensitive persuasive strategies that focus on user's reactions towards donation and contribute to increasing dialogue success.",
        "DOI": "10.1145/3503252.3531313",
        "paper_author": "Tran N.",
        "affiliation_name": "Department of Computer Science",
        "affiliation_city": "Pittsburgh",
        "affiliation_country": "United States",
        "affiliation_id": "60159156",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "Editorial: Digital Health for Palliative Care",
        "publication": "Frontiers in Digital Health",
        "citied_by": "3",
        "cover_date": "2022-04-07",
        "Abstract": "NA",
        "DOI": "10.3389/fdgth.2022.888419",
        "paper_author": "Payne C.",
        "affiliation_name": "European Association for Palliative Care",
        "affiliation_city": "Vilvoorde",
        "affiliation_country": "Belgium",
        "affiliation_id": "121355755",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Planning sequential interventions to tackle depression in large uncertain social networks using deep reinforcement learning",
        "publication": "Neurocomputing",
        "citied_by": "1",
        "cover_date": "2022-04-07",
        "Abstract": "Studies, with the increasing concern for mental health, have shown that interventions along with social support can reduce stress and depression. However, counselling centers do not have enough resources to provide counselling and social support to all the participants in their interest. This paper helps social support organizations (e.g., university counselling centers) sequentially select the participants for interventions. Meanwhile, Deep Reinforcement Learning (DRL) has shown significant success in learning an efficient policy for sequential decision-making problems in both fully observable environments and partially observable environments with small action space. In this paper, we consider emotion propagation from other neighbours of the influencees, initial uncertainties of mental states and influence in the student network. We propose a new architecture called DRLPSO (Deep Reinforcement Learning with Particle Swarm Optimization) to enhance learning performance in a partially observable environment with a large state and action space. DRLPSO consists of two stages: the Discrete Particle Swarm Optimization (DPSO) and Deep Q-learning integrated with Long Short-Term Memory (DQ-LSTM). In the first stage, we apply DPSO by initializing n particles that converge to multiple optimal actions for each belief state. In the second stage, the action with the best Q-value from the DPSO action set is executed to obtain belief and observation (history of action). We evaluated the proposed method empirically with the simulated student networks with mental state propagation compared to the state-of-the-art algorithms. The experimental results demonstrate that DRLPSO outperforms the state-of-the-art DRL methods by an average of 32%.",
        "DOI": "10.1016/j.neucom.2022.01.030",
        "paper_author": "Aung A.P.P.",
        "affiliation_name": "School of Computer Science and Engineering",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore",
        "affiliation_id": "60078616",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Forecasting Energy Consumption Based on SVR and Markov Model: A Case Study of China",
        "publication": "Frontiers in Environmental Science",
        "citied_by": "24",
        "cover_date": "2022-04-06",
        "Abstract": "Forecasting energy demand in emerging nations is a critical policy tool utilized by decision makers worldwide. However, as estimated economic and demographic characteristics frequently diverge from realizations, precise forecast results are difficult to get due to the economic system’s intrinsic complexity. This work proposed a machine learning model for estimating energy consumption in China using the support vector regression model (SVR). Additionally, Markov Chain (MC) is employed to forecast and analyze the evolving energy consumption structure. The results demonstrate that SVR model is more accurate (98.4%) than the linear model (Moving Average model), the nonlinear model (Grey model), and past research in predicting energy usage. Under the current rate of energy consumption, China’s total energy consumption will break through six billion in the next 4 years. Furthermore, it is expected that China’s energy consumption structure will be more rational in 2025, with increased non-fossil energy consumption and decreased coal consumption, while natural gas consumption continues to grow at a low rate. It provides scientific basis for the implementation of carbon emission peak action, energy security and energy development plan during the 14th Five-Year Plan period.",
        "DOI": "10.3389/fenvs.2022.883711",
        "paper_author": "Meng Z.",
        "affiliation_name": "Ocean University of China",
        "affiliation_city": "Qingdao",
        "affiliation_country": "China",
        "affiliation_id": "60022422",
        "affiliation_state": "Shandong"
    },
    {
        "paper_title": "Proceedings of the Workshop on Challenges and Opportunities of Efficient and Performant Storage Systems, CHEOPS 2022 - In Conjunction with EuroSys 2022",
        "publication": "Proceedings of the Workshop on Challenges and Opportunities of Efficient and Performant Storage Systems, CHEOPS 2022 - In Conjunction with EuroSys 2022",
        "citied_by": "0",
        "cover_date": "2022-04-05",
        "Abstract": "The proceedings contain 5 papers. The topics discussed include: analysis and workload characterization of the CERN EOS storage system; data-aware compression for HPC using machine learning; TONE: cutting tail-latency in learned indexes; understanding the performance of erasure codes in Hadoop distributed file system; and SLRL: a simple least remaining lifetime file eviction policy for HPC multi-tier storage systems.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Research on Method of Recognizing Interdisciplinary Features Based on Domain Topics -Taking Medical Informatics for Example",
        "publication": "Journal of Modern Information",
        "citied_by": "2",
        "cover_date": "2022-04-01",
        "Abstract": "[Purpose/ Signficance]In order to explore the interdisciplinary rule more accurately, the study constructs a framework to recognize the interdisciplinary features based on domain topics, which helps to identify interdisciplinary topics, interdisciplinary situation and interdisciplinary structure.[Method/ Process]45546 articles in the field of medical informatics in WOS database from 2000—2020 were used for empirical research. Firstly, domain topics were divided based on LDA topic model. Then, Div index was introduced to compare and analyze the interdisciplinary situation. Finally, the disciplinary edge-core subgroup structure was analysed based on co-occurrence network and betweeness centrality.[Result/ Conclusion]There are nine sub topics divided, including heart signal sensing system, electronic health technology, electronic medical record system, health app and use behavior, medical care electronic system, randomized treatment experiment, image segmentation and clustering, feature recognition based on machine learning, and cancer treatment risk assessment. Among them, the interdisciplinary degree of the first five subjects shows a fluctuating upward trend in recent five years; In terms of discipline structure, engineering and computer science are the core subjects with deep interdisciplinary degree. This study is helpful for scientific research administrations and researchers to formulate relevant policies, optimize resource allocation, and identify the frontier of disciplines.[Limitations]Due to the incomplete citation information of early journals, the accuracy of interdisciplinary degree calculation would be affected to a certain extent.",
        "DOI": "10.3969/j.issn.1008-0821.2022.04.002",
        "paper_author": "Chen Q.",
        "affiliation_name": "Nanjing University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60033100",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "Benchmark study of reinforcement learning in controlling and optimizing batch processes",
        "publication": "Journal of Advanced Manufacturing and Processing",
        "citied_by": "6",
        "cover_date": "2022-04-01",
        "Abstract": "In this article, multiple reinforcement learning (RL) methods such as value-based, policy-based, and actor-critic algorithms are investigated for typical control tasks found in the chemical industries. Through a critical assessment of these novel techniques, their main advantages are highlighted, but also the challenges that still need to be resolved are discussed. Two batch control tasks are used as benchmarks, namely, production maximization, and setpoint control. Using these testing environments, a direct comparison of different RL approaches is presented, which could guide the algorithm selection in future RL applications for batch process control. Furthermore, the results obtained with a traditional control method, model predictive control (MPC), are shown to provide a baseline for comparison with RL algorithms. The results show that RL has significant applicability in various control tasks and has comparable control performance to traditional methods but with a lower online computational cost. A batch bioreactor simulation and a simulation of an industrial polyol process are used for illustration purposes.",
        "DOI": "10.1002/amp2.10113",
        "paper_author": "Zhu W.",
        "affiliation_name": "Cain Department of Chemical Engineering",
        "affiliation_city": "Baton Rouge",
        "affiliation_country": "United States",
        "affiliation_id": "60019488",
        "affiliation_state": "LA"
    },
    {
        "paper_title": "Ecosystems Determinants of Nutritional Adequacy Among the Indian Preschool Children",
        "publication": "Journal of the Indian Institute of Science",
        "citied_by": "0",
        "cover_date": "2022-04-01",
        "Abstract": "Given the specified importance of dietary diversity in reducing the burden of malnutrition, our study explores the reasons for the high rate of malnutrition in India through assessment of a comprehensive range of ecosystem factors leading to poor nutrients intake. The study uses the Dietary Diversity Score (DDS) to investigate preschoolers, through differences in wealth, gender, and health. Demographic and Health Survey (2015–16) data of 1,40,470 preschool children between the ages of 2–5 years, is investigated using the Bronfenbrenner’s Ecological Systems Theory. Multiple linear regression models developed to investigate the association between variables, depict the importance of vaccination (p-value < 0.01, 95% CI 0.02–0.06) as positively impacting the outcome measures. Interestingly, overall wealth index does not impact the dietary diversity of the child. The lower wealth index, however, significantly impacts the DDS of the female child as compared to the male child (p-value < 0.1, 95% CI − 0.03 to 0.02), indicating that the lower wealth index plays a role in developing the non-egalitarian gender attitudes for female children. Policy implications involve adapting biofortified foods with higher density of nutrients with major focus on female children to minimize the gender gap and leveraging the digital technology such as telemedicine, and advanced techniques such as artificial intelligence, machine learning, and big data to offer real-time surveillance to address the healthcare needs in the ongoing immunization programs.",
        "DOI": "10.1007/s41745-022-00339-4",
        "paper_author": "Afsharinia B.",
        "affiliation_name": "Indian Institute of Science",
        "affiliation_city": "Bengaluru",
        "affiliation_country": "India",
        "affiliation_id": "60014097",
        "affiliation_state": "KA"
    },
    {
        "paper_title": "Predicting the Mortality in the Patients Hospitalized in Intensive Care Units (ICU) Based on Machine Learning Techniques",
        "publication": "Science and Technology Asia",
        "citied_by": "0",
        "cover_date": "2022-04-01",
        "Abstract": "The patients hospitalized in an Intensive Care Unit (ICU) are always at risk of death. Use of machine learning for predicting the mortality of ICU patients is essential to assess the severity of the disease and to judge the value of new treatments, interventions, and health care policies. Therefore, this study aims to predict the mortality in patients hospitalized in the ICU based on machine learning techniques. The present study was conducted in four steps: data understanding, preprocessing, modeling, and evaluation. First, the mortality factors in the patients hospitalized in the ICU were identified and were extracted from 800 records of patient data. Next, different prediction algorithms including decision tree (J48), MLP, KNN, random forest, and SVM were developed based on 19 confirmed factors. The random forest algorithm worked appropriately based on receiver operating characteristic (ROC), accuracy, precision, sensitivity, and specificity. The SVM algorithm was the weakest algorithm according to ROC, sensitivity, accuracy and precision. Also, ventilation was the most effective factor for predicting the patients' death. Our study shows that machine learning algorithms can appropriately predict the mortality rate in patients hospitalized in the ICU with high accuracy and precision, extracting deeper specificities. Since diagnosis by humans is a time-consuming process with a higher probability of mistakes, using these algorithms can decrease treatment decision mistakes made by the ICU physicians and increase the chance of successful treatment.",
        "DOI": "NA",
        "paper_author": "Moulaei K.",
        "affiliation_name": "Medical Informatics Research Center, Institute for Futures Studies in Health, Kerman University of Medical Sciences",
        "affiliation_city": "Kerman",
        "affiliation_country": "Iran",
        "affiliation_id": "60203850",
        "affiliation_state": "Kerman"
    },
    {
        "paper_title": "Research status and prospect of cardiac arrest early warning scoring system",
        "publication": "Chinese Critical Care Medicine",
        "citied_by": "0",
        "cover_date": "2022-04-01",
        "Abstract": "Cardiac arrest is the fourth stage of sudden cardiac death, which is characterized by the cessation of electrical activity in the heart, rapid circulatory and respiratory failure, and the prognosis is often poor. How to effectively predict cardiac arrest is the key and difficult point in the diagnosis and treatment process. In recent years, the research on the application of early warning scoring system in cardiac arrest has made continuous breakthroughs, from initially formulating a traditional scoring system containing only basic vital signs indicators according to a certain number of samples to continuously increasing and changing indicators, increasing the sample size, and formulating an improved scoring system with better sensitivity and specificity. Nowadays, with the continuous development of electronic information technology, machine learning technology is introduced into the formulation of scoring system, which breaks through the limitations of previous scoring system and has achieved good results in clinic. This article analyzes and compares the relevant research and cutting-edge progress of different early warning scoring systems at home and abroad, and summarizes the research results, gaps and shortcomings. Finally, combined with the relevant policies of graded diagnosis and treatment in China, this paper discusses the development and application direction of cardiac arrest early warning scoring system in the future.",
        "DOI": "10.3760/cma.j.cn121430-20201231-00791",
        "paper_author": "Lyu Z.",
        "affiliation_name": "Henan Provincial People's Hospital",
        "affiliation_city": "Zhengzhou",
        "affiliation_country": "China",
        "affiliation_id": "60073733",
        "affiliation_state": "Henan"
    },
    {
        "paper_title": "Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning",
        "publication": "Journal of Machine Learning Research",
        "citied_by": "25",
        "cover_date": "2022-04-01",
        "Abstract": "Intrinsically motivated spontaneous exploration is a key enabler of autonomous developmental learning in human children. It enables the discovery of skill repertoires through autotelic learning, i.e. the self-generation, self-selection, self-ordering and self-experimentation of learning goals. We present an algorithmic approach called Intrinsically Motivated Goal Exploration Processes (IMGEP) to enable similar properties of autonomous learning in machines. The IMGEP architecture relies on several principles: 1) self-generation of goals, generalized as parameterized fitness functions; 2) selection of goals based on intrinsic rewards; 3) exploration with incremental goal-parameterized policy search and exploitation with a batch learning algorithm; 4) systematic reuse of information acquired when targeting a goal for improving towards other goals. We present a particularly efficient form of IMGEP, called AMB, that uses a population-based policy and an object-centered spatiotemporal modularity. We provide several implementations of this architecture and demonstrate their ability to automatically generate a learning curriculum within several experimental setups. One of these experiments includes a real humanoid robot exploring multiple spaces of goals with several hundred continuous dimensions and with distractors. While no particular target goal is provided to these autotelic agents, this curriculum allows the discovery of diverse skills that act as stepping stones for learning more complex skills, e.g. nested tool use",
        "DOI": "NA",
        "paper_author": "Forestier S.",
        "affiliation_name": "INRIA Institut National de Recherche en Informatique et en Automatique",
        "affiliation_city": "Le Chesnay",
        "affiliation_country": "France",
        "affiliation_id": "60013373",
        "affiliation_state": "Ile-de-France"
    },
    {
        "paper_title": "Pattern-oriented analysis of system dynamics models via random forests",
        "publication": "System Dynamics Review",
        "citied_by": "4",
        "cover_date": "2022-04-01",
        "Abstract": "System dynamics (SD) modeling studies aim to reveal the causes of problematic dynamic behaviors and eliminate them through policy design and analysis. The analyst conducts sensitivity/scenario analyses and what-if experiments to reveal the input–output relationships during modeling. However, during these analyses and investigations, the identification of input-parameter spaces that cause the generation of different SD model behavior patterns is time consuming and susceptible to human bias. Therefore, we propose a metamodel-based procedure for SD models that considers the necessity for unbiased and automated analysis and insight generation. The approach uses the random forest algorithm for metamodel generation and extracts interpretable IF–THEN rules from the metamodel, thereby identifying input subspaces that generate different qualitative or numerical SD model outputs. We illustrate the proposed approach using two well-established SD models. These case studies reveal how the model analyst can utilize the proposed method to capture input–output relationships. © 2022 System Dynamics Society.",
        "DOI": "10.1002/sdr.1706",
        "paper_author": "Edali M.",
        "affiliation_name": "Yıldız Teknik Üniversitesi",
        "affiliation_city": "Istanbul",
        "affiliation_country": "Turkey",
        "affiliation_id": "60019963",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Exploring the Contributions by Transportation Features to Urban Economy: An Experiment of a Scalable Tree-Boosting Algorithm with Big Data",
        "publication": "Land",
        "citied_by": "10",
        "cover_date": "2022-04-01",
        "Abstract": "Previous studies regarding transportation impacts on economic development in urban areas have three major issues—the limited scope of analysis mostly with the change of property values, the exclusion of smart transportation systems as features despite their potential for urban areas, and stereotyped approaches with limited types of variables. To surmount such limitations, this research adopted the concept of Big Data with machine learning techniques. As such, a total of 67 features from main categories, including the change of business, geographical boundary, socioeconomic, land value, transportation, smart transportation, sales, and floating population were analyzed with XGBoost and SHAP algorithms. Given that the rise and fall of business is a major consideration for economic development in urban areas, the change in the total number of sales was selected as a target value. As a result, sales-related features showed the largest contribution to the rise of business, among others. It was also noted that features related to smart transportation systems obviously affected the success of business, even more than traditional ones from transportation. It is thus expected that the findings from this research will provide insights for decision-makers and researchers to make customized policies for boosting economic development in urban areas that are a major part of the urban economy to achieve sustainability.",
        "DOI": "10.3390/land11040577",
        "paper_author": "Lee C.",
        "affiliation_name": "United Nations Economic and Social Commission for Asia and the Pacific",
        "affiliation_city": "Bangkok",
        "affiliation_country": "Thailand",
        "affiliation_id": "60077704",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "The effect of haze pollution on insurance development: Evidence from 268 prefecture-level cities in China",
        "publication": "PLoS ONE",
        "citied_by": "2",
        "cover_date": "2022-04-01",
        "Abstract": "The relationship between haze pollution and insurance development is investigated based on the concentration of PM2.5 of 268 Chinese cities during 2009~2018. Subsequently, the effect of haze pollution on the development of insurance and the underlying mechanisms are also explored. The regional governance of haze pollution and its impact on insurance development is estimated by using a unified framework of two-stage least squares. The machine learning method-elastic network is adopted to filter the control variables and avoid multi-collinearity. The results show that haze pollution has an adverse effect on the insurance development through two important underlying mechanisms, residents’ emotions and economic development. Haze pollution affects residents’ emotions, and the impact coefficient is approximately equal to -0.18, which further inhibits residents’ participation in insurance. Moreover, pollution restricts residents’ budgets by hindering economic development, the impact coefficient is about -0.07, thus, the development of insurance is suppressed. These two negative effects exhibit regional variations, which gradually attenuate from eastern, western to the Chinese central region. The regional governance has a positive effect on haze pollution with the coefficient of -0.07, while impact coefficient of haze pollution on insurance development decreases to -0.02. The policy implication is that government supervision can formulate reasonable environmental and insurance policies based on the heterogeneity of regional development to alleviate haze pollution and promote insurance development.",
        "DOI": "10.1371/journal.pone.0267830",
        "paper_author": "Chang Y.",
        "affiliation_name": "Chongqing University",
        "affiliation_city": "Chongqing",
        "affiliation_country": "China",
        "affiliation_id": "60023380",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Predicting COVID-19 Cases From Atmospheric Parameters Using Machine Learning Approach",
        "publication": "GeoHealth",
        "citied_by": "8",
        "cover_date": "2022-04-01",
        "Abstract": "The dynamical nature of COVID-19 cases in different parts of the world requires robust mathematical approaches for prediction and forecasting. In this study, we aim to (a) forecast future COVID-19 cases based on past infections, (b) predict current COVID-19 cases using PM2.5, temperature, and humidity data, using four different machine learning classifiers (Decision Tree, K-nearest neighbor, Support Vector Machine, and Random Forest). Based on RMSE values, k-nearest neighbor and support vector machine algorithms were found to be the best for predicting future incidences of COVID-19 based on past histories. From the RMSE values obtained, temperature was found to be the best predictor for number of COVID-19 cases, followed by relative humidity. Decision tree models was found to perform poorly in the prediction of COVID-19 cases considering particulate matter and atmospheric parameters as predictors. Our results suggests the possibility of predicting virus infection using machine learning. This will guide policy makers in proactive monitoring and control.",
        "DOI": "10.1029/2021GH000509",
        "paper_author": "Ogunjo S.T.",
        "affiliation_name": "Federal University of Technology, Akure",
        "affiliation_city": "Akure",
        "affiliation_country": "Nigeria",
        "affiliation_id": "60023571",
        "affiliation_state": "Ondo"
    },
    {
        "paper_title": "Seeing beyond the Trees: Using Machine Learning to Estimate the Impact of Minimum Wages on Labor Market Outcomes",
        "publication": "Journal of Labor Economics",
        "citied_by": "12",
        "cover_date": "2022-04-01",
        "Abstract": "We assess the effect of the minimum wage on labor market outcomes. First, we apply modern machine learning tools to predict who is affected by the policy. Second, we implement an event study using 172 prominent minimum wage increases between 1979 and 2019. We find a clear increase in wages of affected workers and no change in employment. Furthermore, minimum wage increases have no effect on the unemployment rate, labor force participation, or labor market transitions. Overall, these findings provide little evidence of changing search effort in response to a minimum wage increase.",
        "DOI": "10.1086/718497",
        "paper_author": "Cengiz D.",
        "affiliation_name": "OM Partners",
        "affiliation_city": null,
        "affiliation_country": "United States",
        "affiliation_id": "101121045",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A machine learning and clustering-based approach for county-level COVID-19 analysis",
        "publication": "PLoS ONE",
        "citied_by": "11",
        "cover_date": "2022-04-01",
        "Abstract": "COVID-19 is a global pandemic threatening the lives and livelihood of millions of people across the world. Due to its novelty and quick spread, scientists have had difficulty in creating accurate forecasts for this disease. In part, this is due to variation in human behavior and environmental factors that impact disease propagation. This is especially true for regionally specific predictive models due to either limited case histories or other unique factors characterizing the region. This paper employs both supervised and unsupervised methods to identify the critical county-level demographic, mobility, weather, medical capacity, and health related county-level factors for studying COVID-19 propagation prior to the widespread availability of a vaccine. We use this feature subspace to aggregate counties into meaningful clusters to support more refined disease analysis efforts.",
        "DOI": "10.1371/journal.pone.0267558",
        "paper_author": "Nicholson C.",
        "affiliation_name": "Gallogly College of Engineering",
        "affiliation_city": "Norman",
        "affiliation_country": "United States",
        "affiliation_id": "60279906",
        "affiliation_state": "OK"
    },
    {
        "paper_title": "Policy search with rare significant events: Choosing the right partner to cooperate with",
        "publication": "PLoS ONE",
        "citied_by": "2",
        "cover_date": "2022-04-01",
        "Abstract": "This paper focuses on a class of reinforcement learning problems where significant events are rare and limited to a single positive reward per episode. A typical example is that of an agent who has to choose a partner to cooperate with, while a large number of partners are simply not interested in cooperating, regardless of what the agent has to offer. We address this problem in a continuous state and action space with two different kinds of search methods: a gradient policy search method and a direct policy search method using an evolution strategy. We show that when significant events are rare, gradient information is also scarce, making it difficult for policy gradient search methods to find an optimal policy, with or without a deep neural architecture. On the other hand, we show that direct policy search methods are invariant to the rarity of significant events, which is yet another confirmation of the unique role evolutionary algorithms has to play as a reinforcement learning method.",
        "DOI": "10.1371/journal.pone.0266841",
        "paper_author": "Ecoffet P.",
        "affiliation_name": "Sorbonne Université",
        "affiliation_city": "Paris",
        "affiliation_country": "France",
        "affiliation_id": "60001422",
        "affiliation_state": "Ile-de-France"
    },
    {
        "paper_title": "Predicting the Severity of Lockdown-Induced Psychiatric Symptoms with Machine Learning",
        "publication": "Diagnostics",
        "citied_by": "7",
        "cover_date": "2022-04-01",
        "Abstract": "During the COVID-19 pandemic, an increase in the incidence of psychiatric disorders in the general population and an increase in the severity of symptoms in psychiatric patients have been reported. Anxiety and depression symptoms are the most commonly observed during large-scale dramatic events such as pandemics and wars, especially when these implicate an extended lockdown. The early detection of higher risk clinical and non-clinical individuals would help prevent the new onset and/or deterioration of these symptoms. This in turn would lead to the implementation of public policies aimed at protecting vulnerable populations during these dramatic contingencies, therefore optimising the effectiveness of interventions and saving the resources of national healthcare systems. We used a supervised machine learning method to identify the predictors of the severity of psychiatric symptoms during the Italian lockdown due to the COVID-19 pandemic. Via a case study, we applied this methodology to a small sample of healthy individuals, obsessive-compulsive disorder patients, and adjustment disorder patients. Our preliminary results show that our models were able to predict depression, anxiety, and obsessive-compulsive symptoms during the lockdown with up to 92% accuracy based on demographic and clinical characteristics collected before the pandemic. The presented methodology may be used to predict the psychiatric prognosis of individuals under a large-scale lockdown and thus supporting the related clinical decisions.",
        "DOI": "10.3390/diagnostics12040957",
        "paper_author": "D’urso G.",
        "affiliation_name": "Università degli Studi di Napoli Federico II",
        "affiliation_city": "Naples",
        "affiliation_country": "Italy",
        "affiliation_id": "60017293",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Machine Learning in Ratemaking, an Application in Commercial Auto Insurance",
        "publication": "Risks",
        "citied_by": "2",
        "cover_date": "2022-04-01",
        "Abstract": "This paper explores the tuning and results of two-part models on rich datasets provided through the Casualty Actuarial Society (CAS). These datasets include bodily injury (BI), property damage (PD) and collision (COLL) coverage, each documenting policy characteristics and claims across a four-year period. The datasets are explored, including summaries of all variables, then the methods for modeling are set forth. Models are tuned and the tuning results are displayed, after which we train the final models and seek to explain select predictions. Data were provided by a private insurance carrier to the CAS after anonymizing the dataset. These data are available to actuarial researchers for well-defined research projects that have universal benefit to the insurance industry and the public. Our hope is that the methods demonstrated here can be a good foundation for future ratemaking models to be developed and tested more efficiently.",
        "DOI": "10.3390/risks10040080",
        "paper_author": "Matthews S.",
        "affiliation_name": "Donald Bren School of Information &amp; Computer Sciences",
        "affiliation_city": "Irvine",
        "affiliation_country": "United States",
        "affiliation_id": "60142654",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Developing a Deep Neural Network with Fuzzy Wavelets and Integrating an Inline PSO to Predict Energy Consumption Patterns in Urban Buildings",
        "publication": "Mathematics",
        "citied_by": "21",
        "cover_date": "2022-04-01",
        "Abstract": "Energy has been one of the most important topics of political and social discussion in recent decades. A significant proportion of the country’s revenues is derived from energy resources, making it one of the most important and strategic macro policy and sustainable development areas. Energy demand modeling is one of the essential strategies for better managing the energy sector and developing appropriate policies to increase productivity. With the increasing global demand for energy, it is necessary to develop intelligent forecasting methods and algorithms. Different economic and non-economic indicators can be used to estimate the energy demand, including linear and non-linear statistical methods, mathematics, and simulation models. This non-linear relationship between these indicators and energy demand has led researchers to search for intelligent solutions, such as artificial neural networks for non-linear modeling and prediction. The purpose of this study was to use a deep neural network with fuzzy wavelets to predict energy demand in Iran. For the training of the presented components, a hybrid training method incorporating both an inline PSO and a gradient-based algorithm is presented. The provided technique predicts energy consumption in Tehran, Mashhad, Ahvaz, and Urmia from 2010 to 2021. This study shows that the presented method provides high-performance prediction at a lower level of complexity.",
        "DOI": "10.3390/math10081270",
        "paper_author": "Ahmadi M.",
        "affiliation_name": "Urmia University of Technology",
        "affiliation_city": "Urmia",
        "affiliation_country": "Iran",
        "affiliation_id": "60104370",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Exploring Patterns of Transportation-Related CO<inf>2</inf> Emissions Using Machine Learning Methods",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "24",
        "cover_date": "2022-04-01",
        "Abstract": "While the transportation sector is one of largest economic growth drivers for many countries, the adverse impacts of transportation on air quality are also well-noted, especially in developing countries. Carbon dioxide (CO2) emissions are one of the direct results of a transportation sector powered by burning fossil-based fuels. Detailed knowledge of CO2 emissions produced by the transportation sectors in various countries is essential for these countries to revise their future energy investments and policies. In this framework, three machine learning algorithms, ordinary least squares regression (OLS), support vector machine (SVM), and gradient boosting regression (GBR), are used to forecast transportation-based CO2 emissions. Both socioeconomic factors and transportation factors are also included as features in the study. We study the top 30 CO2 emissions-producing countries, including the Tier 1 group (the top five countries, accounting for 61% of global CO2 emissions production) and the Tier 2 group (the next 25 countries, accounting for 35% of total CO2 emissions production). We evaluate our model using four-fold cross-validation and report four frequently used statistical metrics (R2, MAE, rRMSE, and MAPE). Of the three machine learning algorithms, the GBR model with features combining socioeconomic and transportation factors (GBR_ALL) has the best performance, with an R2 value of 0.9943, rRMSE of 0.1165, and MAPE of 0.1408. We also find that both transportation features and socioeconomic features are important for transportation-based CO2 emission prediction. Transportation features are more important in modeling for 30 countries, while socioeconomic features (especially GDP and population) are more important when modeling for Tier 1 and Tier 2 countries.",
        "DOI": "10.3390/su14084588",
        "paper_author": "Li X.",
        "affiliation_name": "Anhui Polytechnic University",
        "affiliation_city": "Wuhu",
        "affiliation_country": "China",
        "affiliation_id": "60083914",
        "affiliation_state": "Anhui"
    },
    {
        "paper_title": "Change Detection of Amazonian Alluvial Gold Mining Using Deep Learning and Sentinel-2 Imagery",
        "publication": "Remote Sensing",
        "citied_by": "41",
        "cover_date": "2022-04-01",
        "Abstract": "Monitoring changes within the land surface and open water bodies is critical for natural resource management, conservation, and environmental policy. While the use of satellite imagery for these purposes is common, fine-scale change detection can be a technical challenge. Difficulties arise from variable atmospheric conditions and the problem of assigning pixels to individual objects. We examined the degree to which two machine learning approaches can better characterize change detection in the context of a current conservation challenge, artisanal small-scale gold mining (ASGM). We obtained Sentinel-2 imagery and consulted with domain experts to construct an open-source labeled land-cover change dataset. The focus of this dataset is the Madre de Dios (MDD) region in Peru, a hotspot of ASGM activity. We also generated datasets of active ASGM areas in other countries (Venezuela, Indonesia, and Myanmar) for out-of-sample testing. With these labeled data, we utilized a supervised (E-ReCNN) and semi-supervised (SVM-STV) approach to study binary and multi-class change within mining ponds in the MDD region. Additionally, we tested how the inclusion of multiple channels, histogram matching, and La*b* color metrics improved the performance of the models and reduced the influence of atmospheric effects. Empirical results show that the supervised E-ReCNN method on 6-Channel histogram-matched images generated the most accurate detection of change not only in the focal region (Kappa: 0.92 (± 0.04), Jaccard: 0.88 (± 0.07), F1: 0.88 (± 0.05)) but also in the out-of-sample prediction regions (Kappa: 0.90 (± 0.03), Jaccard: 0.84 (± 0.04), and F1: 0.77 (± 0.04)). While semi-supervised methods did not perform as accurately on 6-or 10-channel imagery, histogram matching and the inclusion of La*b* metrics generated accurate results with low memory and resource costs. These results show that E-ReCNN is capable of accurately detecting specific and object-oriented environmental changes related to ASGM. E-ReCNN is scalable to areas outside the focal area and is a method of change detection that can be extended to other forms of land-use modification.",
        "DOI": "10.3390/rs14071746",
        "paper_author": "Camalan S.",
        "affiliation_name": "Wake Forest University",
        "affiliation_city": "Winston Salem",
        "affiliation_country": "United States",
        "affiliation_id": "60033114",
        "affiliation_state": "NC"
    },
    {
        "paper_title": "How sharing is the “sharing economy”? Evidence from 97 Airbnb markets",
        "publication": "PLoS ONE",
        "citied_by": "9",
        "cover_date": "2022-04-01",
        "Abstract": "Digital platforms such as Airbnb have become a major economic and political force in recent years, presenting themselves as a “sharing economy”–a new, more just way of organizing social and economic activity–while functioning as owners and managers of proprietary markets. These platforms have in recent years been subject to variegated but growing regulations, begging questions of how these affect their platform markets. This paper examines these claims by a large-scale international comparative analysis of the revenue distribution of Airbnb markets in 97 cities and regions, focusing on the level and evolution of revenue inequality, and estimating the racial and gender revenue gaps by using machine learning classification of host profile pictures. Examining 834,722 listings, 513,785 hosts, and 13,466,854 reviews, the paper finds an average Gini coefficient of 0.68, implying that a majority of the market revenue tends to go to about 10% of the hosts. The level of centralization varies significantly across cities, but is consistently growing over time, with government regulation appearing as a counteracting factor, which however only temporarily slows down the growing dominance of a small minority of large-scale hosts. The paper furthermore finds large gender and race revenue gaps, as Black hosts receive on average 22% less revenue for their listings, and women an average of 12% less. These findings contribute important data to ongoing academic and policy debates, as well as a starting point for further research on inequality in the sharing economy, and how it can be regulated.",
        "DOI": "10.1371/journal.pone.0266998",
        "paper_author": "Törnberg P.",
        "affiliation_name": "Universiteit van Amsterdam",
        "affiliation_city": "Amsterdam",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60002483",
        "affiliation_state": "Noord-Holland"
    },
    {
        "paper_title": "From high school to postsecondary education, training, and employment: Predicting outcomes for young adults with autism spectrum disorder",
        "publication": "Autism and Developmental Language Impairments",
        "citied_by": "5",
        "cover_date": "2022-04-01",
        "Abstract": "Background and Aims: The fastest growing group of students with disabilities are those with Autism Spectrum Disorder (ASD). States annually report on post-high school outcomes (PSO) of exited students. This study sought to fill two gaps in the literature related to PSO for exited high-school students with ASD and the use of state data and predictive modeling. Methods: Data from two states were analyzed using two predictive analytics (PA) methods: multilevel logistic regression and machine learning. The receiver operating characteristic curve (ROC) analysis was used to assess predictive performance. Results: Data analyses produced two results. One, the strongest predictor of PSO for exited students with ASD was graduating from high school. Two, machine learning performed better than multilevel logistic regression in predicting PSO engagement across the two states. Conclusion: This study contributed two new and important findings to the literature: (a) PA models should be applied to state PSO data because they produce useful information, and (b) PA models are accurate and reliable over time. Implications: These findings can be used to support state and local educators to make decisions about policies, programs, and practices for exited high school students with ASD, to help them successfully transition to adult life.",
        "DOI": "10.1177/23969415221095019",
        "paper_author": "Yamamoto S.H.",
        "affiliation_name": "University of Oregon",
        "affiliation_city": "Eugene",
        "affiliation_country": "United States",
        "affiliation_id": "60012317",
        "affiliation_state": "OR"
    },
    {
        "paper_title": "Understanding the Urban Environment from Satellite Images with New Classification Method—Focusing on Formality and Informality",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "6",
        "cover_date": "2022-04-01",
        "Abstract": "Urbanization plays a critical role in changing the urban environment. Most developed countries have almost completed urbanization. However, with more and more people moving to cities, the urban environment in developing countries is undergoing significant changes. Sustainable development cannot be achieved without significant changes in building, managing, and responding to changes in the urban environment. The classified measurement and analysis of the urban environment in developing countries and the real-time understanding of the evolution and characteristics of the urban environment are of great significance for decision-makers to manage and plan cities more effectively and maintain the sustainability of the urban environment. Hence, a method readily applicable for the state-of-the-art computational analysis can help conceive the rapidly changing urban socio-environmental dynamics that can make the policy-making process even more informative and help monitor the changes almost in real-time. Based on easily accessible data from Google Earth, this work develops and proposes a new urban environment classification method focusing on formality and informality. Firstly, the method gives a new model to scrutinize the urban environment based on the buildings and their surroundings. Secondly, the method is suited for the state-of-the-art machine learning processes that make it applicable and scalable for forecasting, analytics, or computational modeling. The paper first demonstrates the model and its applicability based on the urban environment in the developing world. The method divides the urban environment into 16 categories under four classes. Then it is used to draw the urban environment classes maps of the following emerging cities: Nairobi in Kenya, Mumbai in India, Guangzhou in China, Jakarta in Indonesia, Cairo in Egypt, and Lima in Chile. Then, we discuss the characteristics of different urban environments and the differences between the same class in different cities. We also demonstrate the agility of the proposed method by showing how this classification method can be easily augmented with other data such as population per square kilometer to aid the decision-making process. This mapping should help urban designers who are working on analyzing formality and informality in the developing world. Moreover, from the application point of view, this will provide training data sets for future deep learning algorithms and automate them, help establish databases, and significantly reduce the cost of acquiring data for urban environments that change over time. The method can become a necessary tool for decision-makers to plan sustainable urban spaces in the future to design and manage cities more effectively.",
        "DOI": "10.3390/su14074336",
        "paper_author": "Cheng Q.",
        "affiliation_name": "The University of Tokyo",
        "affiliation_city": "Tokyo",
        "affiliation_country": "Japan",
        "affiliation_id": "60025272",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Significance of Meteorological Feature Selection and Seasonal Variation on Performance and Calibration of a Low-Cost Particle Sensor",
        "publication": "Atmosphere",
        "citied_by": "5",
        "cover_date": "2022-04-01",
        "Abstract": "Poor air quality is a major environmental concern worldwide, but people living in low-and middle-income countries are disproportionately affected. Measurement of PM2.5 is essential for establishing regulatory standards and developing policy frameworks. Low-cost sensors (LCS) can construct a high spatiotemporal resolution PM2.5 network, but the calibration dependencies and subject to biases of LCS due to variable meteorological parameters limit their deployment for air-quality measurements. This study used data collected from June 2019 to April 2021 from a PurpleAir Monitor and Met One Instruments’ Model BAM 1020 as a reference instrument at Alberta, Canada. The objective of this study is to identify the relevant meteorological parameters for each season that significantly affect the performance of LCS. The meteorological features considered are relative humidity (RH), temperature (T), wind speed (WS) and wind direction (WD). This study applied Multiple Linear Regression (MLR), k-Nearest Neighbor (kNN), Random Forest (RF) and Gradient Boosting (GB) models with varying features in a stepwise manner across all the seasons, and only the best results are presented in this study. Improvement in the performance of calibration models is observed by incorporating different features for different seasons. The best performance is achieved when RF is applied but with different features for different seasons. The significant meteorological features are PM2.5_LCS in Summer, PM2.5_LCS, RH and T in Autumn, PM2.5_LCS, T and WS in Winter and PM2.5_LCS, RH, T and WS in Spring. The improvement in R2 for each season (values in parentheses) is Summer (0.66–0.94), Autumn (0.73–0.96), Winter (0.70–0.95) and Spring (0.70–0.94). This study signifies selecting the right combination of models and features to attain the best results for LCS calibration.",
        "DOI": "10.3390/atmos13040587",
        "paper_author": "Kumar V.",
        "affiliation_name": "Indian Institute of Technology Bombay",
        "affiliation_city": "Mumbai",
        "affiliation_country": "India",
        "affiliation_id": "60014153",
        "affiliation_state": "MH"
    },
    {
        "paper_title": "Open-Source Clinical Machine Learning Models: Critical Appraisal of Feasibility, Advantages, and Challenges",
        "publication": "JMIR Formative Research",
        "citied_by": "12",
        "cover_date": "2022-04-01",
        "Abstract": "Machine learning applications promise to augment clinical capabilities and at least 64 models have already been approved by the US Food and Drug Administration. These tools are developed, shared, and used in an environment in which regulations and market forces remain immature. An important consideration when evaluating this environment is the introduction of open-source solutions in which innovations are freely shared; such solutions have long been a facet of digital culture. We discuss the feasibility and implications of open-source machine learning in a health care infrastructure built upon proprietary information. The decreased cost of development as compared to drugs and devices, a longstanding culture of open-source products in other industries, and the beginnings of machine learning-friendly regulatory pathways together allow for the development and deployment of open-source machine learning models. Such tools have distinct advantages including enhanced product integrity, customizability, and lower cost, leading to increased access. However, significant questions regarding engineering concerns about implementation infrastructure and model safety, a lack of incentives from intellectual property protection, and nebulous liability rules significantly complicate the ability to develop such open-source models. Ultimately, the reconciliation of open-source machine learning and the proprietary information-driven health care environment requires that policymakers, regulators, and health care organizations actively craft a conducive market in which innovative developers will continue to both work and collaborate.",
        "DOI": "10.2196/33970",
        "paper_author": "Harish K.B.",
        "affiliation_name": "NYU Grossman School of Medicine",
        "affiliation_city": "New York",
        "affiliation_country": "United States",
        "affiliation_id": "60024541",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "A Novel Multi-Factor Three-Step Feature Selection and Deep Learning Framework for Regional GDP Prediction: Evidence from China",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "15",
        "cover_date": "2022-04-01",
        "Abstract": "Gross domestic product (GDP) is an important index reflecting the economic development of a region. Accurate GDP prediction of developing regions can provide technical support for sustainable urban development and economic policy formulation. In this paper, a novel multi-factor three-step feature selection and deep learning framework are proposed for regional GDP prediction. The core modeling process is mainly composed of the following three steps: In Step I, the feature crossing algorithm is used to deeply excavate hidden feature information of original datasets and fully extract key information. In Step II, BorutaRF and Q-learning algorithms analyze the deep correlation between extracted features and targets from two different perspectives and determine the features with the highest quality. In Step III, selected features are used as the input of TCN (Temporal convolutional network) to build a GDP prediction model and obtain final prediction results. Based on the experimental analysis of three datasets, the following conclusions can be drawn: (1) The proposed three-stage feature selection method effectively improves the prediction accuracy of TCN by more than 10%. (2) The proposed GDP prediction framework proposed in the paper has achieved better forecasting performance than 14 benchmark models. In addition, the MAPE values of the models are lower than 5% in all cases.",
        "DOI": "10.3390/su14084408",
        "paper_author": "Li Q.",
        "affiliation_name": "Hunan Industry Polytechnic",
        "affiliation_city": "Changsha",
        "affiliation_country": "China",
        "affiliation_id": "127316769",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "EU Net-Zero Policy Achievement Assessment in Selected Members through Automated Forecasting Algorithms",
        "publication": "ISPRS International Journal of Geo-Information",
        "citied_by": "15",
        "cover_date": "2022-04-01",
        "Abstract": "The European Union (EU) has positioned itself as a frontrunner in the worldwide battle against climate change and has set increasingly ambitious pollution mitigation targets for its mem-bers. The burden is heavier for the more vulnerable economies in Central and Eastern Europe (CEE), who must juggle meeting strict greenhouse gas emission (GHG) reduction goals, significant fossil-fuel reliance, and pressure to respond to current pandemic concerns that require an increasing share of limited public resources, while facing severe repercussions for non-compliance. Thus, the main goals of this research are: (i) to generate reliable aggregate GHG projections for CEE countries; (ii) to assess whether these economies are on track to meet their binding pollution reduction targets; (iii) to pin-point countries where more in-depth analysis using spatial inventories of GHGs at a finer resolution is further needed to uncover specific areas that should be targeted by additional measures; and (iv) to perform geo-spatial analysis for the most at-risk country, Poland. Seven statistical and machine-learning models are fitted through automated forecasting algorithms to predict the aggregate GHGs in nine CEE countries for the 2019–2050 horizon. Estimations show that CEE countries (except Romania and Bulgaria) will not meet the set pollution reduction targets for 2030 and will unanimously miss the 2050 carbon neutrality target without resorting to carbon credits or offsets. Austria and Slovenia are the least likely to meet the 2030 emissions reduction targets, whereas Poland (in absolute terms) and Slovenia (in relative terms) are the farthest from meeting the EU’s 2050 net-zero policy targets. The findings thus stress the need for additional measures that go beyond the status quo, particularly in Poland, Austria, and Slovenia. Geospatial analysis for Poland uncovers that Krakow is the city where pollution is the most concentrated with several air pollutants surpassing EU standards. Short-term projections of PM2.5 levels indicate that the air quality in Krakow will remain below EU and WHO standards, highlighting the urgency of policy interventions. Further geospatial data analysis can provide valuable insights into other geo-loca-tions that require the most additional efforts, thereby, assisting in the achievement of EU climate goals with targeted measures and minimum socio-economic costs. The study concludes that statistical and geo-spatial data, and consequently research based on these data, complement and enhance each other. An integrated framework would consequently support sustainable development through bettering policy and decision-making processes.",
        "DOI": "10.3390/ijgi11040232",
        "paper_author": "Tudor C.",
        "affiliation_name": "Bucharest University of Economic Studies",
        "affiliation_city": "Bucharest",
        "affiliation_country": "Romania",
        "affiliation_id": "60107810",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Predicting the Risk of Future Multiple Suicide Attempt among First-Time Suicide Attempters: Implications for Suicide Prevention Policy",
        "publication": "Healthcare (Switzerland)",
        "citied_by": "2",
        "cover_date": "2022-04-01",
        "Abstract": "Suicide is listed in the top ten causes of death in Taiwan. Previous studies have pointed out that psychiatric patients having suicide attempts in their history are more likely to attempt suicide again than non-psychiatric patients. Therefore, how to predict the future multiple suicide attempts of psychiatric patients is an important issue of public health. Different from previous studies, we collect the psychiatric patients who have a suicide diagnosis in the National Health Insurance Research Database (NHIRD) as the study cohort. Study variables include psychiatric patients’ characteristics, medical behavior characteristics, physician characteristics, and hospital characteristics. Three machine learning techniques, including decision tree (DT), support vector machine (SVM), and artificial neural network (ANN), are used to develop models for predicting the risk of future multiple suicide attempts. The Adaboost technique is further used to improve prediction performance in model development. The experimental results show that Adaboost+DT performs the best in predicting the behavior of multiple suicide attempts among psychiatric patients. The findings of this study can help clinical staffs to early identify high-risk patients and improve the effectiveness of suicide prevention.",
        "DOI": "10.3390/healthcare10040667",
        "paper_author": "Lin I.L.",
        "affiliation_name": "Chiayi Christian Hospital",
        "affiliation_city": "Chiayi",
        "affiliation_country": "Taiwan",
        "affiliation_id": "60072370",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Permissions-Based Detection of Android Malware Using Machine Learning",
        "publication": "Symmetry",
        "citied_by": "25",
        "cover_date": "2022-04-01",
        "Abstract": "Malware applications (Apps) targeting mobile devices are widespread, and compromise the sensitive and private information stored on the devices. This is due to the asymmetry between informative permissions and irrelevant and redundant permissions for benign Apps. It also depends on the characteristics of the Android platform, such as adopting an open-source policy, supporting unofficial App stores, and the great tolerance for App verification; therefore the Android platform is destined to face such malicious intrusions. In this paper, we propose a permissions-based malware detection system (PerDRaML) that determines the App’s maliciousness based on the usage of suspicious permissions. The system uses a multi-level based methodology; we first extract and identify the significant features such as permissions, smali sizes, and permission rates from a manually collected dataset of 10,000 applications. Further, we employ various machine learning models to categorize the Apps into their malicious or benign categories. Through extensive experimentations, the proposed method successfully identifies the 5× most significant features to predict malicious Apps. The proposed method outperformed the existing techniques by achieving high accuracies of malware detection i.e., 89.7% with Support Vector Machine, 89.96% with Random Forest, 86.25% with Rotation Forest, and 89.52% with Naïve Bayes models. Moreover, the proposed method optimized up to ~77% of the feature set as compared to the recent approaches, while improving the evaluation metrics such as precision, sensitivity, accuracy, and F-measure. The experimental results show that the proposed system provides a high level of symmetry between irrelevant permissions and malware Apps. Further, the proposed system is promising and may provide a low-cost alternative for Android malware detection for malicious or repackaged Apps.",
        "DOI": "10.3390/sym14040718",
        "paper_author": "Akbar F.",
        "affiliation_name": "School of Electrical Engineering and Computer Science",
        "affiliation_city": "Islamabad",
        "affiliation_country": "Pakistan",
        "affiliation_id": "60117496",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Anticheat System Based on Reinforcement Learning Agents in Unity",
        "publication": "Information (Switzerland)",
        "citied_by": "4",
        "cover_date": "2022-04-01",
        "Abstract": "Game cheating is a common occurrence that may degrade the experience of “honest” players. It can be hindered by using appropriate anticheat systems, which are being considered as a subset of security-related issues. In this paper, we implement and test an anticheat system whose main goal is to help differentiate human players from AI players. For this purpose, we first developed a multiplayer game inside game engine Unity that would serve as a framework for training the reinforcement learning agent. This agent would thus learn to differentiate human players from bots within the game. We implemented the Machine Learning Agents Toolkit library, which uses the proximal policy optimization algorithm. AI players are implemented using state machines, and perform certain actions depending on which condition is satisfied. Two experiments were carried out for testing the agent and showed promising results for identifying artificial players.",
        "DOI": "10.3390/info13040173",
        "paper_author": "Lukas M.",
        "affiliation_name": "A.C.T.d.o.o.",
        "affiliation_city": "Zagreb",
        "affiliation_country": "Croatia",
        "affiliation_id": "100812309",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Xenophobic Bullying and COVID-19: An Exploration Using Big Data and Qualitative Analysis",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "3",
        "cover_date": "2022-04-01",
        "Abstract": "Extant literature suggests that xenophobic bullying is intensified by isolated national or global events; however, the analysis of such occurrences is methodologically limited to the use of self-reported data. Examining disclosures of racist bullying episodes enables us to contextualize various perspectives that are shared online and generate insights on how COVID-19 has exacerbated the issue. Moreover, understanding the rationale and characteristics present in xenophobic bullying may have important implications for our social wellbeing, mental health, and inclusiveness as a global community both in the short and long term. This study employs a mixed-method approach using Big Data techniques as well as qualitative analysis of xenophobic bullying disclosures on Twitter following the spread of COVID-19. The data suggests that about half of the sample represented xenophobic bullying. The qualitative analysis also found that 64% of xenophobic bullying-related tweets referred to occasions that perpetuated racist stereotypes. Relatedly, the rationale for almost 75% of xenophobic bullying incidents was due to being Chinese or Asian. The findings of this study, coupled with anti-hate reports from around the world, are used to suggest multipronged policy interventions and considerations of how social media sites such as Twitter can be used to curb the spread of misinformation and xenophobic bullying.",
        "DOI": "10.3390/ijerph19084824",
        "paper_author": "Sainju K.D.",
        "affiliation_name": "Ontario Tech University",
        "affiliation_city": "Oshawa",
        "affiliation_country": "Canada",
        "affiliation_id": "60002146",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "Improving the Forecasting of Winter Wheat Yields in Northern China with Machine Learning–Dynamical Hybrid Subseasonal-to-Seasonal Ensemble Prediction",
        "publication": "Remote Sensing",
        "citied_by": "40",
        "cover_date": "2022-04-01",
        "Abstract": "Subseasonal-to-seasonal (S2S) prediction of winter wheat yields is crucial for farmers and decision-makers to reduce yield losses and ensure food security. Recently, numerous researchers have utilized machine learning (ML) methods to predict crop yield, using observational climate variables and satellite data. Meanwhile, some studies also illustrated the potential of state-of-the-art dynamical atmospheric prediction in crop yield forecasting. However, the potential of coupling both methods has not been fully explored. Herein, we aimed to establish a skilled ML–dynamical hybrid model for crop yield forecasting (MHCF v1.0), which hybridizes ML and a global dynamical atmospheric prediction system, and applied it to northern China at the S2S time scale. In this study, we adopted three mainstream machining learning algorithms (XGBoost, RF, and SVR) and the multiple linear regression (MLR) model, and three major datasets, including satellite data from MOD13C1, observational climate data from CRU, and S2S atmospheric prediction data from IAP CAS, used to predict winter wheat yield from 2005 to 2014, at the grid level. We found that, among the four models examined in this work, XGBoost reached the highest skill with the S2S prediction as inputs, scoring R2 of 0.85 and RMSE of 0.78 t/ha 3–4 months, leading the winter wheat harvest. Moreover, the results demonstrated that crop yield forecasting with S2S dynamical predictions generally outperforms that with observational climate data. Our findings highlighted that the coupling of ML and S2S dynamical atmospheric prediction provided a useful tool for yield forecasting, which could guide agricultural practices, policy-making and agricultural insurance.",
        "DOI": "10.3390/rs14071707",
        "paper_author": "Cao J.",
        "affiliation_name": "Central China Normal University",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60010591",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Exploring the Relationship between Urban Youth Sentiment and the Built Environment Using Machine Learning and Weibo Comments",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "12",
        "cover_date": "2022-04-01",
        "Abstract": "As the relationship between the built environment and the sense of human experience becomes increasingly important, emotional geography has begun to focus on sentiments in space and time and improving the quality of urban construction from the perspective of public emotion and mental health. While youth is a powerful force in urban construction, there are no studies on the relationship between urban youth sentiments and the built environment. With the development of the Internet, social media has provided a large source of data for the metrics of youth sentiment. Based on data from more than 10,000 geolocated Sina Weibo comments posted over one week (from 19 to 25 July 2021) in Shanghai and using a machine learning algorithm for attention mechanism, this study calculates the sentiment label and sentiment intensity of each comment. Ten elements in five aspects were selected to assess the built environment at different scales and also to explore the correlations between built environment elements and sentiment intensity at different scales. The study finds that the overall sentiment of Shanghai youth tends to be negative. Sentiment intensity is significantly associated with most built environment elements at smaller scales. Urban youth have a higher proportion of both happy and sad sentiments, within which sad sentiments are more closely related to the built environment and are significantly related to all built environment elements. This study uses a deep learning algorithm to improve the accuracy of sentiment classification and confirms that the built environment has a great impact on sentiment. This research can help cities develop built environment optimization measures and policies to create positive emotional environments and enhance the well-being of urban youth.",
        "DOI": "10.3390/ijerph19084794",
        "paper_author": "Duan S.",
        "affiliation_name": "Tongji University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60073652",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Investigating Factors of Active Aging Among Chinese Older Adults: A Machine Learning Approach",
        "publication": "Gerontologist",
        "citied_by": "12",
        "cover_date": "2022-04-01",
        "Abstract": "Background and Objectives: With the extension of healthy life expectancy, promoting active aging has become a policy response to rapid population aging in China. Yet, it has been inconclusive about the relative importance of the determinants of active aging. By applying a machine learning approach, this study aims to identify the most important determinants of active aging in 3 domains, i.e., paid/unpaid work, caregiving, and social activities, among Chinese older adults. Research Design and Methods: Data were drawn from the first wave of the China Health and Retirement Longitudinal Study, which surveys a nationally representative sample of adults aged 60 years and older (N = 7,503). We estimated Random Forest and the least absolute shrinkage and selection operator regression models (LASSO) to determine the most important factors related to active aging. Results: Health has a generic effect on all outcomes of active aging. Our findings also identified the domain-specific determinants of active aging. Urban/rural residency is among the most important factors determining the likelihood of engaging in paid/unpaid work. Living in a multigenerational household is especially important in predicting caregiving activities. Neighborhood infrastructure and facilities have the strongest influence on older adults' participation in social activities. Discussion and Implications: The application of feature selection models provides a fruitful first step in identifying the most important determinants of active aging among Chinese older adults. These results provide evidence-based recommendations for policies and practices promoting active aging.",
        "DOI": "10.1093/geront/gnab058",
        "paper_author": "Yu J.",
        "affiliation_name": "Case Western Reserve University",
        "affiliation_city": "Cleveland",
        "affiliation_country": "United States",
        "affiliation_id": "60000305",
        "affiliation_state": "OH"
    },
    {
        "paper_title": "A General Approach for the Automation of Hydraulic Excavator Arms Using Reinforcement Learning",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "40",
        "cover_date": "2022-04-01",
        "Abstract": "This article presents a general approach to derive an end effector trajectory tracking controller for highly nonlinear hydraulic excavator arms. Rather than requiring an analytical model of the system, we use a neural network model that is trained based on measurements collected during operation of the machine. The data-driven model effectively represents the actuator dynamics including the cylinder-to-joint-space conversion. Requiring only the distances between the individual joints, a simulation is set up to train a control policy using reinforcement learning (RL). The policy outputs pilot stage control commands that can be directly applied to the machine without further fine-tuning. The proposed approach is implemented on a Menzi Muck M545, a 12 t hydraulic excavator, and tested in different task space trajectory tracking scenarios, with and without soil interaction. Compared to a commercial grading controller, which requires laborious hand-tuning by expert engineers, the learned controller shows higher tracking accuracy, indicating that the achieved performance is sufficient for the practical application on construction sites and that the proposed approach opens a new avenue for future machine automation.",
        "DOI": "10.1109/LRA.2022.3152865",
        "paper_author": "Egli P.",
        "affiliation_name": "ETH Zürich",
        "affiliation_city": "Zurich",
        "affiliation_country": "Switzerland",
        "affiliation_id": "60025858",
        "affiliation_state": "ZH"
    },
    {
        "paper_title": "Predicting Sepsis Mortality in a Population-Based National Database: Machine Learning Approach",
        "publication": "Journal of Medical Internet Research",
        "citied_by": "16",
        "cover_date": "2022-04-01",
        "Abstract": "Background: Although machine learning (ML) algorithms have been applied to point-of-care sepsis prognostication, ML has not been used to predict sepsis mortality in an administrative database. Therefore, we examined the performance of common ML algorithms in predicting sepsis mortality in adult patients with sepsis and compared it with that of the conventional context knowledge–based logistic regression approach. Objective: The aim of this study is to examine the performance of common ML algorithms in predicting sepsis mortality in adult patients with sepsis and compare it with that of the conventional context knowledge–based logistic regression approach. Methods: We examined inpatient admissions for sepsis in the US National Inpatient Sample using hospitalizations in 2010-2013 as the training data set. We developed four ML models to predict in-hospital mortality: logistic regression with least absolute shrinkage and selection operator regularization, random forest, gradient-boosted decision tree, and deep neural network. To estimate their performance, we compared our models with the Super Learner model. Using hospitalizations in 2014 as the testing data set, we examined the models’ area under the receiver operating characteristic curve (AUC), confusion matrix results, and net reclassification improvement. Results: Hospitalizations of 923,759 adults were included in the analysis. Compared with the reference logistic regression (AUC: 0.786, 95% CI 0.783-0.788), all ML models showed superior discriminative ability (P<.001), including logistic regression with least absolute shrinkage and selection operator regularization (AUC: 0.878, 95% CI 0.876-0.879), random forest (AUC: 0.878, 95% CI 0.877-0.880), xgboost (AUC: 0.888, 95% CI 0.886-0.889), and neural network (AUC: 0.893, 95% CI 0.891-0.895). All 4 ML models showed higher sensitivity, specificity, positive predictive value, and negative predictive value compared with the reference logistic regression model (P<.001). We obtained similar results from the Super Learner model (AUC: 0.883, 95% CI 0.881-0.885). Conclusions: ML approaches can improve sensitivity, specificity, positive predictive value, negative predictive value, discrimination, and calibration in predicting in-hospital mortality in patients hospitalized with sepsis in the United States. These models need further validation and could be applied to develop more accurate models to compare risk-standardized mortality rates across hospitals and geographic regions, paving the way for research and policy initiatives studying disparities in sepsis care.",
        "DOI": "10.2196/29982",
        "paper_author": "Park J.Y.",
        "affiliation_name": "Harvard T.H. Chan School of Public Health",
        "affiliation_city": "Boston",
        "affiliation_country": "United States",
        "affiliation_id": "60032499",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Explainable death toll motion modeling: COVID-19 data-driven narratives",
        "publication": "PLoS ONE",
        "citied_by": "1",
        "cover_date": "2022-04-01",
        "Abstract": "Models have gained the spotlight in many discussions surrounding COVID-19. The urgency for timely decisions resulted in a multitude of models as informed policy actions must be made even when so many uncertainties about the pandemic still remain. In this paper, we use machine learning algorithms to build intuitive country-level COVID-19 motion models described by death toll velocity and acceleration. Model explainability techniques provide insightful data-driven narratives about COVID-19 death toll motion models—while velocity is explained by factors that are increasing/reducing death toll pace now, acceleration anticipates the effects of public health measures on slowing the death toll pace. This allows policymakers and epidemiologists to understand factors driving the outbreak and to evaluate the impacts of different public health measures.",
        "DOI": "10.1371/journal.pone.0264893",
        "paper_author": "Veloso A.",
        "affiliation_name": "Universidade Federal de Minas Gerais",
        "affiliation_city": "Belo Horizonte",
        "affiliation_country": "Brazil",
        "affiliation_id": "60030074",
        "affiliation_state": "MG"
    },
    {
        "paper_title": "ELM-Based Non-Singular Fast Terminal Sliding Mode Control Strategy for Vehicle Platoon",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "5",
        "cover_date": "2022-04-01",
        "Abstract": "Vehicle platoon is one of the innovations in the automated highway systems, which has the potential to reduce fuel consumption, alleviate traffic congestion and lighten the driver’s burden. How to control the vehicle effectively to ensure the stability of the queue is a challenge. Aiming to overcome the shortcomings of the platoon control method based on traditional sliding mode control, a non-singular terminal sliding mode control method optimized by the extreme learning machine is proposed in this paper. Firstly, the vehicle longitudinal dynamics are derived from the analysis of the forces acting on the vehicle in the longitudinal direction. A constant time headway policy is taken as the spacing policy. The modified non-singular terminal sliding mode control method has outstanding performance, simulation results demonstrate that the following vehicles can rapidly track the trajectory of the leading vehicle in the platoon with less spacing error and guarantee string stability. In this study, several experiments are set up to consider the disturbance and other uncertain practical factors. The performance of the proposed method is superior to the traditional sliding mode control method. Experimental results show that the proposed method can significantly reduce chattering and has good robustness under the circumstances of the disturbance.",
        "DOI": "10.3390/su14074020",
        "paper_author": "Wang C.",
        "affiliation_name": "Key Laboratory of Road and Traffic Engineering of the State Ministry of Education",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60129238",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Modeling of Land Use and Land Cover (LULC) Change Based on Artificial Neural Networks for the Chapecó River Ecological Corridor, Santa Catarina/Brazil",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "22",
        "cover_date": "2022-04-01",
        "Abstract": "The simulation and analysis of future land use and land cover—LULC scenarios using artificial neural networks (ANN)—has been applied in the last 25 years, producing information for environmental and territorial policy making and implementation. LULC changes have impacts on many levels, e.g., climate change, biodiversity and ecosystem services, soil quality, which, in turn, have implications for the landscape. Therefore, it is fundamental that planning is informed by sci-entific evidence. The objective of this work was to develop a geographic model to identify the main patterns of LULC transitions between the years 2000 and 2018, to simulate a baseline scenario for the year 2036, and to assess the effectiveness of the Chapecó River ecological corridor (an area cre-ated by State Decree No. 2.957/2010), regarding the recovery and conservation of forest remnants and natural fields. The results indicate that the forest remnants have tended to recover their area, systematically replacing silviculture areas. However, natural fields (grassland) are expected to dis-appear in the near future if proper measures are not taken to protect this ecosystem. If the current agricultural advance pattern is maintained, only 0.5% of natural fields will remain in the ecological corridor by 2036. This LULC trend exposes the low effectiveness of the ecological corridor (EC) in protecting and restoring this vital ecosystem.",
        "DOI": "10.3390/su14074038",
        "paper_author": "de Souza J.M.",
        "affiliation_name": "Agricultural Research and Extension Service Institution of the State of Santa Catarina",
        "affiliation_city": "Florianopolis",
        "affiliation_country": "Brazil",
        "affiliation_id": "116350529",
        "affiliation_state": "SC"
    },
    {
        "paper_title": "Prediction of Water Quality Classification of the Kelantan River Basin, Malaysia, Using Machine Learning Techniques",
        "publication": "Water (Switzerland)",
        "citied_by": "59",
        "cover_date": "2022-04-01",
        "Abstract": "Machine Learning (ML) has been used for a long time and has gained wide attention over the last several years. It can handle a large amount of data and allow non-linear structures by using complex mathematical computations. However, traditional ML models do suffer some problems, such as high bias and overfitting. Therefore, this has resulted in the advancement and improvement of ML techniques, such as the bagging and boosting approach, to address these problems. This study explores a series of ML models to predict the water quality classification (WQC) in the Kelantan River using data from 2005 to 2020. The proposed methodology employed 13 physical and chemical parameters of water quality and 7 ML models that are Decision Tree, Artificial Neural Networks, K-Nearest Neighbors, Naïve Bayes, Support Vector Machine, Random Forest and Gradient Boosting. Based on the analysis, the ensemble model of Gradient Boosting with a learning rate of 0.1 exhibited the best prediction performance compared to the other algorithms. It had the highest accuracy (94.90%), sensitivity (80.00%) and f-measure (86.49%), with the lowest classification error. Total Suspended Solid (TSS) was the most significant variable for the Gradient Boosting (GB) model to predict WQC, followed by Ammoniacal Nitrogen (NH3N), Biochemical Oxygen Demand (BOD) and Chemical Oxygen Demand (COD). Based on the accurate water quality prediction, the results could help to improve the National Environmental Policy regarding water resources by continuously improving water quality.",
        "DOI": "10.3390/w14071067",
        "paper_author": "Malek N.H.A.",
        "affiliation_name": "Universiti Teknologi MARA",
        "affiliation_city": "Shah Alam",
        "affiliation_country": "Malaysia",
        "affiliation_id": "60004351",
        "affiliation_state": "Selangor"
    },
    {
        "paper_title": "Developing an Optimized Policy Tree-Based Reservoir Operation Model for High Aswan Dam Reservoir, Nile River",
        "publication": "Water (Switzerland)",
        "citied_by": "4",
        "cover_date": "2022-04-01",
        "Abstract": "The impacts of climate change on the Nile River and Grand Ethiopian Renaissance Dam (GERD) along with the increased water demand downstream suggest an urgent need for more efficient management of the reservoir system that is well-informed by accurate modeling and optimization of the reservoir operation. This study provides an updated water balance model for Aswan High Dam Reservoir, which was validated using combined heterogeneous sources of information, including in situ gauge data, bias-corrected reanalyzed data, and remote sensing information. To investigate the future challenges, the spatial distribution of the annual/seasonal Aswan High Dam Reservoir surface air temperature trends over the period from 1979 to 2018 was studied. An increase of around 0.48◦C per decade in average annual temperature was detected, a trend that is expected to continue until 2100. Moreover, a set of machine learning models were developed and utilized to bias-correct the reanalyzed inflow and outflow data available for Aswan High Dam Reservoir. Finally, a policy tree optimization model was developed to inform the decision-making process and operation of the reservoir system. Results from the historical test simulations show that including reliable inflow data, accurate estimation of evaporation losses, and including new regulations and added projects, such as the Toshka Project, greatly affect the simulation results and guide managers through how the reservoir system should be operated in the future.",
        "DOI": "10.3390/w14071061",
        "paper_author": "Goharian E.",
        "affiliation_name": "Molinaroli College of Engineering and Computing",
        "affiliation_city": "Columbia",
        "affiliation_country": "United States",
        "affiliation_id": "60152637",
        "affiliation_state": "SC"
    },
    {
        "paper_title": "Trinity: Neural Network Adaptive Distributed Parallel Training Method Based on Reinforcement Learning",
        "publication": "Algorithms",
        "citied_by": "1",
        "cover_date": "2022-04-01",
        "Abstract": "Deep learning, with increasingly large datasets and complex neural networks, is widely used in computer vision and natural language processing. A resulting trend is to split and train large-scale neural network models across multiple devices in parallel, known as parallel model training. Existing parallel methods are mainly based on expert design, which is inefficient and requires specialized knowledge. Although automatically implemented parallel methods have been proposed to solve these problems, these methods only consider a single optimization aspect of run time. In this paper, we present Trinity, an adaptive distributed parallel training method based on reinforcement learning, to automate the search and tuning of parallel strategies. We build a multidimensional performance evaluation model and use proximal policy optimization to cooptimize multiple optimization aspects. Our experiment used the CIFAR10 and PTB datasets based on InceptionV3, NMT, NASNet and PNASNet models. Compared with Google’s Hierarchical method, Trinity achieves up to 5% reductions in runtime, communication, and memory overhead, and up to a 40% increase in parallel strategy search speeds.",
        "DOI": "10.3390/a15040108",
        "paper_author": "Zeng Y.",
        "affiliation_name": "Hangzhou Dianzi University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60013614",
        "affiliation_state": "Zhejiang"
    },
    {
        "paper_title": "What Affects Emotional Well-Being during Travel? Identifying the Factors by Maximal Information Coefficient",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "5",
        "cover_date": "2022-04-01",
        "Abstract": "Traveler emotional well-being as a specific domain of subjective well-being has attracted attention across the field of transportation. Studies on identifying factors of travel-related emotional well-being can help policy makers to formulate concrete strategies to improve travelers’ experiences and public health. This research used the Maximal Information Coefficient (MIC) to select important factors which have much influence on emotional well-being during travel. American Time Use Survey data collected in 2010, 2012, and 2013 were used in this study and 10 factors have been selected to illustrate the relationship with emotional well-being, including rest, weekly earnings, activity time for well-being, health, self-evaluation of activities, pain medication taken yesterday, travel purpose, travel duration, weekly working hours and age based on MIC values in Descending sort. Among these 10 selected features, 2 factors, travel purpose and travel duration, are related to travel contexts; the other factors are related to personal and social characteristics. It is found that an individual’s physical condition and self-evaluation of activities have much influence on travel-related emotional well-being, while traveling mode and interaction during travel have a relatively small impact on emotional well-being compared to other identified factors. This finding is different from previous research findings. The paper presents traffic strategies related to improving emotional well-being of travelers while traveling based on the findings from this research.",
        "DOI": "10.3390/ijerph19074326",
        "paper_author": "Ma Y.",
        "affiliation_name": "Southeast University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60005244",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "Predicting Change in Adaptation Strategies of Households to Geological Hazards in the Longmenshan Area, China Using Machine Learning and GIS",
        "publication": "Water (Switzerland)",
        "citied_by": "1",
        "cover_date": "2022-04-01",
        "Abstract": "Hydrological changes combined with earthquakes easily trigger secondary disasters, in-cluding geological hazards. The secondary hazard of precipitation is the main disaster type in the Longmenshan Area (China). The 2008 Wenchuan earthquake caused more than 60,000 landslides, severely affecting rural households. This study aimed to answer two questions: (1) How did households adapt to the landslide-prone post-earthquake environment? (2) How will the households’ adaptation strategies change if landslide frequency changes? Different post-disaster adaptation strategies of households in Longmenshan Town, Sichuan, China were identified through a questionnaire survey and then clustered into groups based on similarity using a K-means algorithm. Afterward, a gradient boosting decision tree (GBDT) was used to predict change in adaptation strategies if there was a change in the frequency of landslides. The results show that there are three types of landslide adaptation strategies in the study area: (1) autonomous adaptation; (2) policy-dependent adaptation; and (3) hybrid adaptation, which is a mixture of the first two types. If the frequency of landslides is increased, then around 5% of households previously under the autonomous adaptation type would be converted to policy-dependent and hybrid adaptation types. If the frequency of landslides is reduced, then around 5% of households with policy-dependent adaptation strategies would be converted to the autonomous adaptation type. This exploratory study provides a glimpse of how machine learning can be utilized to predict how adaptation strategies would be modified if hazard frequency changed. A follow-up long-term study in Longmenshan Town is needed to confirm whether the predictions are indeed correct.",
        "DOI": "10.3390/w14071023",
        "paper_author": "Su H.",
        "affiliation_name": "The Hong Kong Polytechnic University",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong",
        "affiliation_id": "60008928",
        "affiliation_state": "Hong Kong"
    },
    {
        "paper_title": "A new perspective of AI study in disaster mental health",
        "publication": "Psychiatry and Clinical Neurosciences",
        "citied_by": "1",
        "cover_date": "2022-04-01",
        "Abstract": "NA",
        "DOI": "10.1111/pcn.13345",
        "paper_author": "Kim Y.",
        "affiliation_name": "National Center of Neurology and Psychiatry",
        "affiliation_city": "Kodaira",
        "affiliation_country": "Japan",
        "affiliation_id": "60017451",
        "affiliation_state": "Tokyo"
    },
    {
        "paper_title": "The Future Ethics of Artificial Intelligence in Medicine: Making Sense of Collaborative Models",
        "publication": "Science and Engineering Ethics",
        "citied_by": "22",
        "cover_date": "2022-04-01",
        "Abstract": "This article examines the role of medical doctors, AI designers, and other stakeholders in making applied AI and machine learning ethically acceptable on the general premises of shared decision-making in medicine. Recent policy documents such as the EU strategy on trustworthy AI and the research literature have often suggested that AI could be made ethically acceptable by increased collaboration between developers and other stakeholders. The article articulates and examines four central alternative models of how AI can be designed and applied in patient care, which we call the ordinary evidence model, the ethical design model, the collaborative model, and the public deliberation model. We argue that the collaborative model is the most promising for covering most AI technology, while the public deliberation model is called for when the technology is recognized as fundamentally transforming the conditions for ethical shared decision-making.",
        "DOI": "10.1007/s11948-022-00369-2",
        "paper_author": "Gundersen T.",
        "affiliation_name": "Centre for the Study of Professions",
        "affiliation_city": "Oslo",
        "affiliation_country": "Norway",
        "affiliation_id": "60121117",
        "affiliation_state": "Oslo"
    },
    {
        "paper_title": "Editorial: Connecting patient care and clinical epidemiology: The anniversary of the key concepts in clinical epidemiology series",
        "publication": "Journal of Clinical Epidemiology",
        "citied_by": "0",
        "cover_date": "2022-04-01",
        "Abstract": "NA",
        "DOI": "10.1016/j.jclinepi.2022.02.016",
        "paper_author": "van Amelsvoort L.",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Predicting hospitalisations related to ambulatory care sensitive conditions with machine learning for population health planning: derivation and validation cohort study",
        "publication": "BMJ Open",
        "citied_by": "7",
        "cover_date": "2022-04-01",
        "Abstract": "Objective To predict older adults' risk of avoidable hospitalisation related to ambulatory care sensitive conditions (ACSC) using machine learning applied to administrative health data of Ontario, Canada. Design, setting and participants A retrospective cohort study was conducted on a large cohort of all residents covered under a single-payer system in Ontario, Canada over the period of 10 years (2008-2017). The study included 1.85 million Ontario residents between 65 and 74 years old at any time throughout the study period. Data sources Administrative health data from Ontario, Canada obtained from the (ICES formely known as the Institute for Clinical Evaluative Sciences Data Repository. Main outcome measures Risk of hospitalisations due to ACSCs 1 year after the observation period. Results The study used a total of 1 854 116 patients, split into train, validation and test sets. The ACSC incidence rates among the data points were 1.1% for all sets. The final XGBoost model achieved an area under the receiver operating curve of 80.5% and an area under precision-recall curve of 0.093 on the test set, and the predictions were well calibrated, including in key subgroups. When ranking the model predictions, those at the top 5% of risk as predicted by the model captured 37.4% of those presented with an ACSC-related hospitalisation. A variety of features such as the previous number of ambulatory care visits, presence of ACSC-related hospitalisations during the observation window, age, rural residence and prescription of certain medications were contributors to the prediction. Our model was also able to capture the geospatial heterogeneity of ACSC risk in Ontario, and especially the elevated risk in rural and marginalised regions. Conclusions This study aimed to predict the 1-year risk of hospitalisation from ambulatory-care sensitive conditions in seniors aged 65-74 years old with a single, large-scale machine learning model. The model shows the potential to inform population health planning and interventions to reduce the burden of ACSC-related hospitalisations.",
        "DOI": "10.1136/bmjopen-2021-051403",
        "paper_author": "Yi S.E.",
        "affiliation_name": "Layer 6 AI",
        "affiliation_city": "Toronto",
        "affiliation_country": "Canada",
        "affiliation_id": "121657972",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "Pulling the investment levers on implementation research in oncology",
        "publication": "The Lancet Oncology",
        "citied_by": "3",
        "cover_date": "2022-04-01",
        "Abstract": "NA",
        "DOI": "10.1016/S1470-2045(22)00025-0",
        "paper_author": "Basu P.",
        "affiliation_name": "Centre international de Recherche sur le Cancer",
        "affiliation_city": "Lyon",
        "affiliation_country": "France",
        "affiliation_id": "60024610",
        "affiliation_state": "Auvergne-Rhone-Alpes"
    },
    {
        "paper_title": "Regional Prediction of Ozone and Fine Particulate Matter Using Diffusion Convolutional Recurrent Neural Network",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "12",
        "cover_date": "2022-04-01",
        "Abstract": "Accurate air quality forecasts can provide data-driven supports for governmental departments to control air pollution and further protect the health of residents. However, existing air quality forecasting models mainly focus on site-specific time series forecasts at a local level, and rarely consider the spatiotemporal relationships among regional monitoring stations. As a novelty, we construct a diffusion convolutional recurrent neural network (DCRNN) model that fully considers the influence of geographic distance and dominant wind direction on the regional variations in air quality through different combinations of directed and undirected graphs. The hourly fine particulate matter (PM2.5 ) and ozone data from 123 air quality monitoring stations in the Yangtze River Delta, China are used to evaluate the performance of the DCRNN model in the regional prediction of PM2.5 and ozone concentrations. Results show that the proposed DCRNN model outperforms the baseline models in prediction accuracy. Compared with the undirected graph model, the directed graph model considering the effects of wind direction performs better in 24 h predictions of pollutant concentrations. In addition, more accurate forecasts of both PM2.5 and ozone are found at a regional level where monitoring stations are distributed densely rather than sparsely. Therefore, the proposed model can assist environmental researchers to further improve the technologies of air quality forecasts and could also serve as tools for environmental policymakers to implement pollution control measures.",
        "DOI": "10.3390/ijerph19073988",
        "paper_author": "Wang D.",
        "affiliation_name": "State Key Laboratory of Ocean Engineering",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60123520",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Improving the generalization of glaucoma detection on fundus images via feature alignment between augmented views",
        "publication": "Biomedical Optics Express",
        "citied_by": "6",
        "cover_date": "2022-04-01",
        "Abstract": "Convolutional neural networks (CNNs) are commonly used in glaucoma detection. Due to the various data distribution shift, however, a well-behaved model may be plummeting in performance when deployed in a new environment. On the other hand, the most straightforward method, data collection, is costly and even unrealistic in practice. To address these challenges, we propose a new method named data augmentation-based (DA) feature alignment (DAFA) to improve the out-of-distribution (OOD) generalization with a single dataset, which is based on the principle of feature alignment to learn the invariant features and eliminate the effect of data distribution shifts. DAFA creates two views of a sample by data augmentation and performs the feature alignment between that augmented views through latent feature recalibration and semantic representation alignment. Latent feature recalibration is normalizing the middle features to the same distribution by instance normalization (IN) layers. Semantic representation alignment is conducted by minimizing the Topk NT-Xent loss and the maximum mean discrepancy (MMD), which maximize the semantic agreement across augmented views from individual and population levels. Furthermore, a benchmark is established with seven glaucoma detection datasets and a new metric named mean of clean area under curve (mcAUC) for a comprehensive evaluation of the model performance. Experimental results of five-fold cross-validation demonstrate that DAFA can consistently and significantly improve the out-of-distribution generalization (up to +16.3% mcAUC) regardless of the training data, network architectures, and augmentation policies and outperform lots of state-of-the-art methods.",
        "DOI": "10.1364/BOE.450543",
        "paper_author": "Zhou C.",
        "affiliation_name": "Shanghai Jiao Tong University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60025084",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "End-to-end learning for off-road terrain navigation using the Chrono open-source simulation platform",
        "publication": "Multibody System Dynamics",
        "citied_by": "7",
        "cover_date": "2022-04-01",
        "Abstract": "This contribution (i) describes an open-source, physics-based simulation infrastructure that can be used to learn and test control policies in off-road navigation; and (ii) demonstrates the use of the simulation platform in an end-to-end learning exercise that relies on simulated sensor data fusion (camera, GPS and IMU). For (i), the 0.5 million lines of open-source code support vehicle dynamics (wheeled/tracked vehicles, rovers), deformable & non-deformable terrains, and virtual sensing. The library has a Python API for interfacing with existing Machine Learning frameworks. For (ii) , we use a Gator off-road vehicle to demonstrate how a policy learned on non-deformable terrain performs when used in hilly conditions while navigating around a course of randomly placed obstacles on deformable terrain. The hilly terrain covers an 80×80 m patch and the soil can be controlled by the user to assume various behavior, e.g. non-deformable, deformable hard (silt-like), deformable soft (snow-like), etc. To the best of our knowledge, there is no other open-source, physics-based engine that can be used to simulate off-road mobility of autonomous agents operating on deformable terrains. The results reported herein can be reproduced with models and data available in a public repository (UW-Madison Simulation Based Engineering Laboratory, Supporting models, scripts, data, https://go.wisc.edu/arflqq, 2021). Animations associated with the tests run are available online (UW-Madison Simulation Based Engineering Laboratory, Supporting simulations, https://go.wisc.edu/256xb9, 2021).",
        "DOI": "10.1007/s11044-022-09816-1",
        "paper_author": "Benatti S.",
        "affiliation_name": "UW-Madison College of Engineering",
        "affiliation_city": "Madison",
        "affiliation_country": "United States",
        "affiliation_id": "60153131",
        "affiliation_state": "WI"
    },
    {
        "paper_title": "Erratum: Withdrawal notice to: “Granular computing based machine learning in the era of big data” [Inf. Sci. 591 (2022) 422–423] (Information Sciences (2022) 591 (422–423), (S0020025516311434), (10.1016/j.ins.2016.10.012))",
        "publication": "Information Sciences",
        "citied_by": "0",
        "cover_date": "2022-04-01",
        "Abstract": "The Publisher regrets that this article is an accidental duplication of an article that has already been published in INS, 378 (2017) 242–243, https://doi.org/10.1016/j.ins.2016.10.048. The duplicate article has therefore been withdrawn. The full Elsevier Policy on Article Withdrawal can be found at https://www.elsevier.com/about/our-business/policies/article-withdrawal.",
        "DOI": "10.1016/j.ins.2022.02.043",
        "paper_author": "Hu Q.",
        "affiliation_name": "Tianjin University",
        "affiliation_city": "Tianjin",
        "affiliation_country": "China",
        "affiliation_id": "60019533",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Machine learning-based prediction for grassland degradation using geographic, meteorological, plant and microbial data",
        "publication": "Ecological Indicators",
        "citied_by": "17",
        "cover_date": "2022-04-01",
        "Abstract": "Extensive grassland degradation under climate change and intensified human activities has threatened ecological security and caused a variety of environmental problems. However, it is still challenging to predict the grassland degradation status on a large scale because it is a multi-factorial phenomenon with complex changes in ecosystem structure and function, which is hard to be fully characterized through mechanism models. The emergence of machine learning algorithms provides a potential to model complex systems and mine information from multi-source data without elucidating underlying mechanisms. Here, we utilized random forest and neural network algorithms to predict the grassland degradation represented by the net primary productivity (NPP) changing rate based on multi-source data including geographic, meteorological, plant traits, land use type and microbial variables in the Chinese Northern grassland. Particularly, the microbial roles in determining the degradation status were concerned. Results show that a high prediction precision was achieved by random forest model, rather than by neural network model, with a mean relative error of 16.9% and a mean square error of 9.273e-05. Besides identified longitude, arid index and current NPP state, specific soil microbial groups, mainly Solirubrobacter, were screened as credible biomarkers. Regarding model fitting, geographic, meteorological and plant variables explained 61.8% of the total variance, which was enhanced up to 72.8% by the rest microbial markers. These findings provide a theoretical basis to establish a pre-warning system for grassland management and policy-making.",
        "DOI": "10.1016/j.ecolind.2022.108738",
        "paper_author": "Yan H.",
        "affiliation_name": "University of Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60027363",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Assessing the value of data for prediction policies: The case of antibiotic prescribing",
        "publication": "Economics Letters",
        "citied_by": "3",
        "cover_date": "2022-04-01",
        "Abstract": "We quantify the value of data for the prediction policy problem of reducing antibiotic prescribing to curb antibiotic resistance. Using varying combinations of administrative data, we evaluate machine learning predictions for diagnosing bacterial urinary tract infections and the outcomes of prescription rules based on these predictions. Simple patient demographics improve prediction quality substantially but larger reductions in prescribing can be achieved by making use of rich health data. Our results suggest decreasing returns to data for prediction quality and increasing returns for policy outcomes. Hence, data needs for prediction policy problems must be assessed based on the policy objective and not only on prediction quality.",
        "DOI": "10.1016/j.econlet.2022.110360",
        "paper_author": "Huang S.",
        "affiliation_name": "German Institute for Economic Research",
        "affiliation_city": "Berlin",
        "affiliation_country": "Germany",
        "affiliation_id": "60006658",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Deep non-crossing probabilistic wind speed forecasting with multi-scale features",
        "publication": "Energy Conversion and Management",
        "citied_by": "23",
        "cover_date": "2022-04-01",
        "Abstract": "Clean and renewable wind energy has made an outstanding contribution to alleviating the energy crisis. However, the randomness and volatility of wind brings great risk to the integration of wind power to the grid. Therefore, it is essential to obtain reliable and efficient wind speed forecasts. Quantile-based machine learning techniques, which usually produce satisfied quantile-based prediction intervals (PIs) for wind energy, have received widespread attention. However, the obtained PIs are usually crossed and violate the monotonicity of different conditional quantiles. In addition, the completeness and quality of features directly affect the forecasting performance of the models. Therefore, mining effective and sufficient information from the limited input data helps to improve the forecasting performance. In this paper, a novel method is developed for probabilistic wind speed forecasting based on deep learning, non-crossing quantile loss, multi-scale feature (MSF) extraction, and kernel density estimation (KDE). In terms of feature extraction, sufficient MSFs with simple pattern will be extracted based on a multi-layer convolutional neural network. Attention-based long short-term memory is used to further extract and encode temporal information for features of each scale and reduce computational cost. The final feature is obtained by concatenating all the encoded feature vectors. Instead of directly outputting different conditional quantiles, this study obtains the positive difference of adjacent conditional quantiles. On this basis, a non-crossing quantile loss is designed to ensure the monotonicity of different conditional quantiles. To understand the forecasting uncertainty comprehensively, KDE is used to estimate the continuous probability distribution function for various PIs. The proposed method is verified on four wind speed datasets collected form South Dakota. The results demonstrate that the proposed method has an excellent ability of generating high-quality, high-precision, and non-crossing probabilistic wind speed forecasts.",
        "DOI": "10.1016/j.enconman.2022.115433",
        "paper_author": "Zou R.",
        "affiliation_name": "Central South University",
        "affiliation_city": "Changsha",
        "affiliation_country": "China",
        "affiliation_id": "60017060",
        "affiliation_state": "Hunan"
    },
    {
        "paper_title": "A review of UAV platforms, sensors, and applications for monitoring of sugarcane crops",
        "publication": "Remote Sensing Applications: Society and Environment",
        "citied_by": "88",
        "cover_date": "2022-04-01",
        "Abstract": "Recent advancements in the application of unmanned aerial vehicles (UAVs) based remote sensing (RS) in precision agricultural practices have been critical in enhancing crop health and management. UAV-based RS and advanced computational algorithms including Artificial Intelligence (AI), Machine Learning (ML) and Deep Learning (DL), are progressively being applied to make predictions, solve decisions to optimize the production and operation processes in many farming industries such as sugarcane. UAVs with various advanced sensors, including RGB, multispectral, hyperspectral, LIDAR, and thermal cameras, have been used for crop RS applications as they can provide new approaches and research opportunities in precision sugarcane production. This review focuses on the use of UAVs in the sugarcane industry for pest and disease management, yield estimation, phenotypic measurement, soil moisture assessment, and nutritional status evaluation to improve the productivity and environmental sustainability. The goals of this review were to: (1) assemble information on the application of UAVs in the sugarcane industry; and (2) discuss their benefits and limitations in a variety of applications in UAV-based sugarcane cultivation. A literature review was conducted utilizing three bibliographic databases, including Google Scholar, Scopus, Web of Science, and 179 research articles that are relevant to UAV applications in sugarcane and other general information about UAV and sensors collected from the databases mentioned earlier. The study concluded that UAV-based crop RS can be an effective method for sugarcane monitoring and management to improve yield and quality and significantly benefits on social, economic, and environmental aspects. However, UAV-based RS should also consider some of the challenges in sugar industries include technological adaptations, high initial cost, inclement weather, communication failures, policy, and regulations.",
        "DOI": "10.1016/j.rsase.2022.100712",
        "paper_author": "Amarasingam N.",
        "affiliation_name": "Queensland University of Technology",
        "affiliation_city": "Brisbane",
        "affiliation_country": "Australia",
        "affiliation_id": "60011019",
        "affiliation_state": "QLD"
    },
    {
        "paper_title": "A multinomial probit model with Choquet integral and attribute cut-offs",
        "publication": "Transportation Research Part B: Methodological",
        "citied_by": "5",
        "cover_date": "2022-04-01",
        "Abstract": "Several non-linear functions and machine learning methods have been developed for flexible specification of the systematic utility in discrete choice models. However, they lack interpretability, do not ensure monotonicity conditions, and restrict substitution patterns. We address the first two challenges by modeling the systematic utility using the Choquet Integral (CI) function and the last one by embedding CI into the multinomial probit (MNP) choice probability kernel. We also extend the MNP-CI model to account for attribute cut-offs that enable a modeler to approximately mimic the semi-compensatory behavior using the traditional choice experiment data. The MNP-CI model is estimated using a constrained maximum likelihood approach, and its statistical properties are validated through a comprehensive Monte Carlo study. The CI-based choice model is empirically advantageous as it captures interaction effects while maintaining monotonicity. It also provides information on the complementarity between pairs of attributes coupled with their importance ranking as a by-product of the estimation. These insights could potentially assist policymakers in making policies to improve the preference level for an alternative. These advantages of the MNP-CI model with attribute cut-offs are illustrated in an empirical application to understand New Yorkers’ preferences towards mobility-on-demand services.",
        "DOI": "10.1016/j.trb.2022.02.007",
        "paper_author": "Dubey S.",
        "affiliation_name": "Department of Transport &amp; Planning, TU Delft",
        "affiliation_city": "Delft",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60118020",
        "affiliation_state": "Zuid-Holland"
    },
    {
        "paper_title": "The evaluation of operational efficiencies of Turkish airports: An integrated spherical fuzzy AHP/DEA approach",
        "publication": "Applied Soft Computing",
        "citied_by": "34",
        "cover_date": "2022-04-01",
        "Abstract": "The demand for air transport services has significantly increased around the globe, which has brought new investments in airports, which, in turn, requires in-depth efficiency analysis of these capital-intensive endeavors. This study examines the operational efficiencies of 46 Turkish civil airports from 2015 to 2018. We employ a novel hybrid methodology that combines Spherical Fuzzy Sets based Analytic Hierarchy Process (SFS-AHP) and Data Envelopment Analysis (DEA), which provides a solid basis for efficiency analysis. To this end, it can handle the hesitancy and uncertainty that the subjective evaluation process of input and output factors possess. Then, we use Self Organizing Maps (SOM), a machine learning method for clustering, to examine the effect of outlier airports on the efficiency scores. Finally, a posthoc analysis is conducted with Tobit regression model to assess the explanatory power of external factors on the efficiency scores, i.e., tourism potential, number of international flights, distance to the city center, population, public/private ownership, and age of airport. The findings show that 67.2% of the Turkish airports operate below the optimal efficiency level, and 93.5% of them should make considerable efforts to refine their operations by implementing managerial and structural changes to reduce input factors. The results also suggest that the airports located in high-density touristic areas achieve higher efficiency levels. Those relatively closer to the city center lead to more airport traffic, generating more revenues. Thus, both factors have a significant impact on efficiency scores. The study provides a novel efficiency analysis framework for airport operators and policy makers that helps them make informed decisions.",
        "DOI": "10.1016/j.asoc.2022.108620",
        "paper_author": "Yilmaz M.K.",
        "affiliation_name": "İbn Haldun Üniversitesi",
        "affiliation_city": "Istanbul",
        "affiliation_country": "Turkey",
        "affiliation_id": "60118045",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Machine learning approach for spatial modeling of ridesourcing demand",
        "publication": "Journal of Transport Geography",
        "citied_by": "21",
        "cover_date": "2022-04-01",
        "Abstract": "Accurately forecasting ridesourcing demand is important for effective transportation planning and policy-making. With the rise of Artificial Intelligence (AI), researchers have started to utilize machine learning models to forecast travel demand, which, in many cases, can produce higher prediction accuracy than statistical models. However, most existing machine-learning studies used a global model to predict the demand and ignored the influence of spatial heterogeneity (i.e., the spatial variations in the impacts of explanatory variables). Spatial heterogeneity can drive the parameter estimations varying over space; failing to consider the spatial variations may limit the model's prediction performance. To account for spatial heterogeneity, this study proposes a Clustering-aided Ensemble Method (CEM) to forecast the zone-to-zone (census-tract-to-census-tract) travel demand for ridesourcing services. Specifically, we develop an interactive clustering approach (powered by human-in-the-loop AI) to split the origin-destination pairs into different clusters and ensemble the cluster-specific machine learning models for prediction. We implement and test the proposed methodology by using the ridesourcing-trip data in Chicago. The results show that, with a more transparent and flexible model structure, the CEM significantly improves the prediction accuracy than the benchmark models (i.e., global machine-learning and statistical models directly trained on all observations). This study offers transportation researchers and practitioners a new methodology of travel demand forecasting, especially for new travel modes like ridesourcing and micromobility.",
        "DOI": "10.1016/j.jtrangeo.2022.103310",
        "paper_author": "Zhang X.",
        "affiliation_name": "Herbert Wertheim College of Engineering",
        "affiliation_city": "Gainesville",
        "affiliation_country": "United States",
        "affiliation_id": "60154244",
        "affiliation_state": "FL"
    },
    {
        "paper_title": "Workplace Predictors of Quality and Safe Patient Care Delivery among Nurses Using Machine Learning Techniques",
        "publication": "Journal of Nursing Care Quality",
        "citied_by": "7",
        "cover_date": "2022-04-01",
        "Abstract": "Background: Working in unhealthy environments is associated with negative nurse and patient outcomes. Previous body of evidence in this area is limited as it investigated only a few factors within nurses' workplaces. Purpose: The purpose of this study was to identify the most important workplace factors predicting nurses' provision of quality and safe patient care using a 13-factor measure of workplace conditions. Methods: A cross-sectional correlational survey study involving 4029 direct care nurses in British Columbia was conducted using random forest data analytics methods. Results: Nurses' reports of healthier workplaces, particularly workload management, psychological protection, physical safety and engagement, were associated with higher ratings of quality and safe patient care. Conclusion: These workplace conditions are perceived to impact patient care through influencing nurses' mental health. To ensure a high standard of patient care, data-driven policies and interventions promoting overall nurse mental health and well-being are urgently required.",
        "DOI": "10.1097/NCQ.0000000000000600",
        "paper_author": "Havaei F.",
        "affiliation_name": "The University of British Columbia",
        "affiliation_city": "Vancouver",
        "affiliation_country": "Canada",
        "affiliation_id": "60010365",
        "affiliation_state": "BC"
    },
    {
        "paper_title": "Operational framework to predict field level crop biomass using remote sensing and data driven models",
        "publication": "International Journal of Applied Earth Observation and Geoinformation",
        "citied_by": "18",
        "cover_date": "2022-04-01",
        "Abstract": "Remote Sensing (RS) based monitoring provides opportunities to acquire timely and reliable information on crop growth at diverse scales. Crop yield forecasts can help decision makers to formulate policies on maintaining national food reserves, sustaining food supply chains and attaining national food security. Inter field crop yield heterogeneity arising from varied field management and agricultural practices necessitates this forecasting to be done at field-level. Such field level information can aid farmers to identify gaps in water and farm management practices and take corrective actions, if needed. So far, no scalable tools are available to predict crop yields at field level that leverage the availability of open-access high-resolution RS data. This research implements an operational framework to predict field level crop biomass by evaluating different regression algorithms to develop data driven models, leveraging historical and near real time open-access high resolution optical satellite data from Sentinel-2, radar data from Sentinel-1 and evapotranspiration (ETa) and Net Primary Production (NPP) data from Food and Agriculture Organization's (FAO) Water Productivity through Open-access Remotely sensed data (WaPOR) platform. NPP is used as a proxy for biomass production/yield. Five of the most commonly used regression algorithms were tested to build a data-driven model for sugarcane NPP prediction in Wonji-Shoa sugarcane estate, located in the Awash Basin, Ethiopia. The models tested were the Multivariate Linear Regression (MLR), Stepwise Multivariate Linear Regression (SMLR), Boosted Regression Trees (BRT), Support Vector Regression (SVR), and Random Forest Regression (RFR). The results revealed that for seasonal sugarcane NPP predictions, the linear regression models (MLR and SMLR) yielded more accurate predictions than the non-linear machine learning models (BRT, SVR and RFR) tested. The highest accuracy was achieved for MLR models for which estimates with 89% accuracy could be made 4 months prior to the harvest and with accuracies of 79% up to 200 days (approx. 6.5 months) before the harvest. The non-linear machine learning models, however, could not provide reliable estimates of sugarcane NPP (accuracies < 61%). Cumulative vegetation indices (VIs) were found to have higher predictive power than standard VIs for predicting future sugarcane NPP. Cumulative Enhanced Vegetation Index (EVI) was found to be the variable with the highest predictive power, followed by VH polarized Sentinel-1 Synthetic Aperture Radar data and WaPOR ETa. The study shows the usefulness of high-resolution RS information to predict seasonal NPP at field level. The methods presented here can be translated into an automated framework towards an operational system.",
        "DOI": "10.1016/j.jag.2022.102725",
        "paper_author": "Servia H.",
        "affiliation_name": "IHE Delft Institute for Water Education",
        "affiliation_city": "Delft",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60022091",
        "affiliation_state": "Zuid-Holland"
    },
    {
        "paper_title": "Car accidents, smartphone adoption and 3G coverage",
        "publication": "Journal of Economic Behavior and Organization",
        "citied_by": "4",
        "cover_date": "2022-04-01",
        "Abstract": "This paper examines the relationship between smartphone use by drivers and traffic accidents in California between 2001 and 2013. In order to estimate smartphone use, we first show that widespread adoption of modern smartphones began in 2009 after the release of the iPhone 3G and T-Mobile G1. This information is combined with annual 3G coverage maps that are constructed from cellular tower information in a machine learning framework. In a difference-in-differences framework, we estimate the combined effect of smartphone adoption and 3G coverage along quarter-mile road segments. Controlling for census tract population density, road and year fixed effects, Poisson regression results show that there is a statistically significant increase in the traffic accident rate along a road segment when smartphone use becomes possible. Our preferred specification suggests smartphones caused accident rates to increase by 2.9 percent, resulting in 3500 additional accidents per year in California. Event study results rule out the possibility that our smartphone treatment is capturing a trend in the accident rate. The results are robust to a variety of specifications and consistent with individual-level studies showing that cell phone use leads to lower driving quality. The findings also provide guidance for policies aimed at reducing cell phone related accidents and distracted driving.",
        "DOI": "10.1016/j.jebo.2022.01.033",
        "paper_author": "Hersh J.",
        "affiliation_name": "Chapman University",
        "affiliation_city": "Orange",
        "affiliation_country": "United States",
        "affiliation_id": "60016569",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Fairness by awareness? On the inclusion of protected features in algorithmic decisions",
        "publication": "Computer Law and Security Review",
        "citied_by": "3",
        "cover_date": "2022-04-01",
        "Abstract": "AI decisions are increasingly determining our everyday lives. At present, European anti-discrimination law is process-oriented; it prohibits the inclusion of sensitive data that is particularly protected. However, especially in the context of AI decisions, constellations can be identified in which the inclusion of sensitive characteristics will lead to better and sometimes even less discriminatory result. A result-oriented approach, therefore, might be a more fitting strategy for algorithmic decision making. In this paper we examine the legal framework for including sensitive features in a Support Vector Machine for a fictitious scenario and discuss the resulting challenges in practical application. It turns out that generally ignoring sensitive features - as has been the practice up to now - does not seem to be a fitting strategy for algorithmic decision making. A process-oriented procedure only supposedly comes closer to individual case justice: If one assumes that fewer errors occur when protected characteristics are included, individuals will ultimately also be assessed incorrectly less often, especially when one protected group is more prone to errors than the other. This paper aims to support the current debate about legal regulation of algorithmic decision making systems by discussing an often neglected perspective.",
        "DOI": "10.1016/j.clsr.2022.105658",
        "paper_author": "Hoffmann H.",
        "affiliation_name": "University of Münster",
        "affiliation_city": "Munster",
        "affiliation_country": "Germany",
        "affiliation_id": "60000401",
        "affiliation_state": "Nordrhein-Westfalen"
    },
    {
        "paper_title": "Using cognition and risk to explain the intention-behavior gap on bioenergy production: Based on machine learning logistic regression method",
        "publication": "Energy Economics",
        "citied_by": "20",
        "cover_date": "2022-04-01",
        "Abstract": "Bioenergy production is a certain economy energy utilization mode, which is of great economic and ecological benefits. Only when pig farmers have consistent intentions and behaviors to participate in bioenergy production, can the intentions play their role in effectively predicting behaviors. Based on the machine learning logistic regression method, taking biogas produced by swine manure as an example, we explore the role of cognition and risk in bridging the intention-behavior gap in bioenergy production. Unlike previous studies, we find that for bioenergy production, a pro-environmental behavior with positive externalities, an individual's perception of environmental policy plays a better role in driving the intention-to-behavior transition than the individual's perception of bioenergy production. From the risk perspective, our results also suggest that the key factor hindering an intention to change behavior is the individual's risk preferences rather than the degree of risk associated with bioenergy production. Policy makers could consider this observed heterogeneity when it comes to aspects such as greater highlight on environmental policy advocacy, and collaboration with insurance companies to develop products for bioenergy production.",
        "DOI": "10.1016/j.eneco.2022.105885",
        "paper_author": "He K.",
        "affiliation_name": "Huazhong Agricultural University",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60032955",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Big data for human security: The case of COVID-19",
        "publication": "Journal of Computational Science",
        "citied_by": "4",
        "cover_date": "2022-04-01",
        "Abstract": "The COVID-19 epidemic has changed the world dramatically since societies are changing their behaviour according to the new normal, which comes along with numerous challenges and uncertainties. These uncertainties have led to instabilities in several facets of society, most notably health, economy and public order. Measures to contain the pandemic by governments have occasionally met with increasing discontent from societies and have triggered social unrest, imposing serious threats to human security. Big Data Analytics can provide a powerful force multiplier to support policy and decision makers to contain the virus while at the same time dealing with such threats to human security. This paper presents the utilisation of a big data forecasting and analytics framework and its utilisation to deal with COVID-19 triggered social unrest. The paper is an extended version of paper Cárdenas et al. (2021) presented at the 2021 International Conference on Computational Science.",
        "DOI": "10.1016/j.jocs.2022.101574",
        "paper_author": "Cárdenas P.",
        "affiliation_name": "Durham University",
        "affiliation_city": "Durham",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60022175",
        "affiliation_state": "County Durham"
    },
    {
        "paper_title": "A physics-informed reinforcement learning-based strategy for local and coordinated ramp metering",
        "publication": "Transportation Research Part C: Emerging Technologies",
        "citied_by": "51",
        "cover_date": "2022-04-01",
        "Abstract": "This paper proposes a physics-informed reinforcement learning(RL)-based ramp metering strategy, which trains the RL model using a combination of historic data and synthetic data generated from a traffic flow model. The optimal policy of the RL model is updated through an iterative training process, where in each iteration a new batch of historic data is collected and fed into the training data set. Such iterative training process can evaluate the control policy from reality rather than from a simulator, thus avoiding the RL model being trapped in an inaccurate training environment. The proposed strategy is applied to both local and coordinated ramp metering. Results from extensive microscopic simulation experiments demonstrate that the proposed strategy (i) significantly improves the traffic performance in terms of total time spent savings; (ii) outperforms classical feedback-based ramp metering strategies; and (iii) achieves higher improvements than an existing RL-based ramp metering strategy, which trains the RL model merely by a simulator. We also test the performance of two different learning algorithms in the simulation experiment, namely a conventional tabular approach and a batch-constrained deep RL approach. It is found that the deep RL approach is not as effective as the conventional tabular approach in the proposed strategy due to the limited amount of training data.",
        "DOI": "10.1016/j.trc.2022.103584",
        "paper_author": "Han Y.",
        "affiliation_name": "Southeast University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60005244",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "Meta-Residual Policy Learning: Zero-Trial Robot Skill Adaptation via Knowledge Fusion",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "13",
        "cover_date": "2022-04-01",
        "Abstract": "Adapting the mastered manipulation skill to novel objects is still challenging for robots. Recent works have attempted to endow the robot with the ability to adapt to unseen tasks by leveraging meta-learning. However, these methods are data-hungry in the training phase, which limits their application in the real world. In this paper, we propose Meta-Residual Policy Learning (MRPL) to reduce the cost of policy learning and adaptation. During meta-training, MRPL accelerates the learning process by focusing on the residual task-shared knowledge that is hard to be embedded in the hand-engineered controller. During testing, MRPL achieves fast adaptation on similar unseen tasks through fusing task-specific knowledge in the demonstration with task-shared knowledge in the learned policy. We conduct a series of simulated and real-world peg-in-hole tasks to evaluate the proposed method. The experimental results demonstrate that MRPL outperforms prior methods in robot skill adaptation. Code for this work is available at https://github.com/Bartopt/code4MRPL.",
        "DOI": "10.1109/LRA.2022.3146916",
        "paper_author": "Hao P.",
        "affiliation_name": "Institute of Automation Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60018486",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Assessing gully erosion susceptibility and its conditioning factors in southeastern Brazil using machine learning algorithms and bivariate statistical methods: A regional approach",
        "publication": "Geomorphology",
        "citied_by": "33",
        "cover_date": "2022-04-01",
        "Abstract": "Despite being a very common phenomenon worldwide, in Brazil, the factors and mechanisms that control gully erosion on a regional scale are still little known, which leads to the neglect of this environmental hazard by territorial and environmental management policies. In order to reducing this gap, we explored the potential of four common supervised machine learning algorithms, named random forest (RF), logistic regression (LR), naïve Bayes (NB) and artificial neural network (ANN) to produce gully erosion susceptibility models for two gullied watersheds, located in the state of Minas Gerais, southeastern Brazil. The modeling was based on the construction of a solid gully inventory and a database consisting of fifteen geo-environmental factors (GEF), whose influence was determined from the information gain ratio (IGR) and two bivariate statistical methods, named frequency ratio (FR) and modified information value (MIV). The predictive performance of the models was evaluated by the area under the receiver operating characteristic curve (AUC), overall accuracy (ACC) and sufficiency analysis. The results revealed that random forest achieved the highest overall performance in correct prediction of gullies and produced the most realistic gully susceptibility maps. The IGR data indicated that all GEF considered in the analysis contributed to the predictive model, although lithology, elevation and rainfall are the most influential variables. From an integrated analysis between the gully inventory, field observations, FR and MIV values, we found that gullies seem to be triggered by high annual average rainfall, but only develop where a set of specific geo-environmental conditions occur simultaneously. Finally, despite the limited land use data available, anthropogenic activities do not seem to affect the regional distribution pattern of gullies, although we have not excluded their local influence in triggering some erosive features.",
        "DOI": "10.1016/j.geomorph.2022.108159",
        "paper_author": "Lana J.C.",
        "affiliation_name": "Geological Survey of Brazil (CPRM)",
        "affiliation_city": "Belo Horizonte",
        "affiliation_country": "Brazil",
        "affiliation_id": "100538300",
        "affiliation_state": "MG"
    },
    {
        "paper_title": "Entropy Tucker model: Mining latent mobility patterns with simultaneous estimation of travel impedance parameters",
        "publication": "Transportation Research Part C: Emerging Technologies",
        "citied_by": "5",
        "cover_date": "2022-04-01",
        "Abstract": "With the rapid increase in the availability of passive data in the field of transportation, combining machine learning with transportation models has emerged as an important research topic in recent years. This study proposes an entropy Tucker model that integrates (1) a Tucker decomposition technique, i.e., an existing machine learning method, and (2) an entropy maximizing model, i.e., an existing model used for modeling trip distribution in the field of transportation. In addition, an optimization algorithm is presented to empirically identify the proposed model. The proposed model provides a solid theoretical foundation for the machine learning method, substantially improves prediction performance, and provides richer behavioral implications through empirical parameter estimation of travel impedance. We conducted a case study using public transit smart card data. The results showed that the proposed model improves the prediction performance and interpretability of the results compared to the conventional nonnegative Tucker decomposition technique. Further, we empirically confirmed that the travel impedance varies with the origin–destination pair, time of the day, and day of the week. Finally, we discussed how embedding the theoretical foundations of transport modeling into machine learning methods can facilitate the use of various passive data in wider practical contexts of transport policy decision making.",
        "DOI": "10.1016/j.trc.2022.103559",
        "paper_author": "Ishii Y.",
        "affiliation_name": "Toyota Central Research Development Laboratory Inc",
        "affiliation_city": "Aichi District",
        "affiliation_country": "Japan",
        "affiliation_id": "60003416",
        "affiliation_state": "Aichi"
    },
    {
        "paper_title": "Female enrollment, child mortality and corruption are good predictors of a country's UN Education Index",
        "publication": "International Journal of Educational Development",
        "citied_by": "4",
        "cover_date": "2022-04-01",
        "Abstract": "The United Nations (UN) Education Index (EI), a widely accepted measure of educational attainment within a country and a component of the Human Development Index, is measured by combining the average adult years of schooling with expected years of schooling for children. It is concerning that sub-Saharan African (SSA) countries tend to have a relatively low average EI of 0.45 compared to a global average of 0.65 for all countries. This suggests that SSA countries are underperforming in this regard. Insufficient attention has been devoted to identifying key factors that could explain the variability in educational attainment and, most especially, the underperformance in SSA countries. This study investigates the predictive power of female enrollment, child mortality, and corruption, among other variables, to explain the EI. The data comprised 165 countries and 47 explanatory variables, and one target variable (EI), obtained from the World Bank and Transparency International. Machine learning and variable selection techniques were utilized to identify the relevant indicators. Predictive performance (in-sample and out-of-sample using cross-validation) facilitated a deeper understanding of the indicators that best explain the variation in the EI. The variable selection process was accomplished using the least absolute shrinkage and selection operator (LASSO). The mortality rate of children under the age of five and the corruption perception index are two non-education-related indicators that contributed most to the prediction of the dependent variable (EI). The education-related predictor was identified as the net female enrollment in secondary schools. Afterward, a linear regression was performed on the selected variables. To ensure that the model can generalize, a cross-validation technique was applied using the leave one out (LOO) approach. The out-of-sample performance of R2 = 0.89 implies that the model explains much of the variability in the EI. The identified relationships offer a basis for policy recommendations.",
        "DOI": "10.1016/j.ijedudev.2022.102561",
        "paper_author": "Adeleke O.",
        "affiliation_name": "Carnegie Mellon University",
        "affiliation_city": "Pittsburgh",
        "affiliation_country": "United States",
        "affiliation_id": "60027950",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "A multilayered block network model to forecast large dynamic transportation graphs: An application to US air transport",
        "publication": "Transportation Research Part C: Emerging Technologies",
        "citied_by": "9",
        "cover_date": "2022-04-01",
        "Abstract": "Dynamic transportation networks have been analyzed for years by means of static graph-based indicators in order to study the temporal evolution of relevant network components, and to reveal complex dependencies that would not be easily detected by a direct inspection of the data. This paper presents a state-of-the-art probabilistic latent network model to forecast multilayer dynamic graphs that are increasingly common in transportation and proposes a community-based extension to reduce the computational burden. Flexible time series analysis is obtained by modeling the probability of edges between vertices through latent Gaussian processes. The models and Bayesian inference are illustrated on a sample of 10-year data from four major airlines within the US air transportation system. Results show how the estimated latent parameters from the models are related to the airlines’ connectivity dynamics, and their ability to project the multilayer graph into the future for out-of-sample full network forecasts, while stochastic blockmodeling allows for the identification of relevant communities. Reliable network predictions would allow policy-makers to better understand the dynamics of the transport system, and help in their planning on e.g. route development, or the deployment of new regulations.",
        "DOI": "10.1016/j.trc.2022.103556",
        "paper_author": "Rodriguez-Deniz H.",
        "affiliation_name": "Linköpings Universitet",
        "affiliation_city": "Linkoping",
        "affiliation_country": "Sweden",
        "affiliation_id": "60009358",
        "affiliation_state": "Östergötland"
    },
    {
        "paper_title": "Effective identification of distributed energy resources using smart meter net-demand data",
        "publication": "IET Smart Grid",
        "citied_by": "11",
        "cover_date": "2022-04-01",
        "Abstract": "International policies and targets to globally reduce carbon dioxide emissions have contributed to increasing penetration of distributed energy resources (DER) in low-voltage distribution networks. The growth of technologies such as rooftop photovoltaic (PV) systems and electric vehicles (EV) has, to date, not been rigorously monitored and record keeping is deficient. Non-intrusive load monitoring (NILM) methods contribute to the effective integration of clean technologies within existing distribution networks. In this study, a novel NILM method is developed for the identification of DER electrical signatures from smart meter net-demand data. Electrical profiles of EV and PV systems are allocated within aggregated measurements including conventional electrical appliances. Data from several households in the United States are used to train and test classification and regression models. The usage of conventional machine learning techniques provides the proposed algorithm with fast processing times and low system complexity, key factors needed to differentiate highly variable DER power profiles from other loads. The results confirm the effectiveness of the proposed methodology to individually classify DER with performance metrics of 96% for EV and 99% for PV. This demonstrates the potential of the proposed method as an embedded function of smart meters to increase observability in distribution networks.",
        "DOI": "10.1049/stg2.12056",
        "paper_author": "Moreno Jaramillo A.F.",
        "affiliation_name": "Queen's University Belfast",
        "affiliation_city": "Belfast",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60029738",
        "affiliation_state": "Northern Ireland"
    },
    {
        "paper_title": "Interpretable Autonomous Flight Via Compact Visualizable Neural Circuit Policies",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "9",
        "cover_date": "2022-04-01",
        "Abstract": "We learn interpretable end-to-end controllers based on Neural Circuit Policies (NCPs) to enable goal reaching and dynamic obstacle avoidance in flight domains. In addition to being able to learn high-quality control, NCP networks are designed with a small number of neurons. This property allows for the learned policies to be interpreted at the neuron level and interrogated, leading to more robust understanding of why the artificial agents make the decisions that they do. We also demonstrate transfer of the learned policy to physical flight hardware by deploying a small NCP (200 KB of memory) capable of real-time inference on a Raspberry Pi Zero controlling a DJI Tello drone. Designing interpretable artificial agents is crucial for building trustworthy AIs, both as fully autonomous systems and also for parallel autonomy, where humans and AIs work on collaboratively solving problems in the same environment.",
        "DOI": "10.1109/LRA.2022.3146555",
        "paper_author": "Tylkin P.",
        "affiliation_name": "MIT Computer Science &amp; Artificial Intelligence Laboratory",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States",
        "affiliation_id": "60006320",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Pedestrian volume prediction with high spatiotemporal granularity in urban areas by the enhanced learning model",
        "publication": "Sustainable Cities and Society",
        "citied_by": "11",
        "cover_date": "2022-04-01",
        "Abstract": "Pedestrian volume prediction is a key strategy to explore the spatial patterns of pedestrian mobility and develop urban policies. However, due to the expensive costs of field sampling, most existing models are established on insufficient pedestrian samples and obtain limited prediction performance. Therefore, this study proposes an enhanced learning model for pedestrian volume prediction with high spatiotemporal granularity in urban areas. The enhanced learning model is applied to a case study in the central business district (CBD) of the city of Melbourne, Victoria, Australia. More than 1400 features are constructed for pedestrian estimation, covering macro aspects of transportation, socioeconomics, road networks, time, land use and place of interest. Compared with the optimal supervised learning model of LightGBM (Light Gradient Boosting Machine), the LightGBM-based enhanced learning model can significantly improve the performance of pedestrian estimation with root-mean-square error (RMSE) reduced by 41.75% and R-squared (R2) improved by 27.75%. The important parameters (i.e., spatial resolution, combination parameter) of the enhanced learning model are validated to significantly affect model performance. According to the spatiotemporal analysis of pedestrian volume in geographic information system (GIS) maps, different measures are proposed to optimize urban mobility and improve city management.",
        "DOI": "10.1016/j.scs.2021.103653",
        "paper_author": "Jiang F.",
        "affiliation_name": "The University of Hong Kong",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong",
        "affiliation_id": "60006541",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Towards a machine-learning based approach for splitting cities in freight logistics context: Benchmarks of clustering and prediction models",
        "publication": "Computers and Industrial Engineering",
        "citied_by": "6",
        "cover_date": "2022-04-01",
        "Abstract": "Urban mobility consists of three basic components that create the essential functioning for the well-being of communities namely facilities, structures, and processes. Often these components have commentary roles where processes must assess infrastructures and related operations to support policies that relate to spatial structure and transportation patterns. Zoning tools are sample processes that are designed to split areas according to given criteria and purposes in order to better implement transport facilities, investments, and plans. This article proposes a sequential approach combining several machine-learning tools of clustering and forecasting that are thought efficient according to the Key Performance Indicators (KPI). In both processes, the proposed machine-learning zoning approach (MLZA) has considered the location of sites requiring logistics services and the evolution of their demand, respectively, in order to accomplish a long-term splitting of urban land. For improving the performance of the clustering process, we have used 30 KPIs including all combinations of a number of built clusters. In doing so, this step has not aimed not only to validate a clustering tool but also to identify the optimal number of established zones. Based on simulated benchmarks, results have indicated that the clustering phase of the MLZA is still appropriate using the k-means algorithm. To evaluating forecast accuracy in the forecasting phase, we have measured the standard KPIs namely the MSE (Mean Squared Error), RMSE (Root Mean Square Error), MAPE (Mean Absolute Percentage Error), and R2 (R-squared). The Support Vector Machine (SVM) algorithm has been deemed to be the most efficient forecasting algorithm regarding the average values of the obtained performance measurements.",
        "DOI": "10.1016/j.cie.2022.107975",
        "paper_author": "El Ouadi J.",
        "affiliation_name": "Hassan II University of Casablanca",
        "affiliation_city": "Casablanca",
        "affiliation_country": "Morocco",
        "affiliation_id": "60025457",
        "affiliation_state": "Casablanca-Settat"
    },
    {
        "paper_title": "Habitat selection across nested scales and home range assessments of the juvenile black-necked crane (Grus nigricollis) in the post-breeding period",
        "publication": "Global Ecology and Conservation",
        "citied_by": "4",
        "cover_date": "2022-04-01",
        "Abstract": "To know the details of the habitat selection and home range of black-necked cranes (Grus nigricollis) and how grazing influences them, we observed juvenile members of the population with satellite tracking in the Danghe wetland of the Yanchiwan National Nature Reserve in Gansu from 2018 to 2020 during the months of July–August. Population monitoring was also conducted during the same period. The home range was quantified with kernel density estimation methods. Then, we utilized remote sensing image interpretation with machine learning to identify different habitat types in the Danghe wetland. Manly's selection ratios and random forest model were employed to assess habitat selection in home range scale and habitat scale. In the study area, a grazing restriction policy was implemented in 2019, and the response of Black-necked cranes suggest as follows: a) the number of young cranes increased from 23 to 50, which indicates a grazing regime affects cranes fitness; b) the current grazing regime does not affect the areas of home range and the selection of habitat types, but it affects the crane's use of space as the mean overlap index of the home range was 1.39% ± 3.47% and 0.98% ± 4.15% in 2018 and 2020 years, respectively; c) there was an overall increasing trend in mean daily movement distance and instantaneous velocity indicate a movement ability increases of young cranes, and the ratio of disturbed cranes becomes greater; d) Human disturbance factors have little effect on habitat selection, and cranes are hardly affected by houses and roads currently. The cranes selected lakes, but comparing the home range and habitat scale selection, marsh, river and mountain range cannot be ignored. Therefore, we believe that continuing the grazing restriction policy will help to reduce the overlap of home ranges and subsequently reduce intraspecific competition, and then it increases the safety of movements of young cranes, and ultimately increases population fitness. Further, it is important to manage the water resources and maintain the existing distribution of roads and buildings throughout the wetlands.",
        "DOI": "10.1016/j.gecco.2022.e02011",
        "paper_author": "Li X.",
        "affiliation_name": "Beijing Forestry University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60006782",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Correct Me if i am Wrong: Interactive Learning for Robotic Manipulation",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "24",
        "cover_date": "2022-04-01",
        "Abstract": "Learning to solve complex manipulation tasks from visual observations is a dominant challenge for real-world robot learning. Although deep reinforcement learning algorithms have recently demonstrated impressive results in this context, they still require an impractical amount of time-consuming trial-and-error iterations. In this work, we consider the promising alternative paradigm of interactive learning in which a human teacher provides feedback to the policy during execution, as opposed to imitation learning where a pre-collected dataset of perfect demonstrations is used. Our proposed CEILing (Corrective and Evaluative Interactive Learning) framework combines both corrective and evaluative feedback from the teacher to train a stochastic policy in an asynchronous manner, and employs a dedicated mechanism to trade off human corrections with the robot's own experience. We present results obtained with our framework in extensive simulation and real-world experiments to demonstrate that CEILing can effectively solve complex robot manipulation tasks directly from raw images in less than one hour of real-world training.",
        "DOI": "10.1109/LRA.2022.3145516",
        "paper_author": "Chisari E.",
        "affiliation_name": "Universität Freiburg",
        "affiliation_city": "Freiburg im Breisgau",
        "affiliation_country": "Germany",
        "affiliation_id": "60025641",
        "affiliation_state": "Baden-Wurttemberg"
    },
    {
        "paper_title": "Focus on Impact: Indoor Exploration with Intrinsic Motivation",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "14",
        "cover_date": "2022-04-01",
        "Abstract": "Exploration of indoor environments has recently experienced a significant interest, also thanks to the introduction of deep neural agents built in a hierarchical fashion and trained with Deep Reinforcement Learning (DRL) on simulated environments. Current state-of-the-art methods employ a dense extrinsic reward that requires the complete a priori knowledge of the layout of the training environment to learn an effective exploration policy. However, such information is expensive to gather in terms of time and resources. In this work, we propose to train the model with a purely intrinsic reward signal to guide exploration, which is based on the impact of the robot's actions on its internal representation of the environment. So far, impact-based rewards have been employed for simple tasks and in procedurally generated synthetic environments with countable states. Since the number of states observable by the agent in realistic indoor environments is non-countable, we include a neural-based density model and replace the traditional count-based regularization with an estimated pseudo-count of previously visited states. The proposed exploration approach outperforms DRL-based competitors relying on intrinsic rewards and surpasses the agents trained with a dense extrinsic reward computed with the environment layouts. We also show that a robot equipped with the proposed approach seamlessly adapts to point-goal navigation and real-world deployment.",
        "DOI": "10.1109/LRA.2022.3145971",
        "paper_author": "Bigazzi R.",
        "affiliation_name": "Università degli Studi di Modena e Reggio Emilia",
        "affiliation_city": "Modena",
        "affiliation_country": "Italy",
        "affiliation_id": "60004591",
        "affiliation_state": "MO"
    },
    {
        "paper_title": "Reinforcement Learning with Evolutionary Trajectory Generator: A General Approach for Quadrupedal Locomotion",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "40",
        "cover_date": "2022-04-01",
        "Abstract": "Recently reinforcement learning (RL) has emerged as a promising approach for quadrupedal locomotion, which can save the manual effort in conventional approaches such as designing skill-specific controllers. However, due to the complex nonlinear dynamics in quadrupedal robots and reward sparsity, it is still difficult for RL to learn effective gaits from scratch, especially in challenging tasks such as walking over the balance beam. To alleviate such difficulty, we propose a novel RL-based approach that contains an evolutionary foot trajectory generator. Unlike prior methods that use a fixed trajectory generator, the generator continually optimizes the shape of the output trajectory for the given task, providing diversified motion priors to guide the policy learning. The policy is trained with reinforcement learning to output residual control signals that fit different gaits. We then optimize the trajectory generator and policy network alternatively to stabilize the training and share the exploratory data to improve sample efficiency. As a result, our approach can solve a range of challenging tasks in simulation by learning from scratch, including walking on a balance beam and crawling through the cave. To further verify the effectiveness of our approach, we deploy the controller learned in the simulation on a 12-DoF quadrupedal robot, and it can successfully traverse challenging scenarios with efficient gaits. We provide a video to show the learned gaits in different tasks in YouTube.11[Online]. Available: youtube.com/watch?v=hgBLR09MEOw, and code is available in Github: github.com/PaddlePaddle/PaddleRobotics",
        "DOI": "10.1109/LRA.2022.3145495",
        "paper_author": "Shi H.",
        "affiliation_name": "The Chinese University of Hong Kong, Shenzhen",
        "affiliation_city": "Shenzhen",
        "affiliation_country": "China",
        "affiliation_id": "60108865",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Predicting individuals' car accident risk by trajectory, driving events, and geographical context",
        "publication": "Computers, Environment and Urban Systems",
        "citied_by": "23",
        "cover_date": "2022-04-01",
        "Abstract": "With the prevalence of GPS tracking technologiees, car insurance companies have started to adopt usage-based insurance policies, which adapt insurance premiums according to the customers' driving behavior. Although many risk models for assessing an individual driver's accident risk based on the history of driving trajectories, driving events, and exposure records exist, these models do not take the geographical context of the driven trajectories and driving events into account. This study explores the influence of enriching the existing purely driving-behavior-based feature set by multiple geographical context features for the task of differentiating between accident and accident-free drivers. Prediction performances of five machine learning classifiers—logistic regression, random forest, XGBoost, feed-forward neural networks (FFNN), and long short-term memory (LSTM) networks—were evaluated on the usage records of over 8000 vehicles in one year from Italy. The results show that the inclusion of geographical information such as weather, points of interest (POIs), and land use can increase the relative predictive performance in terms of AUC by up to 8%, among which land use is the most informative. For the data of this study, XGBoost generally yielded the best performance and made most use out of the geographical information, while logistic regression is only slightly outperformed by more complex models if the proposed geographical information is not available. LSTM did not outperform the other methods, possibly due to the small volume of training data available. The results outline the potential of including the geographical context in usage-based car insurance risk modeling to improve the accuracy, leading to fairer usage-based insurance policies.",
        "DOI": "10.1016/j.compenvurbsys.2022.101760",
        "paper_author": "Brühwiler L.",
        "affiliation_name": "Universität Zürich",
        "affiliation_city": "Zurich",
        "affiliation_country": "Switzerland",
        "affiliation_id": "60012614",
        "affiliation_state": "ZH"
    },
    {
        "paper_title": "Learning Contraction Policies from Offline Data",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "3",
        "cover_date": "2022-04-01",
        "Abstract": "This letter proposes a data-driven method for learning convergent control policies from offline data using Contraction theory. Contraction theory enables constructing a policy that makes the closed-loop system trajectories inherently convergent towards a unique trajectory. At the technical level, identifying the contraction metric, which is the distance metric with respect to which a robot's trajectories exhibit contraction is often non-trivial. We propose to jointly learn the control policy and its corresponding contraction metric while enforcing contraction. To achieve this, we learn an implicit dynamics model of the robotic system from an offline data set consisting of the robot's state and input trajectories. We propose a data augmentation algorithm for learning contraction policies using this learned dynamics model. We randomly generate samples in the state space and propagate them forward in time through the learned dynamics model to generate auxiliary sample trajectories. We then learn both the control policy and the contraction metric such that the distance between the trajectories from the offline data set and our generated auxiliary sample trajectories decreases over time. We evaluate the performance of our proposed framework on simulated robotic goal-reaching tasks and demonstrate that enforcing contraction results in faster convergence and greater robustness of the learned policy.",
        "DOI": "10.1109/LRA.2022.3145100",
        "paper_author": "Rezazadeh N.",
        "affiliation_name": "Samueli School of Engineering",
        "affiliation_city": "Irvine",
        "affiliation_country": "United States",
        "affiliation_id": "60142622",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "What drives the performance of Chinese urban and rural secondary schools: A machine learning approach using PISA 2018",
        "publication": "Cities",
        "citied_by": "19",
        "cover_date": "2022-04-01",
        "Abstract": "Despite Chinese students' excellence in international academic assessment, a clear achievement gap exists between urban and rural areas. This study examines factors driving school-level academic performance disparity between urban and rural areas. Micro-level data from the 2018 Programme for International Student Assessment survey and a machine learning method are employed. Intermediate learning outcomes and student characteristics (e.g., class size, grade repetition rate, parents' socioeconomic status and emotional support, students' learning time, and credibility assessment skills) are essential in the difference in academic performance between the best and worst schools in urban areas. However, in rural areas, school characteristics such as complexity of admission policies, quality of learning environment, and number of computers are important. These findings highlight the need for tailored policies in both urban and rural areas.",
        "DOI": "10.1016/j.cities.2022.103609",
        "paper_author": "Lee H.",
        "affiliation_name": "Southwestern University of Finance and Economics",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China",
        "affiliation_id": "60018540",
        "affiliation_state": "Sichuan"
    },
    {
        "paper_title": "Reinforcement learning–based tool orientation optimization for five-axis machining",
        "publication": "International Journal of Advanced Manufacturing Technology",
        "citied_by": "8",
        "cover_date": "2022-04-01",
        "Abstract": "Tool orientation planning is an essential process in five-axis machining for sculptured parts with complex cavity features. Unlike traditional tool orientation optimization approaches using heuristic algorithms, which demand particularly prolonged time to achieve converged optimal result, this paper presents a novel method for tool orientation optimization based on reinforcement learning algorithm. As input to the method, the rasterized feasible region is first computed via support vector machine based on an active learning fashion that requires highly reduced sampled points. The tool orientation optimization is then converted into a reinforcement learning task, in which a soft actor-critic model is utilized and trained to obtain the optimal policy. According to preliminary testing results, the proposed method is proved to be feasible for tool orientation optimization problem, and effective to produce comparable results more efficiently compared with graph-based optimization method.",
        "DOI": "10.1007/s00170-022-08668-5",
        "paper_author": "Zhang Y.",
        "affiliation_name": "Nanjing University of Aeronautics and Astronautics",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60021666",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "Linear Policies are Sufficient to Realize Robust Bipedal Walking on Challenging Terrains",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "18",
        "cover_date": "2022-04-01",
        "Abstract": "In this work, we demonstrate robust walking in the bipedal robot Digit on uneven terrains by just learning a single linear policy. In particular, we propose a new control pipeline, wherein the high-level trajectory modulator shapes the end-foot ellipsoidal trajectories, and the low-level gait controller regulates the torso and ankle orientation. The foot-trajectory modulator uses a linear policy and the regulator uses a linear PD control law. As opposed to neural network based policies, the proposed linear policy has only 13 learnable parameters, thereby not only guaranteeing sample efficient learning but also enabling simplicity and interpretability of the policy. This is achieved with no loss of performance on challenging terrains like slopes, stairs and outdoor landscapes. We first demonstrate robust walking in the custom simulation environment, MuJoCo, and then directly transfer to hardware with no modification of the control pipeline. We subject the biped to a series of pushes and terrain height changes, both indoors and outdoors, thereby validating the presented work.",
        "DOI": "10.1109/LRA.2022.3143227",
        "paper_author": "Krishna L.",
        "affiliation_name": "Indian Institute of Technology (BHU) Varanasi",
        "affiliation_city": "Varanasi",
        "affiliation_country": "India",
        "affiliation_id": "60019106",
        "affiliation_state": "UP"
    },
    {
        "paper_title": "Deep Reinforcement Learning Based Latency Minimization for Mobile Edge Computing With Virtualization in Maritime UAV Communication Network",
        "publication": "IEEE Transactions on Vehicular Technology",
        "citied_by": "123",
        "cover_date": "2022-04-01",
        "Abstract": "The rapid development of maritime activities has led to the emergence of more and more computation-intensive applications. In order to meet the huge demand for wireless communications in maritime environment, mobile edge computing (MEC) is considered as an effective solution to provide powerful computing capabilities for maritime terminals of resource scarcity or latency sensitive. A basic technology to implement MEC is virtual machine (VM) multiplexing, through which multi-task parallel computing on a server is realized. In this paper, a two-layer unmanned aerial vehicles (UAVs) maritime communication network with a centralized top-UAV (T-UAV) and a group of distributed bottom-UAVs (B-UAVs) is established and MEC is used on T-UAV. We aim to solve the latency minimization problem for both communication and computation in this maritime UAV swarm mobile edge computing network. We reformulate this problem into a Markov decision process (MDP), since it is a non-convex and multiply constrained but has the characteristics of MDP. Based on this MDP model, we take deep reinforcement learning (DRL) as our tool to propose a deep Q-network (DQN) and a deep deterministic policy gradient (DDPG) algorithms to optimize the trajectory of T-UAV and configuration of virtual machines (VMs). Using these two proposed algorithms, we can minimize the system latency. Simulation results show that the given solutions are valid and effective.",
        "DOI": "10.1109/TVT.2022.3141799",
        "paper_author": "Liu Y.",
        "affiliation_name": "Jilin University",
        "affiliation_city": "Changchun",
        "affiliation_country": "China",
        "affiliation_id": "60007711",
        "affiliation_state": "Jilin"
    },
    {
        "paper_title": "A machine learning approach to find the determinants of Peruvian coca illegal crops",
        "publication": "Decision Science Letters",
        "citied_by": "1",
        "cover_date": "2022-04-01",
        "Abstract": "The current study analyzed the determinants of the Peruvian coca illegal plantations in the period 2003-2019. Hence, the DEVIDA database variables were gathered at first. Then, a machine learning-based technique is employed to select the most relevant variables for the study. That technique, Lasso, selected as accurate variables eradication of coca plantations and pasta base. Both OLS and VAR are employed to analyze the relevance of the selected variables. OLS finds that eradication was negatively related to the dependent variable. Nonetheless, pb confiscation had a positive relationship with illegal coca crops. Furthermore, VAR encounters that only pb confiscation affected the dependent variable. Supplementary tests are carried to ensure the accuracy of the results. In consequence, it is concluded that eradication policies by themselves were not enough to discourage the coca plantations. Farmers should get instruction about alternative crops and financial help. Furthermore, it has been claimed that pb confiscation generates scarcity of the drug, which elevates its price. Thus, coca farmers are more motivated to plant coca because of the higher prices. Therefore, as long as the international demand, which is disposed to pay high prices, the coca illegal crops and its illicit products will exist.",
        "DOI": "10.5267/j.dsl.2021.12.003",
        "paper_author": "Romero D.B.C.",
        "affiliation_name": "Universidad Continental",
        "affiliation_city": "Huancayo",
        "affiliation_country": "Peru",
        "affiliation_id": "60108878",
        "affiliation_state": "Junin"
    },
    {
        "paper_title": "A taxonomy of green governance: A qualitative and quantitative analysis towards sustainable development",
        "publication": "Sustainable Cities and Society",
        "citied_by": "52",
        "cover_date": "2022-04-01",
        "Abstract": "It is undeniable that our environment is constantly evolving and citizens are facing new issues and challenges related to the environment around the world. Green governance is essential to achieve the goals agreed upon by local and global governments. The concept of green governance makes it possible to understand the integration of the actors of each governance form during decision-making. In this article, we identify the research gap and propose a taxonomy of green governance for sustainable development. We used factor analysis to construct the taxonomy of green governance. We also proposed the critical influencing factors of green governance to build sustainable development. To evaluate the importance of green governance for reducing CO2 emissions and other energy-related consumption, this study conducted two case studies with empirical analysis on the OECD Indian dataset of green growth indicators. The Indian green growth indicators are predicted using a machine learning technique that employs linear digression, support vector machine (SVM), and Gaussian process. The analysis shows that the taxonomy of green governance—global governance, adaptive governance, climate governance, ecological governance, self-governance, energy governance, and information technology (IT) governance—are related to each other and can work on the same objective by pursuing different activities. In addition, the case study analysis shows that the SVM is the superior technique in terms of predicting the time series data in this study. Based on the analysis, this study suggest that green governance is vital for achieving global sustainable goals for future growth, and policy-makers should keep this in mind when making environmental policy decisions.",
        "DOI": "10.1016/j.scs.2022.103693",
        "paper_author": "Debbarma J.",
        "affiliation_name": "Inha University",
        "affiliation_city": "Incheon",
        "affiliation_country": "South Korea",
        "affiliation_id": "60028876",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Determining customer limits by data mining methods in credit allocation process",
        "publication": "International Journal of Electrical and Computer Engineering",
        "citied_by": "2",
        "cover_date": "2022-04-01",
        "Abstract": "The demand for credit is increasing constantly. Banks are looking for various methods of credit evaluation that provide the most accurate results in a shorter period in order to minimize their rising risks. This study focuses on various methods that enable the banks to increase their asset quality without market loss regarding the credit allocation process. These methods enable the automatic evaluation of loan applications in line with the sector practices, and enable determination of credit policies/strategies based on actual needs. Within the scope of this study, the relationship between the predetermined attributes and the credit limit outputs are analyzed by using a sample data set of consumer loans. Random forest (RF), sequential minimal optimization (SMO), PART, decision table (DT), J48, multilayer perceptron (MP), JRip, naïve Bayes (NB), one rule (OneR) and zero rule (ZeroR) algorithms were used in this process. As a result of this analysis, SMO, PART and random forest algorithms are the top three approaches for determining customer credit limits.",
        "DOI": "10.11591/ijece.v12i2.pp1910-1915",
        "paper_author": "Ayhan T.",
        "affiliation_name": "Bahçeşehir Üniversitesi",
        "affiliation_city": "Istanbul",
        "affiliation_country": "Turkey",
        "affiliation_id": "60021379",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Critical Period for Robust Curriculum-Based Deep Reinforcement Learning of Sequential Action in a Robot Arm",
        "publication": "Topics in Cognitive Science",
        "citied_by": "5",
        "cover_date": "2022-04-01",
        "Abstract": "Many everyday activities are sequential in nature. That is, they can be seen as a sequence of subactions and sometimes subgoals. In the motor execution of sequential action, context effects are observed in which later subactions modulate the execution of earlier subactions (e.g., reaching for an overturned mug, people will optimize their grasp to achieve a comfortable end state). A trajectory (movement) adaptation of an often-used paradigm in the study of sequential action, the serial response time task, showed several context effects of which centering behavior is of special interest. Centering behavior refers to the tendency (or strategy) of subjects to move their arm or mouse cursor to a position equidistant to all stimuli in the absence of predictive information, thereby reducing movement time to all possible targets. In the current study, we investigated sequential action in a virtual robotic agent trained using proximal policy optimization, a state-of-the-art deep reinforcement learning algorithm. The agent was trained to reach for appearing targets, similar to a serial response time task given to humans. We found that agents were more likely to develop centering behavior similar to human subjects after curricularized learning. In our curriculum, we first rewarded agents for reaching targets before introducing a penalty for energy expenditure. When the penalty was applied with no curriculum, many agents failed to learn the task due to a lack of action space exploration, resulting in high variability of agents' performance. Our findings suggest that in virtual agents, similar to infants, early energetic exploration can promote robust later learning. This may have the same effect as infants' curiosity-based learning by which they shape their own curriculum. However, introducing new goals cannot wait too long, as there may be critical periods in development after which agents (as humans) cannot flexibly learn to incorporate new objectives. These lessons are making their way into machine learning and offer exciting new avenues for studying both human and machine learning of sequential action.",
        "DOI": "10.1111/tops.12595",
        "paper_author": "de Kleijn R.",
        "affiliation_name": "Universiteit Leiden",
        "affiliation_city": "Leiden",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60019816",
        "affiliation_state": "Zuid-Holland"
    },
    {
        "paper_title": "Deep learning-based national scale soil organic carbon mapping with Sentinel-3 data",
        "publication": "Geoderma",
        "citied_by": "46",
        "cover_date": "2022-04-01",
        "Abstract": "Mapping of soil organic carbon (SOC) at the regional level is critical for climate change policy and the mitigation of its adverse effects. However, reliable SOC estimates particularly over a large extent remains a major challenge due to among others limited sample points, quality of simulation data and the algorithm adopted. Remote sensing (RS) strategies have emerged as a suitable alternative to field and laboratory SOC determination, especially at large spatial extent. The use of Sentinel-3 sensor, the latest of the Sentinel series is minimal and has not been fully developed, despite its impressive attributes that include high spectral-temporal resolution and large coverage. Compared to linear and classical ML models, deep learning (DL) models offer a considerable improvement in data analysis due to their ability to extract more representative features and identify complex spatial patterns associated with big data. Yet, there is paucity in literature on the application of DL-based remote sensing strategies for SOC prediction. Consequently, this study adopted a deep neural network (DNN) to predict SOC at a national scale, using Sentinel-3 image, and compared the results with random forest (RF), support vector machine (SVM) and artificial neural network (ANN) models. The models were trained based on 10-fold cross-validation with 1936 soil samples and 31 predictors. The DNN model generated the best result with a root mean square error (RMSE) of 10.35 t/ha (26 % of the mean), followed by RF (RMSE = 11.2 t/ha), ANN (RMSE = 11.6 t/ha) and SVM (RMSE = 13.6 t/ha). The analytical prowess of the DNN, together with its ability to handle big data by learning patterns through a series of hidden layers (10) to draw conclusions, gives it an edge over other classical ML models. The study concluded that the DNN model with Sentinel-3 data is promising and provides an effective framework for continuous national level SOC modelling.",
        "DOI": "10.1016/j.geoderma.2022.115695",
        "paper_author": "Odebiri O.",
        "affiliation_name": "University of KwaZulu-Natal",
        "affiliation_city": "Durban",
        "affiliation_country": "South Africa",
        "affiliation_id": "60010499",
        "affiliation_state": "KwaZulu-Natal"
    },
    {
        "paper_title": "ReCom: A deep reinforcement learning approach for semi-supervised tabular data labeling",
        "publication": "Information Sciences",
        "citied_by": "8",
        "cover_date": "2022-04-01",
        "Abstract": "One of the main obstacles in applying machine learning to a new domain is the limited availability of labeled data. A common approach for overcoming this challenge is using semi-supervised learning, where labeled and unlabeled data are used together to label additional samples. One of the most common automatic labeling approaches is co-training, which trains two learners on different views of the data, and then proceeds to collaboratively and iteratively label additional samples. Despite their effectiveness in multiple domains, existing co-training approaches for tabular data are either heuristic, and therefore error-prone, or use a greedy approach that leads to sub-optimal performance. We present ReCom, a deep reinforcement learning-based co-training approach. Our approach models multiple aspects of both the dataset and the two learners and develops advanced labeling strategies that achieve state-of-the-art performance. ReCom overcomes the challenge of limited data availability by simultaneously training on multiple datasets, thus producing a generic and robust labeling policy that can be applied to new datasets without the need for any additional training. Our experiments, conducted on a diverse group of 32 datasets, demonstrate the merits of our approach.",
        "DOI": "10.1016/j.ins.2021.12.076",
        "paper_author": "Zaks G.",
        "affiliation_name": "Ben-Gurion University of the Negev",
        "affiliation_city": "Beer-Sheva",
        "affiliation_country": "Israel",
        "affiliation_id": "60027161",
        "affiliation_state": "Southern District"
    },
    {
        "paper_title": "Application of risky driving behavior in crash detection and analysis",
        "publication": "Physica A: Statistical Mechanics and its Applications",
        "citied_by": "22",
        "cover_date": "2022-04-01",
        "Abstract": "Traffic crash detection is a promising and challenging research topic. Due to the limitations of data collection, previous studies mainly used traffic flow variables to establish a traffic crash detection model, and the contribution of risky driving behavior to the traffic crash detection model was not clear. The widespread application of traffic detectors and in-vehicle AutoNavigator software make it possible to collect and update real-time traffic flow data and risky driving behavior data in a short period of time. These data lay the foundation for this study, which aims to quantify the improvement degree of risky driving behavior in a traffic crash detection model and then analyze the coupling effect of risky driving behavior and traffic operation state on the impact of traffic crashes. In this research, we investigated real-time and dynamic traffic flow data and risky driving behavior data by using eXtreme Gradient Boosting (XGBoost) and the logistic regression algorithm, respectively. In addition, SHapley Additive exPlanation (SHAP) was employed to analyze the results and the importance of individual features. The results indicate that the model with the combined inputs has increased accuracy of 8% and nearly a 5% reduction in the false alarm rate. The results of feature importance analysis show that in the variables of risky driving behavior and traffic flow, the most important feature influencing traffic crashes is sharp deceleration. In addition, the characteristics of risky driving behavior increase or decrease the probability of traffic crashes caused by traffic flow characteristics. The results of this paper will help with real-time crash detection and relevant policy-making.",
        "DOI": "10.1016/j.physa.2021.126808",
        "paper_author": "Guo M.",
        "affiliation_name": "Beijing University of Technology",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60022281",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Electrical consumption forecasting: a framework for high frequency data",
        "publication": "Neural Computing and Applications",
        "citied_by": "7",
        "cover_date": "2022-04-01",
        "Abstract": "Knowing the demand for electrical consumption beforehand is important for efficient energy programming policies that can help with climate change, life cycle-costs, and optimal primary resource extraction. In this paper, we propose a framework to improve forecasting performance of high frequency electrical consumption data. We use different models for each day of the week, and then compose them to obtain the total forecast. We apply both machine learning (Long-Short Term Memory network) and econometric models (AutoRegressive Integrated Moving Average and Holtz-Winters) that consider time dependence in the data comparing model performance. We find that a classical ARIMA model outperforms other models; however, in applying the proposed framework, LSTM manages to outperform all other models. The results are statistically significant as indicated by the Model Confidence Set test constructed for Mean Absolute Percentage Error and Mean Square Error.",
        "DOI": "10.1007/s00521-021-06735-8",
        "paper_author": "Michell K.",
        "affiliation_name": "Universidad Técnica Federico Santa María",
        "affiliation_city": "Valparaiso",
        "affiliation_country": "Chile",
        "affiliation_id": "60007087",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Spatial modeling of zoonotic cutaneous leishmaniasis with regard to potential environmental factors using ANFIS and PCA-ANFIS methods",
        "publication": "Acta Tropica",
        "citied_by": "7",
        "cover_date": "2022-04-01",
        "Abstract": "This study compares two adaptive neuro-fuzzy inference system (ANFIS) and principal component analysis (PCA)-ANFIS techniques for spatial modeling and forecasting of zoonotic cutaneous leishmaniasis (ZCL) cases in rural districts of Golestan province, Iran. We collected and prepared data on ZCL cases and climatic, topographic, vegetation, and human population factors. By applying the PCA algorithm, the parameters affecting the ZCL incidence were decomposed into principal components (PCs), and their dimensions were reduced. Then, PCs were used to train the ANFIS model. To evaluate the proposed approaches in model assessment phase, we used test data in 2016. In this phase, we showed that PCA-ANFIS model with values ​​of R2 = 0.791, MAE = 0.681, RMSE = 0.904 compared to ANFIS model with values ​​of R2 = 0.705, MAE = 0.827, RMSE = 1.073 has better performance in prediction of the ZCL cases. Actual and predicted maps of ZCL cases in 2016 by both models demonstrated that the high-risk regions of the disease are located in the northeastern, northern parts, and some central rural districts of Golestan province. Sensitivity analysis of the ANFIS model showed that population, vegetation, average wind speed, elevation, and average soil temperature, respectively, are the most significant factors in predicting the ZCL cases. The findings indicated the importance of machine learning (ML) techniques (ANFIS and PCA-ANFIS) in medical geography studies. By using these approaches, with less cost and shorter time, high-risk areas of diseases can be predicted, and the most effective factors on the spatial prediction of diseases can be identified. Public health policymakers can use these useful tools to control and prevent the disease and to allocate resources to disease-prone areas.",
        "DOI": "10.1016/j.actatropica.2021.106296",
        "paper_author": "Babaie E.",
        "affiliation_name": "K. N. Toosi University of Technology",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran",
        "affiliation_id": "60016248",
        "affiliation_state": "Tehran"
    },
    {
        "paper_title": "Incorporating FAT and privacy aware AI modeling approaches into business decision making frameworks",
        "publication": "Decision Support Systems",
        "citied_by": "32",
        "cover_date": "2022-04-01",
        "Abstract": "We present a formal approach to build and evaluate AI systems that include principles of Fairness, Accountability and Transparency (FAT), which are extremely important in various domains where AI models are used, yet their utilization in business settings is scant. We develop and instantiate a FAT-based framework with a privacy-constrained dataset and build a model to demonstrate the balance among these 3 dimensions. These principles are gaining prominence with higher awareness of privacy and fairness in business and society. Our results indicate that FAT can co-exist in a well-designed system. Our contribution lies in presenting and evaluating a functional, FAT-based machine learning model in an affinity prediction scenario. Contrary to common belief, we show that explainable AI/ML systems need not have a major negative impact on predictive performance. Our approach is applicable in a variety of fields such as insurance, health diagnostics, government funds allocation and other business settings. Our work has broad policy implications as well, by making AI and AI-based decisions more ethical, less controversial, and hence, trustworthy. Our work contributes to emerging AI policy perspectives worldwide.",
        "DOI": "10.1016/j.dss.2021.113715",
        "paper_author": "Zhdanov D.",
        "affiliation_name": "J. Mack Robinson College of Business",
        "affiliation_city": "Atlanta",
        "affiliation_country": "United States",
        "affiliation_id": "60098091",
        "affiliation_state": "GA"
    },
    {
        "paper_title": "Learning Free Gait Transition for Quadruped Robots Via Phase-Guided Controller",
        "publication": "IEEE Robotics and Automation Letters",
        "citied_by": "47",
        "cover_date": "2022-04-01",
        "Abstract": "Gaits and transitions are key components in legged locomotion. For legged robots, describing and reproducing gaits as well as transitions remain longstanding challenges. Reinforcement learning has become a powerful tool to formulate controllers for legged robots. Learning multiple gaits and transitions, nevertheless, is related to the multi-task learning problems. In this work, we present a novel framework for training a simple control policy for a quadruped robot to locomote in various gaits. Four independent phases are used as the interface between the gait generator and the control policy, which characterizes the movement of four feet. Guided by the phases, the quadruped robot is able to locomote according to the generated gaits, such as walk, trot, pacing and bounding, and to make transitions among those gaits. More general phases can be used to generate complex gaits, such as mixed rhythmic dancing. With the control policy, the Black Panther robot, a medium-dog-sized quadruped robot, can perform all learned motor skills while following the velocity commands smoothly and robustly in natural environment.",
        "DOI": "10.1109/LRA.2021.3136645",
        "paper_author": "Shao Y.",
        "affiliation_name": "Zhejiang University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60003970",
        "affiliation_state": "Zhejiang"
    },
    {
        "paper_title": "A framework for privacy-preservation of IoT healthcare data using Federated Learning and blockchain technology",
        "publication": "Future Generation Computer Systems",
        "citied_by": "231",
        "cover_date": "2022-04-01",
        "Abstract": "With the dramatically increasing deployment of IoT (Internet-of-Things) and communication, data has always been a major priority to achieve intelligent healthcare in a smart city. For the modern environment, valuable assets are user IoT data. The privacy policy is even the biggest necessity to secure user's data in a deep-rooted fundamental infrastructure of network and advanced applications, including smart healthcare. Federated learning acts as a special machine learning technique for privacy-preserving and offers to contextualize data in a smart city. This article proposes Blockchain and Federated Learning-enabled Secure Architecture for Privacy-Preserving in Smart Healthcare, where Blockchain-based IoT cloud platforms are used for security and privacy. Federated Learning technology is adopted for scalable machine learning applications like healthcare. Furthermore, users can obtain a well-trained machine learning model without sending personal data to the cloud. Moreover, it also discussed the applications of federated learning for a distributed secure environment in a smart city.",
        "DOI": "10.1016/j.future.2021.11.028",
        "paper_author": "Singh S.",
        "affiliation_name": "Dongguk University, Seoul",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea",
        "affiliation_id": "60009387",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "The new paradigm of economic complexity",
        "publication": "Research Policy",
        "citied_by": "119",
        "cover_date": "2022-04-01",
        "Abstract": "Economic complexity offers a potentially powerful paradigm to understand key societal issues and challenges of our time. The underlying idea is that growth, development, technological change, income inequality, spatial disparities, and resilience are the visible outcomes of hidden systemic interactions. The study of economic complexity seeks to understand the structure of these interactions and how they shape various socioeconomic processes. This emerging field relies heavily on big data and machine learning techniques. This brief introduction to economic complexity has three aims. The first is to summarize key theoretical foundations and principles of economic complexity. The second is to briefly review the tools and metrics developed in the economic complexity literature that exploit information encoded in the structure of the economy to find new empirical patterns. The final aim is to highlight the insights from economic complexity to improve prediction and political decision-making. Institutions including the World Bank, the European Commission, the World Economic Forum, the OECD, and a range of national and regional organizations have begun to embrace the principles of economic complexity and its analytical framework. We discuss policy implications of this field, in particular the usefulness of building recommendation systems for major public investment decisions in a complex world.",
        "DOI": "10.1016/j.respol.2021.104450",
        "paper_author": "Balland P.A.",
        "affiliation_name": "Universiteit Utrecht",
        "affiliation_city": "Utrecht",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60007989",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Similarity Caching: Theory and Algorithms",
        "publication": "IEEE/ACM Transactions on Networking",
        "citied_by": "10",
        "cover_date": "2022-04-01",
        "Abstract": "This paper focuses on similarity caching systems, in which a user request for an object o that is not in the cache can be (partially) satisfied by a similar stored object o', at the cost of a loss of user utility. Similarity caching systems can be effectively employed in several application areas, like multimedia retrieval, recommender systems, genome study, and machine learning training/serving. However, despite their relevance, the behavior of such systems is far from being well understood. In this paper, we provide a first comprehensive analysis of similarity caching in the offline, adversarial, and stochastic settings. We show that similarity caching raises significant new challenges, for which we propose the first dynamic policies with some optimality guarantees. We evaluate the performance of our schemes under both synthetic and real request traces.",
        "DOI": "10.1109/TNET.2021.3126368",
        "paper_author": "Neglia G.",
        "affiliation_name": "Centre Inria d'Université Côte d'Azur",
        "affiliation_city": "Sophia Antipolis",
        "affiliation_country": "France",
        "affiliation_id": "60032385",
        "affiliation_state": "Provence-Alpes-Cote d'Azur"
    },
    {
        "paper_title": "A comparison of neural and non-neural machine learning models for food safety risk prediction with European Union RASFF data",
        "publication": "Food Control",
        "citied_by": "15",
        "cover_date": "2022-04-01",
        "Abstract": "European Union launched the RASFF portal in 1977 to ensure cross-border monitoring and a quick reaction when public health risks are detected in the food chain. There are not enough resources available to guarantee a comprehensive inspection policy, but RASFF data has enormous potential as a preventive tool. However, there are few studies of food and feed risk issues prediction and none with RASFF data. Although deep learning models are good prediction systems, it must be confirmed whether in this field they behave better than other machine learning techniques. The importance of categorical variables encoding as input for numerical models should be specially studied. Results in this paper show that deep learning with entity embedding is the best combination, with accuracies of 86.81%, 82.31%, and 88.94% in each of the three stages of the simplified RASFF process in which the tests were carried out. However, the random forest models with one hot encoding offer only slightly worse results, so it seems that in the quality of the results the coding has more weight than the prediction technique. Our work also demonstrates that the use of probabilistic predictions (an advantage of neural models) can also be used to optimize the number of inspections that can be carried out.",
        "DOI": "10.1016/j.foodcont.2021.108697",
        "paper_author": "Nogales A.",
        "affiliation_name": "Universidad Francisco de Vitoria",
        "affiliation_city": "Pozuelo de Alarcon",
        "affiliation_country": "Spain",
        "affiliation_id": "60108692",
        "affiliation_state": "Madrid"
    },
    {
        "paper_title": "Application of Machine-Learning in Network-Level Road Maintenance Policy-Making: The Case of Iran",
        "publication": "Expert Systems with Applications",
        "citied_by": "11",
        "cover_date": "2022-04-01",
        "Abstract": "A framework was proposed to find optimal maintenance policies in a road network. The framework included: identifying factors that contribute to policy-making; clustering the network based on these factors; identifying criteria that impact optimal policies; and determining optimal policies and periods using these criteria. To test the framework applicability, it was applied to Iran roads network step-by-step. First, road functional class and climate were identified as factors that contribute to policy-making. Second, the network was clustered into six sub-networks based on two road functional classes and three climates. Third; maintenance cost, network condition, and maintenance period were identified as criteria that impact optimal policies. Fourth, 96 maintenance policies were applied to each sub-network considering two-year, four-year, six-year, and twelve-year maintenance periods. To quantify policies cost, seven machine-learning algorithms including Gradient Boosting Regression, Lasso, Ridge, Random Forest Regression, Elastic Net, Neural Network, and Multiple Linear Regression were tested. Using the coefficient of determination (R2) as the accuracy metric, it was found that in all sub-networks the Gradient Boosting Regression had the highest accuracy on testing set (greater than 90%) while that of other algorithms was between 50% and 90%. Sub-networks condition was modeled using the Markov Chain model and was measured by the average Pavement Condition Index (PCI). Having policies cost and sub-networks PCI, the optimal policy was selected using the Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS). It was concluded that in all sub-networks, the four-year maintenance period was optimal. Roads in warm zone demanded the most intense policies, followed by those in cold and humid zones. The same applied to arterial roads followed by local ones.",
        "DOI": "10.1016/j.eswa.2021.116283",
        "paper_author": "Mahpour A.",
        "affiliation_name": "University of Toronto",
        "affiliation_city": "Toronto",
        "affiliation_country": "Canada",
        "affiliation_id": "60016849",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "Systematic review of structural and functional neuroimaging studies of cannabis use in adolescence and emerging adulthood: evidence from 90 studies and 9441 participants",
        "publication": "Neuropsychopharmacology",
        "citied_by": "30",
        "cover_date": "2022-04-01",
        "Abstract": "Cannabis use peaks in adolescence, and adolescents may be more vulnerable to the neural effects of cannabis and cannabis-related harms due to ongoing brain development during this period. In light of ongoing cannabis policy changes, increased availability, reduced perceptions of harm, heightened interest in medicinal applications of cannabis, and drastic increases in cannabis potency, it is essential to establish an understanding of cannabis effects on the developing adolescent brain. This systematic review aims to: (1) synthesize extant literature on functional and structural neural alterations associated with cannabis use during adolescence and emerging adulthood; (2) identify gaps in the literature that critically impede our ability to accurately assess the effect of cannabis on adolescent brain function and development; and (3) provide recommendations for future research to bridge these gaps and elucidate the mechanisms underlying cannabis-related harms in adolescence and emerging adulthood, with the long-term goal of facilitating the development of improved prevention, early intervention, and treatment approaches targeting adolescent cannabis users (CU). Based on a systematic search of Medline and PsycInfo and other non-systematic sources, we identified 90 studies including 9441 adolescents and emerging adults (n = 3924 CU, n = 5517 non-CU), which provide preliminary evidence for functional and structural alterations in frontoparietal, frontolimbic, frontostriatal, and cerebellar regions among adolescent cannabis users. Larger, more rigorous studies are essential to reconcile divergent results, assess potential moderators of cannabis effects on the developing brain, disentangle risk factors for use from consequences of exposure, and elucidate the extent to which cannabis effects are reversible with abstinence. Guidelines for conducting this work are provided.",
        "DOI": "10.1038/s41386-021-01226-9",
        "paper_author": "Lichenstein S.D.",
        "affiliation_name": "Yale School of Medicine",
        "affiliation_city": "New Haven",
        "affiliation_country": "United States",
        "affiliation_id": "60017994",
        "affiliation_state": "CT"
    },
    {
        "paper_title": "Identifying artificial intelligence (AI) invention: a novel AI patent dataset",
        "publication": "Journal of Technology Transfer",
        "citied_by": "37",
        "cover_date": "2022-04-01",
        "Abstract": "Artificial intelligence (AI) is an area of increasing scholarly and policy interest. To help researchers, policymakers, and the public, this paper describes a novel dataset identifying AI in over 13.2 million patents and pre-grant publications (PGPubs). The dataset, called the Artificial Intelligence Patent Dataset (AIPD), was constructed using machine learning models for each of eight AI component technologies covering areas such as natural language processing, AI hardware, and machine learning. The AIPD contains two data files, one identifying the patents and PGPubs predicted to contain AI and a second file containing the patent documents used to train the machine learning classification models. We also present several evaluation metrics based on manual review by patent examiners with focused expertise in AI, and show that our machine learning approach achieves state-of-the-art performance across existing alternatives in the literature. We believe releasing this dataset will strengthen policy formulation, encourage additional empirical work, and provide researchers with a common base for building empirical knowledge on the determinants and impacts of AI invention.",
        "DOI": "10.1007/s10961-021-09900-2",
        "paper_author": "Giczy A.V.",
        "affiliation_name": "United States Patent and Trademark Office",
        "affiliation_city": "Alexandria",
        "affiliation_country": "United States",
        "affiliation_id": "60017449",
        "affiliation_state": "VA"
    },
    {
        "paper_title": "Intelligent transmission control layer for efficient node management in SDN",
        "publication": "Cluster Computing",
        "citied_by": "1",
        "cover_date": "2022-04-01",
        "Abstract": "The Software Defined Networking (SDN) promises exciting new networking functionality. However, there is always remains a chance of programming errors that result in unreliable data communication. The centralized programming model helps decrease bugs' probability where a single controller manages the whole network. Yet, many real-time events occur at switches and end hosts, which often affect and add a delay in the communication process. One of those events includes unannounced destination host migration after installing flow rules during receiving of data packets. Such destination host movement results in the loss of packets because controller is not aware of this recent event. Therefore, we need an efficient approach to transmit packets without any packet loss despite destination host migration. This paper proposed a design to achieve the objective as mentioned above by defining a layer named Intelligent Transmission Control Layer (ITCL). It monitors all the end hosts' connections at their specific locations and performs necessary actions whenever the connection state changes for one or multiple hosts. The controller collects information of end nodes and state change through ITCL using A star search algorithm. After that, it updates flow tables accordingly to accommodate a location-change scenario with a route-change policy. ICTL is developed on prototype-based implementation using a popular POX controller platform. By comparing ITCL with the existing solution, we conclude that our proposed approach exhibits efficient performance in terms of Packet loss, Bandwidth usage, and Network Throughput.",
        "DOI": "10.1007/s10586-021-03449-3",
        "paper_author": "Aldabbas H.",
        "affiliation_name": "Al-Balqa Applied University",
        "affiliation_city": "Salt",
        "affiliation_country": "Jordan",
        "affiliation_id": "60062395",
        "affiliation_state": "Al-Balqa"
    },
    {
        "paper_title": "Machine learning enables national assessment of wind plant controls with implications for land use",
        "publication": "Wind Energy",
        "citied_by": "9",
        "cover_date": "2022-04-01",
        "Abstract": "As deployment of wind energy continues to expand, computationally efficient tools for predicting wind plant performance over a wide range of layout designs, technology innovations, and spatial locations are increasingly important for policy and investment decisions. We demonstrate two approaches to training a surrogate model to predict annual energy production (AEP) of parameterized wind plant layouts: one using a Gaussian process (GP) and the other using a fully convolutional neural network (FCNN). We leverage the powerful FCNN architecture by encoding wind plant design parameters and output response surface as an image. The FCNN produces more accurate results than the GP with mean absolute errors equivalent to 1% and 1.9% of plant rated power, respectively, although the GP performs well under limited training data and provides useful uncertainty information. We also evaluate a surrogate model for wake steering, enabling a nationwide assessment of the impact of plant control strategies and plant layout decisions. Across two million locations, we find that wake steering strategies boost AEP with relative gains upwards of 3%. Gains are most pronounced at sites without a dominant wind direction and where layout optimization is less fruitful. Additionally, we perform a nationwide sensitivity analysis showing that wake steering can mitigate wake losses from higher density plant layouts. Our results suggest that regions which have not been previously viable for wind deployment due to moderate wind resources are especially well enhanced by wake steering strategies that could help overcome land constraints and inflexible layout options, potentially identifying new deployment opportunities.",
        "DOI": "10.1002/we.2689",
        "paper_author": "Harrison-Atlas D.",
        "affiliation_name": "National Renewable Energy Laboratory",
        "affiliation_city": "Golden",
        "affiliation_country": "United States",
        "affiliation_id": "60030451",
        "affiliation_state": "CO"
    },
    {
        "paper_title": "The moon, the ghetto and artificial intelligence: Reducing systemic racism in computational algorithms",
        "publication": "Government Information Quarterly",
        "citied_by": "45",
        "cover_date": "2022-04-01",
        "Abstract": "Computational algorithms and automated decision making systems that include them offer potential to improve public policy and organizations. But computational algorithms based on biased data encode those biases into algorithms, models and their outputs. Systemic racism is institutionalized bias with respect to race, ethnicity and related attributes. Such bias is located in data that encode the results and outputs of decisions that have been discriminatory, in procedures and processes that may intentionally or unintentionally disadvantage people based on race, and in policies that may discriminate by race. Computational algorithms may exacerbate systemic racism if they are not designed, developed, and used–that is, enacted–with attention to identifying and remedying bias specific to race. Advancing social equity in digital governance requires systematic, ongoing efforts to assure that automated decision making systems, and their enactment in complex public organizational arrangements, are free from bias.",
        "DOI": "10.1016/j.giq.2021.101645",
        "paper_author": "Fountain J.E.",
        "affiliation_name": "University of Massachusetts Amherst",
        "affiliation_city": "Amherst",
        "affiliation_country": "United States",
        "affiliation_id": "60014313",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Investigating the Effects of Robot Engagement Communication on Learning from Demonstration",
        "publication": "International Journal of Social Robotics",
        "citied_by": "2",
        "cover_date": "2022-04-01",
        "Abstract": "Robot learning from demonstration (RLfD) is a technique for robots to derive policies from instructors’ examples. Although the reciprocal effects of student engagement on teacher behavior are widely recognized in the educational community, it is unclear whether the same phenomenon holds for RLfD. To fill this gap, we first design three types of robot engagement behavior (gaze, imitation, and a hybrid of the two) based on the learning literature. We then conduct, in a simulation environment, a within-subject user study to investigate the impact of different robot engagement cues on humans compared to a “without-engagement” condition. Results suggest that engagement communication has significantly negative influences on the human’s estimation of the simulated robots’ capability and significantly raises their expectation towards the learning outcomes, even though we do not run actual imitation learning algorithms in the experiments. Moreover, imitation behavior affects humans more than gaze does in all metrics, while their combination has the most profound influences on humans. We also find that communicating engagement via imitation or the combined behavior significantly improves humans’ perception towards the quality of simulated demonstrations, even if all demonstrations are of the same quality.",
        "DOI": "10.1007/s12369-021-00825-2",
        "paper_author": "Sun M.",
        "affiliation_name": "Hong Kong University of Science and Technology",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong",
        "affiliation_id": "60008592",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Retrospective identification of latent subgroups of emergency department patients: A machine learning approach",
        "publication": "EMA - Emergency Medicine Australasia",
        "citied_by": "2",
        "cover_date": "2022-04-01",
        "Abstract": "Objective: This research aims to (i) identify latent subgroups of ED presentations in Australian public EDs using a data-driven approach and (ii) compare clinical, socio-demographic and time-related characteristics of ED presentations broadly using the subgroups. Methods: We examined presentations to four public hospital EDs in Queensland from 2009 to 2014. An unsupervised machine learning algorithm, Clustering Large Applications, was used to cluster ED presentations. Results: There were six subgroups common across the EDs, primarily distinguishable by age, and subsequently by triage category, ED length of stay, arrival mode, departure status and several time-related attributes. Around 10% to 30% of the total presentations had high resource utilisation, with half of these from older patients (55+ years). ED resource utilisation per population was highest among the oldest cohort (75+ years). Children and young adults more frequently presented to the ED outside general-practitioner hours, mostly on Sundays. Older persons were more likely to present at any time, rather than specific hours, days or seasons. ED service performance measured against commonly used access-target indicators were rarely satisfied for older people and frequently satisfied for children. Conclusion: Clustering Large Applications is effective in finding latent groups in large-scale mixed-type data, as demonstrated in the present study. Six types of ED presentations were identified and described using clinically relevant characteristics. The present study provides evidence for policy makers in Australia to develop alternative ED models of care tailored around the care needs of the differing groups of patients and thereby supports the sustainable delivery of acute healthcare.",
        "DOI": "10.1111/1742-6723.13875",
        "paper_author": "Duwalage K.I.",
        "affiliation_name": "Queensland University of Technology",
        "affiliation_city": "Brisbane",
        "affiliation_country": "Australia",
        "affiliation_id": "60011019",
        "affiliation_state": "QLD"
    },
    {
        "paper_title": "Deep-Reinforcement-Learning-Based Predictive Maintenance Model for Effective Resource Management in Industrial IoT",
        "publication": "IEEE Internet of Things Journal",
        "citied_by": "47",
        "cover_date": "2022-04-01",
        "Abstract": "Unplanned breakdown of critical equipment interrupts production throughput in Industrial IoT (IIoT), and data-driven predictive maintenance (PdM) becomes increasingly important for companies seeking a competitive business advantage. Manufacturers, however, are constantly faced with the onerous challenge of manually allocating suitably competent manpower resources in the event of an unexpected machine breakdown. Furthermore, human error has a negative rippling impact on both overall equipment downtime and production schedules. In this article, we formulate the complex resource management problem as a resource optimization problem to determine if a model-free deep reinforcement learning (DRL)-based PdM framework can be used to automatically learn an optimal decision policy from a stochastic environment. Unlike the existing PdM frameworks, our approach considers PdM sensor information and the resources of both physical equipment and human as part of the optimization problem. The proposed DRL-based framework and proximal policy optimization long short term memory (PPO-LSTM) model are evaluated alongside baselines results from human participants using a maintenance repair simulator. Empirical results indicate that our PPO-LSTM efficiently learns the optimal decision-policy for the resource management problem, outperforming comparable DRL methods and human participants by 53% and 65%, respectively. Overall, the simulation results corroborate the proposed DRL-based PdM framework's superiority in terms of convergence efficiency, simulation performance, and flexibility.",
        "DOI": "10.1109/JIOT.2021.3109955",
        "paper_author": "Ong K.S.H.",
        "affiliation_name": "School of Computer Science and Engineering",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore",
        "affiliation_id": "60078616",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Quality Analysis of YouTube Videos Presenting Shoulder Exercises after Breast Cancer Surgery",
        "publication": "Breast Care",
        "citied_by": "4",
        "cover_date": "2022-04-01",
        "Abstract": "Background: The prolonged immobilization suggested after breast cancer (BC) surgery causes morbidity. Patients search the Internet, especially social networks, for recommended exercises. Objective: The aim of this observational study was to assess the quality of YouTube videos, accessible for any patient, about exercises after BC surgery. Methods: A systematic search was performed on YouTube. One hundred and fifty videos were selected and analyzed. Two statistical analyses were conducted based on machine-learning techniques. Videos were classified as \"Relevant\"and \"Non-Relevant\"using principal component analysis models. Popularity was evaluated by Video Power Index (VPI), informational quality and accuracy were measured using the DISCERN Scale and Global Quality Scale (GQS). Scoring criteria for exercises were established according to the exercises recommended by the Oncology Section of the American Physical Therapy Association (APTA). Interobserver agreement and individual correlations were statistically examined. Results: DISCERN scored a mean of 50.97 (standard deviation [SD] 19.19). HONcode scored 78.30 (11.02) and GQS scored 3.49 (0.74). Average number of views was 53,963 (SD 67,376), mean duration was 9:42 min (9:15), mean days online was 2,158 (922), mean view ratio was 27.14 (30.24), mean likes was 245 (320.5), mean dislikes was 13.4 (14.2), and mean VPI was 93.48 (5.42). Conclusion: The quality of YouTube videos of recommended exercises post-BC surgery is high and can be a translational activity to improve patients' behavior. Health institutions and NGOs, with higher popularity levels than academic institutions, should consider this information when implementing new policies focused on video quality which can contribute to adaptive behavior in patients.",
        "DOI": "10.1159/000518265",
        "paper_author": "Rodriguez Rodriguez A.M.",
        "affiliation_name": "Universidad de Oviedo",
        "affiliation_city": "Oviedo",
        "affiliation_country": "Spain",
        "affiliation_id": "60006793",
        "affiliation_state": "Asturias"
    },
    {
        "paper_title": "A reinforcement learning approach to adaptive remediation in online training",
        "publication": "Journal of Defense Modeling and Simulation",
        "citied_by": "5",
        "cover_date": "2022-04-01",
        "Abstract": "Advances in artificial intelligence (AI) and machine learning can be leveraged to tailor training based on the goals, learning needs, and preferences of learners. A key component of adaptive training systems is tutorial planning, which controls how scaffolding is structured and delivered to learners to create dynamically personalized learning experiences. The goal of this study was to induce data-driven policies for tutorial planning using reinforcement learning (RL) to provide adaptive scaffolding based on the Interactive, Constructive, Active, Passive framework for cognitive engagement. We describe a dataset that was collected to induce RL-based scaffolding policies, and we present the results of our policy analyses. Results showed that the best performing policies optimized learning gains by inducing an adaptive fading approach in which learners received less cognitively engaging forms of remediation as they advanced through the training course. This policy was consistent with preliminary analyses that showed constructive remediation became less effective as learners progressed through the training session. Results also showed that learners’ prior knowledge impacted the type of scaffold that was recommended, thus showing evidence of an aptitude–treatment interaction. We conclude with a discussion of how AI-based training can be leveraged to enhance training effectiveness as well as directions for future research.",
        "DOI": "10.1177/15485129211028317",
        "paper_author": "Spain R.",
        "affiliation_name": "NC State University",
        "affiliation_city": "Raleigh",
        "affiliation_country": "United States",
        "affiliation_id": "60004923",
        "affiliation_state": "NC"
    },
    {
        "paper_title": "CIMMEP: constrained integrated method for CBR maintenance based on evidential policies",
        "publication": "Applied Intelligence",
        "citied_by": "3",
        "cover_date": "2022-04-01",
        "Abstract": "The quality of the proposed solutions by Case-Based Reasoning (CBR) systems is highly dependent on recorded experiences and their describing attributes. Hence, to keep them offering accurate and efficient responses for a long time frame, the maintenance of Case Bases (CB) and Vocabulary knowledge is required. However, maintenance operations are usually unable to exploit provided domain-experts knowledge although this kind of systems are widely applied in several real-life contexts. This offered prior knowledge is handled, in our work, in form of pairwise constraints: Regarding cases, Must-Link (ML) affirms that two given problems should have the same solution, and Cannot-Link (CL) informs that two problems cannot have the same solution. These constraints may also regard vocabulary knowledge in such a way that ML is generated when prior knowledge affirm that two given features offer correlated values, therefore, similar information, and CL is built when they provide different information. This paper proposes a new constrained & integrated method, named CIMMEP, encoding Constrained & Integrated Maintaining Method based on Evidential Policies, for maintaining both vocabulary and CB through eliminating redundancy and noisiness. Since CBR systems handle real-world experiences, which are full of uncertainty, CIMMEP manages this imperfection using a powerful tool called the belief function theory.",
        "DOI": "10.1007/s10489-020-02159-4",
        "paper_author": "Ayed S.B.",
        "affiliation_name": "Laboratoire de Recherche Opérationnelle, de Décision et de Contrôle de processus",
        "affiliation_city": "Le Bardo",
        "affiliation_country": "Tunisia",
        "affiliation_id": "60070639",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Multiplayer Stackelberg-Nash Game for Nonlinear System via Value Iteration-Based Integral Reinforcement Learning",
        "publication": "IEEE Transactions on Neural Networks and Learning Systems",
        "citied_by": "51",
        "cover_date": "2022-04-01",
        "Abstract": "In this article, we study a multiplayer Stackelberg-Nash game (SNG) pertaining to a nonlinear dynamical system, including one leader and multiple followers. At the higher level, the leader makes its decision preferentially with consideration of the reaction functions of all followers, while, at the lower level, each of the followers reacts optimally to the leader's strategy simultaneously by playing a Nash game. First, the optimal strategies for the leader and the followers are derived from down to the top, and these strategies are further shown to constitute the Stackelberg-Nash equilibrium points. Subsequently, to overcome the difficulty in calculating the equilibrium points analytically, we develop a novel two-level value iteration-based integral reinforcement learning (VI-IRL) algorithm that relies only upon partial information of system dynamics. We establish that the proposed method converges asymptotically to the equilibrium strategies under the weak coupling conditions. Moreover, we introduce effective termination criteria to guarantee the admissibility of the policy (strategy) profile obtained from a finite number of iterations of the proposed algorithm. In the implementation of our scheme, we employ neural networks (NNs) to approximate the value functions and invoke the least-squares methods to update the involved weights. Finally, the effectiveness of the developed algorithm is verified by two simulation examples.",
        "DOI": "10.1109/TNNLS.2020.3042331",
        "paper_author": "Li M.",
        "affiliation_name": "University of Science and Technology of China",
        "affiliation_city": "Hefei",
        "affiliation_country": "China",
        "affiliation_id": "60019118",
        "affiliation_state": "Anhui"
    },
    {
        "paper_title": "Model-Free Reinforcement Learning by Embedding an Auxiliary System for Optimal Control of Nonlinear Systems",
        "publication": "IEEE Transactions on Neural Networks and Learning Systems",
        "citied_by": "10",
        "cover_date": "2022-04-01",
        "Abstract": "In this article, a novel integral reinforcement learning (IRL) algorithm is proposed to solve the optimal control problem for continuous-time nonlinear systems with unknown dynamics. The main challenging issue in learning is how to reject the oscillation caused by the externally added probing noise. This article challenges the issue by embedding an auxiliary trajectory that is designed as an exciting signal to learn the optimal solution. First, the auxiliary trajectory is used to decompose the state trajectory of the controlled system. Then, by using the decoupled trajectories, a model-free policy iteration (PI) algorithm is developed, where the policy evaluation step and the policy improvement step are alternated until convergence to the optimal solution. It is noted that an appropriate external input is introduced at the policy improvement step to eliminate the requirement of the input-to-state dynamics. Finally, the algorithm is implemented on the actor-critic structure. The output weights of the critic neural network (NN) and the actor NN are updated sequentially by the least-squares methods. The convergence of the algorithm and the stability of the closed-loop system are guaranteed. Two examples are given to show the effectiveness of the proposed algorithm.",
        "DOI": "10.1109/TNNLS.2020.3042589",
        "paper_author": "Xu Z.",
        "affiliation_name": "Sophia University",
        "affiliation_city": "Tokyo",
        "affiliation_country": "Japan",
        "affiliation_id": "60002784",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Model-Free Reinforcement Learning for Fully Cooperative Consensus Problem of Nonlinear Multiagent Systems",
        "publication": "IEEE Transactions on Neural Networks and Learning Systems",
        "citied_by": "40",
        "cover_date": "2022-04-01",
        "Abstract": "This article presents an off-policy model-free algorithm based on reinforcement learning (RL) to optimize the fully cooperative (FC) consensus problem of nonlinear continuous-time multiagent systems (MASs). First, the optimal FC consensus problem is transformed into solving the coupled Hamilton-Jacobian-Bellman (HJB) equation. Then, we propose a policy iteration (PI)-based algorithm, which is further proved to be effective to solve the coupled HJB equation. To implement this scheme in a model-free way, a model-free Bellman equation is derived to find the optimal value function and the optimal control policy for each agent. Then, based on the least-squares approach, the tuning law for actor and critic weights is derived by employing actor and critic neural networks into the model-free Bellman equation to approximate the target policies and the value function. Finally, we propose an off-policy model-free integral RL (IRL) algorithm, which can be used to optimize the FC consensus problem of the whole system in real time by using measured data. The effectiveness of this proposed algorithm is verified by the simulation results.",
        "DOI": "10.1109/TNNLS.2020.3042508",
        "paper_author": "Wang H.",
        "affiliation_name": "University of Science and Technology of China",
        "affiliation_city": "Hefei",
        "affiliation_country": "China",
        "affiliation_id": "60019118",
        "affiliation_state": "Anhui"
    },
    {
        "paper_title": "Short-term forecasting of the coronavirus pandemic",
        "publication": "International Journal of Forecasting",
        "citied_by": "27",
        "cover_date": "2022-04-01",
        "Abstract": "We have been publishing real-time forecasts of confirmed cases and deaths from coronavirus disease 2019 (COVID-19) since mid-March 2020 (published at www.doornik.com/COVID-19). These forecasts are short-term statistical extrapolations of past and current data. They assume that the underlying trend is informative regarding short-term developments but without requiring other assumptions about how the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) virus is spreading, or whether preventative policies are effective. Thus, they are complementary to the forecasts obtained from epidemiological models. The forecasts are based on extracting trends from windows of data using machine learning and then computing the forecasts by applying some constraints to the flexible extracted trend. These methods have been applied previously to various other time series data and they performed well. They have also proved effective in the COVID-19 setting where they provided better forecasts than some epidemiological models in the earlier stages of the pandemic.",
        "DOI": "10.1016/j.ijforecast.2020.09.003",
        "paper_author": "Doornik J.A.",
        "affiliation_name": "Nuffield College, University Of Oxford",
        "affiliation_city": "Oxford",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60010018",
        "affiliation_state": "Oxfordshire"
    },
    {
        "paper_title": "System Architecture of a European Platform for Health Policy Decision Making: MIDAS",
        "publication": "Frontiers in Public Health",
        "citied_by": "2",
        "cover_date": "2022-03-31",
        "Abstract": "Background: Healthcare data is a rich yet underutilized resource due to its disconnected, heterogeneous nature. A means of connecting healthcare data and integrating it with additional open and social data in a secure way can support the monumental challenge policy-makers face in safely accessing all relevant data to assist in managing the health and wellbeing of all. The goal of this study was to develop a novel health data platform within the MIDAS (Meaningful Integration of Data Analytics and Services) project, that harnesses the potential of latent healthcare data in combination with open and social data to support evidence-based health policy decision-making in a privacy-preserving manner. Methods: The MIDAS platform was developed in an iterative and collaborative way with close involvement of academia, industry, healthcare staff and policy-makers, to solve tasks including data storage, data harmonization, data analytics and visualizations, and open and social data analytics. The platform has been piloted and tested by health departments in four European countries, each focusing on different region-specific health challenges and related data sources. Results: A novel health data platform solving the needs of Public Health decision-makers was successfully implemented within the four pilot regions connecting heterogeneous healthcare datasets and open datasets and turning large amounts of previously isolated data into actionable information allowing for evidence-based health policy-making and risk stratification through the application and visualization of advanced analytics. Conclusions: The MIDAS platform delivers a secure, effective and integrated solution to deal with health data, providing support for health policy decision-making, planning of public health activities and the implementation of the Health in All Policies approach. The platform has proven transferable, sustainable and scalable across policies, data and regions.",
        "DOI": "10.3389/fpubh.2022.838438",
        "paper_author": "Shi X.",
        "affiliation_name": "KU Leuven",
        "affiliation_city": "Leuven",
        "affiliation_country": "Belgium",
        "affiliation_id": "60025063",
        "affiliation_state": "Vlaams-Brabant"
    },
    {
        "paper_title": "Packet Routing Method for Multi-Stage Networks Based on Reinforcement Learning",
        "publication": "Dianzi Keji Daxue Xuebao/Journal of the University of Electronic Science and Technology of China",
        "citied_by": "0",
        "cover_date": "2022-03-30",
        "Abstract": "Multi-stage networks are widely used in machine learning clusters. Due to the large number of available paths in a multi-stage network, packet routing is a combinatorial optimization problem. Existing routing algorithms based on heuristics lack performance guarantee, which seriously affects the packet transmission delay. This paper proposes a packet routing method based on reinforcement learning for multi-stage networks, using a novel policy iteration algorithm to compute an optimal routing policy by learning. In the policy evaluation step, this algorithm uses the maximum likelihood estimator of the value function, which overcomes the low sample efficiency problem of Monte Carlo (MC) or Temporal-Difference (TD) value function estimators in reinforcement learning. To deal with the high computational complexity of the combinatorial optimization problem in the policy improvement step, this algorithm decomposes the optimization over a combinatorial action space into a sequential optimization of each action. Experiments based on NS-3 network simulator show that the routing policy learnt by the algorithm reduces 13.9% of the average packet transmission delay compared to existing best routing heuristics.",
        "DOI": "10.12178/1001-0548.2021260",
        "paper_author": "Gao Y.",
        "affiliation_name": "University of Electronic Science and Technology of China",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China",
        "affiliation_id": "60005465",
        "affiliation_state": "Sichuan"
    },
    {
        "paper_title": "Modeling Performance and Emissions of a Spark Ignition Engine with Machine Learning Approaches",
        "publication": "SAE Technical Papers",
        "citied_by": "5",
        "cover_date": "2022-03-29",
        "Abstract": "In the foreseeable future, the growing energy crisis and environmental pollution problem pose severe challenges to the automobile powertrains and exhaust systems. However, conventional optimization methods, including multi-dimensional computational fluid dynamics model and bench experiments, are very time-consuming or expensive. Adding the application of data-driven models to engine research and development has the potential to reduce computational costs or the number of in-depth experiments. This purpose of this study was to compare the performance of widely used artificial neural network (ANN) and random forest (RF) model for predicting the fuel consumption and engine-out emissions of a calibrated spark ignition (SI) engine for any given condition. To evaluate the performance of machine models established in this work, engine performance of ~2000 steady-state conditions were collected by a validated one-dimensional (1D) computational fluid dynamics (CFD) model with various spark timings, engine speeds and loads. In detail, a randomly selected 80#x00025; of dataset were used to train and the remaining 20#x00025; were used to validate the proposed machine learning models. A subset from the model predictions formed the test dataset to assess the model performance from a combustion viewpoint in addition to merely statistical indexes. The results indicated that both algorithms can serve as the tool to assist engine combustion analysis. Moreover, the ANN model performed better than RF, as evidenced by the lower root-mean-square error (RMSE) and larger coefficient of determination (R2) regardless of the dataset. Further, the ANN algorithm better characterized the relationship between the engine control variables and the engine fuel consumption rate or engine-out emissions. This was probably due to the fact that engine combustion related responses such as efficiency and emissions were better described by multiple interacted mathematical functions. Accordingly, it is recommended to use ANN model to forecast engine combustion related responses at least for the spark ignition engines.",
        "DOI": "10.4271/2022-01-0380",
        "paper_author": "Yang R.",
        "affiliation_name": "Zhejiang University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60003970",
        "affiliation_state": "Zhejiang"
    },
    {
        "paper_title": "Adaptive governance: learning from what organizations do and managing the role they play",
        "publication": "Kybernetes",
        "citied_by": "15",
        "cover_date": "2022-03-29",
        "Abstract": "Purpose: The purpose of this paper is to deepen the understanding of adaptive governance, which is advocated for as a manner to deal with dramatic changes in society and/or environment. To re-think the possible contributions of organizations and organization theory, to adaptive governance. Design/methodology/approach: Based on social systems theory this study makes a distinction between “governance organizations” and “governance communities.” Organizations are conceptualized as the decision machines which organize and (co-)steer governance. Communities are seen as the social environments against which the governance system orients its operations. This study considers the adaptive mechanisms of organizations and reflect on the roles of organizations to enhance adaptive governance in communities and societies. Findings: Diverse types of organizations can link or couple in different ways to communities in their social environment. Such links can enhance the coordinative capacity of the governance system and can also spur innovation to enable adaptation. Yet, linking with communities can also slow down responses to change and complexify the processes of deliberation in governance. Not all adaptive mechanisms available to organizations can be used in communicating with communities or can be institutionalized, but the continuous innovation in the field of organizations can inspire continuous testing of small-scale adaptive mechanisms at higher levels. Society can thus enhance its adaptive capacity by managing the role of organizations. Originality/value: The harnessing of insights in organization theory and systems theory for improving understanding of adaptive governance. The finding that both experiment and coordination at societal level are needed, toward adaptive governance, and that organizations can contribute to both.",
        "DOI": "10.1108/K-11-2020-0759",
        "paper_author": "van Assche K.",
        "affiliation_name": "University of Alberta",
        "affiliation_city": "Edmonton",
        "affiliation_country": "Canada",
        "affiliation_id": "60030835",
        "affiliation_state": "AB"
    },
    {
        "paper_title": "Data-Driven Framework for Understanding and Predicting Air Quality in Urban Areas",
        "publication": "Frontiers in Big Data",
        "citied_by": "12",
        "cover_date": "2022-03-25",
        "Abstract": "Monitoring, predicting, and controlling the air quality in urban areas is one of the effective solutions for tackling the climate change problem. Leveraging the availability of big data in different domains like pollutant concentration, urban traffic, aerial imagery of terrains and vegetation, and weather conditions can aid in understanding the interactions between these factors and building a reliable air quality prediction model. This research proposes a novel cost-effective and efficient air quality modeling framework including all these factors employing state-of-the-art artificial intelligence techniques. The framework also includes a novel deep learning-based vegetation detection system using aerial images. The pilot study conducted in the UK city of Cambridge using the proposed framework investigates various predictive models ranging from statistical to machine learning and deep recurrent neural network models. This framework opens up possibilities of broadening air quality modeling and prediction to other domains like vegetation or green space planning or green traffic routing for sustainable urban cities. The research is mainly focused on extracting strong pieces of evidence which could be useful in proposing better policies around climate change.",
        "DOI": "10.3389/fdata.2022.822573",
        "paper_author": "Babu Saheer L.",
        "affiliation_name": "Anglia Ruskin University",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60000913",
        "affiliation_state": "Cambridgeshire"
    },
    {
        "paper_title": "Decoding Political Trust in China: A Machine Learning Analysis",
        "publication": "China Quarterly",
        "citied_by": "17",
        "cover_date": "2022-03-25",
        "Abstract": "Survey results inflate political trust in China if the observed trust in the central government is mistaken for the latent trust in the Centre. The target of trust in the country is the Centre, which is ultimately the top leader. The critical issue domain for assessing the Centre's trustworthiness is policy implementation rather than policymaking. The Centre's trustworthiness has two dimensions: commitment to good governance and the capacity to discipline local officials. Observed trust in the central government indicates trust in the Centre's commitment, while observed trust in the local government reflects confidence in the Centre's capacity. A machine learning analysis of a national survey reveals how much conventional reading overestimates political trust. At first glance, 85 per cent of the respondents trust the central government. Upon further inspection, 18 per cent have total trust in the Centre, 34 per cent have partial trust and 33 per cent are sceptical.",
        "DOI": "10.1017/S0305741021001077",
        "paper_author": "Li L.",
        "affiliation_name": "Chinese University of Hong Kong",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong",
        "affiliation_id": "60002798",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Reinforcement Learning Approaches for the Optimization of the Partial Oxidation Reaction of Methane",
        "publication": "Industrial and Engineering Chemistry Research",
        "citied_by": "10",
        "cover_date": "2022-03-23",
        "Abstract": "Optimizing reactions in the chemical industry is one of the major challenges in the pursuit of economic and ecological sustainability. With ongoing research in this field, the amount of available data has greatly increased, which makes it suitable for machine learning approaches. In this paper, the application of reinforcement learning for finding optimal reaction conditions of the partial oxidation of methane (POX) is tested. Q-learning (QL) agents and deep deterministic policy gradient (DDPG) agents are trained to maximize H2production by partial oxidation of methane in a simulated plug flow reactor. Although the QL agent showed promising results in a simplified environment, it was not able to achieve improvements in the simulation environment. A clear superiority of the DDPG agent was observed, as it was able to maximize H2production by adjusting temperature, pressure, flow velocity, and substrate composition. This proves that reinforcement learning is applicable for reaction optimization and a promising concept to improve efficiency in chemical processes.",
        "DOI": "10.1021/acs.iecr.1c04622",
        "paper_author": "Neumann M.",
        "affiliation_name": "Rheinisch-Westfälische Technische Hochschule Aachen",
        "affiliation_city": "Aachen",
        "affiliation_country": "Germany",
        "affiliation_id": "60016653",
        "affiliation_state": "Nordrhein-Westfalen"
    },
    {
        "paper_title": "AQX: Explaining Air Quality Forecast for Verifying Domain Knowledge using Feature Importance Visualization",
        "publication": "International Conference on Intelligent User Interfaces, Proceedings IUI",
        "citied_by": "7",
        "cover_date": "2022-03-22",
        "Abstract": "Air pollution forecast has become critical because of its direct impact on human health and its increased production caused by rapid industrialization. Machine learning (ML) solutions are being drastically explored in this domain because they can potentially produce highly accurate results with access to historical data. However, experts in the environmental area are skeptical about adopting ML solutions in real-world applications and policy making due to their black-box nature. In contrast, despite having low accuracy sometimes, the existing traditional simulation model (e.g., CMAQ) are widely used and follows well-defined and transparent equations. Therefore, presenting the knowledge learned by the ML model can make it transparent as well as comprehensible. In addition, validating the ML model's learning with the existing domain knowledge might aid in addressing their skepticism, building appropriate trust, and better utilizing ML models. In collaboration with three experts with an average of five years of research experience in the air pollution domain, we identified that feature (meteorological feature like wind) contribution, towards the final forecast as the major information to be verified with domain knowledge. In addition, the accuracy of ML models compared with traditional simulation models and raw wind trajectories are essential for domain experts to validate the feature contribution. Based on the identified information, we designed and developed AQX, a visual analytics system to help experts validate and verify the ML model's learning with their domain knowledge. The system includes multiple coordinated views to present the contributions of input features at different levels of aggregation in both temporal and spatial dimensions. It also provides a performance comparison of ML and traditional models in terms of accuracy and spatial map, along with the animation of raw wind trajectories for the input period. We further demonstrated two case studies and conducted expert interviews with two domain experts to show the effectiveness and usefulness of AQX.",
        "DOI": "10.1145/3490099.3511150",
        "paper_author": "Palaniyappan Velumani R.",
        "affiliation_name": "Hong Kong University of Science and Technology",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong",
        "affiliation_id": "60008592",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Design, Implementation, and Clinical Impact of a Machine Learning–Assisted Intervention Bundle to Improve Opioid Prescribing",
        "publication": "NEJM Catalyst Innovations in Care Delivery",
        "citied_by": "5",
        "cover_date": "2022-03-16",
        "Abstract": "Opioid-prescribing patterns, including those after surgery, have been implicated as a significant contributor to the ongoing U.S. opioid crisis. Personalized postoperative opioid prescribing, tailored to a patient’s unique characteristics, is now feasible using modern machine-learning techniques and large data sets. However, there are many challenges facing machine learning–based interventions, including data set shift and barriers to converting model predictions into action that delivers clinical impact. A multidisciplinary group at the Beth Israel Deaconess Medical Center and Harvard Medical School built, deployed, and evaluated the impact of a machine learning–assisted intervention aimed at improving the postsurgical opioid prescribing by providers. The intervention consisted of: (1) a machine learning–augmented point-of-care dashboard to guide personalized opioid prescribing, (2) a system for continual automated collection of patient opioid consumption data to adjust for temporal changes in opioid use and to update the machine-learning model, and (3) an automated near–real-time feedback system to promote durable behavior change in surgical opioid prescribers. The authors describe how their group built the infrastructure to automate rapid feedback and data collection and how the group was able to evaluate the effect of an intervention that existed in a context of rapidly changing policy norms and a pandemic that dramatically reduced surgical volumes. Over the course of 2 years, the intervention was associated with a reduced size and reduced frequency of inappropriately large opioid prescriptions, prescribing practices evolved to be closer to typical patient opioid consumption, and the volume of excess pills prescribed dropped significantly. Beth Israel continues to update the intervention with the most recently collected data and user feedback and iterate on the design to properly target surgical opioid prescribers in the health system.",
        "DOI": "10.1056/CAT.21.0477",
        "paper_author": "Marwaha J.S.",
        "affiliation_name": "Beth Israel Deaconess Medical Center",
        "affiliation_city": "Boston",
        "affiliation_country": "United States",
        "affiliation_id": "60030058",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Extraction of complex crop structure in the Hetao Irrigation District of Inner Mongolia using GEE and machine learning",
        "publication": "Nongye Gongcheng Xuebao/Transactions of the Chinese Society of Agricultural Engineering",
        "citied_by": "17",
        "cover_date": "2022-03-15",
        "Abstract": "Hetao Irrigation District (HTID) has been the largest self-flowing irrigation district with one water intake in Asia, serving an important commercial grain and oil production base in China. The annual grain production in the HTID reached 2.55 million tons in 2018, accounting for 3.9‰ of the total crop cultivation area in China. Therefore, accurate and rapid extraction of crop structure can be of great practical significance in agricultural production for the food security of the HTID. However, it is difficult to distinguish the pixels of major crops in the remote sensing images, due to the severe soil salinization, fragmented and scattered crop distribution, as well as the same crop with the different spectrum of various crops. Moreover, there are the close growth periods of major crops in the HTID, which can mix the elements in the images. In this study, Sentinel-2 high-resolution remote sensing images and the GlobeLand30 dataset were used to extract the crop planting structure of the HTID using the Google Earth Engine cloud computing platform. Nearly 1 200 sample points were filtrated using the OTSU algorithm and Google Earth visual interpretation. The features of spectra, frequently-used vegetation, red-edge vegetation, and crop texture were input into four classifiers, including the Random Forest (RF), Support Vector Machine (SVM), Naive Bayes (NB), Classification and Regression Tree (CART). The Overall Accuracy (OA) and Kappa coefficient were used to evaluate the performances of model for the extraction of crop planting structure. Firstly, the impacts of classification features and classifiers combinations on the classification accuracy were explored to identify the classifier with the highest classification accuracy. Then, the feature optimization was performed on the five irrigation sub-districts using out-of-bag error rates for each irrigation sub-district. Finally, the optimal classifier and feature combinations were achieved to derive the cropping structure of four crops in the HTID in 2018. The results show that the RF classifier presented the highest classification accuracy using all feature bands, where the average OA of the HTID (81%) was 6 percentage points and 11 percentage points higher than that of the SVM and NB classifier, respectively. The Kappa coefficient reached 0.68, which was much higher than the rest. Furthermore, the importance of feature bands filtered by the RF was ranked first for the spectral features, the second for the vegetation features, and last for the gray texture features. The indexes were calculated using red-edge bands, indicating the better performance over the other commonly-used remote sensing vegetation indices in crop recognition. In addition, the feature-optimized scheme was the combination with the highest average OA of 86% and Kappa coefficient of 0.78, while the scheme containing 25 bands of spectral, vegetation and texture features presented an OA of 85% and Kappa coefficient of 0.75. Therefore, the new sights can be offered for extracting crop spatial distribution using remote sensing cloud computing platform in complex planting structure area. The finding can provide a strong reference to adjust the agricultural production structure, and further formulate the food macro-control policies in the Hetao Irrigation District.",
        "DOI": "10.11975/j.issn.1002-6819.2022.06.019",
        "paper_author": "Niu Q.",
        "affiliation_name": "China Agricultural University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60013551",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Quantitative Assessment on Ecological Compensation Based on Water Resources Value Accounting: A Case Study of Water Source Area of the Middle Route of South-To-North Water Transfer Project in China",
        "publication": "Frontiers in Environmental Science",
        "citied_by": "6",
        "cover_date": "2022-03-15",
        "Abstract": "As an economic means to adjust the contradiction between ecology and development, ecological compensation plays an important role in promoting the good operation of interbasin water transfer projects and the sustainable development of regional economy. The accounting of ecological compensation is the key and difficult point of ecological compensation as well as the basis of ecological compensation policy and practice. Watershed ecological compensation based on water resources value accounting is an early exploration field of ecological compensation research, and water resources value calculation needs to consider both water quantity and water quality comprehensively. Taking the water source area of the Middle Route of South-to-North Water Transfer Project (SNWTP) as an example, this article tries to establish the payment standard of watershed ecological compensation from the perspective of water resources value. The results show that: 1) The water resources value of the six core regions in the water source area has shown an overall upward trend since 2000, and the northern regions have demonstrated higher value of water resources than the southern regions. 2) The LSTM neural network model is used to forecast the value of water resources in the six regions from 2020 to 2022, and it is found that the value of water resources would increase in the next few years. 3) The compensation price of the six regions in the water source area is predicted in the range of 0.5–1.5 yuan/m3 from 2020 to 2022, and an upward trend in the ecological compensation amount is forecast. Based on the above conclusions, this article puts forward suggestions to establish an ecological compensation accounting system in line with the Middle Route water source area from the perspectives of ecological compensation legislation, allocation of ecological compensation amount, and introduction of market mechanism.",
        "DOI": "10.3389/fenvs.2022.854150",
        "paper_author": "Chen J.",
        "affiliation_name": "Hohai University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60010851",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "Multi-omics in COVID-19: Seeing the unseen but overlooked in the clinic",
        "publication": "Cell Reports Medicine",
        "citied_by": "8",
        "cover_date": "2022-03-15",
        "Abstract": "COVID-19 is an ongoing pandemic of global concern and is unlikely to disappear. This commentary discusses how multi-omics technologies have helped uncover the molecular processes and dynamics underlying COVID-19 initiation, progression, and transmission, and how lack of standardization has limited their application in clinical settings.",
        "DOI": "10.1016/j.xcrm.2022.100580",
        "paper_author": "Lu T.",
        "affiliation_name": "Westlake University",
        "affiliation_city": "Hangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60117660",
        "affiliation_state": "Zhejiang"
    },
    {
        "paper_title": "A soft actor-critic-based energy management strategy for electric vehicles with hybrid energy storage systems",
        "publication": "Journal of Power Sources",
        "citied_by": "64",
        "cover_date": "2022-03-15",
        "Abstract": "With the rapid development of machine learning, deep reinforcement learning (DRL) algorithms have been widely applied to energy management strategies (EMSs) of hybrid vehicles recently. However, current DRL algorithms show the drawbacks of slower convergence rate, brittle training stability, and dissatisfactory optimization effects. In this research, a new DRL algorithm, i.e. the soft actor-critic (SAC) is applied to the EMS of an electric vehicle (EV) with a hybrid energy storage system (HESS). Particularly, the knowledge extracted from the dynamic programming (DP), which is regarded as the benchmark for control methods, is adopted to improve the control performance of the SAC-based EMS and the parallel computing is applied to accelerate the training process. Results of this research indicate that the SAC-based EMS decreases the HESS energy loss by 8.75% and 6.09% compared to the deep Q-network (DQN) and deep deterministic policy gradient (DDPG)-based EMSs respectively while it narrows the gap with the DP-based EMS to 5.19%. Additionally, as the same actor-critic framework, the convergence rate of the SAC-based EMS is faster than that of the DDPG-based EMS by 205.66%. Furthermore, the adaptability validation results present that the SAC-based EMS outperforms the DQN and DDPG-based EMSs in energy saving up to 32.24%.",
        "DOI": "10.1016/j.jpowsour.2022.231099",
        "paper_author": "Xu D.",
        "affiliation_name": "Shenzhen Institute of Advanced Technology",
        "affiliation_city": "Shenzhen",
        "affiliation_country": "China",
        "affiliation_id": "60102083",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "TBSM: A traffic burst-sensitive model for short-term prediction under special events",
        "publication": "Knowledge-Based Systems",
        "citied_by": "68",
        "cover_date": "2022-03-15",
        "Abstract": "Traffic prediction is an important management tool for traffic guidance and control and an effective decision-making tool to help travelers plan routes and avoid congested road sections. However, due to the transient and sudden nature of traffic bursts caused by events and data limitations, mainstream methods do not perform well in short-term traffic prediction for special events (SEs). To address this challenge, we propose a traffic burst-sensitive model (TBSM) for short-term traffic prediction. Specifically, we first define a new state unit with the short-term trend and observed state to represent both the burst case and usual case. Second, a state-and-trend unit similarity degree (SD) measurement method and increment-based prediction model are proposed. The key parameter of this model balances the weight of the short-term trend with the observed state. Finally, we use a deep deterministic policy gradient (DDPG) framework containing long short-term memory (LSTM) networks to realize the self-learning and adjustment of weights to ensure the generality and burst sensitivity of the model. The TBSM is implemented in the district of Beijing Workers’ Stadium, where SEs occur frequently. The results demonstrate that the proposed model performs significantly better than other traditional machine learning approaches and deep learning approaches for SEs. Our TensorFlow implementation of the TBSM is available at https://github.com/buaajh/TBSM-Traffic-burst-sensitive-model-.",
        "DOI": "10.1016/j.knosys.2022.108120",
        "paper_author": "Ren Y.",
        "affiliation_name": "Beihang University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60013789",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Changes in hourly electricity consumption under COVID mandates: A glance to future hourly residential power consumption pattern with remote work in Arizona",
        "publication": "Applied Energy",
        "citied_by": "22",
        "cover_date": "2022-03-15",
        "Abstract": "The transition to remote work brings uncertainty to the future power consumption pattern. The COVID mandates in 2020 have accelerated the transition to remote work, generating major uncertainty regarding how residential power consumption patterns will shift. Understanding these shifts is vital for regional operators who will need to implement long-term planning strategies if companies continue to adopt remote work practices. Additionally, if new COVID variants prompt extended stay-at-home mandates, the resulting behavior shifts will decide the optimal combination of power generation in a region. Our study examines changes in hourly residential power consumption patterns resulting from COVID mandates in Arizona. We estimate how the COVID mandates and subsequent remote work practices could change the power consumption patterns using individual-consumer-level hourly power consumption data for 6,309 consumers and a machine learning framework. We also simulate how the hourly power consumption pattern will change with increasing penetration of remote work under winter and summer temperature settings. We then use our simulations to test the policy effectiveness of changing time-of-use (TOU) rates. Our results show that COVID mandates likely increase the power consumption in the afternoon by 13%, and can change the power consumption pattern in winter from a two-peaked shape to a one-peaked shape. Furthermore, we show that the residents' income, race, and house size are significantly correlated with the changes in power consumption, and the correlation is not linear. We find that, by increasing the peak hour prices and decreasing the off-peak hour prices by 10% of the TOU pricing, the peak electricity demand could be reduced by 10%. Our results show under the new remote work era: (1) the need for modifying previous energy generation combination planning due to changing peak demand hours; (2) equity concerns regarding TOU pricing and the inability of vulnerable groups to shift electricity consumption; (3) the ability of governments and utilities to lower the maximum load of power consumption by modifying the TOU rates.",
        "DOI": "10.1016/j.apenergy.2022.118539",
        "paper_author": "Ku A.L.",
        "affiliation_name": "University of Maryland, College Park",
        "affiliation_city": "College Park",
        "affiliation_country": "United States",
        "affiliation_id": "60020304",
        "affiliation_state": "MD"
    },
    {
        "paper_title": "Unraveling nonlinear and interaction effects of various determinants on bus gaseous emissions",
        "publication": "Science of the Total Environment",
        "citied_by": "5",
        "cover_date": "2022-03-15",
        "Abstract": "Urban transit buses equipped with large-displacement engines operate on circular routes several times throughout the day, emitting large amounts of environmentally hazardous exhaust. Hence, understanding the intricate associations between bus emissions and multiple contributors is beneficial for creating sustainable transportation systems, while previous studies focusing on statistical methods fail to unravel them. This paper innovatively leverages the bagged decision tree approach to delineate such complex relationships based on the data collected from CNG-fueled and diesel-powered buses. Relative importance indicates that velocity appears to be the primary factor and is therefore selected as the research objective. Results suggest that the effects of different contributors on bus emissions present nonlinear patterns. More specifically, the influence of speed on CO, CO2, and NOx exhaust generally reveals an increasing-stabilizing tendency while that of HC represents a decreasing-stabilizing mode. Besides, the phenomenon of synergies between determinants is also prevalent, for instance, buses within high-speed and large-slope conditions tend to produce more emissions. These findings can provide nuanced guidance for policy-making and bus route planning issues in consideration of environmental protection and pollution mitigation.",
        "DOI": "10.1016/j.scitotenv.2021.151427",
        "paper_author": "Hu L.",
        "affiliation_name": "Southeast University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60005244",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "A data-mining approach for developing site-specific fertilizer response functions across the wheat-growing environments in Ethiopia",
        "publication": "Experimental Agriculture",
        "citied_by": "9",
        "cover_date": "2022-03-11",
        "Abstract": "The use of chemical fertilizers is among the main innovations brought by the 1960s Green Revolution. In Ethiopia, fertilizer application during the last four decades has led to significant yield gains, yet yield remains below its potential across much of the country. One of the main challenges responsible for low yield response to fertilizer application has been the use of 'blanket' recommendations, whereby no tailoring of fertilizer amount and frequency is done based on soil requirements. As a result, the amount of fertilizer applied ranges widely, and can be either sub- or supra-optimal. There is thus an increasing need for site-specific fertilizer recommendations which take into account site characteristics such as climate variables (temperature, rainfall, and solar radiation); soil factors (soil organic carbon, moisture, pH, texture, cation exchange capacity, and level of macro- and micronutrients); and topographic position indices. This article reports on a data-mining approach we developed on a large dataset of 6585 wheat (Triticum aestivum) field trials. The dataset includes detailed, site-specific biophysical variables to create nutrient response functions that can guide optimal site-specific fertilizer application. The approach used a machine-learning model (random forest) to capture the relationship between nutrients - nitrogen (N), phosphorous (P), potassium (K), and sulfur (S) - and wheat yield. The model explained about 83, 82, 47, and 69% of variances of yield for N, P, K, and S omission, respectively, with consistent performance across training and testing datasets. Expectedly, for N and P omission data, the most important explanatory variables are nutrient rate, followed by soil organic carbon and soil pH. For K and S, however, climatic variables played an important role alongside nutrient rates. The site-specific yield-fertilizer response curves derived from our model are highly variable from location to location, as they are affected by the climatic, soil, or topographic conditions of the site. Importantly, using principal component analysis, we showed that the shape of the fertilizer response curves is a result of the multiple environmental factors (including soil, topography, and climate) that are at play at a given site, rather than of a specific dominant one. The research output is expected to respond to the national policy demands for a sound method to identify the optimal fertilizer rate to increase economic returns of fertilizer investments and take fertilizer utilization research one step further.",
        "DOI": "10.1017/S0014479722000047",
        "paper_author": "Abera W.",
        "affiliation_name": "International Center for Tropical Agriculture (CIAT)",
        "affiliation_city": "Addis Ababa",
        "affiliation_country": "Ethiopia",
        "affiliation_id": "122977041",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "An Overview of Neuromarketing Research in Developing Countries: Prospects and Challenges",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "2",
        "cover_date": "2022-03-10",
        "Abstract": "Neuromarketing has opened a new door in marketing research understanding behavioral economics with the help of Neuroscience. Over the past decade, Neuroscientists, Psychiatrists, Engineers, and Market-researchers have conducted several groundbreaking studies aiming to understand consumers' motivations, preferences, and decisions. However, these studies and practices are mainly based on developed countries. In this study, we outline the opportunities, real-life applications, future scenarios and shed light on the challenges faced by the researchers, marketers, and policymakers in developing countries including Bangladesh. Moreover, we have focused on the significant brain lobe involving neuromarketing research with the explanation of current technologies used in this area. We have concluded with some feasible recommendations to continue and sustain the growth of the neuromarketing field in developing countries. We expect that this study will give the directions on the inauguration of neuromarketing research in developing countries like Bangladesh that will help technologists, researchers, and marketers understand the advantages, challenges, and state-of-art of neuromarketing research.",
        "DOI": "10.1145/3542954.3542977",
        "paper_author": "Biswas A.",
        "affiliation_name": "United International University",
        "affiliation_city": "Dhaka",
        "affiliation_country": "Bangladesh",
        "affiliation_id": "60002203",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Reinforcement learning-based real-time control of coastal urban stormwater systems to mitigate flooding and improve water quality",
        "publication": "Environmental Science: Water Research and Technology",
        "citied_by": "21",
        "cover_date": "2022-03-04",
        "Abstract": "Real-time control of stormwater systems can reduce flooding and improve water quality. Current industry real-time control strategies use simple rules based on water quantity parameters at a local scale. However, system-level control methods that also incorporate observations of water quality could provide improved control and performance. Therefore, the objective of this research is to evaluate the impact of local and system-level control approaches on flooding and sediment-related water quality in a stormwater system within the flood-prone coastal city of Norfolk, Virginia, USA. Deep reinforcement learning (RL), an emerging machine learning technique, is used to learn system-level control policies that attempt to balance flood mitigation and treatment of sediment. RL is compared to the conventional stormwater system and two methods of local-scale rule-based control: (i) industry standard predictive rule-based control with a fixed detention time and (ii) rules based on water quality observations. For the studied system, both methods of rule-based control improved water quality compared to the passive system, but increased total system flooding due to uncoordinated releases of stormwater. An RL agent learned controls that maintained target pond levels while reducing total system flooding by 4% compared to the passive system. When pre-trained from the RL agent that learned to reduce flooding, another RL agent was able to learn to decrease TSS export by an average of 52% compared to the passive system and with an average of 5% less flooding than the rule-based control methods. As the complexity of stormwater RTC implementations grows and climate change continues, system-level control approaches such as the RL used here will be needed to help mitigate flooding and protect water quality.",
        "DOI": "10.1039/d1ew00582k",
        "paper_author": "Bowes B.D.",
        "affiliation_name": "University of Virginia",
        "affiliation_city": "Charlottesville",
        "affiliation_country": "United States",
        "affiliation_id": "60021918",
        "affiliation_state": "VA"
    },
    {
        "paper_title": "Reinforcement Learning With Vision-Proprioception Model for Robot Planar Pushing",
        "publication": "Frontiers in Neurorobotics",
        "citied_by": "15",
        "cover_date": "2022-03-02",
        "Abstract": "We propose a vision-proprioception model for planar object pushing, efficiently integrating all necessary information from the environment. A Variational Autoencoder (VAE) is used to extract compact representations from the task-relevant part of the image. With the real-time robot state obtained easily from the hardware system, we fuse the latent representations from the VAE and the robot end-effector position together as the state of a Markov Decision Process. We use Soft Actor-Critic to train the robot to push different objects from random initial poses to target positions in simulation. Hindsight Experience replay is applied during the training process to improve the sample efficiency. Experiments demonstrate that our algorithm achieves a pushing performance superior to a state-based baseline model that cannot be generalized to a different object and outperforms state-of-the-art policies which operate on raw image observations. At last, we verify that our trained model has a good generalization ability to unseen objects in the real world.",
        "DOI": "10.3389/fnbot.2022.829437",
        "paper_author": "Cong L.",
        "affiliation_name": "Universität Hamburg",
        "affiliation_city": "Hamburg",
        "affiliation_country": "Germany",
        "affiliation_id": "60028229",
        "affiliation_state": "Hamburg"
    },
    {
        "paper_title": "Auditing and Debugging Deep Learning Models via Flip Points: Individual-Level and Group-Level Analysis",
        "publication": "Matematica",
        "citied_by": "2",
        "cover_date": "2022-03-01",
        "Abstract": "Deep learning models have been criticized for their lack of easy interpretation, which undermines confidence in their use for important applications. Nevertheless, they are consistently utilized in many applications, consequential to humans’ lives, usually because of their better performance. Therefore, there is a great need for computational methods that can explain, audit, and debug such models. Here, we use flip points to accomplish these goals for deep learning classifiers used in social applications. A trained deep learning classifier is a mathematical function that maps inputs to classes. By way of training, the function partitions its domain and assigns a class to each of the partitions. Partitions are defined by the decision boundaries which are expected to be geometrically complex. This complexity is usually what makes deep learning models powerful classifiers. Flip points are points on those boundaries and, therefore, the key to understanding and changing the functional behavior of models. We use advanced numerical optimization techniques and state-of-the-art methods in numerical linear algebra, such as rank determination and reduced-order models to compute and analyze them. The resulting insight into the decision boundaries of a deep model can clearly explain the model’s output on the individual level, via an explanation report that is understandable by non-experts. We also develop a procedure to understand and audit model behavior towards groups of people. We show that examining decision boundaries of models in certain subspaces can reveal hidden biases that are not easily detectable. Flip points can also be used as synthetic data to alter the decision boundaries of a model and improve their functional behaviors. We demonstrate our methods by investigating several models trained on standard datasets used in social applications of machine learning. We also identify the features that are most responsible for particular classifications and misclassifications. Finally, we discuss the implications of our auditing procedure in the public policy domain.",
        "DOI": "10.1007/s44007-021-00003-w",
        "paper_author": "Yousefzadeh R.",
        "affiliation_name": "Yale School of Medicine",
        "affiliation_city": "New Haven",
        "affiliation_country": "United States",
        "affiliation_id": "60017994",
        "affiliation_state": "CT"
    },
    {
        "paper_title": "A decision-centric impact assessment of operational performance of the Yongdam Dam, South Korea",
        "publication": "Journal of Korea Water Resources Association",
        "citied_by": "3",
        "cover_date": "2022-03-01",
        "Abstract": "Amidst the global climate crisis, dam operation policies formulated under the stationary climate assumption could lead to unsatisfactory water management. In this work, we assessed status-quo performance of the Yongdam Dam in Korea under various climatic stresses in flood risk reduction and water supply reliability for 2021-2040. To this end, we employed a decision-centric framework equipped with a stochastic weather generator, a conceptual streamflow model, and a machine-learning reservoir operation rule. By imposing 294 climate perturbations to dam release simulations, we found that the current operation rule of the Yongdam dam could redundantly secure water storage, while inefficiently enhancing the supply reliability. On the other hand, flood risks were likely to increase substantially due to rising mean and variability of daily precipitation. Here, we argue that the current operation rules of the Yongdam Dam seem to be overly focused on securing water storage, and thus need to be adjusted to efficiently improve supply reliability and reduce flood risks in downstream areas.",
        "DOI": "10.3741/JKWRA.2022.55.3.205",
        "paper_author": "Kim D.",
        "affiliation_name": "Jeonbuk National University",
        "affiliation_city": "Jeonju",
        "affiliation_country": "South Korea",
        "affiliation_id": "60001187",
        "affiliation_state": "Jeollabuk-do"
    },
    {
        "paper_title": "Weight-Biased Language across 30 Years of Australian News Reporting on Obesity: Associations with Public Health Policy",
        "publication": "Obesities",
        "citied_by": "3",
        "cover_date": "2022-03-01",
        "Abstract": "(1) Background: This study tracked the reporting of obesity in the Australian news media over three decades and how changing representations over time were linked to obesity-related public health policy developments. (2) Methods: Machine learning and computational language analysis techniques (word embedding, dichotomous bias mapping) were used to identify language biases associated with obesity in 157,237 relevant articles drawn from the Australian Dow Jones digital database of print news media articles from 1990 to 2019. (3) Results: Obesity-related terms were stigmatised on four key dimensions (gender, health, socioeconomic status, stereotypes), with language biased towards femininity and lower socioeconomic status in particular. Biases remained relatively steady from 2005 to 2019, despite recent policy initiatives directly seeking to address obesity stigma. To some degree, for each of the four dimensions, cosine values moved toward 0 over time (i.e., no association with one dimension poll or the other), but remained around 0.20. There was a strong relationship between news media and public health policy discourse over the 30-year study period. (4) Conclusions: With increasing recognition of the health consequences of weight stigma, policymakers and the media must work together to ensure public weight management narratives avoid discourse that may stigmatise heavier individuals, particularly women, and/or reinforce negative obesity stereotypes.",
        "DOI": "10.3390/obesities2010010",
        "paper_author": "Grant S.",
        "affiliation_name": "Swinburne University of Technology",
        "affiliation_city": "Hawthorn",
        "affiliation_country": "Australia",
        "affiliation_id": "60030804",
        "affiliation_state": "VIC"
    },
    {
        "paper_title": "Reinforcement Learning for Collaborative Robots Pick-and-Place Applications: A Case Study †",
        "publication": "Automation",
        "citied_by": "18",
        "cover_date": "2022-03-01",
        "Abstract": "The number of applications in which industrial robots share their working environment with people is increasing. Robots appropriate for such applications are equipped with safety systems according to ISO/TS 15066:2016 and are often referred to as collaborative robots (cobots). Due to the nature of human-robot collaboration, the working environment of cobots is subjected to unforeseeable modifications caused by people. Vision systems are often used to increase the adaptability of cobots, but they usually require knowledge of the objects to be manipulated. The application of machine learning techniques can increase the flexibility by enabling the control system of a cobot to continuously learn and adapt to unexpected changes in the working environment. In this paper we address this issue by investigating the use of Reinforcement Learning (RL) to control a cobot to perform pick-and-place tasks. We present the implementation of a control system that can adapt to changes in position and enables a cobot to grasp objects which were not part of the training. Our proposed system uses deep Q-learning to process color and depth images and generates an (Formula presented.) -greedy policy to define robot actions. The Q-values are estimated using Convolution Neural Networks (CNNs) based on pre-trained models for feature extraction. To reduce training time, we implement a simulation environment to first train the RL agent, then we apply the resulting system on a real cobot. System performance is compared when using the pre-trained CNN models ResNext, DenseNet, MobileNet, and MNASNet. Simulation and experimental results validate the proposed approach and show that our system reaches a grasping success rate of 89.9% when manipulating a never-seen object operating with the pre-trained CNN model MobileNet.",
        "DOI": "10.3390/automation3010011",
        "paper_author": "Gomes N.M.",
        "affiliation_name": "Hanzehogeschool Groningen",
        "affiliation_city": "Groningen",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60103967",
        "affiliation_state": "DA"
    },
    {
        "paper_title": "Mapping global cropping system: Challenges, opportunities, and future perspectives",
        "publication": "Crop and Environment",
        "citied_by": "21",
        "cover_date": "2022-03-01",
        "Abstract": "Spatially explicit global cropping system data products, which provide critical information on harvested areas, crop yields, and other management variables, are imperative to tackle current grand challenges such as global food security and climate change. These cropping system datasets are also very useful for researchers as they can support various scientific analyses in research projects. Yet, effectively searching, navigating, and fully understanding various global datasets can be a daunting task for researchers and policy analysts. In this review, we first compare a few selected global data products, which use crop census and statistical data as the main data source, and identify key problems and challenges of the global crop mapping such as data accuracy and consistency. We then pointed out the future perspectives and directions in further improving the global cropping data products. Collective mechanisms and efforts with the support of open-access data hosting platforms, standard protocols, and consistent financial support are necessary to produce high-quality datasets for researchers, practitioners, and policymakers. Moreover, machine learning and data fusion approaches can also be further explored in future mapping exercises.",
        "DOI": "10.1016/j.crope.2022.03.006",
        "paper_author": "You L.",
        "affiliation_name": "International Food Policy Research Institute",
        "affiliation_city": "Washington, D.C.",
        "affiliation_country": "United States",
        "affiliation_id": "60000840",
        "affiliation_state": "DC"
    },
    {
        "paper_title": "Integration of Sentinel-3 and MODIS Vegetation Indices with ERA-5 Agro-Meteorological Indicators for Operational Crop Yield Forecasting",
        "publication": "Remote Sensing",
        "citied_by": "14",
        "cover_date": "2022-03-01",
        "Abstract": "Timely crop yield forecasts at a national level are substantial to support food policies, to assess agricultural production, and to subsidize regions affected by food shortage. This study presents an operational crop yield forecasting system for Poland that employs freely available satellite and agro-meteorological products provided by the Copernicus programme. The crop yield predictors consist of: (1) Vegetation condition indicators provided daily by Sentinel-3 OLCI (optical) and SLSTR (thermal) imagery, (2) a backward extension of Sentinel-3 data (before 2018) derived from cross-calibrated MODIS data, and (3) air temperature, total precipitation, surface radiation, and soil moisture derived from ERA-5 climate reanalysis generated by the European Centre for Medium-Range Weather Forecasts. The crop yield forecasting algorithm is based on thermal time (growing degree days derived from ERA-5 data) to better follow the crop development stage. The recursive feature elimination is used to derive an optimal set of predictors for each administrative unit, which are ultimately employed by the Extreme Gradient Boosting regressor to forecast yields using official yield statistics as a reference. According to intensive leave-one-year-out cross validation for the 2000–2019 period, the relative RMSE for voivodships (NUTS-2) are: 8% for winter wheat, and 13% for winter rapeseed and maize. Respectively, for municipalities (LAU) it equals 14% for winter wheat, 19% for winter rapeseed, and 27% for maize. The system is designed to be easily applicable in other regions and to be easily adaptable to cloud computing environments such as Data and Information Access Services (DIAS) or Amazon AWS, where data sets from the Copernicus programme are directly accessible.",
        "DOI": "10.3390/rs14051238",
        "paper_author": "Bojanowski J.S.",
        "affiliation_name": "Instytut Geodezji i Kartografii, Warszawa",
        "affiliation_city": "Warsaw",
        "affiliation_country": "Poland",
        "affiliation_id": "60085685",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Exposure to eye-level greenspace reduces health inequalities of high blood pressure: A gender difference perspective",
        "publication": "Hygiene and Environmental Health Advances",
        "citied_by": "5",
        "cover_date": "2022-03-01",
        "Abstract": "Existing evidence suggests that exposure to greenspace reduces the risk of high blood pressure (e.g., hypertension). In addition, greenspace may also narrow the socioeconomic and gender inequities of various health outcomes. However, exposure to greenspace was often defined from an over-head perspective. The effect of eye-level greenspace exposure, which may better represent people's actual exposure to greenspace, was not explored yet. Furthermore, it remains unclear whether exposure to greenspace may reduce the socioeconomic and gender inequities of high blood pressure. In this study, the blood pressure data of 24,845 adult participants were retrieved from the 33 Chinese Community Health Study in China. We quantified participants’ exposure to eye-level greenspace via street view images and machine learning technique. Multilevel linear and logistic regressions were applied. While controlling for confounders, we found that exposure to eye-level greenspace was both related to gender and socioeconomic status (SES). More specifically, greenspace exposure was inversely associated with the risk of hypertension for females, but not for males. We observed that greenspace-hypertension associations are more pronounced for SES disadvantaged groups (those uneducated and/ or with low household income). This study provides profound insights into how exposure to eye-level greenspace reduces the gender and socioeconomic inequities in terms of high blood pressure, which suggests that policy makers and urban planners should pay close attention to the equalizing effect of urban greenspace on residents’ health outcomes in the long run.",
        "DOI": "10.1016/j.heha.2022.100001",
        "paper_author": "Wang R.",
        "affiliation_name": "Queen's University Belfast",
        "affiliation_city": "Belfast",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60029738",
        "affiliation_state": "Northern Ireland"
    },
    {
        "paper_title": "Artificial Intelligence in “Code Stroke”—A Paradigm Shift: Do Radiologists Need to Change Their Practice?",
        "publication": "Radiology: Artificial Intelligence",
        "citied_by": "7",
        "cover_date": "2022-03-01",
        "Abstract": "NA",
        "DOI": "10.1148/ryai.210204",
        "paper_author": "Vagal A.",
        "affiliation_name": "University of Cincinnati Medical Center",
        "affiliation_city": "Cincinnati",
        "affiliation_country": "United States",
        "affiliation_id": "60122663",
        "affiliation_state": "OH"
    },
    {
        "paper_title": "Predicting Dividend Omission Behaviour of Indian Firms using Machine Learning Algorithms",
        "publication": "Finance India",
        "citied_by": "2",
        "cover_date": "2022-03-01",
        "Abstract": "The life-cycle theory of dividends suggests that dividend omissions may indicate significant strategic changes in the firm’s life-cycle. Such behaviours at the same time have implications for investor perception as dividend omissions may signal weak operating performance or financial distress situation. A firm’s preference for dividend payments relative to omitting dividend paymentsis also used to cater to investor time-varying preferences.This paper aims to test the prediction models of dividend omission behaviour of firms in India.The financial data of 12942 firm- year observations from 2013 to 2018 indicate 55 percent dividend omissions. The paper uses five classes of machine learning algorithmsto predict this behaviour. The multi-layer perceptron (MLP)ANN approach using the RProp algorithm achieves a predictive accuracy of 82.36 percent with an ROC (area under the curve) of 0.901. The feature set relating to the financial parameters of a firm contributes to the prediction accuracy.",
        "DOI": "NA",
        "paper_author": "Bhat R.",
        "affiliation_name": "Narsee Monjee Institute of Management Studies, Mumbai",
        "affiliation_city": "Mumbai",
        "affiliation_country": "India",
        "affiliation_id": "60079592",
        "affiliation_state": "MH"
    },
    {
        "paper_title": "Predicting the Probability of Failure of Central Public Sector Enterprises: A Statistical &amp; Machine Learning Approach",
        "publication": "Finance India",
        "citied_by": "0",
        "cover_date": "2022-03-01",
        "Abstract": "The present study is an attempt to identify factors of failure of Heavy, Medium & Light Engineering CPSE, how to predict failure and effective prediction methods. The study is apparent as the escalating number of failures, forcing the management and policy makers to design new strategies and regulation to avoid probable failure. The Principle Component Analysis shows the degree of the factors is responsiblefor the failure of CPSEs. Further, these variables were tested by using a Statistical and machine learning approach to develop the prediction model. The result of logistic regression has an accuracy of 83.9% in predicting the failure. The prediction accuracy of the Support vector machine is 93.5% whereas Random forest has 96.3% accuracy. The results show that the accuracy of machine learning approach is higher than statistical approach. The failure of the CPSEs may be avoided if indications are timely established and the correct prediction model must be applied.",
        "DOI": "NA",
        "paper_author": "Pardeshi B.",
        "affiliation_name": "S. B. Patil Institute of Management",
        "affiliation_city": "Pune",
        "affiliation_country": "India",
        "affiliation_id": "123994398",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "The Need for Modernization of Biosecurity in the Post-COVID World",
        "publication": "mSphere",
        "citied_by": "2",
        "cover_date": "2022-03-01",
        "Abstract": "At present, there are two hypotheses about the emergence of SARS-CoV-2; the first is that it was due to a naturally occurring zoonotic jump, and the second contends that it spread due to an accidental dispersion of a laboratory-acquired infection in Wuhan, China. While the pandemic’s actual origins remain occluded, it is useful to examine the latter possibility as a paradigm for evaluating biosecurity policy in the post-COVID world. While the pandemic may not have emerged from a research lab, this is possible with research on dangerous pathogens and prompts questions for biosecurity. How might biosecurity protections for such research be modernized while still enabling important, necessary public health research that utilizes dual-use or gain-of-function capabilities? As the world takes urgent action to mitigate shortcomings in the response to COVID-19, such questions and their potential solutions are vital to inform and direct future life science and technology endeavors.",
        "DOI": "10.1128/msphere.00025-22",
        "paper_author": "DiEuliis D.",
        "affiliation_name": "National Defense University",
        "affiliation_city": "Washington, D.C.",
        "affiliation_country": "United States",
        "affiliation_id": "60031754",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Framework for estimating renal function using magnetic resonance imaging",
        "publication": "Journal of Medical Imaging",
        "citied_by": "2",
        "cover_date": "2022-03-01",
        "Abstract": "Purpose: Nephrologists have empirically predicted renal function from renal morphology. In diagnosing a case of renal dysfunction of unknown course, acute kidney injury and chronic kidney disease are diagnosed from blood tests and an imaging study including magnetic resonance imaging (MRI), and an examination/treatment policy is determined. A framework for the estimation of renal function from water images obtained using the Dixon method is proposed to provide information that helps clinicians reach a diagnosis by accurately estimating renal function on the basis of renal MRI. Approach: The proposed framework consists of four steps. First, the kidney area is extracted by MRI using the Dixon method with a U-net by deep learning. Second, the extracted renal region is registered with the target mask. Third, the kidney features are calculated based on the target mask classification information created by a specialist. Fourth, the estimated glomerular filtration rate (eGFR) representing the renal function is estimated using a regression support vector machine from the calculated features. Results: For the accuracy evaluation, we conducted an experiment to estimate the eGFR when MRI was performed and the eGFR slope, which is the annual rate of decline in eGFR. When the accuracy was evaluated for 165 subjects, the eGFR was estimated to have a root mean square error (RMSE) of 11.99 and a correlation coefficient of 0.83. Moreover, the eGFR slope was estimated to have an RMSE of 4.8 and a correlation coefficient of 0.5. Conclusions: Therefore, the proposed method shows the possibility of estimating the prognosis of renal function based on water images obtained by the Dixon method.",
        "DOI": "10.1117/1.JMI.9.2.024501",
        "paper_author": "Ishikawa M.",
        "affiliation_name": "Saitama Medical University",
        "affiliation_city": "Moroyama",
        "affiliation_country": "Japan",
        "affiliation_id": "60020563",
        "affiliation_state": "Saitama"
    },
    {
        "paper_title": "A Hybrid Approach to Analyze Cybersecurity News Articles by Utilizing Information Extraction &amp; Sentiment Analysis Methods",
        "publication": "International Journal of Semantic Computing",
        "citied_by": "2",
        "cover_date": "2022-03-01",
        "Abstract": "Cybersecurity is becoming indispensable for everyone and everything in the times of the Internet of Things (IoT) revolution. Every aspect of human society - be it political, financial, technological, or cultural - is affected by cyber-attacks or incidents in one way or another. Newspapers are an excellent source that perfectly captures this web of cybersecurity. By implementing various NLP techniques such as tf-idf, word embedding and sentiment analysis (SA) (machine learning method), this research will examine the cybersecurity-related articles from 18 major newspapers (English language online version) from six countries (three newspapers from each country) collected within one year from April 2018 till March 2019. The first objective is to extract the crucial events from each country, which we will achieve by our first step - 'information extraction.' The next objective is to find out what kind of sentiments those crucial issues garnered, which we will accomplish from our second step - 'SA.' SA of news articles would also help in understanding each 'nation's mood' on critical cybersecurity issues, which can aid decision-makers in charting new policies.",
        "DOI": "10.1142/S1793351X22500015",
        "paper_author": "Ghasiya P.",
        "affiliation_name": "Kyushu University",
        "affiliation_city": "Fukuoka",
        "affiliation_country": "Japan",
        "affiliation_id": "60011047",
        "affiliation_state": "Fukuoka"
    },
    {
        "paper_title": "TUMKFCM-ELM: An Unsupervised Multiple Kernelized Fuzzy C-Means Extreme Learning Machine Approach for Heterogeneous Datasets",
        "publication": "International Journal of Performability Engineering",
        "citied_by": "2",
        "cover_date": "2022-03-01",
        "Abstract": "Heterogeneity is one of the critical aspects of big data that results in data integration challenges that big data analysis. Heterogeneous data types are also necessary for preprocessing to be unified. The heterogeneity of the benchmark data is summed up when, along with their sampling rate and storage policy, the data type can be indicated. Currently, Kernelized Fuzzy C-Means clustering methodology gained favor in the researching area where numerous functions generated by the kernel are employed in a similarity measure rather than a Euclidean distance, utilized in the traditional Fuzzy C-Means clustering methodology. This methodology also has inconsistencies in the effectiveness like the conventional Fuzzy C-Means clustering methodology because the initial cluster centers are created herein, too, based on the randomized user-defined membership values of objects. This current study presents a modified strategy for eliminating and improving the overall efficiency of the random selection of the Kernelized Fuzzy C-Means clustering approach. This work aims to implement Three-Phase Unsupervised Multiple Kernels Fuzzy C-Means Extreme Learning Machine (TUMKFCM-ELM) approach. Here, we have done work in Three-Phases of this approach: Data Preprocessing (1st phase) and Unsupervised Multiple Kernels Fuzzy C-Means (2nd phase) clustering technique to determine the centroids and membership matrix followed by data preprocessing. These centroids and membership values are updated until the stop criterion is met and obtain the final clusters. At last, ELM has been applied as the 3rd phase of this proposed method to achieve optimal coefficient. Multiple heterogeneous datasets have been collected from numerous sources for this simulation and show an Explanatory data analysis and cluster distributions. We have compared the proposed approach with the previous TUMK-ELM methodology using Accuracy, NMI and purity three validity metrics. This work visualizes the results after the clustering performance and comparable performance in terms of effectiveness. It also provides results in terms of time cost.",
        "DOI": "10.23940/ijpe.22.03.p5.188200",
        "paper_author": "Mune A.R.",
        "affiliation_name": "Babasaheb Naik College of Engineering, Pusad",
        "affiliation_city": "Pusad",
        "affiliation_country": "India",
        "affiliation_id": "60283211",
        "affiliation_state": "MH"
    },
    {
        "paper_title": "The Impact of the U.S. Macroeconomic Variables on the CBOE VIX Index",
        "publication": "Journal of Risk and Financial Management",
        "citied_by": "11",
        "cover_date": "2022-03-01",
        "Abstract": "The purpose of this study is to find the influence of various macroeconomic factors on the volatility index, as macroeconomic factors affect stock market volatility, resulting in an impact on the VIX Index, representing the risk in the stock market. To estimate the significance and importance of the U.S. macroeconomic variables on stock market volatility and risk, classification problems from machine learning are constructed to predict the daily and weekly trends of the VIX Index. Data from May 2007 to December 2021 is considered for analysis. The selected models are trained with twenty-four daily features and twenty-four plus nine weekly features. The outcomes suggest that the decisions made by the Light GBM and XG Boost on ranking features can be significantly accepted over logistic regression. It is found from the results that economic policy uncertainty indices, gold price, the USD Index, and crude oil are signified as strong predictors. The Financial Stress Index, initial claims, M2, TED spread, Fed rate, and credit spread are also strong predictors, while various yields on fixed income securities make a little less impact on the VIX Index. The TED spread, Financial Stress Index, and Equity Market Volatility (Infectious Disease Tracker) are positively associated with the VIX.",
        "DOI": "10.3390/jrfm15030126",
        "paper_author": "Prasad A.",
        "affiliation_name": "S P Jain School of Global Management",
        "affiliation_city": "Mumbai",
        "affiliation_country": "India",
        "affiliation_id": "122274448",
        "affiliation_state": "Mumbai"
    },
    {
        "paper_title": "Performance analysis of a hybrid agent for quantum-accessible reinforcement learning",
        "publication": "New Journal of Physics",
        "citied_by": "4",
        "cover_date": "2022-03-01",
        "Abstract": "In the last decade quantum machine learning has provided fascinating and fundamental improvements to supervised, unsupervised and reinforcement learning (RL). In RL, a so-called agent is challenged to solve a task given by some environment. The agent learns to solve the task by exploring the environment and exploiting the rewards it gets from the environment. For some classical task environments, an analogue quantum environment can be constructed which allows to find rewards quadratically faster by applying quantum algorithms. In this paper, we analytically analyze the behavior of a hybrid agent which combines this quadratic speedup in exploration with the policy update of a classical agent. This leads to a faster learning of the hybrid agent compared to the classical agent. We demonstrate that if the classical agent needs on average (J) rewards and (T) cl epochs to learn how to solve the task, the hybrid agent will take (T)q ≤ αs αo √ (T)cl (J) epochs on average. Here, α s and α o denote constants depending on details of the quantum search and are independent of the problem size. Additionally, we prove that if the environment allows for maximally α o k max sequential coherent interactions, e.g. due to noise effects, an improvement given by (T) q ≈ α o (T) cl/(4k max) is still possible.",
        "DOI": "10.1088/1367-2630/ac5b56",
        "paper_author": "Hamann A.",
        "affiliation_name": "Universität Innsbruck",
        "affiliation_city": "Innsbruck",
        "affiliation_country": "Austria",
        "affiliation_id": "60009999",
        "affiliation_state": "Tyrol"
    },
    {
        "paper_title": "Bridge Response and Heavy Truck Classification Framework Based on a Two-Step Machine Learning Algorithm",
        "publication": "Transportation Research Record",
        "citied_by": "1",
        "cover_date": "2022-03-01",
        "Abstract": "Collecting information on heavy trucks and monitoring the bridges which they regularly cross is important for many facets of infrastructure management. In this paper, a two-step algorithm is developed using bridge and truck data, by deploying sequentially unsupervised and supervised machine learning techniques. Longitudinal clustering of bridge data, concerning strain waveforms, is adopted to perform the first step of the algorithm, while image visual inspection and classification tree methods are applied to truck data concurrently in the second step Both bridge and truck traffic must be monitored for a limited, yet significant, amount of time to calibrate the algorithm, which is then used to build a classification framework. The framework provides the same benefits of two data collection systems while only one needs to be operative. Depending on which monitoring system remains available, the framework enables the use of bridge data to identify the truck’s profile which generated it, or to estimate bridge response given the truck’s information. As a result, the present study aims to provide decision-makers with an effective way to monitor the whole bridge-traffic system, bridge managers to plan effective maintenance, and policymakers to develop ad hoc regulations.",
        "DOI": "10.1177/03611981211052027",
        "paper_author": "Mete F.",
        "affiliation_name": "Robert R. McCormick School of Engineering and Applied Science",
        "affiliation_city": "Evanston",
        "affiliation_country": "United States",
        "affiliation_id": "60147353",
        "affiliation_state": "IL"
    },
    {
        "paper_title": "Interpretable Optimal Stopping",
        "publication": "Management Science",
        "citied_by": "18",
        "cover_date": "2022-03-01",
        "Abstract": "Optimal stopping is the problem of deciding when to stop a stochastic system to obtain the greatest reward, arising in numerous application areas such as finance, healthcare, and marketing. State-of-the-art methods for high-dimensional optimal stopping involve approximating the value function or the continuation value and then using that approximation within a greedy policy. Although such policies can perform very well, they are generally not guaranteed to be interpretable; that is, a decision maker may not be able to easily see the link between the current system state and the policy’s action. In this paper, we propose a new approach to optimal stopping wherein the policy is represented as a binary tree, in the spirit of naturally interpretable tree models commonly used in machine learning. We show that the class of tree policies is rich enough to approximate the optimal policy. We formulate the problem of learning such policies from observed trajectories of the stochastic system as a sample average approximation (SAA) problem. We prove that the SAA problem converges under mild conditions as the sample size increases but that, computationally, even immediate simplifications of the SAA problem are theoretically intractable. We thus propose a tractable heuristic for approximately solving the SAA problem by greedily constructing the tree from the top down. We demonstrate the value of our approach by applying it to the canonical problem of option pricing, using both synthetic instances and instances using real Standard & Poor’s 500 Index data. Our method obtains policies that (1) outperform state-of-the-art noninterpretable methods, based on simulation regression and martingale duality, and (2) possess a remarkably simple and intuitive structure.",
        "DOI": "10.1287/mnsc.2020.3592",
        "paper_author": "Ciocan D.F.",
        "affiliation_name": "INSEAD, Europe",
        "affiliation_city": "Fontainebleau",
        "affiliation_country": "France",
        "affiliation_id": "60004888",
        "affiliation_state": "Ile-de-France"
    },
    {
        "paper_title": "ISpray: Reducing Urban Air Pollution with IntelligentWater Spraying",
        "publication": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",
        "citied_by": "4",
        "cover_date": "2022-03-01",
        "Abstract": "Despite regulations and policies to improve city-level air quality in the long run, there lack precise control measures to protect critical urban spots from heavy air pollution. In this work, we propose iSpray, the first-of-its-kind data analytics engine for fine-grained PM2.5 and PM10 control at key urban areas via cost-effective water spraying. iSpray combines domain knowledge with machine learning to profile and model how water spraying affects PM25 and PM10 concentrations in time and space. It also utilizes predictions of pollution propagation paths to schedule a minimal number of sprayers to keep the pollution concentrations at key spots under control. In-field evaluations show that compared with scheduling based on real-time pollution concentrations, iSpray reduces the total sprayer switch-on time by 32%, equivalent to 1, 782 m3 water and 18, 262 kWh electricity in our deployment, while decreasing the days of poor air quality at key spots by up to 16%.",
        "DOI": "10.1145/3517227",
        "paper_author": "Cheng Y.",
        "affiliation_name": "ETH Zürich",
        "affiliation_city": "Zurich",
        "affiliation_country": "Switzerland",
        "affiliation_id": "60025858",
        "affiliation_state": "ZH"
    },
    {
        "paper_title": "Tribe or Not? Critical Inspection of Group Differences Using TribalGram",
        "publication": "ACM Transactions on Interactive Intelligent Systems",
        "citied_by": "6",
        "cover_date": "2022-03-01",
        "Abstract": "With the rise of AI and data mining techniques, group profiling and group-level analysis have been increasingly used in many domains, including policy making and direct marketing. In some cases, the statistics extracted from data may provide insights to a group's shared characteristics; in others, the group-level analysis can lead to problems, including stereotyping and systematic oppression. How can analytic tools facilitate a more conscientious process in group analysis? In this work, we identify a set of accountable group analytics design guidelines to explicate the needs for group differentiation and preventing overgeneralization of a group. Following the design guidelines, we develop TribalGram, a visual analytic suite that leverages interpretable machine learning algorithms and visualization to offer inference assessment, model explanation, data corroboration, and sense-making. Through the interviews with domain experts, we showcase how our design and tools can bring a richer understanding of \"groups\"mined from the data.",
        "DOI": "10.1145/3484509",
        "paper_author": "Ahn Y.",
        "affiliation_name": "University of Pittsburgh",
        "affiliation_city": "Pittsburgh",
        "affiliation_country": "United States",
        "affiliation_id": "60015543",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "Hydropower Operation Optimization Using Machine Learning: A Systematic Review",
        "publication": "AI (Switzerland)",
        "citied_by": "39",
        "cover_date": "2022-03-01",
        "Abstract": "The optimal dispatch of hydropower plants consists of the challenge of taking advantage of both available head and river flows. Despite the objective of delivering the maximum power to the grid, some variables are uncertain, dynamic, non-linear, and non-parametric. Nevertheless, some models may help hydropower generating players with computer science evolution, thus maximizing the hydropower plants’ power production. Over the years, several studies have explored Machine Learning (ML) techniques to optimize hydropower plants’ dispatch, being applied in the pre-operation, real-time and post-operation phases. Hence, this work consists of a systematic review to analyze how ML models are being used to optimize energy production from hydropower plants. The analysis focused on criteria that interfere with energy generation forecasts, operating policies, and performance evaluation. Our discussions aimed at ML techniques, schedule forecasts, river systems, and ML applications for hydropower optimization. The results showed that ML techniques have been more applied for river flow forecast and reservoir operation optimization. The long-term scheduling horizon is the most common application in the analyzed studies. Therefore, supervised learning was more applied as ML technique segment. Despite being a widely explored theme, new areas present opportunities for disruptive research, such as real-time schedule forecast, run-of-river system optimization and low-head hydropower plant operation.",
        "DOI": "10.3390/ai3010006",
        "paper_author": "Bernardes J.",
        "affiliation_name": "Universidade Federal de Itajubá",
        "affiliation_city": "Itajuba",
        "affiliation_country": "Brazil",
        "affiliation_id": "60025299",
        "affiliation_state": "MG"
    },
    {
        "paper_title": "Mapping Canadian Data Assets to Generate Real-World Evidence: Lessons Learned from Canadian Real-World Evidence for Value of Cancer Drugs (CanREValue) Collaboration’s RWE Data Working Group",
        "publication": "Current Oncology",
        "citied_by": "3",
        "cover_date": "2022-03-01",
        "Abstract": "Canadian provinces routinely collect patient-level data for administrative purposes. These real-world data (RWD) can be used to generate real-world evidence (RWE) to inform clinical care and healthcare policy. The CanREValue Collaboration is developing a framework for the use of RWE in cancer drug funding decisions. A Data Working Group (WG) was established to identify data assets across Canada for generating RWE of oncology drugs. The mapping exercise was conducted using an iterative scan with informant surveys and teleconference. Data experts from ten provinces convened for a total of three teleconferences and two in-person meetings from March 2018 to September 2019. Following each meeting, surveys were developed and shared with the data experts which focused on identifying databases and data elements, as well as a feasibility assessment of conducting RWE studies using existing data elements and resources. Survey responses were compiled into an interim data report, which was used for public stakeholder consultation. The feedback from the public consultation was used to update the interim data report. We found that databases required to conduct real-world studies are often held by multiple different data custodians. Ninety-seven databases were identified across Canada. Provinces held on average 9 distinct databases (range: 8–11). An Essential RWD Table was compiled that contains data elements that are necessary, at a minimal, to conduct an RWE study. An Expanded RWD Table that contains a more comprehensive list of potentially relevant data elements was also compiled and the availabilities of these data elements were mapped. While most provinces have data on patient demographics (e.g., age, sex) and cancer-related variables (e.g., morphology, topography), the availability and linkability of data on cancer treatment, clinical characteristics (e.g., morphology and topography), and drug costs vary among provinces. Based on current resources, data availability, and access processes, data experts in most provinces noted that more than 12 months would be required to complete an RWE study. The CanREValue Collaboration’s Data WG identified key data holdings, access considerations, as well as gaps in oncology treatment-specific data. This data catalogue can be used to facilitate future oncology-specific RWE analyses across Canada.",
        "DOI": "10.3390/curroncol29030165",
        "paper_author": "Dai W.F.",
        "affiliation_name": "University of Toronto Faculty of Medicine",
        "affiliation_city": "Toronto",
        "affiliation_country": "Canada",
        "affiliation_id": "60021600",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "Implementing Artificial Intelligence Techniques to Predict Environmental Impacts: Case of Construction Products",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "27",
        "cover_date": "2022-03-01",
        "Abstract": "Nowadays, product designers, manufacturers, and consumers consider the environmental impacts of products, processes, and services in their decision-making process. Life Cycle Assessment (LCA) is a tool that assesses the environmental impacts over a product’s life cycle. Conducting a life cycle assessment (LCA) requires meticulous data sourcing and collection and is often time-consuming for both practitioner and verifier. However, predicting the environmental impacts of products and services can help stakeholders and decision-makers identify the hotspots. Our work proposes using Artificial Intelligence (AI) techniques to predict the environmental performance of a product or service to assist LCA practitioners and verifiers. This approach uses data from environmental product declarations of construction products. The data is processed utilizing natural language processing (NLP) which is then trained to random forest algorithm, an ensemble tree-based machine learning method. Finally, we trained the model with information on the product and their environmental impacts using seven impact category values and verified the results using a testing dataset (20% of EPD data). Our results demonstrate that the model was able to predict the values of impact categories: global warming potential, abiotic depletion potential for fossil resources, acidification potential, and photochemical ozone creation potential with an accuracy (measured using R2 metrics, a measure to score the correlation of predicted values to real value) of 81%, 77%, 68%, and 70%, respectively. Our method demonstrates the capability to predict environmental performance with a defined variability by learning from the results of the previous LCA studies. The model’s performance also depends on the amount of data available for training. However, this approach does not replace a detailed LCA but is rather a quick prediction and assistance to LCA practitioners and verifiers in realizing an LCA.",
        "DOI": "10.3390/su14063699",
        "paper_author": "Koyamparambath A.",
        "affiliation_name": "Université de Bordeaux",
        "affiliation_city": "Bordeaux",
        "affiliation_country": "France",
        "affiliation_id": "60102125",
        "affiliation_state": "Nouvelle-Aquitaine"
    },
    {
        "paper_title": "Modeling the effects of environmental and perceptual uncertainty using deterministic reinforcement learning dynamics with partial observability",
        "publication": "Physical Review E",
        "citied_by": "7",
        "cover_date": "2022-03-01",
        "Abstract": "Assessing the systemic effects of uncertainty that arises from agents' partial observation of the true states of the world is critical for understanding a wide range of scenarios, from navigation and foraging behavior to the provision of renewable resources and public infrastructures. Yet previous modeling work on agent learning and decision-making either lacks a systematic way to describe this source of uncertainty or puts the focus on obtaining optimal policies using complex models of the world that would impose an unrealistically high cognitive demand on real agents. In this work we aim to efficiently describe the emergent behavior of biologically plausible and parsimonious learning agents faced with partially observable worlds. Therefore we derive and present deterministic reinforcement learning dynamics where the agents observe the true state of the environment only partially. We showcase the broad applicability of our dynamics across different classes of partially observable agent-environment systems. We find that partial observability creates unintuitive benefits in several specific contexts, pointing the way to further research on a general understanding of such effects. For instance, partially observant agents can learn better outcomes faster, in a more stable way, and even overcome social dilemmas. Furthermore, our method allows the application of dynamical systems theory to partially observable multiagent leaning. In this regard we find the emergence of catastrophic limit cycles, a critical slowing down of the learning processes between reward regimes, and the separation of the learning dynamics into fast and slow directions, all caused by partial observability. Therefore, the presented dynamics have the potential to become a formal, yet practical, lightweight and robust tool for researchers in biology, social science, and machine learning to systematically investigate the effects of interacting partially observant agents.",
        "DOI": "10.1103/PhysRevE.105.034409",
        "paper_author": "Barfuss W.",
        "affiliation_name": "Eberhard Karls Universität Tübingen",
        "affiliation_city": "Tubingen",
        "affiliation_country": "Germany",
        "affiliation_id": "60017246",
        "affiliation_state": "Baden-Wurttemberg"
    },
    {
        "paper_title": "Combining Sample Plot Stratification and Machine Learning Algorithms to Improve Forest Aboveground Carbon Density Estimation in Northeast China Using Airborne LiDAR Data",
        "publication": "Remote Sensing",
        "citied_by": "15",
        "cover_date": "2022-03-01",
        "Abstract": "Timely, accurate estimates of forest aboveground carbon density (AGC) are essential for understanding the global carbon cycle and providing crucial reference information for climate-change-related policies. To date, airborne LiDAR has been considered as the most precise remote-sensing-based technology for forest AGC estimation, but it suffers great challenges from various uncertainty sources. Stratified estimation has the potential to reduce the uncertainty and improve the forest AGC estimation. However, the impact of stratification and how to effectively combine stratification and modeling algorithms have not been fully investigated in forest AGC estimation. In this study, we performed a comparative analysis of different stratification approaches (non-stratification, forest type stratification (FTS) and dominant species stratification (DSS)) and different modeling algorithms (stepwise regression, random forest (RF), Cubist, extreme gradient boosting (XGBoost) and categorical boosting (CatBoost)) to identify the optimal stratification approach and modeling algorithm for forest AGC estimation, using airborne LiDAR data. The analysis of variance (ANOVA) was used to quantify and determine the factors that had a significant effect on the estimation accuracy. The results revealed the superiority of stratified estimation models over the unstratified ones, with higher estimation accuracy achieved by the DSS models. Moreover, this improvement was more significant in coniferous species than broadleaf species. The ML algorithms outperformed stepwise regression and the CatBoost models based on DSS provided the highest estimation accuracy (R2 = 0.8232, RMSE = 5.2421, RRMSE = 20.5680, MAE = 4.0169 and Bias = 0.4493). The ANOVA of the prediction error indicated that the stratification method was a more important factor than the regression algorithm in forest AGC estimation. This study demonstrated the positive effect of stratification and how the combination of DSS and the CatBoost algorithm can effectively improve the estimation accuracy of forest AGC. Integrating this strategy with national forest inventory could help improve the monitoring of forest carbon stock over large areas.",
        "DOI": "10.3390/rs14061477",
        "paper_author": "Chen M.",
        "affiliation_name": "Beijing Forestry University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60006782",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Quantifying the spatial homogeneity of urban road networks via graph neural networks",
        "publication": "Nature Machine Intelligence",
        "citied_by": "53",
        "cover_date": "2022-03-01",
        "Abstract": "Quantifying the topological similarities of different parts of urban road networks enables us to understand urban growth patterns. Although conventional statistics provide useful information about the characteristics of either a single node’s direct neighbours or the entire network, such metrics fail to measure the similarities of subnetworks or capture local, indirect neighbourhood relationships. Here we propose a graph-based machine learning method to quantify the spatial homogeneity of subnetworks. We apply the method to 11,790 urban road networks across 30 cities worldwide to measure the spatial homogeneity of road networks within each city and across different cities. We find that intracity spatial homogeneity is highly associated with socioeconomic status indicators such as gross domestic product and population growth. Moreover, intercity spatial homogeneity values obtained by transferring the model across different cities reveal the intercity similarity of urban network structures originating in Europe, passed on to cities in the United States and Asia. The socioeconomic development and intercity similarity revealed using our method can be leveraged to understand and transfer insights between cities. It also enables us to address urban policy challenges including network planning in rapidly urbanizing areas and regional inequality.",
        "DOI": "10.1038/s42256-022-00462-y",
        "paper_author": "Xue J.",
        "affiliation_name": "Lyles School of Civil and Construction Engineering",
        "affiliation_city": "West Lafayette",
        "affiliation_country": "United States",
        "affiliation_id": "60032781",
        "affiliation_state": "IN"
    },
    {
        "paper_title": "Emissions from fossil fuels produced on US federal lands and waters present opportunities for climate mitigation",
        "publication": "Climatic Change",
        "citied_by": "5",
        "cover_date": "2022-03-01",
        "Abstract": "Between 2005 and 2019, a quarter of US fossil fuel production came from federal lands and waters. We estimate that the extraction, transportation and combustion of these fuels resulted in emissions equivalent to roughly 1.4 billion metric tons of carbon dioxide equivalent per year. To better understand their future role in the US emissions profile, we use publicly available data and machine learning to model coal, oil and natural gas production on federal lands and waters to 2030, and calculate associated life cycle climate emissions. We estimate that total emissions from fossil fuels produced on federal lands and waters decline 6% below 2019 levels by 2030; and note that absent additional policy, further reductions may be challenging as some of the cheapest fossil fuels occur on federally owned lands and many are effectively subsidized.",
        "DOI": "10.1007/s10584-021-03302-x",
        "paper_author": "Ratledge N.",
        "affiliation_name": "Stanford University",
        "affiliation_city": "Stanford",
        "affiliation_country": "United States",
        "affiliation_id": "60012708",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Reinforcement learning for online optimization of job-shop scheduling in a smart manufacturing factory",
        "publication": "Advances in Mechanical Engineering",
        "citied_by": "16",
        "cover_date": "2022-03-01",
        "Abstract": "The job-shop scheduling problem (JSSP) is a complex combinatorial problem, especially in dynamic environments. Low-volume-high-mix orders contain various design specifications that bring a large number of uncertainties to manufacturing systems. Traditional scheduling methods are limited in handling diverse manufacturing resources in a dynamic environment. In recent years, artificial intelligence (AI) arouses the interests of researchers in solving dynamic scheduling problems. However, it is difficult to optimize the scheduling policies for online decision making while considering multiple objectives. Therefore, this paper proposes a smart scheduler to handle real-time jobs and unexpected events in smart manufacturing factories. New composite reward functions are formulated to improve the decision-making abilities and learning efficiency of the smart scheduler. Based on deep reinforcement learning (RL), the smart scheduler autonomously learns to schedule manufacturing resources in real time and improve its decision-making abilities dynamically. We evaluate and validate the proposed scheduling model with a series of experiments on a smart factory testbed. Experimental results show that the smart scheduler not only achieves good learning and scheduling performances by optimizing the composite reward functions, but also copes with unexpected events (e.g. urgent or simultaneous orders, machine failures) and balances between efficiency and profits.",
        "DOI": "10.1177/16878132221086120",
        "paper_author": "Zhou T.",
        "affiliation_name": "Nanjing University of Aeronautics and Astronautics",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60021666",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "Identifying Country-Level Risk Factors for the Spread of COVID-19 in Europe Using Machine Learning",
        "publication": "Viruses",
        "citied_by": "1",
        "cover_date": "2022-03-01",
        "Abstract": "Coronavirus disease 2019 (COVID-19) has resulted in approximately 5 million deaths around the world with unprecedented consequences in people’s daily routines and in the global economy. Despite vast increases in time and money spent on COVID-19-related research, there is still limited information about the factors at the country level that affected COVID-19 transmission and fatality in EU. The paper focuses on the identification of these risk factors using a machine learning (ML) predictive pipeline and an associated explainability analysis. To achieve this, a hybrid dataset was created employing publicly available sources comprising heterogeneous parameters from the majority of EU countries, e.g., mobility measures, policy responses, vaccinations, and demographics/generic country-level parameters. Data pre-processing and data exploration techniques were initially applied to normalize the available data and decrease the feature dimensionality of the data problem considered. Then, a linear ε-Support Vector Machine (ε-SVM) model was employed to implement the regression task of predicting the number of deaths for each one of the three first pandemic waves (with mean square error of 0.027 for wave 1 and less than 0.02 for waves 2 and 3). Post hoc explainability analysis was finally applied to uncover the rationale behind the decision-making mechanisms of the ML pipeline and thus enhance our understanding with respect to the contribution of the selected country-level parameters to the prediction of COVID-19 deaths in EU.",
        "DOI": "10.3390/v14030625",
        "paper_author": "Moustakidis S.",
        "affiliation_name": "AIDEAS OÜ",
        "affiliation_city": "Tallinn",
        "affiliation_country": "Estonia",
        "affiliation_id": "122579551",
        "affiliation_state": "Harju maakond"
    },
    {
        "paper_title": "Joint Beamforming, Power Allocation, and Splitting Control for SWIPT-Enabled IoT Networks with Deep Reinforcement Learning and Game Theory",
        "publication": "Sensors",
        "citied_by": "22",
        "cover_date": "2022-03-01",
        "Abstract": "Future wireless networks promise immense increases on data rate and energy efficiency while overcoming the difficulties of charging the wireless stations or devices in the Internet of Things (IoT) with the capability of simultaneous wireless information and power transfer (SWIPT). For such networks, jointly optimizing beamforming, power control, and energy harvesting to enhance the communication performance from the base stations (BSs) (or access points (APs)) to the mobile nodes (MNs) served would be a real challenge. In this work, we formulate the joint optimization as a mixed integer nonlinear programming (MINLP) problem, which can be also realized as a complex multiple resource allocation (MRA) optimization problem subject to different allocation constraints. By means of deep reinforcement learning to estimate future rewards of actions based on the reported information from the users served by the networks, we introduce single-layer MRA algorithms based on deep Q-learning (DQN) and deep deterministic policy gradient (DDPG), respectively, as the basis for the downlink wireless transmissions. Moreover, by incorporating the capability of data-driven DQN technique and the strength of noncooperative game theory model, we propose a two-layer iterative approach to resolve the NP-hard MRA problem, which can further improve the communication performance in terms of data rate, energy harvesting, and power consumption. For the two-layer approach, we also introduce a pricing strategy for BSs or APs to determine their power costs on the basis of social utility maximization to control the transmit power. Finally, with the simulated environment based on realistic wireless networks, our numerical results show that the two-layer MRA algorithm proposed can achieve up to 2.3 times higher value than the single-layer counterparts which represent the data-driven deep reinforcement learning-based algorithms extended to resolve the problem, in terms of the utilities designed to reflect the trade-off among the performance metrics considered.",
        "DOI": "10.3390/s22062328",
        "paper_author": "Liu J.",
        "affiliation_name": "Providence University Taiwan",
        "affiliation_city": "Sha-Lu",
        "affiliation_country": "Taiwan",
        "affiliation_id": "60014390",
        "affiliation_state": "Taichung"
    },
    {
        "paper_title": "Implicit GPS-based bicycle route choice model using clustering methods and a LSTM network",
        "publication": "PLoS ONE",
        "citied_by": "6",
        "cover_date": "2022-03-01",
        "Abstract": "Biking is gaining in popularity all around the world as a healthy and environmentally friendly mode of transportation. Urban policies tend to encourage citizens to use bicycles. This can be done by creating new cycling infrastructures, the renovation of old ones or the deployment of bike-sharing systems (BSS). These policies having a cost, understanding and predicting the behavior of cyclists has become a necessity in order to optimize them. Classical methods analyzing cyclists’ route choices use external factors and generated choice sets of paths along with a logit model to create a discrete route choice model. Nevertheless, few studies focus on the predictive capacity that this type of model can offer. In this paper, we developed a prediction-centered bicycle route choice model. Our model is created without using external factors or choice sets of paths as in the more classical methods. The idea of our method is to use deep and machine learning algorithms on GPS tracks. These algorithms learn representations from the data which replace explicit factors. To build the model, we clustered the GPS tracks using DBSCAN. The clusters allow to identify the cyclists’ preferred road segments and are used to create paths using them. A method weighting the road graph weights is developed to create paths passing through the preferred road segments of a given cluster. A LSTM is finally trained in order to retrieve a cluster from a shortest path between an origin/destination pair. Tracks created by our model are more similar to the original GPS tracks than the shortest paths or tracks generated by a prominent path computation service.",
        "DOI": "10.1371/journal.pone.0264196",
        "paper_author": "Magnana L.",
        "affiliation_name": "INSA Lyon",
        "affiliation_city": "Villeurbanne",
        "affiliation_country": "France",
        "affiliation_id": "60016253",
        "affiliation_state": "Auvergne-Rhone-Alpes"
    },
    {
        "paper_title": "A Bayesian Approach to Predict Football Matches with Changed Home Advantage in Spectator-Free Matches after the COVID-19 Break",
        "publication": "Entropy",
        "citied_by": "8",
        "cover_date": "2022-03-01",
        "Abstract": "Since the coronavirus disease 2019 (COVID-19) pandemic, most professional sports events have been held without spectators. It is generally believed that home teams deprived of enthusiastic support from their home fans experience reduced benefits of playing on their home fields, thus becoming less likely to win. This study attempts to confirm if this belief is true in four major European football leagues through statistical analysis. This study proposes a Bayesian hierarchical Poisson model to estimate parameters reflecting the home advantage and the change in such advantage. These parameters are used to improve the performance of machine-learning-based prediction models for football matches played after the COVID-19 break. The study describes the statistical analysis on the impact of the COVID-19 pandemic on football match results in terms of the expected score and goal difference. It also shows that estimated parameters from the proposed model reflect the changed home advantage. Finally, the study verifies that these parameters, when included as additional features, enhance the performance of various football match prediction models. The home advantage in European football matches has changed because of the behind-closed-doors policy implemented due to the COVID-19 pandemic. Using parameters reflecting the pandemic’s impact, it is possible to predict more precise results of spectator-free matches after the COVID-19 break.",
        "DOI": "10.3390/e24030366",
        "paper_author": "Lee J.",
        "affiliation_name": "Sungkyunkwan University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea",
        "affiliation_id": "60007511",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Economic costs of childhood stunting to the private sector in low- and middle-income countries",
        "publication": "eClinicalMedicine",
        "citied_by": "32",
        "cover_date": "2022-03-01",
        "Abstract": "Background: Stunting during childhood has long-term consequences on human capital, including decreased physical growth, and lower educational attainment, cognition, workforce productivity and wages. Previous research has quantified the costs of stunting to national economies however beyond a few single-country datasets there has been a limited number of which have used diverse datasets and have had a dedicated focus on the private sector, which employs nearly 90% of the workforce in many low- and middle-income countries (LMICs). We aimed to examine (i) the impact of childhood stunting on income loss of private sector workforce in LMICs; (ii) to quantify losses in sales to private firms in LMICs due to childhood stunting; and (iii) to estimate potential gains (benefit-cost ratios) if stunting levels are reduced in select high prevalence countries. Methods: This multiple-methods study engaged multi-disciplinary technical advisers, executed several literature reviews, used innovative statistical methods, and implemented health and labor economic models. We analyzed data from seven longitudinal datasets (up to 30+ years of follow-up; 1982–2016; Peru, Ethiopia, India, Vietnam, Philippines, Tanzania, Brazil), 108 private firm datasets (spanning 2008–2020), and many global datasets including Joint Malnutrition Estimates, and World Development Indicators to produce estimates for 120+ LMICs (with estimates up to 2021). We studied the impact of childhood stunting on adult cognition, education, and height as pathways to wages/productivity in adulthood. We employed cloud-based artificial intelligence (AI) platforms, and conducted comparative analyses using three analytic approaches: traditional frequentist statistics, Bayesian inferential statistics and machine learning. We employed labour and health economic models to estimate wage losses to the private sector worker and firm revenue losses due to stunting. We also estimated benefit-cost ratios for countries investing in nutrition-specific interventions to prevent stunting. Findings: Across 95 LMICs, childhood stunting costs the private sector at least US$135.4 billion in sales annually. Firms from countries in Latin America and the Caribbean and East Asia and Pacific regions had the greatest losses. Totals sales losses to the private sector accumulated to 0.01% to 1.2% of national GDP across countries. Sectors most affected by childhood stunting were manufacturing (non-metallic mineral, fabricated metal, other), garments and food sectors. Sale losses were highest for larger sized private firms. Across regions (representing 123 LMICs), US$700 million (Middle East and North Africa) to US$16.5 billion (East Asia and Pacific) monthly income was lost among private sector workers. Investing in stunting reduction interventions yields gains from US$2 to US$81 per $1 invested annually (or 100% to 8000% across countries). Across sectors, the highest returns were in elementary occupations (US$46) and the lowest were among agricultural workers (US$8). By gender, women incurred a higher income penalty from childhood stunting and earned less than men; due to their relatively higher earnings, the returns for investing in stunting reduction were consistently higher for men across most countries studied. Interpretation: Childhood stunting costs the private sector in LMICs billions of dollars in sales and earnings for the workforce annually. Returns to nutrition interventions show that there is an economic case to be made for investing in childhood nutrition, alongside a moral one for both the public and private sector. This research could be used to motivate strong public-private sector partnerships to invest in childhood undernutrition for benefits in the short and long-term.",
        "DOI": "10.1016/j.eclinm.2022.101320",
        "paper_author": "Akseer N.",
        "affiliation_name": "Johns Hopkins Bloomberg School of Public Health",
        "affiliation_city": "Baltimore",
        "affiliation_country": "United States",
        "affiliation_id": "60006183",
        "affiliation_state": "MD"
    },
    {
        "paper_title": "State of the Art of Machine Learning-Enabled Clinical Decision Support in Intensive Care Units: Literature Review",
        "publication": "JMIR Medical Informatics",
        "citied_by": "35",
        "cover_date": "2022-03-01",
        "Abstract": "Background: Modern clinical care in intensive care units is full of rich data, and machine learning has great potential to support clinical decision-making. The development of intelligent machine learning-based clinical decision support systems is facing great opportunities and challenges. Clinical decision support systems may directly help clinicians accurately diagnose, predict outcomes, identify risk events, or decide treatments at the point of care. Objective: We aimed to review the research and application of machine learning-enabled clinical decision support studies in intensive care units to help clinicians, researchers, developers, and policy makers better understand the advantages and limitations of machine learning-supported diagnosis, outcome prediction, risk event identification, and intensive care unit point-of-care recommendations. Methods: We searched papers published in the PubMed database between January 1980 and October 2020. We defined selection criteria to identify papers that focused on machine learning-enabled clinical decision support studies in intensive care units and reviewed the following aspects: research topics, study cohorts, machine learning models, analysis variables, and evaluation metrics. Results: A total of 643 papers were collected, and using our selection criteria, 97 studies were found. Studies were categorized into 4 topics-monitoring, detection, and diagnosis (13/97, 13.4%), early identification of clinical events (32/97, 33.0%), outcome prediction and prognosis assessment (46/97, 47.6%), and treatment decision (6/97, 6.2%). Of the 97 papers, 82 (84.5%) studies used data from adult patients, 9 (9.3%) studies used data from pediatric patients, and 6 (6.2%) studies used data from neonates. We found that 65 (67.0%) studies used data from a single center, and 32 (33.0%) studies used a multicenter data set; 88 (90.7%) studies used supervised learning, 3 (3.1%) studies used unsupervised learning, and 6 (6.2%) studies used reinforcement learning. Clinical variable categories, starting with the most frequently used, were demographic (n=74), laboratory values (n=59), vital signs (n=55), scores (n=48), ventilation parameters (n=43), comorbidities (n=27), medications (n=18), outcome (n=14), fluid balance (n=13), nonmedicine therapy (n=10), symptoms (n=7), and medical history (n=4). The most frequently adopted evaluation metrics for clinical data modeling studies included area under the receiver operating characteristic curve (n=61), sensitivity (n=51), specificity (n=41), accuracy (n=29), and positive predictive value (n=23). Conclusions: Early identification of clinical and outcome prediction and prognosis assessment contributed to approximately 80% of studies included in this review. Using new algorithms to solve intensive care unit clinical problems by developing reinforcement learning, active learning, and time-series analysis methods for clinical decision support will be greater development prospects in the future.",
        "DOI": "10.2196/28781",
        "paper_author": "Hong N.",
        "affiliation_name": "Digital China Health Technologies Co. Ltd.",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "118272936",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Global evidence of expressed sentiment alterations during the COVID-19 pandemic",
        "publication": "Nature Human Behaviour",
        "citied_by": "72",
        "cover_date": "2022-03-01",
        "Abstract": "The COVID-19 pandemic has created unprecedented burdens on people’s physical health and subjective well-being. While countries worldwide have developed platforms to track the evolution of COVID-19 infections and deaths, frequent global measurements of affective states to gauge the emotional impacts of pandemic and related policy interventions remain scarce. Using 654 million geotagged social media posts in over 100 countries, covering 74% of world population, coupled with state-of-the-art natural language processing techniques, we develop a global dataset of expressed sentiment indices to track national- and subnational-level affective states on a daily basis. We present two motivating applications using data from the first wave of COVID-19 (from 1 January to 31 May 2020). First, using regression discontinuity design, we provide consistent evidence that COVID-19 outbreaks caused steep declines in expressed sentiment globally, followed by asymmetric, slower recoveries. Second, applying synthetic control methods, we find moderate to no effects of lockdown policies on expressed sentiment, with large heterogeneity across countries. This study shows how social media data, when coupled with machine learning techniques, can provide real-time measurements of affective states.",
        "DOI": "10.1038/s41562-022-01312-y",
        "paper_author": "Wang J.",
        "affiliation_name": "Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60031150",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Sampling Rate Decay in Hindsight Experience Replay for Robot Control",
        "publication": "IEEE Transactions on Cybernetics",
        "citied_by": "25",
        "cover_date": "2022-03-01",
        "Abstract": "Training agents via deep reinforcement learning with sparse rewards for robotic control tasks in vast state space are a big challenge, due to the rareness of successful experience. To solve this problem, recent breakthrough methods, the hindsight experience replay (HER) and aggressive rewards to counter bias in HER (ARCHER), use unsuccessful experiences and consider them as successful experiences achieving different goals, for example, hindsight experiences. According to these methods, hindsight experience is used at a fixed sampling rate during training. However, this usage of hindsight experience introduces bias, due to a distinct optimal policy, and does not allow the hindsight experience to take variable importance at different stages of training. In this article, we investigate the impact of a variable sampling rate, representing the variable rate of hindsight experience, on training performance and propose a sampling rate decay strategy that decreases the number of hindsight experiences as training proceeds. The proposed method is validated with three robotic control tasks included in the OpenAI Gym suite. The experimental results demonstrate that the proposed method achieves improved training performance and increased convergence speed over the HER and ARCHER with two of the three tasks and comparable training performance and convergence speed with the other one.",
        "DOI": "10.1109/TCYB.2020.2990722",
        "paper_author": "Vecchietti L.F.",
        "affiliation_name": "Korea Advanced Institute of Science and Technology",
        "affiliation_city": "Daejeon",
        "affiliation_country": "South Korea",
        "affiliation_id": "60032144",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Use of Machine Learning to Predict the Occurrence of Deaths in the Departments Most Affected by Covid-19 in Peru",
        "publication": "International Journal of Engineering Trends and Technology",
        "citied_by": "2",
        "cover_date": "2022-03-01",
        "Abstract": "This article shows the use of machine learning to predict the occurrence of deaths in the areas most affected by covid-19 in Peru, where the records of deaths during the pandemic are found reflecting the damage caused by this pandemic, according to a MINSA report in a standard format for analysis that contains all the detailed information of each person. The machine learning procedure is a method of data analysis that automates the construction of analytical models in which we will apply the decision tree where we will use the Python programming language to make the predictions of the deaths caused by covid-19 in the departments, and it will also help us to train the model for greater accuracy in obtaining expected results. In such a way, it can elaborate scenario predictions or initiate operations that are the solution for a specific task. As a case study, it was carried out in the 25 departments of Peru to analyze the departments with the highest mortality rates in our country. As a result of the study were that the departments of Lima, Piura, Huánuco, Ica have the highest rate of deaths by covid-19; this may be due to the biosecurity measures and social distancing; it is worth mentioning that to date they are the departments that have had more policy interventions in recent years. The results of this study may help the authorities to create prevention and sanitary control strategies by implementing rigorous measures in Peru.",
        "DOI": "10.14445/22315381/IJETT-V70I3P206",
        "paper_author": "Ortega-Espinoza E.",
        "affiliation_name": "Universidad de Ciencias y Humanidades",
        "affiliation_city": "Los Olivos",
        "affiliation_country": "Peru",
        "affiliation_id": "60110778",
        "affiliation_state": "Lima"
    },
    {
        "paper_title": "SIMLR: Machine Learning inside the SIR Model for COVID-19 Forecasting",
        "publication": "Forecasting",
        "citied_by": "23",
        "cover_date": "2022-03-01",
        "Abstract": "Accurate forecasts of the number of newly infected people during an epidemic are critical for making effective timely decisions. This paper addresses this challenge using the SIMLR model, which incorporates machine learning (ML) into the epidemiological SIR model. For each region, SIMLR tracks the changes in the policies implemented at the government level, which it uses to estimate the time-varying parameters of an SIR model for forecasting the number of new infections one to four weeks in advance. It also forecasts the probability of changes in those government policies at each of these future times, which is essential for the longer-range forecasts. We applied SIMLR to data from in Canada and the United States, and show that its mean average percentage error is as good as state-of-the-art forecasting models, with the added advantage of being an interpretable model. We expect that this approach will be useful not only for forecasting COVID-19 infections, but also in predicting the evolution of other infectious diseases.",
        "DOI": "10.3390/forecast4010005",
        "paper_author": "Vega R.",
        "affiliation_name": "University of Alberta",
        "affiliation_city": "Edmonton",
        "affiliation_country": "Canada",
        "affiliation_id": "60030835",
        "affiliation_state": "AB"
    },
    {
        "paper_title": "Recent advances in statistical methodologies in evaluating program for high-dimensional data",
        "publication": "Applied Mathematics",
        "citied_by": "2",
        "cover_date": "2022-03-01",
        "Abstract": "The era of big data brings opportunities and challenges to developing new statistical methods and models to evaluate social programs or economic policies or interventions. This paper provides a comprehensive review on some recent advances in statistical methodologies and models to evaluate programs with high-dimensional data. In particular, four kinds of methods for making valid statistical inferences for treatment effects in high dimensions are addressed. The first one is the so-called doubly robust type estimation, which models the outcome regression and propensity score functions simultaneously. The second one is the covariate balance method to construct the treatment effect estimators. The third one is the sufficient dimension reduction approach for causal inferences. The last one is the machine learning procedure directly or indirectly to make statistical inferences to treatment effect. In such a way, some of these methods and models are closely related to the de-biased Lasso type methods for the regression model with high dimensions in the statistical literature. Finally, some future research topics are also discussed.",
        "DOI": "10.1007/s11766-022-4489-3",
        "paper_author": "Zhan M.f.",
        "affiliation_name": "Xiamen University",
        "affiliation_city": "Xiamen",
        "affiliation_country": "China",
        "affiliation_id": "60018205",
        "affiliation_state": "Fujian"
    },
    {
        "paper_title": "Hybrid Differential Evolution-Based Regression Tree Model for Predicting Downstream Dam Hazard Potential",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "3",
        "cover_date": "2022-03-01",
        "Abstract": "There are a large number of dams throughout the United States, and a considerable portion of them are categorized as having high hazard potential. This state of affairs constitutes a challenge, especially when coupled with their rapid deterioration. As such, this research paper proposes an optimized data-driven model for the fast and efficient prediction of dam hazard potential. The proposed model is envisioned on two main components, namely model development and model assessment. In the first component, a hybridization of the differential evolution algorithm and regression tree to forecast downstream dam hazard potential is proposed. In this context, the differential evolution (DE) algorithm is deployed to: (1) automatically retrieve the optimal set of input features affecting dam hazard potential; and (2) amplify the search mechanism of regression tree (REGT) through optimizing its hyper parameters. As for the second component, the developed DE-REGT model is validated using four folds of comparative assessments to evaluate its prediction capabilities. In the first fold, the developed DE-REGT model is trialed against nine highly regarded machine learning and deep learning models. The second fold is designated to structure, an integrative ranking of the investigated data-driven models, counting on their scores in the performance evaluation metrics. The third fold is used to study the effectiveness of using differential evolution for the hyper parameter optimization of regression tree. The fourth fold aims at testing the usefulness of using differential evolution as a feature extractor algorithm. Performance comparative analysis demonstrated that the developed DE-REGT model outperformed the remainder of the data-driven models. It accomplished mean absolute percentage error, relative absolute error, mean absolute error, root squared error, root mean squared error and a Nash–Sutcliffe efficiency of 9.62%, 0.27, 0.17, 0.31, 0.41 and 0.74, respectively. Results also revealed that the developed model managed to perform better than other meta-heuristic-based regression tree models and classical feature extraction algorithms, exemplifying the appropriateness of using differential evolution for hyper parameter optimization and feature extraction. It can be argued that the developed model could assist policy makers in the prioritization of their maintenance management plans and reduce impairments caused by the failure or misoperation of dams.",
        "DOI": "10.3390/su14053013",
        "paper_author": "Abdelkader E.M.",
        "affiliation_name": "Faculty of Engineering",
        "affiliation_city": "Cairo",
        "affiliation_country": "Egypt",
        "affiliation_id": "60002575",
        "affiliation_state": "Cairo"
    },
    {
        "paper_title": "Examining the Intersection between Gender, Community Health Workers, and Vector Control Policies: A Text Mining Literature Review",
        "publication": "American Journal of Tropical Medicine and Hygiene",
        "citied_by": "1",
        "cover_date": "2022-03-01",
        "Abstract": "Gender intersects with healthcare systems; this is equally true for arboviral vector control efforts. However, there is as yet no comprehensive analysis as to how vector control is gendered. Hence, our objective is to provide the first thematic scoping and spatial distribution of the literature on gender, community health workers, and vector control. The authors use a systematic review approach to collect the academic literature on gender, community health workers, and vector control in Web of Science, Scopus, and PubMed (7,367 articles). After applying the exclusion criteria, 2,812 articles were analyzed using machine learning techniques: text mining and quantitative text analysis. The authors use topic modeling to assess the thematic scope of the literature and analyze the spatial distribution of themes. Our results show that the literature’s spatial scope is strongly represented by the global south as research was conducted mainly in Latin America, Africa, and Asia, places with greater incidence of vector-borne disease and with health systems, which incorporate community healthcare workers. However, there are significant spatial heterogeneities in where and how research is conducted. The topic analysis reveals that the literature predominantly considers issues of sex (e.g., pregnancy) and gender as it relates motherhood. Gendered considerations occur upon implementation of vector control policies, rather than being mainstreamed into their development and delivery. There is a need to deepen the analysis to allow for gendered aspects to be understood beyond binary sex differences and/or reproductive health.",
        "DOI": "10.4269/ajtmh.21-0619",
        "paper_author": "De Menezes A.",
        "affiliation_name": "London School of Economics and Political Science",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60003059",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "PSTO: Learning Energy-Efficient Locomotion for Quadruped Robots",
        "publication": "Machines",
        "citied_by": "5",
        "cover_date": "2022-03-01",
        "Abstract": "Energy efficiency is critical for the locomotion of quadruped robots. However, energy efficiency values found in simulations do not transfer adequately to the real world. To address this issue, we present a novel method, named Policy Search Transfer Optimization (PSTO), which combines deep reinforcement learning and optimization to create energy-efficient locomotion for quadruped robots in the real world. The deep reinforcement learning and policy search process are performed by the TD3 algorithm and the policy is transferred to the open-loop control trajectory further optimized by numerical methods, and conducted on the robot in the real world. In order to ensure the high uniformity of the simulation results and the behavior of the hardware platform, we introduce and validate the accurate model in simulation including consistent size and fine-tuning parameters. We then validate those results with real-world experiments on the quadruped robot Ant by executing dynamic walking gaits with different leg lengths and numbers of amplifications. We analyze the results and show that our methods can outperform the control method provided by the state-of-the-art policy search algorithm TD3 and sinusoid function on both energy efficiency and speed.",
        "DOI": "10.3390/machines10030185",
        "paper_author": "Zhu W.",
        "affiliation_name": "ShanghaiTech University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60105232",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Exploring remedies for defective artificial intelligence aids in clinical decision-making in post-Brexit England and Wales",
        "publication": "Medical Law International",
        "citied_by": "4",
        "cover_date": "2022-03-01",
        "Abstract": "Artificially intelligent systems (AISs) are in development to aid the clinicians and patients of the United Kingdom’s National Health Service. We assess the statutory requirements for product liability claims against producers of defective AISs in clinical use and set out the criteria for bringing a successful claim against a producer to the courts of England and Wales. We argue that the mismatch between product liability and safety regulation leaves patients, and consumers more generally, without an adequate remedy for the consequences of AIS defects. We also discuss the intertwinement of the consumer Protection Act 1987 and the Medical Devices Regulations 2002. Recent developments such as United Kingdom’s withdrawal from the European Union and the updated Medicines and Medical Devices Act 2021 are discussed. In addition, we offer novel discussion regarding the tort of ‘breach of statutory duty’ as provided for by the Medicines and Medical Devices Act 2021.",
        "DOI": "10.1177/09685332221076124",
        "paper_author": "Smith H.",
        "affiliation_name": "University of Bristol",
        "affiliation_city": "Bristol",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60020650",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Analysis of Job Failure and Prediction Model for Cloud Computing Using Machine Learning",
        "publication": "Sensors",
        "citied_by": "13",
        "cover_date": "2022-03-01",
        "Abstract": "Modern applications, such as smart cities, home automation, and eHealth, demand a new approach to improve cloud application dependability and availability. Due to the enormous scope and diversity of the cloud environment, most cloud services, including hardware and software, have encountered failures. In this study, we first analyze and characterize the behaviour of failed and completed jobs using publicly accessible traces. We have designed and developed a failure prediction model to determine failed jobs before they occur. The proposed model aims to enhance resource consumption and cloud application efficiency. Based on three publicly available traces: the Google cluster, Mustang, and Trinity, we evaluate the proposed model. In addition, the traces were also subjected to various machine learning models to find the most accurate one. Our results indicate a significant correlation between unsuccessful tasks and requested resources. The evaluation results also revealed that our model has high precision, recall, and F1-score. Several solutions, such as predicting job failure, developing scheduling algorithms, changing priority policies, or limiting re-submission of tasks, can improve the reliability and availability of cloud services.",
        "DOI": "10.3390/s22052035",
        "paper_author": "Jassas M.S.",
        "affiliation_name": "Ontario Tech University",
        "affiliation_city": "Oshawa",
        "affiliation_country": "Canada",
        "affiliation_id": "60002146",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "Monitoring nurturing care environments for early childhood from the national to the municipal level",
        "publication": "Maternal and Child Nutrition",
        "citied_by": "0",
        "cover_date": "2022-03-01",
        "Abstract": "NA",
        "DOI": "10.1111/mcn.13327",
        "paper_author": "Pérez-Escamilla R.",
        "affiliation_name": "Yale University",
        "affiliation_city": "New Haven",
        "affiliation_country": "United States",
        "affiliation_id": "60005455",
        "affiliation_state": "CT"
    },
    {
        "paper_title": "COVID-19 Vaccination-Related Sentiments Analysis: A Case Study Using Worldwide Twitter Dataset",
        "publication": "Healthcare (Switzerland)",
        "citied_by": "31",
        "cover_date": "2022-03-01",
        "Abstract": "COVID-19 pandemic has caused a global health crisis, resulting in endless efforts to reduce infections, fatalities, and therapies to mitigate its after-effects. Currently, large and fast-paced vaccination campaigns are in the process to reduce COVID-19 infection and fatality risks. Despite recommendations from governments and medical experts, people show conceptions and perceptions regarding vaccination risks and share their views on social media platforms. Such opinions can be analyzed to determine social trends and devise policies to increase vaccination acceptance. In this regard, this study proposes a methodology for analyzing the global perceptions and perspectives towards COVID-19 vaccination using a worldwide Twitter dataset. The study relies on two techniques to analyze the sentiments: natural language processing and machine learning. To evaluate the performance of the different lexicon-based methods, different machine and deep learning models are studied. In addition, for sentiment classification, the proposed ensemble model named long short-term memory-gated recurrent neural network (LSTM-GRNN) is a combination of LSTM, gated recurrent unit, and recurrent neural networks. Results suggest that the TextBlob shows better results as compared to VADER and AFINN. The proposed LSTM-GRNN shows superior performance with a 95% accuracy and outperforms both machine and deep learning models. Performance analysis with state-of-the-art models proves the significance of the LSTM-GRNN for sentiment analysis.",
        "DOI": "10.3390/healthcare10030411",
        "paper_author": "Reshi A.A.",
        "affiliation_name": "Taibah University",
        "affiliation_city": "Medina",
        "affiliation_country": "Saudi Arabia",
        "affiliation_id": "60008920",
        "affiliation_state": "Al Madinah al Munawwarah"
    },
    {
        "paper_title": "Ecological Quality Response to Multi-Scenario Land-Use Changes in the Heihe River Basin",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "8",
        "cover_date": "2022-03-01",
        "Abstract": "To investigate the spatial-temporal effects of land-use changes on ecological quality and future trends, an integrated framework combining the Dyna-CLUE model and the remote sensing ecological index (RSEI) was developed. Land-use changes from 2000 to 2035 were simulated and projected under the current trend scenario (CTS), economic development scenario (EDS) and ecological protection scenario (EPS) in the Heihe River Basin, while the RSEI was predicted using the elastic net regression (machine learning method); finally, the predicted results were synthesized and analyzed. The results showed that forest, grassland and water were positively correlated with ecological quality, with the green space coverage under the CTS, EPS and EDS accounting for 34.15%, 70.65% and 34.72% of the total transferred land area, respectively. The increase in the area of build-up land and unutilized land was detrimental to ecological quality, with the area of building land in the EDS being 1.75 times larger than in the year 2000. The EDS contributes to the sustainable development of the upstream area and the EPS is more conducive to the midstream and downstream areas by limiting the expansion of build-up land and by developing unutilized land in a limited way to increase the area of green space after reconciling economic conditions. Projection results promote the rational allocation of various land-use types in the future (semi) arid region, such as artificial forestation, unutilized land development and restriction of urban expansion, and also lay the foundation for the formulation of policies such as water allocation and ecological protection to facilitate the sustainable development of regional society, economy and ecology.",
        "DOI": "10.3390/su14052716",
        "paper_author": "Wang S.",
        "affiliation_name": "Northwest Institute of Eco-Environment and Resources",
        "affiliation_city": "Lanzhou",
        "affiliation_country": "China",
        "affiliation_id": "60272995",
        "affiliation_state": "Gansu"
    },
    {
        "paper_title": "Stakeholders’ power in the networking structuration processes of the urban resilience concept in Habitat III agenda (2012-2016)",
        "publication": "Geography and Sustainability",
        "citied_by": "1",
        "cover_date": "2022-03-01",
        "Abstract": "The urban resilience concept was introduced in 2016 as a key concept in the Habitat III New Urban Agenda for the next 20 years. We wonder how this urban resilience concept was elaborated and who influenced it the most? The preparatory events structured several stakeholders’ networks. The relations between stakeholders allowed the flow of ideas in the consultation and production process. Some influential stakeholders strongly oriented definition of urban resilience concepts by taking power in the networking process of the consecutive meetings. The paper analyzes the network of stakeholders/concepts, during the building process between 2012 and 2016 (5,539 discourses from 290 stakeholders, in 357 events). The application of textual mining and machine learning topic modelling algorithm exposed the structure of the principal topics for building the concept of urban resilience, and presented how relations of main stakeholders with funders was crucial for the investment in policy interventions. Therefore, we underlined for the first time in an empirical way, different kinds of actors’ power in the construction process that supported the Habitat III resilience concept. We demonstrated how far some official stakeholders, but also external and private ones, oriented the construction of ideologies to validate the knowledge that supported the related actions in laboratory cities.",
        "DOI": "10.1016/j.geosus.2022.02.001",
        "paper_author": "Mariño D.",
        "affiliation_name": "Université de Lausanne (UNIL)",
        "affiliation_city": "Lausanne",
        "affiliation_country": "Switzerland",
        "affiliation_id": "60000239",
        "affiliation_state": "VD"
    },
    {
        "paper_title": "Temperature Control for Automated Tape Laying with Infrared Heaters Based on Reinforcement Learning",
        "publication": "Machines",
        "citied_by": "9",
        "cover_date": "2022-03-01",
        "Abstract": "The use of fiber-reinforced lightweight materials in the field of electromobility offers great opportunities to increase the range of electric vehicles and also enhance the functionality of the components themselves. In order to meet the demand for a high number of variants, flexible production technologies are required which can quickly adapt to different component variants and thereby avoid long setup times of the required production equipment. By applying the formflexible process of automated tape laying (ATL), it is possible to build lightweight components in a variant-flexible way. Unidirectional (UD) tapes are often used to build up lightweight structures according to a predefined load path. However, the UD tape which is used to build the components is particularly sensitive to temperature fluctuations due to its low thickness. Temperature fluctuations within the production sites as well as the warming of the tape layer and the deposit surface over longer process times have an impact on the heat flow which is infused to the tape and make an adaptive control of the tape heating indispensable. At present, several model-based control strategies are available. However, these strategies require a comprehensive understanding of the ATL system and its environment and are therefore difficult to design. With the possibility of model-free reinforcement learning, it is possible to build a temperature control system that learns the common dependencies of both the process being used and its operating environment, without the need to rely on a complete understanding of the physical interrelationships. In this paper, a reinforcement learning approach based on the deep deterministic policy gradient (DDPG) algorithm is presented, with the aim to control the temperature of an ATL endeffector based on infrared emitters. The algorithm was adapted to the thermal inertia of the system and trained in a real process environment. With only a small amount of training data, the trained DDPG agent was able to reliably maintain the ATL process temperatures within a specified tolerance range. By applying this technique, UD tape can be deposited at a consistent process temperature over longer process times without the need for a cooling system. Reducing process complexity can help to increase the prevalence of lightweight components and thus contribute to lower energy consumption of electric vehicles.",
        "DOI": "10.3390/machines10030164",
        "paper_author": "Römer M.",
        "affiliation_name": "Technische Universität Braunschweig",
        "affiliation_city": "Braunschweig",
        "affiliation_country": "Germany",
        "affiliation_id": "60007902",
        "affiliation_state": "Niedersachsen"
    },
    {
        "paper_title": "Semi-Automatic Systematic Literature Reviews and Information Extraction of COVID-19 Scientific Evidence: Description and Preliminary Results of the COKE Project",
        "publication": "Information (Switzerland)",
        "citied_by": "8",
        "cover_date": "2022-03-01",
        "Abstract": "The COVID-19 pandemic highlighted the importance of validated and updated scientific information to help policy makers, healthcare professionals, and the public. The speed in disseminating reliable information and the subsequent guidelines and policy implementation are also essential to save as many lives as possible. Trustworthy guidelines should be based on a systematic evidence review which uses reproducible analytical methods to collect secondary data and analyse them. However, the guidelines’ drafting process is time consuming and requires a great deal of resources. This paper aims to highlight the importance of accelerating and streamlining the extraction and synthesis of scientific evidence, specifically within the systematic review process. To do so, this paper describes the COKE (COVID-19 Knowledge Extraction framework for next generation discovery science) Project, which involves the use of machine reading and deep learning to design and implement a semi-automated system that supports and enhances the systematic literature review and guideline drafting processes. Specifically, we propose a framework for aiding in the literature selection and navigation process that employs natural language processing and clustering techniques for selecting and organizing the literature for human consultation, according to PICO (Population/Problem, Intervention, Comparison, and Outcome) elements. We show some preliminary results of the automatic classification of sentences on a dataset of abstracts related to COVID-19.",
        "DOI": "10.3390/info13030117",
        "paper_author": "Golinelli D.",
        "affiliation_name": "Università degli Studi di Bologna, Facoltà di Medicina e Chirurgia",
        "affiliation_city": "Bologna",
        "affiliation_country": "Italy",
        "affiliation_id": "60013883",
        "affiliation_state": "BO"
    },
    {
        "paper_title": "Toward a Reinforcement Learning Environment Toolbox for Intelligent Electric Motor Control",
        "publication": "IEEE Transactions on Neural Networks and Learning Systems",
        "citied_by": "36",
        "cover_date": "2022-03-01",
        "Abstract": "Electric motors are used in many applications, and their efficiency is strongly dependent on their control. Among others, linear feedback approaches or model predictive control methods are well known in the scientific literature and industrial practice. A novel approach is to use reinforcement learning (RL) to have an agent learn electric drive control from scratch merely by interacting with a suitable control environment. RL achieved remarkable results with superhuman performance in many games (e.g., Atari classics or Go) and also becomes more popular in control tasks, such as cart-pole or swinging pendulum benchmarks. In this work, the open-source Python package gym-electric-motor (GEM) is developed for ease of training of RL-agents for electric motor control. Furthermore, this package can be used to compare the trained agents with other state-of-the-art control approaches. It is based on the OpenAI Gym framework that provides a widely used interface for the evaluation of RL-agents. The package covers different dc and three-phase motor variants, as well as different power electronic converters and mechanical load models. Due to the modular setup of the proposed toolbox, additional motor, load, and power electronic devices can be easily extended in the future. Furthermore, different secondary effects, such as converter interlocking time or noise, are considered. An intelligent controller example based on the deep deterministic policy gradient algorithm that controls a series dc motor is presented and compared to a cascaded proportional-integral controller as a baseline for future research. Here, safety requirements are particularly highlighted as an important constraint for data-driven control algorithms applied to electric energy systems. Fellow researchers are encouraged to use the GEM framework in their RL investigations or contribute to the functional scope (e.g., further motor types) of the package.",
        "DOI": "10.1109/TNNLS.2020.3029573",
        "paper_author": "Traue A.",
        "affiliation_name": "Paderborn University",
        "affiliation_city": "Paderborn",
        "affiliation_country": "Germany",
        "affiliation_id": "60020238",
        "affiliation_state": "Nordrhein-Westfalen"
    },
    {
        "paper_title": "The Contribution of Data-Driven Technologies in Achieving the Sustainable Development Goals",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "83",
        "cover_date": "2022-03-01",
        "Abstract": "The United Nations’ Sustainable Development Goals (SDGs) set out to improve the quality of life of people in developed, emerging, and developing countries by covering social and economic aspects, with a focus on environmental sustainability. At the same time, data-driven technologies influence our lives in all areas and have caused fundamental economical and societal changes. This study presents a comprehensive literature review on how data-driven approaches have enabled or inhibited the successful achievement of the 17 SDGs to date. Our findings show that data-driven analytics and tools contribute to achieving the 17 SDGs, e.g., by making information more reliable, supporting better-informed decision-making, implementing data-based policies, prioritizing actions, and optimizing the allocation of resources. Based on a qualitative content analysis, results were aggregated into a conceptual framework, including the following categories: (1) uses of data-driven methods (e.g., monitoring, measurement, mapping or modeling, forecasting, risk assessment, and planning purposes), (2) resulting positive effects, (3) arising challenges, and (4) recommendations for action to overcome these challenges. Despite positive effects and versatile applications, problems such as data gaps, data biases, high energy consumption of computational resources, ethical concerns, privacy, ownership, and security issues stand in the way of achieving the 17 SDGs.",
        "DOI": "10.3390/su14052497",
        "paper_author": "Bachmann N.",
        "affiliation_name": "University of Applied Sciences Upper Austria, School of Management",
        "affiliation_city": "Steyr",
        "affiliation_country": "Austria",
        "affiliation_id": "60032193",
        "affiliation_state": "Upper Austria"
    },
    {
        "paper_title": "Predicting Gasoline Vehicle Fuel Consumption in Energy and Environmental Impact Based on Machine Learning and Multidimensional Big Data",
        "publication": "Energies",
        "citied_by": "12",
        "cover_date": "2022-03-01",
        "Abstract": "The underestimation of fuel consumption impacts various aspects. In the vehicle market, manufacturers often advertise fuel economy for marketing. In fact, the fuel consumption reference value provided by the manufacturer is quite different from the real-world fuel consumption of the vehicles. The divergence between reference fuel consumption and real-world fuel consumption also has negative effect on the aspects of policy and environment. In order to effectively promote the sustainable development of transport, it is urged to recognize the real-world fuel consumption of vehicles. The gaps in previous studies includes small sample size, single data dimension, and lack of feature weight evaluation. To fill the research gap, in this study, we conduct a comparative analysis through building five regression models to forecast the real-world fuel consumption rate of light-duty gasoline vehicles in China based on big data from the perspectives of vehicle factors, environment factors, and driving behavior factors. Results show that the random forest regression model performs best among the five candidate models, with a mean absolute error of 0.630 L/100 km, a mean absolute percentage error of 7.5%, a mean squared error of 0.805, an R squared of 0.776, and a 10-fold cross-validation score of 0.791. Further, we capture the most important features affecting fuel consumption among the 25 factors from the above three perspectives. According to the relative weight of each factor in the most optimal model, the three most important factors are brake and accelerator habits, engine power, and the fuel economy consciousness of vehicle owners in sequence.",
        "DOI": "10.3390/en15051602",
        "paper_author": "Yang Y.",
        "affiliation_name": "Beijing University of Posts and Telecommunications",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60016930",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Widespread use of National Academies consensus reports by the American public",
        "publication": "Proceedings of the National Academy of Sciences of the United States of America",
        "citied_by": "6",
        "cover_date": "2022-03-01",
        "Abstract": "In seeking to understand how to protect the public information sphere from corruption, researchers understandably focus on dysfunction. However, parts of the public information ecosystem function very well, and understanding this as well will help in protecting and developing existing strengths. Here, we address this gap, focusing on public engagement with high-quality science-based information, consensus reports of the National Academies of Science, Engineering, and Medicine (NASEM). Attending to public use is important to justify public investment in producing and making freely available high-quality, scientifically based reports. We deploy Bidirectional Encoder Representations from Transformers (BERT), a high-performing, supervised machine learning model, to classify 1.6 million comments left by US downloaders of National Academies reports responding to a prompt asking how they intended to use the report. The results provide detailed, nationwide evidence of how the public uses open access scientifically based information. We find half of reported use to be academic—research, teaching, or studying. The other half reveals adults across the country seeking the highest-quality information to improve how they do their job, to help family members, to satisfy their curiosity, and to learn. Our results establish the existence of demand for high-quality information by the public and that such knowledge is widely deployed to improve provision of services. Knowing the importance of such information, policy makers can be encouraged to protect it.",
        "DOI": "10.1073/pnas.2107760119",
        "paper_author": "Hicks D.",
        "affiliation_name": "Georgia Institute of Technology",
        "affiliation_city": "Atlanta",
        "affiliation_country": "United States",
        "affiliation_id": "60019647",
        "affiliation_state": "GA"
    },
    {
        "paper_title": "The Impact of COVID-19 on Airfares—A Machine Learning Counterfactual Analysis",
        "publication": "Econometrics",
        "citied_by": "7",
        "cover_date": "2022-03-01",
        "Abstract": "This paper studies the performance of machine learning predictions for the counterfactual analysis of air transport. It is motivated by the dynamic and universally regulated international air transport market, where ex post policy evaluations usually lack counterfactual control scenarios. As an empirical example, this paper studies the impact of the COVID-19 pandemic on airfares in 2020 as the difference between predicted and actual airfares. Airfares are important from a policy makers’ perspective, as air transport is crucial for mobility. From a methodological point of view, airfares are also of particular interest given their dynamic character, which makes them challenging for prediction. This paper adopts a novel multi-step prediction technique with walk-forward validation to increase the transparency of the model’s predictive quality. For the analysis, the universe of worldwide airline bookings is combined with detailed airline information. The results show that machine learning with walk-forward validation is powerful for the counterfactual analysis of airfares.",
        "DOI": "10.3390/econometrics10010008",
        "paper_author": "Wozny F.",
        "affiliation_name": "Deutsches Zentrum für Luft- und Raumfahrt (DLR)",
        "affiliation_city": "Koln",
        "affiliation_country": "Germany",
        "affiliation_id": "60007798",
        "affiliation_state": "Nordrhein-Westfalen"
    },
    {
        "paper_title": "Trends in volcano seismology: 2010 to 2020 and beyond",
        "publication": "Bulletin of Volcanology",
        "citied_by": "20",
        "cover_date": "2022-03-01",
        "Abstract": "Volcano seismology has been fundamental to our current understanding of crustal magma migration and eruption. The increasing availability of portable seismic networks with the creative use of seismic sources and ambient noise has led to a better understanding of the volcanic structure of many volcanoes and is producing increasingly detailed images of the volcanic subsurface. The past decade (2010-2020) has seen advances in our understanding of seismic sources under and surrounding volcanoes through precise locations, and through analysis of source mechanisms from seismic signals that are more varied and smaller in magnitude, reaching beyond traditional techniques. In tandem with continued research on fundamental physics-based understanding of volcano-seismic sources, new advances in computational analyses including machine learning methods will push our understanding of volcanic processes into the future. Incorporation of multidisciplinary geophysical observations (especially infrasound) has become commonplace, and our understanding of infrasound propagation and sources will feed back into our ability to monitor ongoing eruptions and surficial mass movements. Open-source codes will permit widespread evaluation and adoption of new methodologies for volcano-seismic analysis and inversion. Combined with quantitative and conceptual source models using improved structural constraints, these new methodologies will better characterize the range of volcano-seismic signal evolution scenarios and hold promise for creating better short-term forecasts. Finally, permanent instrumentation is available on an expanding range of volcanoes, and open data policies are increasingly making these data available to the scientific community in near real time.",
        "DOI": "10.1007/s00445-022-01530-2",
        "paper_author": "Thelen W.A.",
        "affiliation_name": "USGS Cascades Volcano Observatory",
        "affiliation_city": "Vancouver",
        "affiliation_country": "United States",
        "affiliation_id": "60030877",
        "affiliation_state": "WA"
    },
    {
        "paper_title": "Health Services Research in Anesthesia: A Brief Overview of Common Methodologies",
        "publication": "Anesthesia and Analgesia",
        "citied_by": "4",
        "cover_date": "2022-03-01",
        "Abstract": "The use of large data sources such as registries and claims-based data sets to perform health services research in anesthesia has increased considerably, ultimately informing clinical decisions, supporting evaluation of policy or intervention changes, and guiding further research. These observational data sources come with limitations that must be addressed to effectively examine all aspects of health care services and generate new individual- and population-level knowledge. Several statistical methods are growing in popularity to address these limitations, with the goal of mitigating confounding and other biases. In this article, we provide a brief overview of common statistical methods used in health services research when using observational data sources, guidance on their interpretation, and examples of how they have been applied to anesthesia-related health services research. Methods described involve regression, propensity scoring, instrumental variables, difference-in-differences, interrupted time series, and machine learning.",
        "DOI": "10.1213/ANE.0000000000005884",
        "paper_author": "Illescas A.",
        "affiliation_name": "Hospital for Special Surgery - New York",
        "affiliation_city": "New York",
        "affiliation_country": "United States",
        "affiliation_id": "60022875",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Selecting Data Analytic and Modeling Methods to Support Air Pollution and Environmental Justice Investigations: A Critical Review and Guidance Framework",
        "publication": "Environmental Science and Technology",
        "citied_by": "41",
        "cover_date": "2022-03-01",
        "Abstract": "Given the serious adverse health effects associated with many pollutants, and the inequitable distribution of these effects between socioeconomic groups, air pollution is often a focus of environmental justice (EJ) research. However, EJ analyses that aim to illuminate whether and how air pollution hazards are inequitably distributed may present a unique set of requirements for estimating pollutant concentrations compared to other air quality applications. Here, we perform a scoping review of the range of data analytic and modeling methods applied in past studies of air pollution and environmental injustice and develop a guidance framework for selecting between them given the purpose of analysis, users, and resources available. We include proxy, monitor-based, statistical, and process-based methods. Upon critically synthesizing the literature, we identify four main dimensions to inform method selection: accuracy, interpretability, spatiotemporal features of the method, and usability of the method. We illustrate the guidance framework with case studies from the literature. Future research in this area includes an exploration of increasing data availability, advanced statistical methods, and the importance of science-based policy.",
        "DOI": "10.1021/acs.est.1c01739",
        "paper_author": "Gardner-Frolick R.",
        "affiliation_name": "The University of British Columbia",
        "affiliation_city": "Vancouver",
        "affiliation_country": "Canada",
        "affiliation_id": "60010365",
        "affiliation_state": "BC"
    },
    {
        "paper_title": "Automated Annotations for AI Data and Model Transparency",
        "publication": "Journal of Data and Information Quality",
        "citied_by": "2",
        "cover_date": "2022-03-01",
        "Abstract": "The data and Artificial Intelligence revolution has had a massive impact on enterprises, governments, and society alike. It is fueled by two key factors. First, data have become increasingly abundant and are often available openly. Enterprises have more data than they can process. Governments are spearheading open data initiatives by setting up data portals such as data.gov and releasing large amounts of data to the public. Second, AI engineering development is becoming increasingly democratized. Open source frameworks have enabled even an individual developer to engineer sophisticated AI systems. But with such ease of use comes the potential for irresponsible use of data.Ensuring that AI systems adhere to a set of ethical principles is one of the major problems of our age. We believe that data and model transparency has a key role to play in mitigating the deleterious effects of AI systems. In this article, we describe a framework to synthesize ideas from various domains such as data transparency, data quality, data governance among others to tackle this problem. Specifically, we advocate an approach based on automated annotations (of both data and the AI model), which has a number of appealing properties. The annotations could be used by enterprises to get visibility of potential issues, prepare data transparency reports, create and ensure policy compliance, and evaluate the readiness of data for diverse downstream AI applications. We propose a model architecture and enumerate its key components that could achieve these requirements. Finally, we describe a number of interesting challenges and opportunities.",
        "DOI": "10.1145/3460000",
        "paper_author": "Thirumuruganathan S.",
        "affiliation_name": "Qatar Computing Research Institute",
        "affiliation_city": "Doha",
        "affiliation_country": "Qatar",
        "affiliation_id": "60104768",
        "affiliation_state": "Ad-Dawhah"
    },
    {
        "paper_title": "Earth for AI: A Political Ecology of Data-Driven Climate Initiatives",
        "publication": "Geoforum",
        "citied_by": "25",
        "cover_date": "2022-03-01",
        "Abstract": "Emerging narratives around artificial intelligence (AI) and machine learning place great faith in these technologies’ ability to ameliorate threats posed by climate change. They promise the capacity to analyze vast amounts of more precise and real-time data, improving how decision-makers predict, respond, and adapt. Yet scholars in political ecology have long observed that technocentric approaches typically reduce complex human-environment relationships in ways that fail to account for social relations and power dynamics. This paper charts the emerging political economy of “climate AI” – the philanthropies, NGOs, private consultancies, and tech giants investing in data-driven climate initiatives. Mapping out two case studies, we show that environmental and climate crises are grist for tech solutions and find that many climate AI actors are interested in it for surveillance, greenwashing, and commodifying algorithms. We pay special attention to how neocolonial and racialized power structures manifest in climate AI and outline three ways for political ecologists and digital geographers to research its socio-materiality: how computational resources are environmentally embedded, how disasters become “shocks” that the AI industry capitalizes on, and how climate AI shapes material investment flows and landscapes. Highlighting how data-driven approaches to climate crises reproduce injustices already faced by marginalized communities, our analysis contributes to research on environmental data justice.",
        "DOI": "10.1016/j.geoforum.2022.01.016",
        "paper_author": "Nost E.",
        "affiliation_name": "University of Guelph",
        "affiliation_city": "Guelph",
        "affiliation_country": "Canada",
        "affiliation_id": "60015881",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "Using machine learning models to predict the willingness to carry lightweight goods by bike and kick-scooter",
        "publication": "Transportation Research Interdisciplinary Perspectives",
        "citied_by": "4",
        "cover_date": "2022-03-01",
        "Abstract": "The social transformation caused by the COVID-19 pandemic can help cities become healthier and more sustainable, with more space for active modes of transportation. This research addresses people's willingness to go shopping by bike or kick-scooter and to transport lightweight goods in cities with low maturity for cycling and scooting. Data collection was based on a survey, applied in the two largest cities of Brazil (São Paulo and Rio de Janeiro) and Portugal (Lisbon and Porto). The dataset was processed considering only two categories of respondents (i.e., potential users and regular users) and then four machine learning models (K-Nearest Neighbor, Support Vector Machine, Decision Tree, and Random Forest) were applied to predict shopping by bike or kick-scooter. In terms of all performance measures, the Support Vector Machine model was the optimum. The results indicate that people are willing to transport lightweight goods by bike or kick-scooter, as long as the infrastructure is safe and comfortable. This research contributes to understanding mobility behavior changes and identifying barriers to going shopping by bike or kick-scooter. It also presents some policy recommendations for improving cycling and scooting use for shopping, which public authorities can carry out.",
        "DOI": "10.1016/j.trip.2022.100568",
        "paper_author": "Silveira-Santos T.",
        "affiliation_name": "Universidad Politécnica de Madrid",
        "affiliation_city": "Madrid",
        "affiliation_country": "Spain",
        "affiliation_id": "60028442",
        "affiliation_state": "Madrid"
    },
    {
        "paper_title": "Multi-agent DRL-based data-driven approach for PEVs charging/discharging scheduling in smart grid",
        "publication": "Journal of the Franklin Institute",
        "citied_by": "23",
        "cover_date": "2022-03-01",
        "Abstract": "This paper studies the charging/discharging scheduling problem of plug-in electric vehicles (PEVs) in smart grid, considering the users’ satisfaction with state of charge (SoC) and the degradation cost of batteries. The objective is to collectively determine the energy usage patterns of all participating PEVs so as to minimize the energy cost of all PEVs while ensuring the charging needs of PEV owners. The challenges herein are mainly in three folds: 1) the randomness of electricity price and PEVs’ commuting behavior; 2) the unknown dynamics model of SoC; and 3) a large solution space, which make it challenging to directly develop a model-based optimization algorithm. To this end, we first reformulate the above energy cost minimization problem as a Markov game with unknown transition probabilities. Then a multi-agent deep reinforcement learning (DRL)-based data-driven approach is developed to solve the Markov game. Specifically, the proposed approach consists of two networks: an extreme learning machine (ELM)-based feedforward neural network (NN) for uncertainty prediction of electricity price and PEVs’ commuting behavior and a Q network for optimal action-value function approximation. Finally, the comparison results with three benchmark solutions show that our proposed algorithm can not only adaptively decide the optimal charging/discharging policy by on-line learning process, but also yield a lower energy cost within an unknown market environment.",
        "DOI": "10.1016/j.jfranklin.2022.01.016",
        "paper_author": "Wan Y.",
        "affiliation_name": "University of Science and Technology of China",
        "affiliation_city": "Hefei",
        "affiliation_country": "China",
        "affiliation_id": "60019118",
        "affiliation_state": "Anhui"
    },
    {
        "paper_title": "Measuring Personality through Images: Validating a Forced-Choice Image-Based Assessment of the Big Five Personality Traits",
        "publication": "Journal of Intelligence",
        "citied_by": "15",
        "cover_date": "2022-03-01",
        "Abstract": "Selection methods are commonly used in talent acquisition to predict future job performance and to find the best candidates, but questionnaire-based assessments can be lengthy and lead to candidate fatigue and poor engagement, affecting completion rates and producing poor data. Gamification can mitigate some of these issues through greater engagement and shorter testing times. One avenue of gamification is image-based tests. Although such assessments are starting to gain traction in personnel selection, few studies describing their validity and psychometric properties exist. The current study explores the potential of a five-minute, forced-choice, image-based assessment of the Big Five personality traits to be used in selection. Study 1 describes the creation of the image pairs and the selection of the 150 best-performing items based on a sample of 300 respondents. Study 2 describes the creation of machine-learning-based scoring algorithms and tests of their convergent and discriminate validity and adverse impact based on a sample of 431 respondents. All models showed good levels of convergent validity with the IPIP-NEO-120 (openness r =.71, conscientiousness r =.70, extraversion r =.78, agreeableness r =.60, and emotional stability r =.70) and were largely free from potential adverse impact. The implications for recruitment policy and practice and the need for further validation are discussed.",
        "DOI": "10.3390/jintelligence10010012",
        "paper_author": "Hilliard A.",
        "affiliation_name": "Goldsmiths, University of London",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60010964",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Ecological cruising control of connected electric vehicle: a deep reinforcement learning approach",
        "publication": "Science China Technological Sciences",
        "citied_by": "10",
        "cover_date": "2022-03-01",
        "Abstract": "Ecological cruising control methods of vehicles have been extensively studied to further cut down energy consumption by optimizing vehicles’ speed profiles. However, most controllers cannot be put into practical application because of future terrain data requirements and excessive computational demand. In this paper, an eco-cruising strategy with real-time capability utilizing deep reinforcement learning is proposed for electric vehicles (EVs) propelled by in-wheel motors. The deep deterministic policy gradient algorithm is leveraged to continuously regulate the motor torque in response to road elevation changes. By comparing the proposed strategy to the energy economy benchmark optimized with dynamic programming (DP), and traditional constant speed (CS) strategy, its learning ability, optimality, and generalization performance are verified. The simulation results show that without a priori knowledge about the future trip, the proposed strategy provides 3.8% energy saving compared with the CS strategy. It also yields a smaller gap than the globally optimal solution of DP. By testing on other driving cycles, the trained strategy reveals good generalization performance and impressive computational efficiency (about 2 ms per simulation step), making it practical and implementable. Additionally, the model-free characteristic of the proposed strategy makes it applicable for EVs with different powertrain topologies.",
        "DOI": "10.1007/s11431-021-1994-7",
        "paper_author": "Wang Q.",
        "affiliation_name": "Nanjing University of Science and Technology",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60010080",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "Prediction-free, real-time flexible control of tidal lagoons through Proximal Policy Optimisation: A case study for the Swansea Lagoon",
        "publication": "Ocean Engineering",
        "citied_by": "8",
        "cover_date": "2022-03-01",
        "Abstract": "Tidal Range Structures (TRS) have been considered for large-scale electricity generation for their potential ability to produce reasonably predictable energy without the emission of greenhouse gases. Once the main forcing components for driving the tides have deterministic dynamics, the available energy in a given TRS has been estimated, through analytical and numerical optimisation routines, as a mostly predictable event. This constraint imposes state-of-art flexible operation methods to rely on tidal predictions to infer best operational strategies for TRS, with the additional cost of requiring to run optimisation routines for every new tide. In this paper, a Deep Reinforcement Learning approach (Proximal Policy Optimisation through Unity ML-Agents) is introduced to perform automatic operation of TRS. For validation, the performance of the proposed method is compared with six different operation optimisation approaches devised from the literature, utilising the Swansea Bay Tidal Lagoon as a case study. We show that our approach is successful in maximising energy generation through an optimised operational policy of turbines and sluices, yielding competitive results with state-of-art optimisation strategies, with the clear advantages of requiring training once and performing real-time automatic control of TRS with measured ocean data only.",
        "DOI": "10.1016/j.oceaneng.2022.110657",
        "paper_author": "Moreira T.M.",
        "affiliation_name": "Universidade Federal de Minas Gerais",
        "affiliation_city": "Belo Horizonte",
        "affiliation_country": "Brazil",
        "affiliation_id": "60030074",
        "affiliation_state": "MG"
    },
    {
        "paper_title": "Developing machine learning-based models to help identify child abuse and neglect: key ethical challenges and recommended solutions",
        "publication": "Journal of the American Medical Informatics Association",
        "citied_by": "16",
        "cover_date": "2022-03-01",
        "Abstract": "Child abuse and neglect are public health issues impacting communities throughout the United States. The broad adoption of electronic health records (EHR) in health care supports the development of machine learning-based models to help identify child abuse and neglect. Employing EHR data for child abuse and neglect detection raises several critical ethical considerations. This article applied a phenomenological approach to discuss and provide recommendations for key ethical issues related to machine learning-based risk models development and evaluation: (1) biases in the data; (2) clinical documentation system design issues; (3) lack of centralized evidence base for child abuse and neglect; (4) lack of \"gold standard \"in assessment and diagnosis of child abuse and neglect; (5) challenges in evaluation of risk prediction performance; (6) challenges in testing predictive models in practice; and (7) challenges in presentation of machine learning-based prediction to clinicians and patients. We provide recommended solutions to each of the 7 ethical challenges and identify several areas for further policy and research.",
        "DOI": "10.1093/jamia/ocab286",
        "paper_author": "Landau A.Y.",
        "affiliation_name": "Columbia University School of Nursing",
        "affiliation_city": "New York",
        "affiliation_country": "United States",
        "affiliation_id": "60001864",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Ranking the importance of demographic, socioeconomic, and underlying health factors on US COVID-19 deaths: A geographical random forest approach",
        "publication": "Health and Place",
        "citied_by": "86",
        "cover_date": "2022-03-01",
        "Abstract": "A growing number of studies show that the uneven spatial distribution of COVID-19 deaths is related to demographic and socioeconomic disparities across space. However, most studies fail to assess the relative importance of each factor to COVID-19 death rate and, more importantly, how this importance varies spatially. Here, we assess the variables that are more important locally using Geographical Random Forest (GRF), a local non-linear regression method. Through GRF, we estimated the non-linear relationships between the COVID-19 death rate and 29 socioeconomic and health-related factors during the first year of the pandemic in the USA (county level). GRF outputs are compared to global (Random Forest and OLS) and local (Geographically Weighted Regression) models. Results show that GRF outperforms all models and that the importance of variables highly varies by location. For example, lack of health insurance is the most important factor in one-third (34.86%) of the US counties. Most of these counties are (concentrated mainly in the Midwest region and South region). On the other hand, no leisure-time physical activity is the most important primary factor for 19.86% of the US counties. These counties are found in California, Oregon, Washington, and parts of the South region. Understanding the location-based characteristics and spatial patterns of socioeconomic and health factors linked to COVID-19 deaths is paramount for policy designing and decision making. In this way, interventions can be designed and implemented based on the most important factors locally, avoiding thus general guidelines addressed for the entire nation.",
        "DOI": "10.1016/j.healthplace.2022.102744",
        "paper_author": "Grekousis G.",
        "affiliation_name": "Sun Yat-Sen University",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60021182",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Village-level poverty identification using machine learning, high-resolution images, and geospatial data",
        "publication": "International Journal of Applied Earth Observation and Geoinformation",
        "citied_by": "40",
        "cover_date": "2022-03-01",
        "Abstract": "Tracking progress in poverty alleviation and promptly identifying the distribution of poor areas are critical for strategic policy interventions, especially for regions with poor statistical systems. The massive satellite imagery and geospatial data provide great opportunities for timely and cost-effective socioeconomic evaluations. However, existing research on poverty identification is mostly based on satellite images, and the potential of combined multi-source geospatial data on poverty identification has not been fully explored. Here, we propose an approach that evaluates how village-level poverty can be identified by integrating high-resolution imagery (HRI), point-of-interest (POI), OpenStreetMap (OSM), and digital surface model (DSM) data. The study area included 338 villages from Yunyang County, located in Hubei Province, central China. We extracted the explanatory variables indicating access to facilities and services, agricultural production conditions, village construction, and the spatial distribution of village settlements from the HRI, POI, OSM, and DSM data. The random forest algorithm was then used to model the relationship between village-level poverty and explanatory variables. The results demonstrated a 54% accuracy in the prediction of village-level poverty; the best prediction performance (72%) was observed for the villages categorized as poor. The built-up land proportion and the time cost to the facilities and services contributed the most to the identification of village-level poverty, while the proxy variables of agricultural production conditions contributed the least. This study provides an approach to village-level poverty identification using satellite imagery and geospatial data and proves that the data employed in this study could identify the poorest areas that are highly coupled with natural geographical conditions and backward public services.",
        "DOI": "10.1016/j.jag.2022.102694",
        "paper_author": "Hu S.",
        "affiliation_name": "Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60031150",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A review of machine learning methods for drought hazard monitoring and forecasting: Current research trends, challenges, and future research directions",
        "publication": "Environmental Modelling and Software",
        "citied_by": "96",
        "cover_date": "2022-03-01",
        "Abstract": "Machine learning is a dynamic field with wide-ranging applications, including drought modeling and forecasting. Drought is a complex, devastating natural disaster for which it is challenging to develop effective prediction models. Therefore, our review focuses on basic information about machine learning methods (MLMs) and their potential applications in developing efficient and effective drought forecasting models. We observed that MLMs have achieved significant advances in the robustness, effectiveness, and accuracy of the algorithms for drought modelling in recent years. The performance comparison of MLMs with other models provides a comprehensive conception of different model evaluation metrics. Further challenges of MLMs, such as inadequate training data sets, noise, outliers, and observation bias for spatial data sets, are explored. Finally, our review conveys in-depth understanding to researchers on machine learning applications in forecasting and modeling and provides drought mitigation strategy guidance for policymakers.",
        "DOI": "10.1016/j.envsoft.2022.105327",
        "paper_author": "Prodhan F.A.",
        "affiliation_name": "Aerospace Information Research Institute",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60273019",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Gaining Insights into Dwelling Characteristics Using Machine Learning for Policy Making on Nearly Zero-Energy Buildings with the Use of Smart Meter and Weather Data",
        "publication": "Journal of Sustainable Development of Energy, Water and Environment Systems",
        "citied_by": "15",
        "cover_date": "2022-03-01",
        "Abstract": "Machine learning models have proven to be reliable methods in classification tasks. However, little research has been conducted on the classification of dwelling characteristics based on smart meter and weather data before. Gaining insights into dwelling characteristics, which comprise of the type of heating system used, the number of inhabitants, and the number of solar panels installed, can be helpful in creating or improving the policies to create new dwellings at nearly zero-energy standard. This paper compares different supervised machine learning algorithms, namely Logistic Regression, Support Vector Machine, K-Nearest Neighbor, and Long-short term memory, and methods used to correctly implement these algorithms. These methods include data pre-processing, model validation, and evaluation. Smart meter data, which was used to train several machine learning algorithms, was provided by Groene Mient. The models that were generated by the algorithms were compared on their performance. The results showed that the Long-short term memory performed the best with 96% accuracy. Cross Validation was used to validate the models, where 80% of the data was used for training purposes and 20% was used for testing purposes. Evaluation metrics were used to produce classification reports, which indicates that the Long-short term memory outperforms the compared models on the evaluation metrics for this specific problem.",
        "DOI": "10.13044/j.sdewes.d9.0388",
        "paper_author": "Čurčić T.",
        "affiliation_name": "The Hague University of Applied Sciences",
        "affiliation_city": "The Hague",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60102765",
        "affiliation_state": "Zuid-Holland"
    },
    {
        "paper_title": "New JBI policy emphasizes clinically-meaningful novel machine learning methods",
        "publication": "Journal of Biomedical Informatics",
        "citied_by": "2",
        "cover_date": "2022-03-01",
        "Abstract": "NA",
        "DOI": "10.1016/j.jbi.2022.104003",
        "paper_author": "Tucker A.",
        "affiliation_name": "Brunel University London",
        "affiliation_city": "Uxbridge",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60020623",
        "affiliation_state": "Middlesex"
    },
    {
        "paper_title": "Role of insurance in wildfire risk mitigation",
        "publication": "Economic Modelling",
        "citied_by": "12",
        "cover_date": "2022-03-01",
        "Abstract": "With wildfire-risk rising globally, the role of home insurance continues to remain under-studied in the search for mechanisms to mitigate loss from wildfire. This study investigates whether insurance policies effectively discourage homeownership in fire-prone zones. Using zip-code level data from Los Angeles between 2011 and 2018, we employ linear regression, GMM and machine-learning to examine the extent to which wildfire-risk impacts FAIR plan vis-a-vis market insurance. Results indicate that insurance policies do not effectively disincentivize homeowners away from high-risk zones which is novel to the existing literature. This is because the marginal impact of wildfire-risk on both insurance premiums, is relatively lower compared to other factors, especially for private insurance. Moderate-to-high risk areas continue to remain underinsured. Evidence of underinsurance was found in racially diverse neighborhoods. Recommendations include designing policies such that difference in risk-premium reflects difference in wildfire-risk to discourage homeownership in high-risk zones.",
        "DOI": "10.1016/j.econmod.2022.105768",
        "paper_author": "Hazra D.",
        "affiliation_name": "California State University, Los Angeles",
        "affiliation_city": "Los Angeles",
        "affiliation_country": "United States",
        "affiliation_id": "60030759",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Soft-cooperation via data sharing eases transboundary conflicts in the Lancang-Mekong River Basin",
        "publication": "Journal of Hydrology",
        "citied_by": "15",
        "cover_date": "2022-03-01",
        "Abstract": "Water resources management in the Lancang-Mekong River Basin (LMRB) is challenging in face of the complex trans-national geopolitical context, the acceleration of hydropower dam expansion, and the growing demands for energy, food, and riverine ecosystem preservation across the basin. Centralised management through coordinated operation of all the basin water infrastructures (“hard cooperation”) across the river basin is ideal but hard to achieve given the existing geopolitical context of the region. To overcome these barriers and facilitate a more adaptive and reliable operation of the LMRB's water system, this paper focuses on the concept of “soft-cooperation”, i.e. the idea of indirectly fostering cooperation by sharing data across countries to inform the non-cooperative (or independent) operation of their systems. Here, we first quantify existing riparian conflicts and synergies for the non-cooperative scenario, where upstream and downstream act independently without any form of cooperation, and for the “hard cooperation” configuration, where an ideal centralised operator jointly operates all the infrastructures. Then a “soft cooperation” scheme assuming data sharing across riparian countries is evaluated. The most effective and informative data is selected via machine learning and used to inform the design of the river reservoir system operation via evolutionary multiobjective direct policy search. Results show that the riparian conflicts around water mainly exist between the sectors, i.e. hydropower production vs ecosystem conservation, rather than among countries. For reservoirs with various capacities and locations, the value of information varies. Generally, sharing data about the water currently available in the different river storages results in a large improvement in hydropower production. This study highlights the value of appropriate data sharing and selection in water system operation as an indirect way of promoting cooperation in transboundary river management.",
        "DOI": "10.1016/j.jhydrol.2022.127464",
        "paper_author": "Gao J.",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60025278",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Nonlinear public transit accessibility effects on housing prices: Heterogeneity across price segments",
        "publication": "Transport Policy",
        "citied_by": "31",
        "cover_date": "2022-03-01",
        "Abstract": "Although numerous studies have been conducted to analyze the relationship between public transit accessibility and housing prices, they often assumed a linear relationship. Using a dataset of 196,232 s-hand residential properties in Shanghai (China), this study applies the gradient boosting regression trees (GBRT) method to investigate the complicated relationships between public transit accessibility and housing prices across price segments. The results show that for low- and median-priced houses, the travel time by public transit to the central business district (CBD) contributes the most to housing prices while for high-priced houses, the systemwide metro accessibility contributes the most. Public transit accessibility has significant nonlinear and threshold effects on housing prices. Public transit accessibility has an overall positive effect on housing prices, though some negative effects occur within certain intervals. Different patterns are observed across price segments. High-priced houses receive a much larger premium from the closeness to CBD and systemwide metro accessibility but are more negatively affected by both local and systemwide bus accessibility, compared to low- and median-priced houses. The differences in transit's value-added effects across price segments may bring about inequity in accessibility across different income groups. Relevant policies on how to reduce such an inequity issue are finally discussed.",
        "DOI": "10.1016/j.tranpol.2022.01.004",
        "paper_author": "Jin T.",
        "affiliation_name": "Universiteit Gent",
        "affiliation_city": "Ghent",
        "affiliation_country": "Belgium",
        "affiliation_id": "60033316",
        "affiliation_state": "VOV"
    },
    {
        "paper_title": "Kinematic Modeling for Biped Robot Gait Trajectory Using Machine Learning Techniques",
        "publication": "Journal of Bionic Engineering",
        "citied_by": "14",
        "cover_date": "2022-03-01",
        "Abstract": "This paper presents the predictive models for biped robot trajectory generation. Predictive models are parametrizing as a continuous function of joint angle trajectories. In a previous work, the authors had collected the human locomotion dataset at RAMAN Lab, MNIT, Jaipur, India. The MNIT gait dataset consists of walking data on a plane surface of 120 human subjects from different age groups and genders. Thirty-two machine learning models (linear, support vector, k-nearest neighbor, ensemble, probabilistic, and deep learning) trained using the collected dataset. In addition, two types of mapping, (a) one-to-one and (b) many-to-one, are presented for each model. These mapping models act as a reference policy for the control of joints and prediction of state for the next time instant in advance if the onboard sensor fails. Results show that the deep learning and probabilistic learning models perform better for both types of mappings. Also, the probabilistic model outperforms the deep learning-based models in terms of maximum error, because the probabilistic model effectively deals with the prediction uncertainty. In addition, many-to-one outperforms the one-to-one mapping because it captures the correlation between knee, hip, and ankle trajectories. Therefore, this study suggests a many-to-one mapping using the probabilistic model for biped robot trajectory generation.",
        "DOI": "10.1007/s42235-021-00142-4",
        "paper_author": "Singh B.",
        "affiliation_name": "Malaviya National Institute of Technology Jaipur",
        "affiliation_city": "Jaipur",
        "affiliation_country": "India",
        "affiliation_id": "60017757",
        "affiliation_state": "RJ"
    },
    {
        "paper_title": "Fulfilling the Promise of Artificial Intelligence in the Health Sector: Let's Get Real",
        "publication": "Value in Health",
        "citied_by": "19",
        "cover_date": "2022-03-01",
        "Abstract": "Objectives: This study aimed to showcase the potential and key concerns and risks of artificial intelligence (AI) in the health sector, illustrating its application with current examples, and to provide policy guidance for the development, assessment, and adoption of AI technologies to advance policy objectives. Methods: Nonsystematic scan and analysis of peer-reviewed and gray literature on AI in the health sector, focusing on key insights for policy and governance. Results: The application of AI in the health sector is currently in the early stages. Most applications have not been scaled beyond the research setting. The use in real-world clinical settings is especially nascent, with more evidence in public health, biomedical research, and “back office” administration. Deploying AI in the health sector carries risks and hazards that must be managed proactively by policy makers. For AI to produce positive health and policy outcomes, 5 key areas for policy are proposed, including health data governance, operationalizing AI principles, flexible regulation, skills among health workers and patients, and strategic public investment. Conclusions: AI is not a panacea, but a tool to address specific problems. Its successful development and adoption require data governance that ensures high-quality data are available and secure; relevant actors can access technical infrastructure and resources; regulatory frameworks promote trustworthy AI products; and health workers and patients have the information and skills to use AI products and services safely, effectively, and efficiently. All of this requires considerable investment and international collaboration.",
        "DOI": "10.1016/j.jval.2021.11.1369",
        "paper_author": "Hashiguchi T.C.O.",
        "affiliation_name": "L'Organisation de Coopération et de Développement Economiques",
        "affiliation_city": "Paris",
        "affiliation_country": "France",
        "affiliation_id": "60009357",
        "affiliation_state": "Ile-de-France"
    },
    {
        "paper_title": "Dealing With the Kidney Discard Problem in the United States—One Potential Solution for a Difficult Problem",
        "publication": "American Journal of Kidney Diseases",
        "citied_by": "0",
        "cover_date": "2022-03-01",
        "Abstract": "NA",
        "DOI": "10.1053/j.ajkd.2021.09.022",
        "paper_author": "Friedewald J.J.",
        "affiliation_name": "Northwestern University Feinberg School of Medicine",
        "affiliation_city": "Chicago",
        "affiliation_country": "United States",
        "affiliation_id": "60013227",
        "affiliation_state": "IL"
    },
    {
        "paper_title": "Automating cognitive impairment screening in emergency departments: A small step forward",
        "publication": "Journal of the American Geriatrics Society",
        "citied_by": "0",
        "cover_date": "2022-03-01",
        "Abstract": "NA",
        "DOI": "10.1111/jgs.17654",
        "paper_author": "Hirshon J.M.",
        "affiliation_name": "University of Maryland School of Medicine",
        "affiliation_city": "Baltimore",
        "affiliation_country": "United States",
        "affiliation_id": "60012957",
        "affiliation_state": "MD"
    },
    {
        "paper_title": "The importance of diverse key stakeholders in deciding the role of artificial intelligence for HIV research and policy",
        "publication": "Health Policy and Technology",
        "citied_by": "3",
        "cover_date": "2022-03-01",
        "Abstract": "NA",
        "DOI": "10.1016/j.hlpt.2022.100599",
        "paper_author": "Garett R.",
        "affiliation_name": "ElevateU",
        "affiliation_city": "Irvine",
        "affiliation_country": "United States",
        "affiliation_id": "126837532",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Existing land uses constrain climate change mitigation potential of forest restoration in India",
        "publication": "Conservation Letters",
        "citied_by": "19",
        "cover_date": "2022-03-01",
        "Abstract": "Many countries have made ambitious pledges to increase forest areas to mitigate climate change. However, the availability of land to meet these goals is not well understood. Global studies indicate substantial potential, but do not account for local land-use and regional variation, crucial for policy making. Using India as a case study, we use a machine learning framework to define the bioclimatic envelope of forest cover and map this against current land-uses with varying suitability for restoration. We estimate the additional feasible area for restoration to be only 1.58 Mha, cumulatively sequestering 61.3 TgC, which is substantially less than estimates derived from global studies. However, we also find up to 14.67 Mha of opportunity for agroforestry in current agricultural land, delivering up to 98.1 TgC nationally. In the UN Decade of Restoration, we recommend developing forest restoration strategies that are compatible with existing land-uses, such as agroforestry, especially in countries that have large smallholder agriculture holdings.",
        "DOI": "10.1111/conl.12867",
        "paper_author": "Gopalakrishna T.",
        "affiliation_name": "University of Oxford",
        "affiliation_city": "Oxford",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60026851",
        "affiliation_state": "Oxfordshire"
    },
    {
        "paper_title": "Forecasting municipal solid waste in Lithuania by incorporating socioeconomic and geographical factors",
        "publication": "Waste Management",
        "citied_by": "24",
        "cover_date": "2022-03-01",
        "Abstract": "Forecasting municipal solid waste (MSW) generation and composition plays an essential role in effective waste management, policy decision-making and the MSW treatment process. An intelligent forecasting system could be used for short-term and long-term waste handling, ensuring a circular economy and a sustainable use of resources. This study contributes to the field by proposing a hybrid k-nearest neighbours (H-kNN) approach to forecasting municipal solid waste and its composition in the regions that experience data incompleteness and inaccessibility, as is the case for Lithuania and many other countries. For this purpose, the average MSW generation of neighbouring municipalities, as a geographical factor, was used to impute missing values, and socioeconomic factors together with demographic indicator affecting waste collected in municipalities were identified and quantified using correlation analysis. Among them, the most influential factors, such as population density, GDP per capita, private property, foreign investment per capita, and tourism, were then incorporated in the hierarchical setting of the H-kNN approach. The results showed that, in forecasting MSW generation, H-kNN achieved MAPE of 11.05%, on average, including all Lithuanian municipalities, which is by 7.17 percentage points lower than obtained using kNN. This implies that by finding relevant factors at the municipal level, we can compensate for the data incompleteness and enhance the forecasting results of MSW generation and composition.",
        "DOI": "10.1016/j.wasman.2022.01.004",
        "paper_author": "Paulauskaite-Taraseviciene A.",
        "affiliation_name": "Kaunas University of Technology",
        "affiliation_city": "Kaunas",
        "affiliation_country": "Lithuania",
        "affiliation_id": "60042282",
        "affiliation_state": "Kaunas"
    },
    {
        "paper_title": "The role of data frequency and method selection in electricity price estimation: Comparative evidence from Turkey in pre-pandemic and pandemic periods",
        "publication": "Renewable Energy",
        "citied_by": "33",
        "cover_date": "2022-03-01",
        "Abstract": "The study examines the role of data frequency and estimation methods in electricity price estimation by applying selected machine learning algorithms and time series econometric models. In this context, Turkey is selected as an emerging country example, seven explanatory variables including COVID-19 pandemic is considered, and daily and weekly data between February 20, 2019 and March 26, 2021 that includes pre-pandemic and pandemic periods are used. The empirical results show that (i) machine learning algorithms perform better than time series econometric models for both pre-pandemic and pandemic periods; (ii) high-frequency data increases the performance of estimation models; (iii) machine learning algorithms perform better with high-frequency (daily) data with regard to low-frequency (weekly) data; (iv) the pandemic causes an adverse effect on the performance of estimation models; (v) energy-related variables are more important than other variables although all are significant; (vi) the share of renewable sources in electricity production is the most important variable on the electricity prices in both periods and data types. Hence, the findings highlight the role of data frequency and method selection in electricity prices estimation. Moreover, policy implications are discussed.",
        "DOI": "10.1016/j.renene.2021.12.136",
        "paper_author": "Kılıç Depren S.",
        "affiliation_name": "Yıldız Teknik Üniversitesi",
        "affiliation_city": "Istanbul",
        "affiliation_country": "Turkey",
        "affiliation_id": "60019963",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Automatic modeling of socioeconomic drivers of energy consumption and pollution using Bayesian symbolic regression",
        "publication": "Sustainable Production and Consumption",
        "citied_by": "18",
        "cover_date": "2022-03-01",
        "Abstract": "Predicting countries’ energy consumption and pollution levels precisely from socioeconomic drivers will be essential to support sustainable policy-making in an effective manner. Current predictive models, like the widely used STIRPAT equation, are based on rigid mathematical expressions that assume constant elasticities. Using a Bayesian approach to symbolic regression, here we explore a vast amount of suitable mathematical expressions to model the link between energy-related impacts and socioeconomic drivers. We find closed-form analytical expressions that outperform the well-established STIRPAT equation and whose mathematical structure challenges the assumption of constant elasticities adopted in the literature. Our work unfolds new avenues to apply machine learning algorithms to derive analytical expressions from data in environmental studies, which could help find better models and solutions in energy-related problems.",
        "DOI": "10.1016/j.spc.2021.12.025",
        "paper_author": "Vázquez D.",
        "affiliation_name": "ETH Zürich",
        "affiliation_city": "Zurich",
        "affiliation_country": "Switzerland",
        "affiliation_id": "60025858",
        "affiliation_state": "ZH"
    },
    {
        "paper_title": "How effective is carbon pricing?—A machine learning approach to policy evaluation",
        "publication": "Journal of Environmental Economics and Management",
        "citied_by": "31",
        "cover_date": "2022-03-01",
        "Abstract": "While carbon taxes are generally seen as a rational policy response to climate change, knowledge about their performance from an ex-post perspective is still limited. This paper analyzes the emissions and cost impacts of the UK CPS, a carbon tax levied on all fossil-fired power plants. To overcome the problem of a missing control group, we propose a policy evaluation approach which leverages economic theory and machine learning for counterfactual prediction. Our results indicate that in the period 2013–2016 the CPS lowered emissions by 6.2 percent at an average cost of €18 per ton. We find substantial temporal heterogeneity in tax-induced impacts which stems from variation in relative fuel prices. An important implication for climate policy is that whether a higher carbon tax leads to higher emissions reductions and higher costs depends on relative fuel prices.",
        "DOI": "10.1016/j.jeem.2021.102589",
        "paper_author": "Abrell J.",
        "affiliation_name": "Zentrum für Europäische Wirtschaftsforschung GmbH",
        "affiliation_city": "Mannheim",
        "affiliation_country": "Germany",
        "affiliation_id": "60076709",
        "affiliation_state": "Baden-Wurttemberg"
    },
    {
        "paper_title": "Prediction and evaluation of spatial distributions of ozone and urban heat island using a machine learning modified land use regression method",
        "publication": "Sustainable Cities and Society",
        "citied_by": "58",
        "cover_date": "2022-03-01",
        "Abstract": "In summer, Ozone (O3) pollution and urban heat island (UHI) pose serious health risks to humans. To obtain the spatial distributions of ozone and urban heat island in Xi'an in summer and develop a simultaneous control strategy of ozone and urban heat island, the land use regression model is modified and improved using the machine learning random forest algorithm. The LUR-Kriging-RF integrated prediction model is then established. The land use regression and kriging are used to extract the feature variables, while random forest is used to establish a regression model. The spatial distribution maps of ozone and urban heat island in Xi'an are obtained by regression mapping of the prediction model, and the spatial relationships between them are analyzed. The SHapley Additive explanation (SHAP) and partial dependence plot (PDP) are adopted to explain the way feature variables act on ozone and urban heat island. Based on the spatial distribution and interaction mode, a simultaneous control strategy of ozone and urban heat island in Xi'an is put forward. For ozone, the R2 of the integrated prediction model (0.65) is higher than that of land use regression (0.4), while the RMSE (28.18) of the integrated model is lower than that of land use regression (35.66). For temperature, the R2 of the integrated model (0.93) is higher than that of land use regression (0.8), while its RMSE (0.92) is lower than that of land use regression (1.52). The performance of the LUR-Kriging-RF integrated prediction model is better than that of land use regression. This study reveals the spatial interactions between ozone and land use regression in the central urban areas. The suitable strategies for mapping ozone pollution and urban heat island control include reducing VOCs emissions from industrial sources and agricultural sources, increasing plants with low VOCs emissions, and spray humidification. This study can be used to evaluate ozone exposure and thermal exposure, provide scientific support for environmental protection and urban heat island control policies, contribute to reducing public health threats, promote the sustainability of urban environments, and promote the practical application of machine learning in this field.",
        "DOI": "10.1016/j.scs.2021.103643",
        "paper_author": "Han L.",
        "affiliation_name": "Chang'an University",
        "affiliation_city": "Xi'an",
        "affiliation_country": "China",
        "affiliation_id": "60028891",
        "affiliation_state": "Shaanxi"
    },
    {
        "paper_title": "The efficacy of deep learning based LSTM model in forecasting the outbreak of contagious diseases",
        "publication": "Infectious Disease Modelling",
        "citied_by": "35",
        "cover_date": "2022-03-01",
        "Abstract": "The coronavirus disease that outbreak in 2019 has caused various health issues. According to the WHO, the first positive case was detected in Bangladesh on 7th March 2020, but while writing this paper in June 2021, the total confirmed, recovered, and death cases were 826922, 766266 and 13118, respectively. Due to the emergence of COVID-19 in Bangladesh, the country is facing a major public health crisis. Unfortunately, the country does not have a comprehensive health policy to address this issue. This makes it hard to predict how the pandemic will affect the population. Machine learning techniques can help us detect the disease's spread. To predict the trend, parameters, risks, and to take preventive measure in Bangladesh; this work utilized the Recurrent Neural Networks based Deep Learning methodologies like LongShort-Term Memory. Here, we aim to predict the epidemic's progression for a period of more than a year under various scenarios in Bangladesh. We extracted the data for daily confirmed, recovered, and death cases from March 2020 to August 2021. The obtained Root Mean Square Error (RMSE) values of confirmed, recovered, and death cases indicates that our result is more accurate than other contemporary techniques. This study indicates that the LSTM model could be used effectively in predicting contagious diseases. The obtained results could help in explaining the seriousness of the situation, also mayhelp the authorities to take precautionary steps to control the situation.",
        "DOI": "10.1016/j.idm.2021.12.005",
        "paper_author": "Absar N.",
        "affiliation_name": "BGC Trust University Bangladesh",
        "affiliation_city": "Chittagong",
        "affiliation_country": "Bangladesh",
        "affiliation_id": "122797804",
        "affiliation_state": "Chittagong"
    },
    {
        "paper_title": "Predicting effects of built environment on fatal pedestrian accidents at location-specific level: Application of XGBoost and SHAP",
        "publication": "Accident Analysis and Prevention",
        "citied_by": "65",
        "cover_date": "2022-03-01",
        "Abstract": "Understanding locally heterogeneous physical contexts in built environment is of great importance in developing preemptive countermeasures to mitigate pedestrian fatality risks. In this study, we aim to investigate the non-linear relationship between physical factors and pedestrian fatality at a location-specific level using a machine learning approach. The state-of-art machine learning algorithm, eXtreme Gradient Boosting (XGBoost), is employed for a binary classification problem, in which nationwide locations where fatal pedestrian accidents occurred for the years from 2012 to 2019 in Korea serve as positive samples (np = 13,366). For negative samples, locations with no pedestrian accidents are selected randomly to the size that is 10 times larger (nn = 133,660) than positive samples. Fifteen features under the categories of road conditions, road facilities, road networks, and land uses are assigned to both the positive and negative sample locations using Geographic Information System (GIS). A method is proposed to avoid the class imbalance problem, and a final unbiased model is utilized to predict fatal pedestrian risks at the negative sample locations. In addition, Shapley Additive Explanations (SHAP) is introduced to provide a robust interpretation of the XGBoos prediction results. It is shown that 21.6% of the negative sample locations have a probability of fatal pedestrian accidents greater than 0.5 (or 78.4% accuracy). Generally, a road segment that lies in many of the shortest routes in a dense residential area with many lively activities from aligned buildings is a potential spot for fatal pedestrian accidents. However, based on the SHAP interpretation, the relationships between the features and pedestrian fatality are found nonlinear and locally heterogeneous. We discuss the implications of this result has for drafting policy recommendations to reduce pedestrian fatalities.",
        "DOI": "10.1016/j.aap.2021.106545",
        "paper_author": "Chang I.",
        "affiliation_name": "Gacheon University",
        "affiliation_city": "Seongnam",
        "affiliation_country": "South Korea",
        "affiliation_id": "116250501",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Measuring vulnerability to multidimensional poverty with Bayesian network classifiers",
        "publication": "Economic Analysis and Policy",
        "citied_by": "11",
        "cover_date": "2022-03-01",
        "Abstract": "Bayesian network methods have recently gained great popularity in machine learning literature and applications to model uncertainty in complex phenomena that include relationships between multiple random variables. However, these models are not commonly applied in economics and development studies. Here, we introduce the Bayesian network classifier models to estimate the probability of a person to be welfare deprived in one and multiple dimensions. These probabilities are then used for measuring vulnerability to multidimensional poverty (VMP) in four alternative measurement frameworks. Currently, two of them can be found in the literature, but have been estimated with Probit and Logit models, which are unidimensional strategies. Instead of that, in this study, we follow a multidimensional strategy to solve an estimation problem that is multidimensional in nature. Two new VMP measurement procedures based on Bayesian network classifiers estimates are also introduced in this article. We illustrate the four estimation procedures using the household survey and the census data from Chile 2017. A 5-fold cross-validation exercise verifies a high predictive performance of these Bayesian network classifier models, with the highest accuracy being that of one of the new measurements that we put forward. Our findings reveal that the Bayesian network classifier models offer an adequate alternative to face the policy challenge of measuring vulnerability to multidimensional poverty.",
        "DOI": "10.1016/j.eap.2021.11.018",
        "paper_author": "Gallardo M.",
        "affiliation_name": "Universidad Católica del Norte",
        "affiliation_city": "Antofagasta",
        "affiliation_country": "Chile",
        "affiliation_id": "60029195",
        "affiliation_state": "AN"
    },
    {
        "paper_title": "Evaluation of survey and remote sensing data products used to estimate land use change in the United States: Evolving issues and emerging opportunities",
        "publication": "Environmental Science and Policy",
        "citied_by": "22",
        "cover_date": "2022-03-01",
        "Abstract": "Transparent, consistent, and statistically reliable land use/ land cover area estimates are needed to assess land use change and greenhouse gas emissions associated with biofuel production and other land uses that are influenced by policy. As relevant studies have increased rapidly during past decades, the methods used to combine data extracted from land use land cover (LULC) surveys and remote sensing-based products and track or report sources of uncertainty vary notably. This paper reviews six data sources that are most commonly used to investigate LULC and change in the contiguous U.S. by highlighting the main characteristics, strengths and weaknesses and considering how uncertainty is assessed by the June Area Survey (JAS), the Census of Agriculture (COA), the Farm Survey Agency (FSA) acreage, the National Resources Inventory (NRI), the National Wetlands Inventory (NWI), and the Forest Inventory and Analysis (FIA); and two remote sensing-based data products, the Cropland Data Layer (CDL) and the National Land Cover Database (NLCD). The summary and conclusion identify important research gaps or challenges limiting current land use/land cover and change studies (e.g., lack of high-quality reference data and uncertainty quantification, etc.) and opportunities and emerging techniques (data fusion and machine learning) that will improve reliability of land use/land cover assessments and associated policies. Blended approaches that marry high quality ground truth data that are more finely resolved than data supplied by government surveys with multitemporal imagery are needed track use of non-agricultural lands vulnerable to agricultural expansion. These considerations are notably important as the U.S. considers the renewal and possibly revision of its Renewable Fuel Standard, which includes provisions that require monitoring of agricultural land expansion.",
        "DOI": "10.1016/j.envsci.2021.12.021",
        "paper_author": "Wang M.",
        "affiliation_name": "University of Illinois Urbana-Champaign",
        "affiliation_city": "Urbana",
        "affiliation_country": "United States",
        "affiliation_id": "60000745",
        "affiliation_state": "IL"
    },
    {
        "paper_title": "A Framework for Using Real-World Data and Health Outcomes Modeling to Evaluate Machine Learning–Based Risk Prediction Models",
        "publication": "Value in Health",
        "citied_by": "8",
        "cover_date": "2022-03-01",
        "Abstract": "Objectives: We propose a framework of health outcomes modeling with dynamic decision making and real-world data (RWD) to evaluate the potential utility of novel risk prediction models in clinical practice. Lung transplant (LTx) referral decisions in cystic fibrosis offer a complex case study. Methods: We used longitudinal RWD for a cohort of adults (n = 4247) from the Cystic Fibrosis Foundation Patient Registry to compare outcomes of an LTx referral policy based on machine learning (ML) mortality risk predictions to referral based on (1) forced expiratory volume in 1 second (FEV1) alone and (2) heterogenous usual care (UC). We then developed a patient-level simulation model to project number of patients referred for LTx and 5-year survival, accounting for transplant availability, organ allocation policy, and heterogenous treatment effects. Results: Only 12% of patients (95% confidence interval 11%-13%) were referred for LTx over 5 years under UC, compared with 19% (18%-20%) under FEV1 and 20% (19%-22%) under ML. Of 309 patients who died before LTx referral under UC, 31% (27%-36%) would have been referred under FEV1 and 40% (35%-45%) would have been referred under ML. Given a fixed supply of organs, differences in referral time did not lead to significant differences in transplants, pretransplant or post-transplant deaths, or overall survival in 5 years. Conclusions: Health outcomes modeling with RWD may help to identify novel ML risk prediction models with high potential real-world clinical utility and rule out further investment in models that are unlikely to offer meaningful real-world benefits.",
        "DOI": "10.1016/j.jval.2021.11.1360",
        "paper_author": "Rodriguez P.J.",
        "affiliation_name": "University of Washington",
        "affiliation_city": "Seattle",
        "affiliation_country": "United States",
        "affiliation_id": "60015481",
        "affiliation_state": "WA"
    },
    {
        "paper_title": "Modeling patent clarity",
        "publication": "Research Policy",
        "citied_by": "14",
        "cover_date": "2022-03-01",
        "Abstract": "This study uses machine learning techniques to model patent claim clarity and analyze how clarity relates to important patent policy objectives. Specifically, machine learning models are trained on a dataset of over 600,000 U.S. patent applications that were (or were not) rejected for indefiniteness, a proxy for claim clarity, using features based on the linguistic attributes of each application. The model is then applied to over 2 million issued patents and their corresponding applications, deriving estimates of the clarity of each patent's claim set at application and issuance. First, the properties of claim clarity and its relationship with the patent examination process are studied. Wordiness and repetitiveness corresponds to reduced clarity, whereas more descriptiveness whereas clearer claims tend to be more descriptive. Clarity also changes during patent examination, indicating that patent office policies may affect claim clarity. Next, the relationship between claim clarity and cumulative innovation is studied. Clear patents are found to receive more citations by applicants of unrelated future patents, a key indicator of cumulative innovation. However, unclear patents tend to receive more examiner citations, particularly in later years, and the technological relevance of examiner citations also tends to decline over time. This raises important questions about the role of late-stage examiner citations in the patent examination process, which are framed for future research. Finally, this study evaluates the impact of the U.S. Supreme Court's Nautilus v. Biosig decision, which sought to improve patent claim clarity. A difference-in-difference analysis of applications examined under the old versus new standard is conducted to evaluate the causal effect of Nautilus on the claims of patents filed under the old standard but examined under the new standard. This reveals a significant improvement in patent claim clarity post-Nautilus.",
        "DOI": "10.1016/j.respol.2021.104415",
        "paper_author": "Ashtor J.H.",
        "affiliation_name": "Benjamin N. Cardozo School of Law",
        "affiliation_city": "New York",
        "affiliation_country": "United States",
        "affiliation_id": "60097947",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "A real-time demand-side management system considering user preference with adaptive deep Q learning in home area network",
        "publication": "Sustainable Energy, Grids and Networks",
        "citied_by": "19",
        "cover_date": "2022-03-01",
        "Abstract": "With the increase in global energy consumption, the demand-side management (DSM) system has grown into an important research topic because of its ability to reduce the total electricity cost and peak-to-average ratio (PAR) by rescheduling loads. Besides, the large amount of sensor data in the home area network (HAN) helps to record the energy demand and achieve the DSM capacity, and the machine learning skills such as reinforcement learning can be applied to solve the DSM problem. However, determining a suitable energy management strategy is complicated because the user behaviors are uncertain. In this study, a real-time multi-agent DSM system based on HAN was proposed to find a suitable control policy for reducing the energy cost in a smart home. This system integrated the Deep Q-Network (DQN) agents that adaptively learn the preference of appliance usage to control different types of appliances and energy storage system. The simulation results show that the proposed DSM system reduced peak value, PAR value, and electricity cost by 28.9%, 20.9%, and 28.6% respectively. This system can also be applied to REDD dataset and achieved 74.9% cost reduction.",
        "DOI": "10.1016/j.segan.2021.100572",
        "paper_author": "Tai C.S.",
        "affiliation_name": "National Taiwan University",
        "affiliation_city": "Taipei",
        "affiliation_country": "Taiwan",
        "affiliation_id": "60005429",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A machine learning approach for predictive warehouse design",
        "publication": "International Journal of Advanced Manufacturing Technology",
        "citied_by": "21",
        "cover_date": "2022-03-01",
        "Abstract": "Warehouse management systems (WMS) track warehousing and picking operations, generating a huge volumes of data quantified in millions to billions of records. Logistic operators incur significant costs to maintain these IT systems, without actively mining the collected data to monitor their business processes, smooth the warehousing flows, and support the strategic decisions. This study explores the impact of tracing data beyond the simple traceability purpose. We aim at supporting the strategic design of a warehousing system by training classifiers that can predict the storage technology (ST), the material handling system (MHS), the storage allocation strategy (SAS), and the picking policy (PP) of a storage system. We introduce the definition of a learning table, whose attributes are benchmarking metrics applicable to any storage system. Then, we investigate how the availability of data in the warehouse management system (i.e. varying the number of attributes of the learning table) affects the accuracy of the predictions. To validate the approach, we illustrate a generalisable case study which collects data from sixteen different real companies belonging to different industrial sectors (automotive, manufacturing, food and beverage, cosmetics and publishing) and different players (distribution centres and third-party logistic providers). The benchmarking metrics are applied and used to generate learning tables with varying number of attributes. A bunch of classifiers is used to identify the crucial input data attributes in the prediction of ST, MHS, SAS, and PP. The managerial relevance of the data-driven methodology for warehouse design is showcased for 3PL providers experiencing a fast rotation of the SKUs stored in their storage systems.",
        "DOI": "10.1007/s00170-021-08035-w",
        "paper_author": "Tufano A.",
        "affiliation_name": "Alma Mater Studiorum Università di Bologna",
        "affiliation_city": "Bologna",
        "affiliation_country": "Italy",
        "affiliation_id": "60028218",
        "affiliation_state": "BO"
    },
    {
        "paper_title": "Distributed deep reinforcement learning-based coordination performance optimization method for proton exchange membrane fuel cell system",
        "publication": "Sustainable Energy Technologies and Assessments",
        "citied_by": "11",
        "cover_date": "2022-03-01",
        "Abstract": "A proton exchange membrane fuel cell (PEMFC) is a multi-input multi-output nonlinear complicated system, the output power and operating efficiency of which are affected by various operating variables. To achieve coordinated control of multiple operating variables and thus improve the stability, performance, and efficiency of the PEMFC power system, a data-driven power coordination management method is proposed. Furthermore, a metaverse-based multiagent double delay deep deterministic policy gradient (MET-MADDPG) algorithm is also presented. The design of this algorithm is structured on the concept of the metaverse in that it employs different metaverses during pre-learning and combines imitation learning and curriculum learning to enable agents in different combinations to be fully trained in different environments to improve coordination strategy robustness. In this algorithm, the hydrogen controller, air controller, water pump controller and radiator controller are regarded as four agents; the objective of the training is to enable agents with their different goals to coordinate. The effectiveness of the proposed algorithm is demonstrated in a series of experiments in which it is compared with existing algorithms.",
        "DOI": "10.1016/j.seta.2021.101814",
        "paper_author": "Li J.",
        "affiliation_name": "South China University of Technology",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60024542",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Examining non-linear built environment effects on injurious traffic collisions: A gradient boosting decision tree analysis",
        "publication": "Journal of Transport and Health",
        "citied_by": "26",
        "cover_date": "2022-03-01",
        "Abstract": "Introduction: During rapid urbanization, the optimization of a built environment (BE) in metropolises to reduce the frequency of traffic collisions can considerably promote traffic safety. Previous studies have seldom discussed the non-linearity and threshold effects of the BE on injurious collisions, and they propose few evidence-based synergistic effects for policy development. Methods: With the injurious collision frequency as the dependent variable, this study established a BE indicator system including travel demand, roadway design, non-motorized infrastructure, land use, traffic control and disadvantage neighborhoods as explanatory variables. The gradient boosting decision tree (GBDT) model based on machine learning was used to explore the relative importance and the non-linearity of each variable. Furthermore, bivariate partial dependence plots were applied to reflect the synergistic effects of intersection density and other high-contribution BE variables on injurious collision frequency. Results: (1) The riding volume, intersection density, and distance from CBDs are the top three contributing variables, which should attract more attention when planning. (2) Several thresholds were found: controling the width of arterial roads within 25 m, limiting the average vehicle speed in 36–38 km/h, setting subway stations within 1 km away from key city locations, adjusting the land use mix below 0.6, regulating the density of floating population around 3000 people/km2and setting the greenway density at 2 km/km2are all beneficial to reduce the injurious collision frequency. (3) Retaining the above thresholds from univariate PDPs, the synergistic effects not only causes high collision frequency, but also improves the risk growth rate. Conclusion: Aiming at the variables with high contribution rates and exerting these threshold effects in transportation plans will effectively reduce the occurrence of injurious collisions.",
        "DOI": "10.1016/j.jth.2021.101296",
        "paper_author": "An R.",
        "affiliation_name": "Wuhan University",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60029306",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Generative Adversarial Network Approach to Future Sermonizing of Housing Dispersal in Emerging Cities",
        "publication": "Journal of Urban Planning and Development",
        "citied_by": "2",
        "cover_date": "2022-03-01",
        "Abstract": "This study aims to visualize the future housing dispersal of expatriates, based on the predicted urban growth in emerging cities. Generalized adversarial networks (GANs) will be utilized to predict the future urban growth of Doha Metropolitan emerging city. The housing dispersal of expatriates will be visualized on the predicted urban growth map to investigate housing preferences, which will be based on Gordon's theory. This study will prove the feasibility of a process approach when practicing the management of urban growth in emerging cities worldwide. It could be a robust solution for the worsening imbalance in the urban morphology of metropolitan cities. The findings of the broad-spectrum housing dispersal guidelines could benefit the policymakers and planners for the realities of spatial patterns and future urban growth.",
        "DOI": "10.1061/(ASCE)UP.1943-5444.0000783",
        "paper_author": "Ibrahim H.",
        "affiliation_name": "Department of Architecture and Urban Planning, College of Engineering, Qatar University",
        "affiliation_city": "Doha",
        "affiliation_country": "Qatar",
        "affiliation_id": "60197134",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Model-Based Reinforcement Learning Protocol for Routing in Vehicular Ad hoc Network",
        "publication": "Wireless Personal Communications",
        "citied_by": "12",
        "cover_date": "2022-03-01",
        "Abstract": "Todays by equipping vehicles with wireless technologies, Vehicular Ad Hoc Network (VANET) has been emerged. This type of network can be utilized in many fields such as emergency, safety or entertainment. It is also considered as a main component of intelligent transportation system. However, due to the nodes velocity (vehicles velocity), varying density, obstacles and lack of fixed infrastructure, finding and maintaining a route between nodes are always challenging in VANET. Any routing protocol can be effective only if the nodes can learn and adapt themselves with such a dynamic environment. One way to achieve this adaptation is using machine learning techniques. In this paper we try to reach this goal by applying Multi-Agent Reinforcement Learning (MARL) that enables agents to solve routing optimization problems in a distributed way. Although model-free Reinforcement Learning (RL) schemes are introduced for this purpose, such techniques learn using a trial and error scheme in a real environment so they cannot reach an optimal policy in a short time. To deal with such a problem, we have proposed a mode-based RL based routing scheme. We have also developed a Fuzzy Logic (FL) system to evaluate the quality of links between neighbor nodes based on parameters such as velocity and connection quality. Outputs of this fuzzy system have been used to form the state transition model, needed in MARL. Results of evaluations have shown that our approach can improve some routing metrics like delivery ratio, end-to-end delay and traffic overhead.",
        "DOI": "10.1007/s11277-021-09166-9",
        "paper_author": "Jafarzadeh O.",
        "affiliation_name": "Islamic Azad University, Qazvin Branch",
        "affiliation_city": "Qazvin",
        "affiliation_country": "Iran",
        "affiliation_id": "60013126",
        "affiliation_state": "Qazvin Province"
    },
    {
        "paper_title": "Assessment of the soil fertility status in Benin (West Africa) – Digital soil mapping using machine learning",
        "publication": "Geoderma Regional",
        "citied_by": "34",
        "cover_date": "2022-03-01",
        "Abstract": "A soil fertility index map (SFIm) can provide key information to decision-makers in regard to spatial planning in the context of sustainable land management. The establishment of such SFIm requires basic soil properties that can be modelled for spatial mapping. The objective of this study was to take advantage of Benin soil legacy data to produce a digital SFIm at a national level based on 8 soil properties (soil organic matter, nitrogen, pH (water), exchangeable potassium, assimilable phosphorus, sum of bases, cation exchange capacity and base saturation). Specific research aims were (1) to model and develop digital soil maps, (2) to identify the key covariates influencing soil nutrients, and (3) to build an SFIm using digital maps of the soil properties. For each soil property, modelling procedures involved the use of different covariates, including soil type, topographic, bioclimatic and spectral data, along with the comparative assessment of the cubist (CB) and quantile random forest (QRF) models. Models were evaluated not only on the basis of classical error metrics (RMSE, R2) but also on the ability to predict local uncertainty based on the prediction interval coverage probability (PICP). The results revealed that CB performed marginally better than the QRF based on classical error metrics (R2, RMSE) but produced the worst uncertainty with an overestimation of the local uncertainty. This suggested that the use of accuracy plots such as PICP to evaluate models can identify accuracy problems not evident with classical error metrics. The analysis revealed that the distance to the nearest stream, which was part of topographic covariates, had strong predictive ability for all the soil properties along with the bioclimatic variables. The spatial distribution of the different classes of SFIm showed a preponderance of low fertility levels with severe limitations for crop development. A limited number of high and average fertility level soils were found in the low elevation areas of southern Benin, and policy could advocate for their sole use for agricultural purposes and promote sustainable management practices.",
        "DOI": "10.1016/j.geodrs.2021.e00444",
        "paper_author": "Hounkpatin K.O.L.",
        "affiliation_name": "Sveriges lantbruksuniversitet",
        "affiliation_city": "Uppsala",
        "affiliation_country": "Sweden",
        "affiliation_id": "60027152",
        "affiliation_state": "Uppsala"
    },
    {
        "paper_title": "What patients want to know about shoulder arthroplasty: a Google search analysis",
        "publication": "Seminars in Arthroplasty JSES",
        "citied_by": "5",
        "cover_date": "2022-03-01",
        "Abstract": "Background: People are increasingly turning to online search engines for health information. We analyzed Google search queries to determine the most frequently asked topics and questions related to shoulder arthroplasty and the sources of the websites provided to address these questions. Methods: Search terms for shoulder arthroplasty were entered into Google. Machine learning–based natural language processing algorithms within Google were used to mine search logs to identify frequently asked questions about shoulder arthroplasty. These questions with their associated websites were extracted and categorized by two independent reviewers. Results: A total of 300 questions were extracted with 156 associated websites. The most popular question topics were related to the timeline of recovery (16.3%), ability to perform specific activities (15.7%), technical details of surgery (14.3%), and activity restrictions (12%). Frequently asked questions had to do with the fastest means of recovery, duration of physical therapy, sling use, time spent sleeping upright in a recliner, time off work, location of surgical incision, resumption of sports, and the ability to use the bathroom. The most common websites provided to address the questions were academic (25.3%), private practice (23%), and government (20.3%). Conclusion: The most frequently asked questions on Google about shoulder arthroplasty centered around postoperative rehabilitation and restrictions, with information originating from variable sources. Our data can be used to counsel patients and set expectations about recovery and serve as a framework for the development of targeted patient education materials. Level of evidence: Level VI; Descriptive Study",
        "DOI": "10.1053/j.sart.2021.08.005",
        "paper_author": "Sudah S.Y.",
        "affiliation_name": "Monmouth Medical Center",
        "affiliation_city": "Long Branch",
        "affiliation_country": "United States",
        "affiliation_id": "60021467",
        "affiliation_state": "NJ"
    },
    {
        "paper_title": "Zero-Shot Adaptation for mmWave Beam-Tracking on Overhead Messenger Wires Through Robust Adversarial Reinforcement Learning_net",
        "publication": "IEEE Transactions on Cognitive Communications and Networking",
        "citied_by": "7",
        "cover_date": "2022-03-01",
        "Abstract": "Millimeter wave (mmWave) beam-tracking based on machine learning enables the development of accurate tracking policies while obviating the need to periodically solve beam-optimization problems. However, its applicability is still arguable when training-test gaps exist in terms of environmental parameters that affect the node dynamics. From this skeptical point of view, the contribution of this study is twofold. First, by considering an example scenario, we confirm that the training-test gap adversely affects the beam-tracking performance. More specifically, we consider nodes placed on overhead messenger wires, where the node dynamics are affected by several environmental parameters, e.g., the wire mass and tension. Although these are particular scenarios, they yield insight into the validation of the training-test gap problems. Second, we demonstrate the feasibility of zero-shot adaptation as a solution, where a learning agent adapts to environmental parameters unseen during training. This is achieved by leveraging a robust adversarial reinforcement learning (RARL) technique, where such training-and-test gaps are regarded as disturbances by adversaries that are jointly trained with a legitimate beam-tracking agent. Numerical evaluations demonstrate that the beam-tracking policy learned via RARL can be applied to a wide range of environmental parameters without severely degrading the received power.",
        "DOI": "10.1109/TCCN.2021.3116231",
        "paper_author": "Shinzaki M.",
        "affiliation_name": "Graduate School of Informatics",
        "affiliation_city": "Kyoto",
        "affiliation_country": "Japan",
        "affiliation_id": "60119325",
        "affiliation_state": "Kyoto"
    },
    {
        "paper_title": "Communications between Borrowers and Servicers: Evidence from COVID-19 Mortgage Forbearance Program",
        "publication": "Quarterly Journal of Finance",
        "citied_by": "3",
        "cover_date": "2022-03-01",
        "Abstract": "In this paper, I utilize proprietary servicer call transcripts between a single servicer and the corresponding borrowers, whose loans they service, to shed light on borrower responses to the mortgage forbearance program contained in the CARES Act. My analysis reveals that borrowers (especially with non-performing loans) did not actively seek out mortgage forbearance (conditional on communication) in response to this policy, which was intended to prevent a pandemic-induced foreclosure. This is an outcome of the servicer's differential treatment between government and private loans, as the CARES Act was designed for government loans only and left scope for servicer discretion for private loans. These results bring into question the effectiveness of ad hoc laws and the implementation thereof during the COVID-19 pandemic.",
        "DOI": "10.1142/S2010139222400043",
        "paper_author": "Bandyopadhyay A.P.",
        "affiliation_name": "Leeds School of Business",
        "affiliation_city": "Boulder",
        "affiliation_country": "United States",
        "affiliation_id": "60093261",
        "affiliation_state": "CO"
    },
    {
        "paper_title": "Model-based deep reinforcement learning for wind energy bidding",
        "publication": "International Journal of Electrical Power and Energy Systems",
        "citied_by": "24",
        "cover_date": "2022-03-01",
        "Abstract": "Wind energy is an important source of clean energy. Due to the common trade through bidding, many attempts have been made to apply deep reinforcement learning techniques to generate appropriate bidding policies to maximize profits. However, these algorithms are based entirely on a model-free strategy. The present study aims to develop a dynamic model capable of strategic bidding for wind energy. Thus, the model MB-A3C is implemented and proves to be quite resilient. Herein, “Nord Pool”, a conventional benchmark that comprises six datasets representing each wind power site in Denmark and Sweden is duly investigated. Results show that the policies generated by MB-A3C are less costly than those produced by both previous model-free and model-based algorithms i.e. Conv-A3C, DPPO, DDPG, and MBPG. The optimal bidding approach demonstrated in this study can be utilized to optimize profits and overcome the uncertainties in both the energy and reserve markets.",
        "DOI": "10.1016/j.ijepes.2021.107625",
        "paper_author": "Sanayha M.",
        "affiliation_name": "Chulalongkorn University",
        "affiliation_city": "Bangkok",
        "affiliation_country": "Thailand",
        "affiliation_id": "60028190",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Multi-objective Q-learning-based hyper-heuristic with Bi-criteria selection for energy-aware mixed shop scheduling",
        "publication": "Swarm and Evolutionary Computation",
        "citied_by": "46",
        "cover_date": "2022-03-01",
        "Abstract": "Owning to diverse customer demands and enormous product varieties, mixed shop production systems are applied in practice to improve responsiveness and productivity along with energy-saving. This work addresses a mixture of job-shop and flow-shop production scheduling problem with a speed-scaling policy and no-idle time strategy. A mixed-integer linear programming model is formulated to determine the speed level of operations and the sequence of job-shop and flow-shop products, aiming at the simultaneous optimization of production efficiency and energy consumption. Then, a multi-objective Q-learning-based hyper-heuristic with Bi-criteria selection (QHH-BS) is developed to obtain a set of high-quality Pareto frontier solutions. In this algorithm, a new three-layer encoding is designed to represent the production sequence of job-shop and flow-shop products; the Pareto-based and indicator-based selection criteria are sequentially implemented to encourage diversity and convergence; Q-learning with a multi-objective metric-based reward mechanism is applied to select an optimizer from three prominent optimizers in each iteration for better exploration and exploitation. Three conclusions are drawn from extensive experiments: (1) Bi-criteria selection is superior to single-criterion selections; (2) Q-learning-based hyper-heuristic is more effective and robust than single optimizer-based algorithms and simple hyper-heuristics; (3) QHH-BS outperforms the existing state-of-the-art multi-objective algorithms in convergence and diversity.",
        "DOI": "10.1016/j.swevo.2021.100985",
        "paper_author": "Cheng L.",
        "affiliation_name": "Wuhan University of Science and Technology",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60014643",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "DAS: Dynamic Adaptive Scheduling for Energy-Efficient Heterogeneous SoCs",
        "publication": "IEEE Embedded Systems Letters",
        "citied_by": "6",
        "cover_date": "2022-03-01",
        "Abstract": "Domain-specific systems-on-chip (DSSoCs) aim at bridging the gap between application-specific integrated circuits (ASICs) and general-purpose processors. Traditional operating system (OS) schedulers can undermine the potential of DSSoCs since their execution times can be orders of magnitude larger than the execution time of the task itself. To address this problem, we propose a dynamic adaptive scheduling (DAS) framework that combines the benefits of a fast (low-overhead) scheduler and a slow (sophisticated, high-performance but high-overhead) scheduler. Experiments with five real-world streaming applications show that DAS consistently outperforms both the fast and slow schedulers. For 40 different workloads, DAS achieves 1.29× speedup and 45% lower EDP than the sophisticated scheduler at low data rates and 1.28× speedup and 37% lower EDP than the fast scheduler when the workload intensifies.",
        "DOI": "10.1109/LES.2021.3110426",
        "paper_author": "Goksoy A.A.",
        "affiliation_name": "UW-Madison College of Engineering",
        "affiliation_city": "Madison",
        "affiliation_country": "United States",
        "affiliation_id": "60153131",
        "affiliation_state": "WI"
    },
    {
        "paper_title": "Prediction of global spread of COVID-19 pandemic: a review and research challenges",
        "publication": "Artificial Intelligence Review",
        "citied_by": "8",
        "cover_date": "2022-03-01",
        "Abstract": "Since the initial reports of the Coronavirus surfacing in Wuhan, China, the novel virus currently without a cure has spread like wildfire across the globe, the virus spread exponentially across all inhabited continent, catching local governments by surprise in many cases and bringing the world economy to a standstill. As local authorities work on a response to deal with the virus, the scientific community has stepped in to help analyze and predict the pattern and conditions that would influence the spread of this unforgiving virus. Using existing statistical modeling tools to the latest artificial intelligence technology, the scientific community has used public and privately available data to help with predictions. A lot of this data research has enabled local authorities to plan their response—whether that is to deploy tightly available medical resources like ventilators or how and when to enforce policies to social distance, including lockdowns. On the one hand, this paper shows what accuracy of research brings to enable fighting this disease; while on the other hand, it also shows what lack of response from local authorities can do in spreading this virus. This is our attempt to compile different research methods and comparing their accuracy in predicting the spread of COVID-19.",
        "DOI": "10.1007/s10462-021-09988-w",
        "paper_author": "Shah S.",
        "affiliation_name": "Saginaw Valley State University",
        "affiliation_city": "Saginaw",
        "affiliation_country": "United States",
        "affiliation_id": "60019713",
        "affiliation_state": "MI"
    },
    {
        "paper_title": "Simulation of the pollution abatement behavior of regional metal-related enterprises based on the interactive perspective of industrial agglomerations and emission reduction effects",
        "publication": "Environmental Geochemistry and Health",
        "citied_by": "3",
        "cover_date": "2022-03-01",
        "Abstract": "A machine learning method was used to process a multiagent information database to study the spatial distribution characteristics of agglomerations of metal-related enterprises and to analyze the spatial and temporal differentiation characteristics of pollution reduction in metal-related enterprises. Based on the spatial distribution of enterprises and a simulation of their pollution reduction behaviors, the layout of 380 enterprises sample is optimized, and the direction of industrial transfer is planned to give full play to the pollution reduction effect of enterprise agglomeration. The results showed that (1) the metal-related enterprises in the Chang-Zhu-Tan urban agglomeration have obvious spatial heterogeneity and are mainly distributed in the district of Changsha, the Qingshuitang Industrial Zone, Liling city and the Qibaoshan Industrial Zone of Liuyang city, while the metal-related enterprises in Shaoshan city, Zhuzhou County and Liling city are scattered. (2) The pollution emission behaviors of enterprises differ in time and space, and the pollution concentrations are highest in industrial parks such as Qingshuitang and Zhubu Port. (3) There is an interactive relationship between the degree of enterprise agglomeration and the pollution reduction effect. The spatial positive coupling degree between the concentration of metal-related enterprises and the degree of metal-related pollution is significant, accounting for 94.96% of the study area. Low pollution-high agglomeration areas, high pollution-low agglomeration areas, high pollution-high agglomeration areas, and low pollution-low agglomeration area account for 1.01%, 4.03%, 2.87%, and 92.09% of the study area, respectively. Finally, based on the new development concept of dual circulation and the theory of a two-oriented society in the new era, the paper puts forward suggestions and policies for the sustainable development of industrial transfer.",
        "DOI": "10.1007/s10653-021-01015-9",
        "paper_author": "Xiong L.X.",
        "affiliation_name": "Central South University of Forestry and Technology",
        "affiliation_city": "Changsha",
        "affiliation_country": "China",
        "affiliation_id": "60005949",
        "affiliation_state": "Hunan"
    },
    {
        "paper_title": "Online tribunal judgments and the limits of open justice",
        "publication": "Legal Studies",
        "citied_by": "4",
        "cover_date": "2022-03-01",
        "Abstract": "The principle of open justice is a constituent element of the rule of law: it demands publicity of legal proceedings, including the publication of judgments. Since 2017, the UK government has systematically published first instance Employment Tribunal decisions in an online repository. Whilst a veritable treasure trove for researchers and policy makers, the database also has darker potential – from automating blacklisting to creating new and systemic barriers to access to justice. Our scrutiny of existing legal safeguards, from anonymity orders to equality law and data protection, finds a number of gaps, which threaten to make the principle of open justice as embodied in the current publication regime inimical to equal access to justice.",
        "DOI": "10.1017/lst.2021.30",
        "paper_author": "Adams Z.",
        "affiliation_name": "King's College Cambridge",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60100632",
        "affiliation_state": "Cambridgeshire"
    },
    {
        "paper_title": "Data-driven test strategy for COVID-19 using machine learning: A study in Lahore, Pakistan",
        "publication": "Socio-Economic Planning Sciences",
        "citied_by": "6",
        "cover_date": "2022-03-01",
        "Abstract": "Aims: We aimed at giving a preliminary analysis of the weakness of a current test strategy, and proposing a data-driven strategy that was self-adaptive to the dynamic change of pandemic. The effect of driven-data selection over time and space was also within the deep concern. Methods: A mathematical definition of the test strategy were given. With the real COVID-19 test data from March to July collected in Lahore, a significance analysis of the possible features was conducted. A machine learning method based on logistic regression and priority ranking were proposed for the data-driven test strategy. With performance assessed by the area under the receiver operating characteristic curve (AUC), time series analysis and spatial cross-test were conducted. Results: The transition of risk factors accounted for the failure of the current test strategy. The proposed data-driven strategy could enhance the positive detection rate from 2.54% to 28.18%, and the recall rate from 8.05% to 89.35% under strictly limited test capacity. Much more optimal utilization of test resources could be realized where 89.35% of total positive cases could be detected with merely 48.17% of the original test amount. The strategy showed self-adaptability with the development of pandemic, while the strategy driven by local data was proved to be optimal. Conclusions: We recommended a generalization of such a data-driven test strategy for a better response to the global developing pandemic. Besides, the construction of the COVID-19 data system should be more refined on space for local applications.",
        "DOI": "10.1016/j.seps.2021.101091",
        "paper_author": "Huang C.",
        "affiliation_name": "University of Science and Technology of China",
        "affiliation_city": "Hefei",
        "affiliation_country": "China",
        "affiliation_id": "60019118",
        "affiliation_state": "Anhui"
    },
    {
        "paper_title": "Using Integrated City Data and Machine Learning to Identify and Intervene Early on Housing-Related Public Health Problems",
        "publication": "Journal of Public Health Management and Practice",
        "citied_by": "5",
        "cover_date": "2022-03-01",
        "Abstract": "Context: Housing is more than a physical structure-it has a profound impact on health. Enforcing housing codes is a primary strategy for breaking the link between poor housing and poor health. Objective: The objective of this study was to determine whether machine learning algorithms can identify properties with housing code violations at a higher rate than inspector-informed prioritization. We also show how city data can be used to describe the prevalence and location of housing-related health risks, which can inform public health policy and programs. Setting: This study took place in Chelsea, Massachusetts, a demographically diverse, densely populated, low-income city near Boston. Design: Using data from 1611 proactively inspected properties, representative of the city's housing stock, we developed machine learning models to predict the probability that a given property would have (1) any housing code violation, (2) a set of high-risk health violations, and (3) a specific violation with a high risk to health and safety (overcrowding). We generated predicted probabilities of each outcome for all residential properties in the city (N = 5989). Results: Housing code violations were present in 54% of inspected properties, 85% of which were classified as high-risk health violations. We predict that if the city were to use integrated city data and machine learning to identify at-risk properties, it could achieve a 1.8-fold increase in the number of inspections that identify code violations as compared with current practices. Conclusion: Given the strong connection between housing and health, reducing public health risk at more properties-without the need for additional inspection resources-represents an opportunity for significant public health gains. Integrated city data and machine learning can be used to describe the prevalence and location of housing-related health problems and make housing code enforcement more efficient, effective, and equitable in responding to public health threats.",
        "DOI": "10.1097/PHH.0000000000001343",
        "paper_author": "Robb K.",
        "affiliation_name": "John F. Kennedy School of Government",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States",
        "affiliation_id": "60006332",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Cross-country differences in the size of venture capital financing rounds: a machine learning approach",
        "publication": "Empirical Economics",
        "citied_by": "3",
        "cover_date": "2022-03-01",
        "Abstract": "We analyze the potential determinants of the size of venture capital financing rounds. We employ stacked generalization and boosted trees, two of the most powerful machine learning tools in terms of predictive power, to examine a large data set on start-ups, venture capital funds and financing transactions. We find that the size of financing rounds is mainly associated with the characteristics of the firms being financed and with the features of the countries in which the firms are headquartered. Cross-country differences in the degree of development of the venture capital industry, while highly correlated with the size of funding rounds, are not significant once we control for other country-level characteristics. We discuss how our findings contribute to the debate about policy interventions aimed at stimulating start-up financing.",
        "DOI": "10.1007/s00181-021-02066-8",
        "paper_author": "Taboga M.",
        "affiliation_name": "Banca d'Italia",
        "affiliation_city": "Rome",
        "affiliation_country": "Italy",
        "affiliation_id": "60082963",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Uncovering the significant socio-economic attributes of low- and high-emission countries using rough sets",
        "publication": "Clean Technologies and Environmental Policy",
        "citied_by": "5",
        "cover_date": "2022-03-01",
        "Abstract": "Drivers to global carbon emissions have been widely investigated in the scientific literature. However, most previous studies have been limited to supervised learning approaches such as decomposition analysis. Thus, the effects of more specific socio-economic factors such as research expenditure, poverty incidence, education level and trading of goods have seldom been probed into, and often, drivers significantly vary from country to country. However, it is hypothesized in this study that patterns can be derived among high- and low-emission countries using a more detailed approach. Thus, a novel approach using rough sets is developed to uncover the effects of detailed socio-economic attributes to the emissions of 194 countries and regions. A significant advantage of rough set theory is its ability to work with incomplete data sets. As comprehensive as they are, global data sets such as that of the World Bank would still have gaps especially in less developed countries. The rough set model developed in this study has a validity of 94%. The most significant factors for low-income countries are having low to mid agricultural exports and having mid to high pump prices for diesel. For high-income countries, this was having high gross domestic product per capita. Overall, the most interesting insight from this study is that countries do not simply grow as they increase emissions, but also evolve. As countries develop, they also change their priority sectors, policies and demographics. Moreover, the findings suggest that the understanding of low-emission countries benefits more from a comprehensive study. High-emission countries have been well studied in the literature already. Graphical abstract: [Figure not available: see fulltext.].",
        "DOI": "10.1007/s10098-021-02067-2",
        "paper_author": "Lopez N.S.",
        "affiliation_name": "De La Salle University",
        "affiliation_city": "Manila",
        "affiliation_country": "Philippines",
        "affiliation_id": "60071464",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A data analytics approach for COVID-19 spread and end prediction (with a case study in Iran)",
        "publication": "Modeling Earth Systems and Environment",
        "citied_by": "14",
        "cover_date": "2022-03-01",
        "Abstract": "World is now experiencing the new pandemic caused by COVID-19 virus and all countries are affected by this disease specially Iran. From the beginning of the outbreak until April 30, 2020, over 90,000 confirmed cases of COVID-19 have been reported in Iran. Due to socio-economic problems of this disease, it is required to predict the trend of the outbreak and propose a beneficial method to find out the correct trend. In this paper, we compiled a dataset including the number of confirmed cases, the daily number of death cases and the number of recovered cases. Furthermore, by combining case number variables like behavior and policies that are changing over time and machine-learning (ML) algorithms such as logistic function using inflection point, we created new rates such as weekly death rate, life rate and new approaches to mortality rate and recovery rate. Gaussian functions show superior performance which is helpful for government to improve its awareness about important factors that have significant impacts on future trends of this virus.",
        "DOI": "10.1007/s40808-021-01086-8",
        "paper_author": "Behnam A.",
        "affiliation_name": "Iran University of Science and Technology",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran",
        "affiliation_id": "60012835",
        "affiliation_state": "Tehran"
    },
    {
        "paper_title": "Between technochauvinism and human-centrism: Can algorithms improve decision-making in democratic politics?",
        "publication": "European Political Science",
        "citied_by": "14",
        "cover_date": "2022-03-01",
        "Abstract": "The promise of algorithmic decision-making (ADM) lies in its capacity to support or replace human decision-making based on a superior ability to solve specific cognitive tasks. Applications have found their way into various domains of decision-making—and even find appeal in the realm of politics. Against the backdrop of widespread dissatisfaction with politicians in established democracies, there are even calls for replacing politicians with machines. Our discipline has hitherto remained surprisingly silent on these issues. The present article argues that it is important to have a clear grasp of when and how ADM is compatible with political decision-making. While algorithms may help decision-makers in the evidence-based selection of policy instruments to achieve pre-defined goals, bringing ADM to the heart of politics, where the guiding goals are set, is dangerous. Democratic politics, we argue, involves a kind of learning that is incompatible with the learning and optimization performed by algorithmic systems.",
        "DOI": "10.1057/s41304-020-00298-3",
        "paper_author": "König P.D.",
        "affiliation_name": "Rheinland-Pfälzische Technische Universität Kaiserslautern-Landau",
        "affiliation_city": "Kaiserslautern",
        "affiliation_country": "Germany",
        "affiliation_id": "60280671",
        "affiliation_state": "Rheinland-Pfalz"
    },
    {
        "paper_title": "Online Minimax Q Network Learning for Two-Player Zero-Sum Markov Games",
        "publication": "IEEE Transactions on Neural Networks and Learning Systems",
        "citied_by": "65",
        "cover_date": "2022-03-01",
        "Abstract": "The Nash equilibrium is an important concept in game theory. It describes the least exploitability of one player from any opponents. We combine game theory, dynamic programming, and recent deep reinforcement learning (DRL) techniques to online learn the Nash equilibrium policy for two-player zero-sum Markov games (TZMGs). The problem is first formulated as a Bellman minimax equation, and generalized policy iteration (GPI) provides a double-loop iterative way to find the equilibrium. Then, neural networks are introduced to approximate Q functions for large-scale problems. An online minimax Q network learning algorithm is proposed to train the network with observations. Experience replay, dueling network, and double Q-learning are applied to improve the learning process. The contributions are twofold: 1) DRL techniques are combined with GPI to find the TZMG Nash equilibrium for the first time and 2) the convergence of the online learning algorithm with a lookup table and experience replay is proven, whose proof is not only useful for TZMGs but also instructive for single-agent Markov decision problems. Experiments on different examples validate the effectiveness of the proposed algorithm on TZMG problems.",
        "DOI": "10.1109/TNNLS.2020.3041469",
        "paper_author": "Zhu Y.",
        "affiliation_name": "Institute of Automation Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60018486",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "E-learning of medical residents during COVID-19: perspective from a developing nation",
        "publication": "Postgraduate Medical Journal",
        "citied_by": "3",
        "cover_date": "2022-03-01",
        "Abstract": "NA",
        "DOI": "10.1136/postgradmedj-2020-139022",
        "paper_author": "Ish P.",
        "affiliation_name": "VMMC &amp; Safdarjang Hospital",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India",
        "affiliation_id": "60016396",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Domain-General Tutor Authoring with Apprentice Learner Models",
        "publication": "International Journal of Artificial Intelligence in Education",
        "citied_by": "17",
        "cover_date": "2022-03-01",
        "Abstract": "Intelligent tutoring systems are effective for improving students’ learning outcomes (Pane et al. 2013; Koedinger and Anderson, International Journal of Artificial Intelligence in Education, 8, 1–14, 1997; Bowen et al. Journal of Policy Analysis and Management, 1, 94–111 2013). However, constructing tutoring systems that are pedagogically effective has been widely recognized as a challenging problem (Murray 2003; Murray, International Journal of Artificial Intelligence in Education, 10, 98–129, 1999). In this paper, we explore the use of computational models of apprentice learning, or computer models that learn interactively from examples and feedback, for authoring expert-models via demonstrations and feedback (Matsuda et al. International Journal of Artificial Intelligence in Education, 25(1), 1–34 2014) across a wide range of domains. To support these investigations, we present the Apprentice Learner Architecture, which posits the types of knowledge, performance, and learning components needed for apprentice learning. We use this architecture to create two models: the Decision Tree model, which non-incrementally learns skills, and the Trestle model, which instead learns incrementally. Both models draw on the same small set of prior knowledge (six operators and three types of relational knowledge) to support expert model authoring. Despite their limited prior knowledge, we demonstrate their use for efficiently authoring a novel experimental design tutor and show that they are capable of learning an expert model for seven additional tutoring systems that teach a wide range of knowledge types (associations, categories, and skills) across multiple domains (language, math, engineering, and science). This work shows that apprentice learner models are efficient for authoring tutors that would be difficult to build with existing non-programmer authoring approaches (e.g., experimental design or stoichiometry tutors). Further, we show that these models can be applied to author tutors across eight tutor domains even though they only have a small, fixed set of prior knowledge. This work lays the foundation for new interactive machine-learning based authoring paradigms that empower teachers and other non-programmers to build pedagogically effective educational technologies at scale.",
        "DOI": "10.1007/s40593-020-00214-2",
        "paper_author": "MacLellan C.J.",
        "affiliation_name": "College of Computing &amp; Informatics",
        "affiliation_city": "Philadelphia",
        "affiliation_country": "United States",
        "affiliation_id": "60139958",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "TD-PSO: Task distribution approach based on particle swarm optimization for vehicular ad hoc network",
        "publication": "Transactions on Emerging Telecommunications Technologies",
        "citied_by": "11",
        "cover_date": "2022-03-01",
        "Abstract": "The rapid advancement of the artificial intelligence revolution during the past decade has significantly affected vehicular ad hoc networks (VANETs). Several applications have been introduced that must meet the requirements of a VANET, including automatic driving and preaccident alerts and broadcasting of video. The customization of vehicles for implementation of these applications is costly and might not be possible due to many constraints, particularly resource limitations. In order to achieve compliance with the VANET framework within the resource limitations, this article proposes limiting the time frame related with the individual parts of the process. To this end, this article recommends that the resource-intensive ciphertext-policy attribute-based encryption (CP-ABE) task be simplified by virtue of partitioning it into subtasks. This can be achieved by a machine-learning technique (decision tree) in a manner that significantly influences the completion times of all subtasks. An approach based on particle swarm optimization (PSO), called task-distribution PSO (TD-PSO), is proposed to perform the CP-ABE task distribution on a VANET. The performance of this approach is evaluated by comparison with a genetic algorithm (GA), followed by comparison of these two solutions with the optimal solution proposed by the linear programming (LP) method. Results show that the TD-PSO approach consumes less overhead than the GA. Moreover, comparison with the optimal solution proposed by LP shows that the near-optimal solution obtained using TD-PSO is more accurate than that obtained using the GA in most scenarios.",
        "DOI": "10.1002/ett.3860",
        "paper_author": "Bany Taha M.",
        "affiliation_name": "École de Technologie Supérieure",
        "affiliation_city": "Montreal",
        "affiliation_country": "Canada",
        "affiliation_id": "60026786",
        "affiliation_state": "QC"
    },
    {
        "paper_title": "Non-linear relationships between the built environment and walking to school: Applying extreme gradient boosting method",
        "publication": "Progress in Geography",
        "citied_by": "12",
        "cover_date": "2022-02-28",
        "Abstract": "Walking is not only a primitive and convenient transport mode but also an important integrant of physical activity, which is beneficial for the promotion of public health, alleviation of traffic congestion, and mitigation of transportation-induced pollution. In modern China, cities are expanding rapidly, people are enjoying a dramatic improvement in living standards, and the pace of life is accelerating. As a result, urban people, including adolescents, tend to travel in motorized modes increasingly more and walk less. The prevalence of physical inactivity among adolescents has brought about a series of health issues, such as deterioration of physical fitness, obesity, and some non- communicable diseases (for example, diabetes and hypertension). Travel to school is among the most important routine travels for adolescents. Promoting adolescents' propensity of walking to school can effectively help them integrate physical activity into daily life and thus enhance their overall physical activity level. Hence, scholars from diverse disciplines (for example, geography, urban planning, and public health) have been drawn to examine the relationships between the built environment and walking to school. However, the current research is insufficient in the following two aspects. First, the existing research is mainly based on the Western context, whereas few studies have been conducted in China. Second, the majority of existing studies assumed a linear or generalized linear (for example, log-linear) relationship between the built environment and walking to school, and no studies, to the best of our knowledge, have examined the non-linear relationships between them. Therefore, this study, taking Xiamen, China as the case and employing its large-scale travel behavior survey data-set in 2015, explored the non- linear effects of the built environment on adolescents' propensity of walking to school. We applied a state-of-the-art machine learning method, namely extreme gradient boosting method (XG-Boost), to fit the model, and interpreted the model with relative importance and partial dependence plots. The results show that: 1) Distance from home to school is the most important factor influencing walking to school, with the relative importance of 39.99%. 2) The built environment, which is characterized by the 5Ds (density, diversity, design, destination accessibility, and distance to transit) model, is an important contributor, and relative contributions of the built environment variables at home and school collectively contributed 36.28% of the model's explanatory power, only second to distance to school, much higher than that of sociodemographic variables (23.73%). Distance to city center and population density around both home and school contribute a great deal. 3) All the built environment variables at both ends of school trips and the key sociodemographic variables have non-linear effects on adolescents' propensity of walking to school, and there exist obvious threshold effects. This study can inform decision makers with nuanced policy insights for promoting adolescents' behavior of walking to school.",
        "DOI": "10.18306/dlkxjz.2022.02.006",
        "paper_author": "Liu J.",
        "affiliation_name": "The University of Hong Kong",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong",
        "affiliation_id": "60006541",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Membrane-type acoustic metamaterial using cork sheets and attached masses based on reused materials",
        "publication": "Applied Acoustics",
        "citied_by": "56",
        "cover_date": "2022-02-28",
        "Abstract": "To contain the environmental impact of modern buildings it is necessary to design in a sustainable way, making efficient use of energy resources and raw materials. The reduction of waste requires a review of the policy of valorizing raw materials by resorting to recycling and reuse as much as possible. In this study, a new membrane-type acoustic metamaterial was developed using a recycled cork membrane and fixing to this membrane masses obtained by reusing thumbtacks and buttons. About 42 samples were packaged by attaching different combinations of masses to the cork membrane. The specimens thus obtained were used to measure the sound absorption coefficient by means of an impedance tube. To improve the acoustic properties of the material, the benefits of membrane resonance absorption and cavity resonance absorption were coupled, leaving a 50 mm cavity at the end of the tube. Subsequently, the measurements were used to train a regression tree-based sound absorption coefficient prediction model. The results obtained suggest using these structures for the acoustic correction of the rooms.",
        "DOI": "10.1016/j.apacoust.2021.108605",
        "paper_author": "Ciaburro G.",
        "affiliation_name": "Università degli Studi della Campania Luigi Vanvitelli",
        "affiliation_city": "Naples",
        "affiliation_country": "Italy",
        "affiliation_id": "60026777",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Learned Buffer Replacement for Database Systems",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "0",
        "cover_date": "2022-02-25",
        "Abstract": "Most current database buffering schemes adopt an empirical design, which cannot adapt to the change of workloads. In this paper, we show how we can use machine learning to help design a new buffer replacement policy for database systems. We name the new policy LBR (Learned Buffer Replacement). The key idea of LBR is to use machine learning models to periodically learn the access pattern from historical requests to make the buffer replacement adaptive to the workload change. Particularly, we present two ways to learn the access pattern. One is a classifier named LBR-c, which can distinguish hot pages from cold ones based on the training on historical requests; the other is a regressor called LBR-r, which can predict the future replacement behavior according to historical accesses. We implement the proposed LBR-c and LBR-r and compare them to a number of existing schemes, including the theoretically optimal Belady's algorithm, three traditional algorithms (LRU, 2Q, and ARC), and LeCaR, which is a recently-proposed adaptive buffer scheme. The results show that our algorithms achieve a higher hit ratio than LRU, ARC, 2Q, and LeCaR. In addition, both LBR-c and LBR-r can adapt to workload changes, which is better than LRU, 2Q, ARC, and LeCaR. Overall, our proposal achieves comparable performance with the optimal buffer replacement algorithm, advancing the state-of-The-Art in the well-studied area of buffer management in DBMSs.",
        "DOI": "10.1145/3528114.3528118",
        "paper_author": "Yuan Y.",
        "affiliation_name": "University of Science and Technology of China",
        "affiliation_city": "Hefei",
        "affiliation_country": "China",
        "affiliation_id": "60019118",
        "affiliation_state": "Anhui"
    },
    {
        "paper_title": "Optimizing Hammerstein-Wiener Model for Forecasting Confirmed Cases of Covid-19",
        "publication": "IAENG International Journal of Applied Mathematics",
        "citied_by": "3",
        "cover_date": "2022-02-24",
        "Abstract": "Noise poses challenge to nonlinear Hammerstein-Wiener (HW) subsystem model application, because HW subsystem need large number of parameter interactions. However, flexibility, soft computing, and automatic adjustment to dynamic observation for best model fitting make it potential for forecasting nonlinear data. In this article, we adopted improved HW inference from Levenberg-Marquardt optimization algorithm to optimize HW subsystem and to select best model parameters. Therefore, the adopted model is tested on COVID-19 confirmed reported cases, to estimate transmission rate of COVID-19 virus for period from 15th March 2020 to 29th April 2020. Model validation is carried out on small dataset, which outperforms some existing models. The adopted model is further evaluated using statistical metrics and reported best accuracy of 0.127 and 0.998 for Mean Absolute percentage error (MAPE) and coefficient of determination (R2) respectively, with best model complexity of 1.86. The obtained results are promising enough in predicting spread of COVID-19 virus and may inspire as guidance to relax lockdown restriction policies.",
        "DOI": "NA",
        "paper_author": "Abdullahi S.B.",
        "affiliation_name": "Nigeria Police",
        "affiliation_city": "Abuja",
        "affiliation_country": "Nigeria",
        "affiliation_id": "105131173",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Application of P4 (Predictive, Preventive, Personalized, Participatory) Approach to Occupational Medicine",
        "publication": "Medicina del Lavoro",
        "citied_by": "16",
        "cover_date": "2022-02-22",
        "Abstract": "In recent years there has been a growth in the role of prevention in controlling the disease burden. Increasing efforts have been conveyed in the screening implementation and public health policies, and the spreading knowledge on risk factors reflects on major attention to health checks. Despite this, lifestyle changes are difficult to be adopted and the adherence to current public health services like screening and vaccinations remains suboptimal. Additionally, the prevalence and outcome of different chronic diseases and cancers is burdened by social disparities. P4 [predictive, preventive, personalized, participatory] medicine is the conceptualization of a new health care model, based on multidimensional data and machine-learning algorithms in order to develop public health intervention and monitoring the health status of the population with focus on wellbeing and healthy ageing. Each of the characteristics of P4 medicine is relevant to occupational medicine, and indeed the P4 approach appears to be particularly relevant to this discipline. In this review, we discuss the potential applications of P4 to occupational medicine, showing examples of its introduction on workplaces and hypothesizing its further implementation at the occupational level.",
        "DOI": "10.23749/mdl.v113i1.12622",
        "paper_author": "Collatuzzo G.",
        "affiliation_name": "Alma Mater Studiorum Università di Bologna",
        "affiliation_city": "Bologna",
        "affiliation_country": "Italy",
        "affiliation_id": "60028218",
        "affiliation_state": "BO"
    },
    {
        "paper_title": "Multi-View Federated Learning with Data Collaboration",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "10",
        "cover_date": "2022-02-18",
        "Abstract": "Under the privacy protection policy, federated learning has received more and more attention. Vertical federated learning (VFL) uses the same samples local in different parties to build prediction model. However, the same samples (overlapping samples) may be limited, while a large number of non-overlapping samples in each party are not utilized. If the non-overlapping samples can be utilized for training, it can benefit the prediction model. In this paper, we propose a novel VFL method, called Multi-View Federated Learning with Data collaboration (FedMC), to solve the problem of insufficient overlapping samples by exploiting suitable non-overlapping samples for data training. The proposed FedMC method first constructs a common feature space based on the overlapping samples, then projects the non-overlapping samples into the common feature space. We measure the similarity for each pair of the non-overlapping samples by calculating their distance in this space. When the distance is less than a threshold, we match them and add this pair to the overlapping samples. The expanded overlapping samples are finally used for training to build the prediction model. We evaluate the proposed method on real-world datasets. The experimental results show that the proposed method can improve the classification result by exploiting the non-overlapping samples for training.",
        "DOI": "10.1145/3529836.3529904",
        "paper_author": "Yang Y.",
        "affiliation_name": "University of Tsukuba",
        "affiliation_city": "Tsukuba",
        "affiliation_country": "Japan",
        "affiliation_id": "60014256",
        "affiliation_state": "Ibaraki"
    },
    {
        "paper_title": "Use of a deep learning and random forest approach to track changes in the predictive nature of socioeconomic drivers of under-5 mortality rates in sub-Saharan Africa",
        "publication": "BMJ Open",
        "citied_by": "7",
        "cover_date": "2022-02-17",
        "Abstract": "Objectives We used machine learning algorithms to track how the ranks of importance and the survival outcome of four socioeconomic determinants (place of residence, mother's level of education, wealth index and sex of the child) of under-5 mortality rate (U5MR) in sub-Saharan Africa have evolved. Settings This work consists of multiple cross-sectional studies. We analysed data from the Demographic Health Surveys (DHS) collected from four countries; Uganda, Zimbabwe, Chad and Ghana, each randomly selected from the four subregions of sub-Saharan Africa. Participants Each country has multiple DHS datasets and a total of 11 datasets were selected for analysis. A total of n=85 688 children were drawn from the eleven datasets. Primary and secondary outcomes The primary outcome variable is U5MR; the secondary outcomes were to obtain the ranks of importance of the four socioeconomic factors over time and to compare the two machine learning models, the random survival forest (RSF) and the deep survival neural network (DeepSurv) in predicting U5MR. Results Mother's education level ranked first in five datasets. Wealth index ranked first in three, place of residence ranked first in two and sex of the child ranked last in most of the datasets. The four factors showed a favourable survival outcome over time, confirming that past interventions targeting these factors are yielding positive results. The DeepSurv model has a higher predictive performance with mean concordance indexes (between 67% and 80%), above 50% compared with the RSF model. Conclusions The study reveals that children under the age of 5 in sub-Saharan Africa have favourable survival outcomes associated with the four socioeconomic factors over time. It also shows that deep survival neural network models are efficient in predicting U5MR and should, therefore, be used in the big data era to draft evidence-based policies to achieve the third sustainable development goal.",
        "DOI": "10.1136/bmjopen-2021-049786",
        "paper_author": "Nasejje J.B.",
        "affiliation_name": "University of the Witwatersrand, Johannesburg",
        "affiliation_city": "Johannesburg",
        "affiliation_country": "South Africa",
        "affiliation_id": "60016218",
        "affiliation_state": "Gauteng"
    },
    {
        "paper_title": "Early environmental quality and life-course mental health effects: The Equal-Life project",
        "publication": "Environmental Epidemiology",
        "citied_by": "25",
        "cover_date": "2022-02-16",
        "Abstract": "Background: There is increasing evidence that a complex interplay of factors within environments in which children grows up, contributes to children's suboptimal mental health and cognitive development. The concept of the life-course exposome helps to study the impact of the physical and social environment, including social inequities, on cognitive development and mental health over time. Methods: Equal-Life develops and tests combined exposures and their effects on children's mental health and cognitive development. Data from eight birth-cohorts and three school studies (N = 240.000) linked to exposure data, will provide insights and policy guidance into aspects of physical and social exposures hitherto untapped, at different scale levels and timeframes, while accounting for social inequities. Reasoning from the outcome point of view, relevant stakeholders participate in the formulation and validation of research questions, and in the formulation of environmental hazards. Exposure assessment combines GIS-based environmental indicators with omics approaches and new data sources, forming the early-life exposome. Statistical tools integrate data at different spatial and temporal granularity and combine exploratory machine learning models with hypothesis-driven causal modeling. Conclusions: Equal-Life contributes to the development and utilization of the exposome concept by (1) integrating the internal, physical and social exposomes, (2) studying a distinct set of life-course effects on a child's development and mental health (3) characterizing the child's environment at different developmental stages and in different activity spaces, (4) looking at supportive environments for child development, rather than merely pollutants, and (5) combining physical, social indicators with novel effect markers and using new data sources describing child activity patterns and environments.",
        "DOI": "10.1097/EE9.0000000000000183",
        "paper_author": "van Kamp I.",
        "affiliation_name": "Rijksinstituut voor Volksgezondheid en Milieu",
        "affiliation_city": "Bilthoven",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60026125",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Proactive vs Reactive Machine Learning in Health Care: Lessons from the COVID-19 Pandemic",
        "publication": "JAMA",
        "citied_by": "16",
        "cover_date": "2022-02-15",
        "Abstract": "NA",
        "DOI": "10.1001/jama.2021.24935",
        "paper_author": "Luo Y.",
        "affiliation_name": "Northwestern University Feinberg School of Medicine",
        "affiliation_city": "Chicago",
        "affiliation_country": "United States",
        "affiliation_id": "60013227",
        "affiliation_state": "IL"
    },
    {
        "paper_title": "Learning manipulation skills with demonstrations for the swing process control of dredgers",
        "publication": "Ocean Engineering",
        "citied_by": "7",
        "cover_date": "2022-02-15",
        "Abstract": "The Cutter Suction Dredger (CSD) is one of the key equipment dedicated to the construction and maintenance projects of harbours, ports and navigational channels. Among the dredging manipulations, the swing process is the most tedious and recurring work for human operators, which often leads to accidents because of carelessness or fatigue of the operators. This paper aims at producing a learning approach for the intelligent control of the swing process of a CSD so as to release human operators from such a boring and heavy task. To this end, the swing process control is formulated as a sequential decision making problem, and Deep Reinforcement Learning (DRL) is employed to design the learning approach based on deterministic policy gradient. The novel feature of the proposed approach is that the manipulation skills are obtained via trial-and-error interactions with a predicting network constructed by human demonstration data. In our approach, human demonstrations can provide a channel to predict state transitions, and also can regulate the exploration procedure for the learning agent. In addition, we carry out empirical studies to investigate how to treat the demonstration data with regard to self exploration, and the experimental results show that the proposed approach provides an effective means of controlling the swing process of CSDs.",
        "DOI": "10.1016/j.oceaneng.2022.110545",
        "paper_author": "Wei C.",
        "affiliation_name": "Hohai University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60010851",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "Fault-tolerant control for nonlinear offshore steel jacket platforms based on reinforcement learning",
        "publication": "Ocean Engineering",
        "citied_by": "6",
        "cover_date": "2022-02-15",
        "Abstract": "In this article, reinforcement learning based fault-tolerant control for the nonlinear offshore platform is studied. First, the dynamic model of the nonlinear steel-jacket offshore platform with active mass damper and actuator fault is investigated then an affine nonlinear representation for the offshore platform is obtained. By estimating actuator fault and irregular wave force and inserting their estimation into the cost function, the disturbance control and fault-tolerant control problem (FTC) are converted into optimal control problem. Online policy iteration is used to minimize the performance index. Finally, simulation results show the effectiveness of our method.",
        "DOI": "10.1016/j.oceaneng.2021.110247",
        "paper_author": "Ziaei A.",
        "affiliation_name": "University of Tabriz",
        "affiliation_city": "Tabriz",
        "affiliation_country": "Iran",
        "affiliation_id": "60006622",
        "affiliation_state": "East Azerbaijan"
    },
    {
        "paper_title": "A latent batch-constrained deep reinforcement learning approach for precision dosing clinical decision support",
        "publication": "Knowledge-Based Systems",
        "citied_by": "16",
        "cover_date": "2022-02-15",
        "Abstract": "Precise prescription of medication dosing is crucial to patients, especially among Intensive Care Unit (ICU) patients. However, improper administration of some sensitive therapeutic medications (e.g., heparin) might place patients at unneeded risk, even life-threatening. Numerous factors such as a patient's clinical phenotype, genotype, and environmental factors will affect the heparin dose response. As a result, it is challenging to prescribe the optimal initial dose of heparin. In this paper, an individualized dosing policy is proposed to determine the optimal initial dose and minimize the risk of mis-dosing, as well as preventing the patients from late complications associated with medications dosing. A latent batch-constrained deep reinforcement learning (RL) algorithm is proposed to guarantee the safety of the medication recommendation system. The agent can observe a latent representation of patents and generate medication dosing solutions in successive and limited action spaces. The individualized dosing policy aims to reduce the extrapolation errors in the off-policy algorithms, by evaluating over-dosing and under-dosing of heparin in patients. Our results evaluated on Medical Information Mart for Intensive Care III (MIMIC-III) database demonstrate that the latent batch-constrained RL algorithm can work effectively from the retrospective data, showing promise to be used in future medication dosing policies.",
        "DOI": "10.1016/j.knosys.2021.107689",
        "paper_author": "Qiu X.",
        "affiliation_name": "Shanghai University of Engineering Science",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60032504",
        "affiliation_state": "Shanghai"
    },
    {
        "paper_title": "A novel machine-learning approach for evaluating rebounds-associated environmental footprint of households and application to cooperative housing",
        "publication": "Journal of Environmental Management",
        "citied_by": "11",
        "cover_date": "2022-02-15",
        "Abstract": "Multiple environmental policies aim to increase resource efficiency and reduce consumption of goods and services with high environmental impact. This may lead to cost-savings and, consequently, additional consumption with environmental impacts (rebound effects). In this study, a supervised machine-learning model (i.e. an application of random forest regression) is developed to quantify consumption rebound effects. In contrast to previous approaches, it is a versatile method, which allows to estimate any income-related rebound at household level considering specific household properties and the entire profile of consumption. Socio-economic properties (e.g. income, age group) of the households are used as the independent properties for the regressor to detect the dependent consumption expenses of the households. Thus, this method can be used as a bottom-up study for understanding rebounds and developing targeted measures to prevent or reduce rebound effects. To illustrate the application of the method, it is applied to the case of cooperative housing in Switzerland. In addition to environmental goals, the cooperative aims to provide affordable housing, and the reduced rent increases the disposable income of tenants. The results show that households tend to spend the ‘extra’ income on housing (e.g. for larger apartments) and travel. For the former, the cooperative already has a policy in place regulating the apartment area permitted per person, which delimits induced environmental impacts. For the latter, households with lower income particularly spend their extra-money on purchase and operation of vehicles, while higher-income groups rather spend it on recreation and package holidays. Travel, housing, clothing and personal care products have highest emissions per Swiss Franc (∼0.3–0.6 kg CO2-eq/CHF). Thus, it is recommended to provide incentives for shifting these expenses to other consumption, to avoid jeopardizing environmental goals. The method was also used for a range of other applications e.g. rebounds due to energy-efficient devices to illustrate its versatility.",
        "DOI": "10.1016/j.jenvman.2021.114205",
        "paper_author": "Shinde R.",
        "affiliation_name": "ETH Zürich",
        "affiliation_city": "Zurich",
        "affiliation_country": "Switzerland",
        "affiliation_id": "60025858",
        "affiliation_state": "ZH"
    },
    {
        "paper_title": "Forecasting plastic waste generation and interventions for environmental hazard mitigation",
        "publication": "Journal of Hazardous Materials",
        "citied_by": "95",
        "cover_date": "2022-02-15",
        "Abstract": "Plastic waste and its environmental hazards have been attracting public attention as a global sustainability issue. This study builds a neural network model to forecast plastic waste generation of the EU-27 in 2030 and evaluates how the interventions could mitigate the adverse impact of plastic waste on the environment. The black-box model is interpreted using SHapley Additive exPlanations (SHAP) for managerial insights. The dependence on predictors (i.e., energy consumption, circular material use rate, economic complexity index, population, and real gross domestic product) and their interactions are discussed. The projected plastic waste generation of the EU-27 is estimated to reach 17 Mt/y in 2030. With an EU targeted recycling rate (55%) in 2030, the environmental impacts would still be higher than in 2018, especially global warming potential and plastic marine pollution. This result highlights the importance of plastic waste reduction, especially for the clustering algorithm-based grouped countries with a high amount of untreated plastic waste per capita. Compared to the other assessed scenarios, Scenario 4 with waste reduction (50% recycling, 47.6% energy recovery, 2.4% landfill) shows the lowest impact in acidification, eutrophication, marine aquatic toxicity, plastic marine pollution, and abiotic depletion. However, the global warming potential (8.78 Gt CO2eq) is higher than that in 2018, while Scenario 3 (55% recycling, 42.6% energy recovery, 2.4% landfill) is better in this aspect than Scenario 4. This comprehensive analysis provides pertinent insights into policy interventions towards environmental hazard mitigation.",
        "DOI": "10.1016/j.jhazmat.2021.127330",
        "paper_author": "Fan Y.V.",
        "affiliation_name": "Brno University of Technology, Faculty of Mechanical Engineering",
        "affiliation_city": "Brno",
        "affiliation_country": "Czech Republic",
        "affiliation_id": "60108596",
        "affiliation_state": "South Moravian Region"
    },
    {
        "paper_title": "CASE: A Context-Aware Security Scheme for Preserving Data Privacy in IoT-Enabled Society 5.0",
        "publication": "IEEE Internet of Things Journal",
        "citied_by": "14",
        "cover_date": "2022-02-15",
        "Abstract": "This article introduces the concept of context-aware attribute learning with cipher policy-attribute-based encryption (CP-ABE) to preserve the privacy of users' information in IoT-enabled Society 5.0. The concept of Society 5.0 pioneers an abstract system unifying different smart environments (SEs) to provide seamless services to the citizens. While serving different applications, these SEs store users' information in the cloud engendering users' privacy. CP-ABE is one of the conventional security systems that preserves privacy with group data accessibility. Contemporary CP-ABE solutions enforce users to manually provide their contextual information, namely, attributes, to encrypt/decrypt data. From these solutions it can be conjectured that incorrect attribute selection by a user raises the issue of unauthenticated access to information. To address these issues, we propose a scheme, named the context-aware attribute learning scheme (CASE), which autonomously learns users' contextual information, exploiting edge intelligence, generates attributes, and reduces the post-encryption data size using the learned attributes. We examine the performance of CASE with the help of a case study on CP-ABE over smart healthcare systems (SHSs). Extensive experimental results show that CASE outperforms the existing CP-ABE-based security schemes by reducing 32%-33% average network delay, 33%-35% average energy consumption, and 31%-36% average packet loss. Additionally, we analyze the performance of attribute learning schemes using the support vector machine (SVM), decision tree (DT), and naive Bayes (NB) learning models. We observe that DT reports better performance over SVM and NB in prediction accuracy, prediction time, and clock cycles required for execution.",
        "DOI": "10.1109/JIOT.2021.3101115",
        "paper_author": "Ghosh T.",
        "affiliation_name": "Indian Institute of Technology Kharagpur",
        "affiliation_city": "Kharagpur",
        "affiliation_country": "India",
        "affiliation_id": "60004750",
        "affiliation_state": "WB"
    },
    {
        "paper_title": "FLEX: A Platform for Scalable Service Placement in Multi-Fog and Multi-Cloud Environments",
        "publication": "ACM International Conference Proceeding Series",
        "citied_by": "6",
        "cover_date": "2022-02-14",
        "Abstract": "With the recent development in the Internet of Things (IoT), big data, and machine learning, the number of services has dramatically increased. These services are heterogeneous in terms of the amount of resources and quality of service (QoS) requirements. To cope with the limitations of Cloud infrastructure providers (CIPs) for latency-sensitive services, many Fog infrastructure providers (FIPs) have recently emerged and their numbers are increasing continually. Due to difficulties such as the different requirements of services, location of end-users, and profile cost of IPs, distributing services across multiple FIPs and CIPs has become a fundamental challenge. Motivated by this, a flexible and scalable platform, FLEX, is proposed in this work for the service placement problem (SPP) in multi-Fog and multi-Cloud computing. For each service, FLEX broadcasts the service's requirements to the resource managers (RMs) of all providers and then based on the RMs' responses, it selects the most suitable provider for that service. The proposed platform is flexible and scalable as it leaves it up to the RMs to have their own policy for service placement. The problem is formulated as an optimization problem and an efficient heuristic algorithm is proposed to solve it. Our simulation results show that the proposed algorithm can meet the requirements of services.",
        "DOI": "10.1145/3511616.3513105",
        "paper_author": "Farzin P.",
        "affiliation_name": "University of Kurdistan",
        "affiliation_city": "Sanandaj",
        "affiliation_country": "Iran",
        "affiliation_id": "60001619",
        "affiliation_state": "Kurdistan"
    },
    {
        "paper_title": "Sub-AVG: Overestimation reduction for cooperative multi-agent reinforcement learning",
        "publication": "Neurocomputing",
        "citied_by": "15",
        "cover_date": "2022-02-14",
        "Abstract": "Decomposing the centralized joint action value(JAV) into per-agent individual action value(IAV) is attractive in cooperative multi-agent reinforcement learning(MARL). In such tasks, IAVs based on local observation can perform decentralized policies, and the JAV is used for end-to-end training through traditional reinforcement learning methods, especially through the Q-learning algorithm. However, the Q-learning-based method suffers from overestimation, in which the overestimated action values may result in a suboptimal policy. In this paper, we show that such overestimation can occur in the above Q-learning-based decomposition method. Our solution is Sub-AVG, which utilizes a lower update target by discarding the larger of previously learned IAVs and averaging the retained ones, thus eliminating the excessive overestimation errors. Experiments in the StarCraft Multi-Agent Challenge(SMAC) environment show that Sub-AVG can lead to lower JAV estimations and better-performing policies.",
        "DOI": "10.1016/j.neucom.2021.12.039",
        "paper_author": "Wu H.",
        "affiliation_name": "Sichuan University",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China",
        "affiliation_id": "60016521",
        "affiliation_state": "Sichuan"
    },
    {
        "paper_title": "Development of base tax liability insurance premium calculator for the south African construction industry-a machine learning approach",
        "publication": "Handbook of Intelligent Computing and Optimization for Sustainable Development",
        "citied_by": "2",
        "cover_date": "2022-02-11",
        "Abstract": "This study researched the tax risk profile of South African construction companies. The tax risk profile of South African construction companies is characterized by the book value, the cash flow position, headcount, firm earnings, debt size, and the type of firm. The model that defines this tax risk is a Neural Network (NN) boosted Generalized Linear Model (GLM). The main aim of this study is to develop an artificial intelligent pricing model. This model will be presented as a tool that can be used for tax liability insurance underwriting. This study was conducted using an examination of financial statements of construction companies. Such an examination was done using 10 years worth of financial data. For a view on company option data, utilization of standard bank data came in handy, while that of major economic events was obtained qualitatively from industrial clips and Statistics South Africa (Stats SA). Model ensemble techniques that involve the use of statistical and machine trained models were used. Mathematical adjustments conducted were on the GLM as the baseline model. This was done on special considerations of precision versus accuracy for a much more error free pricing calculator. The pricing model that defines the price of tax risk of South African construction firms was developed. The study highlighted the key determinants of the price of tax risk for construction firms. Modeling techniques used to build the pricing model were discussed in detail. These techniques came from families of statistical and Machine Learning (ML) models. Challenges encountered in the research process were also highlighted and discussed in detail. The role of the South African Revenue Services (SARS) in making studies of this nature simpler to carry out was also made clear. The importance of an open data policy for ML and Artificial Intelligence (AI) was also emphasized in this study through a discussion of cross collaboration between key role players in the economy such as insurance, banks and tax administration.",
        "DOI": "10.1002/9781119792642.ch19",
        "paper_author": "Mabusela-Motsosi B.",
        "affiliation_name": "North-West University",
        "affiliation_city": "Potchefstroom",
        "affiliation_country": "South Africa",
        "affiliation_id": "60029714",
        "affiliation_state": "North West"
    },
    {
        "paper_title": "PrivacyCheck v3: Empowering users with higher-level understanding of privacy policies",
        "publication": "WSDM 2022 - Proceedings of the 15th ACM International Conference on Web Search and Data Mining",
        "citied_by": "11",
        "cover_date": "2022-02-11",
        "Abstract": "Online privacy policies are lengthy and hard to read, yet are profoundly important as they communicate the practices of an organization pertaining to user data privacy. Privacy Enhancing Technologies, or PETs, seek to inform users by summarizing these privacy policies. Efforts in the research and development of such PETs, however, have largely been limited to tools that recap the policy or visualize it. We present the next generation of our research and publicly available tool, PrivacyCheck v3, that utilizes machine learning to inform and empower users with respect to privacy policies. PrivacyCheck v3 adds capabilities that are commonly absent from similar PETs on the web. In particular, it adds the ability to (1) find the competitors of an organization with Alexa traffic analysis and compare policies across them, (2) follow privacy policies to which the user has agreed and notify the user when policies change, (3) track policies over time and report how often policies change and their trends, (4) automatically find privacy policies in domains, and (5) provide a bird's-eye view of privacy policies. The new features of PrivacyCheck not only inform users about details of privacy policies, but also empower them to understand privacy policies at a higher level, make informed decisions, and even select competitors with better privacy policies.",
        "DOI": "10.1145/3488560.3502184",
        "paper_author": "Nokhbeh Zaeem R.",
        "affiliation_name": "The University of Texas at Austin",
        "affiliation_city": "Austin",
        "affiliation_country": "United States",
        "affiliation_id": "60013372",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "Opportunities of and threats to consumer well-being in the age of Fourth Industrial Revolution (IR 4.0) technologies",
        "publication": "Digital Policy, Regulation and Governance",
        "citied_by": "5",
        "cover_date": "2022-02-11",
        "Abstract": "Purpose: Fourth Industrial Revolution (IR 4.0) technologies have strong potential to affect consumer well-being, positively or negatively, so the current paper aims to review potential opportunities and threats that these technologies represent for consumers in several core economic sectors: health care, education, financial services, manufacturing and retailing. Design/methodology/approach: This paper proposes a conceptual framework for how IR 4.0 technologies affect consumer well-being in five representative sectors: health care, education, financial services, manufacturing and retailing. The authors argue that the potential transformations of these specific sectors, facilitated by these technologies, may have profound effects on consumer well-being, with urgent public policy implications. Findings: Emerging technologies, such as artificial intelligence, robotics, the Internet of Things, three-dimensional printing, machine learning and blockchain, provide customers with novel approaches toward decisions regarding health, education, finances and other fundamental parts of their lives. The organizations that provide these services, such as hospitals, universities and banks, actively adopt the innovations offered by IR 4.0. These evolving and disruptive technologies thus are changing reality for consumers and providers. Originality/value: This paper proposes some novel public policy implications of IR 4.0 technologies for consumer well-being, and it outlines further research directions that can enhance understanding of relevant technologies and the consequences of their use for society.",
        "DOI": "10.1108/DPRG-06-2021-0080",
        "paper_author": "Boninsegni M.F.",
        "affiliation_name": "IPAG Business School",
        "affiliation_city": "Paris",
        "affiliation_country": "France",
        "affiliation_id": "60278992",
        "affiliation_state": "Ile-de-France"
    },
    {
        "paper_title": "An Analysis of the Authenticity of Financial Data of Listed Companies Based on Vector Machines",
        "publication": "Advances in Transdisciplinary Engineering",
        "citied_by": "0",
        "cover_date": "2022-02-10",
        "Abstract": "Driven by the benefits of listing at a high premium, financial fraud in listed companies has become a widespread problem worldwide. In the face of economic interests, even developed countries with relatively complete laws and policies cannot completely eliminate financial fraud. The financial fraud incidents of listed companies have seriously disrupted the normal order of my country's capital market. Therefore, the specific financial data of listed companies are analyzed to obtain the influence of financial indicators on the identification of financial fraud and to determine whether there is financial fraud in listed companies.",
        "DOI": "10.3233/ATDE220092",
        "paper_author": "Gong M.",
        "affiliation_name": "Wuhan College",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "113993446",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Related Parallel Machine Online Scheduling Method Based on LSTM-PPO Reinforcement Learning",
        "publication": "Zhongguo Jixie Gongcheng/China Mechanical Engineering",
        "citied_by": "6",
        "cover_date": "2022-02-10",
        "Abstract": "To solve the related parallel machine online scheduling problems, the total weighted completion time was taken into account, and an online scheduling method was proposed based on LSTM-PPO reinforcement learning. A LSTM-integrated agent was designed to record the historical variations of workshop states and the corresponding scheduling policy adjustment, and then online scheduling decision was made according to the state information. Meanwhile, the workshop state matrix was designed to describe the problem constraints and optimization goals, additional machine waiting was introduced in scheduling action space to expand solution space, and the reward function was designed to decompose the optimization goal into step-by-step rewards to achieve scheduling decision evaluation. Finally, the model updating and global optimization of parameters was achieved by PPO algorithm. Experimental results show that the proposed method has competitive solutions than the existing heuristic rules, and the proposed algorithm is applied to the production scheduling of the actual workshops, which effectively reduces the total weighted completion time.",
        "DOI": "10.3969/j.issn.1004-132X.2022.03.009",
        "paper_author": "He J.",
        "affiliation_name": "Donghua University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60010953",
        "affiliation_state": "Shanghai"
    },
    {
        "paper_title": "High-resolution mapping of regional traffic emissions using land-use machine learning models",
        "publication": "Atmospheric Chemistry and Physics",
        "citied_by": "15",
        "cover_date": "2022-02-10",
        "Abstract": "On-road vehicle emissions are a major contributor to significant atmospheric pollution in populous metropolitan areas. We developed an hourly link-level emissions inventory of vehicular pollutants using two land-use machine learning methods based on road traffic monitoring datasets in the Beijing-Tianjin-Hebei (BTH) region. The results indicate that a land-use random forest (LURF) model is more capable of predicting traffic profiles than other machine learning models on most occasions in this study. The inventories under three different traffic scenarios depict a significant temporal and spatial variability in vehicle emissions. NOx, fine particulate matter (PM2.5), and black carbon (BC) emissions from heavy-duty trucks (HDTs) generally have a higher emission intensity on the highways connecting to regional ports. The model found a general reduction in light-duty passenger vehicles when traffic restrictions were implemented but a much more spatially heterogeneous impact on HDTs, with some road links experiencing up to 40ĝ% increases in the HDT traffic volume. This study demonstrates the power of machine learning approaches to generate data-driven and high-resolution emission inventories, thereby providing a platform to realize the near-real-time process of establishing high-resolution vehicle emission inventories for policy makers to engage in sophisticated traffic management.",
        "DOI": "10.5194/acp-22-1939-2022",
        "paper_author": "Wu X.",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60025278",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Imputation of trip data for a docked bike-sharing system",
        "publication": "Current Science",
        "citied_by": "2",
        "cover_date": "2022-02-10",
        "Abstract": "Mobile application-based transportation services are reshaping the urban transportation industries of both the developed and developing worlds. They generate massive amounts of data, which have the potential to provide deeper insights into urban travel activity than ever before. The bike-sharing service (BSS) market is growing at a breakneck pace with new service providers entering the arena. However, we have seen the failure of several BSS start-ups in India in recent years. All these cases have one aspect in common: user dissatisfaction because of insufficient/ineffective rebalancing approaches. The BSS operators rely on data insights to drive their policies and strategies. However, the data generated by these services are found to have several incomplete records as a result of various technical errors, like missing origin/destination. As most BSS modelling focuses on trip origin and destination, completely ignoring (or listwise deleting) trips with missing information results in the loss of valuable data that are still present in other observed variables, which include trip duration, date and time of the trip, and so on. This study proposes two methods for imputing missing data: (i) a probabilistic approach based on Bayes’ theorem, and (ii) a machine learning approach based on the k-nearest neighbor algorithm. The methodologies for their analyses are presented in detail. Data from a BSS that operated in the Indian Institute of Science campus, Bengaluru, India, are used to illustrate the proposed approaches. This is followed by a brief discussion of the results and a comparison of the performance.",
        "DOI": "10.18520/cs/v122/i3/310-318",
        "paper_author": "Thomas M.M.",
        "affiliation_name": "Rajiv Gandhi Institute of Technology Kottayam",
        "affiliation_city": "Kottayam",
        "affiliation_country": "India",
        "affiliation_id": "60094056",
        "affiliation_state": "KL"
    },
    {
        "paper_title": "Outracing champion Gran Turismo drivers with deep reinforcement learning",
        "publication": "Nature",
        "citied_by": "251",
        "cover_date": "2022-02-10",
        "Abstract": "Many potential applications of artificial intelligence involve making real-time decisions in physical systems while interacting with humans. Automobile racing represents an extreme example of these conditions; drivers must execute complex tactical manoeuvres to pass or block opponents while operating their vehicles at their traction limits1. Racing simulations, such as the PlayStation game Gran Turismo, faithfully reproduce the non-linear control challenges of real race cars while also encapsulating the complex multi-agent interactions. Here we describe how we trained agents for Gran Turismo that can compete with the world’s best e-sports drivers. We combine state-of-the-art, model-free, deep reinforcement learning algorithms with mixed-scenario training to learn an integrated control policy that combines exceptional speed with impressive tactics. In addition, we construct a reward function that enables the agent to be competitive while adhering to racing’s important, but under-specified, sportsmanship rules. We demonstrate the capabilities of our agent, Gran Turismo Sophy, by winning a head-to-head competition against four of the world’s best Gran Turismo drivers. By describing how we trained championship-level racers, we demonstrate the possibilities and challenges of using these techniques to control complex dynamical systems in domains where agents must respect imprecisely defined human norms.",
        "DOI": "10.1038/s41586-021-04357-7",
        "paper_author": "Wurman P.R.",
        "affiliation_name": "Sony AI",
        "affiliation_city": "New York",
        "affiliation_country": "United States",
        "affiliation_id": "127726787",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Reinforcement-learning-based control of confined cylinder wakes with stability analyses",
        "publication": "Journal of Fluid Mechanics",
        "citied_by": "61",
        "cover_date": "2022-02-10",
        "Abstract": "This work studies the application of a reinforcement learning (RL)-based flow control strategy to the flow past a cylinder confined between two walls to suppress vortex shedding. The control action is blowing and suction of two synthetic jets on the cylinder. The theme of this study is to investigate how to use and embed physical information of the flow in the RL-based control. First, global linear stability and sensitivity analyses based on the time-mean flow and the steady flow (which is a solution to the Navier-Stokes equations) are conducted in a range of blockage ratios and Reynolds numbers. It is found that the most sensitive region in the wake extends itself when either parameter increases in the parameter range we investigated here. Then, we use these physical results to help design RL-based control policies. We find that the controlled wake converges to the unstable steady base flow, where the vortex shedding can be successfully suppressed. A persistent oscillating control seems necessary to maintain this unstable state. The RL algorithm is able to outperform a gradient-based optimisation method (optimised in a certain period of time) in the long run. Furthermore, when the flow stability information is embedded in the reward function to penalise the instability, the controlled flow may become more stable. Finally, according to the sensitivity analyses, the control is most efficient when the probes are placed in the most sensitive region. The control can be successful even when few probes are properly placed in this manner.",
        "DOI": "10.1017/jfm.2021.1045",
        "paper_author": "Li J.",
        "affiliation_name": "National University of Singapore",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore",
        "affiliation_id": "60017161",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Projection of future drought and its impact on simulated crop yield over South Asia using ensemble machine learning approach",
        "publication": "Science of the Total Environment",
        "citied_by": "70",
        "cover_date": "2022-02-10",
        "Abstract": "Understanding the development mechanism of drought events, characterization of future drought metrics, and its impact on crop yield is crucial to ensure food security globally, and more importantly, in South Asia. Therefore, the present study assessed the changes in future projected drought metrics and evaluated the future risk of yield reduction under drought intensity. We characterized the magnitude, intensity, and duration of future drought by means of the SPEI drought index using CMIP6 (Coupled Model Inter-comparison Phase-6) climate models. The impact of future drought on crop yield was quantified from the ISI-MP (Inter-Sectoral Impact Model Inter-comparison Project) crop model by a proposed non-linear ensemble of Random Forest (RF) and Gradient Boosting Machine (GBM). Results suggested that high drought magnitude with a longer drought duration is projected in some regions of South Asia while high drought intensity comes with a shorter duration. It was also found that Afghanistan, Pakistan, and India will experience a longer drought duration in the future. Our proposed ensemble machine learning (EML) approach had high predictive skill with a minimum value of RMSE (0.358–0.390), MAE (0.222–0.299), and a maximum value of R2 (0.705–0.918) compared to the stand-alone methods of RF and GBM for yield loss risk projection. The drought-driven impact on crop yield demonstrates a high risk of yield loss under extreme drought events, which will encounter 54.15%, 29.30%, and 50.66% loss in the future for rice, wheat, and maize crops, respectively. Furthermore, drought and yield loss risk dynamics suggested a one unit decrease in SPEI value would lead to a 14.2%, 7.5%, and 10.9% decrease in yield for rice, wheat, and maize crops, respectively. This study will provide a notable direction for policy agencies to build resistance to crop production against the drought impact in the regions that are critical to climate change.",
        "DOI": "10.1016/j.scitotenv.2021.151029",
        "paper_author": "Prodhan F.A.",
        "affiliation_name": "Aerospace Information Research Institute",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60273019",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Data-based decentralized learning scheme for nonlinear systems with mismatched interconnections",
        "publication": "Neurocomputing",
        "citied_by": "2",
        "cover_date": "2022-02-07",
        "Abstract": "In this paper, the decentralized learning scheme for nonlinear systems with mismatched interconnections is developed by using the off-policy integral reinforcement leaning algorithm. First, the decentralized control of the overall system is transformed into the optimal control of each subsystem by introducing an auxiliary control. In order to relax the knowledge of system dynamics, a model-free policy iteration algorithm is derived based on the off-policy integral reinforcement learning. Then, the model-free policy iteration algorithm is used to solve the related Hamilton–Jacobi-Bellman equations, where only the collected system data is required. For implementation purpose, neural networks are employed to approximate the optimal cost functions and the optimal control policies, respectively. Moreover, the least squares method and the experience replay technique are combined to learn neural network weights. Finally, a mismatched interconnected system and a photovoltaic power system are presented to verify the effectiveness of the proposed algorithm.",
        "DOI": "10.1016/j.neucom.2021.11.002",
        "paper_author": "Mu C.",
        "affiliation_name": "Tianjin University",
        "affiliation_city": "Tianjin",
        "affiliation_country": "China",
        "affiliation_id": "60019533",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "The “Ecosystem as a Service (EaaS)” approach to advance clinical artificial intelligence (cAI)",
        "publication": "PLOS Digital Health",
        "citied_by": "6",
        "cover_date": "2022-02-01",
        "Abstract": "The application of machine learning and artificial intelligence to clinical settings for prevention, diagnosis, treatment, and the improvement of clinical care have been demonstrably cost-effective. However, current clinical AI (cAI) support tools are predominantly created by non-domain experts and algorithms available in the market have been criticized for the lack of transparency behind their creation. To combat these challenges, the Massachusetts Institute of Technology Critical Data (MIT-CD) consortium, an affiliation of research labs, organizations, and individuals that contribute to research in and around data that has a critical impact on human health, has iteratively developed the “Ecosystem as a Service (EaaS)” approach, providing a transparent education and accountability platform for clinical and technical experts to collaborate and advance cAI. The EaaS approach provides a range of resources, from open-source databases and specialized human resources to networking and collaborative opportunities. While mass deployment of the ecosystem still faces several hurdles, here we discuss our initial implementation efforts. We hope this will promote further exploration and expansion of the EaaS approach, while also informing or realizing policies that will accelerate multinational, multidisciplinary, and multisectoral collaborations in cAI research and development, and provide localized clinical best practices for equitable healthcare access.",
        "DOI": "10.1371/journal.pdig.0000011",
        "paper_author": "Ishii-Rousseau J.E.",
        "affiliation_name": "Massachusetts Institute of Technology",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States",
        "affiliation_id": "60022195",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Design Strategy Network: A Deep Hierarchical Framework to Represent Generative Design Strategies in Complex Action Spaces",
        "publication": "Journal of Mechanical Design",
        "citied_by": "20",
        "cover_date": "2022-02-01",
        "Abstract": "Generative design problems often encompass complex action spaces that may be divergent over time, contain state-dependent constraints, or involve hybrid (discrete and continuous) domains. To address those challenges, this work introduces Design Strategy network (DSN), a data-driven deep hierarchical framework that can learn strategies over these arbitrary complex action spaces. The hierarchical architecture decomposes every action decision into first predicting a preferred spatial region in the design space and then outputting a probability distribution over a set of possible actions from that region. This framework comprises a convolutional encoder to work with image-based design state representations, a multi-layer perceptron to predict a spatial region, and a weight-sharing network to generate a probability distribution over unordered set-based inputs of feasible actions. Applied to a truss design study, the framework learns to predict the actions of human designers in the study, capturing their truss generation strategies in the process. Results show that DSNs significantly outperform nonhierarchical methods of policy representation, demonstrating their superiority in complex action space problems.",
        "DOI": "10.1115/1.4052566",
        "paper_author": "Raina A.",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Pittsburgh",
        "affiliation_country": "United States",
        "affiliation_id": "60104842",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "Analyzing Real Options and Flexibility in Engineering Systems Design Using Decision Rules and Deep Reinforcement Learning",
        "publication": "Journal of Mechanical Design",
        "citied_by": "9",
        "cover_date": "2022-02-01",
        "Abstract": "Engineering systems provide essential services to society, e.g., power generation, transportation. Their performance, however, is directly affected by their ability to cope with uncertainty, especially given the realities of climate change and pandemics. Standard design methods often fail to recognize uncertainty in early conceptual activities, leading to rigid systems that are vulnerable to change. Real options and flexibility in design are important paradigms to improve a system’s ability to adapt and respond to unforeseen conditions. Existing approaches to analyze flexibility, however, do not leverage sufficiently recent developments in machine learning enabling deeper exploration of the computational design space. There is untapped potential for new solutions that are not readily accessible using existing methods. Here, a novel approach to analyze flexibility is proposed based on deep reinforcement learning (DRL). It explores available datasets systematically and considers a wider range of adaptability strategies. The methodology is evaluated on an example waste-to-energy (WTE) system. Low and high flexibility DRL models are compared against stochastically optimal inflexible and flexible solutions using decision rules. The results show highly dynamic solutions, with action space parametrized via artificial neural network (ANN). They show improved expected economic value up to 69% compared with previous solutions. Combining information from action space probability distributions along expert insights and risk tolerance helps make better decisions in real-world design and system operations. Out of sample testing shows that the policies are generalizable, but subject to tradeoffs between flexibility and inherent limitations of the learning process.",
        "DOI": "10.1115/1.4052299",
        "paper_author": "Caputo C.",
        "affiliation_name": "Imperial College London",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60015150",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Fire association with respiratory disease and COVID-19 complications in the State of Pará, Brazil",
        "publication": "The Lancet Regional Health - Americas",
        "citied_by": "5",
        "cover_date": "2022-02-01",
        "Abstract": "Background: Brazil has faced two simultaneous problems related to respiratory health: forest fires and the high mortality rate due to COVID-19 pandemics. The Amazon rain forest is one of the Brazilian biomes that suffers the most with fires caused by droughts and illegal deforestation. These fires can bring respiratory diseases associated with air pollution, and the State of Pará in Brazil is the most affected. COVID-19 pandemics associated with air pollution can potentially increase hospitalizations and deaths related to respiratory diseases. Here, we aimed to evaluate the association of fire occurrences with the COVID-19 mortality rates and general respiratory diseases hospitalizations in the State of Pará, Brazil. Methods: We employed machine learning technique for clustering k-means accompanied with the elbow method used to identify the ideal quantity of clusters for the k-means algorithm, clustering 10 groups of cities in the State of Pará where we selected the clusters with the highest and lowest fires occurrence from the 2015 to 2019. Next, an Auto-regressive Integrated Moving Average Exogenous (ARIMAX) model was proposed to study the serial correlation of respiratory diseases hospitalizations and their associations with fire occurrences. Regarding the COVID-19 analysis, we computed the mortality risk and its confidence level considering the quarterly incidence rate ratio in clusters with high and low exposure to fires. Findings: Using the k-means algorithm we identified two clusters with similar DHI (Development Human Index) and GDP (Gross Domestic Product) from a group of ten clusters that divided the State of Pará but with diverse behavior considering the hospitalizations and forest fires in the Amazon biome. From the auto-regressive and moving average model (ARIMAX), it was possible to show that besides the serial correlation, the fires occurrences contribute to the respiratory diseases increase, with an observed lag of six months after the fires for the case with high exposure to fires. A highlight that deserves attention concerns the relationship between fire occurrences and deaths. Historically, the risk of mortality by respiratory diseases is higher (about the double) in regions and periods with high exposure to fires than the ones with low exposure to fires. The same pattern remains in the period of the COVID-19 pandemic, where the risk of mortality for COVID-19 was 80% higher in the region and period with high exposure to fires. Regarding the SARS-COV-2 analysis, the risk of mortality related to COVID-19 is higher in the period with high exposure to fires than in the period with low exposure to fires. Another highlight concerns the relationship between fire occurrences and COVID-19 deaths. The results show that regions with high fire occurrences are associated with more cases of COVID deaths. Interpretation: The decision-make process is a critical problem mainly when it involves environmental and health control policies. Environmental policies are often more cost-effective as health measures than the use of public health services. This highlight the importance of data analyses to support the decision making and to identify population in need of better infrastructure due to historical environmental factors and the knowledge of associated health risk. The results suggest that The fires occurrences contribute to the increase of the respiratory diseases hospitalization. The mortality rate related to COVID-19 was higher for the period with high exposure to fires than the period with low exposure to fires. The regions with high fire occurrences is associated with more COVID-19 deaths, mainly in the months with high number of fires. Funding: No additional funding source was required for this study.",
        "DOI": "10.1016/j.lana.2021.100102",
        "paper_author": "Schroeder L.",
        "affiliation_name": "Universidade do Vale do Rio dos Sinos",
        "affiliation_city": "Sao Leopoldo",
        "affiliation_country": "Brazil",
        "affiliation_id": "60024559",
        "affiliation_state": "RS"
    },
    {
        "paper_title": "Methods and Applications of Social Media Monitoring of Mental Health During Disasters: Scoping Review",
        "publication": "JMIR Mental Health",
        "citied_by": "15",
        "cover_date": "2022-02-01",
        "Abstract": "Background: With the increasing frequency and magnitude of disasters internationally, there is growing research and clinical interest in the application of social media sites for disaster mental health surveillance. However, important questions remain regarding the extent to which unstructured social media data can be harnessed for clinically meaningful decision-making. Objective: This comprehensive scoping review synthesizes interdisciplinary literature with a particular focus on research methods and applications. Methods: A total of 6 health and computer science databases were searched for studies published before April 20, 2021, resulting in the identification of 47 studies. Included studies were published in peer-reviewed outlets and examined mental health during disasters or crises by using social media data. Results: Applications across 31 mental health issues were identified, which were grouped into the following three broader themes: estimating mental health burden, planning or evaluating interventions and policies, and knowledge discovery. Mental health assessments were completed by primarily using lexical dictionaries and human annotations. The analyses included a range of supervised and unsupervised machine learning, statistical modeling, and qualitative techniques. The overall reporting quality was poor, with key details such as the total number of users and data features often not being reported. Further, biases in sample selection and related limitations in generalizability were often overlooked. Conclusions: The application of social media monitoring has considerable potential for measuring mental health impacts on populations during disasters. Studies have primarily conceptualized mental health in broad terms, such as distress or negative affect, but greater focus is required on validating mental health assessments. There was little evidence for the clinical integration of social media-based disaster mental health monitoring, such as combining surveillance with social media-based interventions or developing and testing real-world disaster management tools. To address issues with study quality, a structured set of reporting guidelines is recommended to improve the methodological quality, replicability, and clinical relevance of future research on the social media monitoring of mental health during disasters.",
        "DOI": "10.2196/33058",
        "paper_author": "Teague S.J.",
        "affiliation_name": "Deakin University",
        "affiliation_city": "Geelong",
        "affiliation_country": "Australia",
        "affiliation_id": "60018805",
        "affiliation_state": "VIC"
    },
    {
        "paper_title": "Machine-Learning-Assisted Quantum Control in a Random Environment",
        "publication": "Physical Review Applied",
        "citied_by": "14",
        "cover_date": "2022-02-01",
        "Abstract": "Disorder in condensed matter and atomic physics is responsible for a great variety of fascinating quantum phenomena, which are still challenging for understanding, not to mention the relevant dynamical control. Here we introduce proof of the concept and analyze a neural-network-based machine-learning algorithm for achieving feasible high-fidelity quantum control of a particle in random environment. To explicitly demonstrate its capabilities, we show that convolutional neural networks are able to solve this problem as they can recognize the disorder and, by supervised learning, further produce the policy for the efficient low-energy cost control of a quantum particle in a time-dependent random potential. We show that the accuracy of the proposed algorithm is enhanced by a higher-dimensional mapping of the disorder pattern and using two neural networks, each properly trained for the given task. The designed method, being computationally more efficient than the gradient-descent optimization, can be applicable to identify and control various noisy quantum systems on a heuristic basis.",
        "DOI": "10.1103/PhysRevApplied.17.024040",
        "paper_author": "Huang T.",
        "affiliation_name": "Shanghai University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60023813",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Segmented Actor-Critic-Advantage Architecture for Reinforcement Learning Tasks",
        "publication": "TEM Journal",
        "citied_by": "0",
        "cover_date": "2022-02-01",
        "Abstract": "The article focuses on experiments with a multi module neural networks type of architecture for neuron-like machine used in reinforcing learning. This type of architecture can be used to solve complex robotic or policy optimization tasks and allows segmented storage of trained memory. Such technique speeds up the training process compared to existing actor-critical algorithms.",
        "DOI": "10.18421/TEM111-27",
        "paper_author": "Kaloev M.",
        "affiliation_name": "University of Rousse",
        "affiliation_city": "Ruse",
        "affiliation_country": "Bulgaria",
        "affiliation_id": "60006060",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Nanopore Detection of Cancer Biomarkers: A Challenge to Science",
        "publication": "Technology in Cancer Research and Treatment",
        "citied_by": "2",
        "cover_date": "2022-02-01",
        "Abstract": "Cancer is the most complex and leading cause of fatality worldwide. Despite meritorious research in the field of cancer, it is still a substantial threat to human life. In this article, we address a question on the present strategies and manifest the importance of critical biomarkers for cancer screening and early diagnosis before the symptoms appear. However, this goal can only be achieved if scientists will focus on ultra-sensitive detection techniques such as “Nanopore.” Nanopore sensing is a simple and rapid single-molecule detection technique that can detect multiple cancer biomarkers in femto-Molar concentrations in real time. Last but not least, we propose a systematic policy to win the war against cancer that is a big challenge to science.",
        "DOI": "10.1177/15330338221076669",
        "paper_author": "Bhatti H.",
        "affiliation_name": "Southeast University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60005244",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "Real-Time Highly Resolved Spatial-Temporal Vehicle Energy Consumption Estimation Using Machine Learning and Probe Data",
        "publication": "Transportation Research Record",
        "citied_by": "2",
        "cover_date": "2022-02-01",
        "Abstract": "Real-time highly resolved spatial-temporal vehicle energy consumption is a key missing dimension in transportation data. Most roadway link-level vehicle energy consumption data are estimated using average annual daily traffic measures derived from the Highway Performance Monitoring System; however, this method does not reflect day-to-day energy consumption fluctuations. As transportation planners and operators are becoming more environmentally attentive, they need accurate real-time link-level vehicle energy consumption data to assess energy and emissions; to incentivize energy-efficient routing; and to estimate energy impact caused by congestion, major events, and severe weather. This paper presents a computational workflow to automate the estimation of time-resolved vehicle energy consumption for each link in a road network of interest using vehicle probe speed and count data in conjunction with machine learning methods in real time. The real-time pipeline can deliver energy estimates within a couple seconds on query to its interface. The proposed method was evaluated on the transportation network of the metropolitan area of Chattanooga, Tennessee. The volume estimation results were validated with ground truth traffic volume data collected in the field. To demonstrate the effectiveness of the proposed method, the energy consumption pipeline was applied to real-world data to quantify road transportation-related energy reduction because of mitigation policies to slow the spread of COVID-19 and to measure energy loss resulting from congestion.",
        "DOI": "10.1177/03611981211039163",
        "paper_author": "Severino J.",
        "affiliation_name": "National Renewable Energy Laboratory",
        "affiliation_city": "Golden",
        "affiliation_country": "United States",
        "affiliation_id": "60030451",
        "affiliation_state": "CO"
    },
    {
        "paper_title": "Deep Learning and TextBlob Based Sentiment Analysis for Coronavirus (COVID-19) Using Twitter Data",
        "publication": "International Journal on Artificial Intelligence Tools",
        "citied_by": "26",
        "cover_date": "2022-02-01",
        "Abstract": "The novel Coronavirus (COVID-19) has affected the normal life of people around the world and has forced them to maintain social distance, self-quarantine and closing of many businesses. So, the people wish to share the information regarding the pandemic through social media e.g. Twitter. The key objective of this paper is to identify the sentiment associated with the pandemic by utilizing the tweets regarding the Coronavirus (COVID-19) and by using the python libraries to perform the task. We analyze the sentiment of people when Coronavirus has attained a peak level using the machine, deep learning techniques and TextBlob. We have classified the sentiments into positive and negative classes using the Machine Learning (NB, SVM, Logistic regression) approaches and deep learning-based Bi-LSTM model. The Bi-LSTM approach has achieved better accuracy (0.87) compared to the traditional machine learning models for Twitter sentiment classification. From the overall analysis, we could conclude that people are confident and they express more positivity towards the recovery from the COVID-19 pandemic. Such an analysis will help the policy and decision-makers to address the needs of the public appropriately.",
        "DOI": "10.1142/S0218213022500117",
        "paper_author": "Chandrasekaran G.",
        "affiliation_name": "Karunya Institute of Technology and Sciences",
        "affiliation_city": "Coimbatore",
        "affiliation_country": "India",
        "affiliation_id": "60100082",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Data Quality Barriers for Transparency in Public Procurement",
        "publication": "Information (Switzerland)",
        "citied_by": "18",
        "cover_date": "2022-02-01",
        "Abstract": "Governments need to be accountable and transparent for their public spending decisions in order to prevent losses through fraud and corruption as well as to build healthy and sustainable economies. Open data act as a major instrument in this respect by enabling public administrations, service providers, data journalists, transparency activists, and regular citizens to identify fraud or uncompetitive markets through connecting related, heterogeneous, and originally unconnected data sources. To this end, in this article, we present our experience in the case of Slovenia, where we successfully applied a number of anomaly detection techniques over a set of open disparate data sets integrated into a Knowledge Graph, including procurement, company, and spending data, through a linked data-based platform called TheyBuyForYou. We then report a set of guidelines for publishing high quality procurement data for better procurement analytics, since our experience has shown us that there are significant shortcomings in the quality of data being published. This article contributes to enhanced policy making by guiding public administrations at local, regional, and national levels on how to improve the way they publish and use procurement-related data; developing technologies and solutions that buyers in the public and private sectors can use and adapt to become more transparent, make markets more competitive, and reduce waste and fraud; and providing a Knowledge Graph, which is a data resource that is designed to facilitate integration across multiple data silos by showing how it adds context and domain knowledge to machine-learning-based procurement analytics.",
        "DOI": "10.3390/info13020099",
        "paper_author": "Soylu A.",
        "affiliation_name": "OsloMet – StorbyUniversitetet",
        "affiliation_city": "Oslo",
        "affiliation_country": "Norway",
        "affiliation_id": "60068730",
        "affiliation_state": "Oslo"
    },
    {
        "paper_title": "Use of Triangulation in Comparing the Blockchain Knowledge Structure between China and South Korea: Scientometric Network, Topic Modeling, and Prediction Technique",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "3",
        "cover_date": "2022-02-01",
        "Abstract": "Blockchain, as a new innovative technology, has become a popular topic in many fields in recent years. In this study, triangulation was used to investigate the development of knowledge struc-tures. First, scientometric network analysis was employed to identify the cooperation of knowledge networks. It was found that the structure of blockchain knowledge networks in China is relatively more complex and diverse than in South Korea. Since increased teamwork in blockchain is conducive to the creation of high-quality knowledge products, the Chinese government appears to strongly promote diversified cooperation on blockchain technology through centralized policies. Second, machine-learning topic modeling was used to analyze the content exchanged via a collaborative network. As a result, it was found that both countries lacked the societal and commercial aspects of blockchain technology. Finally, we developed a prediction technique based on the Ernie model to automatically categorize the nature of blockchain research.",
        "DOI": "10.3390/su14042326",
        "paper_author": "Zhu Y.P.",
        "affiliation_name": "Yeungnam University",
        "affiliation_city": "Gyeongsan",
        "affiliation_country": "South Korea",
        "affiliation_id": "60001170",
        "affiliation_state": "Gyeongsangbuk-do"
    },
    {
        "paper_title": "Time Series Network Data Enabling Distributed Intelligence. A Holistic IoT Security Platform Solution",
        "publication": "Electronics (Switzerland)",
        "citied_by": "9",
        "cover_date": "2022-02-01",
        "Abstract": "The Internet of Things (IoT) encompasses multiple fast-emerging technologies controlling and connecting millions of new devices every day in several application domains. The increased number of interconnected IoT devices, their limited computational power, and the evolving sophistication of cyber security threats, results in increased security challenges for the IoT ecosystem. The diversity of IoT devices, and the variety of QoS requirements among several domains of IoT application, impose considerable challenges in designing and implementing a robust IoT security solution. The aim of this paper is to present an efficient, robust, and easy-to-use system, for IoT cyber security operators. Following a by-design security approach, the proposed system is a platform comprising four distinct yet cooperating components; a distributed AI-enhanced detection of potential threats and anomalies mechanisms, an AI-based generation of effective mitigation strategies according to the severity of detected threats, a system for the verification of SDN routing decisions along with networkand resource-related policies, and a comprehensive and intuitive security status visualization and analysis. The distributed anomaly detection scheme implementing multiple AI-powered agents is deployed across the IoT network nodes aiming to efficiently monitor the entire network infrastructure. Network traffic data are fed to the AI agents, which process consecutive traffic samples from the network in a time series analysis manner, where consecutive time windows framing the traffic of the surrounding nodes are processed by a graph neural network algorithm. Any detected anomalies are handled by a mitigation engine employing a distributed neural network algorithm, which exploits the recorded anomalous events and deploys appropriate responses for optimal threat mitigation. The implemented platform also includes the hypothesis testing module, and a multi-objective optimization tool for the quick verification of routing decisions. The system incorporates visualization and analytics functionality and a customizable user interface.",
        "DOI": "10.3390/electronics11040529",
        "paper_author": "Protogerou A.",
        "affiliation_name": "Information Technologies Institute",
        "affiliation_city": "Thermi",
        "affiliation_country": "Greece",
        "affiliation_id": "126514833",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Contracts in the Age of Smart Readers",
        "publication": "George Washington Law Review",
        "citied_by": "11",
        "cover_date": "2022-02-01",
        "Abstract": "What does it mean to have machines that can read, explain, and evaluate contracts? Recent advances in machine learning have led to a fundamental breakthrough in machine language models, portending a profound shift in the ability of machines to process text. Such a shift has far-reaching consequences for diverse areas of law, which are predicated on, and justified by, the existence of information barriers. Our object here is to provide a general framework for evaluating the legal and policy implications of employing language models as “smart readers”-tools that read, analyze, and assess contracts, disclosures, and privacy policies. Synthesizing state-of-the-art developments, we identify four core capabilities of smart readers. Based on real-world examples produced by new machine-learning models, we demonstrate that smart readers can: simplify complex legal language; personalize the contractual presentation to the user's specific sociocultural identity; interpret the meaning of contractual terms; and benchmark and rank contracts based on their quality. Nevertheless, the implications of smart readers are more complex than initially meets the eye. Although smart readers can overcome traditional information barriers and empower parties, they rely on black-box models that sophisticated parties can exploit. Smart readers can close some of the gaps in access to justice, but they also introduce concerns about contractual bias and discrimination. And even though smart readers can improve term transparency, they might lead judges and policymakers to relax their guard prematurely. The current body of doctrine and scholarship is ill equipped to address both the prospects and risks of smart reader technology. This Article narrows this gap. It maps the necessary theoretical, policy, and doctrinal adaptations to the age when machines can automate the reading of contracts.",
        "DOI": "NA",
        "paper_author": "Arbel Y.A.",
        "affiliation_name": "University of Alabama",
        "affiliation_city": "Pensacola",
        "affiliation_country": "United States",
        "affiliation_id": "122975271",
        "affiliation_state": "FL"
    },
    {
        "paper_title": "Early Identification of Maternal Cardiovascular Risk Through Sourcing and Preparing Electronic Health Record Data: Machine Learning Study",
        "publication": "JMIR Medical Informatics",
        "citied_by": "2",
        "cover_date": "2022-02-01",
        "Abstract": "Background: Health care data are fragmenting as patients seek care from diverse sources. Consequently, patient care is negatively impacted by disparate health records. Machine learning (ML) offers a disruptive force in its ability to inform and improve patient care and outcomes. However, the differences that exist in each individual's health records, combined with the lack of health data standards, in addition to systemic issues that render the data unreliable and that fail to create a single view of each patient, create challenges for ML. Although these problems exist throughout health care, they are especially prevalent within maternal health and exacerbate the maternal morbidity and mortality crisis in the United States. Objective: This study aims to demonstrate that patient records extracted from the electronic health records (EHRs) of a large tertiary health care system can be made actionable for the goal of effectively using ML to identify maternal cardiovascular risk before evidence of diagnosis or intervention within the patient's record. Maternal patient records were extracted from the EHRs of a large tertiary health care system and made into patient-specific, complete data sets through a systematic method. Methods: We outline the effort that was required to define the specifications of the computational systems, the data set, and access to relevant systems, while ensuring that data security, privacy laws, and policies were met. Data acquisition included the concatenation, anonymization, and normalization of health data across multiple EHRs in preparation for their use by a proprietary risk stratification algorithm designed to establish patient-specific baselines to identify and establish cardiovascular risk based on deviations from the patient's baselines to inform early interventions. Results: Patient records can be made actionable for the goal of effectively using ML, specifically to identify cardiovascular risk in pregnant patients. Conclusions: Upon acquiring data, including their concatenation, anonymization, and normalization across multiple EHRs, the use of an ML-based tool can provide early identification of cardiovascular risk in pregnant patients.",
        "DOI": "10.2196/34932",
        "paper_author": "Shara N.",
        "affiliation_name": "Georgetown-Howard Universities Center for Clinical and Translational Science",
        "affiliation_city": "Washington, D.C.",
        "affiliation_country": "United States",
        "affiliation_id": "60138837",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Mapping spatial distribution and geographic shifts of East African highland banana (Musa spp.) in Uganda",
        "publication": "PLoS ONE",
        "citied_by": "13",
        "cover_date": "2022-02-01",
        "Abstract": "East African highland banana (Musa acuminata genome group AAA-EA; hereafter referred to as banana) is critical for Uganda’s food supply, hence our aim to map current distribution and to understand changes in banana production areas over the past five decades. We collected banana presence/absence data through an online survey based on high-resolution satellite images and coupled this data with independent covariates as inputs for ensemble machine learning prediction of current banana distribution. We assessed geographic shifts of production areas using spatially explicit differences between the 1958 and 2016 banana distribution maps. The biophysical factors associated with banana spatial distribution and geographic shift were determined using a logistic regression model and classification and regression tree, respectively. Ensemble models were superior (AUC = 0.895; 0.907) compared to their constituent algorithms trained with 12 and 17 covariates, respectively: random forests (AUC = 0.883; 0.901), gradient boosting machines (AUC = 0.878; 0.903), and neural networks (AUC = 0.870; 0.890). The logistic regression model (AUC = 0.879) performance was similar to that for the ensemble model and its constituent algorithms. In 2016, banana cultivation was concentrated in the western (44%) and central (36%) regions, while only a small proportion was in the eastern (18%) and northern (2%) regions. About 60% of increased cultivation since 1958 was in the western region; 50% of decreased cultivation in the eastern region; and 44% of continued cultivation in the central region. Soil organic carbon, soil pH, annual precipitation, slope gradient, bulk density and blue reflectance were associated with increased banana cultivation while precipitation seasonality and mean annual temperature were associated with decreased banana cultivation over the past 50 years. The maps of spatial distribution and geographic shift of banana can support targeting of context-specific intensification options and policy advocacy to avert agriculture driven environmental degradation.",
        "DOI": "10.1371/journal.pone.0263439",
        "paper_author": "Ochola D.",
        "affiliation_name": "International Institute of Tropical Agriculture Uganda",
        "affiliation_city": "Kampala",
        "affiliation_country": "Uganda",
        "affiliation_id": "60071690",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Analysis of Performance Measure in Q Learning with UCB Exploration",
        "publication": "Mathematics",
        "citied_by": "6",
        "cover_date": "2022-02-01",
        "Abstract": "Compared to model-based Reinforcement Learning (RL) approaches, model-free RL algorithms, such as Q-learning, require less space and are more expressive, since specifying value functions or policies is more flexible than specifying the model for the environment. This makes model-free algorithms more prevalent in modern deep RL. However, model-based methods can more efficiently extract the information from available data. The Upper Confidence Bound (UCB) bandit can improve the exploration bonuses, and hence increase the data efficiency in the Q-learning framework. The cumulative regret of the Q-learning algorithm with an UCB exploration policy in the episodic Markov Decision Process has recently been explored in the underlying environment of finite state-action space. In this paper, we study the regret bound of the Q-learning algorithm with UCB exploration in the scenario of compact state-action metric space. We present an algorithm that adaptively discretizes the continuous state-action space and iteratively updates Q-values. The algorithm is able to efficiently optimize rewards and minimize cumulative regret.",
        "DOI": "10.3390/math10040575",
        "paper_author": "Ye W.",
        "affiliation_name": "Credit Suisse USA",
        "affiliation_city": "New York",
        "affiliation_country": "United States",
        "affiliation_id": "60097823",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "A Parallel Deep Reinforcement Learning Framework for Controlling Industrial Assembly Lines",
        "publication": "Electronics (Switzerland)",
        "citied_by": "7",
        "cover_date": "2022-02-01",
        "Abstract": "Decision-making in a complex, dynamic, interconnected, and data-intensive industrial environment can be improved with the assistance of machine-learning techniques. In this work, a complex instance of industrial assembly line control is formalized and a parallel deep reinforcement learning approach is presented. We consider an assembly line control problem in which a set of tasks (e.g., vehicle assembly tasks) needs to be planned and controlled during their execution, with the aim of optimizing given key performance criteria. Specifically, the aim will be that of planning the task in order to minimize the total time taken to execute all the tasks (also called cycle time). Tasks run on workstations in the assembly line. To run, tasks need specific resources. Therefore, the tackled problem is that of optimally mapping tasks and resources to workstations, and deciding the optimal execution times of the tasks. In doing so, several constraints need to be respected (e.g., precedence constraints among the tasks, constraints on needed resources to run tasks, deadlines, etc.). The proposed approach uses deep reinforcement learning to learn a tasks/resources mapping policy that is effective in minimizing the resulting cycle time. The proposed method allows us to explicitly take into account all the constraints, and, once training is complete, can be used in real time to dynamically control the execution of tasks. Another motivation for the proposed work is in the ability of the used method to also work in complex scenarios, and in the presence of uncertainties. As a matter of fact, the use of deep neural networks allows for learning the model of the assembly line problem, in contrast with, e.g., optimization-based techniques, which require explicitly writing all the equations of the model of the problem. In order to speed up the training phase, we adopt a learning scheme in which more agents are trained in parallel. Simulations show that the proposed method can provide effective real-time decision support to industrial operators for scheduling and rescheduling activities, achieving the goal of minimizing the total tasks’ execution time.",
        "DOI": "10.3390/electronics11040539",
        "paper_author": "Tortorelli A.",
        "affiliation_name": "Sapienza Università di Roma",
        "affiliation_city": "Rome",
        "affiliation_country": "Italy",
        "affiliation_id": "60032350",
        "affiliation_state": "RM"
    },
    {
        "paper_title": "Mapping Sugarcane in Central India with Smartphone Crowdsourcing",
        "publication": "Remote Sensing",
        "citied_by": "8",
        "cover_date": "2022-02-01",
        "Abstract": "In India, the second-largest sugarcane producing country in the world, accurate mapping of sugarcane land is a key to designing targeted agricultural policies. Such a map is not available, however, as it is challenging to reliably identify sugarcane areas using remote sensing due to sug-arcane’s phenological characteristics, coupled with a range of cultivation periods for different vari-eties. To produce a modern sugarcane map for the Bhima Basin in central India, we utilized crowdsourced data and applied supervised machine learning (neural network) and unsupervised classification methods individually and in combination. We highlight four points. First, smartphone crowdsourced data can be used as an alternative ground truth for sugarcane mapping but requires careful correction of potential errors. Second, although the supervised machine learning method performs best for sugarcane mapping, the combined use of both classification methods improves sugarcane mapping precision at the cost of worsening sugarcane recall and missing some actual sugarcane area. Third, machine learning image classification using high-resolution satellite imagery showed significant potential for sugarcane mapping. Fourth, our best estimate of the sugarcane area in the Bhima Basin is twice that shown in government statistics. This study provides useful insights into sugarcane mapping that can improve the approaches taken in other regions.",
        "DOI": "10.3390/rs14030703",
        "paper_author": "Lee J.Y.",
        "affiliation_name": "Stanford University",
        "affiliation_city": "Stanford",
        "affiliation_country": "United States",
        "affiliation_id": "60012708",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Variability in Physical Inactivity Responses of University Students during COVID-19 Pandemic: A Monitoring of Daily Step Counts Using a Smartphone Application",
        "publication": "International Journal of Environmental Research and Public Health",
        "citied_by": "2",
        "cover_date": "2022-02-01",
        "Abstract": "This study investigated the changes in physical inactivity of university students during the COVID-19 pandemic, with reference to their academic calendar. We used the daily step counts recorded by a smartphone application (iPhone Health App) from April 2020 to January 2021 (287 days) for 603 participants. The data for 287 days were divided into five periods based on their academic calendar. The median value of daily step counts across each period was calculated. A k-means clustering analysis was performed to classify the 603 participants into subgroups to demonstrate the variability in the physical inactivity responses. The median daily step counts, with a 7-day moving average, dramatically decreased from 5000 to 2000 steps/day in early April. It remained at a lower level (less than 2000 steps/day) during the first semester, then increased to more than 5000 steps/day at the start of summer vacation. The clustering analysis demonstrated the variability in physical inactivity responses. The inactive students did not recover daily step counts throughout the year. Consequently, promoting physical activity is recommended for inactive university students over the course of the whole semester.",
        "DOI": "10.3390/ijerph19041958",
        "paper_author": "Konda S.",
        "affiliation_name": "Graduate School of Medicine",
        "affiliation_city": "Suita",
        "affiliation_country": "Japan",
        "affiliation_id": "60175967",
        "affiliation_state": "Osaka"
    },
    {
        "paper_title": "Verification of De-Identification Techniques for Personal Information Using Tree-Based Methods with Shapley Values",
        "publication": "Journal of Personalized Medicine",
        "citied_by": "14",
        "cover_date": "2022-02-01",
        "Abstract": "With the development of big data and cloud computing technologies, the importance of pseudonym information has grown. However, the tools for verifying whether the de-identification methodology is correctly applied to ensure data confidentiality and usability are insufficient. This paper proposes a verification of de-identification techniques for personal healthcare information by considering data confidentiality and usability. Data are generated and preprocessed by considering the actual statistical data, personal information datasets, and de-identification datasets based on medical data to represent the de-identification technique as a numeric dataset. Five tree-based regression models (i.e., decision tree, random forest, gradient boosting machine, extreme gradient boosting, and light gradient boosting machine) are constructed using the de-identification dataset to effectively discover nonlinear relationships between dependent and independent variables in numerical datasets. Then, the most effective model is selected from personal information data in which pseudonym processing is essential for data utilization. The Shapley additive explanation, an explainable artificial intelligence technique, is applied to the most effective model to establish pseudonym processing policies and machine learning to present a machine-learning process that selects an appropriate de-identification methodology.",
        "DOI": "10.3390/jpm12020190",
        "paper_author": "Lee J.",
        "affiliation_name": "Chung-Ang University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea",
        "affiliation_id": "60014237",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Smart City Projects Boost Urban Energy Efficiency in China",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "5",
        "cover_date": "2022-02-01",
        "Abstract": "Policy makers around the world are turning to smart city projects in an effort to address the challenges of population growth, energy efficiency, and environmental sustainability. Previous studies have evaluated the effect of smart city projects on air quality. However, evidence on the impact of the projects on energy efficiency remains unclear. This study gathered prefecture-level city panel data in China, and used three strategies, namely a difference-in-differences estimator, a matching difference-in-differences estimator, and a counterfactual model using a machine learning algorithm, to assess the impact of smart city projects on energy efficiency. This study reported similar results across these strategies above. That is, after the introduction of a smart city project, energy efficiency had a remarkable and sizeable increase, ranging from 4 to 7 per cent. Moreover, this study shows that the effects of smart city projects increased over time. In addition, this study found that the effects varied according to the characteristics of the cities.",
        "DOI": "10.3390/su14031814",
        "paper_author": "Tu Z.",
        "affiliation_name": "Central China Normal University",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60010591",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Assessing Inequities in COVID-19 Vaccine Roll-Out Strategy Programs: A Cross-Country Study Using a Machine Learning Approach",
        "publication": "Vaccines",
        "citied_by": "13",
        "cover_date": "2022-02-01",
        "Abstract": "After the start of the COVID-19 pandemic and its spread across the world, countries have adopted containment measures to stop its transmission, limit fatalities, and relieve hospitals from straining and overwhelming conditions imposed by the virus. Many countries implemented social distancing and lockdown strategies that negatively impacted their economies and the psychological wellbeing of their citizens, even though they contributed to saving lives. Recently approved and available, COVID-19 vaccines can provide a really viable and sustainable option for controlling the pandemic. However, their uptake represents a global challenge due to vaccine hesitancy and logistic– organizational hurdles that have made its distribution stagnant in several developed countries despite several appeals by the media, policy-and decision-makers, and community leaders. Vaccine distribution is also a concern in developing countries, where there is a scarcity of doses. The objective of the present study was to set up a metric to assess vaccination uptake and identify national socio-economic factors influencing this indicator. We conducted a cross-country study. We first estimated the vaccination uptake rate across countries by fitting a logistic model to reported daily case numbers. Using the uptake rate, we estimated the vaccine roll-out index. Next, we used Random Forest, an “off-the-shelf” machine learning algorithm, to study the association between vaccination uptake rate and socio-economic factors. We found that the mean vaccine roll-out index is 0.016 (standard deviation 0.016), with a range between 0.0001 (Haiti) and 0.0829 (Mongolia). The top four factors associated with the vaccine roll-out index are the median per capita income, human development index, percentage of individuals who have used the internet in the last three months, and health expenditure per capita. The still-ongoing COVID-19 pandemic has shed light on the disparity in vaccine adoption across low-and high-income countries, which represents a global public health challenge. We must pave the way for universal access to vaccines and other approved treatments, regardless of demographic structures and underlying health conditions. Income disparity remains, instead, an important cause of vaccine inequity, which restricts the functioning of the global vaccine allocation framework and, thus, the ending of the pandemic. Stronger mechanisms are needed to foster countries’ political willingness to promote vaccine and drug access equity in a globalized society where future pandemics and other global health crises can be anticipated.",
        "DOI": "10.3390/vaccines10020194",
        "paper_author": "Kazemi M.",
        "affiliation_name": "Faculty of Science",
        "affiliation_city": "Toronto",
        "affiliation_country": "Canada",
        "affiliation_id": "60191707",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "A machine learning framework to quantify and assess the impact of COVID-19 on the power sector: An Indian context",
        "publication": "Advances in Applied Energy",
        "citied_by": "12",
        "cover_date": "2022-02-01",
        "Abstract": "As the COVID-19 continues to disrupt the global norms, there is the requirement of modeling frameworks to accurately assess and quantify the impact of the pandemic on the electricity sector and its emissions. In this study, we devise machine learning models to estimate the pandemic induced reduction in electricity consumption based on weather, econometrics, and social-distancing parameters for seven major Indian states. As per our baseline electricity consumption model, we find that the electricity consumption dropped by 15–33% in 2020 (March-May) during the complete lockdown phase, followed by 6–13% (June-August) during the unlock phases and gradually reached the norms by September 2020. As a result, the net CO2 emissions from power generation in 2020 dropped by 7% and 5% compared to 2018 and 2019 respectively. Amidst the ongoing second wave since mid-April 2021, we projected the electricity consumption across states from May-August by accounting for two scenarios. Under the reference and worst-case scenarios, the electricity consumption approximates 106% and 96% of the non-pandemic situation, respectively. The modeling framework developed in this study is purely data-orientated, cross-deployable across spatio-temporal scales and can serve as a valuable tool to inform current and future energy policies amidst and post COVID-19.",
        "DOI": "10.1016/j.adapen.2021.100078",
        "paper_author": "Suvarna M.",
        "affiliation_name": "National University of Singapore",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore",
        "affiliation_id": "60017161",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Proactive Handover Decision for UAVs with Deep Reinforcement Learning",
        "publication": "Sensors",
        "citied_by": "14",
        "cover_date": "2022-02-01",
        "Abstract": "The applications of Unmanned Aerial Vehicles (UAVs) are rapidly growing in domains such as surveillance, logistics, and entertainment and require continuous connectivity with cellular networks to ensure their seamless operations. However, handover policies in current cellular networks are primarily designed for ground users, and thus are not appropriate for UAVs due to frequent fluctuations of signal strength in the air. This paper presents a novel handover decision scheme deploying Deep Reinforcement Learning (DRL) to prevent unnecessary handovers while maintaining stable connectivity. The proposed DRL framework takes the UAV state as an input for a proximal policy optimization algorithm and develops a Received Signal Strength Indicator (RSSI) based on a reward function for the online learning of UAV handover decisions. The proposed scheme is evaluated in a 3D-emulated UAV mobility environment where it reduces up to 76 and 73% of unnecessary handovers compared to greedy and Q-learning-based UAV handover decision schemes, respectively. Furthermore, this scheme ensures reliable communication with the UAV by maintaining the RSSI above −75 dBm more than 80% of the time.",
        "DOI": "10.3390/s22031200",
        "paper_author": "Jang Y.",
        "affiliation_name": "SKKU College of Information and Communication Engineering",
        "affiliation_city": "Suwon",
        "affiliation_country": "South Korea",
        "affiliation_id": "60117156",
        "affiliation_state": "Gyeonggi-do"
    },
    {
        "paper_title": "Pregnancy in a pandemic: inequalities in maternal health",
        "publication": "The Lancet Digital Health",
        "citied_by": "1",
        "cover_date": "2022-02-01",
        "Abstract": "NA",
        "DOI": "10.1016/S2589-7500(22)00005-X",
        "paper_author": "The Lancet Digital Health ",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Retrieval of Fine-Grained PM2.5 Spatiotemporal Resolution Based on Multiple Machine Learning Models",
        "publication": "Remote Sensing",
        "citied_by": "18",
        "cover_date": "2022-02-01",
        "Abstract": "Due to the country’s rapid economic growth, the problem of air pollution in China is becoming increasingly serious. In order to achieve a win-win situation for the environment and urban development, the government has issued many policies to strengthen environmental protec-tion. PM2.5 is the primary particulate matter in air pollution, so an accurate estimation of PM2.5 distribution is of great significance. Although previous studies have attempted to retrieve PM2.5 using geostatistical or aerosol remote sensing retrieval methods, the current rough resolution and accuracy remain as limitations of such methods. This paper proposes a fine-grained spatiotemporal PM2.5 retrieval method that comprehensively considers various datasets, such as Landsat 8 satellite images, ground monitoring station data, and socio-economic data, to explore the applicability of different machine learning algorithms in PM2.5 retrieval. Six typical algorithms were used to train the multi-dimensional elements in a series of experiments. The characteristics of retrieval accuracy in different scenarios were clarified mainly according to the validation index, R2. The random forest algorithm was shown to have the best numerical and PM2.5-based air-quality-category accuracy, with a cross-validated R2 of 0.86 and a category retrieval accuracy of 0.83, while both maintained excellent retrieval accuracy and achieved a high spatiotemporal resolution. Based on this retrieval model, we evaluated the PM2.5 distribution characteristics and hourly variation in the sample area, as well as the functions of different input variables in the model. The PM2.5 retrieval method proposed in this paper provides a new model for fine-grained PM2.5 concentration estimation to determine the distribution laws of air pollutants and thereby specify more effective measures to realize the high-quality development of the city.",
        "DOI": "10.3390/rs14030599",
        "paper_author": "Ma P.",
        "affiliation_name": "Nantong University",
        "affiliation_city": "Nantong",
        "affiliation_country": "China",
        "affiliation_id": "60021783",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "COVID-19: Role of Robotics, Artificial Intelligence and Machine Learning During the Pandemic",
        "publication": "Current Medical Imaging",
        "citied_by": "27",
        "cover_date": "2022-02-01",
        "Abstract": "The outbreak of COVID-19 has led to a global health emergency. Emerging from China, it has now been declared as a pandemic. Owing to the fast pace at which it spreads, its control and prevention have now become the greatest challenge. The inner structural analysis of the virus is an important area of research for the invention of the potential drug. The countries are following different strategies and policies to fight against COVID-19; various schemes have also been employed to cope up with the economic crisis. While the government is struggling to balance between the public health sector and the economic collapse, the researchers and medicine practitioners are inclined towards obtaining treatment and early detection of the deadly disease. Further, the impact of COVID-19 on Dentistry is alarming and posing severe threats to the professionals as well. Now, the technology is helping the countries fight against the disease. ML and AI based applications are substantially aiding the process of detection and diagnosis of novel coronavirus. Science of Robotics is another approach followed with an aim to improve patient care.",
        "DOI": "10.2174/1573405617666210224115722",
        "paper_author": "Sodhi G.K.",
        "affiliation_name": "Model Institute of Engineering and Technology",
        "affiliation_city": "Jammu",
        "affiliation_country": "India",
        "affiliation_id": "60097184",
        "affiliation_state": "JK"
    },
    {
        "paper_title": "A Data-Centric Machine Learning Methodology: Application on Predictive Maintenance of Wind Turbines",
        "publication": "Energies",
        "citied_by": "25",
        "cover_date": "2022-02-01",
        "Abstract": "Nowadays, the energy sector is experiencing a profound transition. Among all renewable energy sources, wind energy is the most developed technology across the world. To ensure the profitability of wind turbines, it is essential to develop predictive maintenance strategies that will optimize energy production while preventing unexpected downtimes. With the huge amount of data collected every day, machine learning is seen as a key enabling approach for predictive maintenance of wind turbines. However, most of the effort is put into the optimization of the model architectures and its parameters, whereas data-related aspects are often neglected. The goal of this paper is to contribute to a better understanding of wind turbines through a data-centric machine learning methodology. In particular, we focus on the optimization of data preprocessing and feature selection steps of the machine learning pipeline. The proposed methodology is used to detect failures affecting five components on a wind farm composed of five turbines. Despite the simplicity of the used machine learning model (a decision tree), the methodology outperformed model-centric approach by improving the prediction of the remaining useful life of the wind farm, making it more reliable and contributing to the global efforts towards tackling climate change.",
        "DOI": "10.3390/en15030826",
        "paper_author": "Garan M.",
        "affiliation_name": "Technická Univerzita v Liberci",
        "affiliation_city": "Liberec",
        "affiliation_country": "Czech Republic",
        "affiliation_id": "60032743",
        "affiliation_state": "Liberec Region"
    },
    {
        "paper_title": "Differentiated Protection and Hot/Cold-Aware Data Placement Policies through k-Means Clustering Analysis for 3D-NAND SSDs",
        "publication": "Electronics (Switzerland)",
        "citied_by": "2",
        "cover_date": "2022-02-01",
        "Abstract": "3D-NAND flash memory provides high capacity per unit area by stacking 2D-NAND cells having a planar structure. However, because of the nature of the lamination process, the frequency of error occurrence varies depending on each layer or physical cell location. This phenomenon becomes more pronounced as the number of flash memory write/erase (Program/Erasure) operations increases. Error correction code (ECC) is used for error correction in the majority of flash-based storage devices, such as SSDs (Solid State Drive). As this method provides a constant level of data protection for all-flash memory pages, there is a limitation in 3D-NAND flash memory, where the error rate varies depending on physical location. Consequently, in this paper, pages and layers with varying error rates are classified into clusters using the k-means machine-learning algorithm, and each cluster is assigned a different level of data protection strength. We classify pages and layers based on the number of error occurrences measured at the end of the endurance test, and for areas vulnerable to errors, it is shown as an example of providing differentiated data protection strength by adding parity data to the stripe. Furthermore, areas vulnerable to retention errors are identified based on retention error rates, and bit error rates are significantly reduced through our hot/cold-aware data placement policy. We show that the proposed differential data protection and hot/cold-aware data placement policies improve the reliability and lifespan of 3D-NAND flash memory compared with the existing ECC-or RAID-type data protection scheme.",
        "DOI": "10.3390/electronics11030398",
        "paper_author": "Son S.",
        "affiliation_name": "Gyeongsang National University",
        "affiliation_city": "Jinju",
        "affiliation_country": "South Korea",
        "affiliation_id": "60023075",
        "affiliation_state": "Kyongsangnam-do"
    },
    {
        "paper_title": "Special Issue on Computational Approaches for COVID-19 Disease Medical Image Analysis",
        "publication": "Current Medical Imaging",
        "citied_by": "0",
        "cover_date": "2022-02-01",
        "Abstract": "NA",
        "DOI": "10.2174/1573405618666220104191827",
        "paper_author": "Dhiman G.",
        "affiliation_name": "Government Bikram College of Commerce",
        "affiliation_city": "Patiala",
        "affiliation_country": "India",
        "affiliation_id": "60276078",
        "affiliation_state": "PB"
    },
    {
        "paper_title": "Markov Decision Process and Prior Control Vector for Weak Condition Natural Language Generation",
        "publication": "Jisuanji Xuebao/Chinese Journal of Computers",
        "citied_by": "0",
        "cover_date": "2022-02-01",
        "Abstract": "Communicating with computers by using natural language is one of the ultimate goals of artificial intelligence. The automatic text generation, i.e., natural language generation, is an important yet challenging problem. One popular choice of natural language generation is using language models, including n-gram and feed-forward neural network. Recently, with the development of recurrent neural networks, especially long short-term memory (LSTM), the ability of language models to process sequential text data has been greatly enhanced. Similarly, the LSTM based encoder-decoder has been deployed in many applications involving with natural language generation such as machine translation, video caption and speech recognition. However, LSTM language models and encoder-decoder networks are trained by the cross entropy criterion at the word level, which is to maximize the log likelihood of oracle word sequence. This may lead to the exposure bias problem: the input of the model during the training phase is the oracle reference sequence, while during the test phase the input is its own predicted sequence. Another problem is that the cross entropy is calculated at the word level. During the test phase, the model tries to generate the word which maximizes the probability at this specific frame, which ignores the naturality of the whole sequence. In other words, the mismatch between the training criterion and the final test metric may lead to performance degradation. One way to bridge the gap between the word level training criterion and sequence level test metric is to provide extra prior knowledge to the model. Normally, the commonly used structured language model with additional input such as pos tag or topic model could be used to bridge the gap. Some researchers also focus on extracting sentence level embedding to generate better sequences. These methods, including schedule sampling, can also reduce the influence of exposure bias. Another possible solution is to modify the training criterion. Generative adversarial network (GAN) has shown the powerful ability to generate images. Some researchers also implement sequence GAN by using policy gradient. However, sequence GAN does not directly optimize the test metric, and it is hard to train and converge. In this paper, we focus on the natural language generation with the weak condition, i.e., the text itself. We reformulate the natural language generation problem with Markov decision process and incorporate prior control vector derived from training corpus to guide the generation. Prior control vectors can be considered as representation of prior partitions of sequence-level sentence space. It is extracted from the training data unsupervisedly and provides prior knowledge to help the model generate better results. With this provided prior knowledge, the language generation space is effectively narrowed and more appropriately guided. By the Markov decision process definition, we utilize policy gradient to train the LSTM network directly with the test metric rather than the cross entropy loss. The experimental results show that the proposed methods can address the above mentioned problems and significantly improve the generation performance. The proposed model gain about 2%-3% performance improvement on BLEU score compared with baseline model on three different dataset.",
        "DOI": "10.11897/SP.J.1016.2022.00289",
        "paper_author": "Liu Q.",
        "affiliation_name": "Shanghai Jiao Tong University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60025084",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Satellite Remote Sensing of Water Quality Variation in a Semi-Enclosed Bay (Yueqing Bay) under Strong Anthropogenic Impact",
        "publication": "Remote Sensing",
        "citied_by": "17",
        "cover_date": "2022-02-01",
        "Abstract": "The semi-enclosed bays impacted by heavy anthropogenic activities have weak water exchange and purification capacities. Most of the sea bays have suffered severe eutrophication, water quality deterioration, ecosystem degradation and other problems. Although many countries and local governments have carried out corresponding environmental protection actions, the evaluation of their effectiveness still requires monitoring technology and data support for long-term water environment change. In this study, we take Yueqing Bay, the fourth largest bay in China, as a case to study the satellite-based water quality monitoring and variation analysis. We established a nutrient retrieval model for Yueqing Bay to produce a long-term series of nutrient concentration products in Yueqing Bay from 2013 to 2020, based on Landsat remote sensing images and long-term observation data, combined with support vector machine learning and water temperature and satellite spectra as input parameters, and then we analyzed its spatiotemporal variations and driving factors. In general, nutrient concentrations in the western part of the bay were higher than those in the eastern part. Levels of dissolved inorganic nitrogen (DIN) were lower in summer than in spring and winter, and reactive phosphate (PO4-P) levels were lower in summer and higher in autumn. In terms of natural factors, physical effects (e.g., seasonal variations in flow field) and biological effects (e.g., seasonal differences in the intensity of plankton photosynthesis) were the main causes of seasonal differences in nutrient concentration in Yueqing Bay. Nutrient concentration generally increased from 2013 to 2015 but decreased slightly after 2015. Over the past decade, the economy and industry of Yueqing Bay basin have developed rapidly. Wastewater resulting from anthropogenic production and consumption was transported via streams into Yueqing Bay, leading to the continuous increase in nutrient concentrations (the variation rates: aDI N > 0, aPO4 −P > 0), which directly or indirectly caused high nutrient concentrations in some areas of the bay (e.g., Southwest Shoal at the mouth of Yueqing Bay). After 2015, the various ecological remediation policies adopted by cities around Yueqing Bay have mitigated, to some extent, the increasing nutrient concentration trends (the variation rates: aDI N < 0, aPO4 −P < 0), but not significantly (P > 0.1). The environmental restoration of Yueqing Bay also requires continuous and long-term ecological protection and restoration work to be effective. This research can provide a reference for ecological environment monitoring and remote sensing data application for similar semi-enclosed bays, and support the sustainable development of the bay.",
        "DOI": "10.3390/rs14030550",
        "paper_author": "Zhu B.",
        "affiliation_name": "Southern Marine Science and Engineering Guangdong Laboratory",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60272288",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Linking Distributed Optimization Models for Food, Water, and Energy Security Nexus Management",
        "publication": "Sustainability (Switzerland)",
        "citied_by": "8",
        "cover_date": "2022-02-01",
        "Abstract": "Traditional integrated modeling (IM) is based on developing and aggregating all relevant (sub)models and data into a single integrated linear programming (LP) model. Unfortunately, this approach is not applicable for IM under asymmetric information (ASI), i.e., when “private” information regarding sectoral/regional models is not available, or it cannot be shared by modeling teams (sectoral agencies). The lack of common information about LP submodels makes LP methods inapplicable for integrated LP modeling. The aim of this paper is to develop a new approach to link and optimize distributed sectoral/regional optimization models, providing a means of decentralized cross-sectoral coordination in the situation of ASI. Thus, the linkage methodology enables the investigation of policies in interdependent systems in a “decentralized” fashion. For linkage, the sectoral/regional models do not need recoding or reprogramming. They also do not require additional data harmonization tasks. Instead, they solve their LP submodels independently and in parallel by a specific iterative subgradient algorithm for nonsmooth optimization. The submodels continue to be the same separate LP models. A social planner (regulatory agency) only needs to adjust the joint resource constraints to simple subgradient changes calculated by the algorithm. The approach enables more stable and resilient systems’ performance and resource allocation as compared to the independent policies designed by separate models without accounting for interdependencies. The paper illustrates the application of the methodology to link detailed energy and agricultural production planning models under joint constraints on water and land use.",
        "DOI": "10.3390/su14031255",
        "paper_author": "Ermoliev Y.",
        "affiliation_name": "International Institute for Applied Systems Analysis, Laxenburg",
        "affiliation_city": "Laxenburg",
        "affiliation_country": "Austria",
        "affiliation_id": "60019497",
        "affiliation_state": "Lower Austria"
    },
    {
        "paper_title": "Bit‐Level Automotive Controller Area Network Message Reverse Framework Based on Linear Regression",
        "publication": "Sensors",
        "citied_by": "10",
        "cover_date": "2022-02-01",
        "Abstract": "Modern intelligent and networked vehicles are increasingly equipped with electronic control units (ECUs) with increased computing power. These electronic devices form an in‐vehicle network via the Controller Area Network (CAN) bus, the de facto standard for modern vehicles. Alt-hough many ECUs provide convenience to drivers and passengers, they also increase the potential for cyber security threats in motor vehicles. Numerous attacks on vehicles have been reported, and the commonality among these attacks is that they inject malicious messages into the CAN network. To close the security holes of CAN, original equipment manufacturers (OEMs) keep the Database CAN (DBC) file describing the content of CAN messages, confidential. This policy is ineffective against cyberattacks but limits in‐depth investigation of CAN messages and hinders the development of in‐vehicle intrusion detection systems (IDS) and CAN fuzz testing. Current research re-verses CAN messages through tokenization, machine learning, and diagnostic information matching to obtain details of CAN messages. However, the results of these algorithms yield only a fraction of the information specified in the DBC file regarding CAN messages, such as field boundaries and message IDs associated with specific functions. In this study, we propose multiple linear regression-based frameworks for bit‐level inversion of CAN messages that can approximate the inversion of DBC files. The framework builds a multiple linear regression model for vehicle behavior and CAN traffic, filters the candidate messages based on the decision coefficients, and finally locates the bits describing the vehicle behavior to obtain the data length and alignment based on the model param-eters. Moreover, this work shows that the system has high reversion accuracy and outperforms ex-isting systems in boundary delineation and filtering relevant messages in actual vehicles.",
        "DOI": "10.3390/s22030981",
        "paper_author": "Bi Z.",
        "affiliation_name": "Beijing University of Posts and Telecommunications",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60016930",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Environment-Adaptable Printed-Circuit Board Positioning Using Deep Reinforcement Learning",
        "publication": "IEEE Transactions on Components, Packaging and Manufacturing Technology",
        "citied_by": "10",
        "cover_date": "2022-02-01",
        "Abstract": "Vision-based object positioning is very important in the electronic industry for assembly and inspection tasks. Many methods have been proposed to tackle the problem, either by traditional machine vision or by deep learning (DL) techniques. The traditional methods rely on template matching or feature point correspondence. They are computationally intensive and are easily affected by illumination changes and noise. DL models such as convolutional neural networks (CNNs) are computationally very efficient but are also sensitive against environmental changes. In this article, a deep reinforcement learning (DRL) model based on the Actor-Critic style Proximal Policy Optimization algorithm(s) (AC-PPO) is proposed. The proposed method is applied for the positioning of printed circuit boards (PCBs). The model uses as the current environment the sensed image and the reference template as a guide. It requires only a single manually marked template in the reference image. All possible training images are automatically and randomly generated during the neural network training without human intervention. The proposed reinforcement learning (RL) model is shown to be adaptive to environmental changes, including illumination, noise, de-focusing, and template occlusion, compared with the CNN regressor. Experimental results indicate that the proposed model on average can achieve estimation errors less than 1 pixel in translation and 1° in orientation, with fast evaluation for the real-time PCB positioning task.",
        "DOI": "10.1109/TCPMT.2022.3142033",
        "paper_author": "Solorzano C.",
        "affiliation_name": "Yuan Ze University",
        "affiliation_city": "Taoyuan",
        "affiliation_country": "Taiwan",
        "affiliation_id": "60013395",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "The Impact of Climate Change on a University Campus’ Energy Use: Use of Machine Learning and Building Characteristics",
        "publication": "Buildings",
        "citied_by": "12",
        "cover_date": "2022-02-01",
        "Abstract": "Global warming is expected to increase 1.5◦ C between 2030 and 2052. This may lead to an increase in building energy consumption. With the changing climate, university campuses need to prepare to mitigate risks with building energy forecasting models. Although many scholars have developed buildings energy models (BEMs), only a few have focused on the interpretation of the meaning of BEM, including climate change and its impacts. Additionally, despite several review papers on BEMs, there is no comprehensive guideline indicating which variables are appropriate to use to explain building energy consumption. This study developed building energy prediction models by using statistical analysis: multivariate regression models, multiple linear regression (MLR) models, and relative importance analysis. The outputs are electricity (ELC) and steam (STM) consumption. The independent variables used as inputs are building characteristics, temporal variables, and meteorological variables. Results showed that categorizing the campus buildings by building type is critical, and the equipment power density is the most important factor for ELC consumption, while the heating degree is the most critical factor for STM consumption. The laboratory building type is the most STM-consumed building type, so it needs to be monitored closely. The prediction models give an insight into which building factors remain essential and applicable to campus building policy and campus action plans. Increasing STM is to raise awareness of the severity of climate change through future weather scenarios.",
        "DOI": "10.3390/buildings12020108",
        "paper_author": "Im H.",
        "affiliation_name": "University of Florida",
        "affiliation_city": "Gainesville",
        "affiliation_country": "United States",
        "affiliation_id": "60013959",
        "affiliation_state": "FL"
    },
    {
        "paper_title": "Predicting airline customers’ recommendations using qualitative and quantitative contents of online reviews",
        "publication": "Multimedia Tools and Applications",
        "citied_by": "14",
        "cover_date": "2022-02-01",
        "Abstract": "Customers generally give ratings and reviews for different services that they get online or offline. These reviews and ratings aspects are effectively helpful to both the company and customers to receive feedback and make the right decisions, respectively. However, the number of reviews and ratings can increase exponentially, bringing a new challenge for the company to manage and track. Under these circumstances, it will also be hard for the customer to make the right decision. In this work, we summarize text reviews and ratings given by passengers for different airlines. The objective of this research is to predict whether the recommendation made by the customer is positive or negative. Two types of features, namely, textual feature and explicit ratings, are extracted from the dataset and other attributes. We found the relationship between such sentiments and feelings expressed in online reviews and predictive consumer recommendation decisions. We have considered quantitative content with qualitative content of online reviews in predicting recommendation decisions, which shows the work’s novelty. Additionally, the obtained results yield an essential contribution to the existing literature in terms of service evaluation, making managerial policies, and predictive consumer recommendations, etc. Moreover, we hope that this work would be helpful for practitioners who wish to utilize the technique to make the quick and essential hidden information by combining textual reviews and various service aspects ratings.",
        "DOI": "10.1007/s11042-022-11972-7",
        "paper_author": "Jain P.K.",
        "affiliation_name": "Indian Institute of Technology (Indian School of Mines), Dhanbad",
        "affiliation_city": "Dhanbad",
        "affiliation_country": "India",
        "affiliation_id": "60008898",
        "affiliation_state": "JH"
    },
    {
        "paper_title": "Acceptance of a Pay-How-You-Drive pricing scheme for city traffic: The case of Athens",
        "publication": "Transportation Research Part A: Policy and Practice",
        "citied_by": "12",
        "cover_date": "2022-02-01",
        "Abstract": "In order to investigate the acceptance of a Pay-How-You-Drive (PHYD) road pricing system in Athens, Greece, a novel scheme is proposed which is designed to provide recommendations to the drivers in order to change their behavior towards a more ecological one and charges more those that would not follow them. For modeling the acceptance of the system's recommendations, first, a stated preferences questionnaire survey was designed and conducted on 600 drivers. Next, several Machine Learning models were developed to identify the characteristics that may affect the acceptance of such a scheme. To enrich the modeling process, a clustering framework was exploited to separate the respondents into different profiles, based on their mobility pattern and their attitude towards eco-driving. Furthermore, novel interpretation techniques, namely the calculation of permutation importance and partial dependence, were applied to gain a deeper understanding of the models’ outputs. Findings revealed that the percent of travel time increase when following the system's recommendation and the offered discount are the most important determinants of the system's acceptance. More specifically, the estimation of the value of time showed that those who would drive ecologically would expect a discount of 0.21€ for each minute of additional travel time. The results of this research can be exploited in the design of fair toll systems that will meet the requirements of potential users, as well as the needs of demand-responsive and eco-friendly urban road networks.",
        "DOI": "10.1016/j.tra.2022.01.009",
        "paper_author": "Fafoutellis P.",
        "affiliation_name": "National Technical University of Athens (NTUA)",
        "affiliation_city": "Athens",
        "affiliation_country": "Greece",
        "affiliation_id": "60002947",
        "affiliation_state": "Attica"
    },
    {
        "paper_title": "Collective eXplainable AI: Explaining Cooperative Strategies and Agent Contribution in Multiagent Reinforcement Learning with Shapley Values",
        "publication": "IEEE Computational Intelligence Magazine",
        "citied_by": "50",
        "cover_date": "2022-02-01",
        "Abstract": "While Explainable Artificial Intelligence (XAI) is increasingly expanding more areas of application, little has been applied to make deep Reinforcement Learning (RL) more comprehensible. As RL becomes ubiquitous and used in critical and general public applications, it is essential to develop methods that make it better understood and more interpretable. This study proposes a novel approach to explain cooperative strategies in multiagent RL using Shapley values, a game theory concept used in XAI that successfully explains the rationale behind decisions taken by Machine Learning algorithms. Through testing common assumptions of this technique in two cooperation-centered socially challenging multi-agent environments environments, this article argues that Shapley values are a pertinent way to evaluate the contribution of players in a cooperative multi-agent RL context. To palliate the high overhead of this method, Shapley values are approximated using Monte Carlo sampling. Experimental results on Multiagent Particle and Sequential Social Dilemmas show that Shapley values succeed at estimating the contribution of each agent. These results could have implications that go beyond games in economics, (e.g., for non-discriminatory decision making, ethical and responsible AI-derived decisions or policy making under fairness constraints). They also expose how Shapley values only give general explanations about a model and cannot explain a single run, episode nor justify precise actions taken by agents. Future work should focus on addressing these critical aspects.",
        "DOI": "10.1109/MCI.2021.3129959",
        "paper_author": "Heuillet A.",
        "affiliation_name": "Université Paris-Saclay",
        "affiliation_city": "Gif-sur-Yvette",
        "affiliation_country": "France",
        "affiliation_id": "60106017",
        "affiliation_state": "Ile-de-France"
    },
    {
        "paper_title": "AI-GlobalEvents: A Software for analyzing, identifying and explaining global events with Artificial Intelligence",
        "publication": "Software Impacts",
        "citied_by": "24",
        "cover_date": "2022-02-01",
        "Abstract": "AI-GlobalEvents is a decision support dashboard for policy planners to conduct strategic threat assessments based on global news and events. It uses AI services and algorithms to automatically aggregate global news from online news portals, websites and social media for analyzing the events and identifying critical events requiring imminent attention. The software uses AI algorithms like Entity Detection, Sentiment Analysis, Anomaly detection and Regression to produce explanations in natural language. It is capable of presenting the result in a wide range of platforms including mobile phones (Android or iOS), tablets or desktop environments. AI-GlobalEvents is available at https://github.com/DrSufi/GlobalEvent.",
        "DOI": "10.1016/j.simpa.2022.100218",
        "paper_author": "Sufi F.K.",
        "affiliation_name": "Federal Government",
        "affiliation_city": "Canberra",
        "affiliation_country": "Australia",
        "affiliation_id": "109942976",
        "affiliation_state": "AC"
    },
    {
        "paper_title": "Digitalisation and Artificial Intelligence for sustainable food systems",
        "publication": "Trends in Food Science and Technology",
        "citied_by": "70",
        "cover_date": "2022-02-01",
        "Abstract": "Background: The European Commission (EC) has launched the European Green Deal communication, setting out the path for a fundamental transformation of Europe. Key element in this policy is a fully sustainable food system outlined in the farm-to-fork strategy. Such strategy requires a systems approach in which all aspects related to the production and consumption of sufficient and healthy food are considered, including economic, environmental (climate, ecosystems) and social aspects. Scope and approach: Here, we present the systems approach concept for food production, following the farm-to-fork principle as embraced by the EC, and elaborate on how digitalisation and Artificial Intelligence (AI) can solve the challenges that a sustainable food system imposes. Key findings and conclusions: We present a number of research and innovation challenges and illustrate these by some specific examples. It is concluded that AI and digitalisation show great potential to support the transition towards a sustainable food system. This development will impact the roles and interactions of the actors in the entire value chain from farmers to consumers. Policy recommendations are made for a successful future implementation of AI in sustainable food production.",
        "DOI": "10.1016/j.tifs.2022.01.020",
        "paper_author": "Marvin H.J.P.",
        "affiliation_name": "Wageningen University &amp; Research",
        "affiliation_city": "Wageningen",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60004156",
        "affiliation_state": "Gelderland"
    },
    {
        "paper_title": "Monitoring System for the Management of the Common Agricultural Policy Using Machine Learning and Remote Sensing",
        "publication": "Electronics (Switzerland)",
        "citied_by": "13",
        "cover_date": "2022-02-01",
        "Abstract": "The European Commission promotes new technologies and data generated by the Copernicus Programme. These technologies are intended to improve the management of the Common Agricultural Policy aid, implement new monitoring controls to replace on-the-spot checks, and apply up to 100% of the applications continuously for an agricultural year. This paper presents a generic methodology developed for implementing monitoring controls. To achieve this, the dataset provided by the Sentinel-2 time series is transformed into information through the combination of classifications with machine learning using random forest and remote sensing-based biophysical indices. This work focuses on monitoring the helpline associated with rice cultivation, using 13 Sentinel-2 images whose grouping and characteristics change depending on the event or landmark being sought. Moreover, the functionality to check, before harvesting the crop, that the area declared is equal to the area cultivated is added. The 2020 results are around 96% for most of the metrics analysed, demonstrating the potential of Sentinel-2 for controlling subsidies, particularly for rice. After the quality assessment, the hit rate is 98%. The methodology is transformed into a tool for regular use to improve decision making by determining which declarants comply with the crop-specific aid obligations, contributing to optimising the administrations’ resources and a fairer distribution of funds.",
        "DOI": "10.3390/electronics11030325",
        "paper_author": "López-Andreu F.J.",
        "affiliation_name": "Murcia Institute of Agri-Food Research and Development (IMIDA)",
        "affiliation_city": "Murcia",
        "affiliation_country": "Spain",
        "affiliation_id": "113094186",
        "affiliation_state": "Murcia"
    },
    {
        "paper_title": "Imitation Learning Enabled Task Scheduling for Online Vehicular Edge Computing",
        "publication": "IEEE Transactions on Mobile Computing",
        "citied_by": "168",
        "cover_date": "2022-02-01",
        "Abstract": "Vehicular edge computing (VEC) is a promising paradigm based on the Internet of vehicles to provide computing resources for end users and relieve heavy traffic burden for cellular networks. In this paper, we consider a VEC network with dynamic topologies, unstable connections and unpredictable movements. Vehicles inside can offload computation tasks to available neighboring VEC clusters formed by onboard resources, with the purpose of both minimizing system energy consumption and satisfying task latency constraints. For online task scheduling, existing researches either design heuristic algorithms or leverage machine learning, e.g., deep reinforcement learning (DRL). However, these algorithms are not efficient enough because of their low searching efficiency and slow convergence speeds for large-scale networks. Instead, we propose an imitation learning enabled online task scheduling algorithm with near-optimal performance from the initial stage. Specially, an expert can obtain the optimal scheduling policy by solving the formulated optimization problem with a few samples offline. For online learning, we train agent policies by following the expert's demonstration with an acceptable performance gap in theory. Performance results show that our solution has a significant advantage with more than 50 percent improvement compared with the benchmark.",
        "DOI": "10.1109/TMC.2020.3012509",
        "paper_author": "Wang X.",
        "affiliation_name": "The Hong Kong Polytechnic University",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong",
        "affiliation_id": "60008928",
        "affiliation_state": "Hong Kong"
    },
    {
        "paper_title": "Primary risk stratification for neonatal jaundice among term neonates using machine learning algorithm",
        "publication": "Early Human Development",
        "citied_by": "10",
        "cover_date": "2022-02-01",
        "Abstract": "Background: Neonatal jaundice occurs in approximately 60% of term newborns. Although risk factors for neonatal jaundice have been studied, all the suggested strategies are based on various newborn tests for bilirubin levels. We aim to stratify neonates into risk groups for clinically significant neonatal jaundice using a combined data analysis approach, without serum bilirubin evaluation. Study design: Term (gestational week 37–42) neonates born in a single medical center, 2005–2018 were identified. Anonymized data were analyzed using machine learning. Thresholds for stratification into risk groups were established. Associations were evaluated statistically using neonates with and without clinically significant neonatal jaundice from the study population. Results: A total of 147,667 consecutive term live neonates were included. The machine learning diagnostic ability to evaluate the risk for neonatal jaundice was 0.748; 95% CI 0.743–0.754 (AUC). The most important factors were (in order of importance) maternal blood type, maternal age, gestational age at delivery, estimated birth weight, parity, CBC at admission, and maternal blood pressure at admission. Neonates were then stratified by risk: 61% (n = 90,140) were classed as low-risk, 39% (n = 57,527) as higher-risk. Prevalence of jaundice was 4.14% in the full cohort, and 1.47% and 8.29% in the low- and high-risk cohorts, respectively; OR 6.06 (CI: 5.7–6.45) for neonatal jaundice in high-risk group. Conclusion: A population tailored “first step” screening policy using machine learning model presents potential of neonatal jaundice risk stratification for term neonates. Future development and validation of this computational model are warranted.",
        "DOI": "10.1016/j.earlhumdev.2022.105538",
        "paper_author": "Guedalia J.",
        "affiliation_name": "Bar-Ilan University",
        "affiliation_city": "Ramat Gan",
        "affiliation_country": "Israel",
        "affiliation_id": "60002765",
        "affiliation_state": "Tel Aviv District"
    },
    {
        "paper_title": "Government Construction Project Budget Prediction Using Machine Learning",
        "publication": "Journal of Advances in Information Technology",
        "citied_by": "25",
        "cover_date": "2022-02-01",
        "Abstract": "The construction industry could not avoid the technology disruptive era. Therefore, the Thai government has created a new policy and directed all departments to implement big data technology. Big data technology includes Machine Learning (ML). The present study attempts to predict over-budget construction projects using an ML algorithm. Data were collected from the comptroller general’s department of Thailand for over-budget project cases. Information about 692 projects completed in Thailand in 2019, covering all types of construction projects, was collected and analyzed. ML, an analytical technique for big data technology, was used as a tool in this study. In addition, k-Nearest Neighbors (KNN), an ML algorithm, was used to classify over-budget projects. The input data have four attributes: department of project, construction site location, type of project, and methods of procurement; the output is a yes/no decision on whether a project has been over budget. The dataset was preprocessed for analysis and modeled using the KNN function in Python 3. According to the test results, the KNN model achieves an accuracy (precision) of 0.86. Finally, the developed model has demonstrated that it can be used to predict the over-budget construction projects for the Thai government.",
        "DOI": "10.12720/jait.13.1.29-35",
        "paper_author": "Kusonkhum W.",
        "affiliation_name": "Khon Kaen University",
        "affiliation_city": "Khon Kaen",
        "affiliation_country": "Thailand",
        "affiliation_id": "60017165",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "AUV position tracking and trajectory control based on fast-deployed deep reinforcement learning method",
        "publication": "Ocean Engineering",
        "citied_by": "90",
        "cover_date": "2022-02-01",
        "Abstract": "Aiming at the difficult problem of motion control of under-actuated and X-rudder autonomous underwater vehicle (AUV), the present work adopts deep reinforcement learning (DRL) method for its posture control. First, an AUV agent is trained with deep deterministic policy gradient (DDPG) algorithm in a simulation environment, and three-degree-of-freedom posture control of the AUV at a constant speed, fixed roll, variable pitch, and variable yaw, is successfully achieved. Subsequently, the AUV's yaw angle range is extended, and the control failure problem when AUV's yaw angle approaches a critical value is solved, realizing the rapid deployment of the DRL algorithm for AUV control. On this basis, the position-tracking task of AUV for targets in different orientations in three-dimensional space is completed, achieving a six-degree-of-freedom control of AUV. Additionally, by decomposing the trajectory control task of AUV in three-dimensional space into multiple position-tracking missions, the trajectory control of AUV in the underwater horizontal plane and underwater three-dimensional space is realized, demonstrating the significant task generalization ability of the control methods proposed.",
        "DOI": "10.1016/j.oceaneng.2021.110452",
        "paper_author": "Fang Y.",
        "affiliation_name": "Naval University of Engineering",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60069736",
        "affiliation_state": "Hebei"
    },
    {
        "paper_title": "A novel data-driven controller for plug-in hybrid electric vehicles with improved adaptabilities to driving environment",
        "publication": "Journal of Cleaner Production",
        "citied_by": "13",
        "cover_date": "2022-02-01",
        "Abstract": "Instantaneous application optimality is one of the indispensable indicators to assess energy management performance of plug-in hybrid electric vehicles (PHEVs). The momentary optimality, nevertheless, cannot be flexibly reachable under various driving environments due to the partial unobservabilities in control algorithms. To cope with it, a novel data-driven controller for PHEVs is proposed in this paper to achieve the instantaneous optimality of energy management. The well-designed machine learning based controller translates the knowledge of global optimization to real-time controlling scheme with the consideration of adaptabilities to disperse driving conditions. To start with, the universal global optimal control policies for varying driving environment are generated offline based on the chaotic quantum particle swarm optimization with sequential quadratic programming (CQPSO-SQP). Then, the offline optimized global control policies are assembled to construct the dataset for training the least square support vector machine (LSSVM) based controller, which features the superior capability in instantly optimal policy making under different driving conditions. At last, the detailed assessment is performed in simulation test and hardware-in-loop (HIL) test to validate the promising role of CQPSO-PSO and LSSVM in designing the novel energy management controller, and the corresponding results highlight the preferable controlling performance of the proposed novel controller in practical applications.",
        "DOI": "10.1016/j.jclepro.2021.130250",
        "paper_author": "Liu Y.",
        "affiliation_name": "China Automotive Technology and Research Center",
        "affiliation_city": "Tianjin",
        "affiliation_country": "China",
        "affiliation_id": "113459969",
        "affiliation_state": "Tianjin"
    },
    {
        "paper_title": "Prediction of coronary heart disease based on combined reinforcement multitask progressive time-series networks",
        "publication": "Methods",
        "citied_by": "22",
        "cover_date": "2022-02-01",
        "Abstract": "Coronary heart disease is the first killer of human health. At present, the most widely used approach of coronary heart disease diagnosis is coronary angiography, a surgery that could potentially cause some physical damage to the patients, together with some complications and adverse reactions. Furthermore, coronary angiography is expensive thus cannot be widely used in under development country. On the other hand, the heart color Doppler echocardiography report, blood biochemical indicators and personal information, such as gender, age and diabetes, can reflect the degree of heart damage in patients to some extent. This paper proposes a combined reinforcement multitask progressive time-series networks (CRMPTN) model to predict the grade of coronary heart disease through heart color Doppler echocardiography report, blood biochemical indicators and ten basic body information items about the patients. In this model, the first step is to perform deep reinforcement learning (DRL) pre-training through asynchronous advantage actor-critic (A3C). Training data is adopted to optimize the recurrent neural network (RNN) that parameterizes the stochastic policy. In the second step, soft parameter sharing module, hard parameter sharing module and progressive time-series networks are used to predict the status of coronary heart disease. The experimental results show that after DRL pre-training, the multiple tasks in the model interact with each other and learn together to achieve satisfactory results and outperform other state-of-the-art methods.",
        "DOI": "10.1016/j.ymeth.2021.12.009",
        "paper_author": "Li W.",
        "affiliation_name": "Donghua University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60010953",
        "affiliation_state": "Shanghai"
    },
    {
        "paper_title": "The informal economy at times of COVID-19 pandemic",
        "publication": "China Economic Review",
        "citied_by": "38",
        "cover_date": "2022-02-01",
        "Abstract": "We provide a first view of vulnerable informal economy after the blows from COVID-19, using transaction-level business data of around 80 million offline micro businesses (OMBs) owners from the largest Fintech company in China and employing machine learning method for causal inference. We find that the OMBs activities in China experienced an immediate and dramatic drop of 50% during the trough. The businesses had rebounded to around 80% of where they should be seven weeks after the COVID-19 outbreak, but had remained at this level until the end of our time window. We find a larger disruption to the OMBs in urban areas, the female merchants and the merchants who were not grown up in the places where they conducted businesses. We discuss the implications for policy support to the most vulnerable, and highlight the importance to take full advantage of digital development to follow up the informal economy.",
        "DOI": "10.1016/j.chieco.2021.101722",
        "paper_author": "Guo F.",
        "affiliation_name": "Shanghai University of Finance and Economics",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60032744",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Developing future human-centered smart cities: Critical analysis of smart city security, Data management, and Ethical challenges",
        "publication": "Computer Science Review",
        "citied_by": "131",
        "cover_date": "2022-02-01",
        "Abstract": "As the globally increasing population drives rapid urbanization in various parts of the world, there is a great need to deliberate on the future of the cities worth living. In particular, as modern smart cities embrace more and more data-driven artificial intelligence services, it is worth remembering that (1) technology can facilitate prosperity, wellbeing, urban livability, or social justice, but only when it has the right analog complements (such as well-thought out policies, mature institutions, responsible governance); and (2) the ultimate objective of these smart cities is to facilitate and enhance human welfare and social flourishing. Researchers have shown that various technological business models and features can in fact contribute to social problems such as extremism, polarization, misinformation, and Internet addiction. In the light of these observations, addressing the philosophical and ethical questions involved in ensuring the security, safety, and interpretability of such AI algorithms that will form the technological bedrock of future cities assumes paramount importance. Globally there are calls for technology to be made more humane and human-centered. In this paper, we analyze and explore key challenges including security, robustness, interpretability, and ethical (data and algorithmic) challenges to a successful deployment of AI in human-centric applications, with a particular emphasis on the convergence of these concepts/challenges. We provide a detailed review of existing literature on these key challenges and analyze how one of these challenges may lead to others or help in solving other challenges. The paper also advises on the current limitations, pitfalls, and future directions of research in these domains, and how it can fill the current gaps and lead to better solutions. We believe such rigorous analysis will provide a baseline for future research in the domain.",
        "DOI": "10.1016/j.cosrev.2021.100452",
        "paper_author": "Ahmad K.",
        "affiliation_name": "Hamad Bin Khalifa University, College of Science and Engineering",
        "affiliation_city": "Doha",
        "affiliation_country": "Qatar",
        "affiliation_id": "60113885",
        "affiliation_state": "Ad-Dawhah"
    },
    {
        "paper_title": "Greedy Autoaugment for classification of mycobacterium tuberculosis image via generalized deep CNN using mixed pooling based on minimum square rough entropy",
        "publication": "Computers in Biology and Medicine",
        "citied_by": "32",
        "cover_date": "2022-02-01",
        "Abstract": "Although tuberculosis (TB) is a disease whose cause, epidemiology and treatment are well known, some infected patients in many parts of the world are still not diagnosed by current methods, leading to further transmission in society. Creating an accurate image-based processing system for screening patients can help in the early diagnosis of this disease. We provided a dataset containing1078 confirmed negative and 469 positive Mycobacterium tuberculosis instances. An effective method using an improved and generalized convolutional neural network (CNN) was proposed for classifying TB bacteria in microscopic images. In the preprocessing phase, the insignificant parts of microscopic images are excluded with an efficient algorithm based on the square rough entropy (SRE) thresholding. Top 10 policies of data augmentation were selected with the proposed model based on the Greedy AutoAugment algorithm to resolve the overfitting problem. In order to improve the generalization of CNN, mixed pooling was used instead of baseline one. The results showed that employing generalized pooling, batch normalization, Dropout, and PReLU have improved the classification of Mycobacterium tuberculosis images. The output of classifiers such as Naïve Bayes-LBP, KNN-LBP, GBT-LBP, Naïve Bayes-HOG, KNN-HOG, SVM-HOG, GBT-HOG indicated that proposed CNN has the best results with an accuracy of 93.4%. The improvements of CNN based on the proposed model can yield promising results for diagnosing TB.",
        "DOI": "10.1016/j.compbiomed.2021.105175",
        "paper_author": "Momeny M.",
        "affiliation_name": "Yazd University",
        "affiliation_city": "Yazd",
        "affiliation_country": "Iran",
        "affiliation_id": "60000159",
        "affiliation_state": "Yazd"
    },
    {
        "paper_title": "Publisher Correction: Machine learning in Earth and environmental science requires education and research policy reforms (Nature Geoscience, (2021), 14, 12, (878-880), 10.1038/s41561-021-00865-3)",
        "publication": "Nature Geoscience",
        "citied_by": "0",
        "cover_date": "2022-02-01",
        "Abstract": "In the version of this article originally published, there were errors in refs. 5–7. Originally listed as Hutchinson (2018), Karpatne (2017), McGovern (2019), the order of refs. 5–7 has been corrected to read McGovern (2019), Hutchinson (2018)), Karpatne (2017), respectively. The change has been made to the online version of the article.",
        "DOI": "10.1038/s41561-021-00881-3",
        "paper_author": "Fleming S.W.",
        "affiliation_name": "USDA Natural Resources Conservation Service",
        "affiliation_city": "Washington, D.C.",
        "affiliation_country": "United States",
        "affiliation_id": "60204083",
        "affiliation_state": "DC"
    },
    {
        "paper_title": "Building a Targeted Automatic e-Consult (TACo) Program",
        "publication": "Joint Commission Journal on Quality and Patient Safety",
        "citied_by": "11",
        "cover_date": "2022-02-01",
        "Abstract": "Driving Forces: Traditional specialty consults are resource intensive and may be delayed or omitted if the treating physician does not recognize the need for specialty advice. Targeted automatic e-consults (TACos) address these limitations by prospectively identifying patients using the electronic health record (EHR) and presenting pertinent information on a dashboard, enabling consultants to provide a virtual consult with written recommendations. The TACo model may improve value by facilitating more expert input without a proportional increase in cost. Building a TACo: Through our experience developing a TACo program, we have identified four key steps. First, identify appropriate conditions that have support from the health system and from frontline clinicians. Second, design the digital infrastructure, including lists and dashboards. Third, create a funding plan to support the consultant's time, either through internal grants, external grants, e-consult billing codes, or some combination of the three. Fourth, pilot on a select number of services, iterate, and scale. Challenges: Funding for TACos has been a major barrier to adoption. Fortunately, new e-consult billing codes may make it possible to recoup as least part of the program's cost. Technological hurdles also exist, particularly in building real-time lists within the EHR to prospectively identify patients based on complex criteria. Next Steps: We look for this model to gain popularity as evidence of clinical and operational benefit mounts. We anticipate reimbursement policies may be updated to support this type of consult. Finally, we expect machine learning to play a role in identifying patients and providing recommendations in the future.",
        "DOI": "10.1016/j.jcjq.2021.10.007",
        "paper_author": "Judson T.J.",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "What will we ask to artificial intelligence for cardiovascular medicine in the next decade?",
        "publication": "Minerva Cardiology and Angiology",
        "citied_by": "6",
        "cover_date": "2022-02-01",
        "Abstract": "Artificial intelligence (AI) comprises a wide range of technologies and methods with heterogeneous degrees of complexity, applications, and abilities. In the cardiovascular field, AI holds the potential to fulfil many unsolved challenges, eventually translating into improved patient care. In particular, AI appears as the most promising tool to overcome the gap between ever-increasing data-rich technologies and their practical implementation in cardiovascular research, in the cardiologist routine, in the patient daily life and at the healthcare-policy level. A multiplicity of AI technologies is progressively pervading several aspects of precision cardiovascular medicine including early diagnosis, automated imaging processing and interpretation, disease sub-phenotyping, risk prediction and remote monitoring systems. Several methodological, logistical, educational, and ethical challenges are emerging by integrating AI systems at any stage of cardiovascular medicine. This review will discuss the basics of AI methods, the growing body of evidence supporting the role of AI in the cardiovascular field and the challenges to overcome for an effective AI-integrated cardiovascular medicine.",
        "DOI": "10.23736/S2724-5683.21.05753-7",
        "paper_author": "GALLONE G.",
        "affiliation_name": "Azienda Sanitaria Ospedaliera Molinette San Giovanni Battista Di Torino",
        "affiliation_city": "Turin",
        "affiliation_country": "Italy",
        "affiliation_id": "60005091",
        "affiliation_state": "TO"
    },
    {
        "paper_title": "Machine learning–based diagnostic evaluation of shear-wave elastography in BI-RADS category 4 breast cancer screening: a multicenter, retrospective study",
        "publication": "Quantitative Imaging in Medicine and Surgery",
        "citied_by": "15",
        "cover_date": "2022-02-01",
        "Abstract": "Background: Ultrasound is commonly used in breast cancer screening but lacks quantification ability and diagnostic power due to its low specificity, which can lead to overdiagnosis and unnecessary biopsies. This study evaluated the diagnostic efficacy and clinical utility of adding shear-wave elastography (SWE) to the screening of the Breast Imaging Reporting and Data System (BI-RADS) category 4 breast cancer. Methods: A machine learning–based diagnostic model was constructed using data retrospectively collected from 3 independent cohorts with features selected using lasso regression and support vector machine-recursive feature elimination algorithms. Propensity score matching (PSM) was used to preclude confounding baseline characteristics between malignant and benign lesions. A decision curve analysis (DCA) was used to evaluate the clinical benefit of the diagnostic model in identifying high-risk tumor patients for intervention while simultaneously avoiding overtreatment of low-risk patients with integrative evaluation using a net benefit value and treatment reduction rate. Results: In our training center, a total of 122 patients were enrolled, and 577 breast tumors were collected. The comparison between malignant and benign lesions revealed significant differences in patient age, tumor size, resistance index (RI), and elasticity values. The maximum elasticity value (Emax) was identified as an independent diagnostic feature and was included in the diagnostic model. The combination of Emax with BIRADS category 4 demonstrated a significantly better diagnostic efficacy than the BI-RADS category alone [BIRADS+Emax: AUC =0.908, 95% confidence interval (CI): 0.842−0.974; BI-RADS: AUC =0.862, 95% CI: 0.784−0.94; P=0.024] and significantly increased the clinical benefit for patients and policy makers by effectively reducing overdiagnosis and biopsy rates. In the BI-RADS category 4A subgroup, adding Emax to breast cancer screening benefited patients and showed a greater absolute benefit than did the BI-RADS category alone when used for patients with a higher probability of cancer (>0.403), demonstrating a 50% overtreatment reduction. Conclusions: Adding Emax to BI-RADS category 4 breast cancer screening using SWE significantly reduced overdiagnosis and biopsy rates compared with the BI-RADS category alone, especially for BI-RADS 4A patients.",
        "DOI": "10.21037/qims-21-341",
        "paper_author": "Tang Y.",
        "affiliation_name": "Guangdong Provincial Hospital of Traditional Chinese Medicine",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60073528",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Brain signals of a Surprise-Actor-Critic model: Evidence for multiple learning modules in human decision making",
        "publication": "NeuroImage",
        "citied_by": "5",
        "cover_date": "2022-02-01",
        "Abstract": "Learning how to reach a reward over long series of actions is a remarkable capability of humans, and potentially guided by multiple parallel learning modules. Current brain imaging of learning modules is limited by (i) simple experimental paradigms, (ii) entanglement of brain signals of different learning modules, and (iii) a limited number of computational models considered as candidates for explaining behavior. Here, we address these three limitations and (i) introduce a complex sequential decision making task with surprising events that allows us to (ii) dissociate correlates of reward prediction errors from those of surprise in functional magnetic resonance imaging (fMRI); and (iii) we test behavior against a large repertoire of model-free, model-based, and hybrid reinforcement learning algorithms, including a novel surprise-modulated actor-critic algorithm. Surprise, derived from an approximate Bayesian approach for learning the world-model, is extracted in our algorithm from a state prediction error. Surprise is then used to modulate the learning rate of a model-free actor, which itself learns via the reward prediction error from model-free value estimation by the critic. We find that action choices are well explained by pure model-free policy gradient, but reaction times and neural data are not. We identify signatures of both model-free and surprise-based learning signals in blood oxygen level dependent (BOLD) responses, supporting the existence of multiple parallel learning modules in the brain. Our results extend previous fMRI findings to a multi-step setting and emphasize the role of policy gradient and surprise signalling in human learning.",
        "DOI": "10.1016/j.neuroimage.2021.118780",
        "paper_author": "Liakoni V.",
        "affiliation_name": "École Polytechnique Fédérale de Lausanne",
        "affiliation_city": "Lausanne",
        "affiliation_country": "Switzerland",
        "affiliation_id": "60028186",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Economic Modelling at thirty-five: A retrospective bibliometric survey",
        "publication": "Economic Modelling",
        "citied_by": "32",
        "cover_date": "2022-02-01",
        "Abstract": "Economic modelling (EM) is a premier journal for policy-relevant economic models. However, so far, no retrospective studies exist for the journal. This study addresses this gap using a machine learning n-gram (bigram and trigram) analysis. The survey results find that the journal has contributed to 9517 topics, with 69 topics covered in at least 10 studies between 1984 and 2019. Through a co-occurrence analysis of bigram and trigram terms, this study reveals that the major topics in the journal converge to nine themes: international economics, development economics, regional and real estate economics, economic growth and development, financial economics, monetary economics, general economic equilibrium, international finance, and non-conventional finance and macroeconomics. This study concludes with key takeaways and suggestions for prospective authors intending to publish their best papers in EM.",
        "DOI": "10.1016/j.econmod.2021.105712",
        "paper_author": "Pattnaik D.",
        "affiliation_name": "Woxsen University",
        "affiliation_city": "Hyderabad",
        "affiliation_country": "India",
        "affiliation_id": "60274762",
        "affiliation_state": "TS"
    },
    {
        "paper_title": "The role of human and social capital in earthquake recovery in Nepal",
        "publication": "Nature Sustainability",
        "citied_by": "13",
        "cover_date": "2022-02-01",
        "Abstract": "Human and social capital help households cope with disastrous shocks. We analyse panel survey data from before and after the 2015 Nepal earthquakes to disentangle the association between post-earthquake income recovery of households and their social and human capital before the earthquake. Our analysis uses multidimensional measures of human and social capital and a machine-learning algorithm, the Bayesian additive regression tree. This approach helps us address measurement and estimation challenges that commonly affect social science analyses of observational data with many covariates and confounding variables. Our analysis shows the relative association of human capital with income recovery is greater on average than that of social capital, human and social capital serve as partial substitutes for each other when it comes to household income recovery, and the association of different capitals with economic recovery is nonlinear and heterogeneous across household education levels. Our results suggest that disaster-support policies can be structured with respect to human and social capital endowments to support more effective recovery of disaster-affected households.",
        "DOI": "10.1038/s41893-021-00805-4",
        "paper_author": "Liu W.",
        "affiliation_name": "University of Michigan, Ann Arbor",
        "affiliation_city": "Ann Arbor",
        "affiliation_country": "United States",
        "affiliation_id": "60025778",
        "affiliation_state": "MI"
    },
    {
        "paper_title": "Machine learning for regional crop yield forecasting in Europe",
        "publication": "Field Crops Research",
        "citied_by": "66",
        "cover_date": "2022-02-01",
        "Abstract": "Crop yield forecasting at national level relies on predictors aggregated from smaller spatial units to larger ones according to harvested crop areas. Such crop areas come from land cover maps or reported statistics, both of which can have errors and uncertainties. Sub-national or regional crop yield forecasting minimizes the propagation of these errors to some extent. In addition, regional forecasts provide added value and insights to stakeholders on regional differences within a country, which would otherwise compensate each other at national level. We propose a crop yield forecasting approach for multiple spatial levels based on regional crop yield forecasts from machine learning. Machine learning, with its data-driven approach, can leverage larger data sizes and capture nonlinear relationships between predictors and yield at regional level. We designed a generic machine learning workflow to demonstrate the benefits of regional crop yield forecasting in Europe. To evaluate the quality and usefulness of regional forecasts, we predicted crop yields for 35 case studies, including nine countries that are major producers of six crops (soft wheat, spring barley, sunflower, grain maize, sugar beets and potatoes). Machine learning models at regional level had lower normalized root mean squared errors (NRMSE) and uncertainty than a linear trend model, with Wilcoxon p-values of 3e-7 and 2e-7 for 60 days before harvest and end of season respectively. Similarly, regional machine learning forecasts aggregated to national level had lower NRMSEs than forecasts from an operational system in 18 out of 35 cases 60 days before harvest, with a Wilcoxon p-value of 0.95 indicating similar performance. Our models have room for improvement, especially during extreme years. Nevertheless, regional crop yield forecasts from machine learning and aggregated national forecasts provide a consistent forecasting method across spatial levels and insights from regional differences to support important policy decisions.",
        "DOI": "10.1016/j.fcr.2021.108377",
        "paper_author": "Paudel D.",
        "affiliation_name": "Wageningen University &amp; Research",
        "affiliation_city": "Wageningen",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60004156",
        "affiliation_state": "Gelderland"
    },
    {
        "paper_title": "Reconceptualizing urban heat island: Beyond the urban-rural dichotomy",
        "publication": "Sustainable Cities and Society",
        "citied_by": "56",
        "cover_date": "2022-02-01",
        "Abstract": "Past decades have seen drastically increasing research effort and progress on the study of the phenomenon of urban heat island (UHI). Despite its simplicity, this convenient concept has promoted significant advances in scientific research and policy making processes in the urban environmental community. Nevertheless, the oversimplification and inadequacy of the urban-rural dichotomy, inherited in the UHI concept, is increasingly manifest today in the continuously urbanized world. In this study, we conduct a holistic and in-depth survey of the inadequacy of the urban-rural dichotomy intrinsic to the definition of UHI, from theoretical, technical, and practical perspectives. In addition, in the light of recent research advances, we urge to radically reconceptualize UHI by proposing a novel paradigm by treating the total urban environment as a complex dynamic system. The new framework broadens the frontier of conventional urban environmental study by utilizing advanced techniques of complex systems and data sciences, including complex network theory, machine learning techniques, causal inference, etc. The reconceptualization of UHI is also expected to foster decision making and urban planning, and to avoid the one-sidedness of the singular and often too exclusive aim of heat mitigation.",
        "DOI": "10.1016/j.scs.2021.103581",
        "paper_author": "Wang Z.H.",
        "affiliation_name": "Ira A. Fulton Schools of Engineering",
        "affiliation_city": "Tempe",
        "affiliation_country": "United States",
        "affiliation_id": "60139200",
        "affiliation_state": "AZ"
    },
    {
        "paper_title": "Improvements of response surface modeling with self-adaptive machine learning method for PM<inf>2.5</inf> and O<inf>3</inf> predictions",
        "publication": "Journal of Environmental Management",
        "citied_by": "16",
        "cover_date": "2022-02-01",
        "Abstract": "Quickly quantifying the PM2.5 or O3 response to their precursor emission changes is a key point for developing effective control policies. The polynomial function-based response surface model (pf-RSM) can rapidly predict the nonlinear response of PM2.5 and O3 to precursors, but has drawbacks of overload computation and marginal effects (relatively larger prediction errors under strict control scenarios). To improve the performance of pf-RSM, a novel self-adaptive RSM (SA-RSM) was proposed by integrating the machine learning-based stepwise regression for establishing robust models to increase the computational efficiency and the collinearity diagnosis for reducing marginal effects caused by overfitting. The pilot study case demonstrated that compared with pf-RSM, SA-RSM can effectively reduce the training number by 70% and 40% and the fitting time by 40% and 52%, and decrease the prediction error by 49% and 74% for PM2.5 and O3 predictions respectively; moreover, the isopleths of PM2.5 or O3 as a function of their precursors generated by SA-RSM were more similar to those derived by chemical transport model (CTM), after successfully addressing the marginal effect issue. With the improved computation efficiency and prediction performance, SA-RSM is expected as a better scientific tool for decision-makers to make sound PM2.5 and O3 control policies.",
        "DOI": "10.1016/j.jenvman.2021.114210",
        "paper_author": "Li J.",
        "affiliation_name": "Xiangtan University",
        "affiliation_city": "Xiangtan",
        "affiliation_country": "China",
        "affiliation_id": "60028739",
        "affiliation_state": "Hunan"
    },
    {
        "paper_title": "A hybridization of distributed policy and heuristic augmentation for improving federated learning approach",
        "publication": "Neural Networks",
        "citied_by": "23",
        "cover_date": "2022-02-01",
        "Abstract": "Modifying the existing models of classifiers’ operation is primarily aimed at increasing the effectiveness as well as minimizing the training time. An additional advantage is the ability to quickly implement a given solution to the real needs of the market. In this paper, we propose a method that can implement various classifiers using the federated learning concept and taking into account parallelism. Also, an important element is the analysis and selection of the best classifier depending on its reliability found for separated datasets extended by new, augmented samples. The proposed augmentation technique involves image processing techniques, neural architectures, and heuristic methods and improves the operation in federated learning by increasing the role of the server. The proposition has been presented and tested for the fruit image classification problem. The conducted experiments have shown that the described technique can be very useful as an implementation method even in the case of a small database. Obtained results are discussed concerning the advantages and disadvantages in the context of practical application like higher accuracy.",
        "DOI": "10.1016/j.neunet.2021.11.018",
        "paper_author": "Połap D.",
        "affiliation_name": "Silesian University of Technology",
        "affiliation_city": "Gliwice",
        "affiliation_country": "Poland",
        "affiliation_id": "60009081",
        "affiliation_state": "Silesian"
    },
    {
        "paper_title": "Estimating heterogeneous treatment effects in road safety analysis using generalized random forests",
        "publication": "Accident Analysis and Prevention",
        "citied_by": "23",
        "cover_date": "2022-02-01",
        "Abstract": "Numerous evaluation studies have been conducted on a variety of road safety measures. However, the issue of treatment heterogeneity, defined as the variation in treatment effects, has rarely been investigated before. This paper contributes to the literature by introducing generalized random forests (GRF) for estimation of heterogeneous treatment effects (HTEs) in road safety analysis. GRF has high functional flexibility and is able to search for complex treatment heterogeneity. We first perform a series of simulation experiments to compare GRF with three causal methods that have been used in road safety studies, i.e., outcome regression method, propensity score method, and doubly robust estimation method. The simulation results suggest that GRF is superior to these three methods in terms of model specification, especially with the existence of nonlinearity and nonadditivity. On the other hand, a large dataset is required for accurate GRF estimation. Then we conduct a case study on the UK's speed camera program. Our results indicate significant reductions in the number of road accidents at speed camera sites. And the heterogeneity in treatment effects is found to be statistically significant. We further consider the associations between the baseline accident records, traffic volume, local socio-economic characteristics, and the safety effects of speed cameras. In general, the effect of speed cameras is larger at the sites with more baseline accident records, higher traffic volume, and in more densely-populated and deprived areas. Several policy suggestions are provided based on these findings. The evaluation of HTEs likely offers more comprehensive information to local authorities and policy makers, and improves the performance of speed camera programs. Moreover, GRF can be a promising approach for revealing treatment effect heterogeneity in road safety analysis.",
        "DOI": "10.1016/j.aap.2021.106507",
        "paper_author": "Zhang Y.",
        "affiliation_name": "Southeast University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60005244",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "Learning a Generic Olfactory Search Strategy from Silk Moths by Deep Inverse Reinforcement Learning",
        "publication": "IEEE Transactions on Medical Robotics and Bionics",
        "citied_by": "3",
        "cover_date": "2022-02-01",
        "Abstract": "Despite their simple nervous systems, insects efficiently search for and find sources of odorants. Hence, it is necessary to model and implement such behavior in artificial agents (robots), to enable them to detect dangerous substances such as drugs, gas leaks, and explosives. Previous studies have approached behavioral modeling with either statistical or machine-learning methods. In this study, we determined the behavior trajectories of male silk moths using a virtual reality (VR) system. We then modeled these trajectories as a Markov decision process (MDP) and employed inverse reinforcement learning (IRL) to learn their reward function. Furthermore, we estimated the optimal policy from the learned reward function. We then conducted olfactory search simulations and determined that the IRL-based policy could locate odor sources with a high success rate. This was also investigated under environmental conditions different from those faced by real moths on the VR system. The obtained results indicate that IRL can generically represent olfactory search strategies that are adaptable to various environments.",
        "DOI": "10.1109/TMRB.2021.3129113",
        "paper_author": "Hernandez-Reyes C.",
        "affiliation_name": "Institute of Science Tokyo",
        "affiliation_city": "Tokyo",
        "affiliation_country": "Japan",
        "affiliation_id": "60283164",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Machine Learning-based Approach for Groundwater Mapping",
        "publication": "Natural Resources Research",
        "citied_by": "1",
        "cover_date": "2022-02-01",
        "Abstract": "In Bangladesh, groundwater is the main source of both drinking water and irrigation. Suction lift pumps and force mode of operation are the predominant technologies for groundwater abstraction in Bangladesh. For a sustainable usage policy, it is thus important to identify which technology would be more appropriate for which area in Bangladesh. With that aim in mind, this paper proposes a methodology that leverages the power of machine learning that can potentially learn intricate relationships between the (annual maximum) groundwater level (GWL) and the relevant hydrogeological factors (HGFs). A number of machine learning algorithms—both classification and regression models—was trained. Our classification models were trained as a binary classifier to predict the abstraction technology of a particular point. Notably, our best classification model was based on the Random Forest algorithm, which achieved an accuracy of 91% and an excellent value of 96% for the area under receiver operating characteristics curve, indicating its strong discriminant capability. We also identified (elevation derived from) digital elevation model, specific yield and lithology as the three most important HGFs for GWL in Bangladesh. On the other hand, to predict the actual (annual maximum) GWL, we employed a two-stage approach, where we first employed the above-mentioned classification model to identify the suitable abstraction technology for the point of interest and subsequently predict the actual GWL using the appropriate Random Forest regressor. This also had a reasonable accuracy (minimum absolute error was less than 1 for suction mode and less than 5 for the force mode). Finally, using our prediction models, we prepared groundwater (technology) maps for the whole Bangladesh.",
        "DOI": "10.1007/s11053-021-09977-4",
        "paper_author": "Zzaman R.U.",
        "affiliation_name": "Bangladesh University of Engineering and Technology",
        "affiliation_city": "Dhaka",
        "affiliation_country": "Bangladesh",
        "affiliation_id": "60021158",
        "affiliation_state": "Dhaka"
    },
    {
        "paper_title": "A review of motion planning algorithms for intelligent robots",
        "publication": "Journal of Intelligent Manufacturing",
        "citied_by": "131",
        "cover_date": "2022-02-01",
        "Abstract": "Principles of typical motion planning algorithms are investigated and analyzed in this paper. These algorithms include traditional planning algorithms, classical machine learning algorithms, optimal value reinforcement learning, and policy gradient reinforcement learning. Traditional planning algorithms investigated include graph search algorithms, sampling-based algorithms, interpolating curve algorithms, and reaction-based algorithms. Classical machine learning algorithms include multiclass support vector machine, long short-term memory, Monte-Carlo tree search and convolutional neural network. Optimal value reinforcement learning algorithms include Q learning, deep Q-learning network, double deep Q-learning network, dueling deep Q-learning network. Policy gradient algorithms include policy gradient method, actor-critic algorithm, asynchronous advantage actor-critic, advantage actor-critic, deterministic policy gradient, deep deterministic policy gradient, trust region policy optimization and proximal policy optimization. New general criteria are also introduced to evaluate the performance and application of motion planning algorithms by analytical comparisons. The convergence speed and stability of optimal value and policy gradient algorithms are specially analyzed. Future directions are presented analytically according to principles and analytical comparisons of motion planning algorithms. This paper provides researchers with a clear and comprehensive understanding about advantages, disadvantages, relationships, and future of motion planning algorithms in robots, and paves ways for better motion planning algorithms in academia, engineering, and manufacturing.",
        "DOI": "10.1007/s10845-021-01867-z",
        "paper_author": "Zhou C.",
        "affiliation_name": "Itä-Suomen yliopisto",
        "affiliation_city": "Kuopio",
        "affiliation_country": "Finland",
        "affiliation_id": "60103673",
        "affiliation_state": "IS"
    },
    {
        "paper_title": "Cost-Efficient and Skew-Aware Data Scheduling for Incremental Learning in 5G Networks",
        "publication": "IEEE Journal on Selected Areas in Communications",
        "citied_by": "4",
        "cover_date": "2022-02-01",
        "Abstract": "To facilitate the emerging applications in 5G networks, mobile network operators will provide many network functions in terms of control and prediction. Recently, they have recognized the power of machine learning (ML) and started to explore its potential to facilitate those network functions. Nevertheless, the current ML models for network functions are often derived in an offline manner, which is inefficient due to the excessive overhead for transmitting a huge volume of dataset to remote ML training clouds and failing to provide the incremental learning capability for the continuous model updating. As an alternative solution, we propose Cocktail, an incremental learning framework within a reference 5G network architecture. To achieve cost efficiency while increasing trained model accuracy, an efficient online data scheduling policy is essential. To this end, we formulate an online data scheduling problem to optimize the framework cost while alleviating the data skew issue caused by the capacity heterogeneity of training workers from the long-term perspective. We exploit the stochastic gradient descent to devise an online asymptotically optimal algorithm, including two optimal policies based on novel graph constructions for skew-aware data collection and data training. Small-scale testbed and large-scale simulations validate the superior performance of our proposed framework.",
        "DOI": "10.1109/JSAC.2021.3118430",
        "paper_author": "Pu L.",
        "affiliation_name": "Nankai University",
        "affiliation_city": "Tianjin",
        "affiliation_country": "China",
        "affiliation_id": "60018038",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Forecasting the macrolevel determinants of entrepreneurial opportunities using artificial intelligence models",
        "publication": "Technological Forecasting and Social Change",
        "citied_by": "36",
        "cover_date": "2022-02-01",
        "Abstract": "To date, entrepreneurship researchers have tended to avoid state-of-the-art artificial intelligence techniques; in this paper, we fill that gap. Based on eclectic entrepreneurship theory, we present an original work that uses artificial intelligence to forecast the macrolevel determinants of entrepreneurial opportunity. Modern artificial intelligence could open new areas for future research opportunities in entrepreneurship and help close the gap between theory and practice. Our empirical analysis offers two major results by using a panel dataset of 149 countries covering 2007–2018 and six machine-learning models. First, entrepreneurs prefer to exploit opportunities in countries with stable economic governance that provide high education standards, health, social capital, and a safe, natural environment. Second, CatBoost regression performs better in predicting entrepreneurial opportunity compared to linear regression and more advanced machine-learning models. Recommendations for policy-makers and managers and directions for future studies are also discussed.",
        "DOI": "10.1016/j.techfore.2021.121353",
        "paper_author": "Jabeur S.B.",
        "affiliation_name": "ESDES Business School",
        "affiliation_city": "Lyon",
        "affiliation_country": "France",
        "affiliation_id": "60288038",
        "affiliation_state": "Auvergne-Rhone-Alpes"
    },
    {
        "paper_title": "Simulation and impact analysis of behavioral and socioeconomic dimensions of energy consumption",
        "publication": "Energy",
        "citied_by": "29",
        "cover_date": "2022-02-01",
        "Abstract": "Human-oriented factors present unavoidable challenges and uncertainties in building energy strategic planning. The uncertainties escalate when the target society is not fully known to the decision-maker and can create performance gaps between the expected and actual outcomes of sustainability targets. This article aims to investigate the role of socioeconomic and behavioral dimensions in residential energy consumption patterns among regions that host high proportions of migrant communities with diverse cultural and ethnic traits. This study evaluates the patterns in human-building interactions and energy behaviors among local and migrant communities based on empirical evidence and survey analysis. The survey data are investigated via machine learning approaches to identify the interdependencies between and feature importance of critical factors that influence human-building interactions and to determine elements that help to discern the energy behavior of locals and migrants. A simulation analysis is conducted to analyze residential energy consumption under different human indoor thermal comfort preferences in multiple case scenarios to demonstrate how improvements in human-building interaction can create saving opportunities. The findings capture the main socioeconomic and behavioral contributors in residential energy consumption and demonstrate the impact of human factor at a high level in regions with imbalanced demographics and societies in transition.",
        "DOI": "10.1016/j.energy.2021.122502",
        "paper_author": "Ghofrani A.",
        "affiliation_name": "Department of International Affairs, College of Arts and Sciences, Qatar University",
        "affiliation_city": "Doha",
        "affiliation_country": "Qatar",
        "affiliation_id": "60197151",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A novel decision support system for managing predictive maintenance strategies based on machine learning approaches",
        "publication": "Safety Science",
        "citied_by": "81",
        "cover_date": "2022-02-01",
        "Abstract": "Nowadays, the industrial environment is characterised by growing competitiveness, short response times, cost reduction and reliability of production to meet customer needs. Thus, the new industrial paradigm of Industry 4.0 has gained interest worldwide, leading many manufacturers to a significant digital transformation. Digital technologies have enabled a novel approach to decision-making processes based on data-driven strategies, where knowledge extraction relies on the analysis of a large amount of data from sensor-equipped factories. In this context, Predictive Maintenance (PdM) based on Machine Learning (ML) is one of the most prominent data-driven analytical approaches for monitoring industrial systems aiming to maximise reliability and efficiency. In fact, PdM aims not only to reduce equipment failure rates but also to minimise operating costs by maximising equipment life. When considering industrial applications, industries deal with different issues and constraints relating to process digitalisation. The main purpose of this study is to develop a new decision support system based on decision trees (DTs) that guides the decision-making process of PdM implementation, considering context-aware information, quality and maturity of collected data, severity, occurrence and detectability of potential failures (identified through FMECA analysis) and direct and indirect maintenance costs. The decision trees allow the study of different scenarios to identify the conditions under which a PdM policy, based on the ML algorithm, is economically profitable compared to corrective maintenance, considered to be the current scenario. The results show that the proposed methodology is a simple and easy way to implement tool to support the decision process by assessing the different levels of occurrence and severity of failures. For each level, savings and the potential costs have been evaluated at leaf nodes of the trees aimed at defining the most suitable maintenance strategy implementation. Finally, the proposed DTs are applied to a real industrial case to illustrate their applicability and robustness.",
        "DOI": "10.1016/j.ssci.2021.105529",
        "paper_author": "Arena S.",
        "affiliation_name": "Università degli Studi di Cagliari",
        "affiliation_city": "Cagliari",
        "affiliation_country": "Italy",
        "affiliation_id": "60032259",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Sharing runtime permission issues for developers based on similar-app review mining",
        "publication": "Journal of Systems and Software",
        "citied_by": "5",
        "cover_date": "2022-02-01",
        "Abstract": "The Android operating system introduces an ask-on-first-use permission policy after 6.0 version to regulate access to user data, which raises Permission-Related Issues (PRIS for short). Relevant research has been conducted to identify the PRIS through investigating users’ opinions towards runtime permissions. These efforts mainly focus on helping users understand and be aware of permissions, but neglect to assist developers in discovering permission requirements. In this paper, we propose a novel framework named PRISharer, which mines potential permission issues from the reviews of similar apps to assist developers in discovering possible permission requirements at runtime. PRISharer first builds a deep fine-grained classifier to identify similar apps, and then employs sentiment analysis based keywords extraction to mine permission-related reviews from similar apps’ reviews. Finally, the <category, permission, issues> mappings based on a multi-label learning method are generated to provide a PRIS profile for developers. The results of comparative experiments on more than 12 million reviews of 17,741 Android apps demonstrate that PRISharer achieves (i) superior performance in terms of F1-score for PRIS analysis, with an average improvement of 24.4%, (ii) the best recall (89.3%) in extracting permission-related reviews and (iii) 82.4% positive responses by expert developers, through which the effectiveness of PRISharer is well verified.",
        "DOI": "10.1016/j.jss.2021.111118",
        "paper_author": "Gao H.",
        "affiliation_name": "Nankai University",
        "affiliation_city": "Tianjin",
        "affiliation_country": "China",
        "affiliation_id": "60018038",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Food systems and structural and rural transformation: a quantitative synthesis for low and middle-income countries",
        "publication": "Food Security",
        "citied_by": "4",
        "cover_date": "2022-02-01",
        "Abstract": "Structural and rural transformation in a country are intricately linked to food system outcomes. Structural transformation captures a country’s level of dependence on agriculture, while rural transformation captures the productivity in the agricultural sector. Specifically, the agri-food system and employment transitions accompany country transformations and shape the spatial distribution of populations by influencing where people live, work and eat, all of which closely relate to food system transitions. We create a food systems index (FSI) capturing a rich set of drivers established in the literature. Using country level data from 85 low and middle-income countries (LMIC’s), we analyse the linkages between food system, structural and rural transformations as well as spatial population distributions. We also analyse a large number of policy relevant variables using machine-learning methodology to shed light on patterns related to institutions, female empowerment, infrastructure and health. Our analysis indicates that rural-dominant countries in the lowest FSI group will see their youth populations more than double in the next 30 years, indicating that their food system investments today will affect one third of global youth in the future. Medium FSI countries need to invest more in the semi-rural and peri-urban areas. We find that structural transformation is a necessary but not sufficient condition for desirable food system outcomes. Rural transformation by itself without structural transformation is not enough either. For LMIC’s, broad development interventions such as financial and digital connectivity as well as women’s empowerment loom more important than narrowly focused interventions regarding progress in the food system.",
        "DOI": "10.1007/s12571-021-01223-2",
        "paper_author": "Arslan A.",
        "affiliation_name": "International Fund for Agricultural Development",
        "affiliation_city": "Rome",
        "affiliation_country": "Italy",
        "affiliation_id": "60091230",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Automated reverse engineering of role-based access control policies of web applications",
        "publication": "Journal of Systems and Software",
        "citied_by": "8",
        "cover_date": "2022-02-01",
        "Abstract": "Access control (AC) is an important security mechanism used in software systems to restrict access to sensitive resources. Therefore, it is essential to validate the correctness of AC implementations with respect to policy specifications or intended access rights. However, in practice, AC policy specifications are often missing or poorly documented; in some cases, AC policies are hard-coded in business logic implementations. This leads to difficulties in validating the correctness of policy implementations and detecting AC defects. In this paper, we present a semi-automated framework for reverse-engineering of AC policies from Web applications. Our goal is to learn and recover role-based access control (RBAC) policies from implementations, which are then used to validate implemented policies and detect AC issues. Our framework, built on top of a suite of security tools, automatically explores a given Web application, mines domain input specifications from access logs, and systematically generates and executes more access requests using combinatorial test generation. To learn policies, we apply machine learning on the obtained data to characterize relevant attributes that influence AC. Finally, the inferred policies are presented to the security engineer, for validation with respect to intended access rights and for detecting AC issues. Inconsistent and insufficient policies are highlighted as potential AC issues, being either vulnerabilities or implementation errors. We evaluated our approach on four Web applications (three open-source and a proprietary one built by our industry partner) in terms of the correctness of inferred policies. We also evaluated the usefulness of our approach by investigating whether it facilitates the detection of AC issues. The results show that 97.8% of the inferred policies are correct with respect to the actual AC implementation; the analysis of these policies led to the discovery of 64 AC issues that were reported to the developers.",
        "DOI": "10.1016/j.jss.2021.111109",
        "paper_author": "Le H.T.",
        "affiliation_name": "Getcare Pharma Corporation",
        "affiliation_city": null,
        "affiliation_country": "Viet Nam",
        "affiliation_id": "127144249",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Exploring ensemble oversampling method for imbalanced keyword extraction learning in policy text based on three-way decisions and SMOTE",
        "publication": "Expert Systems with Applications",
        "citied_by": "23",
        "cover_date": "2022-02-01",
        "Abstract": "The e-government platform not only enables the government department to publish policy texts online, but also makes it easier for users to access the policy, especially for the convenience of understanding the policies by reading the keywords. For a given policy text, keywords take up only a small proportion, which can be seen as an unbalanced data set. Therefore, in this paper, we try to design automatic keyword extraction method of policy text with unbalanced data set. In order to achieve this goal, we firstly propose a new ensemble oversampling method to synthesize new data. In this case, we sample data from the training set by using Bagging method. During each sampling process, we train a logistic regression model to classify the training set. Based on the predicted probabilities, we utilize the classification confidence to divide training set into three regions by using three-way decisions (3WD). Then, we implement different strategies to synthesize new data. Besides, for keyword extraction of policy text, we conduct a series of experiments by using the classical supervised and unsupervised methods. In our experiment results, we can find that both in the public data sets and manual data sets, our sampling method can achieve better performance of F-measure and G-mean indexes, no matter what the supervised machine learning method is. This can also explain the advantage of 3WD. Different regions have different strategies to synthesize new data.",
        "DOI": "10.1016/j.eswa.2021.116051",
        "paper_author": "Liang D.",
        "affiliation_name": "School of Management and Economics of UESTC",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China",
        "affiliation_id": "60122365",
        "affiliation_state": "Sichuan"
    },
    {
        "paper_title": "Transient Hybrid Electric Vehicle Powertrain Control Based on Iterative Dynamic Programing",
        "publication": "Journal of Dynamic Systems, Measurement and Control, Transactions of the ASME",
        "citied_by": "4",
        "cover_date": "2022-02-01",
        "Abstract": "This research proposes an iterative dynamic programing (IDP) algorithm that generates an optimal supervisory control policy for hybrid electric vehicles (HEVs) considering transient powertrain dynamics. The proposed algorithm tries to solve the \"curse of dimensionality\"and the \"curse of modeling\"of conventional dynamic programing (DP). The proposed IDP algorithm iteratively updates the DP formulation using a machine learning-based powertrain model. The machine learning model is recursively trained using the outputs from the driving cycle simulation with a high-fidelity model. Once the reduced model converges to the high-fidelity model accuracy, the resulting control policy yields a 9.1% fuel economy (FE) improvement compared to the baseline nonpredictive rule-based control for the urban dynamometer driving schedule (UDDS) driving cycle. A conventional DP control strategy based on a quasi-static powertrain model and a perfect preview of future power demand yields 14.2% FE improvement. However, the FE improvement reduces to 5.7% when the policy is validated with the high-fidelity model. It is concluded that capturing the transient powertrain dynamics is critical to generating a realistic fuel economy prediction and relevant powertrain control policy. The proposed IDP strategy employs targeted state-space exploration to leverage the improving state trajectory from previous iterations. Compared to conventional fixed state-space sampling methods, this method improves the accuracy of the DP policy against discretization error. It also significantly reduces the computational load of the relatively high number of states of the transient powertrain model.",
        "DOI": "10.1115/1.4052230",
        "paper_author": "Zhu Q.",
        "affiliation_name": "Clemson University College of Engineering, Computing and Applied Sciences",
        "affiliation_city": "Clemson",
        "affiliation_country": "United States",
        "affiliation_id": "60139609",
        "affiliation_state": "SC"
    },
    {
        "paper_title": "Policy invariant explicit shaping: an efficient alternative to reward shaping",
        "publication": "Neural Computing and Applications",
        "citied_by": "3",
        "cover_date": "2022-02-01",
        "Abstract": "Reinforcement learning (RL) is a powerful learning paradigm in which agents can learn to maximize sparse and delayed reward signals. Although RL has had many impressive successes in complex domains, learning can take hours, days, or even years of training data. A major challenge of contemporary RL research is to discover how to learn with less data. Previous work has shown that domain information can be successfully used to shape the reward; by adding additional reward information, the agent can learn with much less data. Furthermore, if the reward is constructed from a potential function, the optimal policy is guaranteed to be unaltered. While such potential-based reward shaping (PBRS) holds promise, it is limited by the need for a well-defined potential function. Ideally, we would like to be able to take arbitrary advice from a human or other agent and improve performance without affecting the optimal policy. The recently introduced dynamic potential-based advice (DPBA) was proposed to tackle this challenge by predicting the potential function values as part of the learning process. However, this article demonstrates theoretically and empirically that, while DPBA can facilitate learning with good advice, it does in fact alter the optimal policy. We further show that when adding the correction term to “fix” DPBA it no longer shows effective shaping with good advice. We then present a simple method called policy invariant explicit shaping (PIES) and show theoretically and empirically that PIES can use arbitrary advice, speed-up learning, and leave the optimal policy unchanged.",
        "DOI": "10.1007/s00521-021-06259-1",
        "paper_author": "Behboudian P.",
        "affiliation_name": "University of Alberta",
        "affiliation_city": "Edmonton",
        "affiliation_country": "Canada",
        "affiliation_id": "60030835",
        "affiliation_state": "AB"
    },
    {
        "paper_title": "Exposing the ideal combination of endogenous–exogenous drivers for companies’ ecoinnovative orientation: Results from machine-learning methods",
        "publication": "Socio-Economic Planning Sciences",
        "citied_by": "11",
        "cover_date": "2022-02-01",
        "Abstract": "This study provides an XGBoost model to characterize the environmental orientation of innovative firms. This novel approach, using state-of-the-art machine learning methodologies and multiple recognized drivers of eco-innovation, provides solid empirical support for the understanding of the mechanisms that are crucial for firms' transition to a low-carbon economy. Although many drivers have been considered to affect firms’ eco-innovation, our feature selection process using the BorutaShap algorithm demonstrates that few aspects are truly relevant. Furthermore, analyzing a tree surrogate of the final model, our study explores the different paths or combinations of aspects that consistently lead to a specific eco-innovation orientation. The accuracy of the model and the large and complete spectrum of innovative companies in the sample contribute to the generalizability of the results. This study is particularly relevant because the main drivers of firms’ eco-innovative orientation depend on their innovative behavior, indicating that the managerial and policy work has to be directed to raising awareness of the different externalities derived from innovation. On one side, policy regulations should continue to pressure firms with environmental standards. On the other side, managers can stimulate the creation of a corporate innovative culture oriented toward improving operational efficiency (reducing unnecessary costs), improving the workplace environment, and focusing on new customer demands, which, in essence, will guide the organization to be more environmentally and socially responsible.",
        "DOI": "10.1016/j.seps.2021.101145",
        "paper_author": "Peiró-Signes Á.",
        "affiliation_name": "Universitat Politècnica de València",
        "affiliation_city": "Valencia",
        "affiliation_country": "Spain",
        "affiliation_id": "60011476",
        "affiliation_state": "Valencia"
    },
    {
        "paper_title": "Exploring machine learning techniques to predict deforestation to enhance the decision-making of road construction projects",
        "publication": "Journal of Industrial Ecology",
        "citied_by": "15",
        "cover_date": "2022-02-01",
        "Abstract": "Land use changes (LUCs), which are defined as the modification in the use of land due to anthropogenic activities, are important sources of GHG emissions. In this context, understanding future trends of LUCs, such as deforestation, in a spatial manner is relevant. The main objective of this study is to generate a deforestation prediction model for a given period of time (i.e., 2002–2017 and 2010–2017) to estimate the potential carbon emissions associated with different anthropogenic variables in the Peruvian Amazon using machine learning (ML) algorithms. This study was motivated in the analysis of a road project previously studied using life cycle assessment (LCA). Models using neural networks and random forest algorithms were trained and evaluated in a fully cloud-based environment using Google Earth Engine. ML-related results demonstrated that random forest is a quicker and straightforward response to model the system under study, especially considering that data do not require additional processing during the modeling and prediction stages. Predicted results suggest that expected road expansion may be related to considerable carbon emissions in the future. Calculated values are relevant especially if the mitigation efforts that Peru has complied with in the Paris Agreement are considered. The increased complexity of the framework is justified since it allows identifying the location of hotspots and may potentially complement the utility of LCA in policy support in the areas of territorial planning and tropical road expansion.",
        "DOI": "10.1111/jiec.13185",
        "paper_author": "Larrea-Gallegos G.",
        "affiliation_name": "Pontificia Universidad Catolica del Peru",
        "affiliation_city": "Lima",
        "affiliation_country": "Peru",
        "affiliation_id": "60071236",
        "affiliation_state": "Lima"
    },
    {
        "paper_title": "Predicting success in United States Air Force pilot training using machine learning techniques",
        "publication": "Socio-Economic Planning Sciences",
        "citied_by": "12",
        "cover_date": "2022-02-01",
        "Abstract": "The chronic pilot shortage that has plagued the United States Air Force over the past three years poses a national-level problem that senior military members are working to overcome. Unfortunately, not all pilot candidates successfully complete the necessary training requirements to become fully qualified Air Force pilots, which wastes critical time and resources and only further exacerbates the pilot shortage problem. Therefore, it is important for the Air Force to carefully consider whom they select to attend pilot training. This research examines historical specialized undergraduate pilot training (SUPT) candidate data leveraging a variety of machine learning techniques to obtain insights on candidate success. Computational experimentation is performed to determine how selected machine learning techniques and their respective hyperparameters affect solution quality. Results reveal that the extremely randomized tree machine learning technique can achieve nearly 94% accuracy in predicting candidate success. Additional analysis indicates degree type and commissioning source are the most important features in determining candidate success. Ultimately, this research can inform the modification of future SUPT candidate selection criteria and other related Air Force personnel policies.",
        "DOI": "10.1016/j.seps.2021.101121",
        "paper_author": "Jenkins P.R.",
        "affiliation_name": "AFIT Graduate School of Engineering and Management",
        "affiliation_city": "Dayton",
        "affiliation_country": "United States",
        "affiliation_id": "60279872",
        "affiliation_state": "OH"
    },
    {
        "paper_title": "Deep reinforcement learning for the optimal placement of cryptocurrency limit orders",
        "publication": "European Journal of Operational Research",
        "citied_by": "31",
        "cover_date": "2022-02-01",
        "Abstract": "This paper presents the first large-scale application of deep reinforcement learning to optimize execution at cryptocurrency exchanges by learning optimal limit order placement strategies. Execution optimization is highly relevant for both professional asset managers and private investors as execution quality affects portfolio performance at economically significant levels and is the target of regulatory supervision. To optimize execution with deep reinforcement learning, we design a problem-specific training environment that introduces a purpose-built reward function, hand-crafted market state features and a virtual limit order exchange. We empirically compare state-of-the-art deep reinforcement learning algorithms to several benchmarks with market data from major cryptocurrency exchanges, which represent an ideal test bed for our study as liquidity costs are relatively high. In total, we leverage 18 months of high-frequency data for several currency pairs with 300 million trades and more than 3.5 million order book states. We find proximal policy optimization to reliably learn superior order placement strategies. By interacting with our simulated limit order exchange, it learns cryptocurrency execution strategies that are empirically known from established markets. Order placement becomes more aggressive in anticipation of lower execution probabilities, which is indicated by trade and order imbalances.",
        "DOI": "10.1016/j.ejor.2021.04.050",
        "paper_author": "Schnaubelt M.",
        "affiliation_name": "Friedrich-Alexander-Universität Erlangen-Nürnberg",
        "affiliation_city": "Erlangen",
        "affiliation_country": "Germany",
        "affiliation_id": "60000765",
        "affiliation_state": "Bayern"
    },
    {
        "paper_title": "Public perceptions and implementation considerations on the use of artificial intelligence in health",
        "publication": "Journal of Evaluation in Clinical Practice",
        "citied_by": "1",
        "cover_date": "2022-02-01",
        "Abstract": "NA",
        "DOI": "10.1111/jep.13580",
        "paper_author": "Romero R.A.",
        "affiliation_name": "University of California, Irvine",
        "affiliation_city": "Irvine",
        "affiliation_country": "United States",
        "affiliation_id": "60007278",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Predicting sun protection measures against skin diseases using machine learning approaches",
        "publication": "Journal of Cosmetic Dermatology",
        "citied_by": "4",
        "cover_date": "2022-02-01",
        "Abstract": "Background: The substantial growth rate of skin cancer has necessitated adequate protection from solar radiation. Consequently, analyzing sun protection practices is an imperative research area in dermatology and pharmacology. Aims: This paper aims to analyze public sun-protection manners in the Arabian Peninsula regions. Methods: A simple random survey was conducted to assess public sun protection manners. Artificial neural network (ANN) and support vector machine (SVM) were selected from several machine learning algorithms to create the models for predicting public sun protection measures based on the prediction accuracy. Model performances were evaluated based on several performance indicators depending on the confusion matrices and receiver operating characteristic curves. Results: 51% of the respondents have a low level, and 49% have a high level of sun protection practices. The results showed that the SVM performed considerably amended than the ANN for predicting the response. The relative importance of the predictors for the best predictive SVM model was also analyzed. The predictors are ranked as: the number of times having sunburnt >gender > use seat belt while driving/riding a vehicle >considers the UV index for personal sun exposure >income based on the expenses >sports/exercise activities >consciousness of the chance for having sunburnt on extended exposure to the sun >age > having any skin problem >nationality > skin type. Conclusion: These identified significant predictors might be considered for developing an effective policy to increase public consciousness using proper protection from solar radiation's detrimental effect to rule out skin diseases.",
        "DOI": "10.1111/jocd.14120",
        "paper_author": "Sultana N.",
        "affiliation_name": "Imam Abdulrahman Bin Faisal University",
        "affiliation_city": "Dammam",
        "affiliation_country": "Saudi Arabia",
        "affiliation_id": "60104334",
        "affiliation_state": "Ash Sharqiyah"
    },
    {
        "paper_title": "Comparative performance analysis of support vector regression and artificial neural network for prediction of municipal solid waste generation",
        "publication": "Waste Management and Research",
        "citied_by": "33",
        "cover_date": "2022-02-01",
        "Abstract": "The evolution of machine learning (ML) algorithms provides researchers and engineers with state-of-the-art tools to dynamically model complex relationships. The design and operation of municipal solid waste (MSW) management systems require accurate estimation of generation rates. In this study, we applied rapid, non-linear and non-parametric data driven ML algorithms independently, multi-layer perceptron artificial neural network (MLP-ANN) and support vector regression (SVR) models to predict annual MSW generation rates in Bahrain. Models were trained and tested with MSW generation data for period of 1997–2019. The population, gross domestic product, annual tourist numbers, annual electricity consumption and total annual CO2 emissions were selected as explanatory variables and incorporated into developed models. The zero score normalization (ZSN) and minimum maximum normalization (MMN) methods were utilized to improve the quality of data and subsequently enhances the performance of ML algorithms. Statistical metrics were employed to discriminate performance of MLP-ANN and SVR models. The linear, polynomial, radial basis function (RBF) and sigmoid kernel functions were investigated to find the optimal SVR model. Results showed that RBF-SVR model with R2 value of 0.97% and 4.82% and absolute forecasting error (AFE) for the period of 2008 and 2019 exhibits superior prediction and robustness in comparison to MLP-ANN. The efficacy of MLP-ANN model was also reasonably successful with R2 value of 0.94. It was shown that MMN pre-processing generated optimal MLP-ANN model while ZSN pre-processing produced optimal RBF-SVR model. This work also highlights the importance of application of ML modelling approaches to plan and implement their roadmap for waste management systems by policymakers.",
        "DOI": "10.1177/0734242X211008526",
        "paper_author": "Jassim M.S.",
        "affiliation_name": "University of Bahrain",
        "affiliation_city": "Zallaq",
        "affiliation_country": "Bahrain",
        "affiliation_id": "60000237",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Raven: Scheduling Virtual Machine Migration During Datacenter Upgrades with Reinforcement Learning",
        "publication": "Mobile Networks and Applications",
        "citied_by": "9",
        "cover_date": "2022-02-01",
        "Abstract": "Physical machines in modern datacenters are routinely upgraded due to their maintenance requirements, which involve migrating all the virtual machines they currently host to alternative physical machines. For this kind of datacenter upgrades, it is critical to minimize the time it takes to upgrade all the physical machines in the datacenter, so as to reduce disruptions to cloud services. To minimize the upgrade time, it is essential to carefully schedule the migration of virtual machines on each physical machine during its upgrade, without violating any constraints imposed by virtual machines that are currently running. Rather than resorting to heuristic algorithms as existing work, we propose a new scheduler, Raven, that uses an experience-driven approach with deep reinforcement learning to schedule the virtual machine migration. With our design of the state space, action space and reward function, Raven trains a fully-connected neural network using the cross-entropy method to approximate the policy of choosing a destination physical machine for each virtual machine before its migration. We compare Raven with state-of-the-art algorithms in the literature, and our results show that Raven can effectively shorten the time to complete the datacenter upgrade under different datacenter settings.",
        "DOI": "10.1007/s11036-020-01632-1",
        "paper_author": "Ying C.",
        "affiliation_name": "University of Toronto",
        "affiliation_city": "Toronto",
        "affiliation_country": "Canada",
        "affiliation_id": "60016849",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "Estimation and Spatiotemporal Analysis of NO<inf>2</inf> Pollution in East Asia During 2001–2016",
        "publication": "Journal of Geophysical Research: Atmospheres",
        "citied_by": "4",
        "cover_date": "2022-01-27",
        "Abstract": "Ambient nitrogen dioxide (NO2) not only has adverse health effects on humans but also contributes to the production of two major secondary atmospheric pollutants, ozone (O3) and fine particulate matter (PM2.5). In this study, surface NO2 concentrations in East Asia from 2001 to 2016 were estimated by combining an ensemble backpropagation neural network method, satellite NO2 column data, and reanalysis data. The estimated monthly and annual mean NO2 concentrations were well-correlated with the observations, with R (correlation coefficient) values of 0.89 and 0.91, respectively. Our results indicate that the NO2 concentrations in most areas of East Asia peaked during 2011–2013. The NO2 concentrations in autumn and winter, especially in the eastern and northern parts of China, were much higher than those in summer. In terms of population NO2 exposure, over 25 million South Korea residents (∼45% of the population) were exposed to NO2 concentrations higher than the 2005 World Health Organization's annual standard (40 μg/m3, ∼22 ppbv at 25°C) in 2015. In contrast, the entire populations of some developing countries, such as Vietnam, Cambodia, Myanmar, the Philippines, and Lao PDR, were exposed to NO2 concentrations less than 14 ppbv. Based on the estimation, NO2-related asthma cases in East Asia increased by 1.37% annually from 2001 to 2015, reaching 139,000 cases (95% confidence interval: 37,400–263,400) in 2015. NOx emission inventories vary from country to country in East Asia; thus, more targeted NOx emission-control policies are urgently required.",
        "DOI": "10.1029/2021JD035129",
        "paper_author": "Hu M.",
        "affiliation_name": "Hong Kong University of Science and Technology",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong",
        "affiliation_id": "60008592",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Artificial Intelligence and Statistics: Just the Old Wine in New Wineskins?",
        "publication": "Frontiers in Digital Health",
        "citied_by": "14",
        "cover_date": "2022-01-26",
        "Abstract": "NA",
        "DOI": "10.3389/fdgth.2022.833912",
        "paper_author": "Faes L.",
        "affiliation_name": "Moorfields Eye Hospital NHS Foundation Trust",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60016691",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A hybrid optimization technique for proficient energy management in smart grid environment",
        "publication": "International Journal of Hydrogen Energy",
        "citied_by": "22",
        "cover_date": "2022-01-26",
        "Abstract": "Electrical energy is one of the key components for the development and sustainability of any nation. India is a developing country and blessed with a huge amount of renewable energy resources still there are various remote areas where the grid supply is rarely available. As electrical energy is the basic requirement, therefore it must be taken up on priority to exploit the available renewable energy resources integrated with storage devices like fuel cells and batteries for power generation and help the planners in providing the energy-efficient and alternative solution. This solution will not only meet electricity demand but also helps reduce greenhouse gas emissions as a result the efficient, sustainable and eco-friendly solution can be achieved which would contribute a lot to the smart grid environment. In this paper, a modified grey wolf optimizer approach is utilized to develop a hybrid microgrid based on available renewable energy resources considering modern power grid interactions. The proposed approach would be able to provide a robust and efficient microgrid that utilizes solar photovoltaic technology and wind energy conversion system. This approach integrates renewable resources with the meta-heuristic optimization algorithm for optimal dispatch of energy in grid-connected hybrid microgrid system. The proposed approach is mainly aimed to provide the optimal sizing of renewable energy-based microgrids based on the load profile according to time of use. To validate the proposed approach, a comparative study is also conducted through a case study and shows a significant savings of 30.88% and 49.99% of the rolling cost in comparison with fuzzy logic and mixed integer linear programming-based energy management system respectively.",
        "DOI": "10.1016/j.ijhydene.2021.11.188",
        "paper_author": "Kumar A.",
        "affiliation_name": "Delhi Technological University",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India",
        "affiliation_id": "60002874",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Classification of Online School Problems from Tweets on Twitter Using Support Vector Algorithm",
        "publication": "AIP Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2022-01-25",
        "Abstract": "The government has implemented a self-quarantine policy to prevent the spread of the coronavirus. It means prohibiting the gathering of many people. The Ministry of Education and Culture implements an online school policy, so the students still receive an education. At first, people thought that this policy was appropriate and helpful to understand technology better. In any case, people started to complain about almost the different impacts they felt, such as share expenses and stress on online schools. People complain about their problems on social media, Twitter. Hence, it is conceivable to recognize online learning problems through Twitter by categorizing them into two categories, technical and psychological. This scientific research is to classify online school problems to find out the most problem need to be fixed to improve online school quality. The classification of online learning problems uses the text mining method with the Support Vector Machine (SVM) algorithm. S V M algorithm is used to maximized the separation 2 class to decrease errors. The data used in this study were 549 documents with 52% of psychological problems and 48% of technical issues. Using the K-Fold Cross-validation method with K = 10, the average of the training accuracy data is 99.811%, and the average of the testing data accuracy is 90.3%. In addition, the results of the Pres's Q test show that the model is consistent in predicting the testing data. This research indicates that the Support Vector Machine method is suitable to classify data on online learning problems.",
        "DOI": "10.1063/5.0104576",
        "paper_author": "Fitri D.W.",
        "affiliation_name": "Universitas Airlangga",
        "affiliation_city": "Surabaya",
        "affiliation_country": "Indonesia",
        "affiliation_id": "60069383",
        "affiliation_state": "East Java"
    },
    {
        "paper_title": "Formulating Operational Mitigation Options and Examining Intra-Urban Social Inequality Using Evidence-Based Urban Warming Effects",
        "publication": "Frontiers in Environmental Science",
        "citied_by": "12",
        "cover_date": "2022-01-21",
        "Abstract": "Human-induced climate change is bringing warmer conditions to the Southwestern United States. More extreme urban heat island (UHI) effects are not distributed equally, and often impact socioeconomically vulnerable populations the most. This study aims to quantify how land surface temperature (LST) changes with increasing green vegetation landscapes, identify disparities in urban warming exposure, and provide a method for developing evidence-based mitigation options. ECOSTRESS LST products, detailed land use and land cover (LULC) classes, and socioeconomic variables were used to facilitate the analysis. We examined the relationship between LST and the fractions of LULC and socioeconomic factors in the city of Phoenix, Arizona. A machine learning approach (Random Forest) was used to model LST changes by taking the LULC fractions (scenario-based approaches) as the explanatory variables. We found that vegetation features—trees, grass, and shrubs—were the most important factors mitigating UHI effects during the summer daytime. Trees tended to lower surface temperature more effectively, whereas we observed elevated daytime LST most often near roads. Meanwhile, higher summer daytime temperatures were observed on land with unmanaged soil compared to the built environment. We found that affluent neighborhoods experienced lower temperatures, while low-income communities experienced higher temperatures. Scenario analyses suggest that replacing 50% of unmanaged soil with trees could reduce average summer daytime temperatures by 1.97°C if the intervention was implemented across all of Phoenix and by 1.43°C if implemented within the urban core only. We suggest that native trees requiring little to no additional water other than rainfall should be considered. We quantify mitigation options for urban warming effect under vegetation management interventions, and our results provide some vital insight into existing disparities in UHI impacts. Future UHI mitigation strategies seriously need to consider low-income communities to improve environmental justice. These can be used to guide the development of sustainable and equitable policies for vegetation management to mitigate heat exposure impacts on communities.",
        "DOI": "10.3389/fenvs.2021.795474",
        "paper_author": "Zhu Y.",
        "affiliation_name": "Arizona State University",
        "affiliation_city": "Tempe",
        "affiliation_country": "United States",
        "affiliation_id": "60003892",
        "affiliation_state": "AZ"
    },
    {
        "paper_title": "Microestimates of wealth for all low- and middle-income countries",
        "publication": "Proceedings of the National Academy of Sciences of the United States of America",
        "citied_by": "89",
        "cover_date": "2022-01-18",
        "Abstract": "Many critical policy decisions, from strategic investments to the allocation of humanitarian aid, rely on data about the geographic distribution of wealth and poverty. Yet many poverty maps are out of date or exist only at very coarse levels of granularity. Here we develop microestimates of the relative wealth and poverty of the populated surface of all 135 low- and middle-income countries (LMICs) at 2.4 km resolution. The estimates are built by applying machine-learning algorithms to vast and heterogeneous data from satellites, mobile phone networks, and topographic maps, as well as aggregated and deidentified connectivity data from Facebook. We train and calibrate the estimates using nationally representative household survey data from 56 LMICs and then validate their accuracy using four independent sources of household survey data from 18 countries. We also provide confidence intervals for each microestimate to facilitate responsible downstream use. These estimates are provided free for public use in the hope that they enable targeted policy response to the COVID-19 pandemic, provide the foundation for insights into the causes and consequences of economic development and growth, and promote responsible policymaking in support of sustainable development.",
        "DOI": "10.1073/pnas.2113658119",
        "paper_author": "Chi G.",
        "affiliation_name": "UC Berkeley School of Information",
        "affiliation_city": "Berkeley",
        "affiliation_country": "United States",
        "affiliation_id": "60104850",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "An efficient deep reinforcement machine learning-based control reverse osmosis system for water desalination",
        "publication": "Desalination",
        "citied_by": "55",
        "cover_date": "2022-01-15",
        "Abstract": "Water scarcity is a permanent problem that faces all over the world. Artificial Intelligence (AI) has many machine learning methods used to solve many problems in all fields. This paper suggests a novel and efficient approach to finding a trans-membrane pressure using Deep Reinforcement Learning (DRL). Our system uses Deep Deterministic Policy Gradient (DDPG) agent to adjust the pressure across the membrane. This adjustment considers the Salt Rejection (SR) to be 99% to investigate the desired water flux. The system takes the maximum height of the water in the tank (hmax), the salt concentration of feed flow (C), the temperature of feed flow (T), the recovery ratio (R), and the salt rejection ratio (SR) as input, and returns the water flux Qp. The results show the effectiveness and the power of the DDPG agent in finding that pressure. The agent is trained in a small number of episodes (150), and the average reward value is high.",
        "DOI": "10.1016/j.desal.2021.115443",
        "paper_author": "Bonny T.",
        "affiliation_name": "University of Sharjah",
        "affiliation_city": "Sharjah",
        "affiliation_country": "United Arab Emirates",
        "affiliation_id": "60070813",
        "affiliation_state": "Sharjah"
    },
    {
        "paper_title": "Transfer-Reinforcement-Learning-Based rescheduling of differential power grids considering security constraints",
        "publication": "Applied Energy",
        "citied_by": "8",
        "cover_date": "2022-01-15",
        "Abstract": "The power system rescheduling based on model-free methods has obvious defects in practical application, such as poor scenario transferability, long data training time, and waste of domain knowledge. To overcome the above defects, a transfer-reinforcement-learning-based rescheduling method of differential power grids considering security constraints is proposed. When constructing the Markov decision-making process of security-constrained rescheduling, both the off-limits of line power and node voltage are considered in the reward. The action space of deep reinforcement learning is narrowed by calculating the sensitivities of devices and mapped to control the active and reactive power regulating devices to reschedule active and reactive power simultaneously. According to the change degree of transfer object, the applications of transfer learning are divided into two scenarios. For the security-constrained rescheduling transfer scenario of different structures of the same power grid, a domain-adaption transfer learning method is formed, realizing good data adaptability after structure changes with the original model. Moreover, a policy-based transfer learning method is constructed for the security-constrained rescheduling transfer scenario of different power grids, enhancing the training speed and training effect of target power grid. Two standard systems and two actual power grids are utilized to verify the effectiveness of the method. For the actual power grids, the effects of the two scenarios are improved by 5.8% and 3.9% with transfer learning. Compared with other methods, this method not only has obvious advantages in transferability, but also has a shorter learning process and lower control cost.",
        "DOI": "10.1016/j.apenergy.2021.118121",
        "paper_author": "Wang T.",
        "affiliation_name": "China Electric Power Research Institute",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60016518",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A new carbon price prediction model",
        "publication": "Energy",
        "citied_by": "50",
        "cover_date": "2022-01-15",
        "Abstract": "The excessive emission of carbon is one of the important factors causing environmental pollution, and the prediction of carbon trading market price is an important mean of emission reduction. In order to accurately predict the carbon price, a new carbon price prediction model is proposed in this paper. Firstly, the data is decomposed into multiple intrinsic mode functions (IMFs) by optimized variational mode decomposition (OVMD). Secondly, the complexity of IMFs is analyzed by spatial-dependence recurrence sample entropy (SdrSampEn). Thirdly, the IMFs with higher complexity are integrated and decomposed by complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN) to get high complexity IMFs. Then, particle swarm optimized extreme learning machine (PSOELM) is used to predict the high complexity IMFs, and extreme learning machine (ELM) is used to predict other. Finally, the predicted value is reconstructed to complete the prediction. In this paper, OVMD is proposed to solve the selection of decomposition layers K by variational mode decomposition (VMD) from the perspective of variance contribution rate. Through the experimental results, the effectiveness of the proposed model is verified, and it can be used to predict the supply and demand of carbon market and evaluate the effectiveness of current carbon trading policies.",
        "DOI": "10.1016/j.energy.2021.122324",
        "paper_author": "Li G.",
        "affiliation_name": "Xi'an Institute of Posts and Telecommunications",
        "affiliation_city": "Xi'an",
        "affiliation_country": "China",
        "affiliation_id": "60073708",
        "affiliation_state": "Shaanxi"
    },
    {
        "paper_title": "Reputation-Based Federated Learning for Secure Wireless Networks",
        "publication": "IEEE Internet of Things Journal",
        "citied_by": "65",
        "cover_date": "2022-01-15",
        "Abstract": "The dilemma between the ever-increasing demands for data processing, and the limited capabilities of mobile devices in a wireless communication system calls for the appearance of federated learning (FL). As a distributed machine learning (ML) method, FL executes in an iterative manner by distributing the global model parameters and aggregating the local model parameters, which avoids the transmission of huge raw data and preserves data privacy during the training process. However, since FL cannot control the local training and transmission process, this gives malicious users the opportunity to deteriorate the global aggregation. We adopt a reputation model based on beta distribution function to measure the credibility of local users, and propose a reputation-based scheduling policy with user fairness constraint. By taking into account the impact of wireless channel conditions and malicious attack features, we derive tractable expressions for the convergence rate of FL in a wireless setting. Moreover, we validate the superiority of the proposed reputation-based scheduling policy via numerical analysis and empirical simulations. The results show that the proposed secure wireless FL framework can not only distinguish malicious users from normal users but also effectively defend against several typical attack types featured in attack intensity and attack frequency. The analysis also reveals that the effect of average attack intensity on the convergence performance of FL is dominated by the percentage of malicious user equipments (UEs), and imposes even greater negative effect on the convergence performance of FL as the percentage of malicious UEs increases.",
        "DOI": "10.1109/JIOT.2021.3079104",
        "paper_author": "Song Z.",
        "affiliation_name": "Northwest A&amp;F University",
        "affiliation_city": "Yangling",
        "affiliation_country": "China",
        "affiliation_id": "60031041",
        "affiliation_state": "Shaanxi"
    },
    {
        "paper_title": "Regulation Effect of Smart Grid on Green Transformation of Electric Power Enterprises: Based on the Investigation of “Leader” Trap",
        "publication": "Frontiers in Energy Research",
        "citied_by": "7",
        "cover_date": "2022-01-12",
        "Abstract": "The 2060 carbon neutral target reflects the long-term equilibrium and stability of production activities and the natural environment. As an important part of Chinese energy structure, the operation and transformation of power enterprises will face higher requirements. Although the rapid development of smart grids provides necessary technical support for power enterprises to build a modern energy system with green power as the core, whether power enterprises can use smart grids to improve their operating performance and environmental performance has yet to be discussed. The differences caused by the heterogeneity of property rights will also have an impact on the green transformation and development of enterprises. This paper selects 25 Chinese power enterprises as the research objects and uses the 2011–2019 enterprise panel data and the data envelopment analysis model to evaluate the operating performance and environmental performance of power enterprises. The results show that the overall fluctuation trend of the total factor productivity index and green total factor productivity index of power enterprises are W-shaped, and technological progress is the main driving force for the improvement of power operating performance and environmental performance; Compared with enterprises with a single power generation method, enterprises with diversified power generation methods performed better in their overall total factor productivity index. After that, text mining and machine learning methods are used to classify the text of the enterprise’s annual report to determine whether the enterprise applies smart grid technology for production and operation activities. Finally, using feasible generalized least squares method (FLGS) and dynamic panel system generalized moment estimation (SYS-GMM) to analyze the impact of smart grid on the operating performance and environmental performance of power enterprises, and the nature of corporate property rights in this process. It is found that smart grids can improve the operating performance and environmental performance of power enterprises; compared with state-owned enterprises, non-state-owned enterprises can achieve better performance in the application of smart grids to improve operating performance and environmental performance. Finally, this study provides corresponding policy recommendations for power enterprises to achieve performance improvement and green transformation development.",
        "DOI": "10.3389/fenrg.2021.783786",
        "paper_author": "Li C.",
        "affiliation_name": "Zhengzhou University of Light Industry",
        "affiliation_city": "Zhengzhou",
        "affiliation_country": "China",
        "affiliation_id": "60092439",
        "affiliation_state": "Henan"
    },
    {
        "paper_title": "SADRL: Merging human experience with machine intelligence via supervised assisted deep reinforcement learning",
        "publication": "Neurocomputing",
        "citied_by": "14",
        "cover_date": "2022-01-07",
        "Abstract": "Deep Reinforcement Learning (DRL) has proven its capability to learn optimal policies in decision-making problems by directly interacting with environments. Meanwhile, supervised learning methods also show great capability of learning from data. However, how to combine DRL with supervised learning and leverage additional knowledge and data to assist the DRL agent remains difficult. This study proposes a novel Supervised Assisted Deep Reinforcement Learning (SADRL) framework integrating deep Q-learning from dynamic demonstrations with a behavioral cloning model (DQfDD-BC). Specifically, the proposed DQfDD-BC method leverages historical demonstrations to pre-train a behavioral cloning model and consistently update it by learning the dynamically updated demonstrations. A supervised expert loss function is designed to compare actions generated by the DRL model with those obtained from the BC model to provide advantageous guidance for policy improvements. Experimental results in several OpenAI Gym environments show that the proposed approach accelerates the learning processes, and meanwhile, adapts to different performance levels of demonstrations. As illustrated in an ablation study, the dynamic demonstration and expert loss mechanisms using a BC model contribute to improving the learning convergence performance compared with the baseline models. We believe that SADRL provides an elegant framework and the proposed method can promote the integration of human experience and machine intelligence.",
        "DOI": "10.1016/j.neucom.2021.09.064",
        "paper_author": "Li X.",
        "affiliation_name": "Institute of Automation Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60018486",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Soil Erosion Assessment of Alpine Grassland in the Source Park of the Yellow River on the Qinghai-Tibetan Plateau, China",
        "publication": "Frontiers in Ecology and Evolution",
        "citied_by": "6",
        "cover_date": "2022-01-05",
        "Abstract": "The source park of the Yellow River (SPYR), as a vital ecological shelter on the Qinghai-Tibetan Plateau, is suffering different degrees of degradation and desertification, resulting in soil erosion in recent decades. Therefore, studying the mechanism, influencing factors and current situation of soil erosion in the alpine grassland ecosystems of the SPYR are significant for protecting the ecological and productive functions. Based on the 137Cs element tracing technique and machine learning algorithms, five strategic variable selection algorithms based on machine learning algorithms are used to identify the minimal optimal set and analyze the main factors that influence soil erosion in the SPYR. The optimal model for estimating soil erosion in the SPYR is obtained by comparisons model outputs between the RUSLE and machine learning algorithms combined with variable selection models. We identify the spatial distribution pattern of soil erosion in the study area by the optimal model. The results indicated that: (1) A comprehensive set of variables is more objective than the RUSLE model. In terms of verification accuracy, the simulated annealing -Cubist model (R = 0.67, RMSD = 1,368 t km–2⋅a–1) simulation results represents the best while the RUSLE model (R = 0.49, RMSD = 1,769 t⋅km–2⋅a–1) goes on the worst. (2) The soil erosion is more severe in the north than the southeast of the SPYR. The average erosion modulus is 6,460.95 t⋅km–2⋅a–1 and roughly 99% of the survey region has an intensive erosion modulus (5,000–8,000 t⋅km–2⋅a–1). (3) Total erosion loss is relatively 8.45⋅108 t⋅a–1 in the SPYR, which is commonly 12.64 times greater than the allowable soil erosion loss. The economic monetization of SOC loss caused by soil erosion in the entire research area was almost $47.90 billion in 2014. These results will help provide scientific evidences not only for farmers and herdsmen but also for environmental science managers and administrators. In addition, a new ecological policy recommendation was proposed to balance grassland protection and animal husbandry economic production based on the value of soil erosion reclassification.",
        "DOI": "10.3389/fevo.2021.771439",
        "paper_author": "Lin H.",
        "affiliation_name": "Lanzhou University",
        "affiliation_city": "Lanzhou",
        "affiliation_country": "China",
        "affiliation_id": "60028265",
        "affiliation_state": "Gansu"
    },
    {
        "paper_title": "Automatic Segmentation of Kidneys and Kidney Tumors: The KiTS19 International Challenge",
        "publication": "Frontiers in Digital Health",
        "citied_by": "13",
        "cover_date": "2022-01-04",
        "Abstract": "Purpose: Clinicians rely on imaging features to calculate complexity of renal masses based on validated scoring systems. These scoring methods are labor-intensive and are subjected to interobserver variability. Artificial intelligence has been increasingly utilized by the medical community to solve such issues. However, developing reliable algorithms is usually time-consuming and costly. We created an international community-driven competition (KiTS19) to develop and identify the best system for automatic segmentation of kidneys and kidney tumors in contrast CT and report the results. Methods: A training and test set of CT scans that was manually annotated by trained individuals were generated from consecutive patients undergoing renal surgery for whom demographic, clinical and outcome data were available. The KiTS19 Challenge was a machine learning competition hosted on grand-challenge.org in conjunction with an international conference. Teams were given 3 months to develop their algorithm using a full-annotated training set of images and an unannotated test set was released for 2 weeks from which average Sørensen-Dice coefficient between kidney and tumor regions were calculated across all 90 test cases. Results: There were 100 valid submissions that were based on deep neural networks but there were differences in pre-processing strategies, architectural details, and training procedures. The winning team scored a 0.974 kidney Dice and a 0.851 tumor Dice resulting in 0.912 composite score. Automatic segmentation of the kidney by the participating teams performed comparably to expert manual segmentation but was less reliable when segmenting the tumor. Conclusion: Rapid advancement in automated semantic segmentation of kidney lesions is possible with relatively high accuracy when the data is released publicly, and participation is incentivized. We hope that our findings will encourage further research that would enable the potential of adopting AI into the medical field.",
        "DOI": "10.3389/fdgth.2021.797607",
        "paper_author": "Sathianathen N.J.",
        "affiliation_name": "University of Minnesota Twin Cities",
        "affiliation_city": "Minneapolis",
        "affiliation_country": "United States",
        "affiliation_id": "60029445",
        "affiliation_state": "MN"
    },
    {
        "paper_title": "What Determine the Corporate Tax Rates During the COVID-19? Evidence From 113 Countries",
        "publication": "Frontiers in Public Health",
        "citied_by": "5",
        "cover_date": "2022-01-04",
        "Abstract": "Fiscal policy implications become an important tool to soften the negative consequences of the COVID-19 pandemic. Given this backdrop, this paper analyses the drivers of corporate tax rates during the COVID-19 pandemic (i.e., in 2020 and 2021). The results from 113 advanced and developing economies show that a higher level of the COVID-19-related uncertainty is positively associated with the corporate tax rates. Similarly, the country size (measured by total population) increases the corporate tax rates. Per capita income is negatively related to the corporate tax rates, but this evidence is insufficient to consider different estimation techniques. The paper also discusses potential fiscal policy implications for the driving mechanism of corporate tax rates for the post-COVID-19 era.",
        "DOI": "10.3389/fpubh.2021.816561",
        "paper_author": "Li R.",
        "affiliation_name": "Henan Normal University",
        "affiliation_city": "Xinxiang",
        "affiliation_country": "China",
        "affiliation_id": "60029943",
        "affiliation_state": "Henan"
    },
    {
        "paper_title": "The Role of Environmental Policies in Promoting Venture Capital Investments in Cleantech Companies",
        "publication": "Review of Corporate Finance",
        "citied_by": "35",
        "cover_date": "2022-01-01",
        "Abstract": "This paper provides insights on the role of environmental policies in promoting venture capital investments in companies involved in the development of clean technologies. Based on a supervised machine learning algorithm, we develop a fully replicable methodology to identify cleantech firms among a comprehensive database of invested companies by venture capital funds. We then analyse the relationship between the stringency level of environmental policies and venture capital investments in cleantech companies operating in 21 OECD countries. We explore whether policies have a differential effect in fostering institutional venture capital (IVC) and governmental venture capital (GVC) investments. Our findings indicate that IVC investments in cleantech are mainly driven by the level of environmental taxes and market pull mechanisms as feed-in tariffs and R&D subsidies, whereas GVC investment decisions are driven by a country’s commitment to reach environmental targets. Moreover, our results suggest that GVC funds are developed as an alternative incentive mechanism: when direct incentives applied by governments are less developed, the relevance of GVC investments increases, which suggests a substitution effect between the two forms of intervention.",
        "DOI": "10.1561/114.00000024",
        "paper_author": "Bianchini R.",
        "affiliation_name": "Politecnico di Milano",
        "affiliation_city": "Milan",
        "affiliation_country": "Italy",
        "affiliation_id": "60023256",
        "affiliation_state": "MI"
    },
    {
        "paper_title": "Prescriptive Analytics for Commodity Procurement Applications",
        "publication": "Lecture Notes in Operations Research",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "In this work, we investigate the implications of commodity price uncertainty for optimal procurement and inventory control decisions. While the existing literature typically relies on the full information paradigm, i.e., optimizing procurement and inventory decisions under full information of the underlying stochastic price process, we develop and test different data-driven approaches that optimize decisions under very limited statistical model assumptions. Our results are data-driven policies and decision rules that can support commodity procurement managers, inventory managers as well as commodity merchants. We furthermore test all optimization models based on real data from different commodity classes (i.e., metals, energy and agricultural). This paper is a summary of the author’s dissertation (Mandl C. (2019). Optimal Procurement and Inventory Control in Volatile Commodity Markets - Advances in Stochastic and Data-Driven Optimization, [1]).",
        "DOI": "10.1007/978-3-031-08623-6_5",
        "paper_author": "Mandl C.",
        "affiliation_name": "Deggendorf Institute of Technology",
        "affiliation_city": "Deggendorf",
        "affiliation_country": "Germany",
        "affiliation_id": "60015176",
        "affiliation_state": "Bayern"
    },
    {
        "paper_title": "Facets of a Smart City: Computational and Experimental Techniques for Sustainable Urban Development",
        "publication": "Facets of a Smart City: Computational and Experimental Techniques for Sustainable Urban Development",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "A smart city uses technology to provide services and solve problems to improve urban policy efficiency, reduce waste, improve quality of life, and maximize social inclusion. By 2050, 66% of the world’s population is expected to be urban, which is a key driver of a global trend toward the creation of smart cities. This trend creates many opportunities for urban planning committees to learn how to design, modernize, and operate smart cities intelligently and effectively. Facets of a Smart City: Computational and Experimental Techniques for Sustainable Urban Development is a collection of topics that are relevant to the design of a smart city. This book aims to complement technical journal articles that require advanced knowledge of the subject of smart cities and applications for readers. It aims to bridge knowledge gaps in sustainable urban design by providing background information via case studies to facilitate students, recent graduates and new practitioners in urban design and planning. Key Features: - This book features 9 chapters that cover 6 major domains, which include (i) information modelling, (ii) internet of things, (iii) intelligent transportation systems, (iv) water supply, (v) waste management and (vi) sustainable environment - Computational techniques are included in the book. These include artificial neural networks, stochastic models, particle swarm optimization, machine learning, and adaptive neuro-fuzzy Inference systems - Goals of case studies presented in this book use computational techniques to offer readers examples of supervised, unsupervised and reinforcement learning strategies in the context of smart city applications.",
        "DOI": "10.2174/97898150490771220101",
        "paper_author": "Samui P.",
        "affiliation_name": "National Institute of Technology Patna",
        "affiliation_city": "Patna",
        "affiliation_country": "India",
        "affiliation_id": "60104351",
        "affiliation_state": "BR"
    },
    {
        "paper_title": "Artificial intelligence in the reduction and management of land pollution",
        "publication": "Current Trends and Advances in Computer-Aided Intelligent Environmental Data Engineering: A volume in Intelligent Data-Centric Systems",
        "citied_by": "2",
        "cover_date": "2022-01-01",
        "Abstract": "The Earth’s surface has been under pollution pressure due to intense agricultural activities and a host of other activities. Thus soil structure has been greatly degraded and polluted. For environmental safety, the reduction and management of land pollution are required. Conventional management methods need improvement to meet the current management demands. Artificial intelligence (AI) has been adopted recently in all areas of life as intelligent agents. It is an enhanced method of solving difficult and complex issues, with a large data-handling capacity. Therefore this chapter explores (1) the use of AI and robotics in system modification for effective on-the-spot minimization of wastes in process industries; (2) AI and robotics in the disposal and smart recycling of wastes; (3) impact of drones and neural network in reforestation; and (4) the use of machine learning in data curation for management policies that will encourage an all organic inclusive agriculture.",
        "DOI": "10.1016/B978-0-323-85597-6.00009-4",
        "paper_author": "Ejimofor M.I.",
        "affiliation_name": "Nnamdi Azikiwe University",
        "affiliation_city": "Awka",
        "affiliation_country": "Nigeria",
        "affiliation_id": "60021842",
        "affiliation_state": "Anambra"
    },
    {
        "paper_title": "Finite Sample Analysis of Minmax Variant of Offline Reinforcement Learning for General MDPs",
        "publication": "IEEE Open Journal of Control Systems",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "In this work, we analyze the finite sample complexity bounds for offline reinforcement learning with general state, general function space and state-dependent action sets. The algorithm analyzed does not require the knowledge of the data-collection policy as compared to earlier works. We show that one can compute an ϵ-optimal Q function (state-action value function) using O(1/ϵ4) i.i.d. samples of state-action-reward-next state tuples.",
        "DOI": "10.1109/OJCSYS.2022.3198660",
        "paper_author": "Regatti J.R.",
        "affiliation_name": "The Ohio State University",
        "affiliation_city": "Columbus",
        "affiliation_country": "United States",
        "affiliation_id": "60003500",
        "affiliation_state": "OH"
    },
    {
        "paper_title": "Next-Generation Technologies for Rare Inherited Disorders",
        "publication": "Omics Technologies for Clinical Diagnosis and Gene Therapy: Medical Applications in Human Genetics",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Rare inherited disorders have become a major public health concern in recent years. Owing to a lack of resources, poorly planned primary and basic health care, and inadequate political structures, treatment, and management policies are daunting challenges in many countries. As a result, these diseases need particular attention, especially in less developed areas, where these disorders remain unnoticed. Similarly, the effect of such severe disorders on underprivileged populations is expected to be devastating. Identifying certain genetic markers can provide a valuable explanation for disease etiology, molecular characterization, and pathogenesis. In this chapter, we highlight the importance of next-generation sequencing to explore and recognize the role of novel causative genes in developing successful treatments for the most prevalent rare genetic disorders. DNA methylation and transcriptome markers have been shown to aid in the prediction of common diseases; however, this has not been tested on rare genetic disorders. Since the rate of rare inherited disorders is higher in developing countries, we believe that these populations can provide us with much stronger clues for the genetic and environmental association. These markers, along with other parameters, can be used to systematically build machine learning models to improve risk prediction; this approach has the potential to reshape how we predict disease risk and save many lives around the world.",
        "DOI": "NA",
        "paper_author": "Kazmi H.",
        "affiliation_name": "Hazara University Pakistan",
        "affiliation_city": "Mansehra",
        "affiliation_country": "Pakistan",
        "affiliation_id": "60070606",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "On Improving the Robustness of Reinforcement Learning Policies against Adversarial Attacks",
        "publication": "Proceedings of the 32nd European Safety and Reliability Conference, ESREL 2022 - Understanding and Managing Risk and Reliability for a Sustainable Future",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "With deep neural networks as universal function approximators, the reinforcement learning paradigm has been adopted in several commonplace services such as autonomous vehicles, aircrafts and domestic assistance, which is raising new safety requirements. Indeed, a deep reinforcement learning agent obtains its states through observations, which may contain natural accuracy errors or malicious adversarial noises. Since the observations may diverge from the true environment states, they can lead the agent into taking risky suboptimal decisions. This vulnerability is well-known in computer vision literature where it has been emphasized via adversarial attacks. In terms of defense, various techniques have been proposed, including heuristic and certified methods, mainly to improve the robustness of deep neural networks-based classifiers. It is therefore necessary to propose solutions adapted to this learning challenge faced by reinforcement learning agents. In this paper, we propose two defense mechanisms based on reward shaping and adversarial training as a countermeasure against attacks on environment observations. The results reported from experiments conducted on autonomous vehicles controlled by reinforcement learning policies demonstrate that our approach successfully provide sufficient information to effectively learn the task in the context of highly perturbed environments. Furthermore, the defense mechanisms improve the robustness and generalization capacities of the learning models decreasing risky decisions in the presence of adversarial attacks.",
        "DOI": "10.3850/978-981-18-5183-4_S22-03-505-cd",
        "paper_author": "Jaafra Y.",
        "affiliation_name": "Expleo France",
        "affiliation_city": "Montigny-le-Bretonneux",
        "affiliation_country": "France",
        "affiliation_id": "124868722",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Global Efforts Towards Establishing Safety Directives for Intelligent Systems: Review",
        "publication": "Proceedings of the 32nd European Safety and Reliability Conference, ESREL 2022 - Understanding and Managing Risk and Reliability for a Sustainable Future",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Intelligent systems have found their way into our lives in almost every conceivable field. There is hardly any area in which the possibilities and applications of machine learning are not thought of. In safety-critical systems, there is an urgent need to examine how a model generates a prediction and whether that response can be trusted. Furthermore, ethics must be considered in the practical development of AI systems to ensure a safe and secure application. Despite these requirements, due to the nature of deep learning, we are confronted with a black box. This disparity needs to be addressed using interpretability and explainable approaches to minimize potential bias and at the same time increase transparency, fairness, justice and inclusion. In order to enhance trust in intelligent systems - accountability, responsibility and robustness must be ensured as well. Appropriate policies and standards need to be put in place to enforce this in practice. We are facing a global challenge here; standards must be set not only at national but also at international level, and a common understanding of how to deal with AI on ethical and legal levels must be found. We provide an overview of efforts that are being made at national and international level by governments and global organizations. We discuss current and upcoming challenges and risks posed by intelligent systems considering ethical guidelines and legal frameworks. In particular, we examine and compare the classification of risk levels and mitigation strategies. To conclude we show the latest state of technical feasibility and possible certification to ensure safe, transparent and robust AI systems and give an outlook on possible certification approaches for safe AI systems meeting the proposed governance frameworks.",
        "DOI": "10.3850/978-981-18-5183-4_S11-11-472-cd",
        "paper_author": "Frischknecht-Gruber C.M.L.",
        "affiliation_name": "ZHAW Zurich University of Applied Sciences",
        "affiliation_city": "Winterthur",
        "affiliation_country": "Switzerland",
        "affiliation_id": "60015769",
        "affiliation_state": "ZH"
    },
    {
        "paper_title": "Techno-economic analysis for decarbonising of container vessels",
        "publication": "Proceedings of the 32nd European Safety and Reliability Conference, ESREL 2022 - Understanding and Managing Risk and Reliability for a Sustainable Future",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "There are growing concerns on the effect of climate change and the environment. The share of shipping emissions in global anthropogenic emissions has increased from 2.76% in 2012 to 2.89% in 2018. In addition, the pace of carbon intensity reduction has slowed since 2015 with the average annual percentage changes ranging from 1 to 2%. The Levelised Cost of Mobility (LCOM) index is used to consider different options on a level field. This index comprises the CAPEX of the engines and tanks, the OPEX of the engines, the cost of the lost cargo space, fuel cost and CO2 cost. A Monte Carlo simulation is used to obtain the final unit of comparison of the LCOM which is expressed in Euros/1000DWT-km. The values utilised are sourced from literature review, or from a trained Artificial Neural Network (ANN) based on telemetry data of a 9000 TEU container vessel. Expected results are that LCOM values provide an indication of the cost that ship owners must bear to consider alternative fuels, or how policies may be invoked to encourage alternative fuels to be economically feasible to mineral fuels. Finally, given that vessels greater than 5000 gross tonnes must install fuel consumption sensors from 1 January 2019, this paper presents a framework on how telemetry data can be incorporated into a Machine Learning pipeline that can help answer specific business questions.",
        "DOI": "10.3850/978-981-18-5183-4_R10-01-148-cd",
        "paper_author": "Fam M.L.",
        "affiliation_name": "Singapore Institute of Technology",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore",
        "affiliation_id": "60105478",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Long Short-Term Memory ANNs for Fast Assessment oflnjection Policies ofWater- Flooding in Oil and Gas Reservoirs",
        "publication": "Proceedings of the 32nd European Safety and Reliability Conference, ESREL 2022 - Understanding and Managing Risk and Reliability for a Sustainable Future",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Models of the reservoirs are used in almost all phases of oil field management. The development of a physics-based model of a reservoir is an effort-intensive and time-consuming task. Even when available, the use of such a model can be computationally expensive. Thus, an emerging research direction is focusing on the use of Machine Learning (ML) to develop problem-specific reservoir models using real field data or datasets generated by model-based simulation. Long Short-Term Memory (LSTM) networks have been recently used due to their capabilities to capture temporal and spatial patterns from multivariate time series. However, they have been applied mainly to small oil fields and/or using simulation datasets with unrealistic characteristics. This paper investigates the efficient use of LSTMs networks to develop dynamic data-driven reservoir models. A procedure for simulating the complex physics-based reservoir model in order to generate input-output datasets of realistic characteristics is presented. The data are, then, used to train an ensemble of LSTM models, each one able to predict the future value of the oil or the water production rate at one production well, as a function of all the water injection rates at all the injection wells. These models are useful for the fast assessment of waterflooding policies with respect to their economic impact. The proposed framework is applied to the Olympus field benchmark case study showing good prediction accuracy and significant reduction in computational time with respect to the reservoir physics-based model.",
        "DOI": "10.3850/978-981-18-5183-4_S01-12-623-cd",
        "paper_author": "Di Federico G.",
        "affiliation_name": "Politecnico di Milano",
        "affiliation_city": "Milan",
        "affiliation_country": "Italy",
        "affiliation_id": "60023256",
        "affiliation_state": "MI"
    },
    {
        "paper_title": "A Multi-Scenario Crowd Data Synthesis Based On Building Information Modeling",
        "publication": "Proceedings of the 29th EG-ICE International Workshop on Intelligent Computing in Engineering",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Deep learning methods have proven to be effective in the field of crowd analysis recently. Nonetheless, the performance of deep learning models is affected by the inadequacy of training datasets. Because of policy implications and privacy restrictions, crowd data is commonly difficult to access. In order to overcome the difficulty of insufficient dataset, the previous work used to synthesize labelled crowd data in outdoor scenes and virtual games. However, these methods perform data synthesis with limited environmental information and inflexible crowd rules, usually in unauthentic environment. In this paper, a tool for synthesizing crowd data in BIM models with multiple scenes is proposed. This tool can make full use of the comprehensive information of real-world buildings, and conduct crowd simulations by setting behavior rules. The synthesized dataset is used for data augmentation for crowd analysis problems and the experimental results clearly confirm the effectiveness of the tool.",
        "DOI": "10.7146/aul.455.c223",
        "paper_author": "Huang H.",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60025278",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Towards a future sustainable housing stock Assessment of the energy performance of dwellings of non-profit housing associations",
        "publication": "A+BE Architecture and the Built Environment",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Climate change asks for urgent action. For decades humankind is aware of the impact of humanity on our environment. The Brundlandt report was a major wake-up call. Furthermore, the Kyoto Protocol and Paris Agreement are major stepping stones to mitigate the effect of climate change. But time marches on. Climate change is still today an urgent matter which needs our immediate attention. Legislation to battle climate change is in place at European and national levels. In Europe, among others, the Energy Performance of Buildings Directive (EPBD) is implemented to reduce the impact of the built environment on climate change. The energy transition in the built environment is a key strategy to mitigate the impact of our daily life on the environment. Dutch non-profit housing associations own one-third of the dwellings in the Netherlands. Other European countries with a large share of social housing are Austria 24%, Denmark 21%, Sweden 17%, UK 17%, France 16%, Norway 14%, and Finland 11%. Due to the share of the stock, changing the non-profit housing stock plays a vital role in the transition to a future sustainable housing stock. This research examines the energy performance of the housing stock of Dutch nonprofit housing associations. The energy performance of dwellings can be defined as the quality of dwellings in relation to the actual energy consumption during the operation phase of the dwelling. The aim of European and national policy is to improve the energy performance, and therewith to lower actual energy consumption, by building new dwellings with a good energy performance and by renovating the existing housing stock. Several scientific challenges appear in understanding and improving the energy performance of dwellings of Dutch non-profit housing associations. First, there is a need to monitor the progress of changes of the energy performance in the housing stock. Monitoring the energy performance of the housing stock helps in establishing a well-founded knowledge base, enabling the evaluation and adaptation of policies aimed at increasing the energy performance of the housing stock. Second, there is a need to improve theoretical calculations of the energy performance through modelling actual energy consumption. Currently, a performance gap exists between theoretical and actual energy consumption of dwellings. Improving the calculations helps in estimating the actual energy savings of renovations and in estimating the energy consumption of new dwellings. Third, there is a need to measure the energy performance of dwellings with heat pumps. Heat pumps are a promising solution for the future and gaining insights about different dwellings and systems helps in understanding the potential of heat pump systems in the Dutch non-profit housing stock. Fourth, there is a need to assist housing associations by benchmarking the energy performance of their housing stock in agreement with changes in policy after 2020. Creating a benchmark model helps in the measuring and evaluation of the energy performance towards a future sustainable housing stock. The aim of this thesis is to cover these challenges by assessing and understanding the improvement of the energy performance of dwellings of non-profit housing associations towards a future sustainable housing stock. The main research question of the study is therefore: How to assess and understand the improvement of the energy performance of dwellings of non-profit housing associations towards a future sustainable housing stock? This is operationalized with four studies: – Study 1: Monitoring energy performance improvement: insights from Dutch housing association dwellings – Study 2: The energy performance of dwellings of Dutch non-profit housing associations: modelling actual energy consumption – Study 3: The energy performance of dwellings with heat pumps of Dutch nonprofit housing associations – Study 4: Benchmarking energy performance: indicators and models for Dutch housing associations Study 1 assesses the energy performance progress between 2017 and 2020. It contributes to the understanding of the improvement of the energy performance of dwellings of non-profit housing associations by giving insights into the development of the housing stock, the effect of changes of and within the stock, the effect of characteristics of housing associations and by relating the improvement of the energy performance to the sectoral goal. Study 2 assesses models to estimate actual energy consumption of dwellings. It contributes to the understanding of the energy performance of dwellings of non-profit housing associations by giving insights into the extent to which advanced modelling of the energy consumption can improve the estimation of energy savings of renovation measures. Study 3 assesses the energy performance of dwellings with heat pumps. It contributes to the understanding of the energy performance of dwellings of non-profit housing associations by giving insights into the energy performance of dwellings with heat pumps as a promising renovation measure towards a future sustainable housing stock. Study 4 assesses the benchmarking of the energy performance. It contributes to the understanding of the energy performance of dwellings of non-profit housing associations by creating a benchmark model related to the changing policy context, therewith contributing to the understanding and improvement of the energy performance of dwellings of nonprofit housing associations towards a future sustainable housing stock. Research methods; Two main data sources are used to answer the research questions in this thesis, the Shaere-database and microdata of energy consumption on address-level from the Dutch Central Bureau of Statistics (CBS). The Shaere-database is a database of energy performance data and building characteristics of over two million dwellings of non-profit housing associations, collected annually under coordination of this research project and maintained by Aedes, the umbrella organization of Dutch nonprofit housing associations. The CBS collects and maintains an annual database of actual energy consumption of Dutch addresses, available in an anonymized analysis environment. The combination of these databases is the main basis for the studies performed in this thesis. Several research methods are used to perform the four studies in this thesis. The collection of raw data, consisting of the energy performance and building characteristics of dwellings of non-profit housing associations is a basis for all studies. Studies 1 and 3, mainly consist of statistical analyses of raw data to answer the stated research questions. In study 2, besides raw data collection and statistical analysis, advanced modelling techniques are applied to reach the desired research results. A linear, a non-linear and a Gradient Boosting Model (GBM) are examined. In study 4, also raw data collections and statistical analysis were used, but action research where the researcher participated in group sessions with experts from nonprofit housing associations is the main research method. Results Study 1, monitoring energy performance improvement: insights from Dutch housing association dwellings, helps to understand the improvement of the energy performance by which measures are taken and which potential is left to renovate or to replace with new construction. The research shows that the energy performance of dwellings of Dutch non-profit housing associations improved steadily between 2017 and 2020. The research shows that the effect of changes of the stock (construction and demolition) to the improvement of the average energy performance is modest (15.6%). The improvement of the average sectoral energy performance happens for 85.4% within the existing stock, mostly with traditional improvements like changing heating installations and adding insulation. The research shows that large urban housing associations drive the improvement of the average sectoral energy performance. The research concludes that the sectoral goal of an average energy-index of 1.40 in 2020 will not be achieved in the year 2020 but can be achieved at the end of 2021. Study 2, the energy performance of dwellings of Dutch non-profit housing associations: modelling actual energy consumption, shows that modelling the actual energy consumption helps to understand the effectiveness of renovation measures in lowering actual energy consumption and related CO2 emissions. The research shows a large performance gap between theoretical and actual energy consumption underlining the need for actual energy consumption modelling. However, the research shows that modelling the actual energy consumption of dwellings is challenging. The actual energy consumption was modelled with three different models, a linear regression, a non-linear regression and a GBM machine learning model. The research shows that the three different models have their own pros and cons. Linear regression models are simple and fast and estimate sectoral cross-sections very well but are not useful in analyzing the effects of detailed renovation measures. A non-linear model can estimate sectoral cross-sections and detailed renovations and uses the structure of actual consumption physics but is only able to use given relations between building features and will therefore not pick up on other relations which could improve the estimations of the effects of renovations. The non-linear model is easier to interpret, which could be a reason to prefer such a model above the other models. A Gradient Boosting Model is able to detect all kinds of relations between building features. It can find correlations and interactions that even specialists in the field are not aware of. However, the model does not use the structure of actual energy consumption physics to its advantage. Therefore, it is more difficult to interpret the results and if some renovation measures (e.g. electrical heat pumps) occur less frequently in the dataset this can result in outcomes that are unrealistic. The research recommends that combining theoretical models with empirical calibrations (grey box models) could be used to enhance the accuracy of estimations of the energy performance. Study 3, the energy performance of dwellings with heat pumps of Dutch nonprofit housing associations, shows that a statistical analysis to assess the energy performance of dwellings with heat pumps helps to understand the potential of heat pump systems in the future Dutch non-profit housing stock. In the research, the characteristics and the average actual energy consumption of dwellings with heat pumps are determined and compared to dwellings with a traditional HR107 condensing gas boiler. 3.2% of the dwellings of non-profit housing associations operates with a heat pump, consisting of all-electric heat pump systems (1.2%), hybrid systems (0.8%), gas absorption heat pumps (0.6%), gas absorption hybrid systems (0.4%) and other configurations (0.2%). Dwellings with all-electric heat pumps have an average higher building quality, no gas consumption, and higher electricity consumption, as opposed to dwellings with hybrid or gas absorption heat pumps, which have an average higher building quality, lower gas consumption, and higher electricity consumption as opposed to dwellings with a traditional HR107 gas boiler. Detailed insights are provided for dwellings with different heat pump systems and for dwellings with different building characteristics. Study 4, benchmarking energy performance: indicators and models for Dutch housing associations, shows that a model to benchmark the energy performance of Dutch non-profit housing associations can be created by following a structured approach. Benchmarking is a method that can be used to measure progress and to create awareness about the performance of organizations. The benchmark model helps to support housing associations to analyse and compare the energy performance of their housing stock in agreement with active policies. Other researchers aiming at benchmarking the energy performance between organizations within their policy context, can adopt and adapt this structured approach. The final policy performance model to measure and benchmark the energy performance of Dutch non-profit housing associations consists of three indicators closely related to active policies regarding the sustainable improvement of the Dutch non-profit housing sector: (1) The average theoretical primary fossil energy consumption, (2) the average distance to the maximum theoretical heating demand, and (3) the average actual CO2 emissions from gas consumption. The first indicator is related to the current policy regarding the energy labelling of dwellings, derived from the EPBD, the NTA8800. The second indicator relates to the policy to decrease the average theoretical heat demand of dwellings. The third indicator is related to the goal for the Dutch built environment to lower actual CO2 emissions. Conclusion and recommendations The main research question of the thesis is: How to assess and understand the improvement of the energy performance of dwellings of non-profit housing associations towards a future sustainable housing stock? The studies performed in this thesis show that in order to assess the energy performance of non-profit housing associations a systematic data collection method is vital. The studies are based on the SHAERE database with the energy performance characteristics of over two million dwellings collected annually and the actual energy consumption of those dwellings available in an anonymized environment at the Dutch Central Bureau of Statistics (CBS). To assess the data different analytical methods help to deliver insights. In this research, a monitoring system, advanced modelling techniques, statistical analysis of data, and a benchmark model are used. These techniques help to gain valuable insights to understand the improvement of the energy performance of dwellings of non-profit housing associations. – Monitoring the energy performance progress helps to understand the improvement of the energy performance of dwellings of non-profit housing associations by giving insights into the development of the housing stock, the effect of changes of and within the stock, the effect of characteristics of housing associations and by relating the improvement of the energy performance to the sectoral goal. – Using advanced modelling techniques to estimate actual energy consumption of dwellings contributes to the understanding of the extent to which a linear model, a non-linear model and a Gradient Boosting Model (GBM) can improve the estimation of the energy consumption of dwellings and therewith the energy savings of renovation measures.",
        "DOI": "NA",
        "paper_author": "van der Bent H.S.",
        "affiliation_name": "Universiteit Twente",
        "affiliation_city": "Enschede",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60020599",
        "affiliation_state": "Overijssel"
    },
    {
        "paper_title": "Forecasting the total market value of a shares traded in the Shenzhen stock exchange via the neural network",
        "publication": "Economics Bulletin",
        "citied_by": "28",
        "cover_date": "2022-01-01",
        "Abstract": "Stock total market value forecasting is a significant issue for policy makers and investors. This study explores usefulness of the nonlinear autoregressive neural network for this forecasting problem in a dataset of the daily total market value of A shares traded in the Shenzhen Stock Exchange during January 4, 2016–August 23, 2021. Through examining various model settings across the algorithm, delay, hidden neuron, and data splitting ratio, the model leading to generally accurate and stable performance is reached. Usefulness of the machine learning technique for the total market value forecasting problem of the A shares is illustrated. Results here might be used on a standalone basis as technical forecasts or combined with fundamental forecasts to form perspectives of total market value trends and perform policy analysis.",
        "DOI": "NA",
        "paper_author": "Xu X.",
        "affiliation_name": "NC State University",
        "affiliation_city": "Raleigh",
        "affiliation_country": "United States",
        "affiliation_id": "60004923",
        "affiliation_state": "NC"
    },
    {
        "paper_title": "Improving the application of Reinforcement Learning for load shifting in a cooling system through state-of-the-art algorithms and hyper-parameter optimization",
        "publication": "Proceedings of ECOS 2022 - 35th International Conference on Efficiency, Cost, Optimization, Simulation and Environmental Impact of Energy Systems",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Increasing volatility in electricity generation makes an increasing degree of flexibility on the consumer side necessary. There are many methods to solve the resulting optimization problem, all of which have their own advantages and disadvantages. Rule-based Control is unsuitable for optimal integration of dynamic constraints, and the promising Model Predictive Control comes with a high engineering overhead. Reinforcement Learning (RL), a subfamily of Machine Learning, has therefore been established as another method in research. In a previous work, we have demonstrated how two RL algorithms can be used for load shifting in a simulated cooling network with three consumer sites, under dynamic constraints. The continuous control DDPG (Deep Deterministic Policy Gradient) algorithm achieved slightly better results compared to the discrete control DQN (Deep Q-Network) algorithm. However, both were generally suitable for the application. In this work, we now investigate how the training and the final control performance change when recent innovations from RL research are used for both algorithms. For the continuous case, a SAC (Soft-Actor-Critic) algorithm is used and the discrete controlling DQN receives two additional design extensions. In addition, a Bayesian hyper-parameter optimization is performed for both algorithms. We can show that the training time is reduced several magnitudes compared to the last study while maintaining the same performance. After completion of training, both algorithms can save about 13 % of the weekly operating costs through load shifting, while maintaining the required temperature limits. Thus, the application to real-world problems becomes more feasible if the latest innovations from RL research are used.",
        "DOI": "NA",
        "paper_author": "Schreiber T.",
        "affiliation_name": "Rheinisch-Westfälische Technische Hochschule Aachen",
        "affiliation_city": "Aachen",
        "affiliation_country": "Germany",
        "affiliation_id": "60016653",
        "affiliation_state": "Nordrhein-Westfalen"
    },
    {
        "paper_title": "Extreme events and climate change. How did wildfires and other stress events affect the social and policy aspects of the energy transition? A machine learning mass media article analysis",
        "publication": "Proceedings of ECOS 2022 - 35th International Conference on Efficiency, Cost, Optimization, Simulation and Environmental Impact of Energy Systems",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Extreme events that unfolded in 2021 because of the intense weather conditions around the globe are a potent reminder that climate change is happening now. Wildfires in south Europe and California floods in west Europe, and hurricane Ida, are only a glimpse of our near future. According to the latest pledges from COP26 in Glasgow, the optimistic scenario is to keep the temperature increase of the planet around 2.1°C degrees, far from Paris Agreement and any other international agreement goals. Besides the environmental impacts, these stress events could potentially have economic and societal consequences. How are citizens and their acceptance of alternative technologies affected by them? What is the reaction from the mass media? How can uncertainty arising from these events potentially affect investments in renewable energy? Analyzing articles from mass media by applying web-scrapping and Natural Language Processing (NLP), we searched for a possible association between disasters and acceptance of alternative technologies. Based on our findings, stress events positively affect the approval of new technologies, such as wildfires in southern Europe and floods in Germany and Belgium. Our results can serve as a guideline to policymakers for better and more accurate decision-making, to identify when and how new policies should be introduced to the public. We anticipate further extending our methods with additional machine learning algorithms and quantifying the acceptance of specific technologies. Finally, other stress events unrelated to weather conditions include elections, ratification of policy acts, and international meeting agreements.",
        "DOI": "NA",
        "paper_author": "Varelas P.",
        "affiliation_name": "Université Libre de Bruxelles",
        "affiliation_city": "Brussels",
        "affiliation_country": "Belgium",
        "affiliation_id": "60000145",
        "affiliation_state": "BRU"
    },
    {
        "paper_title": "Emerging Artificial Intelligence and Machine Learning Techniques and Their Application to Corporate Management and Decision Making in Agriculture",
        "publication": "Artificial Intelligence and Digital Diversity Inclusiveness in Corporate Restructuring",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Academia and industry are exploring the application potential of Artificial Intelligence (AI) and Machine Learning (ML) in different areas, i.e., Decision Support Systems (DSS), market research, customer segmentation, enhancement of operational efficiencies, estimation of customer churn rate and factors causing churn, dynamic pricing, sentiment analysis, automated surveillance, product recommendation, chatbots, forecasting and so on. Freedom from rigid assumptions makes AI and ML more expedient and flexible than many conventional statistical tools and techniques. ML techniques and algorithms are being evaluated and tested constantly to develop better versions of the existing ones. Some of the common algorithms in ML techniques like Neural Networks (NN), Decision Trees, XGBoost, K-Nearest Neighbour (KNN), fuzzy logic, rough sets, logistic regression or a combination of them are being employed for forecasting and decision making in management. ML is also used to forecast weather, agricultural production, the incidence of pests and diseases, animal behaviour, price and demand of agriculture and non-agricultural products. Society can be benefited to a great extent from numerous applications of AI in the arena of agriculture, business, research and development (R&D), and policy formulations but several limitations such as unavailability of data, higher initial investments, lesser expertise and skill in the field are to be addressed.",
        "DOI": "NA",
        "paper_author": "Anirudh K.C.",
        "affiliation_name": "ICAR - National Dairy Research Institute",
        "affiliation_city": "Karnal",
        "affiliation_country": "India",
        "affiliation_id": "60012257",
        "affiliation_state": "HR"
    },
    {
        "paper_title": "Machine Learning Based Admission Data Processing for Early Forecasting Students' Learning Outcomes",
        "publication": "International Journal of Data Warehousing and Mining",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "In this paper, the authors explore the factors to improve the accuracy of predicting student learning outcomes. The method can remove redundant and irrelevant factors to get a “clean” data set without having to solve the NP-Hard problem. The method can improve the graduation outcome prediction accuracy through logistic regression machine learning method for “clean” data set. They empirically evaluate the training and university admission data of Hanoi Metropolitan University from 2016 to 2020. From data processing results and the support from the machine learning techniques application program, they analyze, evaluate, and forecast students’ learning outcomes based on admission data, first-year, and second-year academic performance data. They then submit proposals of training and admission policies and methods of radically and quantitatively solving problems in university admissions.",
        "DOI": "10.4018/IJDWM.313585",
        "paper_author": "Son N.T.K.",
        "affiliation_name": "Hanoi Metropolitan University",
        "affiliation_city": "Hanoi",
        "affiliation_country": "Viet Nam",
        "affiliation_id": "112741404",
        "affiliation_state": "Hanoi"
    },
    {
        "paper_title": "How to Utilize Data Visualization Method to Analyze Information Systems Related Medical Errors",
        "publication": "28th Americas Conference on Information Systems, AMCIS 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Medical errors, such as misdiagnosis, incorrect drug dispensing, surgical injuries, even patient name errors, or misuse computer information systems in healthcare systems can cause serious consequences to patients. A meta-analysis estimates that medical errors cause approximately 22,000 preventable deaths in the United State each year (Rodwin et al., 2020). To help reduce medical errors and enhance patient safety, The Agency for Healthcare Research and Quality (AHRQ), one of twelve agencies within the United States Department of Health and Human Services, have created repository of medical error cases, which are often reports and case studies authored by clinical professionals (e.g., physicians, nurses, and hospital managers). These narratives provide patients and peer healthcare professionals with valuable lessons regarding the causes, contexts, and consequences of various medical errors, and some are following up with comments or suggestions from experienced professionals. In addition, those medical reports often have many attributes subjectively tagged by the report authors such as Computer Information Systems related, EHR related, or Telemedicine related. Some research has applied machine-learning methods (e.g., BERT) to mine medical error reports (Xu et al. 2021). The present study focuses on the attributes of medical error reports and seeks to identify and visualize the correlation between types of medical errors and the types of information systems related errors, and hope to provide patients, clinical professionals, healthcare administrators, and policy makers with straightforward illustrations to understand the medical error issues. We have acquired over 500 medical error reports from AHRQ, from 2003 to 2021. The attributes of these reports include case title, error types, clinical area, safety target, target audience, setting of care, approach to improve safety, etc. Medical errors are categorized into seven major types: Active Errors, Cognitive Errors (\"Mistakes\"), Epidemiology of Errors and Adverse Events, Latent Errors, Near Miss, Non-cognitive Errors (\"Slips & Lapses\"), and Other. Our preliminary study explores all the medical error reports with one analysis focusing on the error cases which can be improved by clinical information systems related approaches, such as Computerized Adverse Event Detection (CAED), Computer-Assisted Therapy (CAT), Clinical Information Systems (CIS), Computerized Decision Support (CDS), Computerized Provider Order Entry (CPOE), Electronic Health Records (EHR), and Telemedicine. The preliminary result (see the chart below) shows that in each of the seven error types, EHR is a major type of approach to improve in six out of seven error categories.",
        "DOI": "NA",
        "paper_author": "HaoFirst H.",
        "affiliation_name": "Bentley University",
        "affiliation_city": "Waltham",
        "affiliation_country": "United States",
        "affiliation_id": "60103124",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Coastal Resilience Decision Making with Machine Learning",
        "publication": "28th Americas Conference on Information Systems, AMCIS 2022",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "Our research aims to understand how social data can be integrated with climate data using machine learning for coastal resilience decisions. Although data analytics techniques have been adapted for decision models, incorporating unstructured data is a challenge. We adapt a design science research approach to develop a coastal resilience decision model that can accommodate various sets of climate criteria and social attributes to help us understand coastal risks in communities vulnerable to coastal hazards. We collected social data from environmental groups and individuals and conducted an exploratory social media data analysis on coastal resilience in the greater Boston, U.S., area. We employ non-negative matrix factorization (NMF), a topic modeling technique, to extract human-interpretable topics from a preliminary dataset of 131 documents from 50 different accounts. The outcomes of this research can help community members and policy makers understand and develop robust sustainability and climate focused decisions.",
        "DOI": "NA",
        "paper_author": "Lee C.",
        "affiliation_name": "Northeastern University",
        "affiliation_city": "Boston",
        "affiliation_country": "United States",
        "affiliation_id": "60028628",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Contracting and Contract Law in the Age of Artificial Intelligence",
        "publication": "Contracting and Contract Law in the Age of Artificial Intelligence",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "This book provides original, diverse, and timely insights into the nature, scope, and implications of Artificial Intelligence (AI), especially machine learning and natural language processing, in relation to contracting practices and contract law. The chapters feature unique, critical, and in-depth analysis of a range of topical issues, including how the use of AI in contracting affects key principles of contract law (from formation to remedies), the implications for autonomy, consent, and information asymmetries in contracting, and how AI is shaping contracting practices and the laws relating to specific types of contracts and sectors. The contributors represent an interdisciplinary team of lawyers, computer scientists, economists, political scientists, and linguists from academia, legal practice, policy, and the technology sector. The chapters not only engage with salient theories from different disciplines, but also examine current and potential real-world applications and implications of AI in contracting and explore feasible legal, policy, and technological responses to address the challenges presented by AI in this field. The book covers major common and civil law jurisdictions, including the EU, Italy, Germany, UK, US, and China. It should be read by anyone interested in the complex and fast-evolving relationship between AI, contract law, and related areas of law such as business, commercial, consumer, competition, and data protection laws.",
        "DOI": "NA",
        "paper_author": "Ebers M.",
        "affiliation_name": "Tartu Ülikool",
        "affiliation_city": "Tartu",
        "affiliation_country": "Estonia",
        "affiliation_id": "60068856",
        "affiliation_state": "Tartumaa"
    },
    {
        "paper_title": "How Do People View COVID-19 Vaccines: Analyses on Tweets About COVID-19 Vaccines Using Natural Language Processing and Sentiment Analysis",
        "publication": "Journal of Global Information Management",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "The COVID-19 pandemic has been the most devastating public health crisis in the recent decade and vaccination is anticipated as the means to terminate the pandemic. People’s views and feelings over COVID-19 vaccines determine the success of vaccination. This study was set to investigate sentiments and common topics about COVID-19 vaccines by machine learning sentiment and topic analyses with natural language processing on massive tweets data. Findings revealed that concern on COVID-19 vaccine grew alongside the introduction and start of vaccination programs. Overall positive sentiments and emotions were greater than negative ones. Common topics include vaccine development for progression, effectiveness, safety, availability, sharing of vaccines received, and updates on pandemics and government policies. Outcomes suggested the current atmosphere and its focus over the COVID-19 vaccine issue for the public health sector and policymakers for better decision-making. Evaluations on analytical methods were performed additionally.",
        "DOI": "10.4018/JGIM.300817",
        "paper_author": "Chang V.",
        "affiliation_name": "Aston University",
        "affiliation_city": "Birmingham",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60014551",
        "affiliation_state": "West Midlands"
    },
    {
        "paper_title": "The Sentiments of Indonesian Urban Citizens Regarding the Lockdown-Like Policy During the COVID-19 Pandemic: A Path Towards an Urban E-Planning Process in a Pandemic Situation INTRODUCTION",
        "publication": "International Journal of E-Planning Research",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "Indonesia implemented large scale social restrictions (LSSR) to cope with COVID-19. This policy generated various opinions between people that demonstrated their sentiments in daily life and social media. This study aims to analyse the sentiments of Indonesian urban citizens with respect to the policy. Using Drone Emprit Academic, a data mining tool that retrieved data from Twitter, this study examined tweets that mentioned “Pembatasan Sosial Berskala Besar” or “PSBB” from March 2nd, 2020 to January 11th, 2021. It reveals that significant events in the real world influence the sentiments pattern and classification during that particular period. The spatial distribution of the tweets reveals that the conversation is concentrated in cities throughout Java. Twitter-based sentiment analysis can be an alternative method for the government to monitor and evaluate its policies in the future, specifically in a pandemic situation.",
        "DOI": "10.4018/IJEPR.297515",
        "paper_author": "Putra Z.D.W.",
        "affiliation_name": "Universitas Gadjah Mada",
        "affiliation_city": "Yogyakarta",
        "affiliation_country": "Indonesia",
        "affiliation_id": "60069380",
        "affiliation_state": "Yogyakarta"
    },
    {
        "paper_title": "Early warning of critical transition in energy and stock market",
        "publication": "Energy Proceedings",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Early warning is an important and challenging issue in governmental policy-making. This study proposes a skillful spillover network-based machine learning model to provide early warnings of critical transition in energy and stock markets. First, the critical transition of stock and energy time series can be detected using a hidden Markov model. Second, a dynamic spillover network is established, which can help to understand the characteristics of return volatility from the perspective of the time-varying structure of spillover relationships. A machine learning algorithm is employed to model the early warning of critical transition based on the topological structures of the network. The results demonstrated that the proposed model can identify the early warning of critical transition with the warning day, e.g., one day or thirty days, with a high generalization ability. Our study enriches critical transition research and can offer important warning signals for policy-makers and market investors.",
        "DOI": "10.46855/energy-proceedings-10028",
        "paper_author": "An S.",
        "affiliation_name": "School of Economics and Management",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60170215",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Policy Forum: Technology and the Evolving Workforce",
        "publication": "Canadian Tax Journal",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Globally, we are seeing record numbers of people leaving their jobs and in many cases making significant career changes. In tax, this is resulting in a shrinking talent pool, making it even harder to find people with the necessary skills to fill these gaps. At the same time, the requirements and demands imposed on tax professionals are increasing and expanding. In this article, the authors look at how tax technologists and technological solutions such as machine learning can help.",
        "DOI": "10.32721/ctj.2022.70.2.pf.cooke",
        "paper_author": "Cooke S.",
        "affiliation_name": "KPMG International",
        "affiliation_city": "Amstelveen",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60083246",
        "affiliation_state": "Noord-Holland"
    },
    {
        "paper_title": "Retraction: Tracking Military soldiers Location and Monitoring Health using Machine Learning and LORA model (2022 IEEE 2nd Mysore Sub Section International Conference (MysuruCon) (2022) DOI: 10.1109/MysuruCon55714.2022.9972391)",
        "publication": "MysuruCon 2022 - 2022 IEEE 2nd Mysore Sub Section International Conference",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "After careful and considered review of the content of this article by a duly constituted expert committee, this article has been found to not comply with IEEE publication principles. Specifically, the authorship credit for the article appeared to have violated IEEE policy requiring authors to have made significant intellectual contribution to the theoretical development, system or experimental design, prototype development, and/or the analysis and interpretation of data associated with the work contained in the article. Therefore, IEEE has retracted the content of this article from Xplore. When notified of this retraction, the authors did not provide a response.",
        "DOI": "10.1109/MysuruCon55714.2022.10478461",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Retraction: A Multiple Linear Regression Model for Crop Production using Machine Learning and Neural Network (2022 IEEE 2nd Mysore Sub Section International Conference (MysuruCon) (2022) DOI: 10.1109/MysuruCon55714.2022.9972651)",
        "publication": "MysuruCon 2022 - 2022 IEEE 2nd Mysore Sub Section International Conference",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "After careful and considered review of the content of this article by a duly constituted expert committee, this article has been found to not comply with IEEE publication principles. Specifically, the authorship credit for the article appeared to have violated IEEE policy requiring authors to have made significant intellectual contribution to the theoretical development, system or experimental design, prototype development, and/or the analysis and interpretation of data associated with the work contained in the article. Therefore, IEEE has retracted the content of this article from Xplore. When notified of this retraction, the authors did not provide a response.",
        "DOI": "10.1109/MysuruCon55714.2022.10478496",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Retraction: Biomedical Images Analysis for Disease Diagnosis using Sensor based Machine Learning Model (2022 IEEE 2nd Mysore Sub Section International Conference (MysuruCon) (2022) DOI: 10.1109/MysuruCon55714.2022.9972711)",
        "publication": "MysuruCon 2022 - 2022 IEEE 2nd Mysore Sub Section International Conference",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "After careful and considered review of the content of this article by a duly constituted expert committee, this article has been found to not comply with IEEE publication principles. Specifically, the authorship credit for the article appeared to have violated IEEE policy requiring authors to have made significant intellectual contribution to the theoretical development, system or experimental design, prototype development, and/or the analysis and interpretation of data associated with the work contained in the article. Therefore, IEEE has retracted the content of this article from Xplore. When notified of this retraction, the authors did not provide a response.",
        "DOI": "10.1109/MysuruCon55714.2022.10478685",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Retraction: Lightweight Machine Learning Algorithm for Automatic Detection of Diabetic Retinopathy in IoT (2022 IEEE 2nd Mysore Sub Section International Conference (MysuruCon) (2022) DOI: 10.1109/MysuruCon55714.2022.9972501)",
        "publication": "MysuruCon 2022 - 2022 IEEE 2nd Mysore Sub Section International Conference",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "After careful and considered review of the content of this article by a duly constituted expert committee, this article has been found to not comply with IEEE publication principles. Specifically, the authorship credit for the article appeared to have violated IEEE policy requiring authors to have made significant intellectual contribution to the theoretical development, system or experimental design, prototype development, and/or the analysis and interpretation of data associated with the work contained in the article. Therefore, IEEE has retracted the content of this article from Xplore. When notified of this retraction, the authors did not provide a response.",
        "DOI": "10.1109/MysuruCon55714.2022.10478696",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Retraction: An Novel Approach for Predicting and Analyzing Diabetes using Machine Learning Technique (2022 IEEE 2nd Mysore Sub Section International Conference (MysuruCon) (2022) DOI: 10.1109/MysuruCon55714.2022.9972649)",
        "publication": "MysuruCon 2022 - 2022 IEEE 2nd Mysore Sub Section International Conference",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "After careful and considered review of the content of this article by a duly constituted expert committee, this article has been found to not comply with IEEE publication principles. Specifically, the authorship credit for the article appeared to have violated IEEE policy requiring authors to have made significant intellectual contribution to the theoretical development, system or experimental design, prototype development, and/or the analysis and interpretation of data associated with the work contained in the article. Therefore, IEEE has retracted the content of this article from Xplore. When notified of this retraction, the authors did not provide a response.",
        "DOI": "10.1109/MysuruCon55714.2022.10478497",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Machine Learning and Exploratory Data Analysis in Cross-Sell Insurance",
        "publication": "Encyclopedia of Data Science and Machine Learning",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Data is playing a central role in the insurance industry. The current journey of insurance industry is conquered by data collection to make future decisions since this is the digital era of the insurance industry in its journey of 700+ years. This chapter focuses on exploratory data analysis (EDA) to identify significant and critical factors to develop business strategy as well as to predict customers' responses in cross-sell health insurance. Response is either acceptance or rejection of a health insurance product offered to existing customers, who may or may not hold policies with the company. Exploratory data analysis (EDA) presents data analysis and visualization from various lookouts to characterize data that can help the insurer in strategic decision making.",
        "DOI": "10.4018/978-1-7998-9220-5.ch039",
        "paper_author": "Rustamji A.J.",
        "affiliation_name": "Institute of Technology",
        "affiliation_city": "Tekanpur",
        "affiliation_country": "India",
        "affiliation_id": "131053163",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Deep Residual Policy Reinforcement Learning as a Corrective Term in Process Control for Alarm Reduction: A Preliminary Report",
        "publication": "Proceedings of the 32nd European Safety and Reliability Conference, ESREL 2022 - Understanding and Managing Risk and Reliability for a Sustainable Future",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Conventional process controllers (such as proportional integral derivative controllers and model predictive controllers) are simple and effective once they have been calibrated for a given system. However, it is difficult and costly to re-tune these controllers if the system deviates from its normal conditions and starts to deteriorate. Recently, reinforcement learning has shown a significant improvement in learning process control policies through direct interaction with a system, without the need of a process model or the system characteristics, as it learns the optimal control by interacting with the environment directly. However, developing such a black-box system is a challenge when the system is complex and it may not be possible to capture the complete dynamics of the system with just a single reinforcement learning agent. Therefore, in this paper, we propose a simple architecture that does not replace the conventional proportional integral derivative controllers but instead augments the control input to the system with a reinforcement learning agent. The agent adds a correction factor to the output provided by the conventional controller to maintain optimal process control even when the system is not operating under its normal condition.",
        "DOI": "10.3850/978-981-18-5183-4_S33-07-668-cd",
        "paper_author": "Abbas A.N.",
        "affiliation_name": "Software Competence Center Hagenberg",
        "affiliation_city": "Hagenberg",
        "affiliation_country": "Austria",
        "affiliation_id": "60013284",
        "affiliation_state": "Upper Austria"
    },
    {
        "paper_title": "HyTM-AP Hybrid Transactional Memory Scheme Using Abort Prediction and Adaptive Retry Policy for Multi-Core In-Memory Databases",
        "publication": "Journal of Database Management",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Recently, works on integrating HTM with STM, called hybrid transactional memory (HyTM), have been intensively studied. However, the existing works consider only the prediction of a conflict between two transactions and provide a static HTM configuration for all workloads. To solve the problems, the authors propose a hybrid transactional memory scheme based on both abort prediction and an adaptive retry policy, called HyTM-AP. First, the HyTM-AP can predict not only conflicts between concurrently running transactions, but also the capacity and other aborts of transactions by collecting the information of transactions previously executed. Second, the HyTM-AP can provide an adaptive retry policy based on machine learning algorithms, according to the characteristic of a given workload. Finally, through the experimental performance analysis using the STAMP benchmark, the HyTM-AP shows 12-13% better performance than the existing HyTM schemes.",
        "DOI": "10.4018/JDM.299555",
        "paper_author": "Kim H.J.",
        "affiliation_name": "Jeonbuk National University",
        "affiliation_city": "Jeonju",
        "affiliation_country": "South Korea",
        "affiliation_id": "60001187",
        "affiliation_state": "Jeollabuk-do"
    },
    {
        "paper_title": "Assessing the Impact of Virtual Health Communities and Environmental Characteristics of Chronic Pain Mobile Health Apps on Users' Privacy Decisions: A Multilevel Perspective",
        "publication": "28th Americas Conference on Information Systems, AMCIS 2022",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "Chronic pain has been identified as one of the most widespread health-related problems. Potential chronic pain apps users seek health communities for current and previous reviews to assess the quality of the apps and make a decision regarding disclosing their information to these apps. In this study, we present a multilevel perspective on how virtual health communities and environmental characteristics of chronic pain mobile health apps impact users' privacy decisions. We used Exploratory Data Analysis and Machine Learning (ML) to operationalize the Theory of Multilevel Information Privacy. The results revealed that the most influential factors affecting users' cost-benefit analysis are Chronic Pain MHA's characteristics related to user's information privacy. The ML results indicate that the existence of information privacy policy can be predicted through the ways the apps use to Collect Data, App's Category, Country, and Store Type, which in turn affect users' decisions.",
        "DOI": "NA",
        "paper_author": "Bantan M.",
        "affiliation_name": "Saudi Electronic University",
        "affiliation_city": "Riyadh",
        "affiliation_country": "Saudi Arabia",
        "affiliation_id": "60110529",
        "affiliation_state": "Ar Riyad"
    },
    {
        "paper_title": "The Impact of COVID-19 on Crime: A Study from the Spatial-temporal Perspective in the Montgomery County, AL",
        "publication": "International Conference on Agents and Artificial Intelligence",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "The policies curbing the spread of COVID-19 can influence the chance of committing a crime. This study aimed to investigate the impacts of COVID-19 on the spatial and temporal patterns of crime in Montgomery City, AL, by wavelet analysis, spatial point test, and machine learning tools. We obtained the crime case records between January 1, 2015 to March 12, 2021 from the police department in the City of Montgomery, and we downloaded demographical data from the U.S. Census. Results show that the overall crime rate in Montgomery decreased during the COVID-19 pandemic. However, crime rates would increase in a shorter time than COVID-19 confirmed cases when the social activities increased. Meanwhile, spatial distributions of simple assault, burglary, and vehicle theft had clustered in Montgomery business and shopping areas. These findings are helpful for the police institution in preventing and minimizing crimes as new COVID-19 variants emerge in the future.",
        "DOI": "10.5220/0010856700003116",
        "paper_author": "Ma L.",
        "affiliation_name": "Troy University",
        "affiliation_city": "Troy",
        "affiliation_country": "United States",
        "affiliation_id": "60000164",
        "affiliation_state": "AL"
    },
    {
        "paper_title": "14th International Conference on Agents and Artificial Intelligence , ICAART 2022",
        "publication": "International Conference on Agents and Artificial Intelligence",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "The proceedings contain 246 papers. The special focus in this conference is on Agents and Artificial Intelligence. The topics include: Allocation Considering Agent Importance in Constrained Robust Multi-Team Formation; multi Robot Surveillance and Planning in Limited Communication Environments; an Interactive Environment to Support Agent-based Graph Programming; coordinated Collision-free Movement of Groups of Agents; online Inference of Robot Navigation Parameters from a Semantic Map; NC4OMAS: A Norms-based Approach for Open Multi-Agent Systems Controllability; LSTM-based Abstraction of Hetero Observation and Transition in Non-Communicative Multi-Agent Reinforcement Learning; requirements Engineering Challenges and Techniques in Building Chatbots; machine-learned Behaviour Models for a Distributed Behaviour Repository; “Robot Steganography”: Opportunities and Challenges; LIMP: Incremental Multi-agent Path Planning with LPA; operationalizing Behavior Change Techniques in Conversational Agents; reinforcement Learning-based Real-time Fair Online Resource Matching; narrative Economics of the Racetrack: An Agent-Based Model of Opinion Dynamics in In-play Betting on a Sports Betting Exchange; coordination Mechanisms with Misinformation; machine-checked Verification of Cognitive Agents; an Inspection Technique Proposal for the Verification of Requirements Specification Documents for Multi-Agent Systems; How Does AI Play Football? An Analysis of RL and Real-world Football Strategies; a Mechanism for Multi-unit Multi-item Commodity Allocation in Economic Networks; highways in Warehouse Multi-Agent Path Finding: A Case Study; multi-agent Policy Gradient Algorithms for Cyber-physical Systems with Lossy Communication; supporting the Adaptation of Agents’ Behavioral Models in Changing Situations by Presentation of Continuity of the Agent’s Behavior Model; statistical Model Checking for Probabilistic Temporal Epistemic Logics; model Analysis of Human Group Behavior Strategy using Cooperative Agents.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "14th International Conference on Agents and Artificial Intelligence , ICAART 2022",
        "publication": "International Conference on Agents and Artificial Intelligence",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "The proceedings contain 246 papers. The special focus in this conference is on Agents and Artificial Intelligence. The topics include: Allocation Considering Agent Importance in Constrained Robust Multi-Team Formation; multi Robot Surveillance and Planning in Limited Communication Environments; an Interactive Environment to Support Agent-based Graph Programming; coordinated Collision-free Movement of Groups of Agents; online Inference of Robot Navigation Parameters from a Semantic Map; NC4OMAS: A Norms-based Approach for Open Multi-Agent Systems Controllability; LSTM-based Abstraction of Hetero Observation and Transition in Non-Communicative Multi-Agent Reinforcement Learning; requirements Engineering Challenges and Techniques in Building Chatbots; machine-learned Behaviour Models for a Distributed Behaviour Repository; “Robot Steganography”: Opportunities and Challenges; LIMP: Incremental Multi-agent Path Planning with LPA; operationalizing Behavior Change Techniques in Conversational Agents; reinforcement Learning-based Real-time Fair Online Resource Matching; narrative Economics of the Racetrack: An Agent-Based Model of Opinion Dynamics in In-play Betting on a Sports Betting Exchange; coordination Mechanisms with Misinformation; machine-checked Verification of Cognitive Agents; an Inspection Technique Proposal for the Verification of Requirements Specification Documents for Multi-Agent Systems; How Does AI Play Football? An Analysis of RL and Real-world Football Strategies; a Mechanism for Multi-unit Multi-item Commodity Allocation in Economic Networks; highways in Warehouse Multi-Agent Path Finding: A Case Study; multi-agent Policy Gradient Algorithms for Cyber-physical Systems with Lossy Communication; supporting the Adaptation of Agents’ Behavioral Models in Changing Situations by Presentation of Continuity of the Agent’s Behavior Model; statistical Model Checking for Probabilistic Temporal Epistemic Logics; model Analysis of Human Group Behavior Strategy using Cooperative Agents.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "14th International Conference on Agents and Artificial Intelligence , ICAART 2022",
        "publication": "International Conference on Agents and Artificial Intelligence",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "The proceedings contain 246 papers. The special focus in this conference is on Agents and Artificial Intelligence. The topics include: Allocation Considering Agent Importance in Constrained Robust Multi-Team Formation; multi Robot Surveillance and Planning in Limited Communication Environments; an Interactive Environment to Support Agent-based Graph Programming; coordinated Collision-free Movement of Groups of Agents; online Inference of Robot Navigation Parameters from a Semantic Map; NC4OMAS: A Norms-based Approach for Open Multi-Agent Systems Controllability; LSTM-based Abstraction of Hetero Observation and Transition in Non-Communicative Multi-Agent Reinforcement Learning; requirements Engineering Challenges and Techniques in Building Chatbots; machine-learned Behaviour Models for a Distributed Behaviour Repository; “Robot Steganography”: Opportunities and Challenges; LIMP: Incremental Multi-agent Path Planning with LPA; operationalizing Behavior Change Techniques in Conversational Agents; reinforcement Learning-based Real-time Fair Online Resource Matching; narrative Economics of the Racetrack: An Agent-Based Model of Opinion Dynamics in In-play Betting on a Sports Betting Exchange; coordination Mechanisms with Misinformation; machine-checked Verification of Cognitive Agents; an Inspection Technique Proposal for the Verification of Requirements Specification Documents for Multi-Agent Systems; How Does AI Play Football? An Analysis of RL and Real-world Football Strategies; a Mechanism for Multi-unit Multi-item Commodity Allocation in Economic Networks; highways in Warehouse Multi-Agent Path Finding: A Case Study; multi-agent Policy Gradient Algorithms for Cyber-physical Systems with Lossy Communication; supporting the Adaptation of Agents’ Behavioral Models in Changing Situations by Presentation of Continuity of the Agent’s Behavior Model; statistical Model Checking for Probabilistic Temporal Epistemic Logics; model Analysis of Human Group Behavior Strategy using Cooperative Agents.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Identifying drivers of residential energy consumption by explainable energy demand forecasting",
        "publication": "Eceee Summer Study Proceedings",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Reducing primary and final energy demand is crucial for substantially decreasing Greenhouse Gas (GHG) emissions and reaching global and European climate targets. To understand what drives energy consumption behaviour is crucial for decision-makers to design policies effectively. Most current studies focus on smart meter data and sensor data. These data should be complemented by contextual, sociological, and behavioural data (acquired for example through surveys), which allow to study more precise user profiles. Integrating user profiles may reveal more valuable information, at the same time too much redundant information may also harm the prediction accuracy. How to select the crucial drivers is still understudied, but has direct impacts on the performance of the prediction. This paper presents an explainable three-step forecasting method, which identifies long-term as well as seasonal trends and the most important drivers of household energy demand. In the first step, times series analysis (Bayesian Structural Time Series - BSTS) is applied to decompose energy demands into long-term, seasonal and residual components. In the second step, features are selected through a hybrid machine-learning approach (combining Extreme Gradient Boosting - XGBoost and Random Forest - RF), which reveals the key drivers of energy consumption. Finally, the energy consumption for each household is predicted with a deep-learning algorithm (Long Short Term Memory - LSTM). Furthermore, drivers of household energy demand − covering energy usage, building information and user profiles − are extracted and validated by domain experts. We apply this approach to a real-world dataset collected in eight German cities. The results demonstrate significant improvement in the prediction accuracy of electricity demand and interpretability of drivers.",
        "DOI": "NA",
        "paper_author": "Jiao J.",
        "affiliation_name": "Fraunhofer Institute for Systems and Innovation Research ISI",
        "affiliation_city": "Karlsruhe",
        "affiliation_country": "Germany",
        "affiliation_id": "60033451",
        "affiliation_state": "Baden-Wurttemberg"
    },
    {
        "paper_title": "PAMMELA: Policy Administration Methodology using Machine Learning",
        "publication": "Proceedings of the International Conference on Security and Cryptography",
        "citied_by": "6",
        "cover_date": "2022-01-01",
        "Abstract": "In recent years, Attribute-Based Access Control (ABAC) has become quite popular and effective for enforcing access control in dynamic and collaborative environments. Implementation of ABAC requires the creation of a set of attribute-based rules which cumulatively form a policy. Designing an ABAC policy ab initio demands a substantial amount of effort from the system administrator. Moreover, organizational changes may necessitate the inclusion of new rules in an already deployed policy. In such a case, re-mining the entire ABAC policy requires a considerable amount of time and administrative effort. Instead, it is better to incrementally augment the policy. In this paper, we propose PAMMELA, a Policy Administration Methodology using Machine Learning to assist system administrators in creating new ABAC policies as well as augmenting existing policies. PAMMELA can generate a new policy for an organization by learning the rules of a policy currently enforced in a similar organization. For policy augmentation, new rules are inferred based on the knowledge gathered from the existing rules. A detailed experimental evaluation shows that the proposed approach is both efficient and effective.",
        "DOI": "10.5220/0011272400003283",
        "paper_author": "Gumma V.",
        "affiliation_name": "Indian Institute of Technology Madras",
        "affiliation_city": "Chennai",
        "affiliation_country": "India",
        "affiliation_id": "60025757",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Inflation Forecasting in India: Are Machine Learning Techniques Useful?",
        "publication": "Reserve Bank of India Occasional Papers",
        "citied_by": "2",
        "cover_date": "2022-01-01",
        "Abstract": "The COVID-19 pandemic and the associated supply chain disruptions have impacted not just the inflation dynamics but also the performance of inflation forecasting models. Traditional econometric models with their implicit assumption of linear as well as time-invariant relationship between the target variable and explanatory variables have been questioned for long, resulting in the emergence of alternative models and techniques to better capture the changing inflation dynamics. This paper uses machine learning (ML) based forecasting techniques to capture the possible non-linear relationships between inflation and its determinants and compare their forecasting performance with some of the popular traditional time series models for both the pre-COVID and post-COVID periods. The empirical results suggest performance gains in using ML-based techniques over traditional ones in forecasting inflation in India over different forecast horizons.",
        "DOI": "NA",
        "paper_author": "Singh N.",
        "affiliation_name": "Bank of India",
        "affiliation_city": "Mumbai",
        "affiliation_country": "India",
        "affiliation_id": "60076407",
        "affiliation_state": "MH"
    },
    {
        "paper_title": "Insights into fish-anthropogenic pressures relationships using machine learning techniques: the case of Castilla-La Mancha (Spain)",
        "publication": "Proceedings of the IAHR World Congress",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Freshwater fish biodiversity is experiencing an alarming decline worldwide and anthropogenic activities are considered to be major drivers of ecosystem degradation. The aim of this study is to identify the causes of the decline of native species richness in Castilla-La Mancha, in central Spain. The analysis of a fish dataset spanning two 20-year periods reveals a strong decrease of native species richness after 2001 in almost 80% of studied sites. The Random Forest algorithm was applied to model the response of different ecologically-based metrics of fish species richness to different stressors variables. According to the results, implementing environmental flows able to mitigate the alteration of high flows could strongly support native species richness and could also help in counteracting the presence of alien limnophilic species. By contrast, only fish eradication seems to be effective to contrast the spread of alien rheophilic species. This study provides new, quantitative insights into pressures-ecosystem relationships in rivers and reveals the main factors that lead to the decline of fish richness in Castilla-La Mancha, which could help inform environmental policy initiatives.",
        "DOI": "10.3850/IAHR-39WC2521716X2022127",
        "paper_author": "Valerio C.",
        "affiliation_name": "Universidad Complutense de Madrid",
        "affiliation_city": "Madrid",
        "affiliation_country": "Spain",
        "affiliation_id": "60027282",
        "affiliation_state": "Madrid"
    },
    {
        "paper_title": "Towards Multi-agent Reinforcement Learning using Quantum Boltzmann Machines",
        "publication": "International Conference on Agents and Artificial Intelligence",
        "citied_by": "2",
        "cover_date": "2022-01-01",
        "Abstract": "Reinforcement learning has driven impressive advances in machine learning. Simultaneously, quantum-enhanced machine learning algorithms using quantum annealing underlie heavy developments. Recently, a multi-agent reinforcement learning (MARL) architecture combining both paradigms has been proposed. This novel algorithm, which utilizes Quantum Boltzmann Machines (QBMs) for Q-value approximation has outperformed regular deep reinforcement learning in terms of time-steps needed to converge. However, this algorithm was restricted to single-agent and small 2x2 multi-agent grid domains. In this work, we propose an extension to the original concept in order to solve more challenging problems. Similar to classic DQNs, we add an experience replay buffer and use different networks for approximating the target and policy values. The experimental results show that learning becomes more stable and enables agents to find optimal policies in grid-domains with higher complexity. Additionally, we assess how parameter sharing influences the agents’ behavior in multi-agent domains. Quantum sampling proves to be a promising method for reinforcement learning tasks, but is currently limited by the Quantum Processing Unit (QPU) size and therefore by the size of the input and Boltzmann machine.",
        "DOI": "10.5220/0010762100003116",
        "paper_author": "Müller T.",
        "affiliation_name": "Ludwig-Maximilians-Universität München",
        "affiliation_city": "Munich",
        "affiliation_country": "Germany",
        "affiliation_id": "60028717",
        "affiliation_state": "Bayern"
    },
    {
        "paper_title": "Research on Routing Optimization in Satellite Internet Based on Deep Reinforcement Learning",
        "publication": "Space-Integrated-Ground Information Networks",
        "citied_by": "3",
        "cover_date": "2022-01-01",
        "Abstract": "With the rapid development of satellite communication, the satellite internet is one of the core technologies of 6G network to realize global coverage, full-time access and full scene service. The high dynamics and limited capacity of satellite network lead to a series of management and control challenges such as heterogeneous network management, dynamic resource allocation and so on. Since the machine learning-based technologies have strength in network design, the intelligent architecture of software-defined satellite internet was put forward. In view of the intelligent routing in satellite internet, and leverages the deep reinforcement algorithm based on double delayed deep deterministic policy gradient (TD3) to solve the network routing optimization problem. The experimental results showed that compared with DDPG algorithm, the TD3 algorithm reduced the delay by 19.19%.",
        "DOI": "10.11959/j.issn.2096-8930.2022033",
        "paper_author": "Wei L.",
        "affiliation_name": "Beijing University of Posts and Telecommunications",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60016930",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "AIGEL 2022 - Proceedings of Selected Papers of the Workshop on Artificial Intelligence Governance Ethics and Law",
        "publication": "CEUR Workshop Proceedings",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "The proceedings contain 15 papers. The topics discussed include: supporting the combating of financing of weapons of mass destruction with AI technologies; generative AI and the rule of law; big techs and antitrust: lessons from a transatlantic comparison; how does a data code become taxable? a brief view from a Spanish law perspective; towards a European law on cooperative, connected and automated mobility (CCAM); privacy compliance with ontologies and blockchain: the OntoROPA Project; old ghosts in the age of ai: the foundations of liberal democracy and its identity crisis; the double-effect principle: from Thomas Aquinas to its current meaning; the use of agent-based simulation of public policy design to study the value alignment problem; and AML/CFT/CPF endeavors in the crypto-space: from blockchain analytics to machine learning.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Construction of a Support Tool for User Reading of Privacy Policies and Assessment of its User Impact",
        "publication": "International Conference on Information Systems Security and Privacy",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "Today’s service providers must notify users of their privacy policies and obtain user consent in advance. Frameworks that impose these requirements have become mandatory. Originally designed to protect user privacy, obtaining user consent in advance has become a mere formality. These problems are introduced by the gap between service providers’ privacy policies, which prioritize the observance of laws and guidelines, and user expectations of these policies. In particular, users wish to easily understand how their data will be handled. To reduce this gap, we provide a tool that supports users in reading privacy policies in Japanese. We assess the effectiveness of the tool in experiments and follow-up questionnaires.",
        "DOI": "10.5220/0010847500003120",
        "paper_author": "Kanamori S.",
        "affiliation_name": "National Institute of Information and Communications Technology",
        "affiliation_city": "Koganei",
        "affiliation_country": "Japan",
        "affiliation_id": "60032325",
        "affiliation_state": "Tokyo"
    },
    {
        "paper_title": "Forecasting and modeling on average rainwater and vapor pressure in Chelyabinsk Russia using deep Learning models",
        "publication": "IET Conference Proceedings",
        "citied_by": "3",
        "cover_date": "2022-01-01",
        "Abstract": "Machine learning technique is full-fledged as a boosting sector to develop modeling and forecasting of complex time series observations in the present environment. This study made an attempt to inspect the future performance of rainfall data and vapor in Chelyabinsk by using a machine learning technique. The data series is divided into a training set (60%) and a test set (40%) for model developing a validation purpose. We further developed deep learning models such as, LSTM, BILSTM, GRU and compared on the basis of ME, RMSE, MAE, MPE, MAPE, ACF1 on the training data set. For testing data set, we compared these deep learning models based on RMSE. LSTM model acts as a superior machine learning model over BILSTM and GRU in this data series. Forecasting performance of these three models significantly at par. This finding may be significant to build a strong literature of the Chelyabinsk weather's forecast, which can be helpful for policy makers and researchers. Also, we strongly believe that, this work could be used as a literature adaptation of machine learning technique for complex time series over statistical models.",
        "DOI": "10.1049/icp.2023.0582",
        "paper_author": "Abotaleb M.",
        "affiliation_name": "South Ural State University",
        "affiliation_city": "Chelyabinsk",
        "affiliation_country": "Russian Federation",
        "affiliation_id": "60008009",
        "affiliation_state": "Chelyabinsk Oblast"
    },
    {
        "paper_title": "Enhancing Sentiment Analysis Performance with Blend of Features and Deep Learning Algorithms",
        "publication": "IET Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "In Sentiment analysis, writer's view about products, events, government policies, services, topics, organizations, and individuals is determined through the text written by them on social media platforms like Twitter, and Facebook. Previous researchers focused on getting sentiment either from syntactic and lexical features or by using pre-trained word embeddings. We have combined GloVe pre-trained word embedding,lexicon based features, and Bag of words to form an ensemble feature(FE). We have considered two Twitter datasets namely, US Airlines Twitter, and Stanford Twitter Sentiment Gold (STSGd). We have applied three machine learning algorithms on TF-IDF features, three deep learning algorithms on GloVe word embedding feature. We have proposed a hybrid model which is a combination of LSTM and CNN model with ensemble feature. The proposed model has performed around 2% better on US Airlines Twitter dataset and around 8% better on STS-Gold dataset than state-of-the-art-results. It is observed that ensemble feature improved the performance of algorithms.",
        "DOI": "10.1049/icp.2022.0583",
        "paper_author": "Ahuja R.",
        "affiliation_name": "Indian Institute of Technology Roorkee",
        "affiliation_city": "Roorkee",
        "affiliation_country": "India",
        "affiliation_id": "60031818",
        "affiliation_state": "UK"
    },
    {
        "paper_title": "Optimisation on pollutants emission control with developing machine learning algorithms for advanced statistical performance monitoring",
        "publication": "IET Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Nitrogen Oxides (NOx) emission from power generation is one of the major air pollution sources in Hong Kong. In support of the Government’s environmental policy and the transition from coal-fired to gas-fired generation, CLP Power Hong Kong Limited (CLP Power) has increased the proportion of natural gas, which is a relatively clean fossil fuel, to around 50% in its fuel mix from 2020 and met the stringent emission cap set by the Government and achieved substantial decreases in air emissions compared with the 2019 levels. Current CLP Power NOx emission forecast methodology for the power generation machines adopted calculating the planned annual fuel consumption and the NOx emission factor. The NOx annual emission can always be achieved within the targeted cap. However, the emission gap existed between the actual and the year-end projection amount which the difference affected by the operating factors such as actual load demand and fuel consumption. This study paper is to develop a NOx emission forecast model based on the power generation operating parameters and are specific for the gas-fired Black Point Power Station (BPPS) of CLP Power. The machine learning model eXtreme Gradient Boosting (XGBoost) are adopted to predict NOx emission performance of gas-fired generation unit C1 at BPPS, which the model is trained with historical operating parameters of high correlation. The model adopted are practical with sound accuracy improvement in NOx emission performance prediction comparing with existing forecast methodology. This study in result providing the benefit on fuel cost saving in utilising coal for power generation.",
        "DOI": "10.1049/icp.2023.0067",
        "paper_author": "Cheuk K.N.",
        "affiliation_name": "Hong Kong University of Science and Technology",
        "affiliation_city": "Hong Kong",
        "affiliation_country": "Hong Kong",
        "affiliation_id": "60008592",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Framework for Integrating Responsible AI into Social Media Platforms",
        "publication": "IET Conference Proceedings",
        "citied_by": "2",
        "cover_date": "2022-01-01",
        "Abstract": "The development of social media has led to calls for increased data oversight when moderating online content to reduce harms to potentially vulnerable users and groups. An analysis of the drivers of data governance and social harm was undertaken to highlight the issues faced by social media companies when moderating content, including human interpretation; mediated protections; and machine learning content classification, to determine how protection can be implemented by those who make commercial social media enterprises their business model. This work outlines a framework for the governance of social media websites that incorporates a socio-technical model of content moderation. A high-level system architecture for the moderation of content is proposed that accounts for the contextual nature of social harms and behaviours, which may demand human interpretation, combined with a proposed ML system that can trawl through the massive amounts of information such networks produce. The framework allows the maintenance and creation of policy goals to minimise the risk of harm whilst ensuring that the privacy and protection needs of vulnerable stakeholders are maintained.",
        "DOI": "10.1049/icp.2022.2051",
        "paper_author": "Singh S.",
        "affiliation_name": "Warwick Business School",
        "affiliation_city": "Coventry",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60115484",
        "affiliation_state": "West Midlands"
    },
    {
        "paper_title": "Free Text to Standardized Concepts to Clinical Decisions",
        "publication": "Encyclopedia of Data Science and Machine Learning",
        "citied_by": "2",
        "cover_date": "2022-01-01",
        "Abstract": "This article discusses the establishment of interoperability among electronic medical records from 800 clinical sites and the use of machine learning for best practice discovery. A novel extraction-mapping algorithm is designed that accurately extracts, summarizes, and maps free text and content to concise structured medical concepts. Clinical decision processes and disease progression are also generated. The machine learning model (DAMIP) uncovers discriminatory feature sets that can predict the quality of treatment outcomes (blind prediction accuracies of 89%-97%) for multiple diseases including heart, hypertension, and chronic kidney disease (CKD). For each disease, the best practice was used at fewer than 5% of the clinical sites, opening up excellent opportunities for knowledge sharing and rapid learning. This work led to the implementation of a new treatment policy for CKD pre-dialysis care management. The new policy offers better outcomes, saves lives, improves the quality of life, and reduces 35% of treatment costs. The system is scalable and generalizable.",
        "DOI": "10.4018/978-1-7998-9220-5.ch028",
        "paper_author": "Lee E.K.",
        "affiliation_name": "Georgia Institute of Technology",
        "affiliation_city": "Atlanta",
        "affiliation_country": "United States",
        "affiliation_id": "60019647",
        "affiliation_state": "GA"
    },
    {
        "paper_title": "Tor Network Traffic Classification Using Machine Learning Based on Time-Related Feature",
        "publication": "IET Conference Proceedings",
        "citied_by": "6",
        "cover_date": "2022-01-01",
        "Abstract": "Application-level defensive security controls like Intrusion Detection and Prevention Systems (IDS/IPS) need to have visibility on the application data portion of the processed network packets to classify network traffic based on protocol and application information and then to make a proper decision against the traffic based on the defined policy. This functionality is required to check the different passing data streams against fingerprints that identify different protocols and applications. Fingerprints can be defined based on signatures, rules, and patterns of the protocols and applications or based on behaviors and deviations from a predefined baseline. The Onion Router (Tor) is one of the most powerful techniques to keep the confidentiality of data and the anonymity of user identities. Tor establishes encrypted tunnels over the distributed network, making it hard for traffic detection appliances to recognize the going Tor sessions. This paper aims to enable traffic detection and analysis appliances to recognize Tor traffic depending on time-related only attributes with high accuracy and low latency by proposing a statistical technique for analyzing and extracting features represented in the network packets. The proposed statistical technique is built based on computing the correlations between the features among only time-related attributes and dropping those with the lowest effectiveness and importance in labeling and classifying data instances and then utilizing Machine Learning to recognize Tor traffic.",
        "DOI": "10.1049/icp.2023.0354",
        "paper_author": "Al-Fayoumi M.",
        "affiliation_name": "Princess Sumaya University",
        "affiliation_city": "Amman",
        "affiliation_country": "Jordan",
        "affiliation_id": "60056335",
        "affiliation_state": "Amman"
    },
    {
        "paper_title": "Advances in model-based reinforcement learning for Adaptive Optics control",
        "publication": "Proceedings of SPIE - The International Society for Optical Engineering",
        "citied_by": "7",
        "cover_date": "2022-01-01",
        "Abstract": "Direct imaging of Earth-like exoplanets is one of the significant scientific drivers of the next generation of ground-based telescopes. Typically, Earth-like exoplanets are located at tiny angular separations from their host stars rendering their identification difficult. Consequently, the adaptive optics (AO) system's control algorithm must be carefully designed to distinguish the exoplanet from the residual light produced by the host star. A new promising avenue of research aimed at improving AO control builds on data-driven control methods such as Reinforcement Learning (RL) methods. It is an active branch of the machine learning research field, where control of a system is learned through interaction with the environment. Thus, RL can be seen as an automated approach for AO control. In particular, model-based reinforcement learning (MBRL) has been shown to cope with both temporal and misregistration errors. Similarly, it has been demonstrated to adapt to non-linear wavefront sensing while being efficient to train and execute. In this work, we implement and adapt an RL method called Policy Optimizations for AO (PO4AO) to the GHOST test bench at ESO headquarters, where we show strong performance on cascaded AO system lab simulation. Further, the results align with the previously obtained results with the method.",
        "DOI": "10.1117/12.2630317",
        "paper_author": "Nousiainen J.",
        "affiliation_name": "LUT University",
        "affiliation_city": "Lappeenranta",
        "affiliation_country": "Finland",
        "affiliation_id": "60014304",
        "affiliation_state": "South Karelia"
    },
    {
        "paper_title": "Performance Reliability of Reinforcement Learning Algorithms in Obstacle Avoidance Game with Differing Reward Formulations",
        "publication": "Proceedings - 2022 International Conference on Computational Science and Computational Intelligence, CSCI 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "When formulating environments for complex application areas, using analogies to games is beneficial as they provide convenient models to test algorithm performance in ways that are transferable to realistic environments. We propose a Frogger like grid based environment containing a simple action space, dynamic obstacles, and discrete game loop for testing Proximal Policy Optimization 2 and Deep Q-Network with comparisons to a heuristic and random agent. The environment contains four different reward function implementations, along with two different environment variations to explore adaptability. Seeing how these different parameters effect not just the average performance of the algorithm, but also the reliability of the performance is of concern as reliability determines the expectations of any single performance of an reinforcement learning agent. Experiments in these environments demonstrate common behaviors of reinforcement learning algorithms showing possible strengths and weakness of the approaches when applied to more complex decision-making scenarios. We explore these behaviors through evaluation techniques meant to measure the agents accumulation of reward in the game. Cross comparing these evaluation techniques elucidates the causation behind agent performance.",
        "DOI": "10.1109/CSCI58124.2022.00022",
        "paper_author": "Hansen B.",
        "affiliation_name": "U.S. Army Engineer Research and Development Center",
        "affiliation_city": "Vicksburg",
        "affiliation_country": "United States",
        "affiliation_id": "60008926",
        "affiliation_state": "MS"
    },
    {
        "paper_title": "Understanding Historical, Socio-Economic, and Policy Contributions to COVID-19 Health Inequities",
        "publication": "Proceedings of the International ISCRAM Conference",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "The COVID-19 pandemic has generated unprecedented, devastating impacts across the United States. However, some communities have disproportionately endured adverse health outcomes and socioeconomic injuries. Ascertaining the factors driving these inequities is crucial to determining how policy could mitigate the impacts of future public health crises. We have established research-driven metrics, aggregated as the Community Vulnerability Index (CVI), that quantify vulnerability to public health and economic impacts of COVID-19. We performed two analyses to better understand similarities between communities in terms of the vulnerabilities represented by the metrics. We performed an unsupervised k-means clustering analysis to understand whether communities can be grouped together based on their levels of negative social and health indicators. Our goal for this analysis is to determine whether attributes of the constructed clusters reveal areas of opportunity for potential policy impacts and future disaster response efforts. We also analyzed similarities between communities across time using time-sensitive clustering analysis to discover whether historical community vulnerabilities were persistent in the years preceding the pandemic and to better understand the historical factors associated with disparate COVID-19 impacts. In particular, we highlight where communities should invest based on their historical health and socioeconomic patterns and related COVID impacts. Through extensive interpretation of our findings, we uncover how health policy can advance equity and improve community resilience.",
        "DOI": "NA",
        "paper_author": "Thais S.",
        "affiliation_name": "Princeton University",
        "affiliation_city": "Princeton",
        "affiliation_country": "United States",
        "affiliation_id": "60003269",
        "affiliation_state": "NJ"
    },
    {
        "paper_title": "Proceedings - 2022 5th Asia Conference on Machine Learning and Computing, ACMLC 2022",
        "publication": "Proceedings - 2022 5th Asia Conference on Machine Learning and Computing, ACMLC 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "The proceedings contain 22 papers. The topics discussed include: a data-driven pricing strategy for automobile insurance policies; a comparison of clustering method to determine depot location for a bike-sharing operation; mobile-based navigation assistant for visually impaired person with real-time obstacle detection using YOLO-based deep learning algorithm; PiXelNet: a DL-based method for diagnosing lung cancer using the histopathological images; rough set model and approximations in fuzzy formal contexts; an evolutionary strategy based training optimization of supervised machine learning algorithms (EStoTimeSMLAs); forecasting for wind farm energy output in South Australia: a comparative analysis of physical methods and deep learning methods; and natural language processing in advertising – a systematic literature review.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Data-Driven Pricing Strategy for Automobile Insurance Policies",
        "publication": "Proceedings - 2022 5th Asia Conference on Machine Learning and Computing, ACMLC 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "An automobile insurance policy premium depends on three factors, the risk associated with the drivers and cars on the policy, the operational costs to manage the policy and the profit margin. The premium is then some function of these. Operational costs are dependent on the company efficiency. The achieved profit margin is dependent on the competition experienced. Risk, however, is a customer dependent factor and hence premiums should take into account potential risk of a new policy. Traditionally, risk tables are used to compute the risk of a new customer but we instead use historical data to predict the average claim amount that would be made on a new policy in the coming year if it was approved. We use this value, as a measure of risk, to better determine the premium that is charged. We illustrate the approach with a single customer feature, the age of the driver, but the approach can be used to take into account several customer and/or car features.",
        "DOI": "10.1109/ACMLC58173.2022.00009",
        "paper_author": "Hosein P.",
        "affiliation_name": "The University of the West Indies, St. Augustine Campus",
        "affiliation_city": "St Augustine",
        "affiliation_country": "Trinidad and Tobago",
        "affiliation_id": "60071706",
        "affiliation_state": "Tunapuna–Piarco"
    },
    {
        "paper_title": "KI-JP 2022 - Joint Proceedings of Workshops, Tutorials and Doctoral Consortium, co-located with the 45th German Conference on Artificial Intelligence, KI 2022",
        "publication": "CEUR Workshop Proceedings",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "The proceedings contain 15 papers. The topics discussed include: evolutionary counterfactual visual explanation; self-explaining variational gaussian processes for transparency and modelling of prior knowledge; study on criteria for explainable AI for laypeople; imitation learning of logical program policies for multi-agent reinforcement learning; mining interesting outlier subgraphs in attributed graphs; multi-perspective anomaly detection on bipartite multi-layer social interaction networks; German to English: fake news detection with machine translation; IRT2: inductive linking and ranking in knowledge graphs of varying scale; explaining hate speech classification with model-agnostic methods; model transformation in description logics; and conference paper assignment problem - a new system for recommending and assigning reviewers to scientific articles.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Adaptive-TTA: accuracy-consistent weighted test time augmentation method for the uncertainty calibration of deep learning classifiers",
        "publication": "BMVC 2022 - 33rd British Machine Vision Conference Proceedings",
        "citied_by": "5",
        "cover_date": "2022-01-01",
        "Abstract": "Building deep machine learning systems to classify image data in real-world applications requires not only a quantification of the accuracy of the models but also an understanding of their reliability. With this in mind, the uncertainty calibration of Deep Neural Networks in the task of image classification is addressed in this work. We propose a novel technique based on test time augmentation, called Adaptive-TTA, that - unlike traditional test time augmentation approaches - improves uncertainty calibration without affecting the model's accuracy. This technique is evaluated with respect to the Brier score - a proper scoring rule for measuring the calibration of predicted probabilities - on the classical CIFAR-10/CIFAR-100 computer vision datasets, as well as on the benchmark satellite imagery dataset AID, using different augmentation policies. Our approach outperforms temperature scaling, a state-of-the-art post-hoc calibration technique, on all the three aforementioned datasets.",
        "DOI": "NA",
        "paper_author": "Conde P.",
        "affiliation_name": "University of Coimbra, Institute of Systems and Robotics",
        "affiliation_city": "Coimbra",
        "affiliation_country": "Portugal",
        "affiliation_id": "60106429",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Data-Driven Machine Learning Model Performance of Real Annual Natural Gas Consumption in Residential Buildings",
        "publication": "ASHRAE and IBPSA-USA Building Simulation Conference",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "To achieve climate neutrality by 2050, Building-Stock Energy Models (BSEMs) are key tools in comparing competing building energy reduction strategies. Yet, at present, existing regulatory energy performance calculation methods poorly estimate the real building energy use and widely overestimates the potential energy savings. Promising data-driven machine learning models, such as gradient boosting machines and support vector machines are gaining considerable traction in a wide range of applications. In this paper, we will evaluate the performance of common data-driven black-box models and evaluate whether they could potentially replace the present regulatory calculation method for prediction and/or policy making.",
        "DOI": "NA",
        "paper_author": "Van Hove M.",
        "affiliation_name": "Universiteit Gent",
        "affiliation_city": "Ghent",
        "affiliation_country": "Belgium",
        "affiliation_id": "60033316",
        "affiliation_state": "VOV"
    },
    {
        "paper_title": "Algorithmic Harm in Consumer Markets",
        "publication": "Journal of Legal Analysis",
        "citied_by": "5",
        "cover_date": "2022-01-01",
        "Abstract": "Machine learning algorithms are increasingly able to predict what goods and services particular people will buy, and at what price. It is possible to imagine a situation in which relatively uniform, or coarsely set, prices and product characteristics are replaced by far more in the way of individualization. Companies might, for example, offer people shirts and shoes that are particularly suited to their situations, that fit with their particular tastes, and that have prices that fit their personal valuations. In many cases, the use of algorithms promises to increase efficiency and to promote social welfare; it might also promote fair distribution. But when consumers suffer from an absence of information or from behavioral biases, algorithms can cause serious harm. Companies might, for example, exploit such biases in order to lead people to purchase products that have little or no value for them or to pay too much for products that do have value for them. Algorithmic harm, understood as the exploitation of an absence of information or of behavioral biases, can disproportionately affect members of identifiable groups, including women and people of color. Since algorithms exacerbate the harm caused to imperfectly informed and imperfectly rational consumers, their increasing use provides fresh support for existing efforts to reduce information and rationality deficits, especially through optimally designed disclosure mandates. In addition, there is a more particular need for algorithm-centered policy responses. Specifically, algorithmic transparency-transparency about the nature, uses, and consequences of algorithms-is both crucial and challenging; novel methods designed to open the algorithmic \"black box\"and \"interpret\"the algorithm's decision-making process should play a key role. In appropriate cases, regulators should also police the design and implementation of algorithms, with a particular emphasis on the exploitation of an absence of information or of behavioral biases.",
        "DOI": "10.1093/jla/laad003",
        "paper_author": "Bar-Gill O.",
        "affiliation_name": "Harvard Law School",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States",
        "affiliation_id": "60017150",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Climate change impacts on water resources: An overview",
        "publication": "Visualization Techniques for Climate Change with Machine Learning and Artificial Intelligence",
        "citied_by": "13",
        "cover_date": "2022-01-01",
        "Abstract": "Recent years have witnessed an upsurge of worldwide interest in potential impacts of climate change on water resources. Climate change is often entwined with alteration of water quantity as well as quality, aggravating the fast-growing water crisis. Over the past few decades, the negative effects of climate extremities are reflected in hydrological cycle, viz., pronounced shifts in global precipitation patterns and increased atmospheric water vapor content, glacier melting, floods, soil erosion, and drought etc. This situation substantially hinders the progress toward the attainment of Sustainable Development Goals (SDGs), thus jeopardizing the needs of future generations. It is therefore necessary to scientifically address the water security issues triggered by the escalating atmospheric and ocean temperatures. Water resource management has an obvious impact on a wide range of policy sectors, including energy, health, food security, and environment. As a result, practitioners need to design appropriate adaptation and mitigation strategies across diverse water-dependent sectors. However, there is a call for scrutinizing the current knowledge gaps in climate change vis-à-vis its implications on water resources. Owing to the complexities of climate system, anticipating these impacts is extremely challenging. Hence climate models related to the hydrological cycle provides a framework to conceptualize future scenario which is important for effective decision making. This chapter briefly discusses climate change impacts on water resources and process based modeling approaches combined with artificial intelligence /machine learning for tackling those issues.",
        "DOI": "10.1016/B978-0-323-99714-0.00008-X",
        "paper_author": "Sukanya S.",
        "affiliation_name": "University of Kerala",
        "affiliation_city": "Thiruvananthapuram",
        "affiliation_country": "India",
        "affiliation_id": "60031566",
        "affiliation_state": "KL"
    },
    {
        "paper_title": "Scheduling and Communication Schemes for Decentralized Federated Learning",
        "publication": "32nd International Conference on Computer Theory and Applications, ICCTA 2022 - Proceedings",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "Federated Learning (FL) is a distributed machine learning paradigm in which a large number of clients coordinate with a central server to learn a model without sharing their own training data. One central server is not enough, due to problems of connectivity with clients. In this paper, a Decentralized Federated Learning (DFL) model with the Stochastic Gradient Descent (SGD) algorithm has been introduced, as a more scalable approach to improve the learning performance in a network of agents with arbitrary topology. Three scheduling policies for DFL have been proposed for communications between the clients and the parallel servers, and the convergence, accuracy, and loss have been tested in a totally decentralized implementation of SGD. The experimental results show that the proposed scheduling polices have an impact both on the speed of convergence and in the final global model.",
        "DOI": "10.1109/ICCTA58027.2022.10206255",
        "paper_author": "Abdelghany B.E.A.",
        "affiliation_name": "Arab Academy for Science",
        "affiliation_city": "Aswan",
        "affiliation_country": "Egypt",
        "affiliation_id": "129947319",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Assessing Instructor Effectiveness Based on Future Student Performance",
        "publication": "Proceedings of the 15th International Conference on Educational Data Mining, EDM 2022",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "Educational institutions rely on instructor assessment to determine course assignments, which instructors to retain or promote, and whom to provide with additional assistance or training. Instructor assessment is most commonly based on student surveys or peer evaluation—which are both subject to the evaluator’s personal biases. This study describes an assessment method based on future student grade performance, which has the potential to avoid these biases. This study is based on eight years of undergraduate course-grade data from over 24,000 students in a large metropolitan university. The methodology introduced in this paper accounts for confounding factors, such as diverse instructor grading policies and varying student abilities. Top and bottom performing instructors are identified for each course.",
        "DOI": "10.5281/zenodo.6852940",
        "paper_author": "Weiss G.M.",
        "affiliation_name": "Fordham University",
        "affiliation_city": "New York",
        "affiliation_country": "United States",
        "affiliation_id": "60015095",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Readability Analysis of Privacy Policies for Large-Scale Websites: A Perspective from Deep Learning and Linguistics",
        "publication": "Proceedings - 2022 IEEE SmartWorld, Ubiquitous Intelligence and Computing, Autonomous and Trusted Vehicles, Scalable Computing and Communications, Digital Twin, Privacy Computing, Metaverse, SmartWorld/UIC/ATC/ScalCom/DigitalTwin/PriComp/Metaverse 2022",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "Privacy policy statements are an essential approach to self-regulation by website operators in the area of personal privacy protection. However, these policies are often lengthy and difficult to understand, with users appearing to actually read the privacy policy in only a few cases. To address these obstacles, we propose a framework, Privacy Policy Analysis Framework for Automatic Annotation and User Interaction (PPAI) that stores, classifies, and categorizes queries on natural language privacy policies. At the core of PPAI is a privacy-centric language model that consists of a smaller fine-grained dataset of privacy policies and a new hierarchy of neural network classifiers that take into account privacy practices with high-level aspects and finegrained details. Our experimental results show that the eight readability metrics of the dataset exhibit a strong correlation. Furthermore, PPAI's neural network classifier achieves an accuracy of 0.78 in the multi-classification task. The robustness experiments reached higher accuracy than the baseline and remained robust even with a small amount of labeled data.",
        "DOI": "10.1109/SmartWorld-UIC-ATC-ScalCom-DigitalTwin-PriComp-Metaverse56740.2022.00249",
        "paper_author": "Ding H.",
        "affiliation_name": "Guangzhou University",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60025345",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Reinforcement Learning with Hybrid Quantum Approximation in the NISQ Context",
        "publication": "Reinforcement Learning with Hybrid Quantum Approximation in the NISQ Context",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "This book explores the combination of Reinforcement Learning and Quantum Computing in the light of complex attacker-defender scenarios. Reinforcement Learning has proven its capabilities in different challenging optimization problems and is now an established method in Operations Research. However, complex attacker-defender scenarios have several characteristics that challenge Reinforcement Learning algorithms, requiring enormous computational power to obtain the optimal solution. The upcoming field of Quantum Computing is a promising path for solving computationally complex problems. Therefore, this work explores a hybrid quantum approach to policy gradient methods in Reinforcement Learning. It proposes a novel quantum REINFORCE algorithm that enhances its classical counterpart by Quantum Variational Circuits. The new algorithm is compared to classical algorithms regarding the convergence speed and memory usage on several attacker-defender scenarios with increasing complexity. In addition, to study its applicability on today's NISQ hardware, the algorithm is evaluated on IBM's quantum computers, which is accompanied by an in-depth analysis of the advantages of Quantum Reinforcement Learning.",
        "DOI": "10.1007/978-3-658-37616-1",
        "paper_author": "Kunczik L.",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "CREATING GLOBAL DIGITAL TWINS TO IMPROVE AIR QUALITY AND COVID OUTCOMES",
        "publication": "Proceedings of the International Astronautical Congress, IAC",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Poor air quality is a persistent challenge to human health, and a constant struggle for governments to effectively measure and regulate. This challenge has been exacerbated by the pandemic. With 200 million cases of COVID and approximately 46 million of those with long-term symptoms, the numbers of those affected by poor air quality are dramatically increasing from those traditionally affected such as asthma sufferers. Predicting What We Breathe is a NASA-funded project at the City of Los Angeles that uses machine learning combining ground and space datasets to measure and predict air quality. It also creates a direct tie to policy changes and interventions at the City and regionally to improving or worsening air quality outcomes. The algorithms and data behind this are provided via open source by the research team at the California State University, Los Angeles and OpenAQ. The ground truth comes from several non-traditional but calibrated sources: neighborhood/citizen science sensor networks, a city's internet of things sensors, and regional ground sensors. Interventions vary but can include traffic regulation, gasoline regulation, pollution limits from manufacturers, green spaces and tree planting, and ensuring employers encourage carpools and telework. An interesting aspect of this is a program of “sister cities” - 50 cities around the globe that are provided support, workshops, training, and data during this program to be able to do the same in their cities. From Freetown to London, from Durban to Mexico City, these cities are creating a digital twin of the key factors that affect air quality in their cities and applying the predictive, machine-learning algorithms to understand the effect of local actions and legislation. Predicting What We Breathe provides the framework for this twinning effort, already incorporating factors such as wildfires, urban heat, particulate matter, ozone, smoke, wind, geography, and traffic. This digital twinning allows one city to quickly learn from another and to find similar attributes (say wildfires in Sydney and Los Angeles) that pairs cities in one or more characteristics. This project is funded through NASA's Advanced Information Systems Technology Program.",
        "DOI": "NA",
        "paper_author": "Holm J.",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Leveraging on Open-Source Data and Machine Learning to Model Urban Growth - A case study of Nakuru Municipality Urban Growth Monitoring",
        "publication": "Proceedings of the International Astronautical Congress, IAC",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "The world has undergone a significant urbanization process over the last few decades. There has been a shift in the kind of settlements that occur in both rural and urban locations over time. In today's society, metropolitan areas have higher population density than the rural areas. Even in the future, this trend is projected to continue. The complex issue of urban expansion and the resulting sprawling pattern can be better understood through dynamic spatial modelling. While land use change is widely regarded as the most important factor determining urban growth, modelling land use changes has long been the focus of urban growth research. Computer-based algorithms have lately been utilized to evaluate and forecast changes in land cover and land use in metropolitan areas. This paper aims to demonstrate the use of satellite data and machine learning algorithms to model the land cover and land usage of an urban setting and utilize the results to create a simulation of the land cover type of the urban area in the future. Using Nakuru Municipality in Kenya as the area of interest, the study utilized Random Forest classification algorithm and Landsat series data in Google Earth Engine to model the temporal land cover features from 1989 to 2020 based on the following land cover classes; Built areas, Forest, Grassland, Wetland, Cropland, Bareland and Rocks. The products obtained were then subjected to a cellular automata model to generate a simulated urban coverage for the year 2030. The results depicted the changes that occurred on the land cover features for the entire study period. It was noted that the built-up environment gradually increased its coverage from approximately 1% in 1989, to 2% in 2000, to 4% in 2010 and to 8% in 2020 of the total land in the area of interest. The 2030 simulated urban growth for the municipality also indicated a likely 12% built-up area coverage. The geo-statistics obtained were subjected to verification and the 2020 study year benefitted a lot from the in-situ data collected. The accuracy assessment depicted a 90.43% overall accuracy with a kappa value of 0.884. This type of modelling technique can allow relevant authorities to visualize and analyze the future of their actions and policies using spatial data obtained from these models. As a result, contributing to a better knowledge and communication of the fundamental dynamics of land cover and land use transitions that influence urban expansion.",
        "DOI": "NA",
        "paper_author": "Okello P.",
        "affiliation_name": "Kenya Space Agency",
        "affiliation_city": "Nairobi",
        "affiliation_country": "Kenya",
        "affiliation_id": "121099052",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "OrbitSuite: A Fast Pipeline for Space Situational Awareness",
        "publication": "Proceedings of the International Astronautical Congress, IAC",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "The advent of mega-constellations has given rise to an unprecedented exponential growth in the numbers of objects in orbit. As the number of objects sharing similar orbital trajectories increases, as do the probabilities of close encounters and subsequent collisions. Collisions produce more objects further increasing the probability of later collisions until the Earth orbit environment is rendered unusable. Accurate prediction of these encounters is key to enabling satellite operators to perform collision avoidance manoeuvres. This prediction is typically performed by a chain of 1) Orbital Propagators, to determine an objects state vector at a given time 2) Encounter Analysers, to determine which objects are sufficiently close to warrant further examination and 3) Statistical Models, to determine the probability that a conjunction will result in a collision. There is a need to rapidly compute data for a timelier response to threats as new observation data is received. The generation of both historical & augmented-future observation data can improve existing statistical models or as training data for machine learning systems. A key issue is how to ingest, propagate and provide statistics on historical observation data from CSpOC - estimated to take >20 years on a naive CPU only pipeline, and >6 months on a traditionally optimised solution. By reapproaching the problem from a data-oriented perspective, we propose a new analysis & prediction pipeline that 1) enables historical analysis of trends for future instrument development & government policy making, 2) is practical to integrate into current & future operator decision making processes for early warning because encounters can be converged upon much more quickly than traditional methods. Our alternative perspective focuses on more completely exploiting underlying Nvidia-CUDA GPU architecture to create highly performant analysis tools. We also reconsider how data is represented in memory, leveraging appropriate levels of detail at each analysis stage to further improve performance. Using our method, we compute the first full historical dataset containing every 5 km conjunction since the first spaceflights of 1957 in approximately 11 days. Using the modest 2x GTX1060 available a traditional solution would have taken ~6 months. New data can be appended as it becomes available each day and executes in under 10 minutes per day. The historical dataset is ~430 GB, can be used in statistical modelling and future opportunities in machine learning training data to determine trends. All datasets & source-code are made available through GitLab & CDN. Full dataset analysis of contemporary data from Space-Track correlates well with traditional statistical methods with approximately 30000 < 5 km conjunctions per day, lending credibility to the accuracy of our method and further validation to the value of traditional methods. We also use our dataset to establish estimates of current observational capacity & highlight its inadequacy when considering future mega-constellation development.",
        "DOI": "NA",
        "paper_author": "Lane S.",
        "affiliation_name": "University of Surrey",
        "affiliation_city": "Guildford",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60021097",
        "affiliation_state": "Surrey"
    },
    {
        "paper_title": "A Short-term Wind Power Output Forecasting Model based on the Enhanced Gradient Boosting Machine (GBM) Algorithms for High Wind Power Penetrations",
        "publication": "IET Conference Proceedings",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "With rising concerns of climate change, there has been a worldwide trend of establishing policies regarding net zero emissions and sustainability. According to Korea's 2050 Carbon Neutral Strategy, the government aims to decarbonize the country's economic structure and increase penetration of renewable energies. Statistics also show that wind power generation in Korea has been increasing steadily over the years. However, the intermittent nature of wind power remains an obstacle in predicting wind power outputs. Therefore, accuracy in wind power forecasts must be improved to facilitate larger integration of renewables to existing electrical grids. In this paper, we propose the implementation of a short-term wind power output forecasting model based on the enhanced Gradient Boosting Machine (G B M) algorithms for high wind power penetrations. G B M is an effective machine learning algorithm which improves its performance by combining previously learned weak learners to form a strong learner. A 15-minute cycle of measured data from Jeju's wind farms is applied to the model as the input data. The results include scatter plots and line graphs depicting the outcome of prediction data by the G B M model and real data.",
        "DOI": "10.1049/icp.2022.2779",
        "paper_author": "Park S.",
        "affiliation_name": "Ewha Womans University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea",
        "affiliation_id": "60001018",
        "affiliation_state": "Seoul"
    },
    {
        "paper_title": "A Human-Centric Perspective on Model Monitoring",
        "publication": "Proceedings of the AAAI Conference on Human Computation and Crowdsourcing",
        "citied_by": "4",
        "cover_date": "2022-01-01",
        "Abstract": "Predictive models are increasingly used to make various consequential decisions in high-stakes domains such as healthcare, finance, and policy. It becomes critical to ensure that these models make accurate predictions, are robust to shifts in the data, do not rely on spurious features, and do not unduly discriminate against minority groups. To this end, several approaches spanning various areas such as explain ability, fairness, and robustness have been proposed in recent literature. Such approaches need to be human-centered as they cater to the understanding of the models to their users. However, there is little to no research on understanding the needs and challenges in monitoring deployed machine learning (ML) models from a human-centric perspective. To address this gap, we conducted semi-structured interviews with 13 practitioners who are experienced with deploying ML models and engaging with customers spanning domains such as financial services, healthcare, hiring, online retail, computational advertising, and conversational assistants. We identified various shuman-centric challenges and requirements for model monitoring in real-world applications. Specifically, we found that relevant stakeholders would want model monitoring systems to provide clear, unambiguous, and easy-to-understand insights that are readily actionable. Furthermore, our study also revealed that stakeholders desire customization of model monitoring systems to cater to domain-specific use cases.",
        "DOI": "10.1609/hcomp.v10i1.21997",
        "paper_author": "Shergadwala M.N.",
        "affiliation_name": "Fiddler Labs",
        "affiliation_city": "Palo Alto",
        "affiliation_country": "United States",
        "affiliation_id": "125066421",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Neural Network Optimal Feedback Control With Guaranteed Local Stability",
        "publication": "IEEE Open Journal of Control Systems",
        "citied_by": "6",
        "cover_date": "2022-01-01",
        "Abstract": "Recent research shows that supervised learning can be an effective tool for designing near-optimal feedback controllers for high-dimensional nonlinear dynamic systems. But the behavior of neural network controllers is still not well understood. In particular, some neural networks with high test accuracy can fail to even locally stabilize the dynamic system. To address this challenge we propose several novel neural network architectures, which we show guarantee local asymptotic stability while retaining the approximation capacity to learn the optimal feedback policy semi-globally. The proposed architectures are compared against standard neural network feedback controllers through numerical simulations of two high-dimensional nonlinear optimal control problems: stabilization of an unstable Burgers-type partial differential equation, and altitude and course tracking for an unmanned aerial vehicle. The simulations demonstrate that standard neural networks can fail to stabilize the dynamics even when trained well, while the proposed architectures are always at least locally stabilizing and can achieve near-optimal performance.",
        "DOI": "10.1109/OJCSYS.2022.3205863",
        "paper_author": "Nakamura-Zimmerer T.",
        "affiliation_name": "Baskin School of Engineering",
        "affiliation_city": "Santa Cruz",
        "affiliation_country": "United States",
        "affiliation_id": "60137794",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Prediction of Auto Insurance Risk Based on t-SNE Dimensionality Reduction",
        "publication": "Advances in Artificial Intelligence and Machine Learning",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Correct risk estimation of policyholders is of great significance to auto insurance companies. While the current tools used in this field have been proven in practice to be quite efficient and beneficial, we argue that there is still a lot of room for development and improvement in the auto insurance risk estimation process. To this end, we develop a framework based on a combination of a neural network together with a dimensionality reduction technique t-SNE (t-distributed stochastic neighbour embedding). This enables us to visually represent the complex structure of the risk as a two-dimensional surface, while still preserving the properties of the local region in the features space. The obtained results, which are based on real insurance data, reveal a clear contrast between the high and the low risk policy holders, and indeed improve upon the actual risk estimation performed by the insurer. Due to the visual accessibility of the portfolio in this approach, we argue that this framework could be advantageous to the auto insurer, both as a main risk prediction tool and as an additional validation stage in other approaches.",
        "DOI": "10.54364/AAIML.2022.1139",
        "paper_author": "Levitas J.",
        "affiliation_name": "kasko2go AG",
        "affiliation_city": "Zug",
        "affiliation_country": "Switzerland",
        "affiliation_id": "129966953",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Safety Guided Policy Optimization",
        "publication": "IEEE International Conference on Intelligent Robots and Systems",
        "citied_by": "2",
        "cover_date": "2022-01-01",
        "Abstract": "In reinforcement learning (RL), exploration is essential to achieve a globally optimal policy but unconstrained exploration can cause damages to robots and nearby people. To handle this safety issue in exploration, safe RL has been proposed to keep the agent under the specified safety constraints while maximizing cumulative rewards. This paper introduces a new safe RL method which can be applied to robots to operate under the safety constraints while learning. The key component of the proposed method is the safeguard module. The safeguard predicts the constraints in the near future and corrects actions such that the predicted constraints are not violated. Since actions are safely modified by the safeguard during exploration and policies are trained to imitate the corrected actions, the agent can safely explore. Additionally, the safeguard is sample efficient as it does not require long horizontal trajectories for training, so constraints can be satisfied within short time steps. The proposed method is extensively evaluated in simulation and experiments using a real robot. The results show that the proposed method achieves the best performance while satisfying safety constraints with minimal interaction with environments in all experiments.",
        "DOI": "10.1109/IROS47612.2022.9981030",
        "paper_author": "Kim D.",
        "affiliation_name": "Automation and Systems Research Institute",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea",
        "affiliation_id": "60120116",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "SOCIOECONOMIC FACTORS AND MACHINE LEARNING ALGORITHMS APPLIED TO NEGLECTED DISEASES RISK PREDICTION.",
        "publication": "Finisterra",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "Analyzing the relation between socioeconomic variables and neglected tropical diseases can help managers in the conception of public policies to reduce cases. The objective of this study was to evaluate, based on machine learning algorithms, which socioeconomic variables are more important for the risk classification of three neglected diseases: leprosy, cutaneous leishmaniasis, and dengue. Three algorithms based on decision trees were evaluated: Random Forest (RF), XGBoost, and C5.0. As a study area, the municipalities of the state of Goiás and of the Federal District – Brazil, were delimited. For the dengue risk classes, both the RF algorithm and the XGBoost showed accuracy values above 0.6. Both emphasizing the low-income conditions, literacy, and race as the most important predictive variables. In the leprosy risk classes case, the three algorithms presented accuracy results above 0.6, indicating the variables water supply, literacy, race, and housing as important. For the cutaneous leishmaniasis risk classes, the algorithms showed an accuracy lower than 0.4, making the evaluation of possible predictive variables to the model unfeasible. The three evaluated algorithms revealed approximate predictive performance; however, the RF was slightly higher. The most important socioeconomic variables for dengue and leprosy risk classes prediction were similar.",
        "DOI": "10.18055/Finis28635",
        "paper_author": "Gioia T.B.",
        "affiliation_name": "Universidade Federal de Goiás",
        "affiliation_city": "Goiania",
        "affiliation_country": "Brazil",
        "affiliation_id": "60027136",
        "affiliation_state": "GO"
    },
    {
        "paper_title": "Sentiments analysis of covid-19 vaccine tweets using machine learning and vader lexicon method",
        "publication": "Advances in Distributed Computing and Artificial Intelligence Journal",
        "citied_by": "3",
        "cover_date": "2022-01-01",
        "Abstract": "The novel Coronavirus disease of 2019 (COVID-19) has subsequently named Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) have tormented the lives of millions of people worldwide. Effective and safe vaccination might curtail the pandemic. This study aims to apply the VADER lexicon, Text Blob, and machine learning approach: To analyse and detect the ongoing sentiments during the affliction of the Covid-19 pandemic on Twitter, to understand public reaction worldwide towards vaccine and concerns about the effectiveness of the vaccine. Over 200000 tweets vaccine-related using hashtags # Covid Vaccine # Vaccines # CornavirusVaccine were retrieved from 18 August 2020 to 20 July 2021. Data analysis conducted by VADER lexicon method to predict sentiments polarity, counts and sentiment distribution, Text Blob to determine the subjectivity and polarity, and compared with two other models such as Random Forest (RF) and Logistic Regression (LR). The results determine sentiments that public have a positive stance towards a vaccine follows by neutral and negative. Machine learning classification models performed better than the VADER lexicon method on vaccine Tweets. It is anticipated this study aims to help the government in long run, to make policies and a better environment for people suffering from negative thoughts during the ongoing pandemic.",
        "DOI": "10.14201/adcaij.27349",
        "paper_author": "Arya V.",
        "affiliation_name": "DIT University",
        "affiliation_city": "Dehradun",
        "affiliation_country": "India",
        "affiliation_id": "60113890",
        "affiliation_state": "UK"
    },
    {
        "paper_title": "KINGFISHER: A FRAMEWORK FOR FAST MACHINE LEARNING INFERENCE FOR AUTONOMOUS ACCELERATOR SYSTEMS",
        "publication": "Proceedings of the International Beam Instrumentation Conference, IBIC",
        "citied_by": "2",
        "cover_date": "2022-01-01",
        "Abstract": "Modern particle accelerator facilities allow new and exciting beam properties and operation modes. Traditional real-time control systems, albeit powerful, have bandwidth and latency constraints that limit the range of operating conditions currently made available to users. The capability of Reinforcement Learning to perform self-learning control policies by interacting with the accelerator is intriguing. The extreme dynamic conditions require fast real-time feedback throughout the whole control loop from the diagnostic, with novel and intelligent detector systems, all the way to the interaction with the accelerator components. In this contribution, the novel KINGFISHER framework based on the modern Xilinx Versal devices will be presented. Versal combines several computational engines, specifically combining powerful FPGA logic with programmable AI Engines in a single device. Furthermore, this system can be natively integrated with the fastest beam diagnostic tools already available, e.g. KAPTURE and KALYPSO.",
        "DOI": "10.18429/JACoW-IBIC2022-MOP42",
        "paper_author": "Scomparin L.",
        "affiliation_name": "Karlsruher Institut für Technologie",
        "affiliation_city": "Karlsruhe",
        "affiliation_country": "Germany",
        "affiliation_id": "60102538",
        "affiliation_state": "Baden-Wurttemberg"
    },
    {
        "paper_title": "Proceedings of the 1st Workshop on Healthcare AI and COVID-19, ICML 2022",
        "publication": "Proceedings of Machine Learning Research",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "The proceedings contain 14 papers. The topics discussed include: deep metric learning by exploring confusing triplet embeddings for COVID-19 medical images diagnosis; ASA-CoroNet: adaptive self-attention network for COVID-19 automated diagnosis using chest X-ray images; temporal multiresolution graph neural networks for epidemic prediction; characterizing and understanding temporal effects in COVID-19 data; mixture of input-output hidden Markov models for heterogeneous disease progression modeling; accurate calibration of agent-based epidemiological models with neural network surrogates; machine learning-powered mitigation policy optimization in epidemiological models; real-time and explainable detection of epidemics with global news data; and prediction of mortality and intervention in COVID-19 patients using generative adversarial networks.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Machine Learning-Powered Mitigation Policy Optimization in Epidemiological Models",
        "publication": "Proceedings of Machine Learning Research",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "A crucial aspect of managing a public health crisis is to effectively balance prevention and mitigation strategies, while taking their socio-economic impact into account. In particular, determining the influence of different non-pharmaceutical interventions (NPIs) on the effective use of public resources is an important problem, given the uncertainties on when a vaccine will be made available. In this paper, we propose a new approach for obtaining optimal policy recommendations based on epidemiological models, which can characterize the disease progression under different interventions, and a look-ahead reward optimization strategy to choose the suitable NPI at different stages of an epidemic. Given the time delay inherent in any epidemiological model and the exponential nature especially of an unmanaged epidemic, we find that such a look-ahead strategy infers non-trivial policies that adhere well to the constraints specified. Using two different epidemiological models, namely SEIR and EpiCast, we evaluate the proposed algorithm to determine the optimal NPI policy, under a constraint on the number of daily new cases and the primary reward being the absence of restrictions.",
        "DOI": "NA",
        "paper_author": "Thiagarajan J.J.",
        "affiliation_name": "Lawrence Livermore National Laboratory",
        "affiliation_city": "Livermore",
        "affiliation_country": "United States",
        "affiliation_id": "60026175",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Modular Software Architecture For Interfacing Online Scheduling Agents With Assembly Planning And Control Systems",
        "publication": "Proceedings of the Conference on Production Systems and Logistics",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Production systems must be more resilient and adaptive due to mass customization and increasingly external disturbances, such as supply chain disruptions or changing policies. As the last chain in the production value stream, assembly systems are especially prone to fluctuations, leading to alternative and more flexible assembly system designs. Online scheduling is a crucial component for dynamically controlling a flexible assembly system. This work presents a modular software architecture that interfaces between online scheduling agents and control systems. A standardized data model of the assembly system allows for exchanging different scheduling agents during the planning or operation phase. Applications are benchmarking competing algorithms, validating scheduling results by comparison, and seamlessly substituting or updating scheduling algorithms. The standardized data model and interface on the assembly system side facilitate the transition between planning and operation. A simulation model can be interchanged with a control system without extra effort to integrate the control system's scheduling agents. Additionally, the modular architecture enables production-parallel simulation to optimize the running system by evaluating and executing alternative scenarios. The long-term assembly system performance can profit from the modular architecture by updating the agent during production with advances in online scheduling algorithms (e.g., machine learning). Furthermore, the modular architecture enables the required resilience and adaptability by fast switching from simulation to real control systems and supporting system optimizations during operation.",
        "DOI": "10.15488/12134",
        "paper_author": "Göppert A.",
        "affiliation_name": "Rheinisch-Westfälische Technische Hochschule Aachen",
        "affiliation_city": "Aachen",
        "affiliation_country": "Germany",
        "affiliation_id": "60016653",
        "affiliation_state": "Nordrhein-Westfalen"
    },
    {
        "paper_title": "Replacement Policy for a Single-component Machine with Limited Spares in a Finite Time Horizon",
        "publication": "IET Conference Proceedings",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "In a maintenance scheduling problem, the optimal policy obtained by using Markov Decision Process (MDP) can be in a control-limit form, which facilitates both computation and implementation of the optimal control. The optimality of control limit policy (CLP) in one dimension has been proven in different models, but few researchers have explored the existence of multidimensional CLP. Besides, most models on maintenance optimization aimed to minimize the long-run expected discounted or average cost, assuming no constraint on the supply of spare parts. However, in industrial systems, since the lead time of spare parts ordering is sometimes fixed and rather long compared to the length of inspection interval, the total number of available spares in a certain time horizon can be limited. Therefore, it is useful to study the replacement policy with limited spares and in a finite time horizon. In this paper, we consider a discrete-time model where the component of a single-unit machine deteriorates according to a Markovian process and needs to be replaced correctively or preventively by consuming a limited number of spare parts. The objective is to find the optimal replacement policy which minimizes the expected total cost in a finite time horizon. MDP is applied to formulate and solve the three-dimensional optimization problem. Structural properties, especially two-dimensional control-limit form of the optimal solution, are studied under certain assumptions. Numerical experiments are performed to verify and explore the behaviors and performance of the optimal policy. Finally, we conclude that the optimal solution is a CLP in health condition of the currently working part, but not necessarily in the remained number of spares. However, the numerical experiments show that the optimal two-dimensional CLP has almost the same total cost rate as the true optimal policy, which will help make replacement decisions in real-world maintenance problems.",
        "DOI": "10.1049/icp.2022.3075",
        "paper_author": "Wang Y.",
        "affiliation_name": "Tsinghua University",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60025278",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Decentralising Access Control for IoT Environment",
        "publication": "2022 IEEE 8th World Forum on Internet of Things, WF-IoT 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "IoT has a profound impact on businesses and individuals with growing adoption. Security and scalability are key subjects for scaling the adoption. Cyber attacks increase each year, and the emerging technologies' addition such as Machine Learning introduce vulnerabilities with additional complexity. Access control can mitigate security threats with proper rights management. XACML is an appropriate way to enforce complex policies in heterogeneous environments like IoT due to its flexibility. Furthermore, the blockchain's advantages like data immutability and availability can aid in building a trustworthy access control system for IoT. Blockchain can support a de-centralised architecture for policy evaluation and avoid single points of failure for the policy evaluation resulting in enhanced security of the IoT network. Smart contracts accommodate the access control policies' evaluation for delivering a decentralised and tamper-proof system with consistent outcomes. This paper proposes a decentralised access control approach following the XACML standard and enabling the access control decision evaluation using smart contracts. The implementation's impact on a complex real-world environment is described. The reference implementation is extensible to a great degree as it has flexibility in including services on top of the blockchain, such as an audit mechanism on the access decisions.",
        "DOI": "10.1109/WF-IoT54382.2022.10152066",
        "paper_author": "Savvaidis C.",
        "affiliation_name": "Centre for Research and Technology-Hellas",
        "affiliation_city": "Thessaloniki",
        "affiliation_country": "Greece",
        "affiliation_id": "60026208",
        "affiliation_state": "Macedonia"
    },
    {
        "paper_title": "Proceedings of the Conference on Health, Inference, and Learning, CHIL 2022",
        "publication": "Proceedings of Machine Learning Research",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "The proceedings contain 23 papers. The topics discussed include: counterfactually guided policy transfer in clinical settings; evaluating domain generalization for survival analysis in clinical studies; estimating model performance on external samples from their limited statistical characteristics; enriching unsupervised user embedding via medical concepts; multi-task adversarial learning for treatment effect estimation in basket trials; neural survival clustering: non-parametric mixture of neural networks for survival clustering; uncertainty-aware text-to-program for question answering on structured electronic health records; how to validate machine learning models prior to deployment: silent trial protocol for evaluation of real-time models at ICU; unifying heterogeneous electronic health records systems via text-based code embedding; and context-sensitive spelling correction of clinical text via conditional independence.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Bibliometric analysis of inclusive technologies for deaf people",
        "publication": "Proceedings - 2022 2nd International Conference on Advanced Enterprise Information System, AEIS 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "This paper presents the review results of research related to technologies adapted for deaf population, in order to identify the main research trends between 1990 and the end of 2019. The methodology used consists of a bibliometric, in which a search equation was formulated that allowed the recovery of 229 records backed by academic peers from the Scopus database, with which it was possible to collect transcendent information around it bibliometric indicators associated with the number of publications, major countries and exhibitors on the subject, the type of publication and the behavior of representative issues relating to the subject under investigation. The results point to the boom in production and outreach for new technologies aimed at meeting the needs of this global sector of the population; like, persistent gaps in some countries, namely those in developing, gender, age, machine human interaction, sign language, and education. It is determined, the relevance of topics related to communication, learning, training, development of new inputs, digital devices and public policy design. The scientific contribution is recognized, through research and technological impetus in the generation of opportunities and well-being for the population under study.",
        "DOI": "10.1109/AEIS59450.2022.00022",
        "paper_author": "Valencia-Arias A.",
        "affiliation_name": "Instituto Tecnológico Metropolitano",
        "affiliation_city": "Medellin",
        "affiliation_country": "Colombia",
        "affiliation_id": "60108696",
        "affiliation_state": "Antioquia"
    },
    {
        "paper_title": "Improved Policy Optimization for Online Imitation Learning",
        "publication": "Proceedings of Machine Learning Research",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "We consider online imitation learning (OIL), where the task is to find a policy that imitates the behavior of an expert via active interaction with the environment. We aim to bridge the gap between the theory and practice of policy optimization algorithms for OIL by analyzing one of the most popular OIL algorithms, DAGGER. Specifically, if the class of policies is sufficiently expressive to contain the expert policy, we prove that DAGGER achieves constant regret. Unlike previous bounds that require the losses to be strongly-convex, our result only requires the weaker assumption that the losses be strongly-convex with respect to the policy’s sufficient statistics (not its parameterization). In order to ensure convergence for a wider class of policies and losses, we augment DAGGER with an additional regularization term. In particular, we propose a variant of Follow-the-Regularized-Leader (FTRL) and its adaptive variant for OIL and develop a memory-efficient implementation, which matches the memory requirements of FTL. Assuming that the loss functions are smooth and convex with respect to the parameters of the policy, we also prove that FTRL achieves constant regret for any sufficiently expressive policy class, while retaining O(√T) regret in the worst-case. We demonstrate the effectiveness of these algorithms with experiments on synthetic and high-dimensional control tasks.",
        "DOI": "NA",
        "paper_author": "Lavington J.W.",
        "affiliation_name": "The University of British Columbia",
        "affiliation_city": "Vancouver",
        "affiliation_country": "Canada",
        "affiliation_id": "60010365",
        "affiliation_state": "BC"
    },
    {
        "paper_title": "MO2: MODEL-BASED OFFLINE OPTIONS",
        "publication": "Proceedings of Machine Learning Research",
        "citied_by": "4",
        "cover_date": "2022-01-01",
        "Abstract": "The ability to discover useful behaviours from past experience and transfer them to new tasks is considered a core component of natural embodied intelligence. Inspired by neuroscience, discovering behaviours that switch at bottleneck states have been long sought after for inducing plans of minimum description length across tasks. Prior approaches have either only supported online, on-policy, bottleneck state discovery, limiting sample-efficiency, or discrete state-action domains, restricting applicability. To address this, we introduce Model-Based Offline Options (MO2), an offline hindsight framework supporting sample-efficient bottleneck option discovery over continuous state-action spaces. Once bottleneck options are learnt offline over source domains, they are transferred online to improve exploration and value estimation on the transfer domain. Our experiments show that on complex long-horizon continuous control tasks with sparse, delayed rewards, MO2’s properties are essential and lead to performance exceeding recent option learning methods. Additional ablations further demonstrate the impact on option predictability and credit assignment.",
        "DOI": "NA",
        "paper_author": "Salter S.",
        "affiliation_name": "DeepMind Technologies Limited",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60111161",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Research on Enterprises' Response to Emergencies Based on Machine Learning",
        "publication": "Proceedings - 2022 3rd International Conference on Computer Science and Management Technology, ICCSMT 2022",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "The outbreak of the Corona Virus Disease 2019 quickly became an international and domestic public health emergency in the spotlight. Under the impact of the epidemic, private companies, especially offline businesses, have been hit hard. Therefore, private enterprises today need to pay more attention to enhancing \"organizational resilience\"and improving their resilience in the face of disruptions and disruptions caused by major emergencies. In this study, three machine learning methods are compared with traditional linear regression models to analyze the impact of organizational resilience on the response of small and medium-sized private enterprises to the epidemic. The same interview-based policy recommendations to boost the organizational resilience of small and medium-sized private enterprises in three northeastern provinces are proposed. The results of this survey show that the degree of influence of resilience commitment is the greatest among the main factors affecting the resilience of enterprises. According to the policy demands of enterprises, the government should help enterprises with difficulties in the epidemic by giving financial policy support, providing cost subsidies, and improving the national emergency management system.",
        "DOI": "10.1109/ICCSMT58129.2022.00011",
        "paper_author": "Fan X.",
        "affiliation_name": "Dalian University of Technology",
        "affiliation_city": "Dalian",
        "affiliation_country": "China",
        "affiliation_id": "60004538",
        "affiliation_state": "Liaoning"
    },
    {
        "paper_title": "LEARNING OBJECT-CENTERED AUTOTELIC BEHAVIORS WITH GRAPH NEURAL NETWORKS",
        "publication": "Proceedings of Machine Learning Research",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Although humans live in an open-ended world and endlessly face new challenges, they do not have to learn from scratch each time they face the next one. Rather, they have access to a handful of previously learned skills, which they rapidly adapt to new situations. In artificial intelligence, autotelic agents — which are intrinsically motivated to represent and set their own goals — exhibit promising skill adaptation capabilities. However, these capabilities are highly constrained by their policy and goal space representations. In this paper, we propose to investigate the impact of these representations on the learning and transfer capabilities of autotelic agents. We study different implementations of autotelic agents using four types of Graph Neural Networks policy representations and two types of goal spaces, either geometric or predicate-based. By testing agents on unseen goals, we show that combining object-centered architectures that are expressive enough with semantic relational goals helps learning to reach more difficult goals. We also release our graph-based implementations to encourage further research in this direction.",
        "DOI": "NA",
        "paper_author": "Akakzia A.",
        "affiliation_name": "Sorbonne Université",
        "affiliation_city": "Paris",
        "affiliation_country": "France",
        "affiliation_id": "60001422",
        "affiliation_state": "Ile-de-France"
    },
    {
        "paper_title": "Online Decision Mediation",
        "publication": "Advances in Neural Information Processing Systems",
        "citied_by": "2",
        "cover_date": "2022-01-01",
        "Abstract": "Consider learning a decision support assistant to serve as an intermediary between (oracle) expert behavior and (imperfect) human behavior: At each time, the algorithm observes an action chosen by a fallible agent, and decides whether to accept that agent's decision, intervene with an alternative, or request the expert's opinion. For instance, in clinical diagnosis, fully-autonomous machine behavior is often beyond ethical affordances, thus real-world decision support is often limited to monitoring and forecasting. Instead, such an intermediary would strike a prudent balance between the former (purely prescriptive) and latter (purely descriptive) approaches, while providing an efficient interface between human mistakes and expert feedback. In this work, we first formalize the sequential problem of online decision mediation -that is, of simultaneously learning and evaluating mediator policies from scratch with abstentive feedback: In each round, deferring to the oracle obviates the risk of error, but incurs an upfront penalty, and reveals the otherwise hidden expert action as a new training data point. Second, we motivate and propose a solution that seeks to trade off (immediate) loss terms against (future) improvements in generalization error; in doing so, we identify why conventional bandit algorithms may fail. Finally, through experiments and sensitivities on a variety of datasets, we illustrate consistent gains over applicable benchmarks on performance measures with respect to the mediator policy, the learned model, and the decision-making system as a whole.",
        "DOI": "NA",
        "paper_author": "Jarrett D.",
        "affiliation_name": "Faculty of Mathematics",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60119937",
        "affiliation_state": "Cambridgeshire"
    },
    {
        "paper_title": "Off-Policy Evaluation with Policy-Dependent Optimization Response",
        "publication": "Advances in Neural Information Processing Systems",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "The intersection of causal inference and machine learning for decision-making is rapidly expanding, but the default decision criterion remains an average of individual causal outcomes across a population. In practice, various operational restrictions ensure that a decision-maker's utility is not realized as an average but rather as an output of a downstream decision-making problem (such as matching, assignment, network flow, minimizing predictive risk). In this work, we develop a new framework for off-policy evaluation with policy-dependent linear optimization responses: causal outcomes introduce stochasticity in objective function coefficients. Under this framework, a decision-maker's utility depends on the policy-dependent optimization, which introduces a fundamental challenge of optimization bias even for the case of policy evaluation. We construct unbiased estimators for the policy-dependent estimand by a perturbation method, and discuss asymptotic variance properties for a set of adjusted plug-in estimators. Lastly, attaining unbiased policy evaluation allows for policy optimization: we provide a general algorithm for optimizing causal interventions. We corroborate our theoretical results with numerical simulations.",
        "DOI": "NA",
        "paper_author": "Guo W.",
        "affiliation_name": "Department of Electrical Engineering and Computer Sciences",
        "affiliation_city": "Berkeley",
        "affiliation_country": "United States",
        "affiliation_id": "60121438",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Human-Robotic Prosthesis as Collaborating Agents for Symmetrical Walking",
        "publication": "Advances in Neural Information Processing Systems",
        "citied_by": "5",
        "cover_date": "2022-01-01",
        "Abstract": "This is the first attempt at considering human influence in the reinforcement learning control of a robotic lower limb prosthesis toward symmetrical walking in real world situations. We propose a collaborative multi-agent reinforcement learning (cMARL) solution framework for this highly complex and challenging human-prosthesis collaboration (HPC) problem. The design of an automatic controller of the robot within the HPC context is based on accessible physical features or measurements that are known to affect walking performance. Comparisons are made with the current state-of-the-art robot control designs, which are single-agent based, as well as existing MARL solution approaches tailored to the problem, including multi-agent deep deterministic policy gradient (MADDPG) and counterfactual multi-agent policy gradient (COMA). Results show that, when compared to these approaches, treating the human and robot as coupled agents and using an estimated human adaption in robot control design can achieve lower stage cost, peak error, and improved symmetry to ensure better human walking performance. Additionally, our approach accelerates learning of walking tasks and increases learning success rate. The proposed framework can potentially be further developed to examine how human and robotic lower limb prosthesis interact, an area that little is known about. Advancing cMARL toward real world applications such as HPC for normative walking sets a good example of how AI can positively impact on people's lives.",
        "DOI": "NA",
        "paper_author": "Wu R.",
        "affiliation_name": "Arizona State University",
        "affiliation_city": "Tempe",
        "affiliation_country": "United States",
        "affiliation_id": "60003892",
        "affiliation_state": "AZ"
    },
    {
        "paper_title": "When to Intervene: Learning Optimal Intervention Policies for Critical Events",
        "publication": "Advances in Neural Information Processing Systems",
        "citied_by": "4",
        "cover_date": "2022-01-01",
        "Abstract": "Providing a timely intervention before the onset of a critical event, such as a system failure, is of importance in many industrial settings. Before the onset of the critical event, systems typically exhibit behavioral changes which often manifest as stochastic co-variate observations which may be leveraged to trigger intervention. In this paper, for the first time, we formulate the problem of finding an optimally timed intervention (OTI) policy as minimizing the expected residual time to event, subject to a constraint on the probability of missing the event. Existing machine learning approaches to intervention on critical events focus on predicting event occurrence within a pre-defined window (a classification problem) or predicting time-to-event (a regression problem). Interventions are then triggered by setting model thresholds. These are heuristic-driven, lacking guarantees regarding optimality. To model the evolution of system behavior, we introduce the concept of a hazard rate process. We show that the OTI problem is equivalent to an optimal stopping problem on the associated hazard rate process. This key link has not been explored in literature. Under Markovian assumptions on the hazard rate process, we show that an OTI policy at any time can be analytically determined from the conditional hazard rate function at that time. Further, we show that our theory includes, as a special case, the important class of neural hazard rate processes generated by recurrent neural networks (RNNs). To model such processes, we propose a dynamic deep recurrent survival analysis (DDRSA) architecture, introducing an RNN encoder into the static DRSA setting. Finally, we demonstrate RNN-based OTI policies with experiments and show that they outperform popular intervention methods.",
        "DOI": "NA",
        "paper_author": "Venkata N.D.",
        "affiliation_name": "HP Inc.",
        "affiliation_city": "Bengaluru",
        "affiliation_country": "India",
        "affiliation_id": "60107879",
        "affiliation_state": "KA"
    },
    {
        "paper_title": "Adversarial Auto-Augment with Label Preservation: A Representation Learning Principle Guided Approach",
        "publication": "Advances in Neural Information Processing Systems",
        "citied_by": "6",
        "cover_date": "2022-01-01",
        "Abstract": "Data augmentation is a critical contributing factor to the success of deep learning but heavily relies on prior domain knowledge which is not always available. Recent works on automatic data augmentation learn a policy to form a sequence of augmentation operations, which are still pre-defined and restricted to limited options. In this paper, we show that a prior-free autonomous data augmentation's objective can be derived from a representation learning principle that aims to preserve the minimum sufficient information of the labels. Given an example, the objective aims at creating a distant “hard positive example” as the augmentation, while still preserving the original label. We then propose a practical surrogate to the objective that can be optimized efficiently and integrated seamlessly into existing methods for a broad class of machine learning tasks, e.g., supervised, semi-supervised, and noisy-label learning. Unlike previous works, our method does not require training an extra generative model but instead leverages the intermediate layer representations of the end-task model for generating data augmentations. In experiments, we show that our method consistently brings non-trivial improvements to the three aforementioned learning tasks from both efficiency and final performance, either or not combined with strong pre-defined augmentations, e.g., on medical images when domain knowledge is unavailable and the existing augmentation techniques perform poorly. Code is available at: https://github.com/kai-wen-yang/LPA3.",
        "DOI": "NA",
        "paper_author": "Yang K.",
        "affiliation_name": "University of Science and Technology of China",
        "affiliation_city": "Hefei",
        "affiliation_country": "China",
        "affiliation_id": "60019118",
        "affiliation_state": "Anhui"
    },
    {
        "paper_title": "Lyapunov Density Models: Constraining Distribution Shift in Learning-Based Control",
        "publication": "Proceedings of Machine Learning Research",
        "citied_by": "13",
        "cover_date": "2022-01-01",
        "Abstract": "Learned models and policies can generalize effectively when evaluated within the distribution of the training data, but can produce unpredictable and erroneous outputs on out-of-distribution inputs. In order to avoid distribution shift when deploying learning-based control algorithms, we seek a mechanism to constrain the agent to states and actions that resemble those that it was trained on. In control theory, Lyapunov stability and control-invariant sets allow us to make guarantees about controllers that stabilize the system around specific states, while in machine learning, density models allow us to estimate the training data distribution. Can we combine these two concepts, producing learning-based control algorithms that constrain the system to in-distribution states using only in-distribution actions? In this work, we propose to do this by combining concepts from Lyapunov stability and density estimation, introducing Lyapunov density models: a generalization of control Lyapunov functions and density models that provides guarantees on an agent's ability to stay in-distribution over its entire trajectory.",
        "DOI": "NA",
        "paper_author": "Kang K.",
        "affiliation_name": "University of California, Berkeley",
        "affiliation_city": "Berkeley",
        "affiliation_country": "United States",
        "affiliation_id": "60025038",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "RieszNet and ForestRiesz: Automatic Debiased Machine Learning with Neural Nets and Random Forests",
        "publication": "Proceedings of Machine Learning Research",
        "citied_by": "13",
        "cover_date": "2022-01-01",
        "Abstract": "Many causal and policy effects of interest are defined by linear functionals of high-dimensional or non-parametric regression functions. √nconsistent and asymptotically normal estimation of the object of interest requires debiasing to reduce the effects of regularization and/or model selection on the object of interest. Debiasing is typically achieved by adding a correction term to the plug-in estimator of the functional, which leads to properties such as semi-parametric efficiency, double robustness, and Neyman orthogonality. We implement an automatic debiasing procedure based on automatically learning the Riesz representation of the linear functional using Neural Nets and Random Forests. Our method only relies on black-box evaluation oracle access to the linear functional and does not require knowledge of its analytic form. We propose a multitasking Neural Net debiasing method with stochastic gradient descent minimization of a combined Riesz representer and regression loss, while sharing representation layers for the two functions. We also propose a Random Forest method which learns a locally linear representation of the Riesz function. Even though our method applies to arbitrary functionals, we experimentally find that it performs well compared to the state of art neural net based algorithm of Shi et al. (2019) for the case of the average treatment effect functional. We also evaluate our method on the problem of estimating average marginal effects with continuous treatments, using semi-synthetic data of gasoline price changes on gasoline demand. Code available at github.com/victor5as/RieszLearning.",
        "DOI": "NA",
        "paper_author": "Chernozhukov V.",
        "affiliation_name": "Massachusetts Institute of Technology",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States",
        "affiliation_id": "60022195",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Controlling Conditional Language Models without Catastrophic Forgetting",
        "publication": "Proceedings of Machine Learning Research",
        "citied_by": "19",
        "cover_date": "2022-01-01",
        "Abstract": "Machine learning is shifting towards general-purpose pretrained generative models, trained in a self-supervised manner on large amounts of data, which can then be applied to solve a large number of tasks. However, due to their generic training methodology, these models often fail to meet some of the downstream requirements (e.g., hallucinations in abstractive summarization or style violations in code generation). This raises the important question of how to adapt pre-trained generative models to meet all requirements without destroying their general capabilities (\"catastrophic forgetting\"). Recent work has proposed to solve this problem by representing task-specific requirements through energy-based models (EBMs) and approximating these EBMs using distributional policy gradients (DPG). Despite its effectiveness, this approach is however limited to unconditional distributions. In this paper, we extend DPG to conditional tasks by proposing Conditional DPG (CDPG). We evaluate CDPG on four different control objectives across three tasks (translation, summarization and code generation) and two pretrained models (T5 and GPT-Neo). Our results show that fine-tuning using CDPG robustly moves these pretrained models closer towards meeting control objectives and - in contrast with baseline approaches - does not result in catastrophic forgetting.",
        "DOI": "NA",
        "paper_author": "Korbak T.",
        "affiliation_name": "University of Sussex",
        "affiliation_city": "Brighton",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60017317",
        "affiliation_state": "East Sussex"
    },
    {
        "paper_title": "Action-Sufficient State Representation Learning for Control with Structural Constraints",
        "publication": "Proceedings of Machine Learning Research",
        "citied_by": "16",
        "cover_date": "2022-01-01",
        "Abstract": "Perceived signals in real-world scenarios are usually high-dimensional and noisy, and finding and using their representation that contains essential and sufficient information required by downstream decision-making tasks will help improve computational efficiency and generalization ability in the tasks. In this paper, we focus on partially observable environments and propose to learn a minimal set of state representations that capture sufficient information for decision-making, termed Action-Sufficient state Representations (ASRs). We build a generative environment model for the structural relationships among variables in the system and present a principled way to characterize ASRs based on structural constraints and the goal of maximizing cumulative reward in policy learning. We then develop a structured sequential Variational Auto-Encoder to estimate the environment model and extract ASRs. Our empirical results on CarRacing and VizDoom demonstrate a clear advantage of learning and using ASRs for policy learning. Moreover, the estimated environment model and ASRs allow learning behaviors from imagined outcomes in the compact latent space to improve sample efficiency.",
        "DOI": "NA",
        "paper_author": "Huang B.",
        "affiliation_name": "Carnegie Mellon University",
        "affiliation_city": "Pittsburgh",
        "affiliation_country": "United States",
        "affiliation_id": "60027950",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "VFDS: Variational Foresight Dynamic Selection in Bayesian Neural Networks for Efficient Human Activity Recognition",
        "publication": "Proceedings of Machine Learning Research",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "In many machine learning tasks, input features with varying degrees of predictive capability are acquired at varying costs. In order to optimize the performance-cost trade-off, one would select features to observe a priori. However, given the changing context with previous observations, the subset of predictive features to select may change dynamically. Therefore, we face the challenging new problem of foresight dynamic selection (FDS): finding a dynamic and light-weight policy to decide which features to observe next, before actually observing them, for overall performance-cost trade-offs. To tackle FDS, this paper proposes a Bayesian learning framework of Variational Foresight Dynamic Selection (VFDS). VFDS learns a policy that selects the next feature subset to observe, by optimizing a variational Bayesian objective that characterizes the trade-off between model performance and feature cost. At its core is an implicit variational distribution on binary gates that are dependent on previous observations, which will select the next subset of features to observe. We apply VFDS on the Human Activity Recognition (HAR) task where the performance-cost trade-off is critical in its practice. Extensive results demonstrate that VFDS selects different features under changing contexts, notably saving sensory costs while maintaining or improving the HAR accuracy. Moreover, the features that VFDS dynamically select are shown to be interpretable and associated with the different activity types. We will release the code.",
        "DOI": "NA",
        "paper_author": "Ardywibowo R.",
        "affiliation_name": "Texas A&amp;M University",
        "affiliation_city": "College Station",
        "affiliation_country": "United States",
        "affiliation_id": "60020547",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "Transformers are Meta-Reinforcement Learners",
        "publication": "Proceedings of Machine Learning Research",
        "citied_by": "30",
        "cover_date": "2022-01-01",
        "Abstract": "The transformer architecture and variants presented a remarkable success across many machine learning tasks in recent years. This success is intrinsically related to the capability of handling long sequences and the presence of context-dependent weights from the attention mechanism. We argue that these capabilities suit the central role of a Meta-Reinforcement Learning algorithm. Indeed, a meta-RL agent needs to infer the task from a sequence of trajectories. Furthermore, it requires a fast adaptation strategy to adapt its policy for a new task - which can be achieved using the self-attention mechanism. In this work, we present TrMRL (Transformers for Meta-Reinforcement Learning), a meta-RL agent that mimics the memory reinstatement mechanism using the transformer architecture. It associates the recent past of working memories to build an episodic memory recursively through the transformer layers. We show that the self-attention computes a consensus representation that minimizes the Bayes Risk at each layer and provides meaningful features to compute the best actions. We conducted experiments in high-dimensional continuous control environments for locomotion and dexterous manipulation. Results show that TrMRL presents comparable or superior asymptotic performance, sample efficiency, and out-of-distribution generalization compared to the baselines in these environments.",
        "DOI": "NA",
        "paper_author": "Melo L.C.",
        "affiliation_name": "Microsoft Corporation",
        "affiliation_city": "Redmond",
        "affiliation_country": "United States",
        "affiliation_id": "60026532",
        "affiliation_state": "WA"
    },
    {
        "paper_title": "Stock Price Prediction Using ARIMA and LSTM",
        "publication": "Proceedings - 2022 6th Annual International Conference on Data Science and Business Analytics, ICDSBA 2022",
        "citied_by": "3",
        "cover_date": "2022-01-01",
        "Abstract": "In the current stock market, fund companies and investment companies use a variety of quantitative strategies to make profitable investments and transactions. In order to construct a profitable quantitative strategy, it is necessary to predict the market price of stocks. Fund companies usually extract valuable information from investors' comments on the stock on social media, the company's financial statement data and the historical price of the stock to reasonably predict the future price of the stock and trade on this basis. However, it is difficult to make accurate prediction of the stock price, because the market price of the stock is not only affected by the behavior of investors, but also by changes in government policies, sudden international events such as COVID-19 and the War between Ukraine and Russia. Therefore, the stock market often fluctuates greatly. Linear models commonly used in other fields will not be suitable for predicting future stock market movements. In this paper, we will select Google's historical data from 2016 to 2022, and use ARIMA model and MACHINE learning LSTM model widely used in quantitative trading to predict the stock price in the future.",
        "DOI": "10.1109/ICDSBA57203.2022.00055",
        "paper_author": "Dong H.",
        "affiliation_name": "Shandong University",
        "affiliation_city": "Jinan",
        "affiliation_country": "China",
        "affiliation_id": "60031031",
        "affiliation_state": "Shandong"
    },
    {
        "paper_title": "Greedy when Sure and Conservative when Uncertain about the Opponents",
        "publication": "Proceedings of Machine Learning Research",
        "citied_by": "12",
        "cover_date": "2022-01-01",
        "Abstract": "We develop a new approach, named Greedy when Sure and Conservative when Uncertain (GSCU), to competing online against unknown and nonstationary opponents. GSCU improves in four aspects: 1) introduces a novel way of learning opponent policy embeddings offline; 2) trains offline a single best response (conditional additionally on our opponent policy embedding) instead of a finite set of separate best responses against any opponent; 3) computes online a posterior of the current opponent policy embedding, without making the discrete and ineffective decision which type the current opponent belongs to; and 4) selects online between a real-time greedy policy and a fixed conservative policy via an adversarial bandit algorithm, gaining a theoretically better regret than adhering to either. Experimental studies on popular benchmarks demonstrate GSCU's superiority over the state-of-the-art methods. The code is available online at https://github.com/YeTianJHU/GSCU.",
        "DOI": "NA",
        "paper_author": "Fu H.",
        "affiliation_name": "Tencent",
        "affiliation_city": "Shenzhen",
        "affiliation_country": "China",
        "affiliation_id": "60114181",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Proceedings of the 39th International Conference on Machine Learning, ICML 2022",
        "publication": "Proceedings of Machine Learning Research",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "The proceedings contain 1229 papers. The topics discussed include: an initial alignment between neural network and target is needed for gradient descent to learn; meaningfully debugging model mistakes using conceptual counterfactual explanations; hierarchical shrinkage: improving the accuracy and interpretability of tree-based models; deep equilibrium networks are sensitive to initialization statistics; learning of cluster-based feature importance for electronic health record time-series; on the convergence of the Shapley value in parametric Bayesian learning games; individual preference stability for clustering; understanding the unstable convergence of gradient descent; how faithful is your synthetic data? sample-level metrics for evaluating and auditing generative models; a natural actor-critic framework for zero-sum Markov games; deploying convolutional networks on untrusted platforms using 2D holographic reduced representations; and optimistic linear support and successor features as a basis for optimal policy transfer.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Path Towards Herd Immunity: Predicting Herd Immunity of Covid-19 Using A Machine Learning Algorithm",
        "publication": "2022 IEEE North Karnataka Subsection Flagship International Conference, NKCon 2022",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "Machine Learning and Data analytics have been used immensely in the fight against COVID-19, from helping us to tackle and monitor the spread of the virus, it has helped governments globally with policy formulation as well as speeding up research in vaccination and treatment of the virus. In the past 2 years, Covid-19 has sequenced from a mere virus to an endemic and later to a global pandemic, with it has come the need for vaccination, and rightfully so, many vaccines has been developed like the Pfizer-BioNTech, Moderna etc. These vaccination have been integral to the fight against COVID-19, the rise in the number of vaccinated persons globally and the length and different waves of the pandemic will eventually lead us to the point of herd immunity. This paper uses different machine learning methodologies to predict when the world is likely reaching the point of herd immunity, what are the likely path to reach this point of herd immunity before others and the underlying factors that will influence reaching the herd immunity. We determined the likely time periods and what needs to happen to reach this time sooner. Herd immunity is an age long adaptation strategy that human evolution has developed as it interacts with a world of viruses and different bacteria's that can be detrimental to his health, understanding this with respect to Covid-19 becomes quintessential as we continue to look at the topic of Covid-19.",
        "DOI": "10.1109/NKCon56289.2022.10126510",
        "paper_author": "Obu U.",
        "affiliation_name": "G.H. Raisoni College of Engineering, Nagpur",
        "affiliation_city": "Nagpur",
        "affiliation_country": "India",
        "affiliation_id": "60102667",
        "affiliation_state": "MH"
    },
    {
        "paper_title": "Fairness Interventions as (Dis)Incentives for Strategic Manipulation",
        "publication": "Proceedings of Machine Learning Research",
        "citied_by": "8",
        "cover_date": "2022-01-01",
        "Abstract": "Although machine learning (ML) algorithms are widely used to make decisions about individuals in various domains, concerns have arisen that (1) these algorithms are vulnerable to strategic manipulation and “gaming the algorithm”; and (2) ML decisions may exhibit bias against certain social groups. Existing works have largely examined these as two separate issues, e.g., by focusing on building ML algorithms robust to strategic manipulation, or on training a fair ML algorithm. In this study, we set out to understand the impact they each have on the other, and examine how to characterize fair policies in the presence of strategic behavior. The strategic interaction between a decision maker and individuals (as decision takers) is modeled as a two-stage (Stackelberg) game; when designing an algorithm, the former anticipates the latter may manipulate their features in order to receive more favorable decisions. We analytically characterize the equilibrium strategies of both, and examine how the algorithms and their resulting fairness properties are affected when the decision maker is strategic (anticipates manipulation), as well as the impact of fairness interventions on equilibrium strategies. In particular, we identify conditions under which anticipation of strategic behavior may mitigate/exacerbate unfairness, and conditions under which fairness interventions can serve as (dis)incentives for strategic manipulation.",
        "DOI": "NA",
        "paper_author": "Zhang X.",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Columbus",
        "affiliation_country": "United States",
        "affiliation_id": "60149838",
        "affiliation_state": "OH"
    },
    {
        "paper_title": "System-Agnostic Meta-Learning for MDP-based Dynamic Scheduling via Descriptive Policy",
        "publication": "Proceedings of Machine Learning Research",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "Dynamic scheduling is an important problem in applications from queuing to wireless networks. It addresses how to choose an item among multiple scheduling items in each timestep to achieve a long-term goal. Conventional approaches for dynamic scheduling find the optimal policy for a given specific system so that the policy from these approaches is usable only for the corresponding system characteristics. Hence, it is hard to use such approaches for a practical system in which system characteristics dynamically change. This paper proposes a novel policy structure for MDP-based dynamic scheduling, a descriptive policy, which has a system-agnostic capability to adapt to unseen system characteristics for an identical task (dynamic scheduling). To this end, the descriptive policy learns a system-agnostic scheduling principle-in a nutshell, “which condition of items should have a higher priority in scheduling”. The scheduling principle can be applied to any system so that the descriptive policy learned in one system can be used for another system. Experiments with simple explanatory and realistic application scenarios demonstrate that it enables system-agnostic meta-learning with very little performance degradation compared with the system-specific conventional policies.",
        "DOI": "NA",
        "paper_author": "Lee H.S.",
        "affiliation_name": "Sejong University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea",
        "affiliation_id": "60027884",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Uncertainty Quantification for Bayesian Optimization",
        "publication": "Proceedings of Machine Learning Research",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Bayesian optimization is a class of global optimization techniques. In Bayesian optimization, the underlying objective function is modeled as a realization of a Gaussian process. Although the Gaussian process assumption implies a random distribution of the Bayesian optimization outputs, quantification of this uncertainty is rarely studied in the literature. In this work, we propose a novel approach to assess the output uncertainty of Bayesian optimization algorithms, which proceeds by constructing confidence regions of the maximum point (or value) of the objective function. These regions can be computed efficiently, and their confidence levels are guaranteed by the uniform error bounds for sequential Gaussian process regression newly developed in the present work. Our theory provides a unified uncertainty quantification framework for all existing sequential sampling policies and stopping criteria.",
        "DOI": "NA",
        "paper_author": "Tuo R.",
        "affiliation_name": "Texas A&amp;M University",
        "affiliation_city": "College Station",
        "affiliation_country": "United States",
        "affiliation_id": "60020547",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "Difference Advantage Estimation for Multi-Agent Policy Gradients",
        "publication": "Proceedings of Machine Learning Research",
        "citied_by": "11",
        "cover_date": "2022-01-01",
        "Abstract": "Multi-agent policy gradient methods in centralized training with decentralized execution recently witnessed many progresses. During centralized training, multi-agent credit assignment is crucial, which can substantially promote learning performance. However, explicit multi-agent credit assignment in multi-agent policy gradient methods still receives less attention. In this paper, we investigate multi-agent credit assignment induced by reward shaping and provide a theoretical understanding in terms of its credit assignment and policy bias. Based on this, we propose an exponentially weighted advantage estimator, which is analogous to GAE, to enable multi-agent credit assignment while allowing the tradeoff with policy bias. Empirical results show that our approach can successfully perform effective multi-agent credit assignment, and thus substantially outperforms other advantage estimators.",
        "DOI": "NA",
        "paper_author": "Li Y.",
        "affiliation_name": "College of Engineering",
        "affiliation_city": "Chennai",
        "affiliation_country": "India",
        "affiliation_id": "100316633",
        "affiliation_state": "KL"
    },
    {
        "paper_title": "Safe Exploration for Efficient Policy Evaluation and Comparison",
        "publication": "Proceedings of Machine Learning Research",
        "citied_by": "5",
        "cover_date": "2022-01-01",
        "Abstract": "High-quality data plays a central role in ensuring the accuracy of policy evaluation. This paper initiates the study of efficient and safe data collection for bandit policy evaluation. We formulate the problem and investigate its several representative variants. For each variant, we analyze its statistical properties, derive the corresponding exploration policy, and design an efficient algorithm for computing it. Both theoretical analysis and experiments support the usefulness of the proposed methods.",
        "DOI": "NA",
        "paper_author": "Wan R.",
        "affiliation_name": "NC State University",
        "affiliation_city": "Raleigh",
        "affiliation_country": "United States",
        "affiliation_id": "60004923",
        "affiliation_state": "NC"
    },
    {
        "paper_title": "Enterprise Digital Transformation and Debt Financing Capability&lt;Subtitle&gt;Evidence from Chinese Manufacturing Firms",
        "publication": "Proceedings - 2022 6th Annual International Conference on Data Science and Business Analytics, ICDSBA 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "We examine the benefits of enterprise digital transformation from the perspective of debt financing capacity. Considering the essentiality of enterprise digital transformation, we quantify digital transformation in two dimensions: the degree of digital technologies application and the level of investment in digital technologies, with textual analysis, machine learning, and manual collation. We find that digital transformation can significantly improve corporates debt financing capacity, and the effect is particularly remarkable among state-owned enterprises, which receive substantial government support and policy subsidies. The findings extend the microeconomic effects of digital transformation and provide empirical evidence for the government in guiding enterprise digital transformation.",
        "DOI": "10.1109/ICDSBA57203.2022.00024",
        "paper_author": "Liang Y.",
        "affiliation_name": "Wuhan University of Technology",
        "affiliation_city": "Wuhan",
        "affiliation_country": "China",
        "affiliation_id": "60022414",
        "affiliation_state": "Hubei"
    },
    {
        "paper_title": "Generative Flow Networks for Discrete Probabilistic Modeling",
        "publication": "Proceedings of Machine Learning Research",
        "citied_by": "10",
        "cover_date": "2022-01-01",
        "Abstract": "We present energy-based generative flow networks (EB-GFN), a novel probabilistic modeling algorithm for high-dimensional discrete data. Building upon the theory of generative flow networks (GFlowNets; Bengio et al., 2021b), we model the generation process by a stochastic data construction policy and thus amortize expensive MCMC exploration into a fixed number of actions sampled from a GFlowNet. We show how GFlowNets can approximately perform large-block Gibbs sampling to mix between modes. We propose a framework to jointly train a GFlowNet with an energy function, so that the GFlowNet learns to sample from the energy distribution, while the energy learns with an approximate MLE objective with negative samples from the GFlowNet. We demonstrate EB-GFN's effectiveness on various probabilistic modeling tasks. Code is publicly available at github.com/zdhNarsil/EB GFN.",
        "DOI": "NA",
        "paper_author": "Zhang D.",
        "affiliation_name": "Montreal Institute for Learning Algorithms",
        "affiliation_city": "Montreal",
        "affiliation_country": "Canada",
        "affiliation_id": "60113142",
        "affiliation_state": "QC"
    },
    {
        "paper_title": "TabNAS: Rejection Sampling for Neural Architecture Search on Tabular Datasets",
        "publication": "Advances in Neural Information Processing Systems",
        "citied_by": "3",
        "cover_date": "2022-01-01",
        "Abstract": "The best neural architecture for a given machine learning problem depends on many factors: not only the complexity and structure of the dataset, but also on resource constraints including latency, compute, energy consumption, etc. Neural architecture search (NAS) for tabular datasets is an important but under-explored problem. Previous NAS algorithms designed for image search spaces incorporate resource constraints directly into the reinforcement learning (RL) rewards. However, for NAS on tabular datasets, this protocol often discovers suboptimal architectures. This paper develops TabNAS, a new and more effective approach to handle resource constraints in tabular NAS using an RL controller motivated by the idea of rejection sampling. TabNAS immediately discards any architecture that violates the resource constraints without training or learning from that architecture. TabNAS uses a Monte-Carlo-based correction to the RL policy gradient update to account for this extra filtering step. Results on several tabular datasets demonstrate the superiority of TabNAS over previous reward-shaping methods: it finds better models that obey the constraints.",
        "DOI": "NA",
        "paper_author": "Yang C.",
        "affiliation_name": "Google LLC",
        "affiliation_city": "Mountain View",
        "affiliation_country": "United States",
        "affiliation_id": "60006191",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "A Temporal-Difference Approach to Policy Gradient Estimation",
        "publication": "Proceedings of Machine Learning Research",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "The policy gradient theorem (Sutton et al., 2000) prescribes the usage of a cumulative discounted state distribution under the target policy to approximate the gradient. Most algorithms based on this theorem, in practice, break this assumption, introducing a distribution shift that can cause the convergence to poor solutions. In this paper, we propose a new approach of reconstructing the policy gradient from the start state without requiring a particular sampling strategy. The policy gradient calculation in this form can be simplified in terms of a gradient critic, which can be recursively estimated due to a new Bellman equation of gradients. By using temporal-difference updates of the gradient critic from an off-policy data stream, we develop the first estimator that side-steps the distribution shift issue in a model-free way. We prove that, under certain realizability conditions, our estimator is unbiased regardless of the sampling strategy. We empirically show that our technique achieves a superior bias-variance trade-off and performance in presence of off-policy samples. The implementation of the experiments can be found at https://github.com/SamuelePolimi/temporal-difference-gradient.",
        "DOI": "NA",
        "paper_author": "Tosatto S.",
        "affiliation_name": "University of Alberta",
        "affiliation_city": "Edmonton",
        "affiliation_country": "Canada",
        "affiliation_id": "60030835",
        "affiliation_state": "AB"
    },
    {
        "paper_title": "Early environmental quality and life-course mental health effects: The Equal-Life project",
        "publication": "Proceedings of the International Congress on Acoustics",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "There is increasing evidence that a complex interplay of factors within children's environments, including environmental noise, contributes to mental ill-health and suboptimal cognitive development later on in life. The concept of the life-course exposome helps to study the impact of the physical (e.g., acoustic) and social environment on cognitive development and mental health in young people over time. Equal-Life develops and tests combined exposures. Data from eleven studies (N=240.000) linked to exposure data will provide insights and policy guidance into aspects of physical and social exposures hitherto untapped at different scale levels and timeframes while accounting for social inequities. Exposure assessment combines GIS-based environmental indicators with omics approaches and new data sources, forming the early-life exposome. Statistical tools integrate data at different spatial and temporal granularity and combine exploratory machine learning models with hypothesis-driven causal modeling. Equal-Life contributes to the exposome concept by 1) integrating the internal, physical and social exposomes, 2) studying life-course effects on a child's development and mental health, 3) characterizing the child's environment at different stages and in different activity spaces, 4) looking at supportive environments for child development, rather than merely pollutants, 5) combining physical, social indicators with novel effect markers.",
        "DOI": "NA",
        "paper_author": "van Kamp I.",
        "affiliation_name": "Rijksinstituut voor Volksgezondheid en Milieu",
        "affiliation_city": "Bilthoven",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60026125",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Visual Navigation of Mobile Robots in Complex Environments Based on Distributed Deep Reinforcement Learning",
        "publication": "Proceedings of 2022 6th Asian Conference on Artificial Intelligence Technology, ACAIT 2022",
        "citied_by": "4",
        "cover_date": "2022-01-01",
        "Abstract": "The increasingly popular method of deep reinforce- ment learning can not only help mobile robots output accurate actions in complex environments but can also search for collisionfree paths. In this paper, a robot visual navigation model in complex environments based on distributed deep reinforcement learning is proposed. According to the characteristics of different regions in the complex environment, the environment is divided into several regions, and we proposed method can realize visual navigation in large scene complex environments. In these regions, we combine long-short term memory (LSTM) and proximal policy optimization (PPO) algorithms as a local visual navigation model and design a new reward function that trains the target through factors such as the action of mobile robots, the distance between robots and the target, and the running time of robots. We create respective experience pool independently through model training. The model of robot visual navigation via distributed deep reinforcement learning uses the RGB-D image obtained from the first perspective of mobile robots and the polar coordinates of the target in mobile robots coordinate system as input, and the continuous motion of mobile robots as output to realize the task of end-to-end visual navigation without maps. Our model can complete accurately robot visual navigation in large complex scenes without maps and human intervention. In our experiments, we verify our proposed model by performing the promising navigation tasks in virtual environments.",
        "DOI": "10.1109/ACAIT56212.2022.10137974",
        "paper_author": "Zhang Y.",
        "affiliation_name": "Shenzhen Institute of Advanced Technology",
        "affiliation_city": "Shenzhen",
        "affiliation_country": "China",
        "affiliation_id": "60102083",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Economic Security 2021 Fiscal Year Report",
        "publication": "Asia-Pacific Review",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Ensuring Japan's technological and industrial competitiveness is an important issue from the perspective of economic security. Given Japan's current advanced technological capabilities, it is necessary to consider the kind of technology that should receive investment from the viewpoint of mid- to long-term economic security; also important is how technology policy should be formulated. In fiscal year 2021, the Economic Security Study Group of Nakasone Peace Institute (NPI) examined AI/machine learning, quantum computers, and next-generation communication technologies, particularly from the perspective of ensuring Japan's superiority and thus indispensability, and compiled a set of recommendations.",
        "DOI": "10.1080/13439006.2022.2174320",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Reinforcement Learning From Scratch: Understanding Current Approaches - with Examples in Java and Greenfoot",
        "publication": "Reinforcement Learning From Scratch: Understanding Current Approaches - with Examples in Java and Greenfoot",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "In ancient games such as chess or go, the most brilliant players can improve by studying the strategies produced by a machine. Robotic systems practice their own movements. In arcade games, agents capable of learning reach superhuman levels within a few hours. How do these spectacular reinforcement learning algorithms work? With easy-to-understand explanations and clear examples in Java and Greenfoot, you can acquire the principles of reinforcement learning and apply them in your own intelligent agents. Greenfoot (M.Kölling, King’s College London) and the hamster model (D. Bohles, University of Oldenburg) are simple but also powerful didactic tools that were developed to convey basic programming concepts. The result is an accessible introduction into machine learning that concentrates on reinforcement learning. Taking the reader through the steps of developing intelligent agents, from the very basics to advanced aspects, touching on a variety of machine learning algorithms along the way, one is allowed to play along, experiment, and add their own ideas and experiments.",
        "DOI": "10.1007/978-3-031-09030-1",
        "paper_author": "Lorenz U.",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "AI in Learning: Designing the Future",
        "publication": "AI in Learning: Designing the Future",
        "citied_by": "11",
        "cover_date": "2022-01-01",
        "Abstract": "AI (Artificial Intelligence) is predicted to radically change teaching and learning in both schools and industry causing radical disruption of work. AI can support well-being initiatives and lifelong learning but educational institutions and companies need to take the changing technology into account. Moving towards AI supported by digital tools requires a dramatic shift in the concept of learning, expertise and the businesses built off of it. Based on the latest research on AI and how it is changing learning and education, this book will focus on the enormous opportunities to expand educational settings with AI for learning in and beyond the traditional classroom. This open access book also introduces ethical challenges related to learning and education, while connecting human learning and machine learning. This book will be of use to a variety of readers, including researchers, AI users, companies and policy makers.",
        "DOI": "10.1007/978-3-031-09687-7",
        "paper_author": "Niemi H.",
        "affiliation_name": "Helsingin Yliopisto",
        "affiliation_city": "Helsinki",
        "affiliation_country": "Finland",
        "affiliation_id": "60002952",
        "affiliation_state": "Uusimaa"
    },
    {
        "paper_title": "Logic Locking: A Practical Approach to Secure Hardware",
        "publication": "Logic Locking: A Practical Approach to Secure Hardware",
        "citied_by": "5",
        "cover_date": "2022-01-01",
        "Abstract": "A subtle change that leads to disastrous consequences—hardware Trojans undoubtedly pose one of the greatest security threats to the modern age. How to protect hardware against these malicious modifications? One potential solution hides within logic locking; a prominent hardware obfuscation technique. In this book, we take a step-by-step approach to understanding logic locking, from its fundamental mechanics, over the implementation in software, down to an in-depth analysis of security properties in the age of machine learning. This book can be used as a reference for beginners and experts alike who wish to dive into the world of logic locking, thereby having a holistic view of the entire infrastructure required to design, evaluate, and deploy modern locking policies.",
        "DOI": "10.1007/978-3-031-19123-7",
        "paper_author": "Sisejkovic D.",
        "affiliation_name": "Rheinisch-Westfälische Technische Hochschule Aachen",
        "affiliation_city": "Aachen",
        "affiliation_country": "Germany",
        "affiliation_id": "60016653",
        "affiliation_state": "Nordrhein-Westfalen"
    },
    {
        "paper_title": "Reinforcement Learning Based Optimal Adversarial Pathway Estimation Using Remotely Sensed Spectral-Terrain Data and Human Value Assessment",
        "publication": "Proceedings of the International Conference on Statistics",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Geo-intelligence organizations are often faced with the need to determine optimal pathways that adversaries may take based on various types of information including remotely sensed imagery and human geo-intelligence. The mobile enemy problem, where the objective is to predict the pathway that a mobile enemy may take, is considered here as a way to develop a statistical/signal processing formulism to assist leadership in making better decisions about how to estimate the whereabouts of an adversary. A two-tier processing pipeline utilizing feature extraction and reinforcement learning-based optimal pathway estimation was created to demonstrate how human/machine learning teaming can be exploited to address a geo-intelligence problem. The information used in the processor development consists of an open-source hyperspectral imagery (HSI) data set [1]. A strip map of terrain HSI was divided into 32 x 32 pixel image chips where principal component analysis [2] was used to reduce the dimension and decrease the noise of the hyperspectral signatures. Spectral dictionary endmembers [3] were estimated from the denoised HSI data using the unsupervised learning algorithms of k-means clustering [4] and automatic target generator processing [3]. This substage was necessary in order to perform image chip value estimation. In this evaluation stage, five different algorithms were used to calculate different value fields. Each technique used a feature extraction method designating the relative value of each image chip comprising the complete HSI scene. The first algorithm used for HSI image chip value estimation consisted of abundance estimation via nonnegative constrained least squares matched filtration [3] along with a Mahalanobis distance metric [5]. The second algorithm used was abundance estimation via orthogonal matching pursuit [6] along with a Euclidean distance metric. The final three algorithmic methods consisted of threshold-based spatial HSI spectral gradient estimation, Laplacian eigenmap kurtosis [7] estimation,, and finally human value assessment of HSI image chips. Value field estimation for each of the five algorithms was quantitatively possible but HSI data noise necessitated robust ways to obtain smoother and physically sensible value fields. A linear combination of all five value fields was proposed as a way to accomplish this. However, since human value assignment is often considered the most important value field component in many geo-intelligence problems, it was used in the development of the second processing part-optimal pathway estimation. Q-learning and State-Action-Reward-State-Action (SARSA) learning [8,9], trial and error reinforcement learning algorithms fueled by human assigned value fields, were used to estimate optimal pathways that an adversary would take over the HSI scene. Human assigned reward fields delineate useful thoroughfares for reaching a goal state providing a utility function Q(s,a) representing the discounted cumulative reward for taking a specific action from a specific state. Q-learning and SARSA learning both utilized the maximum action policy and Dijkstra’s algorithm [10] to estimate the optimal pathway from the Q function which quantifies how good an action is given a certain state. Preliminary results show that Q-learning and the maximum action policy with a learning rate of γ = 0.8 provides an agent or adversarial pathway which varies widely over the HSI scene. Q-learning used with Dijkstra’s algorithm causes the optimal pathway to vary less widely over the HSI data consistent with the least step principle policy. SARSA learning used in conjunction with Dijkstra’s algorithm shows a similar trend of less variation over the scene but with an optimal pathway which does not strictly follow the peaks of reward field. The explicit reason for this is not clear. Preliminary results suggest that the human-computational formulism is a viable platform for future development of a robust geo-intelligence processor.",
        "DOI": "10.11159/icsta22.112",
        "paper_author": "Affourtit J.",
        "affiliation_name": "Improbable LLC",
        "affiliation_city": "Arlington",
        "affiliation_country": "United States",
        "affiliation_id": "129812126",
        "affiliation_state": "VA"
    },
    {
        "paper_title": "Nowcasting short-term indicators with machine learning methods",
        "publication": "Statistical Journal of the IAOS",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "The monthly European Union (EU) harmonized Short-term Business Statistics (STS) represents one of the most important sources for the assessment of the European economy. Timeliness of STS is of fundamental importance for policy makers to be able to react adequately to sudden economic changes. In the past time lags between reference periods and release dates have been quite considerable. However, European countries selected various approaches to shorten release times like optimizing the short-term statistics sample or increasing efforts to access and integrate administrative data. In this paper different machine learning algorithms for early estimation of missing survey data are evaluated in order to further improve timeliness of Austrian STS data and to increase granularity of early estimates as well. Currently a multivariate time series model is used for early estimation of economic indexes for the highly aggregated level of Total Industry and Construction. This model could be adapted to the level of NACE-Divisions with the exception of a few Divisions with small populations. The quality of the results could be improved for several NACE-Divisions and variables with machine learning methods. However, for the prediction of a few branches with small populations alternative methods have to be developed.",
        "DOI": "10.3233/SJI-220002",
        "paper_author": "Fröhlich M.",
        "affiliation_name": "Statistics Austria",
        "affiliation_city": "Vienna",
        "affiliation_country": "Austria",
        "affiliation_id": "120852250",
        "affiliation_state": "Vienna"
    },
    {
        "paper_title": "An Exploratory Analysis of Delhi Air Quality Using Statistics and Machine Learning Models",
        "publication": "2022 IEEE Conference on Interdisciplinary Approaches in Technology and Management for Social Innovation, IATMSI 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Air pollution is one of the most significant concerns of the present era, which has severe and alarming effects on human health and the environment, thereby escalating the climate change issue. Hence, in-depth analysis of air pollution data and accurate air quality forecasting is crucial in controlling the growing pollution levels. It also aids in designing appropriate policies to prevent exposure to toxic pollutants and taking necessary precautionary measures. Air quality in Delhi, the capital of India, is inferior compared to other major cities in the world. In this study, daily and hourly concentrations of air pollutants in the Delhi region were collected and analyzed using various methods. A comparative analysis is performed based on months, seasons, and the topography of different stations. The effect of the Covid-19 lockdown on the reduction of pollutant levels is also studied. A correlation analysis is performed on the available data to show the relationships and dependencies among different pollutants, their relationship with weather parameters, and the correlations between the stations. Various machine learning models were used for air quality forecasting, like Linear Regression, Vector Auto Regression, Gradient Boosting Machine, Random Forest, and Decision Tree Regression. The performance of these models was compared using RMSE, MAE, and MAPE metrics. This study is focused on the dire state of air pollution in Delhi, the primary reasons behind it, and the efficacy of calculated lockdowns in bringing down pollution levels. It also highlights the potential of Linear Regression and Decision Tree Regression models in predicting the air quality for different time intervals.",
        "DOI": "10.1109/IATMSI56455.2022.10119423",
        "paper_author": "Chakravarty A.",
        "affiliation_name": "Pondicherry University",
        "affiliation_city": "Puducherry",
        "affiliation_country": "India",
        "affiliation_id": "60013919",
        "affiliation_state": "PY"
    },
    {
        "paper_title": "X-Fold Variants Approach Policies Based APT Prevention and Detection Solution",
        "publication": "2022 IEEE Information Technologies and Smart Industrial Systems, ITSIS 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Looking to the standardized solutions which fail continuously and need humans intervention at each level or layer is a concern of the data science and needs to be ameliorated. The large amount of data stored continuously in all its types, varieties, and sources like social networks is a good marketplace where we might test the policy's impact when detecting APTs. Today, representing the most important sources of that data, millions are saved and exchanged of files on a daily and continuous basis, such as videos, pictures, Etc. Analyzing big data came to enable the user to benefit from the best available data in a better and more organized manner by using less effort and cost with high efficiency. [1], [2], [4] As patterns and relationships have been identified between the various data that were not present previously, new ideas on how to deal with the massive amount of data in an instant and fast. Also, several tools have been discovered to extract data and analyze and summarize them in an accessible and understandable way.",
        "DOI": "10.1109/ITSIS56166.2022.10118426",
        "paper_author": "Al-Aamri A.S.",
        "affiliation_name": "International Islamic University Malaysia",
        "affiliation_city": "Kuala Lumpur",
        "affiliation_country": "Malaysia",
        "affiliation_id": "60016775",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "How COVID-19 is Accelerating the Digital Revolution: Challenges and Opportunities",
        "publication": "How COVID-19 is Accelerating the Digital Revolution: Challenges and Opportunities",
        "citied_by": "3",
        "cover_date": "2022-01-01",
        "Abstract": "This book explores how digital technologies have proved to be a useful and necessary tool to help ensure that local and regional governments on the frontline of the emergency can continue to provide essential public services during the COVID-19 crisis. Indeed, as the demand for digital technologies grows, local and regional governments are increasingly committed to improving the lives of their citizens under the principles of privacy, freedom of expression and democracy. The Digital Revolution began between the late 1950s and 1970s and represents the evolution of technology from the mechanical and analog to the digital. The advent of digital technology has also changed how humans communicate today using computers, smartphones and the internet. Further, the digital revolution has made a tremendous wealth of information accessible to virtually everyone. In turn, the book focuses on key challenges for local and regional governments concerning digital technologies during this crisis, e.g. the balance between privacy and security, the digital divide, and accessibility. Privacy is a challenge in the mitigation of COVID-19, as governments rely on digital technologies like contact-tracking apps and big data to help trace peoples patterns and movements. While these methods are controversial and may infringe on rights to privacy, they also appear to be effective measures for rapidly controlling and limiting the spread of the virus. Next, the book discusses the 10 technology trends that can help build a resilient society, as well as their effects on how we do business, how we work, how we produce goods, how we learn, how we seek medical services and how we entertain ourselves. Lastly, the book addresses a range of diversified technologies, e.g. Online Shopping and Robot Deliveries, Digital and Contactless Payments, Remote Work, Distance Learning, Telehealth, Online Entertainment, Supply Chain 4.0, 3D Printing, Robotics and Drones, 5G, and Information and Communications Technology (ICT).",
        "DOI": "10.1007/978-3-030-98167-9",
        "paper_author": "Anandan R.",
        "affiliation_name": "Vels Institute of Science, Technology &amp; Advanced Studies",
        "affiliation_city": "Chennai",
        "affiliation_country": "India",
        "affiliation_id": "60105237",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Proceedings - 2022 2nd International Symposium on Artificial Intelligence and its Application on Media, ISAIAM 2022",
        "publication": "Proceedings - 2022 2nd International Symposium on Artificial Intelligence and its Application on Media, ISAIAM 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "The proceedings contain 38 papers. The topics discussed include: cold chain logistics sterilizer based on plasma technology: flipping mechanism and intelligent system design; application research of recommendation algorithm based on K-means clustering and feature tree in human resource information system; a study of aspect-level sentiment analysis based on deep learning; analysis and research on the policy performance of college students returning home to start a business based on data visualization ‘artificial intelligence + agriculture’; research on Guilin cultural and creative products development based on clustering algorithm; simulated annulling in convolutional neural network; path planning of follow-up transport robot based on machine vision; research and implementation of fire warning algorithm based on deep learning; research on differential weight design model of power grid enterprise asset equipment health base on fuzzy logic and machine learning; and research on the construction of virtual simulation resource system for vocational education based on deep learning.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Beyond Counting Words: Assessing Performance of Dictionaries, Supervised Machine Learning, and Embeddings in Topic and Frame Classif ication",
        "publication": "Computational Communication Research",
        "citied_by": "10",
        "cover_date": "2022-01-01",
        "Abstract": "Topics and frames are at the heart of various theories in communication science and other social sciences, making their measurement of key interest to many scholars. The current study compares and contrasts two main deductive computational approaches to measure policy topics and frames: Dictionary (lexicon) based identif ication, and supervised machine learning. Additionally, we introduce domain-specif ic word embeddings to these classif ication tasks. Drawing on a manually coded dataset of Dutch news articles and parliamentary questions, our results indicate that super vised machine learning outperforms dictionar y-based classif ication for both tasks. Furthermore, results show that word embeddings may boost performance at relatively low cost by introducing relevant and domain-specif ic semantic information to the classif ication model.",
        "DOI": "10.5117/CCR2022.2.006.KROO",
        "paper_author": "Kroon A.C.",
        "affiliation_name": "Universiteit van Amsterdam",
        "affiliation_city": "Amsterdam",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60002483",
        "affiliation_state": "Noord-Holland"
    },
    {
        "paper_title": "Talking politics: Building and validating data-driven lexica to measure political discussion quality",
        "publication": "Computational Communication Research",
        "citied_by": "3",
        "cover_date": "2022-01-01",
        "Abstract": "Social media data offers computational social scientists the opportunity to understand how ordinary citizens engage in political activities, such as expressing their ideological stances and engaging in policy discussions. This study curates and develops discussion quality lexica from the Corpus for the Linguistic Analysis of Political Talk ONline (CLAPTON). Supervised machine learning classifiers to characterize political talk are evaluated for out-of-sample label prediction and generalizability to new contexts. The approach yields data-driven lexica, or dictionar-ies, that can be applied to measure the constructiveness, justification, relevance, reciprocity, empathy, and incivility of political discussions. In addition, the findings illustrate how the choices made in training such classifiers, such as the heterogeneity of the data, the feature sets used to train classifiers, and the classification approach, affect their generalizability. The article concludes by summarizing the strengths and weaknesses of applying machine learning methods to social media posts and theoretical insights into the quality and structure of online political discussions.",
        "DOI": "10.5117/CCR2022.2.005.JAID",
        "paper_author": "Jaidka K.",
        "affiliation_name": "National University of Singapore",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore",
        "affiliation_id": "60017161",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "GAINING KNOWLEDGE FROM BIG DATA: ENERGY PERFORMANCE CERTIFICATE AS A SOURCE OF INFORMATION TO DECARBONIZE THE BUILT ENVIRONMENT",
        "publication": "International Multidisciplinary Scientific GeoConference Surveying Geology and Mining Ecology Management, SGEM",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "The decarbonization strategies for the built environment that policy-makers face today from the EU mandate risk being made with incomplete or insufficient information. The consequence of this could be ineffective choices, thus slowing down the ongoing ecological transition, or their high cost, whether borne by the state or citizens. The progressive and unstoppable digitization of the built environment offers information collection and previously unthinkable management opportunities. The construction sector, traditionally lagging behind other industrial sectors, is beginning to produce large quantities of data that can be exploited thanks to the most modern techniques derived from the information technology sector. Among the most promising data sources are energy performance certificates for buildings, which provide a snapshot of the characteristics of buildings, their fabric and plant components, and design forecasts of their energy performances. Analyzing the energy performance certificates through Artificial Intelligence techniques proves the effectiveness of using big data in the construction sector. In particular, in this study, unsupervised machine learning techniques led to an in-depth knowledge of a stock of buildings approaching two hundred thousand units distributed over an almost twenty-four thousand square kilometers area in northern Italy.",
        "DOI": "10.5593/sgem2022V/6.2/s26.54",
        "paper_author": "Re Cecconi F.",
        "affiliation_name": "Politecnico di Milano",
        "affiliation_city": "Milan",
        "affiliation_country": "Italy",
        "affiliation_id": "60023256",
        "affiliation_state": "MI"
    },
    {
        "paper_title": "The application of artificial intelligence assurance in precision farming and agricultural economics",
        "publication": "AI Assurance: Towards Trustworthy, Explainable, Safe, and Ethical AI",
        "citied_by": "5",
        "cover_date": "2022-01-01",
        "Abstract": "Agricultural policy has traditionally been conducted in an ad hoc manner, generally, by responding to natural disasters instinctively or managing unavoidable causes through the implementation of short term financial compensations or long-term loan and insurance programs at farms. The presented model, namely AI2Farm, provides farmers with predictions during conventional and unconventional times to compensate for the little to no guidance that policies, such as the Farm Bill, provide for spur-of-the-moment decisions needed to be made that arise from outlier events. Farmers are an integral part of our economy due to their ability to manage market supply and demand expectations that solidify the nation's food security. Therefore it's important that farmers have access to the most up-to-date technology to make sound decisions that are in the best interest of rural and urban communities. Machine learning (ML) models measure associations, correlations, and causations of global and domestic events via commonplace financial indices with the production, consumption, and pricing of global agricultural commodities in the United States. Consequently, a deeper understanding of changes in behavior displayed by farmers as a result of outlier events aid in the ability to determine how precision agriculture can best assist farmers in the decision-making process. This entire set of information is lastly applied to the analysis of farms in the state of Virginia with smart tools and equipment that can benefit from models such as AI2Farm; the model and its results are presented and discussed.",
        "DOI": "10.1016/B978-0-32-391919-7.00029-9",
        "paper_author": "Williams M.J.",
        "affiliation_name": "University of Mary Washington",
        "affiliation_city": "Fredericksburg",
        "affiliation_country": "United States",
        "affiliation_id": "60029032",
        "affiliation_state": "VA"
    },
    {
        "paper_title": "Socially responsible AI assurance in precision agriculture for farmers and policymakers",
        "publication": "AI Assurance: Towards Trustworthy, Explainable, Safe, and Ethical AI",
        "citied_by": "6",
        "cover_date": "2022-01-01",
        "Abstract": "As one solution to feeding a growing population with finite resources, some farmers, researchers, and agricultural technology providers (ATPs) have turned to precision agriculture (PA). PA is the practice of mapping out precise input application to maximize the yield. To do this, ATPs collect input and output data from farmers and use Artificial Intelligence and machine learning to build prescription maps, which farmers can program farm equipment to follow. The use of PA has allowed farmers to use less resources, which saves money and reduces environmental impact. However, technology is a two-sided coin, benefiting both end-users, the farmers, and ATPs differently. In agriculture, power asymmetry has been cited as a critical issue existing between farmers and ATPs,and this impacts farmers negatively. For farmers to deploy and have more control over data decision-making on their farms, AI assurance methods need to be integrated into their technologies. There are currently a few studies on this subject in agriculture, but many do not involve agricultural end-users or fall short of meeting the needs of the end-users. If end-users and policymakers are not able to understand how their data is collected and used in the agricultural AI models, they will not be able to make educated decisions about their work. This chapter proposes solutions to benefit all agricultural end-users, including prompting the use of participatory design and adopting more user-centered principles when integrating AI assurance models into agricultural technologies.",
        "DOI": "10.1016/B978-0-32-391919-7.00028-7",
        "paper_author": "Posadas B.B.",
        "affiliation_name": "Virginia Polytechnic Institute and State University",
        "affiliation_city": "Blacksburg",
        "affiliation_country": "United States",
        "affiliation_id": "60027090",
        "affiliation_state": "VA"
    },
    {
        "paper_title": "Symptom Based Models of COVID-19 Infection Using AI",
        "publication": "Artificial Intelligence in Covid-19",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Coronavirus Disease 2019 (COVID-19) caused by the Severe Acute Respiratory Syndrome Coronavirus-2 (SARS-CoV2) has spread around the world in a global pandemic [1-4]. Early and daily detection of suspected COVID-19 patients is the most important approach not only for tracing close contacts to prevent further spread [5], but also providing crucial information for healthcare providers and officials to make resource allocation and policy decisions [6].",
        "DOI": "10.1007/978-3-031-08506-2_8",
        "paper_author": "Liu S.",
        "affiliation_name": "Southeast University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60005244",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "AIM in Medical Education",
        "publication": "Artificial Intelligence in Medicine",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Artificial intelligence (AI) is making a global impact on various professions ranging from commerce to healthcare. This section looks at how it is beginning and will continue to impact other areas such as medical education. The multifaceted yet socrato-didactic methods of education need to evolve to cater for the twenty-firstcentury medical educator and trainee. Advances in machine learning and artificial intelligence are paving the way to new discoveries in medical education delivery. Methods This chapter begins by introducing the broad concepts of AI that are relevant to medical education and then addresses some of the emerging technologies employed to directly cater for aspects of medical education methodology and innovations to streamline education delivery, education assessments, and education policy. It then builds on this to further explore the nature of new artificial intelligence concepts for medical education delivery, educational assessments, and clinical education research discovery in a PRISMAguided systematic review and meta-analysis. Results Results from the meta-analysis showed improvement from using either AI alone or with conventional education methods compared to conventional methods alone. A significant pooled weighted mean difference ES estimate of ES 4.789; CI 1.9-7.67; p 1/4 0.001, I2 1/4 93% suggests a 479% learner improvement across domains of accuracy, sensitivity to performing educational tasks, and specificity. Significant amount of bias between studies was identified and a model to reduce bias is proposed. Conclusion AI in medical education shows considerable promise in domains of improving learners’ outcomes; this chapter rounds off its discussion with the role of AI in simulation methodologies and performance assessments for medical education, highlighting areas where it could augment how we deliver training.",
        "DOI": "10.1007/978-3-030-64573-1_30",
        "paper_author": "Davids J.",
        "affiliation_name": "Imperial College Healthcare NHS Trust",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60111785",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Artificial Intelligence (AI) Applications Using Big Data and Survey Data for Exploring the Existence of the Potential Users of Public Transportation System",
        "publication": "International Journal of Information and Management Sciences",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "The government places emphasis on increasing the usage rate of public transportation nowadays due to public transportation having many benefits for the environment. In or-dertounder stand the key factors of trip generation and identify the key trip purposes for selecting transportation modes in a target city, the cell phone data and personal trip survey data were studied by using the machine learning methods of Association Analysis and Inverse Reinforcement Learning. Findings such as hospital, park and elementary school are the most important elements implies that the facilities for mandatory task will attract more people. Also, the elderly age group has very strong tendency to use private vehicle compared to other age groups implies that attracting more young people may be a good strategy. Findings can be a reference for new policy planning, including re-planning the exiting routes of bus systems or integrating different public transportation, by the local government.",
        "DOI": "10.6186/IJIMS.202212_33(4).0001",
        "paper_author": "Chang H.C.",
        "affiliation_name": "Waseda University",
        "affiliation_city": "Tokyo",
        "affiliation_country": "Japan",
        "affiliation_id": "60023462",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Performing streaming analytics on tweets (text and images) data",
        "publication": "Streaming Analytics: Concepts, architectures, platforms, use cases and applications",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "In today’s world, Twitter is one of the most popular and powerful social networking platforms. People interact, voice out their opinions, express their thoughts, start conversations about various issues through “tweets” and the market has started analyzing these tweets to get a better understanding of how the general public feels about their products, schemes, or new policies. This helps them to get their perspective directly and work towards making them more people-friendly. Different organizations and consumers are using various mechanisms to perform the sentiment analysis. In this work, two different methods are proposed to perform Twitter sentiment analysis. One of them is using Node-RED and IBM services where realtime data is harvested i.e. tweets from Twitter using Twitter’s API from its developer tool. Data is then stored into the IBM cloudant and then the tweets are classified into positive, negative or neutral based on their sentiment score which lies between 5 and +5. Using IBM Watson’s text to speech service, the tweets are converted to the audio format so that the user could listen to them one by one and the IBM Watson image analyzer is used to analyze what the image represents and it also shows the confidence and different features of the image. Using the visual recognition service, the tone of the tweet is analyzed among emotions like joy, anger, disgust, etc. and the Node-RED dashboard can be created to see the sentiment scores of various tweets. The other proposed method is using Python in machine learning where the sentiment analysis of the tweets is performed with the sentiment score using different classifiers and those classifiers are compared based on their precision, recall, f1 score, and accuracy to conclude which one of them is the most appropriate one for this work. Real-time data is harvested from Twitter, converted into a CSV file and analyzed into positive, negative, or neutral according to their sentiment scores using the classifiers.",
        "DOI": "NA",
        "paper_author": "Choubey A.",
        "affiliation_name": "Vellore Institute of Technology",
        "affiliation_city": "Vellore",
        "affiliation_country": "India",
        "affiliation_id": "60010618",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "The Prevalence of Big Data Analytics in Public Policy: Is There a Research-Pedagogy Gap?",
        "publication": "Emerging Pedagogies for Policy Education: Insights from Asia",
        "citied_by": "2",
        "cover_date": "2022-01-01",
        "Abstract": "While the emergence of big data creates several opportunities for governance and public policy, new capabilities, processes, standards, strategies, and technologies will be essential for deriving value from big data while addressing issues and mitigating risks posed by it. Arguably, above all, these will require the integration of big data analytics in public policy pedagogy. In this chapter, we examine whether public policy education has kept pace with the growth of big data research in the field, with a specific focus on the Asian context. We start by identifying prominent themes in research on big data in governance and public policy. We supplement that with an analysis of academia’s response by examining the prevalence of big data teaching in public policy programs. Based on our analysis, we highlight the current geographic and institutional dominance in research and teaching on the topic, and conclude with key lessons for the design, content, and delivery of pedagogy on big data in public policy.",
        "DOI": "10.1007/978-981-16-5864-8_6",
        "paper_author": "Goyal N.",
        "affiliation_name": "Faculteit Techniek, Bestuur en Management, TU Delft",
        "affiliation_city": "Delft",
        "affiliation_country": "Netherlands",
        "affiliation_id": "60117886",
        "affiliation_state": "Zuid-Holland"
    },
    {
        "paper_title": "Agriculture, Livestock Production and Aquaculture: Advances for Smallholder Farming Systems: Volume 2",
        "publication": "Agriculture, Livestock Production and Aquaculture: Advances for Smallholder Farming Systems: Volume 2",
        "citied_by": "6",
        "cover_date": "2022-01-01",
        "Abstract": "This two-volume set discusses recent approaches and technological innovations for sustainable agriculture in smallholder farming systems impacted by climate change. The systems covered include crop-based agricultural production, as well as aquaculture and livestock production as related systems using similar techniques to combat food security issues brought about by climate change and resource overuse. The chapters detail innovations involving crop diversification, soil resilience management, geoinformatics and land suitability monitoring for smart farming, information technology in livestock production, and nutrient resource management in fishery aquaculture. Researchers, practitioners and industries will be able to use this information to implement socially and economically sustainable practices to achieve food security in impoverished areas vulnerable to climate change, while also learning about the rapid evolution in information technology that is applicable for and available to small holder farmers. Volume 2 focuses on trends and technologies in food security within the context of sustainable practices, drone technology, microwave data, molecular farming, machine learning, agricultural economics, spatial modeling and agricultural policy. These chapters discuss advancements in fishery resource and aquaculture practices, and also the challenges facing these areas due to climate change.",
        "DOI": "10.1007/978-3-030-93262-6",
        "paper_author": "Kumar A.",
        "affiliation_name": "Rani Lakshmi Bai Central Agricultural University",
        "affiliation_city": "Jhansi",
        "affiliation_country": "India",
        "affiliation_id": "60283012",
        "affiliation_state": "UP"
    },
    {
        "paper_title": "Artificial Intelligence in Epidemiology",
        "publication": "Artificial Intelligence in Medicine",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "John Last defined epidemiology as “The study of the distribution and determinants of health-related states or events in specified populations, and the application of this study to the control of health problems.” It underscores that epidemiologists are not concerned only with disease but with health-related events, and that ultimately epidemiology is committed to control of disease. Initially focused on the disease, the objects of investigation in epidemiology now correspond to any factor that may influence the state of health of the human being, i.e., biological, clinical factors, in relation to the physical, mental, and social environment. Regardless of the field considered and the type of epidemiology (clinical or population based) referred to, the basic brick of epidemiology remains the data. The data must be as valid and precise as possible to ensure validity and reliability of results. The use of artificial intelligence and its methods can occur at different levels and in several areas of epidemiology. At present, we can consider three main use cases. First, AI can add to a long tradition of using more or less sophisticated observational data analysis methods. It has a role to play in causal inference. Second, AI can intervene at the stage of reconciling and structuring siled and varied data sources. Finally, AI can simply bring new ways of exploring and using data, such as sentiment analysis applied to social media. These three use cases are found, in practice, often intermingled and do not necessarily meet in isolation from each other. The private sector (intermediation platforms) and policy makers are the two other actors involved in the forms that AI uses in epidemiology will take.",
        "DOI": "10.1007/978-3-030-64573-1_97",
        "paper_author": "Lefèvre T.",
        "affiliation_name": "Institut de Recherche Interdisciplinaire sur les Enjeux Sociaux (IRIS)",
        "affiliation_city": "Bobigny",
        "affiliation_country": "France",
        "affiliation_id": "60245256",
        "affiliation_state": "Ile-de-France"
    },
    {
        "paper_title": "AIM and Business Models of Healthcare",
        "publication": "Artificial Intelligence in Medicine",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "Artificial intelligence (AI) and machine learning in healthcare are growing at an unprecedented rate. Myriad uses of medical AI, ranging from tumor identification on imaging to workforce management, make use of a wealth of available healthcare data. These models are becoming increasingly commercially available. However, much of the utility of medical AI depends on the quality of the data models trained on, and critically, the contexts and biases within which these models are created. In this chapter, we first describe a business-informed framework that influences product development and commercialization of these technologies. We describe the consumer side that includes purchasers, end users, and patients. Subsequently, we underscore the pitfalls of the assumption that models trained in one context can be applied to another, that is, the myth of generalizability. We propose solutions to these problems and describe the importance of co-creation and multi-stakeholder engagement in designing medical AI. We highlight the need to define value metrics that consider equity and the mitigation of healthcare disparities. Lastly, we draw attention to open ethical, legal, and policy questions that must be answered as the role of AI in medicine progresses and grows.",
        "DOI": "10.1007/978-3-030-64573-1_247",
        "paper_author": "Dee E.C.",
        "affiliation_name": "Harvard Medical School",
        "affiliation_city": "Boston",
        "affiliation_country": "United States",
        "affiliation_id": "60002746",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Koga2022 Dataset: Comprehensive Dataset with Detailed Classification for Network Intrusion Detection Systems",
        "publication": "Proceedings - 2022 10th International Symposium on Computing and Networking Workshops, CANDARW 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "In this paper, we have created a communication dataset that addresses the problems of existing communication datasets and provides a fine-grained classification of malicious communications. Related research used publicly available datasets labeled normal and malicious to increase the accuracy of NIDS detection, as well as to study NIDS specific to certain attack scenarios. However, the existing dataset has an outdated creation date and only labels normal and malignant. Furthermore, even if attack scenarios that indicate broad policies for malicious communications are known, detailed classifications that indicate actual methods, etc., are few or unknown. To solve these problems, we created the Koga2022 Dataset, which is a detailed classification of malicious communication data collected using multiple penetration tools as well as a honeypot that is open to the outside world. This paper presents the statistics of the Koga2022 Dataset that was created. We then discuss validation methods that may be difficult with existing datasets and demonstrate the usefulness of the Koga2022 Dataset.",
        "DOI": "10.1109/CANDARW57323.2022.00024",
        "paper_author": "Sato H.",
        "affiliation_name": "Kogakuin University",
        "affiliation_city": "Tokyo",
        "affiliation_country": "Japan",
        "affiliation_id": "60019032",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "3rd International Conference on Innovations in Computer Science and Software Engineering, ICONICS 2022",
        "publication": "3rd International Conference on Innovations in Computer Science and Software Engineering, ICONICS 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "The proceedings contain 28 papers. The topics discussed include: experimentation for decentralized resource-based multi-pool mining in Ethereum blockchain; performance enhancement in agriculture sector based on image processing; frameworks, applications and challenges in streaming big data analytics: a review; prevention and safety strategies for road accidents by using evolutionary algorithms; intrusion detection using deep learning techniques; analyzing applicant’s pre-admission data and predicting applicant’s performance in pre-admission test using data mining techniques; quantum computing and machine learning algorithms - a review; real-time violence detection using deep learning techniques; a study on the e-commerce trends using data analysis; and a systematic review on the motivations of cyber-criminals and their attacking policies.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Comparison of Undergraduate Students Performance Admitted Through Two Different Admission Criterion",
        "publication": "3rd International Conference on Innovations in Computer Science and Software Engineering, ICONICS 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Data mining is the most common way of utilizing factual strategies to uncover examples and bits of knowledge inside huge datasets. Commonly, the datasets utilized for information mining are huge to the point that would require days, weeks, or months for people to peruse or dissect. Subsequently, information mining frequently includes utilizing programs, Artificial Intelligence (AI), or man-made brainpower to accomplish the work. In any case, human examiners or data set directors frequently be involved to clean and process the information, so that datasets are ready for transformation and examination. With represented information, your information stewards should be proficient in these techniques to prepare machines to uncover these experiences and supervise their outcomes to confirm they are right. This article is a fast aide and introduction to data mining. In this paper, a brief comparison of the performance of undergraduate students has been analyzed, who got admission through two different admission criterions in a public sector engineering university. This research paper aims to identify the consequence of different admission policies on the future performance of engineering students.",
        "DOI": "10.1109/ICONICS56716.2022.10100381",
        "paper_author": "Ul Hoda S.R.",
        "affiliation_name": "NED University of Engineering &amp; Technology",
        "affiliation_city": "Karachi",
        "affiliation_country": "Pakistan",
        "affiliation_id": "60060302",
        "affiliation_state": "Sindh"
    },
    {
        "paper_title": "Time Series Forecasting of Hawai'I Electricity Prices and the Impact of Geothermal Under 100% Renewable Energy Generation Policies",
        "publication": "Transactions - Geothermal Resources Council",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "In 2018, ~72% of Hawai'i's electricity came from fossil fuels, despite Hawai'i's policy objective to reach 100% renewable energy generation by 2045. In 2019, annual median electricity price paid by all sectors was 0.29 $/kWh, more than double the national average. Through machine learning models, including autoregressive, linear, and long short-term memory networks, this study generates a forecast series of future electricity rates. Univariate models offering next-in-sequence predictions often outperform multivariate models with multi-step forecasts. Model results give confidence in predictions of future energy prices under changes to firm power generation, including geothermal and battery storage. Planned and developing firm renewable projects can eliminate almost 80% of coal production on O'ahu and over 15.5% of petroleum production statewide. Using weighted price averaging, estimated electricity rates in Hawai'i for all sectors would increase to 0.32 $/kWh. We posit this cost will decrease as replacement renewable technologies, including geothermal, develop with time.",
        "DOI": "NA",
        "paper_author": "Dores D.",
        "affiliation_name": "School of Ocean and Earth Science and Technology",
        "affiliation_city": "Honolulu",
        "affiliation_country": "United States",
        "affiliation_id": "60003579",
        "affiliation_state": "HI"
    },
    {
        "paper_title": "Reinforcement Learning for Combinatorial Optimization",
        "publication": "Encyclopedia of Data Science and Machine Learning",
        "citied_by": "10",
        "cover_date": "2022-01-01",
        "Abstract": "Combinatorial optimization (CO) problems have many important application domains, including social networks, manufacturing, and transportation. However, as an NP-hard problem, the traditional CO problem-solvers require domain knowledge and hand-crafted heuristics. Facing big data challenges, can we solve these challenging problems with a learning structure within a short time? This article will demonstrate how to solve the combinatorial optimization problems with the deep reinforcement learning (DRL) method. Reinforcement learning (RL) is a subfield of machine learning (ML) that learns the optimal policy over time. Building on Markov decision process, RL has the solid theoretical foundation to obtain the optimal solution. Once parameters of DRL are trained, a new problem case can be solved quickly. Moreover, DRL learns the optimal solution without labels by maximizing the accumulative discounted reward received from the environment. This article will discuss three typical CO problems and present the advantages of DRL over other traditional methods.",
        "DOI": "10.4018/978-1-7998-9220-5.ch170",
        "paper_author": "Wang D.",
        "affiliation_name": "University of Illinois at Chicago",
        "affiliation_city": "Chicago",
        "affiliation_country": "United States",
        "affiliation_id": "60027561",
        "affiliation_state": "IL"
    },
    {
        "paper_title": "IoT for Defense and National Security",
        "publication": "IoT for Defense and National Security",
        "citied_by": "3",
        "cover_date": "2022-01-01",
        "Abstract": "IoT for Defense and National Security covers topics on IoT security, architecture, robotics, sensing, policy, operations, and more, including the latest results from the premier IoT research initiative of the U.S. Defense Department, the Internet of Battle Things. The text also discusses challenges in converting defense industrial operations to IoT and summarizes policy recommendations for regulating government use of IoT in free societies. As a modern reference, this book covers multiple technologies in IoT including survivable tactical IoT using content-based routing, mobile ad-hoc networks, and electronically formed beams. Examples of IoT architectures include using KepServerEX for edge connectivity and AWS IoT Core and Amazon S3 for IoT data. To aid in reader comprehension, the text uses case studies illustrating the challenges and solutions for using robotic devices in defense applications, plus case studies on using IoT for a defense industrial base. Written by leading researchers and practitioners of IoT technology for defense and national security, IoT for Defense and National Security also includes information on: Changes in warfare driven by IoT weapons, logistics, and systems IoT resource allocation (monitoring existing resources and reallocating them in response to adversarial actions) Principles of AI-enabled processing for Internet of Battlefield Things, including machine learning and inference Vulnerabilities in tactical IoT communications, networks, servers and architectures, and strategies for securing them Adapting rapidly expanding commercial IoT to power IoT for defense For application engineers from defense-related companies as well as managers, policy makers, and academics, IoT for Defense and National Security is a one-of-a-kind resource, providing expansive coverage of an important yet sensitive topic that is often shielded from the public due to classified or restricted distributions.",
        "DOI": "10.1002/9781119892199",
        "paper_author": "Douglass R.",
        "affiliation_name": "Alta Montes",
        "affiliation_city": "Sandy",
        "affiliation_country": "United States",
        "affiliation_id": "118426046",
        "affiliation_state": "UT"
    },
    {
        "paper_title": "Intelligent Control of Construction Manufacturing Processes using Deep Reinforcement Learning",
        "publication": "Proceedings of the International Conference on Simulation and Modeling Methodologies, Technologies and Applications",
        "citied_by": "4",
        "cover_date": "2022-01-01",
        "Abstract": "This paper is concerned with the development and evaluation of a reinforcement learning approach to the control of factory based construction operations. The unique challenges associated with controlling construction work is first discussed: uneven and uncertain demand, high customization, the need to fabricate work to order, and a lack of opportunity to stockpile work. This is followed by a review of computational approaches to this problem, specifically those based on heuristics and machine learning. A description is then given of a model of a factory for producing precast reinforced concrete components, and a proposed reinforcement learning strategy for training a neural network based agent to control this system. The performance of this agent is compared to that of rule-of-thumb and random policies for a series of protracted simulation production runs. The reinforcement learning method was found to be promising, outperforming the two competing strategies for much of the time. This is significant given that there is high potential for improvement of the method. The paper concludes with an indication of areas of proposed future research.",
        "DOI": "10.5220/0011309600003274",
        "paper_author": "Flood I.",
        "affiliation_name": "University of Florida",
        "affiliation_city": "Gainesville",
        "affiliation_country": "United States",
        "affiliation_id": "60013959",
        "affiliation_state": "FL"
    },
    {
        "paper_title": "Learning an Interpretable Learning Rate Schedule via the Option Framework",
        "publication": "Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Learning rate is a common and important hyperparameter in many gradient-based optimizers which are used for training machine learning models. Heuristic handcrafted learning rate schedules (LRSs) can work in many practical situations, but their design and tuning is a tedious work, and there is no guarantee that a given handcrafted LRS matches a given problem. Many works have been dedicated to automatically learning an LRS from the training dynamics of the optimization problem, but most of them share a common deficit: they borrow the algorithms designed elsewhere as methods for automatic outer-training, but those methods often lack interpretability in the context of learning an LRS. In this paper, we leverage the option framework, a generalization to the common rein-forcement learning framework, to automatically learn an LRS based on the dynamics of optimization, which takes the idea of temporal abstraction as an underlying interpretation. To meet the requirements of LLRS, the RL state is designed as consisting of the global state and the per-parameter state. We propose a policy architecture which processes these two parts according to their respective structures, and combines them to yield the input for functional computation. Experiments are carried out on classic machine learning tasks and test functions for numerical optimization to demonstrate the effectiveness and the interpretability of our method.",
        "DOI": "10.1109/ICTAI56018.2022.00111",
        "paper_author": "Yao C.",
        "affiliation_name": "Institute of Automation Chinese Academy of Sciences",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60018486",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Lessons Learned from Utilizing Guided Policy Search for Human-Robot Handovers with a Collaborative Robot",
        "publication": "2022 2nd International Conference on Robotics, Automation and Artificial Intelligence, RAAI 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "We evaluate the performance of Guided Policy Search (GPS), a model-based reinforcement learning method, for generating the handover reaching motions of a collaborative robot arm. In a previous work, we evaluated GPS for the same task but only in a simulated environment. This paper provides a replication of the findings in simulation, along with new insights on GPS when used on a physical robot platform. First, we find that a policy learned in simulation does not transfer readily to the physical robot due to differences in model parameters and existing safety constraints on the real robot. Second, in order to successfully train a GPS model, the robot's workspace needs to be severely reduced, owing to the joint-space limitations of the physical robot. Third, a policy trained with moving targets results in large worst-case errors even in regions spatially close to the training target locations. Our findings motivate further research towards utilizing GPS in humanrobot interaction settings, especially where safety constraints are imposed.",
        "DOI": "10.1109/RAAI56146.2022.10092989",
        "paper_author": "Kshirsagar A.",
        "affiliation_name": "Cornell University",
        "affiliation_city": "Ithaca",
        "affiliation_country": "United States",
        "affiliation_id": "60007776",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Performance Evaluation of LSTM Optimizers for Long-Term Electricity Consumption Prediction",
        "publication": "ASSIC 2022 - Proceedings: International Conference on Advancements in Smart, Secure and Intelligent Computing",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "Electricity consumption is an important economic index, and it plays a significant role in drawing up an energy development policy for every country. Thus, having reliable information regarding the prediction of electricity consumption in a country is imperative to policy and decision-makers to plan and schedule the operation of power systems. Studies have shown that the Long Short-Term Memory (LSTM) neural network model is capable of learning long term temporary dependencies and nonlinear characteristic of a time series phenomenon and it can be a better alternative to the traditional neural networks and statistical methods for predicting electricity consumption. The LSTM neural network model has many hyperparameters, and one of the important hyperparameters is the optimization method. The optimization method plays a significant role in the performance of an LSTM neural network model, but selecting it is not a trivial task to end-users as there is no particular approach for selecting an appropriate method for a particular task. In this study, the LSTM neural network model was used to predict long term electricity consumption using socioeconomic data as predictors to analyze six popular optimization methods that have been implemented in the Keras machine learning library. The motivation is to determine which optimization method will be most suitable for electricity consumption prediction using LSTM neural network model. The results of the study show that the Stochastic Gradient Descent (SGD) optimizer is the most outstanding optimization method.",
        "DOI": "10.1109/ASSIC55218.2022.10088353",
        "paper_author": "Ampofo K.A.",
        "affiliation_name": "University of Ghana",
        "affiliation_city": "Accra",
        "affiliation_country": "Ghana",
        "affiliation_id": "60017910",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Analysis of Credit card fraud detection techniques using Machine Learning",
        "publication": "2022 International Conference on Futuristic Technologies, INCOFT 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Banks have been dealing with an increasing credit card default rate as the number of credit card customers grows. As a result, data analytics may help solve the current problem and control credit risks. Predicting a client's future status Choosing between defaulters and non-defaulters is a challenging but crucial task for any bank.The primary goal is to lower the number of non-defaulters among defaulters. This is because the bank can afford to treat non-defaulters as defaulters, but not defaulters as non-defaulters, because the underlying assumption is that the bank has a stringent no-NPA policy. There are three parts to this paper. In the first phase, we analyzed and cleaned our initial data set. The second portion is devoted to data pre-processing, in which we perform one-hot coding on categorical variables to convert them to a machine-readable format. The third component focuses on selecting an appropriate assessment metric, modelling data sets, and comparing them.",
        "DOI": "10.1109/INCOFT55651.2022.10094485",
        "paper_author": "Sharma D.",
        "affiliation_name": "Chandigarh University",
        "affiliation_city": "Mohali",
        "affiliation_country": "India",
        "affiliation_id": "60111704",
        "affiliation_state": "PB"
    },
    {
        "paper_title": "Asymmetric Self-Play for Learning Robust Human-Robot Interaction on Crowd Navigation Tasks",
        "publication": "Proceedings - 2022 3rd International Conference on Electronics, Communications and Information Technology, CECIT 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "A robot can make a sound to aware nearby pedestrians during its navigation, which often results in a more efficient and safer trajectory in a crowded environment. However, it is challenging to integrate such interaction capability into an existing robot navigation policy. In this paper, we propose a deep reinforcement learning (DRL) approach for integration, which results in effective interactive robot navigation policies in crowded environments. In particular, we specify the interaction capability by a set of high-level actions with flexible control. Then the navigation policy can be considered as a specific implementation of the 'move' action. Based on these high-level actions, we can train a robust human-robot interaction policy via asymmetric self-play, where the robot and some pedestrians, considered as naughty kids, play a game. Then the interaction policy can not only interact with pedestrians who cooperate with the robot but also with pedestrians who try to block the robot. We evaluate our approach in various simulation environments and compare it with multiple approaches. The experimental results show that our approach is robust and performs well in dense environments that are challenging for others. We also deploy the trained policy on a robot and evaluate its performance in multiple real-world crowded environments. A demonstration video is available online at https://youtu.be/x32YmivsIh0.",
        "DOI": "10.1109/CECIT58139.2022.00045",
        "paper_author": "Ma J.",
        "affiliation_name": "University of Science and Technology of China",
        "affiliation_city": "Hefei",
        "affiliation_country": "China",
        "affiliation_id": "60019118",
        "affiliation_state": "Anhui"
    },
    {
        "paper_title": "A Fintech-Based Zakat Model Using Artificial Intelligence",
        "publication": "FinTech in Islamic Financial Institutions: Scope, Challenges, and Implications in Islamic Finance",
        "citied_by": "4",
        "cover_date": "2022-01-01",
        "Abstract": "The COVID-19 pandemic and its associated lockdown have created a mammoth economic cost to the economies around the globe. The policy response to the crisis must be fast, secure, and sustainable. It has also created astonishing solidarity among the people with every element of society irrespective of race, caste, creed, or religion working together to save humanity. To overcome the financial and economic disruption caused by the pandemic, it needs immediate attention from the economists and policymakers. Islamic finance has many financial instruments for helping the poor by alleviating poverty, distributing income fairly, and improving social welfare, they comprise,  Zakat, Sadaqat, Awqaf,  etc. Zakat is the compulsory contribution from the Muslims to the poor and needy every year. Zakat is the compulsory donation from the rich and able Muslims which must be given to the poor and needy within a year. This immediate benefit of Zakat is well suited to tackle an economic crisis such as the one caused by COVID-19. Islamic finance in combination with the Fintech-based technologies like AI, Blockchain, machine learning, and natural language processing can work wonders in achieving Islamic finance objectives. The present study proposes an AI-based Islamic Fintech model to helping the needy and poor affected due to COVID-19. The model uses AI and NLP-based Fintech model for collection and dissemination of Zakat money to needy, poor, COVID-affected, and vulnerable sections of the society.",
        "DOI": "10.1007/978-3-031-14941-2_3",
        "paper_author": "Raza Rabbani M.",
        "affiliation_name": "University of Bahrain",
        "affiliation_city": "Zallaq",
        "affiliation_country": "Bahrain",
        "affiliation_id": "60000237",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "An advanced Machine Learning Techniques on Agriculture fileds for future level cultivations",
        "publication": "2022 International Conference on Smart Generation Computing, Communication and Networking, SMART GENCON 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "The conceptual and technological reasons, with the fast and reliable geographical categorization of crop kinds, can rely on remote sensing data is a critical task. Insurance policy, land leasing, supply-chain logistics, and financial market forecasts utilize spatial variability crop-type information to predict crop acreage for a range of monitoring and decision-making purposes. Cropland products derived from remote sensing have become much more accurate in depicting the location and size of agricultural fields. However, differentiating between actively agricultural areas and probably followed fields across agricultural lands, particularly in temperate zone grass barren structures, has received little attention. Crop-fallow rotation techniques are extensively utilized for soil fertility regeneration in the Sahel, one of the world's biggest dryland areas. Nevertheless, since fallow fields aren't clearly distinguished even within cropland class in any current remote sensing-based land use/cover maps, irrespective of the geographical resolution, nothing is recognized about their size. As one of the most precise metrics for evaluating regression modelling, the recently created Random Forest (RF) machine-learning method has gained widespread recognition. One study sought to determine whether the RF regression algorithm could be used to estimate crop solar energy digitally, assess a system's implementation, and compare the RF algorithm's productivity to the existing benchmarks. Results show that the RF-based technique is far better and outperforms the existing benchmarks in terms of performance.",
        "DOI": "10.1109/SMARTGENCON56628.2022.10083658",
        "paper_author": "Venkatanaresh M.",
        "affiliation_name": "Sri Venkateswara University College Of Engineering",
        "affiliation_city": "Tirupati",
        "affiliation_country": "India",
        "affiliation_id": "60107604",
        "affiliation_state": "AP"
    },
    {
        "paper_title": "Transforming Management with AI, Big-Data, and IoT",
        "publication": "Transforming Management with AI, Big-Data, and IoT",
        "citied_by": "7",
        "cover_date": "2022-01-01",
        "Abstract": "This book discusses the effect that artificial intelligence (AI) and Internet of Things (IoT) have on industry. The authors start by showing how the application of these technologies has already stretched across domains such as law, political science, policy, and economics and how it will soon permeate areas of autonomous transportation, education, and space exploration, only to name a few. The authors then discuss applications in a variety of industries. Throughout the volume, the authors provide detailed, well-illustrated treatments of each topic with abundant examples and exercises. This book provides relevant theoretical frameworks and the latest empirical research findings in various applications. The book is written for professionals who want to improve their understanding of the strategic role of trust at different levels of the information and knowledge society, that is, trust at the level of the global economy, of networks and organizations, of teams and work groups, of information systems and, finally, trust at the level of individuals as actors in the networked environments. • Presents research in various industries and how artificial intelligence and Internet of Things is changing the landscape of business and management; • Includes new and innovative features in artificial intelligence and IoT that can help in raising economic efficiency at both micro and macro levels; • Examines case studies with tried and tested approaches to resolution of typical problems in each application of study.",
        "DOI": "10.1007/978-3-030-86749-2",
        "paper_author": "Al-Turjman F.",
        "affiliation_name": "G.L.Bajaj Institute of Technology &amp; Management",
        "affiliation_city": "Greater Noida",
        "affiliation_country": "India",
        "affiliation_id": "60097517",
        "affiliation_state": "UP"
    },
    {
        "paper_title": "Space-Environment Commons: From Big Data Survey to AI, to a Post-capitalist Blockchain Zoning Platform",
        "publication": "Resilient Communities and the Peccioli Charter: Towards the Possibility of an Italian Charter for Resilient Communities",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "One of the current challenges of how cities grow is how reality is measured and validated. Late Capitalism measures and validates reality through private profit and, therefore, appropriated and commercialized the urban and planning policies of the 1800s that secured health, sanitation, equity, communities, and the environment through cities as business and real estate speculation. More recently, information ownership become profitable and subject to economic speculation, distorting information, and creating clusters of self-validation controlling and administering social relationships through privatization of public spaces due to the ability to inform the reality. What would be the means to propose our future ways of living in relation to our environmental commons? How can we secure a real-time system including zoning laws and parameters that include humans, post-humans, and non-humans in a post-Capitalist post-Anthropocene? These issues are addressed in the emergent architecture and urbanism of information, which consists in surveying reality, reading, interpreting, mediating, and organizing information flows identifying conflicts of interests between data acquisition, their representation, and functionality. Ecoinduction III, Rezoning New York Region is an ongoing research that integrates different survey and measuring systems, such as Big Data, simulations, Machine Learning (ML), and Artificial Intelligence (AI) to activate in real time a dynamic varying ecological zoning thinking the city as interconnected space-environments. A blockchain technology platform surveys and integrates multidimensional information in a dynamically continuously changing system that can inform the reality in real time. The platform surveys latent environmental ecologies and through zoning activates them and coordinates them into larger regional ecological space-environments.",
        "DOI": "10.1007/978-3-030-85847-6_17",
        "paper_author": "Lorenzo-Eiroa P.",
        "affiliation_name": "NYIT School of Architecture and Design",
        "affiliation_city": "New York",
        "affiliation_country": "United States",
        "affiliation_id": "129544706",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Informal Social Protection and Poverty",
        "publication": "Informal Social Protection and Poverty",
        "citied_by": "5",
        "cover_date": "2022-01-01",
        "Abstract": "​This book analyzes the importance of informal social protection provided by religious institutions such as madrassas in a low-income country such as Pakistan. This book explains that Madrassas are religious schools that have existed in many Muslim countries for centuries and contributed significantly to preserving, forming, and extending human knowledge in medieval times. Further, madrassas are now more commonly viewed as the providers of a narrow education, supporting religious fundamentalism, that may lead to terrorism. However, this book asserts that education is not the only function performed by madrassas. They are a significant source of welfare support for the vulnerable and marginalized households in many low-income countries. This book helps the readers to understand the concept of informal social protection not conceptualized previously. In addition, its various attributes and institutions providing such a form of welfare worldwide are explained in detail; analyzing the usefulness of such a form of social protection would benefit readers of social policy, national governments, and international donor/aid agencies. This book also provides a prescriptive framework for integrating formal and informal social protection. This book provides a new \"Multiple Regime Framework\", for identifying various regimes in one country at one point in time by applying a novel data collection and analysis methodology. The application of this framework would be of particular interest to social policy scholars, national governments, and donor/aid agencies because it will result in better targeting of social protection policies in the wake of fiscal constraints. Lastly, this book provides a novel data collection and analysis strategy that will benefit the reader of research methodology, development consultants, donor agencies, and policy practitioners interested in using artificial intelligence to make informed and targeted policy decisions.",
        "DOI": "10.1007/978-981-19-6474-9",
        "paper_author": "Mumtaz Z.",
        "affiliation_name": "The Australian National University",
        "affiliation_city": "Canberra",
        "affiliation_country": "Australia",
        "affiliation_id": "60008950",
        "affiliation_state": "ACT"
    },
    {
        "paper_title": "Machine learning approach for climate change impact assessment in agricultural production",
        "publication": "Visualization Techniques for Climate Change with Machine Learning and Artificial Intelligence",
        "citied_by": "7",
        "cover_date": "2022-01-01",
        "Abstract": "Agricultural production mainly depends on weather conditions. Both aspects are interiorly connected with each other in several aspects, as climate change is the key factor of plant biotic and abiotic stresses, which have an adverse influence on global agriculture production. The agricultural land is being affected by climate changes in several ways, for example, variations in annual rainfall and temperature, weed modifications, microbial activity, heat waves, and global atmospheric CO2, or ozone level change. The warning of global climate change has significantly driven the attention of researchers, as these alterations are imparting adverse impacts on crop production and negotiating the global food security system. A timely accurate forecast of crop production is essential for critical policy assessments such as pricing, export-import, marketing circulation, and global food security. The emerging empirical model relationship is being widely used to evaluate the effects of climate change on a specific region. The problematic situation is deriving information from raw datasets, this has led to the expansion of new techniques such as machine learning (ML) that can be used significantly to integrate the information with crop yield assessment. This chapter is designed as an attempt to present crop yield modeling that uses ML methods in high-dimensional datasets. Some statistical approaches such as artificial neural networks, decision tree modeling, regression analysis, fuzzy networks, Markov chain modeling, principal component analysis, cluster analysis, and time-series analysis were summarized.",
        "DOI": "10.1016/B978-0-323-99714-0.00012-1",
        "paper_author": "Singh S.",
        "affiliation_name": "CSIR-National Botanical Research Institute",
        "affiliation_city": "Lucknow",
        "affiliation_country": "India",
        "affiliation_id": "60013111",
        "affiliation_state": "UP"
    },
    {
        "paper_title": "A Machine Learning-Based Comparative Approach to Predict the Crop Yield Using Supervised Learning with Regression Models",
        "publication": "Procedia Computer Science",
        "citied_by": "46",
        "cover_date": "2022-01-01",
        "Abstract": "Agriculture is essential for the existence of all human beings. It is not only a need, but it also contributes to national growth. Agriculture is the only industry that benefits both itself and the rest of the country. It not only offers food and raw materials to a substantial section of the people but also work prospects. Agriculture is the principal source of income for the majority of people in India. It has been in the country for thousands of years, and modern technology and equipment have substituted ancient farming practices. With the advancement of technology in every industry, it has become increasingly vital to incorporate technology into agriculture. Machine learning facilitates the study of large volumes of data and may deliver faster and more precise results, which can aid in the identification of lucrative possibilities and risky threats. The major goal of this research project is to develop a Machine Learning (ML) model to predict farm production. To estimate crop yields, the data was collected and trained using supervised machine learning with six distinct regression models. With a Mean absolute error (MAE) of 468.16 and a Cross-Validation score of 0.6087, Random Forest Regressor outperformed the other models.",
        "DOI": "10.1016/j.procs.2023.01.241",
        "paper_author": "Panigrahi B.",
        "affiliation_name": "Narsee Monjee Institute of Management Studies, Mumbai",
        "affiliation_city": "Mumbai",
        "affiliation_country": "India",
        "affiliation_id": "60079592",
        "affiliation_state": "MH"
    },
    {
        "paper_title": "An Efficient Machine Learning Technique for Crop Yield Prediction",
        "publication": "Proceedings of 2022 IEEE International Conference on Current Development in Engineering and Technology, CCET 2022",
        "citied_by": "2",
        "cover_date": "2022-01-01",
        "Abstract": "The field of agricultural research is flourishing, and the difficulties that lie ahead are being overcome with the aid of technological development. It has been determined that this is very helpful for the economic development and progress of any country. Agricultural growth to be improves by incorporating technology. In order to make strategic plans in agricultural commodities for import-export policies and to enhance farmer earnings, an early and accurate assessment of crop achievement is important in quantitative and financial evaluation at the field level. One of the tough problems in agriculture is predicting crop yields with the use of machine learning algorithms in order to forecast a larger crop output. Using this work, provide a powerful approach of machine-learning to forecast agricultural yields. By using machine learning technique can be helpful for predicting the crop yield. An experimental analysis shows that using machine learning technique crop yield productivity can be enhance.",
        "DOI": "10.1109/CCET56606.2022.10079993",
        "paper_author": "Singh N.",
        "affiliation_name": "Sage University Indore",
        "affiliation_city": "Indore",
        "affiliation_country": "India",
        "affiliation_id": "60275974",
        "affiliation_state": "MP"
    },
    {
        "paper_title": "Application Security using SQL Malware Detection and Prevention Scheme",
        "publication": "Proceedings of 2022 IEEE International Conference on Current Development in Engineering and Technology, CCET 2022",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "SQL (Structure Query Language) injection malware is a harmful instruction intended to cause an adverse effect containing database query that extracts information from the system and supplies unauthenticated access. The SQL queries are less complex and highly flexible, resembling benign queries. In earlier works, such attacks were seen to penetrate the security system of any web application. Machine learning techniques have been applied to various works to trace the SQL malicious query. Also, existing tools such as SEPTIC for the prevention of SQL malware. Earlier models are found to be less robust. This paper introduces an efficient mechanism using SVM (Support vector Machine) to conduct recognition of malware scripts. The SVM is applied to train and test various SQL strings containing benign and malware scripts. The proposed model also performs the prevention of SQL malware attacks. The SQL string prevention is done using a candid dynamic method that performs string analysis. This analysis is used to locate every context of the SQL script and perform its interpretation. In this policy, a parse tree is generated first from the input query. Further, an analysis of the nodes of the parse tree has been performed. The method can identify the benign and malware script. The proposed model is able to secure an average accuracy of 96.24% for detecting and preventing SQL malware queries. Various parameters like true positive rate, false positive rate, true negative rate, and time variation have been calculated concerning data size and accuracy.",
        "DOI": "10.1109/CCET56606.2022.10079943",
        "paper_author": "Yadav D.K.",
        "affiliation_name": "B.N. College of Engineering and Technology",
        "affiliation_city": "Lucknow",
        "affiliation_country": "India",
        "affiliation_id": "119836964",
        "affiliation_state": "UP"
    },
    {
        "paper_title": "Crop Yield Prediction using Machine Learning and Deep Learning Techniques",
        "publication": "Procedia Computer Science",
        "citied_by": "71",
        "cover_date": "2022-01-01",
        "Abstract": "Agriculture is a significant contributor to India's economic growth. The rising population of country and constantly changing climatic conditions have an impact on crop production and food security. A variety of factors influence crop selection, including market price, production rate, soil type, rainfall, temperature, government policies, etc. Many changes are required in the agricultural sector in order to enhance the Indian economy. In this research work authors have implemented various machine learning techniques to estimate the crop yield in Rajasthan state of India on five identified crops. The results indicate that among all the applied algorithms; Random Forest, SVM, Gradient Descent, long short-term memory, and Lasso regression techniques; the random forest performed better than others with 0.963 R2, 0.035 RMSE, and 0.0251 MAE. The results were validated using R2, root mean squared error, and the mean absolute error to cross-validation techniques. This paper intends to put the crop selection method into practice to help farmers solve crop yield problems.",
        "DOI": "10.1016/j.procs.2023.01.023",
        "paper_author": "Jhajharia K.",
        "affiliation_name": "Manipal University Jaipur",
        "affiliation_city": "Jaipur",
        "affiliation_country": "India",
        "affiliation_id": "60108737",
        "affiliation_state": "RJ"
    },
    {
        "paper_title": "Deep invariant networks with differentiable augmentation layers",
        "publication": "Advances in Neural Information Processing Systems",
        "citied_by": "5",
        "cover_date": "2022-01-01",
        "Abstract": "Designing learning systems which are invariant to certain data transformations is critical in machine learning. Practitioners can typically enforce a desired invariance on the trained model through the choice of a network architecture, e.g. using convolutions for translations, or using data augmentation. Yet, enforcing true invariance in the network can be difficult, and data invariances are not always known a piori. State-of-the-art methods for learning data augmentation policies require held-out data and are based on bilevel optimization problems, which are complex to solve and often computationally demanding. In this work we investigate new ways of learning invariances only from the training data. Using learnable augmentation layers built directly in the network, we demonstrate that our method is very versatile. It can incorporate any type of differentiable augmentation and be applied to a broad class of learning problems beyond computer vision. We provide empirical evidence showing that our approach is easier and faster to train than modern automatic data augmentation techniques based on bilevel optimization, while achieving comparable results. Experiments show that while the invariances transferred to a model through automatic data augmentation are limited by the model expressivity, the invariance yielded by our approach is insensitive to it by design.",
        "DOI": "NA",
        "paper_author": "Rommel C.",
        "affiliation_name": "Université Paris-Saclay",
        "affiliation_city": "Gif-sur-Yvette",
        "affiliation_country": "France",
        "affiliation_id": "60106017",
        "affiliation_state": "Ile-de-France"
    },
    {
        "paper_title": "Research of Federated Learning Communication Data Transmission Algorithm Based on User Grouping Policy",
        "publication": "2022 8th International Conference on Control Science and Systems Engineering, ICCSSE 2022",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "Since being proposed, Machine Learning (ML) has gained a great leap. However, the shortcoming of user privacy restricts its further application in communication. Therefore, Federated Learning (FL) has been proposed to overcome this shortcoming. FL allows users to compute gradient information and upload them to the server for global training. Nevertheless, it still has the problem of high communication overhead. In this work, we propose three user grouping policies, that is, parity, random, and orderly, to study the proper method to reduce communication overhead during the training process of federated learning. Simulation results prove that the parity and the random policy outperform the orderly policy and the no grouping one in test accuracy and channel loss.",
        "DOI": "10.1109/ICCSSE55346.2022.10079164",
        "paper_author": "Cai Y.",
        "affiliation_name": "Communication University of China",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60021427",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Intelligent Farm Based on Deep Reinforcement Learning for optimal control",
        "publication": "2022 International Symposium on iNnovative Informatics of Biskra, ISNIB 2022",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "Smart agriculture is the use of automation, sensors, and data to improve crop yields, reduce costs and increase crop quality. Predictive analytics, machine learning, and artificial intelligence are used. Another benefit of using artificial intelligence in agriculture is the ability to optimize the consumption of resources such as water and fertilizer to achieve the greatest yield. However, to date, the technology has not been used to optimize the management of agricultural operations. The objective of this paper is to introduce the concept of smart farming and discuss applying deep reinforcement learning for optimal management in the field. We propose a smart farming system that uses Deep Reinforcement Learning to optimize the operation of a farm. The system uses Deep Learning to recognize patterns in a large dataset of real-world agricultural data, such as crop yields and weather, and uses this knowledge to recommend actions. It also dynamically learns optimal control policies for a variety of crops, depending on the farmer's goals and current conditions.",
        "DOI": "10.1109/ISNIB57382.2022.10076088",
        "paper_author": "Yassine H.M.",
        "affiliation_name": "Peter the Great St. Petersburg Polytechnic University",
        "affiliation_city": "Saint Petersburg",
        "affiliation_country": "Russian Federation",
        "affiliation_id": "60017103",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Day-ahead Wind Power Prediction using Optimised XGBoost and Correlation Analysis based Noise Reduction Technique",
        "publication": "10th IEEE International Conference on Power Electronics, Drives and Energy Systems, PEDES 2022",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "As the integration of Renewable energy into electrical energy network is increasing, accurate forecast of wind power output has become one of the major areas of research in electrical engineering in recent years. The intermittent nature of wind makes wind power foresting a challenge. Among most of the traditional predictive models, artificial intelligence and machine learning techniques have mostly been tried out in this task. In this work two correlation analysis techniques-Pearson's Correlation and Spearman's Correlation have been employed for feature selection and noise reduction before using the dataset for prediction. Wind power prediction was performed using Extreme Gradient Boosted trees (XGBoost). The model result was further improved as Exhaustive grid search (EGS) method was applied to find out the optimal values of hyper-parameters. The model accuracy has been evaluated by two evaluation indexes, such as Mean Absolute Percentage Error (MAPE) and Root Mean Squared Error (RMSE). With accurate forecasting of wind energy generation, it will aid the energy sector in grid planning and managing the stability of network.",
        "DOI": "10.1109/PEDES56012.2022.10079973",
        "paper_author": "Das N.",
        "affiliation_name": "National Institute of Technology Silchar",
        "affiliation_city": "Silchar",
        "affiliation_country": "India",
        "affiliation_id": "60097223",
        "affiliation_state": "AS"
    },
    {
        "paper_title": "Evolution of research and development in the field of artificial intelligence technologies for healthcare in the Russian Federation: results of 2021",
        "publication": "Digital Diagnostics",
        "citied_by": "12",
        "cover_date": "2022-01-01",
        "Abstract": "The use of artificial intelligence technologies in Russian healthcare is a priority area for implementing a national strategy for the development of artificial intelligence in the country. The introduction of digital solutions based on artificial intelligence in healthcare facilities should improve the standard of living of the population and the quality of medical care, including areas of preventive examinations, diagnostics based on image analysis, prediction of disease development, selection of optimal drug dosages, reducing the threat of pandemics, and automating and increasing the accuracy of surgical interventions. Policy management and technical regulation are under development in the field of artificial intelligence in healthcare. The domestic market for relevant solutions has been created, and some products have been certified as medical devices from Roszdravnadzor (Federal Service for Surveillance in Healthcare). Various teams of scientists are conducting research. However, Russia is still behind the leading countries in the field of artificial intelligence, such as the United States and China. Investments in healthcare products based on artificial intelligence decreased significantly in 2021. The major reasons for the lag, at least in terms of market indicators, are low demand and the inability of state medical organizations to fund artificial intelligence projects. There are also other issues related to trust in the safety and effectiveness of such solutions.",
        "DOI": "10.17816/DD107367",
        "paper_author": "Gusev A.V.",
        "affiliation_name": "K-Skai",
        "affiliation_city": "Petrozavodsk",
        "affiliation_country": "Russian Federation",
        "affiliation_id": "128433948",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Proceedings - 21st IEEE International Conference on Machine Learning and Applications, ICMLA 2022",
        "publication": "Proceedings - 21st IEEE International Conference on Machine Learning and Applications, ICMLA 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "The proceedings contain 268 papers. The topics discussed include: addressing sample efficiency and model-bias in model-based reinforcement learning; reinforcement learning based architectures for dynamic generation of smart home services; attention-based partial decoupling of policy and value for generalization in reinforcement learning; an empirical evaluation of multivariate time series classification with input transformation across different dimensions; topological regularization for dense prediction; real-time cattle interaction recognition via triple-stream network; deeper bidirectional neural networks with generalized non-vanishing hidden neurons; recycling material classification using convolutional neural networks; automatic key information extraction from visually rich documents; multi-stream deep residual network for cloud imputation using multi-resolution remote sensing imagery; structural health and intelligent monitoring of wind turbine blades with a motorized telescope; and deep learning based re-identification of wooden euro-pallets.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "MACHINE LEARNING: ARCHITECTURE IN THE AGE OF ARTIFICIAL INTELLIGENCE",
        "publication": "Machine Learning: Architecture in the age of Artificial Intelligence",
        "citied_by": "17",
        "cover_date": "2022-01-01",
        "Abstract": "‘The advent of machine learning-based AI systems demands that our industry does not just share toys, but builds a new sandbox in which to play with them.’ -Phil Bernstein The profession is changing. A new era is rapidly approaching when computers will not merely be instruments for data creation, manipulation and management, but, empowered by artificial intelligence, they will become agents of design themselves. Architects need a strategy for facing the opportunities and threats of these emergent capabilities or risk being left behind. Architecture’s best-known technologist, Phil Bernstein, provides that strategy. Divided into three key sections – Process, Relationships and Results – Machine Learning lays out an approach for anticipating, understanding and managing a world in which computers often augment, but may well also supplant, knowledge workers like architects. Armed with this insight, practices can take full advantage of the new technologies to future-proof their business. Features chapters on: Professionalism Tools and technologies Laws, policy and risk Delivery, means and methods Creating, consuming and curating data Value propositions and business models.",
        "DOI": "10.4324/9781003297192",
        "paper_author": "Bernstein P.",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Monte-Carlo Simulation Based on Patient-Individual Distributions for Supporting Intensive Care Occupancy Management",
        "publication": "Proceedings of the Annual Hawaii International Conference on System Sciences",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "Managing Intensive Care Units (ICUs) in hospitals is a highly challenging endeavor. In particular, decisions such as admitting elective patients and discharging patients from the ICU have to be taken under a high level of uncertainty since the occupancy of ICUs does not only depend on these decisions but also on unknown parameters such emergency patient arrivals and lengths of stay of the patients in the ICU. In this paper, we develop a framework for supporting ICU occupation management by quantifying the impact of admission and discharge decisions on the probability of reaching critical ICU occupancy levels in a given planning horizon. A key component of this framework is the use of data-driven approaches for obtaining probability distributions for the parameters affected by uncertainty. In particular, we use standardized treatment and patient health state data to create patient-specific length-of-stay distributions with a Machine Learning approach. These patient-individual distributions are then validated and/or adjusted by medical experts. The validated distributions form the input to a Monte-Carlo Simulation that is used to approximate the probability distributions of the daily ICU occupancy levels resulting from ICU admission and discharge decisions. We experimentally evaluate our framework in a counterfactual simulation based on one year of historical data from 2019 from a medium-sized ICU in a German hospital. In that evaluation, we use a simple ICU management policy based on the probabilistic occupancy forecasts aiming at reducing the risk of running out of ICU capacity. The results show that following this policy would have avoided hitting critical occupancy levels by around 70% and would have had a smoothing effect on ICU occupancy levels.",
        "DOI": "NA",
        "paper_author": "Witteborg A.H.",
        "affiliation_name": "Universität Bielefeld",
        "affiliation_city": "Bielefeld",
        "affiliation_country": "Germany",
        "affiliation_id": "60015595",
        "affiliation_state": "Nordrhein-Westfalen"
    },
    {
        "paper_title": "Deep Reinforcement Learning Techniques for Solving Hybrid Flow Shop Scheduling Problems: Proximal Policy Optimization (PPO) and Asynchronous Advantage Actor-Critic (A3C)",
        "publication": "Proceedings of the Annual Hawaii International Conference on System Sciences",
        "citied_by": "3",
        "cover_date": "2022-01-01",
        "Abstract": "Well-studied scheduling practices are fundamental for the successful support of core business processes in any manufacturing environment. Particularly, the Hybrid Flow Shop (HFS) scheduling problems are present in many manufacturing environments. The current advances in the field of Deep Reinforcement Learning (DRL) attracted the attention of both practitioners and academics to investigate their adoption beyond synthetic game-like applications. Therefore, we present an approach that is based on DRL techniques in conjunction with a discrete event simulation model to solve a real-world four-stage HFS scheduling problem. The main narrative behind the presented concepts is to expose a DRL agent to a game-like environment using an indirect encoding. Two types of DRL techniques namely, Proximal Policy Optimization (PPO) and Asynchronous Advantage Actor-Critic (A3C), are evaluated for solving problems of different complexity. The computational results suggest that the DRL agents successfully learn appropriate policies for solving the investigated problem. In addition, the investigation shows that the agent can adjust their policies when we expose them to a different problem. We further evaluate the approach to solving problem instances published in the literature to establish a comparison.",
        "DOI": "NA",
        "paper_author": "Nahhas A.",
        "affiliation_name": "Otto-von-Guericke-Universität Magdeburg",
        "affiliation_city": "Magdeburg",
        "affiliation_country": "Germany",
        "affiliation_id": "60018362",
        "affiliation_state": "Sachsen-Anhalt"
    },
    {
        "paper_title": "CH-Go: Online Go System Based on Chunk Data Storage",
        "publication": "Proceedings - 24th IEEE International Conference on High Performance Computing and Communications, 8th IEEE International Conference on Data Science and Systems, 20th IEEE International Conference on Smart City and 8th IEEE International Conference on Dependability in Sensor, Cloud and Big Data Systems and Application, HPCC/DSS/SmartCity/DependSys 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "The training and running of an online Go system require the support of effective data management systems to deal with vast data, such as the initial Go game records, the feature data set obtained by representation learning, the experience data set of self-play, the randomly sampled Monte Carlo tree, and so on. Previous work has rarely mentioned this problem, but the ability and efficiency of data management systems determine the accuracy and speed of the Go system. To tackle this issue, we propose an online Go game system based on the chunk data storage method (CH-Go), which processes the format of 160k Go game data released by Kiseido Go Server (KGS) and designs a Go encoder with 11 planes, a parallel processor and generator for better memory performance. Specifically, we store the data in chunks, take the chunk size of 1024 as a batch, and save the features and labels of each chunk as binary files. Then a small set of data is randomly sampled each time for the neural network training, which is accessed by batch through yield method. The training part of the prototype includes three modules: supervised learning module, reinforcement learning module, and an online module. Firstly, we apply Zobrist-guided hash coding to speed up the Go board construction. Then we train a supervised learning policy network to initialize the self-play for generation of experience data with 160k Go game data released by KGS. Finally, we conduct reinforcement learning based on REINFORCE algorithm. Experiments show that the training accuracy of CH-Go in the sampled 150 games is 99.14%, and the accuracy in the test set is as high as 98.82 %. Under the condition of limited local computing power and time, we have achieved a better level of intelligence. Given the current situation that classical systems such as GOLAXY are not free and open, CH-Go has realized and maintained complete Internet openness.",
        "DOI": "10.1109/HPCC-DSS-SmartCity-DependSys57074.2022.00325",
        "paper_author": "Lu H.",
        "affiliation_name": "Sichuan University",
        "affiliation_city": "Chengdu",
        "affiliation_country": "China",
        "affiliation_id": "60016521",
        "affiliation_state": "Sichuan"
    },
    {
        "paper_title": "A computational method to track the evolution of business models in the Digital Economy",
        "publication": "Proceedings of the Annual Hawaii International Conference on System Sciences",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Companies within the Digital Economy are evolving their business models as they take advantage of the opportunities afforded by emerging digital technologies. There is a need to develop methods that will allow researchers and policy makers to understand the existence of, and relationships between, the different business models within the Digital Economy and track their evolution. Such methods could also help quantify the size and growth of the Digital Economy. This paper presents a computational method, which utilizes machine learning and web scraping, to identify new business models, and a taxonomy of organisations, through the analysis of a firm's webpage. The work seeks to provide an autonomous tool that provides regular output tracking trends in the number of firms in a market, their business model and changes in activity from product to service over time. This information would provide valuable and actionable insight for researchers, firms and markets.",
        "DOI": "NA",
        "paper_author": "Wood Z.",
        "affiliation_name": "University of Exeter",
        "affiliation_city": "Exeter",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60026479",
        "affiliation_state": "Devon"
    },
    {
        "paper_title": "Hybrid model for detection of frauds in credit cards<sup>*</sup>",
        "publication": "Proceedings - 2022 4th International Conference on Advances in Computing, Communication Control and Networking, ICAC3N 2022",
        "citied_by": "2",
        "cover_date": "2022-01-01",
        "Abstract": "The goal of this study is to decide on the correct model that can predict a credit card holder's likelihood of missing their next payment based on their demographic characteristics and payment history. Our goal is to reduce the number of defaulters that are labelled as non-defaulters. That's because our company can afford to have non-defaulters who do default but not non-defaulters who do not default, as it is implied that the bank has a stringent no-NPA policy (more or less). When putting machine learning algorithms into practice, supervised learning must be used since we have known upstream and downstream values and anticipate a prediction model at the conclusion. The correlation amid attributes and target sets is established in the first stage. Classification stage classifies input data into certain categories. In order to examine the performance of recommended approach, the comparison of recommended approach and other available approaches has been conducted in this work along with this achieving the 85% accuracy result.By accurately categorizing credit card delinquencies and non-defaulters, the mortgage lenders will be able to utilize the accuracy of the model to lower the high delinquent rate.",
        "DOI": "10.1109/ICAC3N56670.2022.10074057",
        "paper_author": "Sharma D.",
        "affiliation_name": "Chandigarh University",
        "affiliation_city": "Mohali",
        "affiliation_country": "India",
        "affiliation_id": "60111704",
        "affiliation_state": "PB"
    },
    {
        "paper_title": "Breast Cancer Classification Based on Various CNNs and Classifiers",
        "publication": "Proceedings - 2022 International Symposium on Advances in Informatics, Electronics and Education, ISAIEE 2022",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "Breast cancer is the second leading cause of death from cancer in women around the world. The CAD system utilizing machine learning and deep learning techniques facilitates the early detection of breast cancers. However, few recent studies focused on utilizing multiple feature extractors to compare and analyze the performances of various architectures. This paper analyzes the performances of architectures which are combinations of different feature extractors and classifiers in breast cancer diagnosis. Firstly, we collected histopathological breast cancer images from the BreakHis dataset. Secondly, the normalized data were converted to one-hot encoding for training, validating, and testing. Thirdly, we used VGG-16, VGG-19, Xception, ResNet50, Inception-V3, and Inception-Resnet-V2 to extract features. Next, fully connected layer (FCL), logistic regression (LR), and SVM were employed to classify breast cancers on the BreaKHis dataset. The experimental result shows that with the cyclical learning rate (CLR) policy, the ResNet50-SVM model obtained the optimal accuracy rate of 93.9% on eight-classification. The result shows that our proposed method could diagnose breast cancer with high accuracy.",
        "DOI": "10.1109/ISAIEE57420.2022.00011",
        "paper_author": "Ge Y.",
        "affiliation_name": "Shandong University",
        "affiliation_city": "Jinan",
        "affiliation_country": "China",
        "affiliation_id": "60031031",
        "affiliation_state": "Shandong"
    },
    {
        "paper_title": "Predicting Employee Attrition Using Machine Learning Algorithms",
        "publication": "Proceedings - 2022 4th International Conference on Advances in Computing, Communication Control and Networking, ICAC3N 2022",
        "citied_by": "5",
        "cover_date": "2022-01-01",
        "Abstract": "Employees are considered the foundation of any organization. Due to their importance, the Human resources department implements various policies to sustain them. Yet the attrition rate in any organization is increasing yearly. The attrition rate signifies the number of employees who leaves a firm without being replaced. It is regarded as a well-known issue that requires the administration to make the best choices to retain highly competent staff. It is interesting to note that artificial intelligence is frequently used as a successful technique for foreseeing such an issue. This review paper aims to study the different machine learning approaches that predict employee attrition and factors influencing an employee to attrite from an organization. A Hybrid model comprising the various ensemble models is proposed to predict attrition at its earliest. The forecasted attrition model aids in not only taking preventive action but also in improving recruiting choices and rewarding top performers who contribute to the company's success.",
        "DOI": "10.1109/ICAC3N56670.2022.10074131",
        "paper_author": "George S.",
        "affiliation_name": "Christ University",
        "affiliation_city": "Bengaluru",
        "affiliation_country": "India",
        "affiliation_id": "60106812",
        "affiliation_state": "KA"
    },
    {
        "paper_title": "Entrepreneurship: Analysis by Country Through Machine Learning Techniques",
        "publication": "Proceedings of the European Conference on Innovation and Entrepreneurship, ECIE",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "This research aims to analyze entrepreneurship worldwide through the dimensions and pillars of the entrepreneurship ecosystem of each country, identifying the contribution and patterns of behavior and correlation within the entrepreneurship ecosystem. This analysis intends to show the main actions that countries have carried out in support of entrepreneurship and entrepreneurs. The tool used to analyze is machine learning, where various algorithms are applied. The evidence shows that the most relevant pillars in the entrepreneurial ecosystem are I. Opportunity Startup, II. Technology Absorption, III. Risk Acceptance, IV. Risk Capital and V. Process Innovation. The pillars that best correlate are I. Competition and Opportunity Startup, II. Opportunity Startup, and Risk Acceptance, III. Opportunity Startup and Technology Absorption, IV. Cultural Support and Opportunity Startup, and V. Opportunity Startup and Risk Capital. The present work aims to provide knowledge to decision-makers in both the public and private sectors to channel public policies that support entrepreneurs in this time of crisis and promote the generation and strengthening of entrepreneurial activity. Although there are still no reliable GEI data for the years 2020 to 2022, the economic crisis generated by the stagnation in the development of the countries has reduced support for entrepreneurs, which in many cases can be a key factor for the rescue of the most disadvantaged countries.",
        "DOI": "10.34190/ecie.17.1.475",
        "paper_author": "Martínez-Velasco A.",
        "affiliation_name": "Universidad Panamericana",
        "affiliation_city": "Mexico City",
        "affiliation_country": "Mexico",
        "affiliation_id": "60012895",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "The Technical and Major Difficulties and Risk involved in Integrating the Artificial Intelligence with Cyber Security system: A systematic study",
        "publication": "Proceedings of 5th International Conference on Contemporary Computing and Informatics, IC3I 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "In this study, recorded failures of artificially intelligent systems will be presented and analyzed. We then extrapolate our findings to potential future AIs. I contend that future AI failures will become more frequent and more severe over time. Cyber security professionals' ideas can be used to enhance AI safety. While 's cybersecurity breaches have the same moderate degree of severity as security faults for narrow AIs, broad AI failures have a significantly different consequence. A super intelligent system's single failure could result in a catastrophic scenario with no prospect of recovery. AI Safety aims to ensure that no attacks ever succeed in getting past the system's defenses, while cyber security aims to lower the amount of successful attacks on the system. Regrettably, such It is impossible to execute at a certain level. There is no such thing as a security system that is completely secure; all security systems eventually fail. Our time may be remembered as one of significant change by future generations. In only a few short decades, our civilization transitioned from being machine-dependent to being information-dependent, and as the Information Age advances, society is being forced to get a better grasp of procedures that are algorithmic and data-driven. Artificial agents are machines and systems that make decisions without the automatic, information, or algorithmic learning processes. These agents are increasingly being included into our everyday decision-making procedures. Their development and implementation raise numerous relevant policy issues.",
        "DOI": "10.1109/IC3I56241.2022.10073036",
        "paper_author": "Lourens M.",
        "affiliation_name": "Durban University of Technology",
        "affiliation_city": "Durban",
        "affiliation_country": "South Africa",
        "affiliation_id": "60004001",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Machine Learning Based Corporate Climate Change Disclosure in Integrating Institutional and Resource Approach based on Computational Intelligence Method",
        "publication": "Proceedings of 5th International Conference on Contemporary Computing and Informatics, IC3I 2022",
        "citied_by": "2",
        "cover_date": "2022-01-01",
        "Abstract": "There has to be a coordinated effort between academia, government, and business to evaluate and shape the impact of AI and ML on efforts to slow global warming. It's not clear how ML will affect GHG emissions in the future, but it's already having an impact now. This is due in part to the fact that assessing and predicting the effects of these emissions is complicated by the fact that their various processes are not well characterised. As a result, we present a structured approach for characterising the consequences of ML on GHG emissions, which is comprised of three levels: (1) compute-related implications; (2) immediate repercussions of using ML; and (3) effects on the whole system. This approach is used to assess and rank the importance of the information and studies required for carrying out an effect evaluation, doing a scenario analysis, or identifying crucial policy levers.",
        "DOI": "10.1109/IC3I56241.2022.10073204",
        "paper_author": "Supreeth B.R.",
        "affiliation_name": "Oxford Degree and PG College",
        "affiliation_city": "Bengaluru",
        "affiliation_country": "India",
        "affiliation_id": "129449088",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A city is not a static tree: understanding urban areas through the lens of real-time behavioral data",
        "publication": "ZARCH",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Cities are the main ground on which our society and culture develop today and will evolve in the future. Against the traditional understanding of cities as physical spaces mostly around our neighborhoods, recent use of large-scale mobility datasets has enabled the study of our behavior at unprecedented spatial and temporal scales, much beyond our static residential spaces. Here we show how it is possible to use these datasets to investigate the role that human behavior plays in traditional urban problems like segregation, public health, or epidemics. Apart from measuring or monitoring such problems in a more comprehensive way, the analysis of those large datasets using modern machine learning techniques or causality detection permits to unveil of the behavioral roots behind them. As a result, only by incorporating real-time behavioral data can we design more efficient policies or interventions to improve such critical societal issues in our urban areas.",
        "DOI": "10.26754/OJS_ZARCH/ZARCH.2022197407",
        "paper_author": "Moro E.",
        "affiliation_name": "Universidad Carlos III de Madrid",
        "affiliation_city": "Getafe",
        "affiliation_country": "Spain",
        "affiliation_id": "60001741",
        "affiliation_state": "Madrid"
    },
    {
        "paper_title": "Pre-Trained Language Models for Interactive Decision-Making",
        "publication": "Advances in Neural Information Processing Systems",
        "citied_by": "73",
        "cover_date": "2022-01-01",
        "Abstract": "Language model (LM) pre-training is useful in many language processing tasks. But can pre-trained LMs be further leveraged for more general machine learning problems? We propose an approach for using LMs to scaffold learning and generalization in general sequential decision-making problems. In this approach, goals and observations are represented as a sequence of embeddings, and a policy network initialized with a pre-trained LM predicts the next action. We demonstrate that this framework enables effective combinatorial generalization across different environments and supervisory modalities. We begin by assuming access to a set of expert demonstrations, and show that initializing policies with LMs and fine-tuning them via behavior cloning improves task completion rates by 43.6% in the VirtualHome environment. Next, we integrate an active data gathering procedure in which agents iteratively interact with the environment, relabel past “failed” experiences with new goals, and update their policies in a self-supervised loop. Active data gathering further improves combinatorial generalization, outperforming the best baseline by 25.1%. Finally, we explain these results by investigating three possible factors underlying the effectiveness of the LM-based policy. We find that sequential input representations (vs. fixed-dimensional feature vectors) and LM-based weight initialization are both important for generalization. Surprisingly, however, the format of the policy inputs encoding (e.g. as a natural language string vs. an arbitrary sequential encoding) has little influence. Together, these results suggest that language modeling induces representations that are useful for modeling not just language, but also goals and plans; these representations can aid learning and generalization even outside of language processing.",
        "DOI": "NA",
        "paper_author": "Li S.",
        "affiliation_name": "Massachusetts Institute of Technology",
        "affiliation_city": "Cambridge",
        "affiliation_country": "United States",
        "affiliation_id": "60022195",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Beauty Face: An Android Application for Cosmetic Consumers to Try On and Receive Product Recommendation",
        "publication": "6th International Conference on Information Technology, InCIT 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Nowadays, the number of smartphone users shopping online over the internet is increasing due to the COVID-19 pandemic situation. People need to stay safe at home and work from home according to the social distancing policy. It is difficult for people to go shopping onsite at a physical store. Especially, cosmetic consumers cannot try on cosmetic products, and they are not sure which product is suitable for them. Therefore, to solve these problems, this research project proposes 'Beauty Face', which is an android application for cosmetic consumers who would like to try on products and receive recommendations. The Beauty Face application allows users to find the desired products and view product information. Also, it provides a useful feature for users to try on products by applying augmented reality (AR) technology to simulate cosmetic and beauty products on the user's face. Moreover, the Beauty Face application can recommend products that are appropriate for users by applying machine learning (ML) technology. The Beauty Face application can be helpful for cosmetic users to be more convenient in buying cosmetic and beauty products online and reducing chances of being infected or spreading COVID-19.",
        "DOI": "10.1109/InCIT56086.2022.10067684",
        "paper_author": "Tangsripairoj S.",
        "affiliation_name": "Mahidol University",
        "affiliation_city": "Nakhon Pathom",
        "affiliation_country": "Thailand",
        "affiliation_id": "60012718",
        "affiliation_state": "Nakhon Pathom"
    },
    {
        "paper_title": "Reinforcement Learning and Stochastic Optimization: A Unified Framework for Sequential Decisions",
        "publication": "Reinforcement Learning and Stochastic Optimization: A Unified Framework for Sequential Decisions",
        "citied_by": "85",
        "cover_date": "2022-01-01",
        "Abstract": "REINFORCEMENT LEARNING AND STOCHASTIC OPTIMIZATION Clearing the jungle of stochastic optimization Sequential decision problems, which consist of “decision, information, decision, information,” are ubiquitous, spanning virtually every human activity ranging from business applications, health (personal and public health, and medical decision making), energy, the sciences, all fields of engineering, finance, and e-commerce. The diversity of applications attracted the attention of at least 15 distinct fields of research, using eight distinct notational systems which produced a vast array of analytical tools. A byproduct is that powerful tools developed in one community may be unknown to other communities.Reinforcement Learning and Stochastic Optimization offers a single canonical framework that can model any sequential decision problem using five core components: state variables, decision variables, exogenous information variables, transition function, and objective function. This book highlights twelve types of uncertainty that might enter any model and pulls together the diverse set of methods for making decisions, known as policies, into four fundamental classes that span every method suggested in the academic literature or used in practice.Reinforcement Learning and Stochastic Optimization is the first book to provide a balanced treatment of the different methods for modeling and solving sequential decision problems, following the style used by most books on machine learning, optimization, and simulation. The presentation is designed for readers with a course in probability and statistics, and an interest in modeling and applications. Linear programming is occasionally used for specific problem classes. The book is designed for readers who are new to the field, as well as those with some background in optimization under uncertainty.Throughout this book, readers will find references to over 100 different applications, spanning pure learning problems, dynamic resource allocation problems, general state-dependent problems, and hybrid learning/resource allocation problems such as those that arose in the COVID pandemic. There are 370 exercises, organized into seven groups, ranging from review questions, modeling, computation, problem solving, theory, programming exercises and a \"diary problem\" that a reader chooses at the beginning of the book, and which is used as a basis for questions throughout the rest of the book.",
        "DOI": "10.1002/9781119815068",
        "paper_author": "Powell W.B.",
        "affiliation_name": "Princeton University",
        "affiliation_city": "Princeton",
        "affiliation_country": "United States",
        "affiliation_id": "60003269",
        "affiliation_state": "NJ"
    },
    {
        "paper_title": "CCTV-FullyAware: toward end-to-end feasible privacy-enhancing and CCTV forensics applications",
        "publication": "Proceedings - 2022 IEEE 21st International Conference on Trust, Security and Privacy in Computing and Communications, TrustCom 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "It is estimated that over 1 billion Closed-Circuit Television (CCTV) cameras are operational worldwide. The advertised main benefits of CCTV cameras have always been the same; physical security, safety, and crime deterrence. The current scale and rate of deployment of CCTV cameras bring additional research and technical challenges for CCTV forensics as well, as for privacy enhancements.This paper presents the first end-to-end system for CCTV forensics and feasible privacy-enhancing applications such as exposure measurement, CCTV route recovery, CCTV-aware routing/navigation, and crowd-sourcing. For this, we developed and evaluated four complex and distinct modules (CCTVCV [1], OSRM-CCTV [2], BRIMA [3], CCTV-Exposure [4]), all of which are novel, unique, peer-reviewed, and can be used either separately or within an integrated end-to-end system such as CCTV-FullyAware. We release all our artefacts as open-source/open data. We hope our work will bootstrap policy-driving discussions and large-scale applications such as CCTV forensics and privacy-enhancing technologies.",
        "DOI": "10.1109/TrustCom56396.2022.00170",
        "paper_author": "Turtiainen H.",
        "affiliation_name": "University of Jyväskylä",
        "affiliation_city": "Jyvaskyla",
        "affiliation_country": "Finland",
        "affiliation_id": "60032398",
        "affiliation_state": "Central Finland"
    },
    {
        "paper_title": "Machine Learning and MCDM Approach to Characterize Student Attrition in Higher Education",
        "publication": "Artificial Intelligence in Industry 4.0 and 5G Technology",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "School dropout is a global problem that affects all countries, being worst in underdeveloped countries like those in Africa and Latin America mainly, also in some countries in Asia. This paper presents a methodology based on multiple criteria decision making (MCDM) and machine learning approaches which address the problem of assessing factors related to student attrition in universities for aiding in developing efficient policies to mitigate dropout rates. As a real case study data from the “Universidad Simon Bolivar” was developed to demonstrate the application of the proposed methodology. Some interesting findings of factors related to student dropout are discussed.",
        "DOI": "10.1002/9781119798798.ch11",
        "paper_author": "Arrieta-M Luisa F.",
        "affiliation_name": "Universidad Simón Bolívar",
        "affiliation_city": "Barranquilla",
        "affiliation_country": "Colombia",
        "affiliation_id": "60106970",
        "affiliation_state": "Atlantico"
    },
    {
        "paper_title": "GIS-based Multi-scale Residential Building Energy Performance Prediction using a Data-driven Approach",
        "publication": "Building Simulation Conference Proceedings",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "Urban planning and development strategies are undergoing a transformation from conventional design to more innovative approaches in order to combat climate change. As such, city planners often develop strategic sustainable energy plans to minimize overall energy consumption and CO2 emissions. Planning at such scales could be informed by spatial analysis of the building stock using Geographic Information Systems (GIS) based mapping. A data-driven methodology could aid identification of building energy performance using existing available building data. However, existing studies in literature focus on either a single building or a limited number of buildings for energy performance prediction, thus, ignoring multiple scales. This paper develops a methodology for GIS-based residential building energy performance prediction at multi-scale using a data-driven approach. The machine-learning algorithm predicts building energy ratings from local to national scale using a bottom-up approach. The multi-scale mapping process integrates the predictive modeling results with GIS. This study demonstrates the methodology for the Irish residential building stock to evaluate the energy rating at multiple scales. Modeling results indicate priority geographical areas that have the greatest potential for energy savings.",
        "DOI": "10.26868/25222708.2021.30177",
        "paper_author": "Ali U.",
        "affiliation_name": "University College Dublin",
        "affiliation_city": "Dublin",
        "affiliation_country": "Ireland",
        "affiliation_id": "60005141",
        "affiliation_state": "Leinster"
    },
    {
        "paper_title": "Disguised Face Detection using Machine Learning",
        "publication": "AIST 2022 - 4th International Conference on Artificial Intelligence and Speech Technology",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "The crime rate has been increasing day by day. According to India today, the police have said that 3,06,389 crime cases were reported in 2021 alone. To reduce the crime rate, faster action and stricter law policies are required. Due to the increasing crime rate, it becomes a very difficult task for the police to catch the criminals and address the issue quickly. One strong piece of evidence that helps the Police identify criminals is the CCTV footage. Unfortunately, the images in CCTV footage are generally blurred. Also, when criminals disguise themselves and commit a crime, it becomes even more difficult in identifying their faces. Hence, more time is required to resolve cases. To solve this problem, a tool was developed for disguised detection of images using machine learning algorithms. This tool includes age and gender detection of disguised images, an image search engine that can retrieve similar images, and the calculation of percentage similarity between the images. In our paper testing is performed using various algorithms like CNN, KNN, SVM, Logistic Regression and Decision tree [1]. Apart from this, a dataset is also generated with real time images, and further applied various noises, blurred images, and tested the model. We have observed that using SVM, an accuracy of around 81% is achieved and CNN has performed extremely well with an accuracy of 90.6%.",
        "DOI": "10.1109/AIST55798.2022.10065003",
        "paper_author": "Kumar S.V.",
        "affiliation_name": "Vasavi College of Engineering",
        "affiliation_city": "Hyderabad",
        "affiliation_country": "India",
        "affiliation_id": "60099690",
        "affiliation_state": "TS"
    },
    {
        "paper_title": "Prediction of Wheat Yield in Henan Province Based on xDeepFM Algorithm",
        "publication": "Proceedings - 2022 Chinese Automation Congress, CAC 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Grain production has a very important impact on the national economy and people's livelihood. High-precision prediction of grain yield has important practical significance and theoretical research value. Development of latest machine learning technology opens up new way for it. This paper summarized the data related to wheat yield in five provinces: Henan, Hebei, Shandong, Shanxi and Shaanxi, involving 15 influencing factors: such as wheat sown area, the number of primary industry employees, the total power of agricultural machinery, and so on. Then the correlation analysis on the relevant influencing factors was done. And the wheat yield of Henan Province from 2006 to 2020 was predicted based on the xDeepFM deep learning algorithm with good sparse feature processing ability. Wheat predicted yield data for 15 consecutive years have generally performed well. The maximum absolute percentage error is 12.35%, the minimum is 0.35%, and the mean absolute percentage error is 7.71% Some of the poor annual prediction results may be affected by the particularity of local regional factors such as nature or policy in Henan province, resulting in an error of more than 10% in the prediction yield. The results show that the xDeepFM algorithm can be used to predict wheat yield effectively in Henan Province. It has an important reference value for the prediction of provincial grain yield by using xDeepFM model.",
        "DOI": "10.1109/CAC57257.2022.10056086",
        "paper_author": "Hao Y.",
        "affiliation_name": "North China University of Water Resources and Electric Power",
        "affiliation_city": "Zhengzhou",
        "affiliation_country": "China",
        "affiliation_id": "60103148",
        "affiliation_state": "Henan"
    },
    {
        "paper_title": "An Architecture for Deploying Reinforcement Learning in Industrial Environments",
        "publication": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
        "citied_by": "4",
        "cover_date": "2022-01-01",
        "Abstract": "Industry 4.0 is driven by demands like shorter time-to-market, mass customization of products, and batch size one production. Reinforcement Learning (RL), a machine learning paradigm shown to possess a great potential in improving and surpassing human level performance in numerous complex tasks, allows coping with the mentioned demands. In this paper, we present an OPC UA based Operational Technology (OT)-aware RL architecture, which extends the standard RL setting, combining it with the setting of digital twins. Moreover, we define an OPC UA information model allowing for a generalized plug-and-play like approach for exchanging the RL agent used. In conclusion, we demonstrate and evaluate the architecture, by creating a proof of concept. By means of solving a toy example, we show that this architecture can be used to determine the optimal policy using a real control system.",
        "DOI": "10.1007/978-3-031-25312-6_67",
        "paper_author": "Schäfer G.",
        "affiliation_name": "Fachhochschule Salzburg",
        "affiliation_city": "Salzburg",
        "affiliation_country": "Austria",
        "affiliation_id": "60009709",
        "affiliation_state": "Salzburg"
    },
    {
        "paper_title": "Logic-Driven Traffic Big Data Analytics: Methodology and Applications for Planning",
        "publication": "Logic-Driven Traffic Big Data Analytics: Methodology and Applications for Planning",
        "citied_by": "12",
        "cover_date": "2022-01-01",
        "Abstract": "This book starts from the relationship between urban built environment and travel behavior and focuses on analyzing the origin of traffic phenomena behind the data through multi-source traffic big data, which makes the book unique and different from the previous data-driven traffic big data analysis literature. This book focuses on understanding, estimating, predicting, and optimizing mobility patterns. Readers can find multi-source traffic big data processing methods, related statistical analysis models, and practical case applications from this book. This book bridges the gap between traffic big data, statistical analysis models, and mobility pattern analysis with a systematic investigation of traffic big data’s impact on mobility patterns and urban planning.",
        "DOI": "10.1007/978-981-16-8016-8",
        "paper_author": "Zhong S.",
        "affiliation_name": "Dalian University of Technology",
        "affiliation_city": "Dalian",
        "affiliation_country": "China",
        "affiliation_id": "60004538",
        "affiliation_state": "Liaoning"
    },
    {
        "paper_title": "GIS-AIDED GEOSPATIAL ANALYSIS OF THE FOOD INDUSTRY OF BULGARIA (2010-2020)",
        "publication": "International Multidisciplinary Scientific GeoConference Surveying Geology and Mining Ecology Management, SGEM",
        "citied_by": "2",
        "cover_date": "2022-01-01",
        "Abstract": "The current research aims to apply cluster analysis using the software ArcGIS in the study of the food industry in Bulgaria for the period 2010 to 2020. The use of clustering methods is necessary to differentiate homogeneous groups of administrative-territorial units of NUTS 3 level on certain indicators to reveal several features and implement specific economic policies and measures for areas of a cluster and others. The grouping of the areas according to the considered indicators was done with the tool Grouping Analysis. Grouping and classification techniques are some of the most widely used methods in machine learning. We have selected No_spatial_constraint for the Spatial Constraints parameter, for grouping using the K-Means algorithm. Based on the results of the “average intergroup connection” method, the areas are grouped into 7 clusters (food industry, 2010 and 2020; food and beverage products for the period 2010-2020) and into 4 clusters (tobacco production for the period 2010-2020). The selection of indicators based on which the clusters are formed is following the generally accepted indicators for assessing the state and importance of the food industry in the structure of the economy and their information accessibility. The following indicators were used output for 2010 and 2020, employees for 2010 and 2020, and export earnings for 2010 and 2020 for the given territorial unit The territorial distribution of the population, in combination with the historical and modern economic development of the settlements, forms the regional differences in the development of the food industry in the country. The cluster analysis of certain indicators for the assessment of the food industry at the NUTS 3 level for 2010 and 2020 shows some change in the trends in the territorial development of the industry. The cluster analysis shows that there are slight territorial differences at the NUTS 3 level in food production, with large consumer centers and markets being the most important. In the activities of tobacco and beverage production, the territorial differences are minimal.",
        "DOI": "10.5593/sgem2022/2.1/s11.47",
        "paper_author": "Ravnachka A.",
        "affiliation_name": "National Institute of Geophysics, Geodesy and Geography, Bulgarian Academy of Sciences",
        "affiliation_city": "Sofia",
        "affiliation_country": "Bulgaria",
        "affiliation_id": "60113114",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "To Mask or Not To Mask? A Machine Learning Approach to Covid News Coverage Attitude Prediction Based on Time Series and Text Content",
        "publication": "Proceedings - 2022 IEEE 25th International Conference on Computational Science and Engineering, CSE 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "In the past few decades, with the explosion of information, a large number of computer scientists have devoted themselves to analyzing collected data and applying these findings to many disciplines. Natural language processing (NLP) has been one of the most popular areas for data analysis and pattern recognition. A significantly large amount of data is obtained in text format due to the ease of access nowadays. Most modern techniques focus on exploring large sets of textual data to build forecasting models; they tend to ignore the importance of temporal information which is often the main ingredient to determine the performance of analysis, especially in the public policy view. The contribution of this paper is two-fold. First, a dataset called COVID-News is collected from three news agencies, which consists of article segments related to wearing masks during the COVID-19 pandemic. Second, we propose a long-short term memory (LSTM)-based learning model to predict the attitude of the articles from the three news agencies towards wearing a mask with both temporal and textural information. Experimental results on COVID-News dataset show the effectiveness of the proposed LSTM-based algorithm.",
        "DOI": "10.1109/CSE57773.2022.00018",
        "paper_author": "Zhao J.",
        "affiliation_name": "Lakehead University",
        "affiliation_city": "Thunder Bay",
        "affiliation_country": "Canada",
        "affiliation_id": "60025949",
        "affiliation_state": "ON"
    },
    {
        "paper_title": "Counterfactual harm",
        "publication": "Advances in Neural Information Processing Systems",
        "citied_by": "13",
        "cover_date": "2022-01-01",
        "Abstract": "To act safely and ethically in the real world, agents must be able to reason about harm and avoid harmful actions. However, to date there is no statistical method for measuring harm and factoring it into algorithmic decisions. In this paper we propose the first formal definition of harm and benefit using causal models. We show that any factual definition of harm is incapable of identifying harmful actions in certain scenarios, and show that standard machine learning algorithms that cannot perform counterfactual reasoning are guaranteed to pursue harmful policies following distributional shifts. We use our definition of harm to devise a framework for harm-averse decision making using counterfactual objective functions. We demonstrate this framework on the problem of identifying optimal drug doses using a dose-response model learned from randomized control trial data. We find that the standard method of selecting doses using treatment effects results in unnecessarily harmful doses, while our counterfactual approach identifies doses that are significantly less harmful without sacrificing efficacy.",
        "DOI": "NA",
        "paper_author": "Richens J.G.",
        "affiliation_name": "DeepMind Technologies Limited",
        "affiliation_city": "London",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60111161",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Machine Learning Models based in Supervised Learning for the Detection of Diabetes Mellitus in the City of Guayaquil",
        "publication": "Proceedings of the LACCEI international Multi-conference for Engineering, Education and Technology",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Due to the problems presented by public health centers in the city of Guayaquil in Ecuador, a group of specialists greeted the creation of a comprehensive diabetes diagnosis center, which, with an entrepreneurial initiative, is bringing together private health centers that are interested in providing a technologically assisted service. Using artificial intelligence tools, the comprehensive center detects whether a person has diabetes mellitus or not; Therefore, through this mechanism and applying the established triage, the person detected by the AI model is referred to the respective private health center that is part of the comprehensive center (entrepreneurship). Supervised learning was applied in the implementation of some conventional machine learning techniques, which were compared with an artificial neural network model to determine the model that can obtain the highest accuracy in learning diabetes diagnosis. As a result, it was obtained that the artificial neural network ANN reached 99.33% accuracy compared to the support vector machines SVM, which reached 98.6% accuracy in the classification of patients with diabetes mellitus, so that in view of the fact that the RNA model, is the one who has learned to detect diabetes with greater accuracy and is recommended to be used in the comprehensive diagnostic center, for rapid referral to a private health center that can apply rapid and effective treatment .",
        "DOI": "10.18687/LEIRD2022.1.1.208",
        "paper_author": "Patiño-Pérez D.",
        "affiliation_name": "Universidad de Guayaquil",
        "affiliation_city": "Guayaquil",
        "affiliation_country": "Ecuador",
        "affiliation_id": "60072042",
        "affiliation_state": "Guayas"
    },
    {
        "paper_title": "Proceedings - 2022 IEEE 4th International Conference on Trust, Privacy and Security in Intelligent Systems, and Applications, TPS-ISA 2022",
        "publication": "Proceedings - 2022 IEEE 4th International Conference on Trust, Privacy and Security in Intelligent Systems, and Applications, TPS-ISA 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "The proceedings contain 34 papers. The topics discussed include: privacy and security issues for human digital twins; a universal deduplication architecture for secure and efficient cloud storage; distributed cyber-infrastructures and artificial intelligence in hybrid post-quantum era; auditing metaverse requires multimodal deep learning; privacy challenges and solutions for image data sharing; MnemoSys: a conditional probability estimation protocol for blockchain audited reputation management; managing reputation scores in a blockchain-based decentralized marketplace; efficient blockchain enabled attribute-based access control as a service; money talks: detection of disposable phishing websites by analyzing its building costs; investigating organizational factors associated with GDPR noncompliance using privacy policies: a machine learning approach; enabling location based services with privacy and integrity protection in untrusted environments through blockchain and secure computation; and a method of constructing malware classification dataset using clustering.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Maintenance Scope Optimization, through a Real Time Prediction of Risk of Failure",
        "publication": "Offshore Technology Conference Asia, OTCA 2022",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "Capital intensive industrial assets require highly specialized maintenance activities. Traditional preventive time-based approach, based on OEM maintenance policies, has been gradually evolving towards more sophisticated condition-based maintenance techniques. Further ISO 55000 states that assets exist to provide value to the organization and its stakeholders (BS ISO 55002, 2014). To develop a successful and modern maintenance program, it suggests having a value-based approach when dealing with maintenance decisions, both financial and non-financial constrains needs to be evaluated when decision taken regarding maintenance actions. Higher values can be reaped from an asset when the maintenance intervals are optimized. By optimization it is envisaged that the right number and type of maintenance tasks, at the right intervals, in the right way is performed on the asset to maximize the risk reduction within available budgetary constraints. The paper presents an overview of an analytics framework for predictive maintenance service boosted by Machine Learning and asset knowledge, applied to turbomachinery assets. Optimization of the maintenance scenario is performed through a risk model that assesses online health status and probability of failure, by detecting functional anomalies or aging phenomena and evaluating their impact on asset serviceability.",
        "DOI": "10.4043/31343-MS",
        "paper_author": "Sepe M.",
        "affiliation_name": "Baker Hughes, a GE company",
        "affiliation_city": "Houston",
        "affiliation_country": "United States",
        "affiliation_id": "60013567",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "Investigating Organizational Factors Associated with GDPR Noncompliance using Privacy Policies: A Machine Learning Approach",
        "publication": "Proceedings - 2022 IEEE 4th International Conference on Trust, Privacy and Security in Intelligent Systems, and Applications, TPS-ISA 2022",
        "citied_by": "5",
        "cover_date": "2022-01-01",
        "Abstract": "The General Data Protection Regulation (GDPR) came into effect in May 2018 to ensure and safeguard data subjects' rights. This enactment profoundly shaped, among other things, data processing organizations' privacy policies to comply with the GDPR's transparency requirements - for compliance with the GDPR is compulsory. Nevertheless, despite the potential goodwill to change, complying with the GDPR can be challenging for some organizations, e.g., small and medium-sized enterprises, due to, for example, a lack of resources. This study explores what factors may correlate with GDPR-compliance practices in organizations by analyzing the corresponding privacy policies. The contribution of this study is twofold. First, we have devised a classification model using machine learning (ML) and natural language processing (NLP) techniques to assess the GDPR-compliance practices promised in privacy policies regarding the GDPR core privacy policy requirement of Purpose. Using this model, we have collected a data set of 8 614 organizations active in the European Union (EU) containing organizational information and GDPR-compliance promises derived from organizations' privacy policies, as made publicly available. Our second contribution is an analysis of the resulting classification to identify organizational factors related to the disclosure of the GDPR core privacy policy requirement of Purpose in organizations' privacy policies.",
        "DOI": "10.1109/TPS-ISA56441.2022.00023",
        "paper_author": "Aberkane A.J.",
        "affiliation_name": "Universiteit Gent",
        "affiliation_city": "Ghent",
        "affiliation_country": "Belgium",
        "affiliation_id": "60033316",
        "affiliation_state": "VOV"
    },
    {
        "paper_title": "Accuracy-Fairness Tradeoff in Parole Decision Predictions: A Preliminary Analysis",
        "publication": "Proceedings - 2022 IEEE/ACM 9th International Conference on Big Data Computing, Applications and Technologies, BDCAT 2022",
        "citied_by": "2",
        "cover_date": "2022-01-01",
        "Abstract": "Algorithms play an essential and expanding role in public policy decisions, including those in criminal justice. This short paper reports on the first author's summer research project characterizing the tradeoff between accuracy and fairness in parole decision predictions. The dataset employed in this study contains over 30,000 parole decisions made by the New York State Division of Criminal Justice Services. Each decision contains information on the subject, such as sex, race/ethnicity, and parole decision, as well as predictive features describing the crime committed by the subject and the parole interview held. Logistic regression, decision tree, support vector machine, and random forest models are trained and utilized to analyze parole decision predictions based on the available features. Most models fail to pass standard fairness tests for most fairness metrics. Moreover, while there may be an overall tradeoff between fairness and accuracy, the obtained differences in accuracy are too small to make a well-supported claim. Future research may enhance the preliminary work introduced in this paper by using multiple real-world datasets to investigate the tradeoff between accuracy and fairness.",
        "DOI": "10.1109/BDCAT56447.2022.00047",
        "paper_author": "Gardner J.W.",
        "affiliation_name": "Department of Computer Science",
        "affiliation_city": "Houston",
        "affiliation_country": "United States",
        "affiliation_id": "60151392",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "Segmentation of X-Ray Images of Rocks Using Supervoxels Over-Segmentation",
        "publication": "International Petroleum Technology Conference, IPTC 2022",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "Digital core analysis has gained the interest of many scientific communities because of its impact on our understanding of flow in porous media. A typical workflow in digital core analysis includes scanning, reconstruction, denoising, segmentation, and modeling. Image analysis and modeling highly depend on the quality of the segmentation step. In this regard, conventional image segmentation methods often require user input/interference. This results in user bias and may produce a range of possible segmentation outcomes. To address this, we propose an unsupervised machine learning framework that offers multiple functionalities including improved mineral and micro-porosity identification. Superpixel (2D) and (3D) work by over-segmenting greyscale images using a family of over-segmentation algorithms. Simple Linear Iterative Clustering (SLIC) is one of these algorithms that is recognized for its speed and memory efficiency. The proposed framework utilizes SLIC and unsupervised clustering methods for segmenting greyscale images. SLIC divides the 2D and 3D images into segments having pixels (or voxels) with similar features (i.e., intensity range). Statistical features of each segment are computed and used for identifying the segment label through unsupervised clustering techniques. The unsupervised voting clustering implements a majority voting policy from multiple clustering algorithms including Hierarchical clustering and k-means clustering. A North Sea sandstone 2D X-ray image along with its SEM image were used to validate this framework. Different metrics were used to measure the accuracy of the X-ray segmentation with SEM segmentation. Our results show a mean Jaccard index of 70% and a mean Dice index of 81%. The same workflow is applied using supervoxels on a high-resolution 3D Indiana Limestone image and the results show similar accuracy margins compared to watershed segmentation. Comparison with other segmentation methods shows an average Jaccard score of 74% and an average Dice index score of 83%. To the best of our knowledge, this is the first application of superpixels over-segmentation algorithms in semantic segmentation of X-ray micro-CT images of porous media. The findings of this study highlighted the advantage of these algorithms in detecting sub-resolution porosity regions in greyscale images and obtaining accurate multilabel segmentation.",
        "DOI": "10.2523/IPTC-22131-MS",
        "paper_author": "Alqahtani H.",
        "affiliation_name": "UNSW Sydney",
        "affiliation_city": "Sydney",
        "affiliation_country": "Australia",
        "affiliation_id": "60028333",
        "affiliation_state": "NSW"
    },
    {
        "paper_title": "A Hybrid CNN-LSTM Model with Word-Emoji Embedding for Improving the Twitter Sentiment Analysis on Indonesia's PPKM Policy",
        "publication": "Proceeding - 6th International Conference on Information Technology, Information Systems and Electrical Engineering: Applying Data Sciences and Artificial Intelligence Technologies for Environmental Sustainability, ICITISEE 2022",
        "citied_by": "6",
        "cover_date": "2022-01-01",
        "Abstract": "The policy of limiting community mobilization is implemented to reduce the daily rate of COVID-19. However, a high-accuracy sentiment analysis model can determine public sentiment toward such policies. Our research aims to improve the accuracy of the LSTM model on sentiment analysis of the Jakarta community towards PPKM using Indonesian language Tweets with emoji embedding. The first stage is modeling using the hybrid CNN-LSTM model. It is a combination between CNN and LSTM. The CNN model cites word embedding and emoji embedding features that reflect the dependence on temporary short-term sentiment. At the same time, LSTM builds long-term sentiment relationships between words and emojis. Next, the model evaluation uses Accuracy, Loss, the receiver operating curve (ROC), the precision and recall curve, and the area under curve (AUC) value to see the performance of the designed model. Based on the results of the tests, we conclude that the CNN-LSTM Hybrid Model performs better with the words+emoji dataset. The ROC AUC is 0.966, while the precision-recall curve AUC is 0.957.",
        "DOI": "10.1109/ICITISEE57756.2022.10057720",
        "paper_author": "Pane S.F.",
        "affiliation_name": "Telkom University",
        "affiliation_city": "Bandung",
        "affiliation_country": "Indonesia",
        "affiliation_id": "60103730",
        "affiliation_state": "West Java"
    },
    {
        "paper_title": "GENEDISCO: A BENCHMARK FOR EXPERIMENTAL DESIGN IN DRUG DISCOVERY",
        "publication": "ICLR 2022 - 10th International Conference on Learning Representations",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "In vitro cellular experimentation with genetic interventions, using for example CRISPR technologies, is an essential step in early-stage drug discovery and target validation that serves to assess initial hypotheses about causal associations between biological mechanisms and disease pathologies. With billions of potential hypotheses to test, the experimental design space for in vitro genetic experiments is extremely vast, and the available experimental capacity - even at the largest research institutions in the world - pales in relation to the size of this biological hypothesis space. Machine learning methods, such as active and reinforcement learning, could aid in optimally exploring the vast biological space by integrating prior knowledge from various information sources as well as extrapolating to yet unexplored areas of the experimental design space based on available data. However, there exist no standardised benchmarks and data sets for this challenging task and little research has been conducted in this area to date. Here, we introduce GeneDisco, a benchmark suite for evaluating active learning algorithms for experimental design in drug discovery. GeneDisco contains a curated set of multiple publicly available experimental data sets as well as open-source implementations of state-of-the-art active learning policies for experimental design and exploration.",
        "DOI": "NA",
        "paper_author": "Mehrjou A.",
        "affiliation_name": "GlaxoSmithKline plc.",
        "affiliation_city": "Brentford",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60020649",
        "affiliation_state": "Middlesex"
    },
    {
        "paper_title": "RIESZNET AND FORESTRIESZ: AUTOMATIC DEBIASED MACHINE LEARNING WITH NEURAL NETS AND RANDOM FORESTS",
        "publication": "ICLR 2022 - 10th International Conference on Learning Representations",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Many causal and policy effects of interest are defined by linear functionals of high-dimensional or non-parametric regression functions. √n-consistent and asymptotically normal estimation of the object of interest requires debiasing to reduce the effects of regularization and/or model selection on the object of interest. Debiasing is typically achieved by adding a correction term to the plug-in estimator of the functional, that is derived based on a functional-specific theoretical derivation of what is known as the influence function and which leads to properties such as double robustness and Neyman orthogonality. We instead implement an automatic debiasing procedure based on automatically learning the Riesz representation of the linear functional using Neural Nets and Random Forests. Our method solely requires value query oracle access to the linear functional. We propose a multi-tasking Neural Net debiasing method with stochastic gradient descent minimization of a combined Riesz representer and regression loss, while sharing representation layers for the two functions. We also propose a Random Forest method which learns a locally linear representation of the Riesz function. Even though our methodology applies to arbitrary functionals, we experimentally find that it beats state of the art performance of the prior neural net based estimator of Shi et al. (2019) for the case of the average treatment effect functional. We also evaluate our method on the more challenging problem of estimating average marginal effects with continuous treatments, using semi-synthetic data of gasoline price changes on gasoline demand.",
        "DOI": "NA",
        "paper_author": "Anonymous ",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Natural Question Generation using Transformers and Reinforcement Learning",
        "publication": "Proceedings - 2022 OITS International Conference on Information Technology, OCIT 2022",
        "citied_by": "6",
        "cover_date": "2022-01-01",
        "Abstract": "Natural Question Generation (NQG) is among the most popular open research problems in Natural Language Processing (NLP) alongside Neural Machine Translation, Open Domain Chatbots, etc. Among the many approaches taken up to solve this problem, neural networks have been deemed the benchmark in this particular research area. This paper aims at adopting a generator - evaluator framework in a neural network architecture to allow additional focus on the context of the content used for framing a question. The generator uses NLP architectures like transformers (T5) to generate a question given a context while the evaluator uses Reinforcement Learning (RL) to check the correctness of the generated question. The involvement of RL has improved the results (as shown in Table 2), and there is increased computational efficiency as the training is coupled with the policy of RL. This turns the problem into a reinforcement learning task and allows for the generation of a wide range of questions for the same context-answer pair. The given algorithm is tested on the benchmark dataset - SQuAD with BLEU score as the evaluation metric",
        "DOI": "10.1109/OCIT56763.2022.00061",
        "paper_author": "Biswas D.",
        "affiliation_name": "Amrita Vishwa Vidyapeetham University, Bangalore",
        "affiliation_city": "Bengaluru",
        "affiliation_country": "India",
        "affiliation_id": "60107595",
        "affiliation_state": "KA"
    },
    {
        "paper_title": "Classification and Characterization of Memory Reference Behavior in Machine Learning Workloads",
        "publication": "Proceedings - 2022 IEEE/ACIS 24th International Winter Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing, SNPD 2022",
        "citied_by": "9",
        "cover_date": "2022-01-01",
        "Abstract": "With the recent penetration of artificial intelligence (AI) technologies into many areas of computing, machine learning is being incorporated into modern software design. As the in-memory data of AI workloads increasingly grows, it is important to characterize memory reference behaviors in machine learning workloads. In this paper, we perform a characterization study for memory references in machine learning workloads as the learning types (i.e., supervised vs. unsupervised) and the problem domains (i.e., classification, regression, and clustering) are varied. From this study, we uncover the following five characteristics. First, machine learning workloads exhibit significantly different memory reference patterns from traditional workloads, but they are similar regardless of learning types and problem domains. Second, in all workloads, memory reads and writes continue to appear for a wide range of memory addresses, but there is a specific time period where only reads appear. Third, among references to memory areas (i.e., code, data, heap, stack, library), library accounts for about 90% of total memory references. Fourth, there is a low popularity bias between memory pages referenced in machine learning workloads, especially for writes. Fifth, when estimating the likelihood of re-referencing, temporal locality is dominant in top 100 memory pages, but access frequency provides better information after that ranking. It is expected that the characterization of memory references conducted in this paper will be helpful in the design of memory management policies for machine learning workloads.",
        "DOI": "10.1109/SNPD54884.2022.10051800",
        "paper_author": "Kwon S.",
        "affiliation_name": "Ewha Womans University",
        "affiliation_city": "Seoul",
        "affiliation_country": "South Korea",
        "affiliation_id": "60001018",
        "affiliation_state": "Seoul"
    },
    {
        "paper_title": "Applications of Data Analytics and Machine Learning for Digital Twin-based Precision Biodiversity: A Review",
        "publication": "ICACNIS 2022 - 2022 International Conference on Advanced Creative Networks and Intelligent Systems: Blockchain Technology, Intelligent Systems, and the Applications for Human Life, Proceeding",
        "citied_by": "3",
        "cover_date": "2022-01-01",
        "Abstract": "Biodiversity projections and model evaluation are essential to inform future formulation of biodiversity policy. These could be supported by data analytics and machine learning approaches, as well as precision technologies. However, existing works are segregated by the selection of species under-study and depending on the location. This paper reviews the existing approaches for precision biodiversity covering dashboard and data analytics, deep learning and machine learning, and digital twin for precision biodiversity. We propose a framework based on interactive machine learning that could facilitate a continuous biodiversity projection modeling to facilitate incremental learning and reduce uncertainties from the complex factors that contribute to biodiversity declines. The proposed framework exploits digital twin model based on a research forest setting that pioneers this work in Malaysia. The framework comprises of short-term quick wins and long-term expectation of digitalization transformation towards precision biodiversity.",
        "DOI": "10.1109/ICACNIS57039.2022.10055149",
        "paper_author": "Sharef N.M.",
        "affiliation_name": "Universiti Putra Malaysia",
        "affiliation_city": "Serdang",
        "affiliation_country": "Malaysia",
        "affiliation_id": "60025577",
        "affiliation_state": "Selangor"
    },
    {
        "paper_title": "Stochastic Second-Order Methods Improve Best-Known Sample Complexity of SGD for Gradient-Dominated Functions",
        "publication": "Advances in Neural Information Processing Systems",
        "citied_by": "8",
        "cover_date": "2022-01-01",
        "Abstract": "We study the performance of Stochastic Cubic Regularized Newton (SCRN) on a class of functions satisfying gradient dominance property with 1 ≤ α ≤ 2 which holds in a wide range of applications in machine learning and signal processing. This condition ensures that any first-order stationary point is a global optimum. We prove that the total sample complexity of SCRN in achieving ∊-global optimum is (Equation presented) for 1 ≤ α < 3/2 and (Equation presented) for 3/2 ≤ α ≤ 2. SCRN improves the best-known sample complexity of stochastic gradient descent. Even under a weak version of gradient dominance property, which is applicable to policy-based reinforcement learning (RL), SCRN achieves the same improvement over stochastic policy gradient methods. Additionally, we show that the average sample complexity of SCRN can be reduced to O(∊-2) for α = 1 using a variance reduction method with time-varying batch sizes. Experimental results in various RL settings showcase the remarkable performance of SCRN compared to first-order methods.",
        "DOI": "NA",
        "paper_author": "Masiha S.",
        "affiliation_name": "École Polytechnique Fédérale de Lausanne",
        "affiliation_city": "Lausanne",
        "affiliation_country": "Switzerland",
        "affiliation_id": "60028186",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Big Data Predictive Analytics Model for Cardiovascular Risk Detection using Machine Learning Techniques",
        "publication": "Proceedings - 2022 IEEE 2nd International Symposium on Sustainable Energy, Signal Processing and Cyber Security, iSSSC 2022",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "Cardiovascular Disease (CVD) is a leading global cause of incapacity and premature mortality. The discovery of early sickness symptoms using geospatial data will facilitate the development of fact-based health policies and suitable disease management strategies. Early identification of CVDs is one of the essential methods; researchers in the field have presented several methods for risk prediction. Most practices have used traditional machine learning techniques, which have historically produced excellent results. However, the development of artificial intelligence and neural network models has created countless new prospects in medical science. The present technique has reached 99% recognition accuracy after considerable experimentation with the Artificial Neural Network (ANN) model. A dataset with data on around 1500 patients was used to train and test the presented framework. A patient-level risk detection system will be made available as a result of the research so that patients may be aware of the Risk and implement the recommended preventive measures.",
        "DOI": "10.1109/iSSSC56467.2022.10051293",
        "paper_author": "Kanani J.",
        "affiliation_name": "Pandit Deendayal Energy University",
        "affiliation_city": "Gandhinagar",
        "affiliation_country": "India",
        "affiliation_id": "60106943",
        "affiliation_state": "GJ"
    },
    {
        "paper_title": "Transactive electricity: how decentralized renewable power can support security, resilience and decarbonization",
        "publication": "Intelligent Environments: Advanced Systems for a Healthy Planet, Second Edition",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "The dramatic reduction in the cost of renewable energy production during the 2020s, especially in roof-top solar, followed by a similar cost reduction trajectory with batteries, suggested to many a smooth transition from nuclear and fossil fuels to solar and wind. But centralized and blunt policy and price signals including massive worldwide subsidies for incumbent industries have perpetuated established patterns of much of the generation and distribution system remaining far from where it is consumed, leading to the higher electricity network and grid stabilization costs. To allow renewables to be scaled efficiently, variable renewable energy, such as solar and wind, need to be consumed much closer to where they are produced enabling a much closer link to the consumer. Time and place price signals are needed to make this happen thus creating the need for a transactive electricity system. This change to how the power system works is fundamentally different so resistance to change in the incumbent conventional and centralized energy industry became glaring as renewable power emerged as critical to a climate-stable economy. A Transactive Electricity system challenges the whole centralized power business model and has significant social and security implications but promises multiple benefits. This chapter suggests that the transition is aided by a fundamental overhaul of how the distributed market model can operate and, in particular, how digital systems, including machine learning, AI, and blockchain, potentially support renewables becoming locally dispatchable through transactive energy markets. We provide initial results of trials using the “time and place transactive model”. This model can first leapfrog into places where regulatory push back from traditional grid and supply interests is least intense and then expand across energy markets in a Kuhnian way, creating a new energy paradigm.",
        "DOI": "10.1016/B978-0-12-820247-0.00006-0",
        "paper_author": "Green J.",
        "affiliation_name": "Curtin University",
        "affiliation_city": "Perth",
        "affiliation_country": "Australia",
        "affiliation_id": "60031226",
        "affiliation_state": "WA"
    },
    {
        "paper_title": "Recent artificial intelligence methods and coronaviruses",
        "publication": "Application of Natural Products in SARS-CoV-2",
        "citied_by": "2",
        "cover_date": "2022-01-01",
        "Abstract": "For the third time in the past few decades, the novel coronavirus has been named the most lethal ever, able to infect animals and humans all over the world. Healthcare policy uses advanced technology like artificial intelligence (AI), deep machine learning, and big data to combat and forecast emerging diseases. AI is increasingly being used to aid in disease detection, prevention, response, rehabilitation, and clinical analysis. Since these developments are still in the early stages, minimum development is being made in their application for significant deliberation at the local and international strategy levels. Nonetheless, a new example demonstrates that AI-driven systems are becoming more reliable. Companies like BlueDot and Metabiota used AI to predict the coronavirus disease-2019 (COVID-19) in China before it shocked the world in late 2019 by monitoring its effects and spread. Using computational techniques to discover new target drugs and vaccines in silico is one approach. Drug repurposing is a method for discovering new applications for existing or experimental drugs. A drug repurposing approach is a viable option for novel diseases like COVID-19. Drug discovery and vaccination, biological research, remote video diagnosis, tracking patient contacts, COVID-19 recognition and therapy via smart robots, and identification of noncontact infection are all areas where AI will be used in the future. This chapter aims to look at AI-based technology for coronaviruses such as severe acute respiratory syndrome and the Middle East respiratory syndrome diagnosis, management, drug repurposing medications, novel drug discovery, and vaccines, including during the COVID-19 pandemic.",
        "DOI": "10.1016/B978-0-323-95047-3.00009-5",
        "paper_author": "Ur Rehman S.",
        "affiliation_name": "Cholistan University of Veterinary &amp; Animal Sciences, Bahawalpur",
        "affiliation_city": "Bahawalpur",
        "affiliation_country": "Pakistan",
        "affiliation_id": "60280636",
        "affiliation_state": "Punjab"
    },
    {
        "paper_title": "Efficient Data-Driven Network Functions",
        "publication": "Proceedings - IEEE Computer Society's Annual International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunications Systems, MASCOTS",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "Cloud environments require dynamic and adaptive networking policies. It is preferred to use heuristics over advanced learning algorithms in Virtual Network Functions (VNFs) in production because of high-performance constraints. This paper proposes Aquarius to passively yet efficiently gather observations and enable the use of machine learning to collect, infer, and supply accurate networking state information - without incurring additional signaling and management overhead. This paper illustrates the use of Aquarius with a traffic classifier, an auto-scaling system, and a load balancer - and demonstrates the use of three different machine learning paradigms - unsupervised, supervised, and reinforcement learning, within Aquarius, for inferring network state. Testbed evaluations show that Aquarius increases network state visibility and brings notable performance gains with low overhead.",
        "DOI": "10.1109/MASCOTS56607.2022.00028",
        "paper_author": "Yao Z.",
        "affiliation_name": "Cisco Systems",
        "affiliation_city": "San Jose",
        "affiliation_country": "United States",
        "affiliation_id": "60030003",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "The INFN-LNF Bruno Touschek Visitor Centre: a hub for public engagement activities",
        "publication": "Proceedings of Science",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "The Bruno Touschek Visitor Centre is a permanent exhibition dedicated to the evolution of particle physics, playing a central role in the popularization of science initiatives conducted at INFN Frascati National Laboratory (LNF). The centre was conceived as a public engagement hub to involve people with the main discoveries and the latest developments in technology and research fields investigated by INFN. Since its opening in 2018, it has been the scene where researchers, citizens, students, teachers, policy makers and different stakeholders share the importance of the evolution of science and its applications. In the itinerary path, the history of accelerator machines starting from AdA, the first storage ring accelerating matter and antimatter, designed and realized in Frascati, up to the future perspectives in particle acceleration, the development of particle detectors and the applications of research and technology-based outcomes in everyday life are presented. The exhibition features the exposition of instruments, interactive exhibits and immersive elements that will be described in this contribution. The centre is part of the LNF diffused museum that includes a Cockcroft-Walton accelerator, a section of the Adone storage ring, the KLOE experiment, lately enriched by a digital installation based on video mapping and NAUTILUS, a gravitational waves detector. Close to the exhibition hall, a laboratory named EduLab has recently been realized and it is devoted to hands-on activities and science demonstrations addressed to pupils of Primary and Middle Schools. The Bruno Touschek Visitor Centre and EduLab are hosting both formal and informal science education events, held either in person or virtually, providing, in this latter case, online resources that extend the accessibility of LNF outreach projects and enhance lifelong learning. The Visitor Centre itinerary path, the related public engagement activities and their societal impact are here presented.",
        "DOI": "NA",
        "paper_author": "Bertelli S.",
        "affiliation_name": "INFN, Laboratori Nazionali Di Frascati",
        "affiliation_city": "Frascati",
        "affiliation_country": "Italy",
        "affiliation_id": "60016340",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Cognitive Analysis of Covid-19 on the Africa Economy Using Linear Regression",
        "publication": "Proceedings of the 5th International Conference on Information Technology for Education and Development: Changing the Narratives Through Building a Secure Society with Disruptive Technologies, ITED 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "The coronavirus outbreak in 2020 has made it difficult to implement macroeconomic initiatives and has affected the economy in all countries in Africa. There has been a lot of concern regarding how to stabilize the economy at least to where it was before the coronavirus outbreak. There was increased governmental allocation to combat the spread and reduce COVID-19's impacts. This study evaluates the economic impacts of the COVID-19 pandemic on some African countries and examines the cognitive analysis as it affects the economy considering layoffs and other revenue losses, as well as a consistent recession and deterioration in the banking and economic sectors. A linear regression method was used in the analysis of this work. Although the pandemic affects every aspect of life and society at large, this study examines how it affects the nation's economy. It was recognized that numerous policy instruments, including those connected to health and social protection, fiscal policy, and financial, industrial, and trade policies, needed to be implemented for the economy to recover properly from the financial loss. The analysis of the data, shows that there was a reduction in the GDP of each country during the Covid-19 pandemic. It is predicted that adopting these technologies may minimize suffering among people and aid in the economy's recovery from recession and bankruptcy.",
        "DOI": "10.1109/ITED56637.2022.10051365",
        "paper_author": "Bolarinwa J.D.",
        "affiliation_name": "Federal University of Agriculture, Abeokuta",
        "affiliation_city": "Abeokuta",
        "affiliation_country": "Nigeria",
        "affiliation_id": "60008386",
        "affiliation_state": "Ogun"
    },
    {
        "paper_title": "Challenges in the Adoption of Artificial Intelligence and Machine Learning in Zimbabwe's Insurance industry",
        "publication": "2022 1st Zimbabwe Conference of Information and Communication Technologies, ZCICT 2022",
        "citied_by": "4",
        "cover_date": "2022-01-01",
        "Abstract": "This study sought to investigate the challenges in the adoption of AI and ML in the Zimbabwean insurance industry. The TechnologyOrganisation-Environment (TOE) model was selected as the base theory underpinning the study. The study adopted a pragmatic research philosophy and a census was carried out on twenty insurance companies. Questionnaires were administered on operations managers representing their insurance companies. Interviews were used to collect data from 12 operation managers. NVivo version 16 was used to analyse the data thematically. The study results show that adoption of AI by the insurance sector in Zimbabwe is hindered by shortage of resources, lack of expertise and high cost of AI compliant products. These researchers recommend resource allocation, training of employees, culture change, and updated technological environment to ensure effective adoption of AI. This study will contribute to the body of knowledge, be significant to insurance practitioners and policy makers whilst giving direction for future studies.",
        "DOI": "10.1109/ZCICT55726.2022.10045910",
        "paper_author": "Moyo J.",
        "affiliation_name": "Midlands State University Zimbabwe",
        "affiliation_city": "Gweru",
        "affiliation_country": "Zimbabwe",
        "affiliation_id": "60045281",
        "affiliation_state": "Midlands"
    },
    {
        "paper_title": "14th International Conference on Agents and Artificial Intelligence, ICAART 2022",
        "publication": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "The proceedings contain 9 papers. The special focus in this conference is on Agents and Artificial Intelligence. The topics include: Comparing Beta-VAE to WGAN-GP for Time Series Augmentation to Improve Classification Performance; safe Policy Improvement Approaches and Their Limitations; integrative System of Deep Classifiers Certification: Case of Convolutional Attacks; Co-operative Multi-agent Twin Delayed DDPG for Robust Phase Duration Optimization of Large Road Networks; generating the Gopher’s Grounds: Form, Function, Order, and Alignment; Domain Dependent Parameter Setting in SAT Solver Using Machine Learning Techniques; more Sustainable Text Classification via Uncertainty Sampling and a Human-in-the-Loop.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "System Modelling and Controller Design of a Variable Structure Two-Wheeled Robot Using Robust Adaptive Dynamic Programming",
        "publication": "Proceedings of the 34th Chinese Control and Decision Conference, CCDC 2022",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "In this paper, a variable structure two-wheeled robot is studied. First, we show the system modelling using Appell dynamics. A simplified and decoupled linear model, which describes the dynamic characteristics when the robot's rotational angle of two forks is fixed, is derived in this paper. Then, a robust controller to reject the disturbances is designed by using a learning algorithm based on robust adaptive dynamic programming methods. Finally, the simulation shows the effectiveness of our controller in stabilizing the robot and disturbance rejection.",
        "DOI": "10.1109/CCDC55256.2022.10033602",
        "paper_author": "Zhao H.",
        "affiliation_name": "Beijing University of Posts and Telecommunications",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "60016930",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Predicting Insurance Churn to Reduce Clawback",
        "publication": "2022 IEEE International Conference on Technology Management, Operations and Decisions, ICTMOD 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Many insurance companies still depend on agents for marketing and selling their products. The compensation for these agents is based mainly on commissions and hence such agents try to sell as many policies as possible. In addition to increasing the quantity of policies sold, agents may try to recommend policies with high commissions even if other policies are more suitable for the client. Furthermore, agents may at times ill-advise customers to change their policies in order to gain a new commission even if such a change does not benefit the customer or the insurance company. This last type of transaction is called internal insurance churn or policy churn since the customer remains with the insurance company but they make a policy change. We use Machine Learning techniques to predict policy churn prior to payment of the commission to the agent. This avoids the process of having the agent repay the commission (a process termed clawback) in such cases. We illustrate how the proposed approach can provide significant savings to the insurance company.",
        "DOI": "10.1109/ICTMOD55867.2022.10041878",
        "paper_author": "Goordeen D.",
        "affiliation_name": "The University of the West Indies, St. Augustine Campus",
        "affiliation_city": "St Augustine",
        "affiliation_country": "Trinidad and Tobago",
        "affiliation_id": "60071706",
        "affiliation_state": "Tunapuna–Piarco"
    },
    {
        "paper_title": "METAMORPH: LEARNING UNIVERSAL CONTROLLERS WITH TRANSFORMERS",
        "publication": "ICLR 2022 - 10th International Conference on Learning Representations",
        "citied_by": "37",
        "cover_date": "2022-01-01",
        "Abstract": "Multiple domains like vision, natural language, and audio are witnessing tremendous progress by leveraging Transformers for large scale pre-training followed by task specific fine tuning. In contrast, in robotics we primarily train a single robot for a single task. However, modular robot systems now allow for the flexible combination of general-purpose building blocks into task optimized morphologies. However, given the exponentially large number of possible robot morphologies, training a controller for each new design is impractical. In this work, we propose MetaMorph, a Transformer based approach to learn a universal controller over a modular robot design space. MetaMorph is based on the insight that robot morphology is just another modality on which we can condition the output of a Transformer. Through extensive experiments we demonstrate that large scale pre-training on a variety of robot morphologies results in policies with combinatorial generalization capabilities, including zero shot generalization to unseen robot morphologies. We further demonstrate that our pre-trained policy can be used for sample-efficient transfer to completely new robot morphologies and tasks.",
        "DOI": "NA",
        "paper_author": "Gupta A.",
        "affiliation_name": "Stanford University",
        "affiliation_city": "Stanford",
        "affiliation_country": "United States",
        "affiliation_id": "60012708",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Research on Microservice Coordination Technologies based on Deep Reinforcement Learning",
        "publication": "Proceedings - 2022 2nd International Conference on Electronic Information Technology and Smart Agriculture, ICEITSA 2022",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "In order to solve the problems of scaling, placement and traffic scheduling of a large number of microservices. Based on tensorflow and keras machine learning framework, this paper designs and implements a microservice coordination method based on DDPG (deep deterministic policy gradient) algorithm in DRL (deep reinforcement learning). Firstly, the network model and service traffic model are built, and the optimization objectives are defined, i.e. throughout and delay. Besides, a POMDP (partially observable Markov decision processes) and a traffic scheduling table are designed to build the service coordination framework and finish the ultimate algorithm. Secondly, the utility of our algorithm and other baseline-algorithms is numerically evaluated within different traffic pattern and real-world traffic traces. Thirdly, technology proposed in this paper and 5g core network are combined. Furthermore, the significance of this coordination technology on 5g core network in the trend of microservice is also discussed.",
        "DOI": "10.1109/ICEITSA57468.2022.00012",
        "paper_author": "Zhao J.",
        "affiliation_name": "Beijing Polytechnic",
        "affiliation_city": "Beijing",
        "affiliation_country": "China",
        "affiliation_id": "113721227",
        "affiliation_state": "Beijing"
    },
    {
        "paper_title": "Decision Support System Terms of SME's Credit Lending Based on Machine Learning Approach",
        "publication": "2022 International Conference of Science and Information Technology in Smart Administration, ICSINTESA 2022",
        "citied_by": "2",
        "cover_date": "2022-01-01",
        "Abstract": "Small and medium-sized (SMEs) companies have proven economic competencies and great potential in light of the recent global economic transformations in developed and developing countries, which qualified them to play a major role in development and reform policy, by providing job opportunities, intensifying the industrial fabric, and supporting large industries. It should be noted that these institutions need greater efforts and important components that respond to their aspirations and idiosyncrasies, to be able to survive and compete in the markets, as we have witnessed the bankruptcy and collapse of many companies due to financial conditions and obstacles. In this paper, we proposed building a lending decision support system (DSS) for SMEs and researching the reasons that prevent them from lending, and what are the pillars on which the credit decision in banks depends, and how difficult it is to analyze the variables and their information, the data set represents SMEs lending requests from Jordan Ahli Bank. Four machine learning algorithms were used to build an automated system to support decision-making in granting credit. The classification accuracy of the models was evaluated where the accuracy exceeded 90%, and SVM outperformed other rating models. Our findings show that the challenges that SMEs face to obtain credit are summarized by the lack of the company's capital, the recent date of incorporation, and the lack of guarantees.",
        "DOI": "10.1109/ICSINTESA56431.2022.10041593",
        "paper_author": "Hazboun F.H.",
        "affiliation_name": "Ahli Bank Itd",
        "affiliation_city": "Ramallah",
        "affiliation_country": "Palestine",
        "affiliation_id": "129270338",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A review of challenges and solutions in adopting a participatory geographical information system for disaster management",
        "publication": "Ecocycles",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "Disaster management is a critical component in mitigating the impacts of various natural, and other disasters such as floods, cyclones, forest fires, earthquakes, disease spreading etc. The primary aim of disaster management in a specific region is to empower the local neighborhood to higher determine its natural danger instincts and therefore migrate closer to options for lowering that risk. Conventional techniques of disaster management are majorly driven by the quantitative information collected from various events. Some of the recent techniques have used more advanced data-driven and non-linear approaches such as machine learning, and spatial analysis tools such as GIS for making more informed decisions. However, these techniques cannot often represent the dynamics of demographic units, and event impact in small regions due to a multitude of reasons such as lack of data, equipment, more generalized approaches, etc. Participatory Geographic Information System (PGIS) overcomes some of the limitations present in the traditional techniques by incorporating local communities as stakeholders in making various policies, distributing risk information etc. PGIS has been adopted in various fields such as land cover planning, agriculture information systems, data collection systems etc. Other than these applications, the effectiveness of PGIS in disaster management in handling various natural disasters such as floods, cyclones, forest fires, and disease spread has been demonstrated in several studies. However, in many places, PGIS is not yet evolved and its implementation is still at the infancy level due to several reasons. Despite many advantages, PGIS presents many problems comprising insufficient infrastructure, training facilities, engagement and education of the community members towards a combined decision, etc. therefore provision of necessary infrastructure can improve the overall impact of implementing PGIS. Involving the local community and educating them on the right approach for the success of PGIS is a complex task. Further, the conflict of opinions between technical personnel and locals can be another factor that limits the usage. However, from the results of various studies, the advantages of PGIS implementation can outweigh the limitations of implementation.",
        "DOI": "10.19040/ecocycles.v8i2.245",
        "paper_author": "Ramu P.",
        "affiliation_name": "GMR Institute of Technology",
        "affiliation_city": "Rajam",
        "affiliation_country": "India",
        "affiliation_id": "60079432",
        "affiliation_state": "AP"
    },
    {
        "paper_title": "Predictive Analytics-Based Cybersecurity Framework for Cloud Infrastructure",
        "publication": "International Journal of Cloud Applications and Computing",
        "citied_by": "26",
        "cover_date": "2022-01-01",
        "Abstract": "The most valuable asset for any organization and individual is data and the information it holds. This is the main reason for information security to be the top concern in boardrooms and executive meetings. Security failures and data breaches now can impact an organization or a country’s budget economy. To reduce cybersecurity risks and improve data protection, there is an urgent need to implement a standard framework for cybersecurity. This framework utilizes AI and ML by including policies, guidelines, standards and practices, and data sources from cloud infrastructure systems like networks, servers, security systems, and end-user devices. Combining the data set gathered and risk governance information with artificial intelligence and machine learning, this research presents a framework that collects datasets, enriches and validates logs and datasets, then correlates them to analyze and predict the response to cyber-attacks with a high level of accuracy using the ML model.",
        "DOI": "10.4018/IJCAC.297106",
        "paper_author": "Bhardwaj A.",
        "affiliation_name": "University of Petroleum and Energy Studies",
        "affiliation_city": "Dehradun",
        "affiliation_country": "India",
        "affiliation_id": "60107631",
        "affiliation_state": "UK"
    },
    {
        "paper_title": "Just Fine-tune Twice: Selective Differential Privacy for Large Language Models",
        "publication": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022",
        "citied_by": "8",
        "cover_date": "2022-01-01",
        "Abstract": "Protecting large language models from privacy leakage is becoming increasingly crucial with their wide adoption in real-world products. Yet applying differential privacy (DP), a canonical notion with provable privacy guarantees for machine learning models, to those models remains challenging due to the trade-off between model utility and privacy loss. Utilizing the fact that sensitive information in language data tends to be sparse, Shi et al. (2021) formalized a DP notion extension called Selective Differential Privacy (SDP) to protect only the sensitive tokens defined by a policy function. However, their algorithm only works for RNN-based models. In this paper, we develop a novel framework, Just Fine-tune Twice (JFT), that achieves SDP for state-of-the-art large transformer-based models. Our method is easy to implement: it first fine-tunes the model with redacted in-domain data, and then fine-tunes it again with the original in-domain data using a private training mechanism. Furthermore, we study the scenario of imperfect implementation of policy functions that misses sensitive tokens and develop systematic methods to handle it. Experiments show that our method achieves strong utility compared to previous baselines. We also analyze the SDP privacy guarantee empirically with the canary insertion attack.",
        "DOI": "NA",
        "paper_author": "Shi W.",
        "affiliation_name": "Columbia University",
        "affiliation_city": "New York",
        "affiliation_country": "United States",
        "affiliation_id": "60030162",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Doc2Bot: Accessing Heterogeneous Documents via Conversational Bots",
        "publication": "Findings of the Association for Computational Linguistics: EMNLP 2022",
        "citied_by": "8",
        "cover_date": "2022-01-01",
        "Abstract": "This paper introduces Doc2Bot, a novel dataset for building machines that help users seek information via conversations. This is of particular interest for companies and organizations that own a large number of manuals or instruction books. Despite its potential, the nature of our task poses several challenges: (1) documents contain various structures that hinder the ability of machines to comprehend, and (2) user information needs are often underspecified. Compared to prior datasets that either focus on a single structural type or overlook the role of questioning to uncover user needs, the Doc2Bot dataset is developed to target such challenges systematically. Our dataset contains over 100,000 turns based on Chinese documents from five domains, larger than any prior document-grounded dialog dataset for information seeking. We propose three tasks in Doc2Bot: (1) dialog state tracking to track user intentions, (2) dialog policy learning to plan system actions and contents, and (3) response generation which generates responses based on the outputs of the dialog policy. Baseline methods based on the latest deep learning models are presented, indicating that our proposed tasks are challenging and worthy of further research.",
        "DOI": "NA",
        "paper_author": "Fu H.",
        "affiliation_name": "Nanjing University",
        "affiliation_city": "Nanjing",
        "affiliation_country": "China",
        "affiliation_id": "60033100",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "Solar-Cast: Solar Power Generation Prediction from Weather Forecasts using Machine Learning",
        "publication": "2022 IEEE 10th Power India International Conference, PIICON 2022",
        "citied_by": "5",
        "cover_date": "2022-01-01",
        "Abstract": "The rapid growth of solar generation technology has become a boon in the energy sector. Smart grids have replaced the conventional Grids due to upcoming various distributed energy sources feeding the grid. The correct estimation of solar intensity according to geographical features will help in determining the capacity of smart Grids. In real-time, most smart grids are compelled to change their renewable energy production process according to the real-time availability of energy resources like wind and solar during the day. Thus, to assuage this problem, the possibility is investigated by using readily available weather data on the NSRDB website to predict solar forecasts 48 hours ahead in the future by using various Machine Learning(ML) algorithms. In this paper, a new day-night model has been designed to limit the uncertainty of solar power generation and reduce the dependability of power grids on non-renewable energy sources like fossil fuels. Further, to improve the solar forecasting prediction, multiple weather observations were taken from preceding time intervals to establish a new data set in linear regression and its subtypes (Ridge and Lasso regression).",
        "DOI": "10.1109/PIICON56320.2022.10045237",
        "paper_author": "Singhal R.",
        "affiliation_name": "Indraprastha Institute of Information Technology, Delhi",
        "affiliation_city": "New Delhi",
        "affiliation_country": "India",
        "affiliation_id": "60105479",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "MONITORING OF TECHNOLOGICAL INEQUALITY IN THE MODERN WORLD ECONOMY: ASSESSING THE SCALE OF THE GLOBAL CONFLICT AND SCENARIOS FOR ITS DEVELOPMENT DEPENDING ON CONFLICT MANAGEMENT",
        "publication": "Contributions to Conflict Management, Peace Economics and Development",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Purpose: This chapter aims to study the issue of technological inequality in the modern world economy from the position of assessment of the scale of global conflict and the scenario of its development depending on conflict management. Design/methodology/approach: The performed review of literature sources has shown that they provide an insufficient scientific basis for determining the level of technological inequality in the modern world economy from the position of assessment of the scale of the global conflict and scenarios of its development depending on conflict management. To fill this gap in the system of scientific knowledge, we use the method of comparative and correlation analysis of statistical data. The research objects are China and the United States, as well as other countries of the world that have the highest level of technological development, trade, and digitalization. Findings: This chapter provides a review of factors that determine scientific arguments in favour of technological inequality of countries, which leads to a global conflict. Many forms of inequality have a socio-economic character and are connected to access to the main services (healthcare, education, or accommodation), as well as incomes and access to the sources of income, especially in the sphere of employment. The deficit of decent work and inequality turned the COVID-19 pandemic from the crisis of public healthcare into the crisis of employment and social conflict, which influenced the subsistence of millions of employees. There is a real risk that without comprehensive and well-coordinated political actions, the increase in inequality and reduction of general progress in the labour sphere will be preserved in many dimensions. There’s a need for the measures of international policy to provide develop-ing countries with access to vaccines and financial support, including through restructuring of debts. Originality/value: It is proved that after the creation of the UN, the nature of conflicts and violence underwent serious changes. Conflicts take fewer human lives but last longer, and the frequency of conflicts between groups within a country is higher than the frequency of international conflicts. In certain parts of the world, crimes on a gender basis are increasing in numbers. Besides, technologies allow using robots, drones, cyberattacks, viruses, and hackers for mili-tary purposes. At the same time, international cooperation is weak, similar to the global ability to prevent and regulate conflicts and all possible forms of violence. Technological progress changes the character of the development of conflicts. Achievements in the sphere of AI and machine learning will play an important role in this process of transformation, so the character of threats from the government and non-government subjects will change. The use of AI raises the precision of cyberattacks and physical and biological attacks, making the identification of attackers very difficult.",
        "DOI": "10.1108/S1572-832320220000030009",
        "paper_author": "Sozinova A.A.",
        "affiliation_name": "Vyatka State University",
        "affiliation_city": "Kirov",
        "affiliation_country": "Russian Federation",
        "affiliation_id": "60104421",
        "affiliation_state": "Kirov Oblast"
    },
    {
        "paper_title": "TD3lite: FPGA Acceleration of Reinforcement Learning with Structural and Representation Optimizations",
        "publication": "Proceedings - 2022 32nd International Conference on Field-Programmable Logic and Applications, FPL 2022",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "Reinforcement learning (RL) is an effective and increasingly popular machine learning approach for optimization and decision-making. However, modern reinforcement learning techniques, such as deep Q-learning, often require neural network inference and training, and therefore are computationally expensive. For example, Twin-Delay Deep Deterministic Policy Gradient (TD3), a state-of-the-art RL technique, uses as many as 6 neural networks. In this work, we study the FPGA-based acceleration of TD3. To address the resource and computational overhead due to inference and training of the multiple neural networks of TD3, we propose TD3lite, an integrated approach consisting of a network sharing technique combined with bitwidth-optimized block floating-point arithmetic. TD3lite is evaluated on several robotic benchmarks with continuous state and action spaces. With only 5.7% learning performance degradation, TD3lite achieves 21 ×and 8 ×speedup compared to CPU and GPU implementations, respectively. Its energy efficiency is 26 ×of the GPU implementation. Moreover, it utilizes 25 - 40% fewer FPGA resources compared to a conventional sinale-precision floating-point representation of TD3.",
        "DOI": "10.1109/FPL57034.2022.00023",
        "paper_author": "Hu C.W.",
        "affiliation_name": "Texas A&amp;M University",
        "affiliation_city": "College Station",
        "affiliation_country": "United States",
        "affiliation_id": "60020547",
        "affiliation_state": "TX"
    },
    {
        "paper_title": "Predicting Reuse Interval for Optimized Web Caching: An LSTM-Based Machine Learning Approach",
        "publication": "International Conference for High Performance Computing, Networking, Storage and Analysis, SC",
        "citied_by": "3",
        "cover_date": "2022-01-01",
        "Abstract": "Caching techniques are widely used in the era of cloud computing from applications, such as Web caches to infrastructures, Memcached and memory caches in computer architectures. Prediction of cached data can greatly help improve cache management and hit rate. The recent advancement of deep learning techniques enables the design of novel intelligent cache replacement policies. In this work, we propose a learning-aided approach to predict future data accesses. We find that a powerful LSTM-based recurrent neural network can provide high prediction accuracy based on only a cache trace as input. The high accuracy results from a carefully crafted locality-driven feature design. Inspired by the high prediction accuracy, we propose a pseudo OPT policy and evaluate it upon 13 real-world storage workloads from Microsoft Cloud. Results demonstrate that our new policy improves the state-of-art by up to 19.2% and incurs only 2.3% higher miss ratio than OPT on average.",
        "DOI": "10.1109/SC41404.2022.00091",
        "paper_author": "Li P.",
        "affiliation_name": "TikTok",
        "affiliation_city": "Mountain View",
        "affiliation_country": "United States",
        "affiliation_id": "128287234",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Explainable Artificial Intelligence in Human Resources: a Computational Study",
        "publication": "2022 International Conference on Data Analytics for Business and Industry, ICDABI 2022",
        "citied_by": "3",
        "cover_date": "2022-01-01",
        "Abstract": "The application of Machine Learning in Human Resources is becoming increasingly common. The Human Resources industry relies on Machine Learning to automate processes, improve efficiency, and enhance decision making. However, using Machine Learning in Human Resources is not without its challenges. One of these challenges is the lack of interpretability. Interpretability is essential in Human Resources because many of the decisions made in the field have a direct impact on people's lives. As such, these decisions must be made transparently and understandably. In this paper, we apply a model-agnostic technique for providing post-hoc explanations, known as Anchors, to a real-world dataset from the Human Resources industry. The dataset was used to create a predictive pipeline for employee attrition. The result is a Human Capital Management system capable of explaining the reasons behind employee attrition, allowing the Human Resource professionals to enact retention policies promptly. Moreover, the results suggest that Anchors can be used to create a prescriptive pipeline that can be used to explain the reasons behind every single decision to leave the company, as they are easily interpretable by a non-expert. This system has the advantage of allowing the Decision Maker to act in a prescriptive way and retain valuable resources.",
        "DOI": "10.1109/ICDABI56818.2022.10041624",
        "paper_author": "Abonamah A.",
        "affiliation_name": "Abu Dhabi School of Management",
        "affiliation_city": "Abu Dhabi",
        "affiliation_country": "United Arab Emirates",
        "affiliation_id": "60113237",
        "affiliation_state": "Abu Dhabi"
    },
    {
        "paper_title": "Learning the optimal joint operation of the energy systems of Uruguay, Brazil, Paraguay and Argentina",
        "publication": "2022 IEEE PES Generation, Transmission and Distribution Conference and Exposition - Latin America, IEEE PES GTD Latin America 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "In the continuous fight against Bellman's Curse of Dimensionality, this work presents the first steps towards learning the Optimal Operation Policy of the electricity generation system of Uruguay, Brazil, Paraguay and Argentina with the infrastructures projected for the year 2030. The Operation Policy under consideration involves 76 state variables: one associated to the surface temperature anomaly of the Pacific Ocean in the N34 area, and 75 related to the hydroelectric reservoirs. The proposed methodology includes the design and training of two alternate neural network architectures combined with modern techniques devised for variance reduction and exploration, which were key to the success achieved.",
        "DOI": "10.1109/IEEEPESGTDLatinAmeri53482.2022.10037786",
        "paper_author": "Chaer R.",
        "affiliation_name": "Universidad de la Republica",
        "affiliation_city": "Montevideo",
        "affiliation_country": "Uruguay",
        "affiliation_id": "60071612",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "A Comprehensive Study of Road Traffic Accidents: Hotspot Analysis and Severity Prediction Using Machine Learning",
        "publication": "IBSSC 2022 - IEEE Bombay Section Signature Conference",
        "citied_by": "2",
        "cover_date": "2022-01-01",
        "Abstract": "This study analyses road traffic accident data recorded over a period of time to gain insights to the underlying pain points in the infrastructure and policies. Such insight allows us to focus our efforts in the right direction to make the lives of people safer. The data includes various geographical and meteorological factors affecting the severity of these accidents. We use Kernel density estimation (KDE) plots to analyse hotspots of accident-prone areas weighed against severity over years to understand the evolution of these dangerous zones. Furthermore, we use machine learning algorithms to predict the accident severity given certain parameters and to understand the factors that have a major influence on the severity of the accident. We have studied a publicly available dataset of road traffic accidents in the UK as a proof of concept of the pipeline to understand the underlying patterns of accidents occurring in a region of interest.",
        "DOI": "10.1109/IBSSC56953.2022.10037449",
        "paper_author": "Gupta U.",
        "affiliation_name": "PES University",
        "affiliation_city": "Bengaluru",
        "affiliation_country": "India",
        "affiliation_id": "60097260",
        "affiliation_state": "KA"
    },
    {
        "paper_title": "Flight Delay Prediction for Mitigation of Airport Commercial Revenue Losses Using Machine Learning on Imbalanced Dataset",
        "publication": "Proceeding of the International Conference on Computer Engineering, Network and Intelligent Multimedia, CENIM 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Flight delay is one of the factors that affect the decline in customer satisfaction and airport revenue. In addition to influencing customer perceptions of airport services, flight delay also has an impact on decreasing airport revenue and operation. This study models a flight delay prediction, and the process is carried out using Decision Tree, Random Forest, Gradient Boosted Tree, and XGBoost Tree algorithms. This study has also used and merged the weather characteristic data as secondary data to the airport operational flight data. To anticipate the imbalanced class, several sampling techniques were applied. Synthetic Minority Over Sampling Technique (SMOTE), Random Over-Sampling (ROS), Random Under-Sampling (RUS), and combining ROS with RUS are being used. The result of processing the analysis is in the form of a model to predict the category of flight delay. The model has been evaluated by using the Confusion Matrix and Area Under ROC Curve (AUC) value. The result of this study shows the Random Forest classifier with the combination of ROS + RUS technique and data split ratio of 90:10 gave the highest accuracy, error rate, and AUC value as shown as 82.58%, 17.42%, and 81.1% respectively on data testing. The result of the flight delay prediction model is expected to be a strategic recommendation for determining airport policies in the future. By implementing the best strategy related to the airport operation, it could carry out commercial planning in order to optimize airport commercial revenue.",
        "DOI": "10.1109/CENIM56801.2022.10037369",
        "paper_author": "Sugara R.A.",
        "affiliation_name": "Institut Teknologi Sepuluh Nopember",
        "affiliation_city": "Surabaya",
        "affiliation_country": "Indonesia",
        "affiliation_id": "60070707",
        "affiliation_state": "East Java"
    },
    {
        "paper_title": "Explainable Artificial Intelligence Applied to Deep Reinforcement Learning Controllers for Photovoltaic Maximum Power Point Tracking",
        "publication": "2022 International Conference on Future Trends in Smart Communities, ICFTSC 2022",
        "citied_by": "5",
        "cover_date": "2022-01-01",
        "Abstract": "Deep Reinforcement Learning (DRL) algorithms have been applied to extract maximum power from photovoltaic (PV) modules under a variety of environmental conditions. However, it is difficult for a human to explain how a DRL-based maximum power point tracking (MPPT) controller works as it consists of Neural Networks (NNs) that are generally complex and non-linear. Various Explainable Artificial Intelligence (XAI) techniques have been proposed to interpret NNs in power system applications, but MPPT controllers have yet to be analyzed. This paper presents the application of XAI techniques to the DRL agents for MPPT. Two distinct DRL agents were developed, one with and one without the information of the converter's duty cycle, using the Deep Deterministic Policy Gradient (DDPG) algorithm and analyzed using XAI techniques, namely Local Interpretable Model-Agnostic Explanations (LIME) and Shapley Additive Explanations (SHAP). The results reveal that the converter's input power is the most crucial information for the DRL agents when the converter is operating away from the maximum power point. When the converter approaches operation at the maximum power point, the DRL agents are significantly dependent on the power differential of the converter across time. If the information about the converter's duty cycle is available, the DRL agents are significantly reliant on the converter's duty cycle and disregard other observations for decision-making.",
        "DOI": "10.1109/ICFTSC57269.2022.10040061",
        "paper_author": "Tan P.S.",
        "affiliation_name": "Universiti Teknologi PETRONAS",
        "affiliation_city": "Seri Iskandar",
        "affiliation_country": "Malaysia",
        "affiliation_id": "60001278",
        "affiliation_state": "Perak"
    },
    {
        "paper_title": "Gradient Boosting Approach for Sentiment Analysis for Job Recommendation and Candidate Profiling",
        "publication": "IBSSC 2022 - IEEE Bombay Section Signature Conference",
        "citied_by": "2",
        "cover_date": "2022-01-01",
        "Abstract": "Sentiment Analysis has increasingly been used nowadays in many applications to evaluate opinion of public about products, policies, movies, politics. It is also used by government and law enforcement to understand behavior of people. One of the potential applications of sentiment analysis is candidate profiling and job recommendation. In the proposed research work, we evaluated the performance of supervised machine learning algorithms on dataset generated by us from twitter and indeed. We illustrated the steps involved in preproccesing the dataset generated through web scraping and making it ready for feeding into supervised algorithms. From our experimental study it is observed that Gradient Boosting Classifier gave the highest classification accuracy of 78.08 percent and AUC score of 0.819 on the test dataset.",
        "DOI": "10.1109/IBSSC56953.2022.10037443",
        "paper_author": "Singh S.",
        "affiliation_name": "Narsee Monjee Institute of Management Studies, Mumbai",
        "affiliation_city": "Mumbai",
        "affiliation_country": "India",
        "affiliation_id": "60079592",
        "affiliation_state": "MH"
    },
    {
        "paper_title": "Deep Learning-based Time Series Models for GDP and ICT Growth Prediction in India",
        "publication": "3rd IEEE 2022 International Conference on Computing, Communication, and Intelligent Systems, ICCCIS 2022",
        "citied_by": "4",
        "cover_date": "2022-01-01",
        "Abstract": "Approximately 13% of the country's gross domestic product (GDP) depends on information and communication technology (ICT), and India's digital economy accounts for nearly ${\\$}$200 billion in economic value annually. The literature has well established the function of ICT in stimulating economic growth. The work focuses on the relationship between the ICT use index and GDP based on the last 30-year time series multivariate data. The variables having a high correlation with respect to GDP has taken for the analysis. This work also forecasts the next 10-year growth in ICT use index and GDP w.r.t each other using machine learning and deep learning models which are linear regression(LR), random forest(RF), temporal convolutional network(TCN), Kalman forecaster(KF), Neural Basis Expansion Analysis for Interpretable Time Series Forecasting (NBEATS) model and transformer model. There are eight performance metrics we have used for model evaluation. The transformer model has been suggested as the best predicting model with the least root mean squared error value of 0.326, mean absolute error of 0.51, mean absolute ranged relative error of 52.592, and so on. This paper also suggested policies to foster the country's economic growth using ICT.",
        "DOI": "10.1109/ICCCIS56430.2022.10037636",
        "paper_author": "Kumari S.",
        "affiliation_name": "Mahatma Gandhi Central University",
        "affiliation_city": "Motihari",
        "affiliation_country": "India",
        "affiliation_id": "60282667",
        "affiliation_state": "BR"
    },
    {
        "paper_title": "Machine Learning based Smart Crop Recommender and Yield Predictor",
        "publication": "3rd IEEE 2022 International Conference on Computing, Communication, and Intelligent Systems, ICCCIS 2022",
        "citied_by": "5",
        "cover_date": "2022-01-01",
        "Abstract": "Commercialization of agriculture industry raises the demand for increasing the crop yield. Crop production is highly dependent on the type, and nutrition value of soil, pH, temperature, humidity, rainfall etc. Farmers lack in determining the precise requirements of a crop, crop yield, and profit. This may cause a huge loss to farmers. Therefore, there is a need for a crop recommender system. Various researchers proposed the crop recommender systems but, these systems lack in automatic prediction of nutrient requirements, and crop yield. Also, the existing systems do not consider the export and import policies, hence fails in precise profit estimation. In this research, The authors address the above-stated challenges, and propose a machine learning based crop recommender system that predict crop yield of 22 varieties of crops. they employ machine learning models namely decision tree, light GBM, naive bayes, support vector machine, logistic, random forest, and Xgboost. The models naive bayes, random forest, and Xgboost outperforms other ML models and reports an accuracy of 99%. Furthermore, the system analyses the profit of these crops and recommend the most profitable crops to grow on a specific piece of land.",
        "DOI": "10.1109/ICCCIS56430.2022.10037678",
        "paper_author": "Chhikara S.",
        "affiliation_name": "Manipal University Jaipur",
        "affiliation_city": "Jaipur",
        "affiliation_country": "India",
        "affiliation_id": "60108737",
        "affiliation_state": "RJ"
    },
    {
        "paper_title": "2022 IEEE 1st International Conference on Data, Decision and Systems, ICDDS 2022",
        "publication": "2022 IEEE 1st International Conference on Data, Decision and Systems, ICDDS 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "The proceedings contain 16 papers. The topics discussed include: implementation and performance assessment of wavelet prefiltered platform tilt computation using low-cost MEMS IMU; a machine learning based frame work for classification of neuromuscular disorders; towards direct comparison of community structures in social networks; general transit feed specification assisted effective traffic congestion prediction using decision trees and recurrent neural networks; two step recognition of raags in Hindustani classical music using supervised deep learning; ARMS: an analysis framework for mixed criticality systems; P3: a task migration policy for optimal resource utilization and energy consumption; CNN - time frequency representation based brain wave decoding from magnetoencephalography signals; and supply chain delay mitigation via supplier risk index assessment and reinforcement learning.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "P<sup>3</sup>: A task migration policy for optimal resource utilization and energy consumption",
        "publication": "2022 IEEE 1st International Conference on Data, Decision and Systems, ICDDS 2022",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "Theevolution in modern technologies like artificial intelligence, machine learning, cloud computing, edge computing, data science, etc, focuses on user perspectives like accuracy, response-time, and timeliness but at the same time consumes heavy energy due to large and fast data processing. From the system perspective, resource utilization and energy consumption are also significant design considerations. This work proposes a task migration policy for optimal core utilization and energy savings. The time taken by data analytical tasks to process the data varies, due to variations in the amount of data it analyzes in unit time. This creates variation in the core utilization due to which there exist small inactive intervals in the schedule, consuming energy. If the inactive state is known to continue for a longer duration, the core can be put into a shutdown state which effectively reduces overall energy consumption. Dynamic Procrastination (DP) is a commonly used technique to increase the inactive duration by postponing the tasks whenever possible. To further increase the inactive duration to qualify for shutting down the core, in a homogeneous multi-core (HMC) system, the jobs can be migrated to other cores. This effectively improves core utilization and reduces overall system energy without negatively affecting performance. Combining the DP and migration techniques introduces challenges like meeting deadlines, deciding upon push/pull migration, finding the number of tasks and suitable core for migration, and computation of energy consumption parameters. This paper proposes P3 (Push-Procrastinate-Pull) migration policy for the HMC system. The experimental evaluation with synthetically generated benchmark program suites shows that on an average P3reduces the overall energy by 1.2% and reduces the shutdown duration over the idle period by 2.22% over DP without migration.",
        "DOI": "10.1109/ICDDS56399.2022.10037287",
        "paper_author": "Gawali S.K.",
        "affiliation_name": "Birla Institute of Technology and Science, Pilani",
        "affiliation_city": "Pilani",
        "affiliation_country": "India",
        "affiliation_id": "60000414",
        "affiliation_state": "RJ"
    },
    {
        "paper_title": "MACHINE LEARNING MODELS TO PREDICT COVID-19 VACCINATION INTENTION: AN INDIAN STUDY",
        "publication": "International Journal of Professional Business Review",
        "citied_by": "2",
        "cover_date": "2022-01-01",
        "Abstract": "Purpose: Covid 19 pandemic has taken the world by shock for last few years, and it has greatly impacted the livelihood of people across all walks of life and even the economies of many nations got greatly affected. Governments across the globe revived from the impact of covid-19 pandemic using many strategies and policies which were formulated under the guidance of the world health organization. One of the Prime weapons which helped the governments and public against covid -19 is vaccination. This research which was conducted August 2021 was done to understand the perception of the public towards the covid 19 vaccination and to predict the public intention to take up covid -19 vaccination using the health belief model constructs. Theoretical framework:The Study has used the variables of the health belief model namely the perceived severity, perceived susceptibility, Perceived Benefits, Cues to action and other socio-demographic variables to predict the intent of the respondents towards taking Covid-19 vaccination. Design/methodology/approach: Data was collected using a self-administered online questionnaire distributed to the respondents from Tamil Nadu, India who are above 18 years of age. Machine Learning Algorithms like Logistic Regression, Artificial Neural Networks were used to predict the public intent to take up covid 19 vaccination. Findings: From the Analysis of Logistic Regression and Artificial Neural Network, it was found that Health Belief Model Constructs Perceived Barriers, Perceived Benefits and Cues to action, were significant factors that affect the public intention to vaccinate. Research, Practical & Social implications:Findings of the research will help the government, stake holders to understand the factors impacting the respondent’s intent to covid-19 vaccination which will guide them to plan better strategies for future vaccination drives Originality/value:The Study has used to two different machine learning algorithms to compare and corroborate the research findings and in turn identifying the significant predictors of covid-19 vaccination intent",
        "DOI": "10.26668/businessreview/2022.v7i6.e977",
        "paper_author": "Raj V.S.N.",
        "affiliation_name": "SRM Institute of Science and Technology",
        "affiliation_city": "Kattankulathur",
        "affiliation_country": "India",
        "affiliation_id": "60014340",
        "affiliation_state": "TN"
    },
    {
        "paper_title": "Offline Policy Selection under Uncertainty",
        "publication": "Proceedings of Machine Learning Research",
        "citied_by": "6",
        "cover_date": "2022-01-01",
        "Abstract": "The presence of uncertainty in policy evaluation significantly complicates the process of policy ranking and selection in real-world settings. We formally consider offline policy selection as learning preferences over a set of policy prospects given a fixed experience dataset. While one can select or rank policies based on point estimates of their expected values or high-confidence intervals, access to the full distribution over one's belief of the policy value enables more flexible selection algorithms under a wider range of downstream evaluation metrics. We propose a Bayesian approach for estimating this belief distribution in terms of posteriors of distribution correction ratios derived from stochastic constraints. Empirically, despite being Bayesian, the credible intervals obtained are competitive with state-of-the-art frequentist approaches in confidence interval estimation. More importantly, we show how the belief distribution may be used to rank policies with respect to arbitrary downstream policy selection metrics, and empirically demonstrate that this selection procedure significantly outperforms existing approaches, such as ranking policies according to mean or high-confidence lower bound value estimates.",
        "DOI": "NA",
        "paper_author": "Yang M.",
        "affiliation_name": "Google LLC",
        "affiliation_city": "Mountain View",
        "affiliation_country": "United States",
        "affiliation_id": "60006191",
        "affiliation_state": "CA"
    },
    {
        "paper_title": "Optimal Power Control for Over-The-Air Federated Edge Learning Using Statistical Channel Knowledge",
        "publication": "2022 IEEE 14th International Conference on Wireless Communications and Signal Processing, WCSP 2022",
        "citied_by": "3",
        "cover_date": "2022-01-01",
        "Abstract": "Over-the-air federated edge learning (OTA-FEEL) is an efficient distributed machine learning framework in terms of radio resource requirements. Delicate power control is needed to combat the distortion of the local models caused by channel fading. This paper develops the optimal power control policy under fading channels, to minimize the optimality gap of OTA-FEEL by exploiting the statistical channel state information (CSI), i.e., the mean and variance of the fading. We reveal that the optimal power control policy takes a form where the variance of the effective channel between the server and devices should be minimized when its mean is given. Based on this structure, we propose to iteratively minimize the variance using the Lagrange-duality method and then optimize the mean of the effective channels using a one-dimensional search. A closed-form expression for the optimal power control is derived. Simulations confirm the benefit of the use of the statistical CSI, and the superiority of the proposed optimal power control policy to the existing approaches in the convergence of OTA-FEEL.",
        "DOI": "10.1109/WCSP55476.2022.10039388",
        "paper_author": "Yu X.",
        "affiliation_name": "Fudan University",
        "affiliation_city": "Shanghai",
        "affiliation_country": "China",
        "affiliation_id": "60009860",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Sentiment Classification of Financial Online Reviews Based on Machine Learning Algorithm",
        "publication": "Proceedings - 2022 International Conference on Education, Network and Information Technology, ICENIT 2022",
        "citied_by": "1",
        "cover_date": "2022-01-01",
        "Abstract": "Countries or governments can learn people's opinions or views on the economic situation by referring to financial online reviews, and the sentiment classification of reviews can also help them make financial decisions, which is conducive to mining a large amount of financial information with commercial value. Effective information is of great significance to the implementation of financial policies. This paper analyzes the sentiment tendency of financial online reviews into negative reviews, positive reviews and neutral reviews, analyzes the application of support vector machine and naive Bayes classifier in review sentiment classification, and compares the performance of these two machine learning algorithms. Classification accuracy, the experimental results show that the machine learning algorithm has better classification effect on negative and positive reviews, and the classification effect on neutral reviews is average.",
        "DOI": "10.1109/ICENIT57306.2022.00070",
        "paper_author": "Guan D.",
        "affiliation_name": "Nanning University",
        "affiliation_city": "Nanning",
        "affiliation_country": "China",
        "affiliation_id": "60264278",
        "affiliation_state": "Guangxi"
    },
    {
        "paper_title": "4th International Conference on Machine Learning, Image Processing, Network Security and Data Sciences, MIND 2022",
        "publication": "Communications in Computer and Information Science",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "The proceedings contain 64 papers. The special focus in this conference is on Machine Learning, Image Processing, Network Security and Data Sciences. The topics include: Sensorless Control Algorithm of Permanent Magnet Synchronous Motor on Account of Neural Network; a Techno Aid to Ease in e-Rehabilitation; a Novel Approach to Analyse Lung Cancer Progression and Metastasis Using Page Rank Technique; homomorphic Encryption of Neural Networks; an Empirical Study to Enhance the Accuracy of an Ensemble Learning Model for Crop Recommendation System by Using Bit-Fusion Algorithm; bayesian Learning Model for Predicting Stability of System with Nonlinear Characteristics; Novel ABC: Aspect Based Classification of Sentiments Using Text Mining for COVID-19 Comments; topic Modeling, Sentiment Analysis and Text Summarization for Analyzing News Headlines and Articles; bioBodyComp: A Machine Learning Approach for Estimation of Percentage Body Fat; a Computational Approach to Identify Normal and Abnormal Persons Gait Using Various Machine Learning and Deep Learning Classifier; wearable Technology for Early Detection of Hyperthermia Using Machine Learning; intelligent Evaluation Framework of English Composition Based on Intelligent Algorithm; multimedia English Teaching System Based on Computer Information Technology; Correlation Analysis of Central Bank Communication Behavior and Monetary Policy Independence Based on VR Technology and Machine Learning; power Demand Data Analysis and Recovery for Management of Power Distribution Systems; Optimization Design of Green Building Landscape Space Environment Based on LM-BP Algorithm; design of Computational Thinking Intelligent Training System Under Big Data Technology; creative Graphic Design System Based on Multi-objective Firefly Algorithm; next Generation Ultra-sensitive Surface Plasmon Resonance Biosensors.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Robust Imitation Learning against Variations in Environment Dynamics",
        "publication": "Proceedings of Machine Learning Research",
        "citied_by": "10",
        "cover_date": "2022-01-01",
        "Abstract": "In this paper, we propose a robust imitation learning (IL) framework that improves the robustness of IL when environment dynamics are perturbed. The existing IL framework trained in a single environment can catastrophically fail with perturbations in environment dynamics because it does not capture the situation that underlying environment dynamics can be changed. Our framework effectively deals with environments with varying dynamics by imitating multiple experts in sampled environment dynamics to enhance the robustness in general variations in environment dynamics. In order to robustly imitate the multiple sample experts, we minimize the risk with respect to the Jensen-Shannon divergence between the agent's policy and each of the sample experts. Numerical results show that our algorithm significantly improves robustness against dynamics perturbations compared to conventional IL baselines.",
        "DOI": "NA",
        "paper_author": "Chae J.",
        "affiliation_name": "Korea Advanced Institute of Science and Technology",
        "affiliation_city": "Daejeon",
        "affiliation_country": "South Korea",
        "affiliation_id": "60032144",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "4th International Conference on Machine Learning, Image Processing, Network Security and Data Sciences, MIND 2022",
        "publication": "Communications in Computer and Information Science",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "The proceedings contain 64 papers. The special focus in this conference is on Machine Learning, Image Processing, Network Security and Data Sciences. The topics include: Sensorless Control Algorithm of Permanent Magnet Synchronous Motor on Account of Neural Network; a Techno Aid to Ease in e-Rehabilitation; a Novel Approach to Analyse Lung Cancer Progression and Metastasis Using Page Rank Technique; homomorphic Encryption of Neural Networks; an Empirical Study to Enhance the Accuracy of an Ensemble Learning Model for Crop Recommendation System by Using Bit-Fusion Algorithm; bayesian Learning Model for Predicting Stability of System with Nonlinear Characteristics; Novel ABC: Aspect Based Classification of Sentiments Using Text Mining for COVID-19 Comments; topic Modeling, Sentiment Analysis and Text Summarization for Analyzing News Headlines and Articles; bioBodyComp: A Machine Learning Approach for Estimation of Percentage Body Fat; a Computational Approach to Identify Normal and Abnormal Persons Gait Using Various Machine Learning and Deep Learning Classifier; wearable Technology for Early Detection of Hyperthermia Using Machine Learning; intelligent Evaluation Framework of English Composition Based on Intelligent Algorithm; multimedia English Teaching System Based on Computer Information Technology; Correlation Analysis of Central Bank Communication Behavior and Monetary Policy Independence Based on VR Technology and Machine Learning; power Demand Data Analysis and Recovery for Management of Power Distribution Systems; Optimization Design of Green Building Landscape Space Environment Based on LM-BP Algorithm; design of Computational Thinking Intelligent Training System Under Big Data Technology; creative Graphic Design System Based on Multi-objective Firefly Algorithm; next Generation Ultra-sensitive Surface Plasmon Resonance Biosensors.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "On Reinforcement Learning and Distribution Matching for Fine-Tuning Language Models with no Catastrophic Forgetting",
        "publication": "Advances in Neural Information Processing Systems",
        "citied_by": "17",
        "cover_date": "2022-01-01",
        "Abstract": "The availability of large pre-trained models is changing the landscape of Machine Learning research and practice, moving from a “training from scratch” to a “fine-tuning” paradigm. While in some applications the goal is to “nudge” the pre-trained distribution towards preferred outputs, in others it is to steer it towards a different distribution over the sample space. Two main paradigms have emerged to tackle this challenge: Reward Maximization (RM) and, more recently, Distribution Matching (DM). RM applies standard Reinforcement Learning (RL) techniques, such as Policy Gradients, to gradually increase the reward signal. DM prescribes to first make explicit the target distribution that the model is fine-tuned to approximate. Here we explore the theoretical connections between the two paradigms, and show that methods such as KL-control developed for RM can also be construed as belonging to DM. We further observe that while DM differs from RM, it can suffer from similar training difficulties, such as high gradient variance. We leverage connections between the two paradigms to import the concept of baseline into DM methods. We empirically validate the benefits of adding a baseline on an array of controllable language generation tasks such as constraining topic, sentiment, and gender distributions in texts sampled from a language model. We observe superior performance in terms of constraint satisfaction, stability and sample efficiency.",
        "DOI": "NA",
        "paper_author": "Korbak T.",
        "affiliation_name": "University of Sussex",
        "affiliation_city": "Brighton",
        "affiliation_country": "United Kingdom",
        "affiliation_id": "60017317",
        "affiliation_state": "East Sussex"
    },
    {
        "paper_title": "Improving Architectural Reusability for Resource Allocation Framework in Futuristic Cloud Computing Using Decision Tree Based Multi-objective Automated Approach",
        "publication": "Communications in Computer and Information Science",
        "citied_by": "8",
        "cover_date": "2022-01-01",
        "Abstract": "Cloud computing is a highly popular computing technique. Cloud combined with IoT, fog, edge, and mist computing in 5G networks gives us realtime and highly predictive responses leading to a better and smart life. It requires a highly robust and integrated cloud administration, especially cloud resource allocation. Artificial intelligence and machine learning can be easily implemented along cloud design patterns for efficient resource allocation. In this paper we discuss multi-tenant cloud resource allocation problem. We propose to use a rule-based analysis pattern to dynamically reconfigure resource allocation processes. The pattern uses various attributes of clouds, resources, subscribers and requests along with heuristic data like configurations, policies, strategies, and methods to efficiently identify and apply rule of allocation. We implemented a decision tree to assist pattern to have automated decisions, which rule to follow. The pattern caters for multi-objectivity, simplifies architecture, enables the extension of the cloud framework and makes it possible to interact easily with cloud. This paper describes the architectural framework pattern, which learns from itself. This paper presents CK’s object-oriented metrics comparisons of pattern-based object-oriented code. The comparison shows that object-oriented code improves code quality, making pattern-based code more maintainable, flexible, extendable and secure.",
        "DOI": "10.1007/978-3-031-23092-9_32",
        "paper_author": "Godhrawala H.",
        "affiliation_name": "Marwadi University",
        "affiliation_city": "Rajkot",
        "affiliation_country": "India",
        "affiliation_id": "60138770",
        "affiliation_state": "GJ"
    },
    {
        "paper_title": "Application of Data-driven Method for Automatic Machine Learning in Economic Research",
        "publication": "Proceedings - 2022 21st International Symposium on Distributed Computing and Applications for Business Engineering and Science, DCABES 2022",
        "citied_by": "3",
        "cover_date": "2022-01-01",
        "Abstract": "At present, the role of machine learning in data analysis is becoming increasingly important, and the digital economy has become the major economic form in the world, as well as the core driving force for China's economic development. Machine learning plays an increasingly significant role in economic research based on big data. To reduce the difficulty of using machine learning and improve the efficiency of machine learning, this paper systematically studies the application of automated machine learning (Au-toML) in economic research, focusing on the principles and characteristics of data-driven automated machine learning. Through the experimental comparison of specific automated machine learning methods on the classification of data sets, the optimal applicable method is found. Data-driven automated machine learning can be effectively applied in economic data mining, economic indicator analysis, and policy evaluation.",
        "DOI": "10.1109/DCABES57229.2022.00019",
        "paper_author": "Wang W.",
        "affiliation_name": "China University of Mining and Technology",
        "affiliation_city": "Xuzhou",
        "affiliation_country": "China",
        "affiliation_id": "60073460",
        "affiliation_state": "Jiangsu"
    },
    {
        "paper_title": "Correlation Analysis of Central Bank Communication Behavior and Monetary Policy Independence Based on VR Technology and Machine Learning",
        "publication": "Communications in Computer and Information Science",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Maintaining independence of monetary policy based on VR technology and machine learning is an effective guarantee for the central bank to flexibly use macroeconomic control measures to achieve domestic economic goals. In-depth exploration of the influence of the choice of the RMB exchange rate system on the independence of China's monetary policy has important practical significance for China to choose a suitable exchange rate system and enhance the central bank's macroeconomic control capabilities. This paper combs, analyzes and reviews the domestic and foreign research literature related to the development of machine learning theory and the application of adaptive learning in the research of optimal monetary policy. It points out the explanation of adaptive learning in inflation fluctuation and persistence, provides an explanation closer to the real economy for expectation management, and provides a new perspective for the research of the central bank's optimal monetary policy. The experimental analysis results show that this paper uses the hybrid Phillips curve to regress the price policy issued by the People’s Bank of China, and shows that the residual ADF value after regression of each model is less than the ADF critical value-1.95 at the 5% significance level, indicating The impact of inflation expectations under adaptive learning on inflation is significantly higher than the expected price index of the People's Bank of China.",
        "DOI": "10.1007/978-3-031-24352-3_23",
        "paper_author": "Gao F.",
        "affiliation_name": "Shaanxi University of Science and Technology",
        "affiliation_city": "Xinyang",
        "affiliation_country": "China",
        "affiliation_id": "60069726",
        "affiliation_state": "Shaanxi"
    },
    {
        "paper_title": "Finding Hidden Patterns in High Resolution Wind Flow Model Simulations",
        "publication": "Communications in Computer and Information Science",
        "citied_by": "2",
        "cover_date": "2022-01-01",
        "Abstract": "Wind flow data is critical in terms of investment decisions and policy making. High resolution data from wind flow model simulations serve as a supplement to the limited resource of original wind flow data collection. Given the large size of data, finding hidden patterns in wind flow model simulations are critical for reducing the dimensionality of the analysis. In this work, we first perform dimension reduction with two autoencoder models: the CNN-based autoencoder (CNN-AE) [1], and hierarchical autoencoder (HIER-AE) [2], and compare their performance with the Principal Component Analysis (PCA). We then investigate the super-resolution of the wind flow data. By training a Generative Adversarial Network (GAN) with 300 epochs, we obtained a trained model with resolution enhancement. We compare the results of GAN with Convolutional Neural Network (CNN), and GAN results show finer structure as expected in the data field images. Also, the kinetic energy spectra comparisons show that GAN outperforms CNN in terms of reproducing the physical properties for high wavenumbers and is critical for analysis where high-wavenumber kinetics play an important role.",
        "DOI": "10.1007/978-3-031-23606-8_22",
        "paper_author": "Wang T.",
        "affiliation_name": "Brookhaven National Laboratory",
        "affiliation_city": "Upton",
        "affiliation_country": "United States",
        "affiliation_id": "60006221",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Enhanced Bilevel Optimization via Bregman Distance",
        "publication": "Advances in Neural Information Processing Systems",
        "citied_by": "8",
        "cover_date": "2022-01-01",
        "Abstract": "Bilevel optimization has been recently used in many machine learning problems such as hyperparameter optimization, policy optimization, and meta learning. Although many bilevel optimization methods have been proposed, they still suffer from the high computational complexities and do not consider the more general bilevel problems with nonsmooth regularization. In the paper, thus, we propose a class of enhanced bilevel optimization methods with using Bregman distance to solve bilevel optimization problems, where the outer subproblem is nonconvex and possibly nonsmooth, and the inner subproblem is strongly convex. Specifically, we propose a bilevel optimization method based on Bregman distance (BiO-BreD) to solve deterministic bilevel problems, which achieves a lower computational complexity than the best known results. Meanwhile, we also propose a stochastic bilevel optimization method (SBiO-BreD) to solve stochastic bilevel problems based on stochastic approximated gradients and Bregman distance. Moreover, we further propose an accelerated version of SBiO-BreD method (ASBiO-BreD) using the variance-reduced technique, which can achieve a lower computational complexity than the best known computational complexities with respect to condition number κ and target accuracy ϵ for finding an ϵ-stationary point. We conduct data hyper-cleaning task and hyper-representation learning task to demonstrate that our new algorithms outperform related bilevel optimization approaches.",
        "DOI": "NA",
        "paper_author": "Huang F.",
        "affiliation_name": "Swanson School of Engineering",
        "affiliation_city": "Pittsburgh",
        "affiliation_country": "United States",
        "affiliation_id": "60159192",
        "affiliation_state": "PA"
    },
    {
        "paper_title": "Benchmarking FedAvg and FedCurv for Image Classification Tasks",
        "publication": "CEUR Workshop Proceedings",
        "citied_by": "5",
        "cover_date": "2022-01-01",
        "Abstract": "Classic Machine Learning (ML) techniques require training on data available in a single data lake (either centralized or distributed). However, aggregating data from different owners is not always convenient for different reasons, including security, privacy and secrecy. Data carry a value that might vanish when shared with others; the ability to avoid sharing the data enables industrial applications where security and privacy are of paramount importance, making it possible to train global models by implementing only local policies which can be run independently and even on air-gapped data centres. Federated Learning (FL) is a distributed machine learning approach which has emerged as an effective way to address privacy concerns by only sharing local AI models while keeping the data decentralized. Two critical challenges of Federated Learning are managing the heterogeneous systems in the same federated network and dealing with real data, which are often not independently and identically distributed (non-IID) among the clients. In this paper, we focus on the second problem, i.e., the problem of statistical heterogeneity of the data in the same federated network. In this setting, local models might be strayed far from the local optimum of the complete dataset, thus possibly hindering the convergence of the federated model. Several Federated Learning algorithms, such as FedAvg, FedProx and Federated Curvature (FedCurv), aiming at tackling the non-IID setting, have already been proposed. This work provides an empirical assessment of the behaviour of FedAvg and FedCurv in common non-IID scenarios. Results show that the number of epochs per round is an important hyper-parameter that, when tuned appropriately, can lead to significant performance gains while reducing the communication cost. As a side product of this work, we release the non-IID version of the datasets we used so to facilitate further comparisons from the FL community.",
        "DOI": "NA",
        "paper_author": "Casella B.",
        "affiliation_name": "Università degli Studi di Torino",
        "affiliation_city": "Turin",
        "affiliation_country": "Italy",
        "affiliation_id": "60012259",
        "affiliation_state": "TO"
    },
    {
        "paper_title": "A Model-Based Approach for Voltage and State-of-Charge Estimation of Lithium-ion Batteries",
        "publication": "Proceedings - 2022 IEEE Sustainable Power and Energy Conference, iSPEC 2022",
        "citied_by": "4",
        "cover_date": "2022-01-01",
        "Abstract": "Electric vehicles are equipped with a large number of lithium-ion battery cells. To achieve superior performance and guarantee safety and longevity, there is a fundamental requirement for a Battery Management System (BMS). In the BMS, accurate prediction of the State-of-Charge (SOC) is a crucial task. The SOC information is needed for monitoring, controlling, and protecting the battery, e.g. to avoid hazardous over-charging or over-discharging. Nonetheless, the SOC is an internal cell variable and cannot be straightforwardly obtained. This paper presents a Kalman Filter (KF) approach based on an optimized second-order Rc equivalent circuit model to carefully account for model parameter changes. An effective machine learning technique based on Proximal Policy optimization (PPO) is applied to train the algorithm. The results confirm the high robustness of the proposed method to varying operating conditions.",
        "DOI": "10.1109/iSPEC54162.2022.10032998",
        "paper_author": "Andalibi M.",
        "affiliation_name": "University of Zagreb, Faculty of Electrical Engineering and Computing",
        "affiliation_city": "Zagreb",
        "affiliation_country": "Croatia",
        "affiliation_id": "60159855",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Adaptive Federated Learning for Electric Power Inspection with UAV System",
        "publication": "CEUR Workshop Proceedings",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "With the rapid development of the national power grid, the demand for efficient and reliable power supply is increasing. As the labor cost and the size of the power grid are increasing, the Unmanned Aerial Vehicle (UAV) power inspection is a new and efficient way of detecting power grid abrasion. By analyzing the data collected by the UAVs, a smart detection and maintenance service can be provided. To improve the model robustness and accuracy, data collected from different companies and regions are required, which may violate the data privacy policy. As a distributed machine learning technique, Federated Learning (FL) can collaboratively train global models without sharing private data. In this article, in order to protect the data privacy between different systems, optimize models’ accuracy and convergence performance for non-Independently-and-Identically-Distributed (non-IID) data, we propose an adaptive method that jointly adjusts the learning rate and gradient based on the idea of FL. By recording global gradient information and using the momentum to accelerate the training process, our method adaptively controls the local gradient and learning rate in training of local models, and be more robust to local minima. Finally, we verify the superiority of our model compared with the generic FL model for non-IDD data through experiments.",
        "DOI": "NA",
        "paper_author": "Liang Y.",
        "affiliation_name": "Sun Yat-Sen University",
        "affiliation_city": "Guangzhou",
        "affiliation_country": "China",
        "affiliation_id": "60021182",
        "affiliation_state": "Guangdong"
    },
    {
        "paper_title": "Expert System for Kubernetes Cluster Autoscaling and Resource Management",
        "publication": "4th International Conference on Advancements in Computing, ICAC 2022 - Proceeding",
        "citied_by": "3",
        "cover_date": "2022-01-01",
        "Abstract": "The importance of orchestration tools such as Kubernetes has become paramount with the popularity of software architectural styles such as microservices. Furthermore, advancements in containerization technologies such as Docker has also played a vital role when it comes to advancements in the field of DevOps, enabling developers and system engineers to deploy are manage applications much more effectively. However, infrastructure configuration and management of resources are still challenging due to the disjointed nature of the infrastructure and resource management tools' failure to comprehend the deployed applications and create a holistic view of the services. This is partly due to the extensive knowledge required to operate these tools or due to the inability to perform specific tasks. As a result, multiple tools and platforms need to configure together to automate the deployment, monitoring and management processes to provide the optimal deployment strategy for the applications. In response to this issue, this research proposes an expert system that creates a centralized approach to cluster autoscaling and resource management, which also provides an automated low-latency container management system and resiliency evaluation for dynamic systems. Furthermore, the time series load prediction is done using a BiLSTM and periodically creates an optimized autoscaling policy for cluster performance, thus creating a seamless pipeline from deployment, monitoring scaling, and troubleshooting of distributed applications based on Kubernetes.",
        "DOI": "10.1109/ICAC57685.2022.10025077",
        "paper_author": "Hettiarachchi L.S.",
        "affiliation_name": "Sri Lanka Institute of Information Technology",
        "affiliation_city": "Colombo",
        "affiliation_country": "Sri Lanka",
        "affiliation_id": "60104431",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Operating energy storage to reduce Australia's grid emissions",
        "publication": "Proceedings - 2022 IEEE Sustainable Power and Energy Conference, iSPEC 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "Energy storage will play an essential role in meeting current Commonwealth policy emissions targets. Using two commonly compared metrics in literature, the marginal and average emissions factors (MEF & AEF), this paper looks to quantify and forecast Australia's grid emissions when energy storage is increased. Forecasted MEF values, based on a random forest machine learning algorithm, has an accuracy of 87.68% for 5-minute MEF intervals. The AEF is directly calculated based on current and future generation mix. In 2021, the AEF is seen, on average, higher than the MEF. In 2030, due to the increasing percentage of renewables in the grid, the AEF is seen to be, on average, lower than the forecast MEF. Of the two metrics, it can be seen that MEF is more representative of the impact an intervention has on the electricity network and, therefore, is more appropriate for short-term optimisation of energy storage in the National Electricity Market (NEM). Using MEF, optimising energy storage to minimise emissions reduced grid emissions by 29% compared to optimising energy storage to minimise cost for 2021, and similarly 19% in 2030. This translates to a carbon offset price per tonne of 6.05 & 9.45 respectively.",
        "DOI": "10.1109/iSPEC54162.2022.10033018",
        "paper_author": "Anavatti S.",
        "affiliation_name": "The Australian National University",
        "affiliation_city": "Canberra",
        "affiliation_country": "Australia",
        "affiliation_id": "60008950",
        "affiliation_state": "ACT"
    },
    {
        "paper_title": "2022 IEEE International Symposium on Technologies for Homeland Security, HST 2022",
        "publication": "2022 IEEE International Symposium on Technologies for Homeland Security, HST 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "The proceedings contain 39 papers. The topics discussed include: a distributed privacy-preserving integrity verification framework for the smart grid; complex economic consequence analysis to protect the maritime infrastructure; juvenile morph dataset: a study of attack detectability and recognition vulnerability; machine learning models for network traffic classification in programmable logic; defining time-varying metrics of task conflict in human/robot teams using simulated agents; edge-based infrared-ultrasonic anti-collision radar system for robotic navigation: applications of cost-effective bisensory system for obstacle detection, tracking, and avoidance; urban resilience capabilities: applying capability analysis to improve local policy planning; low-size and cost acoustic buoy for autonomous vessel detection; human trafficking interdiction problem: a data driven approach to modeling and analysis; analytics for cybersecurity policy of cyber-physical systems; and impact-driven sampling strategies for hybrid attack graphs.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Automated Dynamic Algorithm Configuration",
        "publication": "Journal of Artificial Intelligence Research",
        "citied_by": "19",
        "cover_date": "2022-01-01",
        "Abstract": "The performance of an algorithm often critically depends on its parameter configuration. While a variety of automated algorithm configuration methods have been proposed to relieve users from the tedious and error-prone task of manually tuning parameters, there is still a lot of untapped potential as the learned configuration is static, i.e., parameter settings remain fixed throughout the run. However, it has been shown that some algorithm parameters are best adjusted dynamically during execution. Thus far, this is most commonly achieved through hand-crafted heuristics. A promising recent alternative is to automatically learn such dynamic parameter adaptation policies from data. In this article, we give the first comprehensive account of this new field of automated dynamic algorithm configuration (DAC), present a series of recent advances, and provide a solid foundation for future research in this field. Specifically, we (i) situate DAC in the broader historical context of AI research; (ii) formalize DAC as a computational problem; (iii) identify the methods used in prior art to tackle this problem; and (iv) conduct empirical case studies for using DAC in evolutionary optimization, AI planning, and machine learning.",
        "DOI": "10.1613/JAIR.1.13922",
        "paper_author": "Adriaensen S.",
        "affiliation_name": "Universität Freiburg",
        "affiliation_city": "Freiburg im Breisgau",
        "affiliation_country": "Germany",
        "affiliation_id": "60025641",
        "affiliation_state": "Baden-Wurttemberg"
    },
    {
        "paper_title": "An Adversarial Reinforcement Learning Framework for Robust Machine Learning-based Malware Detection",
        "publication": "IEEE International Conference on Data Mining Workshops, ICDMW",
        "citied_by": "3",
        "cover_date": "2022-01-01",
        "Abstract": "Empowered by the recent development in Ma-chine Learning (ML), signatureless ML-based malware detectors present promising performance in identifying unseen mal ware variants and zero days without requiring expensive dynamic malware analysis. However, it has been recently shown that ML-based malware detectors are vulnerable to adversarial malware attacks, in which an attacker modifies a known malware exe-cutable to trick the malware detector into recognizing the modi-fied variant as benign. Adversarial malware example generation has become an emerging area in adversarial ML that studies creating functionality-preserving adversarial malware variants. Advancements in this area have led to an eternal game between the adversary and defender. While the area has attracted much attention in the security community, a large body of these studies merely focuses on attack methods against ML-based malware detectors. There has been little work on understanding how these adversarial variants can be systematically used by the defender to strengthen the robustness of these detectors and stand ahead of the adversary. Latest efforts have led to emergence of adversarial learning. In this work, we propose a simple wargame approach to empirically conduct the adversarial minimax optimization underlying in the adversarial learning for improving the robustness of ML-based malware detectors. Our proposed approach employs adversarial malware variants generated from a reinforcement learning-based adversarial attack policy in a minimax game alternating between strengthening the attack policy and improving the detectors' robustness. We evaluated the effectiveness of our approach on a testbed with 33.2 GB working malware collected from VirusTotal. Despite the sub-optimal nature of our method, it was able to surprisingly enhance the robustness of three known open-source ML-based malware detectors (LGBM, MalConv, and NonNeg) against the adversarial malware variants by 4, 7, and 11 times, respectively.",
        "DOI": "10.1109/ICDMW58026.2022.00079",
        "paper_author": "Ebrahimi M.R.",
        "affiliation_name": "University of South Florida, Tampa",
        "affiliation_city": "Tampa",
        "affiliation_country": "United States",
        "affiliation_id": "60007740",
        "affiliation_state": "FL"
    },
    {
        "paper_title": "Land Use Land Cover Classification using Machine Learning",
        "publication": "International Conference on Automation, Computing and Renewable Systems, ICACRS 2022 - Proceedings",
        "citied_by": "5",
        "cover_date": "2022-01-01",
        "Abstract": "Due to rapid urbanization processes and population exploitation, urban sprawl becomes a challenging task for urban planners. For planning authorities and their decision-making process, particularly in developing nations throughout the late 20th and early 21st centuries, Land Cover Land Use (LULC) is one of the most crucial pieces of information. Instead of doing it manually, the paper's goal is to estimate the LULC Classification using artificial intelligence approaches. The Google Earth Engine (GEE) cloud computing is utilized to easily retrieve satellite photos for this purpose [1]. The suggested approach will make the process of image classification easier so that different land use types may be identified and watched for urbanization. Classification and Regression Trees (CART), a supervised machine learning (ML) technique, is used to perform the classification [2]. Additionally, metrics like classification accuracy, precision and Kappa coefficient are assessed to support the conclusions. The results of the classification revealed a high accuracy of 92.9%. The LULC classification results can be used as a starting point for additional research on a variety of topics, such as river morphology change analysis, ecosystem services analysis, land use policy formulation, management of water resources, management of other natural resources, urbanization, etc. [3].",
        "DOI": "10.1109/ICACRS55517.2022.10029176",
        "paper_author": "Waghela H.",
        "affiliation_name": "University of Mumbai",
        "affiliation_city": "Mumbai",
        "affiliation_country": "India",
        "affiliation_id": "60025929",
        "affiliation_state": "MH"
    },
    {
        "paper_title": "Zero-Sum Stochastic Stackelberg Games",
        "publication": "Advances in Neural Information Processing Systems",
        "citied_by": "5",
        "cover_date": "2022-01-01",
        "Abstract": "Zero-sum stochastic games have found important applications in a variety of fields, from machine learning to economics. Work on this model has primarily focused on the computation of Nash equilibrium due to its effectiveness in solving adversarial board and video games. Unfortunately, a Nash equilibrium is not guaranteed to exist in zero-sum stochastic games when the payoffs at each state are not convex-concave in the players' actions. A Stackelberg equilibrium, however, is guaranteed to exist. Consequently, in this paper, we study zero-sum stochastic Stackelberg games. Going beyond known existence results for (non-stationary) Stackelberg equilibria, we prove the existence of recursive (i.e., Markov perfect) Stackelberg equilibria (recSE) in these games, provide necessary and sufficient conditions for a policy profile to be a recSE, and show that recSE can be computed in (weakly) polynomial time via value iteration. Finally, we show that zero-sum stochastic Stackelberg games can model the problem of pricing and allocating goods across agents and time. More specifically, we propose a zero-sum stochastic Stackelberg game whose recSE correspond to the recursive competitive equilibria of a large class of stochastic Fisher markets. We close with a series of experiments that showcase how our methodology can be used to solve the consumption-savings problem in stochastic Fisher markets.",
        "DOI": "NA",
        "paper_author": "Goktas D.",
        "affiliation_name": "Department of Computer Science",
        "affiliation_city": "Providence",
        "affiliation_country": "United States",
        "affiliation_id": "60280409",
        "affiliation_state": "RI"
    },
    {
        "paper_title": "Reinforcement Learning Control of an Aerial Robot Based on a Tuned Proximal Policy Optimization in Takeoff and Hover Phases",
        "publication": "10th RSI International Conference on Robotics and Mechatronics, ICRoM 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "This paper is dedicated to the implementation of a control system for a flying robot based on reinforcement learning (RL); In which an agent learns its mission based on trial and error. The agent needs to search the environment and learn the appropriate behavior by receiving a simulated reward. In this research, a quadcopter is trained as an agent. Conventional quadcopters use two separate controllers, one for direction and the other for position; However, in quadcopter control based on RL, an integrated control system can be considered. To develop the required platform for the preparation and testing of training data, at the first, the development of the 6DOF simulation environment for the quadcopter is conducted. Moreover, the performance of a tuned Proximal Policy Optimization (PPO) method in the control of the flying robot is investigated and analyzed during the take-off and hovering phases. The quality of the conducted training and their acceptable outputs based on the received rewards promises a capacity to use this control method under uncertainties and nonlinearities during specific flight conditions.",
        "DOI": "10.1109/ICRoM57054.2022.10025140",
        "paper_author": "Esfandiari M.",
        "affiliation_name": "University of Tehran",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran",
        "affiliation_id": "60022927",
        "affiliation_state": "Tehran"
    },
    {
        "paper_title": "Machine Learning Control of an Aerial Robot Based on a Tuned Deep Deterministic Policy Gradient Method: Proceedings of the 10th RSI International Conference on Robotics and Mechatronics",
        "publication": "10th RSI International Conference on Robotics and Mechatronics, ICRoM 2022",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "This paper is devoted to the development of a Machin Learning (ML) control for a quadcopter via a Tuned version of the Deep Deterministic Policy Gradient (DDPG) method. In this regard, the aerial robot explores the environment and acquires the appropriate performance by the reception of a reward in a simulated area. Normally quadcopters exploit two distinct controllers for direction and position control tasks. Based on Reinforcement Learning (RL), quadcopter control can be implemented by an integrated control system. Initially, a 6DOF simulation toolset of the aerial robot is developed for the generation of the required learning dataset. In this work, for take-off and hovering tasks, the performance of a tuned version of the DDPG method in controlling the vehicle is examined. The developed toolsets give a capacity for employment of this control method in other flight phases and more complicated missions.",
        "DOI": "10.1109/ICRoM57054.2022.10025345",
        "paper_author": "Esfandiari M.",
        "affiliation_name": "University of Tehran",
        "affiliation_city": "Tehran",
        "affiliation_country": "Iran",
        "affiliation_id": "60022927",
        "affiliation_state": "Tehran"
    },
    {
        "paper_title": "5th China Conference on Intelligent Networked Things, CINT 2022",
        "publication": "Communications in Computer and Information Science",
        "citied_by": "0",
        "cover_date": "2022-01-01",
        "Abstract": "The proceedings contain 45 papers. The special focus in this conference is on Intelligent Networked Things. The topics include: Joint Feature Fusion Approach to Human Pose Estimation Using mmWave Radar; question Answering Based on Entity-Aware Self-attention; TGNN: A GNN-Based Method with Multi-entity Node for Personal Banking Time Prediction; spatial-Temporal Adaptive Graph Convolution with Attention Network for Traffic Forecasting; sentiment Analysis of Online Travel Reviews Based on Capsule Network and Sentiment Lexicon; preface; prediction of the Melt Pool Size in Single-Layer Single-Channel Selective Laser Melting Based on Neural Network; A Sentiment Analysis Model for Car Review Texts Based on Adversarial Training and Whole Word Mask BERT; Bi-convolution Matrix Factorization Algorithm Based on Improved ConvMF; A Novel Defect Detection Method for Insulators of Power Transmission Line Based on YOLOv5; multi-objective Optimization Based Viscosity Prediction for Inks in Direct Ink Writing Numerical Simulations; a Meta-learning-Based Object Detection Method for Custom Grasping in Human-Robot Collaboration; a Knowledge-Embedding-Based Approach for Process-Decision Knowledge Mining; a Knowledge Content Matching Degree Calculation Method Supporting Process Knowledge Recommendation; fault-Tolerant Control for Underwater Vehicle with Actuator Saturation and Faults; event-Triggered Kernel Recursive Least Squares Algorithm; Distributed Fault-Tolerant Formation Control of Nonholonomic Mobile Robots with SSA Parameter Optimization; implementation of Underwater Vehicle Pipeline Inspection Based on Machine Vision; set-Membership Estimation for Nonlinear Parameter-Varying Systems; adaptive Synchronization of Fractional-Order Multiplex Networks via Quantized Control; synchronization in Fixed/Preassigned Time of Inertial Neural Networks with Time-Varying Delays; a Deep Reinforcement Learning Based Leader-Follower Control Policy for Swarm Systems; research on the Key Technology of Digital Twin-Driven Fracture Reduction Robot Force Control; a Data-driven Feedforward Control Design Method for Nonlinear Systems; tracking Error Boundary of Novel Stable Inversion Based Feedforward Control for a Class of Non-minimum Phase Systems.",
        "DOI": "NA",
        "paper_author": "NA",
        "affiliation_name": "NA",
        "affiliation_city": "NA",
        "affiliation_country": "NA",
        "affiliation_id": "NA",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Predictive Model for Early Detection of Mother’s Mode of Delivery with Feature Selection",
        "publication": "Delivering Distinctive Value in Emerging Economies: Efficient and Sustainably Responsible Perspectives from Management Researchers and Practitioners",
        "citied_by": "5",
        "cover_date": "2022-01-01",
        "Abstract": "At childbirth, a decision needs to be taken regarding the most suitable mode of delivery for mothers. Often, certain historical factors account for this decision, some of which are based on individuals’ personal choices. In this study, secondary data containing attributes and mode of child delivery were analyzed to predict the mother’s mode of delivery using machine learning techniques. We built a predictive model with four different machine learning algorithms where a recursive feature elimination technique was employed to rank the most important feature attributes. Our study shows that mother’s Length of Stay, their Number of Visits to the hospital, and the Number of Assisted Delivery Procedures emerged as the most important attributes for predicting the mode of delivery while Parity, Educational Level, and Location (residence) were the least important. We envision that these findings will guide policy and practitioners’ decisions toward the mode of child delivery of women in Nigeria. Target Audience This book chapter targets medical and healthcare professionals and practitioners, especially those associated with maternal and newborn health and pregnant women. The chapter seeks to guide the choice of delivery mode for expectant mothers to help reduce the rate of morbidity and mortality associated with childbirth. In making choices for the mode of delivery, length of stay, number of visits to hospital, and number of previous cesarean procedures on the expectant mother were found to be critical determinants. Thus, early detection and prediction of the right delivery mode will help avert possible complications, prevent, or reduce both maternal and child mortality.",
        "DOI": "10.4324/9781003152217-22",
        "paper_author": "Kolog E.A.",
        "affiliation_name": "University of Ghana",
        "affiliation_city": "Accra",
        "affiliation_country": "Ghana",
        "affiliation_id": "60017910",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Cold Start Problem in Recommendation System: A Solution Model Based on Clustering and Association Rule Techniques",
        "publication": "2022 5th International Conference on Multimedia, Signal Processing and Communication Technologies, IMPACT 2022",
        "citied_by": "3",
        "cover_date": "2022-01-01",
        "Abstract": "Generally cold start problem refers to the new user who are coming in big data as well it also related to new items which are included in any data set. Problem arises when companies or policy makers don't have the information about new user/item in recommendation of products. Recommendation system shows the user who is interested in particular product implementing contents based, collaborative based or hybrid approaches. Meanwhile recommender system faces the cold start issue which means the recommendation system is not recognizing the new product or new user. In another words the system doesn't have information about preferences in order to make recommendation. To overcome the cold start problem, in this paper we are suggesting a solution by combining clustering techniques and association rule. The study is based on extracts of different approached studies. This paper also lets you understand the proposed models to overcome the problem of cold start.",
        "DOI": "10.1109/IMPACT55510.2022.10029293",
        "paper_author": "Khatwal R.",
        "affiliation_name": "Sangam University",
        "affiliation_city": "Bhilwara",
        "affiliation_country": "India",
        "affiliation_id": "60115872",
        "affiliation_state": "RJ"
    },
    {
        "paper_title": "Data Challenges and Societal Impacts – The Case in Favor of the Blueprint for an AI Bill of Rights (Keynote Remarks)",
        "publication": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
        "citied_by": "2",
        "cover_date": "2022-01-01",
        "Abstract": "Artificial Intelligence (AI) technologies contribute tremendously to various areas of life and society. Therefore, we are witnessing massive investments in this area. Grand-view Research has calibrated the global AI market at 93.5 billion dollars as of 2021. Further, according to their report, it is expected to grow at a compound annual growth rate (CAGR) of 38.1% from 2022 to 2030. It is believed that nations that adopt and use AI will have a competitive edge. AI, in some form, will be part of most products and services we use. This manuscript documents some of the challenges, tradeoffs, and remedies concerning BIG DATA in the age of Artificial Intelligence. The paper also provides a brief overview of the maladies that plague the landscape of BIG DATA and some academic literature that provides solutions to the problems in the context of AI. The driving motivation for writing this article is to highlight our responsibility to create algorithms and automated systems that do not harm and are equitable and just. I also hope to create awareness that leads to businesses and software laboratories that focus on testing software and data that alleviates our fear - modeling the work of the US Food and Drug Administration. The tenets echoed in this concord with the Blueprint for an AI Bill of Rights released by the United States (US) White House Office of Science and Technology Policy (OSTP) on October 4, 2022, and the AI Risk Management Framework by the US National Institute of Standards and Technology.",
        "DOI": "10.1007/978-3-031-24094-2_1",
        "paper_author": "Sharman R.",
        "affiliation_name": "University at Buffalo, The State University of New York",
        "affiliation_city": "Buffalo",
        "affiliation_country": "United States",
        "affiliation_id": "60032083",
        "affiliation_state": "NY"
    },
    {
        "paper_title": "Student Engagement Detection Using Emotion Analysis, Eye Tracking and Head Movement with Machine Learning",
        "publication": "Communications in Computer and Information Science",
        "citied_by": "40",
        "cover_date": "2022-01-01",
        "Abstract": "With the increase of distance learning, in general, and e-learning, in particular, having a system capable of determining the engagement of students is of primordial importance, and one of the biggest challenges, both for teachers, researchers and policy makers. Here, we present a system to detect the engagement level of the students. It uses only information provided by the typical built-in web-camera present in a laptop computer, and was designed to work in real time. We combine information about the movements of the eyes and head, and facial emotions to produce a concentration index with three classes of engagement: “very engaged”, “nominally engaged” and “not engaged at all”. The system was tested in a typical e-learning scenario, and the results show that it correctly identifies each period of time where students were “very engaged”, “nominally engaged” and “not engaged at all”. Additionally, the results also show that the students with best scores also have higher concentration indexes.",
        "DOI": "10.1007/978-3-031-22918-3_5",
        "paper_author": "Sharma P.",
        "affiliation_name": "University of Massachusetts Boston",
        "affiliation_city": "Boston",
        "affiliation_country": "United States",
        "affiliation_id": "60024063",
        "affiliation_state": "MA"
    },
    {
        "paper_title": "Artificial Intelligence Based Smart Government Enterprise Architecture (AI-SGEA) Framework",
        "publication": "Communications in Computer and Information Science",
        "citied_by": "4",
        "cover_date": "2022-01-01",
        "Abstract": "Artificial Intelligence (AI) based Government Enterprise Architecture (GEA) is the inherent AI & Machine Learning (ML) based design and management approach essential across the Government processes coherence leading to alignment, agility, quick decision, effective analysis and efficient assurance. Governments and corporates are collecting an abundant amount of data every day and without an accurate analysis, data is not adequate for actionable insights. Better decision making has the potential to both improve services and save costs. The expanding use of Smart Government Enterprise Architecture (SGEA) framework in government is triggering numerous opportunities for governments worldwide. This framework may also be used by organizations to generate more accurate future prediction and to simulate complex systems that allow experimentation with various policy options. In any Government change management process, the ensuring of interoperability amongst various e-Governance systems, processes and applications across all the Government reference model for analytics is very important. The IT policymakers and the top level decision makers of any country are not concern with how each parameter introduces SGEA deliverables, but concern how national IT strategy is aligned and populated in the form of IT projects, information systems, and other IT assets throughout all references of SGEA. This paper describes the concept framework of Artificial Intelligence driven integrated architecture flow of smart government system. The concept is suitable for the strategic planning and decision making process which may provide possible strategic directions for the organization. This may be able to transform and improve the services for an achievable adaptation efficiency, simplification, cost management, collaboration, and standardization. This may help the organizations to accommodate the rapidly changing digital service usability on digitalization known as “AI driven smart-Government.",
        "DOI": "10.1007/978-3-031-22485-0_30",
        "paper_author": "Mukherjee P.K.",
        "affiliation_name": "The Neotia University",
        "affiliation_city": "Sarisa",
        "affiliation_country": "India",
        "affiliation_id": "60285604",
        "affiliation_state": "WB"
    },
    {
        "paper_title": "Pattern-based Autotuning of OpenMP Loops using Graph Neural Networks",
        "publication": "Proceedings of AI4S 2022: Artificial Intelligence and Machine Learning for Scientific Applications, Held in conjunction with SC 2022: The International Conference for High Performance Computing, Networking, Storage and Analysis",
        "citied_by": "8",
        "cover_date": "2022-01-01",
        "Abstract": "Stagnation of Moore's law has led to the increased adoption of parallel programming for enhancing performance of scientific applications. Frequently occurring code and design patterns in scientific applications are often used for transforming serial code to parallel. But, identifying these patterns is not easy. To this end, we propose using Graph Neural Networks for modeling code flow graphs to identify patterns in such parallel code. Additionally, identifying the runtime parameters for best performing parallel code is also challenging. We propose a pattern-guided deep learning based tuning approach, to help identify the best runtime parameters for OpenMP loops. Overall, we aim to identify commonly occurring patterns in parallel loops and use these patterns to guide auto-tuning efforts. We validate our hypothesis on 20 different applications from Polybench, and STREAM benchmark suites. This deep learning-based approach can identify the considered patterns with an overall accuracy of 91%. We validate the usefulness of using patterns for auto-tuning on tuning the number of threads, scheduling policies and chunk size on a single socket system, and the thread count and affinity on a multi-socket machine. Our approach achieves geometric mean speedups of 1.1× and 4.7× respectively over default OpenMP configurations, compared to brute-force speedups of 1.27× and 4.93× respectively.",
        "DOI": "10.1109/AI4S56813.2022.00010",
        "paper_author": "Dutta A.",
        "affiliation_name": "Department of Computer Science",
        "affiliation_city": "Ames",
        "affiliation_country": "United States",
        "affiliation_id": "60145790",
        "affiliation_state": "IA"
    },
    {
        "paper_title": "Age of Information Optimization by Deep Reinforcement Learning for Random Access in Machine Type Communication",
        "publication": "Proceedings - 2022 IEEE International Conference on Big Data, Big Data 2022",
        "citied_by": "4",
        "cover_date": "2022-01-01",
        "Abstract": "For machine type communication with random access (RA) protocol, finding optimal policy using deep reinforcement learning (DRL) is being actively investigated for various quality of service requirements. In particular, it was shown that throughput-based reward function in DRL can derive policies that outperform conventional exponential backoff (EB)-based algorithms in terms of throughput and fairness. However, age of information (AoI), which is a measure of the freshness of data, was not addressed in the process of training the DRL agent and only used as a measure of fairness. It has been theoretically proven that, even in the simplest queuing system, maximizing throughput does not guarantee minimizing AoI. In this paper, we proposed a novel DRL scheme to directly optimize AoI in a slotted ALOHA RA channel. By taking into account urgency and packet age as extra local information for a reward, AoI could be improved while preserving the throughput. Numerical simulations showed that the proposed DRL approach could achieve an improvement of 22.04% in AoI performance, with a marginal throughput loss of around 3.95%, compared to the existing throughput-based DRL method.",
        "DOI": "10.1109/BigData55660.2022.10020660",
        "paper_author": "Jeong M.",
        "affiliation_name": "Gwangju Institute of Science and Technology",
        "affiliation_city": "Gwangju",
        "affiliation_country": "South Korea",
        "affiliation_id": "60068688",
        "affiliation_state": "NA"
    },
    {
        "paper_title": "Review of Publically Available Health Big Data Sets",
        "publication": "Proceedings - 2022 IEEE International Conference on Big Data, Big Data 2022",
        "citied_by": "4",
        "cover_date": "2022-01-01",
        "Abstract": "There is a growing interest in using public data for open government policy involving health informatics and healthcare systems. This paper investigated the characteristics of publically available data sets in health informatics that were derived from electronic health records (EHRs), healthcare systems, and a variety of open-government libraries, data marts, or data catalogues.Data used in this study consisted of public data sets that did not require any registration to access online. In total, nine web-based platforms on the Internet were used that included: British Columbia (BC) Data Catalogue, Canadian Institute for Health Information (CIHI), Harvard Dataverse, MIMIC-eICU, FigShare, GitHub, Google Dataset, UCI Machine Learning Repository, and Zenodo. Our initial search across these platforms found over 10,000 public use files that had data sets related to health informatics.We found 558 data sets that matched search criterion that ranged from years 2013-2022. The data source types were mostly found using the health informatics search filters followed by the combination of health informatics and healthcare systems, but fewer data sets were found when using EHR as the criterion. Almost 85% of the total data sets were from 2020-2022. The range of data sizes were 11KB to 7.8MB. The eICU (hosted by MIT's MIMIC data mart) platform had the largest data set followed by Zenodo, and GitHub. Additionally, any bioinformatics in the 558 data sets were excluded and further classification on the content and usability, and dashboard visualization towards experiential learning resulted in 117 data sets.Of these 117 data sets, we further tested their usability to graph and create a dashboard within 2-5 minutes of loading the data to Tableau",
        "DOI": "10.1109/BigData55660.2022.10020258",
        "paper_author": "Chrimes D.",
        "affiliation_name": "University of Victoria",
        "affiliation_city": "Victoria",
        "affiliation_country": "Canada",
        "affiliation_id": "60003122",
        "affiliation_state": "BC"
    },
    {
        "paper_title": "Optimizations of Modified Machine Learning Algorithms Using K-Fold Cross Validations for Wheat Productivity: A Hyper Parametric Approach",
        "publication": "Sarhad Journal of Agriculture",
        "citied_by": "2",
        "cover_date": "2022-01-01",
        "Abstract": "An optimized wheat crop productivity model can play a crucial role for evolving effective agricultural policy decisions for food concerns and trepidation. This study measures the efficacies of modified machine learning algorithms using multiple linear regression (MLR), decision tree regression (DTR) and random forest regression (RFR) for wheat productivity using 75% and 25% randomized partitions. The 26,430 field of wheat crop cut experiments (C.C.E) is taken from crop reporting service (CRS), Punjab for the years 2016-17 to 2019-2020. Three generated datasets (D2, D3 and D4) were used to optimize the model performance. The heat plot map shows very strong significance of correlation matrix for D3 and D4, while it was low for D1 and D2. The modified RFR produced lowest values of error for all the datasets, comparing with benchmark DTR and MLR (ErrorMLR > ErrorDTR > ErrorRFR). The modified RFR found best fitted model for the prediction of wheat productivity. The hyper parametric tuning K-Fold cross validation is applied to get the most optimized sub fold for the modified models. It is demonstrated that modified RFR provides superior performance as we advanced from D1 to D4. The results got best when it used D4 for random forest regression with the K Fold-6.",
        "DOI": "10.17582/JOURNAL.SJA/2022/38.5.271.278",
        "paper_author": "Shehzad F.",
        "affiliation_name": "The Islamia University of Bahawalpur",
        "affiliation_city": "Bahawalpur",
        "affiliation_country": "Pakistan",
        "affiliation_id": "60037241",
        "affiliation_state": "Punjab"
    },
    {
        "paper_title": "A Deep Reinforcement Learning Approach for Inventory Control under Stochastic Lead Time and Demand",
        "publication": "Proceedings of the 2022 IEEE Symposium Series on Computational Intelligence, SSCI 2022",
        "citied_by": "5",
        "cover_date": "2022-01-01",
        "Abstract": "In the last few years, the deep learning paradigm has experienced huge success in various machine learning research areas like computer vision, drug discoveries, natural language processing, and combinatorial optimizations. Moreover, the world has witnessed remarkable achievements when combining deep learning with reinforcement learning (now known as Deep Reinforcement Learning) in the areas like robotics, video games, business, and healthcare. One of the strongest parts of Deep Reinforcement Learning (DRL) is the ability to solve sequential decision-making problems. The inventory control problem is one such field where DRL can be applied to learn the optimal ordering policy to minimize the total inventory cost. In this paper, a linear supply chain model is considered with stochastic lead time and demand. The problem is then modeled into Markov Decision Processes (MDP). We then designed three different agents: Q-learning agent, Deep Q-network (DQN, also known as Deep Q-Learning), and (R, S) policy-based agent. The Q-learning and DQN agents were trained and evaluated. The (R, S) policy is used as a baseline as it is one of the most popular policies in business organizations. In comparison to traditional reinforcement learning (i.e Q-learning) and rule-based learning (i.e. (R, S) policy), the DQN model performs better in making the optimal ordering decision so that the total cost is minimized.",
        "DOI": "10.1109/SSCI51031.2022.10022256",
        "paper_author": "Shakya M.",
        "affiliation_name": "School of Computer Science and Engineering",
        "affiliation_city": "Singapore City",
        "affiliation_country": "Singapore",
        "affiliation_id": "60078616",
        "affiliation_state": "NA"
    }
]